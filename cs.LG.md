# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Improving Fairness-Accuracy tradeoff with few Test Samples under Covariate Shift.](http://arxiv.org/abs/2310.07535) | 在协变量转移下，我们提出了一种新的损失函数和表示匹配损失来优化模型的准确性和公平性，通过实验证明在公平性和准确性权衡方面优于其他基线算法，并且提出了一种未经研究的非对称协变量转移设置。 |
| [^2] | [Human-Centered Evaluation of XAI Methods.](http://arxiv.org/abs/2310.07534) | 在人工智能领域中，解释深度学习黑盒子的决策过程是一个关键挑战。本研究以用户为中心，客观评估了三种领先的解释方法的可解释性，并发现它们都提供了可解释的结果。 |
| [^3] | [Provable Advantage of Parameterized Quantum Circuit in Function Approximation.](http://arxiv.org/abs/2310.07528) | 通过分析参数化量子电路在函数逼近中的表达能力，本论文展示了数据重新上传PQCs的显式构造及其对连续和平滑函数逼近的定量误差界限。 |
| [^4] | [Exploiting Causal Graph Priors with Posterior Sampling for Reinforcement Learning.](http://arxiv.org/abs/2310.07518) | 本文提出了一种利用因果图先验和后验采样的方法来提高强化学习样本效率。通过同时学习完整的因果图和分解动态参数，该方法能够更自然地设计先验，并且根据先验知识程度连接了遗憾率与贝叶斯遗憾之间的关系。 |
| [^5] | [A Unified Remote Sensing Anomaly Detector Across Modalities and Scenes via Deviation Relationship Learning.](http://arxiv.org/abs/2310.07511) | 通过学习偏差关系，构建了一个统一的遥感异常检测器，可以跨模态和场景进行检测，并且具有灵活性和成本效益。 |
| [^6] | [Leveraging Hierarchical Feature Sharing for Efficient Dataset Condensation.](http://arxiv.org/abs/2310.07506) | 本文提出了一种利用分层特征共享的数据参数化架构（HMN），旨在更高效地压缩数据。通过将数据存储在三层结构中，HMN能够捕捉到数据集级别、类别级别和样本级别的特征。 |
| [^7] | [Sample-Driven Federated Learning for Energy-Efficient and Real-Time IoT Sensing.](http://arxiv.org/abs/2310.07497) | 本文提出了一种针对具有实时感知能力的IoT网络设计的基于样本驱动的联邦学习方法，通过控制采样过程来减轻过拟合问题，提高整体准确性，并解决能效问题。 |
| [^8] | [Model-based Clustering of Individuals' Ecological Momentary Assessment Time-series Data for Improving Forecasting Performance.](http://arxiv.org/abs/2310.07491) | 本文通过模型聚类分析个体生态瞬时评估数据，以提高预测性能。作者比较了两种基于模型的聚类方法，并使用内在聚类评估与性能测量方法进行了分析。 |
| [^9] | [KwaiYiiMath: Technical Report.](http://arxiv.org/abs/2310.07488) | KwaiYiiMath是一个用于增强数学推理能力的大型语言模型，通过应用监督微调和人类反馈强化学习，在英语和中文数学任务上取得了最先进的性能，并且能够正确解决生成的问题过程。 |
| [^10] | [Nonlinear embeddings for conserving Hamiltonians and other quantities with Neural Galerkin schemes.](http://arxiv.org/abs/2310.07485) | 本文提出了一种基于神经Galerkin方案的非线性嵌入方法，用于保持哈密顿量和其他数量的守恒性。该方法通过在每个时间步骤计算显式嵌入到非线性参数化解场的流形上来保证数量的守恒。 |
| [^11] | [Deep Learning Predicts Biomarker Status and Discovers Related Histomorphology Characteristics for Low-Grade Glioma.](http://arxiv.org/abs/2310.07464) | 该研究提出了一种基于深度学习的多生物标志物组织形态学发现者模型，利用全切片图像预测低级别胶质瘤中五个生物标志物的状态。该模型通过将单类分类纳入多实例学习框架，实现了准确的实例级别监督，在提高生物标志物预测性能方面表现出优势。 |
| [^12] | [Uncovering ECG Changes during Healthy Aging using Explainable AI.](http://arxiv.org/abs/2310.07463) | 本文使用可解释的人工智能技术分析了健康个体的心电图数据，并识别出随年龄增长呼吸率的下降及SDANN值异常高作为老年人的指标。 |
| [^13] | [Efficient machine-learning surrogates for large-scale geological carbon and energy storage.](http://arxiv.org/abs/2310.07461) | 本研究提出了一种专门的机器学习（ML）模型，通过域分解和拓扑嵌入器降低训练成本，提高机器学习在大规模地质储存应用中的效率。 |
| [^14] | [ProbTS: A Unified Toolkit to Probe Deep Time-series Forecasting.](http://arxiv.org/abs/2310.07446) | ProbTS是一个统一的工具包，用于协同和比较定制神经架构和深度生成模型在时间序列预测中的方法，揭示了它们的特点、优势和需要进一步研究的领域。 |
| [^15] | [A Branched Deep Convolutional Network for Forecasting the Occurrence of Hazes in Paris using Meteorological Maps with Different Characteristic Spatial Scales.](http://arxiv.org/abs/2310.07437) | 该论文开发了一种基于分支的深度卷积网络，用于预测巴黎雾霾的发生。该网络利用气象图的不同特征空间尺度来训练，通过使用多年期的气象变量和地面能见度观测数据进行训练和验证。这些新架构提高了网络性能，在未使用训练数据的情况下进行盲预测，具有合理的预测分数。 |
| [^16] | [HealthWalk: Promoting Health and Mobility through Sensor-Based Rollator Walker Assistance.](http://arxiv.org/abs/2310.07434) | HealthWalk通过集成传感器到滚轮行走者设计中，可以解决滚轮行走者用户姿势不好导致的健康问题，并实现其他有趣的用例。 |
| [^17] | [Imitation Learning from Observation with Automatic Discount Scheduling.](http://arxiv.org/abs/2310.07433) | 我们提出了一个新颖的观察学习模仿(ILfO)框架，能够解决由于奖励信号分配错误导致代理无法学习初始行为的问题。 |
| [^18] | [Non-backtracking Graph Neural Networks.](http://arxiv.org/abs/2310.07430) | 非回溯图神经网络(NBA-GNN)通过不考虑先前访问节点的消息来解决图神经网络本地更新中的冗余问题，并且在随机块模型恢复方面表现出良好的性能。 |
| [^19] | [Quantum-Enhanced Forecasting: Leveraging Quantum Gramian Angular Field and CNNs for Stock Return Predictions.](http://arxiv.org/abs/2310.07427) | 该论文提出了一种名为Quantum Gramian Angular Field (QGAF)的新方法，通过将量子计算技术与深度学习相结合，成功将股票回报时间序列数据转化为适合卷积神经网络（CNN）训练的二维图像，从而提高了预测的精度。实验证明，相比传统方法，QGAF方法具有显著的性能优势。 |
| [^20] | [Revisiting Plasticity in Visual Reinforcement Learning: Data, Modules and Training Stages.](http://arxiv.org/abs/2310.07418) | 本文对视觉强化学习中的可塑性进行了研究，发现数据增强对保持可塑性至关重要，评论者的可塑性损失是高效训练的主要限制因素，并且未及时恢复评论者的可塑性将导致灾难性结果。这为解决高重放比困境提供了新的策略。 |
| [^21] | [What can knowledge graph alignment gain with Neuro-Symbolic learning approaches?.](http://arxiv.org/abs/2310.07417) | 本文研究了知识图谱对齐的现状与挑战，提出了使用神经符号学习方法的潜力，以实现解释性、可验证性和高质量的对齐结果。 |
| [^22] | [A Novel Voronoi-based Convolutional Neural Network Framework for Pushing Person Detection in Crowd Videos.](http://arxiv.org/abs/2310.07416) | 本论文提出了一种基于Voronoi的卷积神经网络框架，用于在拥挤视频中微观级别地识别推动行为。通过该框架，能够更深入地了解拥挤场景中推动行为的模式和互动，并为拥挤管理策略和人流量优化提供有价值的见解。 |
| [^23] | [NuTime: Numerically Multi-Scaled Embedding for Large-Scale Time Series Pretraining.](http://arxiv.org/abs/2310.07402) | 本研究通过采用Transformer架构和数值多尺度嵌入模块，使时间序列自监督模型能够扩展到大规模数据集，并在大规模数据集上进行预训练。 |
| [^24] | [Deep Kernel and Image Quality Estimators for Optimizing Robotic Ultrasound Controller using Bayesian Optimization.](http://arxiv.org/abs/2310.07392) | 这项研究通过使用神经网络学习低维核函数，以优化基于贝叶斯优化的机器人超声控制器，解决了在不同患者中优化超声图像质量的挑战。 |
| [^25] | [Histopathological Image Classification and Vulnerability Analysis using Federated Learning.](http://arxiv.org/abs/2310.07380) | 这项研究开发了一种基于联邦学习的隐私保护技术，应用于皮肤癌数据集，发现该模型容易受到数据污染攻击的影响。 |
| [^26] | [Causal Unsupervised Semantic Segmentation.](http://arxiv.org/abs/2310.07379) | 因果无监督语义分割（CAUSE）是一个利用因果推断的新框架，旨在实现无监督语义分割。该方法通过构建概念聚类表作为中介，并与概念自监督学习建立联系，解决了无监督分割中适当聚类水平的挑战。 |
| [^27] | [Experimental quantum natural gradient optimization in photonics.](http://arxiv.org/abs/2310.07371) | 该论文在光子学领域首次使用光子实验估计了量子自然梯度（QNG），并通过获得He-H$^+$阳离子的解离曲线并达到化学精度，验证了QNG优化在光子设备上的超越性能。 |
| [^28] | [Orthogonal Random Features: Explicit Forms and Sharp Inequalities.](http://arxiv.org/abs/2310.07370) | 该论文通过分析正交随机特征的核近似的偏差和方差，提供了明确的表达式，并得出了尖锐指数界限，支持正交随机特征比随机傅里叶特征更具信息性。 |
| [^29] | [Improved Analysis of Sparse Linear Regression in Local Differential Privacy Model.](http://arxiv.org/abs/2310.07367) | 本文在本地差分隐私模型中改进了稀疏线性回归的分析。我们提出的创新算法不仅是首个非交互式本地差分隐私算法，而且产生了一种新颖且高效的估计器。 |
| [^30] | [GraphControl: Adding Conditional Control to Universal Graph Pre-trained Models for Graph Domain Transfer Learning.](http://arxiv.org/abs/2310.07365) | 在图领域迁移学习中，GraphControl通过添加条件控制实现了对通用图预训练模型的有效迁移，克服了不同图域间的属性语义差异问题。 |
| [^31] | [Diagnosing Bipolar Disorder from 3-D Structural Magnetic Resonance Images Using a Hybrid GAN-CNN Method.](http://arxiv.org/abs/2310.07359) | 本研究通过混合GAN-CNN模型从三维结构磁共振图像（sMRI）诊断双相情感障碍，取得了比先前工作更高的准确率和敏感性，同时使用的样本数量更少。 |
| [^32] | [IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training.](http://arxiv.org/abs/2310.07355) | IMITATE是一种临床先验指导的分层视觉语言预训练模型。它利用医学报告的层级结构，从胸部X射线图像中提取多级视觉特征，并与分层医学报告中的描述性和结论性文本进行对齐。 |
| [^33] | [Atom-Motif Contrastive Transformer for Molecular Property Prediction.](http://arxiv.org/abs/2310.07351) | Atom-Motif Contrastive Transformer (AMCT) is proposed to address the issue of overlooking motif interactions in Molecular Property Prediction (MPP). AMCT explores both atom-level and motif-level interactions, enhancing the effectiveness of MPP. |
| [^34] | [Fast-ELECTRA for Efficient Pre-training.](http://arxiv.org/abs/2310.07347) | 提出了一种快速ELECTRA的方法，利用现有的语言模型作为辅助模型，通过温度缩放平滑主模型的输出分布，达到与最先进的ELECTRA预训练方法相媲美的性能，并显著减少了辅助模型共同训练带来的计算和内存成本。 |
| [^35] | [Towards Foundation Models for Learning on Tabular Data.](http://arxiv.org/abs/2310.07338) | 本文提出了Tabular Foundation Models (TabFMs)，通过利用生成型表格学习的潜力，采用预训练的大规模语言模型作为基础模型，并在广泛的表格数据集上进行微调，赋予TabFMs深刻的理解和普遍的能力，从而克服了当前可转移的表格模型的限制。 |
| [^36] | [Exploring Social Motion Latent Space and Human Awareness for Effective Robot Navigation in Crowded Environments.](http://arxiv.org/abs/2310.07335) | 本文提出了一种通过学习社交运动潜空间来生成机器人控制的新方法，实现了在拥挤环境中的有效导航。同时，引入了人类对机器人的意识概念，并证明了这种意识的融入能够改善导航性能。 |
| [^37] | [An Adversarial Example for Direct Logit Attribution: Memory Management in gelu-4l.](http://arxiv.org/abs/2310.07325) | 在gelu-4l中，我们提供了证据表明内存管理对于transformer模型至关重要，并说明了Direct Logit Attribution技术的不准确之处。 |
| [^38] | [Multichannel consecutive data cross-extraction with 1DCNN-attention for diagnosis of power transformer.](http://arxiv.org/abs/2310.07323) | 本文提出了一种用于电力变压器诊断的多通道连续数据交叉提取（MCDC）结构，以全面利用顺序数据中的时间信息，并引入一维卷积神经网络注意力（1DCNN-attention）机制以降低空间复杂性。实验证明了MCDC的有效性和更优越的泛化能力。 |
| [^39] | [On the Impact of Cross-Domain Data on German Language Models.](http://arxiv.org/abs/2310.07321) | 本研究通过对德语语言模型进行实验，发现将数据多样性置于数据质量之上的交叉领域数据集训练方法，可以显著提高模型的性能，并超过了之前的最先进模型。 |
| [^40] | [Molecule-Edit Templates for Efficient and Accurate Retrosynthesis Prediction.](http://arxiv.org/abs/2310.07313) | METRO是一个机器学习模型，利用最小模板进行反应预测，解决了溯源预测中的计算开销大和解释性差的问题，实现了最先进的效果。 |
| [^41] | [WiGenAI: The Symphony of Wireless and Generative AI via Diffusion Models.](http://arxiv.org/abs/2310.07312) | WiGenAI通过引入扩散模型，将生成式人工智能应用于无线通信系统中，为研究奠定基础。这篇文章介绍了扩散模型作为生成模型的最新范式，并讨论了它在无线通信系统中的应用。通过两个案例研究展示了扩散模型在开发韧性的AI本地通信系统中的潜力。 |
| [^42] | [SNOiC: Soft Labeling and Noisy Mixup based Open Intent Classification Model.](http://arxiv.org/abs/2310.07306) | SNOiC模型是一种基于软标签和噪声混合的开放意图分类模型，通过减少偏差和生成伪数据来提高识别开放意图的性能。 |
| [^43] | [Beyond Memorization: Violating Privacy Via Inference with Large Language Models.](http://arxiv.org/abs/2310.07298) | 该论文首次全面研究了预训练大型语言模型从文本中推断个人属性的能力，发现当前的模型可以以较低的成本和时间比例，准确地推断出多种个人属性，这引发了隐私泄露的新威胁。 |
| [^44] | [Score Regularized Policy Optimization through Diffusion Behavior.](http://arxiv.org/abs/2310.07297) | 通过利用扩散行为模型，我们提出了一种在离线强化学习中用于优化策略的得分正则化方法，从而避免了耗时且计算密集的扩散采样方案，并在D4RL任务上实现了超过25倍的动作采样速度提升。 |
| [^45] | [BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations.](http://arxiv.org/abs/2310.07276) | BioT5是一个全面的预训练框架，在生物学中利用化学知识和自然语言关联丰富了跨模态整合，通过鲁棒的分子表示和上下文知识提取，实现了更有效的信息利用，展现出卓越的性能。 |
| [^46] | [Why Does Sharpness-Aware Minimization Generalize Better Than SGD?.](http://arxiv.org/abs/2310.07269) | 这项研究填补了Sharpness-Aware Minimization（SAM）相对于随机梯度下降（SGD）的一定数据模型和卷积神经网络中泛化更好的空白，解释了SAM的优势，尤其是在早期阶段防止噪声学习的能力。 |
| [^47] | [RaftFed: A Lightweight Federated Learning Framework for Vehicular Crowd Intelligence.](http://arxiv.org/abs/2310.07268) | RaftFed是一种面向车载群体智能的轻量级联邦学习框架，解决了车载群体智能中存在的可靠性和数据异质性问题，实现了隐私保护和性能优化。 |
| [^48] | [Classification of Dysarthria based on the Levels of Severity. A Systematic Review.](http://arxiv.org/abs/2310.07264) | 本系统性综述分析了基于严重程度对发音障碍进行分类的方法，并评估了最有效的特征集合和类型以及最佳的人工智能技术。 |
| [^49] | [Deep ReLU networks and high-order finite element methods II: Chebyshev emulation.](http://arxiv.org/abs/2310.07261) | 本论文研究了在有限分割上使用深度ReLU神经网络对连续分段多项式函数的表达速率和稳定性，提出了一种使用切比雪夫多项式展开系数进行编码的新颖ReLU NN替代模型构造，与基于ReLU NN模拟多项式的构造相比，在表达速率和稳定性方面获得了更好的界限。 |
| [^50] | [ADMEOOD: Out-of-Distribution Benchmark for Drug Property Prediction.](http://arxiv.org/abs/2310.07253) | ADMEOOD是一个设计用于药物属性预测的超分布基准，包含27个药物属性和两种数据转移（噪声转移和概念冲突漂移）。 |
| [^51] | [A Comparative Study of Pre-trained CNNs and GRU-Based Attention for Image Caption Generation.](http://arxiv.org/abs/2310.07252) | 本文提出了一种基于预训练CNN和GRU注意力机制的深度神经网络框架，用于图像描述生成。该方法通过多个预训练的卷积神经网络从图像中提取特征，并使用基于GRU的解码器生成描述性句子。实验证明，该方法在MSCOCO和Flickr30k数据集上取得了竞争性的成绩。 |
| [^52] | [Synthesizing Missing MRI Sequences from Available Modalities using Generative Adversarial Networks in BraTS Dataset.](http://arxiv.org/abs/2310.07250) | 本文提出了一种利用生成对抗网络从已有的模态生成缺失的MRI序列的方法，在BraTS数据集上取得了有希望的结果。 |
| [^53] | [Crowd Counting in Harsh Weather using Image Denoising with Pix2Pix GANs.](http://arxiv.org/abs/2310.07245) | 本文提出了使用Pix2Pix GANs对恶劣天气中的人群图像进行去噪来提高人群计数的性能。通过训练生成对抗网络来消除噪声和模糊效果，并在推断引擎中应用预训练的生成器来估计人群密度。实验证明，这种方法在需要高可靠性和准确性时具有重要作用。 |
| [^54] | [Surrogate modeling for stochastic crack growth processes in structural health monitoring applications.](http://arxiv.org/abs/2310.07241) | 本文提出了一种代理模型用于预测结构中裂纹的扩展，并成功地编码了不同的随机不确定性来源。该模型基于高斯过程回归模型，能够生成先验分布用于贝叶斯结构健康监测任务。 |
| [^55] | [Are GATs Out of Balance?.](http://arxiv.org/abs/2310.07235) | 本研究揭示了Graph Attention Network (GAT)梯度流动力学的守恒定律，解释了为什么标准初始化下的GAT中高比例的参数在训练过程中很难改变。我们还提出了一种平衡GAT网络的初始化方案，使得深层网络更容易进行训练，并且相比标准初始化，具有更快的收敛速度。 |
| [^56] | [Hierarchical Decomposition of Prompt-Based Continual Learning: Rethinking Obscured Sub-optimality.](http://arxiv.org/abs/2310.07234) | 本论文通过对基于提示的持续学习目标进行分层分解，在自监督预训练的背景下，解决了任务特定知识整合和预测困难的问题。 |
| [^57] | [Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment.](http://arxiv.org/abs/2310.07229) | 本论文提出了一种通过蛋白质片段和周围对齐的自监督口袋预训练方法，用于模拟并生成大量的配体-受体相互作用复合物。 |
| [^58] | [Deep Learning for blind spectral unmixing of LULC classes with MODIS multispectral time series and ancillary data.](http://arxiv.org/abs/2310.07223) | 这项研究利用MODIS多光谱时间序列数据和深度学习模型，首次实现了对LULC类别的盲目光谱分离。通过添加地理加地形和气候辅助信息，进一步提高了模型的性能。 |
| [^59] | [Using Learnable Physics for Real-Time Exercise Form Recommendations.](http://arxiv.org/abs/2310.07221) | 本文提出了一种使用可学习的物理学的算法流程，能够在实时环境中诊断运动姿势问题并提供矫正建议，通过姿势识别、重复次数计算和动作演变跟踪实现。该系统在六个不同的运动上进行了评估，通过低成本设备如智能手机提供实时建议，使自我练习成为可能，同时降低受伤风险。 |
| [^60] | [COPlanner: Plan to Roll Out Conservatively but to Explore Optimistically for Model-Based RL.](http://arxiv.org/abs/2310.07220) | COPlanner是一个基于规划的框架，用于解决模型预测误差带来的问题。通过保守的模型演算和乐观的环境探索，COPlanner利用不确定性感知模型预测控制来解决动力学模型不准确的问题，并提供更好的解决方案。 |
| [^61] | [Improved Membership Inference Attacks Against Language Classification Models.](http://arxiv.org/abs/2310.07219) | 在这篇论文中，我们提出了一个新的框架，用于对语言分类模型进行成员推理攻击。通过利用集成方法，生成多个专门的攻击模型，我们展示了这种方法在经典和语言分类任务上比单个攻击模型或每个类别标签的攻击模型更准确。 |
| [^62] | [Enhancing Neural Architecture Search with Multiple Hardware Constraints for Deep Learning Model Deployment on Tiny IoT Devices.](http://arxiv.org/abs/2310.07217) | 这项工作提出了一种新颖的方法，通过将多个硬件约束融入神经架构搜索(NAS)技术中，实现在微型物联网设备上部署高效准确的深度学习模型。 |
| [^63] | [Generative Modeling on Manifolds Through Mixture of Riemannian Diffusion Processes.](http://arxiv.org/abs/2310.07216) | 通过混合黎曼扩散过程的原则性框架，我们提出了一种在流形上构建生成过程的方法，与现有的生成模型相比，该方法具有更高的效率和更广泛的适用性。 |
| [^64] | [Bridging the Gap between Newton-Raphson Method and Regularized Policy Iteration.](http://arxiv.org/abs/2310.07211) | 正则化策略迭代和牛顿-拉弗森方法在使用强凸函数对贝尔曼方程平滑化的条件下严格等价，为正则化策略迭代的全局和局部收敛行为提供了统一分析。该算法具有全局线性收敛和局部二次收敛特性。 |
| [^65] | [Robust Safe Reinforcement Learning under Adversarial Disturbances.](http://arxiv.org/abs/2310.07207) | 本文针对现有的安全强化学习算法很少考虑外部干扰的问题，提出了一个鲁棒的安全强化学习框架，通过建立双人零和游戏来解决最坏情况下的干扰，并证明了策略迭代算法的收敛性。 |
| [^66] | [State of the Art on Diffusion Models for Visual Computing.](http://arxiv.org/abs/2310.07204) | 这篇论文旨在介绍最新的视觉计算领域中扩散模型的发展和应用，涵盖了生成人工智能的核心概念和实现细节，并总结了个人化、条件约束和反演等重要方面。 |
| [^67] | [Boosting Learning for LDPC Codes to Improve the Error-Floor Performance.](http://arxiv.org/abs/2310.07194) | 通过利用集成网络提升学习技术和引入分块训练方案，本文提出了一种优化LDPC码解码器的方法，以解决误码底问题，并改善了LDPC码的性能。 |
| [^68] | [Neural networks: deep, shallow, or in between?.](http://arxiv.org/abs/2310.07190) | 本研究探讨了深度和宽度对于神经网络的影响，结果表明只有深度趋近无穷大的神经网络才可能达到比熵数更好的速度，而固定深度并让宽度趋近无穷大则没有收益。 |
| [^69] | [Kernel Cox partially linear regression: building predictive models for cancer patients' survival.](http://arxiv.org/abs/2310.07187) | 本文提出了一种基于核Cox部分线性回归的方法来构建癌症患者生存预测模型，通过核机器方法描述复杂的生存和预测因子关系，并利用正则化加权核机器方法自动去除不相关的因子。与其他竞争方法相比，该方法在模拟中表现最好。 |
| [^70] | [NeuroInspect: Interpretable Neuron-based Debugging Framework through Class-conditional Visualizations.](http://arxiv.org/abs/2310.07184) | NeuroInspect是一个基于神经元的可解释的调试框架，通过确定网络中导致错误的神经元并可视化嵌入其中的特征，提供了人类可解释的解释。引入了CLIP-Illusion来生成特征图像，并以类为条件来考察神经元与决策层之间的联系。 |
| [^71] | [SAM-OCTA: Prompting Segment-Anything for OCTA Image Segmentation.](http://arxiv.org/abs/2310.07183) | 本文提出了一种名为SAM-OCTA的方法，通过低秩适应技术和提示点生成策略，在OCTA图像分割任务中实现了令人瞩目的性能。这种方法解决了在有限样本训练中可能导致的过拟合问题，并成功处理了视网膜血管、黄斑无血管区、毛细血管、动脉和静脉等各种分割任务，同时还实现了局部血管分割和动脉-静脉分割等以前未解决的问题。 |
| [^72] | [Online Speculative Decoding.](http://arxiv.org/abs/2310.07177) | 在线推测解码是通过利用多余计算能力，在LLM服务集群中持续更新草稿模型，从而加速大型语言模型推理的一种方法。 |
| [^73] | [Generalized Neural Sorting Networks with Error-Free Differentiable Swap Functions.](http://arxiv.org/abs/2310.07174) | 本文提出了一种广义神经排序网络，其中采用了具有无误差且可微分的交换函数，同时使用了置换等变Transformer网络来捕捉输入之间的依赖关系。实验证明，该方法在各种排序基准上表现优于或与基准方法相当。 |
| [^74] | [Federated Generalization via Information-Theoretic Distribution Diversification.](http://arxiv.org/abs/2310.07171) | 该论文研究了联邦学习中泛化能力的挑战，特别关注训练分布和测试分布的不匹配。提出了一种信息论的泛化方法来解决这个问题。 |
| [^75] | [Anchor-based Multi-view Subspace Clustering with Hierarchical Feature Descent.](http://arxiv.org/abs/2310.07166) | 本文提出了一种基于锚点的多视图子空间聚类方法，通过分层特征下降挖掘和部署视图之间的依赖关系，从而在聚类过程中实现了对不同视图的特征对齐。这种方法得到了一个共同的潜在空间，被称为“相似空间”，揭示了不同视图之间的相关性和依赖关系。 |
| [^76] | [LLark: A Multimodal Foundation Model for Music.](http://arxiv.org/abs/2310.07160) | LLark是一个通过多模态架构实现音乐理解的模型，能够在零样本泛化上匹配或超出现有基准模型，在字幕生成和推理任务中与人类响应高度一致。 |
| [^77] | [No Privacy Left Outside: On the (In-)Security of TEE-Shielded DNN Partition for On-Device ML.](http://arxiv.org/abs/2310.07152) | 本文研究了TEE保护下设备端机器学习的安全性问题。通过对现有的TSDP解决方案进行评估，发现这些解决方案容易受到隐私窃取攻击的影响。 |
| [^78] | [QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources.](http://arxiv.org/abs/2310.07147) | 我们提出了一种新的QFT框架，可以对LLMs进行内存高效的全参数微调，而不损害性能。 |
| [^79] | [Imitation Learning from Purified Demonstration.](http://arxiv.org/abs/2310.07143) | 本文提出了一种从纯净演示中进行模仿学习的方法，通过引入扩散过程对不完美演示中的潜在扰动进行净化。通过这种方法，可以改善模仿学习在真实场景中的应用挑战。 |
| [^80] | [AE-smnsMLC: Multi-Label Classification with Semantic Matching and Negative Label Sampling for Product Attribute Value Extraction.](http://arxiv.org/abs/2310.07137) | 本论文提出了一种基于语义匹配和负标签采样的多标签分类模型 AE-smnsMLC，用于解决产品属性值提取的问题。该模型将属性值提取任务转化为多标签分类任务，可以应用于只有属性值注释的实际场景，而无需属性值的位置信息注释。 |
| [^81] | [Risk Assessment and Statistical Significance in the Age of Foundation Models.](http://arxiv.org/abs/2310.07132) | 本论文提出了一个分布框架，用于评估具有统计显著性的基础模型的风险。通过一种新的统计相对测试方法，该框架结合了一阶和二阶随机优势，并借鉴了计量经济学和数学金融中常用的平均风险模型。在给定指定度量量化的防护栏的情况下，我们还开发了一种基于风险意识的基础模型选择方法。受数学金融中的投资组合优化和选择理论的启发，我们为每个模型定义了一个"度量组合"，并根据这些组合的随机优势进行模型选择。 |
| [^82] | [Off-Policy Evaluation for Human Feedback.](http://arxiv.org/abs/2310.07123) | 本论文介绍了一个用于人类反馈的非策略评估（OPEHF）框架，可以准确评估人类反馈信号。这个框架解决了现有OPE方法在估计人类反馈信号上的不足。 |
| [^83] | [The Temporal Structure of Language Processing in the Human Brain Corresponds to The Layered Hierarchy of Deep Language Models.](http://arxiv.org/abs/2310.07106) | 本文展示了深度语言模型(DLMs)的分层结构与人脑语言理解的时间动态之间存在强相关性，通过采用电子皮层图(ECoG)数据来优化时间分辨率，为深入了解人脑语言处理机制提供了新的视角。 |
| [^84] | [Machine Learning Methods for Background Potential Estimation in 2DEGs.](http://arxiv.org/abs/2310.07089) | 本文利用扫描门电镜和机器学习方法，提出了一种估计二维电子气体（2DEGs）背景势能的新方法，该方法利用了生成对抗神经网络、细胞神经网络和进化搜索算法。研究结果显示，进化搜索算法在缺陷分析方面具有较好的效果。这项工作不仅推动了对2DEGs的理解，还强调了机器学习在探索量子材料方面的潜力，对量子计算和纳米电子学有重要意义。 |
| [^85] | [Investigating the Adversarial Robustness of Density Estimation Using the Probability Flow ODE.](http://arxiv.org/abs/2310.07084) | 本研究使用概率流动ODE模型探究了密度估计的鲁棒性，并发现在一些情况下，密度估计对高复杂度、高似然度的攻击是鲁棒的，同时对抗样本也具有语义上的意义。 |
| [^86] | [Taking the human out of decomposition-based optimization via artificial intelligence: Part II. Learning to initialize.](http://arxiv.org/abs/2310.07082) | 本论文提出了一种利用机器学习方法学习优化算法的最佳初始化的方法，以减少计算时间。通过采用主动学习和监督学习技术，学习一个替代模型来预测给定初始化的计算性能，并将该方法应用于解决混合整数模型预测控制问题中的初始化。实验结果显示，所提出的方法可以显著减少求解时间，并且主动学习可以减少所需的数据量。 |
| [^87] | [Secure Decentralized Learning with Blockchain.](http://arxiv.org/abs/2310.07079) | 本文提出了基于区块链的分散式联邦学习（BDFL），通过利用区块链进行分散式模型验证和审计，解决了分散式联邦学习中虚假模型和数据攻击的问题。 |
| [^88] | [Auditing and Robustifying COVID-19 Misinformation Datasets via Anticontent Sampling.](http://arxiv.org/abs/2310.07078) | 本文拓展了对COVID-19虚假信息数据集进行审计和强化的研究。首先，发现在小数据上训练的专业分类器在野外环境中的性能表现有限。其次，提出了一种无需人工注释的主动学习方法，通过增加具有挑战性的抗内容来增强分类器。 |
| [^89] | [Taking the human out of decomposition-based optimization via artificial intelligence: Part I. Learning when to decompose.](http://arxiv.org/abs/2310.07068) | 本文提出了一种用于自动确定是使用整体式解法还是基于分解的解法的图分类方法，并展示了如何将该分类器应用于凸混合整数非线性规划问题的求解。 |
| [^90] | [Acoustic Model Fusion for End-to-end Speech Recognition.](http://arxiv.org/abs/2310.07062) | 该论文提出了一种声学模型融合的方法，可以改进端到端语音识别系统中存在的领域不匹配问题，并显著降低词错误率。 |
| [^91] | [DKEC: Domain Knowledge Enhanced Multi-Label Classification for Electronic Health Records.](http://arxiv.org/abs/2310.07059) | 本文提出了DKEC，一种领域知识增强的分类器，用于医学诊断预测。它利用标签的注意力机制和组内训练方法来捕捉医学实体之间的语义关系，并增加罕见类别的样本数量。评估结果显示其在医学数据集上表现良好。 |
| [^92] | [FedMFS: Federated Multimodal Fusion Learning with Selective Modality Communication.](http://arxiv.org/abs/2310.07048) | FedMFS是一种新的多模态融合联邦学习方法，通过选择性模态通信解决了缺乏特定模态的异构客户问题，并设计了最优的模态上传策略以提高学习性能。 |
| [^93] | [A predict-and-optimize approach to profit-driven churn prevention.](http://arxiv.org/abs/2310.07047) | 本文提出了一种利润驱动流失防止的预测和优化方法。该方法利用个体顾客生命周期价值（CLV）来定位最有价值的顾客，并采用预测和优化框架进行解决。研究结果表明，该方法在12个流失预测数据集上取得了最佳的平均利润绩效。 |
| [^94] | [Computational Pathology at Health System Scale -- Self-Supervised Foundation Models from Three Billion Images.](http://arxiv.org/abs/2310.07033) | 该论文通过使用大规模无标签数据集训练自监督基础模型并在大型临床病理学数据集上进行评估，旨在训练最大的学术基础模型并评估最显著的自监督学习算法。 |
| [^95] | [Neural Harmonium: An Interpretable Deep Structure for Nonlinear Dynamic System Identification with Application to Audio Processing.](http://arxiv.org/abs/2310.07032) | 本文提出了一种可解释的深度结构，用于非线性动态系统识别，并应用于音频处理。通过利用谐波分析和递归建模方式，该模型在时间频率域中对系统进行建模，并通过神经网络识别频率间的相互依赖关系。该模型具有高时域和频谱分辨率，能够快速、稳健、精确地进行二阶优化，无需显式计算海森矩阵。 |
| [^96] | [Brain Age Revisited: Investigating the State vs. Trait Hypotheses of EEG-derived Brain-Age Dynamics with Deep Learning.](http://arxiv.org/abs/2310.07029) | 这项研究使用深度学习方法，通过对临床脑电图的研究来探讨大脑的生物年龄动态。研究结果对于理解大脑的状态和特质假说以及脑病变具有重要意义。 |
| [^97] | [Facial Forgery-based Deepfake Detection using Fine-Grained Features.](http://arxiv.org/abs/2310.07028) | 该论文提出了一种基于细粒度特征的Deepfake检测方法，通过抑制背景噪声和学习区分性特征来提高检测的效果。 |
| [^98] | [Utilizing Synthetic Data for Medical Vision-Language Pre-training: Bypassing the Need for Real Images.](http://arxiv.org/abs/2310.07027) | 本研究探讨了使用合成图像在医学VLP中的可行性和有效性，通过用真实医学报告生成的合成图像替换真实图像进行训练，并在图像分类、语义分割和目标检测任务中进行了实证评估。 |
| [^99] | [Automatic Macro Mining from Interaction Traces at Scale.](http://arxiv.org/abs/2310.07023) | 本文介绍了一种基于大型语言模型的新方法，用于从移动交互轨迹中自动提取语义上有意义的宏任务。通过多项研究和实验证明了该方法的有效性和提取的宏任务的实用性。 |
| [^100] | [Neural Relational Inference with Fast Modular Meta-learning.](http://arxiv.org/abs/2310.07015) | 本论文提出了一种基于快速模块化元学习的神经关系推理方法，通过训练神经模块，可以在多种任务中以不同的方式组合，隐式地编码时间不变性，并在一个上下文中推断关系，从而增加了推理能力。 |
| [^101] | [Answer Candidate Type Selection: Text-to-Text Language Model for Closed Book Question Answering Meets Knowledge Graphs.](http://arxiv.org/abs/2310.07008) | 本文提出了一种新颖的方法，通过对预训练的文本到文本问答系统生成的候选答案基于其类型进行过滤和重新排序，以解决在知识图谱问答任务中，模型容量有限且对于含有不太流行实体的问题质量下降的问题。 |
| [^102] | [Sound-skwatter (Did You Mean: Sound-squatter?) AI-powered Generator for Phishing Prevention.](http://arxiv.org/abs/2310.07005) | 声音攻击是一种网络钓鱼攻击方式，通过利用单词发音相似性来欺骗用户，而Sound-skwatter是一种多语言的AI系统，通过学习声音相似性生成声音攻击候选项，具有较高质量和跨语言应用能力。实验证明，Sound-skwatter能够发现大量保护解决方案未知的已知和新型同音词候选项。 |
| [^103] | [CarDS-Plus ECG Platform: Development and Feasibility Evaluation of a Multiplatform Artificial Intelligence Toolkit for Portable and Wearable Device Electrocardiograms.](http://arxiv.org/abs/2310.07000) | 该论文描述了CarDS-Plus ECG平台的开发，该平台是一个创新的多平台系统，旨在快速部署基于人工智能的心电图解决方案用于临床调查和医疗服务交付。 |
| [^104] | [Violation of Expectation via Metacognitive Prompting Reduces Theory of Mind Prediction Error in Large Language Models.](http://arxiv.org/abs/2310.06983) | 本文研究了如何利用违反期望机制在大型语言模型中降低用户预测误差。我们引入了元认知提示框架，并发现存储和检索违反用户期望的事实可以使模型以类似人类学习理论的方式了解用户。 |
| [^105] | [Data Distillation Can Be Like Vodka: Distilling More Times For Better Quality.](http://arxiv.org/abs/2310.06982) | 本论文介绍了一种名为渐进式数据集蒸馏（PDD）的方法，通过合成多个小的合成图像集，并在这些子集的累计联合上进行模型训练，以解决目前数据集蒸馏技术的问题，从而获得更好的泛化性能。 |
| [^106] | [Federated Quantum Machine Learning with Differential Privacy.](http://arxiv.org/abs/2310.06973) | 本文提出了将量子联邦学习（QFL）和量子差分隐私（QDP）相结合的方法，并在量子平台上实施，以实现对数据泄露和模型反演攻击的全面保护。 |
| [^107] | [Flood and Echo: Algorithmic Alignment of GNNs with Distributed Computing.](http://arxiv.org/abs/2310.06970) | 本论文提出了一个基于分布式算法设计原则的新的执行框架：洪水和回声网络。通过波状激活模式，它可以在整个图中传递消息，并且可以有效地处理更大的图形。 |
| [^108] | [Positivity-free Policy Learning with Observational Data.](http://arxiv.org/abs/2310.06969) | 本研究提出了一种无偏性的政策学习框架，该框架利用观测数据进行政策学习，并克服了现实情境中无法满足假设的难题。该框架利用增量倾向分数策略调整倾向分数值，从而实现了快速的收敛率。 |
| [^109] | [ObjectComposer: Consistent Generation of Multiple Objects Without Fine-tuning.](http://arxiv.org/abs/2310.06968) | ObjectComposer是一种不需要微调的方法，可以一致地生成多个对象的组合。这解决了现有模型在不同上下文中生成对象外观不一致的问题。 |
| [^110] | [On the Interpretability of Part-Prototype Based Classifiers: A Human Centric Analysis.](http://arxiv.org/abs/2310.06966) | 本论文提出了一个用于从人类角度评估基于部分原型模型可解释性的框架，并使用亚马逊机械土耳其进行了广泛实验。这些实验不仅展示了框架在评估各种基于部分原型模型的可解释性方面的能力，还是最全面的工作。 |
| [^111] | [Comparing the robustness of modern no-reference image- and video-quality metrics to adversarial attacks.](http://arxiv.org/abs/2310.06958) | 本文比较了现代图像和视频质量评估度量方法对抗攻击的鲁棒性，并发现部分度量方法对对抗攻击表现出较高的抵抗力，为基准测试提供了更安全的选择。 |
| [^112] | [Diffusion Prior Regularized Iterative Reconstruction for Low-dose CT.](http://arxiv.org/abs/2310.06949) | 本文介绍了一种扩散先验正则化的迭代重建算法，通过融合去噪扩散概率模型和数据保真度重建过程，以及采用Nesterov动量加速技术，实现了在低剂量CT下出色的图像重建效果。 |
| [^113] | [A Variational Autoencoder Framework for Robust, Physics-Informed Cyberattack Recognition in Industrial Cyber-Physical Systems.](http://arxiv.org/abs/2310.06948) | 本研究提出了一种基于变分自编码器的混合设计数据驱动框架，通过考虑时间行为和提取传感器测量的时间序列特征，可用于鲁棒、基于物理的工业网络物理系统中的网络攻击的识别、定位和诊断。 |
| [^114] | [LLMs Killed the Script Kiddie: How Agents Supported by Large Language Models Change the Landscape of Network Threat Testing.](http://arxiv.org/abs/2310.06936) | 本文研究了大型语言模型在网络威胁测试中的潜力，通过自动化的决策过程支持特工行动，并讨论了使用大型语言模型加速威胁行为能力的伦理问题。 |
| [^115] | [Quantum Shadow Gradient Descent for Quantum Learning.](http://arxiv.org/abs/2310.06935) | 本文提出了一种称为量子阴影梯度下降的新方法，解决了量子学习中的关键挑战，并通过测量量子阴影样本和模拟态的梯度来提高计算效率。 |
| [^116] | [Prosody Analysis of Audiobooks.](http://arxiv.org/abs/2310.06930) | 本研究通过使用一个含有93个书籍和对应有声书的数据集，提出了改进的模型来预测有声书文本中的韵律属性。结果显示，我们的预测韵律与人类朗读比商业级TTS系统更相关，并且人们更喜欢韵律增强的有声书朗读。 |
| [^117] | [Stochastic Super-resolution of Cosmological Simulations with Denoising Diffusion Models.](http://arxiv.org/abs/2310.06929) | 本文提出了使用去噪扩散模型进行宇宙学超分辨率的方法，通过重新分配不同尺度的重要性来实现准确的结果，该方法能够产生令人信服的超分辨率图像，并且与小尺度特征多样性一致。 |
| [^118] | [PICProp: Physics-Informed Confidence Propagation for Uncertainty Quantification.](http://arxiv.org/abs/2310.06923) | 本文提出了一种名为PICProp的方法，基于双层优化，用于在深度学习和物理信息学习中进行不确定性量化。该方法能够在不进行强大假设的情况下计算有效的置信区间（CI），并且通过传播置信度实现了数据位置到整个域的置信度传播。 |
| [^119] | [Improving Contrastive Learning of Sentence Embeddings with Focal-InfoNCE.](http://arxiv.org/abs/2310.06918) | 本研究提出了一种聚焦-信息熵函数，利用硬负样本挖掘改进了对比学习句子嵌入的方法，并在各种基准测试中验证了其性能提升。 |
| [^120] | [Distributed Transfer Learning with 4th Gen Intel Xeon Processors.](http://arxiv.org/abs/2310.06916) | 本文研究了如何利用第四代英特尔韧性处理器进行分布式迁移学习，通过使用英特尔高级矩阵扩展（AMX）和Horovod在Image Classification TensorFlow数据集上实现了接近最先进的图像分类准确性。 |
| [^121] | [On sparse regression, Lp-regularization, and automated model discovery.](http://arxiv.org/abs/2310.06872) | 该论文通过结合正则化和物理约束的方法，利用神经网络进行自动模型发现，探索了非线性回归中稀疏回归和特征提取的潜力，并针对材料建模提出了常见的指导方针和趋势。 |
| [^122] | [Deep Learning-Based Real-Time Rate Control for Live Streaming on Wireless Networks.](http://arxiv.org/abs/2310.06857) | 提出了一种基于深度学习的实时H.264控制器，利用即时通道质量数据和视频块动态估计最优编码器参数，以实时延迟可忽略的方式维持编码视频比特率略低于可用通道比特率。 |
| [^123] | [Genetic Algorithm-Based Dynamic Backdoor Attack on Federated Learning-Based Network Traffic Classification.](http://arxiv.org/abs/2310.06855) | 本论文研究了基于联邦学习的网络流量分类中的动态后门攻击，并提出了一种基于遗传算法的新型后门攻击方法GABAttack。该方法能够优化后门触发器参数的值和位置，以实现对全局模型的篡改。 |
| [^124] | [DeepCrysTet: A Deep Learning Approach Using Tetrahedral Mesh for Predicting Properties of Crystalline Materials.](http://arxiv.org/abs/2310.06852) | DeepCrysTet是一种新颖的深度学习方法，通过使用3D四面体网格表示晶体结构，预测晶体材料的性质。 |
| [^125] | [DeepTriNet: A Tri-Level Attention Based DeepLabv3+ Architecture for Semantic Segmentation of Satellite Images.](http://arxiv.org/abs/2310.06848) | DeepTriNet是一种基于三级注意力的DeepLabv3+架构，用于卫星图像的语义分割。该方法通过结合SENets和TAUs桥接了编解码器输出与相关特征之间的语义差距，同时通过自我监督确定了更重要和更通用的特征。 |
| [^126] | [Performance Analysis of Various EfficientNet Based U-Net++ Architecture for Automatic Building Extraction from High Resolution Satellite Images.](http://arxiv.org/abs/2310.06847) | 本研究提出了基于EfficientNet背骨的U-Net++架构，通过深度监督和重设计的跳跃连接，提高了高分辨率卫星图像中的建筑物提取准确性。实验证明，该模型显著优于先前的方法。 |
| [^127] | [RobustEdge: Low Power Adversarial Detection for Cloud-Edge Systems.](http://arxiv.org/abs/2310.06845) | RobustEdge是一种低功耗、适用于边缘设备的对抗检测方法，解决了在云边系统中能量浪费和计算开销问题。 |
| [^128] | [Malware Classification using Deep Neural Networks: Performance Evaluation and Applications in Edge Devices.](http://arxiv.org/abs/2310.06841) | 该论文评估了使用深度神经网络进行恶意软件分类的效果和性能，证明了DNNs在不同类型的恶意软件分类上具有潜力。此外，论文还探讨了在边缘设备上实时分类的可行性，提出了优化模型架构和利用边缘计算能力的方法以实现高效性能。这项研究对于推进恶意软件检测技术和提升网络安全措施具有重要贡献。 |
| [^129] | [Hyperdimensional Computing as a Rescue for Efficient Privacy-Preserving Machine Learning-as-a-Service.](http://arxiv.org/abs/2310.06840) | 本文展示了超维计算作为机器学习即服务保护隐私的方法，通过同态加密技术实现计算过程中的数据保密性，为高效的隐私保护提供了解决方案。 |
| [^130] | [Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing.](http://arxiv.org/abs/2310.06306) | 本论文提出了一种面向制造业的AI孵化的上下文强化学习集成活跃学习方法，通过在线更新AI模型的方式来持续改进决策。研究采用了一种名为CBEAL的集成主动学习方法，通过优化地指导数据获取，实现了数据效果的最小化。 |
| [^131] | [MuseChat: A Conversational Music Recommendation System for Videos.](http://arxiv.org/abs/2310.06282) | MuseChat是一种创新的对话式音乐推荐系统，通过模拟用户和推荐系统之间的对话交互，利用预训练的音乐标签和艺术家信息，为用户提供定制的音乐推荐，使用户可以个性化选择他们喜欢的音乐。 |
| [^132] | [The Importance of Prompt Tuning for Automated Neuron Explanations.](http://arxiv.org/abs/2310.06200) | 本文探讨了在自动神经元解释中即时调优的重要性，通过重新格式化解释提示，我们显著提高了解释质量并降低了计算成本。这项研究对于更深入理解大型语言模型的工作原理和安全性具有重要意义。 |
| [^133] | [HyperAttention: Long-context Attention in Near-Linear Time.](http://arxiv.org/abs/2310.05869) | 近似注意力机制HyperAttention解决了在大型语言模型中使用的长上下文的计算挑战，并通过引入两个参数来衡量问题的难度。HyperAttention具有模块化设计，可轻松集成其他快速低级实现。 |
| [^134] | [Are Large Language Models Post Hoc Explainers?.](http://arxiv.org/abs/2310.05797) | 这项工作提出了第一个研究大型语言模型（LLMs）解释其他预测模型有效性的框架，并且提出了多个提示策略，填补了当前对于LLMs在解释其他模型行为方面的缺失。 |
| [^135] | [Analysis of Rainfall Variability and Water Extent of Selected Hydropower Reservoir Using Google Earth Engine (GEE): A Case Study from Two Tropical Countries, Sri Lanka and Vietnam.](http://arxiv.org/abs/2310.05682) | 本研究使用遥感技术分析了斯里兰卡和越南两个热带国家的降雨模式和水库水域范围的变化。研究结果有助于理解遥感降雨数据与水域范围动态变化之间的关系。 |
| [^136] | [Multi-timestep models for Model-based Reinforcement Learning.](http://arxiv.org/abs/2310.05672) | 多步模型的基于模型的强化学习算法通过使用多步目标来训练一步模型，解决了轨迹长度增长时一步预测误差的累积问题，并在噪声数据上表现出显著的性能提升。 |
| [^137] | [On Double-Descent in Reinforcement Learning with LSTD and Random Features.](http://arxiv.org/abs/2310.05518) | 本文研究了在强化学习中网络大小和L2正则化对性能的影响，并观察到了双下降现象。通过使用随机特征和懒惰训练策略，在参数和状态数无限大的情况下研究了正则化的最小二乘时间差分算法，得出了其收敛性和最优性，并阐述了双下降现象在该算法中的影响。 |
| [^138] | [Vibroacoustic Frequency Response Prediction with Query-based Operator Networks.](http://arxiv.org/abs/2310.05469) | 该论文提出了一个基于查询式操作网络的振动声学频响预测方法，并设计了一个用于代表性振动声学问题的结构化基准测试。该方法可以加速频响模拟，有助于设计优化和不确定性量化。 |
| [^139] | [Scaling Studies for Efficient Parameter Search and Parallelism for Large Language Model Pre-training.](http://arxiv.org/abs/2310.05350) | 本研究针对大型语言模型预训练进行了并行和分布式机器学习算法的开发，旨在优化数据处理和提高模型的训练效率。 |
| [^140] | [Learning force laws in many-body systems.](http://arxiv.org/abs/2310.05273) | 在这篇论文中，作者展示了一种结合了物理直觉的机器学习方法，用于推断尘埃等离子体实验中的力学规律。通过对粒子轨迹的训练，该模型考虑了对称性和非相同粒子之间的非互逆力，并提取了每个粒子的质量和电荷。模型的准确性指示出尘埃等离子体中存在超出当前理论分辨率的新物理，并展示了机器学习在引导多体系统科学发现方面的潜力。 |
| [^141] | [Learning Intra- and Inter-Cell Differences for Accurate Battery Lifespan Prediction across Diverse Conditions.](http://arxiv.org/abs/2310.05052) | 该论文介绍了一种跨不同条件精确预测电池寿命的方法，通过捕捉目标电池和参考电池之间的电信号差异，无论材料和老化条件如何，在扩展特征空间的同时为通用的电池寿命预测框架铺平了道路。 |
| [^142] | [Cell Tracking-by-detection using Elliptical Bounding Boxes.](http://arxiv.org/abs/2310.04895) | 本文提出了一种基于经典检测方法的新方法，通过将细胞形状近似为定向椭圆并使用通用定向对象检测器来识别细胞，实现了对细胞的检测和跟踪，减轻了对标注数据的需求。 |
| [^143] | [LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT.](http://arxiv.org/abs/2310.04673) | LauraGPT是一个统一的GPT模型，用于音频识别、理解和生成，具有广泛的应用范围，包括自动语音识别、语音翻译、文本到语音合成、机器翻译等任务。 |
| [^144] | [Leveraging Self-Consistency for Data-Efficient Amortized Bayesian Inference.](http://arxiv.org/abs/2310.04395) | 该论文提出了一种利用自一致性改进数据有效的摊余贝叶斯推理方法，通过反转贝叶斯定理并利用近似表示的联合模型估计边际似然，加速条件神经密度估计器的学习动力学。 |
| [^145] | [Phase Synchrony Component Self-Organization in Brain Computer Interface.](http://arxiv.org/abs/2310.03748) | 本文提出了相位同步成分自组织的概念，利用深度学习端到端网络自动化脑机接口中的预处理和通道选择，从而提高了分析功能性脑连接和识别脑活动的效果。 |
| [^146] | [HyperMask: Adaptive Hypernetwork-based Masks for Continual Learning.](http://arxiv.org/abs/2310.00113) | HyperMask是一种用于持续学习的方法，它使用基于超网络的掩码来训练一个单一网络，以克服人工神经网络在多任务上的灾难性遗忘问题。 |
| [^147] | [Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency.](http://arxiv.org/abs/2309.17382) | 提出了一个名为"RAFA"的原则框架，通过在LLM中将推理视为学习和规划的贝叶斯问题，协调推理和行动。通过一个提示模板进行推理，学习并制定未来的轨迹规划，然后在每一步中采取计划轨迹的初始行动并重新规划未来轨迹。这个框架具有可证明的遗憾保证。 |
| [^148] | [ACGAN-GNNExplainer: Auxiliary Conditional Generative Explainer for Graph Neural Networks.](http://arxiv.org/abs/2309.16918) | 本论文提出了一种新的图神经网络解释方法ACGAN-GNNExplainer，将辅助分类器生成对抗网络（ACGAN）引入到GNN解释领域。通过利用生成器为原始输入图生成解释，并借助鉴别器监督生成过程，提高解释的准确性和保真度。实验证明了该方法的有效性和实用性。 |
| [^149] | [Cross-Prediction-Powered Inference.](http://arxiv.org/abs/2309.16598) | 本文介绍了一种基于机器学习的交叉预测方法，可以有效地进行推理。该方法通过使用一个小型标记数据集和一个大型未标记数据集，通过机器学习填补缺失的标签，并采用去偏差方法纠正预测的不准确性。 |
| [^150] | [LPML: LLM-Prompting Markup Language for Mathematical Reasoning.](http://arxiv.org/abs/2309.13078) | 本论文提出了LPML，一种用于数学推理的LLM提示标记语言。通过将Chain-of-Thought方法和Python REPL与该标记语言结合，我们能够控制LLM生成文本中的错误，并增强其推理能力。我们的方法能够实现利用Python计算纠正错误和解决挑战性数学问题，而只需要零样本提示。 |
| [^151] | [On the Probability of Immunity.](http://arxiv.org/abs/2309.11942) | 本文研究了免疫的概率，提出了免疫的必要和充分条件，以及ε-有界免疫的条件。同时，借助随机对照试验估计受益概率，并得到比现有边界更紧密的概率边界。此外，介绍了间接免疫的概念，并提出了一种用于处理未测量混淆的免疫概率敏感性分析方法。 |
| [^152] | [DiscoverPath: A Knowledge Refinement and Retrieval System for Interdisciplinarity on Biomedical Research.](http://arxiv.org/abs/2309.01808) | DiscoverPath是一个基于知识图的生物医学研究论文搜索引擎，通过命名实体识别和词性标注从文章摘要中提取术语和关系，并展示给用户一个关注查询实体及其邻近节点的子图，以及查询推荐系统，使用户能够循序渐进地细化查询。 |
| [^153] | [Elucidating the Exposure Bias in Diffusion Models.](http://arxiv.org/abs/2308.15321) | 本文系统地研究了扩散模型中的曝光偏差问题，并提出了一种名为Epsilon Scaling的免训练方法来减轻这一问题。实验结果验证了该方法的有效性。 |
| [^154] | [InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4.](http://arxiv.org/abs/2308.12067) | InstructionGPT-4通过仅使用200个例子进行微调，在多模式指令数据质量度量和选择器的帮助下，在各种评估任务中优于原始的MiniGPT-4。 |
| [^155] | [A Benchmark Study on Calibration.](http://arxiv.org/abs/2308.11838) | 这项研究提出了一个模型校准的基准研究，利用神经架构搜索空间探索了模型校准属性。研究结果显示，模型校准可以在不同任务中泛化，并可以同时兼顾模型的准确性和校准性能。 |
| [^156] | [Discovering interpretable elastoplasticity models via the neural polynomial method enabled symbolic regressions.](http://arxiv.org/abs/2307.13149) | 本文介绍了一种通过神经多项式方法实现可解释的弹性塑性模型的机器学习方法，该方法通过分为两个步骤，先通过监督学习得到一组特征映射，再通过符号回归将其转化为数学公式，从而克服了传统神经网络模型的缺乏可解释性的问题。 |
| [^157] | [Deep Network Approximation: Beyond ReLU to Diverse Activation Functions.](http://arxiv.org/abs/2307.06555) | 本文研究了深度神经网络在多种激活函数下的表达能力，证明了可以通过在有界集合上构建一个宽度为6N、深度为2L的varrho激活网络来逼近一个宽度为N、深度为L的ReLU网络，从而将对ReLU网络的逼近结果推广到其他激活函数。 |
| [^158] | [VerifAI: Verified Generative AI.](http://arxiv.org/abs/2307.02796) | 验证生成式人工智能的输出是一个新兴问题，我们提出了通过分析多模态数据湖的底层数据，评估其质量和一致性，来建立评估生成式人工智能模型输出的更坚实基础，并解决错误信息传播的挑战。 |
| [^159] | [Elastic Decision Transformer.](http://arxiv.org/abs/2307.02484) | 弹性决策变压器（EDT）通过在测试时间进行动作推断时调整历史长度来实现轨迹拼接，填补了决策变压器（DT）在这一方面的性能差距，并且在多任务情况下胜过基于Q-Learning的方法。 |
| [^160] | [ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram.](http://arxiv.org/abs/2306.15681) | ECG-QA是第一个专为心电图分析设计的问答数据集，包括涵盖广泛临床相关ECG主题的问题模板和多样化的ECG解读问题。这一资源将为未来的医疗保健问答研究提供有价值的见解。 |
| [^161] | [Machine-Learning-Assisted and Real-Time-Feedback-Controlled Growth of InAs/GaAs Quantum Dots.](http://arxiv.org/abs/2306.12898) | 该论文提出了一种基于机器学习的实时反馈控制InAs/GaAs量子点生长方法。 |
| [^162] | [Semi-Implicit Denoising Diffusion Models (SIDDMs).](http://arxiv.org/abs/2306.12511) | SIDDMs是一种新方法，通过匹配隐式和显式因子，实现在生成模型中快速收敛且一定程度上保证样本多样性和质量。 |
| [^163] | [DCdetector: Dual Attention Contrastive Representation Learning for Time Series Anomaly Detection.](http://arxiv.org/abs/2306.10347) | DCdetector是一种多尺度双重关注的对比表示学习模型，用于时间序列异常检测。它利用双重关注不对称设计和纯对比损失学习置换不变表示，从而有效区分异常样本。 |
| [^164] | [Explore, Establish, Exploit: Red Teaming Language Models from Scratch.](http://arxiv.org/abs/2306.09442) | 本文提出了一种新的红队行动，通过从高层次、抽象的规范出发来考虑语言模型的行为，以探究模型的创新和贡献。 |
| [^165] | [AVIDa-hIL6: A Large-Scale VHH Dataset Produced from an Immunized Alpaca for Predicting Antigen-Antibody Interactions.](http://arxiv.org/abs/2306.03329) | 这是一个大规模的VHH数据集，用于预测抗原-抗体相互作用。通过利用VHHs的结构，该数据集包括573,891个抗原-VHH对，并且是当前公开数据集中最大、最全面的之一。 |
| [^166] | [DYffusion: A Dynamics-informed Diffusion Model for Spatiotemporal Forecasting.](http://arxiv.org/abs/2306.01984) | 提出了一种新的扩散模型，其结合了数据中编码的时间动态，自然地编码了多步和长程预测能力，具有灵活的采样轨迹和折衷性能与加速采样的能力，同时提高了计算效率，可在时空预测方面取得竞争性表现。 |
| [^167] | [Symmetric Exploration in Combinatorial Optimization is Free!.](http://arxiv.org/abs/2306.01276) | 该论文提出了一种免费的技术，通过利用对称性提高了基于DRL的组合优化求解器的性能，无需额外的目标函数评估，适用于广泛的组合优化任务，并在多种任务上进行实证评估证实了其有效性。 |
| [^168] | [Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model.](http://arxiv.org/abs/2305.16340) | 本文提出了一种分段循环Transformer（SRformer）来减少计算/内存成本，并使用RAF层处理跨段的信息，从而提高序列处理能力。 |
| [^169] | [FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation.](http://arxiv.org/abs/2305.14251) | 本文介绍了一种称为FACTSCORE的评估方法，它通过将生成的内容分解为原子事实，并计算被可靠知识源支持的比例来评估大型语言模型生成的长文本的事实准确性。通过广泛的人工评估，我们发现商业语言模型中仅有58%的ChatGPT传记达到了高水平的事实准确性。此外，我们还引入了一种自动化模型，利用检索和强语言模型估计FACTSCORE，误差率低于2%。 |
| [^170] | [Editing Large Language Models: Problems, Methods, and Opportunities.](http://arxiv.org/abs/2305.13172) | 本文深入探讨了编辑大型语言模型的问题、方法和机会，提供了任务定义和挑战的概述、先进方法的实证分析，以及构建了新的基准数据集。这些结果有助于改进LLMs的编辑技术，提高其效果和可行性。 |
| [^171] | [BertRLFuzzer: A BERT and Reinforcement Learning based Fuzzer.](http://arxiv.org/abs/2305.12534) | BertRLFuzzer是一种基于BERT和强化学习的Fuzzer工具，旨在发现Web应用程序的安全漏洞。通过使用BERT模型作为代理来指导Fuzzer进行高效学习，BertRLFuzzer相对于其他黑盒和白盒Fuzzer在时间到首次攻击、新漏洞发现和攻击率方面都取得了显著的改进。 |
| [^172] | [Clifford Group Equivariant Neural Networks.](http://arxiv.org/abs/2305.11141) | 我们引入了Clifford群等变神经网络，它可以构建O(n)和E(n)等变模型。该方法通过调整Clifford群的定义以及保持向量空间和乘法结构的作用来实现多个有利属性。 |
| [^173] | [Equivariant Few-Shot Learning from Pretrained Models.](http://arxiv.org/abs/2305.09900) | 本文提出了一种基于预训练模型的$\lambda$-\textit{equitune}方法，它使用\textit{重要性权重}$\lambda$对特征进行平均，可以显著提高等变小样本学习的表现。 |
| [^174] | [Knowledge Rumination for Pre-trained Language Models.](http://arxiv.org/abs/2305.08732) | 本文提出了一种名为知识反思的新范式，旨在帮助预训练语言模型利用已经编码在其预训练参数中的相关潜在知识，而不需要从外部语料库中检索。这种方法通过在模型中添加提示，并将相关知识注入模型进行整合，取得了在常识推理任务和GLUE基准上的实验结果。 |
| [^175] | [TidyBot: Personalized Robot Assistance with Large Language Models.](http://arxiv.org/abs/2305.05658) | 本文研究了使用机器人进行家庭清扫的个性化问题。通过使用大型语言模型少样本摘要能力，机器人可以学习用户的偏好并将其推广到未来的场景中，从而实现快速适应。 |
| [^176] | [Robust Tensor CUR Decompositions: Rapid Low-Tucker-Rank Tensor Recovery with Sparse Corruption.](http://arxiv.org/abs/2305.04080) | 本文提出了一种快速算法，称为鲁棒性张量CUR分解（RTCUR），用于Tucker秩设置下的大规模非凸TRPCA问题，通过交替投影的框架和张量CUR分解，快速实现对稀疏损坏的低秩张量恢复，并在实际数据集上展示了该方法的有效性和计算优势。 |
| [^177] | [Efficient MILP Decomposition in Quantum Computing for ReLU Network Robustness.](http://arxiv.org/abs/2305.00472) | 本研究提出了一种用于ReLU网络鲁棒性的新型MILP量子分解方法，尤其针对解决量子位可用性、噪声和误差限制，可实现更高的成功率和更高的效率与精确度，对量子计算技术的发展有重要意义。 |
| [^178] | [An Empirical Study of Multimodal Model Merging.](http://arxiv.org/abs/2304.14933) | 本研究通过融合在不同模态上训练的transformer进行多模态模型融合，并提出一种参数有效的模态不可知架构，形成有效的训练配方。 |
| [^179] | [Video alignment using unsupervised learning of local and global features.](http://arxiv.org/abs/2304.06841) | 本文提出了一种无需训练的视频对齐方法，利用全局和局部特征将帧转化为时间序列并使用对角化动态时间规整算法进行对齐。 |
| [^180] | [Multi-kernel Correntropy-based Orientation Estimation of IMUs: Gradient Descent Methods.](http://arxiv.org/abs/2304.06548) | 本文提出了两种基于重对比损失的算法，用于IMU定向估计，相比传统方法具有更好的准确性、鲁棒性和计算效率。 |
| [^181] | [SGDP: A Stream-Graph Neural Network Based Data Prefetcher.](http://arxiv.org/abs/2304.03864) | SGDP是一种新型基于流图神经网络的数据预取器，通过建模LBA增量流并使用图神经网络提取混合特征来进行数据预取，实验结果表明它优于其他方法。 |
| [^182] | [PAIR-Diffusion: Object-Level Image Editing with Structure-and-Appearance Paired Diffusion Models.](http://arxiv.org/abs/2303.17546) | 本论文提出了一种采用结构和外观配对扩散模型进行对象级图像编辑的方法，使用户能够精细控制图像中的不同对象属性，同时自动传播注入的外观到具有相似结构的对象。 |
| [^183] | [The Probabilistic Stability of Stochastic Gradient Descent.](http://arxiv.org/abs/2303.13093) | 本文重新定义了随机梯度下降的稳定性，并使用概率稳定性来回答深度学习理论中的一个根本问题：SGD如何从数量庞大的可能严重过拟合的解中选择有意义的神经网络解。 |
| [^184] | [Construction of Knowledge Graphs: State and Challenges.](http://arxiv.org/abs/2302.11509) | 知识图谱构建面临着增量更新和步骤之间相互作用的挑战。本文讨论了主要图模型和构建流程需求，对构建高质量知识图谱的必要步骤进行了概述，并评估了现有知识图谱构建的研究现状。 |
| [^185] | [Learning Physical Models that Can Respect Conservation Laws.](http://arxiv.org/abs/2302.11002) | 这项工作提出了ProbConserv框架，通过将守恒约束与贝叶斯更新相结合，将守恒约束纳入通用科学机器学习体系结构中，以便在学习高难度的PDE运算中应用。 |
| [^186] | [Learning from Very Little Data: On the Value of Landscape Analysis for Predicting Software Project Health.](http://arxiv.org/abs/2301.06577) | 该论文探讨了在数据稀缺的情况下，利用景观分析来预测软件项目健康状况的价值。通过对数据进行聚类并选择更好的学习控制参数，通过niSNEAK工具能够比之前的方法更快且更有效地找到更准确的配置，减少了预测错误。 |
| [^187] | [Scene-centric vs. Object-centric Image-Text Cross-modal Retrieval: A Reproducibility Study.](http://arxiv.org/abs/2301.05174) | 这项研究关注基于场景和对象的图像-文本跨模态检索的可复现性，通过选择不同体系结构的最先进模型并在不同类型的数据集上进行评估，探讨了其在不同数据集类型上的泛化能力。 |
| [^188] | [Edge Video Analytics: A Survey on Applications, Systems and Enabling Techniques.](http://arxiv.org/abs/2211.15751) | 这项研究综述了边缘视频分析（EVA）的应用、系统和支持技术。随着深度学习的发展和互联设备的普及，EVA作为一种在网络边缘处理视频数据的解决方案受到越来越多的关注。 |
| [^189] | [Data-Driven Network Neuroscience: On Data Collection and Benchmark.](http://arxiv.org/abs/2211.12421) | 本文提供了一个全面且高质量的人脑功能网络数据集，用于神经科学、机器学习和图分析的交叉研究。它利用解剖学和功能性磁共振成像来理解人脑的功能连接，并具有识别潜在神经退行性疾病的重要性。通过利用机器学习和图分析研究大脑的网络形式，能够预测这些疾病的早期发生。脑网络以图形方式表示，保留了丰富的结构和位置信息，传统的检查方法无法捕捉到这些信息。然而，缺乏公开可访问的脑网络数据限制了数据驱动的研究。 |
| [^190] | [Automatic Change-Point Detection in Time Series via Deep Learning.](http://arxiv.org/abs/2211.03860) | 本文介绍了一种通过训练神经网络自动生成新的离线检测方法的方式，应用于时间序列中的自动变点检测，其性能可与标准CUSUM分类器性能竞争。 |
| [^191] | [SpacePhish: The Evasion-space of Adversarial Attacks against Phishing Website Detectors using Machine Learning.](http://arxiv.org/abs/2210.13660) | 该论文研究了对抗机器学习中钓鱼网站检测的攻击以及针对这些攻击的逃避防御空间，并提出了一种现实威胁模型。 |
| [^192] | [Analysis of Convolutions, Non-linearity and Depth in Graph Neural Networks using Neural Tangent Kernel.](http://arxiv.org/abs/2210.09809) | 本研究通过理论方法分析了图神经网络中的卷积、非线性和深度对网络性能的影响，同时对基于图拉普拉斯和邻接矩阵的归一化方法进行了比较，并揭示了线性GNN与非线性ReLU-GNN性能相当的现象缺乏严格的理论解释。 |
| [^193] | [Characterizing and Detecting State-Sponsored Troll Activity on Social Media.](http://arxiv.org/abs/2210.08786) | 该研究提出了一种基于AI的新解决方案，通过分析喷子的轨迹，识别国家赞助的喷子帐户。该方法可以准确识别俄罗斯喷子与有机用户，可提供国家赞助影响活动的早期警报，并对保护民主进程做出贡献。 |
| [^194] | [Diffusion Models: A Comprehensive Survey of Methods and Applications.](http://arxiv.org/abs/2209.00796) | 本调研综合总结了扩散模型的方法和应用研究，包括高效采样、似然估计与特殊结构数据处理，介绍了扩散模型与其他生成模型相结合的潜力并回顾了其在不同领域的广泛应用，为进一步研究提出了可能的研究方向。 |
| [^195] | [On the Trade-Off between Actionable Explanations and the Right to be Forgotten.](http://arxiv.org/abs/2208.14137) | 本文研究了在数据删除请求的背景下，可行解释与被遗忘权之间的权衡问题。通过理论和实证分析，发现当少量数据删除请求触发预测模型的更新时，目前流行的最先进算法生成的可行解释可能失效。 |
| [^196] | [Federated Learning of Large Models at the Edge via Principal Sub-Model Training.](http://arxiv.org/abs/2208.13141) | 本文提出了一种解决资源受限边缘设备下大型模型联邦学习的方法，通过主体子模型训练，在不违反隐私承诺的前提下，使弱但重要的客户端能够参与到协作训练中。 |
| [^197] | [An Equity-Aware Recommender System for Curating Art Exhibits Based on Locally-Constrained Graph Matching.](http://arxiv.org/abs/2207.14367) | 这个论文介绍了一种注重公正性的艺术展推荐系统，通过局部约束图匹配和价值导向的资源分配，实现公共艺术展览的策划。该系统采用Schelling模型构建成本矩阵，并通过优化评分函数，软分配艺术作品到公共空间，以减少内部群体偏好、满足最低代表性和曝光标准。 |
| [^198] | [DataPerf: Benchmarks for Data-Centric AI Development.](http://arxiv.org/abs/2207.10062) | DataPerf是一个由社区主导的基准测试套件，旨在通过竞争、可比性和可重复性促进数据中心人工智能的创新。 |
| [^199] | [Bias Mitigation for Machine Learning Classifiers: A Comprehensive Survey.](http://arxiv.org/abs/2207.07068) | 这项研究提供了一份全面调研，总结了机器学习模型中实现公平性的偏差缓解方法。通过对341篇论文进行分析，研究了不同的干预过程和应用技术，并调查了评估方法。这将为从业者在开发和评估新的偏差缓解方法时提供有益的指导。 |
| [^200] | [EXACT: How to Train Your Accuracy.](http://arxiv.org/abs/2205.09615) | 本文提出了一种新的分类任务优化框架，通过引入随机性，优化期望准确率，取得了强有力的替代分类损失的结果。 |
| [^201] | [Performance of Deep Learning models with transfer learning for multiple-step-ahead forecasts in monthly time series.](http://arxiv.org/abs/2203.11196) | 本研究比较了具有迁移学习和无迁移学习的深度学习模型以及其他传统方法在月度时间序列预测中的性能，结果显示基于TCN、LSTM和CNN的深度学习模型加上迁移学习的性能超过了其他传统方法，并且直接在目标时间序列上训练的TCN和LSTM在某些预测时段的性能相当或更好。 |
| [^202] | [Reachability In Simple Neural Networks.](http://arxiv.org/abs/2203.07941) | 本研究研究了简单神经网络中的可达性问题，并证明了对于仅具有一个隐含层和一个输出维度以及仅具有一个负、零和一个正权重或偏置的神经网络来说，它是NP难度问题。 |
| [^203] | [Probabilistically Robust Recourse: Navigating the Trade-offs between Costs and Robustness in Algorithmic Recourse.](http://arxiv.org/abs/2203.06768) | 本论文提出了一种算法框架，可以帮助用户有效地管理追索成本与鲁棒性之间的权衡，以解决现实世界中机器学习模型决策所带来的问题。 |
| [^204] | [IPD:An Incremental Prototype based DBSCAN for large-scale data with cluster representatives.](http://arxiv.org/abs/2202.07870) | 该论文提出了一种增量原型DBSCAN聚类算法（IPD），可用于大规模数据的任意形状聚类，并为每个簇选择了一组代表。 |
| [^205] | [Neural networks with linear threshold activations: structure and algorithms.](http://arxiv.org/abs/2111.08117) | 本文研究了具有线性阈值激活函数的神经网络。我们确定了这类神经网络可以表示的函数的特点，并发现使用两个隐藏层可以表示该类中的任何函数。我们还给出了表示这类函数所需神经网络大小的界限，并设计了一个能够解决具有固定架构的这类神经网络的经验风险最小化问题的算法。该算法的运行时间与数据样本大小呈多项式关系，而输入维度和网络架构的大小被认为是固定常数。 |
| [^206] | [Rapid Exploration for Open-World Navigation with Latent Goal Models.](http://arxiv.org/abs/2104.05859) | 本论文描述了一个机器人学习系统，可用于自主探索和导航不同的开放世界环境。该系统通过学习得到一个潜在变量模型和图像的拓扑记忆，使用信息瓶颈约束策略，能够在不到20分钟内通过视觉目标表示在开放世界中探索并发现目标。 |
| [^207] | [A Machine Learning Approach for Modelling Parking Duration in Urban Land-use.](http://arxiv.org/abs/2008.01674) | 本研究提出了一个机器学习模型来分析汽车用户的特征对停车时长的影响，采用人工神经网络捕捉其相互关系，并运用Garson算法和LIME进行模型解释。 |
| [^208] | [High-dimensional and universally consistent k-sample tests.](http://arxiv.org/abs/1910.08883) | 本文证明了独立性测试实现了普遍一致的k样本检验，并且发现非参数独立性测试通常比多元方差分析(MANOVA)测试在高斯分布情况下表现更好。 |

# 详细

[^1]: 在协变量转移下，仅凭少量测试样本改善公平性和准确性的权衡

    Improving Fairness-Accuracy tradeoff with few Test Samples under Covariate Shift. (arXiv:2310.07535v1 [cs.LG])

    [http://arxiv.org/abs/2310.07535](http://arxiv.org/abs/2310.07535)

    在协变量转移下，我们提出了一种新的损失函数和表示匹配损失来优化模型的准确性和公平性，通过实验证明在公平性和准确性权衡方面优于其他基线算法，并且提出了一种未经研究的非对称协变量转移设置。

    

    测试数据中的协变量转移会显著降低模型的准确性和公平性表现。在这种情况下，确保不同敏感群体之间的公平性非常重要，因为这涉及到诸如刑事司法等社会影响。我们在无监督的情况下进行操作，只有一小组无标签的测试样本和一个带标签的训练集可用。为了解决这个问题，我们提出了三个贡献。第一个贡献是基于新型复合加权熵的目标函数，用于预测准确性，并通过表示匹配损失来优化公平性。我们通过实验证明，使用我们的损失函数进行优化，在几个标准数据集上在公平性-准确性权衡方面优于许多最先进的基线算法。我们的第二个贡献是一个新的设置，我们称之为非对称协变量转移，在我们所知的范围内尚未研究过非对称协变量转移。

    Covariate shift in the test data can significantly downgrade both the accuracy and the fairness performance of the model. Ensuring fairness across different sensitive groups in such settings is of paramount importance due to societal implications like criminal justice. We operate under the unsupervised regime where only a small set of unlabeled test samples along with a labeled training set is available. Towards this problem, we make three contributions. First is a novel composite weighted entropy based objective for prediction accuracy which is optimized along with a representation matching loss for fairness. We experimentally verify that optimizing with our loss formulation outperforms a number of state-of-the-art baselines in the pareto sense with respect to the fairness-accuracy tradeoff on several standard datasets. Our second contribution is a new setting we term Asymmetric Covariate Shift that, to the best of our knowledge, has not been studied before. Asymmetric covariate shift
    
[^2]: XAI方法的以人为中心的评估

    Human-Centered Evaluation of XAI Methods. (arXiv:2310.07534v1 [cs.AI])

    [http://arxiv.org/abs/2310.07534](http://arxiv.org/abs/2310.07534)

    在人工智能领域中，解释深度学习黑盒子的决策过程是一个关键挑战。本研究以用户为中心，客观评估了三种领先的解释方法的可解释性，并发现它们都提供了可解释的结果。

    

    在不断发展的人工智能领域中，一个关键的挑战是解析深度学习中所谓的“黑盒子”中的决策过程。近年来，出现了许多方法，专门用于解释各种任务的决策。特别是在图像分类等任务中，这些方法通常会识别并强调对分类器预测影响最大的关键像素。有趣的是，这种方法与人类行为相似：当我们被要求解释分类图像的理由时，我们通常会指出最显著的特征或方面。利用这种类似性，我们的研究进行了以用户为中心的研究。我们试图客观地评估三种领先的解释方法的可解释性：（1）典型局部网络、（2）遮挡和（3）层次相关传播。有趣的是，我们的结果表明，尽管这些方法所突出的区域可能差异很大，但它们都提供了可解释的结果。

    In the ever-evolving field of Artificial Intelligence, a critical challenge has been to decipher the decision-making processes within the so-called "black boxes" in deep learning. Over recent years, a plethora of methods have emerged, dedicated to explaining decisions across diverse tasks. Particularly in tasks like image classification, these methods typically identify and emphasize the pivotal pixels that most influence a classifier's prediction. Interestingly, this approach mirrors human behavior: when asked to explain our rationale for classifying an image, we often point to the most salient features or aspects. Capitalizing on this parallel, our research embarked on a user-centric study. We sought to objectively measure the interpretability of three leading explanation methods: (1) Prototypical Part Network, (2) Occlusion, and (3) Layer-wise Relevance Propagation. Intriguingly, our results highlight that while the regions spotlighted by these methods can vary widely, they all offe
    
[^3]: 可证明参数化量子电路在函数逼近中的优势

    Provable Advantage of Parameterized Quantum Circuit in Function Approximation. (arXiv:2310.07528v1 [quant-ph])

    [http://arxiv.org/abs/2310.07528](http://arxiv.org/abs/2310.07528)

    通过分析参数化量子电路在函数逼近中的表达能力，本论文展示了数据重新上传PQCs的显式构造及其对连续和平滑函数逼近的定量误差界限。

    

    理解参数化量子电路（PQCs）在完成机器学习任务中的能力是量子机器学习中最重要的问题之一。本文通过函数逼近的视角分析了PQCs的表达能力。以往对于PQCs的普适逼近定理主要是非构造性的，由此引发了以下问题：PQCs需要多大才能以给定误差逼近目标函数？我们展示了数据重新上传PQCs的显式构造，用于逼近连续和平滑函数，并建立了PQCs的宽度、深度和可训练参数数量的定量逼近误差界限。为了实现这一目标，我们利用了量子信号处理和酉线性组合的技术来构造实现多元多项式的PQCs。我们使用伯恩斯坦多项式和局部泰勒近似技术实现全局和局部逼近方法。

    Understanding the power of parameterized quantum circuits (PQCs) in accomplishing machine learning tasks is one of the most important questions in quantum machine learning. In this paper, we analyze the expressivity of PQCs through the lens of function approximation. Previously established universal approximation theorems for PQCs are mainly nonconstructive, leading us to the following question: How large do the PQCs need to be to approximate the target function up to a given error? We exhibit explicit constructions of data re-uploading PQCs for approximating continuous and smooth functions and establish quantitative approximation error bounds in terms of the width, the depth and the number of trainable parameters of the PQCs. To achieve this, we utilize techniques from quantum signal processing and linear combinations of unitaries to construct PQCs that implement multivariate polynomials. We implement global and local approximation techniques using Bernstein polynomials and local Tayl
    
[^4]: 利用后验采样和因果图先验来提高强化学习的样本效率

    Exploiting Causal Graph Priors with Posterior Sampling for Reinforcement Learning. (arXiv:2310.07518v1 [cs.LG])

    [http://arxiv.org/abs/2310.07518](http://arxiv.org/abs/2310.07518)

    本文提出了一种利用因果图先验和后验采样的方法来提高强化学习样本效率。通过同时学习完整的因果图和分解动态参数，该方法能够更自然地设计先验，并且根据先验知识程度连接了遗憾率与贝叶斯遗憾之间的关系。

    

    后验采样允许利用环境转移动态的先验知识，提高强化学习的样本效率。先验通常被指定为环境变量的（部分）因果图，相比实践中麻烦的参数分布类别指定更加自然，例如在医疗治疗研究中列出生物特征之间的已知因果依赖关系。我们提出了一种新颖的后验采样方法，名为C-PSRL，该方法同时学习更高层的完整因果图和更低层导致的分解动态的参数。对于该方法，我们提供了其贝叶斯遗憾的分析，明确地将遗憾率与先验知识程度相关联。

    Posterior sampling allows the exploitation of prior knowledge of the environment's transition dynamics to improve the sample efficiency of reinforcement learning. The prior is typically specified as a class of parametric distributions, a task that can be cumbersome in practice, often resulting in the choice of uninformative priors. In this work, we propose a novel posterior sampling approach in which the prior is given as a (partial) causal graph over the environment's variables. The latter is often more natural to design, such as listing known causal dependencies between biometric features in a medical treatment study. Specifically, we propose a hierarchical Bayesian procedure, called C-PSRL, simultaneously learning the full causal graph at the higher level and the parameters of the resulting factored dynamics at the lower level. For this procedure, we provide an analysis of its Bayesian regret, which explicitly connects the regret rate with the degree of prior knowledge. Our numerica
    
[^5]: 通过偏差关系学习实现跨模态和场景的统一遥感异常检测

    A Unified Remote Sensing Anomaly Detector Across Modalities and Scenes via Deviation Relationship Learning. (arXiv:2310.07511v1 [cs.CV])

    [http://arxiv.org/abs/2310.07511](http://arxiv.org/abs/2310.07511)

    通过学习偏差关系，构建了一个统一的遥感异常检测器，可以跨模态和场景进行检测，并且具有灵活性和成本效益。

    

    遥感异常检测器可以找到与背景不符的目标作为潜在目标。鉴于地球异常类型的多样性，跨模态和场景的统一异常检测器应该具有成本效益，并且对于新的地球观测源和异常类型具有灵活性。然而，当前的异常检测器仅限于单一模态和单一场景，因为它们旨在学习不断变化的背景分布。在普遍的异常偏差模式的激发下，即异常展现出与其局部环境的偏差特点，我们利用这一特征构建了一个统一的异常检测器。首先，我们将异常检测任务重新定义为基于偏差关系的无向双层图，其中异常评分被建模为在背景和正常对象的模式给定下的条件概率。然后，学习目标被表示为一个条件概率排序问题。此外，我们设计了一种实例化方案。

    Remote sensing anomaly detector can find the objects deviating from the background as potential targets. Given the diversity in earth anomaly types, a unified anomaly detector across modalities and scenes should be cost-effective and flexible to new earth observation sources and anomaly types. However, the current anomaly detectors are limited to a single modality and single scene, since they aim to learn the varying background distribution. Motivated by the universal anomaly deviation pattern, in that anomalies exhibit deviations from their local context, we exploit this characteristic to build a unified anomaly detector. Firstly, we reformulate the anomaly detection task as an undirected bilayer graph based on the deviation relationship, where the anomaly score is modeled as the conditional probability, given the pattern of the background and normal objects. The learning objective is then expressed as a conditional probability ranking problem. Furthermore, we design an instantiation 
    
[^6]: 利用分层特征共享进行高效数据集压缩

    Leveraging Hierarchical Feature Sharing for Efficient Dataset Condensation. (arXiv:2310.07506v1 [cs.CV])

    [http://arxiv.org/abs/2310.07506](http://arxiv.org/abs/2310.07506)

    本文提出了一种利用分层特征共享的数据参数化架构（HMN），旨在更高效地压缩数据。通过将数据存储在三层结构中，HMN能够捕捉到数据集级别、类别级别和样本级别的特征。

    

    在真实世界数据集中，数据压缩（DC）旨在合成一个显著较小的数据集，以高性能进行模型训练。最近的研究提出使用数据参数化增强DC，将数据压缩为参数化的数据容器而不是像素空间。数据参数化的直觉是编码图像的共享特征，以避免额外的存储成本。本文认识到由于分类系统的内在分层结构，图像以分层方式共享共同的特征，这是当前数据参数化方法所忽视的。为了更好地使DC与这种分层性质对齐，并在数据容器内部鼓励更高效的信息共享，我们提出了一种新颖的数据参数化架构，分层记忆网络（HMN）。HMN将压缩数据存储在三层结构中，表示数据集级别、类别级别和样本级别的特征。

    Given a real-world dataset, data condensation (DC) aims to synthesize a significantly smaller dataset that captures the knowledge of this dataset for model training with high performance. Recent works propose to enhance DC with data parameterization, which condenses data into parameterized data containers rather than pixel space. The intuition behind data parameterization is to encode shared features of images to avoid additional storage costs. In this paper, we recognize that images share common features in a hierarchical way due to the inherent hierarchical structure of the classification system, which is overlooked by current data parameterization methods. To better align DC with this hierarchical nature and encourage more efficient information sharing inside data containers, we propose a novel data parameterization architecture, Hierarchical Memory Network (HMN). HMN stores condensed data in a three-tier structure, representing the dataset-level, class-level, and instance-level fea
    
[^7]: 基于样本驱动的联邦学习用于能效和实时IoT感知

    Sample-Driven Federated Learning for Energy-Efficient and Real-Time IoT Sensing. (arXiv:2310.07497v1 [cs.LG])

    [http://arxiv.org/abs/2310.07497](http://arxiv.org/abs/2310.07497)

    本文提出了一种针对具有实时感知能力的IoT网络设计的基于样本驱动的联邦学习方法，通过控制采样过程来减轻过拟合问题，提高整体准确性，并解决能效问题。

    

    在联邦学习系统领域，最近的前沿方法在收敛分析中严重依赖于理想条件。特别地，这些方法假设IoT设备上的训练数据具有与全局数据分布相似的属性。然而，在实时感知联邦学习系统中，这种方法无法捕捉到数据特征的全面范围。为了克服这个限制，我们提出了一种针对具有实时感知能力的IoT网络设计的新方法。我们的方法考虑了由用户数据采样过程引起的泛化差距。通过有效地控制这个采样过程，我们可以减轻过拟合问题，并提高整体准确性。特别地，我们首先制定了一个优化问题，利用采样过程同时减少过拟合和最大化准确性。为了达到这个目标，我们的替代优化问题擅长处理能效问题。

    In the domain of Federated Learning (FL) systems, recent cutting-edge methods heavily rely on ideal conditions convergence analysis. Specifically, these approaches assume that the training datasets on IoT devices possess similar attributes to the global data distribution. However, this approach fails to capture the full spectrum of data characteristics in real-time sensing FL systems. In order to overcome this limitation, we suggest a new approach system specifically designed for IoT networks with real-time sensing capabilities. Our approach takes into account the generalization gap due to the user's data sampling process. By effectively controlling this sampling process, we can mitigate the overfitting issue and improve overall accuracy. In particular, We first formulate an optimization problem that harnesses the sampling process to concurrently reduce overfitting while maximizing accuracy. In pursuit of this objective, our surrogate optimization problem is adept at handling energy ef
    
[^8]: 基于模型的个体生态瞬时评估时间序列数据聚类以提高预测性能

    Model-based Clustering of Individuals' Ecological Momentary Assessment Time-series Data for Improving Forecasting Performance. (arXiv:2310.07491v1 [cs.LG])

    [http://arxiv.org/abs/2310.07491](http://arxiv.org/abs/2310.07491)

    本文通过模型聚类分析个体生态瞬时评估数据，以提高预测性能。作者比较了两种基于模型的聚类方法，并使用内在聚类评估与性能测量方法进行了分析。

    

    通过生态瞬时评估（EMA）研究，收集了跨多个个体的时间序列数据，不断监测情绪行为的各个方面。这种复杂的数据通常在个体层面上进行分析，使用个性化模型。然而，人们认为通过聚类将类似个体的附加信息整合起来，可能会增强这些模型，从而改善个体的描述。因此，本研究通过将最相似的个体分组，并进一步将这些信息用于基于群体的模型中，以提高个体的预测性能。具体而言，本文研究了两种基于模型的聚类方法，第一种方法使用个性化模型的模型提取参数，而第二种方法则通过模型的预测性能进行优化。然后，使用内在聚类评估指标（例如轮廓系数）以及性能测量方法对这两种方法进行了分析。

    Through Ecological Momentary Assessment (EMA) studies, a number of time-series data is collected across multiple individuals, continuously monitoring various items of emotional behavior. Such complex data is commonly analyzed in an individual level, using personalized models. However, it is believed that additional information of similar individuals is likely to enhance these models leading to better individuals' description. Thus, clustering is investigated with an aim to group together the most similar individuals, and subsequently use this information in group-based models in order to improve individuals' predictive performance. More specifically, two model-based clustering approaches are examined, where the first is using model-extracted parameters of personalized models, whereas the second is optimized on the model-based forecasting performance. Both methods are then analyzed using intrinsic clustering evaluation measures (e.g. Silhouette coefficients) as well as the performance o
    
[^9]: KwaiYiiMath: 技术报告

    KwaiYiiMath: Technical Report. (arXiv:2310.07488v1 [cs.CL])

    [http://arxiv.org/abs/2310.07488](http://arxiv.org/abs/2310.07488)

    KwaiYiiMath是一个用于增强数学推理能力的大型语言模型，通过应用监督微调和人类反馈强化学习，在英语和中文数学任务上取得了最先进的性能，并且能够正确解决生成的问题过程。

    

    近年来，大型语言模型（LLMs）在处理各种自然语言处理（NLP）下游任务方面展示出了显著的能力，甚至可以处理需要多步推理的数学任务。在本报告中，我们介绍了KwaiYiiMath，通过应用监督微调（SFT）和人类反馈强化学习（RLHF），增强了KwaiYiiBase1的数学推理能力，包括英语和中文的数学任务。同时，我们还构建了一个小规模的中小学数学测试集（命名为KMath），包含188个例子，用来评估模型生成的问题解决过程的正确性。实证研究表明，与类似规模的模型相比，KwaiYiiMath在GSM8k、CMath和KMath上均能取得最先进的性能（SOTA）。

    Recent advancements in large language models (LLMs) have demonstrated remarkable abilities in handling a variety of natural language processing (NLP) downstream tasks, even on mathematical tasks requiring multi-step reasoning. In this report, we introduce the KwaiYiiMath which enhances the mathematical reasoning abilities of KwaiYiiBase1, by applying Supervised Fine-Tuning (SFT) and Reinforced Learning from Human Feedback (RLHF), including on both English and Chinese mathematical tasks. Meanwhile, we also constructed a small-scale Chinese primary school mathematics test set (named KMath), consisting of 188 examples to evaluate the correctness of the problem-solving process generated by the models. Empirical studies demonstrate that KwaiYiiMath can achieve state-of-the-art (SOTA) performance on GSM8k, CMath, and KMath compared with the similar size models, respectively.
    
[^10]: 非线性嵌入用于保持哈密顿量和其他量的方法——神经Galerkin方案

    Nonlinear embeddings for conserving Hamiltonians and other quantities with Neural Galerkin schemes. (arXiv:2310.07485v1 [math.NA])

    [http://arxiv.org/abs/2310.07485](http://arxiv.org/abs/2310.07485)

    本文提出了一种基于神经Galerkin方案的非线性嵌入方法，用于保持哈密顿量和其他数量的守恒性。该方法通过在每个时间步骤计算显式嵌入到非线性参数化解场的流形上来保证数量的守恒。

    

    本文关注的是在使用非线性参数化（如深度网络）对偏微分方程的解进行近似时，保持哈密顿量、质量和动量等数量的方法。所提出的方法基于基于Dirac-Frenkel变分原理的神经Galerkin方案，通过时间序列训练非线性参数化。我们首先证明，仅仅添加在连续时间上旨在保持数量的约束是不充分的，因为参数的非线性依赖意味着即使对解场是线性的数量也变成了参数的非线性，因此在时间上离散化是具有挑战性的。相反，我们提出神经Galerkin方案，在每个时间步骤计算一个显式嵌入到非线性参数化解场的流形上，以保证数量的守恒。这些嵌入可以与标准的显式和隐式时间积分方案结合使用。

    This work focuses on the conservation of quantities such as Hamiltonians, mass, and momentum when solution fields of partial differential equations are approximated with nonlinear parametrizations such as deep networks. The proposed approach builds on Neural Galerkin schemes that are based on the Dirac--Frenkel variational principle to train nonlinear parametrizations sequentially in time. We first show that only adding constraints that aim to conserve quantities in continuous time can be insufficient because the nonlinear dependence on the parameters implies that even quantities that are linear in the solution fields become nonlinear in the parameters and thus are challenging to discretize in time. Instead, we propose Neural Galerkin schemes that compute at each time step an explicit embedding onto the manifold of nonlinearly parametrized solution fields to guarantee conservation of quantities. The embeddings can be combined with standard explicit and implicit time integration schemes
    
[^11]: 深度学习预测低级别胶质瘤的生物标志物状态并发现相关的组织形态学特征

    Deep Learning Predicts Biomarker Status and Discovers Related Histomorphology Characteristics for Low-Grade Glioma. (arXiv:2310.07464v1 [eess.IV])

    [http://arxiv.org/abs/2310.07464](http://arxiv.org/abs/2310.07464)

    该研究提出了一种基于深度学习的多生物标志物组织形态学发现者模型，利用全切片图像预测低级别胶质瘤中五个生物标志物的状态。该模型通过将单类分类纳入多实例学习框架，实现了准确的实例级别监督，在提高生物标志物预测性能方面表现出优势。

    

    生物标志物检测是低级别胶质瘤（LGG）诊断和治疗中不可或缺的一部分。然而，当前的LGG生物标志物检测方法依赖于昂贵而复杂的分子遗传学测试，需要专业人员分析结果，且常常存在内部一致性差异。为了克服这些挑战，我们提出了一种可解释的深度学习流程，即基于多实例学习（MIL）框架的多生物标志物组织形态学发现者（Multi-Beholder）模型，仅使用苏木精-伊红染色的全切片图像和切片级生物标志物状态标签预测LGG中五个生物标志物的状态。具体而言，通过将单类分类融入MIL框架，实现了准确的示例伪标签，用于实例级别监督，这极大地补充了切片级标签，并提高了生物标志物预测性能。 Multi-Beholder展示了优越的预测性能。

    Biomarker detection is an indispensable part in the diagnosis and treatment of low-grade glioma (LGG). However, current LGG biomarker detection methods rely on expensive and complex molecular genetic testing, for which professionals are required to analyze the results, and intra-rater variability is often reported. To overcome these challenges, we propose an interpretable deep learning pipeline, a Multi-Biomarker Histomorphology Discoverer (Multi-Beholder) model based on the multiple instance learning (MIL) framework, to predict the status of five biomarkers in LGG using only hematoxylin and eosin-stained whole slide images and slide-level biomarker status labels. Specifically, by incorporating the one-class classification into the MIL framework, accurate instance pseudo-labeling is realized for instance-level supervision, which greatly complements the slide-level labels and improves the biomarker prediction performance. Multi-Beholder demonstrates superior prediction performance and g
    
[^12]: 使用可解释的人工智能揭示健康衰老过程中的心电图变化

    Uncovering ECG Changes during Healthy Aging using Explainable AI. (arXiv:2310.07463v1 [eess.SP])

    [http://arxiv.org/abs/2310.07463](http://arxiv.org/abs/2310.07463)

    本文使用可解释的人工智能技术分析了健康个体的心电图数据，并识别出随年龄增长呼吸率的下降及SDANN值异常高作为老年人的指标。

    

    心血管疾病仍然是全球领先的死因。这需要对心脏衰老过程有深入的了解，以诊断心血管健康状况的限制。传统上，对个体心电图（ECG）特征随年龄变化的分析提供了这些见解。然而，这些特征虽然有信息量，但可能掩盖了底层数据关系。在本文中，我们使用深度学习模型和基于树的模型分析来自健康个体的ECG数据，包括原始信号和ECG特征格式。然后，我们使用可解释的AI技术来识别对于区分年龄组别最有辨别力的ECG特征或原始信号特征。我们的分析与基于树的分类器揭示了随年龄增长呼吸率下降，并识别出SDANN值异常高作为老年人的指标，可将其与年轻人区分开来。

    Cardiovascular diseases remain the leading global cause of mortality. This necessitates a profound understanding of heart aging processes to diagnose constraints in cardiovascular fitness. Traditionally, most of such insights have been drawn from the analysis of electrocardiogram (ECG) feature changes of individuals as they age. However, these features, while informative, may potentially obscure underlying data relationships. In this paper, we employ a deep-learning model and a tree-based model to analyze ECG data from a robust dataset of healthy individuals across varying ages in both raw signals and ECG feature format. Explainable AI techniques are then used to identify ECG features or raw signal characteristics are most discriminative for distinguishing between age groups. Our analysis with tree-based classifiers reveal age-related declines in inferred breathing rates and identifies notably high SDANN values as indicative of elderly individuals, distinguishing them from younger adul
    
[^13]: 大规模地质碳和能源储存的高效机器学习替代品

    Efficient machine-learning surrogates for large-scale geological carbon and energy storage. (arXiv:2310.07461v1 [cs.CE])

    [http://arxiv.org/abs/2310.07461](http://arxiv.org/abs/2310.07461)

    本研究提出了一种专门的机器学习（ML）模型，通过域分解和拓扑嵌入器降低训练成本，提高机器学习在大规模地质储存应用中的效率。

    

    地质碳和能源储存对于实现净零碳排放和应对气候变化至关重要。然而，由于地质因素和操作限制，它们面临不确定性，可能引发地震事件或地下水污染。为了克服这些挑战，我们提出了一种专门的机器学习（ML）模型，以高效地管理广泛的储层模型。虽然机器学习方法在地质碳储存方面具有潜力，但大规模分析所需的计算资源是一个障碍。我们开发了一种方法，通过域分解和拓扑嵌入器来降低深度神经算子模型的训练成本，以链接时空点，从而实现在模型领域内对未经训练的数据进行精确预测，提高机器学习在大规模地质储存应用中的效率。

    Geological carbon and energy storage are pivotal for achieving net-zero carbon emissions and addressing climate change. However, they face uncertainties due to geological factors and operational limitations, resulting in possibilities of induced seismic events or groundwater contamination. To overcome these challenges, we propose a specialized machine-learning (ML) model to manage extensive reservoir models efficiently.  While ML approaches hold promise for geological carbon storage, the substantial computational resources required for large-scale analysis are the obstacle. We've developed a method to reduce the training cost for deep neural operator models, using domain decomposition and a topology embedder to link spatio-temporal points. This approach allows accurate predictions within the model's domain, even for untrained data, enhancing ML efficiency for large-scale geological storage applications.
    
[^14]: ProbTS：一种用于探索深度时间序列预测的统一工具包。

    ProbTS: A Unified Toolkit to Probe Deep Time-series Forecasting. (arXiv:2310.07446v1 [cs.LG])

    [http://arxiv.org/abs/2310.07446](http://arxiv.org/abs/2310.07446)

    ProbTS是一个统一的工具包，用于协同和比较定制神经架构和深度生成模型在时间序列预测中的方法，揭示了它们的特点、优势和需要进一步研究的领域。

    

    时间序列预测在各个领域的各种应用中起着关键作用。随着深度学习的发展，这个领域分化成了两个显著的分支：一个专注于为时间序列定制特定的神经架构，另一个利用先进的深度生成模型进行概率预测。虽然这两个分支都取得了显著的进展，但它们在数据情景、方法论焦点和解码方案上的差异提出了深入而未被探索的研究问题。为了填补这一知识鸿沟，我们引入了ProbTS，这是一个创新的工具包，旨在协同和比较这两个不同的分支。ProbTS具备统一的数据模块、模块化的模型模块和全面的评估器模块，使我们能够重新审视和基准测试这两个分支的领先方法。通过ProbTS的审查，突显了它们各自的特点、相对优势和劣势，以及需要进一步研究的领域。

    Time-series forecasting serves as a linchpin in a myriad of applications, spanning various domains. With the growth of deep learning, this arena has bifurcated into two salient branches: one focuses on crafting specific neural architectures tailored for time series, and the other harnesses advanced deep generative models for probabilistic forecasting. While both branches have made significant progress, their differences across data scenarios, methodological focuses, and decoding schemes pose profound, yet unexplored, research questions. To bridge this knowledge chasm, we introduce ProbTS, a pioneering toolkit developed to synergize and compare these two distinct branches. Endowed with a unified data module, a modularized model module, and a comprehensive evaluator module, ProbTS allows us to revisit and benchmark leading methods from both branches. The scrutiny with ProbTS highlights their distinct characteristics, relative strengths and weaknesses, and areas that need further explorat
    
[^15]: 一种基于分支的深度卷积网络用于使用具有不同特征空间尺度的气象图预测巴黎雾霾的发生

    A Branched Deep Convolutional Network for Forecasting the Occurrence of Hazes in Paris using Meteorological Maps with Different Characteristic Spatial Scales. (arXiv:2310.07437v1 [physics.ao-ph])

    [http://arxiv.org/abs/2310.07437](http://arxiv.org/abs/2310.07437)

    该论文开发了一种基于分支的深度卷积网络，用于预测巴黎雾霾的发生。该网络利用气象图的不同特征空间尺度来训练，通过使用多年期的气象变量和地面能见度观测数据进行训练和验证。这些新架构提高了网络性能，在未使用训练数据的情况下进行盲预测，具有合理的预测分数。

    

    开发了一个深度学习平台，用于预测低能见度事件或雾霾的发生。它通过使用多年期的各种气象和水文变量的区域日常图作为输入特征，并将地面能见度观测作为目标来进行训练。为了更好地保留不同输入特征的特征空间信息进行训练，最近开发了两种针对巴黎雾霾的分支架构。这些新架构提高了网络的性能，在验证和盲预测评估中产生了合理的分数，使用了2021年和2022年的数据，这些数据没有在训练和验证中使用。

    A deep learning platform has been developed to forecast the occurrence of the low visibility events or hazes. It is trained by using multi-decadal daily regional maps of various meteorological and hydrological variables as input features and surface visibility observations as the targets. To better preserve the characteristic spatial information of different input features for training, two branched architectures have recently been developed for the case of Paris hazes. These new architectures have improved the performance of the network, producing reasonable scores in both validation and a blind forecasting evaluation using the data of 2021 and 2022 that have not been used in the training and validation.
    
[^16]: HealthWalk：通过基于传感器的滚轮行走者辅助促进健康与移动性

    HealthWalk: Promoting Health and Mobility through Sensor-Based Rollator Walker Assistance. (arXiv:2310.07434v1 [cs.RO])

    [http://arxiv.org/abs/2310.07434](http://arxiv.org/abs/2310.07434)

    HealthWalk通过集成传感器到滚轮行走者设计中，可以解决滚轮行走者用户姿势不好导致的健康问题，并实现其他有趣的用例。

    

    滚轮行走者能够帮助身体受限制的人提高移动能力，并给他们信心和独立性，使他们能更长时间地参与社会。然而，滚轮行走者用户往往姿势不好，导致进一步的健康问题，最坏的情况是摔倒。将传感器集成到滚轮行走者设计中可以解决这个问题，并且可以实现其他一些有趣的用例。本文简要介绍了现有系统以及该领域的当前研究方向和挑战。我们还介绍了我们早期的HealthWalk滚轮行走者原型，用于与老年人、风湿病、多发性硬化症和帕金森病患者以及视力障碍者进行数据收集。

    Rollator walkers allow people with physical limitations to increase their mobility and give them the confidence and independence to participate in society for longer. However, rollator walker users often have poor posture, leading to further health problems and, in the worst case, falls. Integrating sensors into rollator walker designs can help to address this problem and results in a platform that allows several other interesting use cases. This paper briefly overviews existing systems and the current research directions and challenges in this field. We also present our early HealthWalk rollator walker prototype for data collection with older people, rheumatism, multiple sclerosis and Parkinson patients, and individuals with visual impairments.
    
[^17]: 通过自动折扣调度从观察中进行模仿学习

    Imitation Learning from Observation with Automatic Discount Scheduling. (arXiv:2310.07433v1 [cs.RO])

    [http://arxiv.org/abs/2310.07433](http://arxiv.org/abs/2310.07433)

    我们提出了一个新颖的观察学习模仿(ILfO)框架，能够解决由于奖励信号分配错误导致代理无法学习初始行为的问题。

    

    人类通常通过观察和模仿来获得新的技能。对于机器人代理，从互联网上可用的大量无标签视频演示数据中进行学习，需要在没有访问其动作的情况下模仿专家，这是一种称为观察学习模仿（ILfO）的挑战。解决ILfO问题的常见方法是将其转化为逆向强化学习问题，利用从代理和专家观察中计算出的代理奖励。然而，我们发现在具有进展依赖性属性的任务中，这样的方法面临重大挑战；在这些任务中，代理需要在掌握后续行为之前先学习专家的前序行为。我们的研究表明，主要原因是分配给后续步骤的奖励信号妨碍了对初始行为的学习。为了解决这个挑战，我们提出了一个新颖的ILfO框架，使代理能够掌握早期行为。

    Humans often acquire new skills through observation and imitation. For robotic agents, learning from the plethora of unlabeled video demonstration data available on the Internet necessitates imitating the expert without access to its action, presenting a challenge known as Imitation Learning from Observations (ILfO). A common approach to tackle ILfO problems is to convert them into inverse reinforcement learning problems, utilizing a proxy reward computed from the agent's and the expert's observations. Nonetheless, we identify that tasks characterized by a progress dependency property pose significant challenges for such approaches; in these tasks, the agent needs to initially learn the expert's preceding behaviors before mastering the subsequent ones. Our investigation reveals that the main cause is that the reward signals assigned to later steps hinder the learning of initial behaviors. To address this challenge, we present a novel ILfO framework that enables the agent to master earl
    
[^18]: 非回溯图神经网络

    Non-backtracking Graph Neural Networks. (arXiv:2310.07430v1 [cs.LG])

    [http://arxiv.org/abs/2310.07430](http://arxiv.org/abs/2310.07430)

    非回溯图神经网络(NBA-GNN)通过不考虑先前访问节点的消息来解决图神经网络本地更新中的冗余问题，并且在随机块模型恢复方面表现出良好的性能。

    

    著名的图神经网络的消息传递更新允许使用本地和计算上可跟踪的更新来表示大规模图。然而，本地更新受到回溯的影响，即消息通过同一条边两次流动并重访先前访问的节点。由于消息流的数量随着更新的次数呈指数级增加，本地更新中的冗余阻碍了图神经网络准确识别下游任务的特定消息流。在这项工作中，我们通过非回溯的图神经网络（NBA-GNN）解决了这种冗余，该网络在更新消息时不考虑先前访问节点的消息。我们进一步研究了NBA-GNN如何缓解GNN的过度压缩，并建立了NBA-GNN和非回溯更新在随机块模型恢复方面出色性能之间的联系。我们通过实验证实了我们的NBA-

    The celebrated message-passing updates for graph neural networks allow the representation of large-scale graphs with local and computationally tractable updates. However, the local updates suffer from backtracking, i.e., a message flows through the same edge twice and revisits the previously visited node. Since the number of message flows increases exponentially with the number of updates, the redundancy in local updates prevents the graph neural network from accurately recognizing a particular message flow for downstream tasks. In this work, we propose to resolve such a redundancy via the non-backtracking graph neural network (NBA-GNN) that updates a message without incorporating the message from the previously visited node. We further investigate how NBA-GNN alleviates the over-squashing of GNNs, and establish a connection between NBA-GNN and the impressive performance of non-backtracking updates for stochastic block model recovery. We empirically verify the effectiveness of our NBA-
    
[^19]: 增强量子预测能力：利用量子Gramian角度场和CNN进行股票回报预测

    Quantum-Enhanced Forecasting: Leveraging Quantum Gramian Angular Field and CNNs for Stock Return Predictions. (arXiv:2310.07427v1 [cs.LG])

    [http://arxiv.org/abs/2310.07427](http://arxiv.org/abs/2310.07427)

    该论文提出了一种名为Quantum Gramian Angular Field (QGAF)的新方法，通过将量子计算技术与深度学习相结合，成功将股票回报时间序列数据转化为适合卷积神经网络（CNN）训练的二维图像，从而提高了预测的精度。实验证明，相比传统方法，QGAF方法具有显著的性能优势。

    

    我们提出了一种名为Quantum Gramian Angular Field (QGAF)的时间序列预测方法。该方法将量子计算技术与深度学习相结合，旨在提高时间序列分类和预测的精度。通过设计特定的量子电路，我们成功地将股票回报时间序列数据转化为适合卷积神经网络（CNN）训练的二维图像。与经典的Gramian Angular Field (GAF)方法不同，QGAF的独特之处在于消除了数据归一化和反余弦计算的需求，简化了从时间序列数据到二维图像的转换过程。为了验证该方法的有效性，我们在中国A股市场、香港股市和美国股市的数据集上进行了实验。实验结果表明，与经典的GAF方法相比，QGAF方法显著改善了预测性能。

    We propose a time series forecasting method named Quantum Gramian Angular Field (QGAF). This approach merges the advantages of quantum computing technology with deep learning, aiming to enhance the precision of time series classification and forecasting. We successfully transformed stock return time series data into two-dimensional images suitable for Convolutional Neural Network (CNN) training by designing specific quantum circuits. Distinct from the classical Gramian Angular Field (GAF) approach, QGAF's uniqueness lies in eliminating the need for data normalization and inverse cosine calculations, simplifying the transformation process from time series data to two-dimensional images. To validate the effectiveness of this method, we conducted experiments on datasets from three major stock markets: the China A-share market, the Hong Kong stock market, and the US stock market. Experimental results revealed that compared to the classical GAF method, the QGAF approach significantly improv
    
[^20]: 重审视视觉强化学习中的可塑性：数据、模块和训练阶段

    Revisiting Plasticity in Visual Reinforcement Learning: Data, Modules and Training Stages. (arXiv:2310.07418v1 [cs.LG])

    [http://arxiv.org/abs/2310.07418](http://arxiv.org/abs/2310.07418)

    本文对视觉强化学习中的可塑性进行了研究，发现数据增强对保持可塑性至关重要，评论者的可塑性损失是高效训练的主要限制因素，并且未及时恢复评论者的可塑性将导致灾难性结果。这为解决高重放比困境提供了新的策略。

    

    可塑性，神经网络随新数据演进的能力，对于高性能和样本高效的视觉强化学习(VRL)至关重要。虽然重置和正则化等方法可能能够缓解可塑性损失，但VRL框架内各种组件对代理的可塑性的影响仍然知之甚少。在这项工作中，我们进行了系统的经验性探索，重点关注了三个主要尚未充分探索的方面，并得出以下有深入见解的结论：(1)数据增强对于保持可塑性至关重要；(2)评论者的可塑性损失是阻碍高效训练的主要瓶颈；(3)在早期阶段没有及时干预以恢复评论者的可塑性，其损失将变得灾难性。这些见解提出了一种应对高重放比（RR）困境的新策略，其中加剧的可塑性损失妨碍了通过增加重放数量带来的样本效率的潜在改进。

    Plasticity, the ability of a neural network to evolve with new data, is crucial for high-performance and sample-efficient visual reinforcement learning (VRL). Although methods like resetting and regularization can potentially mitigate plasticity loss, the influences of various components within the VRL framework on the agent's plasticity are still poorly understood. In this work, we conduct a systematic empirical exploration focusing on three primary underexplored facets and derive the following insightful conclusions: (1) data augmentation is essential in maintaining plasticity; (2) the critic's plasticity loss serves as the principal bottleneck impeding efficient training; and (3) without timely intervention to recover critic's plasticity in the early stages, its loss becomes catastrophic. These insights suggest a novel strategy to address the high replay ratio (RR) dilemma, where exacerbated plasticity loss hinders the potential improvements of sample efficiency brought by increased
    
[^21]: 使用神经符号学习方法，知识图谱对齐可以获得什么？

    What can knowledge graph alignment gain with Neuro-Symbolic learning approaches?. (arXiv:2310.07417v1 [cs.AI])

    [http://arxiv.org/abs/2310.07417](http://arxiv.org/abs/2310.07417)

    本文研究了知识图谱对齐的现状与挑战，提出了使用神经符号学习方法的潜力，以实现解释性、可验证性和高质量的对齐结果。

    

    知识图谱是许多数据密集型应用程序的基础，因为它们可以表示数据与其意义和上下文的关联。对齐不同领域和提供者的知识图谱是为了实现更全面和一体化的表示。目前知识图谱对齐算法的一个严重局限是它们无法将逻辑思考和推理与词汇、结构和语义数据学习相结合。深度学习模型在知识图谱对齐方面越来越受欢迎，借鉴了它们在其他任务中的良好表现，但它们在解释性、推理能力和数据效率方面存在局限。混合的神经符号学习模型有望将逻辑和数据视角整合起来，产生可解释的高质量对齐结果，并支持以人为中心的验证方法。本文研究了知识图谱对齐领域的现状，并探讨了神经符号整合的潜力，强调了有前景的研究方向。

    Knowledge Graphs (KG) are the backbone of many data-intensive applications since they can represent data coupled with its meaning and context. Aligning KGs across different domains and providers is necessary to afford a fuller and integrated representation. A severe limitation of current KG alignment (KGA) algorithms is that they fail to articulate logical thinking and reasoning with lexical, structural, and semantic data learning. Deep learning models are increasingly popular for KGA inspired by their good performance in other tasks, but they suffer from limitations in explainability, reasoning, and data efficiency. Hybrid neurosymbolic learning models hold the promise of integrating logical and data perspectives to produce high-quality alignments that are explainable and support validation through human-centric approaches. This paper examines the current state of the art in KGA and explores the potential for neurosymbolic integration, highlighting promising research directions for co
    
[^22]: 基于Voronoi的卷积神经网络框架在拥挤视频中推动人员检测中的应用

    A Novel Voronoi-based Convolutional Neural Network Framework for Pushing Person Detection in Crowd Videos. (arXiv:2310.07416v1 [cs.CV])

    [http://arxiv.org/abs/2310.07416](http://arxiv.org/abs/2310.07416)

    本论文提出了一种基于Voronoi的卷积神经网络框架，用于在拥挤视频中微观级别地识别推动行为。通过该框架，能够更深入地了解拥挤场景中推动行为的模式和互动，并为拥挤管理策略和人流量优化提供有价值的见解。

    

    分析拥挤场景中推动行为的微观动态可以为拥挤的模式和互动提供有价值的见解。通过识别拥挤视频中的推动实例，可以更深入地了解这种行为发生的时间、地点和原因。这对于创建更有效的拥挤管理策略、优化人流量和提升整体拥挤体验至关重要。然而，手动在微观层面上识别推动行为是具有挑战性的，现有的自动方法无法检测到这种微观行为。因此，本文介绍了一种新颖的自动化框架，用于在拥挤视频中微观级别地识别推动行为。该框架包括两个主要组成部分：i）特征提取和ii）视频标记。在特征提取组件中，开发了一种基于Voronoi的新方法，用于确定与输入视频中的每个人相关的局部区域。随后，将这些区域输入到模型中进行标记和推动行为的检测。

    Analyzing the microscopic dynamics of pushing behavior within crowds can offer valuable insights into crowd patterns and interactions. By identifying instances of pushing in crowd videos, a deeper understanding of when, where, and why such behavior occurs can be achieved. This knowledge is crucial to creating more effective crowd management strategies, optimizing crowd flow, and enhancing overall crowd experiences. However, manually identifying pushing behavior at the microscopic level is challenging, and the existing automatic approaches cannot detect such microscopic behavior. Thus, this article introduces a novel automatic framework for identifying pushing in videos of crowds on a microscopic level. The framework comprises two main components: i) Feature extraction and ii) Video labeling. In the feature extraction component, a new Voronoi-based method is developed for determining the local regions associated with each person in the input video. Subsequently, these regions are fed in
    
[^23]: NuTime: 大规模时间序列预训练的数值多尺度嵌入

    NuTime: Numerically Multi-Scaled Embedding for Large-Scale Time Series Pretraining. (arXiv:2310.07402v1 [cs.LG])

    [http://arxiv.org/abs/2310.07402](http://arxiv.org/abs/2310.07402)

    本研究通过采用Transformer架构和数值多尺度嵌入模块，使时间序列自监督模型能够扩展到大规模数据集，并在大规模数据集上进行预训练。

    

    最近关于时间序列自监督模型的研究显示出学习语义表示的巨大潜力，然而，这些研究仅限于小规模数据集，例如数千个时间序列。本文的关键技术贡献针对时间序列数据的数值特性，使模型能够扩展到大规模数据集，例如百万个时间序列。我们采用Transformer架构，首先将输入划分为非重叠窗口。然后，通过窗口的标准化形状和两个标量值表示每个窗口内的均值和标准差。为了将可能具有任意数值尺度的标量值嵌入到高维向量中，我们提出了一个数值多尺度嵌入模块，枚举所有可能的标量值尺度。该模型使用提出的数值多尺度嵌入在大规模数据集上进行预训练，采用简单的对比损失函数。

    Recent research on time-series self-supervised models shows great promise in learning semantic representations. However, it has been limited to small-scale datasets, e.g., thousands of temporal sequences. In this work, we make key technical contributions that are tailored to the numerical properties of time-series data and allow the model to scale to large datasets, e.g., millions of temporal sequences. We adopt the Transformer architecture by first partitioning the input into non-overlapping windows. Each window is then characterized by its normalized shape and two scalar values denoting the mean and standard deviation within each window. To embed scalar values that may possess arbitrary numerical scales to high-dimensional vectors, we propose a numerically multi-scaled embedding module enumerating all possible scales for the scalar values. The model undergoes pretraining using the proposed numerically multi-scaled embedding with a simple contrastive objective on a large-scale dataset
    
[^24]: 深度核和图像质量估计器用于基于贝叶斯优化的机器人超声控制器优化

    Deep Kernel and Image Quality Estimators for Optimizing Robotic Ultrasound Controller using Bayesian Optimization. (arXiv:2310.07392v1 [cs.RO])

    [http://arxiv.org/abs/2310.07392](http://arxiv.org/abs/2310.07392)

    这项研究通过使用神经网络学习低维核函数，以优化基于贝叶斯优化的机器人超声控制器，解决了在不同患者中优化超声图像质量的挑战。

    

    超声是一种常用的医学成像模式，需要专家超声技师根据获取的图像手动操纵超声探头。自主机器人超声（A-RUS）是减轻超声技师工作负担的一种吸引人的替代方法。A-RUS面临的关键挑战是在不同患者中优化感兴趣区域的超声图像质量。这需要对解剖学有所了解，识别错误源以及准确的探头位置、方向和压力。在优化与机器人探头控制器相关的这些参数时，样本效率非常重要。贝叶斯优化（BO）是一种样本效率高的优化框架，最近已经应用于优化探头的二维运动。然而，还需要进一步改进样本效率，以适用于探头的高维控制。我们的目标是通过使用神经网络来学习一个低维核函数来解决这个问题。

    Ultrasound is a commonly used medical imaging modality that requires expert sonographers to manually maneuver the ultrasound probe based on the acquired image. Autonomous Robotic Ultrasound (A-RUS) is an appealing alternative to this manual procedure in order to reduce sonographers' workload. The key challenge to A-RUS is optimizing the ultrasound image quality for the region of interest across different patients. This requires knowledge of anatomy, recognition of error sources and precise probe position, orientation and pressure. Sample efficiency is important while optimizing these parameters associated with the robotized probe controller. Bayesian Optimization (BO), a sample-efficient optimization framework, has recently been applied to optimize the 2D motion of the probe. Nevertheless, further improvements are needed to improve the sample efficiency for high-dimensional control of the probe. We aim to overcome this problem by using a neural network to learn a low-dimensional kernel
    
[^25]: 基于联邦学习的组织病理图像分类和脆弱性分析

    Histopathological Image Classification and Vulnerability Analysis using Federated Learning. (arXiv:2310.07380v1 [cs.LG])

    [http://arxiv.org/abs/2310.07380](http://arxiv.org/abs/2310.07380)

    这项研究开发了一种基于联邦学习的隐私保护技术，应用于皮肤癌数据集，发现该模型容易受到数据污染攻击的影响。

    

    医疗保健是机器学习(Machine Learning，ML)的主要应用之一。传统上，ML模型由中央服务器训练，通过汇总来自各个分布式设备的数据来预测新生成数据的结果。这是一个主要关注点，因为模型可以访问敏感的用户信息，引发隐私问题。联邦学习(FL)方法可以帮助解决这个问题：全局模型将其副本发送给所有客户端，这些客户端训练这些副本，并将更新(权重)发送回给全局模型。随着时间的推移，全局模型变得更加准确。训练过程中保护了数据隐私，因为训练是在客户端设备上本地进行的。然而，全局模型容易受到数据污染的影响。我们开发了一种保护隐私的联邦学习技术，应用于一个皮肤癌数据集，并展示了模型容易受到数据污染攻击的情况。十个客户端训练模型，但其中一个故意引入了翻转标签作为攻击。这降低了模型的准确性。

    Healthcare is one of the foremost applications of machine learning (ML). Traditionally, ML models are trained by central servers, which aggregate data from various distributed devices to forecast the results for newly generated data. This is a major concern as models can access sensitive user information, which raises privacy concerns. A federated learning (FL) approach can help address this issue: A global model sends its copy to all clients who train these copies, and the clients send the updates (weights) back to it. Over time, the global model improves and becomes more accurate. Data privacy is protected during training, as it is conducted locally on the clients' devices.  However, the global model is susceptible to data poisoning. We develop a privacy-preserving FL technique for a skin cancer dataset and show that the model is prone to data poisoning attacks. Ten clients train the model, but one of them intentionally introduces flipped labels as an attack. This reduces the accurac
    
[^26]: 因果无监督语义分割

    Causal Unsupervised Semantic Segmentation. (arXiv:2310.07379v1 [cs.CV])

    [http://arxiv.org/abs/2310.07379](http://arxiv.org/abs/2310.07379)

    因果无监督语义分割（CAUSE）是一个利用因果推断的新框架，旨在实现无监督语义分割。该方法通过构建概念聚类表作为中介，并与概念自监督学习建立联系，解决了无监督分割中适当聚类水平的挑战。

    

    无监督语义分割旨在在没有人工标注注释的情况下实现高质量的语义分组。随着自监督预训练的出现，各种框架利用预训练特征训练无监督密集预测的预测头。然而，在这种无监督设置中的一个重要挑战是确定用于分割概念所需的适当聚类水平。为了解决这个问题，我们提出了一个新的框架，称为因果无监督语义分割（CAUSE），它利用了因果推断的见解。具体而言，我们桥接了面向干预的方法（即前门调整），以定义适合无监督预测的两步任务。第一步涉及构建一个概念聚类表作为中介，以离散形式表示不同粒度层次上的可能概念原型。然后，中介与随后的概念自监督学习建立了明确的联系...

    Unsupervised semantic segmentation aims to achieve high-quality semantic grouping without human-labeled annotations. With the advent of self-supervised pre-training, various frameworks utilize the pre-trained features to train prediction heads for unsupervised dense prediction. However, a significant challenge in this unsupervised setup is determining the appropriate level of clustering required for segmenting concepts. To address it, we propose a novel framework, CAusal Unsupervised Semantic sEgmentation (CAUSE), which leverages insights from causal inference. Specifically, we bridge intervention-oriented approach (i.e., frontdoor adjustment) to define suitable two-step tasks for unsupervised prediction. The first step involves constructing a concept clusterbook as a mediator, which represents possible concept prototypes at different levels of granularity in a discretized form. Then, the mediator establishes an explicit link to the subsequent concept-wise self-supervised learning for 
    
[^27]: 光子学中的实验量子自然梯度优化

    Experimental quantum natural gradient optimization in photonics. (arXiv:2310.07371v1 [quant-ph])

    [http://arxiv.org/abs/2310.07371](http://arxiv.org/abs/2310.07371)

    该论文在光子学领域首次使用光子实验估计了量子自然梯度（QNG），并通过获得He-H$^+$阳离子的解离曲线并达到化学精度，验证了QNG优化在光子设备上的超越性能。

    

    变分量子算法（VQA）将参数化量子电路和经典优化器的优点结合起来，为噪声中间尺度量子时代的实际应用提供了可能。VQA的性能严重依赖于优化方法。相比无梯度和普通梯度下降方法，量子自然梯度（QNG）可以更快地收敛，更容易避免局部极小值，从而减少电路执行成本。我们利用一个全可编程的光子芯片首次在光子学中实验估计了QNG。我们获得了He-H$^+$阳离子的解离曲线，并达到了化学精度，验证了QNG优化在光子设备上的超越性能。我们的工作为在光子学中利用QNG实现实用的近期量子应用开辟了新的前景。

    Variational quantum algorithms (VQAs) combining the advantages of parameterized quantum circuits and classical optimizers, promise practical quantum applications in the Noisy Intermediate-Scale Quantum era. The performance of VQAs heavily depends on the optimization method. Compared with gradient-free and ordinary gradient descent methods, the quantum natural gradient (QNG), which mirrors the geometric structure of the parameter space, can achieve faster convergence and avoid local minima more easily, thereby reducing the cost of circuit executions. We utilized a fully programmable photonic chip to experimentally estimate the QNG in photonics for the first time. We obtained the dissociation curve of the He-H$^+$ cation and achieved chemical accuracy, verifying the outperformance of QNG optimization on a photonic device. Our work opens up a vista of utilizing QNG in photonics to implement practical near-term quantum applications.
    
[^28]: 正交随机特征: 明确形式和尖锐不等式

    Orthogonal Random Features: Explicit Forms and Sharp Inequalities. (arXiv:2310.07370v1 [cs.LG])

    [http://arxiv.org/abs/2310.07370](http://arxiv.org/abs/2310.07370)

    该论文通过分析正交随机特征的核近似的偏差和方差，提供了明确的表达式，并得出了尖锐指数界限，支持正交随机特征比随机傅里叶特征更具信息性。

    

    随机特征通过随机化技术被引入以扩展核方法。特别地，随机傅里叶特征和正交随机特征被用来近似流行的高斯核。前者通过随机高斯矩阵执行，并在平均后得到了完全符合高斯核的结果。在这项工作中，我们分析了基于用到Haar正交矩阵的正交随机特征的核近似的偏差和方差。我们使用归一化贝塞尔函数提供了这些量的明确表达式，并推导了支持正交随机特征比随机傅里叶特征更具信息性的尖锐指数界限。

    Random features have been introduced to scale up kernel methods via randomization techniques. In particular, random Fourier features and orthogonal random features were used to approximate the popular Gaussian kernel. The former is performed by a random Gaussian matrix and leads exactly to the Gaussian kernel after averaging. In this work, we analyze the bias and the variance of the kernel approximation based on orthogonal random features which makes use of Haar orthogonal matrices. We provide explicit expressions for these quantities using normalized Bessel functions and derive sharp exponential bounds supporting the view that orthogonal random features are more informative than random Fourier features.
    
[^29]: 在本地差分隐私模型中改进稀疏线性回归的分析

    Improved Analysis of Sparse Linear Regression in Local Differential Privacy Model. (arXiv:2310.07367v1 [cs.LG])

    [http://arxiv.org/abs/2310.07367](http://arxiv.org/abs/2310.07367)

    本文在本地差分隐私模型中改进了稀疏线性回归的分析。我们提出的创新算法不仅是首个非交互式本地差分隐私算法，而且产生了一种新颖且高效的估计器。

    

    本文重新审视了在本地差分隐私模型中稀疏线性回归的问题。现有的非交互式和顺序本地模型的研究主要集中在获取基础参数为1稀疏的下界上，将这些下界扩展到更一般的k稀疏情况证明非常具有挑战性。此外，目前还不清楚是否存在高效的非交互式本地差分隐私算法。为了解决这些问题，我们首先考虑了在ε非交互式本地差分隐私模型中的问题，并对子高斯数据的L2范数估计误差提出了一个下界Ω(sqrt(dklogd)/(sqrtnε))，其中n是样本量，d是空间的维数。我们提出了一种创新的非交互式本地差分隐私算法，这是该问题的首个非交互式本地差分隐私算法。这个算法可以产生一种新颖且高效的估计器作为有价值的副产品。

    In this paper, we revisit the problem of sparse linear regression in the local differential privacy (LDP) model. Existing research in the non-interactive and sequentially local models has focused on obtaining the lower bounds for the case where the underlying parameter is $1$-sparse, and extending such bounds to the more general $k$-sparse case has proven to be challenging. Moreover, it is unclear whether efficient non-interactive LDP (NLDP) algorithms exist. To address these issues, we first consider the problem in the $\epsilon$ non-interactive LDP model and provide a lower bound of $\Omega(\frac{\sqrt{dk\log d}}{\sqrt{n}\epsilon})$ on the $\ell_2$-norm estimation error for sub-Gaussian data, where $n$ is the sample size and $d$ is the dimension of the space. We propose an innovative NLDP algorithm, the very first of its kind for the problem. As a remarkable outcome, this algorithm also yields a novel and highly efficient estimator as a valuable by-product. Our algorithm achieves an 
    
[^30]: GraphControl:为图领域迁移学习中的通用图预训练模型添加条件控制

    GraphControl: Adding Conditional Control to Universal Graph Pre-trained Models for Graph Domain Transfer Learning. (arXiv:2310.07365v1 [cs.LG])

    [http://arxiv.org/abs/2310.07365](http://arxiv.org/abs/2310.07365)

    在图领域迁移学习中，GraphControl通过添加条件控制实现了对通用图预训练模型的有效迁移，克服了不同图域间的属性语义差异问题。

    

    图结构化数据在世界中无处不在，这种数据模型了对象之间的复杂关系，为各种Web应用提供了可能。Web上每天涌现的无标签图数据为这些应用提供了巨大的潜力。图自监督算法在从丰富的无标签图数据中获得通用知识方面取得了显著成功。这些预训练模型可以应用于各种下游Web应用，节省训练时间，提高下游（目标）性能。然而，即使在表面上看起来相似的领域中，不同的图在属性语义方面也可能存在显着差异，这给将预训练模型迁移到下游任务中带来了困难，甚至是不可行性。具体而言，例如，下游任务中的附加特定任务节点信息（特异性）通常会被有意省略，以便利用预训练表示（可迁移性）。这种权衡被称为

    Graph-structured data is ubiquitous in the world which models complex relationships between objects, enabling various Web applications. Daily influxes of unlabeled graph data on the Web offer immense potential for these applications. Graph self-supervised algorithms have achieved significant success in acquiring generic knowledge from abundant unlabeled graph data. These pre-trained models can be applied to various downstream Web applications, saving training time and improving downstream (target) performance. However, different graphs, even across seemingly similar domains, can differ significantly in terms of attribute semantics, posing difficulties, if not infeasibility, for transferring the pre-trained models to downstream tasks. Concretely speaking, for example, the additional task-specific node information in downstream tasks (specificity) is usually deliberately omitted so that the pre-trained representation (transferability) can be leveraged. The trade-off as such is termed as 
    
[^31]: 通过混合GAN-CNN方法从3D结构磁共振图像诊断双相情感障碍

    Diagnosing Bipolar Disorder from 3-D Structural Magnetic Resonance Images Using a Hybrid GAN-CNN Method. (arXiv:2310.07359v1 [eess.IV])

    [http://arxiv.org/abs/2310.07359](http://arxiv.org/abs/2310.07359)

    本研究通过混合GAN-CNN模型从三维结构磁共振图像（sMRI）诊断双相情感障碍，取得了比先前工作更高的准确率和敏感性，同时使用的样本数量更少。

    

    双相情感障碍（BD）是一种通过重复的亢奋和抑郁周期进行诊断的精神疾病。由于诊断BD依赖于长时间内的主观行为评估，基于客观标准的可靠诊断并不直接。本研究通过提出一种混合GAN-CNN模型，从三维结构磁共振图像（sMRI）诊断BD来应对上述障碍。本研究的创新点在于使用sMRI样本而不是传统数据集（如功能性磁共振成像（fMRI），脑电图（EEG）和行为症状）来诊断BD，同时解决了处理sMRI样本时通常遇到的数据不足问题。还通过使用5倍交叉验证来测试不同增强比例的影响。根据结果，本研究获得了75.8％的准确率，60.3％的敏感性和82.5％的特异性，比先前工作高出3-5％，同时使用不到6％的样本数量。

    Bipolar Disorder (BD) is a psychiatric condition diagnosed by repetitive cycles of hypomania and depression. Since diagnosing BD relies on subjective behavioral assessments over a long period, a solid diagnosis based on objective criteria is not straightforward. The current study responded to the described obstacle by proposing a hybrid GAN-CNN model to diagnose BD from 3-D structural MRI Images (sMRI). The novelty of this study stems from diagnosing BD from sMRI samples rather than conventional datasets such as functional MRI (fMRI), electroencephalography (EEG), and behavioral symptoms while removing the data insufficiency usually encountered when dealing with sMRI samples. The impact of various augmentation ratios is also tested using 5-fold cross-validation. Based on the results, this study obtains an accuracy rate of 75.8%, a sensitivity of 60.3%, and a specificity of 82.5%, which are 3-5% higher than prior work while utilizing less than 6% sample counts. Next, it is demonstrated 
    
[^32]: IMITATE: 临床先验指导的分层视觉语言预训练模型

    IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training. (arXiv:2310.07355v1 [cs.CV])

    [http://arxiv.org/abs/2310.07355](http://arxiv.org/abs/2310.07355)

    IMITATE是一种临床先验指导的分层视觉语言预训练模型。它利用医学报告的层级结构，从胸部X射线图像中提取多级视觉特征，并与分层医学报告中的描述性和结论性文本进行对齐。

    

    在医学视觉语言预训练（VLP）领域，人们致力于从临床报告和相关医学图像中提取文本和图像特征。然而，大多数现有的方法可能忽视了利用临床报告固有的层级结构的机会，这些报告通常被分为描述性内容的“发现”和结论性观察的“印象”。当前的医学VLP方法往往将报告简化为一个统一的实体或分散的标记，而没有利用这种丰富的、结构化的格式。在这项工作中，我们提出了一种新的临床先验指导的VLP框架，名为IMITATE，用于从医学报告中学习结构信息，并使用分层视觉语言对齐。该框架从胸部X射线（CXR）图像中提取多级视觉特征，并将这些特征与分层医学报告中的描述性和结论性文本分别对齐。

    In the field of medical Vision-Language Pre-training (VLP), significant efforts have been devoted to deriving text and image features from both clinical reports and associated medical images. However, most existing methods may have overlooked the opportunity in leveraging the inherent hierarchical structure of clinical reports, which are generally split into `findings' for descriptive content and `impressions' for conclusive observation. Instead of utilizing this rich, structured format, current medical VLP approaches often simplify the report into either a unified entity or fragmented tokens. In this work, we propose a novel clinical prior guided VLP framework named IMITATE to learn the structure information from medical reports with hierarchical vision-language alignment. The framework derives multi-level visual features from the chest X-ray (CXR) images and separately aligns these features with the descriptive and the conclusive text encoded in the hierarchical medical report. Furth
    
[^33]: 基于原子-主题对比的转换器用于分子属性预测

    Atom-Motif Contrastive Transformer for Molecular Property Prediction. (arXiv:2310.07351v1 [cs.LG])

    [http://arxiv.org/abs/2310.07351](http://arxiv.org/abs/2310.07351)

    Atom-Motif Contrastive Transformer (AMCT) is proposed to address the issue of overlooking motif interactions in Molecular Property Prediction (MPP). AMCT explores both atom-level and motif-level interactions, enhancing the effectiveness of MPP.

    

    最近，由于其在描述图节点（即分子中的原子）之间的潜在关系方面的可靠性较高，图转换（GT）模型在分子属性预测（MPP）任务中被广泛应用。然而，大多数现有的基于GT的方法通常探索成对原子之间的基本交互，因此它们未能考虑分子中关键组分（例如由几个原子组成的功能基团）之间的重要交互。由于分子中的模式对于确定分子性质（例如毒性和溶解度）非常重要，忽视模式间的交互不可避免地影响了MPP的效果。为了解决这个问题，我们提出了一种新颖的基于原子-主题对比的转换器（AMCT），它不仅探索了原子级别的交互，还考虑了主题级别的交互。

    Recently, Graph Transformer (GT) models have been widely used in the task of Molecular Property Prediction (MPP) due to their high reliability in characterizing the latent relationship among graph nodes (i.e., the atoms in a molecule). However, most existing GT-based methods usually explore the basic interactions between pairwise atoms, and thus they fail to consider the important interactions among critical motifs (e.g., functional groups consisted of several atoms) of molecules. As motifs in a molecule are significant patterns that are of great importance for determining molecular properties (e.g., toxicity and solubility), overlooking motif interactions inevitably hinders the effectiveness of MPP. To address this issue, we propose a novel Atom-Motif Contrastive Transformer (AMCT), which not only explores the atom-level interactions but also considers the motif-level interactions. Since the representations of atoms and motifs for a given molecule are actually two different views of t
    
[^34]: 高效预训练的快速ELECTRA

    Fast-ELECTRA for Efficient Pre-training. (arXiv:2310.07347v1 [cs.CL])

    [http://arxiv.org/abs/2310.07347](http://arxiv.org/abs/2310.07347)

    提出了一种快速ELECTRA的方法，利用现有的语言模型作为辅助模型，通过温度缩放平滑主模型的输出分布，达到与最先进的ELECTRA预训练方法相媲美的性能，并显著减少了辅助模型共同训练带来的计算和内存成本。

    

    ELECTRA通过检测序列中被辅助模型替换的标记来预训练语言模型。虽然ELECTRA大大提高了效率，但其潜力受到了辅助模型带来的训练成本的限制。值得注意的是，这个与主模型共同训练的模型仅用于辅助主模型的训练，并且在训练后被丢弃。这导致大量的训练成本被白白浪费。为了缓解这个问题，我们提出了Fast-ELECTRA，它利用现有的语言模型作为辅助模型。为了构建主模型的学习课程，我们通过温度缩放和递减的方法平滑其输出分布。我们的方法与最先进的ELECTRA风格的预训练方法相媲美，同时显著减少了辅助模型共同训练带来的计算和内存成本。我们的方法还降低了模型对训练数据的敏感性。

    ELECTRA pre-trains language models by detecting tokens in a sequence that have been replaced by an auxiliary model. Although ELECTRA offers a significant boost in efficiency, its potential is constrained by the training cost brought by the auxiliary model. Notably, this model, which is jointly trained with the main model, only serves to assist the training of the main model and is discarded post-training. This results in a substantial amount of training cost being expended in vain. To mitigate this issue, we propose Fast-ELECTRA, which leverages an existing language model as the auxiliary model. To construct a learning curriculum for the main model, we smooth its output distribution via temperature scaling following a descending schedule. Our approach rivals the performance of state-of-the-art ELECTRA-style pre-training methods, while significantly eliminating the computation and memory cost brought by the joint training of the auxiliary model. Our method also reduces the sensitivity t
    
[^35]: 面向基于表格数据的学习的基础模型研究

    Towards Foundation Models for Learning on Tabular Data. (arXiv:2310.07338v1 [cs.LG])

    [http://arxiv.org/abs/2310.07338](http://arxiv.org/abs/2310.07338)

    本文提出了Tabular Foundation Models (TabFMs)，通过利用生成型表格学习的潜力，采用预训练的大规模语言模型作为基础模型，并在广泛的表格数据集上进行微调，赋予TabFMs深刻的理解和普遍的能力，从而克服了当前可转移的表格模型的限制。

    

    基于表格数据的学习支撑着众多实际应用。尽管在开发针对表格数据的有效学习模型方面已经做出了相当大的努力，但目前可转移的表格模型仍然处于初级阶段，要么缺乏直接指令跟随新任务的支持，要么忽视从不同的表格数据集中获取基础知识和能力。在本文中，我们提出了Tabular Foundation Models (TabFMs)来克服这些限制。TabFMs利用生成型表格学习的潜力，采用预训练的大规模语言模型 (LLM) 作为基础模型，并使用经过专门设计的目标在大范围的表格数据集上进行微调。这种方法赋予TabFMs深刻的理解和普遍的能力，对于表格数据的学习至关重要。我们的评估强调了TabFM的有效性：它不仅在零样本和上下文推理等遵循指令的任务中明显出色，

    Learning on tabular data underpins numerous real-world applications. Despite considerable efforts in developing effective learning models for tabular data, current transferable tabular models remain in their infancy, limited by either the lack of support for direct instruction following in new tasks or the neglect of acquiring foundational knowledge and capabilities from diverse tabular datasets. In this paper, we propose Tabular Foundation Models (TabFMs) to overcome these limitations. TabFMs harness the potential of generative tabular learning, employing a pre-trained large language model (LLM) as the base model and fine-tuning it using purpose-designed objectives on an extensive range of tabular datasets. This approach endows TabFMs with a profound understanding and universal capabilities essential for learning on tabular data. Our evaluations underscore TabFM's effectiveness: not only does it significantly excel in instruction-following tasks like zero-shot and in-context inference
    
[^36]: 在拥挤环境中探索社交运动潜空间和人类意识，以实现有效的机器人导航

    Exploring Social Motion Latent Space and Human Awareness for Effective Robot Navigation in Crowded Environments. (arXiv:2310.07335v1 [cs.RO])

    [http://arxiv.org/abs/2310.07335](http://arxiv.org/abs/2310.07335)

    本文提出了一种通过学习社交运动潜空间来生成机器人控制的新方法，实现了在拥挤环境中的有效导航。同时，引入了人类对机器人的意识概念，并证明了这种意识的融入能够改善导航性能。

    

    本文提出了一种新的社交机器人导航方法，通过从社交运动潜空间中学习生成机器人控制，实现了社交导航指标的显著改善，如成功率、导航时间和轨迹长度，同时产生了更平缓的轨迹（减少抖动和角度偏差）和更具预见性的轨迹。通过与基准模型在各种场景中的比较，证明了该方法的优越性。此外，将人类对机器人的意识概念引入社交机器人导航框架中，显示出人类意识的融入能够减少轨迹长度和平滑轨迹，因为人类能够与机器人积极互动。

    This work proposes a novel approach to social robot navigation by learning to generate robot controls from a social motion latent space. By leveraging this social motion latent space, the proposed method achieves significant improvements in social navigation metrics such as success rate, navigation time, and trajectory length while producing smoother (less jerk and angular deviations) and more anticipatory trajectories. The superiority of the proposed method is demonstrated through comparison with baseline models in various scenarios. Additionally, the concept of humans' awareness towards the robot is introduced into the social robot navigation framework, showing that incorporating human awareness leads to shorter and smoother trajectories owing to humans' ability to positively interact with the robot.
    
[^37]: 直接逻辑属性的对抗性样本：gelu-4l中的内存管理

    An Adversarial Example for Direct Logit Attribution: Memory Management in gelu-4l. (arXiv:2310.07325v1 [cs.LG])

    [http://arxiv.org/abs/2310.07325](http://arxiv.org/abs/2310.07325)

    在gelu-4l中，我们提供了证据表明内存管理对于transformer模型至关重要，并说明了Direct Logit Attribution技术的不准确之处。

    

    我们提供了一个4层transformer中内存管理的具体证据。具体来说，我们发现在前向传播过程中，模型组件一致地移除前面组件的输出，这是一种清理行为。我们的研究结果表明，解释性技术Direct Logit Attribution提供了误导性结果。我们展示了明确的例子，证明这种技术是不准确的，因为它没有考虑到清理行为。

    We provide concrete evidence for memory management in a 4-layer transformer. Specifically, we identify clean-up behavior, in which model components consistently remove the output of preceeding components during a forward pass. Our findings suggest that the interpretability technique Direct Logit Attribution provides misleading results. We show explicit examples where this technique is inaccurate, as it does not account for clean-up behavior.
    
[^38]: 使用1DCNN-attention进行多通道连续数据交叉提取以诊断电力变压器

    Multichannel consecutive data cross-extraction with 1DCNN-attention for diagnosis of power transformer. (arXiv:2310.07323v1 [cs.LG])

    [http://arxiv.org/abs/2310.07323](http://arxiv.org/abs/2310.07323)

    本文提出了一种用于电力变压器诊断的多通道连续数据交叉提取（MCDC）结构，以全面利用顺序数据中的时间信息，并引入一维卷积神经网络注意力（1DCNN-attention）机制以降低空间复杂性。实验证明了MCDC的有效性和更优越的泛化能力。

    

    电力变压器在电网基础设施中起着关键作用，其诊断对于保持稳定运行至关重要。然而，目前的变压器诊断方法主要集中在离散的溶解气体分析上，忽视了多通道连续数据的深层特征提取。未被利用的顺序数据包含反映变压器状态的重要时间信息。基于此，本文提出了多通道连续数据交叉提取（MCDC）的结构，以全面利用内在特性并评估变压器的状态。此外，为了更好地适应变压器诊断场景，引入了一维卷积神经网络注意力（1DCNN-attention）机制，并给出了一个更高效的解决方案，降低了空间复杂性。最后，与其他算法相比，验证了MCDC的有效性和更优越的泛化能力。

    Power transformer plays a critical role in grid infrastructure, and its diagnosis is paramount for maintaining stable operation. However, the current methods for transformer diagnosis focus on discrete dissolved gas analysis, neglecting deep feature extraction of multichannel consecutive data. The unutilized sequential data contains the significant temporal information reflecting the transformer condition. In light of this, the structure of multichannel consecutive data cross-extraction (MCDC) is proposed in this article in order to comprehensively exploit the intrinsic characteristic and evaluate the states of transformer. Moreover, for the better accommodation in scenario of transformer diagnosis, one dimensional convolution neural network attention (1DCNN-attention) mechanism is introduced and offers a more efficient solution given the simplified spatial complexity. Finally, the effectiveness of MCDC and the superior generalization ability, compared with other algorithms, are valida
    
[^39]: 关于交叉领域数据对德语语言模型的影响

    On the Impact of Cross-Domain Data on German Language Models. (arXiv:2310.07321v1 [cs.CL])

    [http://arxiv.org/abs/2310.07321](http://arxiv.org/abs/2310.07321)

    本研究通过对德语语言模型进行实验，发现将数据多样性置于数据质量之上的交叉领域数据集训练方法，可以显著提高模型的性能，并超过了之前的最先进模型。

    

    传统上，大型语言模型要么在通用网络抓取数据上训练，要么在特定领域的数据上。然而，生成型大型语言模型的最近成功突显了交叉领域数据集的好处。为了考察数据多样性高于质量的重要性，我们提出了一个包含五个领域文本的德语数据集，以及一个旨在包含高质量数据的数据集。通过在这两个数据集上训练参数范围从122M到750M的一系列模型，我们对多个下游任务进行了全面评估。我们的研究结果表明，使用交叉领域数据集训练的模型优于仅使用质量数据训练的模型，在先前最先进结果上提出了高达4.45%的改进。这些模型可在https://huggingface.co/ikim-uk-essen上找到。

    Traditionally, large language models have been either trained on general web crawls or domain-specific data. However, recent successes of generative large language models, have shed light on the benefits of cross-domain datasets. To examine the significance of prioritizing data diversity over quality, we present a German dataset comprising texts from five domains, along with another dataset aimed at containing high-quality data. Through training a series of models ranging between 122M and 750M parameters on both datasets, we conduct a comprehensive benchmark on multiple downstream tasks. Our findings demonstrate that the models trained on the cross-domain dataset outperform those trained on quality data alone, leading to improvements up to $4.45\%$ over the previous state-of-the-art. The models are available at https://huggingface.co/ikim-uk-essen
    
[^40]: 用于高效准确的溯源预测的分子编辑模板

    Molecule-Edit Templates for Efficient and Accurate Retrosynthesis Prediction. (arXiv:2310.07313v1 [cs.LG])

    [http://arxiv.org/abs/2310.07313](http://arxiv.org/abs/2310.07313)

    METRO是一个机器学习模型，利用最小模板进行反应预测，解决了溯源预测中的计算开销大和解释性差的问题，实现了最先进的效果。

    

    溯源涉及确定从简单前体合成复杂分子的一系列反应。由于这在有机化学中是一个挑战，机器学习提供了解决方案，特别是用于预测给定目标分子的可能反应底物。这些解决方案主要分为基于模板和基于非模板两类。前者高效但依赖于大量预定义的反应模式，而后者虽然更灵活，但计算开销大且解释性较差。为解决这些问题，我们引入了METRO（用于溯源合成的分子编辑模板），这是一个利用最小模板进行反应预测的机器学习模型，简化了反应模式，减少了计算开销，并在标准基准测试中取得了最先进的结果。

    Retrosynthesis involves determining a sequence of reactions to synthesize complex molecules from simpler precursors. As this poses a challenge in organic chemistry, machine learning has offered solutions, particularly for predicting possible reaction substrates for a given target molecule. These solutions mainly fall into template-based and template-free categories. The former is efficient but relies on a vast set of predefined reaction patterns, while the latter, though more flexible, can be computationally intensive and less interpretable. To address these issues, we introduce METRO (Molecule-Edit Templates for RetrOsynthesis), a machine-learning model that predicts reactions using minimal templates - simplified reaction patterns capturing only essential molecular changes - reducing computational overhead and achieving state-of-the-art results on standard benchmarks.
    
[^41]: WiGenAI: 通过扩散模型实现无线和生成式人工智能的交织

    WiGenAI: The Symphony of Wireless and Generative AI via Diffusion Models. (arXiv:2310.07312v1 [cs.IT])

    [http://arxiv.org/abs/2310.07312](http://arxiv.org/abs/2310.07312)

    WiGenAI通过引入扩散模型，将生成式人工智能应用于无线通信系统中，为研究奠定基础。这篇文章介绍了扩散模型作为生成模型的最新范式，并讨论了它在无线通信系统中的应用。通过两个案例研究展示了扩散模型在开发韧性的AI本地通信系统中的潜力。

    

    创新的基础模型，如GPT-3和稳定的扩散模型，已经在人工智能领域实现了范式转变，向生成式人工智能系统发展。从数据通信和网络的角度来看，人工智能和机器学习算法预计将广泛应用于未来无线通信系统的新一代中，强调了在新兴通信场景中需要新颖的AI本地解决方案。本文介绍生成式人工智能在无线通信系统中的应用，为该领域的研究奠定基础。介绍了扩散型生成模型作为生成模型的最新范式，并讨论了它们在无线通信系统中的应用。还提供了两个案例研究，展示了如何利用扩散模型开发具有韧性的AI本地通信系统。具体而言，我们提出了一种基于扩散模型的生成模型，以展示其在生成模型的应用中的优势。

    Innovative foundation models, such as GPT-3 and stable diffusion models, have made a paradigm shift in the realm of artificial intelligence (AI) towards generative AI-based systems. In unison, from data communication and networking perspective, AI and machine learning (AI/ML) algorithms are envisioned to be pervasively incorporated into the future generations of wireless communications systems, highlighting the need for novel AI-native solutions for the emergent communication scenarios. In this article, we outline the applications of generative AI in wireless communication systems to lay the foundations for research in this field. Diffusion-based generative models, as the new state-of-the-art paradigm of generative models, are introduced, and their applications in wireless communication systems are discussed. Two case studies are also presented to showcase how diffusion models can be exploited for the development of resilient AI-native communication systems. Specifically, we propose de
    
[^42]: SNOiC: 基于软标签和噪声混合的开放意图分类模型

    SNOiC: Soft Labeling and Noisy Mixup based Open Intent Classification Model. (arXiv:2310.07306v1 [cs.LG])

    [http://arxiv.org/abs/2310.07306](http://arxiv.org/abs/2310.07306)

    SNOiC模型是一种基于软标签和噪声混合的开放意图分类模型，通过减少偏差和生成伪数据来提高识别开放意图的性能。

    

    本文提出了一种基于软标签和噪声混合的开放意图分类模型（SNOiC）。先前的工作大多使用基于阈值的方法来识别开放意图，容易出现过拟合问题，并可能产生有偏的预测结果。此外，对于开放意图类别需要更多可用数据的需求也是现有模型的另一个限制。SNOiC将软标签和噪声混合策略结合起来，以减少偏差并为开放意图类别生成伪数据。在四个基准数据集上的实验结果显示，SNOiC模型在识别开放意图方面的最低和最高性能分别为68.72％和94.71％。此外，与最先进的模型相比，SNOiC模型在识别开放意图方面的性能提升了0.93％（最低）和12.76％（最高）。通过分析所提出模型中使用的各种参数进一步证明了模型的有效性。还进行了消融研究，其中

    This paper presents a Soft Labeling and Noisy Mixup-based open intent classification model (SNOiC). Most of the previous works have used threshold-based methods to identify open intents, which are prone to overfitting and may produce biased predictions. Additionally, the need for more available data for an open intent class presents another limitation for these existing models. SNOiC combines Soft Labeling and Noisy Mixup strategies to reduce the biasing and generate pseudo-data for open intent class. The experimental results on four benchmark datasets show that the SNOiC model achieves a minimum and maximum performance of 68.72\% and 94.71\%, respectively, in identifying open intents. Moreover, compared to state-of-the-art models, the SNOiC model improves the performance of identifying open intents by 0.93\% (minimum) and 12.76\% (maximum). The model's efficacy is further established by analyzing various parameters used in the proposed model. An ablation study is also conducted, which
    
[^43]: 超越记忆：通过大型语言模型进行推理来侵犯隐私

    Beyond Memorization: Violating Privacy Via Inference with Large Language Models. (arXiv:2310.07298v1 [cs.AI])

    [http://arxiv.org/abs/2310.07298](http://arxiv.org/abs/2310.07298)

    该论文首次全面研究了预训练大型语言模型从文本中推断个人属性的能力，发现当前的模型可以以较低的成本和时间比例，准确地推断出多种个人属性，这引发了隐私泄露的新威胁。

    

    目前关于大型语言模型（LLMs）的隐私研究主要集中在提取记忆训练数据的问题上。同时，模型的推理能力已大幅增强。这引发了一个关键问题，即当前的LLMs是否能够通过从推理时给出的文本中推断个人属性来侵犯个人隐私。在这项工作中，我们首次对预训练LLMs从文本中推断个人属性的能力进行了全面研究。我们构建了一个包含真实Reddit个人资料的数据集，并且显示当前的LLMs可以推断出各种各样的个人属性（例如，位置、收入、性别），在成本（100倍）和时间（240倍）上仅需人类的一小部分，达到了最高1的准确率达到85％，最高3的准确率达到95.8％。随着人们越来越多地与由LLM驱动的聊天机器人在生活的各个方面进行互动，我们还探讨了侵犯隐私的聊天机器人通过似乎无关的对话试图提取个人信息的新威胁。

    Current privacy research on large language models (LLMs) primarily focuses on the issue of extracting memorized training data. At the same time, models' inference capabilities have increased drastically. This raises the key question of whether current LLMs could violate individuals' privacy by inferring personal attributes from text given at inference time. In this work, we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text. We construct a dataset consisting of real Reddit profiles, and show that current LLMs can infer a wide range of personal attributes (e.g., location, income, sex), achieving up to $85\%$ top-1 and $95.8\%$ top-3 accuracy at a fraction of the cost ($100\times$) and time ($240\times$) required by humans. As people increasingly interact with LLM-powered chatbots across all aspects of life, we also explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemi
    
[^44]: 通过扩散行为实现得分正则化策略优化

    Score Regularized Policy Optimization through Diffusion Behavior. (arXiv:2310.07297v1 [cs.LG])

    [http://arxiv.org/abs/2310.07297](http://arxiv.org/abs/2310.07297)

    通过利用扩散行为模型，我们提出了一种在离线强化学习中用于优化策略的得分正则化方法，从而避免了耗时且计算密集的扩散采样方案，并在D4RL任务上实现了超过25倍的动作采样速度提升。

    

    最近的离线强化学习研究展示了扩散建模的巨大潜力，这充分展现了其在表达异质行为策略方面的优越性。然而，从扩散策略中采样非常缓慢，因为需要数十到数百次迭代推理步骤来进行一次动作采样。为了解决这个问题，我们提出了一种从评论家模型和预训练的扩散行为模型中提取高效确定性推理策略的方法，在优化过程中利用后者直接对策略梯度进行正则化，使用行为分布的得分函数。我们的方法在训练和评估过程中充分发挥了扩散建模的强大生成能力，同时完全绕过了计算密集和耗时的扩散采样方案。在D4RL任务上的广泛结果显示，我们的方法将动作采样速度提高了超过25倍，相比于各种领先的基于扩散的方法在运动任务中。

    Recent developments in offline reinforcement learning have uncovered the immense potential of diffusion modeling, which excels at representing heterogeneous behavior policies. However, sampling from diffusion policies is considerably slow because it necessitates tens to hundreds of iterative inference steps for one action. To address this issue, we propose to extract an efficient deterministic inference policy from critic models and pretrained diffusion behavior models, leveraging the latter to directly regularize the policy gradient with the behavior distribution's score function during optimization. Our method enjoys powerful generative capabilities of diffusion modeling while completely circumventing the computationally intensive and time-consuming diffusion sampling scheme, both during training and evaluation. Extensive results on D4RL tasks show that our method boosts action sampling speed by more than 25 times compared with various leading diffusion-based methods in locomotion ta
    
[^45]: BioT5：在生物学中利用化学知识和自然语言关联丰富跨模态整合

    BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations. (arXiv:2310.07276v1 [cs.CL])

    [http://arxiv.org/abs/2310.07276](http://arxiv.org/abs/2310.07276)

    BioT5是一个全面的预训练框架，在生物学中利用化学知识和自然语言关联丰富了跨模态整合，通过鲁棒的分子表示和上下文知识提取，实现了更有效的信息利用，展现出卓越的性能。

    

    最近在生物研究领域的进展利用分子、蛋白质和自然语言的整合来增强药物发现。然而，当前的模型存在一些限制，如生成无效的分子SMILES、对上下文信息的利用不足以及对结构化和非结构化知识的等量处理。为了解决这些问题，我们提出了一个全面的预训练框架BioT5，它通过化学知识和自然语言关联丰富了生物学中的跨模态整合。BioT5利用SELFIES进行100%鲁棒的分子表示，并从非结构化的生物文献中提取生物实体周围上下文的知识。此外，BioT5区分结构化和非结构化知识，从而更有效地利用信息。在微调后，BioT5在各种任务中展现出卓越的性能，表明其强大的能力。

    Recent advancements in biological research leverage the integration of molecules, proteins, and natural language to enhance drug discovery. However, current models exhibit several limitations, such as the generation of invalid molecular SMILES, underutilization of contextual information, and equal treatment of structured and unstructured knowledge. To address these issues, we propose $\mathbf{BioT5}$, a comprehensive pre-training framework that enriches cross-modal integration in biology with chemical knowledge and natural language associations. $\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular representations and extracts knowledge from the surrounding context of bio-entities in unstructured biological literature. Furthermore, $\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge, leading to more effective utilization of information. After fine-tuning, BioT5 shows superior performance across a wide range of tasks, demonstrating its strong capability 
    
[^46]: 为什么锐度感知最小化比随机梯度下降更好地推广？(arXiv:2310.07269v1 [cs.LG])

    Why Does Sharpness-Aware Minimization Generalize Better Than SGD?. (arXiv:2310.07269v1 [cs.LG])

    [http://arxiv.org/abs/2310.07269](http://arxiv.org/abs/2310.07269)

    这项研究填补了Sharpness-Aware Minimization（SAM）相对于随机梯度下降（SGD）的一定数据模型和卷积神经网络中泛化更好的空白，解释了SAM的优势，尤其是在早期阶段防止噪声学习的能力。

    

    过拟合的挑战在大型神经网络的训练中变得越来越重要，它指的是模型记忆训练数据，但在测试数据上无法推广。为了解决这个挑战，锐度感知最小化（SAM）已经成为一种有前途的训练方法，可以在存在标签噪声的情况下改善神经网络的泛化性能。然而，对于非线性神经网络和分类任务的情况下，SAM的工作方式仍然缺乏深入理解。本文通过展示为什么在特定数据模型和两层卷积ReLU网络中，SAM比随机梯度下降更好地推广，来弥补这一空白。我们所研究问题的损失景观是非光滑的，因此目前关于SAM成功的解释基于Hessian信息是不足够的。我们的结果解释了SAM的优势，特别是它在早期阶段防止了噪声学习的能力，从而提高了泛化性能。

    The challenge of overfitting, in which the model memorizes the training data and fails to generalize to test data, has become increasingly significant in the training of large neural networks. To tackle this challenge, Sharpness-Aware Minimization (SAM) has emerged as a promising training method, which can improve the generalization of neural networks even in the presence of label noise. However, a deep understanding of how SAM works, especially in the setting of nonlinear neural networks and classification tasks, remains largely missing. This paper fills this gap by demonstrating why SAM generalizes better than Stochastic Gradient Descent (SGD) for a certain data model and two-layer convolutional ReLU networks. The loss landscape of our studied problem is nonsmooth, thus current explanations for the success of SAM based on the Hessian information are insufficient. Our result explains the benefits of SAM, particularly its ability to prevent noise learning in the early stages, thereby f
    
[^47]: RaftFed:一种轻量级的用于车载群体智能的联邦学习框架

    RaftFed: A Lightweight Federated Learning Framework for Vehicular Crowd Intelligence. (arXiv:2310.07268v1 [cs.LG])

    [http://arxiv.org/abs/2310.07268](http://arxiv.org/abs/2310.07268)

    RaftFed是一种面向车载群体智能的轻量级联邦学习框架，解决了车载群体智能中存在的可靠性和数据异质性问题，实现了隐私保护和性能优化。

    

    车载群体智能是一个新兴的研究领域。通过先进的车载自组织网络和人工智能的支持，各种车载群体智能应用开始出现，如协作感知、定位和制图。车载群体智能应用的协作性通常需要参与者之间共享数据，从而形成网络范围内的智能。如何在不损害数据隐私的情况下实现这个过程仍然是一个具有挑战性的问题。尽管联邦学习是解决这个问题的一种有希望的工具，但将传统的联邦学习框架适应于车载群体智能是非常困难的。首先，由于存在信道条件不利的停滞者，车载群体智能中的集中式模型聚合是不可靠的。其次，现有的联邦学习方案容易受到非独立同分布数据的影响，而车载群体智能中的数据异质性加剧了这一问题。本文提出了一种名为RaftFed的新型联邦学习框架，以促进保护隐私的车载群体智能。实验结果表明，RaftFed在数据隐私保护和性能方面都取得了良好的效果。

    Vehicular crowd intelligence (VCI) is an emerging research field. Facilitated by state-of-the-art vehicular ad-hoc networks and artificial intelligence, various VCI applications come to place, e.g., collaborative sensing, positioning, and mapping. The collaborative property of VCI applications generally requires data to be shared among participants, thus forming network-wide intelligence. How to fulfill this process without compromising data privacy remains a challenging issue. Although federated learning (FL) is a promising tool to solve the problem, adapting conventional FL frameworks to VCI is nontrivial. First, the centralized model aggregation is unreliable in VCI because of the existence of stragglers with unfavorable channel conditions. Second, existing FL schemes are vulnerable to Non-IID data, which is intensified by the data heterogeneity in VCI. This paper proposes a novel federated learning framework called RaftFed to facilitate privacy-preserving VCI. The experimental resu
    
[^48]: 基于严重程度的发音障碍分类。一项系统性综述。

    Classification of Dysarthria based on the Levels of Severity. A Systematic Review. (arXiv:2310.07264v1 [cs.LG])

    [http://arxiv.org/abs/2310.07264](http://arxiv.org/abs/2310.07264)

    本系统性综述分析了基于严重程度对发音障碍进行分类的方法，并评估了最有效的特征集合和类型以及最佳的人工智能技术。

    

    发音障碍是一种神经性语言障碍，严重影响患者的沟通能力和生活质量。准确和客观地对发音障碍进行分类和确定其严重程度对于有效治疗干预至关重要。虽然传统的言语病理学家评估普遍存在，并且往往是主观的、耗时的，并且可能因从业人员而异。新兴的基于机器学习的模型显示出提供更客观的发音障碍评估的潜力，增强了诊断准确性和可靠性。本系统性综述旨在全面分析目前用于基于严重程度分类发音障碍的方法。具体而言，本综述将重点确定可以用于自动患者分类的最有效的特征集合和类型，并评估最佳的人工智能技术。我们将系统地审查现有的文献。

    Dysarthria is a neurological speech disorder that can significantly impact affected individuals' communication abilities and overall quality of life. The accurate and objective classification of dysarthria and the determination of its severity are crucial for effective therapeutic intervention. While traditional assessments by speech-language pathologists (SLPs) are common, they are often subjective, time-consuming, and can vary between practitioners. Emerging machine learning-based models have shown the potential to provide a more objective dysarthria assessment, enhancing diagnostic accuracy and reliability. This systematic review aims to comprehensively analyze current methodologies for classifying dysarthria based on severity levels. Specifically, this review will focus on determining the most effective set and type of features that can be used for automatic patient classification and evaluating the best AI techniques for this purpose. We will systematically review the literature o
    
[^49]: 深度ReLU网络和高阶有限元方法II：切比雪夫模拟

    Deep ReLU networks and high-order finite element methods II: Chebyshev emulation. (arXiv:2310.07261v1 [math.NA])

    [http://arxiv.org/abs/2310.07261](http://arxiv.org/abs/2310.07261)

    本论文研究了在有限分割上使用深度ReLU神经网络对连续分段多项式函数的表达速率和稳定性，提出了一种使用切比雪夫多项式展开系数进行编码的新颖ReLU NN替代模型构造，与基于ReLU NN模拟多项式的构造相比，在表达速率和稳定性方面获得了更好的界限。

    

    研究了在有界区间$(a,b)$上的任意有限分割$\mathcal{T}$上，连续的、分段多项式函数的深度ReLU神经网络（NNs）在定义NN的参数数量方面对Sobolev范数的表达速率和稳定性。开发了新颖的ReLU NN替代模型构造，使用切比雪夫多项式展开系数对近似函数进行编码。可以通过将函数在Clenshaw-Curtis点上的值使用快速傅里叶逆变换轻松地计算出切比雪夫系数。在表达速率和稳定性方面获得了优于基于ReLU NN模拟多项式的构造[Opschoor，Petersen，Schwab，2020]的界限。所有模拟上界都明确地与区间的（任意）分割、目标模拟精度和分割中每个元素的多项式次数有关。提供了ReLU NN模拟误差估计，适用于各种情况。

    Expression rates and stability in Sobolev norms of deep ReLU neural networks (NNs) in terms of the number of parameters defining the NN for continuous, piecewise polynomial functions, on arbitrary, finite partitions $\mathcal{T}$ of a bounded interval $(a,b)$ are addressed. Novel constructions of ReLU NN surrogates encoding the approximated functions in terms of Chebyshev polynomial expansion coefficients are developed. Chebyshev coefficients can be computed easily from the values of the function in the Clenshaw--Curtis points using the inverse fast Fourier transform. Bounds on expression rates and stability that are superior to those of constructions based on ReLU NN emulations of monomials considered in [Opschoor, Petersen, Schwab, 2020] are obtained. All emulation bounds are explicit in terms of the (arbitrary) partition of the interval, the target emulation accuracy and the polynomial degree in each element of the partition. ReLU NN emulation error estimates are provided for variou
    
[^50]: ADMEOOD: 药物属性预测的超分布基准

    ADMEOOD: Out-of-Distribution Benchmark for Drug Property Prediction. (arXiv:2310.07253v1 [cs.LG])

    [http://arxiv.org/abs/2310.07253](http://arxiv.org/abs/2310.07253)

    ADMEOOD是一个设计用于药物属性预测的超分布基准，包含27个药物属性和两种数据转移（噪声转移和概念冲突漂移）。

    

    获得准确有效的药物分子信息是一项关键而具有挑战性的任务。然而，过去100年来，化学知识和信息来自不同地区、实验室和实验目的的积累。在超分布（OOD）问题中，噪音和不一致性很少被探索，这可能导致弱鲁棒性和不满意的性能。本研究提出了一个新的基准ADMEOOD, 一个特别设计用于药物属性预测的系统超分布数据集策划者和基准。ADMEOOD从Chembl和相关文献中获得了27个药物属性。另外，它还包括两种超分布数据转移：噪声转移和概念冲突漂移（CCD）。噪声转移通过将环境分类为不同的置信水平来响应噪声水平。另一方面，CCD描述了原始数据中标签不一致的数据。

    Obtaining accurate and valid information for drug molecules is a crucial and challenging task. However, chemical knowledge and information have been accumulated over the past 100 years from various regions, laboratories, and experimental purposes. Little has been explored in terms of the out-of-distribution (OOD) problem with noise and inconsistency, which may lead to weak robustness and unsatisfied performance. This study proposes a novel benchmark ADMEOOD, a systematic OOD dataset curator and benchmark specifically designed for drug property prediction. ADMEOOD obtained 27 ADME (Absorption, Distribution, Metabolism, Excretion) drug properties from Chembl and relevant literature. Additionally, it includes two kinds of OOD data shifts: Noise Shift and Concept Conflict Drift (CCD). Noise Shift responds to the noise level by categorizing the environment into different confidence levels. On the other hand, CCD describes the data which has inconsistent label among the original data. Finall
    
[^51]: 一种基于预训练CNN和GRU注意力机制的图像描述生成的比较研究

    A Comparative Study of Pre-trained CNNs and GRU-Based Attention for Image Caption Generation. (arXiv:2310.07252v1 [cs.CV])

    [http://arxiv.org/abs/2310.07252](http://arxiv.org/abs/2310.07252)

    本文提出了一种基于预训练CNN和GRU注意力机制的深度神经网络框架，用于图像描述生成。该方法通过多个预训练的卷积神经网络从图像中提取特征，并使用基于GRU的解码器生成描述性句子。实验证明，该方法在MSCOCO和Flickr30k数据集上取得了竞争性的成绩。

    

    图像描述生成是一项具有挑战性的任务，涉及使用计算机视觉和自然语言处理技术为图像生成文本描述。本文提出了一种使用基于GRU的注意力机制的深度神经网络框架，用于图像描述生成。我们的方法使用多个预训练的卷积神经网络作为编码器从图像中提取特征，并使用基于GRU的语言模型作为解码器生成描述性句子。为了提高性能，我们将Bahdanau注意力模型与GRU解码器集成在一起，使其能够学习专注于特定的图像部分。我们使用MSCOCO和Flickr30k数据集对我们的方法进行评估，并展示其与最先进方法相比具有竞争力的分数。我们提出的框架可以弥合计算机视觉和自然语言之间的差距，并可以扩展到特定领域。

    Image captioning is a challenging task involving generating a textual description for an image using computer vision and natural language processing techniques. This paper proposes a deep neural framework for image caption generation using a GRU-based attention mechanism. Our approach employs multiple pre-trained convolutional neural networks as the encoder to extract features from the image and a GRU-based language model as the decoder to generate descriptive sentences. To improve performance, we integrate the Bahdanau attention model with the GRU decoder to enable learning to focus on specific image parts. We evaluate our approach using the MSCOCO and Flickr30k datasets and show that it achieves competitive scores compared to state-of-the-art methods. Our proposed framework can bridge the gap between computer vision and natural language and can be extended to specific domains.
    
[^52]: 在BraTS数据集中使用生成对抗网络从已有的模态生成缺失的MRI序列

    Synthesizing Missing MRI Sequences from Available Modalities using Generative Adversarial Networks in BraTS Dataset. (arXiv:2310.07250v1 [q-bio.QM])

    [http://arxiv.org/abs/2310.07250](http://arxiv.org/abs/2310.07250)

    本文提出了一种利用生成对抗网络从已有的模态生成缺失的MRI序列的方法，在BraTS数据集上取得了有希望的结果。

    

    高度侵袭性和致命的胶质母细胞瘤是一种常见的脑癌。磁共振成像（MRI）由于其无创和无辐射性质，在胶质母细胞瘤患者的诊断、治疗计划和随访中发挥着重要作用。国际脑肿瘤分割（BraTS）挑战为利用四种结构性MRI扫描（T1、T1Gd、T2、T2-FLAIR）准确高效地分割胶质母细胞瘤亚区域提供了许多人工智能算法。然而，这四个MRI序列不总是可用的。为解决这个问题，可以利用生成对抗网络（GANs）合成缺失的MRI序列。在本文中，我们实现并利用开源的GAN方法，以任三个MRI序列作为输入生成缺失的第四个结构序列。我们的方法贡献给了社区驱动的通用深度学习框架（GaNDLF），并在合成缺失的MRI序列方面取得了有希望的结果。

    Glioblastoma is a highly aggressive and lethal form of brain cancer. Magnetic resonance imaging (MRI) plays a significant role in the diagnosis, treatment planning, and follow-up of glioblastoma patients due to its non-invasive and radiation-free nature. The International Brain Tumor Segmentation (BraTS) challenge has contributed to generating numerous AI algorithms to accurately and efficiently segment glioblastoma sub-compartments using four structural (T1, T1Gd, T2, T2-FLAIR) MRI scans. However, these four MRI sequences may not always be available. To address this issue, Generative Adversarial Networks (GANs) can be used to synthesize the missing MRI sequences. In this paper, we implement and utilize an open-source GAN approach that takes any three MRI sequences as input to generate the missing fourth structural sequence. Our proposed approach is contributed to the community-driven generally nuanced deep learning framework (GaNDLF) and demonstrates promising results in synthesizing 
    
[^53]: 利用Pix2Pix GANs进行恶劣天气中的人群计数的图像去噪

    Crowd Counting in Harsh Weather using Image Denoising with Pix2Pix GANs. (arXiv:2310.07245v1 [cs.CV])

    [http://arxiv.org/abs/2310.07245](http://arxiv.org/abs/2310.07245)

    本文提出了使用Pix2Pix GANs对恶劣天气中的人群图像进行去噪来提高人群计数的性能。通过训练生成对抗网络来消除噪声和模糊效果，并在推断引擎中应用预训练的生成器来估计人群密度。实验证明，这种方法在需要高可靠性和准确性时具有重要作用。

    

    视觉人群计数使用卷积神经网络等深度学习模型估计人群密度。模型的性能严重依赖于组成人群图像的训练数据的质量。在恶劣天气如雾、尘土和低光条件下，噪声和模糊图像可能会严重降低推断性能。本文提出了首先使用Pix2Pix生成对抗网络（GAN）对人群图像进行去噪，然后将其传递给计数模型。通过使用从原始人群图像生成的合成噪声图像训练Pix2Pix网络，然后将预训练的生成器用于推断引擎以估计不可见的噪声人群图像中的人群密度。使用JHU-Crowd数据集对性能进行测试，以验证所提方法在高可靠性和准确性要求时的重要性。

    Visual crowd counting estimates the density of the crowd using deep learning models such as convolution neural networks (CNNs). The performance of the model heavily relies on the quality of the training data that constitutes crowd images. In harsh weather such as fog, dust, and low light conditions, the inference performance may severely degrade on the noisy and blur images. In this paper, we propose the use of Pix2Pix generative adversarial network (GAN) to first denoise the crowd images prior to passing them to the counting model. A Pix2Pix network is trained using synthetic noisy images generated from original crowd images and then the pretrained generator is then used in the inference engine to estimate the crowd density in unseen, noisy crowd images. The performance is tested on JHU-Crowd dataset to validate the significance of the proposed method particularly when high reliability and accuracy are required.
    
[^54]: 结构健康监测应用中的随机裂纹扩展过程的代理模型

    Surrogate modeling for stochastic crack growth processes in structural health monitoring applications. (arXiv:2310.07241v1 [stat.ML])

    [http://arxiv.org/abs/2310.07241](http://arxiv.org/abs/2310.07241)

    本文提出了一种代理模型用于预测结构中裂纹的扩展，并成功地编码了不同的随机不确定性来源。该模型基于高斯过程回归模型，能够生成先验分布用于贝叶斯结构健康监测任务。

    

    疲劳裂纹扩展是金属结构中最常见的一种破坏类型，对其可靠性有重要影响。最近在结构健康监测领域的进展促使使用结构响应数据来预测不确定条件下未来的裂纹扩展，以实现向预测性维修的过渡。准确地表示随机裂纹扩展过程中不同的不确定性来源是一项非常困难的任务。本研究在基于物理模型的随机裂纹扩展建模的基础上进行了探索，考虑了材料和载荷相关的不确定性。本文旨在构建计算效率高、概率代理模型，能够成功地编码这些不同的不确定性来源。采用了受潜变量建模启发的方法，利用高斯过程回归模型使代理模型可以用于为不同的贝叶斯结构健康监测任务生成先验分布。

    Fatigue crack growth is one of the most common types of deterioration in metal structures with significant implications on their reliability. Recent advances in Structural Health Monitoring (SHM) have motivated the use of structural response data to predict future crack growth under uncertainty, in order to enable a transition towards predictive maintenance. Accurately representing different sources of uncertainty in stochastic crack growth (SCG) processes is a non-trivial task. The present work builds on previous research on physics-based SCG modeling under both material and load-related uncertainty. The aim here is to construct computationally efficient, probabilistic surrogate models for SCG processes that successfully encode these different sources of uncertainty. An approach inspired by latent variable modeling is employed that utilizes Gaussian Process (GP) regression models to enable the surrogates to be used to generate prior distributions for different Bayesian SHM tasks as th
    
[^55]: GATs是否失衡？

    Are GATs Out of Balance?. (arXiv:2310.07235v1 [cs.LG])

    [http://arxiv.org/abs/2310.07235](http://arxiv.org/abs/2310.07235)

    本研究揭示了Graph Attention Network (GAT)梯度流动力学的守恒定律，解释了为什么标准初始化下的GAT中高比例的参数在训练过程中很难改变。我们还提出了一种平衡GAT网络的初始化方案，使得深层网络更容易进行训练，并且相比标准初始化，具有更快的收敛速度。

    

    虽然图神经网络（GNNs）的表达能力和计算能力已经在理论上得到了研究，但它们的优化和学习动态在大多数情况下仍然未被探索。我们的研究针对图注意力网络（GAT），这是一种流行的GNN架构，其中节点的邻域聚合由参数化的注意力系数加权。我们推导出GAT梯度流动力学的守恒定律，这解释了为什么标准初始化下的GAT中高比例的参数在训练过程中很难改变。这种效应在深层的GAT中被放大，它们的性能要明显差于浅层的GAT。为了缓解这个问题，我们设计了一种平衡GAT网络的初始化方案。我们的方法 i) 可以更有效地传播梯度，从而使深层网络可训练，ii) 相比于标准初始化，可以实现训练和收敛时间的显著加速。

    While the expressive power and computational capabilities of graph neural networks (GNNs) have been theoretically studied, their optimization and learning dynamics, in general, remain largely unexplored. Our study undertakes the Graph Attention Network (GAT), a popular GNN architecture in which a node's neighborhood aggregation is weighted by parameterized attention coefficients. We derive a conservation law of GAT gradient flow dynamics, which explains why a high portion of parameters in GATs with standard initialization struggle to change during training. This effect is amplified in deeper GATs, which perform significantly worse than their shallow counterparts. To alleviate this problem, we devise an initialization scheme that balances the GAT network. Our approach i) allows more effective propagation of gradients and in turn enables trainability of deeper networks, and ii) attains a considerable speedup in training and convergence time in comparison to the standard initialization. O
    
[^56]: 基于提示的持续学习的分层分解：重新思考模糊的次优性

    Hierarchical Decomposition of Prompt-Based Continual Learning: Rethinking Obscured Sub-optimality. (arXiv:2310.07234v1 [cs.LG])

    [http://arxiv.org/abs/2310.07234](http://arxiv.org/abs/2310.07234)

    本论文通过对基于提示的持续学习目标进行分层分解，在自监督预训练的背景下，解决了任务特定知识整合和预测困难的问题。

    

    基于提示的持续学习是利用预训练知识进行下游持续学习的新兴方向，在受监督的预训练下几乎达到了性能峰值。然而，我们的实证研究发现，在更加现实的自监督预训练中，当前的策略未能发挥出其全部潜力，而这在实践中处理大量未标记数据是必要的。这主要是由于任务特定知识通过提示参数被纳入到指示表示中，并在测试时由未指示的表示进行预测的困难。为了解决这个问题，我们在预训练的上下文中对持续学习目标进行了理论分析，并将其分解为分层组成部分：任务内预测、任务身份推断和任务自适应预测。根据这些实证和理论的见解，我们提出了Hierarchical Decomposition方法。

    Prompt-based continual learning is an emerging direction in leveraging pre-trained knowledge for downstream continual learning, and has almost reached the performance pinnacle under supervised pre-training. However, our empirical research reveals that the current strategies fall short of their full potential under the more realistic self-supervised pre-training, which is essential for handling vast quantities of unlabeled data in practice. This is largely due to the difficulty of task-specific knowledge being incorporated into instructed representations via prompt parameters and predicted by uninstructed representations at test time. To overcome the exposed sub-optimality, we conduct a theoretical analysis of the continual learning objective in the context of pre-training, and decompose it into hierarchical components: within-task prediction, task-identity inference, and task-adaptive prediction. Following these empirical and theoretical insights, we propose Hierarchical Decomposition 
    
[^57]: 通过蛋白质片段周围对齐实现自监督口袋预训练

    Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment. (arXiv:2310.07229v1 [cs.LG])

    [http://arxiv.org/abs/2310.07229](http://arxiv.org/abs/2310.07229)

    本论文提出了一种通过蛋白质片段和周围对齐的自监督口袋预训练方法，用于模拟并生成大量的配体-受体相互作用复合物。

    

    口袋表示在各种生物医学应用中起着重要作用，如药物可裸露性估计、配体亲和力预测和新药设计等。然而，现有的几何特征和预训练表示通常将口袋独立于配体处理，忽略了它们之间的基本相互作用。我们提出了一种新颖的口袋预训练方法，借助高分辨率原子蛋白质结构的知识，并辅以高效的预训练小分子表示。将蛋白质结构分段为类似药物的片段及其相应的口袋，我们得到了一种合理的配体-受体相互作用模拟，从而生成了超过500万个复合物。

    Pocket representations play a vital role in various biomedical applications, such as druggability estimation, ligand affinity prediction, and de novo drug design. While existing geometric features and pretrained representations have demonstrated promising results, they usually treat pockets independent of ligands, neglecting the fundamental interactions between them. However, the limited pocket-ligand complex structures available in the PDB database (less than 100 thousand non-redundant pairs) hampers large-scale pretraining endeavors for interaction modeling. To address this constraint, we propose a novel pocket pretraining approach that leverages knowledge from high-resolution atomic protein structures, assisted by highly effective pretrained small molecule representations. By segmenting protein structures into drug-like fragments and their corresponding pockets, we obtain a reasonable simulation of ligand-receptor interactions, resulting in the generation of over 5 million complexes
    
[^58]: 用MODIS多光谱时间序列和辅助数据进行盲目光谱分离的深度学习研究

    Deep Learning for blind spectral unmixing of LULC classes with MODIS multispectral time series and ancillary data. (arXiv:2310.07223v1 [cs.CV])

    [http://arxiv.org/abs/2310.07223](http://arxiv.org/abs/2310.07223)

    这项研究利用MODIS多光谱时间序列数据和深度学习模型，首次实现了对LULC类别的盲目光谱分离。通过添加地理加地形和气候辅助信息，进一步提高了模型的性能。

    

    遥感数据中LULC类别通常存在混合情况，光谱分离是一种从混合像素中提取信息到其组成LULC类型和相应丰度分数的技术。传统上，解决这一任务要么依赖于需要先验知识的经典方法，要么依赖于避免显式成分计算的机器学习方法，也就是盲目光谱分离（BSU）。大多数基于深度学习（DL）的BSU研究侧重于单个时间步的高光谱数据，然而与多光谱数据相比，其获取成本仍然相当高昂。据我们所知，我们在这里提供了第一项关于使用多光谱时间序列数据和DL模型进行LULC类别的BSU研究。我们进一步通过添加地理加地形（geo-topographic）和气候辅助信息来提升基于长短时记忆（LSTM）的模型的性能。我们的实验结果表明，将光谱时间输入数据与地理时态数据相结合可以提高模型性能。

    Remotely sensed data are dominated by mixed Land Use and Land Cover (LULC) types. Spectral unmixing is a technique to extract information from mixed pixels into their constituent LULC types and corresponding abundance fractions. Traditionally, solving this task has relied on either classical methods that require prior knowledge of endmembers or machine learning methods that avoid explicit endmembers calculation, also known as blind spectral unmixing (BSU). Most BSU studies based on Deep Learning (DL) focus on one time-step hyperspectral data, yet its acquisition remains quite costly compared with multispectral data. To our knowledge, here we provide the first study on BSU of LULC classes using multispectral time series data with DL models. We further boost the performance of a Long-Short Term Memory (LSTM)-based model by incorporating geographic plus topographic (geo-topographic) and climatic ancillary information. Our experiments show that combining spectral-temporal input data togeth
    
[^59]: 使用可学习的物理学进行实时运动姿势建议

    Using Learnable Physics for Real-Time Exercise Form Recommendations. (arXiv:2310.07221v1 [cs.AI])

    [http://arxiv.org/abs/2310.07221](http://arxiv.org/abs/2310.07221)

    本文提出了一种使用可学习的物理学的算法流程，能够在实时环境中诊断运动姿势问题并提供矫正建议，通过姿势识别、重复次数计算和动作演变跟踪实现。该系统在六个不同的运动上进行了评估，通过低成本设备如智能手机提供实时建议，使自我练习成为可能，同时降低受伤风险。

    

    良好的姿势和形式对于安全和高效的运动至关重要。即使在健身房环境下，教练可能无法及时提供反馈。因此，康复疗法和健身训练可以从提供实时评估的推荐系统中受益。本文提出了一种算法流程，可以在实时环境中对运动技术中的问题进行诊断，并给出矫正建议，具有高敏感性和特异性。我们使用MediaPipe进行姿势识别，使用峰值突出检测计算重复次数，并使用可学习的物理模拟器跟踪每个运动的动作演变。根据与典型学习动作的偏差，基于统计学习对测试视频进行诊断。该系统在六个全身和上半身运动上进行了评估。通过低成本设备如智能手机提供的实时建议，运动者可以纠正潜在的错误，使自我练习成为可能，同时降低受伤风险。

    Good posture and form are essential for safe and productive exercising. Even in gym settings, trainers may not be readily available for feedback. Rehabilitation therapies and fitness workouts can thus benefit from recommender systems that provide real-time evaluation. In this paper, we present an algorithmic pipeline that can diagnose problems in exercise techniques and offer corrective recommendations, with high sensitivity and specificity in real-time. We use MediaPipe for pose recognition, count repetitions using peak-prominence detection, and use a learnable physics simulator to track motion evolution for each exercise. A test video is diagnosed based on deviations from the prototypical learned motion using statistical learning. The system is evaluated on six full and upper body exercises. These real-time recommendations, counseled via low-cost equipment like smartphones, will allow exercisers to rectify potential mistakes making self-practice feasible while reducing the risk of wo
    
[^60]: COPlanner: 保守的模型规划和乐观的环境探索为基于模型的强化学习提供支持

    COPlanner: Plan to Roll Out Conservatively but to Explore Optimistically for Model-Based RL. (arXiv:2310.07220v1 [cs.LG])

    [http://arxiv.org/abs/2310.07220](http://arxiv.org/abs/2310.07220)

    COPlanner是一个基于规划的框架，用于解决模型预测误差带来的问题。通过保守的模型演算和乐观的环境探索，COPlanner利用不确定性感知模型预测控制来解决动力学模型不准确的问题，并提供更好的解决方案。

    

    Dyna-style基于模型的强化学习包含两个阶段：使用模型生成样本进行策略学习的模型演算阶段和使用当前策略进行真实环境探索以学习动力学模型的阶段。然而，由于复杂的现实环境，难免会学习到一个具有模型预测误差的不完美动力学模型，进而可能误导策略学习并导致次优解。本文提出了一种名为COPlanner的基于规划的框架，用于解决不准确学习的动力学模型问题，其采用保守的模型演算和乐观的环境探索。COPlanner利用一种基于策略引导的不确定性感知模型预测控制（UP-MPC）组件进行多步不确定性评估的规划。这个估计的不确定性在模型演算期间作为惩罚因素，在真实环境探索期间作为奖励因素，以选择动作。因此，COPlanner可以同时保证保守的模型演算和乐观的环境探索，提供更好的解决方案。

    Dyna-style model-based reinforcement learning contains two phases: model rollouts to generate sample for policy learning and real environment exploration using current policy for dynamics model learning. However, due to the complex real-world environment, it is inevitable to learn an imperfect dynamics model with model prediction error, which can further mislead policy learning and result in sub-optimal solutions. In this paper, we propose $\texttt{COPlanner}$, a planning-driven framework for model-based methods to address the inaccurately learned dynamics model problem with conservative model rollouts and optimistic environment exploration. $\texttt{COPlanner}$ leverages an uncertainty-aware policy-guided model predictive control (UP-MPC) component to plan for multi-step uncertainty estimation. This estimated uncertainty then serves as a penalty during model rollouts and as a bonus during real environment exploration respectively, to choose actions. Consequently, $\texttt{COPlanner}$ 
    
[^61]: 改进的对语言分类模型的成员推理攻击

    Improved Membership Inference Attacks Against Language Classification Models. (arXiv:2310.07219v1 [cs.LG])

    [http://arxiv.org/abs/2310.07219](http://arxiv.org/abs/2310.07219)

    在这篇论文中，我们提出了一个新的框架，用于对语言分类模型进行成员推理攻击。通过利用集成方法，生成多个专门的攻击模型，我们展示了这种方法在经典和语言分类任务上比单个攻击模型或每个类别标签的攻击模型更准确。

    

    人工智能系统在日常生活中普遍存在，具有零售、制造、健康等许多领域的用例。随着人工智能采用的增加，已经发现了相关的风险，包括对使用其数据训练模型的人的隐私风险。评估机器学习模型的隐私风险对于是否使用、部署或共享模型做出知情决策至关重要。隐私风险评估的一种常见方法是对模型进行一个或多个已知攻击，并测量它们的成功率。我们提出了一个新颖的框架，用于对分类模型进行成员推理攻击。我们的框架利用集成方法，为不同数据子集生成许多专门的攻击模型。我们展示了这种方法在经典和语言分类任务上比单个攻击模型或每个类别标签的攻击模型都实现了更高的准确性。

    Artificial intelligence systems are prevalent in everyday life, with use cases in retail, manufacturing, health, and many other fields. With the rise in AI adoption, associated risks have been identified, including privacy risks to the people whose data was used to train models. Assessing the privacy risks of machine learning models is crucial to enabling knowledgeable decisions on whether to use, deploy, or share a model. A common approach to privacy risk assessment is to run one or more known attacks against the model and measure their success rate. We present a novel framework for running membership inference attacks against classification models. Our framework takes advantage of the ensemble method, generating many specialized attack models for different subsets of the data. We show that this approach achieves higher accuracy than either a single attack model or an attack model per class label, both on classical and language classification tasks.
    
[^62]: 用于在微型物联网设备上部署深度学习模型的多个硬件约束增强神经架构搜索

    Enhancing Neural Architecture Search with Multiple Hardware Constraints for Deep Learning Model Deployment on Tiny IoT Devices. (arXiv:2310.07217v1 [cs.LG])

    [http://arxiv.org/abs/2310.07217](http://arxiv.org/abs/2310.07217)

    这项工作提出了一种新颖的方法，通过将多个硬件约束融入神经架构搜索(NAS)技术中，实现在微型物联网设备上部署高效准确的深度学习模型。

    

    依赖于物联网设备的计算领域的迅速增长创造了对能够在低功耗设备上运行的高效准确的深度学习模型的迫切需求。然而，传统的深度学习模型对于典型的物联网终端节点来说往往过于复杂且计算密集。为了解决这个挑战，神经架构搜索(NAS)已经成为一种流行的设计自动化技术，用于共同优化深度神经网络的准确性和复杂度。然而，现有的NAS技术需要许多迭代才能产生符合特定硬件约束的网络，如硬件上可用的最大内存或目标应用允许的最大延迟。在这项工作中，我们提出了一种新颖的方法，将多个约束融入所谓的可微分NAS优化方法中，从而能够一次生成符合用户定义的内存和延迟约束的模型。

    The rapid proliferation of computing domains relying on Internet of Things (IoT) devices has created a pressing need for efficient and accurate deep-learning (DL) models that can run on low-power devices. However, traditional DL models tend to be too complex and computationally intensive for typical IoT end-nodes. To address this challenge, Neural Architecture Search (NAS) has emerged as a popular design automation technique for co-optimizing the accuracy and complexity of deep neural networks. Nevertheless, existing NAS techniques require many iterations to produce a network that adheres to specific hardware constraints, such as the maximum memory available on the hardware or the maximum latency allowed by the target application. In this work, we propose a novel approach to incorporate multiple constraints into so-called Differentiable NAS optimization methods, which allows the generation, in a single shot, of a model that respects user-defined constraints on both memory and latency i
    
[^63]: 在流形上通过黎曼扩散过程的混合生成模型

    Generative Modeling on Manifolds Through Mixture of Riemannian Diffusion Processes. (arXiv:2310.07216v1 [cs.LG])

    [http://arxiv.org/abs/2310.07216](http://arxiv.org/abs/2310.07216)

    通过混合黎曼扩散过程的原则性框架，我们提出了一种在流形上构建生成过程的方法，与现有的生成模型相比，该方法具有更高的效率和更广泛的适用性。

    

    在非欧几里得空间中建模数据的分布对于来自不同科学领域的许多应用都至关重要。然而，现有的流形上的生成模型存在着计算复杂的散度或依赖于热核的近似的问题。这些限制限制了它们在简单几何形状上的适用性，并阻碍了在高维空间中的可扩展性。在这项工作中，我们引入了黎曼扩散混合模型，这是一个在流形上构建生成过程的原则性框架，它是一组以端点条件扩散过程作为混合的生成过程，而不依赖于先前扩散模型的去噪方法，对于这些模型，生成过程的特性是它的漂移导向与流形的几何形状相对应的最可能的终点。我们进一步提出了一个简单而高效的训练目标，用于学习混合过程，它可以直接应用于一般流形。我们的方法表现得很好。

    Learning the distribution of data on Riemannian manifolds is crucial for modeling data from non-Euclidean space, which is required by many applications from diverse scientific fields. Yet, existing generative models on manifolds suffer from expensive divergence computation or rely on approximations of heat kernel. These limitations restrict their applicability to simple geometries and hinder scalability to high dimensions. In this work, we introduce the Riemannian Diffusion Mixture, a principled framework for building a generative process on manifolds as a mixture of endpoint-conditioned diffusion processes instead of relying on the denoising approach of previous diffusion models, for which the generative process is characterized by its drift guiding toward the most probable endpoint with respect to the geometry of the manifold. We further propose a simple yet efficient training objective for learning the mixture process, that is readily applicable to general manifolds. Our method outp
    
[^64]: 缩小牛顿-拉弗森方法和正规化策略迭代之间的差距

    Bridging the Gap between Newton-Raphson Method and Regularized Policy Iteration. (arXiv:2310.07211v1 [cs.LG])

    [http://arxiv.org/abs/2310.07211](http://arxiv.org/abs/2310.07211)

    正则化策略迭代和牛顿-拉弗森方法在使用强凸函数对贝尔曼方程平滑化的条件下严格等价，为正则化策略迭代的全局和局部收敛行为提供了统一分析。该算法具有全局线性收敛和局部二次收敛特性。

    

    正则化是强化学习算法中最重要的技术之一。众所周知，软演员-评论家算法是正则化策略迭代的一个特例，其中正则化项选择为Shannon熵。尽管正则化策略迭代在实践中取得了一些成功，但其理论基础仍不清楚。本文证明，在使用强凸函数对贝尔曼方程平滑化的条件下，正则化策略迭代在严格意义上等价于标准的牛顿-拉弗森方法。这种等价性为正则化策略迭代的全局和局部收敛行为奠定了基础。我们证明正则化策略迭代具有全局线性收敛性，收敛速度为$\gamma$（折扣因子）。此外，一旦进入最优值周围的局部区域，该算法将二次收敛。我们还展示了正则化策略迭代的改进版本，即有限的正-----------此处省略部分内容---------------

    Regularization is one of the most important techniques in reinforcement learning algorithms. The well-known soft actor-critic algorithm is a special case of regularized policy iteration where the regularizer is chosen as Shannon entropy. Despite some empirical success of regularized policy iteration, its theoretical underpinnings remain unclear. This paper proves that regularized policy iteration is strictly equivalent to the standard Newton-Raphson method in the condition of smoothing out Bellman equation with strongly convex functions. This equivalence lays the foundation of a unified analysis for both global and local convergence behaviors of regularized policy iteration. We prove that regularized policy iteration has global linear convergence with the rate being $\gamma$ (discount factor). Furthermore, this algorithm converges quadratically once it enters a local region around the optimal value. We also show that a modified version of regularized policy iteration, i.e., with finite
    
[^65]: 在对抗性干扰下的鲁棒安全强化学习

    Robust Safe Reinforcement Learning under Adversarial Disturbances. (arXiv:2310.07207v1 [cs.LG])

    [http://arxiv.org/abs/2310.07207](http://arxiv.org/abs/2310.07207)

    本文针对现有的安全强化学习算法很少考虑外部干扰的问题，提出了一个鲁棒的安全强化学习框架，通过建立双人零和游戏来解决最坏情况下的干扰，并证明了策略迭代算法的收敛性。

    

    安全性是将强化学习应用于现实世界控制任务时的首要关注点，特别是在存在外部干扰的情况下。然而，现有的安全强化学习算法很少考虑外部干扰，限制了它们在实践中的适用性和鲁棒性。为了解决这个挑战，本文提出了一个鲁棒的安全强化学习框架来应对最坏情况下的干扰。首先，本文提出了一个策略迭代方案，以解决鲁棒不变集，即安全集的子集，其中只有状态在这个集合内才能实现持久安全。关键思想是通过利用Hamilton-Jacobi可达性分析中的安全值函数建立一个双人零和游戏，其中主角（即控制输入）旨在保持安全，而对手（即外部干扰）试图破坏安全。本文证明了所提出的策略迭代算法单调收敛到m。

    Safety is a primary concern when applying reinforcement learning to real-world control tasks, especially in the presence of external disturbances. However, existing safe reinforcement learning algorithms rarely account for external disturbances, limiting their applicability and robustness in practice. To address this challenge, this paper proposes a robust safe reinforcement learning framework that tackles worst-case disturbances. First, this paper presents a policy iteration scheme to solve for the robust invariant set, i.e., a subset of the safe set, where persistent safety is only possible for states within. The key idea is to establish a two-player zero-sum game by leveraging the safety value function in Hamilton-Jacobi reachability analysis, in which the protagonist (i.e., control inputs) aims to maintain safety and the adversary (i.e., external disturbances) tries to break down safety. This paper proves that the proposed policy iteration algorithm converges monotonically to the m
    
[^66]: 视觉计算扩散模型的最新进展

    State of the Art on Diffusion Models for Visual Computing. (arXiv:2310.07204v1 [cs.AI])

    [http://arxiv.org/abs/2310.07204](http://arxiv.org/abs/2310.07204)

    这篇论文旨在介绍最新的视觉计算领域中扩散模型的发展和应用，涵盖了生成人工智能的核心概念和实现细节，并总结了个人化、条件约束和反演等重要方面。

    

    随着生成人工智能的出现，视觉计算领域正在迅速发展，它为图像、视频和3D场景的生成、编辑和重建提供了前所未有的能力。在这些领域中，扩散模型是生成人工智能的首选架构。仅在过去一年中，基于扩散的工具和应用的文献数量呈指数增长，并且涉及计算机图形学、计算机视觉和人工智能社区的相关论文每天都在arXiv上发表。这个领域的快速增长使得跟上所有最新发展变得困难。这份最新技术报告（STAR）的目标是介绍扩散模型的基本数学概念、稳定扩散模型的实现细节和设计选择，以及概述这些生成人工智能工具的重要方面，包括个性化、条件约束、反演等。

    The field of visual computing is rapidly advancing due to the emergence of generative artificial intelligence (AI), which unlocks unprecedented capabilities for the generation, editing, and reconstruction of images, videos, and 3D scenes. In these domains, diffusion models are the generative AI architecture of choice. Within the last year alone, the literature on diffusion-based tools and applications has seen exponential growth and relevant papers are published across the computer graphics, computer vision, and AI communities with new works appearing daily on arXiv. This rapid growth of the field makes it difficult to keep up with all recent developments. The goal of this state-of-the-art report (STAR) is to introduce the basic mathematical concepts of diffusion models, implementation details and design choices of the popular Stable Diffusion model, as well as overview important aspects of these generative AI tools, including personalization, conditioning, inversion, among others. Mor
    
[^67]: 提升LDPC码学习以改善误码底的性能

    Boosting Learning for LDPC Codes to Improve the Error-Floor Performance. (arXiv:2310.07194v1 [cs.IT])

    [http://arxiv.org/abs/2310.07194](http://arxiv.org/abs/2310.07194)

    通过利用集成网络提升学习技术和引入分块训练方案，本文提出了一种优化LDPC码解码器的方法，以解决误码底问题，并改善了LDPC码的性能。

    

    由于其强大的纠错能力和简单的解码过程，低密度奇偶校验（LDPC）码已成功商用化于通信系统中。然而，LDPC码的误码底现象使得在实现极低误码率和在对超高可靠性有要求的场景中应用LDPC码时面临挑战。在本文中，我们提出了训练方法来优化对误码底具有鲁棒性的神经min-sum（NMS）解码器。首先，通过利用集成网络的提升学习技术，我们将解码网络分为两个网络，并训练后续网络专门用于对第一个网络中解码失败的未纠正码字进行识别。其次，为解决训练中的梯度消失问题，我们引入分块训练方案，局部训练一块权重，并重新训练前一块权重。最后，我们展示了将NMS解码器分配给每个未解码的码字的相关性信息，从而进一步提高了误码底性能。

    Low-density parity-check (LDPC) codes have been successfully commercialized in communication systems due to their strong error correction ability and simple decoding process. However, the error-floor phenomenon of LDPC codes, in which the error rate stops decreasing rapidly at a certain level, poses challenges in achieving extremely low error rates and the application of LDPC codes in scenarios demanding ultra high reliability. In this work, we propose training methods to optimize neural min-sum (NMS) decoders that are robust to the error-floor. Firstly, by leveraging the boosting learning technique of ensemble networks, we divide the decoding network into two networks and train the post network to be specialized for uncorrected codewords that failed in the first network. Secondly, to address the vanishing gradient issue in training, we introduce a block-wise training schedule that locally trains a block of weights while retraining the preceding block. Lastly, we show that assigning di
    
[^68]: 神经网络：深层、浅层还是中间层？

    Neural networks: deep, shallow, or in between?. (arXiv:2310.07190v1 [stat.ML])

    [http://arxiv.org/abs/2310.07190](http://arxiv.org/abs/2310.07190)

    本研究探讨了深度和宽度对于神经网络的影响，结果表明只有深度趋近无穷大的神经网络才可能达到比熵数更好的速度，而固定深度并让宽度趋近无穷大则没有收益。

    

    我们给出了一种估计方法，用于测量通过宽度为W、深度为l的前馈神经网络以及满足Lipschitz激活函数的输出进行近似的误差。我们证明了，在对数因子解除，仅有深度l趋近无穷大的神经网络才有可能达到比熵数更好的速度，而如果固定深度然后让宽度W趋近无穷大，则没有任何收益。

    We give estimates from below for the error of approximation of a compact subset from a Banach space by the outputs of feed-forward neural networks with width W, depth l and Lipschitz activation functions. We show that, modulo logarithmic factors, rates better that entropy numbers' rates are possibly attainable only for neural networks for which the depth l goes to infinity, and that there is no gain if we fix the depth and let the width W go to infinity.
    
[^69]: 基于核Cox部分线性回归：构建癌症患者生存预测模型

    Kernel Cox partially linear regression: building predictive models for cancer patients' survival. (arXiv:2310.07187v1 [stat.ML])

    [http://arxiv.org/abs/2310.07187](http://arxiv.org/abs/2310.07187)

    本文提出了一种基于核Cox部分线性回归的方法来构建癌症患者生存预测模型，通过核机器方法描述复杂的生存和预测因子关系，并利用正则化加权核机器方法自动去除不相关的因子。与其他竞争方法相比，该方法在模拟中表现最好。

    

    癌症患者的生存存在广泛的异质性，从几个月到几十年不等。为了准确预测临床结果，建立一个能够将患者的分子特征与生存情况关联起来的精确预测模型至关重要。由于生存与高维分子预测因子之间存在复杂的关系，同时进行非参数建模和去除不相关的预测因子是具有挑战性的。本文建立了一个核Cox比例风险半参数模型，并提出了一种新颖的正则化加权核机器（RegGKM）方法来拟合模型。我们使用核机器方法描述生存和预测因子之间的复杂关系，同时通过LASSO惩罚自动去除不相关的参数和非参数预测因子。为所提出的方法开发了一个高维算法。与模拟中的其他竞争方法相比，所提出的方法总是表现最好。

    Wide heterogeneity exists in cancer patients' survival, ranging from a few months to several decades. To accurately predict clinical outcomes, it is vital to build an accurate predictive model that relates patients' molecular profiles with patients' survival. With complex relationships between survival and high-dimensional molecular predictors, it is challenging to conduct non-parametric modeling and irrelevant predictors removing simultaneously. In this paper, we build a kernel Cox proportional hazards semi-parametric model and propose a novel regularized garrotized kernel machine (RegGKM) method to fit the model. We use the kernel machine method to describe the complex relationship between survival and predictors, while automatically removing irrelevant parametric and non-parametric predictors through a LASSO penalty. An efficient high-dimensional algorithm is developed for the proposed method. Comparison with other competing methods in simulation shows that the proposed method alway
    
[^70]: NeuroInspect：通过类条件可视化实现的可解释的基于神经元的调试框架

    NeuroInspect: Interpretable Neuron-based Debugging Framework through Class-conditional Visualizations. (arXiv:2310.07184v1 [cs.CV])

    [http://arxiv.org/abs/2310.07184](http://arxiv.org/abs/2310.07184)

    NeuroInspect是一个基于神经元的可解释的调试框架，通过确定网络中导致错误的神经元并可视化嵌入其中的特征，提供了人类可解释的解释。引入了CLIP-Illusion来生成特征图像，并以类为条件来考察神经元与决策层之间的联系。

    

    尽管深度学习在各个领域取得了显著进展，但深度学习模型仍然容易出错。这个问题需要深度学习从业者使用有效的调试工具来解释网络中的决策过程。然而，现有的调试方法常常需要额外的数据或调整决策过程，限制了它们的适用性。为了解决这个问题，我们提出了NeuroInspect，这是一个基于神经元的可解释的调试框架，包括三个关键阶段：反事实解释、特征可视化和虚假相关性削减。我们的调试框架首先确定网络中导致错误的神经元，然后可视化嵌入在这些神经元中的特征，以便人类解释。为了提供这些解释，我们引入了CLIP-Illusion，一种新颖的特征可视化方法，它生成代表特征的图像，并以类为条件来考察神经元与决策层之间的联系。

    Despite deep learning (DL) has achieved remarkable progress in various domains, the DL models are still prone to making mistakes. This issue necessitates effective debugging tools for DL practitioners to interpret the decision-making process within the networks. However, existing debugging methods often demand extra data or adjustments to the decision process, limiting their applicability. To tackle this problem, we present NeuroInspect, an interpretable neuron-based debugging framework with three key stages: counterfactual explanations, feature visualizations, and false correlation mitigation. Our debugging framework first pinpoints neurons responsible for mistakes in the network and then visualizes features embedded in the neurons to be human-interpretable. To provide these explanations, we introduce CLIP-Illusion, a novel feature visualization method that generates images representing features conditioned on classes to examine the connection between neurons and the decision layer. W
    
[^71]: SAM-OCTA: 用于OCTA图像分割的分段辅助策略

    SAM-OCTA: Prompting Segment-Anything for OCTA Image Segmentation. (arXiv:2310.07183v1 [cs.LG])

    [http://arxiv.org/abs/2310.07183](http://arxiv.org/abs/2310.07183)

    本文提出了一种名为SAM-OCTA的方法，通过低秩适应技术和提示点生成策略，在OCTA图像分割任务中实现了令人瞩目的性能。这种方法解决了在有限样本训练中可能导致的过拟合问题，并成功处理了视网膜血管、黄斑无血管区、毛细血管、动脉和静脉等各种分割任务，同时还实现了局部血管分割和动脉-静脉分割等以前未解决的问题。

    

    在光学相干断层扫描血流动力学(OCTA)图像分析中，分割特定目标是必要的操作。现有方法通常在限定样本数（大约数百个）的有标注数据集上进行训练，这可能导致过拟合。为了解决这个问题，本文采用了低秩适应技术进行基础模型微调，并提出相应的提示点生成策略，以处理OCTA数据集上的各种分割任务。这种方法被命名为SAM-OCTA，并在公开可用的OCTA-500和ROSE数据集上进行了实验证明。该方法实现了或接近了最先进的分割性能指标。详细讨论了提示点对视网膜血管、黄斑无血管区、毛细血管、动脉和静脉分割任务的效果和适用性。此外，SAM-OCTA实现了局部血管分割和有效的动脉-静脉分割，这是之前的研究中尚未解决的问题。

    In the analysis of optical coherence tomography angiography (OCTA) images, the operation of segmenting specific targets is necessary. Existing methods typically train on supervised datasets with limited samples (approximately a few hundred), which can lead to overfitting. To address this, the low-rank adaptation technique is adopted for foundation model fine-tuning and proposed corresponding prompt point generation strategies to process various segmentation tasks on OCTA datasets. This method is named SAM-OCTA and has been experimented on the publicly available OCTA-500 and ROSE datasets. This method achieves or approaches state-of-the-art segmentation performance metrics. The effect and applicability of prompt points are discussed in detail for the retinal vessel, foveal avascular zone, capillary, artery, and vein segmentation tasks. Furthermore, SAM-OCTA accomplishes local vessel segmentation and effective artery-vein segmentation, which was not well-solved in previous works. The cod
    
[^72]: 在线推测解码

    Online Speculative Decoding. (arXiv:2310.07177v1 [cs.AI])

    [http://arxiv.org/abs/2310.07177](http://arxiv.org/abs/2310.07177)

    在线推测解码是通过利用多余计算能力，在LLM服务集群中持续更新草稿模型，从而加速大型语言模型推理的一种方法。

    

    推测解码是通过利用较小的草稿模型来预测目标模型的输出，从而加速大型语言模型（LLM）推理的关键技术。然而，在面对多样的文本输入和草稿模型与目标模型之间的显著能力差距时，其有效性可能受到限制。我们引入了在线推测解码（OSD）来解决这一挑战。其主要思想是利用LLM服务集群中丰富的多余计算能力，根据观察到的用户查询数据持续更新（多个）草稿模型。由于LLM推理受内存限制，典型的LLM服务集群中的剩余计算能力可以用于在线重新训练草稿模型，从而使训练成本保持中性。由于LLM服务的查询分布相对简单，根据查询分布进行重新训练可以使草稿模型更准确地预测目标模型的输出。

    Speculative decoding is a pivotal technique to accelerate the inference of large language models (LLMs) by employing a smaller draft model to predict the target model's outputs. However, its efficacy can be limited due to the low predictive accuracy of the draft model, particularly when faced with diverse text inputs and a significant capability gap between the draft and target models. We introduce online speculative decoding (OSD) to address this challenge. The main idea is to continually update (multiple) draft model(s) on observed user query data using the abundant excess computational power in an LLM serving cluster. Given that LLM inference is memory-bounded, the surplus computational power in a typical LLM serving cluster can be repurposed for online retraining of draft models, thereby making the training cost-neutral. Since the query distribution of an LLM service is relatively simple, retraining on query distribution enables the draft model to more accurately predict the target
    
[^73]: 具有无误差的可微分交换函数的广义神经排序网络

    Generalized Neural Sorting Networks with Error-Free Differentiable Swap Functions. (arXiv:2310.07174v1 [cs.LG])

    [http://arxiv.org/abs/2310.07174](http://arxiv.org/abs/2310.07174)

    本文提出了一种广义神经排序网络，其中采用了具有无误差且可微分的交换函数，同时使用了置换等变Transformer网络来捕捉输入之间的依赖关系。实验证明，该方法在各种排序基准上表现优于或与基准方法相当。

    

    排序是所有计算机系统的基本操作，一直是一个长期的重要研究课题。除了传统排序算法的问题表述，我们通过神经排序网络考虑了更抽象但具有表达力的输入，例如多位数字图像和图像片段。为了学习从高维输入到次序变量的映射，需要保证排序网络的可微分性。在本文中，我们通过可微分的交换函数定义一个柔化误差，并开发了一个无误差的交换函数，该函数满足非减和可微分的条件。此外，采用了具有多头注意力机制的置换等变Transformer网络，以捕捉给定输入之间的依赖关系，并利用其自注意力的模型能力。在多样的排序基准上进行的实验证明，我们的方法优于或与基准方法相当。

    Sorting is a fundamental operation of all computer systems, having been a long-standing significant research topic. Beyond the problem formulation of traditional sorting algorithms, we consider sorting problems for more abstract yet expressive inputs, e.g., multi-digit images and image fragments, through a neural sorting network. To learn a mapping from a high-dimensional input to an ordinal variable, the differentiability of sorting networks needs to be guaranteed. In this paper we define a softening error by a differentiable swap function, and develop an error-free swap function that holds non-decreasing and differentiability conditions. Furthermore, a permutation-equivariant Transformer network with multi-head attention is adopted to capture dependency between given inputs and also leverage its model capacity with self-attention. Experiments on diverse sorting benchmarks show that our methods perform better than or comparable to baseline methods.
    
[^74]: 通过信息论分布多样化实现联邦泛化能力

    Federated Generalization via Information-Theoretic Distribution Diversification. (arXiv:2310.07171v1 [cs.LG])

    [http://arxiv.org/abs/2310.07171](http://arxiv.org/abs/2310.07171)

    该论文研究了联邦学习中泛化能力的挑战，特别关注训练分布和测试分布的不匹配。提出了一种信息论的泛化方法来解决这个问题。

    

    联邦学习（FL）因其在无需直接数据共享的情况下进行协同模型训练的能力而日益突出。然而，客户端之间本地数据分布的巨大差异，通常被称为非独立同分布（non-IID）挑战，对FL的泛化能力构成了重大障碍。当并非所有客户端都参与训练过程时，情况变得更加复杂，这是由于不稳定的网络连接或有限的计算能力而常见。这可能极大地复杂化了对训练模型的泛化能力的评估。尽管最近的大量研究集中在涉及具有不同分布的参与客户端的未见数据的泛化差距问题上，但参与客户端的训练分布和非参与客户端的测试分布之间的差异却被大部分忽视了。为此，我们的论文揭示了一种基于信息论的泛化方法。

    Federated Learning (FL) has surged in prominence due to its capability of collaborative model training without direct data sharing. However, the vast disparity in local data distributions among clients, often termed the non-Independent Identically Distributed (non-IID) challenge, poses a significant hurdle to FL's generalization efficacy. The scenario becomes even more complex when not all clients participate in the training process, a common occurrence due to unstable network connections or limited computational capacities. This can greatly complicate the assessment of the trained models' generalization abilities. While a plethora of recent studies has centered on the generalization gap pertaining to unseen data from participating clients with diverse distributions, the divergence between the training distributions of participating clients and the testing distributions of non-participating ones has been largely overlooked. In response, our paper unveils an information-theoretic genera
    
[^75]: 基于锚点的多视图子空间聚类与分层特征下降

    Anchor-based Multi-view Subspace Clustering with Hierarchical Feature Descent. (arXiv:2310.07166v1 [cs.CV])

    [http://arxiv.org/abs/2310.07166](http://arxiv.org/abs/2310.07166)

    本文提出了一种基于锚点的多视图子空间聚类方法，通过分层特征下降挖掘和部署视图之间的依赖关系，从而在聚类过程中实现了对不同视图的特征对齐。这种方法得到了一个共同的潜在空间，被称为“相似空间”，揭示了不同视图之间的相关性和依赖关系。

    

    多视图聚类因其能够从多个信息源中聚合信息的能力以及在公共事务中有前景的潜力而引起越来越多的关注。迄今为止，在近期文献中提出了许多先进的方法。然而，仍然存在一些难题需要解决。在试图对不同视图的特征进行对齐时，出现了一个常见的困境。我们通过分层特征下降挖掘和部署视图之间的依赖关系，从而导致一个共同的潜在空间（第一阶段）。这个潜在空间首次被视为“相似空间”，因为它揭示了不同视图之间的某些相关性和依赖关系。更准确地说，类别的独热编码也可以被称为终止阶段的相似空间。此外，由于大多数现有的多视图聚类算法起源于k-means聚类和谱聚类，这导致了立方时间复杂度。

    Multi-view clustering has attracted growing attention owing to its capabilities of aggregating information from various sources and its promising horizons in public affairs. Up till now, many advanced approaches have been proposed in recent literature. However, there are several ongoing difficulties to be tackled. One common dilemma occurs while attempting to align the features of different views. We dig out as well as deploy the dependency amongst views through hierarchical feature descent, which leads to a common latent space( STAGE 1). This latent space, for the first time of its kind, is regarded as a 'resemblance space', as it reveals certain correlations and dependencies of different views. To be exact, the one-hot encoding of a category can also be referred to as a resemblance space in its terminal phase. Moreover, due to the intrinsic fact that most of the existing multi-view clustering algorithms stem from k-means clustering and spectral clustering, this results in cubic time 
    
[^76]: LLark: 一种用于音乐的多模态基础模型

    LLark: A Multimodal Foundation Model for Music. (arXiv:2310.07160v1 [cs.SD])

    [http://arxiv.org/abs/2310.07160](http://arxiv.org/abs/2310.07160)

    LLark是一个通过多模态架构实现音乐理解的模型，能够在零样本泛化上匹配或超出现有基准模型，在字幕生成和推理任务中与人类响应高度一致。

    

    音乐具有独特且复杂的结构，对于专业人士和现有的AI系统来说都具有挑战性，并相对于其他形式的音频呈现出独特的挑战。我们提出了LLark，一种针对音乐理解的指令调谐多模型模型。我们详细介绍了我们的数据集创建过程，其中包括增强多样化的开源音乐数据集的注释，并将其转换为统一的指令调谐格式。我们提出了一种多模态架构用于LLark，将预训练的音乐生成模型与预训练的语言模型相结合。在对三种类型的任务（音乐理解、字幕生成和推理）进行评估时，我们展示了我们的模型在音乐理解的零样本泛化上与现有基准模型相匹配或超出，并且在字幕生成和推理任务中人类与模型的响应显示出高度一致性。LLark完全是根据开源音乐数据和模型进行训练的，并且我们公开了我们的训练代码。

    Music has a unique and complex structure which is challenging for both expert humans and existing AI systems to understand, and presents unique challenges relative to other forms of audio. We present LLark, an instruction-tuned multimodal model for music understanding. We detail our process for dataset creation, which involves augmenting the annotations of diverse open-source music datasets and converting them to a unified instruction-tuning format. We propose a multimodal architecture for LLark, integrating a pretrained generative model for music with a pretrained language model. In evaluations on three types of tasks (music understanding, captioning, and reasoning), we show that our model matches or outperforms existing baselines in zero-shot generalization for music understanding, and that humans show a high degree of agreement with the model's responses in captioning and reasoning tasks. LLark is trained entirely from open-source music data and models, and we make our training code
    
[^77]: 设备外没有隐私：关于TEE保护下的设备端机器学习的（不）安全性

    No Privacy Left Outside: On the (In-)Security of TEE-Shielded DNN Partition for On-Device ML. (arXiv:2310.07152v1 [cs.CR])

    [http://arxiv.org/abs/2310.07152](http://arxiv.org/abs/2310.07152)

    本文研究了TEE保护下设备端机器学习的安全性问题。通过对现有的TSDP解决方案进行评估，发现这些解决方案容易受到隐私窃取攻击的影响。

    

    设备端机器学习引入了新的安全挑战：DNN模型可以被设备用户白盒访问。基于白盒信息，攻击者可以进行有效的模型窃取和成员推断攻击。使用可信执行环境（TEE）来保护设备端的DNN模型旨在将（易于进行的）白盒攻击降级为（更难进行的）黑盒攻击。然而，一个主要的缺点是大大增加了延迟（高达50倍）。为了加速使用GPU进行TEE保护的DNN计算，研究人员提出了几种模型分区技术。这些解决方案被称为TEE保护的DNN分区（TSDP），将DNN模型分为两个部分，将隐私不敏感的部分卸载到GPU上，同时将隐私敏感的部分保护在TEE内。本文评估了现有的TSDP解决方案在各种DNN模型、数据集和指标上进行了模型窃取和成员推断攻击的实验。我们发现现有的TSDP解决方案容易受到隐私窃取攻击的影响。

    On-device ML introduces new security challenges: DNN models become white-box accessible to device users. Based on white-box information, adversaries can conduct effective model stealing (MS) and membership inference attack (MIA). Using Trusted Execution Environments (TEEs) to shield on-device DNN models aims to downgrade (easy) white-box attacks to (harder) black-box attacks. However, one major shortcoming is the sharply increased latency (up to 50X). To accelerate TEE-shield DNN computation with GPUs, researchers proposed several model partition techniques. These solutions, referred to as TEE-Shielded DNN Partition (TSDP), partition a DNN model into two parts, offloading the privacy-insensitive part to the GPU while shielding the privacy-sensitive part within the TEE. This paper benchmarks existing TSDP solutions using both MS and MIA across a variety of DNN models, datasets, and metrics. We show important findings that existing TSDP solutions are vulnerable to privacy-stealing attack
    
[^78]: QFT: 使用可承担资源对LLMs进行量化全参数调整

    QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources. (arXiv:2310.07147v1 [cs.CL])

    [http://arxiv.org/abs/2310.07147](http://arxiv.org/abs/2310.07147)

    我们提出了一种新的QFT框架，可以对LLMs进行内存高效的全参数微调，而不损害性能。

    

    大型语言模型（LLMs）在自然语言处理任务中展示出了显著的影响。对这些预训练模型进行微调可以进一步提高性能，但由于其巨大的资源需求，这一过程具有挑战性。为此，现有的努力都集中在参数高效的微调上，不幸的是，它们没有充分发挥全参数微调的潜力。在这项工作中，我们提出了QFT，一种新颖的用于LLMs的量化全参数调整框架，可以在不损害性能的情况下实现高效的内存微调。我们的框架包括两个新颖的思想：（i）我们采用高效的Lion优化器，仅跟踪动量并具有每个参数一致的更新幅度，这对于稳健的量化是一种内在优势；（ii）我们将所有模型状态进行量化，并以整数值存储，同时提供梯度流和参数更新的方法。

    Large Language Models (LLMs) have showcased remarkable impacts across a wide spectrum of natural language processing tasks. Fine-tuning these pre-trained models on downstream datasets provides further significant performance gains, but this process has been challenging due to its extraordinary resource requirements. To this end, existing efforts focus on parameter-efficient fine-tuning, which, unfortunately, fail to capitalize on the powerful potential of full-parameter fine-tuning. In this work, we propose QFT, a novel Quantized Full-parameter Tuning framework for LLMs that enables memory-efficient fine-tuning without harming performance. Our framework incorporates two novel ideas: (i) we adopt the efficient Lion optimizer, which only keeps track of the momentum and has consistent update magnitudes for each parameter, an inherent advantage for robust quantization; and (ii) we quantize all model states and store them as integer values, and present a gradient flow and parameter update s
    
[^79]: 从纯净演示中进行模仿学习

    Imitation Learning from Purified Demonstration. (arXiv:2310.07143v1 [cs.LG])

    [http://arxiv.org/abs/2310.07143](http://arxiv.org/abs/2310.07143)

    本文提出了一种从纯净演示中进行模仿学习的方法，通过引入扩散过程对不完美演示中的潜在扰动进行净化。通过这种方法，可以改善模仿学习在真实场景中的应用挑战。

    

    模仿学习已经成为解决顺序决策问题的一种有希望的方法，假设专家演示是最优的。然而，在现实世界中，专家演示通常是不完美的，导致在有效应用模仿学习时面临挑战。尽管现有研究已经着眼于优化不完美演示，但通常需要一定比例的最优演示来保证性能。为了解决这些问题，我们提出了在不完美演示中净化潜在扰动并随后从纯净演示中进行模仿学习的方法。受扩散模型的成功启发，我们引入了一个通过扩散过程进行的两步净化。在第一步中，我们应用正向扩散过程通过引入额外的噪声来有效地平滑不完美演示中的潜在扰动。随后，进行逆向生成过程。

    Imitation learning has emerged as a promising approach for addressing sequential decision-making problems, with the assumption that expert demonstrations are optimal. However, in real-world scenarios, expert demonstrations are often imperfect, leading to challenges in effectively applying imitation learning. While existing research has focused on optimizing with imperfect demonstrations, the training typically requires a certain proportion of optimal demonstrations to guarantee performance. To tackle these problems, we propose to purify the potential perturbations in imperfect demonstrations and subsequently conduct imitation learning from purified demonstrations. Motivated by the success of diffusion models, we introduce a two-step purification via the diffusion process. In the first step, we apply a forward diffusion process to effectively smooth out the potential perturbations in imperfect demonstrations by introducing additional noise. Subsequently, a reverse generative process is 
    
[^80]: AE-smnsMLC：基于语义匹配和负标签采样的产品属性值提取的多标签分类

    AE-smnsMLC: Multi-Label Classification with Semantic Matching and Negative Label Sampling for Product Attribute Value Extraction. (arXiv:2310.07137v1 [cs.IR])

    [http://arxiv.org/abs/2310.07137](http://arxiv.org/abs/2310.07137)

    本论文提出了一种基于语义匹配和负标签采样的多标签分类模型 AE-smnsMLC，用于解决产品属性值提取的问题。该模型将属性值提取任务转化为多标签分类任务，可以应用于只有属性值注释的实际场景，而无需属性值的位置信息注释。

    

    产品属性值提取在电子商务等许多实际应用中起着重要作用，如产品搜索和推荐。之前的方法将其视为需要更多注释来标注产品文本中值的序列标记任务。这限制了它们在实际场景中的应用，其中每个产品只有属性值的弱标注，而没有它们的位置信息。此外，这些方法只使用产品文本（即产品标题和描述），而不考虑给定产品的多个属性值与其文本之间的语义连接，这可以帮助属性值提取。在本文中，我们将这个任务重新定义为一个多标签分类任务，可以在实际场景中应用，其中只有属性值的注释可用于训练模型（即没有属性值位置信息的注释）。我们提出了一个具有语义匹配和负标签采样的分类模型

    Product attribute value extraction plays an important role for many real-world applications in e-Commerce such as product search and recommendation. Previous methods treat it as a sequence labeling task that needs more annotation for position of values in the product text. This limits their application to real-world scenario in which only attribute values are weakly-annotated for each product without their position. Moreover, these methods only use product text (i.e., product title and description) and do not consider the semantic connection between the multiple attribute values of a given product and its text, which can help attribute value extraction. In this paper, we reformulate this task as a multi-label classification task that can be applied for real-world scenario in which only annotation of attribute values is available to train models (i.e., annotation of positional information of attribute values is not available). We propose a classification model with semantic matching and
    
[^81]: 在基础模型时代的风险评估和统计显著性

    Risk Assessment and Statistical Significance in the Age of Foundation Models. (arXiv:2310.07132v1 [cs.LG])

    [http://arxiv.org/abs/2310.07132](http://arxiv.org/abs/2310.07132)

    本论文提出了一个分布框架，用于评估具有统计显著性的基础模型的风险。通过一种新的统计相对测试方法，该框架结合了一阶和二阶随机优势，并借鉴了计量经济学和数学金融中常用的平均风险模型。在给定指定度量量化的防护栏的情况下，我们还开发了一种基于风险意识的基础模型选择方法。受数学金融中的投资组合优化和选择理论的启发，我们为每个模型定义了一个"度量组合"，并根据这些组合的随机优势进行模型选择。

    

    我们提出了一个分布框架，用于评估具有统计显著性的基础模型的社会技术风险。我们的方法依赖于一种基于实际随机变量的一阶和二阶随机优势的新的统计相对测试。我们表明，这个测试中的二阶统计与在计量经济学和数学金融中常用的平均风险模型相联系，用于在选择方案时平衡风险和效用。利用这个框架，我们正式开发了一种基于风险意识的基础模型选择方法，给定由指定度量量化的防护栏。受数学金融中的投资组合优化和选择理论的启发，我们为每个模型定义了一个"度量组合"，作为聚合一系列度量的手段，并根据这些组合的随机优势进行模型选择。我们的测试的统计显著性在理论上由通过中心极限的渐近分析支持。

    We propose a distributional framework for assessing socio-technical risks of foundation models with quantified statistical significance. Our approach hinges on a new statistical relative testing based on first and second order stochastic dominance of real random variables. We show that the second order statistics in this test are linked to mean-risk models commonly used in econometrics and mathematical finance to balance risk and utility when choosing between alternatives. Using this framework, we formally develop a risk-aware approach for foundation model selection given guardrails quantified by specified metrics. Inspired by portfolio optimization and selection theory in mathematical finance, we define a \emph{metrics portfolio} for each model as a means to aggregate a collection of metrics, and perform model selection based on the stochastic dominance of these portfolios. The statistical significance of our tests is backed theoretically by an asymptotic analysis via central limit th
    
[^82]: 用于人类反馈的非策略评估

    Off-Policy Evaluation for Human Feedback. (arXiv:2310.07123v1 [cs.LG])

    [http://arxiv.org/abs/2310.07123](http://arxiv.org/abs/2310.07123)

    本论文介绍了一个用于人类反馈的非策略评估（OPEHF）框架，可以准确评估人类反馈信号。这个框架解决了现有OPE方法在估计人类反馈信号上的不足。

    

    非策略评估（OPE）对于强化学习（RL）的离线训练和评估之间的差距的缩小非常重要，它通过仅使用离线轨迹估计目标（评估）策略的性能和/或排名。它可以提高数据收集和策略测试过程的安全性和效率，在在线部署成本较高的情况下，如医疗保健中。然而，现有的OPE方法在估计人类反馈（HF）信号方面存在不足，因为HF可能会受到多个潜在因素的影响，而且只是稀疏可用的；而不同于代理定义的环境奖励（用于策略优化），环境奖励通常是在参数函数或分布上决定的。因此，由于HF信号的性质，准确地推断OPE估计是具有挑战性的。为了解决这个问题，我们引入了一个用于HF的OPE框架，它重新使用现有的OPE方法，以准确评估HF信号。具体而言，我们开发了一个方法来

    Off-policy evaluation (OPE) is important for closing the gap between offline training and evaluation of reinforcement learning (RL), by estimating performance and/or rank of target (evaluation) policies using offline trajectories only. It can improve the safety and efficiency of data collection and policy testing procedures in situations where online deployments are expensive, such as healthcare. However, existing OPE methods fall short in estimating human feedback (HF) signals, as HF may be conditioned over multiple underlying factors and is only sparsely available; as opposed to the agent-defined environmental rewards (used in policy optimization), which are usually determined over parametric functions or distributions. Consequently, the nature of HF signals makes extrapolating accurate OPE estimations to be challenging. To resolve this, we introduce an OPE for HF (OPEHF) framework that revives existing OPE methods in order to accurately evaluate the HF signals. Specifically, we deve
    
[^83]: 《人脑中语言处理的时态结构与深度语言模型的层次结构相符》

    The Temporal Structure of Language Processing in the Human Brain Corresponds to The Layered Hierarchy of Deep Language Models. (arXiv:2310.07106v1 [cs.CL])

    [http://arxiv.org/abs/2310.07106](http://arxiv.org/abs/2310.07106)

    本文展示了深度语言模型(DLMs)的分层结构与人脑语言理解的时间动态之间存在强相关性，通过采用电子皮层图(ECoG)数据来优化时间分辨率，为深入了解人脑语言处理机制提供了新的视角。

    

    深度语言模型(DLMs)提供了一种理解人脑自然语言处理机制的新的计算范式。与传统的心理语言学模型不同，DLMs使用分层的连续数值向量序列来表示单词和上下文，使得诸多新的应用成为可能，如类人生成文本。本文通过展示DLMs的分层结构可能用于模拟大脑语言理解的时间动态，证明了DLM层次深度与最能预测人脑的层次时间之间的强相关性。我们之所以能够在时间上解析出每个层次的优势在于我们采用了电子皮质图(ECoG)数据，其时间分辨率比功能性磁共振成像(fMRI)等无损测量方法更高。我们利用ECoG从参与者听30分钟叙述时记录神经活动，同时将相同叙述提供给高性能DLM(GPT2-XL)。

    Deep Language Models (DLMs) provide a novel computational paradigm for understanding the mechanisms of natural language processing in the human brain. Unlike traditional psycholinguistic models, DLMs use layered sequences of continuous numerical vectors to represent words and context, allowing a plethora of emerging applications such as human-like text generation. In this paper we show evidence that the layered hierarchy of DLMs may be used to model the temporal dynamics of language comprehension in the brain by demonstrating a strong correlation between DLM layer depth and the time at which layers are most predictive of the human brain. Our ability to temporally resolve individual layers benefits from our use of electrocorticography (ECoG) data, which has a much higher temporal resolution than noninvasive methods like fMRI. Using ECoG, we record neural activity from participants listening to a 30-minute narrative while also feeding the same narrative to a high-performing DLM (GPT2-XL)
    
[^84]: 二维电子气体中背景势能估计的机器学习方法

    Machine Learning Methods for Background Potential Estimation in 2DEGs. (arXiv:2310.07089v1 [cond-mat.mes-hall])

    [http://arxiv.org/abs/2310.07089](http://arxiv.org/abs/2310.07089)

    本文利用扫描门电镜和机器学习方法，提出了一种估计二维电子气体（2DEGs）背景势能的新方法，该方法利用了生成对抗神经网络、细胞神经网络和进化搜索算法。研究结果显示，进化搜索算法在缺陷分析方面具有较好的效果。这项工作不仅推动了对2DEGs的理解，还强调了机器学习在探索量子材料方面的潜力，对量子计算和纳米电子学有重要意义。

    

    在量子效应设备和材料领域，二维电子气体（2DEGs）是一种基础结构，承诺着具有变革性的技术。然而，2DEGs中的杂质和缺陷存在着重大挑战，影响载流子迁移率、导电性和量子相干时间。为了解决这个问题，我们利用扫描门电镜（SGM）的能力，采用三种不同的机器学习技术，从SGM数据中估计2DEGs的背景势能：使用生成对抗神经网络的图像到图像转换，细胞神经网络和进化搜索。尽管存在数据限制，我们的研究结果突出了进化搜索算法在这个背景下的有效性，为缺陷分析提供了一种新的方法。这项工作不仅推进了我们对2DEGs的理解，也强调了机器学习在探索量子材料方面的潜力，对量子计算和纳米电子学具有重要意义。

    In the realm of quantum-effect devices and materials, two-dimensional electron gases (2DEGs) stand as fundamental structures that promise transformative technologies. However, the presence of impurities and defects in 2DEGs poses substantial challenges, impacting carrier mobility, conductivity, and quantum coherence time. To address this, we harness the power of scanning gate microscopy (SGM) and employ three distinct machine learning techniques to estimate the background potential of 2DEGs from SGM data: image-to-image translation using generative adversarial neural networks, cellular neural network, and evolutionary search. Our findings, despite data constraints, highlight the effectiveness of an evolutionary search algorithm in this context, offering a novel approach for defect analysis. This work not only advances our understanding of 2DEGs but also underscores the potential of machine learning in probing quantum materials, with implications for quantum computing and nanoelectronic
    
[^85]: 使用概率流动ODE探究密度估计的对抗鲁棒性

    Investigating the Adversarial Robustness of Density Estimation Using the Probability Flow ODE. (arXiv:2310.07084v1 [cs.LG])

    [http://arxiv.org/abs/2310.07084](http://arxiv.org/abs/2310.07084)

    本研究使用概率流动ODE模型探究了密度估计的鲁棒性，并发现在一些情况下，密度估计对高复杂度、高似然度的攻击是鲁棒的，同时对抗样本也具有语义上的意义。

    

    除了其令人印象深刻的采样能力之外，基于得分的扩散模型还提供了一种强大的分析工具，即对训练数据分布下的查询样本进行无偏密度估计。在本研究中，我们通过使用概率流动(PF)神经常微分方程(ODE)模型对密度估计的鲁棒性进行了研究，并探讨了与样本复杂性的关系。我们将样本的压缩大小用作其复杂性的衡量指标。我们引入并评估了六种基于梯度的对数似然最大化攻击，包括一种新的反向积分攻击。我们在CIFAR-10数据集上进行的实验评估表明，使用PF ODE进行密度估计对高复杂度、高似然度攻击是鲁棒的，并且在某些情况下，对抗样本是语义上有意义的，这与鲁棒估计器的预期一致。

    Beyond their impressive sampling capabilities, score-based diffusion models offer a powerful analysis tool in the form of unbiased density estimation of a query sample under the training data distribution. In this work, we investigate the robustness of density estimation using the probability flow (PF) neural ordinary differential equation (ODE) model against gradient-based likelihood maximization attacks and the relation to sample complexity, where the compressed size of a sample is used as a measure of its complexity. We introduce and evaluate six gradient-based log-likelihood maximization attacks, including a novel reverse integration attack. Our experimental evaluations on CIFAR-10 show that density estimation using the PF ODE is robust against high-complexity, high-likelihood attacks, and that in some cases adversarial samples are semantically meaningful, as expected from a robust estimator.
    
[^86]: 通过人工智能消除基于分解的优化问题中的人类因素：第二部分。学习初始化。

    Taking the human out of decomposition-based optimization via artificial intelligence: Part II. Learning to initialize. (arXiv:2310.07082v1 [math.OC])

    [http://arxiv.org/abs/2310.07082](http://arxiv.org/abs/2310.07082)

    本论文提出了一种利用机器学习方法学习优化算法的最佳初始化的方法，以减少计算时间。通过采用主动学习和监督学习技术，学习一个替代模型来预测给定初始化的计算性能，并将该方法应用于解决混合整数模型预测控制问题中的初始化。实验结果显示，所提出的方法可以显著减少求解时间，并且主动学习可以减少所需的数据量。

    

    在过程系统工程任务中，经常需要解决大规模优化问题。分解法是一种常用的解决方法，用于减少计算时间，但其实施具有多个难以配置的步骤。我们提出了一种机器学习方法，学习算法的最佳初始化，以最小化计算时间。采用主动学习和监督学习的方法，学习一个替代模型，预测给定初始化的计算性能。我们将该方法应用于广义Benders分解的初始化，用于解决混合整数模型预测控制问题。替代模型用于寻找应在主问题中添加的初始割的最佳数量。结果表明，所提方法可以显著减少求解时间，主动学习可以减少学习所需的数据量。

    The repeated solution of large-scale optimization problems arises frequently in process systems engineering tasks. Decomposition-based solution methods have been widely used to reduce the corresponding computational time, yet their implementation has multiple steps that are difficult to configure. We propose a machine learning approach to learn the optimal initialization of such algorithms which minimizes the computational time. Active and supervised learning is used to learn a surrogate model that predicts the computational performance for a given initialization. We apply this approach to the initialization of Generalized Benders Decomposition for the solution of mixed integer model predictive control problems. The surrogate models are used to find the optimal number of initial cuts that should be added in the master problem. The results show that the proposed approach can lead to a significant reduction in solution time, and active learning can reduce the data required for learning.
    
[^87]: 使用区块链实现安全的分散式学习

    Secure Decentralized Learning with Blockchain. (arXiv:2310.07079v1 [cs.CR])

    [http://arxiv.org/abs/2310.07079](http://arxiv.org/abs/2310.07079)

    本文提出了基于区块链的分散式联邦学习（BDFL），通过利用区块链进行分散式模型验证和审计，解决了分散式联邦学习中虚假模型和数据攻击的问题。

    

    联邦学习是一种在移动设备和物联网设备上的分布式机器学习范式，它保护数据隐私并优化通信效率。为了避免联邦学习中存在的单点故障问题，提出了分散式联邦学习（DFL），利用点对点通信进行模型聚合，这被认为是分布式个人设备上机器学习任务的有吸引力的解决方案。然而，这个过程容易受到共享虚假模型和数据的攻击者的攻击。如果存在一组恶意客户端，他们可能通过进行投毒攻击来损害模型的性能。此外，在DFL中，客户端通常缺乏激励来贡献他们的计算能力来进行模型训练。在本文中，我们提出了基于区块链的分散式联邦学习（BDFL），它利用区块链进行分散式模型验证和审计。BDFL包括一个审核委员会用于模型验证，一个...

    Federated Learning (FL) is a well-known paradigm of distributed machine learning on mobile and IoT devices, which preserves data privacy and optimizes communication efficiency. To avoid the single point of failure problem in FL, decentralized federated learning (DFL) has been proposed to use peer-to-peer communication for model aggregation, which has been considered an attractive solution for machine learning tasks on distributed personal devices. However, this process is vulnerable to attackers who share false models and data. If there exists a group of malicious clients, they might harm the performance of the model by carrying out a poisoning attack. In addition, in DFL, clients often lack the incentives to contribute their computing powers to do model training. In this paper, we proposed Blockchain-based Decentralized Federated Learning (BDFL), which leverages a blockchain for decentralized model verification and auditing. BDFL includes an auditor committee for model verification, a
    
[^88]: 通过抗内容采样对COVID-19虚假信息数据集进行审计和强化

    Auditing and Robustifying COVID-19 Misinformation Datasets via Anticontent Sampling. (arXiv:2310.07078v1 [cs.LG])

    [http://arxiv.org/abs/2310.07078](http://arxiv.org/abs/2310.07078)

    本文拓展了对COVID-19虚假信息数据集进行审计和强化的研究。首先，发现在小数据上训练的专业分类器在野外环境中的性能表现有限。其次，提出了一种无需人工注释的主动学习方法，通过增加具有挑战性的抗内容来增强分类器。

    

    本文有两个关键贡献。首先，它认为在小数据上训练的高度专业的罕见内容分类器通常对野外观察到的负面类别的丰富性和话题多样性（称为抗内容）有限暴露。因此，这些分类器在测试集上观察到的强大性能可能无法在现实世界的环境中有效转化。在COVID-19虚假信息检测的背景下，我们对多个数据集进行了野外审计，并证明了使用几个重要引用的最近数据集进行训练的模型在野外评估时容易受到抗内容的攻击。其次，我们提出了一种新颖的主动学习流程，它不需要任何人工注释，并通过挑战性的抗内容不断增强训练数据，从而强化这些分类器。

    This paper makes two key contributions. First, it argues that highly specialized rare content classifiers trained on small data typically have limited exposure to the richness and topical diversity of the negative class (dubbed anticontent) as observed in the wild. As a result, these classifiers' strong performance observed on the test set may not translate into real-world settings. In the context of COVID-19 misinformation detection, we conduct an in-the-wild audit of multiple datasets and demonstrate that models trained with several prominently cited recent datasets are vulnerable to anticontent when evaluated in the wild. Second, we present a novel active learning pipeline that requires zero manual annotation and iteratively augments the training data with challenging anticontent, robustifying these classifiers.
    
[^89]: 通过人工智能实现基于分解的优化方法的自动化选择：第一部分 - 学习何时分解

    Taking the human out of decomposition-based optimization via artificial intelligence: Part I. Learning when to decompose. (arXiv:2310.07068v1 [math.OC])

    [http://arxiv.org/abs/2310.07068](http://arxiv.org/abs/2310.07068)

    本文提出了一种用于自动确定是使用整体式解法还是基于分解的解法的图分类方法，并展示了如何将该分类器应用于凸混合整数非线性规划问题的求解。

    

    本文提出了一种图分类方法，用于自动确定是使用整体式解法还是基于分解的解法。在这个方法中，优化问题被表示为一个图，通过适当的特征捕捉问题的变量和约束之间的结构和功能关联。基于这种表示，建立一个图分类器来确定给定问题的最佳解法。所提出的方法用于开发一个分类器，确定是否应使用分支定界法或外部逼近算法来解决凸混合整数非线性规划问题。最后，展示了如何将学习到的分类器纳入现有的混合整数优化求解器中。

    In this paper, we propose a graph classification approach for automatically determining whether to use a monolithic or a decomposition-based solution method. In this approach, an optimization problem is represented as a graph that captures the structural and functional coupling among the variables and constraints of the problem via an appropriate set of features. Given this representation, a graph classifier is built to determine the best solution method for a given problem. The proposed approach is used to develop a classifier that determines whether a convex Mixed Integer Nonlinear Programming problem should be solved using branch and bound or the outer approximation algorithm. Finally, it is shown how the learned classifier can be incorporated into existing mixed integer optimization solvers.
    
[^90]: 声学模型融合用于端到端语音识别

    Acoustic Model Fusion for End-to-end Speech Recognition. (arXiv:2310.07062v1 [cs.SD])

    [http://arxiv.org/abs/2310.07062](http://arxiv.org/abs/2310.07062)

    该论文提出了一种声学模型融合的方法，可以改进端到端语音识别系统中存在的领域不匹配问题，并显著降低词错误率。

    

    深度学习和自动语音识别（ASR）的最新进展使得端到端（E2E）ASR系统成为可能，并将准确度提升到一个新的水平。E2E系统在训练语音-文本对的同时隐式地建模了所有传统的ASR组件，如声学模型（AM）和语言模型（LM）。尽管这种简化的系统架构，将一个专门在文本语料库上训练的独立LM融合到E2E系统中被证明是有益的。然而，LM融合的应用存在一定的缺点，例如无法解决内部AM固有的领域不匹配问题。受到LM融合概念的启发，我们提出将外部AM融合到E2E系统中，以更好地解决领域不匹配问题。通过实施这种新颖的方法，我们在不同测试集中实现了显著的词错误率下降，最高达14.3％。我们还发现，这种AM融合方法可以提供额外的性能优势。

    Recent advances in deep learning and automatic speech recognition (ASR) have enabled the end-to-end (E2E) ASR system and boosted the accuracy to a new level. The E2E systems implicitly model all conventional ASR components, such as the acoustic model (AM) and the language model (LM), in a single network trained on audio-text pairs. Despite this simpler system architecture, fusing a separate LM, trained exclusively on text corpora, into the E2E system has proven to be beneficial. However, the application of LM fusion presents certain drawbacks, such as its inability to address the domain mismatch issue inherent to the internal AM. Drawing inspiration from the concept of LM fusion, we propose the integration of an external AM into the E2E system to better address the domain mismatch. By implementing this novel approach, we have achieved a significant reduction in the word error rate, with an impressive drop of up to 14.3% across varied test sets. We also discovered that this AM fusion ap
    
[^91]: DKEC: 领域知识增强的电子病历多标签分类

    DKEC: Domain Knowledge Enhanced Multi-Label Classification for Electronic Health Records. (arXiv:2310.07059v1 [cs.CL])

    [http://arxiv.org/abs/2310.07059](http://arxiv.org/abs/2310.07059)

    本文提出了DKEC，一种领域知识增强的分类器，用于医学诊断预测。它利用标签的注意力机制和组内训练方法来捕捉医学实体之间的语义关系，并增加罕见类别的样本数量。评估结果显示其在医学数据集上表现良好。

    

    医学领域的多标签文本分类任务经常面临长尾标签分布，即罕见类别的训练样本少于频繁类别。虽然之前的工作已经探索了不同的模型架构和层次化标签结构来找到重要特征，但大多数忽略了从医学指南中融入领域知识。本文提出了DKEC，一种增强医学诊断预测的领域知识增强分类器，其中包括两个创新点：（1）一个基于标签的注意力机制，结合异构图和领域本体来捕捉医学实体之间的语义关系，（2）一种基于标签相似性的简单而有效的组内训练方法，用于增加罕见类别的样本数量。我们在两个真实的医学数据集上评估了DKEC：RAA数据集，包含来自急救服务（EMS）事件的4,417个患者护理报告的收集，和来自53898报告的子集。

    Multi-label text classification (MLTC) tasks in the medical domain often face long-tail label distribution, where rare classes have fewer training samples than frequent classes. Although previous works have explored different model architectures and hierarchical label structures to find important features, most of them neglect to incorporate the domain knowledge from medical guidelines. In this paper, we present DKEC, Domain Knowledge Enhanced Classifier for medical diagnosis prediction with two innovations: (1) a label-wise attention mechanism that incorporates a heterogeneous graph and domain ontologies to capture the semantic relationships between medical entities, (2) a simple yet effective group-wise training method based on similarity of labels to increase samples of rare classes. We evaluate DKEC on two real-world medical datasets: the RAA dataset, a collection of 4,417 patient care reports from emergency medical services (EMS) incidents, and a subset of 53,898 reports from the 
    
[^92]: FedMFS: 选择性模态通信的联邦多模态融合学习

    FedMFS: Federated Multimodal Fusion Learning with Selective Modality Communication. (arXiv:2310.07048v1 [cs.LG])

    [http://arxiv.org/abs/2310.07048](http://arxiv.org/abs/2310.07048)

    FedMFS是一种新的多模态融合联邦学习方法，通过选择性模态通信解决了缺乏特定模态的异构客户问题，并设计了最优的模态上传策略以提高学习性能。

    

    联邦学习是一种分布式机器学习范式，通过仅共享模型参数而不访问、侵犯或泄露原始用户数据，使客户能够合作。在物联网中，边缘设备越来越多地利用多模态数据组合和融合范式来提高模型性能。然而，在联邦学习应用中，仍然存在两个主要挑战：（一）解决由于缺乏特定模态的异构客户引起的问题；（二）设计一种最优的模态上传策略，以最小化通信开销同时最大化学习性能。在本文中，我们提出了一种新的多模态融合联邦学习方法，名为FedMFS，可以解决上述挑战。关键思想是利用Shapley值来量化每个模态的贡献和模态模型大小来衡量通信开销，以便每个客户端可以。

    Federated learning (FL) is a distributed machine learning (ML) paradigm that enables clients to collaborate without accessing, infringing upon, or leaking original user data by sharing only model parameters. In the Internet of Things (IoT), edge devices are increasingly leveraging multimodal data compositions and fusion paradigms to enhance model performance. However, in FL applications, two main challenges remain open: (i) addressing the issues caused by heterogeneous clients lacking specific modalities and (ii) devising an optimal modality upload strategy to minimize communication overhead while maximizing learning performance. In this paper, we propose Federated Multimodal Fusion learning with Selective modality communication (FedMFS), a new multimodal fusion FL methodology that can tackle the above mentioned challenges. The key idea is to utilize Shapley values to quantify each modality's contribution and modality model size to gauge communication overhead, so that each client can 
    
[^93]: 利润驱动流失防止的预测和优化方法

    A predict-and-optimize approach to profit-driven churn prevention. (arXiv:2310.07047v1 [cs.LG])

    [http://arxiv.org/abs/2310.07047](http://arxiv.org/abs/2310.07047)

    本文提出了一种利润驱动流失防止的预测和优化方法。该方法利用个体顾客生命周期价值（CLV）来定位最有价值的顾客，并采用预测和优化框架进行解决。研究结果表明，该方法在12个流失预测数据集上取得了最佳的平均利润绩效。

    

    本文介绍一种新颖的利润驱动流失防止的预测和优化方法。我们将定位顾客用于留存活动的任务构建为一个后悔最小化问题。主要目标是利用个体顾客生命周期价值（CLV）确保只有最有价值的顾客被定位。与之相反，许多利润驱动策略关注流失概率同时考虑平均CLV，这往往导致数据聚合带来的信息损失。我们提出的模型符合预测和优化（PnO）框架的指导方针，并可以使用随机梯度下降方法高效解决。来自12个流失预测数据集的结果突出了我们方法的有效性，在平均利润方面实现了最佳的绩效。

    In this paper, we introduce a novel predict-and-optimize method for profit-driven churn prevention. We frame the task of targeting customers for a retention campaign as a regret minimization problem. The main objective is to leverage individual customer lifetime values (CLVs) to ensure that only the most valuable customers are targeted. In contrast, many profit-driven strategies focus on churn probabilities while considering average CLVs. This often results in significant information loss due to data aggregation. Our proposed model aligns with the guidelines of Predict-and-Optimize (PnO) frameworks and can be efficiently solved using stochastic gradient descent methods. Results from 12 churn prediction datasets underscore the effectiveness of our approach, which achieves the best average performance compared to other well-established strategies in terms of average profit.
    
[^94]: 健康系统规模的计算病理学——来自30亿图像的自监督基础模型

    Computational Pathology at Health System Scale -- Self-Supervised Foundation Models from Three Billion Images. (arXiv:2310.07033v1 [cs.CV])

    [http://arxiv.org/abs/2310.07033](http://arxiv.org/abs/2310.07033)

    该论文通过使用大规模无标签数据集训练自监督基础模型并在大型临床病理学数据集上进行评估，旨在训练最大的学术基础模型并评估最显著的自监督学习算法。

    

    最近的自监督学习突破使得可以使用大型无标签数据集来训练视觉基础模型，从而可以推广到各种下游任务中。虽然这种训练范式非常适合医学领域，因为标注往往稀缺，但是在医学领域，特别是病理学领域，大规模预训练的研究还不够充分。以往的病理学自监督学习工作利用较小的数据集进行预训练和评估下游性能。该项目的目标是训练最大的学术基础模型，并通过在大规模临床病理学数据集上进行预训练和评估下游性能来评估最显著的自监督学习算法。我们收集了迄今为止最大的病理学数据集，包括来自超过423,000个显微镜幻灯片的30亿张图像。我们比较了使用掩蔽自编码器（MAE）和D来预训练视觉变换器模型。

    Recent breakthroughs in self-supervised learning have enabled the use of large unlabeled datasets to train visual foundation models that can generalize to a variety of downstream tasks. While this training paradigm is well suited for the medical domain where annotations are scarce, large-scale pre-training in the medical domain, and in particular pathology, has not been extensively studied. Previous work in self-supervised learning in pathology has leveraged smaller datasets for both pre-training and evaluating downstream performance. The aim of this project is to train the largest academic foundation model and benchmark the most prominent self-supervised learning algorithms by pre-training and evaluating downstream performance on large clinical pathology datasets. We collected the largest pathology dataset to date, consisting of over 3 billion images from over 423 thousand microscopy slides. We compared pre-training of visual transformer models using the masked autoencoder (MAE) and D
    
[^95]: 神经谐波：一种可解释的深度结构，用于非线性动态系统识别及其在音频处理中的应用

    Neural Harmonium: An Interpretable Deep Structure for Nonlinear Dynamic System Identification with Application to Audio Processing. (arXiv:2310.07032v1 [cs.SD])

    [http://arxiv.org/abs/2310.07032](http://arxiv.org/abs/2310.07032)

    本文提出了一种可解释的深度结构，用于非线性动态系统识别，并应用于音频处理。通过利用谐波分析和递归建模方式，该模型在时间频率域中对系统进行建模，并通过神经网络识别频率间的相互依赖关系。该模型具有高时域和频谱分辨率，能够快速、稳健、精确地进行二阶优化，无需显式计算海森矩阵。

    

    近年来，提高深度神经网络的可解释性引起了越来越多的关注，特别是在利用深度学习解决物理问题的情况下。可解释性有助于我们理解模型的泛化能力并揭示其局限性。本文介绍了一种因果可解释的深度结构，用于建模动态系统。我们的模型利用谐波分析，在时间频率域中对系统进行建模，同时保持高时域和频谱分辨率。此外，该模型以递归方式构建，允许快速、稳健和精确的二阶优化，无需显式计算海森矩阵。为了克服系统构建模块导致的高维问题，设计了一个神经网络来识别频率间的相互依赖关系。所提出的模型在非线性系统识别问题上进行了演示和验证，以满足音频信号处理的要求。

    Improving the interpretability of deep neural networks has recently gained increased attention, especially when the power of deep learning is leveraged to solve problems in physics. Interpretability helps us understand a model's ability to generalize and reveal its limitations. In this paper, we introduce a causal interpretable deep structure for modeling dynamic systems. Our proposed model makes use of the harmonic analysis by modeling the system in a time-frequency domain while maintaining high temporal and spectral resolution. Moreover, the model is built in an order recursive manner which allows for fast, robust, and exact second order optimization without the need for an explicit Hessian calculation. To circumvent the resulting high dimensionality of the building blocks of our system, a neural network is designed to identify the frequency interdependencies. The proposed model is illustrated and validated on nonlinear system identification problems as required for audio signal proc
    
[^96]: 大脑年龄再访：用深度学习研究基于脑电图的大脑年龄动态的状态假说与特质假说

    Brain Age Revisited: Investigating the State vs. Trait Hypotheses of EEG-derived Brain-Age Dynamics with Deep Learning. (arXiv:2310.07029v1 [q-bio.NC])

    [http://arxiv.org/abs/2310.07029](http://arxiv.org/abs/2310.07029)

    这项研究使用深度学习方法，通过对临床脑电图的研究来探讨大脑的生物年龄动态。研究结果对于理解大脑的状态和特质假说以及脑病变具有重要意义。

    

    大脑的生物年龄被认为是一个有潜力的神经学显著生物标记物。然而，基于纵向磁共振成像数据的最新结果引发了对其解释的质疑。一个核心问题是大脑的生物年龄是否指示了脑病变，并且大脑年龄的变化是否与诊断出的病理相关（状态假说）。或者，大脑年龄的差异是否是每个个体独特的稳定特征（特质假说）？为了回答这个问题，我们对基于临床脑电图的大脑衰老进行了全面的研究，这是对之前基于核磁共振成像的调查的补充。我们采用了最先进的时序卷积网络（TCN）进行年龄回归任务。我们用被明确标记为非病理的Temple大学医院脑电图语料库（TUEG）记录进行训练，并在非病理和病理主题的记录上进行评估。

    The brain's biological age has been considered as a promising candidate for a neurologically significant biomarker. However, recent results based on longitudinal magnetic resonance imaging data have raised questions on its interpretation. A central question is whether an increased biological age of the brain is indicative of brain pathology and if changes in brain age correlate with diagnosed pathology (state hypothesis). Alternatively, could the discrepancy in brain age be a stable characteristic unique to each individual (trait hypothesis)? To address this question, we present a comprehensive study on brain aging based on clinical EEG, which is complementary to previous MRI-based investigations. We apply a state-of-the-art Temporal Convolutional Network (TCN) to the task of age regression. We train on recordings of the Temple University Hospital EEG Corpus (TUEG) explicitly labeled as non-pathological and evaluate on recordings of subjects with non-pathological as well as pathologica
    
[^97]: 使用细粒度特征进行面部伪造的Deepfake检测

    Facial Forgery-based Deepfake Detection using Fine-Grained Features. (arXiv:2310.07028v1 [cs.CV])

    [http://arxiv.org/abs/2310.07028](http://arxiv.org/abs/2310.07028)

    该论文提出了一种基于细粒度特征的Deepfake检测方法，通过抑制背景噪声和学习区分性特征来提高检测的效果。

    

    通过Deepfake的面部伪造已经引发了重大的安全风险并引起了严重的社会关切。为了应对这一问题，提出了许多Deepfake检测方法。其中大多数方法将Deepfake检测建模为使用预训练的骨干卷积神经网络(CNN)架构进行二分类的问题。这些基于CNN的方法在Deepfake检测方面表现出非常高的效果，曲线下面积(AUC)高达0.99。然而，当被应用于不同数据集和Deepfake操纵技术进行评估时，这些方法的性能会显著下降。因此，我们需要学习更加微妙、局部和有辨别力的特征来进行Deepfake检测。本文将Deepfake检测问题视为细粒度分类问题，并提出了一个新的细粒度解决方案。具体而言，我们的方法通过有效地抑制背景噪声和学习区分性特征来学习微妙且具有一般性的特征。

    Facial forgery by deepfakes has caused major security risks and raised severe societal concerns. As a countermeasure, a number of deepfake detection methods have been proposed. Most of them model deepfake detection as a binary classification problem using a backbone convolutional neural network (CNN) architecture pretrained for the task. These CNN-based methods have demonstrated very high efficacy in deepfake detection with the Area under the Curve (AUC) as high as $0.99$. However, the performance of these methods degrades significantly when evaluated across datasets and deepfake manipulation techniques. This draws our attention towards learning more subtle, local, and discriminative features for deepfake detection. In this paper, we formulate deepfake detection as a fine-grained classification problem and propose a new fine-grained solution to it. Specifically, our method is based on learning subtle and generalizable features by effectively suppressing background noise and learning di
    
[^98]: 利用合成数据进行医学视觉-语言预训练：绕过对真实图像的需求

    Utilizing Synthetic Data for Medical Vision-Language Pre-training: Bypassing the Need for Real Images. (arXiv:2310.07027v1 [cs.CV])

    [http://arxiv.org/abs/2310.07027](http://arxiv.org/abs/2310.07027)

    本研究探讨了使用合成图像在医学VLP中的可行性和有效性，通过用真实医学报告生成的合成图像替换真实图像进行训练，并在图像分类、语义分割和目标检测任务中进行了实证评估。

    

    医学视觉-语言预训练(VLP)从医学图像和配对的放射学报告中联合学习表示。它通常需要大规模的图像-文本配对数据集，以实现图像编码器和文本编码器的有效预训练。文本引导生成模型的出现引发了一个引人注目的问题：是否可以仅使用从真实放射学报告中生成的合成图像来实现VLP，从而减轻了对大量配对和筛选图像-文本数据集的需要？在这项工作中，我们通过检查使用合成图像进行医学VLP的可行性和有效性来审查这个问题。我们用真实医学报告生成的合成图像替换真实医学图像。我们使用三种最先进的VLP算法专门在这些合成样本上进行训练。我们的经验评估涵盖了三个连续任务，即图像分类、语义分割和目标检测。

    Medical Vision-Language Pre-training (VLP) learns representations jointly from medical images and paired radiology reports. It typically requires large-scale paired image-text datasets to achieve effective pre-training for both the image encoder and text encoder. The advent of text-guided generative models raises a compelling question: Can VLP be implemented solely with synthetic images generated from genuine radiology reports, thereby mitigating the need for extensively pairing and curating image-text datasets? In this work, we scrutinize this very question by examining the feasibility and effectiveness of employing synthetic images for medical VLP. We replace real medical images with their synthetic equivalents, generated from authentic medical reports. Utilizing three state-of-the-art VLP algorithms, we exclusively train on these synthetic samples. Our empirical evaluation across three subsequent tasks, namely image classification, semantic segmentation and object detection, reveals
    
[^99]: 从大规模交互轨迹中自动挖掘宏任务

    Automatic Macro Mining from Interaction Traces at Scale. (arXiv:2310.07023v1 [cs.HC])

    [http://arxiv.org/abs/2310.07023](http://arxiv.org/abs/2310.07023)

    本文介绍了一种基于大型语言模型的新方法，用于从移动交互轨迹中自动提取语义上有意义的宏任务。通过多项研究和实验证明了该方法的有效性和提取的宏任务的实用性。

    

    宏任务是我们日常手机活动的构建块任务（例如，“登录”或“预定航班”）。有效地提取宏任务对于理解移动交互和实现任务自动化至关重要。然而，这些宏任务在大规模情况下很难提取，因为它们可以由多个步骤组成，同时又隐藏在应用的编程组件中。在本文中，我们介绍了一种基于大型语言模型（LLMs）的新方法，以自动从随机和用户策划的移动交互轨迹中提取语义上有意义的宏任务。我们的方法产生的宏任务自动标记了自然语言描述，并且可以完全执行。为了检验提取的质量，我们进行了多项研究，包括用户评估、与人工策划任务的比较分析以及对这些宏任务的自动执行。这些实验和分析显示了我们方法的有效性以及提取的宏任务在不同的任务中的有用性。

    Macros are building block tasks of our everyday smartphone activity (e.g., "login", or "booking a flight"). Effectively extracting macros is important for understanding mobile interaction and enabling task automation. These macros are however difficult to extract at scale as they can be comprised of multiple steps yet hidden within programmatic components of the app. In this paper, we introduce a novel approach based on Large Language Models (LLMs) to automatically extract semantically meaningful macros from both random and user-curated mobile interaction traces. The macros produced by our approach are automatically tagged with natural language descriptions and are fully executable. To examine the quality of extraction, we conduct multiple studies, including user evaluation, comparative analysis against human-curated tasks, and automatic execution of these macros. These experiments and analyses show the effectiveness of our approach and the usefulness of extracted macros in various dow
    
[^100]: 快速模块化元学习的神经关系推理

    Neural Relational Inference with Fast Modular Meta-learning. (arXiv:2310.07015v1 [cs.LG])

    [http://arxiv.org/abs/2310.07015](http://arxiv.org/abs/2310.07015)

    本论文提出了一种基于快速模块化元学习的神经关系推理方法，通过训练神经模块，可以在多种任务中以不同的方式组合，隐式地编码时间不变性，并在一个上下文中推断关系，从而增加了推理能力。

    

    图神经网络(GNN)是用于包含实体和关系的许多动态系统的有效模型。尽管大多数GNN应用假设实体和关系只有一种类型，但许多情况涉及多种类型的交互。关系推理是从观测数据中推断这些交互并学习动力学的问题。我们将关系推理视为模块化元学习问题，通过训练神经模块以不同的方式组合来解决许多任务。这个元学习框架允许我们隐式地编码时间不变性，并在一个上下文中推断关系，而不是独立地推断，从而增加了推理能力。将推理作为元学习的内循环优化导致了一种基于模型的方法，更加高效利用数据，并能够估计我们不直接观测到的实体的状态，但可以从它们对观测结果的影响中推断出它们的存在。

    \textit{Graph neural networks} (GNNs) are effective models for many dynamical systems consisting of entities and relations. Although most GNN applications assume a single type of entity and relation, many situations involve multiple types of interactions. \textit{Relational inference} is the problem of inferring these interactions and learning the dynamics from observational data. We frame relational inference as a \textit{modular meta-learning} problem, where neural modules are trained to be composed in different ways to solve many tasks. This meta-learning framework allows us to implicitly encode time invariance and infer relations in context of one another rather than independently, which increases inference capacity. Framing inference as the inner-loop optimization of meta-learning leads to a model-based approach that is more data-efficient and capable of estimating the state of entities that we do not observe directly, but whose existence can be inferred from their effect on obser
    
[^101]: 答案候选类型选择：闭书问答中的文本到文本语言模型满足知识图谱

    Answer Candidate Type Selection: Text-to-Text Language Model for Closed Book Question Answering Meets Knowledge Graphs. (arXiv:2310.07008v1 [cs.CL])

    [http://arxiv.org/abs/2310.07008](http://arxiv.org/abs/2310.07008)

    本文提出了一种新颖的方法，通过对预训练的文本到文本问答系统生成的候选答案基于其类型进行过滤和重新排序，以解决在知识图谱问答任务中，模型容量有限且对于含有不太流行实体的问题质量下降的问题。

    

    预训练的文本到文本语言模型（如T5或BART）在知识图谱问答（KGQA）任务中取得了令人期待的结果。然而，模型的容量有限，对于包含不太流行实体的问题，质量下降。在本文中，我们提出了一种新颖的方法，该方法在预训练的文本到文本问答系统的基础上解决了这个问题。我们的简单而有效的方法根据候选答案的类型（来自Wikidata的"instance_of"属性）进行筛选和重新排序。

    Pre-trained Text-to-Text Language Models (LMs), such as T5 or BART yield promising results in the Knowledge Graph Question Answering (KGQA) task. However, the capacity of the models is limited and the quality decreases for questions with less popular entities. In this paper, we present a novel approach which works on top of the pre-trained Text-to-Text QA system to address this issue. Our simple yet effective method performs filtering and re-ranking of generated candidates based on their types derived from Wikidata "instance_of" property.
    
[^102]: 声音攻击预防的AI辅助生成器Sound-skwatter（你是指Sound-squatter吗？）

    Sound-skwatter (Did You Mean: Sound-squatter?) AI-powered Generator for Phishing Prevention. (arXiv:2310.07005v1 [cs.CR])

    [http://arxiv.org/abs/2310.07005](http://arxiv.org/abs/2310.07005)

    声音攻击是一种网络钓鱼攻击方式，通过利用单词发音相似性来欺骗用户，而Sound-skwatter是一种多语言的AI系统，通过学习声音相似性生成声音攻击候选项，具有较高质量和跨语言应用能力。实验证明，Sound-skwatter能够发现大量保护解决方案未知的已知和新型同音词候选项。

    

    声音攻击是通过利用单词发音相似性来欺骗用户进入恶意资源的网络钓鱼攻击。对声音攻击候选项进行主动防御是很复杂的，现有的解决方案都依赖于手动编辑的同音词列表。本文介绍了一种基于多语言的AI系统Sound-skwatter，用于生成声音攻击候选项进行主动防御。Sound-skwatter利用转换网络和声学模型的创新多模态组合来学习声音相似性。实验证明，Sound-skwatter能够自动列出已知的同音词和成千上万个高质量的候选项。此外，它可以应用于跨语言声音攻击，即当阅读者和听者使用不同语言时，支持任意语言组合。我们将Sound-skwatter应用于以域名劫持为基础的网络钓鱼攻击上。我们发现，大约10％的生成域名在现实中存在，其中绝大部分对保护解决方案来说是未知的。

    Sound-squatting is a phishing attack that tricks users into malicious resources by exploiting similarities in the pronunciation of words. Proactive defense against sound-squatting candidates is complex, and existing solutions rely on manually curated lists of homophones. We here introduce Sound-skwatter, a multi-language AI-based system that generates sound-squatting candidates for proactive defense. Sound-skwatter relies on an innovative multi-modal combination of Transformers Networks and acoustic models to learn sound similarities. We show that Sound-skwatter can automatically list known homophones and thousands of high-quality candidates. In addition, it covers cross-language sound-squatting, i.e., when the reader and the listener speak different languages, supporting any combination of languages. We apply Sound-skwatter to network-centric phishing via squatted domain names. We find ~ 10% of the generated domains exist in the wild, the vast majority unknown to protection solutions.
    
[^103]: CarDS-Plus ECG平台：开发和可行性评估一个适用于便携式和可穿戴设备心电图的多平台人工智能工具包

    CarDS-Plus ECG Platform: Development and Feasibility Evaluation of a Multiplatform Artificial Intelligence Toolkit for Portable and Wearable Device Electrocardiograms. (arXiv:2310.07000v1 [cs.LG])

    [http://arxiv.org/abs/2310.07000](http://arxiv.org/abs/2310.07000)

    该论文描述了CarDS-Plus ECG平台的开发，该平台是一个创新的多平台系统，旨在快速部署基于人工智能的心电图解决方案用于临床调查和医疗服务交付。

    

    在现代医疗保健领域快速发展的环境下，将可穿戴和便携技术整合进来为个性化的健康监测提供了一个独特的机会。像Apple Watch、FitBit和AliveCor KardiaMobile这样的设备已经彻底改变了复杂健康数据流的获取和处理。在这些设备收集的各种数据中，单导联心电图（ECG）记录已经成为监测心血管健康的重要信息来源。人工智能在解读这些单导联心电图方面取得了显著进展，有助于临床诊断和罕见心脏疾病的检测。这项设计研究描述了一个创新的多平台系统的开发，旨在快速部署基于人工智能的心电图解决方案用于临床调查和医疗服务交付。该研究探讨了设计考虑因素，并将其与特定应用相一致，开发了数据流

    In the rapidly evolving landscape of modern healthcare, the integration of wearable & portable technology provides a unique opportunity for personalized health monitoring in the community. Devices like the Apple Watch, FitBit, and AliveCor KardiaMobile have revolutionized the acquisition and processing of intricate health data streams. Amidst the variety of data collected by these gadgets, single-lead electrocardiogram (ECG) recordings have emerged as a crucial source of information for monitoring cardiovascular health. There has been significant advances in artificial intelligence capable of interpreting these 1-lead ECGs, facilitating clinical diagnosis as well as the detection of rare cardiac disorders. This design study describes the development of an innovative multiplatform system aimed at the rapid deployment of AI-based ECG solutions for clinical investigation & care delivery. The study examines design considerations, aligning them with specific applications, develops data flow
    
[^104]: 通过元认知提示违反期望降低大型语言模型中的心智理论预测误差

    Violation of Expectation via Metacognitive Prompting Reduces Theory of Mind Prediction Error in Large Language Models. (arXiv:2310.06983v1 [cs.CL])

    [http://arxiv.org/abs/2310.06983](http://arxiv.org/abs/2310.06983)

    本文研究了如何利用违反期望机制在大型语言模型中降低用户预测误差。我们引入了元认知提示框架，并发现存储和检索违反用户期望的事实可以使模型以类似人类学习理论的方式了解用户。

    

    最近的研究表明，大型语言模型(LLMs)在心智理论(ToM)任务中展现出了令人信服的水平。将不可观察的心理状态归因于他人对于人类社会认知至关重要，并且在个体人类与人工智能(AIs)之间的委托-代理关系中可能同样重要。在本文中，我们探讨了一种在发展心理学中研究的机制，即违反期望(VoE)，如何实现以通过利用新生的ToM功能来降低LLM对用户的预测误差。我们引入了一个“元认知提示”框架，以在AI辅导员的情境中应用VoE。通过存储和检索在LLM对用户期望被违反的情况下得到的事实，我们发现LLM能够以与人类学习理论相符的方式了解用户。最后，我们讨论了建模用户心理的潜在危险和增强机会，并提出了控制这些问题的方法。

    Recent research shows that Large Language Models (LLMs) exhibit a compelling level of proficiency in Theory of Mind (ToM) tasks. This ability to impute unobservable mental states to others is vital to human social cognition and may prove equally important in principal-agent relations between individual humans and Artificial Intelligences (AIs). In this paper, we explore how a mechanism studied in developmental psychology known as Violation of Expectation (VoE) can be implemented to reduce errors in LLM prediction about users by leveraging emergent ToM affordances. And we introduce a \textit{metacognitive prompting} framework to apply VoE in the context of an AI tutor. By storing and retrieving facts derived in cases where LLM expectation about the user was violated, we find that LLMs are able to learn about users in ways that echo theories of human learning. Finally, we discuss latent hazards and augmentative opportunities associated with modeling user psychology and propose ways to mi
    
[^105]: 数据蒸馏就像伏特加一样：多次蒸馏获得更好的质量

    Data Distillation Can Be Like Vodka: Distilling More Times For Better Quality. (arXiv:2310.06982v1 [cs.CV])

    [http://arxiv.org/abs/2310.06982](http://arxiv.org/abs/2310.06982)

    本论文介绍了一种名为渐进式数据集蒸馏（PDD）的方法，通过合成多个小的合成图像集，并在这些子集的累计联合上进行模型训练，以解决目前数据集蒸馏技术的问题，从而获得更好的泛化性能。

    

    数据集蒸馏旨在通过创建一小组具有与完整数据集相似的泛化性能的合成图像，从而最小化训练深度网络所需的时间和内存，在大数据集上进行训练。然而，目前的数据集蒸馏技术存在缺陷，与在原始数据上训练相比，表现出明显的性能差距。在这项工作中，我们首次提出仅使用一个合成子集进行蒸馏将无法获得最佳的泛化性能。这是因为深度网络的训练动态在训练过程中发生了显著变化。因此，需要多个合成子集来捕捉训练的不同阶段的动态特征。为了解决这个问题，我们提出了渐进式数据集蒸馏（Progressive Dataset Distillation，简称PDD）。PDD合成多个小的合成图像集，每个集合都是以前集合为条件，并在这些子集的累计联合上训练模型，无需额外的训练时间。

    Dataset distillation aims to minimize the time and memory needed for training deep networks on large datasets, by creating a small set of synthetic images that has a similar generalization performance to that of the full dataset. However, current dataset distillation techniques fall short, showing a notable performance gap when compared to training on the original data. In this work, we are the first to argue that using just one synthetic subset for distillation will not yield optimal generalization performance. This is because the training dynamics of deep networks drastically change during the training. Hence, multiple synthetic subsets are required to capture the training dynamics at different phases of training. To address this issue, we propose Progressive Dataset Distillation (PDD). PDD synthesizes multiple small sets of synthetic images, each conditioned on the previous sets, and trains the model on the cumulative union of these subsets without requiring additional training time
    
[^106]: 具有差分隐私的联邦量子机器学习

    Federated Quantum Machine Learning with Differential Privacy. (arXiv:2310.06973v1 [quant-ph])

    [http://arxiv.org/abs/2310.06973](http://arxiv.org/abs/2310.06973)

    本文提出了将量子联邦学习（QFL）和量子差分隐私（QDP）相结合的方法，并在量子平台上实施，以实现对数据泄露和模型反演攻击的全面保护。

    

    在敏感训练数据上实施人工智能时，隐私保护是一个重要问题。有几种保护数据隐私的技术，但由于无克隆定理，量子计算在本质上更安全，因此成为最理想的计算平台之一。先前的研究中曾独立研究过量子联邦学习（QFL）和量子差分隐私（QDP）保护数据隐私的方法。然而，据我们所知，先前的研究尚未同时解决QFL和QDP。在本文中，我们提出了将这些隐私保护方法结合起来，并在量子平台上实施，以实现对数据泄露（QFL）和模型反演攻击（QDP）的全面保护。这种实现承诺更高效和安全的人工智能。

    The preservation of privacy is a critical concern in the implementation of artificial intelligence on sensitive training data. There are several techniques to preserve data privacy but quantum computations are inherently more secure due to the no-cloning theorem, resulting in a most desirable computational platform on top of the potential quantum advantages. There have been prior works in protecting data privacy by Quantum Federated Learning (QFL) and Quantum Differential Privacy (QDP) studied independently. However, to the best of our knowledge, no prior work has addressed both QFL and QDP together yet. Here, we propose to combine these privacy-preserving methods and implement them on the quantum platform, so that we can achieve comprehensive protection against data leakage (QFL) and model inversion attacks (QDP). This implementation promises more efficient and secure artificial intelligence. In this paper, we present a successful implementation of these privacy-preservation methods b
    
[^107]: 洪水和回声：图神经网络与分布式计算的算法对齐

    Flood and Echo: Algorithmic Alignment of GNNs with Distributed Computing. (arXiv:2310.06970v1 [cs.LG])

    [http://arxiv.org/abs/2310.06970](http://arxiv.org/abs/2310.06970)

    本论文提出了一个基于分布式算法设计原则的新的执行框架：洪水和回声网络。通过波状激活模式，它可以在整个图中传递消息，并且可以有效地处理更大的图形。

    

    图神经网络是学习算法的自然选择。它们可以通过抽象而多功能的图结构直接表示任务，并处理不同规模的输入。这为算法的扩展和外推到更大的图形提供了可能性，这是一种最重要的优势。然而，这提出了两个核心问题：i）如何使节点能够在给定的图中收集所需的信息（即“信息交换”），即使节点距离很远；ii）我们如何设计一个执行框架，以实现该信息交换以便外推到更大的图大小（即“外推时的算法对齐”）。我们提出了一个新的执行框架，受分布式算法的设计原则启发：洪水和回声网络。它以波状激活模式将消息传播到整个图中，自然地推广到更大的实例。通过它的稀疏但并行激活，可以证明它更加高效。

    Graph Neural Networks are a natural fit for learning algorithms. They can directly represent tasks through an abstract but versatile graph structure and handle inputs of different sizes. This opens up the possibility for scaling and extrapolation to larger graphs, one of the most important advantages of an algorithm. However, this raises two core questions i) How can we enable nodes to gather the required information in a given graph ($\textit{information exchange}$), even if is far away and ii) How can we design an execution framework which enables this information exchange for extrapolation to larger graph sizes ($\textit{algorithmic alignment for extrapolation}$). We propose a new execution framework that is inspired by the design principles of distributed algorithms: Flood and Echo Net. It propagates messages through the entire graph in a wave like activation pattern, which naturally generalizes to larger instances. Through its sparse but parallel activations it is provably more ef
    
[^108]: 无偏性政策学习与观测数据

    Positivity-free Policy Learning with Observational Data. (arXiv:2310.06969v1 [stat.ME])

    [http://arxiv.org/abs/2310.06969](http://arxiv.org/abs/2310.06969)

    本研究提出了一种无偏性的政策学习框架，该框架利用观测数据进行政策学习，并克服了现实情境中无法满足假设的难题。该框架利用增量倾向分数策略调整倾向分数值，从而实现了快速的收敛率。

    

    利用观测数据进行政策学习在各个领域都非常重要，其目标是学习最优的处理分配策略，同时满足特定的约束条件，如公平性、预算和简单性。本研究引入了一种新颖的无偏性（随机）政策学习框架，旨在应对现实情境中无法满足假设的困境。该框架利用增量倾向分数策略来调整倾向分数值，而不是给治疗分配固定值。我们对这些增量倾向分数策略进行了表征，并建立了识别条件，利用半参数效率理论提出了能够实现快速收敛率的高效估计器，即使是与先进的机器学习算法集成在一起。本文对政策学习的理论保证进行了深入探讨，并验证了所提出的框架。

    Policy learning utilizing observational data is pivotal across various domains, with the objective of learning the optimal treatment assignment policy while adhering to specific constraints such as fairness, budget, and simplicity. This study introduces a novel positivity-free (stochastic) policy learning framework designed to address the challenges posed by the impracticality of the positivity assumption in real-world scenarios. This framework leverages incremental propensity score policies to adjust propensity score values instead of assigning fixed values to treatments. We characterize these incremental propensity score policies and establish identification conditions, employing semiparametric efficiency theory to propose efficient estimators capable of achieving rapid convergence rates, even when integrated with advanced machine learning algorithms. This paper provides a thorough exploration of the theoretical guarantees associated with policy learning and validates the proposed fr
    
[^109]: ObjectComposer: 不需要微调的一致生成多个对象的方法

    ObjectComposer: Consistent Generation of Multiple Objects Without Fine-tuning. (arXiv:2310.06968v1 [cs.CV])

    [http://arxiv.org/abs/2310.06968](http://arxiv.org/abs/2310.06968)

    ObjectComposer是一种不需要微调的方法，可以一致地生成多个对象的组合。这解决了现有模型在不同上下文中生成对象外观不一致的问题。

    

    最近的文本到图像生成模型可以从文本提示中生成高保真度的图像。然而，这些模型在不同上下文中一致地生成相同外观的对象方面存在困难。一致的对象生成对于许多下游任务如生成具有一致角色和场景的漫画书插图非常重要。许多方法通过对扩展扩散模型的词汇表进行微调来解决这个问题。然而，即使是轻量级微调方法在大规模和实时运行时也可能代价过高。我们引入了一种称为ObjectComposer的方法，用于生成与用户指定的图像相似的多个对象的组合。我们的方法不需要训练，利用现有模型的能力。我们建立在最近的BLIP-Diffusion模型之上，该模型可以根据参考图像生成单个对象的图像。ObjectComposer可以实现生成包含 。。。

    Recent text-to-image generative models can generate high-fidelity images from text prompts. However, these models struggle to consistently generate the same objects in different contexts with the same appearance. Consistent object generation is important to many downstream tasks like generating comic book illustrations with consistent characters and setting. Numerous approaches attempt to solve this problem by extending the vocabulary of diffusion models through fine-tuning. However, even lightweight fine-tuning approaches can be prohibitively expensive to run at scale and in real-time. We introduce a method called ObjectComposer for generating compositions of multiple objects that resemble user-specified images. Our approach is training-free, leveraging the abilities of preexisting models. We build upon the recent BLIP-Diffusion model, which can generate images of single objects specified by reference images. ObjectComposer enables the consistent generation of compositions containing 
    
[^110]: 关于基于部分原型的分类器的可解释性：人类中心分析

    On the Interpretability of Part-Prototype Based Classifiers: A Human Centric Analysis. (arXiv:2310.06966v1 [cs.CV])

    [http://arxiv.org/abs/2310.06966](http://arxiv.org/abs/2310.06966)

    本论文提出了一个用于从人类角度评估基于部分原型模型可解释性的框架，并使用亚马逊机械土耳其进行了广泛实验。这些实验不仅展示了框架在评估各种基于部分原型模型的可解释性方面的能力，还是最全面的工作。

    

    最近，基于部分原型的网络已成为许多黑盒图像分类器的可解释性替代方法。然而，从人类用户的角度来看，这些方法的可解释性还未得到充分探索。在这项工作中，我们设计了一个从人类角度评估基于部分原型模型可解释性的框架。该框架由三个可操作的度量和实验组成。为了展示我们的框架的实用性，我们使用了亚马逊机械土耳其进行了一系列广泛的实验。它们不仅展示了我们的框架在评估各种基于部分原型模型的可解释性方面的能力，而且是我们所知的在一个统一的框架中评估这些方法最全面的工作。

    Part-prototype networks have recently become methods of interest as an interpretable alternative to many of the current black-box image classifiers. However, the interpretability of these methods from the perspective of human users has not been sufficiently explored. In this work, we have devised a framework for evaluating the interpretability of part-prototype-based models from a human perspective. The proposed framework consists of three actionable metrics and experiments. To demonstrate the usefulness of our framework, we performed an extensive set of experiments using Amazon Mechanical Turk. They not only show the capability of our framework in assessing the interpretability of various part-prototype-based models, but they also are, to the best of our knowledge, the most comprehensive work on evaluating such methods in a unified framework.
    
[^111]: 比较现代无参考图像和视频质量评估度量方法对对抗攻击的鲁棒性

    Comparing the robustness of modern no-reference image- and video-quality metrics to adversarial attacks. (arXiv:2310.06958v1 [cs.CV])

    [http://arxiv.org/abs/2310.06958](http://arxiv.org/abs/2310.06958)

    本文比较了现代图像和视频质量评估度量方法对抗攻击的鲁棒性，并发现部分度量方法对对抗攻击表现出较高的抵抗力，为基准测试提供了更安全的选择。

    

    如今，基于神经网络的图像和视频质量评估度量方法相比传统方法表现更好。然而，它们也变得更容易受到对抗性攻击，这些攻击可以提高度量分数但不改善视觉质量。现有的质量度量基准将其性能与主观质量相关性和计算时间进行比较。然而，图像质量度量的对抗鲁棒性也是一个值得研究的领域。本文分析了现代度量方法对不同对抗攻击的鲁棒性。我们采用了计算机视觉任务中的对抗攻击，并比较了这些攻击对15个无参考图像/视频质量度量方法的效果。一些度量方法对对抗攻击表现出了较高的抵抗力，使它们在基准测试中的使用比容易受攻击的方法更安全。该基准测试接受研究人员提交新的度量方法，以使他们的方法对攻击更加鲁棒，或者为他们寻找符合需求的鲁棒度量方法。

    Nowadays neural-network-based image- and video-quality metrics show better performance compared to traditional methods. However, they also became more vulnerable to adversarial attacks that increase metrics' scores without improving visual quality. The existing benchmarks of quality metrics compare their performance in terms of correlation with subjective quality and calculation time. However, the adversarial robustness of image-quality metrics is also an area worth researching. In this paper, we analyse modern metrics' robustness to different adversarial attacks. We adopted adversarial attacks from computer vision tasks and compared attacks' efficiency against 15 no-reference image/video-quality metrics. Some metrics showed high resistance to adversarial attacks which makes their usage in benchmarks safer than vulnerable metrics. The benchmark accepts new metrics submissions for researchers who want to make their metrics more robust to attacks or to find such metrics for their needs. 
    
[^112]: 低剂量CT的扩散先验正则化迭代重建

    Diffusion Prior Regularized Iterative Reconstruction for Low-dose CT. (arXiv:2310.06949v1 [eess.IV])

    [http://arxiv.org/abs/2310.06949](http://arxiv.org/abs/2310.06949)

    本文介绍了一种扩散先验正则化的迭代重建算法，通过融合去噪扩散概率模型和数据保真度重建过程，以及采用Nesterov动量加速技术，实现了在低剂量CT下出色的图像重建效果。

    

    计算机断层扫描（CT）涉及患者接受电离辐射。为了减少辐射剂量，我们可以降低X射线光子计数或者下采样投影视图。然而，这两种方式经常会损害图像质量。为了解决这个挑战，我们在这里介绍了一种使用扩散先验正则化的迭代重建算法。借鉴去噪扩散概率模型（DDPM）的出色成像能力，我们将其与优先考虑数据保真度的重建过程相结合。这种融合利用了两种技术的优点，在无监督框架中实现了出色的重建结果。为了进一步提高重建过程的效率，我们还采用了Nesterov动量加速技术。这种增强有助于在更少的步骤中实现优秀的扩散采样。正如我们的实验所示，我们的方法为高清CT图像重建提供了潜在的途径，同时最大程度地减少了辐射剂量。

    Computed tomography (CT) involves a patient's exposure to ionizing radiation. To reduce the radiation dose, we can either lower the X-ray photon count or down-sample projection views. However, either of the ways often compromises image quality. To address this challenge, here we introduce an iterative reconstruction algorithm regularized by a diffusion prior. Drawing on the exceptional imaging prowess of the denoising diffusion probabilistic model (DDPM), we merge it with a reconstruction procedure that prioritizes data fidelity. This fusion capitalizes on the merits of both techniques, delivering exceptional reconstruction results in an unsupervised framework. To further enhance the efficiency of the reconstruction process, we incorporate the Nesterov momentum acceleration technique. This enhancement facilitates superior diffusion sampling in fewer steps. As demonstrated in our experiments, our method offers a potential pathway to high-definition CT image reconstruction with minimized
    
[^113]: 一种变分自编码器框架用于鲁棒的、基于物理的工业网络物理系统中的网络攻击识别

    A Variational Autoencoder Framework for Robust, Physics-Informed Cyberattack Recognition in Industrial Cyber-Physical Systems. (arXiv:2310.06948v1 [cs.LG])

    [http://arxiv.org/abs/2310.06948](http://arxiv.org/abs/2310.06948)

    本研究提出了一种基于变分自编码器的混合设计数据驱动框架，通过考虑时间行为和提取传感器测量的时间序列特征，可用于鲁棒、基于物理的工业网络物理系统中的网络攻击的识别、定位和诊断。

    

    随着数据通信越来越多地利用无线网络，工业网络物理系统的网络安全引起了巨大关注。许多数据驱动方法被开发用于检测网络攻击，但很少有方法专注于区分网络攻击和设备故障。本文开发了一个数据驱动框架，可用于检测、诊断和定位一种称为隐蔽攻击的网络工业控制系统的网络攻击。该框架采用了变分自编码器(VAE)、循环神经网络(RNN)和深度神经网络(DNN)的混合设计。该数据驱动框架考虑了一个通用物理系统的时间行为，从传感器测量的时间序列中提取特征用于检测隐蔽攻击、区分它们与设备故障以及定位攻击/故障。通过对一个仿真实验中的实际模拟研究来评估所提出方法的性能。

    Cybersecurity of Industrial Cyber-Physical Systems is drawing significant concerns as data communication increasingly leverages wireless networks. A lot of data-driven methods were develope for detecting cyberattacks, but few are focused on distinguishing them from equipment faults. In this paper, we develop a data-driven framework that can be used to detect, diagnose, and localize a type of cyberattack called covert attacks on networked industrial control systems. The framework has a hybrid design that combines a variational autoencoder (VAE), a recurrent neural network (RNN), and a Deep Neural Network (DNN). This data-driven framework considers the temporal behavior of a generic physical system that extracts features from the time series of the sensor measurements that can be used for detecting covert attacks, distinguishing them from equipment faults, as well as localize the attack/fault. We evaluate the performance of the proposed method through a realistic simulation study on a ne
    
[^114]: LLMs杀死了脚本小子：大型语言模型支持下的特工如何改变网络威胁测试的格局。

    LLMs Killed the Script Kiddie: How Agents Supported by Large Language Models Change the Landscape of Network Threat Testing. (arXiv:2310.06936v1 [cs.CR])

    [http://arxiv.org/abs/2310.06936](http://arxiv.org/abs/2310.06936)

    本文研究了大型语言模型在网络威胁测试中的潜力，通过自动化的决策过程支持特工行动，并讨论了使用大型语言模型加速威胁行为能力的伦理问题。

    

    本文研究了大型语言模型（LLMs）在推理威胁、生成有关工具信息和自动化网络攻击中的潜力。我们首先手动探索了LLMs在支持特定威胁相关操作和决策方面的能力。然后，我们将决策过程自动化应用于网络攻击行动中。我们提出了针对威胁行动的计划-行动-报告循环的提示工程方法，以及指导多项行动的连锁提示设计。我们评估了LLM在我们展示的短期攻击中的网络特定知识，并就引发可执行响应的提示设计提供见解。我们讨论了LLMs对威胁格局的潜在影响以及使用LLMs加速威胁行为能力的伦理考虑。我们报告了生成式人工智能在网络威胁中的一种有希望但令人担忧的应用。然而，LLM解决更复杂和持续性威胁的能力还有待探讨。

    In this paper, we explore the potential of Large Language Models (LLMs) to reason about threats, generate information about tools, and automate cyber campaigns. We begin with a manual exploration of LLMs in supporting specific threat-related actions and decisions. We proceed by automating the decision process in a cyber campaign. We present prompt engineering approaches for a plan-act-report loop for one action of a threat campaign and and a prompt chaining design that directs the sequential decision process of a multi-action campaign. We assess the extent of LLM's cyber-specific knowledge w.r.t the short campaign we demonstrate and provide insights into prompt design for eliciting actionable responses. We discuss the potential impact of LLMs on the threat landscape and the ethical considerations of using LLMs for accelerating threat actor capabilities. We report a promising, yet concerning, application of generative AI to cyber threats. However, the LLM's capabilities to deal with mor
    
[^115]: 量子阴影梯度下降算法用于量子学习

    Quantum Shadow Gradient Descent for Quantum Learning. (arXiv:2310.06935v1 [quant-ph])

    [http://arxiv.org/abs/2310.06935](http://arxiv.org/abs/2310.06935)

    本文提出了一种称为量子阴影梯度下降的新方法，解决了量子学习中的关键挑战，并通过测量量子阴影样本和模拟态的梯度来提高计算效率。

    

    本文提出了一种称为量子阴影梯度下降（QSGD）的新方法，解决了关键挑战。我们的方法具有一次性操作的优点，不需要任何样本复制，同时具有与使用精确梯度计算的理想更新规则相当的收敛速度。我们提出了一种生成量子阴影样本（QSS）的新技术，生成量子阴影而不是现有工作中使用的经典阴影。通过测量量子阴影，我们解决了在经典计算机上执行计算时因维度呈指数增长而导致的限制问题。作为第二个主要贡献，我们研究了更一般的非乘积形式的模拟变分哈密顿量，形式为$\exp\{i\sum_j \theta_j A_j\}$。我们证明了梯度可以用易于测量的单参数模拟态的梯度来表示。

    This paper proposes a new procedure called quantum shadow gradient descent (QSGD) that addresses these key challenges. Our method has the benefits of a one-shot approach, in not requiring any sample duplication while having a convergence rate comparable to the ideal update rule using exact gradient computation. We propose a new technique for generating quantum shadow samples (QSS), which generates quantum shadows as opposed to classical shadows used in existing works. With classical shadows, the computations are typically performed on classical computers and, hence, are prohibitive since the dimension grows exponentially. Our approach resolves this issue by measurements of quantum shadows. As the second main contribution, we study more general non-product ansatz of the form $\exp\{i\sum_j \theta_j A_j\}$ that model variational Hamiltonians. We prove that the gradient can be written in terms of the gradient of single-parameter ansatzes that can be easily measured. Our proof is based on 
    
[^116]: 《有声书的韵律分析》

    Prosody Analysis of Audiobooks. (arXiv:2310.06930v1 [cs.SD])

    [http://arxiv.org/abs/2310.06930](http://arxiv.org/abs/2310.06930)

    本研究通过使用一个含有93个书籍和对应有声书的数据集，提出了改进的模型来预测有声书文本中的韵律属性。结果显示，我们的预测韵律与人类朗读比商业级TTS系统更相关，并且人们更喜欢韵律增强的有声书朗读。

    

    最近在文本转语音方面取得了一些进展，使得从文本中生成自然音效的音频成为可能。然而，有声书朗读涉及到读者的戏剧性声音和语调，更多地依赖情感、对话和叙述。使用我们的数据集，包括93本书与其对应的有声书，我们提出了改进的模型，用于从叙述文本中预测韵律属性（音高、音量和语速），并使用语言建模。我们预测的韵律属性与人类朗读的相关性要远高于商业级TTS系统的结果：在24本书中，我们预测的音高对22本书的人类阅读更具相关性，而我们预测的音量属性对23本书的人类阅读更加相似。最后，我们进行了一项人类评估研究，以量化人们更喜欢韵律增强的有声书朗读还是商业级文本转语音系统。

    Recent advances in text-to-speech have made it possible to generate natural-sounding audio from text. However, audiobook narrations involve dramatic vocalizations and intonations by the reader, with greater reliance on emotions, dialogues, and descriptions in the narrative. Using our dataset of 93 aligned book-audiobook pairs, we present improved models for prosody prediction properties (pitch, volume, and rate of speech) from narrative text using language modeling. Our predicted prosody attributes correlate much better with human audiobook readings than results from a state-of-the-art commercial TTS system: our predicted pitch shows a higher correlation with human reading for 22 out of the 24 books, while our predicted volume attribute proves more similar to human reading for 23 out of the 24 books. Finally, we present a human evaluation study to quantify the extent that people prefer prosody-enhanced audiobook readings over commercial text-to-speech systems.
    
[^117]: 用去噪扩散模型的随机超分辨率方法在宇宙学模拟中实现的摘要

    Stochastic Super-resolution of Cosmological Simulations with Denoising Diffusion Models. (arXiv:2310.06929v1 [astro-ph.CO])

    [http://arxiv.org/abs/2310.06929](http://arxiv.org/abs/2310.06929)

    本文提出了使用去噪扩散模型进行宇宙学超分辨率的方法，通过重新分配不同尺度的重要性来实现准确的结果，该方法能够产生令人信服的超分辨率图像，并且与小尺度特征多样性一致。

    

    最近几年，深度学习模型已成功应用于增强低分辨率的宇宙学模拟的小尺度信息，这被称为“超分辨率”任务。迄今为止，这些宇宙学超分辨率模型依赖于生成对抗网络（GANs），这些网络可以实现高度逼真的结果，但存在各种缺点（例如低样本多样性）。我们使用去噪扩散模型作为一种强大的生成模型，来实现超分辨率的宇宙大尺度结构预测（作为二维的首个概念证明）。为了获得准确的小尺度结果，我们开发了一种新的“滤波增强”训练方法，重新分配了像素级训练目标中不同尺度的重要性。我们证明了我们的模型不仅能够产生令人信服的超分辨率图像和百分之一水平一致的功率谱，并且能够重现与小尺度特征多样性一致的结果。

    In recent years, deep learning models have been successfully employed for augmenting low-resolution cosmological simulations with small-scale information, a task known as "super-resolution". So far, these cosmological super-resolution models have relied on generative adversarial networks (GANs), which can achieve highly realistic results, but suffer from various shortcomings (e.g. low sample diversity). We introduce denoising diffusion models as a powerful generative model for super-resolving cosmic large-scale structure predictions (as a first proof-of-concept in two dimensions). To obtain accurate results down to small scales, we develop a new "filter-boosted" training approach that redistributes the importance of different scales in the pixel-wise training objective. We demonstrate that our model not only produces convincing super-resolution images and power spectra consistent at the percent level, but is also able to reproduce the diversity of small-scale features consistent with a
    
[^118]: PICProp：用于不确定性量化的物理信息置信传播

    PICProp: Physics-Informed Confidence Propagation for Uncertainty Quantification. (arXiv:2310.06923v1 [cs.AI])

    [http://arxiv.org/abs/2310.06923](http://arxiv.org/abs/2310.06923)

    本文提出了一种名为PICProp的方法，基于双层优化，用于在深度学习和物理信息学习中进行不确定性量化。该方法能够在不进行强大假设的情况下计算有效的置信区间（CI），并且通过传播置信度实现了数据位置到整个域的置信度传播。

    

    在深度学习和物理信息学习中，标准的不确定性量化方法存在着持久的局限性。特别是，需要对数据的可能性做出强烈的假设，性能在很大程度上取决于先验的选择，并且后验只能以近似的方式进行采样，从而由于相关的计算成本导致近似精度较差。本文提出并研究了对确定性偏微分方程的置信区间（CI）估计作为一个新的问题。即，在整个区域中以概率担保的形式传播置信度，以达到数据位置到整个域的置信度传播。我们提出了一种名为物理信息置信传播（PICProp）的方法，基于双层优化来计算一个有效的CI，而不需要进行大量的假设。我们提供了关于我们方法有效性的定理以及针对物理信息学习的计算实验。

    Standard approaches for uncertainty quantification in deep learning and physics-informed learning have persistent limitations. Indicatively, strong assumptions regarding the data likelihood are required, the performance highly depends on the selection of priors, and the posterior can be sampled only approximately, which leads to poor approximations because of the associated computational cost. This paper introduces and studies confidence interval (CI) estimation for deterministic partial differential equations as a novel problem. That is, to propagate confidence, in the form of CIs, from data locations to the entire domain with probabilistic guarantees. We propose a method, termed Physics-Informed Confidence Propagation (PICProp), based on bi-level optimization to compute a valid CI without making heavy assumptions. We provide a theorem regarding the validity of our method, and computational experiments, where the focus is on physics-informed learning.
    
[^119]: 用聚焦-信息熵改进对比学习句子嵌入

    Improving Contrastive Learning of Sentence Embeddings with Focal-InfoNCE. (arXiv:2310.06918v1 [cs.CL])

    [http://arxiv.org/abs/2310.06918](http://arxiv.org/abs/2310.06918)

    本研究提出了一种聚焦-信息熵函数，利用硬负样本挖掘改进了对比学习句子嵌入的方法，并在各种基准测试中验证了其性能提升。

    

    SimCSE的最新成功极大地推进了句子表示的最新技术。然而，SimCSE的原始表达没有充分利用对比学习中硬负样本的潜力。本研究引入了一种无监督对比学习框架，将SimCSE与硬负样本挖掘相结合，旨在提高句子嵌入的质量。所提出的聚焦-信息熵函数在对比目标中引入了自适应调节项，降低与易负样本相关的损失，并鼓励模型关注于困难负样本。在各种STS基准测试上的实验结果表明，我们的方法在斯皮尔曼相关系数、表示对齐性和一致性方面改进了句子嵌入。

    The recent success of SimCSE has greatly advanced state-of-the-art sentence representations. However, the original formulation of SimCSE does not fully exploit the potential of hard negative samples in contrastive learning. This study introduces an unsupervised contrastive learning framework that combines SimCSE with hard negative mining, aiming to enhance the quality of sentence embeddings. The proposed focal-InfoNCE function introduces self-paced modulation terms in the contrastive objective, downweighting the loss associated with easy negatives and encouraging the model focusing on hard negatives. Experimentation on various STS benchmarks shows that our method improves sentence embeddings in terms of Spearman's correlation and representation alignment and uniformity.
    
[^120]: 利用第四代英特尔韧性处理器进行分布式迁移学习

    Distributed Transfer Learning with 4th Gen Intel Xeon Processors. (arXiv:2310.06916v1 [cs.DC])

    [http://arxiv.org/abs/2310.06916](http://arxiv.org/abs/2310.06916)

    本文研究了如何利用第四代英特尔韧性处理器进行分布式迁移学习，通过使用英特尔高级矩阵扩展（AMX）和Horovod在Image Classification TensorFlow数据集上实现了接近最先进的图像分类准确性。

    

    本文探讨了如何利用迁移学习和英特尔韧性处理器，特别是第四代英特尔韧性可扩展处理器，打破了传统观念，即训练主要依赖于GPU。我们通过使用英特尔高级矩阵扩展（AMX）和Horovod进行分布式训练，在公开可用的Image Classification TensorFlow数据集上实现了接近最先进的图像分类准确性的案例研究。

    In this paper, we explore how transfer learning, coupled with Intel Xeon, specifically 4th Gen Intel Xeon scalable processor, defies the conventional belief that training is primarily GPU-dependent. We present a case study where we achieved near state-of-the-art accuracy for image classification on a publicly available Image Classification TensorFlow dataset using Intel Advanced Matrix Extensions(AMX) and distributed training with Horovod.
    
[^121]: 关于稀疏回归、Lp正则化和自动模型发现的论文

    On sparse regression, Lp-regularization, and automated model discovery. (arXiv:2310.06872v1 [cs.LG])

    [http://arxiv.org/abs/2310.06872](http://arxiv.org/abs/2310.06872)

    该论文通过结合正则化和物理约束的方法，利用神经网络进行自动模型发现，探索了非线性回归中稀疏回归和特征提取的潜力，并针对材料建模提出了常见的指导方针和趋势。

    

    稀疏回归和特征提取是从大规模数据中进行知识发现的基础。它们的目标是发现能够提供科学变量之间简单关系的可解释和预测模型。虽然在线性回归的模型发现方面的统计工具已经得到很好地建立，但在材料建模中将其推广到非线性回归是高度问题特定且理解不够充分的。在这里，我们探索了神经网络在自动模型发现中的潜力，并通过结合两种策略的混合方法来引入稀疏性：正则化和物理约束。我们将Lp正则化的概念与基于本领域在运动学和热力学方面的知识的构成神经网络相结合。我们使用合成数据和真实数据对网络进行训练，并进行了几千次发现运行来推断出共同的指导方针和趋势：L2正则化或岭回归是最佳的选择。

    Sparse regression and feature extraction are the cornerstones of knowledge discovery from massive data. Their goal is to discover interpretable and predictive models that provide simple relationships among scientific variables. While the statistical tools for model discovery are well established in the context of linear regression, their generalization to nonlinear regression in material modeling is highly problem-specific and insufficiently understood. Here we explore the potential of neural networks for automatic model discovery and induce sparsity by a hybrid approach that combines two strategies: regularization and physical constraints. We integrate the concept of Lp regularization for subset selection with constitutive neural networks that leverage our domain knowledge in kinematics and thermodynamics. We train our networks with both, synthetic and real data, and perform several thousand discovery runs to infer common guidelines and trends: L2 regularization or ridge regression is
    
[^122]: 基于深度学习的无线网络实时流媒体速率控制

    Deep Learning-Based Real-Time Rate Control for Live Streaming on Wireless Networks. (arXiv:2310.06857v1 [cs.NI])

    [http://arxiv.org/abs/2310.06857](http://arxiv.org/abs/2310.06857)

    提出了一种基于深度学习的实时H.264控制器，利用即时通道质量数据和视频块动态估计最优编码器参数，以实时延迟可忽略的方式维持编码视频比特率略低于可用通道比特率。

    

    提供高质量视频内容给无线用户变得越来越重要。然而，由于动态视频内容和无线衰落效应引起的波动通道速率，确保一致的视频质量面临挑战。次优的编码器参数选择可能导致视频质量损失，因为带宽利用不足或由于包丢失引入视频伪影。为了解决这个问题，提出了一种基于深度学习的实时H.264控制器。该控制器利用从物理层获得的即时通道质量数据和视频块动态估计最优编码器参数，实时延迟可忽略。其目标是维持编码视频比特率略低于可用通道比特率。在QCIF数据集和来自公共数据集的随机视频的多样选择上进行的实验结果验证了该方法的有效性。

    Providing wireless users with high-quality video content has become increasingly important. However, ensuring consistent video quality poses challenges due to variable encoded bitrate caused by dynamic video content and fluctuating channel bitrate caused by wireless fading effects. Suboptimal selection of encoder parameters can lead to video quality loss due to underutilized bandwidth or the introduction of video artifacts due to packet loss. To address this, a real-time deep learning based H.264 controller is proposed. This controller leverages instantaneous channel quality data driven from the physical layer, along with the video chunk, to dynamically estimate the optimal encoder parameters with a negligible delay in real-time. The objective is to maintain an encoded video bitrate slightly below the available channel bitrate. Experimental results, conducted on both QCIF dataset and a diverse selection of random videos from public datasets, validate the effectiveness of the approach. 
    
[^123]: 基于遗传算法的动态后门攻击对于基于联邦学习的网络流量分类的研究

    Genetic Algorithm-Based Dynamic Backdoor Attack on Federated Learning-Based Network Traffic Classification. (arXiv:2310.06855v1 [cs.CR])

    [http://arxiv.org/abs/2310.06855](http://arxiv.org/abs/2310.06855)

    本论文研究了基于联邦学习的网络流量分类中的动态后门攻击，并提出了一种基于遗传算法的新型后门攻击方法GABAttack。该方法能够优化后门触发器参数的值和位置，以实现对全局模型的篡改。

    

    联邦学习使多个客户端能够共同为由中央服务器组织的全局模型的学习做出贡献。这种学习方案促进了客户端的数据隐私并减少了通信开销。在网络流量分类等应用中，这有助于隐藏网络的漏洞和弱点。然而，联邦学习对后门攻击是脆弱的，即对手向全局模型中注入篡改的模型更新。这些更新在全局模型中注入了显著的功能，可以通过特定的输入模式来启动。尽管如此，基于联邦学习的网络流量分类模型对于这些攻击的脆弱性仍未被探索。在本文中，我们提出了GABAttack，一种基于遗传算法的针对基于联邦学习的网络流量分类的新型后门攻击方法。GABAttack利用遗传算法来优化后门触发器参数的值和位置。

    Federated learning enables multiple clients to collaboratively contribute to the learning of a global model orchestrated by a central server. This learning scheme promotes clients' data privacy and requires reduced communication overheads. In an application like network traffic classification, this helps hide the network vulnerabilities and weakness points. However, federated learning is susceptible to backdoor attacks, in which adversaries inject manipulated model updates into the global model. These updates inject a salient functionality in the global model that can be launched with specific input patterns. Nonetheless, the vulnerability of network traffic classification models based on federated learning to these attacks remains unexplored. In this paper, we propose GABAttack, a novel genetic algorithm-based backdoor attack against federated learning for network traffic classification. GABAttack utilizes a genetic algorithm to optimize the values and locations of backdoor trigger pa
    
[^124]: DeepCrysTet: 使用四面体网格进行深度学习预测晶体材料性质的方法

    DeepCrysTet: A Deep Learning Approach Using Tetrahedral Mesh for Predicting Properties of Crystalline Materials. (arXiv:2310.06852v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2310.06852](http://arxiv.org/abs/2310.06852)

    DeepCrysTet是一种新颖的深度学习方法，通过使用3D四面体网格表示晶体结构，预测晶体材料的性质。

    

    机器学习（ML）在预测材料性质以加速材料发现方面越来越受欢迎。由于材料性质受晶体结构的强烈影响，一个关键问题是将晶体结构转化为用于输入ML模型的特征。目前最常见的方法是将晶体结构转化为图形，并使用图神经网络（GNN）预测其性质。一些GNN模型，如晶体图卷积神经网络（CGCNN）和原子图神经网络（ALIGNN），已经取得了高度准确的材料性质预测。尽管取得了这些成功，使用图形来表示晶体结构存在明显的限制，即丢失晶体结构的三维（3D）信息。在这项工作中，我们提出了DeepCrysTet，一种新颖的深度学习方法，用于预测材料性质，它使用由Delaunay三角剖分生成的3D四面体网格表示晶体结构。

    Machine learning (ML) is becoming increasingly popular for predicting material properties to accelerate materials discovery. Because material properties are strongly affected by its crystal structure, a key issue is converting the crystal structure into the features for input to the ML model. Currently, the most common method is to convert the crystal structure into a graph and predicting its properties using a graph neural network (GNN). Some GNN models, such as crystal graph convolutional neural network (CGCNN) and atomistic line graph neural network (ALIGNN), have achieved highly accurate predictions of material properties. Despite these successes, using a graph to represent a crystal structure has the notable limitation of losing the crystal structure's three-dimensional (3D) information. In this work, we propose DeepCrysTet, a novel deep learning approach for predicting material properties, which uses crystal structures represented as a 3D tetrahedral mesh generated by Delaunay te
    
[^125]: DeepTriNet:一种基于三级注意力的DeepLabv3+结构用于卫星图像的语义分割

    DeepTriNet: A Tri-Level Attention Based DeepLabv3+ Architecture for Semantic Segmentation of Satellite Images. (arXiv:2310.06848v1 [cs.CV])

    [http://arxiv.org/abs/2310.06848](http://arxiv.org/abs/2310.06848)

    DeepTriNet是一种基于三级注意力的DeepLabv3+架构，用于卫星图像的语义分割。该方法通过结合SENets和TAUs桥接了编解码器输出与相关特征之间的语义差距，同时通过自我监督确定了更重要和更通用的特征。

    

    卫星图像的分割在遥感应用中至关重要。现有方法在卫星图像的语义分割中面临识别小尺度目标的挑战，主要原因是忽略了底层网络的低级特征和由不同特征图包含不同数量信息。因此，在本研究中，提出了一种基于三级注意力的DeepLabv3+架构（DeepTriNet）用于卫星图像的语义分割。所提出的混合方法结合了挤压激励网络（SENets）和三级注意力单元（TAUs）与普通的DeepLabv3+架构，其中TAUs用于弥合编解码器输出与SENets用于给相关特征分配更多权重之间的语义特征差距。所提出的DeepTriNet通过自我监督找到哪些特征是更相关且更通用的，而不是我们对它们进行注释。研究表明，所提出的DeepTriNet具有较好的性能。

    The segmentation of satellite images is crucial in remote sensing applications. Existing methods face challenges in recognizing small-scale objects in satellite images for semantic segmentation primarily due to ignoring the low-level characteristics of the underlying network and due to containing distinct amounts of information by different feature maps. Thus, in this research, a tri-level attention-based DeepLabv3+ architecture (DeepTriNet) is proposed for the semantic segmentation of satellite images. The proposed hybrid method combines squeeze-and-excitation networks (SENets) and tri-level attention units (TAUs) with the vanilla DeepLabv3+ architecture, where the TAUs are used to bridge the semantic feature gap among encoders output and the SENets used to put more weight on relevant features. The proposed DeepTriNet finds which features are the more relevant and more generalized way by its self-supervision rather we annotate them. The study showed that the proposed DeepTriNet perfor
    
[^126]: 高分辨率卫星图像中基于EfficientNet的U-Net++架构用于自动建筑物提取的性能分析

    Performance Analysis of Various EfficientNet Based U-Net++ Architecture for Automatic Building Extraction from High Resolution Satellite Images. (arXiv:2310.06847v1 [cs.CV])

    [http://arxiv.org/abs/2310.06847](http://arxiv.org/abs/2310.06847)

    本研究提出了基于EfficientNet背骨的U-Net++架构，通过深度监督和重设计的跳跃连接，提高了高分辨率卫星图像中的建筑物提取准确性。实验证明，该模型显著优于先前的方法。

    

    建筑物提取是遥感科学研究中的重要组成部分，建筑物提取的应用严重依赖于高分辨率遥感图像的语义分割。现有的基于深度学习的方法存在语义信息提取差距限制，可能导致分割结果不准确。为了解决这个问题并以高准确率提取建筑物，本研究提出了基于EfficientNet背骨的多种U-Net++结构。该设计的网络基于U-Net，在深度监督、庞大的重设计的跳跃连接等方面提高模型的灵敏度，从而减少背景中与相关特征区域无关的影响。在训练网络时采用了不同的EfficientNet背骨编码器，以增强模型提取更相关特征的能力。根据实验结果，建议的模型显著优于先前的尖端方法。

    Building extraction is an essential component of study in the science of remote sensing, and applications for building extraction heavily rely on semantic segmentation of high-resolution remote sensing imagery. Semantic information extraction gap constraints in the present deep learning based approaches, however can result in inadequate segmentation outcomes. To address this issue and extract buildings with high accuracy, various efficientNet backbone based U-Net++ has been proposed in this study. The designed network, based on U-Net, can improve the sensitivity of the model by deep supervision, voluminous redesigned skip-connections and hence reducing the influence of irrelevant feature areas in the background. Various effecientNet backbone based encoders have been employed when training the network to enhance the capacity of the model to extract more relevant feature. According on the experimental findings, the suggested model significantly outperforms previous cutting-edge approache
    
[^127]: RobustEdge：面向云边系统的低功耗对抗检测

    RobustEdge: Low Power Adversarial Detection for Cloud-Edge Systems. (arXiv:2310.06845v1 [cs.CR])

    [http://arxiv.org/abs/2310.06845](http://arxiv.org/abs/2310.06845)

    RobustEdge是一种低功耗、适用于边缘设备的对抗检测方法，解决了在云边系统中能量浪费和计算开销问题。

    

    在实际的云边场景中，边缘设备进行数据采集，拥有足够资源的云系统使用深度神经网络进行推理任务，对抗鲁棒性对于可靠性和普及部署至关重要。对抗检测是先前文献中使用的主要对抗防御技术。然而，在先前的检测方法中，检测器附加在分类器模型上，检测器和分类器一起进行对抗检测，这需要很高的计算开销，在低功耗边缘设备上不可行。因此，先前的方法只能在云端进行对抗检测，而不能在边缘设备上进行。这意味着在遭受对抗攻击时，不利的对抗样本必须传送到云端，导致边缘设备的能量浪费。因此，需要一种低功耗、适用于边缘设备的对抗检测方法来提高能量效率。

    In practical cloud-edge scenarios, where a resource constrained edge performs data acquisition and a cloud system (having sufficient resources) performs inference tasks with a deep neural network (DNN), adversarial robustness is critical for reliability and ubiquitous deployment. Adversarial detection is a prime adversarial defence technique used in prior literature. However, in prior detection works, the detector is attached to the classifier model and both detector and classifier work in tandem to perform adversarial detection that requires a high computational overhead which is not available at the low-power edge. Therefore, prior works can only perform adversarial detection at the cloud and not at the edge. This means that in case of adversarial attacks, the unfavourable adversarial samples must be communicated to the cloud which leads to energy wastage at the edge device. Therefore, a low-power edge-friendly adversarial detection method is required to improve the energy efficiency
    
[^128]: 使用深度神经网络进行恶意软件分类：性能评估和在边缘设备中的应用

    Malware Classification using Deep Neural Networks: Performance Evaluation and Applications in Edge Devices. (arXiv:2310.06841v1 [cs.CR])

    [http://arxiv.org/abs/2310.06841](http://arxiv.org/abs/2310.06841)

    该论文评估了使用深度神经网络进行恶意软件分类的效果和性能，证明了DNNs在不同类型的恶意软件分类上具有潜力。此外，论文还探讨了在边缘设备上实时分类的可行性，提出了优化模型架构和利用边缘计算能力的方法以实现高效性能。这项研究对于推进恶意软件检测技术和提升网络安全措施具有重要贡献。

    

    随着现代恶意软件的检测越来越困难，当前恶意软件攻击的范围不断扩大，有必要评估深度神经网络（DNNs）用于恶意软件分类的效果和性能。可以设计和训练多个DNN架构以检测和分类恶意软件二进制文件。结果显示，DNNs在准确分类恶意软件方面具有潜力，不同类型的恶意软件观察到高准确率。此外，将这些DNN模型部署在边缘设备上以实现实时分类的可行性，特别是在资源受限的情况下，对于大型物联网系统来说至关重要。通过优化模型架构和利用边缘计算能力，提出的方法在有限资源下实现高效的性能。该研究对于推进恶意软件检测技术具有贡献，并强调了将网络安全措施整合到早期检测中的重要性。

    With the increasing extent of malware attacks in the present day along with the difficulty in detecting modern malware, it is necessary to evaluate the effectiveness and performance of Deep Neural Networks (DNNs) for malware classification. Multiple DNN architectures can be designed and trained to detect and classify malware binaries. Results demonstrate the potential of DNNs in accurately classifying malware with high accuracy rates observed across different malware types. Additionally, the feasibility of deploying these DNN models on edge devices to enable real-time classification, particularly in resource-constrained scenarios proves to be integral to large IoT systems. By optimizing model architectures and leveraging edge computing capabilities, the proposed methodologies achieve efficient performance even with limited resources. This study contributes to advancing malware detection techniques and emphasizes the significance of integrating cybersecurity measures for the early detec
    
[^129]: 超维计算作为高效保护隐私的机器学习即服务的救援

    Hyperdimensional Computing as a Rescue for Efficient Privacy-Preserving Machine Learning-as-a-Service. (arXiv:2310.06840v1 [cs.CR])

    [http://arxiv.org/abs/2310.06840](http://arxiv.org/abs/2310.06840)

    本文展示了超维计算作为机器学习即服务保护隐私的方法，通过同态加密技术实现计算过程中的数据保密性，为高效的隐私保护提供了解决方案。

    

    机器学习模型通常作为云服务提供，客户将自己的数据发送给服务提供商以获取结果。这种设置是常见的，由于模型的高价值，但它要求客户放弃查询数据可能包含的隐私。同态加密（HE）是解决这个困境的一种有希望的技术。使用HE，服务提供商可以接收加密的数据作为查询，并运行模型而无需解密。结果保持加密，只有客户可以解密。所有这些好处带来的代价是计算成本，因为HE将简单的浮点运算转化为长（度超过1024）多项式之间的计算。之前的工作已经提出了为在加密数据上进行高效计算而量身定制的深度神经网络，但HE又再次放大了已经很高的计算成本，阻碍了性能改进。在这篇论文中，我们展示了超维计算可以作为保护隐私的机器学习即服务的救援。

    Machine learning models are often provisioned as a cloud-based service where the clients send their data to the service provider to obtain the result. This setting is commonplace due to the high value of the models, but it requires the clients to forfeit the privacy that the query data may contain. Homomorphic encryption (HE) is a promising technique to address this adversity. With HE, the service provider can take encrypted data as a query and run the model without decrypting it. The result remains encrypted, and only the client can decrypt it. All these benefits come at the cost of computational cost because HE turns simple floating-point arithmetic into the computation between long (degree over 1024) polynomials. Previous work has proposed to tailor deep neural networks for efficient computation over encrypted data, but already high computational cost is again amplified by HE, hindering performance improvement. In this paper we show hyperdimensional computing can be a rescue for pri
    
[^130]: 面向制造业的AI孵化的上下文强化学习集成活跃学习方法

    Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing. (arXiv:2310.06306v1 [cs.LG])

    [http://arxiv.org/abs/2310.06306](http://arxiv.org/abs/2310.06306)

    本论文提出了一种面向制造业的AI孵化的上下文强化学习集成活跃学习方法，通过在线更新AI模型的方式来持续改进决策。研究采用了一种名为CBEAL的集成主动学习方法，通过优化地指导数据获取，实现了数据效果的最小化。

    

    工业物联网系统中的在线感知和计算资源促进了基于AI的决策。然而，数据质量问题，如类别不平衡，阻碍了离线训练的AI模型。为了解决这个问题，AI模型会通过流式数据进行在线更新以持续改进。然而，由于注释约束，监督学习模型在选择用于更新的优质流式样本方面面临挑战。文献中的主动学习方法通过关注不足或过度表示的区域来提供解决方案。在不断变化的制造背景下平衡这些策略是具有挑战性的。AI学习到的一些获取准则可以动态适应，但可能无法始终处理频繁的变化。我们引入了一种集成主动学习方法CBEAL，专门利用主动学习代理进行探索或利用。代理的权重根据决策有效性进行调整。CBEAL可以优化地指导数据获取，实现数据效果的最小化。

    Online sensing and computational resources in Industrial Cyber-physical Systems (ICPS) facilitate AI-driven decision-making. Yet, issues with data quality, such as imbalanced classes, hinder AI models trained offline. To address this, AI models are updated online with streaming data for continuous improvement. Supervised learning models, however, face challenges in selecting quality streaming samples for updates due to annotation constraints. Active learning methods in literature offer solutions by focusing on under-represented or well-represented regions. Balancing these strategies in changing manufacturing contexts is challenging. Some acquisition criteria learned by AI dynamically adapt but may not consistently handle frequent changes. We introduce an ensemble active learning method, CBEAL, employing active learning agents specifically for exploration or exploitation. Weights of agents are adjusted based on agent decision effectiveness. CBEAL optimally guides data acquisition, minim
    
[^131]: MuseChat:一种视频对话音乐推荐系统

    MuseChat: A Conversational Music Recommendation System for Videos. (arXiv:2310.06282v1 [cs.LG])

    [http://arxiv.org/abs/2310.06282](http://arxiv.org/abs/2310.06282)

    MuseChat是一种创新的对话式音乐推荐系统，通过模拟用户和推荐系统之间的对话交互，利用预训练的音乐标签和艺术家信息，为用户提供定制的音乐推荐，使用户可以个性化选择他们喜欢的音乐。

    

    我们引入了MuseChat，一种创新的基于对话的音乐推荐系统。这个独特的平台不仅提供互动用户参与，还为输入的视频提供了定制的音乐推荐，使用户可以改进和个性化他们的音乐选择。与之相反，以前的系统主要强调内容的兼容性，往往忽视了用户个体偏好的细微差别。例如，所有的数据集都只提供基本的音乐-视频配对，或者带有音乐描述的配对。为了填补这一空白，我们的研究提供了三个贡献。首先，我们设计了一种对话合成方法，模拟了用户和推荐系统之间的两轮交互，利用预训练的音乐标签和艺术家信息。在这个交互中，用户提交一个视频给系统，系统会提供一个合适的音乐片段，并附带解释。之后，用户会表达他们对音乐的偏好，系统会呈现一个改进后的音乐推荐

    We introduce MuseChat, an innovative dialog-based music recommendation system. This unique platform not only offers interactive user engagement but also suggests music tailored for input videos, so that users can refine and personalize their music selections. In contrast, previous systems predominantly emphasized content compatibility, often overlooking the nuances of users' individual preferences. For example, all the datasets only provide basic music-video pairings or such pairings with textual music descriptions. To address this gap, our research offers three contributions. First, we devise a conversation-synthesis method that simulates a two-turn interaction between a user and a recommendation system, which leverages pre-trained music tags and artist information. In this interaction, users submit a video to the system, which then suggests a suitable music piece with a rationale. Afterwards, users communicate their musical preferences, and the system presents a refined music recomme
    
[^132]: 自动神经元解释的及时调优的重要性

    The Importance of Prompt Tuning for Automated Neuron Explanations. (arXiv:2310.06200v1 [cs.CL])

    [http://arxiv.org/abs/2310.06200](http://arxiv.org/abs/2310.06200)

    本文探讨了在自动神经元解释中即时调优的重要性，通过重新格式化解释提示，我们显著提高了解释质量并降低了计算成本。这项研究对于更深入理解大型语言模型的工作原理和安全性具有重要意义。

    

    最近的进展极大地提升了大型语言模型(LLM)的能力，但我们对这些模型及其安全性的理解并没有同步进展。在本文中，我们旨在通过研究它们的个体神经元来更深入地理解LLM。我们在前人研究的基础上，进一步探讨了大型语言模型，如GPT-4，如何解释语言模型中每个神经元的功能。具体地，我们分析了生成解释所使用的提示的效果，并展示了以更自然的方式重新格式化解释提示如何显著提高神经元解释的质量，并大幅降低计算成本。我们通过三种不同的方式演示了我们新提示的效果，包括自动化评估和人工评估。

    Recent advances have greatly increased the capabilities of large language models (LLMs), but our understanding of the models and their safety has not progressed as fast. In this paper we aim to understand LLMs deeper by studying their individual neurons. We build upon previous work showing large language models such as GPT-4 can be useful in explaining what each neuron in a language model does. Specifically, we analyze the effect of the prompt used to generate explanations and show that reformatting the explanation prompt in a more natural way can significantly improve neuron explanation quality and greatly reduce computational cost. We demonstrate the effects of our new prompts in three different ways, incorporating both automated and human evaluations.
    
[^133]: 超级关注力：近似线性时间下的长上下文注意力机制

    HyperAttention: Long-context Attention in Near-Linear Time. (arXiv:2310.05869v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.05869](http://arxiv.org/abs/2310.05869)

    近似注意力机制HyperAttention解决了在大型语言模型中使用的长上下文的计算挑战，并通过引入两个参数来衡量问题的难度。HyperAttention具有模块化设计，可轻松集成其他快速低级实现。

    

    我们提出了一种名为HyperAttention的近似注意力机制，以应对在大型语言模型（LLMs）中使用的长上下文的日益复杂的计算挑战。最近的研究表明，在最坏情况下，除非注意力矩阵的条目被限制或矩阵具有低稳定秩，否则二次时间是必要的。我们引入了两个参数，用于衡量：（1）标准化注意力矩阵中的最大列范数，以及（2）在检测和删除大条目后，非标准化注意力矩阵中行范数的比率。我们使用这些细粒度的参数来捕捉问题的难度。尽管先前存在下界，但我们能够实现一个线性时间的采样算法，即使矩阵具有无界的条目或较大的稳定秩，只要上述参数较小。HyperAttention具有模块化设计，轻松容纳其他快速低级实现，特别是FlashAttention。

    We present an approximate attention mechanism named HyperAttention to address the computational challenges posed by the growing complexity of long contexts used in Large Language Models (LLMs). Recent work suggests that in the worst-case scenario, quadratic time is necessary unless the entries of the attention matrix are bounded or the matrix has low stable rank. We introduce two parameters which measure: (1) the max column norm in the normalized attention matrix, and (2) the ratio of row norms in the unnormalized attention matrix after detecting and removing large entries. We use these fine-grained parameters to capture the hardness of the problem. Despite previous lower bounds, we are able to achieve a linear time sampling algorithm even when the matrix has unbounded entries or a large stable rank, provided the above parameters are small. HyperAttention features a modular design that easily accommodates integration of other fast low-level implementations, particularly FlashAttention.
    
[^134]: 大型语言模型是事后解释器吗？

    Are Large Language Models Post Hoc Explainers?. (arXiv:2310.05797v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05797](http://arxiv.org/abs/2310.05797)

    这项工作提出了第一个研究大型语言模型（LLMs）解释其他预测模型有效性的框架，并且提出了多个提示策略，填补了当前对于LLMs在解释其他模型行为方面的缺失。

    

    大型语言模型（LLM）越来越被广泛应用于各种自然语言处理（NLP）应用中。最近的一项创新，即上下文学习（ICL），使得LLM能够在推理阶段通过在提示中提供少量示例来学习新任务，从而消除了模型微调的需要。虽然LLM已经被应用于多个领域，但其在解释其他模型行为方面的适用性仍相对未被探索。尽管存在越来越多的新解释技术，但很多技术要求对模型具有白盒访问权限和/或计算成本较高，凸显了下一代事后解释器的需求。在这项工作中，我们提出了第一个研究LLM解释其他预测模型有效性的框架。具体而言，我们提出了一个包含多种提示策略的新颖框架：i）基于扰动的ICL，ii）基于预测的ICL，iii）基于指令的ICL，和iv）基于解释的ICL。

    Large Language Models (LLMs) are increasingly used as powerful tools for a plethora of natural language processing (NLP) applications. A recent innovation, in-context learning (ICL), enables LLMs to learn new tasks by supplying a few examples in the prompt during inference time, thereby eliminating the need for model fine-tuning. While LLMs have been utilized in several applications, their applicability in explaining the behavior of other models remains relatively unexplored. Despite the growing number of new explanation techniques, many require white-box access to the model and/or are computationally expensive, highlighting a need for next-generation post hoc explainers. In this work, we present the first framework to study the effectiveness of LLMs in explaining other predictive models. More specifically, we propose a novel framework encompassing multiple prompting strategies: i) Perturbation-based ICL, ii) Prediction-based ICL, iii) Instruction-based ICL, and iv) Explanation-based I
    
[^135]: 使用Google Earth Engine (GEE)分析选定的两个热带国家——斯里兰卡和越南的降雨变异性和水域范围

    Analysis of Rainfall Variability and Water Extent of Selected Hydropower Reservoir Using Google Earth Engine (GEE): A Case Study from Two Tropical Countries, Sri Lanka and Vietnam. (arXiv:2310.05682v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.05682](http://arxiv.org/abs/2310.05682)

    本研究使用遥感技术分析了斯里兰卡和越南两个热带国家的降雨模式和水库水域范围的变化。研究结果有助于理解遥感降雨数据与水域范围动态变化之间的关系。

    

    本研究通过遥感技术对越南和斯里兰卡两个热带季风国家的降雨模式和选定的水电水库水域范围进行了全面分析。研究旨在了解遥感降雨数据与水库水域范围（月度）动态变化之间的关系。分析利用高分辨率光学影像和Sentinel-1合成孔径雷达（SAR）数据观测和监测不同天气条件下水体，特别是季风季节。确定了两国的年均降雨量，并使用1981年至2022年的气候灾害组红外降雨数据集（CHIRPS）在区域和水库流域级别上考察了月均降雨的时空变化。使用2017年至2022年的Sentinel-1 SAR地面范围检测（GRD）图像获取了选定水库的水域范围。

    This study presents a comprehensive remote sensing analysis of rainfall patterns and selected hydropower reservoir water extent in two tropical monsoon countries, Vietnam and Sri Lanka. The aim is to understand the relationship between remotely sensed rainfall data and the dynamic changes (monthly) in reservoir water extent. The analysis utilizes high-resolution optical imagery and Sentinel-1 Synthetic Aperture Radar (SAR) data to observe and monitor water bodies during different weather conditions, especially during the monsoon season. The average annual rainfall for both countries is determined, and spatiotemporal variations in monthly average rainfall are examined at regional and reservoir basin levels using the Climate Hazards Group InfraRed Precipitation with Station (CHIRPS) dataset from 1981 to 2022. Water extents are derived for selected reservoirs using Sentinel-1 SAR Ground Range Detected (GRD) images in Vietnam and Sri Lanka from 2017 to 2022. The images are pre-processed an
    
[^136]: 多步模型的基于模型的强化学习

    Multi-timestep models for Model-based Reinforcement Learning. (arXiv:2310.05672v1 [cs.LG])

    [http://arxiv.org/abs/2310.05672](http://arxiv.org/abs/2310.05672)

    多步模型的基于模型的强化学习算法通过使用多步目标来训练一步模型，解决了轨迹长度增长时一步预测误差的累积问题，并在噪声数据上表现出显著的性能提升。

    

    在基于模型的强化学习中，大多数算法依赖于从数据中学习到的一步动力学模型来模拟轨迹。这种方法的一个关键挑战是随着轨迹长度的增长，一步预测误差的累积。本文通过使用多步目标来训练一步模型来解决这个问题。我们的目标是在各种未来时间段上的一个损失函数（例如，负对数似然）的加权和。我们探索和测试了一系列权重方案。我们发现指数衰减权重导致模型在长时间段的R2得分显著提高。当模型在噪声数据上进行评估时，这种改进尤为明显。最后，我们在纯批量强化学习（RL）和迭代批量RL场景中使用软件演员-评论家（SAC）代理，发现我们的多步模型优于或与标准的一步模型相匹配。这在考虑环境的噪声变体中尤为明显。

    In model-based reinforcement learning (MBRL), most algorithms rely on simulating trajectories from one-step dynamics models learned on data. A critical challenge of this approach is the compounding of one-step prediction errors as length of the trajectory grows. In this paper we tackle this issue by using a multi-timestep objective to train one-step models. Our objective is a weighted sum of a loss function (e.g., negative log-likelihood) at various future horizons. We explore and test a range of weights profiles. We find that exponentially decaying weights lead to models that significantly improve the long-horizon R2 score. This improvement is particularly noticeable when the models were evaluated on noisy data. Finally, using a soft actor-critic (SAC) agent in pure batch reinforcement learning (RL) and iterated batch RL scenarios, we found that our multi-timestep models outperform or match standard one-step models. This was especially evident in a noisy variant of the considered envi
    
[^137]: 关于使用LSTD和随机特征的强化学习中的双下降现象

    On Double-Descent in Reinforcement Learning with LSTD and Random Features. (arXiv:2310.05518v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2310.05518](http://arxiv.org/abs/2310.05518)

    本文研究了在强化学习中网络大小和L2正则化对性能的影响，并观察到了双下降现象。通过使用随机特征和懒惰训练策略，在参数和状态数无限大的情况下研究了正则化的最小二乘时间差分算法，得出了其收敛性和最优性，并阐述了双下降现象在该算法中的影响。

    

    时间差分算法在深度强化学习中被广泛使用，其性能受神经网络大小的影响。然而，在监督学习中过参数化和其带来的好处已经得到了很好的理解，但是在强化学习中情况则不太清楚。本文通过理论分析探讨了网络大小和L2正则化对性能的影响，并将参数个数与访问状态个数之比定义为关键因素，当该比值大于1时称为过参数化。此外，我们观察到了双下降现象，即在参数/状态比为1附近会突然性能下降。通过利用随机特征和懒惰训练策略，我们在无限大的参数和状态数下研究了正则化的最小二乘时间差分算法。我们推导了其收敛性和最优性，并阐述了双下降现象在该算法中的影响。

    Temporal Difference (TD) algorithms are widely used in Deep Reinforcement Learning (RL). Their performance is heavily influenced by the size of the neural network. While in supervised learning, the regime of over-parameterization and its benefits are well understood, the situation in RL is much less clear. In this paper, we present a theoretical analysis of the influence of network size and $l_2$-regularization on performance. We identify the ratio between the number of parameters and the number of visited states as a crucial factor and define over-parameterization as the regime when it is larger than one. Furthermore, we observe a double-descent phenomenon, i.e., a sudden drop in performance around the parameter/state ratio of one. Leveraging random features and the lazy training regime, we study the regularized Least-Square Temporal Difference (LSTD) algorithm in an asymptotic regime, as both the number of parameters and states go to infinity, maintaining a constant ratio. We derive 
    
[^138]: 基于查询式操作网络的振动声学频响预测

    Vibroacoustic Frequency Response Prediction with Query-based Operator Networks. (arXiv:2310.05469v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.05469](http://arxiv.org/abs/2310.05469)

    该论文提出了一个基于查询式操作网络的振动声学频响预测方法，并设计了一个用于代表性振动声学问题的结构化基准测试。该方法可以加速频响模拟，有助于设计优化和不确定性量化。

    

    理解飞机、汽车和房屋等机械结构中的振动声学波传播对保证用户的健康和舒适至关重要。为了分析这些系统，设计师和工程师主要考虑频域中的动态响应，这通过像有限元方法这样的昂贵数值模拟来计算。相比之下，基于数据的替代模型承诺加速这些模拟，从而促进设计优化、不确定性量化和设计空间探索等任务的实施。我们提出了一个结构化的基准测试用于代表性振动声学问题：预测带有不同形式镶边的振动板的频响。该基准测试包含了共计12,000个板几何形状以及相应的数值解，并引入了评估指标以量化预测质量。为了解决频响预测任务，我们提出了一种新颖的频率查询操作模型。

    Understanding vibroacoustic wave propagation in mechanical structures like airplanes, cars and houses is crucial to ensure health and comfort of their users. To analyze such systems, designers and engineers primarily consider the dynamic response in the frequency domain, which is computed through expensive numerical simulations like the finite element method. In contrast, data-driven surrogate models offer the promise of speeding up these simulations, thereby facilitating tasks like design optimization, uncertainty quantification, and design space exploration. We present a structured benchmark for a representative vibroacoustic problem: Predicting the frequency response for vibrating plates with varying forms of beadings. The benchmark features a total of 12,000 plate geometries with an associated numerical solution and introduces evaluation metrics to quantify the prediction quality. To address the frequency response prediction task, we propose a novel frequency query operator model, 
    
[^139]: 针对大型语言模型预训练的高效参数搜索和并行性的扩展研究

    Scaling Studies for Efficient Parameter Search and Parallelism for Large Language Model Pre-training. (arXiv:2310.05350v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2310.05350](http://arxiv.org/abs/2310.05350)

    本研究针对大型语言模型预训练进行了并行和分布式机器学习算法的开发，旨在优化数据处理和提高模型的训练效率。

    

    AI加速器的计算能力和内存限制在很大程度上确定了机器学习工作负载（如训练和推理）在可接受的时间范围内执行的规模。如今，训练最先进的基于transformer的模型需要使用带有高速互连的GPU加速高性能计算机。随着数据集和模型的不断增大，AI的计算需求和内存需求也在不断增长。这些挑战促使开发出分布式算法和基于电路的优化技术，能够在多节点环境中逐步扩展模型，有效地减少神经网络的成本函数以实现更快的收敛，并将更多参数存储在一定数量的可用资源中。在我们的研究项目中，我们专注于并行和分布式机器学习算法的开发，特别是针对优化数据处理和预训练的一组算法。

    AI accelerator processing capabilities and memory constraints largely dictate the scale in which machine learning workloads (e.g., training and inference) can be executed within a desirable time frame. Training a state of the art, transformer-based model today requires use of GPU-accelerated high performance computers with high-speed interconnects. As datasets and models continue to increase in size, computational requirements and memory demands for AI also continue to grow. These challenges have inspired the development of distributed algorithm and circuit-based optimization techniques that enable the ability to progressively scale models in multi-node environments, efficiently minimize neural network cost functions for faster convergence, and store more parameters into a set number of available resources. In our research project, we focus on parallel and distributed machine learning algorithm development, specifically for optimizing the data processing and pre-training of a set of 5 
    
[^140]: 在多体系统中学习力学规律

    Learning force laws in many-body systems. (arXiv:2310.05273v1 [physics.plasm-ph] CROSS LISTED)

    [http://arxiv.org/abs/2310.05273](http://arxiv.org/abs/2310.05273)

    在这篇论文中，作者展示了一种结合了物理直觉的机器学习方法，用于推断尘埃等离子体实验中的力学规律。通过对粒子轨迹的训练，该模型考虑了对称性和非相同粒子之间的非互逆力，并提取了每个粒子的质量和电荷。模型的准确性指示出尘埃等离子体中存在超出当前理论分辨率的新物理，并展示了机器学习在引导多体系统科学发现方面的潜力。

    

    描述自然系统的科学规律可能比我们的直觉更复杂，因此我们发现规律的方法必须改变。机器学习（ML）模型可以分析大量数据，但其结构应该符合基本的物理约束条件以提供有用的见解。在这里，我们展示了一种结合了物理直觉的ML方法，以推断尘埃等离子体实验中的力学法则。通过对3D粒子轨迹进行训练，该模型考虑了固有的对称性和非相同粒子之间的有效非互逆力，并提取出每个粒子的质量和电荷。模型的准确性（R^2 > 0.99）指示出尘埃等离子体中超出当前理论分辨率的新物理，并展示了机器学习驱动的方法如何引导多体系统中的科学发现的新途径。

    Scientific laws describing natural systems may be more complex than our intuition can handle, and thus how we discover laws must change. Machine learning (ML) models can analyze large quantities of data, but their structure should match the underlying physical constraints to provide useful insight. Here we demonstrate a ML approach that incorporates such physical intuition to infer force laws in dusty plasma experiments. Trained on 3D particle trajectories, the model accounts for inherent symmetries and non-identical particles, accurately learns the effective non-reciprocal forces between particles, and extracts each particle's mass and charge. The model's accuracy (R^2 > 0.99) points to new physics in dusty plasma beyond the resolution of current theories and demonstrates how ML-powered approaches can guide new routes of scientific discovery in many-body systems.
    
[^141]: 跨不同条件精确预测电池寿命的学习细胞内和细胞间差异

    Learning Intra- and Inter-Cell Differences for Accurate Battery Lifespan Prediction across Diverse Conditions. (arXiv:2310.05052v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2310.05052](http://arxiv.org/abs/2310.05052)

    该论文介绍了一种跨不同条件精确预测电池寿命的方法，通过捕捉目标电池和参考电池之间的电信号差异，无论材料和老化条件如何，在扩展特征空间的同时为通用的电池寿命预测框架铺平了道路。

    

    电池寿命预测对电池研究和开发具有重要的实际价值。目前，许多数据驱动模型依赖于特定目标电池的早期电信号来预测它们的寿命。一个常见的不足是，大多数现有方法都是基于特定老化条件开发的，这不仅限制了它们的模型能力，而且降低了它们在预测不同条件下的退化效果。因此，这些模型通常无法充分利用其他条件下可用的丰富历史数据。为了解决这个问题，我们引入了一种方法，明确捕捉目标电池和参考电池之间的电信号差异，无论它们的材料和老化条件如何，来预测目标电池的寿命。通过这种细胞间差异，我们不仅扩展了特征空间，还为通用的电池寿命预测框架铺平了道路。显著的是，我们的方法能够在不同条件下精确预测电池寿命。

    Battery life prediction holds significant practical value for battery research and development. Currently, many data-driven models rely on early electrical signals from specific target batteries to predict their lifespan. A common shortfall is that most existing methods are developed based on specific aging conditions, which not only limits their model's capability but also diminishes their effectiveness in predicting degradation under varied conditions. As a result, these models often miss out on fully benefiting from the rich historical data available under other conditions. Here, to address above, we introduce an approach that explicitly captures differences between electrical signals of a target battery and a reference battery, irrespective of their materials and aging conditions, to forecast the target battery life. Through this inter-cell difference, we not only enhance the feature space but also pave the way for a universal battery life prediction framework. Remarkably, our mode
    
[^142]: 利用椭圆边界框进行细胞跟踪的检测

    Cell Tracking-by-detection using Elliptical Bounding Boxes. (arXiv:2310.04895v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.04895](http://arxiv.org/abs/2310.04895)

    本文提出了一种基于经典检测方法的新方法，通过将细胞形状近似为定向椭圆并使用通用定向对象检测器来识别细胞，实现了对细胞的检测和跟踪，减轻了对标注数据的需求。

    

    细胞的检测和跟踪对于生物分析至关重要。最近的方法依赖于基于模型演进的跟踪，通常通过训练端到端的深度学习模型来在图像帧上检测和跟踪细胞，并取得了令人满意的结果。然而，这些方法需要大量的标注数据，而获取这些数据耗时且需要专业的标注员。本文提出了一种基于经典检测方法的新方法，可以减轻对标注数据的需求。具体来说，它将细胞形状近似为定向椭圆，然后使用通用定向对象检测器来识别每个帧中的细胞。然后，我们依靠一个全局数据关联算法，使用概率距离度量来探索细胞的时间相似性，考虑到椭圆与二维高斯分布的关系。我们的结果表明，我们的方法能够实现具有竞争力的检测和跟踪结果。

    Cell detection and tracking are paramount for bio-analysis. Recent approaches rely on the tracking-by-model evolution paradigm, which usually consists of training end-to-end deep learning models to detect and track the cells on the frames with promising results. However, such methods require extensive amounts of annotated data, which is time-consuming to obtain and often requires specialized annotators. This work proposes a new approach based on the classical tracking-by-detection paradigm that alleviates the requirement of annotated data. More precisely, it approximates the cell shapes as oriented ellipses and then uses generic-purpose oriented object detectors to identify the cells in each frame. We then rely on a global data association algorithm that explores temporal cell similarity using probability distance metrics, considering that the ellipses relate to two-dimensional Gaussian distributions. Our results show that our method can achieve detection and tracking results competiti
    
[^143]: LauraGPT：使用GPT进行听、关注、理解和再生音频的研究

    LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT. (arXiv:2310.04673v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2310.04673](http://arxiv.org/abs/2310.04673)

    LauraGPT是一个统一的GPT模型，用于音频识别、理解和生成，具有广泛的应用范围，包括自动语音识别、语音翻译、文本到语音合成、机器翻译等任务。

    

    生成式预训练变换器（GPT）模型在各种自然语言处理任务中取得了显著的性能。然而，将类似的框架应用于音频任务的研究有限。以前提出的用于音频任务的大型语言模型要么缺乏充分的定量评估，要么局限于识别和理解音频内容的任务，要么明显不及现有的最先进模型（SOTA）。本文中，我们提出了LauraGPT，一个用于音频识别、理解和生成的统一GPT模型。LauraGPT是一个通用的语言模型，可以处理音频和文本输入，并在任意模式下生成输出。它可以进行与内容、语义、语音学和音频信号分析相关的各种任务。其中一些值得注意的任务包括自动语音识别、语音到文本翻译、文本到语音合成、机器翻译、语音增强、自动音频捕获等。

    Generative Pre-trained Transformer (GPT) models have achieved remarkable performance on various natural language processing tasks. However, there has been limited research on applying similar frameworks to audio tasks. Previously proposed large language models for audio tasks either lack sufficient quantitative evaluations, or are limited to tasks for recognizing and understanding audio content, or significantly underperform existing state-of-the-art (SOTA) models. In this paper, we propose LauraGPT, a unified GPT model for audio recognition, understanding, and generation. LauraGPT is a versatile language model that can process both audio and text inputs and generate outputs in either modalities. It can perform a wide range of tasks related to content, semantics, paralinguistics, and audio-signal analysis. Some of its noteworthy tasks include automatic speech recognition, speech-to-text translation, text-to-speech synthesis, machine translation, speech enhancement, automated audio capt
    
[^144]: 利用自一致性提高数据有效的摊余贝叶斯推理方法

    Leveraging Self-Consistency for Data-Efficient Amortized Bayesian Inference. (arXiv:2310.04395v1 [cs.LG])

    [http://arxiv.org/abs/2310.04395](http://arxiv.org/abs/2310.04395)

    该论文提出了一种利用自一致性改进数据有效的摊余贝叶斯推理方法，通过反转贝叶斯定理并利用近似表示的联合模型估计边际似然，加速条件神经密度估计器的学习动力学。

    

    我们提出了一种方法，通过利用参数$\theta$和数据$y$的概率联合模型$p(\theta, y)$中的通用对称性，改进了摊余贝叶斯推理（ABI）的效率和准确性。简言之，我们反转贝叶斯定理，并基于近似表示的联合模型估计边际似然。在完美近似情况下，边际似然在所有参数值上都是常数定义的。然而，近似误差导致不同参数值的边际似然估计中存在不可取的方差。我们将这种对称性的违反形式化为损失函数，加速条件神经密度估计器的学习动力学。我们将我们的方法应用于具有显式似然（基于似然）的双峰玩具问题和具有隐式似然（基于模拟）的现实模型。

    We propose a method to improve the efficiency and accuracy of amortized Bayesian inference (ABI) by leveraging universal symmetries in the probabilistic joint model $p(\theta, y)$ of parameters $\theta$ and data $y$. In a nutshell, we invert Bayes' theorem and estimate the marginal likelihood based on approximate representations of the joint model. Upon perfect approximation, the marginal likelihood is constant across all parameter values by definition. However, approximation error leads to undesirable variance in the marginal likelihood estimates across different parameter values. We formulate violations of this symmetry as a loss function to accelerate the learning dynamics of conditional neural density estimators. We apply our method to a bimodal toy problem with an explicit likelihood (likelihood-based) and a realistic model with an implicit likelihood (simulation-based).
    
[^145]: 脑机接口中的相位同步成分自组织

    Phase Synchrony Component Self-Organization in Brain Computer Interface. (arXiv:2310.03748v1 [eess.SP])

    [http://arxiv.org/abs/2310.03748](http://arxiv.org/abs/2310.03748)

    本文提出了相位同步成分自组织的概念，利用深度学习端到端网络自动化脑机接口中的预处理和通道选择，从而提高了分析功能性脑连接和识别脑活动的效果。

    

    相位同步信息在分析功能性脑连接和识别脑活动中起着关键作用。目前广泛采用的特征提取流程由预处理、选择脑电采集通道和相位锁定值（PLV）计算组成，在运动想象分类方面取得了成功。然而，该流程是手动的且依赖专家知识，限制了它在不同应用场景中的便利性和适应性。此外，大多数研究都采用了一般的与数据无关的空间滤波器来抑制噪声，阻碍了更重要的相位同步现象的探索。为了解决这些问题，我们提出了相位同步成分自组织的概念，它能够自适应地学习数据相关的空间滤波器，从而自动化预处理和通道选择程序。基于这个概念，我们开发了第一个深度学习端到端网络，直接提取

    Phase synchrony information plays a crucial role in analyzing functional brain connectivity and identifying brain activities. A widely adopted feature extraction pipeline, composed of preprocessing, selection of EEG acquisition channels, and phase locking value (PLV) calculation, has achieved success in motor imagery classification (MI). However, this pipeline is manual and reliant on expert knowledge, limiting its convenience and adaptability to different application scenarios. Moreover, most studies have employed mediocre data-independent spatial filters to suppress noise, impeding the exploration of more significant phase synchronization phenomena. To address the issues, we propose the concept of phase synchrony component self-organization, which enables the adaptive learning of data-dependent spatial filters for automating both the preprocessing and channel selection procedures. Based on this concept, the first deep learning end-to-end network is developed, which directly extracts 
    
[^146]: HyperMask: 自适应的基于超网络的掩码用于持续学习

    HyperMask: Adaptive Hypernetwork-based Masks for Continual Learning. (arXiv:2310.00113v1 [cs.LG])

    [http://arxiv.org/abs/2310.00113](http://arxiv.org/abs/2310.00113)

    HyperMask是一种用于持续学习的方法，它使用基于超网络的掩码来训练一个单一网络，以克服人工神经网络在多任务上的灾难性遗忘问题。

    

    当人工神经网络在多个任务上顺序训练时，往往会出现灾难性遗忘的问题。为了克服这个问题，已经存在许多持续学习策略，其中最有效的之一是基于超网络的方法。超网络根据任务的特征生成目标模型的权重。然而，该模型的主要限制是超网络对于每个任务可以产生完全不同的网络结构，因此每个任务都是单独解决的。模型在学习后续任务时不使用之前任务所关联的网络信息，并实际上产生了新的网络架构。为了解决这个问题，我们使用了彩票票证假设，该假设认为存在稀疏的子网络（即中奖票），可以保持完整网络的性能。在本文中，我们提出了一种名为HyperMask的方法，该方法为所有任务训练一个单一网络。超网络产生半二进制掩码，以获取目标子网络。

    Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, there exist many continual learning strategies. One of the most effective is the hypernetwork-based approach. The hypernetwork generates the weights of a target model based on the task's identity. The model's main limitation is that hypernetwork can produce completely different nests for each task. Consequently, each task is solved separately. The model does not use information from the network dedicated to previous tasks and practically produces new architectures when it learns the subsequent tasks. To solve such a problem, we use the lottery ticket hypothesis, which postulates the existence of sparse subnetworks, named winning tickets, that preserve the performance of a full network.  In the paper, we propose a method called HyperMask, which trains a single network for all tasks. Hypernetwork produces semi-binary masks to obtain target subnetw
    
[^147]: 未来的原因，现在的行动：一种可证明样本效率的自主LLM智能体的原则框架

    Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency. (arXiv:2309.17382v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2309.17382](http://arxiv.org/abs/2309.17382)

    提出了一个名为"RAFA"的原则框架，通过在LLM中将推理视为学习和规划的贝叶斯问题，协调推理和行动。通过一个提示模板进行推理，学习并制定未来的轨迹规划，然后在每一步中采取计划轨迹的初始行动并重新规划未来轨迹。这个框架具有可证明的遗憾保证。

    

    大型语言模型（LLM）展示了令人印象深刻的推理能力，但在现实世界中将推理转化为行动仍然具有挑战性。特别是，如何通过推理的内部机制在与外部环境的最少交互次数内可证明地完成给定任务仍然不清楚。为此，我们提出了一个有可证明遗憾保证的原则框架来协调推理和行动，称之为“为未来而推理，为现在而行动”（RAFA）。具体来说，我们设计了一个推理的提示模板，从内存缓冲区中学习并制定未来的长期轨迹规划（“为未来而推理”）。在每一步中，LLM智能体采取计划轨迹的初始行动（“为现在而行动”），将收集到的反馈存储在内存缓冲区中，并重新调用推理过程从新状态重新规划未来的轨迹。关键思想是将LLM中的推理视为学习和规划的贝叶斯问题。

    Large language models (LLMs) demonstrate impressive reasoning abilities, but translating reasoning into actions in the real world remains challenging. In particular, it remains unclear how to complete a given task provably within a minimum number of interactions with the external environment, e.g., through an internal mechanism of reasoning. To this end, we propose a principled framework with provable regret guarantees to orchestrate reasoning and acting, which we call "reason for future, act for now" (\texttt{RAFA}). Specifically, we design a prompt template for reasoning that learns from the memory buffer and plans a future trajectory over a long horizon ("reason for future"). At each step, the LLM agent takes the initial action of the planned trajectory ("act for now"), stores the collected feedback in the memory buffer, and reinvokes the reasoning routine to replan the future trajectory from the new state.  The key idea is to cast reasoning in LLMs as learning and planning in Bayes
    
[^148]: ACGAN-GNNExplainer：用于图神经网络的辅助条件生成解释器

    ACGAN-GNNExplainer: Auxiliary Conditional Generative Explainer for Graph Neural Networks. (arXiv:2309.16918v1 [cs.LG])

    [http://arxiv.org/abs/2309.16918](http://arxiv.org/abs/2309.16918)

    本论文提出了一种新的图神经网络解释方法ACGAN-GNNExplainer，将辅助分类器生成对抗网络（ACGAN）引入到GNN解释领域。通过利用生成器为原始输入图生成解释，并借助鉴别器监督生成过程，提高解释的准确性和保真度。实验证明了该方法的有效性和实用性。

    

    图神经网络（GNNs）已经在各种实际应用中证明了其有效性，但其基本机制仍然是一个谜。为了解决这个挑战并实现可靠的决策，近年来提出了许多GNN解释器。然而，这些方法常常面临一些限制，包括对特定实例的依赖性，对未见过的图的一般性不足，可能产生无效的解释以及生成过程中产生的不充分的保真度。为了克服这些限制，本文将辅助分类器生成对抗网络（ACGAN）引入到GNN解释领域，并提出了一种新的GNN解释器ACGAN-GNNExplainer。我们的方法利用生成器为原始输入图生成解释，并运用鉴别器来监督生成过程，确保解释的保真度并提高准确性。实验评估分别在合成数据集和真实世界的图数据集上进行。

    Graph neural networks (GNNs) have proven their efficacy in a variety of real-world applications, but their underlying mechanisms remain a mystery. To address this challenge and enable reliable decision-making, many GNN explainers have been proposed in recent years. However, these methods often encounter limitations, including their dependence on specific instances, lack of generalizability to unseen graphs, producing potentially invalid explanations, and yielding inadequate fidelity. To overcome these limitations, we, in this paper, introduce the Auxiliary Classifier Generative Adversarial Network (ACGAN) into the field of GNN explanation and propose a new GNN explainer dubbed~\emph{ACGAN-GNNExplainer}. Our approach leverages a generator to produce explanations for the original input graphs while incorporating a discriminator to oversee the generation process, ensuring explanation fidelity and improving accuracy. Experimental evaluations conducted on both synthetic and real-world graph
    
[^149]: 基于交叉预测的推理

    Cross-Prediction-Powered Inference. (arXiv:2309.16598v1 [stat.ML])

    [http://arxiv.org/abs/2309.16598](http://arxiv.org/abs/2309.16598)

    本文介绍了一种基于机器学习的交叉预测方法，可以有效地进行推理。该方法通过使用一个小型标记数据集和一个大型未标记数据集，通过机器学习填补缺失的标签，并采用去偏差方法纠正预测的不准确性。

    

    可靠的数据驱动决策依赖于高质量的标注数据，然而获取高质量的标注数据经常需要繁琐的人工标注或者缓慢昂贵的科学测量。机器学习作为一种替代方案正变得越来越有吸引力，因为精密的预测技术可以快速、廉价地产生大量预测标签；例如，预测的蛋白质结构被用来补充实验得到的结构，卫星图像预测的社会经济指标被用来补充准确的调查数据等。由于预测具有不完美和潜在偏差的特点，这种做法对下游推理的有效性产生了质疑。我们引入了基于机器学习的交叉预测方法，用于有效的推理。通过一个小的标记数据集和一个大的未标记数据集，交叉预测通过机器学习填补缺失的标签，并应用一种去偏差的方法来纠正预测不准确性。

    While reliable data-driven decision-making hinges on high-quality labeled data, the acquisition of quality labels often involves laborious human annotations or slow and expensive scientific measurements. Machine learning is becoming an appealing alternative as sophisticated predictive techniques are being used to quickly and cheaply produce large amounts of predicted labels; e.g., predicted protein structures are used to supplement experimentally derived structures, predictions of socioeconomic indicators from satellite imagery are used to supplement accurate survey data, and so on. Since predictions are imperfect and potentially biased, this practice brings into question the validity of downstream inferences. We introduce cross-prediction: a method for valid inference powered by machine learning. With a small labeled dataset and a large unlabeled dataset, cross-prediction imputes the missing labels via machine learning and applies a form of debiasing to remedy the prediction inaccurac
    
[^150]: LPML: 数学推理的LLM提示标记语言

    LPML: LLM-Prompting Markup Language for Mathematical Reasoning. (arXiv:2309.13078v1 [cs.AI])

    [http://arxiv.org/abs/2309.13078](http://arxiv.org/abs/2309.13078)

    本论文提出了LPML，一种用于数学推理的LLM提示标记语言。通过将Chain-of-Thought方法和Python REPL与该标记语言结合，我们能够控制LLM生成文本中的错误，并增强其推理能力。我们的方法能够实现利用Python计算纠正错误和解决挑战性数学问题，而只需要零样本提示。

    

    在利用大型语言模型（LLMs）进行数学推理时，解决LLMs生成文本中的推理和计算错误是一个关键挑战。在本文中，我们提出了一种新的框架，将Chain-of-Thought（CoT）方法与外部工具（Python REPL）相结合。我们发现，通过提示LLMs生成类似XML标记语言的结构化文本，我们可以无缝地集成CoT和外部工具，并控制LLMs的不良行为。通过我们的方法，LLMs可以利用Python计算来纠正CoT中的错误。我们将我们的方法应用于ChatGPT（GPT-3.5）来解决具有挑战性的数学问题，并证明通过标记语言将CoT和Python REPL结合起来可以增强LLMs的推理能力。我们的方法使LLMs能够使用零样本提示编写标记语言，并进行高级数学推理。

    In utilizing large language models (LLMs) for mathematical reasoning, addressing the errors in the reasoning and calculation present in the generated text by LLMs is a crucial challenge. In this paper, we propose a novel framework that integrates the Chain-of-Thought (CoT) method with an external tool (Python REPL). We discovered that by prompting LLMs to generate structured text in XML-like markup language, we could seamlessly integrate CoT and the external tool and control the undesired behaviors of LLMs. With our approach, LLMs can utilize Python computation to rectify errors within CoT. We applied our method to ChatGPT (GPT-3.5) to solve challenging mathematical problems and demonstrated that combining CoT and Python REPL through the markup language enhances the reasoning capability of LLMs. Our approach enables LLMs to write the markup language and perform advanced mathematical reasoning using only zero-shot prompting.
    
[^151]: 关于免疫的概率的研究

    On the Probability of Immunity. (arXiv:2309.11942v1 [stat.ME])

    [http://arxiv.org/abs/2309.11942](http://arxiv.org/abs/2309.11942)

    本文研究了免疫的概率，提出了免疫的必要和充分条件，以及ε-有界免疫的条件。同时，借助随机对照试验估计受益概率，并得到比现有边界更紧密的概率边界。此外，介绍了间接免疫的概念，并提出了一种用于处理未测量混淆的免疫概率敏感性分析方法。

    

    本文致力于研究免疫的概率，即无论暴露与否，效果都会发生。我们导出了免疫的必要和充分条件以及ε-有界免疫的条件，前者允许我们从随机对照试验中估计受益的概率（即只有在暴露的情况下效果才会发生），后者允许我们得到比现有的边界更紧密的受益概率边界。我们还引入了间接免疫的概念（通过介质），并对其进行了前述分析。最后，我们提出了一种用于在未测量混淆情况下进行免疫概率敏感性分析的方法。

    This work is devoted to the study of the probability of immunity, i.e. the effect occurs whether exposed or not. We derive necessary and sufficient conditions for non-immunity and $\epsilon$-bounded immunity, i.e. the probability of immunity is zero and $\epsilon$-bounded, respectively. The former allows us to estimate the probability of benefit (i.e., the effect occurs if and only if exposed) from a randomized controlled trial, and the latter allows us to produce bounds of the probability of benefit that are tighter than the existing ones. We also introduce the concept of indirect immunity (i.e., through a mediator) and repeat our previous analysis for it. Finally, we propose a method for sensitivity analysis of the probability of immunity under unmeasured confounding.
    
[^152]: DiscoverPath：用于生物医学研究的知识细化和检索系统

    DiscoverPath: A Knowledge Refinement and Retrieval System for Interdisciplinarity on Biomedical Research. (arXiv:2309.01808v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2309.01808](http://arxiv.org/abs/2309.01808)

    DiscoverPath是一个基于知识图的生物医学研究论文搜索引擎，通过命名实体识别和词性标注从文章摘要中提取术语和关系，并展示给用户一个关注查询实体及其邻近节点的子图，以及查询推荐系统，使用户能够循序渐进地细化查询。

    

    学术出版物的指数增长需要高级工具来实现高效的文章检索，尤其在跨学科领域中，不同的术语被用来描述相似的研究。传统的基于关键词的搜索引擎往往无法帮助那些对特定术语不熟悉的用户。为了解决这个问题，我们提出了一种基于知识图的生物医学研究论文搜索引擎，以增强用户在发现相关查询和文章方面的体验。该系统被称为DiscoverPath，采用命名实体识别（NER）和词性标注（POS）来从文章摘要中提取术语和关系，创建知识图谱。为了减少信息超载，DiscoverPath给用户展示了一个关注查询实体及其邻近节点的子图，并且还结合了查询推荐系统，使用户能够循序渐进地细化查询。该系统配备了一个易于访问的图形用户界面（GUI）。

    The exponential growth in scholarly publications necessitates advanced tools for efficient article retrieval, especially in interdisciplinary fields where diverse terminologies are used to describe similar research. Traditional keyword-based search engines often fall short in assisting users who may not be familiar with specific terminologies. To address this, we present a knowledge graph-based paper search engine for biomedical research to enhance the user experience in discovering relevant queries and articles. The system, dubbed DiscoverPath, employs Named Entity Recognition (NER) and part-of-speech (POS) tagging to extract terminologies and relationships from article abstracts to create a KG. To reduce information overload, DiscoverPath presents users with a focused subgraph containing the queried entity and its neighboring nodes and incorporates a query recommendation system, enabling users to iteratively refine their queries. The system is equipped with an accessible Graphical Us
    
[^153]: 阐明扩散模型中的曝光偏差问题

    Elucidating the Exposure Bias in Diffusion Models. (arXiv:2308.15321v1 [cs.LG])

    [http://arxiv.org/abs/2308.15321](http://arxiv.org/abs/2308.15321)

    本文系统地研究了扩散模型中的曝光偏差问题，并提出了一种名为Epsilon Scaling的免训练方法来减轻这一问题。实验结果验证了该方法的有效性。

    

    扩散模型展示了令人印象深刻的生成能力，但它们的“曝光偏差”问题，即训练和采样之间的输入不匹配，缺乏深入探索。本文通过首先对采样分布进行分析建模，然后将每个采样步骤的预测误差归因为曝光偏差问题的根本原因，系统地研究了扩散模型中的曝光偏差问题。此外，我们讨论了解决这个问题的潜在方法，并提出了一个直观的度量标准。除了阐明曝光偏差问题，我们提出了一种简单但有效的免训练方法，称为Epsilon Scaling，以减轻曝光偏差。我们展示了Epsilon Scaling通过缩小网络输出（Epsilon）明确地将采样轨迹移近训练阶段学习到的向量场，从而减轻了训练和采样之间的输入不匹配。在各种扩散框架上进行了实验。

    Diffusion models have demonstrated impressive generative capabilities, but their 'exposure bias' problem, described as the input mismatch between training and sampling, lacks in-depth exploration. In this paper, we systematically investigate the exposure bias problem in diffusion models by first analytically modelling the sampling distribution, based on which we then attribute the prediction error at each sampling step as the root cause of the exposure bias issue. Furthermore, we discuss potential solutions to this issue and propose an intuitive metric for it. Along with the elucidation of exposure bias, we propose a simple, yet effective, training-free method called Epsilon Scaling to alleviate the exposure bias. We show that Epsilon Scaling explicitly moves the sampling trajectory closer to the vector field learned in the training phase by scaling down the network output (Epsilon), mitigating the input mismatch between training and sampling. Experiments on various diffusion framework
    
[^154]: InstructionGPT-4: 一个200指令范式用于微调MiniGPT-4

    InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4. (arXiv:2308.12067v1 [cs.LG])

    [http://arxiv.org/abs/2308.12067](http://arxiv.org/abs/2308.12067)

    InstructionGPT-4通过仅使用200个例子进行微调，在多模式指令数据质量度量和选择器的帮助下，在各种评估任务中优于原始的MiniGPT-4。

    

    多模式大型语言模型通过两阶段的训练过程获取其遵循指令的能力：在图像-文本对上进行预训练，然后在监督式视觉-语言指令数据上进行微调。最近的研究表明，即使只有有限量的高质量遵循指令数据，大型语言模型也能取得令人满意的结果。在本文中，我们介绍了InstructionGPT-4，它经过微调的数据集只包含200个例子，约占MiniGPT-4对齐数据集中使用的遵循指令数据的6%。我们首先提出了几个用于评估多模式指令数据质量的度量指标。基于这些度量指标，我们提出了一个简单而有效的数据选择器，自动识别和过滤低质量的视觉-语言数据。通过采用这种方法，InstructionGPT-4在各种评估（如视觉问答、GPT-4偏好）上优于原始的MiniGPT-4。总体而言，我们的研究发现...

    Multimodal large language models acquire their instruction-following capabilities through a two-stage training process: pre-training on image-text pairs and fine-tuning on supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6% of the instruction-following data used in the alignment dataset for MiniGPT-4. We first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present a simple and effective data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations (e.g., visual question answering, GPT-4 preference). Overall, our findi
    
[^155]: 一个关于校准的基准研究

    A Benchmark Study on Calibration. (arXiv:2308.11838v1 [cs.LG])

    [http://arxiv.org/abs/2308.11838](http://arxiv.org/abs/2308.11838)

    这项研究提出了一个模型校准的基准研究，利用神经架构搜索空间探索了模型校准属性。研究结果显示，模型校准可以在不同任务中泛化，并可以同时兼顾模型的准确性和校准性能。

    

    深度神经网络在各种机器学习任务中的应用越来越广泛。然而，随着这些模型复杂性的增加，它们往往面临校准问题，尽管预测准确性有所提高。许多研究通过数据预处理、使用特定损失函数和训练框架来改善校准性能。然而，对校准属性的研究有点被忽视了。我们的研究利用神经架构搜索（NAS）搜索空间，在全面探索校准属性的模型架构空间中提供了一个详尽的模型架构空间。我们特别创建了一个模型校准数据集。该数据集在广泛使用的NATS-Bench搜索空间中评估了90个基于区间的校准度量和12个其他校准度量，涵盖了117,702个独特的神经网络。我们的分析旨在通过我们提出的数据集回答该领域一些长期存在的问题：（i）模型校准能否在不同任务中泛化？（ii）能否同时兼顾模型的准确性和校准性能？

    Deep neural networks are increasingly utilized in various machine learning tasks. However, as these models grow in complexity, they often face calibration issues, despite enhanced prediction accuracy. Many studies have endeavored to improve calibration performance through data preprocessing, the use of specific loss functions, and training frameworks. Yet, investigations into calibration properties have been somewhat overlooked. Our study leverages the Neural Architecture Search (NAS) search space, offering an exhaustive model architecture space for thorough calibration properties exploration. We specifically create a model calibration dataset. This dataset evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks within the widely employed NATS-Bench search space. Our analysis aims to answer several longstanding questions in the field, using our proposed dataset: (i) Can model calibration be generalized across different tasks? (ii) Can rob
    
[^156]: 通过神经多项式方法实现可解释的弹性塑性模型的发现

    Discovering interpretable elastoplasticity models via the neural polynomial method enabled symbolic regressions. (arXiv:2307.13149v1 [cs.CE])

    [http://arxiv.org/abs/2307.13149](http://arxiv.org/abs/2307.13149)

    本文介绍了一种通过神经多项式方法实现可解释的弹性塑性模型的机器学习方法，该方法通过分为两个步骤，先通过监督学习得到一组特征映射，再通过符号回归将其转化为数学公式，从而克服了传统神经网络模型的缺乏可解释性的问题。

    

    传统神经网络弹性塑性模型通常被认为缺乏可解释性。本文介绍了一种两步机器学习方法，可以返回专家可解释的数学模型。具体而言，我们引入了一个替代模型，其中屈服曲面是通过监督学习得到的一组单变量特征映射来表示的。然后，通过符号回归将这组单变量神经网络映射函数重新解释为数学形式。这种分而治之的方法具有几个重要优势。首先，它使我们能够克服符号回归算法的扩展问题。从实际角度来看，它提高了用不同编程语言编写的偏微分方程求解器的学习模型的可移植性。最后，它使我们能够对材料的属性（如凸性和对称性）有一个具体的理解。

    Conventional neural network elastoplasticity models are often perceived as lacking interpretability. This paper introduces a two-step machine-learning approach that returns mathematical models interpretable by human experts. In particular, we introduce a surrogate model where yield surfaces are expressed in terms of a set of single-variable feature mappings obtained from supervised learning. A postprocessing step is then used to re-interpret the set of single-variable neural network mapping functions into mathematical form through symbolic regression. This divide-and-conquer approach provides several important advantages. First, it enables us to overcome the scaling issue of symbolic regression algorithms. From a practical perspective, it enhances the portability of learned models for partial differential equation solvers written in different programming languages. Finally, it enables us to have a concrete understanding of the attributes of the materials, such as convexity and symmetri
    
[^157]: 深度网络逼近：从ReLU到多种激活函数

    Deep Network Approximation: Beyond ReLU to Diverse Activation Functions. (arXiv:2307.06555v1 [cs.LG])

    [http://arxiv.org/abs/2307.06555](http://arxiv.org/abs/2307.06555)

    本文研究了深度神经网络在多种激活函数下的表达能力，证明了可以通过在有界集合上构建一个宽度为6N、深度为2L的varrho激活网络来逼近一个宽度为N、深度为L的ReLU网络，从而将对ReLU网络的逼近结果推广到其他激活函数。

    

    本文探究了深度神经网络在多种激活函数下的表达能力。定义了一个激活函数集合A，包括大多数常用的激活函数，如ReLU、LeakyReLU、ReLU^2、ELU、SELU、Softplus、GELU、SiLU、Swish、Mish、Sigmoid、Tanh、Arctan、Softsign、dSiLU和SRS。我们证明了对于任意激活函数varrho∈A，可以通过一个宽度为6N、深度为2L的varrho激活网络在有界集合上以任意精度逼近一个宽度为N、深度为L的ReLU网络。这一发现使得大部分对于ReLU网络的逼近结果能够推广到其他激活函数，尽管需要稍大的常数代价。

    This paper explores the expressive power of deep neural networks for a diverse range of activation functions. An activation function set $\mathscr{A}$ is defined to encompass the majority of commonly used activation functions, such as $\mathtt{ReLU}$, $\mathtt{LeakyReLU}$, $\mathtt{ReLU}^2$, $\mathtt{ELU}$, $\mathtt{SELU}$, $\mathtt{Softplus}$, $\mathtt{GELU}$, $\mathtt{SiLU}$, $\mathtt{Swish}$, $\mathtt{Mish}$, $\mathtt{Sigmoid}$, $\mathtt{Tanh}$, $\mathtt{Arctan}$, $\mathtt{Softsign}$, $\mathtt{dSiLU}$, and $\mathtt{SRS}$. We demonstrate that for any activation function $\varrho\in \mathscr{A}$, a $\mathtt{ReLU}$ network of width $N$ and depth $L$ can be approximated to arbitrary precision by a $\varrho$-activated network of width $6N$ and depth $2L$ on any bounded set. This finding enables the extension of most approximation results achieved with $\mathtt{ReLU}$ networks to a wide variety of other activation functions, at the cost of slightly larger constants.
    
[^158]: VerifAI：验证生成式人工智能

    VerifAI: Verified Generative AI. (arXiv:2307.02796v1 [cs.DB])

    [http://arxiv.org/abs/2307.02796](http://arxiv.org/abs/2307.02796)

    验证生成式人工智能的输出是一个新兴问题，我们提出了通过分析多模态数据湖的底层数据，评估其质量和一致性，来建立评估生成式人工智能模型输出的更坚实基础，并解决错误信息传播的挑战。

    

    生成式人工智能已经取得了重要的进展，但是对于其输出的准确性和可靠性的担忧仍在增长。这种不准确性可能产生严重后果，如错误决策，传播虚假信息，侵犯隐私，法律责任等。虽然已经在进行应对这些风险的努力，包括可解释的人工智能和负责任的人工智能实践，如透明度，隐私保护，偏见缓解以及社会和环境责任等，但由生成式人工智能引起的错误信息仍然是一个重大挑战。我们提出，从数据管理的角度验证生成式人工智能的输出是生成式人工智能的一个新兴问题。这包括分析来自多模态数据湖的底层数据，包括文本文件，表格和知识图谱，并评估其质量和一致性。通过这样做，我们可以为评估生成式人工智能模型的输出奠定更坚实的基础。这种方法能够帮助解决生成式人工智能的输出验证问题。

    Generative AI has made significant strides, yet concerns about the accuracy and reliability of its outputs continue to grow. Such inaccuracies can have serious consequences such as inaccurate decision-making, the spread of false information, privacy violations, legal liabilities, and more. Although efforts to address these risks are underway, including explainable AI and responsible AI practices such as transparency, privacy protection, bias mitigation, and social and environmental responsibility, misinformation caused by generative AI will remain a significant challenge. We propose that verifying the outputs of generative AI from a data management perspective is an emerging issue for generative AI. This involves analyzing the underlying data from multi-modal data lakes, including text files, tables, and knowledge graphs, and assessing its quality and consistency. By doing so, we can establish a stronger foundation for evaluating the outputs of generative AI models. Such an approach ca
    
[^159]: 弹性决策变压器

    Elastic Decision Transformer. (arXiv:2307.02484v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.02484](http://arxiv.org/abs/2307.02484)

    弹性决策变压器（EDT）通过在测试时间进行动作推断时调整历史长度来实现轨迹拼接，填补了决策变压器（DT）在这一方面的性能差距，并且在多任务情况下胜过基于Q-Learning的方法。

    

    本文介绍了弹性决策变压器（EDT），它是现有决策变压器（DT）及其变体的重大进展。尽管DT声称能够生成最佳轨迹，但实证证据表明它在轨迹拼接方面存在困难，轨迹拼接是指从一组次优轨迹中生成最优或接近最优轨迹的过程。提出的EDT通过在测试时间进行动作推断时调整DT中维护的历史长度来实现轨迹拼接，从而使自己与众不同。此外，当前轨迹是最优的时候，EDT通过保持较长的历史，当当前轨迹是次优的时候，EDT通过保持较短的历史来优化轨迹，使其能够与更优的轨迹进行“拼接”。广泛的实验表明，EDT能够填补基于DT和基于Q-Learning方法之间的性能差距。特别是，EDT在多任务情况下胜过基于Q-Learning的方法。

    This paper introduces Elastic Decision Transformer (EDT), a significant advancement over the existing Decision Transformer (DT) and its variants. Although DT purports to generate an optimal trajectory, empirical evidence suggests it struggles with trajectory stitching, a process involving the generation of an optimal or near-optimal trajectory from the best parts of a set of sub-optimal trajectories. The proposed EDT differentiates itself by facilitating trajectory stitching during action inference at test time, achieved by adjusting the history length maintained in DT. Further, the EDT optimizes the trajectory by retaining a longer history when the previous trajectory is optimal and a shorter one when it is sub-optimal, enabling it to "stitch" with a more optimal trajectory. Extensive experimentation demonstrates EDT's ability to bridge the performance gap between DT-based and Q Learning-based approaches. In particular, the EDT outperforms Q Learning-based methods in a multi-task regi
    
[^160]: ECG-QA：结合心电图的综合问答数据集

    ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram. (arXiv:2306.15681v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.15681](http://arxiv.org/abs/2306.15681)

    ECG-QA是第一个专为心电图分析设计的问答数据集，包括涵盖广泛临床相关ECG主题的问题模板和多样化的ECG解读问题。这一资源将为未来的医疗保健问答研究提供有价值的见解。

    

    由于自然语言处理的显著进展，医疗保健领域中的问答问题（QA）引起了广泛关注。然而，现有的医疗保健QA数据集主要集中在医学影像、临床记录或结构化的电子健康记录表上。这使得将心电图（ECG）数据与这些系统相结合的巨大潜力几乎未被利用。为填补这一空白，我们提出了ECG-QA，这是专门针对ECG分析设计的第一个QA数据集。该数据集包括共70个涵盖了广泛临床相关ECG主题的问题模板，每个问题都经过一名ECG专家的验证，以确保其临床效用。因此，我们的数据集包含了多样化的ECG解读问题，包括需要对两个不同的ECG进行比较分析的问题。此外，我们还进行了许多实验，为未来的研究方向提供了有价值的见解。我们相信ECG-QA将成为一个宝贵的资源，供研究者们探索和应用。

    Question answering (QA) in the field of healthcare has received much attention due to significant advancements in natural language processing. However, existing healthcare QA datasets primarily focus on medical images, clinical notes, or structured electronic health record tables. This leaves the vast potential of combining electrocardiogram (ECG) data with these systems largely untapped. To address this gap, we present ECG-QA, the first QA dataset specifically designed for ECG analysis. The dataset comprises a total of 70 question templates that cover a wide range of clinically relevant ECG topics, each validated by an ECG expert to ensure their clinical utility. As a result, our dataset includes diverse ECG interpretation questions, including those that require a comparative analysis of two different ECGs. In addition, we have conducted numerous experiments to provide valuable insights for future research directions. We believe that ECG-QA will serve as a valuable resource for the de
    
[^161]: 基于机器学习的实时反馈控制InAs/GaAs量子点生长

    Machine-Learning-Assisted and Real-Time-Feedback-Controlled Growth of InAs/GaAs Quantum Dots. (arXiv:2306.12898v1 [cond-mat.mes-hall])

    [http://arxiv.org/abs/2306.12898](http://arxiv.org/abs/2306.12898)

    该论文提出了一种基于机器学习的实时反馈控制InAs/GaAs量子点生长方法。

    

    自组装的InAs / GaAs量子点（QDs）具有用于开发各种光电子器件的极高价值。建立特定密度的QDs的过程参数是一个多维优化挑战，通常通过耗时和迭代的试错来解决。在此，作者使用基于3D ResNet的机器学习（ML）模型，专门训练RHEED视频，并提供有关表面形貌的实时反馈。

    Self-assembled InAs/GaAs quantum dots (QDs) have properties highly valuable for developing various optoelectronic devices such as QD lasers and single photon sources. The applications strongly rely on the density and quality of these dots, which has motivated studies of the growth process control to realize high-quality epi-wafers and devices. Establishing the process parameters in molecular beam epitaxy (MBE) for a specific density of QDs is a multidimensional optimization challenge, usually addressed through time-consuming and iterative trial-and-error. Meanwhile, reflective high-energy electron diffraction (RHEED) has been widely used to capture a wealth of growth information in situ. However, it still faces the challenges of extracting information from noisy and overlapping images. Here, based on 3D ResNet, we developed a machine learning (ML) model specially designed for training RHEED videos instead of static images and providing real-time feedback on surface morphologies for pro
    
[^162]: 半隐式去噪扩散模型（SIDDMs）

    Semi-Implicit Denoising Diffusion Models (SIDDMs). (arXiv:2306.12511v1 [cs.LG])

    [http://arxiv.org/abs/2306.12511](http://arxiv.org/abs/2306.12511)

    SIDDMs是一种新方法，通过匹配隐式和显式因子，实现在生成模型中快速收敛且一定程度上保证样本多样性和质量。

    

    尽管生成模型的数量正在增加，但在推理过程中实现快速采样而不牺牲样本多样性和质量仍然具有挑战性。现有的模型（如 DDPMS）可以提供高质量，丰富多样的样本，但受迭代步骤数量的固有限制而速度较慢。Denoising Diffusion Generative Adversarial Networks (DDGAN) 试图通过集成 GAN 模型用于扩散过程的较大跳跃来规避此限制。然而，当应用于大型数据集时，DDGAN 遇到了可扩展性限制。为了解决这些限制，我们引入了一种新的方法，通过匹配隐式和显式因子来解决问题。更具体地说，我们的方法涉及利用隐式模型来匹配嘈杂数据的边缘分布和前向扩散的显式条件分布。这种组合使我们能够有效地匹配联合去噪分布。与 DDPMS 不同，我们的半隐式去噪扩散模型（SIDDMs）可以在不影响所生成样本的多样性和质量的情况下快速收敛。

    Despite the proliferation of generative models, achieving fast sampling during inference without compromising sample diversity and quality remains challenging. Existing models such as Denoising Diffusion Probabilistic Models (DDPM) deliver high-quality, diverse samples but are slowed by an inherently high number of iterative steps. The Denoising Diffusion Generative Adversarial Networks (DDGAN) attempted to circumvent this limitation by integrating a GAN model for larger jumps in the diffusion process. However, DDGAN encountered scalability limitations when applied to large datasets. To address these limitations, we introduce a novel approach that tackles the problem by matching implicit and explicit factors. More specifically, our approach involves utilizing an implicit model to match the marginal distributions of noisy data and the explicit conditional distribution of the forward diffusion. This combination allows us to effectively match the joint denoising distributions. Unlike DDPM
    
[^163]: DCdetector: 双重关注对比表示学习用于时间序列异常检测

    DCdetector: Dual Attention Contrastive Representation Learning for Time Series Anomaly Detection. (arXiv:2306.10347v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10347](http://arxiv.org/abs/2306.10347)

    DCdetector是一种多尺度双重关注的对比表示学习模型，用于时间序列异常检测。它利用双重关注不对称设计和纯对比损失学习置换不变表示，从而有效区分异常样本。

    

    时间序列异常检测对于许多应用至关重要。其目标是在时间序列中识别出与正常样本分布有差异的异常样本。这个任务的最基本挑战是学习一个能够有效区分异常的表示映射。基于重建的方法仍然主导着该领域，但是使用异常数据进行表示学习可能会对性能产生负面影响。另一方面，对比学习旨在找到一种能够明显区分任何实例的表示，这可以为时间序列异常检测提供更自然和有前景的表示。在本文中，我们提出了DCdetector，一种多尺度双重关注的对比表示学习模型。DCdetector利用新颖的双重关注不对称设计创建置换环境，并使用纯对比损失来引导学习过程，从而学习一个具有优越性能的置换不变表示。

    Time series anomaly detection is critical for a wide range of applications. It aims to identify deviant samples from the normal sample distribution in time series. The most fundamental challenge for this task is to learn a representation map that enables effective discrimination of anomalies. Reconstruction-based methods still dominate, but the representation learning with anomalies might hurt the performance with its large abnormal loss. On the other hand, contrastive learning aims to find a representation that can clearly distinguish any instance from the others, which can bring a more natural and promising representation for time series anomaly detection. In this paper, we propose DCdetector, a multi-scale dual attention contrastive representation learning model. DCdetector utilizes a novel dual attention asymmetric design to create the permutated environment and pure contrastive loss to guide the learning process, thus learning a permutation invariant representation with superior d
    
[^164]: 从零开始实现红队对抗语言模型的探索与建立

    Explore, Establish, Exploit: Red Teaming Language Models from Scratch. (arXiv:2306.09442v1 [cs.CL])

    [http://arxiv.org/abs/2306.09442](http://arxiv.org/abs/2306.09442)

    本文提出了一种新的红队行动，通过从高层次、抽象的规范出发来考虑语言模型的行为，以探究模型的创新和贡献。

    

    部署大型语言模型（LLMs）可能会产生有害输出，例如有毒或不诚实陈述。先前的研究已经引入了工具以调查有害输出，以识别和减轻这些风险。虽然这是确保语言模型安全的有价值步骤，但这些方法通常依赖于现有的针对不希望的输出的分类器。这限制了它们在只有预先知道有害行为类型的情况下的应用。然而，这跳过了红队行动的核心挑战：开发模型可能展示的行为的上下文理解。此外，当这样的分类器已经存在时，红队行动的边际价值有限，因为分类器可以用于过滤训练数据或模型输出。本文考虑在假设对手从高级、抽象的不良行为规范出发的情况下进行红队行动。红队应该在精化/扩展此规范的同时对抗该模型。

    Deploying Large language models (LLMs) can pose hazards from harmful outputs such as toxic or dishonest speech. Prior work has introduced tools that elicit harmful outputs in order to identify and mitigate these risks. While this is a valuable step toward securing language models, these approaches typically rely on a pre-existing classifier for undesired outputs. This limits their application to situations where the type of harmful behavior is known with precision beforehand. However, this skips a central challenge of red teaming: developing a contextual understanding of the behaviors that a model can exhibit. Furthermore, when such a classifier already exists, red teaming has limited marginal value because the classifier could simply be used to filter training data or model outputs. In this work, we consider red teaming under the assumption that the adversary is working from a high-level, abstract specification of undesired behavior. The red team is expected to refine/extend this spec
    
[^165]: AVIDa-hIL6：基于一只被免疫羊驼的大规模VHH数据集用于预测抗原 - 抗体相互作用

    AVIDa-hIL6: A Large-Scale VHH Dataset Produced from an Immunized Alpaca for Predicting Antigen-Antibody Interactions. (arXiv:2306.03329v1 [cs.LG])

    [http://arxiv.org/abs/2306.03329](http://arxiv.org/abs/2306.03329)

    这是一个大规模的VHH数据集，用于预测抗原-抗体相互作用。通过利用VHHs的结构，该数据集包括573,891个抗原-VHH对，并且是当前公开数据集中最大、最全面的之一。

    

    抗体已经成为治疗人类疾病的重要药物。为了加速治疗抗体的发现，计算方法，特别是机器学习，已经引起了相当大的关注，用于预测在抗体候选和目标抗原（如病毒和细菌）之间的特定相互作用。然而，现有研究中公开可用的数据集具有明显的限制，如规模小，缺乏非结合样本和准确的氨基酸序列。为了克服这些限制，我们开发了AVIDa-hIL6, 一个大规模的VHH数据集，用于预测具有人类白细胞介素-6（IL-6）蛋白，作为抗原的VHHs的抗原 - 抗体相互作用。通过利用VHHs的简单结构，有利于通过DNA测序技术识别全长氨基酸序列，AVIDa-hIL6包含573,891个抗原 - VHH对，其中62,067对是结合样本。

    Antibodies have become an important class of therapeutic agents to treat human diseases. To accelerate therapeutic antibody discovery, computational methods, especially machine learning, have attracted considerable interest for predicting specific interactions between antibody candidates and target antigens such as viruses and bacteria. However, the publicly available datasets in existing works have notable limitations, such as small sizes and the lack of non-binding samples and exact amino acid sequences. To overcome these limitations, we have developed AVIDa-hIL6, a large-scale dataset for predicting antigen-antibody interactions in the variable domain of heavy chain of heavy chain antibodies (VHHs), produced from an alpaca immunized with the human interleukin-6 (IL-6) protein, as antigens. By leveraging the simple structure of VHHs, which facilitates identification of full-length amino acid sequences by DNA sequencing technology, AVIDa-hIL6 contains 573,891 antigen-VHH pairs with am
    
[^166]: DYffusion：面向时空预测的动态扩散模型

    DYffusion: A Dynamics-informed Diffusion Model for Spatiotemporal Forecasting. (arXiv:2306.01984v1 [cs.LG])

    [http://arxiv.org/abs/2306.01984](http://arxiv.org/abs/2306.01984)

    提出了一种新的扩散模型，其结合了数据中编码的时间动态，自然地编码了多步和长程预测能力，具有灵活的采样轨迹和折衷性能与加速采样的能力，同时提高了计算效率，可在时空预测方面取得竞争性表现。

    

    尽管扩散模型可以成功地生成数据和做出预测，但它们主要是为静态图像设计的。我们提出了一种方法，可以训练用于动态预测的扩散模型，利用编码在数据中的时间动态，直接将其与网络中的扩散步骤耦合。我们训练了一个随机的、时间条件的插值器和一个骨干预测网络，分别模仿传统扩散模型的前向和后向过程。这种设计选择自然地编码了多步和长程预测能力，允许高度灵活的连续时间采样轨迹，并在推理时能够折衷性能与加速采样的能力。此外，面向动态的扩散过程强加了强的归纳偏差，相比传统基于高斯噪声的扩散模型，可以提高计算效率。我们的方法在概率滑雪预测任务上表现出竞争力。

    While diffusion models can successfully generate data and make predictions, they are predominantly designed for static images. We propose an approach for training diffusion models for dynamics forecasting that leverages the temporal dynamics encoded in the data, directly coupling it with the diffusion steps in the network. We train a stochastic, time-conditioned interpolator and a backbone forecaster network that mimic the forward and reverse processes of conventional diffusion models, respectively. This design choice naturally encodes multi-step and long-range forecasting capabilities, allowing for highly flexible, continuous-time sampling trajectories and the ability to trade-off performance with accelerated sampling at inference time. In addition, the dynamics-informed diffusion process imposes a strong inductive bias, allowing for improved computational efficiency compared to traditional Gaussian noise-based diffusion models. Our approach performs competitively on probabilistic ski
    
[^167]: 组合优化中对称探索是免费的！

    Symmetric Exploration in Combinatorial Optimization is Free!. (arXiv:2306.01276v1 [cs.LG])

    [http://arxiv.org/abs/2306.01276](http://arxiv.org/abs/2306.01276)

    该论文提出了一种免费的技术，通过利用对称性提高了基于DRL的组合优化求解器的性能，无需额外的目标函数评估，适用于广泛的组合优化任务，并在多种任务上进行实证评估证实了其有效性。

    

    最近，深度强化学习（DRL）在解决组合优化（CO）问题方面已经显示出潜力。然而，他们经常需要大量的目标函数评估，这在现实场景中可能耗时。为了解决这个问题，我们提出了一种“免费”的技术，通过利用对称性来增强任何深度强化学习（DRL）求解器的性能，而无需额外的目标函数评估。我们的关键思想是通过保留奖励的变换来增强基于DRL的组合优化求解器的训练。该算法可能具有影响力，因为它简单，易于与现有求解器集成，并适用于广泛的组合优化任务。在NP难的路线优化，调度优化和新型分子优化的广泛实证评估结果表明，我们的方法轻松提高了最先进的DRL算法的样本效率。

    Recently, deep reinforcement learning (DRL) has shown promise in solving combinatorial optimization (CO) problems. However, they often require a large number of evaluations on the objective function, which can be time-consuming in real-world scenarios. To address this issue, we propose a "free" technique to enhance the performance of any deep reinforcement learning (DRL) solver by exploiting symmetry without requiring additional objective function evaluations. Our key idea is to augment the training of DRL-based combinatorial optimization solvers by reward-preserving transformations. The proposed algorithm is likely to be impactful since it is simple, easy to integrate with existing solvers, and applicable to a wide range of combinatorial optimization tasks. Extensive empirical evaluations on NP-hard routing optimization, scheduling optimization, and de novo molecular optimization confirm that our method effortlessly improves the sample efficiency of state-of-the-art DRL algorithms. Ou
    
[^168]: 分段循环Transformer:一种高效的序列到序列模型

    Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model. (arXiv:2305.16340v1 [cs.CL])

    [http://arxiv.org/abs/2305.16340](http://arxiv.org/abs/2305.16340)

    本文提出了一种分段循环Transformer（SRformer）来减少计算/内存成本，并使用RAF层处理跨段的信息，从而提高序列处理能力。

    

    Transformer在许多领域中表现出卓越的性能，包括语言和视觉。然而，随着序列长度的增加，它们的计算成本呈二次增长，使得它们在资源受限的应用中使用成为不可能。为了解决这个问题，我们的方法是将整个序列划分成若干段。然后使用具有循环结构的神经元来聚合跨段的信息，从而实现具有较低计算/内存成本的序列处理能力模型。为了验证这个想法，我们首先研究了使用局部Attention机制对单个段的影响。然后我们提出了一种分段循环Transformer（SRformer），它将分段Attention和循环Attention相结合。它使用循环accumulate and fire（RAF）层在相邻段之间处理信息。通过更新key的产品来补偿减少Attention窗口长度产生的误差。

    Transformers have shown dominant performance across a range of domains including language and vision. However, their computational cost grows quadratically with the sequence length, making their usage prohibitive for resource-constrained applications. To counter this, our approach is to divide the whole sequence into segments. The information across segments can then be aggregated using neurons with recurrence leveraging their inherent memory. Such an approach leads to models with sequential processing capability at a lower computation/memory cost. To investigate this idea, first, we examine the effects of using local attention mechanism on the individual segments. Then we propose a segmented recurrent transformer (SRformer) that combines segmented attention with recurrent attention. It uses recurrent accumulate and fire (RAF) layers to process information between consecutive segments. The loss caused by reducing the attention window length is compensated by updating the product of key
    
[^169]: FActScore: 对长文本生成中事实准确性的细粒度原子评估

    FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. (arXiv:2305.14251v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14251](http://arxiv.org/abs/2305.14251)

    本文介绍了一种称为FACTSCORE的评估方法，它通过将生成的内容分解为原子事实，并计算被可靠知识源支持的比例来评估大型语言模型生成的长文本的事实准确性。通过广泛的人工评估，我们发现商业语言模型中仅有58%的ChatGPT传记达到了高水平的事实准确性。此外，我们还引入了一种自动化模型，利用检索和强语言模型估计FACTSCORE，误差率低于2%。

    

    评估大型语言模型生成的长文本的事实性是一项棘手的任务，因为（1）生成的内容通常包含支持和不支持的信息，使得二元判断质量不足，（2）人工评估耗时且成本高。本文介绍了FACTSCORE，一种新的评估方法，它将生成内容分解为一系列原子事实，并计算被可靠知识源支持的原子事实的百分比。我们进行了广泛的人工评估，得出了几个最先进商业语言模型（InstructGPT、ChatGPT和增强提取PerplexityAI）生成的人物传记的FACTSCORE，并报道了新的分析结果，证明了对于这样的细粒度评分的需求（例如，ChatGPT只达到58%）。由于人工评估费时费力，我们还引入了一种使用检索和强语言模型估计FACTSCORE的自动化模型，误差率不超过2%。

    Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FACTSCORE, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain FACTSCOREs of people biographies generated by several state-of-the-art commercial LMs -- InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI -- and report new analysis demonstrating the need for such a fine-grained score (e.g., ChatGPT only achieves 58%). Since human evaluation is costly, we also introduce an automated model that estimates FACTSCORE using retrieval and a strong language model, with less than a 2% error rate. Finally, w
    
[^170]: 编辑大型语言模型：问题、方法和机会

    Editing Large Language Models: Problems, Methods, and Opportunities. (arXiv:2305.13172v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13172](http://arxiv.org/abs/2305.13172)

    本文深入探讨了编辑大型语言模型的问题、方法和机会，提供了任务定义和挑战的概述、先进方法的实证分析，以及构建了新的基准数据集。这些结果有助于改进LLMs的编辑技术，提高其效果和可行性。

    

    尽管能够训练出表现优秀的大型语言模型（LLMs），但其保持相关性和纠正错误的方法仍然难以确定。为此，最近几年出现了许多编辑LLMs的技术，其目标是在特定领域内高效地改变LLMs的行为，同时不对其他输入的性能产生负面影响。本文深入探讨了与LLMs模型编辑相关的问题、方法和机会。特别是，我们提供了关于模型编辑任务定义和相关挑战的全面概述，以及对目前最先进的方法的深入实证分析。我们还构建了一个新的基准数据集，以促进更强大的评估，并指出现有技术固有的持久问题。我们的目标是为每种编辑技术的效果和可行性提供有价值的见解，从而帮助社区在LLMs的管理中取得更好的结果。

    Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive. To this end, the past few years have witnessed a surge in techniques for editing LLMs, the objective of which is to efficiently alter the behavior of LLMs within a specific domain without negatively impacting performance across other inputs. This paper embarks on a deep exploration of the problems, methods, and opportunities related to model editing for LLMs. In particular, we provide an exhaustive overview of the task definition and challenges associated with model editing, along with an in-depth empirical analysis of the most progressive methods currently at our disposal. We also build a new benchmark dataset to facilitate a more robust evaluation and pinpoint enduring issues intrinsic to existing techniques. Our objective is to provide valuable insights into the effectiveness and feasibility of each editing technique, thereby assisting the community in ma
    
[^171]: BertRLFuzzer: 一种基于BERT和强化学习的Fuzzer

    BertRLFuzzer: A BERT and Reinforcement Learning based Fuzzer. (arXiv:2305.12534v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2305.12534](http://arxiv.org/abs/2305.12534)

    BertRLFuzzer是一种基于BERT和强化学习的Fuzzer工具，旨在发现Web应用程序的安全漏洞。通过使用BERT模型作为代理来指导Fuzzer进行高效学习，BertRLFuzzer相对于其他黑盒和白盒Fuzzer在时间到首次攻击、新漏洞发现和攻击率方面都取得了显著的改进。

    

    本文介绍了一种新颖的工具BertRLFuzzer，它是一种基于BERT和强化学习的Fuzzer，旨在发现Web应用程序的安全漏洞。BertRLFuzzer的工作原理如下：给定一组种子输入，Fuzzer对它们执行遵循语法并引发攻击的变异操作，以生成候选攻击向量。BertRLFuzzer的关键洞察是使用BERT模型作为代理来指导Fuzzer高效学习遵循语法和引发攻击的变异操作符。为了验证BertRLFuzzer的有效性，我们将其与共计13个黑盒和白盒Fuzzer在9个受害者网站的基准测试中进行比较，涉及超过16K行的源代码。相对于最接近的竞争工具，我们观察到时间到首次攻击的显著改进（减少54％），发现的新漏洞（17个新漏洞）和攻击率（生成的攻击向量增加了4.4％）。

    We present a novel tool BertRLFuzzer, a BERT and Reinforcement Learning (RL) based fuzzer aimed at finding security vulnerabilities for Web applications. BertRLFuzzer works as follows: given a set of seed inputs, the fuzzer performs grammar-adhering and attack-provoking mutation operations on them to generate candidate attack vectors. The key insight of BertRLFuzzer is the use of RL with a BERT model as an agent to guide the fuzzer to efficiently learn grammar-adhering and attack-provoking mutation operators. In order to establish the efficacy of BertRLFuzzer we compare it against a total of 13 black box and white box fuzzers over a benchmark of 9 victim websites with over 16K LOC. We observed a significant improvement, relative to the nearest competing tool, in terms of time to first attack (54% less), new vulnerabilities found (17 new vulnerabilities), and attack rate (4.4% more attack vectors generated).
    
[^172]: Clifford群等变神经网络

    Clifford Group Equivariant Neural Networks. (arXiv:2305.11141v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.11141](http://arxiv.org/abs/2305.11141)

    我们引入了Clifford群等变神经网络，它可以构建O(n)和E(n)等变模型。该方法通过调整Clifford群的定义以及保持向量空间和乘法结构的作用来实现多个有利属性。

    

    我们引入了Clifford群等变神经网络：一种构建O(n)和E(n)等变模型的新方法。我们确定并研究了Clifford群，它是Clifford代数中的一个子群，其定义经过调整以实现多个有利属性。主要地，该群的作用形成了一个正交自同构，扩展到整个Clifford代数，同时尊重多矢分级。这导致了对应于多矢分解的多个非等价子表示。此外，我们证明该作用不仅尊重Clifford代数的向量空间结构，还尊重其乘法结构，即几何乘积。这些发现意味着我们可以得到在任意维的内积空间中优雅地推广的表达层。我们特别展示了从一个sin

    We introduce Clifford Group Equivariant Neural Networks: a novel approach for constructing $\mathrm{O}(n)$- and $\mathrm{E}(n)$-equivariant models. We identify and study the $\textit{Clifford group}$, a subgroup inside the Clifford algebra whose definition we adjust to achieve several favorable properties. Primarily, the group's action forms an orthogonal automorphism that extends beyond the typical vector space to the entire Clifford algebra while respecting the multivector grading. This leads to several non-equivalent subrepresentations corresponding to the multivector decomposition. Furthermore, we prove that the action respects not just the vector space structure of the Clifford algebra but also its multiplicative structure, i.e., the geometric product. These findings imply that every polynomial in multivectors, An advantage worth mentioning is that we obtain expressive layers that can elegantly generalize to inner-product spaces of any dimension. We demonstrate, notably from a sin
    
[^173]: 基于预训练模型的等变小样本学习

    Equivariant Few-Shot Learning from Pretrained Models. (arXiv:2305.09900v1 [cs.LG])

    [http://arxiv.org/abs/2305.09900](http://arxiv.org/abs/2305.09900)

    本文提出了一种基于预训练模型的$\lambda$-\textit{equitune}方法，它使用\textit{重要性权重}$\lambda$对特征进行平均，可以显著提高等变小样本学习的表现。

    

    高效的迁移学习算法是基础模型在有限数据情况下在各种下游任务上取得成功的关键。最近的作品 \cite{basu2022equi} 和 \cite{kaba2022equivariance} 分别提出了使用从群变换输入得到的特征的群平均值（\textit{equitune}）和基于优化的方法来从不等变的神经网络获取等变输出。虽然 \cite{kaba2022equivariance} 只关注从头开始训练，但我们发现即使在良好的微调结果下，\textit{equitune} 在等变零样本任务上表现不佳。我们认为这是因为预训练模型为某些转换提供了更高质量的特征，而对其进行简单平均会产生不良影响。因此，我们提出了一种使用\textit{重要性权重}$\lambda$对特征进行平均的$\lambda$-\textit{equitune} 方法。这些权重是使用一个小型神经网络直接从数据中学习的，从而导致出色的零样本和微调结果。

    Efficient transfer learning algorithms are key to the success of foundation models on diverse downstream tasks even with limited data. Recent works of \cite{basu2022equi} and \cite{kaba2022equivariance} propose group averaging (\textit{equitune}) and optimization-based methods, respectively, over features from group-transformed inputs to obtain equivariant outputs from non-equivariant neural networks. While \cite{kaba2022equivariance} are only concerned with training from scratch, we find that equitune performs poorly on equivariant zero-shot tasks despite good finetuning results. We hypothesize that this is because pretrained models provide better quality features for certain transformations than others and simply averaging them is deleterious. Hence, we propose $\lambda$-\textit{equitune} that averages the features using \textit{importance weights}, $\lambda$s. These weights are learned directly from the data using a small neural network, leading to excellent zero-shot and finetuned 
    
[^174]: 预训练语言模型的知识反思

    Knowledge Rumination for Pre-trained Language Models. (arXiv:2305.08732v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.08732](http://arxiv.org/abs/2305.08732)

    本文提出了一种名为知识反思的新范式，旨在帮助预训练语言模型利用已经编码在其预训练参数中的相关潜在知识，而不需要从外部语料库中检索。这种方法通过在模型中添加提示，并将相关知识注入模型进行整合，取得了在常识推理任务和GLUE基准上的实验结果。

    

    先前的研究揭示了普通的预训练语言模型（PLMs）单独处理知识密集型NLP任务的能力不足，因此，一些工作尝试将外部知识集成到PLMs中。然而，尽管有着有前途的结果，但我们经验性地观察到，PLM可能已经在其预训练参数中编码了丰富的知识，但在应用到知识密集型任务时未能充分利用它们。在本文中，我们提出了一种名为知识反思的新范式，以帮助预训练语言模型利用相关的潜在知识，而不需要从外部语料库中检索它们。通过简单地在PLMs中添加一个如“据我所知”的提示，我们试图回顾相关的潜在知识，并将其注入模型以进行知识整合。我们将提出的知识反思应用于各种语言模型，包括RoBERTa、DeBERTa和GPT-3。在六个常识推理任务和GLUE基准上的实验结果显示.....

    Previous studies have revealed that vanilla pre-trained language models (PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus, several works have attempted to integrate external knowledge into PLMs. However, despite the promising outcome, we empirically observe that PLMs may have already encoded rich knowledge in their pre-trained parameters but fail to fully utilize them when applying them to knowledge-intensive tasks. In this paper, we propose a new paradigm dubbed Knowledge Rumination to help the pre-trained language model utilize that related latent knowledge without retrieving it from the external corpus. By simply adding a prompt like "As far as I know" to the PLMs, we try to review related latent knowledge and inject them back into the model for knowledge consolidation. We apply the proposed knowledge rumination to various language models, including RoBERTa, DeBERTa, and GPT-3. Experimental results on six commonsense reasoning tasks and GLUE benchmarks dem
    
[^175]: TidyBot: 应用大语言模型的个性化机器人物理辅助

    TidyBot: Personalized Robot Assistance with Large Language Models. (arXiv:2305.05658v1 [cs.RO])

    [http://arxiv.org/abs/2305.05658](http://arxiv.org/abs/2305.05658)

    本文研究了使用机器人进行家庭清扫的个性化问题。通过使用大型语言模型少样本摘要能力，机器人可以学习用户的偏好并将其推广到未来的场景中，从而实现快速适应。

    

    为了使机器人能够有效个性化地提供物理辅助，它必须学习用户的个人喜好并将其应用于未来的场景中。本文研究了使用机器人进行家庭清扫的个性化问题，这些机器人能够通过捡起物品并将其放回原处来整理房间。一个关键的挑战是确定每个物品的正确位置，因为人们的喜好可以因个人品味或文化背景而大不相同。例如，一个人可能喜欢把衬衫放在抽屉里，而另一个人可能喜欢把衬衫放在架子上。我们旨在建立系统，这些系统可以通过与特定人的先前交互学习这样的喜好，而只需要几个示例。我们展示了机器人可以将基于语言的规划和感知与大型语言模型(LLMs)的少样本摘要能力相结合，从而推断出广泛适用于未来交互的用户偏好。这种方法实现了快速适应，并取得了91.2%的准确率。

    For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios. In this work, we investigate personalization of household cleanup with robots that can tidy up rooms by picking up objects and putting them away. A key challenge is determining the proper place to put each object, as people's preferences can vary greatly depending on personal taste or cultural background. For instance, one person may prefer storing shirts in the drawer, while another may prefer them on the shelf. We aim to build systems that can learn such preferences from just a handful of examples via prior interactions with a particular person. We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions. This approach enables fast adaptation and achieves 91.2% accurac
    
[^176]: 鲁棒性张量CUR分解：对稀疏损坏进行快速低Tucker秩张量恢复。

    Robust Tensor CUR Decompositions: Rapid Low-Tucker-Rank Tensor Recovery with Sparse Corruption. (arXiv:2305.04080v1 [math.NA])

    [http://arxiv.org/abs/2305.04080](http://arxiv.org/abs/2305.04080)

    本文提出了一种快速算法，称为鲁棒性张量CUR分解（RTCUR），用于Tucker秩设置下的大规模非凸TRPCA问题，通过交替投影的框架和张量CUR分解，快速实现对稀疏损坏的低秩张量恢复，并在实际数据集上展示了该方法的有效性和计算优势。

    

    我们研究了张量鲁棒主成分分析（TRPCA）问题，它是矩阵鲁棒主成分分析（RPCA）的张量扩展，旨在将给定的张量分解为基础低秩分量和稀疏异常分量。本文提出了一种快速算法，称为鲁棒性张量CUR分解（RTCUR），用于Tucker秩设置下的大规模非凸TRPCA问题。RTCUR是在交替投影的框架下开发出来的，它在低秩张量集和稀疏张量集之间进行投影。我们利用最近开发的张量CUR分解，在每个投影中大大降低了计算复杂度。此外，我们为不同的应用场景开发了四个RTCUR变体。我们在合成和实际数据集上展示了RTCUR的有效性和计算优势，以对抗最先进的方法。

    We study the tensor robust principal component analysis (TRPCA) problem, a tensorial extension of matrix robust principal component analysis (RPCA), that aims to split the given tensor into an underlying low-rank component and a sparse outlier component. This work proposes a fast algorithm, called Robust Tensor CUR Decompositions (RTCUR), for large-scale non-convex TRPCA problems under the Tucker rank setting. RTCUR is developed within a framework of alternating projections that projects between the set of low-rank tensors and the set of sparse tensors. We utilize the recently developed tensor CUR decomposition to substantially reduce the computational complexity in each projection. In addition, we develop four variants of RTCUR for different application settings. We demonstrate the effectiveness and computational advantages of RTCUR against state-of-the-art methods on both synthetic and real-world datasets.
    
[^177]: 量子计算中用于ReLU网络鲁棒性的高效MILP分解

    Efficient MILP Decomposition in Quantum Computing for ReLU Network Robustness. (arXiv:2305.00472v1 [quant-ph])

    [http://arxiv.org/abs/2305.00472](http://arxiv.org/abs/2305.00472)

    本研究提出了一种用于ReLU网络鲁棒性的新型MILP量子分解方法，尤其针对解决量子位可用性、噪声和误差限制，可实现更高的成功率和更高的效率与精确度，对量子计算技术的发展有重要意义。

    

    新兴的量子计算技术，如嘈杂中间规模量子（NISQ）设备，为解决数学优化问题提供了潜在的进展。然而，量子位的可用性、噪声和误差的限制对实际实施构成了挑战。在本研究中，我们研究了两种MILP分解方法，旨在减少原始问题大小并更高效地利用可用的NISQ设备。我们重点关注将原始问题分解成更小的子问题，然后使用组合的量子-经典硬件方法迭代地解决这些子问题。我们对Benders和Dantzig-Wolfe方法的MILP分解进行了详细分析。在我们的分析中，我们表明，在最坏情况下，解决Benders所需的量子比特数呈指数增长，而Dantzig-Wolfe的量子比特数保持不变。此外，我们利用Dantzig-Wolfe分解对证明ReLU网络对抗攻击的鲁棒性时的使用案例。我们的实验结果表明，所提出的量子MILP分解方法可以实现更高的成功率，并在效率和准确性方面优于经典优化方法。

    Emerging quantum computing technologies, such as Noisy Intermediate-Scale Quantum (NISQ) devices, offer potential advancements in solving mathematical optimization problems. However, limitations in qubit availability, noise, and errors pose challenges for practical implementation. In this study, we examine two decomposition methods for Mixed-Integer Linear Programming (MILP) designed to reduce the original problem size and utilize available NISQ devices more efficiently. We concentrate on breaking down the original problem into smaller subproblems, which are then solved iteratively using a combined quantum-classical hardware approach. We conduct a detailed analysis for the decomposition of MILP with Benders and Dantzig-Wolfe methods. In our analysis, we show that the number of qubits required to solve Benders is exponentially large in the worst-case, while remains constant for Dantzig-Wolfe. Additionally, we leverage Dantzig-Wolfe decomposition on the use-case of certifying the robustn
    
[^178]: 一项多模态模型融合的实证研究

    An Empirical Study of Multimodal Model Merging. (arXiv:2304.14933v1 [cs.CV])

    [http://arxiv.org/abs/2304.14933](http://arxiv.org/abs/2304.14933)

    本研究通过融合在不同模态上训练的transformer进行多模态模型融合，并提出一种参数有效的模态不可知架构，形成有效的训练配方。

    

    模型融合（例如插值或任务算术）将在不同任务上训练的多个模型合并以生成多任务解决方案。该技术在先前的研究中已经被证明成功，其中模型是在相似的任务和相同的初始化下训练的。在本文中，我们通过将在不同模态上训练的transformer进行融合，将此概念扩展到多模态设置。此外，我们针对一个新颖的目标进行研究，在该目标中，我们可以将视觉、语言和跨模态的transformer合并到特定模态的架构中，以创建一个参数有效的模态不可知架构。通过全面的实验，我们系统地研究了影响模型融合后性能的关键因素，包括初始化、融合机制和模型架构。我们的分析得出了一个有效的训练配方，可以通过模型融合来匹配模态不可知基线的性能（即从头开始预训练）。我们的代码可供使用。

    Model merging (e.g., via interpolation or task arithmetic) fuses multiple models trained on different tasks to generate a multi-task solution. The technique has been proven successful in previous studies, where the models are trained on similar tasks and with the same initialization. In this paper, we expand on this concept to a multimodal setup by merging transformers trained on different modalities. Furthermore, we conduct our study for a novel goal where we can merge vision, language, and cross-modal transformers of a modality-specific architecture to create a parameter-efficient modality-agnostic architecture. Through comprehensive experiments, we systematically investigate the key factors impacting model performance after merging, including initialization, merging mechanisms, and model architectures. Our analysis leads to an effective training recipe for matching the performance of the modality-agnostic baseline (i.e. pre-trained from scratch) via model merging. Our code is availa
    
[^179]: 无监督学习局部和全局特征用于视频对齐

    Video alignment using unsupervised learning of local and global features. (arXiv:2304.06841v1 [cs.CV])

    [http://arxiv.org/abs/2304.06841](http://arxiv.org/abs/2304.06841)

    本文提出了一种无需训练的视频对齐方法，利用全局和局部特征将帧转化为时间序列并使用对角化动态时间规整算法进行对齐。

    

    本文致力于解决视频对齐的问题，即匹配包含相似活动的一对视频的帧。视频对齐的主要挑战在于，尽管两个视频之间的执行过程和外观有所不同，但仍需要建立精确的对应关系。我们提出了一种使用帧的全局和局部特征进行对齐的无监督方法。特别地，我们利用人物检测、姿态估计和VGG网络三种机器视觉工具为每个视频帧引入有效的特征。然后对这些特征进行处理和组合以构建代表视频的多维时间序列。使用一种名为对角化动态时间规整的新版本（Diagonalized Dynamic Time Warping, DDTW）对生成的时间序列进行对齐。我们的方法的主要优点在于不需要任何训练，因此适用于任何新类型的活动而无需处理。

    In this paper, we tackle the problem of video alignment, the process of matching the frames of a pair of videos containing similar actions. The main challenge in video alignment is that accurate correspondence should be established despite the differences in the execution processes and appearances between the two videos. We introduce an unsupervised method for alignment that uses global and local features of the frames. In particular, we introduce effective features for each video frame by means of three machine vision tools: person detection, pose estimation, and VGG network. Then the features are processed and combined to construct a multidimensional time series that represent the video. The resulting time series are used to align videos of the same actions using a novel version of dynamic time warping named Diagonalized Dynamic Time Warping(DDTW). The main advantage of our approach is that no training is required, which makes it applicable for any new type of action without any need
    
[^180]: 基于多核重对比损失的IMU定向估计：梯度下降法

    Multi-kernel Correntropy-based Orientation Estimation of IMUs: Gradient Descent Methods. (arXiv:2304.06548v1 [eess.SY])

    [http://arxiv.org/abs/2304.06548](http://arxiv.org/abs/2304.06548)

    本文提出了两种基于重对比损失的算法，用于IMU定向估计，相比传统方法具有更好的准确性、鲁棒性和计算效率。

    

    本文介绍了两个计算效率高的算法，用于惯性测量单元（IMUs）的定向估计：重对比梯度下降（CGD）和重对比解耦定向估计（CDOE）。传统方法，如梯度下降（GD）和解耦定向估计（DOE），依赖于均方误差（MSE）准则，使其容易受到外部加速度和磁干扰的影响。为解决这个问题，我们证明了当噪声遵循一种重尾分布时，多核重对比损失（MKCL）是最优的极大似然估计（MLE）目标函数。在某些情况下，即使存在任意大的离群值，MKCL的估计误差也是有界的。通过用MKCL替换标准MSE成本函数，我们开发了CGD和CDOE算法。我们通过比较这些方法在各种传感器设置和运动场景中的表现来评估它们的有效性。实验结果表明，CGD和CDOE在精度、健壮性和计算效率方面均达到了卓越的表现。

    This paper presents two computationally efficient algorithms for the orientation estimation of inertial measurement units (IMUs): the correntropy-based gradient descent (CGD) and the correntropy-based decoupled orientation estimation (CDOE). Traditional methods, such as gradient descent (GD) and decoupled orientation estimation (DOE), rely on the mean squared error (MSE) criterion, making them vulnerable to external acceleration and magnetic interference. To address this issue, we demonstrate that the multi-kernel correntropy loss (MKCL) is an optimal objective function for maximum likelihood estimation (MLE) when the noise follows a type of heavy-tailed distribution. In certain situations, the estimation error of the MKCL is bounded even in the presence of arbitrarily large outliers. By replacing the standard MSE cost function with MKCL, we develop the CGD and CDOE algorithms. We evaluate the effectiveness of our proposed methods by comparing them with existing algorithms in various s
    
[^181]: SGDP: 一种基于流图神经网络的数据预取器

    SGDP: A Stream-Graph Neural Network Based Data Prefetcher. (arXiv:2304.03864v1 [cs.OS])

    [http://arxiv.org/abs/2304.03864](http://arxiv.org/abs/2304.03864)

    SGDP是一种新型基于流图神经网络的数据预取器，通过建模LBA增量流并使用图神经网络提取混合特征来进行数据预取，实验结果表明它优于其他方法。

    

    数据预取对于存储系统优化和访问性能提升非常重要。传统的预取器适用于挖掘顺序逻辑块地址（LBA）的访问模式，但是无法处理现实世界中普遍存在的复杂非顺序模式。最先进的基于学习的预取器覆盖了更多的LBA访问，但是它们不足以充分考虑LBA增量之间的空间依赖关系，导致性能和鲁棒性有限。本文提出了一种新型的基于流图神经网络的数据预取器（SGDP）。具体来说，SGDP使用加权有向图结构来建模LBA增量流以表示LBA增量之间的交互关系，并通过图神经网络进一步提取混合特征进行数据预取。我们对八个真实世界的数据集进行了广泛的实验。实证结果验证了SGDP在命中率方面优于SOTA方法6.21％，效果也优于其他方法。

    Data prefetching is important for storage system optimization and access performance improvement. Traditional prefetchers work well for mining access patterns of sequential logical block address (LBA) but cannot handle complex non-sequential patterns that commonly exist in real-world applications. The state-of-the-art (SOTA) learning-based prefetchers cover more LBA accesses. However, they do not adequately consider the spatial interdependencies between LBA deltas, which leads to limited performance and robustness. This paper proposes a novel Stream-Graph neural network-based Data Prefetcher (SGDP). Specifically, SGDP models LBA delta streams using a weighted directed graph structure to represent interactive relations among LBA deltas and further extracts hybrid features by graph neural networks for data prefetching. We conduct extensive experiments on eight real-world datasets. Empirical results verify that SGDP outperforms the SOTA methods in terms of the hit ratio by 6.21%, the effe
    
[^182]: PAIR-Diffusion: 采用结构和外观配对扩散模型进行对象级图像编辑

    PAIR-Diffusion: Object-Level Image Editing with Structure-and-Appearance Paired Diffusion Models. (arXiv:2303.17546v1 [cs.CV])

    [http://arxiv.org/abs/2303.17546](http://arxiv.org/abs/2303.17546)

    本论文提出了一种采用结构和外观配对扩散模型进行对象级图像编辑的方法，使用户能够精细控制图像中的不同对象属性，同时自动传播注入的外观到具有相似结构的对象。

    

    最近，使用扩散模型进行图像编辑发展迅速。以前的作品可以通过各种方式进行控制和编辑图像，某些作品使用高级条件（例如文本），而其他作品使用低级条件。然而，大多数作品缺乏对图像中不同对象的属性进行精细化控制，即对象级图像编辑。本文将图像视为由多个对象组成，每个对象由不同属性定义。我们发现结构和外观是最直观且最有用于编辑的属性。我们提出了结构和外观配对扩散模型（PAIR-Diffusion），该模型使用从图像中明确提取的结构和外观信息进行训练。所提出的模型使用户能够在对象和全局级别将参考图像的外观注入输入图像中。此外，PAIR-Diffusion自动将注入的外观传播到输入图像中具有类似结构的对象。

    Image editing using diffusion models has witnessed extremely fast-paced growth recently. There are various ways in which previous works enable controlling and editing images. Some works use high-level conditioning such as text, while others use low-level conditioning. Nevertheless, most of them lack fine-grained control over the properties of the different objects present in the image, i.e. object-level image editing. In this work, we consider an image as a composition of multiple objects, each defined by various properties. Out of these properties, we identify structure and appearance as the most intuitive to understand and useful for editing purposes. We propose Structure-and-Appearance Paired Diffusion model (PAIR-Diffusion), which is trained using structure and appearance information explicitly extracted from the images. The proposed model enables users to inject a reference image's appearance into the input image at both the object and global levels. Additionally, PAIR-Diffusion a
    
[^183]: 随机梯度下降的概率稳定性

    The Probabilistic Stability of Stochastic Gradient Descent. (arXiv:2303.13093v1 [cs.LG])

    [http://arxiv.org/abs/2303.13093](http://arxiv.org/abs/2303.13093)

    本文重新定义了随机梯度下降的稳定性，并使用概率稳定性来回答深度学习理论中的一个根本问题：SGD如何从数量庞大的可能严重过拟合的解中选择有意义的神经网络解。

    

    深度学习理论中的一个基本开放问题是如何定义和理解随机梯度下降(SGD)接近固定点的稳定性。传统文献依赖于参数统计矩，特别是参数方差的收敛来量化稳定性。本文重新定义了SGD的稳定性，并使用\textit{概率收敛}条件来定义SGD的\textit{概率稳定性}。提出的稳定性直接回答了深度学习理论中的一个根本问题：SGD如何从数量庞大的可能严重过拟合的解中选择有意义的神经网络解。为了实现这一点，我们表明只有在概率性稳定性的镜头下，SGD才表现出丰富而实际相关的学习阶段，如完全失去稳定性阶段、不正确学习阶段、收敛到低秩鞍点阶段和正确学习阶段。当应用于神经网络时，这些相图意味着具有实际意义的稳定和不稳定区域。

    A fundamental open problem in deep learning theory is how to define and understand the stability of stochastic gradient descent (SGD) close to a fixed point. Conventional literature relies on the convergence of statistical moments, esp., the variance, of the parameters to quantify the stability. We revisit the definition of stability for SGD and use the \textit{convergence in probability} condition to define the \textit{probabilistic stability} of SGD. The proposed stability directly answers a fundamental question in deep learning theory: how SGD selects a meaningful solution for a neural network from an enormous number of solutions that may overfit badly. To achieve this, we show that only under the lens of probabilistic stability does SGD exhibit rich and practically relevant phases of learning, such as the phases of the complete loss of stability, incorrect learning, convergence to low-rank saddles, and correct learning. When applied to a neural network, these phase diagrams imply t
    
[^184]: 知识图谱的构建：现状和挑战

    Construction of Knowledge Graphs: State and Challenges. (arXiv:2302.11509v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.11509](http://arxiv.org/abs/2302.11509)

    知识图谱构建面临着增量更新和步骤之间相互作用的挑战。本文讨论了主要图模型和构建流程需求，对构建高质量知识图谱的必要步骤进行了概述，并评估了现有知识图谱构建的研究现状。

    

    随着知识图谱在诸多应用中扮演着重要角色，如推荐系统和问答系统，构建和持续更新知识图谱的通用流程的需求正在增加。虽然从非结构化数据（如文本）和结构化数据源（如数据库）创建知识图谱的各个步骤已经得到了广泛研究，但是目前对于增量更新知识图谱和各个步骤之间的相互作用的研究还很有限。本文首先讨论了知识图谱的主要图模型，并介绍了未来知识图谱构建流程的主要需求。然后，我们概述了构建高质量知识图谱所需的必要步骤，包括元数据管理、本体发展和质量保证等交叉主题。最后，我们评估了与已介绍的需求相一致的特定知识图谱的构建现状。

    With knowledge graphs (KGs) at the center of numerous applications such as recommender systems and question answering, the need for generalized pipelines to construct and continuously update such KGs is increasing. While the individual steps that are necessary to create KGs from unstructured (e.g. text) and structured data sources (e.g. databases) are mostly well-researched for their one-shot execution, their adoption for incremental KG updates and the interplay of the individual steps have hardly been investigated in a systematic manner so far. In this work, we first discuss the main graph models for KGs and introduce the major requirement for future KG construction pipelines. Next, we provide an overview of the necessary steps to build high-quality KGs, including cross-cutting topics such as metadata management, ontology development, and quality assurance. We then evaluate the state of the art of KG construction w.r.t the introduced requirements for specific popular KGs as well as so
    
[^185]: 学习可以遵守守恒定律的物理模型

    Learning Physical Models that Can Respect Conservation Laws. (arXiv:2302.11002v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11002](http://arxiv.org/abs/2302.11002)

    这项工作提出了ProbConserv框架，通过将守恒约束与贝叶斯更新相结合，将守恒约束纳入通用科学机器学习体系结构中，以便在学习高难度的PDE运算中应用。

    

    科学机器学习的最近一些工作集中在将偏微分方程（PDE）信息融入学习过程中。其中许多工作集中在相对“容易”的PDE算子（例如椭圆和抛物线）上，而相对“困难”的PDE算子（例如双曲线）则较少。在数值PDE方面，后一种问题类需要控制一种体积元素或守恒约束类型，这被视为具有挑战性的问题。为了实现科学机器学习的承诺，需要无缝地将这两种类型的问题融入学习过程中。

    Recent work in scientific machine learning (SciML) has focused on incorporating partial differential equation (PDE) information into the learning process. Much of this work has focused on relatively ``easy'' PDE operators (e.g., elliptic and parabolic), with less emphasis on relatively ``hard'' PDE operators (e.g., hyperbolic). Within numerical PDEs, the latter problem class requires control of a type of volume element or conservation constraint, which is known to be challenging. Delivering on the promise of SciML requires seamlessly incorporating both types of problems into the learning process. To address this issue, we propose ProbConserv, a framework for incorporating conservation constraints into a generic SciML architecture. To do so, ProbConserv combines the integral form of a conservation law with a Bayesian update. We provide a detailed analysis of ProbConserv on learning with the Generalized Porous Medium Equation (GPME), a widely-applicable parameterized family of PDEs that 
    
[^186]: 学习少量数据：关于使用景观分析来预测软件项目健康状况的价值

    Learning from Very Little Data: On the Value of Landscape Analysis for Predicting Software Project Health. (arXiv:2301.06577v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2301.06577](http://arxiv.org/abs/2301.06577)

    该论文探讨了在数据稀缺的情况下，利用景观分析来预测软件项目健康状况的价值。通过对数据进行聚类并选择更好的学习控制参数，通过niSNEAK工具能够比之前的方法更快且更有效地找到更准确的配置，减少了预测错误。

    

    当数据稀缺时，软件分析可能会出现许多错误。例如，考虑学习开源项目健康状况的预测器（例如，在十二个月后的已关闭拉取请求的数量）。这个任务的训练数据可能非常少（例如，五年的数据，每个月收集一次，只有60行的训练数据）。从如此小的数据集生成的模型可能会产生许多预测错误。这些错误可以通过选择更好的学习控制参数进行“景观分析”来解决。我们的niSNEAK工具(a)~对数据进行聚类，以找到超参数的一般景观；然后(b)~从每个部分中探索几个代表性样本。niSNEAK比之前的最新超参数优化算法（例如FLASH、HYPEROPT、OPTUNA）更快且更有效。niSNEAK找到的配置错误远远少于其他方法。例如，对于项目健康指标（如$C$=提交次数；$I$=已关闭的拉取请求次数），niSNEAK的错误要少得多。

    When data is scarce, software analytics can make many mistakes. For example, consider learning predictors for open source project health (e.g. the number of closed pull requests in twelve months time). The training data for this task may be very small (e.g. five years of data, collected every month means just 60 rows of training data). The models generated from such tiny data sets can make many prediction errors.  Those errors can be tamed by a {\em landscape analysis} that selects better learner control parameters. Our niSNEAK tool (a)~clusters the data to find the general landscape of the hyperparameters; then (b)~explores a few representatives from each part of that landscape. niSNEAK is both faster and more effective than prior state-of-the-art hyperparameter optimization algorithms (e.g. FLASH, HYPEROPT, OPTUNA).  The configurations found by niSNEAK have far less error than other methods. For example, for project health indicators such as $C$= number of commits; $I$=number of clos
    
[^187]: 基于场景和对象的图像-文本跨模态检索：一项可复现性研究

    Scene-centric vs. Object-centric Image-Text Cross-modal Retrieval: A Reproducibility Study. (arXiv:2301.05174v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2301.05174](http://arxiv.org/abs/2301.05174)

    这项研究关注基于场景和对象的图像-文本跨模态检索的可复现性，通过选择不同体系结构的最先进模型并在不同类型的数据集上进行评估，探讨了其在不同数据集类型上的泛化能力。

    

    大多数跨模态检索（CMR）方法要么聚焦于以对象为中心的数据集，即每个文档描绘或描述一个单一对象，要么聚焦于以场景为中心的数据集，即每个图像描绘或描述相互关联的多个对象和关系的复杂场景。我们认为一个强大的CMR模型应该在两种数据集类型上都具有良好的泛化能力。尽管CMR取得了一些进展，但结果的可复现性及其在不同数据集类型上的泛化性尚未被研究过。我们填补了这个空白，并关注当在以对象为中心和以场景为中心的数据集上评估时，最先进的CMR结果的可复现性。我们选择了两种具有不同体系结构的最先进CMR模型：（i）CLIP；以及（ii）X-VLM。此外，我们选择了两个以场景为中心的数据集和三个以对象为中心的数据集，并确定了所选模型在这些数据集上的相对性能。

    Most approaches to cross-modal retrieval (CMR) focus either on object-centric datasets, meaning that each document depicts or describes a single object, or on scene-centric datasets, meaning that each image depicts or describes a complex scene that involves multiple objects and relations between them. We posit that a robust CMR model should generalize well across both dataset types. Despite recent advances in CMR, the reproducibility of the results and their generalizability across different dataset types has not been studied before. We address this gap and focus on the reproducibility of the state-of-the-art CMR results when evaluated on object-centric and scene-centric datasets. We select two state-of-the-art CMR models with different architectures: (i) CLIP; and (ii) X-VLM. Additionally, we select two scene-centric datasets, and three object-centric datasets, and determine the relative performance of the selected models on these datasets. We focus on reproducibility, replicability, 
    
[^188]: 边缘视频分析：应用、系统和支持技术综述

    Edge Video Analytics: A Survey on Applications, Systems and Enabling Techniques. (arXiv:2211.15751v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2211.15751](http://arxiv.org/abs/2211.15751)

    这项研究综述了边缘视频分析（EVA）的应用、系统和支持技术。随着深度学习的发展和互联设备的普及，EVA作为一种在网络边缘处理视频数据的解决方案受到越来越多的关注。

    

    视频作为全球数字信息爆炸的关键驱动力，可以为人类社会带来巨大的好处。政府和企业正在部署无数摄像头用于各种应用，例如执法、应急管理、交通控制和安全监控，这些都是由视频分析（VA）所支持。深度学习的快速发展使得对象分类、检测和跟踪的模型更加精确。同时，随着互联设备的广泛使用，每天都会产生海量数据，超越了云的处理能力。边缘计算是一种新兴的范式，它将工作负载和服务从网络核心移动到网络边缘，被广泛认为是一个有希望的解决方案。由此产生的新交叉领域，即边缘视频分析（EVA），开始引起广泛关注。然而，在这个领域中只有少数松散相关的综述存在。EVA的基本概念（例如

    Video, as a key driver in the global explosion of digital information, can create tremendous benefits for human society. Governments and enterprises are deploying innumerable cameras for a variety of applications, e.g., law enforcement, emergency management, traffic control, and security surveillance, all facilitated by video analytics (VA). This trend is spurred by the rapid advancement of deep learning (DL), which enables more precise models for object classification, detection, and tracking. Meanwhile, with the proliferation of Internet-connected devices, massive amounts of data are generated daily, overwhelming the cloud. Edge computing, an emerging paradigm that moves workloads and services from the network core to the network edge, has been widely recognized as a promising solution. The resulting new intersection, edge video analytics (EVA), begins to attract widespread attention. Nevertheless, only a few loosely-related surveys exist on this topic. The basic concepts of EVA (e.g
    
[^189]: 基于数据驱动的网络神经科学：关于数据收集与基准的研究

    Data-Driven Network Neuroscience: On Data Collection and Benchmark. (arXiv:2211.12421v3 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2211.12421](http://arxiv.org/abs/2211.12421)

    本文提供了一个全面且高质量的人脑功能网络数据集，用于神经科学、机器学习和图分析的交叉研究。它利用解剖学和功能性磁共振成像来理解人脑的功能连接，并具有识别潜在神经退行性疾病的重要性。通过利用机器学习和图分析研究大脑的网络形式，能够预测这些疾病的早期发生。脑网络以图形方式表示，保留了丰富的结构和位置信息，传统的检查方法无法捕捉到这些信息。然而，缺乏公开可访问的脑网络数据限制了数据驱动的研究。

    

    本文提供了一份全面且高质量的人脑功能网络数据集，用于神经科学、机器学习和图分析的交叉研究。解剖学和功能性磁共振成像被用于理解人脑的功能连接，并且在识别阿尔茨海默氏症、帕金森症和自闭症等潜在的神经退行性疾病方面尤为重要。最近，利用机器学习和图分析研究以脑网络的形式来研究大脑的方法变得越来越流行，特别是用于预测这些疾病的早期发生。作为一个图形表示的脑网络保留了丰富的结构和位置信息，传统的检查方法无法捕捉到这些信息。然而，缺乏公开可访问的脑网络数据限制了研究人员进行数据驱动的探索。其中主要的困难在于复杂的领域特定的预处理步骤。

    This paper presents a comprehensive and quality collection of functional human brain \emph{network} data for potential research in the intersection of neuroscience, machine learning, and graph analytics. Anatomical and functional MRI images have been used to understand the functional connectivity of the human brain and are particularly important in identifying underlying neurodegenerative conditions such as Alzheimer's, Parkinson's, and Autism. Recently, the study of the brain in the form of brain networks using machine learning and graph analytics has become increasingly popular, especially to predict the early onset of these conditions. A brain network, represented as a graph, retains rich structural and positional information that traditional examination methods are unable to capture. However, the lack of publicly accessible brain network data prevents researchers from data-driven explorations. One of the main difficulties lies in the complicated domain-specific preprocessing steps 
    
[^190]: 通过深度学习实现时间序列中的自动变点检测

    Automatic Change-Point Detection in Time Series via Deep Learning. (arXiv:2211.03860v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.03860](http://arxiv.org/abs/2211.03860)

    本文介绍了一种通过训练神经网络自动生成新的离线检测方法的方式，应用于时间序列中的自动变点检测，其性能可与标准CUSUM分类器性能竞争。

    

    数据中的变点检测具有挑战性，我们通过训练神经网络自动生成新的离线检测方法，并提出了一种理论量化此方法的误差率及其与训练数据量的关系。实证结果表明，即使有限的训练数据，其性能也可与用于检测中变化的标准CUSUM分类器的性能竞争。

    Detecting change-points in data is challenging because of the range of possible types of change and types of behaviour of data when there is no change. Statistically efficient methods for detecting a change will depend on both of these features, and it can be difficult for a practitioner to develop an appropriate detection method for their application of interest. We show how to automatically generate new offline detection methods based on training a neural network. Our approach is motivated by many existing tests for the presence of a change-point being representable by a simple neural network, and thus a neural network trained with sufficient data should have performance at least as good as these methods. We present theory that quantifies the error rate for such an approach, and how it depends on the amount of training data. Empirical results show that, even with limited training data, its performance is competitive with the standard CUSUM-based classifier for detecting a change in m
    
[^191]: SpacePhish: 使用机器学习对抗钓鱼网站检测器的攻击的逃避空间

    SpacePhish: The Evasion-space of Adversarial Attacks against Phishing Website Detectors using Machine Learning. (arXiv:2210.13660v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2210.13660](http://arxiv.org/abs/2210.13660)

    该论文研究了对抗机器学习中钓鱼网站检测的攻击以及针对这些攻击的逃避防御空间，并提出了一种现实威胁模型。

    

    现有的关于对抗机器学习的文献要么展示能够破坏所有机器学习模型的攻击，要么展示能够抵御大部分攻击的防御措施。然而，很少考虑攻击或防御的实际可行性。此外，对抗样本通常是在“特征空间”中生成的，这使得相关评估的价值值得怀疑。简而言之，目前的情况不允许估计对抗攻击所带来的真实威胁，这导致缺乏安全的机器学习系统。本文旨在澄清这种困惑。通过考虑机器学习用于钓鱼网站检测的应用，我们将“逃避空间”形式化为一种能够引入对抗扰动以欺骗机器学习-钓鱼网站检测的空间-证明即使在“特征空间”中的扰动也是有用的。然后，我们提出了一个描述针对机器学习-钓鱼网站检测的逃避攻击的现实威胁模型，这种攻击容易实施，因此更具吸引力。

    Existing literature on adversarial Machine Learning (ML) focuses either on showing attacks that break every ML model, or defenses that withstand most attacks. Unfortunately, little consideration is given to the actual feasibility of the attack or the defense. Moreover, adversarial samples are often crafted in the "feature-space", making the corresponding evaluations of questionable value. Simply put, the current situation does not allow to estimate the actual threat posed by adversarial attacks, leading to a lack of secure ML systems.  We aim to clarify such confusion in this paper. By considering the application of ML for Phishing Website Detection (PWD), we formalize the "evasion-space" in which an adversarial perturbation can be introduced to fool a ML-PWD -- demonstrating that even perturbations in the "feature-space" are useful. Then, we propose a realistic threat model describing evasion attacks against ML-PWD that are cheap to stage, and hence intrinsically more attractive for r
    
[^192]: 使用神经切线核对图神经网络中的卷积，非线性和深度进行分析

    Analysis of Convolutions, Non-linearity and Depth in Graph Neural Networks using Neural Tangent Kernel. (arXiv:2210.09809v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.09809](http://arxiv.org/abs/2210.09809)

    本研究通过理论方法分析了图神经网络中的卷积、非线性和深度对网络性能的影响，同时对基于图拉普拉斯和邻接矩阵的归一化方法进行了比较，并揭示了线性GNN与非线性ReLU-GNN性能相当的现象缺乏严格的理论解释。

    

    图神经网络（GNN）的基本原理是通过使用“图卷积”来聚合相邻节点的结构信息，并选择合适的网络架构（例如深度和激活函数）。因此，理解每个设计选择对网络性能的影响至关重要。基于图拉普拉斯的卷积已成为主流选择，其中对邻接矩阵进行对称归一化是最广泛采用的方法。然而，一些经验研究表明，行归一化的邻接矩阵在节点分类方面表现更好。尽管GNN的使用非常广泛，但目前尚无严格的理论研究关于这些卷积的表示能力，无法解释这种行为。同样，线性GNN的性能与非线性ReLU-GNN的性能相当的经验观察也缺乏严格的理论支持。

    The fundamental principle of Graph Neural Networks (GNNs) is to exploit the structural information of the data by aggregating the neighboring nodes using a `graph convolution' in conjunction with a suitable choice for the network architecture, such as depth and activation functions. Therefore, understanding the influence of each of the design choice on the network performance is crucial. Convolutions based on graph Laplacian have emerged as the dominant choice with the symmetric normalization of the adjacency matrix as the most widely adopted one. However, some empirical studies show that row normalization of the adjacency matrix outperforms it in node classification. Despite the widespread use of GNNs, there is no rigorous theoretical study on the representation power of these convolutions, that could explain this behavior. Similarly, the empirical observation of the linear GNNs performance being on par with non-linear ReLU GNNs lacks rigorous theory.  In this work, we theoretically a
    
[^193]: 在社交媒体上表征和检测国家赞助的喷子活动

    Characterizing and Detecting State-Sponsored Troll Activity on Social Media. (arXiv:2210.08786v5 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2210.08786](http://arxiv.org/abs/2210.08786)

    该研究提出了一种基于AI的新解决方案，通过分析喷子的轨迹，识别国家赞助的喷子帐户。该方法可以准确识别俄罗斯喷子与有机用户，可提供国家赞助影响活动的早期警报，并对保护民主进程做出贡献。

    

    检测在影响活动中运营的国家赞助的喷子是研究社区的一个关键而未解决的挑战，其影响超出了在线领域。为了解决这个挑战，我们提出了一种新的基于AI的解决方案，通过分析喷子的分享活动序列或轨迹，通过两个步骤识别国家赞助的喷子帐户。首先，我们使用基于LSTM的分类器将帐户的轨迹分类为国家赞助的喷子或有机的合法用户。其次，我们利用分类后的轨迹计算一种指标，称为“喷子评分”，来量化帐户的行为与国家赞助的喷子的相似程度。为了评估我们的方法，我们研究了2016年美国总统大选期间的俄罗斯干预活动。我们实验结果表明，我们的方法可以识别帐户轨迹，AUC接近99％，并准确分类俄罗斯喷子和有机用户，F1分数分别为0.95和0.91。我们的解决方案可以集成到现有系统中，提供国家赞助影响活动的早期警报，并对保护民主进程做出贡献。

    The detection of state-sponsored trolls operating in influence campaigns is a critical and unsolved challenge for the research community, which has significant implications beyond the online realm. To address this challenge, we propose a new AI-based solution that identifies state-sponsored troll accounts by analyzing their sharing activity sequences, or trajectories, through a two-step process. First, we classify accounts' trajectories using an LSTM-based classifier as belonging to either a state-sponsored troll or an organic, legitimate user. Second, we utilize the classified trajectories to compute a metric, named ``Troll Score'', to quantify the extent to which an account behaves like a state-sponsored troll. To evaluate our approach, we examine the Russian interference campaign during the 2016 U.S. Presidential election. The results of our experiments show that our method can identify account trajectories with an AUC close to 99% and accurately classify Russian trolls and organic 
    
[^194]: 扩散模型：方法和应用的综合调研

    Diffusion Models: A Comprehensive Survey of Methods and Applications. (arXiv:2209.00796v10 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.00796](http://arxiv.org/abs/2209.00796)

    本调研综合总结了扩散模型的方法和应用研究，包括高效采样、似然估计与特殊结构数据处理，介绍了扩散模型与其他生成模型相结合的潜力并回顾了其在不同领域的广泛应用，为进一步研究提出了可能的研究方向。

    

    扩散模型已成为一类具有记录性能的强大的深度生成模型，在许多应用中，包括图像合成、视频生成和分子设计方面表现出色。在这份调研中，我们概述了关于扩散模型的快速扩展研究，将研究分类为三个关键领域：高效采样、改进的似然估计和处理具有特殊结构的数据。我们还讨论了将扩散模型与其他生成模型相结合以实现增强结果的潜力。我们进一步回顾了扩散模型在计算机视觉、自然语言生成、时间数据建模以及其他科学学科的跨学科应用中的广泛应用。这个调研旨在提供一个具有背景的深入了解扩散模型的现状，确定关键的研究重点，指明可能的进一步研究领域。

    Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language generation, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/Yan
    
[^195]: 论可行解释与被遗忘权之间的权衡

    On the Trade-Off between Actionable Explanations and the Right to be Forgotten. (arXiv:2208.14137v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.14137](http://arxiv.org/abs/2208.14137)

    本文研究了在数据删除请求的背景下，可行解释与被遗忘权之间的权衡问题。通过理论和实证分析，发现当少量数据删除请求触发预测模型的更新时，目前流行的最先进算法生成的可行解释可能失效。

    

    随着机器学习模型在高风险应用中的部署越来越多，决策者提出了更严格的数据保护法规（例如GDPR、CCPA）。其中一个关键原则是“被遗忘权”，即用户有权要求删除其数据。另一个关键原则是可行解释权，也称为算法追索权，允许用户撤销不利的决定。迄今为止，尚不清楚这两个原则是否可以同时实施。因此，我们在数据删除请求的背景下引入并研究了追索无效问题。具体而言，我们从理论和实证角度分析了流行的最先进算法的行为，并证明了这些算法生成的追索很可能会在少量数据删除请求（例如1或2个）引发对预测模型的更新时失效。对于不可区分模型的设置，我们提出一种框架。

    As machine learning (ML) models are increasingly being deployed in high-stakes applications, policymakers have suggested tighter data protection regulations (e.g., GDPR, CCPA). One key principle is the "right to be forgotten" which gives users the right to have their data deleted. Another key principle is the right to an actionable explanation, also known as algorithmic recourse, allowing users to reverse unfavorable decisions. To date, it is unknown whether these two principles can be operationalized simultaneously. Therefore, we introduce and study the problem of recourse invalidation in the context of data deletion requests. More specifically, we theoretically and empirically analyze the behavior of popular state-of-the-art algorithms and demonstrate that the recourses generated by these algorithms are likely to be invalidated if a small number of data deletion requests (e.g., 1 or 2) warrant updates of the predictive model. For the setting of differentiable models, we suggest a fra
    
[^196]: 在边缘进行主体子模型训练的大型模型联邦学习

    Federated Learning of Large Models at the Edge via Principal Sub-Model Training. (arXiv:2208.13141v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.13141](http://arxiv.org/abs/2208.13141)

    本文提出了一种解决资源受限边缘设备下大型模型联邦学习的方法，通过主体子模型训练，在不违反隐私承诺的前提下，使弱但重要的客户端能够参与到协作训练中。

    

    联邦学习（FL）是一种新兴的分散式学习框架，可以实现客户端之间的协作训练，无需共享私有数据，也无需传输至中心服务器。然而，考虑到许多边缘客户端缺乏足够的计算、内存或通信能力，联邦学习大规模模型仍面临重大瓶颈。为了保持弱但重要的客户端参与，之前的工作要么考虑了异构客户端设置，其中客户端使用不同大小的模型进行训练；要么将训练任务转移到服务端。然而，异构客户端设置要求某些客户端训练完整模型，这与资源受限的设置不一致；而后者在与服务器共享中间表示或标签时违反了FL的隐私承诺。为了克服这些限制，在这项工作中，我们在现实中提出了一个更少探索的跨设备FL设置，其中没有任何客户端需要训练完整模型。

    Federated Learning (FL) is emerging as a popular, promising decentralized learning framework that enables collaborative training among clients, with no need to share private data between them or to a centralized server. However, considering many edge clients do not have sufficient computing, memory, or communication capabilities, federated learning of large models still faces significant bottlenecks. To keep such weak but crucial clients in the loop, prior works either consider a heterogeneous-client setting where clients train models with different sizes; or offload training to the server. However, the heterogeneous-client setting requires some clients to train full model, which is not aligned with the resource-constrained setting; while the latter ones break privacy promises in FL when sharing intermediate representations or labels with the server. To overcome these limitations, in this work, we formulate a realistic, but much less explored, cross-device FL setting in which no client
    
[^197]: 一种基于局部约束图匹配的注重公正性的艺术展推荐系统

    An Equity-Aware Recommender System for Curating Art Exhibits Based on Locally-Constrained Graph Matching. (arXiv:2207.14367v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2207.14367](http://arxiv.org/abs/2207.14367)

    这个论文介绍了一种注重公正性的艺术展推荐系统，通过局部约束图匹配和价值导向的资源分配，实现公共艺术展览的策划。该系统采用Schelling模型构建成本矩阵，并通过优化评分函数，软分配艺术作品到公共空间，以减少内部群体偏好、满足最低代表性和曝光标准。

    

    公共艺术塑造了我们共享的空间。公共艺术应该与社区和背景相关，然而最近的研究表明，许多知名机构的艺术作品偏向于过时的文化规范和传统社群。鉴于此，我们开发了一种新颖的推荐系统，用于以内置公正目标和局部基于价值的有限资源分配来策划公共艺术展览。我们利用Schelling的种族分离模型构建了成本矩阵。使用成本矩阵作为输入，通过投影梯度下降优化得到了一个软分配矩阵的评分函数。我们的优化程序以一种方式将艺术作品分配给公共空间，以降低“内部群体”偏好，并满足最低代表性和曝光标准。我们借鉴现有的文献为算法输出开发了一个公正性指标，并评估了我们的方法的效果，并从策展和公正性的角度讨论其潜在问题。

    Public art shapes our shared spaces. Public art should speak to community and context, and yet, recent work has demonstrated numerous instances of art in prominent institutions favoring outdated cultural norms and legacy communities. Motivated by this, we develop a novel recommender system to curate public art exhibits with built-in equity objectives and a local value-based allocation of constrained resources. We develop a cost matrix by drawing on Schelling's model of segregation. Using the cost matrix as an input, the scoring function is optimized via a projected gradient descent to obtain a soft assignment matrix. Our optimization program allocates artwork to public spaces in a way that de-prioritizes "in-group" preferences, by satisfying minimum representation and exposure criteria. We draw on existing literature to develop a fairness metric for our algorithmic output, and we assess the effectiveness of our approach and discuss its potential pitfalls from both a curatorial and equi
    
[^198]: DataPerf：数据中心人工智能开发的基准测试

    DataPerf: Benchmarks for Data-Centric AI Development. (arXiv:2207.10062v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.10062](http://arxiv.org/abs/2207.10062)

    DataPerf是一个由社区主导的基准测试套件，旨在通过竞争、可比性和可重复性促进数据中心人工智能的创新。

    

    长期以来，机器学习研究一直注重模型而不是数据集，并且著名数据集被用于常见的机器学习任务，而忽视了底层问题的广度、难度和准确性。忽视数据的重要性导致了现实应用中的不准确性、偏见和脆弱性，并且现有的数据集基准测试已经达到了饱和状态，阻碍了研究的进展。为此，我们提出了DataPerf，这是一个由社区主导的评估机器学习数据集和数据中心算法的基准测试套件。我们旨在通过竞争、可比性和可重复性促进数据中心人工智能的创新。我们使机器学习社区能够迭代数据集，而不仅仅是架构，并提供一个开放的在线平台，以支持这种迭代开发。DataPerf的第一个迭代包含了五个基准测试，涵盖了视觉、语音、获取等多种数据中心技术、任务和模态。

    Machine learning research has long focused on models rather than datasets, and prominent datasets are used for common ML tasks without regard to the breadth, difficulty, and faithfulness of the underlying problems. Neglecting the fundamental importance of data has given rise to inaccuracy, bias, and fragility in real-world applications, and research is hindered by saturation across existing dataset benchmarks. In response, we present DataPerf, a community-led benchmark suite for evaluating ML datasets and data-centric algorithms. We aim to foster innovation in data-centric AI through competition, comparability, and reproducibility. We enable the ML community to iterate on datasets, instead of just architectures, and we provide an open, online platform with multiple rounds of challenges to support this iterative development. The first iteration of DataPerf contains five benchmarks covering a wide spectrum of data-centric techniques, tasks, and modalities in vision, speech, acquisition, 
    
[^199]: 机器学习分类器的偏差缓解：一项全面调研

    Bias Mitigation for Machine Learning Classifiers: A Comprehensive Survey. (arXiv:2207.07068v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.07068](http://arxiv.org/abs/2207.07068)

    这项研究提供了一份全面调研，总结了机器学习模型中实现公平性的偏差缓解方法。通过对341篇论文进行分析，研究了不同的干预过程和应用技术，并调查了评估方法。这将为从业者在开发和评估新的偏差缓解方法时提供有益的指导。

    

    本文对实现机器学习模型中的公平性的偏差缓解方法进行了全面调研。我们收集了总计341篇关于机器学习分类器偏差缓解的研究论文。这些方法可以根据它们的干预过程（预处理、处处理、后处理）和应用技术进行区分。我们调查了现有的偏差缓解方法在文献中的评估方式。特别是，我们考虑了数据集、评估指标和基准测试。基于所收集到的见解（例如，最流行的公平性指标是什么？有多少数据集用于评估偏差缓解方法？），我们希望能够支持从业者在开发和评估新的偏差缓解方法时作出明智的选择。

    This paper provides a comprehensive survey of bias mitigation methods for achieving fairness in Machine Learning (ML) models. We collect a total of 341 publications concerning bias mitigation for ML classifiers. These methods can be distinguished based on their intervention procedure (i.e., pre-processing, in-processing, post-processing) and the technique they apply. We investigate how existing bias mitigation methods are evaluated in the literature. In particular, we consider datasets, metrics and benchmarking. Based on the gathered insights (e.g., What is the most popular fairness metric? How many datasets are used for evaluating bias mitigation methods?), we hope to support practitioners in making informed choices when developing and evaluating new bias mitigation methods.
    
[^200]: EXACT: 如何提高准确率的训练方法。

    EXACT: How to Train Your Accuracy. (arXiv:2205.09615v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.09615](http://arxiv.org/abs/2205.09615)

    本文提出了一种新的分类任务优化框架，通过引入随机性，优化期望准确率，取得了强有力的替代分类损失的结果。

    

    分类任务通常会以准确率作为评估标准。然而，准确率是不连续的，无法直接使用梯度上升进行优化。流行的方法是通过最小化交叉熵、铰链损失或其他替代损失来优化，但这可能导致次优结果。本文提出了一种新的优化框架，通过向模型的输出引入随机性并优化期望准确率，即随机模型的准确率。对线性模型和深度图像分类进行了广泛的实验，结果表明所提出的优化方法是广泛使用的分类损失的强有力替代方案。

    Classification tasks are usually evaluated in terms of accuracy. However, accuracy is discontinuous and cannot be directly optimized using gradient ascent. Popular methods minimize cross-entropy, hinge loss, or other surrogate losses, which can lead to suboptimal results. In this paper, we propose a new optimization framework by introducing stochasticity to a model's output and optimizing expected accuracy, i.e. accuracy of the stochastic model. Extensive experiments on linear models and deep image classification show that the proposed optimization method is a powerful alternative to widely used classification losses.
    
[^201]: 使用迁移学习的深度学习模型在月度时间序列的多步预测中的性能

    Performance of Deep Learning models with transfer learning for multiple-step-ahead forecasts in monthly time series. (arXiv:2203.11196v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.11196](http://arxiv.org/abs/2203.11196)

    本研究比较了具有迁移学习和无迁移学习的深度学习模型以及其他传统方法在月度时间序列预测中的性能，结果显示基于TCN、LSTM和CNN的深度学习模型加上迁移学习的性能超过了其他传统方法，并且直接在目标时间序列上训练的TCN和LSTM在某些预测时段的性能相当或更好。

    

    深度学习和迁移学习模型被用于生成时间序列预测，然而对于月度时间序列的性能预测，缺乏证据。本文旨在比较具有迁移学习和没有迁移学习的深度学习模型以及其他传统方法在月度预测中的适用性，回答三个关于深度学习和迁移学习生成时间序列预测的问题。实验使用了M4和M3竞赛的时间序列。结果表明，基于TCN、LSTM和CNN的深度学习模型加上迁移学习往往能够超过其他传统方法的性能预测。另一方面，对于某些预测时段，直接在目标时间序列上训练的TCN和LSTM的性能与传统方法相当或更好。

    Deep Learning and transfer learning models are being used to generate time series forecasts; however, there is scarce evidence about their performance prediction that it is more evident for monthly time series. The purpose of this paper is to compare Deep Learning models with transfer learning and without transfer learning and other traditional methods used for monthly forecasts to answer three questions about the suitability of Deep Learning and Transfer Learning to generate predictions of time series. Time series of M4 and M3 competitions were used for the experiments. The results suggest that deep learning models based on TCN, LSTM, and CNN with transfer learning tend to surpass the performance prediction of other traditional methods. On the other hand, TCN and LSTM, trained directly on the target time series, got similar or better performance than traditional methods for some forecast horizons.
    
[^202]: 简单神经网络中的可达性问题研究

    Reachability In Simple Neural Networks. (arXiv:2203.07941v3 [cs.CC] UPDATED)

    [http://arxiv.org/abs/2203.07941](http://arxiv.org/abs/2203.07941)

    本研究研究了简单神经网络中的可达性问题，并证明了对于仅具有一个隐含层和一个输出维度以及仅具有一个负、零和一个正权重或偏置的神经网络来说，它是NP难度问题。

    

    我们研究了（深度）神经网络的可达性问题的复杂性：在给定一些有效输入的情况下，它是否计算出有效输出？最近有人声称，对于一般的神经网络和由线性不等式的合取组成的输入/输出维度的规范，该问题是NP完全问题。 我们总结了证明并修复了原始上界和下界证明中的一些缺陷。受到通用结果的启发，我们展示了NP难度已经适用于简单规范和神经网络的受限类。允许一个隐藏层和一个输出维数以及仅具有一个负、零和一个正权重或偏置的神经网络就足以确保NP难度。此外，我们对神经网络验证研究的这个方向进行了全面的讨论和展望。

    We investigate the complexity of the reachability problem for (deep) neural networks: does it compute valid output given some valid input? It was recently claimed that the problem is NP-complete for general neural networks and specifications over the input/output dimension given by conjunctions of linear inequalities. We recapitulate the proof and repair some flaws in the original upper and lower bound proofs. Motivated by the general result, we show that NP-hardness already holds for restricted classes of simple specifications and neural networks. Allowing for a single hidden layer and an output dimension of one as well as neural networks with just one negative, zero and one positive weight or bias is sufficient to ensure NP-hardness. Additionally, we give a thorough discussion and outlook of possible extensions for this direction of research on neural network verification.
    
[^203]: 概率鲁棒性追索：在算法追索中权衡成本与鲁棒性的交易

    Probabilistically Robust Recourse: Navigating the Trade-offs between Costs and Robustness in Algorithmic Recourse. (arXiv:2203.06768v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.06768](http://arxiv.org/abs/2203.06768)

    本论文提出了一种算法框架，可以帮助用户有效地管理追索成本与鲁棒性之间的权衡，以解决现实世界中机器学习模型决策所带来的问题。

    

    随着机器学习模型越来越多地被用于在现实世界中做重要决策，确保那些受到模型预测影响（例如被拒贷款）的个体有一种补救的手段变得至关重要。虽然已经提出了几种方法来为受影响的个体构建追索，但这些方法产生的追索要么实现成本较低（即易于实施），要么对小扰动具有鲁棒性（即追索的噪声实现），但由于追索成本和鲁棒性之间天然的权衡，两者不能兼得。此外，先前的方法没有为最终用户提供导航上述互换之间的任何机制。在这项工作中，我们通过提出第一个算法框架来解决上述挑战，使用户能够有效地管理追索成本与鲁棒性之间的权衡。具体而言，我们的框架Probabilistically ROBust rEcours

    As machine learning models are increasingly being employed to make consequential decisions in real-world settings, it becomes critical to ensure that individuals who are adversely impacted (e.g., loan denied) by the predictions of these models are provided with a means for recourse. While several approaches have been proposed to construct recourses for affected individuals, the recourses output by these methods either achieve low costs (i.e., ease-of-implementation) or robustness to small perturbations (i.e., noisy implementations of recourses), but not both due to the inherent trade-offs between the recourse costs and robustness. Furthermore, prior approaches do not provide end users with any agency over navigating the aforementioned trade-offs. In this work, we address the above challenges by proposing the first algorithmic framework which enables users to effectively manage the recourse cost vs. robustness trade-offs. More specifically, our framework Probabilistically ROBust rEcours
    
[^204]: IPD：一种用于大规模数据的增量原型DBSCAN聚类方法，具有簇代表

    IPD:An Incremental Prototype based DBSCAN for large-scale data with cluster representatives. (arXiv:2202.07870v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.07870](http://arxiv.org/abs/2202.07870)

    该论文提出了一种增量原型DBSCAN聚类算法（IPD），可用于大规模数据的任意形状聚类，并为每个簇选择了一组代表。

    

    DBSCAN是一种基于密度的基础聚类技术，可以识别任意形状的聚类。然而，处理大数据时不可行。另一方面，基于质心的聚类对于检测数据集中的模式很重要，因为未经处理的数据点可以被标记为最近的质心。然而，它无法检测非球形聚类。对于大型数据，存储和计算每个样本的标签是不可行的。当需要信息时，可以逐步进行。当聚类作为识别簇代表的工具，并通过分配最近代表的簇标签为查询提供服务时，可以实现此目的。在本文中，我们提出了一种增量原型DBSCAN（IPD）算法，该算法旨在识别大规模数据的任意形状聚类。此外，它为每个簇选择了一组代表。

    DBSCAN is a fundamental density-based clustering technique that identifies any arbitrary shape of the clusters. However, it becomes infeasible while handling big data. On the other hand, centroid-based clustering is important for detecting patterns in a dataset since unprocessed data points can be labeled to their nearest centroid. However, it can not detect non-spherical clusters. For a large data, it is not feasible to store and compute labels of every samples. These can be done as and when the information is required. The purpose can be accomplished when clustering act as a tool to identify cluster representatives and query is served by assigning cluster labels of nearest representative. In this paper, we propose an Incremental Prototype-based DBSCAN (IPD) algorithm which is designed to identify arbitrary-shaped clusters for large-scale data. Additionally, it chooses a set of representatives for each cluster.
    
[^205]: 具有线性阈值激活的神经网络：结构与算法

    Neural networks with linear threshold activations: structure and algorithms. (arXiv:2111.08117v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.08117](http://arxiv.org/abs/2111.08117)

    本文研究了具有线性阈值激活函数的神经网络。我们确定了这类神经网络可以表示的函数的特点，并发现使用两个隐藏层可以表示该类中的任何函数。我们还给出了表示这类函数所需神经网络大小的界限，并设计了一个能够解决具有固定架构的这类神经网络的经验风险最小化问题的算法。该算法的运行时间与数据样本大小呈多项式关系，而输入维度和网络架构的大小被认为是固定常数。

    

    本文中我们介绍了关于具有线性阈值激活函数的神经网络的新结果。我们精确地描述了可以由这样的神经网络表示的函数类，并且证明了用2个隐藏层表示该类中的任何可表示函数既是必要的也是充分的。这是一个令人惊讶的结果，考虑到最近使用其他流行激活函数如修正线性单元（ReLU）进行神经网络精确可表示性研究。我们还给出了表示该类中任何函数所需的神经网络大小的精确界限。最后，我们设计了一种算法来解决具有固定架构的这些神经网络的经验风险最小化（ERM）问题，以全局最优性。如果将输入维度和网络架构的大小视为固定常数，则该算法的运行时间是多项式时间。该算法在任何情况下都可使用。

    In this article we present new results on neural networks with linear threshold activation functions. We precisely characterize the class of functions that are representable by such neural networks and show that 2 hidden layers are necessary and sufficient to represent any function representable in the class. This is a surprising result in the light of recent exact representability investigations for neural networks using other popular activation functions like rectified linear units (ReLU). We also give precise bounds on the sizes of the neural networks required to represent any function in the class. Finally, we design an algorithm to solve the empirical risk minimization (ERM) problem to global optimality for these neural networks with a fixed architecture. The algorithm's running time is polynomial in the size of the data sample, if the input dimension and the size of the network architecture are considered fixed constants. The algorithm is unique in the sense that it works for any
    
[^206]: 使用潜在目标模型进行快速开放世界导航探索

    Rapid Exploration for Open-World Navigation with Latent Goal Models. (arXiv:2104.05859v5 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2104.05859](http://arxiv.org/abs/2104.05859)

    本论文描述了一个机器人学习系统，可用于自主探索和导航不同的开放世界环境。该系统通过学习得到一个潜在变量模型和图像的拓扑记忆，使用信息瓶颈约束策略，能够在不到20分钟内通过视觉目标表示在开放世界中探索并发现目标。

    

    我们描述了一个用于自主探索和导航多样化开放世界环境的机器人学习系统。我们方法的核心是一个学习得到的距离和动作的潜在变量模型，以及一个非参数的图像拓扑记忆。我们使用信息瓶颈来规范学习的策略，使我们能够得到(i)紧凑的目标视觉表示，(ii)改进的泛化能力以及(iii)用于探索的可行目标采样机制。通过在大规模离线经验数据集上进行训练，该模型获得了对于任务无关干扰物具有鲁棒性的视觉目标表示。我们在开放世界探索场景中利用移动地面机器人演示了我们的方法。给定一个距离高达80米的目标图像，我们的方法利用其表示在不到20分钟内探索并发现了目标，即使在以前未见的障碍和天气条件下也能实现。请查看项目网站上的视频。

    We describe a robotic learning system for autonomous exploration and navigation in diverse, open-world environments. At the core of our method is a learned latent variable model of distances and actions, along with a non-parametric topological memory of images. We use an information bottleneck to regularize the learned policy, giving us (i) a compact visual representation of goals, (ii) improved generalization capabilities, and (iii) a mechanism for sampling feasible goals for exploration. Trained on a large offline dataset of prior experience, the model acquires a representation of visual goals that is robust to task-irrelevant distractors. We demonstrate our method on a mobile ground robot in open-world exploration scenarios. Given an image of a goal that is up to 80 meters away, our method leverages its representation to explore and discover the goal in under 20 minutes, even amidst previously-unseen obstacles and weather conditions. Please check out the project website for videos o
    
[^207]: 机器学习方法用于建模城市土地使用中的停车时长问题

    A Machine Learning Approach for Modelling Parking Duration in Urban Land-use. (arXiv:2008.01674v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2008.01674](http://arxiv.org/abs/2008.01674)

    本研究提出了一个机器学习模型来分析汽车用户的特征对停车时长的影响，采用人工神经网络捕捉其相互关系，并运用Garson算法和LIME进行模型解释。

    

    停车是发展中国家中不可避免的问题。随着车辆数量的增加，需要为停车分配更多的城市土地。然而，在印度等发展中国家，停车问题一直没有得到足够的关注。本研究提出了一个模型，用于分析汽车用户的社会经济和出行特征对停车时长的影响。具体而言，采用人工神经网络（ANNs）来捕捉驾驶员特征和停车时长之间的相互关系。ANNs在学习和识别参数之间的连接上非常高效，以最佳预测结果。然而，由于其黑盒特性，ANNs的实用性受到严重限制。因此，本研究使用Garson算法和局部可解释模型不受限的解释（LIME）进行模型解释。LIME通过将局部数据与开发的可解释模型进行局部逼近，展示了任何分类的预测结果。

    Parking is an inevitable issue in the fast-growing developing countries. Increasing number of vehicles require more and more urban land to be allocated for parking. However, a little attention has been conferred to the parking issues in developing countries like India. This study proposes a model for analysing the influence of car users' socioeconomic and travel characteristics on parking duration. Specifically, artificial neural networks (ANNs) is deployed to capture the interrelationship between driver characteristics and parking duration. ANNs are highly efficient in learning and recognizing connections between parameters for best prediction of an outcome. Since, utility of ANNs has been critically limited due to its Black Box nature, the study involves the use of Garson algorithm and Local interpretable model-agnostic explanations (LIME) for model interpretations. LIME shows the prediction for any classification, by approximating it locally with the developed interpretable model. T
    
[^208]: 高维度和普遍一致的k样本检验

    High-dimensional and universally consistent k-sample tests. (arXiv:1910.08883v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1910.08883](http://arxiv.org/abs/1910.08883)

    本文证明了独立性测试实现了普遍一致的k样本检验，并且发现非参数独立性测试通常比多元方差分析(MANOVA)测试在高斯分布情况下表现更好。

    

    k样本检验问题涉及确定$k$组数据点是否都来自同一个分布。尽管多元方差分析(MANOVA)是生物医学中常用的k样本检验方法，但它依赖于强大且通常不合适的参数假设。此外，独立性测试和k样本测试密切相关，一些普遍一致的高维独立性测试，如距离相关(Discrepancy)和Hilbert-Schmidt独立性准则(Hsic)，具有坚实的理论和实证性质。在本文中，我们证明了独立性测试实现了普遍一致的k样本检验，并且k样本统计量，如Energy和Maximum Mean Discrepancy(MMD)，与Discrepancy完全等价。对非参数独立性测试的实证评估表明，它们通常比流行的MANOVA测试表现更好，即使在高斯分布的场景中也是如此。

    The k-sample testing problem involves determining whether $k$ groups of data points are each drawn from the same distribution. The standard method for k-sample testing in biomedicine is Multivariate analysis of variance (MANOVA), despite that it depends on strong, and often unsuitable, parametric assumptions. Moreover, independence testing and k-sample testing are closely related, and several universally consistent high-dimensional independence tests such as distance correlation (Dcorr) and Hilbert-Schmidt-Independence-Criterion (Hsic) enjoy solid theoretical and empirical properties. In this paper, we prove that independence tests achieve universally consistent k-sample testing and that k-sample statistics such as Energy and Maximum Mean Discrepancy (MMD) are precisely equivalent to Dcorr. An empirical evaluation of nonparametric independence tests showed that they generally perform better than the popular MANOVA test, even in Gaussian distributed scenarios. The evaluation included se
    

