# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [MaGNAS: A Mapping-Aware Graph Neural Architecture Search Framework for Heterogeneous MPSoC Deployment.](http://arxiv.org/abs/2307.08065) | MaGNAS是一种面向异构MPSoC部署的映射感知图神经架构搜索框架，能够高效处理视觉图神经网络工作负载。 |
| [^2] | [Fast Quantum Algorithm for Attention Computation.](http://arxiv.org/abs/2307.08045) | 这篇论文研究了使用快速量子算法进行注意力计算，以加速大型语言模型 (LLMs) 的计算速度。 |
| [^3] | [Towards Flexible Time-to-event Modeling: Optimizing Neural Networks via Rank Regression.](http://arxiv.org/abs/2307.08044) | 本研究提出了一种深度AFT排名回归模型，用于灵活地进行时间事件建模，从而改善预测性能并减轻严格的假设。 |
| [^4] | [Bivariate DeepKriging for Large-scale Spatial Interpolation of Wind Fields.](http://arxiv.org/abs/2307.08038) | 本文提出了一种名为双变量深度克里金的方法，它利用空间相关的深度神经网络(DNN)和嵌入层以及基于自助法和集成DNN的无分布不确定性量化方法，用于大规模空间插值风场的预测和估计。 |
| [^5] | [Magnetic Field-Based Reward Shaping for Goal-Conditioned Reinforcement Learning.](http://arxiv.org/abs/2307.08033) | 本论文提出了一种基于磁场的奖励塑形方法，用于目标条件强化学习任务。通过将目标和障碍物视为永久磁铁，并根据磁场强度值建立奖励函数，解决了传统奖励塑形方法在动态环境中应用效果不好的问题。 |
| [^6] | [Noise-aware Speech Enhancement using Diffusion Probabilistic Model.](http://arxiv.org/abs/2307.08029) | 本文提出了一种基于噪声感知的语音增强方法，在扩散模型中引入噪声特定信息，通过噪声分类和多任务学习方案来增强噪声调节器的噪声特异性。证实该方法在VoiceBank-DEMAND数据集上取得了良好的实验效果。 |
| [^7] | [Revisiting Implicit Models: Sparsity Trade-offs Capability in Weight-tied Model for Vision Tasks.](http://arxiv.org/abs/2307.08013) | 本研究重新审视了隐式模型，并发现权重绑定模型在视觉任务中比DEQ变体更有效、稳定和高效。通过使用不同的稀疏掩模，我们提出了提高模型容量的方法。 |
| [^8] | [For One-Shot Decoding: Unsupervised Deep Learning-Based Polar Decoder.](http://arxiv.org/abs/2307.08004) | 本文提出了一种基于无监督深度学习的极化解码器的单次解码方法，通过自监督学习训练神经网络，消除了对预定义标签的依赖，实现了在通信系统实际数据上的直接训练，其性能接近最大后验概率解码器，并展示出更优越的泛化能力。 |
| [^9] | [LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network.](http://arxiv.org/abs/2307.07998) | LUCYD是一种特征驱动的解卷积网络，通过融合深层特征和Richardson-Lucy公式，提高了体积显微图像的恢复质量，降低了计算成本，并具有高度可解释性。 |
| [^10] | [MargCTGAN: A "Marginally'' Better CTGAN for the Low Sample Regime.](http://arxiv.org/abs/2307.07997) | MargCTGAN提出一个新的合成数据生成方法，在低样本情况下通过添加特征匹配的去相关边际，改善了CTGAN模型在实用性和统计属性方面的表现。 |
| [^11] | [A Survey of Techniques for Optimizing Transformer Inference.](http://arxiv.org/abs/2307.07982) | 本文综述了优化Transformer推理的技术，包括知识蒸馏、修剪、量化、神经架构搜索和轻量级网络设计。 |
| [^12] | [Byzantine-Robust Distributed Online Learning: Taming Adversarial Participants in An Adversarial Environment.](http://arxiv.org/abs/2307.07980) | 本文研究了在拜占庭攻击下的分布式在线学习中的对手行为。在拜占庭和恶意对手存在的情况下，尽管可以控制线性对手遗憾的常数，但分布式在线梯度下降只能实现线性对手遗憾上界。然而，在环境不完全对抗性的情况下，可以实现次线性的随机遗憾。 |
| [^13] | [Finite element inspired networks: Learning physically-plausible deformable object dynamics from partial observations.](http://arxiv.org/abs/2307.07975) | 该论文提出了一种基于有限元的网络模型，通过动力学网络和物理感知编码器，从部分观察中学习可变形物体的动力学，并通过正运动学解码器进行预测，实现了具有物理解释性的模型。 |
| [^14] | [Heteroscedastic Causal Structure Learning.](http://arxiv.org/abs/2307.07973) | 本研究提出了一种简单而有效的因果结构学习算法（HOST），通过利用正态性，能够在高斯噪声下恢复一个有效的因果排序，从而唯一确定因果DAG，并且算法的时间复杂度与样本大小和维度都以多项式规模扩展。 |
| [^15] | [Enhancing Energy Efficiency and Reliability in Autonomous Systems Estimation using Neuromorphic Approach.](http://arxiv.org/abs/2307.07963) | 本论文介绍了一种基于脉冲编码理论和脉冲神经网络的估计框架，通过设计网络的权重矩阵，实现了能效和可靠性的提升。 |
| [^16] | [Automated Polynomial Filter Learning for Graph Neural Networks.](http://arxiv.org/abs/2307.07956) | 本文提出了一种自动学习多项式滤波器的框架，名为Auto-Polynomial，在同质性和异质性图上取得了显著的性能提升。 |
| [^17] | [Accelerating Distributed ML Training via Selective Synchronization.](http://arxiv.org/abs/2307.07950) | 本文介绍了一种名为SelSync的方法，通过选择性同步，在保持准确性的前提下减少了分布式深度神经网络训练的时间开销。该方法根据每一步的重要性动态选择是否进行通信，达到了与批量同步并行（BSP）相同或更高的准确性。 |
| [^18] | [Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling.](http://arxiv.org/abs/2307.07944) | 本文通过提出一种适用于多类训练设置的新型ReDB框架来解决现有领域自适应方法在多类训练设置下性能下降的问题，通过产生可靠的、多样化的和类平衡的伪三维框来引导目标领域的自训练。 |
| [^19] | [KECOR: Kernel Coding Rate Maximization for Active 3D Object Detection.](http://arxiv.org/abs/2307.07942) | 本论文提出了一种内核编码率最大化（KECOR）策略，可以通过信息论的视角确定最具信息量的点云，以最小化标注所需的比特数。这种策略可以减轻LiDAR物体检测中的注释负担并提高计算效率。 |
| [^20] | [Optimal Compression of Unit Norm Vectors in the High Distortion Regime.](http://arxiv.org/abs/2307.07941) | 本研究探讨了在高失真情况下，将单位范数向量压缩到最少比特数的最优方法。研究发现，简单的压缩方案在这种情况下几乎是最优的。 |
| [^21] | [A Novel Truncated Norm Regularization Method for Multi-channel Color Image Denoising.](http://arxiv.org/abs/2307.07932) | 一种新颖的多通道彩色图像去噪方法，通过双加权截断核范数减去截断Frobenius范数最小化，利用非局部自相似性来提取相似的结构进行去噪，模型兼顾了跨通道差异和噪声的空间变化。 |
| [^22] | [On the Robustness of Split Learning against Adversarial Attacks.](http://arxiv.org/abs/2307.07916) | 该论文评估了分裂学习对抗敌对攻击的鲁棒性，特别关注不受信任服务器只能访问模型中间层的情况。通过仅向服务器公开部分模型，分裂学习可以缓解敌对攻击威胁。 |
| [^23] | [Exploiting FPGA Capabilities for Accelerated Biomedical Computing.](http://arxiv.org/abs/2307.07914) | 本研究利用FPGA技术提出了高级神经网络架构，并利用自定义的张量计算单元加速器对生物医学的心电图信号进行分析。研究的重点是提高算法的鲁棒性和实际应用的性能指标。 |
| [^24] | [Predicting mechanical properties of Carbon Nanotube (CNT) images Using Multi-Layer Synthetic Finite Element Model Simulations.](http://arxiv.org/abs/2307.07912) | 使用多层合成有限元模型模拟，通过基于AI的深度学习模型预测碳纳米管（CNT）图像的力学性能。将2D合成图像混合生成更接近于真实图像的MLS图像，并使用物理模型估计其力学性质。提出的CNTNeXt深度学习架构通过ResNeXt特征表示和随机森林回归估计器改进了预测精度。 |
| [^25] | [MESOB: Balancing Equilibria & Social Optimality.](http://arxiv.org/abs/2307.07911) | 本文提出了一种双目标优化方法MESOB及相应的MESOB-OMO方法，用于解决多层次、多智能体博弈中的竞争和合作问题。实验证明了其可在拍卖出价推荐等问题中平衡不同利益方面的有效性。 |
| [^26] | [Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation.](http://arxiv.org/abs/2307.07907) | 本文研究了如何在强化学习中使模型具有对虚假相关性的鲁棒性，这种虚假相关性是由于不可观察的混杂因素引起的，并且普遍存在于现实世界的任务中。 |
| [^27] | [Anomaly Detection in Automated Fibre Placement: Learning with Data Limitations.](http://arxiv.org/abs/2307.07893) | 本文提出了一种在数据有限的情况下，通过自动编码器进行异常检测的方法，利用纤维层片的深度图进行二分类，并使用重构误差作为量化指标。 |
| [^28] | [Multitemporal SAR images change detection and visualization using RABASAR and simplified GLR.](http://arxiv.org/abs/2307.07892) | 通过使用RABASAR和简化的GLR方法，在多时相SAR图像中进行变化检测和可视化，并开发了新的变化幅度指数方法和改进的变化分类方法。 |
| [^29] | [Handwritten and Printed Text Segmentation: A Signature Case Study.](http://arxiv.org/abs/2307.07887) | 本研究旨在解决手写和打印文本分割的挑战，并提出了一种新的方法来完整地恢复不同类别的文本，特别是在重叠部分提高分割性能。同时，还引入了一个新的数据集SignaTR6K，用于支持该任务。 |
| [^30] | [Gradient-free training of neural ODEs for system identification and control using ensemble Kalman inversion.](http://arxiv.org/abs/2307.07882) | 本论文研究了集合卡尔曼反演方法在训练神经ODE进行系统识别和控制任务中的有效性，并证明了EKI是一种高效的神经ODE训练方法，其解决方案的运行时间和质量与常用的基于梯度的优化器相媲美。 |
| [^31] | [Graph Embedded Intuitionistic Fuzzy RVFL for Class Imbalance Learning.](http://arxiv.org/abs/2307.07881) | 提出了一种嵌入图的直觉模糊RVFL模型，用于解决类别不平衡学习的问题。该模型通过采用加权机制处理不平衡的数据集，并利用图嵌入提取语义丰富的信息。该模型在类别不平衡学习中具有较高的准确性。 |
| [^32] | [Towards Understanding Adversarial Transferability From Surrogate Training.](http://arxiv.org/abs/2307.07873) | 本论文探索了对抗性可转移性的理解，特别关注替代训练。通过研究模型的平滑性和梯度相似性之间的权衡，发现对抗训练可以提高模型的替代能力。研究结果对数据分布的转变提出了新的推测。 |
| [^33] | [Does Double Descent Occur in Self-Supervised Learning?.](http://arxiv.org/abs/2307.07872) | 研究发现，在自监督学习中缺乏双下降现象，进一步的研究有助于揭示其理论基础。 |
| [^34] | [The SocialAI School: Insights from Developmental Psychology Towards Artificial Socio-Cultural Agents.](http://arxiv.org/abs/2307.07871) | 该论文讨论了AI研究应该受发展心理学启发，并研究使代理能够进入文化的社会认知能力。提出了社会AI学校工具以便于进行相关实验。 |
| [^35] | [Large Language Models as Superpositions of Cultural Perspectives.](http://arxiv.org/abs/2307.07870) | 大型语言模型被认为是具有个性或一套价值观的，但实际上它可以看作是具有不同价值观和个性特征的角度的叠加。通过角度可控性的概念，我们研究了大型语言模型在不同角度下展示的价值观和个性特征的变化。实验结果表明，即使在没有明显提示的情况下，大型语言模型也会表达出不同的价值观。 |
| [^36] | [Custom DNN using Reward Modulated Inverted STDP Learning for Temporal Pattern Recognition.](http://arxiv.org/abs/2307.07869) | 本文提出了一种使用奖励调节的反STDP学习的自定义DNN算法，用于在稀疏事件序列数据上进行时间模式识别。该算法通过组合多种学习方法，在短时间间隔的动态数据集中识别模式。在复杂的语音数字数据集上的实验结果表明，该算法具有较高的性能。 |
| [^37] | [Contrasting the efficiency of stock price prediction models using various types of LSTM models aided with sentiment analysis.](http://arxiv.org/abs/2307.07868) | 通过对比不同类型的LSTM模型和情感分析的效果，我们的研究寻求寻找最佳模型，以利用公司的预测和行业表现来正确预测股票价格的短期和长期目标。 |
| [^38] | [Benchmarking the Effectiveness of Classification Algorithms and SVM Kernels for Dry Beans.](http://arxiv.org/abs/2307.07863) | 本研究通过分析和比较不同的分类算法和SVM核函数在豆类数据集上的性能，发现RBF SVM核心算法在准确率、精确率、召回率和F1得分上表现最佳，提供了重要的指导。 |
| [^39] | [Variational Inference with Gaussian Score Matching.](http://arxiv.org/abs/2307.07849) | 本文提出了一种用高斯评分匹配的方法来进行变分推理，通过迭代算法将变分近似与精确后验的评分匹配。当变分分布是高斯分布时，内部优化问题有闭式解。 |
| [^40] | [Neural Video Recovery for Cloud Gaming.](http://arxiv.org/abs/2307.07847) | 本文提出了一个新方法，用于在云游戏中恢复丢失或损坏的视频帧。与传统方法不同的是，我们利用游戏状态和部分解码帧来提高恢复准确性。 |
| [^41] | [Transformers are Universal Predictors.](http://arxiv.org/abs/2307.07843) | Transformers架构在语言建模中具有通用的预测性质，并且在非渐近数据环境中表现良好。 |
| [^42] | [RegExplainer: Generating Explanations for Graph Neural Networks in Regression Task.](http://arxiv.org/abs/2307.07840) | 这项工作提出了一种新的解释方法（XAIG-R），用于解释图回归模型，通过引入信息瓶颈理论的新目标和混合框架来解决回归任务中的挑战，同时还使用对比学习策略来处理连续有序标签。 |
| [^43] | [MixupExplainer: Generalizing Explanations for Graph Neural Networks with Data Augmentation.](http://arxiv.org/abs/2307.07832) | 本文提出了一种通用的图神经网络解释方法MixupExplainer，通过引入广义图信息瓶颈（GIB）和图mixup方法来解决现有解释方法中存在的分布偏移问题。 |
| [^44] | [Minimal Random Code Learning with Mean-KL Parameterization.](http://arxiv.org/abs/2307.07816) | 本文研究了最小随机编码学习（MIRACLE）的两个变体，提出了一种新的参数化方法Mean-KL，在压缩变分贝叶斯神经网络中实现了更快的收敛和良好的预测性能。 |
| [^45] | [Graph Automorphism Group Equivariant Neural Networks.](http://arxiv.org/abs/2307.07810) | 本论文提供了图自同态群等变神经网络的完整特征化，找到了可学习的、线性的层函数之间的矩阵的生成集。 |
| [^46] | [The Interpolating Information Criterion for Overparameterized Models.](http://arxiv.org/abs/2307.07785) | 本文提出了一个插值信息准则，用于过参数化模型的模型选择问题。通过建立贝叶斯对偶形式，该准则将先验选择纳入模型评估，并考虑了先验误设、模型的几何和谱特性。该准则在实证和理论行为方面与已知结果一致。 |
| [^47] | [CatBoost Versus XGBoost and LightGBM: Developing Enhanced Predictive Models for Zero-Inflated Insurance Claim Data.](http://arxiv.org/abs/2307.07771) | 本文比较了CatBoost、XGBoost和LightGBM三种流行的梯度提升库在处理零膨胀保险理赔数据上的效果，并通过对两个不同数据集的分析，证明了CatBoost是最适合训练保险理赔数据和拟合精算频率模型的库。 |
| [^48] | [randomHAR: Improving Ensemble Deep Learners for Human Activity Recognition with Sensor Selection and Reinforcement Learning.](http://arxiv.org/abs/2307.07770) | randomHAR是一种集成深度学习方法，利用传感器选择和强化学习优化了人体活动识别，提高了性能。 |
| [^49] | [Real-time Traffic Classification for 5G NSA Encrypted Data Flows With Physical Channel Records.](http://arxiv.org/abs/2307.07756) | 本文研究了5G NSA加密数据流的实时流量分类方法，通过使用物理信道记录进行分析，并提出基于梯度提升算法的决策树分类方法。通过实时分析加密流量，可用于QoS管理和动态资源分配。 |
| [^50] | [Learning Expressive Priors for Generalization and Uncertainty Estimation in Neural Networks.](http://arxiv.org/abs/2307.07753) | 本文提出了一种用于神经网络的先验学习方法，通过利用可扩展和结构化的神经网络后验作为推广的信息先验，提高了神经网络的推广和不确定性估计能力。我们的方法在大规模上提供了表达性的概率表示，并产生了非空推广界限。我们的技术贡献是推导出可处理的目标函数，并提出了改进的推广界限计算方法。在经验上，我们证明了该方法在不确定性估计和推广方面的有效性。 |
| [^51] | [On the Utility Gain of Iterative Bayesian Update for Locally Differentially Private Mechanisms.](http://arxiv.org/abs/2307.07744) | 本文研究了使用迭代贝叶斯更新对局部差分隐私机制进行数据估计时的效用提升，结果表明在不增加额外隐私成本的情况下，IBU可以提供比传统矩阵求逆更好的效果。 |
| [^52] | [Negative probabilities in Gene Regulatory Networks.](http://arxiv.org/abs/2307.07738) | 该论文提出了一种基于基因表达和相关性符号的方法，用于识别基因之间的符号不确定共表达。通过构建具有符号不确定贡献的概率转移矩阵，可以量化基因调控网络中各种连接的结构和重要性，并解释其对网络几何结构的影响。 |
| [^53] | [A Nearly-Linear Time Algorithm for Structured Support Vector Machines.](http://arxiv.org/abs/2307.07735) | 这篇论文提出了针对结构化支持向量机的接近线性时间算法，解决了二次规划输入规模和解决时间的问题。 |
| [^54] | [Towards Optimal Neural Networks: the Role of Sample Splitting in Hyperparameter Selection.](http://arxiv.org/abs/2307.07726) | 本文通过揭示神经网络模型构建中的样本拆分方法的奥秘，构建了一个理论框架来解释神经网络的有效性。我们的研究结果表明，从样本拆分中得到的最优超参数可以使得神经网络模型最小化预测风险。 |
| [^55] | [Visual Analytics For Machine Learning: A Data Perspective Survey.](http://arxiv.org/abs/2307.07712) | 这篇论文总结了过去十年来利用可视化解释机器学习模型的研究，并从数据视角总结了这些作品。论文将ML模型处理的常见数据分为五类，并突出介绍了擅长学习这些数据的ML模型。此外，论文总结了在不同阶段操作这些数据类型的六个任务，并对VIS4ML领域的研究热点进行了分析。 |
| [^56] | [Identification of Stochasticity by Matrix-decomposition: Applied on Black Hole Data.](http://arxiv.org/abs/2307.07703) | 本文提出了一种基于矩阵分解的算法，用于识别随机性和非随机性的时间序列分类。该方法利用奇异值分解进行拓扑分析和主成分分析提取特征，通过支持向量机分类，成功应用于合成数据的实验。 |
| [^57] | [NeurASP: Embracing Neural Networks into Answer Set Programming.](http://arxiv.org/abs/2307.07700) | NeurASP是将神经网络集成到Answer Set Programming中的简单且有效的方法，通过以概率分布的形式处理神经网络输出，NeurASP能够将子符号和符号计算相结合，并通过应用符号推理改进神经网络的感知结果，并且可以通过使用ASP规则训练神经网络，使其从显式复杂语义约束中学习。 |
| [^58] | [Reducing operator complexity in Algebraic Multigrid with Machine Learning Approaches.](http://arxiv.org/abs/2307.07695) | 本文提出了一种基于机器学习的方法，通过使用神经网络和平滑测试向量，降低代数多重网格中非Galerkin粗网格操作符的复杂性。该方法在保持整体AMG收敛性的同时，能够有效解决参数化偏微分方程问题。 |
| [^59] | [Creating a Dataset Supporting Translation Between OpenMP Fortran and C++ Code.](http://arxiv.org/abs/2307.07686) | 本研究创建了一个数据集，用于训练机器学习模型在OpenMP Fortran和C++代码之间进行翻译。这个数据集通过精细的代码相似性测试确保了可靠性和适用性，并且能够显著提升大规模语言模型的翻译能力。 |
| [^60] | [Learning Subjective Time-Series Data via Utopia Label Distribution Approximation.](http://arxiv.org/abs/2307.07682) | 本研究提出了一种适用于时间序列数据的Utopia标签分布逼近（ULDA）方法，通过将训练标签分布与高斯核进行卷积来使其更接近真实但未知的标签分布，从而提高模型的公平性。 |
| [^61] | [Data-centric Operational Design Domain Characterization for Machine Learning-based Aeronautical Products.](http://arxiv.org/abs/2307.07681) | 该论文对基于机器学习的航空产品的操作设计领域进行了研究，采用了基于数据的方法。通过明确定义ODD参数的维度和对数据进行分类，确定了其对系统层面的影响。示例中以飞机飞行包线为例说明了这些概念。 |
| [^62] | [Sharp Convergence Rates for Matching Pursuit.](http://arxiv.org/abs/2307.07679) | 本文通过提升现有的下界来匹配最佳上界，对匹配追踪的性能进行了精确描述，并构造了一个最坏情况的字典来证明现有上界的无法改进。 |
| [^63] | [On the Robustness of Epoch-Greedy in Multi-Agent Contextual Bandit Mechanisms.](http://arxiv.org/abs/2307.07675) | 本研究展示了在多Agent上下文赌博机制中，最突出的上下文赌博算法$\epsilon$-greedy可以进行扩展，以解决同时存在的激励因素、上下文和损坏问题 |
| [^64] | [An Empirical Study of the Effectiveness of Using a Replay Buffer on Mode Discovery in GFlowNets.](http://arxiv.org/abs/2307.07674) | 本文通过实证研究，探讨了在GFlowNets中使用回放缓冲区的有效性，评估了不同回放缓冲区采样技术对模式发现速度和质量的影响。 |
| [^65] | [Efficient Adversarial Attacks on Online Multi-agent Reinforcement Learning.](http://arxiv.org/abs/2307.07670) | 本研究探讨了对在线多智能体强化学习（MARL）的对抗攻击的影响，首先展示了单独进行动作污染和奖励污染攻击的局限性，然后引入了一种混合攻击策略，该策略可以高效地攻击MARL智能体，即使攻击者没有先验信息。 |
| [^66] | [Efficient Action Robust Reinforcement Learning with Probabilistic Policy Execution Uncertainty.](http://arxiv.org/abs/2307.07666) | 本文研究了具有概率策略执行不确定性的行动鲁棒增强学习问题，并提出了ARRLC算法，该算法在遗憾和样本复杂度上达到了极小极大最优，实验证明其优于非鲁棒算法并且收敛更快。 |
| [^67] | [Machine learning for option pricing: an empirical investigation of network architectures.](http://arxiv.org/abs/2307.07657) | 广义高速公路网络结构在期权定价问题中的应用表现出更高的准确性和更短的训练时间。 |
| [^68] | [DIGEST: Fast and Communication Efficient Decentralized Learning with Local Updates.](http://arxiv.org/abs/2307.07652) | 本文提出了一种名为DIGEST的快速和通信高效的异步分散学习机制，通过结合Gossip和随机游走的思想，并专注于随机梯度下降（SGD），实现了在分散学习中较低的通信成本和较快的收敛时间。 |
| [^69] | [SALC: Skeleton-Assisted Learning-Based Clustering for Time-Varying Indoor Localization.](http://arxiv.org/abs/2307.07650) | SALC是一种基于骨架辅助的学习聚类定位系统，可以适应时变室内环境，提高定位准确性。 |
| [^70] | [DistTGL: Distributed Memory-Based Temporal Graph Neural Network Training.](http://arxiv.org/abs/2307.07649) | DistTGL是一种在分布式GPU集群上训练内存化TGNN的高效可扩展解决方案，相比现有方法在精度和训练吞吐量上都有显著提高。 |
| [^71] | [Towards Model-Size Agnostic, Compute-Free, Memorization-based Inference of Deep Learning.](http://arxiv.org/abs/2307.07631) | 本研究提出了一种基于记忆的推理方法（MBI），可以实现无需计算的深度学习推理。该方法利用了循环注意模型（RAM）的推理机制，并通过存储键值对的表来避免计算。 |
| [^72] | [Generalizable Embeddings with Cross-batch Metric Learning.](http://arxiv.org/abs/2307.07620) | 通过跨批次度量学习，我们提出了一种基于可学习原型的全局平均汇聚方法，用于学习通用实体以表示未见过的类别。 |
| [^73] | [Efficiently Factorizing Boolean Matrices using Proximal Gradient Descent.](http://arxiv.org/abs/2307.07615) | 通过使用连续松弛和弹性二元正则化器，我们提出了一种用近端梯度下降有效地分解布尔矩阵的方法。我们在合成数据和实际数据上进行了广泛的实验，证明了方法的快速收敛和准确性，优于现有技术，结果易于解释和语义有意义。 |
| [^74] | [Towards Generalizable Detection of Urgency of Discussion Forum Posts.](http://arxiv.org/abs/2307.07614) | 本研究旨在解决学生在线课程讨论论坛问题的规模扩展难题。通过构建预测模型，自动确定论坛帖子的紧急程度，并提供给教师注意。与之前的工作不同，本研究通过预测7分制紧急程度级别，实现了更细粒度的预测。通过在大规模数据集上的训练和测试，证明了模型的通用性。 |
| [^75] | [First-order Methods for Affinely Constrained Composite Non-convex Non-smooth Problems: Lower Complexity Bound and Near-optimal Methods.](http://arxiv.org/abs/2307.07605) | 这项研究介绍了一种用于解决复合非凸非光滑优化问题的一阶方法，并建立了其最低复杂度下界以及提出了一种近似的IPG方法。 |
| [^76] | [Smooth Lower Bounds for Differentially Private Algorithms via Padding-and-Permuting Fingerprinting Codes.](http://arxiv.org/abs/2307.07604) | 本论文提出了一种通过填充和置换指纹编码的方法来产生困难实例，从而在各种情景下提供平滑下界。这方法适用于差分隐私平均问题和近似k. |
| [^77] | [Training Discrete Energy-Based Models with Energy Discrepancy.](http://arxiv.org/abs/2307.07595) | 该论文提出了一种用能量差异训练离散能量模型的方法，该方法不依赖于采样策略，通过评估数据点及其扰动对应点的能量函数来实现，能够为各种扰动过程提供理论保证，并在不同数据集上展示了其相对性能。 |
| [^78] | [A Quantitative Approach to Predicting Representational Learning and Performance in Neural Networks.](http://arxiv.org/abs/2307.07575) | 本文介绍了一种使用伪核的工具，可以分析和预测神经网络中学习到的表示。该方法可以预测权重初始化和训练计划对表示学习和并发多任务性能的影响。 |
| [^79] | [Harpa: High-Rate Phase Association with Travel Time Neural Fields.](http://arxiv.org/abs/2307.07572) | 本论文提出了一种名为Harpa的高速率地震相位关联方法，利用深度神经场构建波速和相关走时的生成模型，即使在波速未知的情况下也能实现相位关联。这种方法能够处理较小、高速率的地震事件，提供了关于地下弹性介质属性的宝贵描述。 |
| [^80] | [Variational Prediction.](http://arxiv.org/abs/2307.07568) | 本文介绍了变分预测这一技术，通过使用变分界直接学习后验预测分布，避免了边缘化成本，并展示了在玩具例子上的实验结果。 |
| [^81] | [Reconstruction of 3-Axis Seismocardiogram from Right-to-left and Head-to-foot Components Using A Long Short-Term Memory Network.](http://arxiv.org/abs/2307.07566) | 本研究开发了一个深度学习模型，可从右到左和头到脚的心搏图成分中预测背腹方向的信号。使用长短期记忆网络进行训练和验证，模型表现良好，均方误差为0.09。 |
| [^82] | [Source-Free Domain Adaptation with Temporal Imputation for Time Series Data.](http://arxiv.org/abs/2307.07542) | 本文提出了一种用于时序数据的无源领域适应方法MAPU，通过随机掩蔽和时间插补的方式，捕捉源领域的时间信息并引导目标模型产生目标结果。 |
| [^83] | [Improved Self-Normalized Concentration in Hilbert Spaces: Sublinear Regret for GP-UCB.](http://arxiv.org/abs/2307.07539) | 本文提出了对GP-UCB算法进行改进，使其具有几乎最优的次线性遗憾，并解决了关于遗憾分析的开放问题。 |
| [^84] | [Learning Multiple Coordinated Agents under Directed Acyclic Graph Constraints.](http://arxiv.org/abs/2307.07529) | 本文提出了一种在有向无环图约束下学习多个协调代理的新方法，通过利用DAG结构，提高了学习性能，并在实际环境中的多个任务上取得了优于其他非DAG方法的结果。 |
| [^85] | [Machine Learning for Autonomous Vehicle's Trajectory Prediction: A comprehensive survey, Challenges, and Future Research Directions.](http://arxiv.org/abs/2307.07527) | 这项综合研究调查了自主车辆的轨迹预测方法，通过借鉴现有文献并重点关注机器学习技术，特别是深度学习和强化学习。研究总结了目前的挑战，并提出了未来研究的方向。 |
| [^86] | [The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence.](http://arxiv.org/abs/2307.07522) | 生成型人工智能和大型语言模型可能为基础科学的发现提供机会，通过其自主生成假设和探索假设空间的闭环方法，加速科学发现的进程。 |
| [^87] | [Voting-based Multimodal Automatic Deception Detection.](http://arxiv.org/abs/2307.07516) | 本文提出了一种基于投票的多模态方法用于自动欺骗检测，通过视频的音频、视觉和文本特征进行检测。实验结果表明，我们的解决方案在欺骗检测中表现优于现有技术。 |
| [^88] | [Explainability is NOT a Game.](http://arxiv.org/abs/2307.07514) | Shapley values may provide misleading measures of relative feature importance in XAI, challenging their proposed uses in high-stakes application domains. |
| [^89] | [An empirical study of using radiology reports and images to improve ICU mortality prediction.](http://arxiv.org/abs/2307.07513) | 本研究利用放射学报告和图像构建了一个基于深度学习的多模态数据生存预测模型，用于预测重症监护病房（ICU）的死亡率，并在MIMIC-IV数据集上取得了0.7829的平均C-index。 |
| [^90] | [A Context-Aware Cutting Plane Selection Algorithm for Mixed-Integer Programming.](http://arxiv.org/abs/2307.07322) | 本文提出了一个上下文感知的混合整数规划切割平面选择算法，通过引入新的评分指标、过滤技术和停止准则，使得SCIP在MIPLIB 2017基准集上的性能提升了4%。 |
| [^91] | [On Interpolating Experts and Multi-Armed Bandits.](http://arxiv.org/abs/2307.07264) | 学习专家建议和多臂赌博是两个经典的在线决策问题，我们研究了两者之间的插值问题。我们提出了$\mathbf{m}$-MAB的极小后悔界并设计了$\mathbf{m}$-BAI的最优PAC算法，该算法旨在以尽可能少的轮数确定损失最小的臂。 |
| [^92] | [Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-Training.](http://arxiv.org/abs/2307.07246) | 该论文提出了一个名为KoBo的框架，它通过将临床知识整合到视觉语言语义一致性学习中，解决了医学领域中的大规模语义重叠和转移问题。 |
| [^93] | [DataAssist: A Machine Learning Approach to Data Cleaning and Preparation.](http://arxiv.org/abs/2307.07119) | DataAssist是一种机器学习方法，用于提高数据集质量和节省数据清洗和准备时间。 |
| [^94] | [Provably Faster Gradient Descent via Long Steps.](http://arxiv.org/abs/2307.06324) | 本研究通过计算机辅助分析技术，证明了非常数步长策略下的梯度下降方法经过长距离步骤可以实现更快的收敛速度。 |
| [^95] | [Newell's theory based feature transformations for spatio-temporal traffic prediction.](http://arxiv.org/abs/2307.05949) | 本文提出了一种基于Newell理论的特征转换方法用于时空交通预测，用于改善模型在不同位置的迁移性问题。 |
| [^96] | [Differential Analysis of Triggers and Benign Features for Black-Box DNN Backdoor Detection.](http://arxiv.org/abs/2307.05422) | 本文提出了一种在黑盒情况下检测深度神经网络后门攻击的高效方法，通过量化触发器和良性特征对确定后门网络输出的影响，并使用训练好的新颖性检测器进行检测。 |
| [^97] | [Combating Data Imbalances in Federated Semi-supervised Learning with Dual Regulators.](http://arxiv.org/abs/2307.05358) | 本文提出了一种带有双调节器的新型联邦半监督学习框架FedDure，解决了数据分布不平衡的问题。通过粗调节器和细调节器对本地模型的更新进行规范，以及学习适应性加权方案，适应不同的数据分布。 |
| [^98] | [Supervised Attention Using Homophily in Graph Neural Networks.](http://arxiv.org/abs/2307.05217) | 本文提出了一种新技术，可以在任何图注意力模型中应用，以鼓励共享相同类别标签的节点获得更高的注意力分数，并在多个节点分类数据集上展示了比标准基线模型更高的性能。 |
| [^99] | [Hybrid hidden Markov LSTM for short-term traffic flow prediction.](http://arxiv.org/abs/2307.04954) | 该论文介绍了一种混合隐马尔可夫LSTM模型，用于短期交通流量预测。研究发现，深度学习方法在预测交通变量方面优于传统的参数模型。这种模型结合了循环神经网络和隐马尔可夫模型的优势，能够捕捉交通系统的复杂动态模式和非平稳性。 |
| [^100] | [Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans.](http://arxiv.org/abs/2307.04525) | 本研究提出了一种簇诱导蒙版变换器，在非对比CT扫描中使用深度学习方法进行胃癌筛查。该方法通过自注意和交叉注意与卷积特征交互，以多任务方式分割肿瘤并分类异常，实验结果表明，在保留测试集上达到了85.0%的敏感性和92.6%的特异性，相比两名放射科医生具有更好的性能。 |
| [^101] | [ECS -- an Interactive Tool for Data Quality Assurance.](http://arxiv.org/abs/2307.04368) | 本文提出了一种交互工具ECS，用于保证数据质量。该工具能够检测出在安全关键系统中具有潜在危害属性的数据点。 |
| [^102] | [ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey.](http://arxiv.org/abs/2307.04251) | ChatGPT是由OpenAI创建的一种大型语言模型（LLM），它在自然语言处理（NLP）领域引起了革命性的变革。它是第一个实现公众大规模与生成式人工智能（GAI）互动的关键技术，也引发了类似技术的研究兴趣和应用探索。 |
| [^103] | [Extending the Forward Forward Algorithm.](http://arxiv.org/abs/2307.04205) | 这个论文扩展了前向前向算法，首先在IMDb数据集上进行了情感分析任务，其次引入了金字塔优化策略来改进损失阈值，最后通过参数可视化得出了一些重要的洞察。 |
| [^104] | [Beyond Intuition, a Framework for Applying GPs to Real-World Data.](http://arxiv.org/abs/2307.03093) | 提出了一个框架，用于确定高斯过程在实际问题中的适用性，并建立一个稳健且明确的模型。通过对核函数设计和计算可扩展性选项的指导，该框架在冰川高程变化的案例研究中实现了更准确的结果。 |
| [^105] | [An explainable model to support the decision about the therapy protocol for AML.](http://arxiv.org/abs/2307.02631) | 本文提出了一种可解释的机器学习模型，用于支持AML患者治疗方案的决策，解决了当前风险分类存在的问题和专家需求额外测试和分析的困扰。 |
| [^106] | [PIGNet2: A Versatile Deep Learning-based Protein-Ligand Interaction Prediction Model for Binding Affinity Scoring and Virtual Screening.](http://arxiv.org/abs/2307.01066) | 提出了一种用于蛋白质-配体相互作用预测的多功能深度学习模型PIGNet2，通过引入数据增强策略和物理原理的图神经网络解决了少样本问题，实现了准确评分和高效筛选。 |
| [^107] | [An Adaptive Optimization Approach to Personalized Financial Incentives in Mobile Behavioral Weight Loss Interventions.](http://arxiv.org/abs/2307.00444) | 这项研究研究了在移动行为减重干预中个性化金融激励的自适应优化方法。问题主要在于如何有效地分配有限的干预预算和高昂的资源，以提供最佳的激励结构来促进参与者的依从性和行为改变。 |
| [^108] | [Learning from Synthetic Human Group Activities.](http://arxiv.org/abs/2306.16772) | 提出了M3Act，一个多视图多团队多人的人类原子动作和团队活动数据生成器，通过Unity引擎驱动实现。该生成器具有大规模数据生成、多模态和高质量注释等特点，能够用于研究复杂的人类互动和团队活动。 |
| [^109] | [A First Order Meta Stackelberg Method for Robust Federated Learning.](http://arxiv.org/abs/2306.13800) | 本研究提出了一种鲁棒的联邦学习防御方法，使用元Stackelberg学习算法解决贝叶斯Stackelberg马尔科夫博弈，实现自适应防御，与现有技术相匹配并在实验中表现出色。 |
| [^110] | [Predicting Grokking Long Before it Happens: A look into the loss landscape of models which grok.](http://arxiv.org/abs/2306.13253) | 本文提出了一种低成本方法来预测神经网络中的理解前浪潮，即通过研究前几轮的学习曲线来判断后续是否出现理解前浪潮。使用波形振荡和学习曲线的频谱特征值可以高精度地预测理解前浪潮。 |
| [^111] | [Dual Node and Edge Fairness-Aware Graph Partition.](http://arxiv.org/abs/2306.10123) | 这项论文提出了一种双节点和边公平感知的图分割方法，通过考虑节点平衡和边平衡来实现公平分割，并使用协嵌入框架学习表示。实验证明该方法在节点和边方面均具有平衡分割和良好效用，并可用作伪标签来促进图神经网络的公平行为。 |
| [^112] | [DoubleAdapt: A Meta-learning Approach to Incremental Learning for Stock Trend Forecasting.](http://arxiv.org/abs/2306.09862) | DoubleAdapt是一个增量学习的方法，用于股票趋势预测。它利用元学习技术自动学习如何将股票数据适应到本地平稳分布空间中，从而有效地适应数据和模型，减轻分布漂移的影响。 |
| [^113] | [Pruning the Way to Reliable Policies: A Multi-Objective Deep Q-Learning Approach to Critical Care.](http://arxiv.org/abs/2306.08044) | 该论文介绍了一种深度Q学习方法，通过剪枝动作集来实现将中间生物标志物信号整合到奖励规范中，提高了重症护理策略的可靠性。 |
| [^114] | [Parameter-free version of Adaptive Gradient Methods for Strongly-Convex Functions.](http://arxiv.org/abs/2306.06613) | 本文提出了一种无参数版本的自适应梯度方法，该方法应用于强凸函数时不再依赖于参数λ和学习率η。主要思想是同时运行多个专家并将他们的预测结果合并到主算法中，这使得该算法具有O(d log T)的遗憾边界。 |
| [^115] | [K-Tensors: Clustering Positive Semi-Definite Matrices.](http://arxiv.org/abs/2306.06534) | 本文介绍了一种针对正半定矩阵的自一致性聚类算法（K-张量），通过考虑其特征结构，能够有效地将正半定矩阵进行分区。 |
| [^116] | [PotatoPestNet: A CTInceptionV3-RS-Based Neural Network for Accurate Identification of Potato Pests.](http://arxiv.org/abs/2306.06206) | 该论文提出了一种基于AI的自动马铃薯害虫识别系统PotatoPestNet，使用了八种马铃薯害虫的数据集和五种预训练转移学习模型进行模型训练，并利用随机搜索优化进行超参数调整，能够准确识别不同种类的马铃薯害虫。 |
| [^117] | [Unconstrained Online Learning with Unbounded Losses.](http://arxiv.org/abs/2306.04923) | 本论文提出了一种算法，可用于解决无界域和非Lipschitz损失的在线学习问题，并提供了一个遗憾的度量，以衡量该算法的性能。此外，我们还利用该算法开发了一种新的鞍点优化算法，即使在没有有意义的曲率的情况下，也能够在无界领域中收敛于对偶间隙。最后，我们提供了一种算法，在无界域和非Lipschitz损失的情况下实现了非平凡的动态遗憾，以及相匹配的下界。 |
| [^118] | [Modeling Human-like Concept Learning with Bayesian Inference over Natural Language.](http://arxiv.org/abs/2306.02797) | 该论文通过在自然语言中进行贝叶斯推理来模拟人类类人概念学习，使用大型语言模型作为提议分布并拟合先验以更好地模拟人类学习者，并在生成性和逻辑性概念上进行实验评估。 |
| [^119] | [Navigating Explanatory Multiverse Through Counterfactual Path Geometry.](http://arxiv.org/abs/2306.02786) | 该论文提出了解释性多元宇宙的概念，用于导航和比较所有可能的反事实路径的几何关系。 |
| [^120] | [A Study of Situational Reasoning for Traffic Understanding.](http://arxiv.org/abs/2306.02520) | 本研究提出了三个新的基于文本的交通领域情境推理任务，旨在评估语言模型在情境决策、事件因果关系推理和解决人类驾驶考试方面的能力。研究采用了四种知识增强方法，具有潜力在不同语言推理任务中实现模型的泛化能力。 |
| [^121] | [DWT-CompCNN: Deep Image Classification Network for High Throughput JPEG 2000 Compressed Documents.](http://arxiv.org/abs/2306.01359) | 这篇论文提出了一种名为DWT-CompCNN的深度学习模型，它可以直接对使用HTJ2K算法压缩的文档进行分类，从而提高计算效率。 |
| [^122] | [Towards Fair Disentangled Online Learning for Changing Environments.](http://arxiv.org/abs/2306.01007) | 本论文提出了一种面向变化环境的在线学习算法，该算法通过将模型参数划分为环境不变部分和环境特定部分，从而实现了数据公平性。通过大量的实验，证明了该算法的有效性。 |
| [^123] | [Deep learning and MCMC with aggVAE for shifting administrative boundaries: mapping malaria prevalence in Kenya.](http://arxiv.org/abs/2305.19779) | 本研究提出了一种利用aggVAE进行深度学习和MCMC处理行政边界变化的解决方案，可以更准确地映射以县为层级的聚合级别数据，并处理行政边界的变化，相比最先进的模型表现更好。 |
| [^124] | [W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition.](http://arxiv.org/abs/2305.18624) | W-procer是一种基于加权原型对比学习的医学少样本命名实体识别方法，在构建基于原型的对比损失和加权网络方面具有创新性，优于现有的最先进方法。 |
| [^125] | [Federated Learning of Gboard Language Models with Differential Privacy.](http://arxiv.org/abs/2305.18465) | 本文讨论了在Gboard中使用联合学习和差分隐私(DP)训练语言模型(LMs)的方法，提出了新的客户参与标准，在实现有意义的形式DP保证的同时提供了有利的隐私-效用交换。在对公共数据进行预训练的基础上，我们训练并部署了超过20个LMs以实现高效用和$\rho-$zCDP隐私保证。 |
| [^126] | [Leveraging Human Feedback to Evolve and Discover Novel Emergent Behaviors in Robot Swarms.](http://arxiv.org/abs/2305.16148) | 该论文提出了一种利用人类反馈的方法来发现机器人群体中的新的 emergent behaviors。通过学习群体行为的相似性，结合新奇搜索和聚类技术，自动发现可能出现的群体行为，同时引入启发式方法提高搜索效率。该方法在仿真场景中进行了测试。 |
| [^127] | [Optimal Preconditioning and Fisher Adaptive Langevin Sampling.](http://arxiv.org/abs/2305.14442) | 通过最优预条件和费舍尔自适应 Langevin 采样，提出了一种计算有效且在高维中非常强健的自适应 MCMC 方案。 |
| [^128] | [Systematic Literature Review on Application of Machine Learning in Continuous Integration.](http://arxiv.org/abs/2305.12695) | 本文对过去22年来机器学习在持续集成中的应用进行了系统性综述，识别和描述了相关技术和方法，包括数据工程、特征工程、超参数调优等。同时，总结了持续集成测试的阶段、数据来源和特征类型，并提出了评估方法和指标。 |
| [^129] | [The Waymo Open Sim Agents Challenge.](http://arxiv.org/abs/2305.12032) | Waymo开放模拟代理挑战赛提出使用真实、互动的智能体仿真以促进自动驾驶行为模型的评估和训练，是该领域的首个公开挑战赛，旨在推动逼真模拟器的设计。 |
| [^130] | [FedDWA: Personalized Federated Learning with Online Weight Adjustment.](http://arxiv.org/abs/2305.06124) | 本文提出了一种个性化联邦学习算法，名为FedDWA，采用动态权重调整来保护数据隐私并以更少的通信开销捕捉客户之间的相似性，能够训练高精度和高效的个性化模型。 |
| [^131] | [Assessment of Reinforcement Learning Algorithms for Nuclear Power Plant Fuel Optimization.](http://arxiv.org/abs/2305.05812) | 本文提出了一种基于深度强化学习的方案，以解决核电站燃料优化问题，能够提高核电站的性能和安全。 |
| [^132] | [GPT for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering.](http://arxiv.org/abs/2305.03403) | 介绍了一种名为CAAFE的上下文感知自动特征工程方法，它利用大型语言模型根据数据集描述生成更多具有语义意义的特征，能够提高大多数数据集的性能，平均ROC AUC表现提高至0.822。 |
| [^133] | [Subsample Ridge Ensembles: Equivalences and Generalized Cross-Validation.](http://arxiv.org/abs/2304.13016) | 研究了比例渐近情形下的子采样岭回归集成，证明了最优全岭回归集成的风险与最优岭预测器的风险相匹配，并证明了GCV在估计岭回归集合的预测风险方面的强一致性。 |
| [^134] | [CAFIN: Centrality Aware Fairness inducing IN-processing for Unsupervised Representation Learning on Graphs.](http://arxiv.org/abs/2304.04391) | CAFIN是一种基于节点中心性的公平性增强进程技术，用于无监督学习的图表示学习方法中。实验结果表明，CAFIN在提供最优公平结果的同时，具有竞争力或更好的下游任务性能。 |
| [^135] | [Last-Layer Fairness Fine-tuning is Simple and Effective for Neural Networks.](http://arxiv.org/abs/2304.03935) | 本文提出了一种新颖的公平性微调神经网络的框架，利用已预训练的神经网络和不关注公平性的损失微调神经网络的最后一层。实验证明该方法在基准数据集上实现了高水平的公平性，同时保留标准性能指标。 |
| [^136] | [Torch-Choice: A PyTorch Package for Large-Scale Choice Modelling with Python.](http://arxiv.org/abs/2304.01906) | 本文介绍了一款名为 Torch-Choice 的 PyTorch 软件包，用于管理数据库、构建多项式Logit和嵌套Logit模型，并支持GPU加速，具有灵活性和高效性。 |
| [^137] | [Learning from Similar Linear Representations: Adaptivity, Minimaxity, and Robustness.](http://arxiv.org/abs/2303.17765) | 本文提出了两种算法，适应相似性结构并对异常值任务具有稳健性，适用于表示多任务学习和迁移学习设置。 |
| [^138] | [Bridging Imitation and Online Reinforcement Learning: An Optimistic Tale.](http://arxiv.org/abs/2303.11369) | 本文提出了两种算法，iPSRL和iRLSVI，旨在解决给定离线演示数据集的问题，可以显著减少强化学习中的遗憾，桥接了在线 RL 和模仿学习。 |
| [^139] | [Computationally Budgeted Continual Learning: What Does Matter?.](http://arxiv.org/abs/2303.11165) | 本文针对持续学习中计算预算的限制问题进行了研究，通过大规模基准测试评估了传统持续学习方法在计算受限环境中的性能，并探讨了不同采样策略、蒸馏损失和部分微调方法在数据递增、类递增和时间递增设置中的效果。 |
| [^140] | [Ref-NeuS: Ambiguity-Reduced Neural Implicit Surface Learning for Multi-View Reconstruction with Reflection.](http://arxiv.org/abs/2303.10840) | Ref-NeuS通过减少反射表面的影响以降低歧义，在多视角3D重建中取得了显著的进展。 |
| [^141] | [Batch Updating of a Posterior Tree Distribution over a Meta-Tree.](http://arxiv.org/abs/2303.09705) | 本文提出了一个更高效的批量更新方法，用于在元树上计算后验分布。 |
| [^142] | [Learning to Reconstruct Signals From Binary Measurements.](http://arxiv.org/abs/2303.08691) | 该论文提出了一种新的自监督学习方法SSBM，它只需要二进制数据进行训练，并探索了从不完整的二进制观察中学习的极端情况。这为从二进制测量中恢复信号提供了必要和充分条件，并在一系列真实数据集上展示了SSBM的卓越表现。 |
| [^143] | [Feature representations useful for predicting image memorability.](http://arxiv.org/abs/2303.07679) | 本研究使用Brain-Score评估64个CNN模型中与图像记忆力有关的特征表示，并发现高记忆力预测准确性的层与颞下皮质（IT）的脑部相似性更高。 |
| [^144] | [Considerations on the Theory of Training Models with Differential Privacy.](http://arxiv.org/abs/2303.04676) | 本论文提供了对差分隐私训练模型理论的考虑。研究了在联邦学习中的差分隐私保护方法，包括高斯差分隐私和差分隐私随机梯度下降。提供了框架和可证明性质的概述。 |
| [^145] | [Gradient-Free Structured Pruning with Unlabeled Data.](http://arxiv.org/abs/2303.04185) | 本文提出了一种使用无标签数据的无梯度结构化剪枝方法，在GLUE和SQuAD基准测试上的实验证明了其有效性，仅需几分钟就能将原始FLOP计数的最高40%减少而准确度仅下降不超过4%。 |
| [^146] | [CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning.](http://arxiv.org/abs/2303.03323) | CleanCLIP是一个通过独立重新对齐个别模态的表示来削弱后门攻击引入的虚假关联的微调框架。 |
| [^147] | [Modulated Neural ODEs.](http://arxiv.org/abs/2302.13262) | 变调神经ODEs （MoNODEs）是一种新的框架，能够将动力学状态与基础静态变化因素分开，并改进了现有的神经ODE方法。该方法通过引入时间不变的调制变量来捕捉轨迹间的变化，并在测试中展现出在振荡系统、视频和人类行走轨迹等方面具有提高模型泛化能力的效果。 |
| [^148] | [DoG is SGD's Best Friend: A Parameter-Free Dynamic Step Size Schedule.](http://arxiv.org/abs/2302.12022) | 我们提出了一个参数-free 的动态 SGD 步长公式，称为梯度距离公式（DoG）， 它没有“学习率”参数，但是在局部有界的随机梯度优化中拥有强大的无参数收敛性，并在广泛的视觉和语言转移学习任务中的表现与有调整学习率的 SGD 相当接近。 |
| [^149] | [Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness.](http://arxiv.org/abs/2302.10893) | 这篇论文提出了一种名为“公平扩散”的新策略，可以在生成文本到图像模型部署后减轻偏见并使模型接受公平性指导。 |
| [^150] | [Free-Form Variational Inference for Gaussian Process State-Space Models.](http://arxiv.org/abs/2302.09921) | 本文提出了一种自由形式的变分推断方法，用于高斯过程状态空间模型（GPSSMs）。该方法克服了以前方法的缺点，并展示了在计算效率和推断准确性上的优势。 |
| [^151] | [PAC-Bayesian Generalization Bounds for Adversarial Generative Models.](http://arxiv.org/abs/2302.08942) | 将PAC-Bayesian理论扩展到生成模型，为基于Wasserstein距离和总变差距离的模型提供了泛化界，为Wasserstein GAN和Energy-Based GAN提供了新的训练目标，并在合成数据集上展示出非虚空泛化界。 |
| [^152] | [3D-aware Blending with Generative NeRFs.](http://arxiv.org/abs/2302.06608) | 这篇论文提出了一种使用生成式NeRF的3D感知融合方法，通过3D感知对齐和融合来解决输入图像不对齐的问题，该方法在FFHQ和AFHQ-Cat上验证了优于现有2D方法的性能。 |
| [^153] | [CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection.](http://arxiv.org/abs/2302.06294) | CholecTriplet2022是一个手术行动三元组检测的挑战，通过将手术活动形式化为工具、行动和目标解剖结构的三元组来帮助开发更好的图像引导手术的人工智能辅助。本挑战从识别扩展到检测手术行动三元组建模，并包括对每个工具的边界框定位和对工具-活动的建模。 |
| [^154] | [A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models.](http://arxiv.org/abs/2302.06235) | 这项工作提出了一种简单的零样本提示加权技术，通过提示集成来自动化提示工程，从而提高文本-图像模型的零样本分类准确性。 |
| [^155] | [Exploiting Partial Common Information Microstructure for Multi-Modal Brain Tumor Segmentation.](http://arxiv.org/abs/2302.02521) | 本文提出了一种利用部分共同信息微结构进行多模态脑肿瘤分割的方法，通过识别部分共同信息的潜在微结构，显著提高了图像分割模型的判别能力。 |
| [^156] | [Robust empirical risk minimization via Newton's method.](http://arxiv.org/abs/2301.13192) | 本研究提出了一种鲁棒经验风险最小化的新的牛顿方法变种，并通过使用鲁棒估计方法来替换梯度和海森矩阵，证明了连续迭代收敛到种群水平最小化器周围小球。该方法在广义线性模型中的应用具有潜在的优势，并提出了一种基于共轭梯度方法的算法来获取鲁棒牛顿方向。 |
| [^157] | [Composer's Assistant: An Interactive Transformer for Multi-Track MIDI Infilling.](http://arxiv.org/abs/2301.12525) | Composer's Assistant是一种用于在REAPER音频工作站中进行人机交互创作的系统，它通过训练一个T5-like模型来实现对多轨MIDI填充的任务。该系统提供了一组脚本，使用户能够与模型进行交互，并进行了客观和主观的测试。完整的系统包括源代码、预训练模型和REAPER脚本。 |
| [^158] | [Online discovering governing differential equations of evolving systems from streaming data.](http://arxiv.org/abs/2301.07863) | 本研究提出了一种在线建模方法，逐一处理流数据，可以有效地识别演化系统的微分方程，尤其是变化后的系统产生的测量分布与以前不同的情况下。 |
| [^159] | [StitchNet: Composing Neural Networks from Pre-Trained Fragments.](http://arxiv.org/abs/2301.01947) | StitchNet提出了一种新的神经网络创建方式，它通过组合预训练神经网络的片段来创建高性能的网络，无需传统训练的大量计算资源和数据要求。通过居中核对齐（CKA），可以有效指导片段的选择，以满足特定准确性需求和计算资源限制。此外，StitchNet还可以实现即时个性化模型创建和推断。 |
| [^160] | [Generalization Bounds for Few-Shot Transfer Learning with Pretrained Classifiers.](http://arxiv.org/abs/2212.12532) | 该论文研究了基于预训练分类器学习的表示在少样本迁移学习中的通用性界限，提出了类别特征变异崩塌现象的理论解释，该现象使得在新类别上通过少样本学习的特征映射具有较小的误差。 |
| [^161] | [Establishing a stronger baseline for lightweight contrastive models.](http://arxiv.org/abs/2212.07158) | 本论文旨在为轻量级对比模型建立更强的基准线，解决了在自监督对比学习中使用效率型网络的性能下降问题。通过优化训练配置和引入平滑的损失函数，实现了更好的性能。 |
| [^162] | [Learning Quantum Processes and Hamiltonians via the Pauli Transfer Matrix.](http://arxiv.org/abs/2212.04471) | 本文通过学习量子过程和哈密顿量，展示了通过量子存储可以实现学习任意量子过程的指数级量子优势，并解决了学习 Pauli 传输矩阵和预测不同情况下的波函数期望值的任务。 |
| [^163] | [\{kappa}HGCN: Tree-likeness Modeling via Continuous and Discrete Curvature Learning.](http://arxiv.org/abs/2212.01793) | 本文提出了一种新的\{kappa}HGCN模型，在双曲空间内实现树状结构建模，通过结合连续和离散曲率来学习输入图的基础几何结构，并在多个基准测试和数据集上取得了最先进的性能。 |
| [^164] | [iEnhancer-ELM: improve enhancer identification by extracting position-related multiscale contextual information based on enhancer language models.](http://arxiv.org/abs/2212.01495) | 提出一种基于BERT-like增强子语言模型的增强子鉴定方法(iEnhancer-ELM)，通过提取位置相关的多尺度上下文信息，实现了对原始DNA序列的增强子鉴定性能的改善。 |
| [^165] | [Scalable Hierarchical Over-the-Air Federated Learning.](http://arxiv.org/abs/2211.16162) | 本研究提出了一种针对分布式环境的通信高效的分层联邦学习算法，通过使用可扩展的无线聚合方案和带宽有限的广播方案，解决了设备干扰和边缘服务器干扰的问题。 |
| [^166] | [PAC-Bayes Bounds for Bandit Problems: A Survey and Experimental Comparison.](http://arxiv.org/abs/2211.16110) | 这项调查研究了PAC-Bayes在Bandit问题中的应用，提供了界限的概述，并进行了实验比较。研究发现，PAC-Bayes界限是设计具有性能保证的离线Bandit算法的有用工具，但在线Bandit算法缺乏足够的数据以产生强大的性能保证。 |
| [^167] | [Optimal Approximation Rates for Deep ReLU Neural Networks on Sobolev and Besov Spaces.](http://arxiv.org/abs/2211.14400) | 该论文研究了在Sobolev和Besov空间中，使用ReLU激活函数的深度神经网络能够以怎样的参数效率逼近函数，包括$L_p(\Omega)$范数下的误差度量。我们提供了所有$1\leq p,q \leq \infty$和$s>0$的完整解决方案，并引入了一种新的位提取技术来获得尖锐的上界。 |
| [^168] | [A Two-Stage Active Learning Algorithm for $k$-Nearest Neighbors.](http://arxiv.org/abs/2211.10773) | 本文提出了一种用于训练 $k$ 最近邻分类器的简单直观的主动学习算法，首次保持了 $k$ 最近邻投票概念的预测时间，并提供了一致性保证。 |
| [^169] | [Graph Neural Networks on SPD Manifolds for Motor Imagery Classification: A Perspective from the Time-Frequency Analysis.](http://arxiv.org/abs/2211.02641) | 本文介绍了一种基于SPD流形的图神经网络用于运动想象分类，利用EEG的二阶统计量，相比传统方法具有更好的性能。 |
| [^170] | [A picture of the space of typical learnable tasks.](http://arxiv.org/abs/2210.17011) | 我们使用信息几何技术研究了在不同任务上训练时深度网络学习到的表示，发现任务空间的结构与Wordnet系统进化树的某些部分一致，并且监督学习在一个任务上的进展可以在其他任务上产生一定的影响。 |
| [^171] | [Multi-Target Tracking with Transferable Convolutional Neural Networks.](http://arxiv.org/abs/2210.15539) | 本论文提出了一种使用可迁移的卷积神经网络进行多目标跟踪的方法。该方法利用图像表示目标状态和传感器测量，并通过迁移学习实现了在大规模上进行MTT，并在10个目标的MTT任务中表现优于传统方法，在250个目标的更大MTT任务中性能提高了29%。 |
| [^172] | [FocusedCleaner: Sanitizing Poisoned Graphs for Robust GNN-based Node Classification.](http://arxiv.org/abs/2210.13815) | FocusedCleaner是一种针对数据投毒攻击的清理器，通过逆转攻击过程并提供更准确的搜索区域，逐步清理受污染的图形，显著提高了GNN在节点分类任务上的对抗鲁棒性。 |
| [^173] | [Active Learning for Single Neuron Models with Lipschitz Non-Linearities.](http://arxiv.org/abs/2210.13601) | 该论文提出一种针对具有利普希茨非线性的单个神经元模型的主动学习策略，该策略在敌对标签噪声下拟合线性函数，并在逼近保证方面具有强有力的可证明性能。 |
| [^174] | [Backdoor Attack and Defense in Federated Generative Adversarial Network-based Medical Image Synthesis.](http://arxiv.org/abs/2210.10886) | 本研究调查了联邦生成对抗网络中后门攻击的被忽视问题，并发现成功攻击是由于部分本地判别器对毒素过度拟合所致。 |
| [^175] | [A policy gradient approach for Finite Horizon Constrained Markov Decision Processes.](http://arxiv.org/abs/2210.04527) | 本文提出了一种针对有限时域受限马尔可夫决策过程的策略梯度方法，该方法能够在固定时间后终止，通过函数逼近和策略梯度方法找到最优策略。 |
| [^176] | [A Unified Perspective on Natural Gradient Variational Inference with Gaussian Mixture Models.](http://arxiv.org/abs/2209.11533) | 本论文提出了一种统一的视角来理解使用高斯混合模型进行自然梯度变分推断的方法。研究发现，VIPS和iBayes-GMM这两种目前最有效的方法，在更新各个组件和权重时使用的自然梯度更新是等价的，但其实现和理论保证存在差异。研究还发现，这两种方法在样本选择、自然梯度估计、步长适应以及可信区域或组件数量的调整等设计选择上存在区别，对于学习近似的质量有重要影响。 |
| [^177] | [Memory-Augmented Graph Neural Networks: A Brain-Inspired Review.](http://arxiv.org/abs/2209.10818) | 本文提供了一个关于记忆增强型图神经网络的全面回顾，通过心理学和神经科学的视角，提出了分类法和比较标准，并讨论了其局限性和未来发展方向。 |
| [^178] | [Action-based Early Autism Diagnosis Using Contrastive Feature Learning.](http://arxiv.org/abs/2209.05379) | 本研究提出了一种基于对比特征学习的基于行为的早期自闭症诊断方法，通过简单且少量的被试动作视频剪辑自动化诊断自闭症，克服了可用数据量小和样本变化大的挑战。 |
| [^179] | [Kernel Learning for Explainable Climate Science.](http://arxiv.org/abs/2209.04947) | 本文通过使用具有结构化非平稳核的高斯过程来模拟上游印度河流域的降水模式，解决了对该地区复杂时空降水分布的理解不足的问题。 |
| [^180] | [Optimal Regularized Online Allocation by Adaptive Re-Solving.](http://arxiv.org/abs/2209.00399) | 本文提出了一种自适应重求解的最优正则在线分配算法，该算法通过对偶方法解决了具有非凸累积奖励、硬资源约束和非分离正则化器的问题，并消除了遗憾界中的对数对数因子。该算法具有灵活性和高效性，并且通过不经常重新求解的方案降低了计算需求，同时保持了最优遗憾性能。 |
| [^181] | [Label-Noise Learning with Intrinsically Long-Tailed Data.](http://arxiv.org/abs/2208.09833) | 本文针对自带长尾数据的标签噪声学习问题，提出了一个学习框架并设计了双维样本选择算法 TABASCO，有效地将干净样本与噪声样本分开，特别适用于长尾类别。 |
| [^182] | [SAFARI: Versatile and Efficient Evaluations for Robustness of Interpretability.](http://arxiv.org/abs/2208.09418) | 本文提出了一种名为SAFARI的方法，用于评估深度学习的解释可靠性。该方法针对现有技术无法解决的几个挑战，通过引入两种黑盒评估方法，即最坏情况解释差异和一般情况下的鲁棒性的概率概念，来解决现有度量不全面、XAI技术异质性和误解罕见性等问题。使用遗传算法和子集模拟进行评估。 |
| [^183] | [A Bayesian Bradley-Terry model to compare multiple ML algorithms on multiple data sets.](http://arxiv.org/abs/2208.04935) | 本文提出了一种基于贝叶斯模型的Bradley-Terry模型，用于比较多个机器学习算法在多个数据集上的性能，与传统方法不同，贝叶斯方法能提供更细致的算法之间差异描述，并允许对等效性进行定义。 |
| [^184] | [Unsupervised pre-training of graph transformers on patient population graphs.](http://arxiv.org/abs/2207.10603) | 本研究提出了一种针对多模态临床数据的图变换器的无监督预训练方法，通过在患者人口图上利用图深度学习，可以提高患者预测结果的性能。 |
| [^185] | [Semi-supervised cross-lingual speech emotion recognition.](http://arxiv.org/abs/2207.06767) | 通过半监督学习方法，我们提出了一种基于Transformer的半监督跨语言情绪识别方法，通过在未标注的语句上应用伪标签策略来适应新领域，有效解决了跨语言情绪识别中标注数据不足和领域差异大的问题。 |
| [^186] | [Predicting Out-of-Domain Generalization with Neighborhood Invariance.](http://arxiv.org/abs/2207.02093) | 提出了一种测量分类器输出在局部转换邻域中不变性的方法，用于描述模型的泛化能力，不依赖于数据分布或模型假设，可应用于域外环境。 |
| [^187] | [SGD and Weight Decay Provably Induce a Low-Rank Bias in Neural Networks.](http://arxiv.org/abs/2206.05794) | 使用SGD和权重衰减训练深度ReLU神经网络会导致对于权重矩阵的秩最小化的偏差，特别是在使用较小批量大小、更高学习率或增加权重衰减时更为显著。此外，在中间神经网络崩溃时，学习的权重特别低秩。这种偏差与泛化之间存在关系。 |
| [^188] | [Efficient Reward Poisoning Attacks on Online Deep Reinforcement Learning.](http://arxiv.org/abs/2205.14842) | 本文通过设计一种通用的奖励污染框架展示了现有深度强化学习算法内在的漏洞，提出了两种新攻击方式，成功污染多种最先进DRL算法的智能体在多个环境中学习的效率，给深度强化学习提供了新的安全风险。 |
| [^189] | [Information-Directed Selection for Top-Two Algorithms.](http://arxiv.org/abs/2205.12086) | 本文研究了多臂赌博机中最佳k臂识别问题，提出了一种信息导向选择的算法（IDS），并证明了与IDS集成的顶部两个汤姆逊采样在高斯最佳臂识别中达到了最优。 |
| [^190] | [Logarithmic regret bounds for continuous-time average-reward Markov decision processes.](http://arxiv.org/abs/2205.11168) | 这项研究考虑了连续时间马尔可夫决策过程（MDPs）的平均奖励设置下的强化学习问题，并找到了实例相关的对数遗憾下界，并设计出了一个能够实现对数增长速率的学习算法。 |
| [^191] | [Unsupervised Discovery and Composition of Object Light Fields.](http://arxiv.org/abs/2205.03923) | 本文提出了一种无监督发现和合成物体光场的方法，通过将物体表示为以物体为中心的光场来提高渲染质量和操作效率。 |
| [^192] | [Non-Stationary Bandit Learning via Predictive Sampling.](http://arxiv.org/abs/2205.01970) | 本文提出了一种预测抽样算法，用于解决非平稳赌博机学习问题。该算法通过降低获取信息的优先级，解决了Thompson抽样在非平稳环境下表现不佳的问题，并在所有非平稳环境中优于Thompson抽样。 |
| [^193] | [Exploring the Distributed Knowledge Congruence in Proxy-data-free Federated Distillation.](http://arxiv.org/abs/2204.07028) | 本文提出了一种基于分布式知识一致性的无代理数据联邦蒸馏算法，解决了客户端模型异质性引起的知识差异问题，从而提高了模型表示的准确性。 |
| [^194] | [A PDE-Based Analysis of the Symmetric Two-Armed Bernoulli Bandit.](http://arxiv.org/abs/2202.05767) | 本研究通过关联线性热方程的解，得到了对称双臂伯努利赌博机问题的minmax最优遗憾和伪遗憾的领先项。新的结果改进了先前的研究，并提供了新的非渐近边界。 |
| [^195] | [Certifying Model Accuracy under Distribution Shifts.](http://arxiv.org/abs/2201.12440) | 本文提出了一种针对数据分布变化的模型准确性的可证明鲁棒性保证方法，通过在转换空间内随机化模型输入来实现鲁棒性。该方法适用于不同点之间变化的数据特定的扰动大小，并且能够产生对固定大小扰动的确保下界。 |
| [^196] | [Self-Supervised Beat Tracking in Musical Signals with Polyphonic Contrastive Learning.](http://arxiv.org/abs/2201.01771) | 本论文提出了一种新的自监督学习预训练任务，用于音乐节拍跟踪和下拍估计。通过使用音频信号分离模型，可以实现对节拍的自动标注，并在预训练模型中学习到起始函数。研究发现，预训练模型在节拍跟踪训练集非常小的情况下表现优于随机初始化模型。该工作为音乐信号处理领域的研究和实践提供了重要的贡献。 |
| [^197] | [Approximating Pandora's Box with Correlations.](http://arxiv.org/abs/2108.12976) | 在这项工作中，我们通过研究相关分布下的潘多拉魔盒问题，将其与均匀决策树和最小和集覆盖问题进行了近似转化。我们的主要结果表明，这些问题都可以在亚指数时间内实现常数近似比。 |
| [^198] | [Sketch-Based Anomaly Detection in Streaming Graphs.](http://arxiv.org/abs/2106.04486) | 本文提出了在动态图中以在线的方式为边和子图分配异常分数的方法，其利用了扩展的草图数据结构，并且在真实数据集上表现优于现有的方法。 |
| [^199] | [Neural Distributed Source Coding.](http://arxiv.org/abs/2106.02797) | 这项研究提出了一种神经分布式源编码的框架，可以处理复杂的相关性并实现最先进的峰值信噪比。 |
| [^200] | [Generating Novel Scene Compositions from Single Images and Videos.](http://arxiv.org/abs/2103.13389) | 本研究提出了SIV-GAN，一种无条件生成模型，可以从单个图像或视频中生成新的场景组合。通过引入内容和布局分支的鉴别器架构，该模型能够生成多样化、高质量的图像，并在保留上下文的同时保持视觉逼真。 |
| [^201] | [Conjecturing-Based Discovery of Patterns in Data.](http://arxiv.org/abs/2011.11576) | 本研究提出了一种基于猜想的数据模式发现方法，在数值特征和分类特征之间建立了非线性和布尔关系，并应用于COVID-19患者级别数据，揭示了可能的风险因素。 |
| [^202] | [Semi-Supervised Learning: the Case When Unlabeled Data is Equally Useful.](http://arxiv.org/abs/2005.11018) | 本文研究了半监督学习中未标记数据与标记数据在学习速度方面的关系，并发现在特定条件下，未标记数据在学习速度上同样有用。 |
| [^203] | [A Sub-sampled Tensor Method for Non-convex Optimization.](http://arxiv.org/abs/1911.10367) | 该论文提出了一个用于非凸优化的次采样张量方法，通过使用新颖的张量集中不等式，该方法可以在匹配确定性方法的收敛速度下找到光滑且潜在非凸的目标函数的局部极小值。 |
| [^204] | [Predictive modeling of brain tumor: A Deep learning approach.](http://arxiv.org/abs/1911.02265) | 这篇论文介绍了一种基于卷积神经网络的深度学习方法，通过脑部MRI扫描将脑肿瘤进行分类，并比较了不同预训练模型的性能。实验结果表明，Resnet-50模型具有最高的准确率和最低的假阴性率。 |
| [^205] | [MDP Playground: An Analysis and Debug Testbed for Reinforcement Learning.](http://arxiv.org/abs/1909.07750) | MDP Playground是一个用于强化学习的测试平台，可以根据不同维度的难度控制方式，挑战代理在各种环境中的表现。它提供了参数化的玩具环境集合，并通过实验揭示了这些环境对代理的影响。 |
| [^206] | [Stochastic Gradient MCMC for Nonlinear State Space Models.](http://arxiv.org/abs/1901.10568) | 该论文提出了一种针对非线性状态空间模型的随机梯度MCMC方法，通过扩展已有方法，利用粒子缓冲随机梯度估计量解决了长时间序列下计算和粒子退化的问题。 |

# 详细

[^1]: MaGNAS:一种面向异构MPSoC部署的映射感知图神经架构搜索框架

    MaGNAS: A Mapping-Aware Graph Neural Architecture Search Framework for Heterogeneous MPSoC Deployment. (arXiv:2307.08065v1 [cs.DC])

    [http://arxiv.org/abs/2307.08065](http://arxiv.org/abs/2307.08065)

    MaGNAS是一种面向异构MPSoC部署的映射感知图神经架构搜索框架，能够高效处理视觉图神经网络工作负载。

    

    随着图神经网络在视觉应用中的日益普及，由于其在对图像帧的各个部分之间建模结构和上下文关系方面的内在能力，因此它们变得越来越受欢迎。另一方面，异构多处理器芯片系统（MPSoCs）的最新进展使得边缘上的深度视觉应用越来越受欢迎，并且能够根据实时严格的执行要求进行推理。扩展而言，用于视觉应用的图神经网络必须遵守同样的执行要求。然而，与典型的深度神经网络相反，图学习操作的不规则流程对在这种异构MPSoC平台上运行图神经网络构成了挑战。在本文中，我们提出了一种新颖的统一设计映射方法，用于在异构MPSoC平台上高效处理视觉图神经网络工作负载。 特别地，我们开发了一个映射感知的图神经架构搜索框架MaGNAS。

    Graph Neural Networks (GNNs) are becoming increasingly popular for vision-based applications due to their intrinsic capacity in modeling structural and contextual relations between various parts of an image frame. On another front, the rising popularity of deep vision-based applications at the edge has been facilitated by the recent advancements in heterogeneous multi-processor Systems on Chips (MPSoCs) that enable inference under real-time, stringent execution requirements. By extension, GNNs employed for vision-based applications must adhere to the same execution requirements. Yet contrary to typical deep neural networks, the irregular flow of graph learning operations poses a challenge to running GNNs on such heterogeneous MPSoC platforms. In this paper, we propose a novel unified design-mapping approach for efficient processing of vision GNN workloads on heterogeneous MPSoC platforms. Particularly, we develop MaGNAS, a mapping-aware Graph Neural Architecture Search framework. MaGNA
    
[^2]: 快速量子算法用于注意力计算

    Fast Quantum Algorithm for Attention Computation. (arXiv:2307.08045v1 [quant-ph])

    [http://arxiv.org/abs/2307.08045](http://arxiv.org/abs/2307.08045)

    这篇论文研究了使用快速量子算法进行注意力计算，以加速大型语言模型 (LLMs) 的计算速度。

    

    大型语言模型 (LLMs) 表现出色，在各种任务中显示出异常的性能。这些模型由先进的深度学习技术驱动，已经在自然语言处理 (NLP) 领域引起了革命，并在各种与语言相关的任务中取得了显著的结果。LLMs 在机器翻译、情感分析、问答、文本生成、文本分类、语言建模等任务中表现出色。它们在捕捉复杂的语言模式、理解背景、生成连贯且相关的文本方面非常有效。注意力计算方案在大型语言模型 (LLMs) 的架构中起着关键作用。它是一个基本组件，使得模型能够在语言处理任务中有效地捕捉和利用上下文信息。加快注意力计算方案的速度是加速LLMs计算的核心问题之一。

    Large language models (LLMs) have demonstrated exceptional performance across a wide range of tasks. These models, powered by advanced deep learning techniques, have revolutionized the field of natural language processing (NLP) and have achieved remarkable results in various language-related tasks.  LLMs have excelled in tasks such as machine translation, sentiment analysis, question answering, text generation, text classification, language modeling, and more. They have proven to be highly effective in capturing complex linguistic patterns, understanding context, and generating coherent and contextually relevant text. The attention scheme plays a crucial role in the architecture of large language models (LLMs). It is a fundamental component that enables the model to capture and utilize contextual information during language processing tasks effectively. Making the attention scheme computation faster is one of the central questions to speed up the LLMs computation. It is well-known that
    
[^3]: 柔性时间事件建模：通过排名回归优化神经网络

    Towards Flexible Time-to-event Modeling: Optimizing Neural Networks via Rank Regression. (arXiv:2307.08044v1 [cs.LG])

    [http://arxiv.org/abs/2307.08044](http://arxiv.org/abs/2307.08044)

    本研究提出了一种深度AFT排名回归模型，用于灵活地进行时间事件建模，从而改善预测性能并减轻严格的假设。

    

    时间事件分析，也被称为生存分析，旨在根据一组特征预测事件发生的时间。这个领域面临的一个主要挑战是处理被截尾的数据，这可能使学习算法更加复杂。传统方法如Cox比例风险模型和加速失效时间（AFT）模型在这个领域很受欢迎，但它们经常需要一些假设，如比例风险和线性。特别是，AFT模型通常需要预先指定的参数分布假设。为了提高预测性能和减轻严格的假设，近年来出现了许多基于深度学习的危险模型方法。然而，神经网络文献中对于AFT的表示学习尚未广泛探索，尽管相对于以危险为重点的方法而言，它更加简单和可解释。在这项工作中，我们引入了深度AFT排名回归模型来进行时间事件预测。

    Time-to-event analysis, also known as survival analysis, aims to predict the time of occurrence of an event, given a set of features. One of the major challenges in this area is dealing with censored data, which can make learning algorithms more complex. Traditional methods such as Cox's proportional hazards model and the accelerated failure time (AFT) model have been popular in this field, but they often require assumptions such as proportional hazards and linearity. In particular, the AFT models often require pre-specified parametric distributional assumptions. To improve predictive performance and alleviate strict assumptions, there have been many deep learning approaches for hazard-based models in recent years. However, representation learning for AFT has not been widely explored in the neural network literature, despite its simplicity and interpretability in comparison to hazard-focused methods. In this work, we introduce the Deep AFT Rank-regression model for Time-to-event predic
    
[^4]: 大规模空间插值风场的双变量深度克里金方法

    Bivariate DeepKriging for Large-scale Spatial Interpolation of Wind Fields. (arXiv:2307.08038v1 [stat.ML])

    [http://arxiv.org/abs/2307.08038](http://arxiv.org/abs/2307.08038)

    本文提出了一种名为双变量深度克里金的方法，它利用空间相关的深度神经网络(DNN)和嵌入层以及基于自助法和集成DNN的无分布不确定性量化方法，用于大规模空间插值风场的预测和估计。

    

    高空间分辨率的风场数据对于气候、海洋和气象研究中的各种应用至关重要。由于风数据往往具有非高斯分布、高空间变异性和异质性，因此对具有两个维度速度的双变量风场进行大规模空间插值或下缩放是一项具有挑战性的任务。在空间统计学中，常用cokriging来预测双变量空间场。然而，cokriging预测器除了对高斯过程有效外，并不是最优的。此外，对于大型数据集，cokriging计算量巨大。在本文中，我们提出了一种称为双变量深度克里金的方法，它是一个由空间径向基函数构建的空间相关的深度神经网络(DNN)和嵌入层，用于双变量空间数据预测。然后，我们基于自助法和集成DNN开发了一种无分布不确定性量化方法。我们提出的方法优于传统的cokriging方法。

    High spatial resolution wind data are essential for a wide range of applications in climate, oceanographic and meteorological studies. Large-scale spatial interpolation or downscaling of bivariate wind fields having velocity in two dimensions is a challenging task because wind data tend to be non-Gaussian with high spatial variability and heterogeneity. In spatial statistics, cokriging is commonly used for predicting bivariate spatial fields. However, the cokriging predictor is not optimal except for Gaussian processes. Additionally, cokriging is computationally prohibitive for large datasets. In this paper, we propose a method, called bivariate DeepKriging, which is a spatially dependent deep neural network (DNN) with an embedding layer constructed by spatial radial basis functions for bivariate spatial data prediction. We then develop a distribution-free uncertainty quantification method based on bootstrap and ensemble DNN. Our proposed approach outperforms the traditional cokriging 
    
[^5]: 基于磁场的奖励塑形用于目标条件强化学习

    Magnetic Field-Based Reward Shaping for Goal-Conditioned Reinforcement Learning. (arXiv:2307.08033v1 [cs.LG])

    [http://arxiv.org/abs/2307.08033](http://arxiv.org/abs/2307.08033)

    本论文提出了一种基于磁场的奖励塑形方法，用于目标条件强化学习任务。通过将目标和障碍物视为永久磁铁，并根据磁场强度值建立奖励函数，解决了传统奖励塑形方法在动态环境中应用效果不好的问题。

    

    目标条件强化学习是传统强化学习框架的有趣扩展，动态环境和奖励稀疏性可能导致传统学习算法失败。奖励塑形是通过将人类领域知识嵌入学习过程来提高采样效率的实际方法。现有基于距离度量的目标条件强化学习奖励塑形方法通常建立在线性和各向同性分布的距离度量上，可能无法提供关于高复杂度环境的充分信息。本文提出了一种新颖的基于磁场的目标条件强化学习奖励塑形方法（MFRS），用于具有动态目标和障碍物的任务。受到磁铁的物理特性的启发，我们将目标和障碍物视为永久磁铁，并根据这些磁铁产生的磁场强度值来建立奖励函数。奖励函数具有非线性和各向异性分布。

    Goal-conditioned reinforcement learning (RL) is an interesting extension of the traditional RL framework, where the dynamic environment and reward sparsity can cause conventional learning algorithms to fail. Reward shaping is a practical approach to improving sample efficiency by embedding human domain knowledge into the learning process. Existing reward shaping methods for goal-conditioned RL are typically built on distance metrics with a linear and isotropic distribution, which may fail to provide sufficient information about the ever-changing environment with high complexity. This paper proposes a novel magnetic field-based reward shaping (MFRS) method for goal-conditioned RL tasks with dynamic target and obstacles. Inspired by the physical properties of magnets, we consider the target and obstacles as permanent magnets and establish the reward function according to the intensity values of the magnetic field generated by these magnets. The nonlinear and anisotropic distribution of t
    
[^6]: 基于扩散概率模型的噪声感知语音增强

    Noise-aware Speech Enhancement using Diffusion Probabilistic Model. (arXiv:2307.08029v1 [eess.AS])

    [http://arxiv.org/abs/2307.08029](http://arxiv.org/abs/2307.08029)

    本文提出了一种基于噪声感知的语音增强方法，在扩散模型中引入噪声特定信息，通过噪声分类和多任务学习方案来增强噪声调节器的噪声特异性。证实该方法在VoiceBank-DEMAND数据集上取得了良好的实验效果。

    

    随着扩散模型的最新进展，生成式语音增强（SE）因其对未知测试噪声的巨大潜力而受到了大量研究的关注。然而，现有的研究主要集中在清晰语音的固有特性上进行推理，没有充分利用真实世界条件下变化的噪声信息。在本文中，我们提出了一种噪声感知语音增强（NASE）方法，该方法提取噪声特定信息来引导扩散模型的逆向处理过程。具体而言，我们设计了一个噪声分类（NC）模型，用于产生声学嵌入作为噪声调节器来指导逆向降噪过程。同时，我们设计了多任务学习方案，共同优化SE和NC任务，以增强提取的噪声调节器的噪声特异性。我们提出的NASE被证明是一个即插即用的模块，可以推广到任何扩散SE模型。VoiceBank-DEMAND数据集上的实验证据表明，NASE达到了

    With recent advances of diffusion model, generative speech enhancement (SE) has attracted a surge of research interest due to its great potential for unseen testing noises. However, existing efforts mainly focus on inherent properties of clean speech for inference, underexploiting the varying noise information in real-world conditions. In this paper, we propose a noise-aware speech enhancement (NASE) approach that extracts noise-specific information to guide the reverse process in diffusion model. Specifically, we design a noise classification (NC) model to produce acoustic embedding as a noise conditioner for guiding the reverse denoising process. Meanwhile, a multi-task learning scheme is devised to jointly optimize SE and NC tasks, in order to enhance the noise specificity of extracted noise conditioner. Our proposed NASE is shown to be a plug-and-play module that can be generalized to any diffusion SE models. Experiment evidence on VoiceBank-DEMAND dataset shows that NASE achieves 
    
[^7]: 重新审视隐式模型：在视觉任务中权重绑定模型的稀疏度权衡能力

    Revisiting Implicit Models: Sparsity Trade-offs Capability in Weight-tied Model for Vision Tasks. (arXiv:2307.08013v1 [cs.LG])

    [http://arxiv.org/abs/2307.08013](http://arxiv.org/abs/2307.08013)

    本研究重新审视了隐式模型，并发现权重绑定模型在视觉任务中比DEQ变体更有效、稳定和高效。通过使用不同的稀疏掩模，我们提出了提高模型容量的方法。

    

    隐式模型如深度平衡模型（Deep Equilibrium Models, DEQs）因其能够用优雅的解决方案和恒定的内存占用训练无限层模型而引起了研究者的广泛关注。然而，尽管已经做出了几次尝试，这些方法仍然受到模型低效和优化不稳定性的严重限制。此外，对于视觉任务，缺乏相关方法的公平基准评估。在本研究中，我们重新审视了隐式模型的发展，并将其追溯到原始的权重绑定模型。令人惊讶的是，我们观察到与DEQ变体相比，权重绑定模型在视觉任务上更加有效、稳定和高效。通过这些简单而清晰的权重绑定模型，我们进一步研究了这种模型容量的基本限制，并提出了使用不同稀疏掩模来提高模型容量的方法。最后，我们为从业人员提供了关于深度、宽度和稀疏度的设计指南。

    Implicit models such as Deep Equilibrium Models (DEQs) have garnered significant attention in the community for their ability to train infinite layer models with elegant solution-finding procedures and constant memory footprint. However, despite several attempts, these methods are heavily constrained by model inefficiency and optimization instability. Furthermore, fair benchmarking across relevant methods for vision tasks is missing. In this work, we revisit the line of implicit models and trace them back to the original weight-tied models. Surprisingly, we observe that weight-tied models are more effective, stable, as well as efficient on vision tasks, compared to the DEQ variants. Through the lens of these simple-yet-clean weight-tied models, we further study the fundamental limits in the model capacity of such models and propose the use of distinct sparse masks to improve the model capacity. Finally, for practitioners, we offer design guidelines regarding the depth, width, and spars
    
[^8]: 一种基于无监督深度学习的极化解码器的单次解码方法

    For One-Shot Decoding: Unsupervised Deep Learning-Based Polar Decoder. (arXiv:2307.08004v1 [cs.IT])

    [http://arxiv.org/abs/2307.08004](http://arxiv.org/abs/2307.08004)

    本文提出了一种基于无监督深度学习的极化解码器的单次解码方法，通过自监督学习训练神经网络，消除了对预定义标签的依赖，实现了在通信系统实际数据上的直接训练，其性能接近最大后验概率解码器，并展示出更优越的泛化能力。

    

    我们提出了一种无监督深度学习的解码方案，可以实现极化码的单次解码。在这种方案中，我们不再使用信息位向量作为标签来训练神经网络（NN），而是通过自监督学习利用极化码的生成矩阵来训练NN以实现有界距离解码。这种方法消除了对预定义标签的依赖，增强了直接在通信系统的实际数据上进行训练的可能性，从而提高了可应用性。此外，计算机模拟表明，（i）所提出的方案的误比特率（BER）和块错误率（BLER）性能在非常短的数据包上可以接近最大后验概率（MAP）解码器的性能，（ii）与传统的解码器相比，所提出的NN解码器展示了更优越的泛化能力。

    We propose an unsupervised deep learning-based decoding scheme that enables one-shot decoding of polar codes. In the proposed scheme, rather than using the information bit vectors as labels for training the neural network (NN) through supervised learning as the conventional scheme did, the NN is trained to function as a bounded distance decoder by leveraging the generator matrix of polar codes through self-supervised learning. This approach eliminates the reliance on predefined labels, empowering the potential to train directly on the actual data within communication systems and thereby enhancing the applicability. Furthermore, computer simulations demonstrate that (i) the bit error rate (BER) and block error rate (BLER) performances of the proposed scheme can approach those of the maximum a posteriori (MAP) decoder for very short packets and (ii) the proposed NN decoder exhibits much superior generalization ability compared to the conventional one.
    
[^9]: LUCYD: 一种基于特征驱动的Richardson-Lucy解卷积网络

    LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network. (arXiv:2307.07998v1 [cs.CV])

    [http://arxiv.org/abs/2307.07998](http://arxiv.org/abs/2307.07998)

    LUCYD是一种特征驱动的解卷积网络，通过融合深层特征和Richardson-Lucy公式，提高了体积显微图像的恢复质量，降低了计算成本，并具有高度可解释性。

    

    在生命科学中，获取显微图像常常会导致图像退化和损坏，表现为噪声和模糊，这给准确分析和解释获取的数据带来了重大挑战。本文提出了LUCYD，一种新的方法用于恢复体积显微图像，它将Richardson-Lucy解卷积公式和完全卷积网络获得的深层特征进行融合。通过将图像形成过程整合到特征驱动的恢复模型中，该方法旨在提高恢复后图像的质量，同时降低计算成本并保持高度可解释性。我们的结果表明，LUCYD在合成和真实显微图像中优于现有方法，在图像质量和泛化性能方面表现出卓越的性能。我们还证明该模型可以处理各种显微镜模式和差异。

    The process of acquiring microscopic images in life sciences often results in image degradation and corruption, characterised by the presence of noise and blur, which poses significant challenges in accurately analysing and interpreting the obtained data. This paper proposes LUCYD, a novel method for the restoration of volumetric microscopy images that combines the Richardson-Lucy deconvolution formula and the fusion of deep features obtained by a fully convolutional network. By integrating the image formation process into a feature-driven restoration model, the proposed approach aims to enhance the quality of the restored images whilst reducing computational costs and maintaining a high degree of interpretability. Our results demonstrate that LUCYD outperforms the state-of-the-art methods in both synthetic and real microscopy images, achieving superior performance in terms of image quality and generalisability. We show that the model can handle various microscopy modalities and differ
    
[^10]: MargCTGAN: "边际化" 更好的低样本情况下的CTGAN方法

    MargCTGAN: A "Marginally'' Better CTGAN for the Low Sample Regime. (arXiv:2307.07997v1 [cs.LG])

    [http://arxiv.org/abs/2307.07997](http://arxiv.org/abs/2307.07997)

    MargCTGAN提出一个新的合成数据生成方法，在低样本情况下通过添加特征匹配的去相关边际，改善了CTGAN模型在实用性和统计属性方面的表现。

    

    现实而有用的合成数据具有很大的潜力。然而，当前合成表格数据生成的评估方法主要关注下游任务的有用性，往往忽视了统计属性的重要性。这种疏忽在低样本情况下尤为明显，伴随着这些统计指标的迅速恶化。在本文中，我们通过对三种最先进的合成表格数据生成器在边际分布、列对相关性、联合分布和下游任务实用性表现等方面在高到低样本情况下的评估，来解决这个问题。流行的CTGAN模型在实用性方面表现强劲，但在低样本设置方面的实用性较差。为了克服这个限制，我们提出了MargCTGAN方法，通过添加特征匹配的去相关边际，从而在下游实用性和合成数据的统计属性方面实现了持续的改进。

    The potential of realistic and useful synthetic data is significant. However, current evaluation methods for synthetic tabular data generation predominantly focus on downstream task usefulness, often neglecting the importance of statistical properties. This oversight becomes particularly prominent in low sample scenarios, accompanied by a swift deterioration of these statistical measures. In this paper, we address this issue by conducting an evaluation of three state-of-the-art synthetic tabular data generators based on their marginal distribution, column-pair correlation, joint distribution and downstream task utility performance across high to low sample regimes. The popular CTGAN model shows strong utility, but underperforms in low sample settings in terms of utility. To overcome this limitation, we propose MargCTGAN that adds feature matching of de-correlated marginals, which results in a consistent improvement in downstream utility as well as statistical properties of the syntheti
    
[^11]: 优化Transformer推理技术的综述

    A Survey of Techniques for Optimizing Transformer Inference. (arXiv:2307.07982v1 [cs.LG])

    [http://arxiv.org/abs/2307.07982](http://arxiv.org/abs/2307.07982)

    本文综述了优化Transformer推理的技术，包括知识蒸馏、修剪、量化、神经架构搜索和轻量级网络设计。

    

    近年来，Transformer神经网络的性能和应用范围取得了显著的增长。包括BERT、GPT和ViT在内的Transformer网络家族，在自然语言处理（NLP）和计算机视觉（CV）领域展现了效果。ChatGPT等基于Transformer的网络也影响了普通人的生活。然而，追求高预测性能导致了Transformer的内存和计算消耗的指数增长。研究人员提出了在抽象级别的Transformer推理优化技术。本文对Transformer网络推理阶段的优化技术进行了全面的调查。我们在算法层面上调查了知识蒸馏、修剪、量化、神经架构搜索和轻量级网络设计等技术。

    Recent years have seen a phenomenal rise in performance and applications of transformer neural networks. The family of transformer networks, including Bidirectional Encoder Representations from Transformer (BERT), Generative Pretrained Transformer (GPT) and Vision Transformer (ViT), have shown their effectiveness across Natural Language Processing (NLP) and Computer Vision (CV) domains. Transformer-based networks such as ChatGPT have impacted the lives of common men. However, the quest for high predictive performance has led to an exponential increase in transformers' memory and compute footprint. Researchers have proposed techniques to optimize transformer inference at all levels of abstraction. This paper presents a comprehensive survey of techniques for optimizing the inference phase of transformer networks. We survey techniques such as knowledge distillation, pruning, quantization, neural architecture search and lightweight network design at the algorithmic level. We further review
    
[^12]: 拜占庭鲁棒的分布式在线学习：在拜占庭环境中应对对手的恶意行为

    Byzantine-Robust Distributed Online Learning: Taming Adversarial Participants in An Adversarial Environment. (arXiv:2307.07980v1 [cs.LG])

    [http://arxiv.org/abs/2307.07980](http://arxiv.org/abs/2307.07980)

    本文研究了在拜占庭攻击下的分布式在线学习中的对手行为。在拜占庭和恶意对手存在的情况下，尽管可以控制线性对手遗憾的常数，但分布式在线梯度下降只能实现线性对手遗憾上界。然而，在环境不完全对抗性的情况下，可以实现次线性的随机遗憾。

    

    本文研究了在拜占庭攻击下的分布式在线学习。在线学习算法的性能通常通过（对手的）遗憾来评估，在环境提供对手损失时评估一步决策的质量，而期望得到一个次线性的上界。但我们证明，即使使用一类最先进的鲁棒聚合规则，在拜占庭环境和存在拜占庭参与者的情况下，分布式在线梯度下降只能实现线性的对手遗憾上界，这是紧密的。这是拜占庭攻击的必然结果，尽管我们可以将线性对手遗憾的常数控制在合理的水平上。有趣的是，当环境不是完全对抗性的，即诚实参与者的损失是独立同分布的（i.i.d.），我们表明，与前面提到的对手遗憾相反，可以实现次线性的随机遗憾。

    This paper studies distributed online learning under Byzantine attacks. The performance of an online learning algorithm is often characterized by (adversarial) regret, which evaluates the quality of one-step-ahead decision-making when an environment provides adversarial losses, and a sublinear bound is preferred. But we prove that, even with a class of state-of-the-art robust aggregation rules, in an adversarial environment and in the presence of Byzantine participants, distributed online gradient descent can only achieve a linear adversarial regret bound, which is tight. This is the inevitable consequence of Byzantine attacks, even though we can control the constant of the linear adversarial regret to a reasonable level. Interestingly, when the environment is not fully adversarial so that the losses of the honest participants are i.i.d. (independent and identically distributed), we show that sublinear stochastic regret, in contrast to the aforementioned adversarial regret, is possible
    
[^13]: 基于有限元的网络: 从部分观察中学习合理的可变形物体动力学

    Finite element inspired networks: Learning physically-plausible deformable object dynamics from partial observations. (arXiv:2307.07975v1 [cs.RO])

    [http://arxiv.org/abs/2307.07975](http://arxiv.org/abs/2307.07975)

    该论文提出了一种基于有限元的网络模型，通过动力学网络和物理感知编码器，从部分观察中学习可变形物体的动力学，并通过正运动学解码器进行预测，实现了具有物理解释性的模型。

    

    精确模拟可变形线性物体（DLO）的动力学在需要一个可以被人解读和数据高效的模型并能够提供快速预测的任务中是具有挑战性的。为了得到这样的模型，我们借鉴了刚性有限元方法（R-FEM），将DLO建模为一系列刚体链，其内部状态通过动力学网络以时间展开。由于该状态不能直接观察到，动力学网络与一个物理感知的编码器共同训练，将观察到的运动变量映射到刚体链的状态。为了促使状态获得物理上有意义的表示，我们利用底层R-FEM模型的正运动学（FK）作为解码器。我们在一个机器人实验中证明，这种被称为“有限元启发网络”的架构是一个易于处理但功能强大的DLO动力学模型，可以从部分观察中得出具有物理解释性的预测。

    The accurate simulation of deformable linear object (DLO) dynamics is challenging if the task at hand requires a human-interpretable and data-efficient model that also yields fast predictions. To arrive at such model, we draw inspiration from the rigid finite element method (R-FEM) and model a DLO as a serial chain of rigid bodies whose internal state is unrolled through time by a dynamics network. As this state is not observed directly, the dynamics network is trained jointly with a physics-informed encoder mapping observed motion variables to the body chain's state. To encourage that the state acquires a physically meaningful representation, we leverage the forward kinematics (FK) of the underlying R-FEM model as a decoder. We demonstrate in a robot experiment that this architecture - being termed "Finite element inspired network" - forms an easy to handle, yet capable DLO dynamics model yielding physically interpretable predictions from partial observations.  The project code is ava
    
[^14]: 异方差因果结构学习

    Heteroscedastic Causal Structure Learning. (arXiv:2307.07973v1 [cs.LG])

    [http://arxiv.org/abs/2307.07973](http://arxiv.org/abs/2307.07973)

    本研究提出了一种简单而有效的因果结构学习算法（HOST），通过利用正态性，能够在高斯噪声下恢复一个有效的因果排序，从而唯一确定因果DAG，并且算法的时间复杂度与样本大小和维度都以多项式规模扩展。

    

    迄今为止，学习编码观察数据中嵌入的因果关系的有向无环图（DAG）是一个计算上具有挑战性的问题。最近的研究趋势表明，在相等方差假设下，可以以多项式时间复杂度恢复DAGs。然而，这禁止了噪声的异方差性，这允许更灵活的建模能力，但同时更具挑战性。在本研究中，我们解决了高斯噪声下的异方差因果结构学习问题。通过利用因果机制的正态性，我们可以恢复一个有效的因果排序，从而使用一系列条件独立检验唯一地确认因果DAG。结果是HOST（异方差因果结构学习），一种简单而有效的因果结构学习算法，其样本大小和维度都以多项式规模扩展。

    Heretofore, learning the directed acyclic graphs (DAGs) that encode the cause-effect relationships embedded in observational data is a computationally challenging problem. A recent trend of studies has shown that it is possible to recover the DAGs with polynomial time complexity under the equal variances assumption. However, this prohibits the heteroscedasticity of the noise, which allows for more flexible modeling capabilities, but at the same time is substantially more challenging to handle. In this study, we tackle the heteroscedastic causal structure learning problem under Gaussian noises. By exploiting the normality of the causal mechanisms, we can recover a valid causal ordering, which can uniquely identify the causal DAG using a series of conditional independence tests. The result is HOST (Heteroscedastic causal STructure learning), a simple yet effective causal structure learning algorithm that scales polynomially in both sample size and dimensionality. In addition, via extensi
    
[^15]: 用神经形态方法提高自主系统估计的能效与可靠性

    Enhancing Energy Efficiency and Reliability in Autonomous Systems Estimation using Neuromorphic Approach. (arXiv:2307.07963v1 [cs.LG])

    [http://arxiv.org/abs/2307.07963](http://arxiv.org/abs/2307.07963)

    本论文介绍了一种基于脉冲编码理论和脉冲神经网络的估计框架，通过设计网络的权重矩阵，实现了能效和可靠性的提升。

    

    能效和可靠性一直是保证自主系统计算机的成本效益和安全任务的关键因素。随着空间机器人和先进空中机动性等行业的快速发展，对这些低尺寸、重量、功耗计算机的需求显著增加。本研究旨在引入基于脉冲编码理论和脉冲神经网络的估计框架，利用神经形态计算机的效率和可伸缩性。因此，我们提出了基于脉冲神经网络的卡尔曼滤波器（KF），它是一种基于已定义良好的线性系统的根本和广泛采用的最优策略。此外，基于改进的滑动创新滤波器（MSIF），我们提出了一种稳健的策略，称为SNN-MSIF。值得注意的是，网络的权重矩阵根据系统模型设计，消除了学习的需要。为了评估所提出的策略的有效性，我们将它们与算法对应物进行了比较。

    Energy efficiency and reliability have long been crucial factors for ensuring cost-effective and safe missions in autonomous systems computers. With the rapid evolution of industries such as space robotics and advanced air mobility, the demand for these low size, weight, and power (SWaP) computers has grown significantly. This study focuses on introducing an estimation framework based on spike coding theories and spiking neural networks (SNN), leveraging the efficiency and scalability of neuromorphic computers. Therefore, we propose an SNN-based Kalman filter (KF), a fundamental and widely adopted optimal strategy for well-defined linear systems. Furthermore, based on the modified sliding innovation filter (MSIF) we present a robust strategy called SNN-MSIF. Notably, the weight matrices of the networks are designed according to the system model, eliminating the need for learning. To evaluate the effectiveness of the proposed strategies, we compare them to their algorithmic counterparts
    
[^16]: 图神经网络的自动多项式滤波器学习

    Automated Polynomial Filter Learning for Graph Neural Networks. (arXiv:2307.07956v1 [cs.LG])

    [http://arxiv.org/abs/2307.07956](http://arxiv.org/abs/2307.07956)

    本文提出了一种自动学习多项式滤波器的框架，名为Auto-Polynomial，在同质性和异质性图上取得了显著的性能提升。

    

    多项式图滤波器作为图神经网络设计中的指导原则被广泛应用。最近，自适应学习多项式图滤波器在建模同质性和异质性图信号方面表现出了很大的潜力，具有灵活性和表达性。本文通过一项创新的初步研究，探索了多项式图滤波器学习方法的潜力和限制，揭示了严重的过拟合问题。为了提高多项式图滤波器的效果，我们提出了Auto-Polynomial，这是一个新颖且通用的自动多项式图滤波器学习框架，可以高效地学习更好的滤波器，适应各种复杂的图信号。综合实验和消融研究在考虑多种标签比例的同质性和异质性图上展示了显著且一致的性能提升。

    Polynomial graph filters have been widely used as guiding principles in the design of Graph Neural Networks (GNNs). Recently, the adaptive learning of the polynomial graph filters has demonstrated promising performance for modeling graph signals on both homophilic and heterophilic graphs, owning to their flexibility and expressiveness. In this work, we conduct a novel preliminary study to explore the potential and limitations of polynomial graph filter learning approaches, revealing a severe overfitting issue. To improve the effectiveness of polynomial graph filters, we propose Auto-Polynomial, a novel and general automated polynomial graph filter learning framework that efficiently learns better filters capable of adapting to various complex graph signals. Comprehensive experiments and ablation studies demonstrate significant and consistent performance improvements on both homophilic and heterophilic graphs across multiple learning settings considering various labeling ratios, which u
    
[^17]: 通过选择性同步加速分布式机器学习训练

    Accelerating Distributed ML Training via Selective Synchronization. (arXiv:2307.07950v1 [cs.DC])

    [http://arxiv.org/abs/2307.07950](http://arxiv.org/abs/2307.07950)

    本文介绍了一种名为SelSync的方法，通过选择性同步，在保持准确性的前提下减少了分布式深度神经网络训练的时间开销。该方法根据每一步的重要性动态选择是否进行通信，达到了与批量同步并行（BSP）相同或更高的准确性。

    

    在分布式训练中，深度神经网络（DNN）同时在多个工作者上启动，并使用批量同步并行（BSP）训练中的每个步骤聚合其本地更新。然而，由于聚合的通信成本较高，BSP无法线性扩展。为了减轻这种开销，FedAvg和SSP等替代方案要么降低同步频率，要么完全消除同步，通常以降低最终准确性为代价。在本文中，我们提出了一种名为SelSync的实用、低开销的DNN训练方法，它根据每一步的重要性动态选择是否进行通信。作为\texttt{SelSync}的一部分，我们提出了各种优化方法，以提高在\textit {半同步}训练环境中的收敛性。我们的系统在减少训练时间最多14％的同时，达到与BSP相同或更高的准确性。

    In distributed training, deep neural networks (DNNs) are launched over multiple workers concurrently and aggregate their local updates on each step in bulk-synchronous parallel (BSP) training. However, BSP does not linearly scale-out due to high communication cost of aggregation. To mitigate this overhead, alternatives like Federated Averaging (FedAvg) and Stale-Synchronous Parallel (SSP) either reduce synchronization frequency or eliminate it altogether, usually at the cost of lower final accuracy. In this paper, we present \texttt{SelSync}, a practical, low-overhead method for DNN training that dynamically chooses to incur or avoid communication at each step either by calling the aggregation op or applying local updates based on their significance. We propose various optimizations as part of \texttt{SelSync} to improve convergence in the context of \textit{semi-synchronous} training. Our system converges to the same or better accuracy than BSP while reducing training time by up to 14
    
[^18]: 通过可靠的、多样化的和类平衡的伪标签来重新审视领域自适应三维物体检测

    Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling. (arXiv:2307.07944v1 [cs.CV])

    [http://arxiv.org/abs/2307.07944](http://arxiv.org/abs/2307.07944)

    本文通过提出一种适用于多类训练设置的新型ReDB框架来解决现有领域自适应方法在多类训练设置下性能下降的问题，通过产生可靠的、多样化的和类平衡的伪三维框来引导目标领域的自训练。

    

    无监督领域自适应与伪标签技术的辅助已经成为领域自适应三维物体检测的关键方法。然而，现有的领域自适应方法在应用于多类训练设置时性能大幅下降，原因是伪标签的质量低和类别不平衡问题共存。本文通过提出一种针对同时学习检测所有类别的新型ReDB框架来解决这一挑战。我们的方法产生可靠的、多样化的和类平衡的伪三维框，通过迭代地引导不同分布的目标领域的自训练。为了减轻环境差异（例如，光束数量）带来的干扰，我们提出了跨域检查（CDE），通过将目标实例复制粘贴到源环境中并测量预测的一致性来评估伪标签的正确性。为了减少计算开销和缓解物体的转移（例如，

    Unsupervised domain adaptation (DA) with the aid of pseudo labeling techniques has emerged as a crucial approach for domain-adaptive 3D object detection. While effective, existing DA methods suffer from a substantial drop in performance when applied to a multi-class training setting, due to the co-existence of low-quality pseudo labels and class imbalance issues. In this paper, we address this challenge by proposing a novel ReDB framework tailored for learning to detect all classes at once. Our approach produces Reliable, Diverse, and class-Balanced pseudo 3D boxes to iteratively guide the self-training on a distributionally different target domain. To alleviate disruptions caused by the environmental discrepancy (e.g., beam numbers), the proposed cross-domain examination (CDE) assesses the correctness of pseudo labels by copy-pasting target instances into a source environment and measuring the prediction consistency. To reduce computational overhead and mitigate the object shift (e.g.
    
[^19]: KECOR:用于主动3D物体检测的内核编码率最大化

    KECOR: Kernel Coding Rate Maximization for Active 3D Object Detection. (arXiv:2307.07942v1 [cs.CV])

    [http://arxiv.org/abs/2307.07942](http://arxiv.org/abs/2307.07942)

    本论文提出了一种内核编码率最大化（KECOR）策略，可以通过信息论的视角确定最具信息量的点云，以最小化标注所需的比特数。这种策略可以减轻LiDAR物体检测中的注释负担并提高计算效率。

    

    在自动驾驶中实现可靠的基于LiDAR的物体检测器至关重要，但其成功取决于获取大量精确的3D注释。主动学习（AL）通过使用更少的标签和可以达到与完全监督学习相当的性能的算法来减轻注释负担。尽管AL表现出了潜力，但当前方法优先选择具有高不确定性和/或多样性的未标记点云，导致选择更多实例进行标记并降低计算效率。在本文中，我们采用了一种新颖的内核编码率最大化（KECOR）策略，该策略旨在通过信息论的视角确定最具信息量的点云以获取标签。贪婪搜索被应用于寻找能够最大化编码潜在特征所需的最小比特数的期望点云。为了确定所选样本的独特性和信息量，从模型的角度进行了评估。

    Achieving a reliable LiDAR-based object detector in autonomous driving is paramount, but its success hinges on obtaining large amounts of precise 3D annotations. Active learning (AL) seeks to mitigate the annotation burden through algorithms that use fewer labels and can attain performance comparable to fully supervised learning. Although AL has shown promise, current approaches prioritize the selection of unlabeled point clouds with high uncertainty and/or diversity, leading to the selection of more instances for labeling and reduced computational efficiency. In this paper, we resort to a novel kernel coding rate maximization (KECOR) strategy which aims to identify the most informative point clouds to acquire labels through the lens of information theory. Greedy search is applied to seek desired point clouds that can maximize the minimal number of bits required to encode the latent features. To determine the uniqueness and informativeness of the selected samples from the model perspec
    
[^20]: 高失真条件下单位范数向量的最优压缩

    Optimal Compression of Unit Norm Vectors in the High Distortion Regime. (arXiv:2307.07941v1 [cs.IT])

    [http://arxiv.org/abs/2307.07941](http://arxiv.org/abs/2307.07941)

    本研究探讨了在高失真情况下，将单位范数向量压缩到最少比特数的最优方法。研究发现，简单的压缩方案在这种情况下几乎是最优的。

    

    受到通信高效分布式学习的需求的驱动，我们研究了将单位范数向量压缩到最少比特数的方法，同时允许一定程度的失真恢复。这个问题在速率-失真/覆盖编码文献中已经被研究过，但我们的重点仅限于“高失真”情况。我们在最坏情况下考虑了这个问题，没有任何关于向量的先验信息，但允许使用随机压缩映射。我们研究了有偏和无偏压缩方法，并确定了最优压缩比率。结果表明，简单的压缩方案在这种情况下几乎是最优的。虽然结果是新旧问题的混合，但为了完整起见，它们在本文中予以整理。

    Motivated by the need for communication-efficient distributed learning, we investigate the method for compressing a unit norm vector into the minimum number of bits, while still allowing for some acceptable level of distortion in recovery. This problem has been explored in the rate-distortion/covering code literature, but our focus is exclusively on the "high-distortion" regime. We approach this problem in a worst-case scenario, without any prior information on the vector, but allowing for the use of randomized compression maps. Our study considers both biased and unbiased compression methods and determines the optimal compression rates. It turns out that simple compression schemes are nearly optimal in this scenario. While the results are a mix of new and known, they are compiled in this paper for completeness.
    
[^21]: 一种新颖的多通道彩色图像去噪的截断范数正则化方法

    A Novel Truncated Norm Regularization Method for Multi-channel Color Image Denoising. (arXiv:2307.07932v1 [eess.IV])

    [http://arxiv.org/abs/2307.07932](http://arxiv.org/abs/2307.07932)

    一种新颖的多通道彩色图像去噪方法，通过双加权截断核范数减去截断Frobenius范数最小化，利用非局部自相似性来提取相似的结构进行去噪，模型兼顾了跨通道差异和噪声的空间变化。

    

    鉴于低秩逼近方法具有高度的灵活性和显著的性能，在彩色图像去噪方面得到了广泛的研究。然而，这些方法大多忽略了跨通道差异或噪声的空间变化，这限制了它们在实际彩色图像去噪中的能力。为了克服这些缺点，本文提出了一种双加权截断核范数减去截断Frobenius范数最小化（DtNFM）的方法来去噪彩色图像。通过利用噪声图像的非局部自相似性，相似结构被收集并构造了一系列相似的块矩阵。对于每个分组，对其进行DtNFM模型来估计其去噪版本。通过连接所有去噪块矩阵，得到去噪图像。这种提出的DtNFM模型有两个优点。首先，它对跨通道差异和噪声的空间变化进行了建模和利用。这提供了足够的灵活性。

    Due to the high flexibility and remarkable performance, low-rank approximation methods has been widely studied for color image denoising. However, those methods mostly ignore either the cross-channel difference or the spatial variation of noise, which limits their capacity in real world color image denoising. To overcome those drawbacks, this paper is proposed to denoise color images with a double-weighted truncated nuclear norm minus truncated Frobenius norm minimization (DtNFM) method. Through exploiting the nonlocal self-similarity of the noisy image, the similar structures are gathered and a series of similar patch matrices are constructed. For each group, the DtNFM model is conducted for estimating its denoised version. The denoised image would be obtained by concatenating all the denoised patch matrices. The proposed DtNFM model has two merits. First, it models and utilizes both the cross-channel difference and the spatial variation of noise. This provides sufficient flexibility 
    
[^22]: 论分裂学习对抗敌对攻击的鲁棒性

    On the Robustness of Split Learning against Adversarial Attacks. (arXiv:2307.07916v1 [cs.LG])

    [http://arxiv.org/abs/2307.07916](http://arxiv.org/abs/2307.07916)

    该论文评估了分裂学习对抗敌对攻击的鲁棒性，特别关注不受信任服务器只能访问模型中间层的情况。通过仅向服务器公开部分模型，分裂学习可以缓解敌对攻击威胁。

    

    分裂学习通过避免直接共享原始数据和模型细节（即服务器和客户端仅持有部分子网络并交换中间计算）来实现协同深度学习模型训练的同时保护数据隐私和模型安全性。然而，现有研究主要集中在检验其对隐私保护的可靠性，对模型安全性的研究甚少。具体而言，通过探索完整模型，攻击者可以发起敌对攻击，而分裂学习可以通过仅向不受信任的服务器公开部分模型来缓解这种严重威胁。本文旨在评估分裂学习对抗敌对攻击的鲁棒性，特别是在不受信任的服务器只能访问模型的中间层的最具挑战性的情况下。现有的敌对攻击大多集中在集中式环境而非协同式环境，因此，为了更好地评估分裂学习的鲁棒性，我们提出了一种新的评估方法。

    Split learning enables collaborative deep learning model training while preserving data privacy and model security by avoiding direct sharing of raw data and model details (i.e., sever and clients only hold partial sub-networks and exchange intermediate computations). However, existing research has mainly focused on examining its reliability for privacy protection, with little investigation into model security. Specifically, by exploring full models, attackers can launch adversarial attacks, and split learning can mitigate this severe threat by only disclosing part of models to untrusted servers.This paper aims to evaluate the robustness of split learning against adversarial attacks, particularly in the most challenging setting where untrusted servers only have access to the intermediate layers of the model.Existing adversarial attacks mostly focus on the centralized setting instead of the collaborative setting, thus, to better evaluate the robustness of split learning, we develop a ta
    
[^23]: 利用FPGA加速技术的生物医学计算研究

    Exploiting FPGA Capabilities for Accelerated Biomedical Computing. (arXiv:2307.07914v1 [cs.AR])

    [http://arxiv.org/abs/2307.07914](http://arxiv.org/abs/2307.07914)

    本研究利用FPGA技术提出了高级神经网络架构，并利用自定义的张量计算单元加速器对生物医学的心电图信号进行分析。研究的重点是提高算法的鲁棒性和实际应用的性能指标。

    

    本研究提出了先进的神经网络架构，包括卷积神经网络（CNN）、循环神经网络（RNN）、长短期记忆网络（LSTM）和深度置信网络（DBN），利用可编程门阵列（FPGA）来增强心电图信号分析。我们利用MIT-BIH心律失常数据库进行训练和验证，并引入高斯噪声来提高算法的鲁棒性。所实现的模型具有各种层，用于不同的处理和分类任务，同时采用了早停和丢弃层等技术来减轻过拟合问题。我们的工作还探索了为PYNQ Z1板研发自定义的张量计算单元（TCU）加速器，提供了基于FPGA的机器学习的全面步骤，包括在Docker中设置Tensil开发工具链，选择构架，配置PS-PL以及编译和执行模型。性能指标如延迟和吞吐量也得到了计算，以进行实际的研究评估。

    This study presents advanced neural network architectures including Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Long Short-Term Memory Networks (LSTMs), and Deep Belief Networks (DBNs) for enhanced ECG signal analysis using Field Programmable Gate Arrays (FPGAs). We utilize the MIT-BIH Arrhythmia Database for training and validation, introducing Gaussian noise to improve algorithm robustness. The implemented models feature various layers for distinct processing and classification tasks and techniques like EarlyStopping callback and Dropout layer are used to mitigate overfitting. Our work also explores the development of a custom Tensor Compute Unit (TCU) accelerator for the PYNQ Z1 board, offering comprehensive steps for FPGA-based machine learning, including setting up the Tensil toolchain in Docker, selecting architecture, configuring PS-PL, and compiling and executing models. Performance metrics such as latency and throughput are calculated for practical in
    
[^24]: 使用多层合成有限元模型模拟来预测碳纳米管（CNT）图像的力学性能

    Predicting mechanical properties of Carbon Nanotube (CNT) images Using Multi-Layer Synthetic Finite Element Model Simulations. (arXiv:2307.07912v1 [cs.LG])

    [http://arxiv.org/abs/2307.07912](http://arxiv.org/abs/2307.07912)

    使用多层合成有限元模型模拟，通过基于AI的深度学习模型预测碳纳米管（CNT）图像的力学性能。将2D合成图像混合生成更接近于真实图像的MLS图像，并使用物理模型估计其力学性质。提出的CNTNeXt深度学习架构通过ResNeXt特征表示和随机森林回归估计器改进了预测精度。

    

    我们提出了一个管道，利用基于人工智能（AI）的材料发现的深度学习模型来预测垂直定向碳纳米管（CNT）森林图像的力学性能。我们的方法采用一种创新的数据增强技术，即使用多层合成（MLS）或准2.5D图像，这些图像是通过混合2D合成图像生成的。MLS图像更接近于3D合成和真实扫描电子显微镜（SEM）图像，但不需要执行昂贵的3D模拟或实验的计算成本。基于物理的模型估计了MLS图像的刚度和屈曲荷载。提出的深度学习架构CNTNeXt在我们先前的CNTNet神经网络基础上进行了改进，使用了ResNeXt特征表示，随后使用随机森林回归估计器。我们的机器学习方法利用混合集合来预测CNT的物理性质。

    We present a pipeline for predicting mechanical properties of vertically-oriented carbon nanotube (CNT) forest images using a deep learning model for artificial intelligence (AI)-based materials discovery. Our approach incorporates an innovative data augmentation technique that involves the use of multi-layer synthetic (MLS) or quasi-2.5D images which are generated by blending 2D synthetic images. The MLS images more closely resemble 3D synthetic and real scanning electron microscopy (SEM) images of CNTs but without the computational cost of performing expensive 3D simulations or experiments. Mechanical properties such as stiffness and buckling load for the MLS images are estimated using a physics-based model. The proposed deep learning architecture, CNTNeXt, builds upon our previous CNTNet neural network, using a ResNeXt feature representation followed by random forest regression estimator. Our machine learning approach for predicting CNT physical properties by utilizing a blended set
    
[^25]: MESOB: 平衡均衡与社会最优性

    MESOB: Balancing Equilibria & Social Optimality. (arXiv:2307.07911v1 [cs.GT])

    [http://arxiv.org/abs/2307.07911](http://arxiv.org/abs/2307.07911)

    本文提出了一种双目标优化方法MESOB及相应的MESOB-OMO方法，用于解决多层次、多智能体博弈中的竞争和合作问题。实验证明了其可在拍卖出价推荐等问题中平衡不同利益方面的有效性。

    

    本文受在线广告拍卖中的出价推荐启发，考虑了一类具有两个主要特征的多层次和多智能体博弈：一个是大量的匿名智能体，另一个是竞争和合作之间复杂的相互作用。为了模拟这种复杂系统，我们提出了一种新颖且可操作的双目标优化方法，称为MESOB（Mean-field Equilibria & Social Optimality Balancing），并提出了一种相关的职业度量优化（OMO）方法MESOB-OMO来解决它。MESOB-OMO能够基于MESOB中的竞争和合作的双目标获取近似帕累托有效的解，特别是允许渐近地选择纳什均衡和社会均衡化。我们将MESOB-OMO应用于模拟的按点击付费广告拍卖的出价推荐。实验证明了其在平衡不同利益方面的效果。

    Motivated by bid recommendation in online ad auctions, this paper considers a general class of multi-level and multi-agent games, with two major characteristics: one is a large number of anonymous agents, and the other is the intricate interplay between competition and cooperation. To model such complex systems, we propose a novel and tractable bi-objective optimization formulation with mean-field approximation, called MESOB (Mean-field Equilibria & Social Optimality Balancing), as well as an associated occupation measure optimization (OMO) method called MESOB-OMO to solve it. MESOB-OMO enables obtaining approximately Pareto efficient solutions in terms of the dual objectives of competition and cooperation in MESOB, and in particular allows for Nash equilibrium selection and social equalization in an asymptotic manner. We apply MESOB-OMO to bid recommendation in a simulated pay-per-click ad auction. Experiments demonstrate its efficacy in balancing the interests of different parties an
    
[^26]: 视而不见：针对虚假相关性的鲁棒强化学习

    Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation. (arXiv:2307.07907v1 [cs.LG])

    [http://arxiv.org/abs/2307.07907](http://arxiv.org/abs/2307.07907)

    本文研究了如何在强化学习中使模型具有对虚假相关性的鲁棒性，这种虚假相关性是由于不可观察的混杂因素引起的，并且普遍存在于现实世界的任务中。

    

    鲁棒性在强化学习中得到了广泛研究，用于处理各种不确定性形式，如随机扰动、罕见事件和恶意攻击。本文研究一种关键类型的鲁棒性，即对虚假相关性的鲁棒性，其中状态的不同部分没有因果关系，但却存在由不可观察的混杂因素引起的相关性。这些虚假相关性在现实世界的任务中很普遍，例如，自动驾驶汽车通常在白天观察到交通拥堵，夜晚观察到交通轻松，原因是不可观察的人类活动。模型在学习这种无用甚至有害相关性时，当测试案例的混杂因素偏离训练时，可能会造成灾难性的失败。尽管如此，使模型对虚假相关性具有鲁棒性仍面临重要挑战，因为由不可观察的混杂因素和强化学习的顺序结构形成的不确定性集很难进行表征和识别。现有的鲁棒算法往往无法应对这一挑战。

    Robustness has been extensively studied in reinforcement learning (RL) to handle various forms of uncertainty such as random perturbations, rare events, and malicious attacks. In this work, we consider one critical type of robustness against spurious correlation, where different portions of the state do not have causality but have correlations induced by unobserved confounders. These spurious correlations are ubiquitous in real-world tasks, for instance, a self-driving car usually observes heavy traffic in the daytime and light traffic at night due to unobservable human activity. A model that learns such useless or even harmful correlation could catastrophically fail when the confounder in the test case deviates from the training one. Although motivated, enabling robustness against spurious correlation poses significant challenges since the uncertainty set, shaped by the unobserved confounder and sequential structure of RL, is difficult to characterize and identify. Existing robust alg
    
[^27]: 自动化纤维成型中的异常检测：数据有限的学习

    Anomaly Detection in Automated Fibre Placement: Learning with Data Limitations. (arXiv:2307.07893v1 [cs.CV])

    [http://arxiv.org/abs/2307.07893](http://arxiv.org/abs/2307.07893)

    本文提出了一种在数据有限的情况下，通过自动编码器进行异常检测的方法，利用纤维层片的深度图进行二分类，并使用重构误差作为量化指标。

    

    当前自动化纤维成型(AFP)的缺陷检测系统主要基于端到端的监督学习方法，需要大量标记的有缺陷样本，而这些样本很难生成足够数量。为了解决这个数据稀缺的问题，我们引入了一种基于自动编码器的方法，适用于小型数据集。幸运的是，从基础的角度来看，这个问题可以简化为正常样本和异常样本之间的二分类。所提出的方法使用纤维层片（tow）对纤维铺设表面的深度图进行分割成小窗口。其中不包含异常的窗口子集传递给自动编码器来重构输入。因为自动编码器是用正常样本进行训练的，对于这些样本，它产生的重构比对于异常样本更精确。因此，重构误差的值被用作一个量化指标，用于判断是否存在潜在的异常。

    Current defect detection systems for Automated Fibre Placement (AFP) are mostly based on end-to-end supervised learning methods requiring abundant labelled defective samples, which are not easily generated in sufficient numbers. To address this data scarcity problem, we introduce an autoencoder-based approach compatible with small datasets. Fortunately, the problem from a foundational point of view can be simplified as a binary classification between normal and abnormal samples. The proposed approach uses a depth map of the fibre layup surface, split into small windows aligned to each composite strip (tow). A subset of these windows that do not contain anomalies is passed to an autoencoder to reconstruct the input. Because the autoencoder is trained with normal samples, it produces more accurate reconstructions for these samples than for abnormal ones. Therefore, the value of reconstruction error is used as a quantitative metric for whether there are potential anomalies. These values a
    
[^28]: 多时相SAR图像变化检测和可视化使用RABASAR和简化GLR

    Multitemporal SAR images change detection and visualization using RABASAR and simplified GLR. (arXiv:2307.07892v1 [cs.CV])

    [http://arxiv.org/abs/2307.07892](http://arxiv.org/abs/2307.07892)

    通过使用RABASAR和简化的GLR方法，在多时相SAR图像中进行变化检测和可视化，并开发了新的变化幅度指数方法和改进的变化分类方法。

    

    了解变化区域的状态需要提供关于变化的精确信息。因此，检测不同类型的变化对于地表监测是重要的。SAR传感器非常适合完成这项任务，因为它们具有全天候和全天时间的能力，且在幅度数据中不受大气成分影响的准确的获取几何和等效样本数量。在本研究中，我们提出了一个简化的广义似然比 ($S_{GLR}$) 方法，假设相应的时域像素具有相同的等效样本数量 (ENL)。借助于由基于比率的多时相SAR图像去噪方法 (RABASAR) 提供的去噪数据，我们成功地应用了这种相似性测试方法来计算变化区域。还开发了一种新的变化幅度指数方法和改进的基于光谱聚类的变化分类方法。此外，我们将简化的广义似然比应用于检测最大变化幅度。

    Understanding the state of changed areas requires that precise information be given about the changes. Thus, detecting different kinds of changes is important for land surface monitoring. SAR sensors are ideal to fulfil this task, because of their all-time and all-weather capabilities, with good accuracy of the acquisition geometry and without effects of atmospheric constituents for amplitude data. In this study, we propose a simplified generalized likelihood ratio ($S_{GLR}$) method assuming that corresponding temporal pixels have the same equivalent number of looks (ENL). Thanks to the denoised data provided by a ratio-based multitemporal SAR image denoising method (RABASAR), we successfully applied this similarity test approach to compute the change areas. A new change magnitude index method and an improved spectral clustering-based change classification method are also developed. In addition, we apply the simplified generalized likelihood ratio to detect the maximum change magnitud
    
[^29]: 手写和打印文本分割：一个签名案例研究

    Handwritten and Printed Text Segmentation: A Signature Case Study. (arXiv:2307.07887v1 [cs.CV])

    [http://arxiv.org/abs/2307.07887](http://arxiv.org/abs/2307.07887)

    本研究旨在解决手写和打印文本分割的挑战，并提出了一种新的方法来完整地恢复不同类别的文本，特别是在重叠部分提高分割性能。同时，还引入了一个新的数据集SignaTR6K，用于支持该任务。

    

    在分析扫描文档时，手写文本可能覆盖打印文本。这在文档的光学字符识别（OCR）和数字化过程中造成困难，并且进而影响到下游的自然语言处理（NLP）任务。之前的研究要么仅关注手写文本的二分类，要么进行三类文档的分割，即手写、打印和背景像素的识别。这导致手写和打印重叠的像素只被分配到一个类别中，因此在另一个类别中不被考虑。因此，在这项研究中，我们开发了新的方法来解决手写和打印文本分割的挑战，目标是完整地恢复不同类别的文本，特别是提高重叠部分的分割性能。为了促进这项任务，我们介绍了一个新的数据集SignaTR6K，该数据集收集自真实的法律文件。

    While analyzing scanned documents, handwritten text can overlay printed text. This causes difficulties during the optical character recognition (OCR) and digitization process of documents, and subsequently, hurts downstream NLP tasks. Prior research either focuses only on the binary classification of handwritten text, or performs a three-class segmentation of the document, i.e., recognition of handwritten, printed, and background pixels. This results in the assignment of the handwritten and printed overlapping pixels to only one of the classes, and thus, they are not accounted for in the other class. Thus, in this research, we develop novel approaches for addressing the challenges of handwritten and printed text segmentation with the goal of recovering text in different classes in whole, especially improving the segmentation performance on the overlapping parts. As such, to facilitate with this task, we introduce a new dataset, SignaTR6K, collected from real legal documents, as well as
    
[^30]: 无梯度的集合卡尔曼反演方法用于系统识别和控制的神经ODE训练

    Gradient-free training of neural ODEs for system identification and control using ensemble Kalman inversion. (arXiv:2307.07882v1 [cs.LG])

    [http://arxiv.org/abs/2307.07882](http://arxiv.org/abs/2307.07882)

    本论文研究了集合卡尔曼反演方法在训练神经ODE进行系统识别和控制任务中的有效性，并证明了EKI是一种高效的神经ODE训练方法，其解决方案的运行时间和质量与常用的基于梯度的优化器相媲美。

    

    集合卡尔曼反演（EKI）是一种用于贝叶斯框架下求解反问题的顺序蒙特卡洛方法。与反向传播不同，EKI是一种无梯度优化方法，仅需要前向传递中人工神经网络的评估。在这项研究中，我们考察了EKI在神经常微分方程（神经ODE）的系统识别和控制任务中的有效性。为了将EKI应用于最优控制问题，我们制定了包含Tikhonov型正则化项的反问题。我们的数值结果表明，在系统识别和最优控制问题中，EKI是一种高效的神经ODE训练方法，其运行时间和解的质量与常用的基于梯度的优化器相比具有竞争力。

    Ensemble Kalman inversion (EKI) is a sequential Monte Carlo method used to solve inverse problems within a Bayesian framework. Unlike backpropagation, EKI is a gradient-free optimization method that only necessitates the evaluation of artificial neural networks in forward passes. In this study, we examine the effectiveness of EKI in training neural ordinary differential equations (neural ODEs) for system identification and control tasks. To apply EKI to optimal control problems, we formulate inverse problems that incorporate a Tikhonov-type regularization term. Our numerical results demonstrate that EKI is an efficient method for training neural ODEs in system identification and optimal control problems, with runtime and quality of solutions that are competitive with commonly used gradient-based optimizers.
    
[^31]: 嵌入图的直觉模糊RVFL用于类别不平衡学习

    Graph Embedded Intuitionistic Fuzzy RVFL for Class Imbalance Learning. (arXiv:2307.07881v1 [cs.LG])

    [http://arxiv.org/abs/2307.07881](http://arxiv.org/abs/2307.07881)

    提出了一种嵌入图的直觉模糊RVFL模型，用于解决类别不平衡学习的问题。该模型通过采用加权机制处理不平衡的数据集，并利用图嵌入提取语义丰富的信息。该模型在类别不平衡学习中具有较高的准确性。

    

    机器学习领域面临一个关键的研究领域，即类别不平衡学习，在对少数类别进行准确分类方面存在很大的困难。这个问题可能导致偏向于多数类别的模型在训练过程中优先考虑，从而导致少数类别的代表不足。随机向量函数链接（RVFL）网络是一种广泛使用且有效的分类学习模型，因其速度和效率高而受到广泛应用。然而，当处理不平衡的数据集时，它的准确性较低。为了克服这个局限性，我们提出了一种新颖的嵌入图的直觉模糊RVFL用于类别不平衡学习（GE-IFRVFL-CIL）模型，该模型采用加权机制来处理不平衡的数据集。所提出的GE-IFRVFL-CIL模型具有许多优点，例如：（i）它利用图嵌入从数据集中提取语义丰富的信息，（ii）它使用直觉模糊集来进行类别不平衡学习。

    The domain of machine learning is confronted with a crucial research area known as class imbalance learning, which presents considerable hurdles in the precise classification of minority classes. This issue can result in biased models where the majority class takes precedence in the training process, leading to the underrepresentation of the minority class. The random vector functional link (RVFL) network is a widely-used and effective learning model for classification due to its speed and efficiency. However, it suffers from low accuracy when dealing with imbalanced datasets. To overcome this limitation, we propose a novel graph embedded intuitionistic fuzzy RVFL for class imbalance learning (GE-IFRVFL-CIL) model incorporating a weighting mechanism to handle imbalanced datasets. The proposed GE-IFRVFL-CIL model has a plethora of benefits, such as $(i)$ it leverages graph embedding to extract semantically rich information from the dataset, $(ii)$ it uses intuitionistic fuzzy sets to ha
    
[^32]: 探索从替代训练中理解对抗性可转移性

    Towards Understanding Adversarial Transferability From Surrogate Training. (arXiv:2307.07873v1 [cs.LG])

    [http://arxiv.org/abs/2307.07873](http://arxiv.org/abs/2307.07873)

    本论文探索了对抗性可转移性的理解，特别关注替代训练。通过研究模型的平滑性和梯度相似性之间的权衡，发现对抗训练可以提高模型的替代能力。研究结果对数据分布的转变提出了新的推测。

    

    对DNNs的对抗样本(AEs)已经表明是可转移的：成功欺骗白盒子替代模型的AEs也可以欺骗具有不同架构的其他黑盒模型。虽然许多经验研究提供了生成高度可转移AE的指导，但这些研究缺乏解释甚至导致不一致的建议。本文在理解对抗性可转移性方面迈出了一步，特别关注替代方面。从着名的小健壮性现象开始，通过以轻微扰动的对抗性样本对模型进行对抗训练可以得到更好的替代模型，我们将其归因于两个主要因素之间的权衡：模型的平滑性和梯度相似性。我们的研究集中在它们的共同效果上，而不是它们与可转移性的单独相关性。通过一系列理论和实证分析，我们推测数据分布的转变。

    Adversarial examples (AEs) for DNNs have been shown to be transferable: AEs that successfully fool white-box surrogate models can also deceive other black-box models with different architectures. Although a bunch of empirical studies have provided guidance on generating highly transferable AEs, many of these findings lack explanations and even lead to inconsistent advice. In this paper, we take a further step towards understanding adversarial transferability, with a particular focus on surrogate aspects. Starting from the intriguing little robustness phenomenon, where models adversarially trained with mildly perturbed adversarial samples can serve as better surrogates, we attribute it to a trade-off between two predominant factors: model smoothness and gradient similarity. Our investigations focus on their joint effects, rather than their separate correlations with transferability. Through a series of theoretical and empirical analyses, we conjecture that the data distribution shift in
    
[^33]: 自监督学习中是否会发生双下降现象？

    Does Double Descent Occur in Self-Supervised Learning?. (arXiv:2307.07872v1 [cs.LG])

    [http://arxiv.org/abs/2307.07872](http://arxiv.org/abs/2307.07872)

    研究发现，在自监督学习中缺乏双下降现象，进一步的研究有助于揭示其理论基础。

    

    大多数关于双下降现象的研究都集中在监督模型上，而对于自监督设置的研究却发现这种现象的缺失令人惊讶。这些结果表明，在自监督模型中可能不存在双下降现象。我们通过使用标准和线性自编码器来进行实证研究，发现测试损失要么呈现经典的U型曲线，要么单调递减，而不是呈现双下降曲线。我们希望进一步的研究能够揭示这一现象的理论基础。

    Most investigations into double descent have focused on supervised models while the few works studying self-supervised settings find a surprising lack of the phenomenon. These results imply that double descent may not exist in self-supervised models. We show this empirically using a standard and linear autoencoder, two previously unstudied settings. The test loss is found to have either a classical U-shape or to monotonically decrease instead of exhibiting a double-descent curve. We hope that further work on this will help elucidate the theoretical underpinnings of this phenomenon.
    
[^34]: 社会AI学校：从发展心理学到人工社会文化代理的观点

    The SocialAI School: Insights from Developmental Psychology Towards Artificial Socio-Cultural Agents. (arXiv:2307.07871v1 [cs.AI])

    [http://arxiv.org/abs/2307.07871](http://arxiv.org/abs/2307.07871)

    该论文讨论了AI研究应该受发展心理学启发，并研究使代理能够进入文化的社会认知能力。提出了社会AI学校工具以便于进行相关实验。

    

    发展心理学家长期以来已经确立了社会认知能力在人类智力中的重要性。这些能力使我们能够进入、参与和从人类文化中受益。社会交互代理的AI研究大多关注多智能体环境中文化的出现（通常没有强烈的发展心理学基础）。我们认为AI研究应该受心理学启发，并研究能够进入文化的社会认知能力。我们讨论了Michael Tomasello和Jerome Bruner的理论，介绍了他们的一些概念，并概述了关键概念和社会认知能力。我们提出了社会AI学校——一个包括定制参数化环境的工具，简化了关于这些概念的实验。我们展示了使用RL代理和大型语言模型进行此类实验的示例。这项工作的主要动机是吸引AI社区围绕这些概念进行讨论和研究。

    Developmental psychologists have long-established the importance of socio-cognitive abilities in human intelligence. These abilities enable us to enter, participate and benefit from human culture. AI research on social interactive agents mostly concerns the emergence of culture in a multi-agent setting (often without a strong grounding in developmental psychology). We argue that AI research should be informed by psychology and study socio-cognitive abilities enabling to enter a culture too. We discuss the theories of Michael Tomasello and Jerome Bruner to introduce some of their concepts to AI and outline key concepts and socio-cognitive abilities. We present The SocialAI school - a tool including a customizable parameterized uite of procedurally generated environments, which simplifies conducting experiments regarding those concepts. We show examples of such experiments with RL agents and Large Language Models. The main motivation of this work is to engage the AI community around the 
    
[^35]: 大型语言模型作为文化角度的叠加

    Large Language Models as Superpositions of Cultural Perspectives. (arXiv:2307.07870v1 [cs.CL])

    [http://arxiv.org/abs/2307.07870](http://arxiv.org/abs/2307.07870)

    大型语言模型被认为是具有个性或一套价值观的，但实际上它可以看作是具有不同价值观和个性特征的角度的叠加。通过角度可控性的概念，我们研究了大型语言模型在不同角度下展示的价值观和个性特征的变化。实验结果表明，即使在没有明显提示的情况下，大型语言模型也会表达出不同的价值观。

    

    大型语言模型（LLMs）常常被错误地认为具有个性或一套价值观。我们认为LLMs可以看作是具有不同价值观和个性特征的角度叠加。LLMs表现出依赖于上下文的价值观和个性特征，这些特征基于产生的角度而改变（与人类相反，人类在不同情境下通常具有更一致的价值观和个性特征）。我们引入了“角度可控性”的概念，指的是模型采用不同具有不同价值观和个性特征的角度的能力。在我们的实验中，我们使用心理学问卷（PVQ、VSM、IPIP）来研究展示的价值观和个性特征如何基于不同角度而改变。通过定性实验，我们展示了当提示中（隐式或显式）暗示了某些价值观时，LLMs表达出不同的价值观，即使在没有明显暗示的情况下，LLMs也会表达出不同的价值观。

    Large Language Models (LLMs) are often misleadingly recognized as having a personality or a set of values. We argue that an LLM can be seen as a superposition of perspectives with different values and personality traits. LLMs exhibit context-dependent values and personality traits that change based on the induced perspective (as opposed to humans, who tend to have more coherent values and personality traits across contexts). We introduce the concept of perspective controllability, which refers to a model's affordance to adopt various perspectives with differing values and personality traits. In our experiments, we use questionnaires from psychology (PVQ, VSM, IPIP) to study how exhibited values and personality traits change based on different perspectives. Through qualitative experiments, we show that LLMs express different values when those are (implicitly or explicitly) implied in the prompt, and that LLMs express different values even when those are not obviously implied (demonstrat
    
[^36]: 使用奖励调节的反STDP学习构建自定义DNN进行时间模式识别

    Custom DNN using Reward Modulated Inverted STDP Learning for Temporal Pattern Recognition. (arXiv:2307.07869v1 [cs.NE])

    [http://arxiv.org/abs/2307.07869](http://arxiv.org/abs/2307.07869)

    本文提出了一种使用奖励调节的反STDP学习的自定义DNN算法，用于在稀疏事件序列数据上进行时间模式识别。该算法通过组合多种学习方法，在短时间间隔的动态数据集中识别模式。在复杂的语音数字数据集上的实验结果表明，该算法具有较高的性能。

    

    时间脉冲识别在各个领域中起着重要作用，包括异常检测、关键词识别和神经科学。本文提出了一种新颖的算法，用于在稀疏事件序列数据上进行高效的时间脉冲模式识别。该算法结合奖励调节、Hebbian和反Hebbian学习方法，以识别短时间间隔训练中动态数据集中的模式。算法首先进行预处理，将输入数据理性化并转化为特征丰富且稀疏的脉冲时间序列数据。接下来，线性前馈脉冲神经网络处理这些数据，以识别已训练的模式。最后，下一层进行加权检查，以确保正确的模式已被检测出来。为了评估所提算法的性能，它在包含脉冲信息的语音数字的复杂数据集上进行了训练，并与最先进的方法进行了对比。

    Temporal spike recognition plays a crucial role in various domains, including anomaly detection, keyword spotting and neuroscience. This paper presents a novel algorithm for efficient temporal spike pattern recognition on sparse event series data. The algorithm leverages a combination of reward-modulatory behavior, Hebbian and anti-Hebbian based learning methods to identify patterns in dynamic datasets with short intervals of training. The algorithm begins with a preprocessing step, where the input data is rationalized and translated to a feature-rich yet sparse spike time series data. Next, a linear feed forward spiking neural network processes this data to identify a trained pattern. Finally, the next layer performs a weighted check to ensure the correct pattern has been detected.To evaluate the performance of the proposed algorithm, it was trained on a complex dataset containing spoken digits with spike information and its output compared to state-of-the-art.
    
[^37]: 利用情感分析辅助的各种LSTM模型对比股票价格预测模型的效率

    Contrasting the efficiency of stock price prediction models using various types of LSTM models aided with sentiment analysis. (arXiv:2307.07868v1 [q-fin.ST])

    [http://arxiv.org/abs/2307.07868](http://arxiv.org/abs/2307.07868)

    通过对比不同类型的LSTM模型和情感分析的效果，我们的研究寻求寻找最佳模型，以利用公司的预测和行业表现来正确预测股票价格的短期和长期目标。

    

    我们的研究旨在找到一个最佳模型，利用公司的预测和行业表现，然后根据给定公司的情况，正确预测股票价格的短期和长期目标。

    Our research aims to find the best model that uses companies projections and sector performances and how the given company fares accordingly to correctly predict equity share prices for both short and long term goals.
    
[^38]: 检测豆类的分类算法和支持向量机核函数的有效性的基准测试

    Benchmarking the Effectiveness of Classification Algorithms and SVM Kernels for Dry Beans. (arXiv:2307.07863v1 [cs.LG])

    [http://arxiv.org/abs/2307.07863](http://arxiv.org/abs/2307.07863)

    本研究通过分析和比较不同的分类算法和SVM核函数在豆类数据集上的性能，发现RBF SVM核心算法在准确率、精确率、召回率和F1得分上表现最佳，提供了重要的指导。

    

    植物育种师和农业研究人员可以通过分析豆类数据集来识别理想特征、抗病性和营养含量，从而提高作物产量。本研究分析和比较了不同的支持向量机（SVM）分类算法，包括线性、多项式和径向基函数（RBF），以及其他流行的分类算法。分析是在豆类数据集上进行的，主要使用的评估指标是准确率，而RBF SVM核心算法实现了最高的准确率93.34%，精确率92.61%，召回率92.35%和F1得分91.40%。除了熟练的可视化和经验分析，本研究通过强调考虑不同的SVM算法来处理复杂和非线性结构化数据集，提供了有价值的指导。

    Plant breeders and agricultural researchers can increase crop productivity by identifying desirable features, disease resistance, and nutritional content by analysing the Dry Bean dataset. This study analyses and compares different Support Vector Machine (SVM) classification algorithms, namely linear, polynomial, and radial basis function (RBF), along with other popular classification algorithms. The analysis is performed on the Dry Bean Dataset, with PCA (Principal Component Analysis) conducted as a preprocessing step for dimensionality reduction. The primary evaluation metric used is accuracy, and the RBF SVM kernel algorithm achieves the highest Accuracy of 93.34%, Precision of 92.61%, Recall of 92.35% and F1 Score as 91.40%. Along with adept visualization and empirical analysis, this study offers valuable guidance by emphasizing the importance of considering different SVM algorithms for complex and non-linear structured datasets.
    
[^39]: 用高斯评分匹配进行变分推理

    Variational Inference with Gaussian Score Matching. (arXiv:2307.07849v1 [stat.ML])

    [http://arxiv.org/abs/2307.07849](http://arxiv.org/abs/2307.07849)

    本文提出了一种用高斯评分匹配的方法来进行变分推理，通过迭代算法将变分近似与精确后验的评分匹配。当变分分布是高斯分布时，内部优化问题有闭式解。

    

    变分推理（VI）是一种逼近贝叶斯统计中的计算困难后验分布的方法。通常，VI通过最小化适当的目标函数（例如证据下界ELBO）将简单的参数分布拟合到目标后验分布中。在本文中，我们提出了一种基于评分匹配原理的新型VI方法，即如果两个分布相等，则它们的评分函数（即对数密度的梯度）在其支持集的每个点上都相等。基于这一原理，我们开发了评分匹配VI，这是一个迭代算法，旨在匹配变分近似与精确后验之间的评分。在每次迭代中，评分匹配VI解决了一个内部优化问题，即最小调整当前变分估计，使其与新抽取的潜变量值处的评分匹配。我们证明，当变分分布是高斯分布时，这个内部优化问题有一个闭式解。

    Variational inference (VI) is a method to approximate the computationally intractable posterior distributions that arise in Bayesian statistics. Typically, VI fits a simple parametric distribution to the target posterior by minimizing an appropriate objective such as the evidence lower bound (ELBO). In this work, we present a new approach to VI based on the principle of score matching, that if two distributions are equal then their score functions (i.e., gradients of the log density) are equal at every point on their support. With this, we develop score matching VI, an iterative algorithm that seeks to match the scores between the variational approximation and the exact posterior. At each iteration, score matching VI solves an inner optimization, one that minimally adjusts the current variational estimate to match the scores at a newly sampled value of the latent variables. We show that when the variational family is a Gaussian, this inner optimization enjoys a closed form solution, wh
    
[^40]: 云游戏中的神经视频恢复

    Neural Video Recovery for Cloud Gaming. (arXiv:2307.07847v1 [cs.NI])

    [http://arxiv.org/abs/2307.07847](http://arxiv.org/abs/2307.07847)

    本文提出了一个新方法，用于在云游戏中恢复丢失或损坏的视频帧。与传统方法不同的是，我们利用游戏状态和部分解码帧来提高恢复准确性。

    

    云游戏是一个价值数十亿美元的行业。在云游戏中，客户端将自己的移动发送到互联网上的游戏服务器，服务器将渲染并传输结果视频回来。为了提供良好的游戏体验，需要低于80毫秒的延迟。这意味着视频的渲染、编码、传输、解码和显示必须在这个时间范围内完成，由于服务器过载、网络拥塞和丢包等因素，这一点特别具有挑战性。在本文中，我们提出了一种在云游戏中恢复丢失或损坏视频帧的新方法。与传统视频帧恢复不同，我们的方法利用游戏状态显著提升恢复准确性，并利用部分解码的帧来恢复丢失的部分。我们开发了一个综合性的系统，包括(i)高效提取游戏状态，(ii)修改 H.264 视频解码器生成一个指示需要恢复视频帧哪些部分的掩码，和 (iii)设计一个新颖的神经网络进行视频帧恢复。

    Cloud gaming is a multi-billion dollar industry. A client in cloud gaming sends its movement to the game server on the Internet, which renders and transmits the resulting video back. In order to provide a good gaming experience, a latency below 80 ms is required. This means that video rendering, encoding, transmission, decoding, and display have to finish within that time frame, which is especially challenging to achieve due to server overload, network congestion, and losses. In this paper, we propose a new method for recovering lost or corrupted video frames in cloud gaming. Unlike traditional video frame recovery, our approach uses game states to significantly enhance recovery accuracy and utilizes partially decoded frames to recover lost portions. We develop a holistic system that consists of (i) efficiently extracting game states, (ii) modifying H.264 video decoder to generate a mask to indicate which portions of video frames need recovery, and (iii) designing a novel neural networ
    
[^41]: Transformers是通用的预测器

    Transformers are Universal Predictors. (arXiv:2307.07843v1 [cs.LG])

    [http://arxiv.org/abs/2307.07843](http://arxiv.org/abs/2307.07843)

    Transformers架构在语言建模中具有通用的预测性质，并且在非渐近数据环境中表现良好。

    

    我们找到了Transformer架构在语言建模中的局限性，并证明了它在信息理论意义上具有通用的预测性质。我们进一步分析了在非渐近数据环境中的性能，以了解Transformer架构的各个组件在数据高效训练的背景下的作用。我们通过对合成数据集和真实数据集的实验验证了我们的理论分析。

    We find limits to the Transformer architecture for language modeling and show it has a universal prediction property in an information-theoretic sense. We further analyze performance in non-asymptotic data regimes to understand the role of various components of the Transformer architecture, especially in the context of data-efficient training. We validate our theoretical analysis with experiments on both synthetic and real datasets.
    
[^42]: RegExplainer: 在回归任务中生成图神经网络的解释

    RegExplainer: Generating Explanations for Graph Neural Networks in Regression Task. (arXiv:2307.07840v1 [cs.LG])

    [http://arxiv.org/abs/2307.07840](http://arxiv.org/abs/2307.07840)

    这项工作提出了一种新的解释方法（XAIG-R），用于解释图回归模型，通过引入信息瓶颈理论的新目标和混合框架来解决回归任务中的挑战，同时还使用对比学习策略来处理连续有序标签。

    

    图回归是一项基础任务，在各种图学习任务中受到越来越多的关注。然而，推理过程通常是不可解释的。现有的解释技术大多限于理解分类任务中图神经网络的行为。在这项工作中，我们寻求解释来解释图回归模型（XAIG-R）。我们展示了现有方法忽视了分布偏移和连续有序的决策边界，这阻碍了它们在回归任务中的应用。为了解决这些挑战，我们提出了一种基于信息瓶颈理论的新目标，并引入了一种新的混合框架，可以以模型无关的方式支持各种图神经网络。我们进一步提出了一种对比学习策略来应对回归任务中的连续有序标签。为了从经验上验证所提出的方法的有效性，我们引入了三个基准数据集和一个真实数据集进行评估。

    Graph regression is a fundamental task and has received increasing attention in a wide range of graph learning tasks. However, the inference process is often not interpretable. Most existing explanation techniques are limited to understanding GNN behaviors in classification tasks. In this work, we seek an explanation to interpret the graph regression models (XAIG-R). We show that existing methods overlook the distribution shifting and continuously ordered decision boundary, which hinders them away from being applied in the regression tasks. To address these challenges, we propose a novel objective based on the information bottleneck theory and introduce a new mix-up framework, which could support various GNNs in a model-agnostic manner. We further present a contrastive learning strategy to tackle the continuously ordered labels in regression task. To empirically verify the effectiveness of the proposed method, we introduce three benchmark datasets and a real-life dataset for evaluation
    
[^43]: MixupExplainer:通过数据增强为图神经网络提供通用解释

    MixupExplainer: Generalizing Explanations for Graph Neural Networks with Data Augmentation. (arXiv:2307.07832v1 [cs.LG])

    [http://arxiv.org/abs/2307.07832](http://arxiv.org/abs/2307.07832)

    本文提出了一种通用的图神经网络解释方法MixupExplainer，通过引入广义图信息瓶颈（GIB）和图mixup方法来解决现有解释方法中存在的分布偏移问题。

    

    图神经网络（GNNs）因其学习图结构数据的能力而受到越来越多的关注。然而，它们的预测往往不可解释。已经提出了事后实例级解释方法来理解GNN的预测。这些方法旨在发现解释训练过的GNN预测行为的子结构。本文揭示了现有方法中存在的分布偏移问题，在真实数据集的应用中特别影响解释质量，因为这些数据集具有严格的决策边界。为了解决这个问题，我们引入了一个包括独立于标签的图变量的广义图信息瓶颈（GIB）形式，等价于传统的GIB。受广义GIB的驱动，我们提出了一种图mixup方法，MixupExplainer，具有解决分布偏移问题的理论保证。我们在合成和真实世界数据集上进行了大量实验证明

    Graph Neural Networks (GNNs) have received increasing attention due to their ability to learn from graph-structured data. However, their predictions are often not interpretable. Post-hoc instance-level explanation methods have been proposed to understand GNN predictions. These methods seek to discover substructures that explain the prediction behavior of a trained GNN. In this paper, we shed light on the existence of the distribution shifting issue in existing methods, which affects explanation quality, particularly in applications on real-life datasets with tight decision boundaries. To address this issue, we introduce a generalized Graph Information Bottleneck (GIB) form that includes a label-independent graph variable, which is equivalent to the vanilla GIB. Driven by the generalized GIB, we propose a graph mixup method, MixupExplainer, with a theoretical guarantee to resolve the distribution shifting issue. We conduct extensive experiments on both synthetic and real-world datasets 
    
[^44]: 带有Mean-KL参数化的最小随机编码学习

    Minimal Random Code Learning with Mean-KL Parameterization. (arXiv:2307.07816v1 [cs.LG])

    [http://arxiv.org/abs/2307.07816](http://arxiv.org/abs/2307.07816)

    本文研究了最小随机编码学习（MIRACLE）的两个变体，提出了一种新的参数化方法Mean-KL，在压缩变分贝叶斯神经网络中实现了更快的收敛和良好的预测性能。

    

    本文研究了最小随机编码学习（MIRACLE）的两个变体在压缩变分贝叶斯神经网络中的定性行为和鲁棒性。MIRACLE实现了强大的条件高斯变分近似权重后验$Q_{\mathbf{w}}$，并使用相对熵编码来压缩从后验中抽样的权重，使用高斯编码分布$P_{\mathbf{w}}$。为了达到所需的压缩率，必须对$Q_{\mathbf{w}} \Vert P_{\mathbf{w}}$进行约束，这需要在传统的均值-方差（Mean-Var）参数化下进行计算上昂贵的退火过程。相反，我们通过其平均值和KL散度来参数化$Q_{\mathbf{w}}$，以通过构造将压缩成本约束为所需值。我们证明了使用Mean-KL参数化的变分训练收敛速度是传统方法的两倍，并且在训练后保持了预测性能。

    This paper studies the qualitative behavior and robustness of two variants of Minimal Random Code Learning (MIRACLE) used to compress variational Bayesian neural networks. MIRACLE implements a powerful, conditionally Gaussian variational approximation for the weight posterior $Q_{\mathbf{w}}$ and uses relative entropy coding to compress a weight sample from the posterior using a Gaussian coding distribution $P_{\mathbf{w}}$. To achieve the desired compression rate, $D_{\mathrm{KL}}[Q_{\mathbf{w}} \Vert P_{\mathbf{w}}]$ must be constrained, which requires a computationally expensive annealing procedure under the conventional mean-variance (Mean-Var) parameterization for $Q_{\mathbf{w}}$. Instead, we parameterize $Q_{\mathbf{w}}$ by its mean and KL divergence from $P_{\mathbf{w}}$ to constrain the compression cost to the desired value by construction. We demonstrate that variational training with Mean-KL parameterization converges twice as fast and maintains predictive performance after 
    
[^45]: 图自同态群等变神经网络

    Graph Automorphism Group Equivariant Neural Networks. (arXiv:2307.07810v1 [cs.LG])

    [http://arxiv.org/abs/2307.07810](http://arxiv.org/abs/2307.07810)

    本论文提供了图自同态群等变神经网络的完整特征化，找到了可学习的、线性的层函数之间的矩阵的生成集。

    

    对于任何具有n个顶点和其自同态群Aut(G)的图G，我们提供了所有可能的Aut(G)-等变神经网络的完整特征化，其层是n维实数张量的某些张量幂次。特别地，我们在n维实数空间的标准基下找到了可学习的、线性的Aut(G)-等变层函数之间的矩阵的生成集。

    For any graph $G$ having $n$ vertices and its automorphism group $\textrm{Aut}(G)$, we provide a full characterisation of all of the possible $\textrm{Aut}(G)$-equivariant neural networks whose layers are some tensor power of $\mathbb{R}^{n}$. In particular, we find a spanning set of matrices for the learnable, linear, $\textrm{Aut}(G)$-equivariant layer functions between such tensor power spaces in the standard basis of $\mathbb{R}^{n}$.
    
[^46]: 过参数化模型的插值信息准则

    The Interpolating Information Criterion for Overparameterized Models. (arXiv:2307.07785v1 [stat.ML])

    [http://arxiv.org/abs/2307.07785](http://arxiv.org/abs/2307.07785)

    本文提出了一个插值信息准则，用于过参数化模型的模型选择问题。通过建立贝叶斯对偶形式，该准则将先验选择纳入模型评估，并考虑了先验误设、模型的几何和谱特性。该准则在实证和理论行为方面与已知结果一致。

    

    本文考虑了过参数化估计器的模型选择问题，其中模型参数的数量超过数据集的大小。传统的信息准则通常考虑大数据极限，对模型大小进行惩罚。然而，在现代设置中，这些准则不适用，因为过参数化模型往往表现良好。对于任何过参数化模型，我们证明存在一个对偶的欠参数化模型，具有相同的边缘似然性，从而建立了贝叶斯对偶形式。这使得过参数化设置中可以使用更多经典方法，揭示了插值信息准则，一种自然地将先验选择纳入模型选择的模型质量度量。我们的新信息准则考虑了先验误设、模型的几何和谱特性，并且在该区域与已知的经验和理论行为一致。

    The problem of model selection is considered for the setting of interpolating estimators, where the number of model parameters exceeds the size of the dataset. Classical information criteria typically consider the large-data limit, penalizing model size. However, these criteria are not appropriate in modern settings where overparameterized models tend to perform well. For any overparameterized model, we show that there exists a dual underparameterized model that possesses the same marginal likelihood, thus establishing a form of Bayesian duality. This enables more classical methods to be used in the overparameterized setting, revealing the Interpolating Information Criterion, a measure of model quality that naturally incorporates the choice of prior into the model selection. Our new information criterion accounts for prior misspecification, geometric and spectral properties of the model, and is numerically consistent with known empirical and theoretical behavior in this regime.
    
[^47]: CatBoost对比XGBoost和LightGBM：开发增强的零膨胀保险理赔数据预测模型

    CatBoost Versus XGBoost and LightGBM: Developing Enhanced Predictive Models for Zero-Inflated Insurance Claim Data. (arXiv:2307.07771v1 [cs.LG])

    [http://arxiv.org/abs/2307.07771](http://arxiv.org/abs/2307.07771)

    本文比较了CatBoost、XGBoost和LightGBM三种流行的梯度提升库在处理零膨胀保险理赔数据上的效果，并通过对两个不同数据集的分析，证明了CatBoost是最适合训练保险理赔数据和拟合精算频率模型的库。

    

    在财产和意外事故保险行业中，由于正向理赔数据具有高度右偏分布和过量的零值，构建理赔预测模型面临一些挑战。传统模型，如泊松或负二项广义线性模型(GLM)，经常在处理过量零值时遇到困难。为了应对这个问题，精算科学的研究人员已经采用了“零膨胀”模型，将传统计数模型和二元模型合并，以更有效地处理这些数据集。本文使用了提升算法来处理保险理赔数据，包括零膨胀的遥测数据，以构建理赔频率模型。我们评估和比较了三个流行的梯度提升库 - XGBoost、LightGBM和CatBoost，旨在确定最适合训练保险理赔数据和拟合精算频率模型的库。通过对两个不同数据集的严格分析，我们证明了CatBoost是最优选择。

    In the property and casualty insurance industry, some challenges are presented in constructing claim predictive models due to a highly right-skewed distribution of positive claims with excess zeros. Traditional models, such as Poisson or negative binomial Generalized Linear Models(GLMs), frequently struggle with inflated zeros. In response to this, researchers in actuarial science have employed ``zero-inflated" models that merge a traditional count model and a binary model to address these datasets more effectively. This paper uses boosting algorithms to process insurance claim data, including zero-inflated telematics data, in order to construct claim frequency models. We evaluated and compared three popular gradient boosting libraries - XGBoost, LightGBM, and CatBoost - with the aim of identifying the most suitable library for training insurance claim data and fitting actuarial frequency models. Through a rigorous analysis of two distinct datasets, we demonstrated that CatBoost is sup
    
[^48]: randomHAR：利用传感器选择和强化学习改进人体活动识别的集成深度学习方法

    randomHAR: Improving Ensemble Deep Learners for Human Activity Recognition with Sensor Selection and Reinforcement Learning. (arXiv:2307.07770v1 [cs.LG])

    [http://arxiv.org/abs/2307.07770](http://arxiv.org/abs/2307.07770)

    randomHAR是一种集成深度学习方法，利用传感器选择和强化学习优化了人体活动识别，提高了性能。

    

    深度学习已经证明是人体活动识别（HAR）领域中一种有效的方法，优于其他需要手动特征工程的架构。尽管近年来取得了一些进展，但HAR数据固有的挑战，如噪声数据、类内变异性和类间相似性仍然存在。为了解决这些挑战，我们提出了一种集成方法，称为randomHAR。randomHAR的主要思想是，在给定数据集上从随机选择的传感器数据上训练一系列具有相同架构的深度学习模型。此外，我们还使用强化学习算法训练一个Agent来确定在运行时预测中使用的最优子模型集。与现有的工作相比，该方法优化的是集成过程而不是组成模型的架构。为了评估该方法的性能，我们将其与两种HAR算法进行了比较，包括当前的最新技术，在六个HAR基准数据集上进行评估。

    Deep learning has proven to be an effective approach in the field of Human activity recognition (HAR), outperforming other architectures that require manual feature engineering. Despite recent advancements, challenges inherent to HAR data, such as noisy data, intra-class variability and inter-class similarity, remain. To address these challenges, we propose an ensemble method, called randomHAR. The general idea behind randomHAR is training a series of deep learning models with the same architecture on randomly selected sensor data from the given dataset. Besides, an agent is trained with the reinforcement learning algorithm to identify the optimal subset of the trained models that are utilized for runtime prediction. In contrast to existing work, this approach optimizes the ensemble process rather than the architecture of the constituent models. To assess the performance of the approach, we compare it against two HAR algorithms, including the current state of the art, on six HAR benchm
    
[^49]: 5G NSA加密数据流的实时流量分类方法及其物理信道记录

    Real-time Traffic Classification for 5G NSA Encrypted Data Flows With Physical Channel Records. (arXiv:2307.07756v1 [cs.LG])

    [http://arxiv.org/abs/2307.07756](http://arxiv.org/abs/2307.07756)

    本文研究了5G NSA加密数据流的实时流量分类方法，通过使用物理信道记录进行分析，并提出基于梯度提升算法的决策树分类方法。通过实时分析加密流量，可用于QoS管理和动态资源分配。

    

    对于第五代新无线电（5G-NR）移动网络流量的分类是电信领域的一个新兴话题。它可以用于服务质量（QoS）管理和动态资源分配。然而，传统方法如深度数据包检查（DPI）无法直接应用于加密数据流。因此，需要研究新的实时加密流量分类算法来处理动态传输。本研究通过物理信道记录，使用新的实时加密5G Non-Standalone（NSA）应用级流量分类方法。由于特征数目庞大，基于决策树的梯度提升算法是一种可行的分类方法。我们生成了一个带有多个应用流量的受噪声限制的5G NSA追踪数据集。我们开发了一个新的流程，将物理信道记录序列转换为数值向量。我们测试了一组机器学习模型，并提出了一种新的模型进行流量分类。

    The classification of fifth-generation New-Radio (5G-NR) mobile network traffic is an emerging topic in the field of telecommunications. It can be utilized for quality of service (QoS) management and dynamic resource allocation. However, traditional approaches such as Deep Packet Inspection (DPI) can not be directly applied to encrypted data flows. Therefore, new real-time encrypted traffic classification algorithms need to be investigated to handle dynamic transmission. In this study, we examine the real-time encrypted 5G Non-Standalone (NSA) application-level traffic classification using physical channel records. Due to the vastness of their features, decision-tree-based gradient boosting algorithms are a viable approach for classification. We generate a noise-limited 5G NSA trace dataset with traffic from multiple applications. We develop a new pipeline to convert sequences of physical channel records into numerical vectors. A set of machine learning models are tested, and we propos
    
[^50]: 学习神经网络中的表达性先验，提高推广和不确定性估计

    Learning Expressive Priors for Generalization and Uncertainty Estimation in Neural Networks. (arXiv:2307.07753v1 [cs.LG])

    [http://arxiv.org/abs/2307.07753](http://arxiv.org/abs/2307.07753)

    本文提出了一种用于神经网络的先验学习方法，通过利用可扩展和结构化的神经网络后验作为推广的信息先验，提高了神经网络的推广和不确定性估计能力。我们的方法在大规模上提供了表达性的概率表示，并产生了非空推广界限。我们的技术贡献是推导出可处理的目标函数，并提出了改进的推广界限计算方法。在经验上，我们证明了该方法在不确定性估计和推广方面的有效性。

    

    在这项工作中，我们提出了一种新的先验学习方法，用于提高深度神经网络中的推广和不确定性估计。关键思想是利用可扩展和结构化的神经网络后验作为具有推广保证的信息先验。我们学习到的先验在大规模上提供了表达性的概率表示，类似于在ImageNet上预训练模型的贝叶斯对应物，并进一步产生了非空推广界限。我们还将这个想法扩展到连续学习框架中，我们的先验的有利特性是可取的。主要的推动因素是我们的技术贡献：(1) Kronecker积求和的计算，(2) 推导和优化可处理的目标函数，从而导致改进的推广界限。在经验上，我们详尽地展示了该方法在不确定性估计和推广方面的有效性。

    In this work, we propose a novel prior learning method for advancing generalization and uncertainty estimation in deep neural networks. The key idea is to exploit scalable and structured posteriors of neural networks as informative priors with generalization guarantees. Our learned priors provide expressive probabilistic representations at large scale, like Bayesian counterparts of pre-trained models on ImageNet, and further produce non-vacuous generalization bounds. We also extend this idea to a continual learning framework, where the favorable properties of our priors are desirable. Major enablers are our technical contributions: (1) the sums-of-Kronecker-product computations, and (2) the derivations and optimizations of tractable objectives that lead to improved generalization bounds. Empirically, we exhaustively show the effectiveness of this method for uncertainty estimation and generalization.
    
[^51]: 关于使用迭代贝叶斯更新对局部差分隐私机制的效用提升的研究

    On the Utility Gain of Iterative Bayesian Update for Locally Differentially Private Mechanisms. (arXiv:2307.07744v1 [cs.CR])

    [http://arxiv.org/abs/2307.07744](http://arxiv.org/abs/2307.07744)

    本文研究了使用迭代贝叶斯更新对局部差分隐私机制进行数据估计时的效用提升，结果表明在不增加额外隐私成本的情况下，IBU可以提供比传统矩阵求逆更好的效果。

    

    本文研究了使用迭代贝叶斯更新（IBU）对使用局部差分隐私（LDP）机制模糊化的数据进行私有离散分布估计的效用提升。我们将IBU的性能与矩阵求逆（MI）进行比较，MI是一种标准的估计技术，用于七种用于一次数据收集的LDP机制和其他七种用于多次数据收集（例如RAPPOR）的LDP机制。为了扩大研究范围，我们还变化了效用度量标准、用户数n、域大小k和隐私参数ε，使用了合成数据和真实世界数据。我们的结果表明，IBU可以成为一种有用的后处理工具，可以在不增加额外隐私成本的情况下提高LDP机制的效用。例如，我们的实验表明，在高隐私情况下（即ε很小）IBU可以提供比MI更好的效用。我们的论文为实践者提供了一些见解。

    This paper investigates the utility gain of using Iterative Bayesian Update (IBU) for private discrete distribution estimation using data obfuscated with Locally Differentially Private (LDP) mechanisms. We compare the performance of IBU to Matrix Inversion (MI), a standard estimation technique, for seven LDP mechanisms designed for one-time data collection and for other seven LDP mechanisms designed for multiple data collections (e.g., RAPPOR). To broaden the scope of our study, we also varied the utility metric, the number of users n, the domain size k, and the privacy parameter {\epsilon}, using both synthetic and real-world data. Our results suggest that IBU can be a useful post-processing tool for improving the utility of LDP mechanisms in different scenarios without any additional privacy cost. For instance, our experiments show that IBU can provide better utility than MI, especially in high privacy regimes (i.e., when {\epsilon} is small). Our paper provides insights for practiti
    
[^52]: 基因调控网络中的负概率

    Negative probabilities in Gene Regulatory Networks. (arXiv:2307.07738v1 [q-bio.MN])

    [http://arxiv.org/abs/2307.07738](http://arxiv.org/abs/2307.07738)

    该论文提出了一种基于基因表达和相关性符号的方法，用于识别基因之间的符号不确定共表达。通过构建具有符号不确定贡献的概率转移矩阵，可以量化基因调控网络中各种连接的结构和重要性，并解释其对网络几何结构的影响。

    

    我们介绍了一种基于已知表达和给定相关性符号来识别基因之间存在的符号不确定共表达的自然框架。具体而言，我们给出了基因之间的亲和关系信息（即基因调控网络中的连接性）和它们促进/抑制共表达蛋白产生的知识，并寻求能够解释蛋白质平衡分布的速率。我们提议将它们的“促进 vs. 抑制”功能封装在一个符号不确定的概率转移矩阵中，该矩阵的行和为一，但是除此之外符号不确定。构建具有符号不确定贡献的相互作用网络的表示的目的是量化各种连接的结构和重要性，并解释这些连接如何影响网络的几何结构，突显调控机制的重要性。

    We introduce a natural framework to identify sign-indefinite co-expressions between genes based on the known expressions and given the sign of their respective correlations. Specifically, given information concerning the affinity among genes (i.e., connectivity in the gene regulatory network) and knowledge whether they promote/inhibit co-expression of the respective protein production, we seek rates that may explain the observed stationary distributions at the level of proteins. We propose to encapsulate their ``promotion vs.\ inhibition'' functionality in a sign-indefinite probability transition matrix--a matrix whose row-sums equal to one, but is otherwise sign indefinite. The purpose of constructing such a representation for the interaction network with sign-indefinite contributions in protein regulation, is to quantify the structure and significance of various links, and to explain how these may affect the geometry of the network, highlighting the significance of the regulatory fun
    
[^53]: 结构化支持向量机的接近线性时间算法

    A Nearly-Linear Time Algorithm for Structured Support Vector Machines. (arXiv:2307.07735v1 [math.OC])

    [http://arxiv.org/abs/2307.07735](http://arxiv.org/abs/2307.07735)

    这篇论文提出了针对结构化支持向量机的接近线性时间算法，解决了二次规划输入规模和解决时间的问题。

    

    二次规划是凸优化领域中的基本问题。许多实际任务可以表示为二次规划，例如支持向量机（SVM）。在深度学习方法盛行之前，线性SVM是过去三十年来最流行的机器学习工具之一。一般来说，一个二次规划的输入规模为Θ(n^2)（其中n是变量的数量），因此解决该问题需要Ω(n^2)的时间。然而，SVM产生的二次规划的输入规模为O(n)，这使得设计接近线性时间算法成为可能。两个重要的SVM类别是具有低秩核因式分解和低树宽规模的程序。低树宽凸优化在过去几年中引起了越来越多的关注（例如线性规划[Dong, Lee and Ye 2021]和半定规划[Gu and Song 2022]）。因此，一个重要的开放问题是是否存在接近线性时间算法。

    Quadratic programming is a fundamental problem in the field of convex optimization. Many practical tasks can be formulated as quadratic programming, for example, the support vector machine (SVM). Linear SVM is one of the most popular tools over the last three decades in machine learning before deep learning method dominating.  In general, a quadratic program has input size $\Theta(n^2)$ (where $n$ is the number of variables), thus takes $\Omega(n^2)$ time to solve. Nevertheless, quadratic programs coming from SVMs has input size $O(n)$, allowing the possibility of designing nearly-linear time algorithms. Two important classes of SVMs are programs admitting low-rank kernel factorizations and low-treewidth programs. Low-treewidth convex optimization has gained increasing interest in the past few years (e.g.~linear programming [Dong, Lee and Ye 2021] and semidefinite programming [Gu and Song 2022]). Therefore, an important open question is whether there exist nearly-linear time algorithms
    
[^54]: 迈向最优神经网络：样本拆分在超参数选择中的作用

    Towards Optimal Neural Networks: the Role of Sample Splitting in Hyperparameter Selection. (arXiv:2307.07726v1 [stat.ML])

    [http://arxiv.org/abs/2307.07726](http://arxiv.org/abs/2307.07726)

    本文通过揭示神经网络模型构建中的样本拆分方法的奥秘，构建了一个理论框架来解释神经网络的有效性。我们的研究结果表明，从样本拆分中得到的最优超参数可以使得神经网络模型最小化预测风险。

    

    当人工神经网络在各个领域展现出卓越的实践成功时，关于它们的理论特性，如逼近能力、统计性质和泛化性能等的研究也取得了显著进展。在本文中，我们通过揭示神经网络模型构建中一种常见实践背后的奥秘：样本拆分，构建了一个新颖的理论来理解神经网络的有效性。我们的理论证明，从样本拆分中得到的最优超参数可以使得神经网络模型渐进地最小化预测风险。我们在不同的应用场景和网络结构中进行了大量实验，实验结果证实了我们的理论的有效性。

    When artificial neural networks have demonstrated exceptional practical success in a variety of domains, investigations into their theoretical characteristics, such as their approximation power, statistical properties, and generalization performance, have made significant strides. In this paper, we construct a novel theory for understanding the effectiveness of neural networks by discovering the mystery underlying a common practice during neural network model construction: sample splitting. Our theory demonstrates that, the optimal hyperparameters derived from sample splitting can enable a neural network model that asymptotically minimizes the prediction risk. We conduct extensive experiments across different application scenarios and network architectures, and the results manifest our theory's effectiveness.
    
[^55]: 机器学习的视觉分析:数据视角综述

    Visual Analytics For Machine Learning: A Data Perspective Survey. (arXiv:2307.07712v1 [cs.LG])

    [http://arxiv.org/abs/2307.07712](http://arxiv.org/abs/2307.07712)

    这篇论文总结了过去十年来利用可视化解释机器学习模型的研究，并从数据视角总结了这些作品。论文将ML模型处理的常见数据分为五类，并突出介绍了擅长学习这些数据的ML模型。此外，论文总结了在不同阶段操作这些数据类型的六个任务，并对VIS4ML领域的研究热点进行了分析。

    

    在过去的十年中，出现了许多利用可视化来解释机器学习模型的工作。相应的研究主题VIS4ML正在快速发展。为了更好地组织这些作品并揭示VIS4ML的发展趋势，我们通过这项调查提供对这些作品的系统回顾。由于数据质量极大地影响了机器学习模型的性能，我们的调查专门从数据视角总结了VIS4ML的作品。首先，我们将ML模型处理的常见数据分为五类，解释了每类数据的独特特征，并突出显示擅长从中学习的ML模型。其次，从众多的VIS4ML作品中，我们总结出在ML流程的不同阶段操作这些数据类型（即以数据为中心的任务）的六个任务，以了解、诊断和改进ML模型。最后，通过研究143篇调查论文的分布，我们对VIS4ML研究的热点进行了分析。

    The past decade has witnessed a plethora of works that leverage the power of visualization (VIS) to interpret machine learning (ML) models. The corresponding research topic, VIS4ML, keeps growing at a fast pace. To better organize the enormous works and shed light on the developing trend of VIS4ML, we provide a systematic review of these works through this survey. Since data quality greatly impacts the performance of ML models, our survey focuses specifically on summarizing VIS4ML works from the data perspective. First, we categorize the common data handled by ML models into five types, explain the unique features of each type, and highlight the corresponding ML models that are good at learning from them. Second, from the large number of VIS4ML works, we tease out six tasks that operate on these types of data (i.e., data-centric tasks) at different stages of the ML pipeline to understand, diagnose, and refine ML models. Lastly, by studying the distribution of 143 surveyed papers across
    
[^56]: 基于矩阵分解的随机性识别：应用于黑洞数据

    Identification of Stochasticity by Matrix-decomposition: Applied on Black Hole Data. (arXiv:2307.07703v1 [cs.LG])

    [http://arxiv.org/abs/2307.07703](http://arxiv.org/abs/2307.07703)

    本文提出了一种基于矩阵分解的算法，用于识别随机性和非随机性的时间序列分类。该方法利用奇异值分解进行拓扑分析和主成分分析提取特征，通过支持向量机分类，成功应用于合成数据的实验。

    

    时间序列分类为随机（噪声样）或非随机（结构化）有助于理解多个领域中的基本动态。本研究提出了一个基于两足矩阵分解的算法，利用了两种互补的技术进行分类。在奇异值分解（SVD）分析部分，我们对包含时间信息的奇异向量进行拓扑分析（Betti数），得到SVD标签。同时，进行时间顺序无关的主成分分析（PCA），并计算所提出的PCA衍生特征。观察到这些特征从两个标签的合成时间序列中提取出来，将时间序列映射到线性可分的特征空间中。使用支持向量机（SVM）生成PCA标签。所提出的方法已应用于合成数据，包括41个白噪声、粉红噪声（随机性）的实验，生长率为4的Logistic映射和Lorentz系统（非随机性）。

    Timeseries classification as stochastic (noise-like) or non-stochastic (structured), helps understand the underlying dynamics, in several domains. Here we propose a two-legged matrix decomposition-based algorithm utilizing two complementary techniques for classification. In Singular Value Decomposition (SVD) based analysis leg, we perform topological analysis (Betti numbers) on singular vectors containing temporal information, leading to SVD-label. Parallely, temporal-ordering agnostic Principal Component Analysis (PCA) is performed, and the proposed PCA-derived features are computed. These features, extracted from synthetic timeseries of the two labels, are observed to map the timeseries to a linearly separable feature space. Support Vector Machine (SVM) is used to produce PCA-label. The proposed methods have been applied to synthetic data, comprising 41 realisations of white-noise, pink-noise (stochastic), Logistic-map at growth-rate 4 and Lorentz-system (non-stochastic), as proof-of
    
[^57]: NeurASP：将神经网络融入到Answer Set Programming 中

    NeurASP: Embracing Neural Networks into Answer Set Programming. (arXiv:2307.07700v1 [cs.AI])

    [http://arxiv.org/abs/2307.07700](http://arxiv.org/abs/2307.07700)

    NeurASP是将神经网络集成到Answer Set Programming中的简单且有效的方法，通过以概率分布的形式处理神经网络输出，NeurASP能够将子符号和符号计算相结合，并通过应用符号推理改进神经网络的感知结果，并且可以通过使用ASP规则训练神经网络，使其从显式复杂语义约束中学习。

    

    我们提出了NeurASP，它是对Answer Set Programs的简单扩展，通过融合神经网络。通过将神经网络输出视为Answer Set Programs中原子事实的概率分布，NeurASP提供了一种简单有效的将子符号和符号计算相结合的方法。我们展示了NeurASP如何在符号计算中利用预训练的神经网络，并通过应用Answer Set Programming中的符号推理来改进神经网络的感知结果。此外，NeurASP可以通过使用ASP规则进行训练来更好地训练神经网络，使其不仅从数据中学习隐式相关性，还从规则所表示的显式复杂语义约束中学习。

    We present NeurASP, a simple extension of answer set programs by embracing neural networks. By treating the neural network output as the probability distribution over atomic facts in answer set programs, NeurASP provides a simple and effective way to integrate sub-symbolic and symbolic computation. We demonstrate how NeurASP can make use of a pre-trained neural network in symbolic computation and how it can improve the neural network's perception result by applying symbolic reasoning in answer set programming. Also, NeurASP can be used to train a neural network better by training with ASP rules so that a neural network not only learns from implicit correlations from the data but also from the explicit complex semantic constraints expressed by the rules.
    
[^58]: 使用机器学习方法降低代数多重网格中的操作复杂性

    Reducing operator complexity in Algebraic Multigrid with Machine Learning Approaches. (arXiv:2307.07695v1 [math.NA])

    [http://arxiv.org/abs/2307.07695](http://arxiv.org/abs/2307.07695)

    本文提出了一种基于机器学习的方法，通过使用神经网络和平滑测试向量，降低代数多重网格中非Galerkin粗网格操作符的复杂性。该方法在保持整体AMG收敛性的同时，能够有效解决参数化偏微分方程问题。

    

    我们提出了一种基于数据驱动和机器学习的方法来计算代数多重网格（AMG）方法中的非Galerkin粗网格操作符，解决了增加操作复杂性的问题。根据AMG理论中关于光谱等效粗网格操作符的指导，我们开发了一种新的ML算法，利用神经网络（NN）结合来自多重网格特征值问题的平滑测试向量。该方法在降低粗网格操作符的复杂性的同时保持了解决参数化偏微分方程（PDE）问题的整体AMG收敛性。通过提供对各向异性旋转的Laplacian和线性弹性问题的数值实验来展示性能，并与现有方法进行比较，用于计算非Galerkin粗网格操作符。

    We propose a data-driven and machine-learning-based approach to compute non-Galerkin coarse-grid operators in algebraic multigrid (AMG) methods, addressing the well-known issue of increasing operator complexity. Guided by the AMG theory on spectrally equivalent coarse-grid operators, we have developed novel ML algorithms that utilize neural networks (NNs) combined with smooth test vectors from multigrid eigenvalue problems. The proposed method demonstrates promise in reducing the complexity of coarse-grid operators while maintaining overall AMG convergence for solving parametric partial differential equation (PDE) problems. Numerical experiments on anisotropic rotated Laplacian and linear elasticity problems are provided to showcase the performance and compare with existing methods for computing non-Galerkin coarse-grid operators.
    
[^59]: 创建一个支持OpenMP Fortran和C++代码相互翻译的数据集

    Creating a Dataset Supporting Translation Between OpenMP Fortran and C++ Code. (arXiv:2307.07686v1 [cs.SE])

    [http://arxiv.org/abs/2307.07686](http://arxiv.org/abs/2307.07686)

    本研究创建了一个数据集，用于训练机器学习模型在OpenMP Fortran和C++代码之间进行翻译。这个数据集通过精细的代码相似性测试确保了可靠性和适用性，并且能够显著提升大规模语言模型的翻译能力。

    

    在本研究中，我们提出了一个新颖的数据集，用于训练在OpenMP Fortran和C++代码之间进行翻译的机器学习模型。通过精细的代码相似性测试，我们确保了数据集的可靠性和适用性。我们使用定量（CodeBLEU）和定性（人工评估）方法评估了我们数据集的有效性。我们展示了这个数据集如何显著提高大规模语言模型的翻译能力，对于没有先前编码知识的模型，提高了5.1倍，对于具有一定编码熟悉度的模型，提高了9.9倍。我们的工作突显了这个数据集在高性能计算的代码翻译领域的潜力。

    In this study, we present a novel dataset for training machine learning models translating between OpenMP Fortran and C++ code. To ensure reliability and applicability, the dataset is initially refined using a meticulous code similarity test. The effectiveness of our dataset is assessed using both quantitative (CodeBLEU) and qualitative (human evaluation) methods. We demonstrate how this dataset can significantly improve the translation capabilities of large-scale language models, with improvements of \times 5.1 for models with no prior coding knowledge and \times 9.9 for models with some coding familiarity. Our work highlights the potential of this dataset to advance the field of code translation for high-performance computing.
    
[^60]: 通过Utopia标签分布逼近学习主观时间序列数据

    Learning Subjective Time-Series Data via Utopia Label Distribution Approximation. (arXiv:2307.07682v1 [cs.LG])

    [http://arxiv.org/abs/2307.07682](http://arxiv.org/abs/2307.07682)

    本研究提出了一种适用于时间序列数据的Utopia标签分布逼近（ULDA）方法，通过将训练标签分布与高斯核进行卷积来使其更接近真实但未知的标签分布，从而提高模型的公平性。

    

    最近，主观时间序列回归（STR）任务引起了越来越多的关注。然而，大多数现有方法忽略了STR数据中的标签分布偏差，导致了偏倚的模型。最新的研究表明，在年龄估计和深度估计等不平衡回归任务中，数据集的先验标签分布是均匀的。然而，我们观察到STR任务中训练集和测试集的标签分布很可能既不均匀也不相同。这一独特的特征需要新的方法来估计更合理的分布以训练公平的模型。在这项工作中，我们提出了适用于时间序列数据的Utopia标签分布逼近（ULDA）方法，使训练标签分布更接近真实世界但未知的（utopia）标签分布。这将增强模型的公平性。具体而言，ULDA首先将训练标签分布与高斯核进行卷积。卷积后，在每个时间步处，样本数量需要通过加权重新获得。

    Subjective time-series regression (STR) tasks have gained increasing attention recently. However, most existing methods overlook the label distribution bias in STR data, which results in biased models. Emerging studies on imbalanced regression tasks, such as age estimation and depth estimation, hypothesize that the prior label distribution of the dataset is uniform. However, we observe that the label distributions of training and test sets in STR tasks are likely to be neither uniform nor identical. This distinct feature calls for new approaches that estimate more reasonable distributions to train a fair model. In this work, we propose Utopia Label Distribution Approximation (ULDA) for time-series data, which makes the training label distribution closer to real-world but unknown (utopia) label distribution. This would enhance the model's fairness. Specifically, ULDA first convolves the training label distribution by a Gaussian kernel. After convolution, the required sample quantity at 
    
[^61]: 基于数据的机器学习在航空产品中的操作设计领域特征化研究

    Data-centric Operational Design Domain Characterization for Machine Learning-based Aeronautical Products. (arXiv:2307.07681v1 [cs.SE])

    [http://arxiv.org/abs/2307.07681](http://arxiv.org/abs/2307.07681)

    该论文对基于机器学习的航空产品的操作设计领域进行了研究，采用了基于数据的方法。通过明确定义ODD参数的维度和对数据进行分类，确定了其对系统层面的影响。示例中以飞机飞行包线为例说明了这些概念。

    

    我们首次对基于机器学习的航空产品的操作设计领域（ODD）进行了严格的特征化。与其他应用领域（如自动驾驶道路车辆）的ODD开发基于场景不同，我们的方法是基于数据的：我们提出了可以明确捕捉定义ODD的参数的维度，同时对ML-based应用在操作中遇到的数据进行分类，同时确定了它们对系统层面的相关性和影响。具体来说，我们讨论了这些数据类别如何有助于确定：驱动ML模型（MLM）设计所需的要求；对MLM和系统层级的潜在影响；可能需要的学习保证过程和系统架构考虑因素。我们通过飞机飞行包线的示例来说明这些概念。

    We give a first rigorous characterization of Operational Design Domains (ODDs) for Machine Learning (ML)-based aeronautical products. Unlike in other application sectors (such as self-driving road vehicles) where ODD development is scenario-based, our approach is data-centric: we propose the dimensions along which the parameters that define an ODD can be explicitly captured, together with a categorization of the data that ML-based applications can encounter in operation, whilst identifying their system-level relevance and impact. Specifically, we discuss how those data categories are useful to determine: the requirements necessary to drive the design of ML Models (MLMs); the potential effects on MLMs and higher levels of the system hierarchy; the learning assurance processes that may be needed, and system architectural considerations. We illustrate the underlying concepts with an example of an aircraft flight envelope.
    
[^62]: 匹配追踪的快速收敛速度

    Sharp Convergence Rates for Matching Pursuit. (arXiv:2307.07679v1 [stat.ML])

    [http://arxiv.org/abs/2307.07679](http://arxiv.org/abs/2307.07679)

    本文通过提升现有的下界来匹配最佳上界，对匹配追踪的性能进行了精确描述，并构造了一个最坏情况的字典来证明现有上界的无法改进。

    

    本文研究了匹配追踪的基本限制，即通过字典中的元素的稀疏线性组合来近似目标函数的纯贪婪算法。当目标函数包含在对应于字典的变化空间中时，许多令人印象深刻的研究在过去几十年中获得了匹配追踪的收敛速度的上界和下界，但它们并不匹配。本文的主要贡献是填补这一差距，并获得匹配追踪性能的精确描述。我们通过改进现有的下界以匹配最佳上界来实现这一目标。具体来说，我们构造了一个最坏情况的字典，证明了现有的上界不能改进。事实证明，与其他贪婪算法变体不同，收敛速度是次优的，并且由解某个非线性方程的解决方案决定。这使我们得出结论，任意程度的收缩都会改善匹配追踪效果。

    We study the fundamental limits of matching pursuit, or the pure greedy algorithm, for approximating a target function by a sparse linear combination of elements from a dictionary. When the target function is contained in the variation space corresponding to the dictionary, many impressive works over the past few decades have obtained upper and lower bounds on the convergence rate of matching pursuit, but they do not match. The main contribution of this paper is to close this gap and obtain a sharp characterization of the performance of matching pursuit. We accomplish this by improving the existing lower bounds to match the best upper bound. Specifically, we construct a worst case dictionary which proves that the existing upper bound cannot be improved. It turns out that, unlike other greedy algorithm variants, the converge rate is suboptimal and is determined by the solution to a certain non-linear equation. This enables us to conclude that any amount of shrinkage improves matching pu
    
[^63]: 关于多节点上下文赌博机制中Epoch-Greedy的鲁棒性

    On the Robustness of Epoch-Greedy in Multi-Agent Contextual Bandit Mechanisms. (arXiv:2307.07675v1 [cs.LG])

    [http://arxiv.org/abs/2307.07675](http://arxiv.org/abs/2307.07675)

    本研究展示了在多Agent上下文赌博机制中，最突出的上下文赌博算法$\epsilon$-greedy可以进行扩展，以解决同时存在的激励因素、上下文和损坏问题

    

    在像点击付费(Pay-Per-Click)拍卖这样的多臂赌博机制中进行高效学习通常涉及三个挑战：1)引导真实出价行为(激励因素)，2)在用户个性化上下文中使用个性化(上下文)，3)规避点击模式中的操纵(损坏行为)。过去文献中每个挑战都被独立研究过；激励因素已在一系列研究中得到解决，上下文问题已通过上下文赌博算法得到广泛解决，而损坏问题则通过最近的关于具有对抗性损坏的赌博机制工作进行讨论。由于这些挑战同时存在，重要的是了解每种方法在解决其他挑战时的鲁棒性，提供可以同时处理所有挑战的算法，并突出这种组合中的固有局限性。在这项工作中，我们展示了最突出的上下文赌博算法$\epsilon$-greedy可以进行扩展，以解决同时存在的激励因素、上下文和损坏问题。

    Efficient learning in multi-armed bandit mechanisms such as pay-per-click (PPC) auctions typically involves three challenges: 1) inducing truthful bidding behavior (incentives), 2) using personalization in the users (context), and 3) circumventing manipulations in click patterns (corruptions). Each of these challenges has been studied orthogonally in the literature; incentives have been addressed by a line of work on truthful multi-armed bandit mechanisms, context has been extensively tackled by contextual bandit algorithms, while corruptions have been discussed via a recent line of work on bandits with adversarial corruptions. Since these challenges co-exist, it is important to understand the robustness of each of these approaches in addressing the other challenges, provide algorithms that can handle all simultaneously, and highlight inherent limitations in this combination. In this work, we show that the most prominent contextual bandit algorithm, $\epsilon$-greedy can be extended to
    
[^64]: 在GFlowNets中使用回放缓冲区对模式发现的有效性的实证研究

    An Empirical Study of the Effectiveness of Using a Replay Buffer on Mode Discovery in GFlowNets. (arXiv:2307.07674v1 [cs.LG])

    [http://arxiv.org/abs/2307.07674](http://arxiv.org/abs/2307.07674)

    本文通过实证研究，探讨了在GFlowNets中使用回放缓冲区的有效性，评估了不同回放缓冲区采样技术对模式发现速度和质量的影响。

    

    强化学习（RL）算法旨在通过迭代采样动作从而学习如何最大化总期望回报$R（x）$来学习最优策略。GFlowNets是一类特殊的算法，通过学习一个近似于$R（x）$的概率采样策略，从离散集合中生成多样的候选样本$x$。与传统的RL算法相比，GFlowNets表现出更好的模式发现能力，对于药物发现和组合搜索等应用非常有用。然而，由于GFlowNets是一个相对较新的算法类别，许多在RL中有用的技术尚未与其关联起来。本文研究了在GFlowNets中利用回放缓冲区。我们通过实证研究了各种回放缓冲区采样技术的影响，评估了模式发现速度和发现的模式质量。在Hypergrid模拟环境和分子合成环境中的实验结果证明了我们的观察结论。

    Reinforcement Learning (RL) algorithms aim to learn an optimal policy by iteratively sampling actions to learn how to maximize the total expected return, $R(x)$. GFlowNets are a special class of algorithms designed to generate diverse candidates, $x$, from a discrete set, by learning a policy that approximates the proportional sampling of $R(x)$. GFlowNets exhibit improved mode discovery compared to conventional RL algorithms, which is very useful for applications such as drug discovery and combinatorial search. However, since GFlowNets are a relatively recent class of algorithms, many techniques which are useful in RL have not yet been associated with them. In this paper, we study the utilization of a replay buffer for GFlowNets. We explore empirically various replay buffer sampling techniques and assess the impact on the speed of mode discovery and the quality of the modes discovered. Our experimental results in the Hypergrid toy domain and a molecule synthesis environment demonstrat
    
[^65]: 在线多智能体强化学习的高效对抗攻击

    Efficient Adversarial Attacks on Online Multi-agent Reinforcement Learning. (arXiv:2307.07670v1 [cs.LG])

    [http://arxiv.org/abs/2307.07670](http://arxiv.org/abs/2307.07670)

    本研究探讨了对在线多智能体强化学习（MARL）的对抗攻击的影响，首先展示了单独进行动作污染和奖励污染攻击的局限性，然后引入了一种混合攻击策略，该策略可以高效地攻击MARL智能体，即使攻击者没有先验信息。

    

    由于多智能体强化学习（MARL）具有广泛的应用范围，了解对MARL模型的对抗攻击的影响对于安全应用该模型至关重要。出于这个目的，我们研究了对MARL的对抗攻击的影响。在考虑的设置中，存在一个外部攻击者，他可以在智能体接收到奖励之前修改奖励，或在环境接收到动作之前操纵动作。攻击者的目标是将每个智能体引导到目标策略，或在攻击者选择的某个特定奖励函数下最大化累积奖励，同时最小化对反馈和动作的操纵量。我们首先展示了只进行动作污染攻击和只进行奖励污染攻击的局限性。然后，我们介绍了一种同时进行动作污染和奖励污染的混合攻击策略。我们展示了这种混合攻击策略可以高效地攻击MARL智能体，即使攻击者没有先验信息。

    Due to the broad range of applications of multi-agent reinforcement learning (MARL), understanding the effects of adversarial attacks against MARL model is essential for the safe applications of this model. Motivated by this, we investigate the impact of adversarial attacks on MARL. In the considered setup, there is an exogenous attacker who is able to modify the rewards before the agents receive them or manipulate the actions before the environment receives them. The attacker aims to guide each agent into a target policy or maximize the cumulative rewards under some specific reward function chosen by the attacker, while minimizing the amount of manipulation on feedback and action. We first show the limitations of the action poisoning only attacks and the reward poisoning only attacks. We then introduce a mixed attack strategy with both the action poisoning and the reward poisoning. We show that the mixed attack strategy can efficiently attack MARL agents even if the attacker has no pr
    
[^66]: 具有概率策略执行不确定性的高效鲁棒增强学习

    Efficient Action Robust Reinforcement Learning with Probabilistic Policy Execution Uncertainty. (arXiv:2307.07666v1 [cs.LG])

    [http://arxiv.org/abs/2307.07666](http://arxiv.org/abs/2307.07666)

    本文研究了具有概率策略执行不确定性的行动鲁棒增强学习问题，并提出了ARRLC算法，该算法在遗憾和样本复杂度上达到了极小极大最优，实验证明其优于非鲁棒算法并且收敛更快。

    

    鲁棒增强学习旨在在不确定性面前找到优化最坏情况下性能的策略。本文关注具有概率策略执行不确定性的行动鲁棒增强学习，其中代理机器不总是按照策略指定的动作进行，而是以概率$1-\rho$执行策略指定的动作，以概率$\rho$执行替代的对抗动作。我们证明了具有概率策略执行不确定性的行动鲁棒马尔可夫决策过程存在最优策略，并提供了解决其的行动鲁棒贝尔曼最优方程。此外，我们开发了具有证书的行动鲁棒增强学习(ARRLC)算法，该算法实现了极小极大遗憾和样本复杂度最优。此外，我们进行了数值实验来验证我们的方法的鲁棒性，结果表明ARRLC优于非鲁棒增强学习算法，并且比鲁棒TD算法收敛更快。

    Robust reinforcement learning (RL) aims to find a policy that optimizes the worst-case performance in the face of uncertainties. In this paper, we focus on action robust RL with the probabilistic policy execution uncertainty, in which, instead of always carrying out the action specified by the policy, the agent will take the action specified by the policy with probability $1-\rho$ and an alternative adversarial action with probability $\rho$. We establish the existence of an optimal policy on the action robust MDPs with probabilistic policy execution uncertainty and provide the action robust Bellman optimality equation for its solution. Furthermore, we develop Action Robust Reinforcement Learning with Certificates (ARRLC) algorithm that achieves minimax optimal regret and sample complexity. Furthermore, we conduct numerical experiments to validate our approach's robustness, demonstrating that ARRLC outperforms non-robust RL algorithms and converges faster than the robust TD algorithm i
    
[^67]: 机器学习用于期权定价：对网络结构的实证研究

    Machine learning for option pricing: an empirical investigation of network architectures. (arXiv:2307.07657v1 [q-fin.CP])

    [http://arxiv.org/abs/2307.07657](http://arxiv.org/abs/2307.07657)

    广义高速公路网络结构在期权定价问题中的应用表现出更高的准确性和更短的训练时间。

    

    本文考虑了使用适当的输入数据（模型参数）和相应输出数据（期权价格或隐含波动率）来学习期权价格或隐含波动率的监督学习问题。大部分相关文献都使用（普通的）前馈神经网络结构来连接用于学习将输入映射到输出的神经元。在本文中，受到图像分类方法和用于偏微分方程机器学习方法的最新进展的启发，我们通过实证研究来探究网络结构的选择如何影响机器学习算法的精确度和训练时间。我们发现，在期权定价问题中，我们主要关注Black-Scholes和Heston模型，广义高速公路网络结构相较于其他变体在均方误差和训练时间方面表现更好。此外，在计算隐含波动率方面，

    We consider the supervised learning problem of learning the price of an option or the implied volatility given appropriate input data (model parameters) and corresponding output data (option prices or implied volatilities). The majority of articles in this literature considers a (plain) feed forward neural network architecture in order to connect the neurons used for learning the function mapping inputs to outputs. In this article, motivated by methods in image classification and recent advances in machine learning methods for PDEs, we investigate empirically whether and how the choice of network architecture affects the accuracy and training time of a machine learning algorithm. We find that for option pricing problems, where we focus on the Black--Scholes and the Heston model, the generalized highway network architecture outperforms all other variants, when considering the mean squared error and the training time as criteria. Moreover, for the computation of the implied volatility, a
    
[^68]: DIGEST: 快速和通信高效的分散学习与本地更新

    DIGEST: Fast and Communication Efficient Decentralized Learning with Local Updates. (arXiv:2307.07652v1 [cs.LG])

    [http://arxiv.org/abs/2307.07652](http://arxiv.org/abs/2307.07652)

    本文提出了一种名为DIGEST的快速和通信高效的异步分散学习机制，通过结合Gossip和随机游走的思想，并专注于随机梯度下降（SGD），实现了在分散学习中较低的通信成本和较快的收敛时间。

    

    两种广泛考虑的分散学习算法是Gossip和基于随机游走的学习。Gossip算法（同步和异步版本）存在较高的通信成本，而基于随机游走的学习则会增加收敛时间。在本文中，我们设计了一种快速和通信有效的异步分散学习机制DIGEST，利用了Gossip和随机游走的思想，并专注于随机梯度下降（SGD）。DIGEST是一个基于本地SGD算法的异步分散算法，它最初是为通信高效的集中式学习而设计的。我们设计了单流和多流的DIGEST，当流的数量增加时通信开销可能会增加，并且有一种收敛和通信开销的权衡可以利用。我们分析了单流和多流DIGEST的收敛性，并证明了两种算法都接近最优解。

    Two widely considered decentralized learning algorithms are Gossip and random walk-based learning. Gossip algorithms (both synchronous and asynchronous versions) suffer from high communication cost, while random-walk based learning experiences increased convergence time. In this paper, we design a fast and communication-efficient asynchronous decentralized learning mechanism DIGEST by taking advantage of both Gossip and random-walk ideas, and focusing on stochastic gradient descent (SGD). DIGEST is an asynchronous decentralized algorithm building on local-SGD algorithms, which are originally designed for communication efficient centralized learning. We design both single-stream and multi-stream DIGEST, where the communication overhead may increase when the number of streams increases, and there is a convergence and communication overhead trade-off which can be leveraged. We analyze the convergence of single- and multi-stream DIGEST, and prove that both algorithms approach to the optima
    
[^69]: SALC：基于骨架辅助的学习聚类用于时变室内定位

    SALC: Skeleton-Assisted Learning-Based Clustering for Time-Varying Indoor Localization. (arXiv:2307.07650v1 [cs.LG])

    [http://arxiv.org/abs/2307.07650](http://arxiv.org/abs/2307.07650)

    SALC是一种基于骨架辅助的学习聚类定位系统，可以适应时变室内环境，提高定位准确性。

    

    近年来，无线室内定位引起了相当大的关注。使用从WiFi访问点（AP）获取的接收信号强度（RSS）建立指纹数据库是室内定位中广泛使用的方法。然而，现有文献中对室内定位系统的时变问题研究不充分。与传统的静态指纹相比，动态重建的数据库可以适应高度变化的环境，从而实现定位准确性的可持续性。为了解决时变问题，我们提出了一种基于骨架辅助学习聚类定位（SALC）系统，包括基于RSS导向的地图辅助聚类（ROMAC）、基于聚类的在线数据库建立（CODE）和基于聚类的定位估计（CsLE）。SALC方案同时考虑了基于骨架最短路径（SSP）和参考点（RPs）之间的时变RSS测量的相似性。

    Wireless indoor localization has attracted significant amount of attention in recent years. Using received signal strength (RSS) obtained from WiFi access points (APs) for establishing fingerprinting database is a widely utilized method in indoor localization. However, the time-variant problem for indoor positioning systems is not well-investigated in existing literature. Compared to conventional static fingerprinting, the dynamicallyreconstructed database can adapt to a highly-changing environment, which achieves sustainability of localization accuracy. To deal with the time-varying issue, we propose a skeleton-assisted learning-based clustering localization (SALC) system, including RSS-oriented map-assisted clustering (ROMAC), cluster-based online database establishment (CODE), and cluster-scaled location estimation (CsLE). The SALC scheme jointly considers similarities from the skeleton-based shortest path (SSP) and the time-varying RSS measurements across the reference points (RPs)
    
[^70]: DistTGL：基于分布式内存的时间图神经网络训练

    DistTGL: Distributed Memory-Based Temporal Graph Neural Network Training. (arXiv:2307.07649v1 [cs.LG])

    [http://arxiv.org/abs/2307.07649](http://arxiv.org/abs/2307.07649)

    DistTGL是一种在分布式GPU集群上训练内存化TGNN的高效可扩展解决方案，相比现有方法在精度和训练吞吐量上都有显著提高。

    

    基于内存的时间图神经网络是动态图表示学习中强大的工具，在许多现实世界应用中已经展示出卓越的性能。然而，其节点内存更适合较小的批量大小以捕捉图事件中的更多依赖关系，并且需要在所有训练器之间同步维护。因此，当扩展到多个GPU时，现有框架会出现精度损失的问题。更糟糕的是，同步节点内存的巨大开销使得将其部署到分布式GPU集群变得不可行。在这项工作中，我们提出了DistTGL - 一种在分布式GPU集群上训练基于内存的TGNN的高效可扩展解决方案。DistTGL相比现有解决方案有三个改进：增强的TGNN模型，新颖的训练算法和优化的系统。在实验中，DistTGL实现了近线性的收敛加速，精度比最先进的单机方法提高了14.5％，训练吞吐量提高了10.17倍。

    Memory-based Temporal Graph Neural Networks are powerful tools in dynamic graph representation learning and have demonstrated superior performance in many real-world applications. However, their node memory favors smaller batch sizes to capture more dependencies in graph events and needs to be maintained synchronously across all trainers. As a result, existing frameworks suffer from accuracy loss when scaling to multiple GPUs. Evenworse, the tremendous overhead to synchronize the node memory make it impractical to be deployed to distributed GPU clusters. In this work, we propose DistTGL -- an efficient and scalable solution to train memory-based TGNNs on distributed GPU clusters. DistTGL has three improvements over existing solutions: an enhanced TGNN model, a novel training algorithm, and an optimized system. In experiments, DistTGL achieves near-linear convergence speedup, outperforming state-of-the-art single-machine method by 14.5% in accuracy and 10.17x in training throughput.
    
[^71]: 面向模型大小不可知、无需计算、基于记忆的深度学习推理

    Towards Model-Size Agnostic, Compute-Free, Memorization-based Inference of Deep Learning. (arXiv:2307.07631v1 [cs.LG])

    [http://arxiv.org/abs/2307.07631](http://arxiv.org/abs/2307.07631)

    本研究提出了一种基于记忆的推理方法（MBI），可以实现无需计算的深度学习推理。该方法利用了循环注意模型（RAM）的推理机制，并通过存储键值对的表来避免计算。

    

    深度神经网络的快速发展显著提高了各种任务的性能，如图像和语音识别。然而，随着这些模型的复杂性增加，计算成本和参数数量也增加，使得难以在资源受限设备上部署。本文提出了一种新颖的基于记忆的推理方法（MBI），它不需要计算，只需要查找。具体来说，我们的工作利用了循环注意模型（RAM）的推理机制，其中只有一个小窗口的输入域（glance）在一个时间步骤中进行处理，并且来自多个glance的输出通过隐藏向量组合来确定问题的整体分类输出。通过利用glance的低维性，我们的推理过程将由包含glance位置、补丁向量等的键值对存储在一张表中。在推理过程中，通过利用该表来读取键值对，可以避免计算。

    The rapid advancement of deep neural networks has significantly improved various tasks, such as image and speech recognition. However, as the complexity of these models increases, so does the computational cost and the number of parameters, making it difficult to deploy them on resource-constrained devices. This paper proposes a novel memorization-based inference (MBI) that is compute free and only requires lookups. Specifically, our work capitalizes on the inference mechanism of the recurrent attention model (RAM), where only a small window of input domain (glimpse) is processed in a one time step, and the outputs from multiple glimpses are combined through a hidden vector to determine the overall classification output of the problem. By leveraging the low-dimensionality of glimpse, our inference procedure stores key value pairs comprising of glimpse location, patch vector, etc. in a table. The computations are obviated during inference by utilizing the table to read out key-value pai
    
[^72]: 通过跨批次度量学习实现通用嵌入

    Generalizable Embeddings with Cross-batch Metric Learning. (arXiv:2307.07620v1 [cs.LG])

    [http://arxiv.org/abs/2307.07620](http://arxiv.org/abs/2307.07620)

    通过跨批次度量学习，我们提出了一种基于可学习原型的全局平均汇聚方法，用于学习通用实体以表示未见过的类别。

    

    全局平均汇聚（GAP）是深度度量学习（DML）中常用的组件，用于聚合特征。其有效性通常归因于将每个特征向量视为独立的语义实体，并将GAP视为它们的组合。尽管经过证实，但这种解释在学习可用于表示未见过的类别的通用实体的算法意义上仍不清楚，这是DML的关键目标。为了解决这个问题，我们将GAP定义为可学习原型的凸组合。然后，我们展示了原型学习可以被表达为将线性预测器拟合到一批样本的递归过程。基于这个观点，我们在每次迭代中考虑两个不相交类别的批次，并通过使用适应于另一个批次的原型来规范化学习。我们在4个热门DML基准上验证了我们的方法。

    Global average pooling (GAP) is a popular component in deep metric learning (DML) for aggregating features. Its effectiveness is often attributed to treating each feature vector as a distinct semantic entity and GAP as a combination of them. Albeit substantiated, such an explanation's algorithmic implications to learn generalizable entities to represent unseen classes, a crucial DML goal, remain unclear. To address this, we formulate GAP as a convex combination of learnable prototypes. We then show that the prototype learning can be expressed as a recursive process fitting a linear predictor to a batch of samples. Building on that perspective, we consider two batches of disjoint classes at each iteration and regularize the learning by expressing the samples of a batch with the prototypes that are fitted to the other batch. We validate our approach on 4 popular DML benchmarks.
    
[^73]: 用近端梯度下降有效地分解布尔矩阵

    Efficiently Factorizing Boolean Matrices using Proximal Gradient Descent. (arXiv:2307.07615v1 [cs.LG])

    [http://arxiv.org/abs/2307.07615](http://arxiv.org/abs/2307.07615)

    通过使用连续松弛和弹性二元正则化器，我们提出了一种用近端梯度下降有效地分解布尔矩阵的方法。我们在合成数据和实际数据上进行了广泛的实验，证明了方法的快速收敛和准确性，优于现有技术，结果易于解释和语义有意义。

    

    为了解决非负矩阵分解在布尔数据上的可解释性问题，布尔矩阵分解（BMF）使用布尔代数将输入分解为低秩布尔因子矩阵。这些矩阵具有很高的可解释性，在实践中非常有用，但需要解决一个NP难的组合优化问题，计算成本较高。为了减轻计算负担，我们提出了一种通过连续松弛BMF的新型弹性二元正则化器，从中推导出一种近端梯度算法。通过大量的实验，我们证明我们的方法在实践中表现良好：在合成数据上，我们展示了它快速收敛，精确恢复了真实值，并准确估计了模拟秩。在实际数据上，我们在召回率、损失和运行时间方面优于现有技术，并且来自医学领域的案例研究证实了我们的结果易于解释和语义有意义。

    Addressing the interpretability problem of NMF on Boolean data, Boolean Matrix Factorization (BMF) uses Boolean algebra to decompose the input into low-rank Boolean factor matrices. These matrices are highly interpretable and very useful in practice, but they come at the high computational cost of solving an NP-hard combinatorial optimization problem. To reduce the computational burden, we propose to relax BMF continuously using a novel elastic-binary regularizer, from which we derive a proximal gradient algorithm. Through an extensive set of experiments, we demonstrate that our method works well in practice: On synthetic data, we show that it converges quickly, recovers the ground truth precisely, and estimates the simulated rank exactly. On real-world data, we improve upon the state of the art in recall, loss, and runtime, and a case study from the medical domain confirms that our results are easily interpretable and semantically meaningful.
    
[^74]: 实现对讨论论坛帖子紧急性的通用检测

    Towards Generalizable Detection of Urgency of Discussion Forum Posts. (arXiv:2307.07614v1 [cs.LG])

    [http://arxiv.org/abs/2307.07614](http://arxiv.org/abs/2307.07614)

    本研究旨在解决学生在线课程讨论论坛问题的规模扩展难题。通过构建预测模型，自动确定论坛帖子的紧急程度，并提供给教师注意。与之前的工作不同，本研究通过预测7分制紧急程度级别，实现了更细粒度的预测。通过在大规模数据集上的训练和测试，证明了模型的通用性。

    

    参与在线课程（如MOOC）的学生会使用课程的讨论论坛，在遇到问题时向教师提问或寻求帮助。然而，由于需要考虑每条消息的时间，阅读和回复学生的问题难以扩展。结果，一些关键问题可能得不到解决，学生可能失去继续学习的动力。为了解决这个问题，我们建立了能够自动确定每个论坛帖子紧急性的预测模型，以便可以将这些帖子带给教师的注意。本文通过预测不仅二进制决策切割点，而且还预测了帖子在7分制紧急程度上的级别，从而超越了先前的工作。首先，我们在宾夕法尼亚大学的MOOCs的3503条原始数据集上训练和交叉验证了多个模型。其次，为了确定我们模型的通用性，我们在一个29,604条帖子的先前发表的数据集上测试了它们的性能。

    Students who take an online course, such as a MOOC, use the course's discussion forum to ask questions or reach out to instructors when encountering an issue. However, reading and responding to students' questions is difficult to scale because of the time needed to consider each message. As a result, critical issues may be left unresolved, and students may lose the motivation to continue in the course. To help address this problem, we build predictive models that automatically determine the urgency of each forum post, so that these posts can be brought to instructors' attention. This paper goes beyond previous work by predicting not just a binary decision cut-off but a post's level of urgency on a 7-point scale. First, we train and cross-validate several models on an original data set of 3,503 posts from MOOCs at University of Pennsylvania. Second, to determine the generalizability of our models, we test their performance on a separate, previously published data set of 29,604 posts fro
    
[^75]: 用于仿射约束组合非凸非光滑问题的一阶方法：更低的复杂度下界和接近最优的方法。

    First-order Methods for Affinely Constrained Composite Non-convex Non-smooth Problems: Lower Complexity Bound and Near-optimal Methods. (arXiv:2307.07605v1 [math.OC])

    [http://arxiv.org/abs/2307.07605](http://arxiv.org/abs/2307.07605)

    这项研究介绍了一种用于解决复合非凸非光滑优化问题的一阶方法，并建立了其最低复杂度下界以及提出了一种近似的IPG方法。

    

    最近许多关于一阶方法(FOMs)的研究都集中在具有线性和/或非线性函数约束的复合非凸非光滑优化上。已经为这些方法建立了上限(或最坏情况)复杂度的界限。然而，除了一些特殊的光滑非凸情况外，它们的最优性几乎不能得到保证，因为只有很少的下界是已知的。在本文中，我们首次尝试建立用于解决一类具有线性约束的复合非凸非光滑优化的FOMs的复杂度下界。假设两个不同的一阶预言机，我们建立了FOMs的复杂度下界，以产生一个问题（及其改写）的(接近)$\epsilon$-稳定点，其中考虑问题类别的任何给定容差$\epsilon>0$。此外，我们使用两个预测机中更宽松的一个提出了一种不精确的近端梯度(IPG)方法。该IPG的预言机复杂度，用于找到一个(接近)。epsilon-稳定点。

    Many recent studies on first-order methods (FOMs) focus on \emph{composite non-convex non-smooth} optimization with linear and/or nonlinear function constraints. Upper (or worst-case) complexity bounds have been established for these methods. However, little can be claimed about their optimality as no lower bound is known, except for a few special \emph{smooth non-convex} cases. In this paper, we make the first attempt to establish lower complexity bounds of FOMs for solving a class of composite non-convex non-smooth optimization with linear constraints. Assuming two different first-order oracles, we establish lower complexity bounds of FOMs to produce a (near) $\epsilon$-stationary point of a problem (and its reformulation) in the considered problem class, for any given tolerance $\epsilon>0$. In addition, we present an inexact proximal gradient (IPG) method by using the more relaxed one of the two assumed first-order oracles. The oracle complexity of the proposed IPG, to find a (near
    
[^76]: 通过填充和置换指纹编码的方法，对差分隐私算法提供平滑下界

    Smooth Lower Bounds for Differentially Private Algorithms via Padding-and-Permuting Fingerprinting Codes. (arXiv:2307.07604v1 [cs.CR])

    [http://arxiv.org/abs/2307.07604](http://arxiv.org/abs/2307.07604)

    本论文提出了一种通过填充和置换指纹编码的方法来产生困难实例，从而在各种情景下提供平滑下界。这方法适用于差分隐私平均问题和近似k.

    

    指纹编码方法是最广泛用于确定约束差分隐私算法的样本复杂度或错误率的方法。然而，对于许多差分隐私问题，我们并不知道适当的下界，并且即使对于我们知道的问题，下界也不平滑，并且通常在误差大于某个阈值时变得无意义。在这项工作中，我们通过将填充和置换转换应用于指纹编码，提出了一种生成困难实例的简单方法。我们通过在不同情景下提供新的下界来说明这种方法的适用性：1. 低准确度情景下差分隐私平均问题的紧密下界，这尤其意味着新的私有1簇问题的下界 2. 近似k

    Fingerprinting arguments, first introduced by Bun, Ullman, and Vadhan (STOC 2014), are the most widely used method for establishing lower bounds on the sample complexity or error of approximately differentially private (DP) algorithms. Still, there are many problems in differential privacy for which we don't know suitable lower bounds, and even for problems that we do, the lower bounds are not smooth, and usually become vacuous when the error is larger than some threshold.  In this work, we present a simple method to generate hard instances by applying a padding-and-permuting transformation to a fingerprinting code. We illustrate the applicability of this method by providing new lower bounds in various settings:  1. A tight lower bound for DP averaging in the low-accuracy regime, which in particular implies a new lower bound for the private 1-cluster problem introduced by Nissim, Stemmer, and Vadhan (PODS 2016).  2. A lower bound on the additive error of DP algorithms for approximate k
    
[^77]: 用能量差异训练离散能量模型

    Training Discrete Energy-Based Models with Energy Discrepancy. (arXiv:2307.07595v1 [stat.ML])

    [http://arxiv.org/abs/2307.07595](http://arxiv.org/abs/2307.07595)

    该论文提出了一种用能量差异训练离散能量模型的方法，该方法不依赖于采样策略，通过评估数据点及其扰动对应点的能量函数来实现，能够为各种扰动过程提供理论保证，并在不同数据集上展示了其相对性能。

    

    在离散空间上训练能量模型（EBMs）充满挑战，因为对这样的空间进行采样可能很困难。我们提出使用能量差异（ED）来训练离散EBMs，这是一种新型的对比损失函数，只需要评估数据点及其扰动对应点的能量函数，因此不依赖于像马尔可夫链蒙特卡洛（MCMC）这样的采样策略。能量差异为一类广泛的扰动过程提供了理论保证，我们研究了三种类型的扰动：基于伯努利噪声的扰动，基于确定性变换的扰动，以及基于邻域结构的扰动。我们在晶格伊辛模型、二值合成数据和离散图像数据集上展示了它们的相对性能。

    Training energy-based models (EBMs) on discrete spaces is challenging because sampling over such spaces can be difficult. We propose to train discrete EBMs with energy discrepancy (ED), a novel type of contrastive loss functional which only requires the evaluation of the energy function at data points and their perturbed counter parts, thus not relying on sampling strategies like Markov chain Monte Carlo (MCMC). Energy discrepancy offers theoretical guarantees for a broad class of perturbation processes of which we investigate three types: perturbations based on Bernoulli noise, based on deterministic transforms, and based on neighbourhood structures. We demonstrate their relative performance on lattice Ising models, binary synthetic data, and discrete image data sets.
    
[^78]: 一个量化的方法来预测神经网络中的表示学习和性能

    A Quantitative Approach to Predicting Representational Learning and Performance in Neural Networks. (arXiv:2307.07575v1 [cs.LG])

    [http://arxiv.org/abs/2307.07575](http://arxiv.org/abs/2307.07575)

    本文介绍了一种使用伪核的工具，可以分析和预测神经网络中学习到的表示。该方法可以预测权重初始化和训练计划对表示学习和并发多任务性能的影响。

    

    神经网络（包括生物和人工）的一个关键特性是它们如何学习来表示和处理输入信息以解决任务。不同类型的表示可能适合不同类型的任务，因此识别和理解学习到的表示是理解和设计有用网络的关键部分。本文介绍了一种基于伪核的工具，仅基于网络的初始状态和训练计划来分析和预测学习到的表示。我们在简单测试案例上验证了该方法，然后在关于表示学习对顺序单任务与并发多任务性能影响的问题上展示了其使用。我们展示了我们的方法可以用来预测权重初始化的规模和训练计划对表示学习和下游并发多任务性能的影响。

    A key property of neural networks (both biological and artificial) is how they learn to represent and manipulate input information in order to solve a task. Different types of representations may be suited to different types of tasks, making identifying and understanding learned representations a critical part of understanding and designing useful networks. In this paper, we introduce a new pseudo-kernel based tool for analyzing and predicting learned representations, based only on the initial conditions of the network and the training curriculum. We validate the method on a simple test case, before demonstrating its use on a question about the effects of representational learning on sequential single versus concurrent multitask performance. We show that our method can be used to predict the effects of the scale of weight initialization and training curriculum on representational learning and downstream concurrent multitasking performance.
    
[^79]: Harpa: 高速率下的相位关联与走时神经场

    Harpa: High-Rate Phase Association with Travel Time Neural Fields. (arXiv:2307.07572v1 [physics.geo-ph])

    [http://arxiv.org/abs/2307.07572](http://arxiv.org/abs/2307.07572)

    本论文提出了一种名为Harpa的高速率地震相位关联方法，利用深度神经场构建波速和相关走时的生成模型，即使在波速未知的情况下也能实现相位关联。这种方法能够处理较小、高速率的地震事件，提供了关于地下弹性介质属性的宝贵描述。

    

    相位关联是根据其起源地震分组地震波到达的任务。它是地震数据处理流程中的基本任务，但对于较小、高速率的地震事件来说是具有挑战性的，这些事件携带有关地震动力学的基本信息，尤其是在常常假定不准确的波速模型下。因此，大多数关联方法都专注于发生率较低且容易关联的较大事件，尽管微地震活动提供了井下弹性介质属性的宝贵描述。在本文中，我们展示了即使在波速未知的情况下，也可以以比以前报告的更高的速率进行关联。我们提出了Harpa，这是一种高速率地震相位关联方法，利用深度神经场构建波速和相关走时的生成模型，并首先解决联合时空源定位和波速恢复问题，然后进行相位关联。

    Phase association groups seismic wave arrivals according to their originating earthquakes. It is a fundamental task in a seismic data processing pipeline, but challenging to perform for smaller, high-rate seismic events which carry fundamental information about earthquake dynamics, especially with a commonly assumed inaccurate wave speed model. As a consequence, most association methods focus on larger events that occur at a lower rate and are thus easier to associate, even though microseismicity provides a valuable description of the elastic medium properties in the subsurface. In this paper, we show that association is possible at rates much higher than previously reported even when the wave speed is unknown. We propose Harpa, a high-rate seismic phase association method which leverages deep neural fields to build generative models of wave speeds and associated travel times, and first solves a joint spatio--temporal source localization and wave speed recovery problem, followed by ass
    
[^80]: 变分预测

    Variational Prediction. (arXiv:2307.07568v1 [cs.LG])

    [http://arxiv.org/abs/2307.07568](http://arxiv.org/abs/2307.07568)

    本文介绍了变分预测这一技术，通过使用变分界直接学习后验预测分布，避免了边缘化成本，并展示了在玩具例子上的实验结果。

    

    贝叶斯推断相比最大似然具有优势，但也伴随着计算成本。计算后验通常是难以处理的，而将后验边缘化形成后验预测分布也是如此。在本文中，我们提出了变分预测，一种使用变分界直接学习后验预测分布的技术。这种方法可以在没有测试时间边缘化成本的情况下提供良好的预测分布。我们在一个说明性的玩具例子上演示了变分预测。

    Bayesian inference offers benefits over maximum likelihood, but it also comes with computational costs. Computing the posterior is typically intractable, as is marginalizing that posterior to form the posterior predictive distribution. In this paper, we present variational prediction, a technique for directly learning a variational approximation to the posterior predictive distribution using a variational bound. This approach can provide good predictive distributions without test time marginalization costs. We demonstrate Variational Prediction on an illustrative toy example.
    
[^81]: 使用长短期记忆网络从右到左和头到脚的成分重建三轴心搏图

    Reconstruction of 3-Axis Seismocardiogram from Right-to-left and Head-to-foot Components Using A Long Short-Term Memory Network. (arXiv:2307.07566v1 [physics.med-ph])

    [http://arxiv.org/abs/2307.07566](http://arxiv.org/abs/2307.07566)

    本研究开发了一个深度学习模型，可从右到左和头到脚的心搏图成分中预测背腹方向的信号。使用长短期记忆网络进行训练和验证，模型表现良好，均方误差为0.09。

    

    本研究旨在开发一个深度学习模型，以预测从右到左和头到脚方向的心搏图（SCG）信号在背腹方向的信号（SCG_x和SCG_y）。用于训练和验证模型的数据集来自15名健康成年人。使用三轴加速度计将SCG信号记录在每个被试的胸部上。然后使用心电图R波进行分段，将片段进行降采样、归一化和零居中处理。得到的数据集被用来训练和验证一个具有两层和一个避免过拟合的dropout层的长短期记忆（LSTM）网络。网络以100个时间步长的SCG_x和SCG_y作为输入，表示一个心脏周期，输出映射到目标变量的向量。结果显示，LSTM模型的均方误差为0.09。

    This pilot study aims to develop a deep learning model for predicting seismocardiogram (SCG) signals in the dorsoventral direction from the SCG signals in the right-to-left and head-to-foot directions ($\textrm{SCG}_x$ and $\textrm{SCG}_y$). The dataset used for the training and validation of the model was obtained from 15 healthy adult subjects. The SCG signals were recorded using tri-axial accelerometers placed on the chest of each subject. The signals were then segmented using electrocardiogram R waves, and the segments were downsampled, normalized, and centered around zero. The resulting dataset was used to train and validate a long short-term memory (LSTM) network with two layers and a dropout layer to prevent overfitting. The network took as input 100-time steps of $\textrm{SCG}_x$ and $\textrm{SCG}_y$, representing one cardiac cycle, and outputted a vector that mapped to the target variable being predicted. The results showed that the LSTM model had a mean square error of 0.09 b
    
[^82]: 无源领域适应与时间插补的时序数据

    Source-Free Domain Adaptation with Temporal Imputation for Time Series Data. (arXiv:2307.07542v1 [eess.SP])

    [http://arxiv.org/abs/2307.07542](http://arxiv.org/abs/2307.07542)

    本文提出了一种用于时序数据的无源领域适应方法MAPU，通过随机掩蔽和时间插补的方式，捕捉源领域的时间信息并引导目标模型产生目标结果。

    

    无源领域适应（SFDA）旨在在没有访问源领域数据的情况下从已标记的源领域自适应模型到未标记的目标领域，保持源领域的隐私。尽管在视觉应用中很常见，但是在时序应用中，SFDA还很少被研究。现有的主要设计用于视觉应用的SFDA方法可能无法处理时序数据中的时间动态，从而导致自适应性能受损。为了解决这个问题，本文提出了一种简单而有效的无源领域适应的时序数据方法，即MAsk and imPUte (MAPU)。首先，为了捕捉源领域的时间信息，我们的方法对时序信号进行随机掩蔽，同时利用一种新颖的时间插补器在嵌入空间中从掩蔽版本中恢复原始信号。其次，在适应步骤中，插补器网络被利用来引导目标模型产生目标结果。

    Source-free domain adaptation (SFDA) aims to adapt a pretrained model from a labeled source domain to an unlabeled target domain without access to the source domain data, preserving source domain privacy. Despite its prevalence in visual applications, SFDA is largely unexplored in time series applications. The existing SFDA methods that are mainly designed for visual applications may fail to handle the temporal dynamics in time series, leading to impaired adaptation performance. To address this challenge, this paper presents a simple yet effective approach for source-free domain adaptation on time series data, namely MAsk and imPUte (MAPU). First, to capture temporal information of the source domain, our method performs random masking on the time series signals while leveraging a novel temporal imputer to recover the original signal from a masked version in the embedding space. Second, in the adaptation step, the imputer network is leveraged to guide the target model to produce target 
    
[^83]: 在希尔伯特空间中改进自标准化浓度：对GP-UCB算法的次线性遗憾

    Improved Self-Normalized Concentration in Hilbert Spaces: Sublinear Regret for GP-UCB. (arXiv:2307.07539v1 [cs.LG])

    [http://arxiv.org/abs/2307.07539](http://arxiv.org/abs/2307.07539)

    本文提出了对GP-UCB算法进行改进，使其具有几乎最优的次线性遗憾，并解决了关于遗憾分析的开放问题。

    

    在核化赌博机问题中，学习器旨在通过仅在顺序选择的点处进行噪声评估，顺序计算位于再生核希尔伯特空间中的函数的最优解。特别地，学习器旨在最小化遗憾，遗憾是所做选择的次优性度量。可以说最受欢迎的算法是高斯过程上界置信区间（GP-UCB）算法，它涉及根据未知函数的简单线性估计器进行行动。尽管它很受欢迎，但现有的GP-UCB遗憾分析给出了次优遗憾率，对于许多常用的内核（如Matérn内核）而言，遗憾率并不次线性。这引发了一个长期存在的问题：现有的GP-UCB遗憾分析是否紧密，或者是否可以通过使用更复杂的分析技术改进界限？在这项工作中，我们解决了这个开放问题，并证明了GP-UCB具有几乎最优的遗憾。特别地，我们的结果直接暗示了次线性遗憾率。

    In the kernelized bandit problem, a learner aims to sequentially compute the optimum of a function lying in a reproducing kernel Hilbert space given only noisy evaluations at sequentially chosen points. In particular, the learner aims to minimize regret, which is a measure of the suboptimality of the choices made. Arguably the most popular algorithm is the Gaussian Process Upper Confidence Bound (GP-UCB) algorithm, which involves acting based on a simple linear estimator of the unknown function. Despite its popularity, existing analyses of GP-UCB give a suboptimal regret rate, which fails to be sublinear for many commonly used kernels such as the Mat\'ern kernel. This has led to a longstanding open question: are existing regret analyses for GP-UCB tight, or can bounds be improved by using more sophisticated analytical techniques? In this work, we resolve this open question and show that GP-UCB enjoys nearly optimal regret. In particular, our results directly imply sublinear regret rate
    
[^84]: 在有向无环图约束下学习多个协调代理

    Learning Multiple Coordinated Agents under Directed Acyclic Graph Constraints. (arXiv:2307.07529v1 [cs.LG])

    [http://arxiv.org/abs/2307.07529](http://arxiv.org/abs/2307.07529)

    本文提出了一种在有向无环图约束下学习多个协调代理的新方法，通过利用DAG结构，提高了学习性能，并在实际环境中的多个任务上取得了优于其他非DAG方法的结果。

    

    本文提出了一种新颖的多智能体强化学习（MARL）方法，用于在有向无环图（DAG）约束下学习多个协调代理。与现有的MARL方法不同的是，我们的方法明确利用了代理之间的DAG结构，以达到更有效的学习性能。在理论上，我们提出了一种基于合成奖励的MARL模型的新型代理值函数，并证明它作为最优值函数的下界。在计算上，我们提出了一种实际的训练算法，利用领导者代理和奖励生成和分发代理的新概念，引导分解的从属代理在具有DAG约束的环境中更好地探索参数空间。在实证上，我们利用四个DAG环境，包括Intel高产量打包和测试工厂的实际调度，对我们的方法进行了基准测试，并证明它优于其他非DAG方法。

    This paper proposes a novel multi-agent reinforcement learning (MARL) method to learn multiple coordinated agents under directed acyclic graph (DAG) constraints. Unlike existing MARL approaches, our method explicitly exploits the DAG structure between agents to achieve more effective learning performance. Theoretically, we propose a novel surrogate value function based on a MARL model with synthetic rewards (MARLM-SR) and prove that it serves as a lower bound of the optimal value function. Computationally, we propose a practical training algorithm that exploits new notion of leader agent and reward generator and distributor agent to guide the decomposed follower agents to better explore the parameter space in environments with DAG constraints. Empirically, we exploit four DAG environments including a real-world scheduling for one of Intel's high volume packaging and test factory to benchmark our methods and show it outperforms the other non-DAG approaches.
    
[^85]: 自主车辆轨迹预测的机器学习: 一项综合研究、挑战和未来研究方向调查

    Machine Learning for Autonomous Vehicle's Trajectory Prediction: A comprehensive survey, Challenges, and Future Research Directions. (arXiv:2307.07527v1 [cs.LG])

    [http://arxiv.org/abs/2307.07527](http://arxiv.org/abs/2307.07527)

    这项综合研究调查了自主车辆的轨迹预测方法，通过借鉴现有文献并重点关注机器学习技术，特别是深度学习和强化学习。研究总结了目前的挑战，并提出了未来研究的方向。

    

    自主车辆 (AVs) 通过代替人工驾驶员而采用先进的计算机辅助决策系统，成为一种有前景的解决方案。然而，为了能够有效地驾驶道路，AVs 必须具备类似人类驾驶员的预测驾驶能力，即预测周围交通参与者的未来行为。在自动驾驶的背景下，借鉴现有文献对轨迹预测方法进行综合评估是推进该领域和发展全面理解的关键。为了满足这一需求，我们进行了一项综合调查，重点关注AVs的轨迹预测方法，特别是深度学习和强化学习为基础的机器学习技术。我们广泛研究了与AVs轨迹预测相关的两百余项研究。本文首先介绍了车辆轨迹预测的一般问题，并提供了一系列方法来解决这个问题。接着我们讨论了目前存在的挑战，并提出了未来研究的方向。

    Autonomous Vehicles (AVs) have emerged as a promising solution by replacing human drivers with advanced computer-aided decision-making systems. However, for AVs to effectively navigate the road, they must possess the capability to predict the future behavior of nearby traffic participants, similar to the predictive driving abilities of human drivers. Building upon existing literature is crucial to advance the field and develop a comprehensive understanding of trajectory prediction methods in the context of automated driving. To address this need, we have undertaken a comprehensive review that focuses on trajectory prediction methods for AVs, with a particular emphasis on machine learning techniques including deep learning and reinforcement learning-based approaches. We have extensively examined over two hundred studies related to trajectory prediction in the context of AVs. The paper begins with an introduction to the general problem of predicting vehicle trajectories and provides an o
    
[^86]: 由生成闭环人工智能引领的基础科学的未来

    The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence. (arXiv:2307.07522v1 [cs.AI])

    [http://arxiv.org/abs/2307.07522](http://arxiv.org/abs/2307.07522)

    生成型人工智能和大型语言模型可能为基础科学的发现提供机会，通过其自主生成假设和探索假设空间的闭环方法，加速科学发现的进程。

    

    机器学习和人工智能的最新进展，包括生成型人工智能和大型语言模型，正在颠覆技术创新、产品开发和整个社会。人工智能对技术的贡献可以通过多种途径实现，需要大量训练数据集和明确的性能评估标准，范围从模式识别和分类到生成模型。然而，由于科学实践和模型发现需要访问高质量的大型数据集，人工智能对基础科学的贡献较少。生成型人工智能，特别是大型语言模型，可能代表了通过定量模型增强和加速基础深度科学的科学发现的机会。在这里，我们探索和研究了一种由人工智能驱动、自动化的闭环科学发现方法的各个方面，包括自主生成假设和开放式自主探索假设空间。

    Recent advances in machine learning and AI, including Generative AI and LLMs, are disrupting technological innovation, product development, and society as a whole. AI's contribution to technology can come from multiple approaches that require access to large training data sets and clear performance evaluation criteria, ranging from pattern recognition and classification to generative models. Yet, AI has contributed less to fundamental science in part because large data sets of high-quality data for scientific practice and model discovery are more difficult to access. Generative AI, in general, and Large Language Models in particular, may represent an opportunity to augment and accelerate the scientific discovery of fundamental deep science with quantitative models. Here we explore and investigate aspects of an AI-driven, automated, closed-loop approach to scientific discovery, including self-driven hypothesis generation and open-ended autonomous exploration of the hypothesis space. Int
    
[^87]: 基于投票的多模态自动欺骗检测

    Voting-based Multimodal Automatic Deception Detection. (arXiv:2307.07516v1 [cs.LG])

    [http://arxiv.org/abs/2307.07516](http://arxiv.org/abs/2307.07516)

    本文提出了一种基于投票的多模态方法用于自动欺骗检测，通过视频的音频、视觉和文本特征进行检测。实验结果表明，我们的解决方案在欺骗检测中表现优于现有技术。

    

    自动欺骗检测一直是一个热门的研究课题，利用机器学习和深度学习自动检测欺骗给这一旧领域带来了新的光明。在本文中，我们提出了一种基于投票的方法，用于从视频中使用音频、视觉和文本特征进行自动欺骗检测。我们在两个数据集上进行了实验，分别是密歇根大学的真实试验数据集和迈阿密大学的欺骗检测数据集。视频样本被分成图像、音频和手稿的帧。我们提出的多模态投票解决方案包括三个模型。第一个模型是用于从图像中检测欺骗的卷积神经网络（CNN），第二个模型是用于从音频中检测欺骗的Mel频谱图上的支持向量机（SVM），第三个模型是用于从手稿中检测欺骗的支持向量机（SVM）上的Word2Vec。我们提出的解决方案优于现有技术水平。在图像、音频和文本上取得的最佳结果分别为97％、96％、9

    Automatic Deception Detection has been a hot research topic for a long time, using machine learning and deep learning to automatically detect deception, brings new light to this old field. In this paper, we proposed a voting-based method for automatic deception detection from videos using audio, visual and lexical features. Experiments were done on two datasets, the Real-life trial dataset by Michigan University and the Miami University deception detection dataset. Video samples were split into frames of images, audio, and manuscripts. Our Voting-based Multimodal proposed solution consists of three models. The first model is CNN for detecting deception from images, the second model is Support Vector Machine (SVM) on Mel spectrograms for detecting deception from audio and the third model is Word2Vec on Support Vector Machine (SVM) for detecting deception from manuscripts. Our proposed solution outperforms state of the art. Best results achieved on images, audio and text were 97%, 96%, 9
    
[^88]: 解释性不是游戏。(arXiv:2307.07514v1 [cs.AI])

    Explainability is NOT a Game. (arXiv:2307.07514v1 [cs.AI])

    [http://arxiv.org/abs/2307.07514](http://arxiv.org/abs/2307.07514)

    Shapley values may provide misleading measures of relative feature importance in XAI, challenging their proposed uses in high-stakes application domains.

    

    可解释性人工智能（XAI）旨在帮助人类决策者理解复杂的机器学习（ML）模型。XAI的一个重要特征是通过使用Shapley值来理论上证明相对特征重要性的度量。本文在最近的研究基础上，提出一个简单的论证，说明Shapley值可能会给相对特征重要性提供误导，使其为预测中无关的特征分配更高的重要性，而对与预测有关的特征分配较低的重要性。这些结果的意义在于它们有效地挑战了相对特征重要性的多种提议用法，这些用法正在高风险应用领域快速增长。

    Explainable artificial intelligence (XAI) aims to help human decision-makers in understanding complex machine learning (ML) models. One of the hallmarks of XAI are measures of relative feature importance, which are theoretically justified through the use of Shapley values. This paper builds on recent work and offers a simple argument for why Shapley values can provide misleading measures of relative feature importance, by assigning more importance to features that are irrelevant for a prediction, and assigning less importance to features that are relevant for a prediction. The significance of these results is that they effectively challenge the many proposed uses of measures of relative feature importance in a fast-growing range of high-stakes application domains.
    
[^89]: 使用放射学报告和图像改善ICU死亡率预测的实证研究

    An empirical study of using radiology reports and images to improve ICU mortality prediction. (arXiv:2307.07513v1 [cs.AI])

    [http://arxiv.org/abs/2307.07513](http://arxiv.org/abs/2307.07513)

    本研究利用放射学报告和图像构建了一个基于深度学习的多模态数据生存预测模型，用于预测重症监护病房（ICU）的死亡率，并在MIMIC-IV数据集上取得了0.7829的平均C-index。

    

    背景：预测重症监护病房（ICU）的评分系统在ICU管理中起着重要作用，因为它能预测重要的结果，尤其是死亡率。许多评分系统已经在ICU中开发和使用。这些评分系统主要基于电子健康记录（EHR）中的结构化临床数据，但可能会丧失叙述和图像中的重要临床信息。方法：在这项工作中，我们利用多模态数据建立了一个基于深度学习的生存预测模型来预测ICU死亡率。我们调查了四组特征：（1）简化急性生理学评分（SAPS）II的生理测量、（2）放射专家预定义的常见胸部疾病、（3）基于BERT的文本表示和（4）胸部X射线图像特征。我们使用Medical Information Mart for Intensive Care IV（MIMIC-IV）数据集评估了提出的模型。结果：我们的模型达到了0.7829的平均C-index（95%的置信区间）

    Background: The predictive Intensive Care Unit (ICU) scoring system plays an important role in ICU management because it predicts important outcomes, especially mortality. Many scoring systems have been developed and used in the ICU. These scoring systems are primarily based on the structured clinical data in the electronic health record (EHR), which may suffer the loss of important clinical information in the narratives and images. Methods: In this work, we build a deep learning based survival prediction model with multi-modality data to predict ICU mortality. Four sets of features are investigated: (1) physiological measurements of Simplified Acute Physiology Score (SAPS) II, (2) common thorax diseases pre-defined by radiologists, (3) BERT-based text representations, and (4) chest X-ray image features. We use the Medical Information Mart for Intensive Care IV (MIMIC-IV) dataset to evaluate the proposed model. Results: Our model achieves the average C-index of 0.7829 (95% confidence i
    
[^90]: 一个上下文感知的混合整数规划切割平面选择算法

    A Context-Aware Cutting Plane Selection Algorithm for Mixed-Integer Programming. (arXiv:2307.07322v1 [math.OC])

    [http://arxiv.org/abs/2307.07322](http://arxiv.org/abs/2307.07322)

    本文提出了一个上下文感知的混合整数规划切割平面选择算法，通过引入新的评分指标、过滤技术和停止准则，使得SCIP在MIPLIB 2017基准集上的性能提升了4%。

    

    目前在混合整数规划求解器中使用的切割平面选择算法自其创建以来基本保持不变。本文提出了一组新的切割评分指标、切割过滤技术和停止准则，扩展了当前最先进的算法，并在MIPLIB 2017基准集上使SCIP的性能提高了4%。

    The current cut selection algorithm used in mixed-integer programming solvers has remained largely unchanged since its creation. In this paper, we propose a set of new cut scoring measures, cut filtering techniques, and stopping criteria, extending the current state-of-the-art algorithm and obtaining a 4\% performance improvement for SCIP over the MIPLIB 2017 benchmark set.
    
[^91]: 关于插值专家和多臂赌博机的研究

    On Interpolating Experts and Multi-Armed Bandits. (arXiv:2307.07264v1 [cs.LG])

    [http://arxiv.org/abs/2307.07264](http://arxiv.org/abs/2307.07264)

    学习专家建议和多臂赌博是两个经典的在线决策问题，我们研究了两者之间的插值问题。我们提出了$\mathbf{m}$-MAB的极小后悔界并设计了$\mathbf{m}$-BAI的最优PAC算法，该算法旨在以尽可能少的轮数确定损失最小的臂。

    

    学习专家建议和多臂赌博是两个经典的在线决策问题，它们在每一轮观察信息的方式上有所不同。我们研究了这两者之间的插值问题。对于向量$\mathbf{m}=(m_1,\dots,m_K)\in \mathbb{N}^K$，$\mathbf{m}$-MAB的一个实例表示将臂分成$K$组，第$i$组包含$m_i$个臂。一旦拉动一个臂，同一组中所有臂的损失都被观察到。我们证明了$\mathbf{m}$-MAB的紧致极小后悔界，并为其纯探索版本$\mathbf{m}$-BAI设计了一个最优的PAC算法，其中目标是用尽可能少的轮数来识别损失最小的臂。我们证明了$\mathbf{m}$-MAB的极小后悔是$\Theta\left(\sqrt{T\sum_{k=1}^K\log (m_k+1)}\right)$，对于一个$(\epsilon,0.05)$-PAC算法的$\mathbf{m}$-BAI，拉动臂的最小次数是$\Theta\left(\frac{1}{\epsilon^2}\cdot \sum_{k=1}^K\log (m_k+1)\right)$。

    Learning with expert advice and multi-armed bandit are two classic online decision problems which differ on how the information is observed in each round of the game. We study a family of problems interpolating the two. For a vector $\mathbf{m}=(m_1,\dots,m_K)\in \mathbb{N}^K$, an instance of $\mathbf{m}$-MAB indicates that the arms are partitioned into $K$ groups and the $i$-th group contains $m_i$ arms. Once an arm is pulled, the losses of all arms in the same group are observed. We prove tight minimax regret bounds for $\mathbf{m}$-MAB and design an optimal PAC algorithm for its pure exploration version, $\mathbf{m}$-BAI, where the goal is to identify the arm with minimum loss with as few rounds as possible. We show that the minimax regret of $\mathbf{m}$-MAB is $\Theta\left(\sqrt{T\sum_{k=1}^K\log (m_k+1)}\right)$ and the minimum number of pulls for an $(\epsilon,0.05)$-PAC algorithm of $\mathbf{m}$-BAI is $\Theta\left(\frac{1}{\epsilon^2}\cdot \sum_{k=1}^K\log (m_k+1)\right)$. Bot
    
[^92]: 知识增强：重新思考医学对比视觉语言预训练

    Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-Training. (arXiv:2307.07246v1 [cs.CV])

    [http://arxiv.org/abs/2307.07246](http://arxiv.org/abs/2307.07246)

    该论文提出了一个名为KoBo的框架，它通过将临床知识整合到视觉语言语义一致性学习中，解决了医学领域中的大规模语义重叠和转移问题。

    

    基于预训练技术的基础模型从理论上到实践应用显著推进了人工智能的发展。这些模型推动了计算机辅助诊断的可行性，使其广泛应用。医学对比视觉语言预训练是一种有效的方法，它利用诊断报告中的描述信息来指导表征学习，无需人工注释。然而，预训练的效果受制于医学领域的大规模语义重叠和转移问题。为了解决这些问题，我们提出了知识增强对比视觉语言预训练框架（KoBo），该框架将临床知识整合到视觉语言语义一致性学习中。该框架使用无偏的、开集样本级知识表示来衡量负样本噪声，并补充视觉语言互信息与临床知识之间的对应关系。

    The foundation models based on pre-training technology have significantly advanced artificial intelligence from theoretical to practical applications. These models have facilitated the feasibility of computer-aided diagnosis for widespread use. Medical contrastive vision-language pre-training, which does not require human annotations, is an effective approach for guiding representation learning using description information in diagnostic reports. However, the effectiveness of pre-training is limited by the large-scale semantic overlap and shifting problems in medical field. To address these issues, we propose the Knowledge-Boosting Contrastive Vision-Language Pre-training framework (KoBo), which integrates clinical knowledge into the learning of vision-language semantic consistency. The framework uses an unbiased, open-set sample-wise knowledge representation to measure negative sample noise and supplement the correspondence between vision-language mutual information and clinical knowl
    
[^93]: DataAssist:一种用于数据清洗和准备的机器学习方法

    DataAssist: A Machine Learning Approach to Data Cleaning and Preparation. (arXiv:2307.07119v1 [cs.LG])

    [http://arxiv.org/abs/2307.07119](http://arxiv.org/abs/2307.07119)

    DataAssist是一种机器学习方法，用于提高数据集质量和节省数据清洗和准备时间。

    

    目前的自动化机器学习工具主要关注于模型选择和参数优化，忽略了数据清洗和整理所占据的大部分时间。本文介绍了一种名为DataAssist的自动化数据准备和清洗平台，利用机器学习方法提高数据集质量。我们展示了DataAssist提供了一个用于探索性数据分析和数据清洗的管道，包括为用户选择的变量生成可视化，统一数据注释，提供异常值删除建议以及对数据进行预处理。导出的数据集可以轻松与其他自动化机器学习工具或用户指定的模型进行整合，以进行后续分析。我们的数据中心化工具适用于多个领域，包括经济学、商业和预测应用，可节省超过50\%的数据清理和准备时间。

    Current automated machine learning (ML) tools are model-centric, focusing on model selection and parameter optimization. However, the majority of the time in data analysis is devoted to data cleaning and wrangling, for which limited tools are available. Here we present DataAssist, an automated data preparation and cleaning platform that enhances dataset quality using ML-informed methods. We show that DataAssist provides a pipeline for exploratory data analysis and data cleaning, including generating visualization for user-selected variables, unifying data annotation, suggesting anomaly removal, and preprocessing data. The exported dataset can be readily integrated with other autoML tools or user-specified model for downstream analysis. Our data-centric tool is applicable to a variety of fields, including economics, business, and forecasting applications saving over 50\% time of the time spent on data cleansing and preparation.
    
[^94]: 经过长距离步骤的梯度下降的可证明更快收敛速度

    Provably Faster Gradient Descent via Long Steps. (arXiv:2307.06324v1 [math.OC])

    [http://arxiv.org/abs/2307.06324](http://arxiv.org/abs/2307.06324)

    本研究通过计算机辅助分析技术，证明了非常数步长策略下的梯度下降方法经过长距离步骤可以实现更快的收敛速度。

    

    本研究通过计算机辅助分析技术，建立了经过长距离步骤的梯度下降的可证明更快收敛速度。我们的理论允许非常数步长策略，通过分析多次迭代的整体效果而不是典型的一次迭代归纳使用的，从而有可能破坏下降。我们表明，长距离步骤，可能在短期内增加目标值，但在长期内带来更快的收敛速度。此外，我们还提出了一个关于梯度下降更快收敛速度的猜想，并进行了简单的数值验证。

    This work establishes provably faster convergence rates for gradient descent via a computer-assisted analysis technique. Our theory allows nonconstant stepsize policies with frequent long steps potentially violating descent by analyzing the overall effect of many iterations at once rather than the typical one-iteration inductions used in most first-order method analyses. We show that long steps, which may increase the objective value in the short term, lead to provably faster convergence in the long term. A conjecture towards proving a faster $O(1/T\log T)$ rate for gradient descent is also motivated along with simple numerical validation.
    
[^95]: 基于Newell理论的特征转换用于时空交通预测

    Newell's theory based feature transformations for spatio-temporal traffic prediction. (arXiv:2307.05949v1 [cs.LG])

    [http://arxiv.org/abs/2307.05949](http://arxiv.org/abs/2307.05949)

    本文提出了一种基于Newell理论的特征转换方法用于时空交通预测，用于改善模型在不同位置的迁移性问题。

    

    深度学习模型在时空交通流预测中使用卷积或图卷积过滤器以及循环神经网络来捕捉交通数据的空间和时间依赖关系。这些模型, 如CNN-LSTM, 利用邻近检测站的交通流来预测特定位置的流量。然而, 这些模型在捕捉交通系统的更广泛动态方面具有局限性, 因为它们主要学习特定于检测配置和目标位置交通特征的特征。因此, 当在新的位置缺少用于模型训练的数据时, 这些模型的可迁移性变得具有挑战性。为了解决这个问题, 我们提出了一个基于交通流物理学的特征转换方法用于时空深度学习模型。

    Deep learning (DL) models for spatio-temporal traffic flow forecasting employ convolutional or graph-convolutional filters along with recurrent neural networks to capture spatial and temporal dependencies in traffic data. These models, such as CNN-LSTM, utilize traffic flows from neighboring detector stations to predict flows at a specific location of interest. However, these models are limited in their ability to capture the broader dynamics of the traffic system, as they primarily learn features specific to the detector configuration and traffic characteristics at the target location. Hence, the transferability of these models to different locations becomes challenging, particularly when data is unavailable at the new location for model training. To address this limitation, we propose a traffic flow physics-based feature transformation for spatio-temporal DL models. This transformation incorporates Newell's uncongested and congested-state estimators of traffic flows at the target loc
    
[^96]: 黑盒DNN后门检测的触发器和良性特征的差异分析

    Differential Analysis of Triggers and Benign Features for Black-Box DNN Backdoor Detection. (arXiv:2307.05422v1 [cs.CR])

    [http://arxiv.org/abs/2307.05422](http://arxiv.org/abs/2307.05422)

    本文提出了一种在黑盒情况下检测深度神经网络后门攻击的高效方法，通过量化触发器和良性特征对确定后门网络输出的影响，并使用训练好的新颖性检测器进行检测。

    

    本文提出了一种在黑盒情况下，对深度神经网络进行高效检测的方法，用于防御后门攻击。该方法的思路是，与其他良性特征相比，与触发器相关的特征对确定后门网络输出具有更高的影响力。为了定量衡量触发器和良性特征对确定后门网络输出的影响，我们引入了五个度量指标。为了计算给定输入的五个度量值，我们首先通过将输入的部分内容注入到干净的验证样本中生成几个合成样本。然后，通过使用相应合成样本的输出标签计算出五个度量指标。本文的一个贡献是使用了一个小型的干净验证数据集。在计算出五个度量值后，我们从验证数据集中训练出五个新颖性检测器。一个元新颖性检测器将五个训练好的新颖性检测器的输出进行融合。

    This paper proposes a data-efficient detection method for deep neural networks against backdoor attacks under a black-box scenario. The proposed approach is motivated by the intuition that features corresponding to triggers have a higher influence in determining the backdoored network output than any other benign features. To quantitatively measure the effects of triggers and benign features on determining the backdoored network output, we introduce five metrics. To calculate the five-metric values for a given input, we first generate several synthetic samples by injecting the input's partial contents into clean validation samples. Then, the five metrics are computed by using the output labels of the corresponding synthetic samples. One contribution of this work is the use of a tiny clean validation dataset. Having the computed five metrics, five novelty detectors are trained from the validation dataset. A meta novelty detector fuses the output of the five trained novelty detectors to 
    
[^97]: 使用双调节器解决联邦半监督学习中的数据不平衡问题

    Combating Data Imbalances in Federated Semi-supervised Learning with Dual Regulators. (arXiv:2307.05358v1 [cs.LG])

    [http://arxiv.org/abs/2307.05358](http://arxiv.org/abs/2307.05358)

    本文提出了一种带有双调节器的新型联邦半监督学习框架FedDure，解决了数据分布不平衡的问题。通过粗调节器和细调节器对本地模型的更新进行规范，以及学习适应性加权方案，适应不同的数据分布。

    

    联邦学习已经成为一种从分散异构数据中学习的流行方法。由于分散客户端上标签稀缺，联邦半监督学习（FSSL）出现以从少量标记数据中训练模型。现有的FSSL方法假设客户端之间的标签数据独立且具有相同分布，并且在客户端内部标记和未标记数据之间具有一致的类别分布。本文研究了FSSL的更实际和具有挑战性的情况，即数据分布不仅在客户端之间不同，在客户端内部标记和未标记数据之间也不同。为了解决这个挑战，本文提出了一种带有双调节器的新型FSSL框架，FedDure。FedDure通过粗调节器（C-reg）和细调节器（F-reg）解除了以前的假设：C-reg通过跟踪标记数据分布的学习效果来规范本地模型的更新；F-reg学习一个适应性加权方案，以适应客户端内不同的数据分布。

    Federated learning has become a popular method to learn from decentralized heterogeneous data. Federated semi-supervised learning (FSSL) emerges to train models from a small fraction of labeled data due to label scarcity on decentralized clients. Existing FSSL methods assume independent and identically distributed (IID) labeled data across clients and consistent class distribution between labeled and unlabeled data within a client. This work studies a more practical and challenging scenario of FSSL, where data distribution is different not only across clients but also within a client between labeled and unlabeled data. To address this challenge, we propose a novel FSSL framework with dual regulators, FedDure.} FedDure lifts the previous assumption with a coarse-grained regulator (C-reg) and a fine-grained regulator (F-reg): C-reg regularizes the updating of the local model by tracking the learning effect on labeled data distribution; F-reg learns an adaptive weighting scheme tailored f
    
[^98]: 基于同质性的有监督注意力图神经网络

    Supervised Attention Using Homophily in Graph Neural Networks. (arXiv:2307.05217v1 [cs.LG])

    [http://arxiv.org/abs/2307.05217](http://arxiv.org/abs/2307.05217)

    本文提出了一种新技术，可以在任何图注意力模型中应用，以鼓励共享相同类别标签的节点获得更高的注意力分数，并在多个节点分类数据集上展示了比标准基线模型更高的性能。

    

    图神经网络已经成为处理图上学习问题的标准方法。在不同变种的图神经网络中，图注意力网络（GATs）被成功应用于不同的任务。在GAT模型中，每个节点使用注意力机制为其邻居分配重要性分数。然而，类似于其他图神经网络，GAT聚合来自属于不同类别的节点的信息，因此产生的节点表示在不同类别方面不够明确，这可能会影响它们的性能。在这项工作中，为了缓解这个问题，我们提出了一种新的技术，可以将其纳入到任何图注意力模型中，以鼓励共享相同类别标签的节点之间获得更高的注意力分数。我们在几个节点分类数据集上评估了所提出的方法，并展示了与标准基线模型相比的性能提升。

    Graph neural networks have become the standard approach for dealing with learning problems on graphs. Among the different variants of graph neural networks, graph attention networks (GATs) have been applied with great success to different tasks. In the GAT model, each node assigns an importance score to its neighbors using an attention mechanism. However, similar to other graph neural networks, GATs aggregate messages from nodes that belong to different classes, and therefore produce node representations that are not well separated with respect to the different classes, which might hurt their performance. In this work, to alleviate this problem, we propose a new technique that can be incorporated into any graph attention model to encourage higher attention scores between nodes that share the same class label. We evaluate the proposed method on several node classification datasets demonstrating increased performance over standard baseline models.
    
[^99]: 混合隐马尔可夫LSTM用于短期交通流量预测

    Hybrid hidden Markov LSTM for short-term traffic flow prediction. (arXiv:2307.04954v1 [cs.LG])

    [http://arxiv.org/abs/2307.04954](http://arxiv.org/abs/2307.04954)

    该论文介绍了一种混合隐马尔可夫LSTM模型，用于短期交通流量预测。研究发现，深度学习方法在预测交通变量方面优于传统的参数模型。这种模型结合了循环神经网络和隐马尔可夫模型的优势，能够捕捉交通系统的复杂动态模式和非平稳性。

    

    深度学习方法在预测交通变量的短期和近短期未来方面已经优于参数模型，如历史平均、ARIMA和其变体，这对于交通管理至关重要。具体来说，循环神经网络（RNN）及其变体（例如长短期记忆）被设计用于保留长期时序相关性，因此非常适用于建模序列。然而，多制度模型假设交通系统以不同特征的多个状态（例如畅通、拥堵）演变，因此需要训练不同模型以表征每个制度内的交通动态。例如，使用隐马尔可夫模型进行制度识别的马尔可夫切换模型能够捕捉复杂的动态模式和非平稳性。有趣的是，隐马尔可夫模型和LSTM都可以用于建模从一组潜在的或隐藏状态变量中的观察序列。在LSTM中，潜在变量可以从上一个时间步的隐藏状态变量传递过来。

    Deep learning (DL) methods have outperformed parametric models such as historical average, ARIMA and variants in predicting traffic variables into short and near-short future, that are critical for traffic management. Specifically, recurrent neural network (RNN) and its variants (e.g. long short-term memory) are designed to retain long-term temporal correlations and therefore are suitable for modeling sequences. However, multi-regime models assume the traffic system to evolve through multiple states (say, free-flow, congestion in traffic) with distinct characteristics, and hence, separate models are trained to characterize the traffic dynamics within each regime. For instance, Markov-switching models with a hidden Markov model (HMM) for regime identification is capable of capturing complex dynamic patterns and non-stationarity. Interestingly, both HMM and LSTM can be used for modeling an observation sequence from a set of latent or, hidden state variables. In LSTM, the latent variable 
    
[^100]: 通过非对比CT扫描进行胃癌有效筛查的簇诱导蒙版变换器

    Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans. (arXiv:2307.04525v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2307.04525](http://arxiv.org/abs/2307.04525)

    本研究提出了一种簇诱导蒙版变换器，在非对比CT扫描中使用深度学习方法进行胃癌筛查。该方法通过自注意和交叉注意与卷积特征交互，以多任务方式分割肿瘤并分类异常，实验结果表明，在保留测试集上达到了85.0%的敏感性和92.6%的特异性，相比两名放射科医生具有更好的性能。

    

    胃癌是全球肿瘤相关死亡的第三大原因，然而目前没有规范推荐的筛查测试方法。现有方法可能侵入性强、昂贵且对早期胃癌的敏感性不足。本研究探索了在非对比CT扫描中使用深度学习方法进行胃癌检测的可行性。我们提出了一种新颖的簇诱导蒙版变换器，以多任务方式同时分割肿瘤并分类异常。我们的模型结合了可学习的簇，用于编码胃癌的纹理和形状原型，并利用自注意和交叉注意与卷积特征交互。在实验中，所提出的方法在由100名癌症患者和148名正常人组成的保留测试集上达到了85.0%的敏感性和92.6%的特异性。相比之下，两名放射科医生的平均敏感性为73.5%，特异性为84.3%。

    Gastric cancer is the third leading cause of cancer-related mortality worldwide, but no guideline-recommended screening test exists. Existing methods can be invasive, expensive, and lack sensitivity to identify early-stage gastric cancer. In this study, we explore the feasibility of using a deep learning approach on non-contrast CT scans for gastric cancer detection. We propose a novel cluster-induced Mask Transformer that jointly segments the tumor and classifies abnormality in a multi-task manner. Our model incorporates learnable clusters that encode the texture and shape prototypes of gastric cancer, utilizing self- and cross-attention to interact with convolutional features. In our experiments, the proposed method achieves a sensitivity of 85.0% and specificity of 92.6% for detecting gastric tumors on a hold-out test set consisting of 100 patients with cancer and 148 normal. In comparison, two radiologists have an average sensitivity of 73.5% and specificity of 84.3%. We also obtai
    
[^101]: ECS -- 用于数据质量保证的交互工具

    ECS -- an Interactive Tool for Data Quality Assurance. (arXiv:2307.04368v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.04368](http://arxiv.org/abs/2307.04368)

    本文提出了一种交互工具ECS，用于保证数据质量。该工具能够检测出在安全关键系统中具有潜在危害属性的数据点。

    

    随着机器学习系统的能力不断增强及其在安全关键系统中的潜在应用，确保高质量的数据变得越来越重要。本文提出了一种新颖的数据质量保证方法。首先讨论了数学基础，然后通过多个示例介绍了这种方法。该方法能够检测出在安全关键系统中具有潜在危害属性的数据点。

    With the increasing capabilities of machine learning systems and their potential use in safety-critical systems, ensuring high-quality data is becoming increasingly important. In this paper we present a novel approach for the assurance of data quality. For this purpose, the mathematical basics are first discussed and the approach is presented using multiple examples. This results in the detection of data points with potentially harmful properties for the use in safety-critical systems.
    
[^102]: ChatGPT在生成式AI和大语言模型时代的应用：一份简洁的调查报告

    ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey. (arXiv:2307.04251v1 [cs.CL])

    [http://arxiv.org/abs/2307.04251](http://arxiv.org/abs/2307.04251)

    ChatGPT是由OpenAI创建的一种大型语言模型（LLM），它在自然语言处理（NLP）领域引起了革命性的变革。它是第一个实现公众大规模与生成式人工智能（GAI）互动的关键技术，也引发了类似技术的研究兴趣和应用探索。

    

    ChatGPT是由OpenAI创建的一种大型语言模型（LLM），经过精心训练并使用了大量数据。它在自然语言处理（NLP）领域引起了革命性的变革，并推动了LLM能力的边界。ChatGPT在大规模范围内实现了普遍公众与生成式人工智能（GAI）的互动，起到了关键作用。它还引发了开发类似技术和研究其应用和影响的兴趣。本文的主要目标是对ChatGPT及其演化的当前研究方向进行简明调查。我们同时考虑了ChatGPT的玻璃盒和黑盒视角，包括技术的组成部分和基本要素，以及其应用、影响和影响。玻璃盒方法着重于理解技术的内部运作，而黑盒方法将其视为一个复杂系统，因此研究其输入，

    ChatGPT is a large language model (LLM) created by OpenAI that has been carefully trained on a large amount of data. It has revolutionized the field of natural language processing (NLP) and has pushed the boundaries of LLM capabilities. ChatGPT has played a pivotal role in enabling widespread public interaction with generative artificial intelligence (GAI) on a large scale. It has also sparked research interest in developing similar technologies and investigating their applications and implications. In this paper, our primary goal is to provide a concise survey on the current lines of research on ChatGPT and its evolution. We considered both the glass box and black box views of ChatGPT, encompassing the components and foundational elements of the technology, as well as its applications, impacts, and implications. The glass box approach focuses on understanding the inner workings of the technology, and the black box approach embraces it as a complex system, and thus examines its inputs,
    
[^103]: 扩展前向前向算法

    Extending the Forward Forward Algorithm. (arXiv:2307.04205v1 [cs.LG])

    [http://arxiv.org/abs/2307.04205](http://arxiv.org/abs/2307.04205)

    这个论文扩展了前向前向算法，首先在IMDb数据集上进行了情感分析任务，其次引入了金字塔优化策略来改进损失阈值，最后通过参数可视化得出了一些重要的洞察。

    

    前向前向算法是Geoffrey Hinton于2022年11月提出的一种用于训练神经网络的新方法，作为对反向传播的替代方法。在这个项目中，我们在MNIST数据集上复制了Hinton的实验，并随后通过两个重要的贡献扩展了该方法的范围。首先，我们为前向前向网络在IMDb电影评论数据集上建立了一个基准性能。据我们所知，我们在这个情感分析任务上的结果标志着该算法在计算机视觉之外的首次扩展。其次，我们引入了一种新颖的金字塔优化策略，用于损失阈值，这是前向前向方法特有的超参数。我们的金字塔方法表明，一个好的阈值策略会导致测试错误率的差异高达8%。最后，我们对训练参数进行可视化，并得出了一些重要的洞察，例如权重的平均值和方差显著增加了10-20倍。

    The Forward Forward algorithm, proposed by Geoffrey Hinton in November 2022, is a novel method for training neural networks as an alternative to backpropagation. In this project, we replicate Hinton's experiments on the MNIST dataset, and subsequently extend the scope of the method with two significant contributions. First, we establish a baseline performance for the Forward Forward network on the IMDb movie reviews dataset. As far as we know, our results on this sentiment analysis task marks the first instance of the algorithm's extension beyond computer vision. Second, we introduce a novel pyramidal optimization strategy for the loss threshold - a hyperparameter specific to the Forward Forward method. Our pyramidal approach shows that a good thresholding strategy causes a difference of upto 8% in test error. 1 Lastly, we perform visualizations of the trained parameters and derived several significant insights, such as a notably larger (10-20x) mean and variance in the weights acquire
    
[^104]: 超越直觉，将高斯过程应用于实际数据的框架

    Beyond Intuition, a Framework for Applying GPs to Real-World Data. (arXiv:2307.03093v1 [cs.LG])

    [http://arxiv.org/abs/2307.03093](http://arxiv.org/abs/2307.03093)

    提出了一个框架，用于确定高斯过程在实际问题中的适用性，并建立一个稳健且明确的模型。通过对核函数设计和计算可扩展性选项的指导，该框架在冰川高程变化的案例研究中实现了更准确的结果。

    

    高斯过程（GPs）提供了一种用于小型、结构化和相关数据集的回归的吸引人的方法。然而，它们的应用受到计算成本的限制，并且对于如何将GPs应用于复杂的高维数据集的指导有限。我们提出了一个框架，用于确定GPs在给定问题中的适用性以及如何建立一个强大且明确的GP模型。指导方针形式化了经验丰富的GP实践者的决策，特别强调了核函数设计和计算可扩展性选项。然后，我们将该框架应用于冰川高程变化的案例研究中，在测试时产生了更准确的结果。

    Gaussian Processes (GPs) offer an attractive method for regression over small, structured and correlated datasets. However, their deployment is hindered by computational costs and limited guidelines on how to apply GPs beyond simple low-dimensional datasets. We propose a framework to identify the suitability of GPs to a given problem and how to set up a robust and well-specified GP model. The guidelines formalise the decisions of experienced GP practitioners, with an emphasis on kernel design and options for computational scalability. The framework is then applied to a case study of glacier elevation change yielding more accurate results at test time.
    
[^105]: 一种可解释的模型以支持AML治疗方案的决策

    An explainable model to support the decision about the therapy protocol for AML. (arXiv:2307.02631v1 [cs.LG])

    [http://arxiv.org/abs/2307.02631](http://arxiv.org/abs/2307.02631)

    本文提出了一种可解释的机器学习模型，用于支持AML患者治疗方案的决策，解决了当前风险分类存在的问题和专家需求额外测试和分析的困扰。

    

    急性髓细胞白血病（AML）是一种最具侵略性的血液肿瘤。为了支持专家关于合适治疗的决策，AML患者根据其细胞遗传和分子特征获得预后信息，通常分为有利、中等和不利三个风险类别。然而，当前的风险分类存在已知问题，如同一风险组中患者之间的异质性和中风险类别的清晰定义缺失。此外，由于大多数AML患者被归为中风险分类，专家常需进行其他测试和分析，导致治疗延迟和患者临床状况恶化。本文提出了数据分析和一种可解释的机器学习模型，以支持根据患者生存预测确定最合适的治疗方案的决策。

    Acute Myeloid Leukemia (AML) is one of the most aggressive types of hematological neoplasm. To support the specialists' decision about the appropriate therapy, patients with AML receive a prognostic of outcomes according to their cytogenetic and molecular characteristics, often divided into three risk categories: favorable, intermediate, and adverse. However, the current risk classification has known problems, such as the heterogeneity between patients of the same risk group and no clear definition of the intermediate risk category. Moreover, as most patients with AML receive an intermediate-risk classification, specialists often demand other tests and analyses, leading to delayed treatment and worsening of the patient's clinical condition. This paper presents the data analysis and an explainable machine-learning model to support the decision about the most appropriate therapy protocol according to the patient's survival prediction. In addition to the prediction model being explainable
    
[^106]: PIGNet2：一种用于结合亲和力评分和虚拟筛选的多功能深度学习蛋白质-配体相互作用预测模型

    PIGNet2: A Versatile Deep Learning-based Protein-Ligand Interaction Prediction Model for Binding Affinity Scoring and Virtual Screening. (arXiv:2307.01066v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2307.01066](http://arxiv.org/abs/2307.01066)

    提出了一种用于蛋白质-配体相互作用预测的多功能深度学习模型PIGNet2，通过引入数据增强策略和物理原理的图神经网络解决了少样本问题，实现了准确评分和高效筛选。

    

    在药物发现中，蛋白质-配体相互作用（PLI）的预测在指导有效结合目标蛋白的分子的鉴定和优化方面起着关键作用。尽管基于深度学习的PLI预测取得了显著进展，但开发一种能够准确评分结合亲和力和进行高效虚拟筛选的通用模型仍然是一个挑战。实验结构-亲和力数据的稀缺性是实现这一目标的主要障碍，限制了现有模型的泛化能力。在本文中，我们提出了一个可行的解决方案，通过引入一种新颖的数据增强策略以及一种基于物理原理的图神经网络，来应对这一挑战。该模型在评分和筛选方面显示出显著的改进，在各种测试中超过了特定任务的深度学习模型，并且与基于距离的现有性能相当。

    Prediction of protein-ligand interactions (PLI) plays a crucial role in drug discovery as it guides the identification and optimization of molecules that effectively bind to target proteins. Despite remarkable advances in deep learning-based PLI prediction, the development of a versatile model capable of accurately scoring binding affinity and conducting efficient virtual screening remains a challenge. The main obstacle in achieving this lies in the scarcity of experimental structure-affinity data, which limits the generalization ability of existing models. Here, we propose a viable solution to address this challenge by introducing a novel data augmentation strategy combined with a physics-informed graph neural network. The model showed significant improvements in both scoring and screening, outperforming task-specific deep learning models in various tests including derivative benchmarks, and notably achieving results comparable to the state-of-the-art performance based on distance lik
    
[^107]: 移动行为减重干预的个性化金融激励的自适应优化方法

    An Adaptive Optimization Approach to Personalized Financial Incentives in Mobile Behavioral Weight Loss Interventions. (arXiv:2307.00444v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.00444](http://arxiv.org/abs/2307.00444)

    这项研究研究了在移动行为减重干预中个性化金融激励的自适应优化方法。问题主要在于如何有效地分配有限的干预预算和高昂的资源，以提供最佳的激励结构来促进参与者的依从性和行为改变。

    

    肥胖是影响美国的关键医疗问题。对于肥胖症，最低风险的治疗方法是行为干预，旨在促进饮食和运动。通常，这些干预措施包括移动组件，允许干预者收集参与者的数据，并为参与者提供激励和目标，以促进长期行为改变。最近，人们对使用直接经济激励促进行为改变产生了兴趣。然而，在这些干预措施中，依从性是一个挑战，因为每个参与者对不同的激励结构和金额会有不同的反应，导致研究人员考虑个性化干预。个性化的关键挑战在于临床医生事先不知道如何最好地向参与者提供激励，并在有限的干预预算下如何高效地使用昂贵的资源。在本文中，我们考虑了设计个性化减重干预的挑战。

    Obesity is a critical healthcare issue affecting the United States. The least risky treatments available for obesity are behavioral interventions meant to promote diet and exercise. Often these interventions contain a mobile component that allows interventionists to collect participants level data and provide participants with incentives and goals to promote long term behavioral change. Recently, there has been interest in using direct financial incentives to promote behavior change. However, adherence is challenging in these interventions, as each participant will react differently to different incentive structure and amounts, leading researchers to consider personalized interventions. The key challenge for personalization, is that the clinicians do not know a priori how best to administer incentives to participants, and given finite intervention budgets how to disburse costly resources efficiently. In this paper, we consider this challenge of designing personalized weight loss interv
    
[^108]: 从合成的人类团队活动中学习

    Learning from Synthetic Human Group Activities. (arXiv:2306.16772v1 [cs.CV])

    [http://arxiv.org/abs/2306.16772](http://arxiv.org/abs/2306.16772)

    提出了M3Act，一个多视图多团队多人的人类原子动作和团队活动数据生成器，通过Unity引擎驱动实现。该生成器具有大规模数据生成、多模态和高质量注释等特点，能够用于研究复杂的人类互动和团队活动。

    

    在以人为中心的计算机视觉中，对复杂的人类互动和团队活动的理解引起了人们的关注。然而，相关任务的进展受到了获取大规模标记的真实世界数据集的困难的限制。为了缓解这个问题，我们提出了M3Act，一个多视图多团队多人的人类原子动作和团队活动数据生成器。M3Act采用Unity引擎驱动，包含可供仿真使用的三维场景和人物资源，可配置的照明和摄像系统，高度参数化的模块化团队活动，以及在数据生成过程中具有大量领域随机化的特点。我们的数据生成器能够生成具有多个视图、模态（RGB图像、2D姿势、3D动作）和高质量注释的大规模人类活动数据集（2D边界框、实例分割掩模、个体动作和团队活动类别）。利用M3Act，我们可以生成大规模的人类活动数据集，用于研究人类互动和团队活动。

    The understanding of complex human interactions and group activities has garnered attention in human-centric computer vision. However, the advancement of the related tasks is hindered due to the difficulty of obtaining large-scale labeled real-world datasets. To mitigate the issue, we propose M3Act, a multi-view multi-group multi-person human atomic action and group activity data generator. Powered by the Unity engine, M3Act contains simulation-ready 3D scenes and human assets, configurable lighting and camera systems, highly parameterized modular group activities, and a large degree of domain randomization during the data generation process. Our data generator is capable of generating large-scale datasets of human activities with multiple viewpoints, modalities (RGB images, 2D poses, 3D motions), and high-quality annotations for individual persons and multi-person groups (2D bounding boxes, instance segmentation masks, individual actions and group activity categories). Using M3Act, we
    
[^109]: 一种鲁棒联邦学习的一阶Meta Stackelberg方法

    A First Order Meta Stackelberg Method for Robust Federated Learning. (arXiv:2306.13800v1 [cs.LG])

    [http://arxiv.org/abs/2306.13800](http://arxiv.org/abs/2306.13800)

    本研究提出了一种鲁棒的联邦学习防御方法，使用元Stackelberg学习算法解决贝叶斯Stackelberg马尔科夫博弈，实现自适应防御，与现有技术相匹配并在实验中表现出色。

    

    先前的研究表明，联邦学习系统面临着各种安全风险。尽管提出了多种防御策略，但它们往往是非自适应的，只针对某些类型的攻击，从而无法抵御不可预测或自适应的威胁。本研究将对抗性联邦学习建模为贝叶斯Stackelberg马尔科夫博弈(BSMG)以捕捉防御者对各种攻击类型的不完全信息。我们提出了元Stackelberg学习(meta-SL)，这是一种可证明有效的元学习算法，用于解决BSMG中的均衡策略，从而实现自适应的FL防御。我们证明，meta-SL在$O(\varepsilon^{-2})$梯度迭代中收敛于一阶$\varepsilon$-均衡点，每次迭代需要$O(\varepsilon^{-4})$个样本，与现有技术相匹配。经验证据表明，我们的元Stackelberg框架在强大的模型污染和后门攻击方面表现出色。

    Previous research has shown that federated learning (FL) systems are exposed to an array of security risks. Despite the proposal of several defensive strategies, they tend to be non-adaptive and specific to certain types of attacks, rendering them ineffective against unpredictable or adaptive threats. This work models adversarial federated learning as a Bayesian Stackelberg Markov game (BSMG) to capture the defender's incomplete information of various attack types. We propose meta-Stackelberg learning (meta-SL), a provably efficient meta-learning algorithm, to solve the equilibrium strategy in BSMG, leading to an adaptable FL defense. We demonstrate that meta-SL converges to the first-order $\varepsilon$-equilibrium point in $O(\varepsilon^{-2})$ gradient iterations, with $O(\varepsilon^{-4})$ samples needed per iteration, matching the state of the art. Empirical evidence indicates that our meta-Stackelberg framework performs exceptionally well against potent model poisoning and backdo
    
[^110]: 提前预测理解前浪潮：研究掌握技能模型的损失函数表面

    Predicting Grokking Long Before it Happens: A look into the loss landscape of models which grok. (arXiv:2306.13253v1 [cs.LG])

    [http://arxiv.org/abs/2306.13253](http://arxiv.org/abs/2306.13253)

    本文提出了一种低成本方法来预测神经网络中的理解前浪潮，即通过研究前几轮的学习曲线来判断后续是否出现理解前浪潮。使用波形振荡和学习曲线的频谱特征值可以高精度地预测理解前浪潮。

    

    本文研究了神经网络中出现理解前浪潮的预测，该现象是完美概括在出现过拟合或记忆迹象之后很长一段时间才出现。报告称，只有在特定的超参数下才能观察到理解前浪潮。这使得确定导致理解前浪潮的参数至关重要。然而，由于理解前浪潮需要大量的迭代轮数，因此寻找导致它的超参数是很耗时的。本文提出了一种低成本方法来预测理解前浪潮，而无需训练大量的迭代次数。本文通过研究前几轮的学习曲线，展示了一种可以预测后续出现理解前浪潮的方法。具体而言，如果在前几轮中出现某些振荡，那么可以期望在模型训练更长时间后出现理解前浪潮。我们提出使用学习曲线的频谱特征值来预测理解前浪潮的概率。实验结果表明，我们的方法可以高精度地预测理解前浪潮。

    This paper focuses on predicting the occurrence of grokking in neural networks, a phenomenon in which perfect generalization emerges long after signs of overfitting or memorization are observed. It has been reported that grokking can only be observed with certain hyper-parameters. This makes it critical to identify the parameters that lead to grokking. However, since grokking occurs after a large number of epochs, searching for the hyper-parameters that lead to it is time-consuming. In this paper, we propose a low-cost method to predict grokking without training for a large number of epochs. In essence, by studying the learning curve of the first few epochs, we show that one can predict whether grokking will occur later on. Specifically, if certain oscillations occur in the early epochs, one can expect grokking to occur if the model is trained for a much longer period of time. We propose using the spectral signature of a learning curve derived by applying the Fourier transform to quant
    
[^111]: 双节点和边公平感知图分割

    Dual Node and Edge Fairness-Aware Graph Partition. (arXiv:2306.10123v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2306.10123](http://arxiv.org/abs/2306.10123)

    这项论文提出了一种双节点和边公平感知的图分割方法，通过考虑节点平衡和边平衡来实现公平分割，并使用协嵌入框架学习表示。实验证明该方法在节点和边方面均具有平衡分割和良好效用，并可用作伪标签来促进图神经网络的公平行为。

    

    对社交网络进行公平的图分割是确保无监督用户分析中的公正和非歧视待遇的关键步骤。当前的公平分割方法通常考虑节点平衡，即追求各个人口群体之间节点数量的比例平衡，但忽视了每个聚类中不平衡边所引起的偏差。为了解决这一问题，我们提出了边平衡的概念，用于衡量连接不同人口群体之间边的比例。我们分析了节点平衡和边平衡之间的关系，然后通过线图转换，提出了一个协嵌入框架，用于学习对图分割具有双重节点和边公平感知性的表示。我们通过多个社交网络数据集验证了我们的框架，并观察到在节点和边方面都具有平衡分割和良好效用。此外，我们证明了我们的公平分割可以用作伪标签，以促进图神经网络的公平行为。

    Fair graph partition of social networks is a crucial step toward ensuring fair and non-discriminatory treatments in unsupervised user analysis. Current fair partition methods typically consider node balance, a notion pursuing a proportionally balanced number of nodes from all demographic groups, but ignore the bias induced by imbalanced edges in each cluster. To address this gap, we propose a notion edge balance to measure the proportion of edges connecting different demographic groups in clusters. We analyze the relations between node balance and edge balance, then with line graph transformations, we propose a co-embedding framework to learn dual node and edge fairness-aware representations for graph partition. We validate our framework through several social network datasets and observe balanced partition in terms of both nodes and edges along with good utility. Moreover, we demonstrate our fair partition can be used as pseudo labels to facilitate graph neural networks to behave fair
    
[^112]: DoubleAdapt：一种用于股票趋势预测的增量学习元学习方法

    DoubleAdapt: A Meta-learning Approach to Incremental Learning for Stock Trend Forecasting. (arXiv:2306.09862v1 [q-fin.ST])

    [http://arxiv.org/abs/2306.09862](http://arxiv.org/abs/2306.09862)

    DoubleAdapt是一个增量学习的方法，用于股票趋势预测。它利用元学习技术自动学习如何将股票数据适应到本地平稳分布空间中，从而有效地适应数据和模型，减轻分布漂移的影响。

    

    股票趋势预测是量化投资的基本任务之一，准确预测价格趋势是不可或缺的。作为一项在线服务，股票数据随时随地持续到达。使用最新数据对预测模型进行增量更新是实用而高效的，因为这些新数据可能揭示了未来股票市场中会重复出现的一些新模式。然而，由于分布漂移（即概念漂移）的挑战，股票趋势预测的增量学习仍然没有得到充分探索。随着股票市场动态演变，未来数据的分布可能会与增量数据稍微或显着地不同，从而阻碍增量更新的有效性。为了解决这一挑战，我们提出了一个利用两个适配器的端到端框架——DoubleAdapt，可以有效地适应数据和模型，以减轻分布漂移的影响。我们的关键洞察力是利用元学习技术自动学习如何将股票数据适应到本地平稳分布空间中。

    Stock trend forecasting is a fundamental task of quantitative investment where precise predictions of price trends are indispensable. As an online service, stock data continuously arrive over time. It is practical and efficient to incrementally update the forecast model with the latest data which may reveal some new patterns recurring in the future stock market. However, incremental learning for stock trend forecasting still remains under-explored due to the challenge of distribution shifts (a.k.a. concept drifts). With the stock market dynamically evolving, the distribution of future data can slightly or significantly differ from incremental data, hindering the effectiveness of incremental updates. To address this challenge, we propose DoubleAdapt, an end-to-end framework with two adapters, which can effectively adapt the data and the model to mitigate the effects of distribution shifts. Our key insight is to automatically learn how to adapt stock data into a locally stationary distri
    
[^113]: 剪枝方式提高可靠策略：一种多目标深度Q学习方法应用于重症护理

    Pruning the Way to Reliable Policies: A Multi-Objective Deep Q-Learning Approach to Critical Care. (arXiv:2306.08044v1 [cs.LG])

    [http://arxiv.org/abs/2306.08044](http://arxiv.org/abs/2306.08044)

    该论文介绍了一种深度Q学习方法，通过剪枝动作集来实现将中间生物标志物信号整合到奖励规范中，提高了重症护理策略的可靠性。

    

    大多数医疗决策具有连续性，因此，强化学习可能有望制定精确的数据驱动治疗计划。然而，该领域的主要挑战之一是主要基于死亡率的奖励函数的稀疏性，导致离线估计的稳定性降低。本研究引入了一种深度Q学习方法，能够获得更可靠的重症护理策略。该方法将相关但嘈杂的中间生物标志物信号整合到奖励规范中，同时不会损害感兴趣的主要结果（例如患者生存率）的优化。通过根据所有可用奖励对动作集进行剪枝，然后基于稀疏主要奖励，使用受限动作集进行最终模型训练，通过解离准确和近似奖励来最小化主要目标的潜在扭曲，实现了上述目标。

    Most medical treatment decisions are sequential in nature. Hence, there is substantial hope that reinforcement learning may make it possible to formulate precise data-driven treatment plans. However, a key challenge for most applications in this field is the sparse nature of primarily mortality-based reward functions, leading to decreased stability of offline estimates. In this work, we introduce a deep Q-learning approach able to obtain more reliable critical care policies. This method integrates relevant but noisy intermediate biomarker signals into the reward specification, without compromising the optimization of the main outcome of interest (e.g. patient survival). We achieve this by first pruning the action set based on all available rewards, and second training a final model based on the sparse main reward but with a restricted action set. By disentangling accurate and approximated rewards through action pruning, potential distortions of the main objective are minimized, all whi
    
[^114]: 强凸函数的无参数版本自适应梯度方法

    Parameter-free version of Adaptive Gradient Methods for Strongly-Convex Functions. (arXiv:2306.06613v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06613](http://arxiv.org/abs/2306.06613)

    本文提出了一种无参数版本的自适应梯度方法，该方法应用于强凸函数时不再依赖于参数λ和学习率η。主要思想是同时运行多个专家并将他们的预测结果合并到主算法中，这使得该算法具有O(d log T)的遗憾边界。

    

    自适应梯度方法应用于强凸函数时的最优学习率依赖于参数λ和学习率η。本文通过采用类似Metagrad的通用算法，摆脱对λ和η的依赖。主要思想是同时运行多个专家，并将他们的预测结果合并到主算法中。该主算法具有O(d log T)的遗憾边界。

    The optimal learning rate for adaptive gradient methods applied to {\lambda}-strongly convex functions relies on the parameters {\lambda} and learning rate {\eta}. In this paper, we adapt a universal algorithm along the lines of Metagrad, to get rid of this dependence on {\lambda} and {\eta}. The main idea is to concurrently run multiple experts and combine their predictions to a master algorithm. This master enjoys O(d log T) regret bounds.
    
[^115]: K-Tensors：对正半定矩阵进行聚类

    K-Tensors: Clustering Positive Semi-Definite Matrices. (arXiv:2306.06534v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06534](http://arxiv.org/abs/2306.06534)

    本文介绍了一种针对正半定矩阵的自一致性聚类算法（K-张量），通过考虑其特征结构，能够有效地将正半定矩阵进行分区。

    

    本文介绍了一种新颖的自一致性聚类算法（K-Tensors），用于基于它们的特征结构将正半定矩阵进行分区。由于正半定矩阵可以在 p≥2 的空间中表示为椭球体，因此保持它们的结构信息以进行有效的聚类至关重要。然而，传统的矩阵聚类算法常常涉及将矩阵向量化，导致关键结构信息的丢失。为了解决这个问题，我们提出了一种基于正半定矩阵结构信息的距离度量来进行聚类。这种距离度量使得聚类算法能够考虑正半定矩阵与它们在由一组正半定矩阵定义的正交向量张成的共同空间上的投影之间的差异。这是一种创新的聚类方法。

    This paper introduces a novel self-consistency clustering algorithm ($K$-Tensors) designed for {partitioning a distribution of} positive-semidefinite matrices based on their eigenstructures. As positive semi-definite matrices can be represented as ellipsoids in $\Re^p$, $p \ge 2$, it is critical to maintain their structural information to perform effective clustering. However, traditional clustering algorithms {applied to matrices} often {involve vectorization of} the matrices, resulting in a loss of essential structural information. To address this issue, we propose a distance metric {for clustering} that is specifically based on the structural information of positive semi-definite matrices. This distance metric enables the clustering algorithm to consider the differences between positive semi-definite matrices and their projections onto {a} common space spanned by \thadJulyTen{orthonormal vectors defined from a set of} positive semi-definite matrices. This innovative approach to clus
    
[^116]: “PotatoPestNet：一种基于CTInceptionV3-RS的神经网络，用于准确识别马铃薯害虫。”(arXiv：2306.06206v1 [cs.CV])

    PotatoPestNet: A CTInceptionV3-RS-Based Neural Network for Accurate Identification of Potato Pests. (arXiv:2306.06206v1 [cs.CV])

    [http://arxiv.org/abs/2306.06206](http://arxiv.org/abs/2306.06206)

    该论文提出了一种基于AI的自动马铃薯害虫识别系统PotatoPestNet，使用了八种马铃薯害虫的数据集和五种预训练转移学习模型进行模型训练，并利用随机搜索优化进行超参数调整，能够准确识别不同种类的马铃薯害虫。

    

    马铃薯是全球第三大食品作物，但由于侵袭性害虫的困扰，其产量经常遇到困难。本研究的目的是调查这些害虫的各种类型和特征，并提出一种高效的基于AI的自动马铃薯害虫识别系统PotatoPestNet。为了实现这一目标，我们筛选了包括八种马铃薯害虫的可靠数据集。我们利用五个经过定制的预训练转移学习模型：CMobileNetV2、CNASLargeNet、CXception、CDenseNet201和CInceptionV3的强大之处，提出了一个稳健的PotatoPestNet模型来准确分类马铃薯害虫。为了提高模型的性能，我们应用了各种数据增强技术，加入一个全局均值池化层，并实施适当的正则化方法。为了进一步提高模型的性能，我们利用随机搜索（RS）优化进行超参数调整。

    Potatoes are the third-largest food crop globally, but their production frequently encounters difficulties because of aggressive pest infestations. The aim of this study is to investigate the various types and characteristics of these pests and propose an efficient PotatoPestNet AI-based automatic potato pest identification system. To accomplish this, we curated a reliable dataset consisting of eight types of potato pests. We leveraged the power of transfer learning by employing five customized, pre-trained transfer learning models: CMobileNetV2, CNASLargeNet, CXception, CDenseNet201, and CInceptionV3, in proposing a robust PotatoPestNet model to accurately classify potato pests. To improve the models' performance, we applied various augmentation techniques, incorporated a global average pooling layer, and implemented proper regularization methods. To further enhance the performance of the models, we utilized random search (RS) optimization for hyperparameter tuning. This optimization 
    
[^117]: 无约束在线学习和无界损失的算法

    Unconstrained Online Learning with Unbounded Losses. (arXiv:2306.04923v1 [cs.LG])

    [http://arxiv.org/abs/2306.04923](http://arxiv.org/abs/2306.04923)

    本论文提出了一种算法，可用于解决无界域和非Lipschitz损失的在线学习问题，并提供了一个遗憾的度量，以衡量该算法的性能。此外，我们还利用该算法开发了一种新的鞍点优化算法，即使在没有有意义的曲率的情况下，也能够在无界领域中收敛于对偶间隙。最后，我们提供了一种算法，在无界域和非Lipschitz损失的情况下实现了非平凡的动态遗憾，以及相匹配的下界。

    

    在线学习算法通常需要一个或多个有界性假设：即域是有界的，损失是Lipschitz的或两者都有。在本文中，我们为具有无界域和非Lipschitz损失的在线学习开发了一个新的设置。针对该场景，我们提供了一种算法，可以保证在任何满足子梯度满足$\|g_{t}\|\le G+L\|w_{t}\|$的问题中，其遗憾的度量值$R_{T}(u)\le \tilde O(G\|u\|\sqrt{T}+L\|u\|^{2}\sqrt{T})$，并且表明除非有进一步 假设，否则该界限是不能进一步改进的。

    Algorithms for online learning typically require one or more boundedness assumptions: that the domain is bounded, that the losses are Lipschitz, or both. In this paper, we develop a new setting for online learning with unbounded domains and non-Lipschitz losses. For this setting we provide an algorithm which guarantees $R_{T}(u)\le \tilde O(G\|u\|\sqrt{T}+L\|u\|^{2}\sqrt{T})$ regret on any problem where the subgradients satisfy $\|g_{t}\|\le G+L\|w_{t}\|$, and show that this bound is unimprovable without further assumptions. We leverage this algorithm to develop new saddle-point optimization algorithms that converge in duality gap in unbounded domains, even in the absence of meaningful curvature. Finally, we provide the first algorithm achieving non-trivial dynamic regret in an unbounded domain for non-Lipschitz losses, as well as a matching lower bound. The regret of our dynamic regret algorithm automatically improves to a novel $L^{*}$ bound when the losses are smooth.
    
[^118]: 用贝叶斯推理模拟人类类人概念学习

    Modeling Human-like Concept Learning with Bayesian Inference over Natural Language. (arXiv:2306.02797v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.02797](http://arxiv.org/abs/2306.02797)

    该论文通过在自然语言中进行贝叶斯推理来模拟人类类人概念学习，使用大型语言模型作为提议分布并拟合先验以更好地模拟人类学习者，并在生成性和逻辑性概念上进行实验评估。

    

    我们通过在自然语言中进行贝叶斯推理来模拟对抽象符号概念的学习。为了高效推理，我们使用一个大型语言模型作为提议分布。我们根据人类数据拟合先验以更好地模拟人类学习者，并在生成性和逻辑性概念上进行评估。

    We model learning of abstract symbolic concepts by performing Bayesian inference over utterances in natural language. For efficient inference, we use a large language model as a proposal distribution. We fit a prior to human data to better model human learners, and evaluate on both generative and logical concepts.
    
[^119]: 通过反事实路径几何导航解释性多元宇宙

    Navigating Explanatory Multiverse Through Counterfactual Path Geometry. (arXiv:2306.02786v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02786](http://arxiv.org/abs/2306.02786)

    该论文提出了解释性多元宇宙的概念，用于导航和比较所有可能的反事实路径的几何关系。

    

    反事实解释是解释（不透明的）预测模型决策的事实标准。其生成往往受到算法和特定领域约束的影响，如基于密度的可行性和属性的（不）可变性或变化的方向性，旨在最大化其在现实生活中的实用性。除了对反事实实例本身的要求之外，已知算法可行性路径与事实数据点之间的连接，即算法可诉求，已成为重要的技术考虑因素。尽管这两个要求确保了旅程的步骤和目的地的合理性，但目前的文献忽略了这种反事实路径的多样性。为了解决这个缺点，我们引入了一种新颖的解释性多元宇宙概念，涵盖了所有可能的反事实旅程；然后展示了如何导航、推理和比较这些轨迹的几何关系。

    Counterfactual explanations are the de facto standard when tasked with interpreting decisions of (opaque) predictive models. Their generation is often subject to algorithmic and domain-specific constraints -- such as density-based feasibility and attribute (im)mutability or directionality of change -- that aim to maximise their real-life utility. In addition to desiderata with respect to the counterfactual instance itself, existence of a viable path connecting it with the factual data point, known as algorithmic recourse, has become an important technical consideration. While both of these requirements ensure that the steps of the journey as well as its destination are admissible, current literature neglects the multiplicity of such counterfactual paths. To address this shortcoming we introduce the novel concept of explanatory multiverse that encompasses all the possible counterfactual journeys; we then show how to navigate, reason about and compare the geometry of these trajectories -
    
[^120]: 交通理解的情境推理研究

    A Study of Situational Reasoning for Traffic Understanding. (arXiv:2306.02520v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.02520](http://arxiv.org/abs/2306.02520)

    本研究提出了三个新的基于文本的交通领域情境推理任务，旨在评估语言模型在情境决策、事件因果关系推理和解决人类驾驶考试方面的能力。研究采用了四种知识增强方法，具有潜力在不同语言推理任务中实现模型的泛化能力。

    

    智能交通监控(ITMo)技术有潜力改善道路安全/安全性，实现智能城市基础设施。了解交通情况需要将感知信息与领域特定和因果常识知识复杂融合。尽管之前的工作已经为交通监控提供了基准和方法，但模型能否有效地对齐这些信息来源并在新场景中推理仍不清楚。为了解决这种评估差距，我们设计了三个用于交通领域情境推理的新型文本任务：i) BDD-QA，评估语言模型(LMs)执行情境决策的能力，ii) TV-QA，评估LMs推理复杂事件因果关系的能力，iii) HDT-QA，评估模型解决人类驾驶考试的能力。我们采用了之前工作中已经显示出语言推理任务通用性的四种知识增强方法。

    Intelligent Traffic Monitoring (ITMo) technologies hold the potential for improving road safety/security and for enabling smart city infrastructure. Understanding traffic situations requires a complex fusion of perceptual information with domain-specific and causal commonsense knowledge. Whereas prior work has provided benchmarks and methods for traffic monitoring, it remains unclear whether models can effectively align these information sources and reason in novel scenarios. To address this assessment gap, we devise three novel text-based tasks for situational reasoning in the traffic domain: i) BDD-QA, which evaluates the ability of Language Models (LMs) to perform situational decision-making, ii) TV-QA, which assesses LMs' abilities to reason about complex event causality, and iii) HDT-QA, which evaluates the ability of models to solve human driving exams. We adopt four knowledge-enhanced methods that have shown generalization capability across language reasoning tasks in prior work
    
[^121]: DWT-CompCNN：用于高吞吐量JPEG 2000压缩文档的深度图像分类网络

    DWT-CompCNN: Deep Image Classification Network for High Throughput JPEG 2000 Compressed Documents. (arXiv:2306.01359v1 [cs.CV])

    [http://arxiv.org/abs/2306.01359](http://arxiv.org/abs/2306.01359)

    这篇论文提出了一种名为DWT-CompCNN的深度学习模型，它可以直接对使用HTJ2K算法压缩的文档进行分类，从而提高计算效率。

    

    对于任何包含文档图像的数字应用程序，如检索，文档图像的分类成为必要的阶段。传统上，为了达到这个目的，文档的完整版本，即未压缩的文档图像构成输入数据集，这会因数据量大而带来威胁。因此，如果可以使用文档的压缩表示（在部分解压缩的情况下），直接完成相同的分类任务以使整个过程计算效率更高，那将会是一项创新。本研究提出了一种新颖的深度学习模型DWT-CompCNN，用于使用高吞吐量JPEG 2000（HTJ2K）算法压缩的文档的分类。所提出的DWT-CompCNN包括五个卷积层，卷积核大小分别为16、32、64、128和256用于从提取的小波系数中提高学习能力。

    For any digital application with document images such as retrieval, the classification of document images becomes an essential stage. Conventionally for the purpose, the full versions of the documents, that is the uncompressed document images make the input dataset, which poses a threat due to the big volume required to accommodate the full versions of the documents. Therefore, it would be novel, if the same classification task could be accomplished directly (with some partial decompression) with the compressed representation of documents in order to make the whole process computationally more efficient. In this research work, a novel deep learning model, DWT CompCNN is proposed for classification of documents that are compressed using High Throughput JPEG 2000 (HTJ2K) algorithm. The proposed DWT-CompCNN comprises of five convolutional layers with filter sizes of 16, 32, 64, 128, and 256 consecutively for each increasing layer to improve learning from the wavelet coefficients extracted
    
[^122]: 面向变化环境的公平解缠在线学习

    Towards Fair Disentangled Online Learning for Changing Environments. (arXiv:2306.01007v1 [cs.LG])

    [http://arxiv.org/abs/2306.01007](http://arxiv.org/abs/2306.01007)

    本论文提出了一种面向变化环境的在线学习算法，该算法通过将模型参数划分为环境不变部分和环境特定部分，从而实现了数据公平性。通过大量的实验，证明了该算法的有效性。

    

    在面对变化环境的在线学习问题中，数据按时间顺序一个接一个地接收，并且它们的分布假设可能经常变化。虽然现有方法通过提供对动态遗憾或自适应遗憾的严格界限来展示其学习算法的有效性，但它们大多完全忽略了带有模型公平性的学习，其定义为跨不同子族群（例如，种族和性别）的统计平等。另一个缺点是，在适应新环境时，在线学习者需要使用全局更改更新模型参数，这是昂贵和低效的。受到稀疏机制转移假设的启发，我们声称在线学习中的变化环境可以归因于特定于环境的部分学习参数的部分变化，其余部分保持不变。为此，本文在假设从不同子人群收集的数据具有公平的模型表示的前提下，提出了一种新算法，将模型参数分为环境不变部分和环境特定部分。我们推导了每个子人群模型表示公正性的统计保证，并证明了我们提出的算法的收敛速率。此外，我们通过对合成和真实世界数据集的广泛实验证明了我们方法的有效性。

    In the problem of online learning for changing environments, data are sequentially received one after another over time, and their distribution assumptions may vary frequently. Although existing methods demonstrate the effectiveness of their learning algorithms by providing a tight bound on either dynamic regret or adaptive regret, most of them completely ignore learning with model fairness, defined as the statistical parity across different sub-population (e.g., race and gender). Another drawback is that when adapting to a new environment, an online learner needs to update model parameters with a global change, which is costly and inefficient. Inspired by the sparse mechanism shift hypothesis, we claim that changing environments in online learning can be attributed to partial changes in learned parameters that are specific to environments and the rest remain invariant to changing environments. To this end, in this paper, we propose a novel algorithm under the assumption that data coll
    
[^123]: 利用aggVAE进行深度学习和MCMC以处理行政边界变化：以肯尼亚的疟疾患病率为例

    Deep learning and MCMC with aggVAE for shifting administrative boundaries: mapping malaria prevalence in Kenya. (arXiv:2305.19779v1 [cs.LG])

    [http://arxiv.org/abs/2305.19779](http://arxiv.org/abs/2305.19779)

    本研究提出了一种利用aggVAE进行深度学习和MCMC处理行政边界变化的解决方案，可以更准确地映射以县为层级的聚合级别数据，并处理行政边界的变化，相比最先进的模型表现更好。

    

    基于模型的疾病映射是公共卫生和疾病监测中基本的政策信息工具，分层贝叶斯模型是当前最先进的方法。当处理区域数据，如行政区划单位（例如县或省）的聚合数据时，常用的模型依赖于区域单元的相邻结构以考虑空间相关性。疾病监测系统的目标是随时间跟踪疾病结果，但在危机情况下（例如政治变化导致行政边界更改），这将带来挑战。我们提出了一种新颖、实用和易于实施的解决方案，该方案依赖于组合深层生成模型和全贝叶斯推断。我们建立在现有的变分自编码器(VAE) 工作上，并展示我们提出的聚合VAE(aggVAE)体系结构可用于在以县为层级的聚合级别处理数据，以映射肯尼亚的疟疾患病率。我们的模型可以以连续的方式考虑空间相关性，而不依赖于相邻性假设，并且能够处理行政边界的变化。结果表明，相比最先进的模型，我们的模型表现出更好的性能和更准确的疟疾患病率映射。

    Model-based disease mapping remains a fundamental policy-informing tool in public health and disease surveillance with hierarchical Bayesian models being the current state-of-the-art approach. When working with areal data, e.g. aggregates at the administrative unit level such as district or province, routinely used models rely on the adjacency structure of areal units to account for spatial correlations. The goal of disease surveillance systems is to track disease outcomes over time, but this provides challenging in situations of crises, such as political changes, leading to changes of administrative boundaries. Kenya is an example of such country. Moreover, adjacency-based approach ignores the continuous nature of spatial processes and cannot solve the change-of-support problem, i.e. when administrative boundaries change. We present a novel, practical, and easy to implement solution relying on a methodology combining deep generative modelling and fully Bayesian inference. We build on 
    
[^124]: W-procer: 基于加权原型对比学习的医学少样本命名实体识别

    W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition. (arXiv:2305.18624v1 [cs.CL])

    [http://arxiv.org/abs/2305.18624](http://arxiv.org/abs/2305.18624)

    W-procer是一种基于加权原型对比学习的医学少样本命名实体识别方法，在构建基于原型的对比损失和加权网络方面具有创新性，优于现有的最先进方法。

    

    对比学习已成为少样本命名实体识别（NER）的一种受欢迎的解决方案。传统配置力求减少具有相同标签的标记之间的距离，并增加具有不同标签的标记之间的距离。然而，在医学领域中存在大量被注释为“O”（即“OUTSIDE”）的实体，并且它们不希望被推离到当前对比学习方法标记为“O”以外的其他实体，这种设定效果不佳，可能会得出含有噪声原型标签的语义表示，尽管存在许多“O”标签实体与有标签实体相关。为解决这个挑战，我们提出了一种名为医学少样本命名实体识别中基于加权原型的对比学习方法（W-PROCER）。我们的方法主要围绕构建基于原型的对比损失和加权网络展开。这些组件在协助在医学领域中的迁移学习方面发挥了至关重要的作用。在实验中，我们将W-PROCER应用于一个公共的医学数据集，并展示了其相对于现有的最先进方法的优异表现。

    Contrastive learning has become a popular solution for few-shot Name Entity Recognization (NER). The conventional configuration strives to reduce the distance between tokens with the same labels and increase the distance between tokens with different labels. The effect of this setup may, however, in the medical domain, there are a lot of entities annotated as OUTSIDE (O), and they are undesirably pushed apart to other entities that are not labeled as OUTSIDE (O) by the current contrastive learning method end up with a noisy prototype for the semantic representation of the label, though there are many OUTSIDE (O) labeled entities are relevant to the labeled entities. To address this challenge, we propose a novel method named Weighted Prototypical Contrastive Learning for Medical Few Shot Named Entity Recognization (W-PROCER). Our approach primarily revolves around constructing the prototype-based contractive loss and weighting network. These components play a crucial role in assisting t
    
[^125]: 《带差分隐私的Gboard语言模型联合学习》

    Federated Learning of Gboard Language Models with Differential Privacy. (arXiv:2305.18465v1 [cs.LG])

    [http://arxiv.org/abs/2305.18465](http://arxiv.org/abs/2305.18465)

    本文讨论了在Gboard中使用联合学习和差分隐私(DP)训练语言模型(LMs)的方法，提出了新的客户参与标准，在实现有意义的形式DP保证的同时提供了有利的隐私-效用交换。在对公共数据进行预训练的基础上，我们训练并部署了超过20个LMs以实现高效用和$\rho-$zCDP隐私保证。

    

    本文在谷歌键盘(Gboard)中使用联合学习和差分隐私(DP)训练语言模型(LMs)。我们应用DP-FTRL算法，在不要求对客户设备进行均匀采样的情况下实现了有意义的形式 DP 保证。为了提供有利的隐私-效用交换，我们引入了新的客户参与标准，并讨论了其在大规模系统中的配置影响。我们展示了如何将基于分位数的剪切估计与DP-FTRL相结合，以在训练过程中自适应选择剪切范数或减少超参数调整以准备训练。借助于对公共数据的预训练，我们训练并部署了超过20个Gboard LM，这些模型在$\rho \in (0.2, 2)$下实现了高效用和$\rho-$zCDP隐私保证，其中两个模型还使用了安全合并。

    We train language models (LMs) with federated learning (FL) and differential privacy (DP) in the Google Keyboard (Gboard). We apply the DP-Follow-the-Regularized-Leader (DP-FTRL)~\citep{kairouz21b} algorithm to achieve meaningfully formal DP guarantees without requiring uniform sampling of client devices. To provide favorable privacy-utility trade-offs, we introduce a new client participation criterion and discuss the implication of its configuration in large scale systems. We show how quantile-based clip estimation~\citep{andrew2019differentially} can be combined with DP-FTRL to adaptively choose the clip norm during training or reduce the hyperparameter tuning in preparation for training. With the help of pretraining on public data, we train and deploy more than twenty Gboard LMs that achieve high utility and $\rho-$zCDP privacy guarantees with $\rho \in (0.2, 2)$, with two models additionally trained with secure aggregation~\citep{bonawitz2017practical}. We are happy to announce tha
    
[^126]: 利用人类反馈在机器人群体中发现新的 emergent behaviors

    Leveraging Human Feedback to Evolve and Discover Novel Emergent Behaviors in Robot Swarms. (arXiv:2305.16148v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2305.16148](http://arxiv.org/abs/2305.16148)

    该论文提出了一种利用人类反馈的方法来发现机器人群体中的新的 emergent behaviors。通过学习群体行为的相似性，结合新奇搜索和聚类技术，自动发现可能出现的群体行为，同时引入启发式方法提高搜索效率。该方法在仿真场景中进行了测试。

    

    机器人群体经常表现出令人着迷的 emergent behaviors，然而很难预测在特定的多智能体系统下会出现哪些群体行为。我们致力于通过有效地利用人类输入来自动发现可能产生的 collective behaviors 的分类法，而不需要人类事先知道哪些行为是有趣或可能的。我们提出的方法通过自监督学习和人机交互查询学习了一个群体行为的相似度空间，来适应用户的偏好。我们将学习到的相似度度量与新奇搜索和聚类相结合，探索并分类可能的群体行为空间。我们还提出了一些通用启发式的方法，通过优先考虑可能会产生有趣 emergent behaviors 的机器人控制器，提高了新奇搜索的效率。我们在两个仿真场景中测试了我们的方法。

    Robot swarms often exhibit emergent behaviors that are fascinating to observe; however, it is often difficult to predict what swarm behaviors can emerge under a given set of agent capabilities. We seek to efficiently leverage human input to automatically discover a taxonomy of collective behaviors that can emerge from a particular multi-agent system, without requiring the human to know beforehand what behaviors are interesting or even possible. Our proposed approach adapts to user preferences by learning a similarity space over swarm collective behaviors using self-supervised learning and human-in-the-loop queries. We combine our learned similarity metric with novelty search and clustering to explore and categorize the space of possible swarm behaviors. We also propose several general-purpose heuristics that improve the efficiency of our novelty search by prioritizing robot controllers that are likely to lead to interesting emergent behaviors. We test our approach in simulation on two 
    
[^127]: 最优预条件和费舍尔自适应 Langevin 采样

    Optimal Preconditioning and Fisher Adaptive Langevin Sampling. (arXiv:2305.14442v1 [stat.ML])

    [http://arxiv.org/abs/2305.14442](http://arxiv.org/abs/2305.14442)

    通过最优预条件和费舍尔自适应 Langevin 采样，提出了一种计算有效且在高维中非常强健的自适应 MCMC 方案。

    

    我们通过分析最大化预期平方跳跃距离，为 Langevin 扩散定义了最优预条件。这导致最优预条件为反费舍尔信息协方差矩阵，其中协方差矩阵是在目标下平均对数目标梯度的外积。我们将此结果应用于 Metropolis 调整 Langevin 算法 (MALA)，并推导出一种从算法运行产生的梯度历史中学习预条件的计算有效的自适应 MCMC 方案。我们在几个实验中展示了所提出的算法在高维中非常强健，并且明显优于其他方法，包括使用标准自适应 MCMC 学习预条件和位置相关的 Riemann 流形 MALA 采样器的密切相关的自适应 MALA 方案。

    We define an optimal preconditioning for the Langevin diffusion by analytically maximizing the expected squared jumped distance. This yields as the optimal preconditioning an inverse Fisher information covariance matrix, where the covariance matrix is computed as the outer product of log target gradients averaged under the target. We apply this result to the Metropolis adjusted Langevin algorithm (MALA) and derive a computationally efficient adaptive MCMC scheme that learns the preconditioning from the history of gradients produced as the algorithm runs. We show in several experiments that the proposed algorithm is very robust in high dimensions and significantly outperforms other methods, including a closely related adaptive MALA scheme that learns the preconditioning with standard adaptive MCMC as well as the position-dependent Riemannian manifold MALA sampler.
    
[^128]: 对机器学习在持续集成中的应用的系统性文献综述

    Systematic Literature Review on Application of Machine Learning in Continuous Integration. (arXiv:2305.12695v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2305.12695](http://arxiv.org/abs/2305.12695)

    本文对过去22年来机器学习在持续集成中的应用进行了系统性综述，识别和描述了相关技术和方法，包括数据工程、特征工程、超参数调优等。同时，总结了持续集成测试的阶段、数据来源和特征类型，并提出了评估方法和指标。

    

    本研究对过去22年来机器学习方法在持续集成领域的文献进行了系统回顾。研究旨在识别和描述机器学习解决方案中使用的技术，并分析数据工程、特征工程、超参数调优、机器学习模型、评估方法和评估指标等各个方面。本文展示了持续集成测试的各个阶段、它们之间的联系以及在训练机器学习模型时使用的技术。本文还提供了九种数据来源和选定研究中数据准备的四个步骤。此外，通过对选定研究的主题分析，我们识别出四种特征类型和九个数据特征子集。此外，本文还展示了五种选择和调优超参数的方法。另外，我们总结了文献中使用的评估方法，并识别出了十五种不同的评估指标。

    This research conducted a systematic review of the literature on machine learning (ML)-based methods in the context of Continuous Integration (CI) over the past 22 years. The study aimed to identify and describe the techniques used in ML-based solutions for CI and analyzed various aspects such as data engineering, feature engineering, hyper-parameter tuning, ML models, evaluation methods, and metrics. In this paper, we have depicted the phases of CI testing, the connection between them, and the employed techniques in training the ML method phases. We presented nine types of data sources and four taken steps in the selected studies for preparing the data. Also, we identified four feature types and nine subsets of data features through thematic analysis of the selected studies. Besides, five methods for selecting and tuning the hyper-parameters are shown. In addition, we summarised the evaluation methods used in the literature and identified fifteen different metrics. The most commonly u
    
[^129]: Waymo开放模拟代理挑战赛

    The Waymo Open Sim Agents Challenge. (arXiv:2305.12032v1 [cs.CV])

    [http://arxiv.org/abs/2305.12032](http://arxiv.org/abs/2305.12032)

    Waymo开放模拟代理挑战赛提出使用真实、互动的智能体仿真以促进自动驾驶行为模型的评估和训练，是该领域的首个公开挑战赛，旨在推动逼真模拟器的设计。

    

    本文定义了Waymo开放模拟代理挑战赛(WOSAC)。通过与真实、互动的智能体进行仿真是自动驾驶软件开发的关键任务。WOSAC是第一个公开的挑战赛，旨在解决该任务并提出相应的评估指标。该挑战的目标是激发设计逼真模拟器的兴趣，以用于评估和训练自动驾驶的行为模型。我们概述了评估方法，并展示了几种基准仿真代理方法的初步结果。

    In this work, we define the Waymo Open Sim Agents Challenge (WOSAC). Simulation with realistic, interactive agents represents a key task for autonomous vehicle software development. WOSAC is the first public challenge to tackle this task and propose corresponding metrics. The goal of the challenge is to stimulate the design of realistic simulators that can be used to evaluate and train a behavior model for autonomous driving. We outline our evaluation methodology and present preliminary results for a number of different baseline simulation agent methods.
    
[^130]: FedDWA: 个性化联邦学习与动态权重调整

    FedDWA: Personalized Federated Learning with Online Weight Adjustment. (arXiv:2305.06124v1 [cs.LG])

    [http://arxiv.org/abs/2305.06124](http://arxiv.org/abs/2305.06124)

    本文提出了一种个性化联邦学习算法，名为FedDWA，采用动态权重调整来保护数据隐私并以更少的通信开销捕捉客户之间的相似性，能够训练高精度和高效的个性化模型。

    

    与传统的联邦学习不同，个性化联邦学习（PFL）能够根据每个客户端的独特需求来训练定制化模型。主流方法是采用一种加权聚合方法来生成个性化模型，其中权重是由不同客户端之间的损失值或模型参数确定的。然而，这种方法要求客户端下载其他模型，不仅增加了通信流量，而且可能侵犯数据隐私。我们在本文中提出了一种新的PFL算法，称为FedDWA（带动态权重调整的联邦学习），来解决上述问题，该算法利用参数服务器（PS）根据从客户端收集的模型计算个性化聚合权重。这样，FedDWA可以以更少的通信开销捕捉客户之间的相似性。我们将PFL问题制定为一种优化问题，并通过引入动态权重调整机制设计了一种新算法。FedDWA能够学习高精度和高效的个性化模型，同时保护数据隐私。我们在综合合成和真实数据集上进行了广泛的实验，证明了FedDWA的有效性。

    Different from conventional federated learning, personalized federated learning (PFL) is able to train a customized model for each individual client according to its unique requirement. The mainstream approach is to adopt a kind of weighted aggregation method to generate personalized models, in which weights are determined by the loss value or model parameters among different clients. However, such kinds of methods require clients to download others' models. It not only sheer increases communication traffic but also potentially infringes data privacy. In this paper, we propose a new PFL algorithm called \emph{FedDWA (Federated Learning with Dynamic Weight Adjustment)} to address the above problem, which leverages the parameter server (PS) to compute personalized aggregation weights based on collected models from clients. In this way, FedDWA can capture similarities between clients with much less communication overhead. More specifically, we formulate the PFL problem as an optimization 
    
[^131]: 核电站燃料优化的强化学习算法评估

    Assessment of Reinforcement Learning Algorithms for Nuclear Power Plant Fuel Optimization. (arXiv:2305.05812v1 [cs.LG])

    [http://arxiv.org/abs/2305.05812](http://arxiv.org/abs/2305.05812)

    本文提出了一种基于深度强化学习的方案，以解决核电站燃料优化问题，能够提高核电站的性能和安全。

    

    自商业核能产业诞生以来，人们一直在研究核燃料装载模式的优化问题。它具有多个目标和约束条件，候选模式数量非常高，因此无法明确解决。不同的核能公用事业和供应商使用随机优化方法来执行燃料循环重装设计，但手动设计的解决方案仍然是主流。为了改进现有状态的燃料循环重装模式，我们旨在创建一种尽可能可扩展的方法，符合设计师的性能和安全目标。为了帮助完成此任务，我们利用了深度强化学习（RL），特别是近端策略优化。最近，RL在游戏中的成功给它带来了强劲的推动。本文阐述了这种方法的基础，并提议研究影响RL算法性能的几个超参数的行为。

    The nuclear fuel loading pattern optimization problem has been studied since the dawn of the commercial nuclear energy industry. It is characterized by multiple objectives and constraints, with a very high number of candidate patterns, which makes it impossible to solve explicitly. Stochastic optimization methodologies are used by different nuclear utilities and vendors to perform fuel cycle reload design. Nevertheless, hand-designed solutions continue to be the prevalent method in the industry. To improve the state-of-the-art core reload patterns, we aim to create a method as scalable as possible, that agrees with the designer's goal of performance and safety. To help in this task Deep Reinforcement Learning (RL), in particular, Proximal Policy Optimization is leveraged. RL has recently experienced a strong impetus from its successes applied to games. This paper lays out the foundation of this method and proposes to study the behavior of several hyper-parameters that influence the RL 
    
[^132]: GPT用于半自动化数据科学：引入CAAFE实现上下文感知自动特征工程

    GPT for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering. (arXiv:2305.03403v1 [cs.AI])

    [http://arxiv.org/abs/2305.03403](http://arxiv.org/abs/2305.03403)

    介绍了一种名为CAAFE的上下文感知自动特征工程方法，它利用大型语言模型根据数据集描述生成更多具有语义意义的特征，能够提高大多数数据集的性能，平均ROC AUC表现提高至0.822。

    

    随着自动化机器学习（AutoML）领域的发展，将领域知识纳入这些系统中变得越来越重要。我们利用大型语言模型（LLMs）的强大功能提出了一种方法来实现这一目标。具体地，我们介绍了一种用于表格数据的特征工程方法，名为上下文感知自动特征工程（CAAFE），它利用LLM根据数据集的描述生成更多具有语义意义的特征。该方法产生用于创建新特征的Python代码，并提供生成特征的效用说明。尽管方法论上很简单，但CAAFE提高了14个数据集中11个数据集的性能，与2个数据集并列，只有1个数据集性能下降，从而使所有数据集的平均ROC AUC表现从0.798提升至0.822。对于所评估的数据集，这一改进与使用随机森林（AUC 0.782）代替逻辑回归（AUC 0.754）所获得的平均改进相似。此外，

    As the field of automated machine learning (AutoML) advances, it becomes increasingly important to include domain knowledge within these systems. We present an approach for doing so by harnessing the power of large language models (LLMs). Specifically, we introduce Context-Aware Automated Feature Engineering (CAAFE), a feature engineering method for tabular datasets that utilizes an LLM to generate additional semantically meaningful features for tabular datasets based on their descriptions. The method produces both Python code for creating new features and explanations for the utility of the generated features.  Despite being methodologically simple, CAAFE enhances performance on 11 out of 14 datasets, ties on 2 and looses on 1 - boosting mean ROC AUC performance from 0.798 to 0.822 across all datasets. On the evaluated datasets, this improvement is similar to the average improvement achieved by using a random forest (AUC 0.782) instead of logistic regression (AUC 0.754).  Furthermore,
    
[^133]: 子采样岭回归集成：等效性和广义交叉验证

    Subsample Ridge Ensembles: Equivalences and Generalized Cross-Validation. (arXiv:2304.13016v1 [math.ST])

    [http://arxiv.org/abs/2304.13016](http://arxiv.org/abs/2304.13016)

    研究了比例渐近情形下的子采样岭回归集成，证明了最优全岭回归集成的风险与最优岭预测器的风险相匹配，并证明了GCV在估计岭回归集合的预测风险方面的强一致性。

    

    我们研究了比例渐近情形下的子采样岭回归集成，其中特征大小与样本大小成比例增长，使得它们的比率收敛到一个常数。通过分析岭回归集合的平方预测风险作为显式惩罚$\lambda$和极限子样本方面比$\phi_s$（特征大小与子样本大小的比率）的函数，我们表征了在任何可达风险下的$(\lambda, \phi_s)$-平面上的轮廓。因此，我们证明最优全岭回归集成（适合于所有可能的子样本）的风险与最优岭预测器的风险相匹配。此外，我们证明对于估计岭回归集合的预测风险，基于广义交叉验证（GCV）的子样本大小强一致性。这允许无需样本拆分基于GCV优化全局岭回归集成，并产生一个风险与最优岭回归风险相匹配的预测器。

    We study subsampling-based ridge ensembles in the proportional asymptotics regime, where the feature size grows proportionally with the sample size such that their ratio converges to a constant. By analyzing the squared prediction risk of ridge ensembles as a function of the explicit penalty $\lambda$ and the limiting subsample aspect ratio $\phi_s$ (the ratio of the feature size to the subsample size), we characterize contours in the $(\lambda, \phi_s)$-plane at any achievable risk. As a consequence, we prove that the risk of the optimal full ridgeless ensemble (fitted on all possible subsamples) matches that of the optimal ridge predictor. In addition, we prove strong uniform consistency of generalized cross-validation (GCV) over the subsample sizes for estimating the prediction risk of ridge ensembles. This allows for GCV-based tuning of full ridgeless ensembles without sample splitting and yields a predictor whose risk matches optimal ridge risk.
    
[^134]: CAFIN: 基于节点中心性的公平性增强进程的无监督图表示学习方法

    CAFIN: Centrality Aware Fairness inducing IN-processing for Unsupervised Representation Learning on Graphs. (arXiv:2304.04391v1 [cs.LG])

    [http://arxiv.org/abs/2304.04391](http://arxiv.org/abs/2304.04391)

    CAFIN是一种基于节点中心性的公平性增强进程技术，用于无监督学习的图表示学习方法中。实验结果表明，CAFIN在提供最优公平结果的同时，具有竞争力或更好的下游任务性能。

    

    由于所学嵌入的紧凑性和丰富性以及未标记图数据的丰富性，无监督学习的图表示在(大型)图上已经受到研究界的重视。当这些节点表示被部署时，必须使用适当的公平性约束条件生成以减少它们对下游任务造成的偏差。因此，对于特定的下游任务，已经调查了图学习算法的群体和个体公平性概念。这些公平性概念的主要局限性是没有考虑连接模式在图中导致的不同节点影响(或中心性能量)。在本文中，我们为归纳图表示学习算法设计了一个基于中心性的公平框架。我们提出了CAFIN（Centrality Aware Fairness inducing IN-processing），一种利用图结构改进GraphSAGE表示的进程技术——无监督图学习文献中的一种流行框架。对真实世界数据集的广泛实验表明，CAFIN在提供具有竞争力或更好的下游任务性能的同时，实现了最先进的公平结果。

    Unsupervised representation learning on (large) graphs has received significant attention in the research community due to the compactness and richness of the learned embeddings and the abundance of unlabelled graph data. When deployed, these node representations must be generated with appropriate fairness constraints to minimize bias induced by them on downstream tasks. Consequently, group and individual fairness notions for graph learning algorithms have been investigated for specific downstream tasks. One major limitation of these fairness notions is that they do not consider the connectivity patterns in the graph leading to varied node influence (or centrality power). In this paper, we design a centrality-aware fairness framework for inductive graph representation learning algorithms. We propose CAFIN (Centrality Aware Fairness inducing IN-processing), an in-processing technique that leverages graph structure to improve GraphSAGE's representations - a popular framework in the unsup
    
[^135]: 神经网络的最后一层公平微调简单有效

    Last-Layer Fairness Fine-tuning is Simple and Effective for Neural Networks. (arXiv:2304.03935v1 [cs.LG])

    [http://arxiv.org/abs/2304.03935](http://arxiv.org/abs/2304.03935)

    本文提出了一种新颖的公平性微调神经网络的框架，利用已预训练的神经网络和不关注公平性的损失微调神经网络的最后一层。实验证明该方法在基准数据集上实现了高水平的公平性，同时保留标准性能指标。

    

    随着机器学习在现代数据科学中得到广泛应用，算法公平性已成为一个重要关注点，并且提出了多种公平性标准。其中，在学习过程中施加公平性约束，即进行处理公平性训练，已成为一种流行的训练方法，因为与后处理方法不同，它们不需要在测试期间访问敏感属性。虽然在经典机器学习模型中对施加公平性约束进行了广泛研究，但这些技术对深度神经网络的影响仍不清楚。最近的研究表明，在目标函数中添加公平性约束会导致大型模型过度拟合公平性标准，如何解决这一挑战是一个重要的开放问题。为了解决这个问题，我们利用预训练和微调的智慧和能力，并开发了一种简单而新颖的框架来训练公平的神经网络。我们的方法是在满足感兴趣的公平性约束的少量标记数据上，使用不关注公平性的损失微调已预训练的神经网络。实验证明，我们的方法在基准数据集上实现了高水平的公平性，而不牺牲标准性能指标。此外，我们证明了我们的方法足够灵活，可以纳入任何感兴趣的公平性约束，并且对于高维数据可以进行几乎没有额外计算的扩展。

    As machine learning has been deployed ubiquitously across applications in modern data science, algorithmic fairness has become a great concern and varieties of fairness criteria have been proposed. Among them, imposing fairness constraints during learning, i.e. in-processing fair training, has been a popular type of training method because they don't require accessing sensitive attributes during test time in contrast to post-processing methods. Although imposing fairness constraints have been studied extensively for classical machine learning models, the effect these techniques have on deep neural networks is still unclear. Recent research has shown that adding fairness constraints to the objective function leads to severe over-fitting to fairness criteria in large models, and how to solve this challenge is an important open question. To address this challenge, we leverage the wisdom and power of pre-training and fine-tuning and develop a simple but novel framework to train fair neural
    
[^136]: Torch-Choice: 用Python实现大规模选择建模的PyTorch包

    Torch-Choice: A PyTorch Package for Large-Scale Choice Modelling with Python. (arXiv:2304.01906v1 [cs.LG])

    [http://arxiv.org/abs/2304.01906](http://arxiv.org/abs/2304.01906)

    本文介绍了一款名为 Torch-Choice 的 PyTorch 软件包，用于管理数据库、构建多项式Logit和嵌套Logit模型，并支持GPU加速，具有灵活性和高效性。

    

    $\texttt{torch-choice}$ 是一款开源软件包，使用Python和PyTorch实现灵活、快速的选择建模。它提供了 $\texttt{ChoiceDataset}$ 数据结构，以便灵活而高效地管理数据库。本文演示了如何从各种格式的数据库中构建 $\texttt{ChoiceDataset}$，并展示了 $\texttt{ChoiceDataset}$ 的各种功能。该软件包实现了两种常用的模型: 多项式Logit和嵌套Logit模型，并支持模型估计期间的正则化。该软件包还支持使用GPU进行估计，使其可以扩展到大规模数据集而且在计算上更高效。模型可以使用R风格的公式字符串或Python字典进行初始化。最后，我们比较了 $\texttt{torch-choice}$ 和 R中的 $\texttt{mlogit}$ 在以下几个方面的计算效率: (1) 观测数增加时，(2) 协变量个数增加时， (3) 测试数升高时。

    The $\texttt{torch-choice}$ is an open-source library for flexible, fast choice modeling with Python and PyTorch. $\texttt{torch-choice}$ provides a $\texttt{ChoiceDataset}$ data structure to manage databases flexibly and memory-efficiently. The paper demonstrates constructing a $\texttt{ChoiceDataset}$ from databases of various formats and functionalities of $\texttt{ChoiceDataset}$. The package implements two widely used models, namely the multinomial logit and nested logit models, and supports regularization during model estimation. The package incorporates the option to take advantage of GPUs for estimation, allowing it to scale to massive datasets while being computationally efficient. Models can be initialized using either R-style formula strings or Python dictionaries. We conclude with a comparison of the computational efficiencies of $\texttt{torch-choice}$ and $\texttt{mlogit}$ in R as (1) the number of observations increases, (2) the number of covariates increases, and (3) th
    
[^137]: 学习相似的线性表示：适应性、极小化、以及稳健性

    Learning from Similar Linear Representations: Adaptivity, Minimaxity, and Robustness. (arXiv:2303.17765v1 [stat.ML])

    [http://arxiv.org/abs/2303.17765](http://arxiv.org/abs/2303.17765)

    本文提出了两种算法，适应相似性结构并对异常值任务具有稳健性，适用于表示多任务学习和迁移学习设置。

    

    表示多任务学习和迁移学习在实践中取得了巨大的成功，然而对这些方法的理论理解仍然欠缺。本文旨在理解从具有相似但并非完全相同的线性表示的任务中学习，同时处理异常值任务。我们提出了两种算法，适应相似性结构并对异常值任务具有稳健性，适用于表示多任务学习和迁移学习设置，我们的算法在单任务或仅目标学习时表现优异。

    Representation multi-task learning (MTL) and transfer learning (TL) have achieved tremendous success in practice. However, the theoretical understanding of these methods is still lacking. Most existing theoretical works focus on cases where all tasks share the same representation, and claim that MTL and TL almost always improve performance. However, as the number of tasks grow, assuming all tasks share the same representation is unrealistic. Also, this does not always match empirical findings, which suggest that a shared representation may not necessarily improve single-task or target-only learning performance. In this paper, we aim to understand how to learn from tasks with \textit{similar but not exactly the same} linear representations, while dealing with outlier tasks. We propose two algorithms that are \textit{adaptive} to the similarity structure and \textit{robust} to outlier tasks under both MTL and TL settings. Our algorithms outperform single-task or target-only learning when
    
[^138]: 《桥接模仿学习和在线强化学习：一个乐观的故事》

    Bridging Imitation and Online Reinforcement Learning: An Optimistic Tale. (arXiv:2303.11369v1 [cs.LG])

    [http://arxiv.org/abs/2303.11369](http://arxiv.org/abs/2303.11369)

    本文提出了两种算法，iPSRL和iRLSVI，旨在解决给定离线演示数据集的问题，可以显著减少强化学习中的遗憾，桥接了在线 RL 和模仿学习。

    

    本文研究以下问题：给定一个来自不完美专家的离线演示数据集，最好的方式是什么来利用它来引导 MDP 中的在线学习表现。我们首先提出了一种基于知情后验采样的 RL（iPSRL）算法，它使用离线数据集和专家的行为策略信息来生成离线数据集。如果专家足够能干，则其累积贝叶斯遗憾在离线数据集大小 N 下会指数快速下降到零。由于该算法计算时间复杂度过高，我们随后提出了 iRLSVI 算法，可看作是在线 RL 和模仿学习的 RLSVI 算法的组合。我们的实验结果表明，与两个基准（没有离线数据，或使用离线数据集但不利用生成策略信息）相比，所提出的 iRLSVI 算法能够显著减少遗憾。我们的算法桥接了在线 RL 和模仿学习。

    In this paper, we address the following problem: Given an offline demonstration dataset from an imperfect expert, what is the best way to leverage it to bootstrap online learning performance in MDPs. We first propose an Informed Posterior Sampling-based RL (iPSRL) algorithm that uses the offline dataset, and information about the expert's behavioral policy used to generate the offline dataset. Its cumulative Bayesian regret goes down to zero exponentially fast in N, the offline dataset size if the expert is competent enough. Since this algorithm is computationally impractical, we then propose the iRLSVI algorithm that can be seen as a combination of the RLSVI algorithm for online RL, and imitation learning. Our empirical results show that the proposed iRLSVI algorithm is able to achieve significant reduction in regret as compared to two baselines: no offline data, and offline dataset but used without information about the generative policy. Our algorithm bridges online RL and imitation
    
[^139]: 计算预算的持续学习：什么才是重要的？

    Computationally Budgeted Continual Learning: What Does Matter?. (arXiv:2303.11165v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.11165](http://arxiv.org/abs/2303.11165)

    本文针对持续学习中计算预算的限制问题进行了研究，通过大规模基准测试评估了传统持续学习方法在计算受限环境中的性能，并探讨了不同采样策略、蒸馏损失和部分微调方法在数据递增、类递增和时间递增设置中的效果。

    

    持续学习旨在通过保持以前的知识并适应新数据的方式，对流入的数据流进行顺序训练模型，其数据分布不断变化。当前的持续学习文献主要关注对以前的数据有限访问的问题，而对训练的计算预算没有施加任何限制。这在野外应用中是不合理的，因为系统主要受到计算和时间预算的限制，而不是存储。我们通过一个大规模的基准测试来重新审视这个问题，并分析传统持续学习方法在计算受限环境中的性能，其中由于有限的计算，训练中使用的有效内存样本可以被隐式限制。我们对两个大规模数据集（ImageNet2K和Continual Google Landmarks V2）进行了实验，评估了各种持续学习采样策略、蒸馏损失和部分微调，在数据递增、类递增和时间递增设置中进行了评估。通过扩展实验评价了各种持续学习采样策略、蒸馏损失和部分微调等方法。

    Continual Learning (CL) aims to sequentially train models on streams of incoming data that vary in distribution by preserving previous knowledge while adapting to new data. Current CL literature focuses on restricted access to previously seen data, while imposing no constraints on the computational budget for training. This is unreasonable for applications in-the-wild, where systems are primarily constrained by computational and time budgets, not storage. We revisit this problem with a large-scale benchmark and analyze the performance of traditional CL approaches in a compute-constrained setting, where effective memory samples used in training can be implicitly restricted as a consequence of limited computation. We conduct experiments evaluating various CL sampling strategies, distillation losses, and partial fine-tuning on two large-scale datasets, namely ImageNet2K and Continual Google Landmarks V2 in data incremental, class incremental, and time incremental settings. Through extensi
    
[^140]: Ref-NeuS: 减少歧义的神经隐式表面学习，用于带有反射的多视角重建

    Ref-NeuS: Ambiguity-Reduced Neural Implicit Surface Learning for Multi-View Reconstruction with Reflection. (arXiv:2303.10840v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.10840](http://arxiv.org/abs/2303.10840)

    Ref-NeuS通过减少反射表面的影响以降低歧义，在多视角3D重建中取得了显著的进展。

    

    神经隐式表面学习在多视角3D重建中取得了显著进展，其中对象通过多层感知器表示，提供连续的隐式表面表示和视角相关辐射。然而，当前的方法常常无法准确重建反射表面，导致严重的歧义。为了克服这个问题，我们提出了Ref-NeuS，旨在通过削弱反射表面的影响来减少歧义。具体来说，我们利用异常检测器来估计一个显式的反射分数，并在多视角上下文的指导下定位反射表面。然后，我们设计了一个反射感知的光度损失，通过将渲染的颜色建模为高斯分布来自适应地减少歧义，其中反射分数代表方差。我们证明，结合反射方向相关的辐射，我们的模型能够在反射表面上实现高质量的表面重建，并超过了其他方法。

    Neural implicit surface learning has shown significant progress in multi-view 3D reconstruction, where an object is represented by multilayer perceptrons that provide continuous implicit surface representation and view-dependent radiance. However, current methods often fail to accurately reconstruct reflective surfaces, leading to severe ambiguity. To overcome this issue, we propose Ref-NeuS, which aims to reduce ambiguity by attenuating the effect of reflective surfaces. Specifically, we utilize an anomaly detector to estimate an explicit reflection score with the guidance of multi-view context to localize reflective surfaces. Afterward, we design a reflection-aware photometric loss that adaptively reduces ambiguity by modeling rendered color as a Gaussian distribution, with the reflection score representing the variance. We show that together with a reflection direction-dependent radiance, our model achieves high-quality surface reconstruction on reflective surfaces and outperforms t
    
[^141]: 在元树上批量更新后验树分布。

    Batch Updating of a Posterior Tree Distribution over a Meta-Tree. (arXiv:2303.09705v1 [cs.LG])

    [http://arxiv.org/abs/2303.09705](http://arxiv.org/abs/2303.09705)

    本文提出了一个更高效的批量更新方法，用于在元树上计算后验分布。

    

    以前，我们提出了一个由不可观察的树和一个序列更新方法表示的概率数据生成模型，用于计算一组树上的后验分布。该集合称为元树。在本文中，我们提出了一种更高效的批量更新方法。

    Previously, we proposed a probabilistic data generation model represented by an unobservable tree and a sequential updating method to calculate a posterior distribution over a set of trees. The set is called a meta-tree. In this paper, we propose a more efficient batch updating method.
    
[^142]: 从二进制测量中学习信号重构

    Learning to Reconstruct Signals From Binary Measurements. (arXiv:2303.08691v1 [eess.SP])

    [http://arxiv.org/abs/2303.08691](http://arxiv.org/abs/2303.08691)

    该论文提出了一种新的自监督学习方法SSBM，它只需要二进制数据进行训练，并探索了从不完整的二进制观察中学习的极端情况。这为从二进制测量中恢复信号提供了必要和充分条件，并在一系列真实数据集上展示了SSBM的卓越表现。

    

    无监督学习的最新进展突出了仅从噪声和不完整的线性测量中学习信号重构的可能性。这些方法在医学和科学成像以及传感中起到关键作用，其中地面真实数据经常稀缺或难以获得。然而，在实践中，测量不仅噪声和不完整，而且还被量化。在这里，我们探索从二进制观察中学习的极端情况，并提供了关于从不完整二进制数据中识别一组信号所需的测量数量的必要和充分条件。我们的结果是对从二进制测量中信号恢复现有界限的补充。此外，我们引入了一种新颖的自监督学习方法，我们将其命名为“SSBM”，它仅需要二进制数据进行训练。我们在一系列真实数据集上的实验证明SSBM与监督学习相当，并优于稀疏重构方法。

    Recent advances in unsupervised learning have highlighted the possibility of learning to reconstruct signals from noisy and incomplete linear measurements alone. These methods play a key role in medical and scientific imaging and sensing, where ground truth data is often scarce or difficult to obtain. However, in practice, measurements are not only noisy and incomplete but also quantized. Here we explore the extreme case of learning from binary observations and provide necessary and sufficient conditions on the number of measurements required for identifying a set of signals from incomplete binary data. Our results are complementary to existing bounds on signal recovery from binary measurements. Furthermore, we introduce a novel self-supervised learning approach, which we name SSBM, that only requires binary data for training. We demonstrate in a series of experiments with real datasets that SSBM performs on par with supervised learning and outperforms sparse reconstruction methods wit
    
[^143]: 预测图像记忆力所需的特征表示

    Feature representations useful for predicting image memorability. (arXiv:2303.07679v1 [cs.CV])

    [http://arxiv.org/abs/2303.07679](http://arxiv.org/abs/2303.07679)

    本研究使用Brain-Score评估64个CNN模型中与图像记忆力有关的特征表示，并发现高记忆力预测准确性的层与颞下皮质（IT）的脑部相似性更高。

    

    预测图像记忆力已经吸引了各个领域的兴趣。因此，使用卷积神经网络（CNN）模型的预测精度已经接近基于人类一致性估计的经验上限。然而，确定嵌入在CNN模型中的哪些特征表示对于记忆力的高预测准确性负责仍然是个悬而未决的问题。为了解决这个问题，本研究尝试使用脑部相似性来识别CNN模型中与记忆力有关的特征表示。具体而言，通过Brain-Score评估了在64个用于物体识别的CNN模型的16,860层中高记忆力预测准确性和脑部相似性。这项全面的分析显示出一个明显的趋势，即具有高记忆力预测准确性的层与最高阶段的颞下皮质（IT）的脑部相似性更高。此外，对64个CNN模型进行微调以预测图像记忆力并没有进一步提高准确性，这意味着现有的CNN模型已经包含了与记忆力相关的特征表示。

    Predicting image memorability has attracted interest in various fields. Consequently, prediction accuracy with convolutional neural network (CNN) models has been approaching the empirical upper bound estimated based on human consistency. However, identifying which feature representations embedded in CNN models are responsible for such high prediction accuracy of memorability remains an open question. To tackle this problem, this study sought to identify memorability-related feature representations in CNN models using brain similarity. Specifically, memorability prediction accuracy and brain similarity were examined and assessed by Brain-Score across 16,860 layers in 64 CNN models pretrained for object recognition. A clear tendency was shown in this comprehensive analysis that layers with high memorability prediction accuracy had higher brain similarity with the inferior temporal (IT) cortex, which is the highest stage in the ventral visual pathway. Furthermore, fine-tuning the 64 CNN m
    
[^144]: 对差分隐私训练模型理论的考虑

    Considerations on the Theory of Training Models with Differential Privacy. (arXiv:2303.04676v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04676](http://arxiv.org/abs/2303.04676)

    本论文提供了对差分隐私训练模型理论的考虑。研究了在联邦学习中的差分隐私保护方法，包括高斯差分隐私和差分隐私随机梯度下降。提供了框架和可证明性质的概述。

    

    在联邦学习中，一组客户端进行协作学习，每个客户端都希望控制其本地训练数据的使用方式，尤其是每个客户端的本地训练数据如何保持私密？差分隐私是一种限制隐私泄漏的方法。我们提供了其框架和可证明性质的概述，采用最新的基于假设的定义，即高斯差分隐私或$f$-DP，并讨论了差分隐私随机梯度下降（DP-SGD）。我们保持在元水平上，并尝试以直观的方式解释和洞察。

    In federated learning collaborative learning takes place by a set of clients who each want to remain in control of how their local training data is used, in particular, how can each client's local training data remain private? Differential privacy is one method to limit privacy leakage. We provide a general overview of its framework and provable properties, adopt the more recent hypothesis based definition called Gaussian DP or $f$-DP, and discuss Differentially Private Stochastic Gradient Descent (DP-SGD). We stay at a meta level and attempt intuitive explanations and insights \textit{in this book chapter}.
    
[^145]: 无梯度结构化剪枝方法与无标签数据

    Gradient-Free Structured Pruning with Unlabeled Data. (arXiv:2303.04185v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04185](http://arxiv.org/abs/2303.04185)

    本文提出了一种使用无标签数据的无梯度结构化剪枝方法，在GLUE和SQuAD基准测试上的实验证明了其有效性，仅需几分钟就能将原始FLOP计数的最高40%减少而准确度仅下降不超过4%。

    

    大型语言模型在许多领域中解决困难任务方面取得了巨大成功，但这种成功伴随着高计算成本和推理延迟。随着开发人员和第三方对这些模型进行个性化定制，提供高效的推理需求也越来越大。许多努力尝试通过剪枝和蒸馏等模型压缩技术来减少推理成本。然而，这些技术要么需要有标签的数据，要么因为需要重新训练压缩模型以恢复准确性而耗时。本文提出了一种仅使用无标签数据的无梯度结构化剪枝框架。使用BERT$_{BASE}$和DistilBERT在GLUE和SQuAD基准测试上的评估结果表明了该方法的有效性。仅使用预训练模型的权重和无标签数据，在单个GPU上仅需几分钟，即可将原始FLOP计数的最高40%减少，准确度下降不超过4%。

    Large Language Models (LLMs) have achieved great success in solving difficult tasks across many domains, but such success comes with a high computation cost, and inference latency. As developers and third parties customize these models, the need to provide efficient inference has increased. Many efforts have attempted to reduce inference cost through model compression techniques such as pruning and distillation. However, these techniques either require labeled data, or are time-consuming as they require the compressed model to be retrained to regain accuracy. In this paper, we propose a gradient-free structured pruning framework that uses only unlabeled data. An evaluation on the GLUE and SQuAD benchmarks using BERT$_{BASE}$ and DistilBERT illustrates the effectiveness of the proposed approach. By only using the weights of the pre-trained model and unlabeled data, in a matter of a few minutes on a single GPU, up to 40% of the original FLOP count can be reduced with less than a 4% accur
    
[^146]: 清洁CLIP: 缓解多模态对比学习中的数据污染攻击

    CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning. (arXiv:2303.03323v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.03323](http://arxiv.org/abs/2303.03323)

    CleanCLIP是一个通过独立重新对齐个别模态的表示来削弱后门攻击引入的虚假关联的微调框架。

    

    多模态对比预训练已被用于在大量配对的图文数据上训练多模态表示模型，如CLIP。然而，先前的研究揭示了这类模型容易受到后门攻击的影响。具体而言，当在含有后门的示例上进行训练时，CLIP学习到了嵌入式后门触发器与目标标签之间的虚假相关性，并将它们在联合嵌入空间中进行了对齐。即使注入了少量的毒化示例，例如在3000000个预训练数据中注入了75个示例，也能显著操纵模型的行为，使其难以检测或忘记这种相关性。为了解决这个问题，我们提出了CleanCLIP，一种通过独立重新对齐个别模态的表示来削弱后门攻击引入的学习到的虚假关联的微调框架。我们通过使用多模态对比和单模态自监督的组合进行无监督微调来证明这一点。

    Multimodal contrastive pretraining has been used to train multimodal representation models, such as CLIP, on large amounts of paired image-text data. However, previous studies have revealed that such models are vulnerable to backdoor attacks. Specifically, when trained on backdoored examples, CLIP learns spurious correlations between the embedded backdoor trigger and the target label, aligning their representations in the joint embedding space. Injecting even a small number of poisoned examples, such as 75 examples in 3 million pretraining data, can significantly manipulate the model's behavior, making it difficult to detect or unlearn such correlations. To address this issue, we propose CleanCLIP, a finetuning framework that weakens the learned spurious associations introduced by backdoor attacks by independently re-aligning the representations for individual modalities. We demonstrate that unsupervised finetuning using a combination of multimodal contrastive and unimodal self-supervi
    
[^147]: 变调神经ODEs

    Modulated Neural ODEs. (arXiv:2302.13262v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13262](http://arxiv.org/abs/2302.13262)

    变调神经ODEs （MoNODEs）是一种新的框架，能够将动力学状态与基础静态变化因素分开，并改进了现有的神经ODE方法。该方法通过引入时间不变的调制变量来捕捉轨迹间的变化，并在测试中展现出在振荡系统、视频和人类行走轨迹等方面具有提高模型泛化能力的效果。

    

    神经常微分方程（NODEs）已被证明对于学习任意轨迹的非线性动力学很有用。然而，当前的NODE方法仅通过初始状态值或自回归编码器更新来捕捉轨迹间的变化。在这项工作中，我们引入了变调神经ODEs（MoNODEs），这是一个将动力学状态与基础静态变化因素分开并改进现有NODE方法的新框架。特别地，我们引入了从数据中学习的“时间不变调制变量”。我们将我们提出的框架结合到四种现有的NODE变体中。我们在振荡系统、视频和人类行走轨迹上对MoNODE进行了测试，其中每个轨迹都具有轨迹特定的调制。我们的框架始终提高了现有模型的泛化能力，使其能够适应新的动态参数化并进行远期预测。此外，我们验证了提出的调制变量的信息量。

    Neural ordinary differential equations (NODEs) have been proven useful for learning non-linear dynamics of arbitrary trajectories. However, current NODE methods capture variations across trajectories only via the initial state value or by auto-regressive encoder updates. In this work, we introduce Modulated Neural ODEs (MoNODEs), a novel framework that sets apart dynamics states from underlying static factors of variation and improves the existing NODE methods. In particular, we introduce $\textit{time-invariant modulator variables}$ that are learned from the data. We incorporate our proposed framework into four existing NODE variants. We test MoNODE on oscillating systems, videos and human walking trajectories, where each trajectory has trajectory-specific modulation. Our framework consistently improves the existing model ability to generalize to new dynamic parameterizations and to perform far-horizon forecasting. In addition, we verify that the proposed modulator variables are infor
    
[^148]: DoG是SGD最好的朋友：一个无需参数调整的动态步长大小计划

    DoG is SGD's Best Friend: A Parameter-Free Dynamic Step Size Schedule. (arXiv:2302.12022v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12022](http://arxiv.org/abs/2302.12022)

    我们提出了一个参数-free 的动态 SGD 步长公式，称为梯度距离公式（DoG）， 它没有“学习率”参数，但是在局部有界的随机梯度优化中拥有强大的无参数收敛性，并在广泛的视觉和语言转移学习任务中的表现与有调整学习率的 SGD 相当接近。

    

    我们提出了一个无需参数调整的动态SGD步长公式，称为梯度距离公式（DoG）。DoG步长依赖于简单的经验量（初始点距离和梯度范数），并且没有“学习率”参数。在理论上，我们证明了DoG公式的一个略微变化可以保证具有强大的无参数收敛性，假定只有局部有界的随机梯度优化。在实践中，我们考虑了广泛的视觉和语言转移学习任务，并显示DoG的性能接近具有调整的学习率的SGD。我们还提出了一种逐层变量的DoG变体，通常优于调整的SGD，并接近调整的Adam的性能。PyTorch实现可在https://github.com/formll/dog获取。

    We propose a tuning-free dynamic SGD step size formula, which we call Distance over Gradients (DoG). The DoG step sizes depend on simple empirical quantities (distance from the initial point and norms of gradients) and have no ``learning rate'' parameter. Theoretically, we show that a slight variation of the DoG formula enjoys strong parameter-free convergence guarantees for stochastic convex optimization assuming only \emph{locally bounded} stochastic gradients. Empirically, we consider a broad range of vision and language transfer learning tasks, and show that DoG's performance is close to that of SGD with tuned learning rate. We also propose a per-layer variant of DoG that generally outperforms tuned SGD, approaching the performance of tuned Adam. A PyTorch implementation is available at https://github.com/formll/dog
    
[^149]: 公平扩散：训练文本到图像生成模型实现公平性

    Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness. (arXiv:2302.10893v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10893](http://arxiv.org/abs/2302.10893)

    这篇论文提出了一种名为“公平扩散”的新策略，可以在生成文本到图像模型部署后减轻偏见并使模型接受公平性指导。

    

    最近，生成式AI模型在质量方面取得了惊人的成果，并因此被广泛应用于越来越多的应用中。但由于它们高度依赖于从互联网上随机抽取的十亿级数据集，因此它们也会受到退化和偏见的人类行为的影响，正如我们所展示的那样。事实上，它们甚至可能加剧这些偏见。为了不仅揭示而且对抗这些不良影响，我们提出了一种新的策略，称为公平扩散，以在生成文本到图像模型部署后减轻偏见。具体而言，我们展示了基于人类指导的偏差转移，可在任何方向上产生任意新的比例，例如，身份组。正如我们的实证评估所示，这种控制使生成图像模型在公平性方面能够接受指导，无需数据过滤和额外的训练。

    Generative AI models have recently achieved astonishing results in quality and are consequently employed in a fast-growing number of applications. However, since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer from degenerated and biased human behavior, as we demonstrate. In fact, they may even reinforce such biases. To not only uncover but also combat these undesired effects, we present a novel strategy, called Fair Diffusion, to attenuate biases after the deployment of generative text-to-image models. Specifically, we demonstrate shifting a bias, based on human instructions, in any direction yielding arbitrarily new proportions for, e.g., identity groups. As our empirical evaluation demonstrates, this introduced control enables instructing generative image models on fairness, with no data filtering and additional training required.
    
[^150]: 自由形式的高斯过程状态空间模型的变分推断

    Free-Form Variational Inference for Gaussian Process State-Space Models. (arXiv:2302.09921v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09921](http://arxiv.org/abs/2302.09921)

    本文提出了一种自由形式的变分推断方法，用于高斯过程状态空间模型（GPSSMs）。该方法克服了以前方法的缺点，并展示了在计算效率和推断准确性上的优势。

    

    高斯过程状态空间模型（GPSSMs）为建模潜在状态的动态提供了一种有原则和灵活的方法，通过似然模型以离散时间点观测。然而，由于模型中潜在变量的数量较大且它们之间存在强时序依赖性，因此在 GPSSMs 中进行推断是计算上和统计上具有挑战性的。在本文中，我们提出了一种在贝叶斯 GPSSMs 中进行推断的新方法，克服了以前方法的缺点，即过于简化的假设和高计算要求。我们的方法基于在诱导变量形式主义内通过随机梯度哈密顿蒙特卡罗进行自由形式的变分推断。此外，通过利用我们提出的变分分布，我们提供了一种折叠扩展方法，其中诱导变量在解析上进行边际化。我们还展示了将我们的框架与粒子 MCMC 方法相结合的结果。我们展示了在s上的结果表明，我们的方法在计算效率和推断准确性上都具有优势。

    Gaussian process state-space models (GPSSMs) provide a principled and flexible approach to modeling the dynamics of a latent state, which is observed at discrete-time points via a likelihood model. However, inference in GPSSMs is computationally and statistically challenging due to the large number of latent variables in the model and the strong temporal dependencies between them. In this paper, we propose a new method for inference in Bayesian GPSSMs, which overcomes the drawbacks of previous approaches, namely over-simplified assumptions, and high computational requirements. Our method is based on free-form variational inference via stochastic gradient Hamiltonian Monte Carlo within the inducing-variable formalism. Furthermore, by exploiting our proposed variational distribution, we provide a collapsed extension of our method where the inducing variables are marginalized analytically. We also showcase results when combining our framework with particle MCMC methods. We show that, on s
    
[^151]: 面向对抗生成模型的PAC-Bayesian泛化界

    PAC-Bayesian Generalization Bounds for Adversarial Generative Models. (arXiv:2302.08942v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08942](http://arxiv.org/abs/2302.08942)

    将PAC-Bayesian理论扩展到生成模型，为基于Wasserstein距离和总变差距离的模型提供了泛化界，为Wasserstein GAN和Energy-Based GAN提供了新的训练目标，并在合成数据集上展示出非虚空泛化界。

    

    我们将PAC-Bayesian理论扩展到生成模型，并为基于Wasserstein距离和总变差距离的模型开发了泛化界。我们第一个关于Wasserstein距离的结果假设实例空间是有界的，而我们的第二个结果利用了降维的优势。我们的结果自然适用于Wasserstein GAN和Energy-Based GAN，而我们的界限为这两种GAN提供了新的训练目标。尽管我们的工作主要是理论性的，但我们进行了数值实验，展示了Wasserstein GAN在合成数据集上的非虚空泛化界。

    We extend PAC-Bayesian theory to generative models and develop generalization bounds for models based on the Wasserstein distance and the total variation distance. Our first result on the Wasserstein distance assumes the instance space is bounded, while our second result takes advantage of dimensionality reduction. Our results naturally apply to Wasserstein GANs and Energy-Based GANs, and our bounds provide new training objectives for these two. Although our work is mainly theoretical, we perform numerical experiments showing non-vacuous generalization bounds for Wasserstein GANs on synthetic datasets.
    
[^152]: 具有生成式NeRF的3D感知融合

    3D-aware Blending with Generative NeRFs. (arXiv:2302.06608v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.06608](http://arxiv.org/abs/2302.06608)

    这篇论文提出了一种使用生成式NeRF的3D感知融合方法，通过3D感知对齐和融合来解决输入图像不对齐的问题，该方法在FFHQ和AFHQ-Cat上验证了优于现有2D方法的性能。

    

    图像融合旨在无缝地合并多个图像。对于现有的基于2D的方法来说，如果输入图像由于3D相机姿态和物体形状的差异而不对齐，仍然具有挑战性。为了解决这些问题，我们提出了一种使用生成式神经辐射场（NeRF）的3D感知融合方法，包括两个关键组件：3D感知对齐和3D感知融合。对于3D感知对齐，我们首先估计与生成式NeRF相关的参考图像的相机姿态，然后对每个部分进行3D局部对齐。为了进一步利用生成式NeRF的3D信息，我们提出了基于3D感知的融合，它直接在NeRF的潜在表示空间上进行图像融合，而不是在原始像素空间上进行。通过对FFHQ和AFHQ-Cat进行广泛的定量和定性评估，我们的方法优于现有的2D基线。

    Image blending aims to combine multiple images seamlessly. It remains challenging for existing 2D-based methods, especially when input images are misaligned due to differences in 3D camera poses and object shapes. To tackle these issues, we propose a 3D-aware blending method using generative Neural Radiance Fields (NeRF), including two key components: 3D-aware alignment and 3D-aware blending. For 3D-aware alignment, we first estimate the camera pose of the reference image with respect to generative NeRFs and then perform 3D local alignment for each part. To further leverage 3D information of the generative NeRF, we propose 3D-aware blending that directly blends images on the NeRF's latent representation space, rather than raw pixel space. Collectively, our method outperforms existing 2D baselines, as validated by extensive quantitative and qualitative evaluations with FFHQ and AFHQ-Cat.
    
[^153]: CholecTriplet2022：展示给我一个工具，并告诉我三元组——一个用于手术行动三元组检测的内窥镜视觉挑战

    CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection. (arXiv:2302.06294v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2302.06294](http://arxiv.org/abs/2302.06294)

    CholecTriplet2022是一个手术行动三元组检测的挑战，通过将手术活动形式化为工具、行动和目标解剖结构的三元组来帮助开发更好的图像引导手术的人工智能辅助。本挑战从识别扩展到检测手术行动三元组建模，并包括对每个工具的边界框定位和对工具-活动的建模。

    

    将手术活动形式化为所使用的工具、执行的行动和目标解剖结构的三元组，已经成为手术活动建模的黄金标准方法。这种形式化有助于更详细地了解工具与组织之间的相互作用，可以用来开发更好的图像引导手术的人工智能辅助。早期的努力和2021年引入的CholecTriplet挑战提出了旨在识别手术录像中的这些三元组的技术。估计三元组的空间位置还将为计算机辅助干预提供更精确的术中环境感知决策支持。本文介绍了CholecTriplet2022挑战，从识别扩展到检测手术行动三元组建模。它包括每个可见手术仪器（或工具）的弱监督边界框定位，作为关键因素，以及对每个工具-活动的建模。

    Formalizing surgical activities as triplets of the used instruments, actions performed, and target anatomies is becoming a gold standard approach for surgical activity modeling. The benefit is that this formalization helps to obtain a more detailed understanding of tool-tissue interaction which can be used to develop better Artificial Intelligence assistance for image-guided surgery. Earlier efforts and the CholecTriplet challenge introduced in 2021 have put together techniques aimed at recognizing these triplets from surgical footage. Estimating also the spatial locations of the triplets would offer a more precise intraoperative context-aware decision support for computer-assisted intervention. This paper presents the CholecTriplet2022 challenge, which extends surgical action triplet modeling from recognition to detection. It includes weakly-supervised bounding box localization of every visible surgical instrument (or tool), as the key actors, and the modeling of each tool-activity in
    
[^154]: 一种简单的零样本提示加权技术，以改善文本-图像模型中的提示集成

    A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models. (arXiv:2302.06235v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06235](http://arxiv.org/abs/2302.06235)

    这项工作提出了一种简单的零样本提示加权技术，通过提示集成来自动化提示工程，从而提高文本-图像模型的零样本分类准确性。

    

    对比训练的文本-图像模型具有显著的零样本分类能力，即将以前未见过的图像分类为模型从未明确训练过的类别。然而，这些零样本分类器需要提示工程来达到高准确性。提示工程通常需要手工创建一组用于个别下游任务的提示。在这项工作中，我们的目标是通过提示集成来自动化这个提示工程，并提高零样本准确性。具体而言，我们提出了一个问题：“给定大量的提示，我们是否可以自动评分提示并集成那些对特定下游数据集最合适的提示，而无需访问有标签的验证数据？”我们证明这是可能的。在这样做的过程中，我们确定了一个天真的提示评分方法中的几个病理问题，其中分数很容易因预训练和测试数据中的偏见而过于自信，我们提出了一种新颖的提示评分方法。

    Contrastively trained text-image models have the remarkable ability to perform zero-shot classification, that is, classifying previously unseen images into categories that the model has never been explicitly trained to identify. However, these zero-shot classifiers need prompt engineering to achieve high accuracy. Prompt engineering typically requires hand-crafting a set of prompts for individual downstream tasks. In this work, we aim to automate this prompt engineering and improve zero-shot accuracy through prompt ensembling. In particular, we ask "Given a large pool of prompts, can we automatically score the prompts and ensemble those that are most suitable for a particular downstream dataset, without needing access to labeled validation data?". We demonstrate that this is possible. In doing so, we identify several pathologies in a naive prompt scoring method where the score can be easily overconfident due to biases in pre-training and test data, and we propose a novel prompt scoring
    
[^155]: 利用部分共同信息微结构进行多模态脑肿瘤分割

    Exploiting Partial Common Information Microstructure for Multi-Modal Brain Tumor Segmentation. (arXiv:2302.02521v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.02521](http://arxiv.org/abs/2302.02521)

    本文提出了一种利用部分共同信息微结构进行多模态脑肿瘤分割的方法，通过识别部分共同信息的潜在微结构，显著提高了图像分割模型的判别能力。

    

    多模态学习对于从磁共振成像数据中自动进行脑肿瘤分割十分关键。明确优化所有模态间共享的共同信息（例如通过最大化总相关性），已经被证明可以实现更好的特征表示，从而提高分割性能。然而，现有方法对于子集间共享的部分共同信息却无视。本文中，我们展示了识别这种部分共同信息能够显著增强图像分割模型的判别能力。具体而言，我们引入了部分共同信息掩码（PCI-mask）的新概念，以细粒度地描述哪些模态子集之间共享了部分共同信息。通过解决一个掩码相关性最大化问题，并同时学习一个最优的PCI-mask，我们识别出了部分共同信息的潜在微结构，并在脑肿瘤分割中利用它。

    Learning with multiple modalities is crucial for automated brain tumor segmentation from magnetic resonance imaging data. Explicitly optimizing the common information shared among all modalities (e.g., by maximizing the total correlation) has been shown to achieve better feature representations and thus enhance the segmentation performance. However, existing approaches are oblivious to partial common information shared by subsets of the modalities. In this paper, we show that identifying such partial common information can significantly boost the discriminative power of image segmentation models. In particular, we introduce a novel concept of partial common information mask (PCI-mask) to provide a fine-grained characterization of what partial common information is shared by which subsets of the modalities. By solving a masked correlation maximization and simultaneously learning an optimal PCI-mask, we identify the latent microstructure of partial common information and leverage it in a
    
[^156]: 通过牛顿方法实现鲁棒经验风险最小化研究

    Robust empirical risk minimization via Newton's method. (arXiv:2301.13192v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.13192](http://arxiv.org/abs/2301.13192)

    本研究提出了一种鲁棒经验风险最小化的新的牛顿方法变种，并通过使用鲁棒估计方法来替换梯度和海森矩阵，证明了连续迭代收敛到种群水平最小化器周围小球。该方法在广义线性模型中的应用具有潜在的优势，并提出了一种基于共轭梯度方法的算法来获取鲁棒牛顿方向。

    

    本文研究了一种新的牛顿方法变种，用于经验风险最小化。在优化算法的每次迭代中，目标函数的梯度和海森矩阵被替换为现有文献中针对多变量数据的鲁棒估计方法。在证明了连续迭代收敛到种群水平最小化器周围小球的一般定理之后，研究了当数据来自Huber的epsilon污染模型和/或重尾分布时，该理论在广义线性模型中的后果。还提出了一种基于共轭梯度方法获取鲁棒牛顿方向的算法，这可能更适用于高维情况，并提出了关于结果算法收敛性的猜想。与鲁棒梯度下降相比，所提出的算法能够实现更快的收敛速度。

    A new variant of Newton's method for empirical risk minimization is studied, where at each iteration of the optimization algorithm, the gradient and Hessian of the objective function are replaced by robust estimators taken from existing literature on robust mean estimation for multivariate data. After proving a general theorem about the convergence of successive iterates to a small ball around the population-level minimizer, consequences of the theory in generalized linear models are studied when data are generated from Huber's epsilon-contamination model and/or heavytailed distributions. An algorithm for obtaining robust Newton directions based on the conjugate gradient method is also proposed, which may be more appropriate for high-dimensional settings, and conjectures about the convergence of the resulting algorithm are offered. Compared to robust gradient descent, the proposed algorithm enjoys the faster rates of convergence for successive iterates often achieved by second-order al
    
[^157]: Composer's Assistant：一种用于多轨MIDI填充的交互式Transformer

    Composer's Assistant: An Interactive Transformer for Multi-Track MIDI Infilling. (arXiv:2301.12525v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2301.12525](http://arxiv.org/abs/2301.12525)

    Composer's Assistant是一种用于在REAPER音频工作站中进行人机交互创作的系统，它通过训练一个T5-like模型来实现对多轨MIDI填充的任务。该系统提供了一组脚本，使用户能够与模型进行交互，并进行了客观和主观的测试。完整的系统包括源代码、预训练模型和REAPER脚本。

    

    我们介绍了Composer's Assistant，这是一个用于在REAPER数字音频工作站中进行人机交互创作的系统。我们考虑到了当从一个MIDI文件的连续小节中删除了任意轨道小节时的多轨MIDI填充任务，并训练了一个类似于T5的模型来完成这个任务。Composer's Assistant由这个模型和能够与REAPER中的模型进行交互的脚本组成。我们进行了客观和主观的模型测试。我们发布了我们的完整系统，包括源代码、预训练模型和REAPER脚本。我们的模型只在授权许可的MIDI文件上进行了训练。

    We introduce Composer's Assistant, a system for interactive human-computer composition in the REAPER digital audio workstation. We consider the task of multi-track MIDI infilling when arbitrary track-measures have been deleted from a contiguous slice of measures from a MIDI file, and we train a T5-like model to accomplish this task. Composer's Assistant consists of this model together with scripts that enable interaction with the model in REAPER. We conduct objective and subjective tests of our model. We release our complete system, consisting of source code, pretrained models, and REAPER scripts. Our models were trained only on permissively-licensed MIDI files.
    
[^158]: 从流数据中在线发现演化系统的控制微分方程

    Online discovering governing differential equations of evolving systems from streaming data. (arXiv:2301.07863v2 [physics.comp-ph] UPDATED)

    [http://arxiv.org/abs/2301.07863](http://arxiv.org/abs/2301.07863)

    本研究提出了一种在线建模方法，逐一处理流数据，可以有效地识别演化系统的微分方程，尤其是变化后的系统产生的测量分布与以前不同的情况下。

    

    从现有观测数据中发现演化系统的控制方程是至关重要和具有挑战性的。本文考虑了一个新的场景：从流数据中发现控制方程。目前的方法难以综合考虑样本数据，从而无法处理这个任务。我们提出了一种在线建模方法，通过建模流数据而非处理整个数据集，逐一处理样本数据。该方法在发现普通微分方程和偏微分方程方面表现良好。演化系统随时间变化而变化，其状态也随之改变。因此找到精确的变化点至关重要。由变化后的系统产生的测量分布与以前不同，因此可以通过所提出的方法识别出差异。我们所提出的方法在识别演化系统的微分方程中有竞争力。

    Discovering the governing equations of evolving systems from available observations is essential and challenging. In this paper, we consider a new scenario: discovering governing equations from streaming data. Current methods struggle to discover governing differential equations with considering measurements as a whole, leading to failure to handle this task. We propose an online modeling method capable of handling samples one by one sequentially by modeling streaming data instead of processing the entire dataset. The proposed method performs well in discovering ordinary differential equations (ODEs) and partial differential equations (PDEs) from streaming data. Evolving systems are changing over time, which invariably changes with system status. Thus, finding the exact change points is critical. The measurement generated from a changed system is distributed dissimilarly to before; hence, the difference can be identified by the proposed method. Our proposal is competitive in identifyin
    
[^159]: StitchNet: 从预训练片段组合神经网络

    StitchNet: Composing Neural Networks from Pre-Trained Fragments. (arXiv:2301.01947v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.01947](http://arxiv.org/abs/2301.01947)

    StitchNet提出了一种新的神经网络创建方式，它通过组合预训练神经网络的片段来创建高性能的网络，无需传统训练的大量计算资源和数据要求。通过居中核对齐（CKA），可以有效指导片段的选择，以满足特定准确性需求和计算资源限制。此外，StitchNet还可以实现即时个性化模型创建和推断。

    

    我们提出了一种新颖的神经网络创建范式StitchNet，它将来自多个预训练神经网络的片段（一个或多个连续的网络层）拼接在一起。StitchNet允许创建高性能的神经网络，而无需传统的基于反向传播训练的大量计算和数据要求。我们利用居中核对齐（CKA）作为一种兼容性度量，以有效地指导选择这些片段，以组合适合特定准确性需求和计算资源限制的任务网络。然后，我们展示了这些片段可以被拼接在一起，以在计算资源和数据要求的一小部分下创建与传统训练网络相媲美准确度的神经网络。最后，我们探索了这种新范式所能实现的一种新颖的即时个性化模型创建和推断应用。

    We propose StitchNet, a novel neural network creation paradigm that stitches together fragments (one or more consecutive network layers) from multiple pre-trained neural networks. StitchNet allows the creation of high-performing neural networks without the large compute and data requirements needed under traditional model creation processes via backpropagation training. We leverage Centered Kernel Alignment (CKA) as a compatibility measure to efficiently guide the selection of these fragments in composing a network for a given task tailored to specific accuracy needs and computing resource constraints. We then show that these fragments can be stitched together to create neural networks with comparable accuracy to traditionally trained networks at a fraction of computing resource and data requirements. Finally, we explore a novel on-the-fly personalized model creation and inference application enabled by this new paradigm.
    
[^160]: 预训练分类器的少样本迁移学习的泛化界限

    Generalization Bounds for Few-Shot Transfer Learning with Pretrained Classifiers. (arXiv:2212.12532v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.12532](http://arxiv.org/abs/2212.12532)

    该论文研究了基于预训练分类器学习的表示在少样本迁移学习中的通用性界限，提出了类别特征变异崩塌现象的理论解释，该现象使得在新类别上通过少样本学习的特征映射具有较小的误差。

    

    我们研究了基础模型学习可传输到新的、未见过的类别的分类表示能力。最近的文献结果表明，由一个单一的分类器学得的表示在少样本学习问题上与专门为这些问题设计的特殊算法学得的表示有竞争力。我们提出了一个理论解释，基于最近发现的类别特征变异崩塌现象，即在深度分类网络训练过程中，属于同一类别的样本的特征嵌入倾向于聚集在它们的类别均值附近。具体地，我们证明了在类别特征变异崩塌的情况下，学得的特征映射在新的类别上的少样本误差小，即使用从每个新类别的少量随机样本学得的中心来进行最近类别中心分类器的分类误差小。

    We study the ability of foundation models to learn representations for classification that are transferable to new, unseen classes. Recent results in the literature show that representations learned by a single classifier over many classes are competitive on few-shot learning problems with representations learned by special-purpose algorithms designed for such problems. We offer a theoretical explanation for this behavior based on the recently discovered phenomenon of class-feature-variability collapse, that is, that during the training of deep classification networks the feature embeddings of samples belonging to the same class tend to concentrate around their class means. More specifically, we show that the few-shot error of the learned feature map on new classes (defined as the classification error of the nearest class-center classifier using centers learned from a small number of random samples from each new class) is small in case of class-feature-variability collapse, under the a
    
[^161]: 为轻量级对比模型建立更强的基准线

    Establishing a stronger baseline for lightweight contrastive models. (arXiv:2212.07158v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.07158](http://arxiv.org/abs/2212.07158)

    本论文旨在为轻量级对比模型建立更强的基准线，解决了在自监督对比学习中使用效率型网络的性能下降问题。通过优化训练配置和引入平滑的损失函数，实现了更好的性能。

    

    最近的研究报告称，在诸如MobileNet和EfficientNet等特定设计的高效网络中，自监督对比学习的性能出现下降。解决这个问题的常见做法是引入预训练的对比教师模型，并使用教师生成的蒸馏信号来训练轻量级网络。然而，当教师模型不可用时，预训练教师模型是一项耗时和资源消耗大的工作。在这项工作中，我们的目标是在不使用预训练教师模型的情况下为轻量级对比模型建立更强的基准线。具体来说，我们展示了高效模型的最佳训练配置与较大模型不同，使用先前研究中的与ResNet50相同的训练设置是不合适的。此外，我们观察到在对比学习中存在一种常见问题，即正或负视图中的一个可能会有噪声，并提出了一种平滑的InfoNCE损失函数来减轻这个问题。结果，我们的方法在轻量级对比模型上取得了更好的性能。

    Recent research has reported a performance degradation in self-supervised contrastive learning for specially designed efficient networks, such as MobileNet and EfficientNet. A common practice to address this problem is to introduce a pretrained contrastive teacher model and train the lightweight networks with distillation signals generated by the teacher. However, it is time and resource consuming to pretrain a teacher model when it is not available. In this work, we aim to establish a stronger baseline for lightweight contrastive models without using a pretrained teacher model. Specifically, we show that the optimal recipe for efficient models is different from that of larger models, and using the same training settings as ResNet50, as previous research does, is inappropriate. Additionally, we observe a common issu e in contrastive learning where either the positive or negative views can be noisy, and propose a smoothed version of InfoNCE loss to alleviate this problem. As a result, w
    
[^162]: 通过 Pauli 传输矩阵学习量子过程和哈密顿量

    Learning Quantum Processes and Hamiltonians via the Pauli Transfer Matrix. (arXiv:2212.04471v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2212.04471](http://arxiv.org/abs/2212.04471)

    本文通过学习量子过程和哈密顿量，展示了通过量子存储可以实现学习任意量子过程的指数级量子优势，并解决了学习 Pauli 传输矩阵和预测不同情况下的波函数期望值的任务。

    

    通过量子增强实验，依靠量子存储和量子处理来学习物理系统，可以优于只有经典存储和处理的实验。虽然在各种状态学习任务中已经确定了量子优势，但量子过程学习只有在问题得到仔细定义并且理解较少的情况下才能达到相似的优势。我们确定了学习未知的 n 量子比特量子过程 的指数级量子优势。我们展示了量子存储可以高效地解决以下任务：(a) 学习任意 $\mathcal{N}$ 的 Pauli 传输矩阵，(b) 在输入 Pauli 稀疏态时预测任意 $\mathcal{N}$ 输出处测量的有界 Pauli 稀疏可观测量的期望值，以及 (c) 在未知 $\mathcal{N}$ 的输出上预测有界可观测量的期望值，其中 $\mathcal{N}$ 的 Pauli 传输矩阵稀疏。

    Learning about physical systems from quantum-enhanced experiments, relying on a quantum memory and quantum processing, can outperform learning from experiments in which only classical memory and processing are available. Whereas quantum advantages have been established for a variety of state learning tasks, quantum process learning allows for comparable advantages only with a careful problem formulation and is less understood. We establish an exponential quantum advantage for learning an unknown $n$-qubit quantum process $\mathcal{N}$. We show that a quantum memory allows to efficiently solve the following tasks: (a) learning the Pauli transfer matrix of an arbitrary $\mathcal{N}$, (b) predicting expectation values of bounded Pauli-sparse observables measured on the output of an arbitrary $\mathcal{N}$ upon input of a Pauli-sparse state, and (c) predicting expectation values of arbitrary bounded observables measured on the output of an unknown $\mathcal{N}$ with sparse Pauli transfer m
    
[^163]: \{kappa}HGCN: 通过连续和离散曲率学习实现树状结构建模

    \{kappa}HGCN: Tree-likeness Modeling via Continuous and Discrete Curvature Learning. (arXiv:2212.01793v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.01793](http://arxiv.org/abs/2212.01793)

    本文提出了一种新的\{kappa}HGCN模型，在双曲空间内实现树状结构建模，通过结合连续和离散曲率来学习输入图的基础几何结构，并在多个基准测试和数据集上取得了最先进的性能。

    

    树状结构在现实世界中广泛存在，包括层次结构和幂律分布。最近，利用双曲空间进行树状结构建模受到了广泛关注，由于其呈指数增长，相比于平坦的欧几里得空间，曲面双曲空间提供了更易处理和嵌入的空间，特别适用于展现隐含树状结构的数据集。然而，真实世界树状数据的复杂性提出了一个重要挑战，因为它经常展示出树状、平坦和圆形区域的异质组成。将这样异质的结构直接嵌入一个同质化的嵌入空间（即双曲空间）必然导致重大失真。为了缓解上述缺点，本研究致力于探索双曲空间的曲率，以实现灵活准确地建模树状结构。具体而言，我们提出了一种新的\{kappa}HGCN模型，将连续和离散曲率相结合，学习输入图的基础几何结构。我们的模型在不同的基准测试和数据集上均取得了最先进的性能，证明了其在捕捉输入数据的树状结构方面的有效性。

    The prevalence of tree-like structures, encompassing hierarchical structures and power law distributions, exists extensively in real-world applications, including recommendation systems, ecosystems, financial networks, social networks, etc. Recently, the exploitation of hyperbolic space for tree-likeness modeling has garnered considerable attention owing to its exponential growth volume. Compared to the flat Euclidean space, the curved hyperbolic space provides a more amenable and embeddable room, especially for datasets exhibiting implicit tree-like architectures. However, the intricate nature of real-world tree-like data presents a considerable challenge, as it frequently displays a heterogeneous composition of tree-like, flat, and circular regions. The direct embedding of such heterogeneous structures into a homogeneous embedding space (i.e., hyperbolic space) inevitably leads to heavy distortions. To mitigate the aforementioned shortage, this study endeavors to explore the curvatur
    
[^164]: iEnhancer-ELM:基于增强子语言模型提取位置相关的多尺度上下文信息以改善增强子鉴定

    iEnhancer-ELM: improve enhancer identification by extracting position-related multiscale contextual information based on enhancer language models. (arXiv:2212.01495v2 [q-bio.GN] UPDATED)

    [http://arxiv.org/abs/2212.01495](http://arxiv.org/abs/2212.01495)

    提出一种基于BERT-like增强子语言模型的增强子鉴定方法(iEnhancer-ELM)，通过提取位置相关的多尺度上下文信息，实现了对原始DNA序列的增强子鉴定性能的改善。

    

    动机：增强子是重要的顺式调控元件，可以调控各种生物功能并增强目标基因的转录。尽管已经提出了许多特征提取方法来改善增强子鉴定的性能，但它们不能从原始DNA序列中学习位置相关的多尺度上下文信息。结果：本文提出了一种基于BERT-like增强子语言模型的新型增强子鉴定方法（iEnhancer-ELM）。iEnhancer-ELM使用多尺度k-mers对DNA序列进行标记化，并通过多头注意机制提取与位置相关的不同尺度k-mers的上下文信息。首先评估不同尺度k-mers的性能，然后将它们集成以提高增强子鉴定的性能。对两个流行的基准数据集进行的实验结果表明，我们的模型优于目前最先进的方法。我们进一步说明了iEnhancer的可解释性。

    Motivation: Enhancers are important cis-regulatory elements that regulate a wide range of biological functions and enhance the transcription of target genes. Although many feature extraction methods have been proposed to improve the performance of enhancer identification, they cannot learn position-related multiscale contextual information from raw DNA sequences.  Results: In this article, we propose a novel enhancer identification method (iEnhancer-ELM) based on BERT-like enhancer language models. iEnhancer-ELM tokenizes DNA sequences with multi-scale k-mers and extracts contextual information of different scale k-mers related with their positions via an multi-head attention mechanism. We first evaluate the performance of different scale k-mers, then ensemble them to improve the performance of enhancer identification. The experimental results on two popular benchmark datasets show that our model outperforms stateof-the-art methods. We further illustrate the interpretability of iEnhanc
    
[^165]: 可扩展的分层无线联邦学习算法

    Scalable Hierarchical Over-the-Air Federated Learning. (arXiv:2211.16162v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2211.16162](http://arxiv.org/abs/2211.16162)

    本研究提出了一种针对分布式环境的通信高效的分层联邦学习算法，通过使用可扩展的无线聚合方案和带宽有限的广播方案，解决了设备干扰和边缘服务器干扰的问题。

    

    本研究提出了一种针对包含核心服务器和多个边缘服务器及设备集群的分布式环境的通信高效的分层联邦学习算法。假设不同的学习任务，具有相同任务的集群进行协作。为了在无线链路上实现算法，我们提出了一种可扩展的分簇无线聚合方案，用于上行链路，同时采用带宽有限的广播方案用于下行链路，每个算法迭代只需要一个资源块，不受边缘服务器和设备数量的影响。这种设置面临着上行链路设备干扰和下行链路边缘服务器干扰的问题，需要进行严格的建模。我们首先通过将设备建模为一个泊松集群过程，在设置中建立了一个空间模型，并对由干扰引起的上行链路和下行链路的误差进行量化。然后，我们提出了一种全面的数学方法来推导收敛性。

    In this work, we propose a communication-efficient hierarchical federated learning algorithm for distributed setups including core servers and multiple edge servers with clusters of devices. Assuming different learning tasks, clusters with a same task collaborate. To implement the algorithm over wireless links, we propose a scalable clustered over-the-air aggregation scheme for the uplink with a bandwidth-limited broadcast scheme for the downlink that requires only a single resource block for each algorithm iteration, independent of the number of edge servers and devices. This setup is faced with interference of devices in the uplink and interference of edge servers in the downlink that are to be modeled rigorously. We first develop a spatial model for the setup by modeling devices as a Poisson cluster process over the edge servers and quantify uplink and downlink error terms due to the interference. Accordingly, we present a comprehensive mathematical approach to derive the convergenc
    
[^166]: PAC-Bayes定理在Bandit问题中的应用：一项调查与实验比较

    PAC-Bayes Bounds for Bandit Problems: A Survey and Experimental Comparison. (arXiv:2211.16110v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.16110](http://arxiv.org/abs/2211.16110)

    这项调查研究了PAC-Bayes在Bandit问题中的应用，提供了界限的概述，并进行了实验比较。研究发现，PAC-Bayes界限是设计具有性能保证的离线Bandit算法的有用工具，但在线Bandit算法缺乏足够的数据以产生强大的性能保证。

    

    PAC-Bayes最近重新出现作为一种有效的理论，可以用来推导出具有紧密性能保证的有原则的学习算法。然而，PAC-Bayes在Bandit问题中的应用相对较少，这是一个很大的遗憾。在医疗保健、金融和自然科学等许多决策问题中，都可以将其建模为Bandit问题。在许多这些应用中，带有强大性能保证的有原则算法将会受到很高的赞赏。本调查提供了关于Bandit问题的PAC-Bayes界限的概述，并进行了这些界限的实验比较。一方面，我们发现PAC-Bayes界限是设计具有性能保证的离线Bandit算法的有用工具。在我们的实验中，一种PAC-Bayesian离线上下文Bandit算法能够学习具有竞争性预期奖励和非空性能保证的随机化神经网络策略。另一方面，PAC-Bayesian在线Bandit算法则缺乏足够的数据以产生强大的性能保证。

    PAC-Bayes has recently re-emerged as an effective theory with which one can derive principled learning algorithms with tight performance guarantees. However, applications of PAC-Bayes to bandit problems are relatively rare, which is a great misfortune. Many decision-making problems in healthcare, finance and natural sciences can be modelled as bandit problems. In many of these applications, principled algorithms with strong performance guarantees would be very much appreciated. This survey provides an overview of PAC-Bayes bounds for bandit problems and an experimental comparison of these bounds. On the one hand, we found that PAC-Bayes bounds are a useful tool for designing offline bandit algorithms with performance guarantees. In our experiments, a PAC-Bayesian offline contextual bandit algorithm was able to learn randomised neural network polices with competitive expected reward and non-vacuous performance guarantees. On the other hand, the PAC-Bayesian online bandit algorithms that
    
[^167]: 在Sobolev和Besov空间上，关于深度ReLU神经网络的最佳逼近速率研究

    Optimal Approximation Rates for Deep ReLU Neural Networks on Sobolev and Besov Spaces. (arXiv:2211.14400v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.14400](http://arxiv.org/abs/2211.14400)

    该论文研究了在Sobolev和Besov空间中，使用ReLU激活函数的深度神经网络能够以怎样的参数效率逼近函数，包括$L_p(\Omega)$范数下的误差度量。我们提供了所有$1\leq p,q \leq \infty$和$s>0$的完整解决方案，并引入了一种新的位提取技术来获得尖锐的上界。

    

    本文研究了使用ReLU激活函数的深度神经网络在Sobolev空间$W^s(L_q(\Omega))$和Besov空间$B^s_r(L_q(\Omega))$中以$L_p(\Omega)$范数度量误差的参数效率问题。我们的研究对于在科学计算和信号处理等领域中应用神经网络非常重要，在过去只有当$p=q=\infty$时才完全解决。我们的贡献是提供了所有$1\leq p,q\leq \infty$和$s>0$的完整解决方案，包括渐近匹配的上下界。关键的技术工具是一种新的位提取技术，它提供了稀疏向量的最佳编码。这使我们能够在$p>q$的非线性区域获得尖锐的上界。我们还提供了一种基于的$L_p$逼近下界推导的新方法。

    Let $\Omega = [0,1]^d$ be the unit cube in $\mathbb{R}^d$. We study the problem of how efficiently, in terms of the number of parameters, deep neural networks with the ReLU activation function can approximate functions in the Sobolev spaces $W^s(L_q(\Omega))$ and Besov spaces $B^s_r(L_q(\Omega))$, with error measured in the $L_p(\Omega)$ norm. This problem is important when studying the application of neural networks in a variety of fields, including scientific computing and signal processing, and has previously been completely solved only when $p=q=\infty$. Our contribution is to provide a complete solution for all $1\leq p,q\leq \infty$ and $s > 0$, including asymptotically matching upper and lower bounds. The key technical tool is a novel bit-extraction technique which gives an optimal encoding of sparse vectors. This enables us to obtain sharp upper bounds in the non-linear regime where $p > q$. We also provide a novel method for deriving $L_p$-approximation lower bounds based upon
    
[^168]: 一种 $k$ 最近邻的两阶段主动学习算法

    A Two-Stage Active Learning Algorithm for $k$-Nearest Neighbors. (arXiv:2211.10773v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.10773](http://arxiv.org/abs/2211.10773)

    本文提出了一种用于训练 $k$ 最近邻分类器的简单直观的主动学习算法，首次保持了 $k$ 最近邻投票概念的预测时间，并提供了一致性保证。

    

    $k$ 最近邻分类是一种流行的非参数方法，因为它具有自动适应分布比例变化等优点。然而，迄今为止，针对自然保留这些优秀特性的本地投票分类器的训练，设计主动学习策略一直是困难的，因此 $k$ 最近邻分类的主动学习策略在文献中一直缺失。在本文中，我们介绍了一种简单直观的主动学习算法，用于训练 $k$ 最近邻分类器，这是文献中第一次保持了 $k$ 最近邻投票概念的预测时间。我们为通过我们方案获取的样本提供了一种修改的 $k$ 最近邻分类器的一致性保证，并且当条件概率函数 $\mathbb{P}(Y=y|X=x)$ 足够平滑并且 Tsybakov 噪声条件成立时，我们的主动训练的方法表现良好。

    $k$-nearest neighbor classification is a popular non-parametric method because of desirable properties like automatic adaption to distributional scale changes. Unfortunately, it has thus far proved difficult to design active learning strategies for the training of local voting-based classifiers that naturally retain these desirable properties, and hence active learning strategies for $k$-nearest neighbor classification have been conspicuously missing from the literature. In this work, we introduce a simple and intuitive active learning algorithm for the training of $k$-nearest neighbor classifiers, the first in the literature which retains the concept of the $k$-nearest neighbor vote at prediction time. We provide consistency guarantees for a modified $k$-nearest neighbors classifier trained on samples acquired via our scheme, and show that when the conditional probability function $\mathbb{P}(Y=y|X=x)$ is sufficiently smooth and the Tsybakov noise condition holds, our actively trained
    
[^169]: 基于SPD流形的图神经网络用于运动想象分类：来自时频分析的视角

    Graph Neural Networks on SPD Manifolds for Motor Imagery Classification: A Perspective from the Time-Frequency Analysis. (arXiv:2211.02641v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2211.02641](http://arxiv.org/abs/2211.02641)

    本文介绍了一种基于SPD流形的图神经网络用于运动想象分类，利用EEG的二阶统计量，相比传统方法具有更好的性能。

    This paper introduces a graph neural network based on SPD manifolds for motor imagery classification, which utilizes second-order statistics of EEG signals and outperforms traditional methods.

    运动想象（MI）的分类是脑电图（EEG）基础脑机接口（BCI）领域中备受追捧的研究课题，具有巨大的商业价值。过去二十年，MI-EEG分类器的趋势发生了根本性的转变，其性能逐渐提高。 Tensor-CSPNet的出现是BCI研究中第一个几何深度学习（GDL）框架的必要性，其归因于信号的非欧几里德性质的特征化。从根本上讲，Tensor-CSPNet是一种基于深度学习的分类器，利用EEG的二阶统计量。与利用EEG信号的一阶统计量的传统方法相比，利用这些二阶统计量代表了经典的处理方法。这些统计量提供了足够的区分信息，使它们适用于MI-EEG分类。在本研究中，我们介绍了另一种GDL分类器，

    The classification of motor imagery (MI) is a highly sought-after research topic in the field of Electroencephalography (EEG)-based brain-computer interfaces (BCIs), with immense commercial value. Over the past two decades, there has been a fundamental shift in the trend of MI-EEG classifiers, resulting in a gradual increase in their performance. The emergence of Tensor-CSPNet, the first geometric deep learning (GDL) framework in BCI research, is attributed to the imperative of characterizing the non-Euclidean nature of signals. Fundamentally, Tensor-CSPNet is a deep learning-based classifier that capitalizes on the second-order statistics of EEGs. In contrast to the conventional approach of utilizing first-order statistics for EEG signals, the utilization of these second-order statistics represents the classical treatment. These statistics provide adequate discriminative information, rendering them suitable for MI-EEG classification. In this study, we introduce another GDL classifier,
    
[^170]: 典型可学习任务空间的图像

    A picture of the space of typical learnable tasks. (arXiv:2210.17011v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.17011](http://arxiv.org/abs/2210.17011)

    我们使用信息几何技术研究了在不同任务上训练时深度网络学习到的表示，发现任务空间的结构与Wordnet系统进化树的某些部分一致，并且监督学习在一个任务上的进展可以在其他任务上产生一定的影响。

    

    我们利用信息几何技术来理解深度网络在使用监督学习、元学习、半监督学习和对比学习训练在不同任务上学到的表示。我们揭示了与任务空间结构相关的以下现象：(1)使用不同表示学习方法在不同任务上训练的概率模型流形实际上是低维的；(2)在一个任务上进行监督学习即使在表面上看起来是不相似的任务上也能取得出乎意料的进展；如果训练任务具有多样的类别，则其在其他任务上的进展更大；(3)通过我们的分析所指示的任务空间结构与Wordnet系统进化树中的某些部分一致；(4)在训练过程中，情境元学习算法和监督学习遵循不同的轨迹，但最终适应相似的模型；(5)对比学习和半监督学习方法遵循类似的轨迹。

    We develop information geometric techniques to understand the representations learned by deep networks when they are trained on different tasks using supervised, meta-, semi-supervised and contrastive learning. We shed light on the following phenomena that relate to the structure of the space of tasks: (1) the manifold of probabilistic models trained on different tasks using different representation learning methods is effectively low-dimensional; (2) supervised learning on one task results in a surprising amount of progress even on seemingly dissimilar tasks; progress on other tasks is larger if the training task has diverse classes; (3) the structure of the space of tasks indicated by our analysis is consistent with parts of the Wordnet phylogenetic tree; (4) episodic meta-learning algorithms and supervised learning traverse different trajectories during training but they fit similar models eventually; (5) contrastive and semi-supervised learning methods traverse trajectories similar
    
[^171]: 使用可迁移的卷积神经网络进行多目标跟踪

    Multi-Target Tracking with Transferable Convolutional Neural Networks. (arXiv:2210.15539v3 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2210.15539](http://arxiv.org/abs/2210.15539)

    本论文提出了一种使用可迁移的卷积神经网络进行多目标跟踪的方法。该方法利用图像表示目标状态和传感器测量，并通过迁移学习实现了在大规模上进行MTT，并在10个目标的MTT任务中表现优于传统方法，在250个目标的更大MTT任务中性能提高了29%。

    

    多目标跟踪（MTT）是一个经典的信号处理任务，目标是从噪声传感器测量中估计出未知数量的移动目标的状态。在本文中，我们从深度学习的角度重新审视了MTT，并提出了一种卷积神经网络（CNN）架构来解决该问题。我们将目标状态和传感器测量表示为图像，并将问题重新定义为图像到图像的预测任务。然后，我们在小范围的跟踪区域训练了一个完全卷积模型，并将其迁移到具有大量目标和传感器的更大范围。这种迁移学习方法使得MTT能够在大规模上实现，并且在理论上得到了我们的新颖分析的支持，该分析限制了泛化误差。在实践中，所提出的可迁移CNN架构在具有10个目标的MTT任务上优于随机有限集滤波器，并且在不重新训练的情况下将其迁移到具有250个目标的更大的MTT任务中，性能提高了29%。

    Multi-target tracking (MTT) is a classical signal processing task, where the goal is to estimate the states of an unknown number of moving targets from noisy sensor measurements. In this paper, we revisit MTT from a deep learning perspective and propose a convolutional neural network (CNN) architecture to tackle it. We represent the target states and sensor measurements as images and recast the problem as an image-to-image prediction task. Then we train a fully convolutional model at small tracking areas and transfer it to much larger areas with numerous targets and sensors. This transfer learning approach enables MTT at a large scale and is also theoretically supported by our novel analysis that bounds the generalization error. In practice, the proposed transferable CNN architecture outperforms random finite set filters on the MTT task with 10 targets and transfers without re-training to a larger MTT task with 250 targets with a 29% performance improvement.
    
[^172]: FocusedCleaner：用于稳健GNN节点分类的清理受污染图形

    FocusedCleaner: Sanitizing Poisoned Graphs for Robust GNN-based Node Classification. (arXiv:2210.13815v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13815](http://arxiv.org/abs/2210.13815)

    FocusedCleaner是一种针对数据投毒攻击的清理器，通过逆转攻击过程并提供更准确的搜索区域，逐步清理受污染的图形，显著提高了GNN在节点分类任务上的对抗鲁棒性。

    

    图神经网络（GNN）易受数据投毒攻击的影响，这会生成一个被污染的图形作为GNN模型的输入。我们提出FocusedCleaner作为一种污染图形清理器，能够有效识别攻击者注入的毒素。具体而言，FocusedCleaner提供了一个清理框架，包括两个模块：双层结构学习和受害节点检测。其中，结构学习模块将逆转攻击过程，稳定地清理图形，而检测模块提供了“焦点”——一个更窄、更准确的搜索区域——给结构学习模块使用。这两个模块将迭代运行，并相互增强，逐步清理被污染的图形。作为一个重要应用，我们展示了在清理后的图上训练的GNN对于节点分类任务的对抗鲁棒性显著提高。广泛的实验表明，FocusedCleaner优于最先进的方法。

    Graph Neural Networks (GNNs) are vulnerable to data poisoning attacks, which will generate a poisoned graph as the input to the GNN models. We present FocusedCleaner as a poisoned graph sanitizer to effectively identify the poison injected by attackers. Specifically, FocusedCleaner provides a sanitation framework consisting of two modules: bi-level structural learning and victim node detection. In particular, the structural learning module will reverse the attack process to steadily sanitize the graph while the detection module provides ``the focus" -- a narrowed and more accurate search region -- to structural learning. These two modules will operate in iterations and reinforce each other to sanitize a poisoned graph step by step. As an important application, we show that the adversarial robustness of GNNs trained over the sanitized graph for the node classification task is significantly improved. Extensive experiments demonstrate that FocusedCleaner outperforms the state-of-the-art b
    
[^173]: 具有利普希茨非线性的单个神经元模型的主动学习

    Active Learning for Single Neuron Models with Lipschitz Non-Linearities. (arXiv:2210.13601v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13601](http://arxiv.org/abs/2210.13601)

    该论文提出一种针对具有利普希茨非线性的单个神经元模型的主动学习策略，该策略在敌对标签噪声下拟合线性函数，并在逼近保证方面具有强有力的可证明性能。

    

    我们考虑在敌对标签噪声下的对单个神经元模型进行主动学习的问题，有时也被称为“岭函数”。这些模型已被证明广泛有效地模拟物理现象，并构建用于偏微分方程的代理数据驱动模型。令人惊讶的是，我们证明对于具有任何利普希茨非线性（如ReLU函数、sigmoid函数、绝对值函数、低次多项式函数等）的单个神经元模型，可以使用已知的在敌对标签噪声下拟合“线性函数”的主动学习策略获得强有力的可证明的逼近保证。

    We consider the problem of active learning for single neuron models, also sometimes called ``ridge functions'', in the agnostic setting (under adversarial label noise). Such models have been shown to be broadly effective in modeling physical phenomena, and for constructing surrogate data-driven models for partial differential equations.  Surprisingly, we show that for a single neuron model with any Lipschitz non-linearity (such as the ReLU, sigmoid, absolute value, low-degree polynomial, among others), strong provable approximation guarantees can be obtained using a well-known active learning strategy for fitting \emph{linear functions} in the agnostic setting. % -- i.e. for the case when there is no non-linearity. Namely, we can collect samples via statistical \emph{leverage score sampling}, which has been shown to be near-optimal in other active learning scenarios. We support our theoretical results with empirical simulations showing that our proposed active learning strategy based o
    
[^174]: 基于联邦生成对抗网络的医学图像合成中的后门攻击与防御

    Backdoor Attack and Defense in Federated Generative Adversarial Network-based Medical Image Synthesis. (arXiv:2210.10886v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.10886](http://arxiv.org/abs/2210.10886)

    本研究调查了联邦生成对抗网络中后门攻击的被忽视问题，并发现成功攻击是由于部分本地判别器对毒素过度拟合所致。

    

    深度学习基于图像合成的技术已经应用于医疗保健研究中，用于生成医学图像以支持开放研究并增加医学数据集。训练生成对抗神经网络（GANs）通常需要大量的训练数据。联邦学习（FL）提供了一种使用分布式数据训练中央模型并保持本地原始数据的方法。然而，考虑到FL服务器无法访问原始数据，它容易受到后门攻击的影响，后门攻击是一种通过污染训练数据的对抗性攻击。大多数后门攻击策略集中在分类模型和中心化领域。现有的后门攻击能否影响GAN训练仍然是一个开放问题，如果可以影响，如何在FL环境中进行防御也是一个问题。在这项研究中，我们调查了联邦GANs（FedGANs）中后门攻击这个被忽视的问题。攻击的成功随后被确定为部分本地判别器对毒素过度拟合的结果。

    Deep Learning-based image synthesis techniques have been applied in healthcare research for generating medical images to support open research and augment medical datasets. Training generative adversarial neural networks (GANs) usually require large amounts of training data. Federated learning (FL) provides a way of training a central model using distributed data while keeping raw data locally. However, given that the FL server cannot access the raw data, it is vulnerable to backdoor attacks, an adversarial by poisoning training data. Most backdoor attack strategies focus on classification models and centralized domains. It is still an open question if the existing backdoor attacks can affect GAN training and, if so, how to defend against the attack in the FL setting. In this work, we investigate the overlooked issue of backdoor attacks in federated GANs (FedGANs). The success of this attack is subsequently determined to be the result of some local discriminators overfitting the poison
    
[^175]: 有限时域受限马尔可夫决策过程的策略梯度方法

    A policy gradient approach for Finite Horizon Constrained Markov Decision Processes. (arXiv:2210.04527v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.04527](http://arxiv.org/abs/2210.04527)

    本文提出了一种针对有限时域受限马尔可夫决策过程的策略梯度方法，该方法能够在固定时间后终止，通过函数逼近和策略梯度方法找到最优策略。

    

    无限时域设置通常用于强化学习问题，导致产生最优的固定策略。然而，在许多情况下，有限时域控制问题更具有实际意义，并且在这种情况下，最优策略通常随时间变化。最近，约束强化学习的设置也越来越受到关注，其中代理同时在最大化奖励的同时满足某些给定的约束条件。然而，这个设置仅在无限时域马尔可夫决策过程的背景下得到了研究，其中固定策略是最优的。本文提出了一种在有限时域设置下进行约束强化学习的算法，其中在一个固定的时间后终止。我们在算法中使用函数逼近，这在状态和动作空间较大或连续的情况下是必不可少的，并使用策略梯度方法来找到最优策略。我们得到的最优策略取决于时间段。

    The infinite horizon setting is widely adopted for problems of reinforcement learning (RL). These invariably result in stationary policies that are optimal. In many situations, finite horizon control problems are of interest and for such problems, the optimal policies are time-varying in general. Another setting that has become popular in recent times is of Constrained Reinforcement Learning, where the agent maximizes its rewards while it also aims to satisfy some given constraint criteria. However, this setting has only been studied in the context of infinite horizon MDPs where stationary policies are optimal. We present an algorithm for constrained RL in the Finite Horizon Setting where the horizon terminates after a fixed (finite) time. We use function approximation in our algorithm which is essential when the state and action spaces are large or continuous and use the policy gradient method to find the optimal policy. The optimal policy that we obtain depends on the stage and so is
    
[^176]: 使用高斯混合模型的自然梯度变分推断的统一视角

    A Unified Perspective on Natural Gradient Variational Inference with Gaussian Mixture Models. (arXiv:2209.11533v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.11533](http://arxiv.org/abs/2209.11533)

    本论文提出了一种统一的视角来理解使用高斯混合模型进行自然梯度变分推断的方法。研究发现，VIPS和iBayes-GMM这两种目前最有效的方法，在更新各个组件和权重时使用的自然梯度更新是等价的，但其实现和理论保证存在差异。研究还发现，这两种方法在样本选择、自然梯度估计、步长适应以及可信区域或组件数量的调整等设计选择上存在区别，对于学习近似的质量有重要影响。

    

    使用高斯混合模型（GMM）进行变分推断能够以高度可行但多模态的方式学习难以处理的目标分布，具有最多几百个维度。目前对于基于GMM的变分推断来说，VIPS和iBayes-GMM是最有效的两种方法，它们都使用独立的自然梯度更新来更新各个组件及其权重。我们首次证明了它们派生的更新是等价的，尽管它们的实际实现和理论保证有所不同。我们确定了几个区分这两种方法的设计选择，包括样本选择、自然梯度估计、步长适应以及是否强制实施可信区域或调整组件的数量。我们认为，对于这两种方法，所学近似的质量可能会受到相应设计选择的严重影响：通过使用混合模型中的样本来更新各个组件，iBayes-GMM的学习近似质量可能受到更严重影响。

    Variational inference with Gaussian mixture models (GMMs) enables learning of highly tractable yet multi-modal approximations of intractable target distributions with up to a few hundred dimensions. The two currently most effective methods for GMM-based variational inference, VIPS and iBayes-GMM, both employ independent natural gradient updates for the individual components and their weights. We show for the first time, that their derived updates are equivalent, although their practical implementations and theoretical guarantees differ. We identify several design choices that distinguish both approaches, namely with respect to sample selection, natural gradient estimation, stepsize adaptation, and whether trust regions are enforced or the number of components adapted. We argue that for both approaches, the quality of the learned approximations can heavily suffer from the respective design choices: By updating the individual components using samples from the mixture model, iBayes-GMM of
    
[^177]: 基于记忆增强的图神经网络：一个受启发于大脑的综述

    Memory-Augmented Graph Neural Networks: A Brain-Inspired Review. (arXiv:2209.10818v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.10818](http://arxiv.org/abs/2209.10818)

    本文提供了一个关于记忆增强型图神经网络的全面回顾，通过心理学和神经科学的视角，提出了分类法和比较标准，并讨论了其局限性和未来发展方向。

    

    我们通过心理学和神经科学的视角对现有的记忆增强型图神经网络文献进行了全面回顾。我们提出了一个记忆增强型图神经网络的分类法和一组用于比较它们记忆机制的标准。我们还对这些研究的局限性进行了深入讨论。最后，我们讨论了这个领域的挑战和未来发展方向。

    We provide a comprehensive review of the existing literature on memory-augmented GNNs. We review these works through the lens of psychology and neuroscience, which has several established theories on how multiple memory systems and mechanisms operate in biological brains. We propose a taxonomy of memory-augmented GNNs and a set of criteria for comparing their memory mechanisms. We also provide critical discussions on the limitations of these works. Finally, we discuss the challenges and future directions for this area.
    
[^178]: 基于对比特征学习的基于行为的早期自闭症诊断

    Action-based Early Autism Diagnosis Using Contrastive Feature Learning. (arXiv:2209.05379v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.05379](http://arxiv.org/abs/2209.05379)

    本研究提出了一种基于对比特征学习的基于行为的早期自闭症诊断方法，通过简单且少量的被试动作视频剪辑自动化诊断自闭症，克服了可用数据量小和样本变化大的挑战。

    

    自闭症，也被称为自闭症谱系障碍（ASD），是一种神经性疾病。其主要症状包括(口语和/或非口语)沟通困难和僵化/重复行为。这些症状常常难以与正常（对照）个体区分，因此这种障碍在儿童早期仍然未被诊断出来，从而导致延迟治疗。由于在早期年龄段学习曲线陡峭，早期诊断自闭症可以在正确的时间采取适当的干预措施，可能对自闭症孩子的成长产生积极影响。此外，传统的自闭症诊断方法需要多次就诊于专科精神科医生，然而这一过程可能耗时。在本文中，我们提出了一种基于学习的方法，利用简单且小的被试动作视频剪辑来自动化自闭症诊断。这项任务特别具有挑战性，因为可用的注释数据量很小，并且样本之间存在变化。

    Autism, also known as Autism Spectrum Disorder (or ASD), is a neurological disorder. Its main symptoms include difficulty in (verbal and/or non-verbal) communication, and rigid/repetitive behavior. These symptoms are often indistinguishable from a normal (control) individual, due to which this disorder remains undiagnosed in early childhood leading to delayed treatment. Since the learning curve is steep during the initial age, an early diagnosis of autism could allow to take adequate interventions at the right time, which might positively affect the growth of an autistic child. Further, the traditional methods of autism diagnosis require multiple visits to a specialized psychiatrist, however this process can be time-consuming. In this paper, we present a learning based approach to automate autism diagnosis using simple and small action video clips of subjects. This task is particularly challenging because the amount of annotated data available is small, and the variations among samples
    
[^179]: 可解释的气候科学的核心学习

    Kernel Learning for Explainable Climate Science. (arXiv:2209.04947v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.04947](http://arxiv.org/abs/2209.04947)

    本文通过使用具有结构化非平稳核的高斯过程来模拟上游印度河流域的降水模式，解决了对该地区复杂时空降水分布的理解不足的问题。

    

    喜马拉雅山上游印度河流域为2.7亿人口和无数生态系统提供水资源。然而，降水作为水文模拟的关键组成部分，在这个地区的理解还很有限。这种不确定性围绕在河流域的复杂时空降水分布。本文提出使用具有结构化非平稳核的高斯过程来模拟上游印度河流域的降水模式。以往在印度喜马拉雅山区量化或模拟降水的尝试往往是定性的，包括了粗糙的假设和简化，无法解决低分辨率下的问题。此外，这些研究几乎没有考虑误差传播。我们利用非平稳的吉布斯核和依赖输入的长度参数来考虑降水的空间变化，使得后验函数样本能够适应这一区域固有的降水模式的多样性。

    The Upper Indus Basin, Himalayas provides water for 270 million people and countless ecosystems. However, precipitation, a key component to hydrological modelling, is poorly understood in this area. A key challenge surrounding this uncertainty comes from the complex spatial-temporal distribution of precipitation across the basin. In this work we propose Gaussian processes with structured non-stationary kernels to model precipitation patterns in the UIB. Previous attempts to quantify or model precipitation in the Hindu Kush Karakoram Himalayan region have often been qualitative or include crude assumptions and simplifications which cannot be resolved at lower resolutions. This body of research also provides little to no error propagation. We account for the spatial variation in precipitation with a non-stationary Gibbs kernel parameterised with an input dependent lengthscale. This allows the posterior function samples to adapt to the varying precipitation patterns inherent in the distin
    
[^180]: 自适应重求解的最优正则在线分配算法

    Optimal Regularized Online Allocation by Adaptive Re-Solving. (arXiv:2209.00399v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.00399](http://arxiv.org/abs/2209.00399)

    本文提出了一种自适应重求解的最优正则在线分配算法，该算法通过对偶方法解决了具有非凸累积奖励、硬资源约束和非分离正则化器的问题，并消除了遗憾界中的对数对数因子。该算法具有灵活性和高效性，并且通过不经常重新求解的方案降低了计算需求，同时保持了最优遗憾性能。

    

    本文介绍了一种基于对偶的算法框架，用于解决具有潜在非凸累积奖励、硬资源约束和非分离正则化器的正则在线资源分配问题。在自适应更新资源约束的策略下，所提出的框架仅要求对经验对偶问题进行近似解，以达到一定的精度，并在局部二阶增长条件下实现了最优的对数遗憾。令人惊讶的是，对偶目标函数的精细分析使得我们能够消除遗憾界中的臭名昭著的对数对数因子。这种灵活的框架立即适用于著名的计算快速算法，例如对偶随机梯度下降。此外，还提出了一种不经常重新求解的方案，可以显著减少计算需求，且不会影响最优遗憾性能。同时建立了最坏情况下的平方根遗憾下界。

    This paper introduces a dual-based algorithm framework for solving the regularized online resource allocation problems, which have potentially non-concave cumulative rewards, hard resource constraints, and a non-separable regularizer. Under a strategy of adaptively updating the resource constraints, the proposed framework only requests approximate solutions to the empirical dual problems up to a certain accuracy and yet delivers an optimal logarithmic regret under a locally second-order growth condition. Surprisingly, a delicate analysis of the dual objective function enables us to eliminate the notorious log-log factor in regret bound. The flexible framework renders renowned and computationally fast algorithms immediately applicable, e.g., dual stochastic gradient descent. Additionally, an infrequent re-solving scheme is proposed, which significantly reduces computational demands without compromising the optimal regret performance. A worst-case square-root regret lower bound is establ
    
[^181]: 自带长尾数据的标签噪声学习

    Label-Noise Learning with Intrinsically Long-Tailed Data. (arXiv:2208.09833v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.09833](http://arxiv.org/abs/2208.09833)

    本文针对自带长尾数据的标签噪声学习问题，提出了一个学习框架并设计了双维样本选择算法 TABASCO，有效地将干净样本与噪声样本分开，特别适用于长尾类别。

    

    标签噪声是导致深度学习模型泛化能力差的关键因素之一。现有的标签噪声学习方法通常假设训练数据的真实类别是平衡的。然而，现实世界的数据往往是不平衡的，导致观察到的和内在类别分布之间存在不一致的标签噪声。在这种情况下，很难在具有未知内在类别分布的长尾类上区分干净样本和噪声样本。本文提出了一种用于自带长尾数据的标签噪声学习的学习框架。具体而言，我们提出了两阶段的双维样本选择（TABASCO）来更好地将干净样本与噪声样本分开，尤其是对于尾部类别。TABASCO包括两个新的分离度量，相互补充，弥补了在样本分离中使用单个度量的局限性。对我们提出的具有真实数据的基准进行了大量实验证明。

    Label noise is one of the key factors that lead to the poor generalization of deep learning models. Existing label-noise learning methods usually assume that the ground-truth classes of the training data are balanced. However, the real-world data is often imbalanced, leading to the inconsistency between observed and intrinsic class distribution with label noises. In this case, it is hard to distinguish clean samples from noisy samples on the intrinsic tail classes with the unknown intrinsic class distribution. In this paper, we propose a learning framework for label-noise learning with intrinsically long-tailed data. Specifically, we propose two-stage bi-dimensional sample selection (TABASCO) to better separate clean samples from noisy samples, especially for the tail classes. TABASCO consists of two new separation metrics that complement each other to compensate for the limitation of using a single metric in sample separation. Extensive experiments on benchmarks we proposed with real-
    
[^182]: SAFARI：鲁棒性可解释性评估的多功能高效方法

    SAFARI: Versatile and Efficient Evaluations for Robustness of Interpretability. (arXiv:2208.09418v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.09418](http://arxiv.org/abs/2208.09418)

    本文提出了一种名为SAFARI的方法，用于评估深度学习的解释可靠性。该方法针对现有技术无法解决的几个挑战，通过引入两种黑盒评估方法，即最坏情况解释差异和一般情况下的鲁棒性的概率概念，来解决现有度量不全面、XAI技术异质性和误解罕见性等问题。使用遗传算法和子集模拟进行评估。

    

    深度学习的可解释性是建立可信赖的人工智能的一道障碍。尽管可解释人工智能（XAI）社区做出了巨大的努力，但解释缺乏鲁棒性——无法区分的输入扰动可能会导致不同的解释结果。因此，针对给定的XAI方法评估深度学习可解释性的鲁棒性至关重要。本文识别了现有技术无法共同应对的几个挑战：i)现有指标不全面；ii)XAI技术高度异质；iii)误解通常是罕见事件。为了解决这些挑战，我们引入了两种黑盒评估方法，分别涉及最坏情况解释差异和一般情况下的鲁棒性的概率概念。使用具有定制适应度函数的遗传算法（GA）来解决约束优化，以实现高效的最坏情况评估。使用专门用于估计罕见事件概率的子集模拟（SS）来进行整体评估。

    Interpretability of Deep Learning (DL) is a barrier to trustworthy AI. Despite great efforts made by the Explainable AI (XAI) community, explanations lack robustness -- indistinguishable input perturbations may lead to different XAI results. Thus, it is vital to assess how robust DL interpretability is, given an XAI method. In this paper, we identify several challenges that the state-of-the-art is unable to cope with collectively: i) existing metrics are not comprehensive; ii) XAI techniques are highly heterogeneous; iii) misinterpretations are normally rare events. To tackle these challenges, we introduce two black-box evaluation methods, concerning the worst-case interpretation discrepancy and a probabilistic notion of how robust in general, respectively. Genetic Algorithm (GA) with bespoke fitness function is used to solve constrained optimisation for efficient worst-case evaluation. Subset Simulation (SS), dedicated to estimate rare event probabilities, is used for evaluating overa
    
[^183]: 一种比较多个机器学习算法在多个数据集上的贝叶斯Bradley-Terry模型

    A Bayesian Bradley-Terry model to compare multiple ML algorithms on multiple data sets. (arXiv:2208.04935v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.04935](http://arxiv.org/abs/2208.04935)

    本文提出了一种基于贝叶斯模型的Bradley-Terry模型，用于比较多个机器学习算法在多个数据集上的性能，与传统方法不同，贝叶斯方法能提供更细致的算法之间差异描述，并允许对等效性进行定义。

    

    本文提出了一种基于贝叶斯模型的方法，用于比较多个算法在多个数据集上的表现。该模型基于Bradley-Terry模型，统计了一个算法在不同数据集上优于另一个算法的次数。与频率派方法（如Demsar（2006）的平均排名比较测试和Benavoli等人（2016）的多个配对Wilcoxon测试与p调整过程）相比，基于贝叶斯的Bradley-Terry模型（BBT）具有不同的特点。特别是，贝叶斯方法允许对算法进行更加细致的描述，而不仅仅声称差异具有或不具有统计显著性。贝叶斯方法还允许定义两个算法在实际目的下是否等效，或实际等效区域（ROPE）。与Benavoli等人（2017）提出的贝叶斯符号秩比较方法不同，我们的方法具有一些独特的特点。

    This paper proposes a Bayesian model to compare multiple algorithms on multiple data sets, on any metric. The model is based on the Bradley-Terry model, that counts the number of times one algorithm performs better than another on different data sets. Because of its Bayesian foundations, the Bayesian Bradley Terry model (BBT) has different characteristics than frequentist approaches to comparing multiple algorithms on multiple data sets, such as Demsar (2006) tests on mean rank, and Benavoli et al. (2016) multiple pairwise Wilcoxon tests with p-adjustment procedures. In particular, a Bayesian approach allows for more nuanced statements regarding the algorithms beyond claiming that the difference is or it is not statistically significant. Bayesian approaches also allow to define when two algorithms are equivalent for practical purposes, or the region of practical equivalence (ROPE). Different than a Bayesian signed rank comparison procedure proposed by Benavoli et al. (2017), our approa
    
[^184]: 无监督预训练医疗图谱中的图变换器

    Unsupervised pre-training of graph transformers on patient population graphs. (arXiv:2207.10603v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.10603](http://arxiv.org/abs/2207.10603)

    本研究提出了一种针对多模态临床数据的图变换器的无监督预训练方法，通过在患者人口图上利用图深度学习，可以提高患者预测结果的性能。

    

    预训练在计算机视觉、自然语言处理和医学图像等机器学习领域已经取得了成功，但在临床数据分析方面尚未得到充分探索。在小型医院或处理罕见疾病的情况下，虽然记录了大量的临床记录，但数据和标签仍然可能稀缺。在这种情况下，预训练在更大规模的无标签临床数据集上可能会提高性能。本文提出了针对多模态临床数据的新型无监督预训练技术，用于患者预测结果，灵感来自掩码语言建模（MLM），通过在患者人口图上利用图深度学习。为此，我们进一步提出了一种基于图变换器的网络，用于处理异质临床数据。通过将基于掩码的预训练与基于变换器的网络相结合，我们将在其他领域中基于掩码的预训练的成功应用于临床数据分析。

    Pre-training has shown success in different areas of machine learning, such as Computer Vision, Natural Language Processing (NLP), and medical imaging. However, it has not been fully explored for clinical data analysis. An immense amount of clinical records are recorded, but still, data and labels can be scarce for data collected in small hospitals or dealing with rare diseases. In such scenarios, pre-training on a larger set of unlabelled clinical data could improve performance. In this paper, we propose novel unsupervised pre-training techniques designed for heterogeneous, multi-modal clinical data for patient outcome prediction inspired by masked language modeling (MLM), by leveraging graph deep learning over population graphs. To this end, we further propose a graph-transformer-based network, designed to handle heterogeneous clinical data. By combining masking-based pre-training with a transformer-based network, we translate the success of masking-based pre-training in other domain
    
[^185]: 半监督的跨语言语音情绪识别

    Semi-supervised cross-lingual speech emotion recognition. (arXiv:2207.06767v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2207.06767](http://arxiv.org/abs/2207.06767)

    通过半监督学习方法，我们提出了一种基于Transformer的半监督跨语言情绪识别方法，通过在未标注的语句上应用伪标签策略来适应新领域，有效解决了跨语言情绪识别中标注数据不足和领域差异大的问题。

    

    过去几年来，由于深度学习技术的使用，单语种语音情绪识别（SER）的性能大幅提升。然而，由于源语种和目标语种之间存在较大差异以及目标语种中缺乏标注数据，跨语言SER在实际应用中仍然是一个挑战。考虑到这些因素，本文提出了一种半监督学习方法，用于当目标语种（即新语言）中只有少量标注样本可用时的跨语言情绪识别。我们的方法基于Transformer，并通过在未标注的语句上利用伪标签策略来适应新领域。具体而言，我们研究了硬伪标签和软伪标签两种方法。我们对提出的方法在独立说话人的设定下进行了全面评估，同时涵盖了两个目标语种。

    Performance in Speech Emotion Recognition (SER) on a single language has increased greatly in the last few years thanks to the use of deep learning techniques. However, cross-lingual SER remains a challenge in real-world applications due to two main factors: the first is the big gap among the source and the target domain distributions; the second factor is the major availability of unlabeled utterances in contrast to the labeled ones for the new language. Taking into account previous aspects, we propose a Semi-Supervised Learning (SSL) method for cross-lingual emotion recognition when only few labeled examples in the target domain (i.e. the new language) are available. Our method is based on a Transformer and it adapts to the new domain by exploiting a pseudo-labeling strategy on the unlabeled utterances. In particular, the use of a hard and soft pseudo-labels approach is investigated. We thoroughly evaluate the performance of the proposed method in a speaker-independent setup on both 
    
[^186]: 使用邻域不变性预测域外泛化

    Predicting Out-of-Domain Generalization with Neighborhood Invariance. (arXiv:2207.02093v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.02093](http://arxiv.org/abs/2207.02093)

    提出了一种测量分类器输出在局部转换邻域中不变性的方法，用于描述模型的泛化能力，不依赖于数据分布或模型假设，可应用于域外环境。

    

    安全地开发和部署机器学习模型取决于对其泛化能力在新环境中的特征和比较能力。尽管最近的工作提出了一系列可以直接预测或理论上限制模型的泛化能力的方法，但它们都依赖于匹配的训练/测试分布和访问模型梯度等强假设。为了在这些假设不满足时描述泛化能力，我们提出了邻域不变性，一种分类器在局部转换邻域中输出不变的度量。具体而言，我们采样一组转换，对于一个输入测试点，计算不变性作为被分类为同一类别的转换点的最大比例。关键的是，我们的度量方法简单易计算，不依赖于测试点的真实标签，不对数据分布或模型做任何假设，甚至可以在域外环境下应用。

    Developing and deploying machine learning models safely depends on the ability to characterize and compare their abilities to generalize to new environments. Although recent work has proposed a variety of methods that can directly predict or theoretically bound the generalization capacity of a model, they rely on strong assumptions such as matching train/test distributions and access to model gradients. In order to characterize generalization when these assumptions are not satisfied, we propose neighborhood invariance, a measure of a classifier's output invariance in a local transformation neighborhood. Specifically, we sample a set of transformations and given an input test point, calculate the invariance as the largest fraction of transformed points classified into the same class. Crucially, our measure is simple to calculate, does not depend on the test point's true label, makes no assumptions about the data distribution or model, and can be applied even in out-of-domain (OOD) setti
    
[^187]: SGD和权重衰减在神经网络中被证明会引入低秩偏差

    SGD and Weight Decay Provably Induce a Low-Rank Bias in Neural Networks. (arXiv:2206.05794v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.05794](http://arxiv.org/abs/2206.05794)

    使用SGD和权重衰减训练深度ReLU神经网络会导致对于权重矩阵的秩最小化的偏差，特别是在使用较小批量大小、更高学习率或增加权重衰减时更为显著。此外，在中间神经网络崩溃时，学习的权重特别低秩。这种偏差与泛化之间存在关系。

    

    我们研究了使用随机梯度下降（SGD）在训练深度ReLU神经网络时学习低秩权重矩阵的偏差。我们的结果表明，使用小批量SGD和权重衰减来训练神经网络会导致对于权重矩阵的秩最小化的偏差。具体而言，我们通过理论和实验证明，当使用较小的批量大小、更高的学习率或增加的权重衰减时，这种偏差更加显著。此外，我们预测并通过实验证明，权重衰减是实现这种偏差的必要条件。此外，我们还发现在中间神经网络崩溃的情况下，学习的权重特别低秩。与先前的文献不同，我们的分析不依赖于关于数据、收敛性或权重矩阵优化的假设。此外，它适用于任意宽度或深度的各种神经网络结构。最后，我们通过实验证明了这种偏差与泛化之间的关系。

    We study the bias of Stochastic Gradient Descent (SGD) to learn low-rank weight matrices when training deep ReLU neural networks. Our results show that training neural networks with mini-batch SGD and weight decay causes a bias towards rank minimization over the weight matrices. Specifically, we show, both theoretically and empirically, that this bias is more pronounced when using smaller batch sizes, higher learning rates, or increased weight decay. Additionally, we predict and observe empirically that weight decay is necessary to achieve this bias. In addition, we show that in the presence of intermediate neural collapse, the learned weights are particularly low-rank. Unlike previous literature, our analysis does not rely on assumptions about the data, convergence, or optimality of the weight matrices. Furthermore, it applies to a wide range of neural network architectures of any width or depth. Finally, we empirically investigate the connection between this bias and generalization, 
    
[^188]: 在线深度强化学习上的高效奖励污染攻击

    Efficient Reward Poisoning Attacks on Online Deep Reinforcement Learning. (arXiv:2205.14842v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.14842](http://arxiv.org/abs/2205.14842)

    本文通过设计一种通用的奖励污染框架展示了现有深度强化学习算法内在的漏洞，提出了两种新攻击方式，成功污染多种最先进DRL算法的智能体在多个环境中学习的效率，给深度强化学习提供了新的安全风险。

    

    本文研究了在线深度强化学习中的奖励污染攻击，攻击者对智能体使用的学习算法和环境动态一无所知。我们通过设计一种称为对抗MDP攻击的通用黑盒奖励污染框架，展示了现有深度强化学习算法内在的漏洞。我们将该框架实例化为两种新攻击，只破坏了总训练时间步数的少量奖励，并使代理学习低效策略。我们对攻击效率进行了理论分析，并进行了广泛的实证评估。结果表明我们的攻击能有效地污染使用多种最先进DRL算法（如DQN、PPO、SAC等）来学习的智能体，在多个受欢迎的经典控制和MuJoCo环境中具有较高效率。

    We study reward poisoning attacks on online deep reinforcement learning (DRL), where the attacker is oblivious to the learning algorithm used by the agent and the dynamics of the environment. We demonstrate the intrinsic vulnerability of state-of-the-art DRL algorithms by designing a general, black-box reward poisoning framework called adversarial MDP attacks. We instantiate our framework to construct two new attacks which only corrupt the rewards for a small fraction of the total training timesteps and make the agent learn a low-performing policy. We provide a theoretical analysis of the efficiency of our attack and perform an extensive empirical evaluation. Our results show that our attacks efficiently poison agents learning in several popular classical control and MuJoCo environments with a variety of state-of-the-art DRL algorithms, such as DQN, PPO, SAC, etc.
    
[^189]: 信息导向选择的前两个算法

    Information-Directed Selection for Top-Two Algorithms. (arXiv:2205.12086v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.12086](http://arxiv.org/abs/2205.12086)

    本文研究了多臂赌博机中最佳k臂识别问题，提出了一种信息导向选择的算法（IDS），并证明了与IDS集成的顶部两个汤姆逊采样在高斯最佳臂识别中达到了最优。

    

    我们考虑多臂赌博机的最佳k臂识别问题，其目标是通过顺序分配测量努力来选择具有最高平均奖励的k臂准确集合。我们使用对偶变量来表征最优分配的必要和充分条件。值得注意的是，这些最优性条件导致了顶部两个算法设计原则（Russo, 2020）的扩展，这最初是为了最佳臂识别而提出的。此外，我们的最优性条件引出了一种简单而有效的选择规则，称为信息导向选择（IDS），它根据信息增益的度量选择前两个候选中的一个。作为理论保证，我们证明了与IDS集成的顶部两个汤姆逊采样在高斯最佳臂识别中（渐近地）达到了最优，从而解决了纯探索文献中突出的一个未解决问题（Russo, 2020）。作为副产品，我们还表明，对于k > 1，顶部两个算法无法实现最优化。

    We consider the best-k-arm identification problem for multi-armed bandits, where the objective is to select the exact set of k arms with the highest mean rewards by sequentially allocating measurement effort. We characterize the necessary and sufficient conditions for the optimal allocation using dual variables. Remarkably these optimality conditions lead to the extension of top-two algorithm design principle (Russo, 2020), initially proposed for best-arm identification. Furthermore, our optimality conditions induce a simple and effective selection rule dubbed information-directed selection (IDS) that selects one of the top-two candidates based on a measure of information gain. As a theoretical guarantee, we prove that integrated with IDS, top-two Thompson sampling is (asymptotically) optimal for Gaussian best-arm identification, solving a glaring open problem in the pure exploration literature (Russo, 2020). As a by-product, we show that for k > 1, top-two algorithms cannot achieve op
    
[^190]: 连续时间平均奖励马尔可夫决策过程的对数遗憾界限

    Logarithmic regret bounds for continuous-time average-reward Markov decision processes. (arXiv:2205.11168v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.11168](http://arxiv.org/abs/2205.11168)

    这项研究考虑了连续时间马尔可夫决策过程（MDPs）的平均奖励设置下的强化学习问题，并找到了实例相关的对数遗憾下界，并设计出了一个能够实现对数增长速率的学习算法。

    

    我们考虑了在无限时间跨度、平均奖励设定下的连续时间马尔可夫决策过程（MDPs）的强化学习。与离散时间MDPs不同，连续时间过程在采取行动后会移动到一个状态并在此停留一个随机持续时间。在未知的转移概率和指数持续时间变化率下，我们得到了一个与时间跨度对数相关的实例相关遗憾下界。此外，我们设计了一个学习算法，并建立了一个有限时间的遗憾界限，能够实现对数增长速率。我们的分析建立在上限置信增强学习、均值持续时间的精细估计以及点过程的随机比较之上。

    We consider reinforcement learning for continuous-time Markov decision processes (MDPs) in the infinite-horizon, average-reward setting. In contrast to discrete-time MDPs, a continuous-time process moves to a state and stays there for a random holding time after an action is taken. With unknown transition probabilities and rates of exponential holding times, we derive instance-dependent regret lower bounds that are logarithmic in the time horizon. Moreover, we design a learning algorithm and establish a finite-time regret bound that achieves the logarithmic growth rate. Our analysis builds upon upper confidence reinforcement learning, a delicate estimation of the mean holding times, and stochastic comparison of point processes.
    
[^191]: 无监督发现和合成物体光场

    Unsupervised Discovery and Composition of Object Light Fields. (arXiv:2205.03923v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2205.03923](http://arxiv.org/abs/2205.03923)

    本文提出了一种无监督发现和合成物体光场的方法，通过将物体表示为以物体为中心的光场来提高渲染质量和操作效率。

    

    近期，神经场景表示法，包括连续和离散表示，已经成为三维场景理解的强大新范式。最近的研究致力于无监督发现以物体为中心的神经场景表示法。然而，射线行进的高成本，加上每个物体表示法都必须单独进行射线行进的事实，导致采样不足的亮度场，从而产生噪点渲染、低帧率、高内存和时间复杂度的训练和渲染。在这里，我们提出将物体以物体为中心的光场表示法来表示。我们提出了一种新颖的光场复合模块，可以从一组以物体为中心的光场重建全局光场。我们的方法被称为组合性物体光场（COLF），可以实现无监督学习以物体为中心的神经场景表示法，在标准数据集上实现最先进的重建和新视角合成性能。

    Neural scene representations, both continuous and discrete, have recently emerged as a powerful new paradigm for 3D scene understanding. Recent efforts have tackled unsupervised discovery of object-centric neural scene representations. However, the high cost of ray-marching, exacerbated by the fact that each object representation has to be ray-marched separately, leads to insufficiently sampled radiance fields and thus, noisy renderings, poor framerates, and high memory and time complexity during training and rendering. Here, we propose to represent objects in an object-centric, compositional scene representation as light fields. We propose a novel light field compositor module that enables reconstructing the global light field from a set of object-centric light fields. Dubbed Compositional Object Light Fields (COLF), our method enables unsupervised learning of object-centric neural scene representations, state-of-the-art reconstruction and novel view synthesis performance on standard 
    
[^192]: 非平稳赌博机学习的预测抽样方法

    Non-Stationary Bandit Learning via Predictive Sampling. (arXiv:2205.01970v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.01970](http://arxiv.org/abs/2205.01970)

    本文提出了一种预测抽样算法，用于解决非平稳赌博机学习问题。该算法通过降低获取信息的优先级，解决了Thompson抽样在非平稳环境下表现不佳的问题，并在所有非平稳环境中优于Thompson抽样。

    This paper proposes a predictive sampling algorithm to solve the non-stationary bandit learning problem. By deprioritizing the acquisition of information that quickly loses usefulness, the algorithm outperforms Thompson sampling in all non-stationary environments examined.

    Thompson抽样已经在广泛的平稳赌博机环境中证明了其有效性。然而，正如我们在本文中所展示的，当应用于非平稳环境时，它的表现可能很差。我们表明，这样的失败是由于在探索时，算法没有根据由于非平稳性导致信息快速失去有用性的速度区分行动。基于这一洞见，我们提出了预测抽样算法，该算法降低了获取信息的优先级，这些信息由于快速失去有用性而不再重要。通过贝叶斯遗憾界，我们建立了预测抽样性能的理论保证。我们提供了预测抽样的版本，其计算可扩展到实际感兴趣的复杂赌博机环境。通过数值模拟，我们证明了预测抽样在所有非平稳环境中都优于Thompson抽样。

    Thompson sampling has proven effective across a wide range of stationary bandit environments. However, as we demonstrate in this paper, it can perform poorly when applied to non-stationary environments. We show that such failures are attributed to the fact that, when exploring, the algorithm does not differentiate actions based on how quickly the information acquired loses its usefulness due to non-stationarity. Building upon this insight, we propose predictive sampling, an algorithm that deprioritizes acquiring information that quickly loses usefulness. Theoretical guarantee on the performance of predictive sampling is established through a Bayesian regret bound. We provide versions of predictive sampling for which computations tractably scale to complex bandit environments of practical interest. Through numerical simulations, we demonstrate that predictive sampling outperforms Thompson sampling in all non-stationary environments examined.
    
[^193]: 探索无代理数据的分布式知识一致性在联邦蒸馏中的应用

    Exploring the Distributed Knowledge Congruence in Proxy-data-free Federated Distillation. (arXiv:2204.07028v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.07028](http://arxiv.org/abs/2204.07028)

    本文提出了一种基于分布式知识一致性的无代理数据联邦蒸馏算法，解决了客户端模型异质性引起的知识差异问题，从而提高了模型表示的准确性。

    

    联邦学习 (FL) 是一种保护隐私的机器学习范 paradigm, 在此服务器周期性地收集客户端的本地模型参数, 而不组装其私有数据. 有限的通讯和个性化需求对FL提出了严峻挑战. 联邦蒸馏 (FD) 被提出同时解决上述两个问题, 与此服务器和客户端之间交换知识, 支持异构本地模型同时显著减少通讯开销. 然而，大多数现有的FD方法需要一个代理数据集，而这在现实中通常是不可用的. 一些最近的无代理数据的FD方法可以消除额外的公共数据的需求, 但由于客户端模型的异质性而产生了明显的差异, 导致服务器上的模型表示不明确，并且不可避免地降低了准确性.

    Federated learning (FL) is a privacy-preserving machine learning paradigm in which the server periodically aggregates local model parameters from clients without assembling their private data.  Constrained communication and personalization requirements pose severe challenges to FL. Federated distillation (FD) is proposed to simultaneously address the above two problems, which exchanges knowledge between the server and clients, supporting heterogeneous local models while significantly reducing communication overhead. However, most existing FD methods require a proxy dataset, which is often unavailable in reality.  A few recent proxy-data-free FD approaches can eliminate the need for additional public data, but suffer from remarkable discrepancy among local knowledge due to client-side model heterogeneity, leading to ambiguous representation on the server and inevitable accuracy degradation.  To tackle this issue, we propose a proxy-data-free FD algorithm based on distributed knowledge c
    
[^194]: 基于PDE的对称双臂伯努利赌博机的分析

    A PDE-Based Analysis of the Symmetric Two-Armed Bernoulli Bandit. (arXiv:2202.05767v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.05767](http://arxiv.org/abs/2202.05767)

    本研究通过关联线性热方程的解，得到了对称双臂伯努利赌博机问题的minmax最优遗憾和伪遗憾的领先项。新的结果改进了先前的研究，并提供了新的非渐近边界。

    

    本研究探讨了一个版本的双臂伯努利赌博机问题，其中两个臂的平均值之和为1（即对称的双臂伯努利赌博机）。在臂之间的差距趋近于零且预测期数趋近于无穷大的情况下，我们通过将每个解与线性热方程的解关联，得到了该问题的minmax最优遗憾和伪遗憾的领先项。我们的结果改进了先前已知的结果；具体而言，在三种不同的差距缩放模式下，我们明确计算了这些领先项。此外，我们还得到了任何给定时间范围的新的非渐近边界。

    This work addresses a version of the two-armed Bernoulli bandit problem where the sum of the means of the arms is one (the symmetric two-armed Bernoulli bandit). In a regime where the gap between these means goes to zero and the number of prediction periods approaches infinity, we obtain the leading order terms of the minmax optimal regret and pseudoregret for this problem by associating each of them with a solution of a linear heat equation. Our results improve upon the previously known results; specifically, we explicitly compute these leading order terms in three different scaling regimes for the gap. Additionally, we obtain new non-asymptotic bounds for any given time horizon.
    
[^195]: 在分布变化下验证模型准确性

    Certifying Model Accuracy under Distribution Shifts. (arXiv:2201.12440v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.12440](http://arxiv.org/abs/2201.12440)

    本文提出了一种针对数据分布变化的模型准确性的可证明鲁棒性保证方法，通过在转换空间内随机化模型输入来实现鲁棒性。该方法适用于不同点之间变化的数据特定的扰动大小，并且能够产生对固定大小扰动的确保下界。

    

    机器学习中的认证鲁棒性主要关注数据分布中每个数据点的固定攻击预算下对输入的对抗扰动。本文提出了在数据分布的低阶Wasserstein变换下对模型准确性的可证明鲁棒性保证。我们展示了一种简单的方法，通过在一个转换空间内对模型输入进行随机化，从而在转换下证明其对分布变化具有鲁棒性。我们的框架允许数据特定的扰动大小在输入分布的不同点之间变化，并且也包括固定大小的扰动。我们的证书为原始分布周围的任何（自然或对抗性）输入分布的Wasserstein球内的模型性能提供了确保的下界。我们将我们的技术应用于以下领域：（i）对图像进行自然（非对抗性）转换的鲁棒性认证，例如颜色转换。

    Certified robustness in machine learning has primarily focused on adversarial perturbations of the input with a fixed attack budget for each point in the data distribution. In this work, we present provable robustness guarantees on the accuracy of a model under bounded Wasserstein shifts of the data distribution. We show that a simple procedure that randomizes the input of the model within a transformation space is provably robust to distributional shifts under the transformation. Our framework allows the datum-specific perturbation size to vary across different points in the input distribution and is general enough to include fixed-sized perturbations as well. Our certificates produce guaranteed lower bounds on the performance of the model for any (natural or adversarial) shift of the input distribution within a Wasserstein ball around the original distribution. We apply our technique to: (i) certify robustness against natural (non-adversarial) transformations of images such as color 
    
[^196]: 音乐信号中的自监督节拍跟踪与多声对比学习

    Self-Supervised Beat Tracking in Musical Signals with Polyphonic Contrastive Learning. (arXiv:2201.01771v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2201.01771](http://arxiv.org/abs/2201.01771)

    本论文提出了一种新的自监督学习预训练任务，用于音乐节拍跟踪和下拍估计。通过使用音频信号分离模型，可以实现对节拍的自动标注，并在预训练模型中学习到起始函数。研究发现，预训练模型在节拍跟踪训练集非常小的情况下表现优于随机初始化模型。该工作为音乐信号处理领域的研究和实践提供了重要的贡献。

    

    标注音乐节拍是一项非常漫长乏味的过程。为了解决这个问题，我们提出了一种新的自监督学习预训练任务，用于节拍跟踪和下拍估计。该任务利用了音频信号分离模型Spleeter，将歌曲的鼓声与其余信号分离开来。第一组信号被用作对比学习预训练的正样本和负样本，而去鼓声的信号则作为锚点。通过使用这个预训练任务来对完全卷积和递归模型进行预训练，可以学习到一个起始函数。在某些情况下，发现这个函数被映射到歌曲中的周期性元素。研究发现，在节拍跟踪训练集非常小的情况下（小于10个示例），预训练模型优于随机初始化模型。当情况不是这样时，预训练会导致学习速度加快，模型过拟合于训练集。总的来说，这项工作定义了新的节拍跟踪任务和预训练方法，为音乐信号处理领域的研究和实践提供了重要的贡献。

    Annotating musical beats is a very long and tedious process. In order to combat this problem, we present a new self-supervised learning pretext task for beat tracking and downbeat estimation. This task makes use of Spleeter, an audio source separation model, to separate a song's drums from the rest of its signal. The first set of signals are used as positives, and by extension negatives, for contrastive learning pre-training. The drum-less signals, on the other hand, are used as anchors. When pre-training a fully-convolutional and recurrent model using this pretext task, an onset function is learned. In some cases, this function is found to be mapped to periodic elements in a song. We find that pre-trained models outperform randomly initialized models when a beat tracking training set is extremely small (less than 10 examples). When this is not the case, pre-training leads to a learning speed-up that causes the model to overfit to the training set. More generally, this work defines new
    
[^197]: 用相关性逼近潘多拉魔盒

    Approximating Pandora's Box with Correlations. (arXiv:2108.12976v3 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2108.12976](http://arxiv.org/abs/2108.12976)

    在这项工作中，我们通过研究相关分布下的潘多拉魔盒问题，将其与均匀决策树和最小和集覆盖问题进行了近似转化。我们的主要结果表明，这些问题都可以在亚指数时间内实现常数近似比。

    

    我们重新审视了在相关分布下的经典潘多拉魔盒（PB）问题。最近的研究得出了一种针对问题中按固定顺序访问盒子的一类限制策略的常数近似算法。在这项工作中，我们研究了近似最优策略的复杂性，该策略可以根据迄今为止看到的数值自适应地选择下一个要访问的盒子。我们的主要结果将PB与经典的均匀决策树（UDT）问题以及变体的最小和集覆盖（MSSC_f）问题进行了近似转化。对于具有支持度$m$的分布，UDT具有$\log m$的近似比，尽管在多项式时间内实现常数近似比一直是一个长期存在的开放问题，但在亚指数时间内实现常数近似比是可能的（arXiv:1906.11385）。我们的主要结果表明，PB和MSSC_f也具有相同的性质。我们还研究了c

    We revisit the classic Pandora's Box (PB) problem under correlated distributions on the box values. Recent work of arXiv:1911.01632 obtained constant approximate algorithms for a restricted class of policies for the problem that visit boxes in a fixed order. In this work, we study the complexity of approximating the optimal policy which may adaptively choose which box to visit next based on the values seen so far.  Our main result establishes an approximation-preserving equivalence of PB to the well studied Uniform Decision Tree (UDT) problem from stochastic optimization and a variant of the Min-Sum Set Cover ($\text{MSSC}_f$) problem. For distributions of support $m$, UDT admits a $\log m$ approximation, and while a constant factor approximation in polynomial time is a long-standing open problem, constant factor approximations are achievable in subexponential time (arXiv:1906.11385). Our main result implies that the same properties hold for PB and $\text{MSSC}_f$.  We also study the c
    
[^198]: 在流图中基于草图的异常检测

    Sketch-Based Anomaly Detection in Streaming Graphs. (arXiv:2106.04486v3 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2106.04486](http://arxiv.org/abs/2106.04486)

    本文提出了在动态图中以在线的方式为边和子图分配异常分数的方法，其利用了扩展的草图数据结构，并且在真实数据集上表现优于现有的方法。

    

    在动态图的图边流中，如何以在线的方式为边和子图分配异常分数，以检测异常行为，并在常数时间和内存下进行？本文首先将计数最小化草图数据结构扩展为高阶草图，该高阶草图具有保留稠密子图结构的有用属性（输入中的稠密子图转化为数据结构中的密集子矩阵）。然后，我们提出了4个利用这个增强数据结构的在线算法，这些算法（a）同时检测边和图的异常；（b）以常数内存和常数更新时间处理每个新到达的边的边和图；（c）在4个真实数据集上优于现有的基线方法。我们的方法是第一个将稠密子图搜索纳入流式方法以检测图形异常的方法。

    Given a stream of graph edges from a dynamic graph, how can we assign anomaly scores to edges and subgraphs in an online manner, for the purpose of detecting unusual behavior, using constant time and memory? For example, in intrusion detection, existing work seeks to detect either anomalous edges or anomalous subgraphs, but not both. In this paper, we first extend the count-min sketch data structure to a higher-order sketch. This higher-order sketch has the useful property of preserving the dense subgraph structure (dense subgraphs in the input turn into dense submatrices in the data structure). We then propose 4 online algorithms that utilize this enhanced data structure, which (a) detect both edge and graph anomalies; (b) process each edge and graph in constant memory and constant update time per newly arriving edge, and; (c) outperform state-of-the-art baselines on 4 real-world datasets. Our method is the first streaming approach that incorporates dense subgraph search to detect gra
    
[^199]: 神经分布式源编码

    Neural Distributed Source Coding. (arXiv:2106.02797v3 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2106.02797](http://arxiv.org/abs/2106.02797)

    这项研究提出了一种神经分布式源编码的框架，可以处理复杂的相关性并实现最先进的峰值信噪比。

    

    分布式源编码(DSC)是在没有相互关联的边际信息可供解码器使用的情况下对输入进行编码的任务。值得注意的是，Slepian和Wolf在1973年证明，没有访问边际信息的编码器可以渐近地实现与边际信息可用情况下相同的压缩率。虽然在这个领域有广泛的先前工作，但实践中的DSC一直局限于合成数据集和特定的相关结构。在这里，我们提出了一个对相关结构不可知且能够扩展到高维度的有损DSC框架。我们的方法不依赖于手工设计的源模型，而是利用条件向量量化变分自动编码器(VQ-VAE)来学习分布式编码器和解码器。我们在多个数据集上评估了我们的方法，并展示了我们的方法可以处理复杂的相关性，并实现了最先进的峰值信噪比(PSNR)。

    Distributed source coding (DSC) is the task of encoding an input in the absence of correlated side information that is only available to the decoder. Remarkably, Slepian and Wolf showed in 1973 that an encoder without access to the side information can asymptotically achieve the same compression rate as when the side information is available to it. While there is vast prior work on this topic, practical DSC has been limited to synthetic datasets and specific correlation structures. Here we present a framework for lossy DSC that is agnostic to the correlation structure and can scale to high dimensions. Rather than relying on hand-crafted source modeling, our method utilizes a conditional Vector-Quantized Variational Autoencoder (VQ-VAE) to learn the distributed encoder and decoder. We evaluate our method on multiple datasets and show that our method can handle complex correlations and achieves state-of-the-art PSNR.
    
[^200]: 从单个图像和视频中生成新颖的场景组合

    Generating Novel Scene Compositions from Single Images and Videos. (arXiv:2103.13389v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2103.13389](http://arxiv.org/abs/2103.13389)

    本研究提出了SIV-GAN，一种无条件生成模型，可以从单个图像或视频中生成新的场景组合。通过引入内容和布局分支的鉴别器架构，该模型能够生成多样化、高质量的图像，并在保留上下文的同时保持视觉逼真。

    

    在训练数据集较大的情况下，生成对抗网络（GAN）可以在图像合成任务中取得显著的性能。然而，在极低数据环境中训练GAN仍然是一个挑战，因为往往会发生过拟合，导致记忆或训练发散。在本研究中，我们引入了SIV-GAN，这是一个无条件的生成模型，可以从单个训练图像或单个视频剪辑中生成新的场景组合。我们提出了一个具有内容和布局分支的两支鉴别器架构，分别设计用于判断内部内容和场景布局的真实性。这种鉴别器设计可以合成视觉上逼真、新颖的场景组合，具有不同的内容和布局，同时保留原始样本的上下文。与以前的单图像GAN相比，我们的模型生成了更多样化、质量更高的图像，同时不限于单个图像设置。我们进一步引入了一种新的具有挑战性的情况

    Given a large dataset for training, generative adversarial networks (GANs) can achieve remarkable performance for the image synthesis task. However, training GANs in extremely low data regimes remains a challenge, as overfitting often occurs, leading to memorization or training divergence. In this work, we introduce SIV-GAN, an unconditional generative model that can generate new scene compositions from a single training image or a single video clip. We propose a two-branch discriminator architecture, with content and layout branches designed to judge internal content and scene layout realism separately from each other. This discriminator design enables synthesis of visually plausible, novel compositions of a scene, with varying content and layout, while preserving the context of the original sample. Compared to previous single image GANs, our model generates more diverse, higher quality images, while not being restricted to a single image setting. We further introduce a new challengin
    
[^201]: 基于猜想的数据模式发现

    Conjecturing-Based Discovery of Patterns in Data. (arXiv:2011.11576v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2011.11576](http://arxiv.org/abs/2011.11576)

    本研究提出了一种基于猜想的数据模式发现方法，在数值特征和分类特征之间建立了非线性和布尔关系，并应用于COVID-19患者级别数据，揭示了可能的风险因素。

    

    我们提出了一种猜想机器，它以非线性项的边界以及分类特征的布尔表达式的形式建议特征之间的关系。所提出的猜想框架可以从数据中恢复已知的非线性和布尔关系。在两种情况下，真实的基础关系被揭示出来。然后，我们将该方法与先前提出的符号回归框架进行比较，以确定在数据集中满足的方程恢复的能力。然后，该框架应用于COVID-19结果的患者级别数据，以提供可能与医学文献中确认的风险因素。

    We propose the use of a conjecturing machine that suggests feature relationships in the form of bounds involving nonlinear terms for numerical features and boolean expressions for categorical features. The proposed Conjecturing framework recovers known nonlinear and boolean relationships among features from data. In both settings, true underlying relationships are revealed. We then compare the method to a previously-proposed framework for symbolic regression on the ability to recover equations that are satisfied among features in a dataset. The framework is then applied to patient-level data regarding COVID-19 outcomes to suggest possible risk factors that are confirmed in the medical literature.
    
[^202]: 半监督学习：当未标记数据同样有用时的情况

    Semi-Supervised Learning: the Case When Unlabeled Data is Equally Useful. (arXiv:2005.11018v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2005.11018](http://arxiv.org/abs/2005.11018)

    本文研究了半监督学习中未标记数据与标记数据在学习速度方面的关系，并发现在特定条件下，未标记数据在学习速度上同样有用。

    

    半监督学习算法试图利用较便宜的未标记数据来提高学习性能。本文考虑了数据分布可以由连续参数来描述的统计模型。我们展示了在分布满足一定条件的情况下，未标记数据在学习速度方面与标记数据同样有用。具体而言，设$n，m$分别为标记和未标记数据的数量。研究表明，如果$m\sim n$，半监督学习的学习速度按$O(1/n)$缩放；如果$m\sim n^{1+\gamma}$，半监督学习的学习速度按$O(1/n^{1+\gamma})$缩放，其中$\gamma>0$，而监督学习的学习速度按$O(1/n)$缩放。

    Semi-supervised learning algorithms attempt to take advantage of relatively inexpensive unlabeled data to improve learning performance. In this work, we consider statistical models where the data distributions can be characterized by continuous parameters. We show that under certain conditions on the distribution, unlabeled data is equally useful as labeled date in terms of learning rate. Specifically, let $n, m$ be the number of labeled and unlabeled data, respectively. It is shown that the learning rate of semi-supervised learning scales as $O(1/n)$ if $m\sim n$, and scales as $O(1/n^{1+\gamma})$ if $m\sim n^{1+\gamma}$ for some $\gamma>0$, whereas the learning rate of supervised learning scales as $O(1/n)$.
    
[^203]: 一个用于非凸优化的次采样张量方法

    A Sub-sampled Tensor Method for Non-convex Optimization. (arXiv:1911.10367v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/1911.10367](http://arxiv.org/abs/1911.10367)

    该论文提出了一个用于非凸优化的次采样张量方法，通过使用新颖的张量集中不等式，该方法可以在匹配确定性方法的收敛速度下找到光滑且潜在非凸的目标函数的局部极小值。

    

    我们提出了一种随机优化方法，该方法使用四阶正则化模型来寻找光滑且潜在非凸的目标函数的局部极小值，而不是使用精确的导数。该算法使用次采样导数而不是精确数量。通过使用新颖的张量集中不等式，我们证明了该方法可以在至多 $\bigO\left(\max\left(\epsilon_1^{-4/3}, \epsilon_2^{-2}, \epsilon_3^{-4}\right)\right)$ 次迭代中找到一个 $(\epsilon_1,\epsilon_2,\epsilon_3)$ 三阶临界点，从而与确定性方法的收敛速度相匹配。

    We present a stochastic optimization method that uses a fourth-order regularized model to find local minima of smooth and potentially non-convex objective functions with a finite-sum structure. This algorithm uses sub-sampled derivatives instead of exact quantities. The proposed approach is shown to find an $(\epsilon_1,\epsilon_2,\epsilon_3)$-third-order critical point in at most $\bigO\left(\max\left(\epsilon_1^{-4/3}, \epsilon_2^{-2}, \epsilon_3^{-4}\right)\right)$ iterations, thereby matching the rate of deterministic approaches. In order to prove this result, we derive a novel tensor concentration inequality for sums of tensors of any order that makes explicit use of the finite-sum structure of the objective function.
    
[^204]: 预测脑肿瘤的深度学习方法

    Predictive modeling of brain tumor: A Deep learning approach. (arXiv:1911.02265v6 [cs.CV] UPDATED)

    [http://arxiv.org/abs/1911.02265](http://arxiv.org/abs/1911.02265)

    这篇论文介绍了一种基于卷积神经网络的深度学习方法，通过脑部MRI扫描将脑肿瘤进行分类，并比较了不同预训练模型的性能。实验结果表明，Resnet-50模型具有最高的准确率和最低的假阴性率。

    

    图像处理技术可以可视化人体不同的解剖结构。深度学习在脑部磁共振成像（MRI）扫描中检测癌组织生长方面取得了重要进展。本文提出了基于卷积神经网络（CNN）的迁移学习方法，使用三个预训练模型将脑MRI扫描分类为两类。将这些模型的性能进行了比较。实验结果显示，Resnet-50模型的准确率最高，假阴性率最低，分别为95％和零。其次是VGG-16模型和Inception-V3模型，准确率分别为90％和55％。

    Image processing concepts can visualize the different anatomy structure of the human body. Recent advancements in the field of deep learning have made it possible to detect the growth of cancerous tissue just by a patient's brain Magnetic Resonance Imaging (MRI) scans. These methods require very high accuracy and meager false negative rates to be of any practical use. This paper presents a Convolutional Neural Network (CNN) based transfer learning approach to classify the brain MRI scans into two classes using three pre-trained models. The performances of these models are compared with each other. Experimental results show that the Resnet-50 model achieves the highest accuracy and least false negative rates as 95% and zero respectively. It is followed by VGG-16 and Inception-V3 model with an accuracy of 90% and 55% respectively.
    
[^205]: MDP Playground: 一种用于强化学习的分析和调试测试平台

    MDP Playground: An Analysis and Debug Testbed for Reinforcement Learning. (arXiv:1909.07750v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1909.07750](http://arxiv.org/abs/1909.07750)

    MDP Playground是一个用于强化学习的测试平台，可以根据不同维度的难度控制方式，挑战代理在各种环境中的表现。它提供了参数化的玩具环境集合，并通过实验揭示了这些环境对代理的影响。

    

    我们提出了MDP Playground，一个用于强化学习代理的测试平台，可以根据难度的不同维度进行控制，以挑战代理并在玩具和复杂的强化学习环境中获得不同程度的难度。我们考虑并允许对各种维度进行控制，包括延迟奖励、序列长度、奖励密度、随机性、图像表示、无关特征、时间单位、动作范围等。我们通过在OpenAI Gym中变化这些维度来定义一个参数化的快速运行的玩具环境集合，并建议使用这些环境来更好地了解代理。然后，我们展示了如何使用MDP Playground设计实验，以深入了解玩具环境。我们还提供了可以将许多这些维度注入到任何Gym环境中的包装器。我们在Atari和Mujoco上使用这些包装器进行实验，以了解这些维度对比玩具环境更复杂的环境的影响。

    We present MDP Playground, a testbed for Reinforcement Learning (RL) agents with dimensions of hardness that can be controlled independently to challenge agents in different ways and obtain varying degrees of hardness in toy and complex RL environments. We consider and allow control over a wide variety of dimensions, including delayed rewards, sequence lengths, reward density, stochasticity, image representations, irrelevant features, time unit, action range and more. We define a parameterised collection of fast-to-run toy environments in OpenAI Gym by varying these dimensions and propose to use these to understand agents better. We then show how to design experiments using MDP Playground to gain insights on the toy environments. We also provide wrappers that can inject many of these dimensions into any Gym environment. We experiment with these wrappers on Atari and Mujoco to allow for understanding the effects of these dimensions on environments that are more complex than the toy envi
    
[^206]: 非线性状态空间模型的随机梯度MCMC方法

    Stochastic Gradient MCMC for Nonlinear State Space Models. (arXiv:1901.10568v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1901.10568](http://arxiv.org/abs/1901.10568)

    该论文提出了一种针对非线性状态空间模型的随机梯度MCMC方法，通过扩展已有方法，利用粒子缓冲随机梯度估计量解决了长时间序列下计算和粒子退化的问题。

    

    状态空间模型（SSM）通过潜在的随机过程提供了建模复杂时间序列的灵活框架。对于非线性、非高斯的SSM推断通常使用粒子方法，但这些方法在处理长时间序列时不具备良好的可扩展性。挑战有两方面：计算与时间线性扩展，粒子滤波器在长序列中还会出现逐渐退化的问题。已经开发了使用缓冲随机梯度估计量来应对时序依赖性的有限状态隐马尔可夫模型和线性SSM的随机梯度MCMC方法，我们将这些方法扩展到了非线性SSM，并提出了误差界限，考虑了缓冲误差和粒子误差，适用于在潜在过程中具有对数凹性的非线性SSM情况。我们使用随机梯度MCMC方法评估了我们提出的粒子缓冲随机梯度方法。

    State space models (SSMs) provide a flexible framework for modeling complex time series via a latent stochastic process. Inference for nonlinear, non-Gaussian SSMs is often tackled with particle methods that do not scale well to long time series. The challenge is two-fold: not only do computations scale linearly with time, as in the linear case, but particle filters additionally suffer from increasing particle degeneracy with longer series. Stochastic gradient MCMC methods have been developed to scale Bayesian inference for finite-state hidden Markov models and linear SSMs using buffered stochastic gradient estimates to account for temporal dependencies. We extend these stochastic gradient estimators to nonlinear SSMs using particle methods. We present error bounds that account for both buffering error and particle error in the case of nonlinear SSMs that are log-concave in the latent process. We evaluate our proposed particle buffered stochastic gradient using stochastic gradient MCMC
    

