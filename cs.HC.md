# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Parachute: Evaluating Interactive Human-LM Co-writing Systems.](http://arxiv.org/abs/2303.06333) | 本文提出了一个以人为中心的评估框架Parachute，用于交互式共同撰写系统的评估，该框架包含了分类的实用指标，可以用于评估和比较共同撰写系统。 |
| [^2] | [Virtual Mouse And Assistant: A Technological Revolution Of Artificial Intelligence.](http://arxiv.org/abs/2303.06309) | 本文介绍了虚拟助手的性能提升，虚拟助手是一种能够理解自然语言语音命令并能代表您执行任务的软件，可以完成几乎任何您自己可以完成的特定智能手机或PC活动，而且列表不断扩大。 |
| [^3] | [An Interactive UI to Support Sensemaking over Collections of Parallel Texts.](http://arxiv.org/abs/2303.06264) | AVTALER是一种交互式UI，结合了人类的独特技能和自动化的优势，支持用户对可比较的文本摘录进行意义建构和对比。 |
| [^4] | [Who's Thinking? A Push for Human-Centered Evaluation of LLMs using the XAI Playbook.](http://arxiv.org/abs/2303.06223) | 本文探讨了以人为中心的人工智能系统评估，将相对成熟的可解释AI领域与快速发展的大型语言模型研究进行了类比，认为人类的需求应该成为LLMs评估的核心。 |
| [^5] | [Explainable AI is Dead, Long Live Explainable AI! Hypothesis-driven decision support.](http://arxiv.org/abs/2302.12389) | 本文主张从当前的可解释性人工智能（XAI）模式转变，采用基于假设的决策支持系统，以更好地支持人类决策。 |
| [^6] | ["Correct answers" from the psychology of artificial intelligence.](http://arxiv.org/abs/2302.07267) | 本文使用OpenAI的GPT3.5模型重新复制了Many Labs 2复制项目中的14项研究，其中8项研究的结果被成功复制。然而，对于剩下的6项研究，GPT3.5以极其预定的方式回答了调查问题，导致无法分析这些研究。 |
| [^7] | [D-Shape: Demonstration-Shaped Reinforcement Learning via Goal Conditioning.](http://arxiv.org/abs/2210.14428) | D-Shape是一种新的结合IL和RL的方法，它使用奖励塑形和目标条件化RL的思想来解决次优演示与回报最大化目标之间的冲突，能够在稀疏奖励网格世界领域中提高样本效率并一致地收敛到最优策略。 |
| [^8] | [Developing a Trusted Human-AI Network for Humanitarian Benefit.](http://arxiv.org/abs/2112.11191) | 本文提出了一种可信的人工智能通信网络，将通信协议、区块链技术和信息融合与AI集成，以改善冲突通信，为人道主义利益提供可问责信息交换。 |
| [^9] | [A trained humanoid robot can perform human-like crossmodal social attention and conflict resolution.](http://arxiv.org/abs/2111.01906) | 本研究采用跨模态冲突解决的神经机器人范例，使机器人表现出类人的社交关注，为增强人机社交互动提供了新思路。 |

# 详细

[^1]: 降落伞：评估交互式人机共同撰写系统

    Parachute: Evaluating Interactive Human-LM Co-writing Systems. (arXiv:2303.06333v1 [cs.HC])

    [http://arxiv.org/abs/2303.06333](http://arxiv.org/abs/2303.06333)

    本文提出了一个以人为中心的评估框架Parachute，用于交互式共同撰写系统的评估，该框架包含了分类的实用指标，可以用于评估和比较共同撰写系统。

    This paper proposes a human-centered evaluation framework, Parachute, for interactive co-writing systems, which includes categorized practical metrics and can be used to evaluate and compare co-writing systems.

    语言模型的飞速发展引起了人们对于利用语言模型构建共同撰写系统的极大兴趣，其中人类和语言模型交互地为共同的写作成果做出贡献。然而，缺乏对于交互式环境下共同撰写系统的评估研究。我们提出了一个以人为中心的评估框架Parachute，用于交互式共同撰写系统的评估。Parachute展示了交互评估的综合视角，其中每个评估方面都包含了分类的实用指标。此外，我们提供了一个使用案例来演示如何使用Parachute评估和比较共同撰写系统。

    A surge of advances in language models (LMs) has led to significant interest in using LMs to build co-writing systems, in which humans and LMs interactively contribute to a shared writing artifact. However, there is a lack of studies assessing co-writing systems in interactive settings. We propose a human-centered evaluation framework, Parachute, for interactive co-writing systems. Parachute showcases an integrative view of interaction evaluation, where each evaluation aspect consists of categorized practical metrics. Furthermore, we present Parachute with a use case to demonstrate how to evaluate and compare co-writing systems using Parachute.
    
[^2]: 虚拟鼠标和助手：人工智能的技术革命

    Virtual Mouse And Assistant: A Technological Revolution Of Artificial Intelligence. (arXiv:2303.06309v1 [cs.HC])

    [http://arxiv.org/abs/2303.06309](http://arxiv.org/abs/2303.06309)

    本文介绍了虚拟助手的性能提升，虚拟助手是一种能够理解自然语言语音命令并能代表您执行任务的软件，可以完成几乎任何您自己可以完成的特定智能手机或PC活动，而且列表不断扩大。

    This paper introduces the performance improvement of virtual assistants, which are software that understands natural language voice commands and can perform tasks on your behalf. They can complete practically any specific smartphone or PC activity that you can complete on your own, and the list is continually expanding.

    本文旨在提高虚拟助手的性能。那么什么是虚拟助手？应用软件，通常称为虚拟助手，也称为AI助手或数字助手，是一种能够理解自然语言语音命令并能代表您执行任务的软件。虚拟助手可以完成几乎任何您自己可以完成的特定智能手机或PC活动，而且列表不断扩大。虚拟助手通常可以完成各种各样的任务，包括安排会议、发送消息和监控天气。以前的虚拟助手，如Google助手和Cortana，在某些方面有限制，因为它们只能执行搜索，而不是完全自动化。例如，这些引擎没有能力前进和倒带歌曲，以保持歌曲的控制功能；它们只能具有搜索歌曲的模块。

    The purpose of this paper is to enhance the performance of the virtual assistant. So, what exactly is a virtual assistant. Application software, often called virtual assistants, also known as AI assistants or digital assistants, is software that understands natural language voice commands and can perform tasks on your behalf. What does a virtual assistant do. Virtual assistants can complete practically any specific smartphone or PC activity that you can complete on your own, and the list is continually expanding. Virtual assistants typically do an impressive variety of tasks, including scheduling meetings, delivering messages, and monitoring the weather. Previous virtual assistants, like Google Assistant and Cortana, had limits in that they could only perform searches and were not entirely automated. For instance, these engines do not have the ability to forward and rewind the song in order to maintain the control function of the song; they can only have the module to search for songs 
    
[^3]: 一种支持对平行文本集合进行意义建构的交互式UI

    An Interactive UI to Support Sensemaking over Collections of Parallel Texts. (arXiv:2303.06264v1 [cs.HC])

    [http://arxiv.org/abs/2303.06264](http://arxiv.org/abs/2303.06264)

    AVTALER是一种交互式UI，结合了人类的独特技能和自动化的优势，支持用户对可比较的文本摘录进行意义建构和对比。

    AVTALER is an interactive UI that combines human skills and the advantages of automation to support users in sensemaking and contrasting comparable text excerpts.

    科学家和科学记者等人经常需要理解大量论文及其在范围、重点、发现或其他重要因素方面的比较。然而，对于大量的论文，逐一进行比较和对比是认知上具有挑战性的。完全自动化这个审查过程是不可行的，因为它通常需要领域特定的知识，以及理解审查的背景和动机。虽然有现有的工具来帮助组织和注释文献综述的论文，但它们仍然依赖于人们逐个阅读论文并手动理解相关信息。我们提出了AVTALER，它结合了人们独特的技能、上下文意识和知识，以及自动化的优势。给定一组可比较的文本摘录，它支持用户进行意义建构和对比。

    Scientists and science journalists, among others, often need to make sense of a large number of papers and how they compare with each other in scope, focus, findings, or any other important factors. However, with a large corpus of papers, it's cognitively demanding to pairwise compare and contrast them all with each other. Fully automating this review process would be infeasible, because it often requires domain-specific knowledge, as well as understanding what the context and motivations for the review are. While there are existing tools to help with the process of organizing and annotating papers for literature reviews, at the core they still rely on people to serially read through papers and manually make sense of relevant information.  We present AVTALER, which combines peoples' unique skills, contextual awareness, and knowledge, together with the strength of automation. Given a set of comparable text excerpts from a paper corpus, it supports users in sensemaking and contrasting pa
    
[^4]: 谁在思考？使用XAI Playbook推动以人为中心的LLMs评估

    Who's Thinking? A Push for Human-Centered Evaluation of LLMs using the XAI Playbook. (arXiv:2303.06223v1 [cs.HC])

    [http://arxiv.org/abs/2303.06223](http://arxiv.org/abs/2303.06223)

    本文探讨了以人为中心的人工智能系统评估，将相对成熟的可解释AI领域与快速发展的大型语言模型研究进行了类比，认为人类的需求应该成为LLMs评估的核心。

    This paper explores human-centered evaluation of AI-based systems, drawing parallels between the relatively mature field of explainable AI and the rapidly evolving research boom around large language models. The authors argue that humans' needs should be held front and center in evaluating LLMs.

    部署的人工智能（AI）经常影响人类，而评估这些工具没有一种适合所有情况的度量标准。以人为中心的AI系统评估结合了定量和定性分析以及人类输入。它已经在可解释的AI（XAI）和人机交互（HCI）社区中得到了深入探讨。仍然存在差距，但社区已经接受了人类与AI及其附带的解释进行交互，以及应该将人类的需求（包括他们的认知偏见和怪癖）放在首位的基本理解。在本文中，我们将相对成熟的XAI领域与快速发展的大型语言模型（LLMs）研究热潮之间进行了类比。接受的LLMs评估指标不是以人为中心的。我们认为，在讨论LLMs时，XAI社区在过去十年中走过的许多相同路径将被重新踏上。具体而言，我们认为人类的倾向 - 再次完全包括他们的认知偏见和怪癖 - 应该成为LLMs评估的核心。

    Deployed artificial intelligence (AI) often impacts humans, and there is no one-size-fits-all metric to evaluate these tools. Human-centered evaluation of AI-based systems combines quantitative and qualitative analysis and human input. It has been explored to some depth in the explainable AI (XAI) and human-computer interaction (HCI) communities. Gaps remain, but the basic understanding that humans interact with AI and accompanying explanations, and that humans' needs -- complete with their cognitive biases and quirks -- should be held front and center, is accepted by the community. In this paper, we draw parallels between the relatively mature field of XAI and the rapidly evolving research boom around large language models (LLMs). Accepted evaluative metrics for LLMs are not human-centered. We argue that many of the same paths tread by the XAI community over the past decade will be retread when discussing LLMs. Specifically, we argue that humans' tendencies -- again, complete with the
    
[^5]: 可解释性人工智能已死，可解释性人工智能万岁！基于假设的决策支持

    Explainable AI is Dead, Long Live Explainable AI! Hypothesis-driven decision support. (arXiv:2302.12389v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.12389](http://arxiv.org/abs/2302.12389)

    本文主张从当前的可解释性人工智能（XAI）模式转变，采用基于假设的决策支持系统，以更好地支持人类决策。

    This paper argues for a paradigm shift from the current model of explainable artificial intelligence (XAI) to hypothesis-driven decision support systems to better support human decision making.

    本文主张从当前的可解释性人工智能（XAI）模式转变，因为它可能会妨碍更好的人类决策。我们认为，人们不总是会接受和遵循建议，因为他们不信任它们，或者更糟糕的是，即使建议是错误的，人们也会盲目地遵循它们。可解释性人工智能通过帮助人们理解模型为什么会给出某些建议来缓解这种情况。然而，最近的研究表明，人们并不总是足够参与解释工具以帮助改善决策。我们认为，这是因为我们没有考虑到两件事情。首先，建议（及其解释）可能与人们的假设和信仰相冲突。其次，人们的决策往往是基于假设的，而不是基于事实的。因此，我们主张采用基于假设的决策支持系统，以更好地支持人类决策。

    In this paper, we argue for a paradigm shift from the current model of explainable artificial intelligence (XAI), which may be counter-productive to better human decision making. In early decision support systems, we assumed that we could give people recommendations and that they would consider them, and then follow them when required. However, research found that people often ignore recommendations because they do not trust them; or perhaps even worse, people follow them blindly, even when the recommendations are wrong. Explainable artificial intelligence mitigates this by helping people to understand how and why models give certain recommendations. However, recent research shows that people do not always engage with explainability tools enough to help improve decision making. The assumption that people will engage with recommendations and explanations has proven to be unfounded. We argue this is because we have failed to account for two things. First, recommendations (and their expla
    
[^6]: 人工智能心理学中的“正确答案”

    "Correct answers" from the psychology of artificial intelligence. (arXiv:2302.07267v3 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2302.07267](http://arxiv.org/abs/2302.07267)

    本文使用OpenAI的GPT3.5模型重新复制了Many Labs 2复制项目中的14项研究，其中8项研究的结果被成功复制。然而，对于剩下的6项研究，GPT3.5以极其预定的方式回答了调查问题，导致无法分析这些研究。

    This paper replicates 14 studies from the Many Labs 2 replication project with OpenAI's text-davinci-003 model, and successfully replicates the results of 8 studies. However, for the remaining 6 studies, GPT3.5 answered survey questions in an extremely predetermined way, making it impossible to analyze these studies.

    大型语言模型的能力已经大大增强。这种AI系统的一个提出的应用是支持社会和认知科学中的数据收集，目前完美的实验控制是不可行的，而大规模、代表性数据集的收集通常是昂贵的。在本文中，我们使用OpenAI的text-davinci-003模型（俗称GPT3.5）重新复制了Many Labs 2复制项目中的14项研究。我们通过将每项研究的调查作为文本输入，从GPT3.5的默认设置中收集了响应。在我们可以分析的八项研究中，我们的GPT样本复制了原始结果的37.5%以及Many Labs 2结果的37.5%。出乎意料的是，我们无法像预先注册的计划那样分析剩下的六项研究。这是因为对于这六项研究中的每一项，GPT3.5以极其预定的方式回答了调查问题（无论是因变量还是条件变量）：一个未知的

    Large Language Models have vastly grown in capabilities. One proposed application of such AI systems is to support data collection in the social and cognitive sciences, where perfect experimental control is currently unfeasible and the collection of large, representative datasets is generally expensive. In this paper, we re-replicate 14 studies from the Many Labs 2 replication project with OpenAI's text-davinci-003 model, colloquially known as GPT3.5. We collected responses from the default setting of GPT3.5 by inputting each study's survey as text. Among the eight studies we could analyse, our GPT sample replicated 37.5% of the original results as well as 37.5% of the Many Labs 2 results. Unexpectedly, we could not analyse the remaining six studies as we had planned in our pre-registration. This was because for each of these six studies, GPT3.5 answered at least one of the survey questions (either a dependent variable or a condition variable) in an extremely predetermined way: an unex
    
[^7]: D-Shape: 通过目标条件化实现演示形状的强化学习

    D-Shape: Demonstration-Shaped Reinforcement Learning via Goal Conditioning. (arXiv:2210.14428v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14428](http://arxiv.org/abs/2210.14428)

    D-Shape是一种新的结合IL和RL的方法，它使用奖励塑形和目标条件化RL的思想来解决次优演示与回报最大化目标之间的冲突，能够在稀疏奖励网格世界领域中提高样本效率并一致地收敛到最优策略。

    D-Shape is a new method that combines imitation learning (IL) and reinforcement learning (RL) using reward shaping and goal-conditioned RL to resolve the conflict between suboptimal demonstrations and return-maximization objective of RL. It improves sample efficiency and consistently converges to the optimal policy in sparse-reward gridworld domains.

    将模仿学习（IL）和强化学习（RL）相结合是解决自主行为获取中样本效率低下的一种有前途的方法，但这样做的方法通常假定所需的行为演示由专家提供，该专家相对于任务奖励表现最佳。然而，如果提供的演示是次优的，则面临一个基本挑战，即IL的演示匹配目标与RL的回报最大化目标冲突。本文介绍了D-Shape，一种新的结合IL和RL的方法，它使用奖励塑形和目标条件化RL的思想来解决上述冲突。D-Shape允许从次优演示中学习，同时保留了找到相对于任务奖励的最优策略的能力。我们在稀疏奖励网格世界领域实验验证了D-Shape，结果表明它在样本效率方面优于RL，并且能够一致地收敛到最优策略。

    While combining imitation learning (IL) and reinforcement learning (RL) is a promising way to address poor sample efficiency in autonomous behavior acquisition, methods that do so typically assume that the requisite behavior demonstrations are provided by an expert that behaves optimally with respect to a task reward. If, however, suboptimal demonstrations are provided, a fundamental challenge appears in that the demonstration-matching objective of IL conflicts with the return-maximization objective of RL. This paper introduces D-Shape, a new method for combining IL and RL that uses ideas from reward shaping and goal-conditioned RL to resolve the above conflict. D-Shape allows learning from suboptimal demonstrations while retaining the ability to find the optimal policy with respect to the task reward. We experimentally validate D-Shape in sparse-reward gridworld domains, showing that it both improves over RL in terms of sample efficiency and converges consistently to the optimal polic
    
[^8]: 为人道主义利益开发可信的人工智能网络

    Developing a Trusted Human-AI Network for Humanitarian Benefit. (arXiv:2112.11191v3 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2112.11191](http://arxiv.org/abs/2112.11191)

    本文提出了一种可信的人工智能通信网络，将通信协议、区块链技术和信息融合与AI集成，以改善冲突通信，为人道主义利益提供可问责信息交换。

    This paper proposes a trusted human-AI communication network that integrates communication protocols, blockchain technology, and information fusion with AI to improve conflict communications for accountable information exchange regarding protected entities, critical infrastructure, and humanitarian signals and status updates for humans and machines in conflicts.

    人工智能（AI）将越来越多地在冲突中以数字和物理方式参与，但缺乏与人类进行人道主义目的的可信通信。本文考虑将通信协议（“白旗协议”）、分布式账本“区块链”技术和信息融合与AI集成，以改善冲突通信，称为“受保护的保证理解情况和实体”PAUSE。这样一个可信的人工智能通信网络可以提供关于受保护实体、关键基础设施、人道主义信号和人类和机器在冲突中的状态更新的可问责信息交换。我们研究了几个现实的潜在案例研究，将这些技术集成到一个可信的人工智能网络中，以实现人道主义利益，包括实时映射冲突区域的平民和战斗人员，为避免事故做准备，并使用网络管理错误信息。

    Artificial intelligences (AI) will increasingly participate digitally and physically in conflicts, yet there is a lack of trused communications with humans for humanitarian purposes. In this paper we consider the integration of a communications protocol (the 'whiteflag protocol'), distributed ledger 'blockchain' technology, and information fusion with AI, to improve conflict communications called 'protected assurance understanding situation and entitities' PAUSE. Such a trusted human-AI communication network could provide accountable information exchange regarding protected entities, critical infrastructure, humanitiarian signals and status updates for humans and machines in conflicts. We examine several realistic potential case studies for the integration of these technologies into a trusted human-AI network for humanitarian benefit including mapping a conflict zone with civilians and combatants in real time, preparation to avoid incidents and using the network to manage misinformatio
    
[^9]: 训练过的人形机器人可以执行类人的跨模态社交关注和冲突解决

    A trained humanoid robot can perform human-like crossmodal social attention and conflict resolution. (arXiv:2111.01906v5 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2111.01906](http://arxiv.org/abs/2111.01906)

    本研究采用跨模态冲突解决的神经机器人范例，使机器人表现出类人的社交关注，为增强人机社交互动提供了新思路。

    This study adopts the neurorobotic paradigm of crossmodal conflict resolution to make a robot express human-like social attention, providing a new approach to enhance human-robot social interaction.

    为了增强人机社交互动，机器人在复杂的现实环境中处理多个社交线索至关重要。然而，跨模态输入信息的不一致性是不可避免的，这可能对机器人的处理造成挑战。为了解决这个问题，我们的研究采用了跨模态冲突解决的神经机器人范例，使机器人表现出类人的社交关注。我们对37名参与者进行了一项行为实验。我们设计了一个圆桌会议场景，有三个动画化的头像，以提高生态效度。每个头像都戴着医用口罩，遮盖了鼻子、嘴巴和下巴的面部线索。中央头像移动其眼睛注视，而外围头像则发出声音。凝视方向和声音位置要么是空间上一致的，要么是不一致的。我们观察到，中央头像的动态凝视可以触发跨模态社交关注反应。特别是，人类表现

    To enhance human-robot social interaction, it is essential for robots to process multiple social cues in a complex real-world environment. However, incongruency of input information across modalities is inevitable and could be challenging for robots to process. To tackle this challenge, our study adopted the neurorobotic paradigm of crossmodal conflict resolution to make a robot express human-like social attention. A behavioural experiment was conducted on 37 participants for the human study. We designed a round-table meeting scenario with three animated avatars to improve ecological validity. Each avatar wore a medical mask to obscure the facial cues of the nose, mouth, and jaw. The central avatar shifted its eye gaze while the peripheral avatars generated sound. Gaze direction and sound locations were either spatially congruent or incongruent. We observed that the central avatar's dynamic gaze could trigger crossmodal social attention responses. In particular, human performances are 
    

