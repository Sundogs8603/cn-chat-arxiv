<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;SR$_{\text{LLM}}$&#65292;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#20840;&#38754;&#30340;&#23433;&#20840;&#39118;&#38505;&#20998;&#31867;&#27861;&#21644;&#19987;&#23478;&#26631;&#27880;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#36890;&#36807;&#25351;&#20196;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#26377;&#25928;&#20943;&#23569;&#20102;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2404.01399</link><description>&lt;p&gt;
&#24320;&#21457;&#23433;&#20840;&#21644;&#36127;&#36131;&#20219;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; - &#19968;&#20010;&#20840;&#38754;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Developing Safe and Responsible Large Language Models -- A Comprehensive Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01399
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;SR$_{\text{LLM}}$&#65292;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#20840;&#38754;&#30340;&#23433;&#20840;&#39118;&#38505;&#20998;&#31867;&#27861;&#21644;&#19987;&#23478;&#26631;&#27880;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#36890;&#36807;&#25351;&#20196;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#26377;&#25928;&#20943;&#23569;&#20102;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#20154;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23433;&#20840;&#24615;&#21644;&#39118;&#38505;&#26085;&#30410;&#20851;&#27880;&#65292;&#21457;&#23637;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23433;&#20840;&#21644;&#36127;&#36131;&#20219;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SR$_{\text{LLM}}$&#65289;&#65292;&#36825;&#20010;&#27169;&#22411;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;LLM&#26469;&#22686;&#24378;&#35821;&#35328;&#29983;&#25104;&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;LLM&#23433;&#20840;&#39118;&#38505;&#20998;&#31867;&#27861;&#65292;&#24182;&#21033;&#29992;&#19987;&#23478;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#19982;&#36825;&#31181;&#20998;&#31867;&#27861;&#30456;&#19968;&#33268;&#12290;SR$_{\text{LLM}}$&#26088;&#22312;&#35782;&#21035;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#20869;&#23481;&#24182;&#20135;&#29983;&#33391;&#24615;&#21464;&#21270;&#12290;&#23427;&#37319;&#29992;&#22522;&#20110;&#25351;&#20196;&#30340;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#20351;&#24471;&#35813;&#27169;&#22411;&#19981;&#20165;&#26377;&#25928;&#22320;&#22686;&#24378;&#23433;&#20840;&#24615;&#65292;&#32780;&#19988;&#36164;&#28304;&#39640;&#25928;&#19988;&#26131;&#20110;&#35843;&#25972;&#12290;&#22312;&#25105;&#20204;&#23545;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#19987;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19981;&#23433;&#20840;&#20869;&#23481;&#29983;&#25104;&#30340;&#26174;&#33879;&#20943;&#23569;&#12290;&#27492;&#22806;&#65292;&#22312;&#23454;&#26045;&#23433;&#20840;&#25514;&#26045;&#21518;&#65292;&#20986;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01399v1 Announce Type: new  Abstract: Given the growing concerns around the safety and risks of Large Language Models (LLMs), it is essential to develop methods for mitigating these issues. We introduce Safe and Responsible Large Language Model (SR$_{\text{LLM}}$) , a model designed to enhance the safety of language generation using LLMs. Our approach incorporates a comprehensive LLM safety risk taxonomy and utilizes a dataset annotated by experts that align with this taxonomy. SR$_{\text{LLM}}$ is designed to identify potentially unsafe content and produce benign variations. It employs instruction-based and parameter-efficient fine-tuning methods, making the model not only effective in enhancing safety but also resource-efficient and straightforward to adjust. Through our testing on five benchmark datasets and two proprietary datasets, we observed notable reductions in the generation of unsafe content. Moreover, following the implementation of safety measures, there was a s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22235;&#31181;&#20248;&#21270;&#25216;&#26415;&#26469;&#22312;&#25163;&#26426;GPU&#19978;&#39640;&#25928;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23454;&#29616;&#22312;&#31227;&#21160;&#25512;&#26029;&#24341;&#25806;Transformer-Lite&#20013;&#65292;&#25552;&#39640;&#20102;&#25512;&#26029;&#36895;&#24230;&#21644;&#38477;&#20302;&#20102;&#25163;&#26426;&#28382;&#21518;&#65292;&#20174;&#32780;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2403.20041</link><description>&lt;p&gt;
Transformer-Lite: &#22312;&#25163;&#26426;GPU&#19978;&#39640;&#25928;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20041
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22235;&#31181;&#20248;&#21270;&#25216;&#26415;&#26469;&#22312;&#25163;&#26426;GPU&#19978;&#39640;&#25928;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23454;&#29616;&#22312;&#31227;&#21160;&#25512;&#26029;&#24341;&#25806;Transformer-Lite&#20013;&#65292;&#25552;&#39640;&#20102;&#25512;&#26029;&#36895;&#24230;&#21644;&#38477;&#20302;&#20102;&#25163;&#26426;&#28382;&#21518;&#65292;&#20174;&#32780;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26234;&#33021;&#21161;&#25163;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#32763;&#35793;&#21644;&#25163;&#26426;&#19978;&#30340;&#22810;&#27169;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35774;&#22791;&#19978;LLM&#37096;&#32626;&#26041;&#27861;&#36895;&#24230;&#36739;&#24930;&#65292;&#23548;&#33268;&#29992;&#25143;&#20307;&#39564;&#19981;&#20339;&#12290;&#20026;&#20102;&#23454;&#29616;&#22312;&#35774;&#22791;GPU&#19978;&#39640;&#25928;&#37096;&#32626;LLM&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#31181;&#20248;&#21270;&#25216;&#26415;&#65306;&#65288;a&#65289;&#22522;&#20110;&#31526;&#21495;&#34920;&#36798;&#30340;&#26041;&#27861;&#25903;&#25345;&#21160;&#24577;&#24418;&#29366;&#27169;&#22411;&#25512;&#26029;&#65307;&#65288;b&#65289;&#25805;&#20316;&#20248;&#21270;&#21644;&#25191;&#34892;&#20248;&#20808;&#32423;&#35774;&#32622;&#20197;&#25552;&#39640;&#25512;&#26029;&#36895;&#24230;&#24182;&#20943;&#23569;&#25163;&#26426;&#28382;&#21518;&#65307;&#65288;c&#65289;&#19968;&#31181;&#21517;&#20026;M0E4&#30340;FP4&#37327;&#21270;&#26041;&#27861;&#20197;&#20943;&#23569;&#21435;&#37327;&#21270;&#24320;&#38144;&#65307;&#65288;d&#65289;&#19968;&#31181;&#22522;&#20110;&#23376;&#24352;&#37327;&#30340;&#25216;&#26415;&#26469;&#22312;LLM&#25512;&#26029;&#21518;&#28040;&#38500;&#22797;&#21046;KV&#32531;&#23384;&#30340;&#38656;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#31227;&#21160;&#25512;&#26029;&#24341;&#25806;Transformer-Lite&#20013;&#23454;&#29616;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#35813;&#24341;&#25806;&#19982;&#39640;&#36890;&#21644;MTK&#22788;&#29702;&#22120;&#20860;&#23481;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;Transformer-Lite&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20041v1 Announce Type: new  Abstract: The Large Language Model (LLM) is widely employed for tasks such as intelligent assistants, text summarization, translation, and multi-modality on mobile phones. However, the current methods for on-device LLM deployment maintain slow inference speed, which causes poor user experience. To facilitate high-efficiency LLM deployment on device GPUs, we propose four optimization techniques: (a) a symbolic expression-based approach to support dynamic shape model inference; (b) operator optimizations and execution priority setting to enhance inference speed and reduce phone lagging; (c) an FP4 quantization method termed M0E4 to reduce dequantization overhead; (d) a sub-tensor-based technique to eliminate the need for copying KV cache after LLM inference. Furthermore, we implement these methods in our mobile inference engine, Transformer-Lite, which is compatible with both Qualcomm and MTK processors. We evaluated Transformer-Lite's performance u
&lt;/p&gt;</description></item><item><title>Skipformer&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#36339;&#36807;&#21644;&#24674;&#22797;&#8221;&#30340;Conformer&#26550;&#26500;&#65292;&#21487;&#20197;&#21160;&#24577;&#12289;&#19981;&#22343;&#21248;&#22320;&#21387;&#32553;&#24207;&#21015;&#36755;&#20837;&#38271;&#24230;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#39044;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#65292;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08258</link><description>&lt;p&gt;
Skipformer&#65306;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#35821;&#38899;&#35782;&#21035;&#30340;&#36339;&#36807;&#21644;&#24674;&#22797;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Skipformer: A Skip-and-Recover Strategy for Efficient Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08258
&lt;/p&gt;
&lt;p&gt;
Skipformer&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#36339;&#36807;&#21644;&#24674;&#22797;&#8221;&#30340;Conformer&#26550;&#26500;&#65292;&#21487;&#20197;&#21160;&#24577;&#12289;&#19981;&#22343;&#21248;&#22320;&#21387;&#32553;&#24207;&#21015;&#36755;&#20837;&#38271;&#24230;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#39044;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#65292;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Conformer&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#24050;&#25104;&#20026;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#30340;&#20107;&#23454;&#26631;&#26438;&#27169;&#22411;&#12290;&#36890;&#24120;&#24341;&#20837;&#19968;&#20010;&#31354;&#30333;&#31526;&#21495;&#26469;&#23545;&#40784;CTC&#25110;RNN-T&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#24207;&#21015;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#38271;&#36755;&#20837;&#38271;&#24230;&#20250;&#23545;&#35745;&#31639;&#39044;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#36896;&#25104;&#20108;&#27425;&#36127;&#33655;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Skipformer&#30340;&#8220;&#36339;&#36807;&#21644;&#24674;&#22797;&#8221;Conformer&#26550;&#26500;&#65292;&#20197;&#21160;&#24577;&#21644;&#19981;&#22343;&#21248;&#22320;&#21387;&#32553;&#24207;&#21015;&#36755;&#20837;&#38271;&#24230;&#12290;Skipformer&#20351;&#29992;&#20013;&#38388;CTC&#36755;&#20986;&#20316;&#20026;&#26631;&#20934;&#23558;&#24103;&#20998;&#20026;&#19977;&#32452;&#65306;&#20851;&#38190;&#12289;&#36339;&#36807;&#21644;&#24573;&#30053;&#12290;&#20851;&#38190;&#32452;&#39304;&#36865;&#21040;&#19979;&#19968;&#20010;Conformer&#22359;&#65292;&#20854;&#36755;&#20986;&#19982;&#36339;&#36807;&#32452;&#36890;&#36807;&#21407;&#22987;&#26102;&#38388;&#39034;&#24207;&#32852;&#25509;&#20316;&#20026;&#26368;&#32456;&#32534;&#30721;&#22120;&#36755;&#20986;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;Aishell-1&#19978;&#23558;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#20943;&#23569;&#20102;31&#20493;&#65292;&#22312;Librispeech&#35821;&#26009;&#24211;&#19978;&#20943;&#23569;&#20102;22&#20493;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22411;&#21487;&#23454;&#29616;&#26356;&#22909;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08258v1 Announce Type: new  Abstract: Conformer-based attention models have become the de facto backbone model for Automatic Speech Recognition tasks. A blank symbol is usually introduced to align the input and output sequences for CTC or RNN-T models. Unfortunately, the long input length overloads computational budget and memory consumption quadratically by attention mechanism. In this work, we propose a "Skip-and-Recover" Conformer architecture, named Skipformer, to squeeze sequence input length dynamically and inhomogeneously. Skipformer uses an intermediate CTC output as criteria to split frames into three groups: crucial, skipping and ignoring. The crucial group feeds into next conformer blocks and its output joint with skipping group by original temporal order as the final encoder output. Experiments show that our model reduces the input sequence length by 31 times on Aishell-1 and 22 times on Librispeech corpus. Meanwhile, the model can achieve better recognition accu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26368;&#22823;&#21270;&#20004;&#20010;&#20219;&#21153;&#30340;&#34920;&#31034;&#29305;&#24449;&#30340;&#20114;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24605;&#32500;&#38142;&#33976;&#39311;&#20013;&#26631;&#31614;&#39044;&#27979;&#20219;&#21153;&#19982;&#30693;&#35782;&#38598;&#25104;&#19981;&#36275;&#38382;&#39064;&#30340;&#21464;&#20998;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.03348</link><description>&lt;p&gt;
&#23398;&#20064;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#36827;&#34892;&#24605;&#32500;&#38142;&#25552;&#28860;
&lt;/p&gt;
&lt;p&gt;
Learning to Maximize Mutual Information for Chain-of-Thought Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03348
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26368;&#22823;&#21270;&#20004;&#20010;&#20219;&#21153;&#30340;&#34920;&#31034;&#29305;&#24449;&#30340;&#20114;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24605;&#32500;&#38142;&#33976;&#39311;&#20013;&#26631;&#31614;&#39044;&#27979;&#20219;&#21153;&#19982;&#30693;&#35782;&#38598;&#25104;&#19981;&#36275;&#38382;&#39064;&#30340;&#21464;&#20998;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#23558;&#22823;&#22411;&#22797;&#26434;&#27169;&#22411;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#36739;&#23567;&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#26159;&#23454;&#29616;&#39640;&#25928;&#20154;&#24037;&#26234;&#33021;&#37096;&#32626;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#36890;&#36807;&#21033;&#29992;&#24605;&#32500;&#38142; (CoT) &#33976;&#39311;&#30340;&#26032;&#26041;&#27861;&#8212;&#8212;&#36880;&#27493;&#33976;&#39311; (DSS)&#65292;&#24050;&#32463;&#23637;&#31034;&#20986;&#20026;&#36739;&#23567;&#27169;&#22411;&#36171;&#20104;&#20854;&#36739;&#22823;&#21516;&#34892;&#30340;&#20248;&#36234;&#25512;&#29702;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;&#22312;DSS&#20013;&#65292;&#33976;&#39311;&#27169;&#22411;&#36890;&#36807;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#21516;&#26102;&#33719;&#24471;&#29983;&#25104;&#29702;&#30001;&#21644;&#39044;&#27979;&#26631;&#31614;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;DSS&#24573;&#30053;&#20102;&#36825;&#20004;&#20010;&#35757;&#32451;&#20219;&#21153;&#20043;&#38388;&#30340;&#20869;&#22312;&#20851;&#31995;&#65292;&#23548;&#33268;CoT&#30693;&#35782;&#19982;&#26631;&#31614;&#39044;&#27979;&#20219;&#21153;&#30340;&#26377;&#25928;&#25972;&#21512;&#19981;&#36275;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;&#20449;&#24687;&#29942;&#39048;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20004;&#20010;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#24182;&#23558;&#20854;&#34920;&#36848;&#20026;&#26368;&#22823;&#21270;&#20004;&#20010;&#20219;&#21153;&#30340;&#34920;&#31034;&#29305;&#24449;&#30340;&#20114;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03348v1 Announce Type: cross  Abstract: Knowledge distillation, the technique of transferring knowledge from large, complex models to smaller ones, marks a pivotal step towards efficient AI deployment. Distilling Step-by-Step (DSS), a novel method utilizing chain-of-thought (CoT) distillation, has demonstrated promise by imbuing smaller models with the superior reasoning capabilities of their larger counterparts. In DSS, the distilled model acquires the ability to generate rationales and predict labels concurrently through a multi-task learning framework. However, DSS overlooks the intrinsic relationship between the two training tasks, leading to ineffective integration of CoT knowledge with the task of label prediction. To this end, we investigate the mutual relationship of the two tasks from Information Bottleneck perspective and formulate it as maximizing the mutual information of the representation features of the two tasks. We propose a variational approach to solve thi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#33719;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#20013;&#20196;&#20154;&#22256;&#25200;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#32763;&#35793;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.19267</link><description>&lt;p&gt;
&#24378;&#22823;&#30340;&#26080;&#30417;&#30563;&#25968;&#25454;&#36873;&#25321;&#25351;&#23548;&#65306;&#25429;&#33719;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#20013;&#20196;&#20154;&#22256;&#25200;&#30340;&#21629;&#21517;&#23454;&#20307;
&lt;/p&gt;
&lt;p&gt;
Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19267
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#33719;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#20013;&#20196;&#20154;&#22256;&#25200;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#32763;&#35793;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#38598;&#21487;&#20197;&#35757;&#32451;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65307;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#26080;&#27861;&#20934;&#30830;&#32763;&#35793;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#21477;&#23376;&#12290;&#33719;&#24471;&#21644;&#32763;&#35793;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#34429;&#28982;&#25104;&#26412;&#39640;&#26114;&#65292;&#20294;&#23545;&#20110;&#39640;&#36136;&#37327;&#32763;&#35793;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#20013;&#25214;&#21040;&#26368;&#8220;&#26377;&#25928;&#8221;&#30340;&#25968;&#25454;&#25104;&#20026;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#30340;&#23454;&#29992;&#31574;&#30053;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#36890;&#36807;&#36873;&#25321;&#8220;&#36866;&#24403;&#22256;&#38590;&#30340;&#25968;&#25454;&#8221;&#26469;&#25214;&#21040;&#36825;&#20123;&#26377;&#25928;&#25968;&#25454;&#65292;&#36825;&#24847;&#21619;&#30528;&#25968;&#25454;&#19981;&#24212;&#36807;&#20110;&#22256;&#38590;&#25110;&#36807;&#20110;&#31616;&#21333;&#65292;&#23588;&#20854;&#26159;&#22312;&#25968;&#25454;&#37327;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#24314;&#31435;&#26080;&#30417;&#30563;&#25968;&#25454;&#36873;&#25321;&#26631;&#20934;&#20173;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#8220;&#36866;&#24403;&#22256;&#38590;&#24230;&#8221;&#21487;&#33021;&#22240;&#25152;&#35757;&#32451;&#30340;&#25968;&#25454;&#39046;&#22495;&#32780;&#24322;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#65292;&#8216;Capturing Perplexing Named Entities&#8217;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19267v1 Announce Type: cross  Abstract: Employing extensive datasets enables the training of multilingual machine translation models; however, these models often fail to accurately translate sentences within specialized domains. Although obtaining and translating domain-specific data incurs high costs, it is inevitable for high-quality translations. Hence, finding the most 'effective' data with an unsupervised setting becomes a practical strategy for reducing labeling costs. Recent research indicates that this effective data could be found by selecting 'properly difficult data' based on its volume. This means the data should not be excessively challenging or overly simplistic, especially if the amount of data is limited. However, we found that establishing a criterion for unsupervised data selection remains challenging, as the 'proper difficulty' might vary based on the data domain being trained on. We introduce a novel unsupervised data selection method, 'Capturing Perplexi
&lt;/p&gt;</description></item><item><title>Matryoshka&#34920;&#31034;&#23398;&#20064;(MRL)&#20197;&#26356;&#32454;&#31890;&#24230;&#22320;&#32534;&#30721;&#20449;&#24687;&#65292;&#20197;&#36866;&#24212;&#20020;&#26102;&#20219;&#21153;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#23567;&#30340;&#23884;&#20837;&#22823;&#23567;&#65292;&#20174;&#32780;&#21152;&#24555;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.14776</link><description>&lt;p&gt;
2D Matryoshka&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
2D Matryoshka Sentence Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14776
&lt;/p&gt;
&lt;p&gt;
Matryoshka&#34920;&#31034;&#23398;&#20064;(MRL)&#20197;&#26356;&#32454;&#31890;&#24230;&#22320;&#32534;&#30721;&#20449;&#24687;&#65292;&#20197;&#36866;&#24212;&#20020;&#26102;&#20219;&#21153;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#23567;&#30340;&#23884;&#20837;&#22823;&#23567;&#65292;&#20174;&#32780;&#21152;&#24555;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14776v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#24120;&#35265;&#26041;&#27861;&#20381;&#36182;&#20110;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#22266;&#23450;&#38271;&#24230;&#30340;&#23884;&#20837;&#21521;&#37327;&#20316;&#20026;&#21477;&#23376;&#23884;&#20837;&#65292;&#29992;&#20110;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;&#30001;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#23384;&#22312;&#26410;&#30693;&#30340;&#35745;&#31639;&#32422;&#26463;&#21644;&#39044;&#31639;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#28789;&#27963;&#24615;&#19978;&#21463;&#21040;&#38480;&#21046;&#12290;Matryoshka&#34920;&#31034;&#23398;&#20064;(MRL)(Kusupati&#31561;&#20154;&#65292;2022)&#20197;&#26356;&#32454;&#31890;&#24230;&#22320;&#32534;&#30721;&#20449;&#24687;&#65292;&#21363;&#20351;&#29992;&#36739;&#20302;&#30340;&#23884;&#20837;&#32500;&#24230;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#36866;&#24212;&#20020;&#26102;&#20219;&#21153;&#12290;&#21487;&#20197;&#36890;&#36807;&#36739;&#23567;&#30340;&#23884;&#20837;&#22823;&#23567;&#36798;&#21040;&#31867;&#20284;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#21152;&#24555;&#19979;&#28216;&#20219;&#21153;&#12290;&#23613;&#31649;&#20854;&#25913;&#36827;&#20102;&#25928;&#29575;&#65292;MRL&#20173;&#35201;&#22312;&#33719;&#24471;&#23884;&#20837;&#20043;&#21069;&#36941;&#21382;&#25152;&#26377;Transformer&#23618;&#65292;&#36825;&#20173;&#28982;&#26159;&#26102;&#38388;&#21644;&#20869;&#23384;&#28040;&#32791;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#36825;&#24341;&#21457;&#20102;&#26159;&#21542;&#22266;&#23450;&#25968;&#37327;&#30340;Transformer&#23618;&#20250;&#24433;&#21709;&#34920;&#31034;&#36136;&#37327;&#20197;&#21450;&#20351;&#29992;&#20013;&#38388;&#23618;&#36827;&#34892;&#21477;&#23376;&#34920;&#31034;&#26159;&#21542;&#21487;&#34892;&#30340;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14776v1 Announce Type: new  Abstract: Common approaches rely on fixed-length embedding vectors from language models as sentence embeddings for downstream tasks such as semantic textual similarity (STS). Such methods are limited in their flexibility due to unknown computational constraints and budgets across various applications. Matryoshka Representation Learning (MRL) (Kusupati et al., 2022) encodes information at finer granularities, i.e., with lower embedding dimensions, to adaptively accommodate ad hoc tasks. Similar accuracy can be achieved with a smaller embedding size, leading to speedups in downstream tasks. Despite its improved efficiency, MRL still requires traversing all Transformer layers before obtaining the embedding, which remains the dominant factor in time and memory consumption. This prompts consideration of whether the fixed number of Transformer layers affects representation quality and whether using intermediate layers for sentence representation is feas
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Concept-1K&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23454;&#20363;&#22686;&#37327;&#23398;&#20064;&#65288;IIL&#65289;&#22330;&#26223;&#21644;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#21313;&#20159;&#21442;&#25968;&#30340;PLM&#20173;&#28982;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24433;&#21709;&#22240;&#32032;&#21253;&#25324;&#27169;&#22411;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#21644;&#32531;&#20914;&#21306;&#22823;&#23567;&#12290;&#29616;&#26377;&#30340;IL&#26041;&#27861;&#21644;LoRA&#25216;&#26415;&#26080;&#27861;&#28385;&#36275;&#24615;&#33021;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#25506;&#32034;&#21644;&#32531;&#35299;PLM&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2402.08526</link><description>&lt;p&gt;
Concept-1K&#65306;&#19968;&#31181;&#29992;&#20110;&#23454;&#20363;&#22686;&#37327;&#23398;&#20064;&#30340;&#26032;&#22411;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Concept-1K: A Novel Benchmark for Instance Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08526
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Concept-1K&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23454;&#20363;&#22686;&#37327;&#23398;&#20064;&#65288;IIL&#65289;&#22330;&#26223;&#21644;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#21313;&#20159;&#21442;&#25968;&#30340;PLM&#20173;&#28982;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24433;&#21709;&#22240;&#32032;&#21253;&#25324;&#27169;&#22411;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#21644;&#32531;&#20914;&#21306;&#22823;&#23567;&#12290;&#29616;&#26377;&#30340;IL&#26041;&#27861;&#21644;LoRA&#25216;&#26415;&#26080;&#27861;&#28385;&#36275;&#24615;&#33021;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#25506;&#32034;&#21644;&#32531;&#35299;PLM&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#23398;&#20064;&#65288;IL&#65289;&#23545;&#20110;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20154;&#31867;&#32423;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;IL&#22330;&#26223;&#21644;&#25968;&#25454;&#38598;&#26080;&#27861;&#35780;&#20272;PLM&#20013;&#30340;&#36951;&#24536;&#65292;&#20351;&#20154;&#35823;&#20197;&#20026;PLM&#19981;&#20250;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;IL&#22330;&#26223;&#65292;&#31216;&#20026;&#23454;&#20363;&#22686;&#37327;&#23398;&#20064;&#65288;IIL&#65289;&#65292;&#20197;&#21450;&#19968;&#20010;&#25903;&#25345;&#25968;&#37327;&#32423;&#26356;&#22823;&#30340;IL&#27493;&#39588;&#30340;&#26032;&#25968;&#25454;&#38598;Concept-1K&#12290;&#22522;&#20110;&#23545;Concept-1K&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21313;&#20159;&#21442;&#25968;&#30340;PLM&#20173;&#28982;&#36973;&#21463;&#30528;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#19988;&#36951;&#24536;&#21463;&#27169;&#22411;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#21644;&#32531;&#20914;&#21306;&#22823;&#23567;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;IL&#26041;&#27861;&#21644;&#19968;&#31181;&#27969;&#34892;&#30340;&#24494;&#35843;&#25216;&#26415;LoRA&#37117;&#26410;&#33021;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22330;&#26223;&#65292;&#25506;&#32034;PLM&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#40723;&#21169;&#35774;&#35745;&#26356;&#24378;&#22823;&#30340;&#25216;&#26415;&#20197;&#20943;&#36731;PLM&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incremental learning (IL) is essential to realize the human-level intelligence in the neural network. However, existing IL scenarios and datasets are unqualified for assessing forgetting in PLMs, giving an illusion that PLMs do not suffer from catastrophic forgetting. To this end, we propose a challenging IL scenario called instance-incremental learning (IIL) and a novel dataset called Concept-1K, which supports an order of magnitude larger IL steps. Based on the experiments on Concept-1K, we reveal that billion-parameter PLMs still suffer from catastrophic forgetting, and the forgetting is affected by both model scale, pretraining, and buffer size. Furthermore, existing IL methods and a popular finetuning technique, LoRA, fail to achieve satisfactory performance. Our study provides a novel scenario for future studies to explore the catastrophic forgetting of PLMs and encourage more powerful techniques to be designed for alleviating the forgetting in PLMs. The data, code and scripts ar
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#22914;&#20309;&#40065;&#26834;&#22320;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#39640;&#31572;&#26696;&#30340;&#26469;&#28304;&#36136;&#37327;&#21644;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#21644;&#22235;&#20010;&#27979;&#35797;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24494;&#35843;&#21487;&#20197;&#25913;&#21892;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08277</link><description>&lt;p&gt;
&#26397;&#30528;&#24544;&#23454;&#21644;&#24378;&#22823;&#30340;&#22522;&#20110;&#35777;&#25454;&#30340;&#38382;&#31572;&#19987;&#23478;&#30340;&#26041;&#21521;&#21069;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08277
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#22914;&#20309;&#40065;&#26834;&#22320;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#39640;&#31572;&#26696;&#30340;&#26469;&#28304;&#36136;&#37327;&#21644;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#21644;&#22235;&#20010;&#27979;&#35797;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24494;&#35843;&#21487;&#20197;&#25913;&#21892;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26356;&#24544;&#23454;&#21644;&#21487;&#36861;&#36394;&#30340;&#31572;&#26696;&#30340;&#36827;&#27493;&#23545;&#20110;&#21508;&#31181;&#30740;&#31350;&#21644;&#23454;&#36341;&#27963;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#19968;&#31181;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#21487;&#38752;&#30340;&#26469;&#28304;&#25552;&#20379;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;&#35777;&#25454;&#30340;&#38382;&#31572;&#22312;&#20351;&#29992;LLM&#26102;&#24050;&#32463;&#35777;&#26126;&#22312;&#24341;&#29992;&#27491;&#30830;&#30340;&#26469;&#28304;&#65288;&#26469;&#28304;&#36136;&#37327;&#65289;&#21644;&#20934;&#30830;&#22320;&#34920;&#31034;&#26469;&#28304;&#20013;&#30340;&#20449;&#24687;&#65288;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#65289;&#26041;&#38754;&#24037;&#20316;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22914;&#20309;&#40065;&#26834;&#22320;&#24494;&#35843;LLM&#65292;&#20197;&#25552;&#39640;&#26469;&#28304;&#36136;&#37327;&#21644;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#21160;&#25968;&#25454;&#36136;&#37327;&#36807;&#28388;&#22120;&#65292;&#21487;&#20197;&#22823;&#35268;&#27169;&#21512;&#25104;&#22810;&#26679;&#21270;&#30340;&#39640;&#36136;&#37327;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22235;&#20010;&#27979;&#35797;&#38598;&#65292;&#20197;&#23545;&#24494;&#35843;&#21518;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#22312;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;%&#22522;&#20110;&#35777;&#25454;&#30340;&#38382;&#31572;&#26696;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29992;&#20110;&#35780;&#20272;&#30340;&#22235;&#20010;&#27979;&#35797;&#38598;&#65292;&#20197;&#35780;&#20272;&#24494;&#35843;&#21518;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for various research and practical endeavors. One avenue in reaching this goal is basing the answers on reliable sources. However, this Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability). In this work, we systematically investigate how to robustly fine-tune LLMs for better source quality and answer attributability. Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale. We further introduce four test sets to benchmark the robustness of fine-tuned specialist models. Extensive evaluation shows that fine-tuning on synthetic data improves performance on both in- and out-of-distribution. %Evidence-Based QA cases. Furthermore, we sho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25991;&#26412;&#20013;&#24515;&#23545;&#40784;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65288;TAMML&#65289;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#20316;&#20026;&#32479;&#19968;&#30340;&#35821;&#20041;&#31354;&#38388;&#65292;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#27169;&#24577;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#22312;&#22788;&#29702;&#26410;&#35265;&#36807;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#12289;&#19981;&#21487;&#39044;&#27979;&#30340;&#27169;&#24577;&#32452;&#21512;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.08086</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#25991;&#26412;&#20013;&#24515;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Text-centric Alignment for Multi-Modality Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25991;&#26412;&#20013;&#24515;&#23545;&#40784;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65288;TAMML&#65289;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#20316;&#20026;&#32479;&#19968;&#30340;&#35821;&#20041;&#31354;&#38388;&#65292;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#27169;&#24577;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#22312;&#22788;&#29702;&#26410;&#35265;&#36807;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#12289;&#19981;&#21487;&#39044;&#27979;&#30340;&#27169;&#24577;&#32452;&#21512;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#25991;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#27169;&#24577;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#21363;&#25512;&#29702;&#38454;&#27573;&#21487;&#29992;&#30340;&#27169;&#24577;&#19982;&#35757;&#32451;&#38454;&#27573;&#19981;&#21516;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25991;&#26412;&#20013;&#24515;&#23545;&#40784;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65288;TAMML&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#20511;&#21161;&#22522;&#30784;&#27169;&#22411;&#22686;&#24378;&#22810;&#27169;&#24577;&#31995;&#32479;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#25991;&#26412;&#20316;&#20026;&#32479;&#19968;&#30340;&#35821;&#20041;&#31354;&#38388;&#30340;&#29420;&#29305;&#29305;&#24615;&#65292;TAMML&#22312;&#22788;&#29702;&#26410;&#35265;&#36807;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#12289;&#19981;&#21487;&#39044;&#27979;&#30340;&#27169;&#24577;&#32452;&#21512;&#26041;&#38754;&#23637;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;TAMML&#19981;&#20165;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#27169;&#24577;&#65292;&#36824;&#33021;&#20445;&#25345;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#20811;&#26381;&#20256;&#32479;&#30340;&#22266;&#23450;&#27169;&#24577;&#26694;&#26550;&#20013;&#30340;&#34920;&#31034;&#23884;&#20837;&#38480;&#21046;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#27169;&#24577;&#30340;&#21487;&#29992;&#24615;&#21487;&#33021;&#20250;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research paper addresses the challenge of modality mismatch in multimodal learning, where the modalities available during inference differ from those available at training. We propose the Text-centric Alignment for Multi-Modality Learning (TAMML) approach, an innovative method that utilizes Large Language Models (LLMs) with in-context learning and foundation models to enhance the generalizability of multimodal systems under these conditions. By leveraging the unique properties of text as a unified semantic space, TAMML demonstrates significant improvements in handling unseen, diverse, and unpredictable modality combinations. TAMML not only adapts to varying modalities but also maintains robust performance, showcasing the potential of foundation models in overcoming the limitations of traditional fixed-modality frameworks in embedding representations. This study contributes to the field by offering a flexible, effective solution for real-world applications where modality availabili
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Reveal&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22312;&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#31572;&#35774;&#32622;&#20013;&#23545;&#22797;&#26434;&#24605;&#32500;&#38142;&#30340;&#33258;&#21160;&#39564;&#35777;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#35814;&#23613;&#30340;&#26631;&#31614;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#31572;&#26696;&#20013;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#30340;&#30456;&#20851;&#24615;&#12289;&#24402;&#22240;&#21644;&#36923;&#36753;&#27491;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00559</link><description>&lt;p&gt;
&#19968;&#26465;&#24605;&#32500;&#38142;&#26465;&#30340;&#24378;&#24230;&#21462;&#20915;&#20110;&#26368;&#24369;&#30340;&#29615;&#33410;&#65306;&#19968;&#20010;&#39564;&#35777;&#25512;&#29702;&#38142;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Reveal&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22312;&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#31572;&#35774;&#32622;&#20013;&#23545;&#22797;&#26434;&#24605;&#32500;&#38142;&#30340;&#33258;&#21160;&#39564;&#35777;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#35814;&#23613;&#30340;&#26631;&#31614;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#31572;&#26696;&#20013;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#30340;&#30456;&#20851;&#24615;&#12289;&#24402;&#22240;&#21644;&#36923;&#36753;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20419;&#20351;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#36880;&#27493;&#22238;&#31572;&#65288;&#20363;&#22914;&#8220;&#24605;&#32500;&#38142;&#8221;&#65289;&#26159;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#20854;&#20013;&#26356;&#20934;&#30830;&#30340;&#25512;&#29702;&#38142;&#36890;&#24120;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#25991;&#29486;&#35752;&#35770;&#20102;&#33258;&#21160;&#39564;&#35777;&#25512;&#29702;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#21644;&#25913;&#21892;&#20854;&#27491;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#32454;&#31890;&#24230;&#30340;&#27493;&#39588;&#32423;&#25968;&#25454;&#38598;&#65292;&#26080;&#27861;&#23545;&#36825;&#31867;&#39564;&#35777;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Reveal&#65306;&#25512;&#29702;&#39564;&#35777;&#35780;&#20272;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22312;&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#31572;&#35774;&#32622;&#20013;&#23545;&#22797;&#26434;&#24605;&#32500;&#38142;&#30340;&#33258;&#21160;&#39564;&#35777;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;Reveal&#21253;&#25324;&#23545;&#35821;&#35328;&#27169;&#22411;&#31572;&#26696;&#20013;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#30340;&#30456;&#20851;&#24615;&#12289;&#24402;&#22240;&#20110;&#35777;&#25454;&#27573;&#33853;&#20197;&#21450;&#36923;&#36753;&#27491;&#30830;&#24615;&#30340;&#20840;&#38754;&#26631;&#31614;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting language models to provide step-by-step answers (e.g., "Chain-of-Thought") is the prominent approach for complex reasoning tasks, where more accurate reasoning chains typically improve downstream task performance. Recent literature discusses automatic methods to verify reasoning steps to evaluate and improve their correctness. However, no fine-grained step-level datasets are available to enable thorough evaluation of such verification methods, hindering progress in this direction. We introduce Reveal: Reasoning Verification Evaluation, a new dataset to benchmark automatic verifiers of complex Chain-of-Thought reasoning in open-domain question answering settings. Reveal includes comprehensive labels for the relevance, attribution to evidence passages, and logical correctness of each reasoning step in a language model's answer, across a wide variety of datasets and state-of-the-art language models.
&lt;/p&gt;</description></item><item><title>LLaMA&#21644;ChatGPT&#27604;&#36739;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;SMILES&#23383;&#31526;&#20018;&#23884;&#20837;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#21644;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#20013;&#65292;LLaMA&#30456;&#23545;&#20110;ChatGPT&#34920;&#29616;&#26356;&#22909;&#24182;&#19988;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2402.00024</link><description>&lt;p&gt;
LLaMA&#21644;ChatGPT&#23884;&#20837;&#22312;&#20998;&#23376;&#23884;&#20837;&#20013;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of LLaMA and ChatGPT Embeddings for Molecule Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00024
&lt;/p&gt;
&lt;p&gt;
LLaMA&#21644;ChatGPT&#27604;&#36739;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;SMILES&#23383;&#31526;&#20018;&#23884;&#20837;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#21644;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#20013;&#65292;LLaMA&#30456;&#23545;&#20110;ChatGPT&#34920;&#29616;&#26356;&#22909;&#24182;&#19988;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;ChatGPT&#21644;LLaMA&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#21040;&#37325;&#35270;&#65292;&#29305;&#21035;&#26159;&#22312;&#35299;&#37322;Simplified Molecular Input Line Entry System (SMILES)&#26041;&#38754;&#12290;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;SMILES&#23383;&#31526;&#20018;&#35299;&#30721;&#20026;&#21521;&#37327;&#34920;&#31034;&#65292;&#20026;&#29702;&#35299;&#21270;&#23398;&#22270;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#30740;&#31350;&#20102;ChatGPT&#21644;LLaMA&#22312;&#23884;&#20837;SMILES&#23383;&#31526;&#20018;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#38598;&#20013;&#22312;&#20004;&#20010;&#20851;&#38190;&#24212;&#29992;&#39046;&#22495;&#65306;&#20998;&#23376;&#24615;&#36136;&#65288;MP&#65289;&#39044;&#27979;&#21644;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65288;DDI&#65289;&#39044;&#27979;&#65292;&#36825;&#22312;&#33647;&#29289;&#24320;&#21457;&#21644;&#21307;&#30103;&#20445;&#20581;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;LLaMA&#29983;&#25104;&#30340;SMILES&#23884;&#20837;&#22312;MP&#21644;DDI&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;ChatGPT&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22522;&#20110;LLaMA&#30340;SMILES&#23884;&#20837;&#22312;&#36825;&#20004;&#20010;&#39044;&#27979;&#20219;&#21153;&#20013;&#26174;&#31034;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;&#32467;&#35770;&#65306;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#20013;&#24212;&#29992;LLMs&#65292;&#29305;&#21035;&#26159;&#22312;&#21033;&#29992;SMILES&#36827;&#34892;&#23884;&#20837;&#26041;&#38754;&#65292;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: Large Language Models (LLMs) like ChatGPT and LLaMA are increasingly recognized for their potential in the field of cheminformatics, particularly in interpreting Simplified Molecular Input Line Entry System (SMILES), a standard method for representing chemical structures. These LLMs can decode SMILES strings into vector representations, providing a novel approach to understanding chemical graphs.   Methods: We investigate the performance of ChatGPT and LLaMA in embedding SMILES strings. Our evaluation focuses on two key applications: molecular property (MP) prediction and drug-drug interaction (DDI) prediction, both essential in drug development and healthcare.   Results: We find that SMILES embeddings generated using LLaMA outperform those from ChatGPT in both MP and DDI prediction tasks. Notably, LLaMA-based SMILES embeddings show results comparable to existing methods in both prediction tasks.   Conclusion: The application of LLMs in cheminformatics, particularly in utilizi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;LLMs&#26469;&#22686;&#24378;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#36127;&#38754;&#24433;&#21709;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19982;&#26032;&#38395;&#23186;&#20307;&#23545;&#40784;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;&#21313;&#20010;&#31867;&#21035;&#30340;&#20154;&#24037;&#26234;&#33021;&#24433;&#21709;&#20998;&#31867;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#25351;&#20196;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;LLMs&#27169;&#22411;&#22312;&#29983;&#25104;&#24433;&#21709;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.18028</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#25903;&#25345;&#39044;&#26399;&#27835;&#29702;: &#36890;&#36807;&#19982;&#26032;&#38395;&#23186;&#20307;&#23545;&#40784;&#65292;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#30340;&#36127;&#38754;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Supporting Anticipatory Governance using LLMs: Evaluating and Aligning Large Language Models with the News Media to Anticipate the Negative Impacts of AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;LLMs&#26469;&#22686;&#24378;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#36127;&#38754;&#24433;&#21709;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19982;&#26032;&#38395;&#23186;&#20307;&#23545;&#40784;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;&#21313;&#20010;&#31867;&#21035;&#30340;&#20154;&#24037;&#26234;&#33021;&#24433;&#21709;&#20998;&#31867;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#25351;&#20196;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;LLMs&#27169;&#22411;&#22312;&#29983;&#25104;&#24433;&#21709;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#21457;&#23637;&#30340;&#26089;&#26399;&#38454;&#27573;&#65292;&#39044;&#27979;&#20854;&#21487;&#33021;&#24102;&#26469;&#30340;&#36127;&#38754;&#24433;&#21709;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20351;&#29992;LLMs&#22686;&#24378;&#21644;&#25351;&#23548;&#36825;&#19968;&#36807;&#31243;&#26159;&#19968;&#31181;&#19981;&#22826;&#34987;&#30740;&#31350;&#30340;&#39044;&#27979;&#26041;&#27861;&#12290;&#23613;&#31649;LLMs&#21644;&#35780;&#20272;&#25351;&#26631;&#22312;&#29983;&#25104;&#25991;&#26412;&#20013;&#32771;&#34385;&#20559;&#24046;&#26041;&#38754;&#26377;&#25152;&#36827;&#23637;&#65292;&#20294;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#22914;&#20309;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20351;&#29992;LLMs&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#24433;&#21709;&#24341;&#21457;&#20102;&#20851;&#20110;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#31867;&#21035;&#30340;&#36136;&#37327;&#21644;&#33539;&#22260;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20016;&#23500;&#30340;&#21253;&#21547;&#23545;&#26032;&#20852;&#25216;&#26415;&#30340;&#35268;&#33539;&#24615;&#35780;&#20272;&#30340;&#25968;&#25454;&#26469;&#28304;&#8212;&#8212;&#26032;&#38395;&#23186;&#20307;&#65292;&#21046;&#23450;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#31867;&#21035;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#35745;&#31639;&#20998;&#26512;&#20840;&#29699;&#25968;&#30334;&#20010;&#22312;&#32447;&#26032;&#38395;&#23186;&#20307;&#21457;&#24067;&#30340;&#25968;&#21315;&#31687;&#26032;&#38395;&#25991;&#31456;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;&#21313;&#20010;&#31867;&#21035;&#30340;&#20154;&#24037;&#26234;&#33021;&#24433;&#21709;&#20998;&#31867;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#25351;&#20196;&#30340;LLMs&#27169;&#22411;&#65288;GPT-4&#31561;&#65289;&#21644;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#26681;&#25454;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#29983;&#25104;&#30340;&#24433;&#21709;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anticipating the negative impacts of emerging AI technologies is a challenge, especially in the early stages of development. An understudied approach to such anticipation is the use of LLMs to enhance and guide this process. Despite advancements in LLMs and evaluation metrics to account for biases in generated text, it is unclear how well these models perform in anticipatory tasks. Specifically, the use of LLMs to anticipate AI impacts raises questions about the quality and range of categories of negative impacts these models are capable of generating. In this paper we leverage news media, a diverse data source that is rich with normative assessments of emerging technologies, to formulate a taxonomy of impacts to act as a baseline for comparing against. By computationally analyzing thousands of news articles published by hundreds of online news domains around the world, we develop a taxonomy consisting of ten categories of AI impacts. We then evaluate both instruction-based (GPT-4 and 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#24694;&#24847;&#21644;&#26080;&#23475;&#26597;&#35810;&#20043;&#38388;&#30340;&#21306;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20248;&#21270;&#23433;&#20840;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2401.18018</link><description>&lt;p&gt;
&#36890;&#36807;&#23450;&#21521;&#34920;&#31034;&#20248;&#21270;&#23454;&#29616;&#30340;&#23433;&#20840;&#25552;&#31034;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Prompt-Driven LLM Safeguarding via Directed Representation Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18018
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#24694;&#24847;&#21644;&#26080;&#23475;&#26597;&#35810;&#20043;&#38388;&#30340;&#21306;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20248;&#21270;&#23433;&#20840;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#65292;&#20351;&#29992;&#23433;&#20840;&#25552;&#31034;&#22312;&#27169;&#22411;&#36755;&#20837;&#20043;&#21069;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#20445;&#25252;&#23454;&#36341;&#65292;&#20197;&#20351;&#20854;&#19981;&#36981;&#20174;&#21253;&#21547;&#24694;&#24847;&#24847;&#22270;&#30340;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#23433;&#20840;&#25552;&#31034;&#30340;&#24037;&#20316;&#26426;&#21046;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#65292;&#36825;&#22952;&#30861;&#20102;&#33258;&#21160;&#20248;&#21270;&#20854;&#20197;&#25913;&#21892;LLM&#23433;&#20840;&#24615;&#30340;&#28508;&#21147;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#34920;&#31034;&#30340;&#35282;&#24230;&#35843;&#26597;&#20102;&#23433;&#20840;&#25552;&#31034;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#65292;&#26377;&#23475;&#21644;&#26080;&#23475;&#30340;&#26597;&#35810;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21306;&#20998;&#24320;&#26469;&#65292;&#20294;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#36825;&#19968;&#21306;&#20998;&#12290;&#30456;&#21453;&#65292;&#19981;&#21516;&#23433;&#20840;&#25552;&#31034;&#23548;&#33268;&#26597;&#35810;&#30340;&#34920;&#31034;&#26397;&#30528;&#30456;&#20284;&#30340;&#26041;&#21521;&#31227;&#21160;&#65292;&#20351;&#24471;&#27169;&#22411;&#21363;&#20351;&#22312;&#26597;&#35810;&#26080;&#23475;&#26102;&#20063;&#26356;&#23481;&#26131;&#25298;&#32477;&#25552;&#20379;&#21327;&#21161;&#12290;&#21463;&#21040;&#36825;&#20123;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#65288;&#23450;&#21521;&#34920;&#31034;&#20248;&#21270;&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#23433;&#20840;&#25552;&#31034;&#20248;&#21270;&#12290;DRO&#23558;&#23433;&#20840;&#25552;&#31034;&#35270;&#20026;&#35201;&#20248;&#21270;&#30340;&#34920;&#31034;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prepending model inputs with safety prompts is a common practice of safeguarding large language models (LLMs) from complying with queries that contain harmful intents. However, the working mechanisms of safety prompts have not yet been fully understood, which hinders the potential for automatically optimizing them for improved LLM safety. Motivated by this problem, we investigate the impact of safety prompts from the perspective of model representations. We find that in models' representation space, harmful and harmless queries can be largely distinguished, but this is not noticeably enhanced by safety prompts. Instead, the queries' representations are moved by different safety prompts in similar directions, where models become more prone to refusal (i.e., refusing to provide assistance) even when the queries are harmless. Inspired by these findings, we propose a method called DRO (Directed Representation Optimization) for automatic safety prompt optimization. DRO treats safety prompts
&lt;/p&gt;</description></item><item><title>Gemini&#23478;&#26063;&#26159;&#19968;&#31995;&#21015;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20854;&#20013;&#26368;&#20855;&#33021;&#21147;&#30340;Gemini Ultra&#27169;&#22411;&#22312;30&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#25512;&#36827;&#20102;&#25216;&#26415;&#21069;&#27839;&#65292;&#24182;&#25913;&#36827;&#20102;&#25152;&#26377;20&#20010;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#30340;&#25216;&#26415;&#29366;&#24577;&#12290;</title><link>https://arxiv.org/abs/2312.11805</link><description>&lt;p&gt;
Gemini&#65306;&#19968;&#31995;&#21015;&#39640;&#24615;&#33021;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gemini: A Family of Highly Capable Multimodal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11805
&lt;/p&gt;
&lt;p&gt;
Gemini&#23478;&#26063;&#26159;&#19968;&#31995;&#21015;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20854;&#20013;&#26368;&#20855;&#33021;&#21147;&#30340;Gemini Ultra&#27169;&#22411;&#22312;30&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#25512;&#36827;&#20102;&#25216;&#26415;&#21069;&#27839;&#65292;&#24182;&#25913;&#36827;&#20102;&#25152;&#26377;20&#20010;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#30340;&#25216;&#26415;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#31995;&#21015;Gemini&#65292;&#23637;&#31034;&#20986;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;Gemini&#31995;&#21015;&#21253;&#25324;Ultra&#12289;Pro&#21644;Nano&#23610;&#23544;&#65292;&#36866;&#29992;&#20110;&#20174;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#21040;&#35774;&#22791;&#20869;&#23384;&#21463;&#38480;&#24212;&#29992;&#30340;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#26368;&#20855;&#33021;&#21147;&#30340;Gemini Ultra&#27169;&#22411;&#22312;32&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;30&#20010;&#20013;&#25512;&#36827;&#20102;&#25216;&#26415;&#21069;&#27839; - &#26174;&#33879;&#22320;&#26159;&#31532;&#19968;&#20010;&#22312;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#32771;&#35797;&#22522;&#20934;&#27979;&#35797;MMLU&#19978;&#23454;&#29616;&#20154;&#31867;&#19987;&#23478;&#27700;&#24179;&#34920;&#29616;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#25105;&#20204;&#30740;&#31350;&#30340;&#27599;&#19968;&#20010;20&#20010;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#20013;&#25913;&#36827;&#20102;&#25216;&#26415;&#21069;&#27839;&#12290;&#25105;&#20204;&#30456;&#20449;Gemini&#31995;&#21015;&#22312;&#36328;&#27169;&#24577;&#25512;&#29702;&#21644;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#30340;&#26032;&#33021;&#21147;&#23558;&#33021;&#22815;&#25903;&#25345;&#21508;&#31181;&#29992;&#20363;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;&#22320;&#21521;&#29992;&#25143;&#25552;&#20379;Gemini&#27169;&#22411;&#30340;&#35757;&#32451;&#21518;&#21644;&#37096;&#32626;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11805v2 Announce Type: replace-cross  Abstract: This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services includi
&lt;/p&gt;</description></item><item><title>&#20013;&#22269;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#38656;&#35201;&#26356;&#21152;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#28779;&#28976;"&#65288;Flames&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#24120;&#35265;&#30340;&#26080;&#23475;&#21407;&#21017;&#21644;&#29305;&#23450;&#20013;&#22269;&#20215;&#20540;&#35266;&#65292;&#20197;&#21450;&#22797;&#26434;&#22330;&#26223;&#21644;&#38544;&#21547;&#24694;&#24847;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.06899</link><description>&lt;p&gt;
&#28779;&#28976;: &#35780;&#20272;&#20013;&#22269;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#22865;&#21512;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Flames: Benchmarking Value Alignment of Chinese Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06899
&lt;/p&gt;
&lt;p&gt;
&#20013;&#22269;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#38656;&#35201;&#26356;&#21152;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#28779;&#28976;"&#65288;Flames&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#24120;&#35265;&#30340;&#26080;&#23475;&#21407;&#21017;&#21644;&#29305;&#23450;&#20013;&#22269;&#20215;&#20540;&#35266;&#65292;&#20197;&#21450;&#22797;&#26434;&#22330;&#26223;&#21644;&#38544;&#21547;&#24694;&#24847;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#22320;&#21306;&#30340;&#24191;&#27867;&#24212;&#29992;&#24378;&#35843;&#20102;&#35780;&#20272;&#23427;&#20204;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#30340;&#36843;&#20999;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#26410;&#33021;&#26377;&#25928;&#22320;&#25581;&#31034;LLMs&#20013;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;&#23613;&#31649;&#35768;&#22810;&#27169;&#22411;&#22312;&#36825;&#20123;&#35780;&#20272;&#20013;&#24471;&#20998;&#24456;&#39640;&#65292;&#19988;&#8220;&#21517;&#21015;&#21069;&#33541;&#8221;&#65292;&#20294;&#22312;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#28145;&#23618;&#22865;&#21512;&#24615;&#21644;&#23454;&#29616;&#30495;&#27491;&#26080;&#23475;&#26041;&#38754;&#20173;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#28779;&#28976;"&#65288;Flames&#65289;&#30340;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#28085;&#30422;&#20102;&#24120;&#35265;&#30340;&#26080;&#23475;&#21407;&#21017;&#65292;&#20197;&#21450;&#19968;&#20010;&#25972;&#21512;&#20102;&#29305;&#23450;&#20013;&#22269;&#20215;&#20540;&#35266;&#22914;&#21644;&#35856;&#30340;&#29420;&#29305;&#36947;&#24503;&#32500;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#21253;&#21547;&#22797;&#26434;&#24773;&#22659;&#21644;&#22823;&#22810;&#24102;&#26377;&#38544;&#21547;&#24694;&#24847;&#30340;&#30772;&#35299;&#26041;&#27861;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#12290;&#36890;&#36807;&#23545;17&#20010;&#20027;&#27969;LLMs&#36827;&#34892;&#25552;&#31034;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#27169;&#22411;&#30340;&#22238;&#24212;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35814;&#32454;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06899v2 Announce Type: replace-cross  Abstract: The widespread adoption of large language models (LLMs) across various regions underscores the urgent need to evaluate their alignment with human values. Current benchmarks, however, fall short of effectively uncovering safety vulnerabilities in LLMs. Despite numerous models achieving high scores and 'topping the chart' in these evaluations, there is still a significant gap in LLMs' deeper alignment with human values and achieving genuine harmlessness. To this end, this paper proposes a value alignment benchmark named Flames, which encompasses both common harmlessness principles and a unique morality dimension that integrates specific Chinese values such as harmony. Accordingly, we carefully design adversarial prompts that incorporate complex scenarios and jailbreaking methods, mostly with implicit malice. By prompting 17 mainstream LLMs, we obtain model responses and rigorously annotate them for detailed evaluation. Our findin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;</title><link>http://arxiv.org/abs/2401.01854</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;
&lt;/p&gt;
&lt;p&gt;
Multilingual Instruction Tuning With Just a Pinch of Multilinguality. (arXiv:2401.01854v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20840;&#29699;&#37319;&#32435;&#65292;&#23427;&#20204;&#22312;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#36890;&#36807;&#22312;&#21478;&#19968;&#31181;&#35821;&#35328;&#19978;&#24494;&#35843;&#65292;&#27169;&#22411;&#21487;&#20197;&#22312;&#26576;&#31181;&#35821;&#35328;&#19978;&#33719;&#24471;&#29305;&#23450;&#30340;&#21151;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;LLM&#22312;&#25351;&#20196;&#35843;&#20248;&#36807;&#31243;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#39318;&#20808;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#33521;&#35821;&#35843;&#20248;&#38598;&#21512;&#20013;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#65292;&#22312;&#35843;&#20248;&#36807;&#31243;&#20013;&#19981;&#35770;&#26159;&#24050;&#35265;&#35821;&#35328;&#36824;&#26159;&#26410;&#35265;&#35821;&#35328;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
As instruction-tuned large language models (LLMs) gain global adoption, their ability to follow instructions in multiple languages becomes increasingly crucial. One promising approach is cross-lingual transfer, where a model acquires specific functionality on some language by finetuning on another language. In this work, we investigate how multilinguality during instruction tuning of a multilingual LLM affects instruction-following across languages. We first show that many languages transfer some instruction-following capabilities to other languages from even monolingual tuning. Furthermore, we find that only 40 multilingual examples in an English tuning set substantially improve multilingual instruction-following, both in seen and unseen languages during tuning. In general, we observe that models tuned on multilingual mixtures exhibit comparable or superior performance in several languages compared to monolingually tuned models, despite training on 10x fewer examples in those language
&lt;/p&gt;</description></item><item><title>BioCoder&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.16458</link><description>&lt;p&gt;
BioCoder: &#19968;&#31181;&#24102;&#26377;&#19978;&#19979;&#25991;&#35821;&#29992;&#30693;&#35782;&#30340;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. (arXiv:2308.16458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16458
&lt;/p&gt;
&lt;p&gt;
BioCoder&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#26174;&#33879;&#25913;&#36827;&#20102;&#20195;&#30721;&#29983;&#25104;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#30340;&#25193;&#22823;&#65292;&#38656;&#35201;&#36755;&#20986;&#26469;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#30340;&#38656;&#27714;&#20063;&#36234;&#26469;&#36234;&#22810;&#12290;&#27492;&#22806;&#65292;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#65292;&#29983;&#25104;&#21151;&#33021;&#31243;&#24207;&#30001;&#20110;&#39046;&#22495;&#30693;&#35782;&#37327;&#22823;&#12289;&#38656;&#35201;&#22797;&#26434;&#30340;&#25968;&#25454;&#25805;&#20316;&#21644;&#22797;&#26434;&#30340;&#21151;&#33021;&#20381;&#36182;&#20851;&#31995;&#32780;&#38754;&#20020;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BioCoder&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#29616;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#12290;&#19982;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#26377;&#20851;&#65292;BioCoder&#28085;&#30422;&#20102;&#21487;&#33021;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#12290;&#23427;&#21253;&#25324;&#26469;&#33258;GitHub&#30340;1026&#20010;Python&#21644;Java&#20989;&#25968;&#21644;1243&#20010;&#26041;&#27861;&#65292;&#20197;&#21450;&#26469;&#33258;Rosalind&#39033;&#30446;&#30340;253&#20010;&#31034;&#20363;&#12290;BioCoder&#36824;&#32467;&#21512;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#65292;&#25105;&#20204;&#24050;&#32463;&#24212;&#29992;&#23427;&#26469;&#35780;&#20272;&#35768;&#22810;&#27169;&#22411;&#65292;&#21253;&#25324;InCoder&#12289;CodeGen&#12289;CodeGen2&#12289;SantaCoder&#12289;StarCoder&#12289;StarCoder+&#12289;InstructCodeT&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models like ChatGPT have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks. Moreover, in bioinformatics, generating functional programs poses additional notable challenges due to the amount of domain knowledge, the need for complicated data operations, and intricate functional dependencies between the operations. Here, we present BioCoder, a benchmark developed to evaluate existing pre-trained models in generating bioinformatics code. In relation to function-code generation, BioCoder covers potential package dependencies, class declarations, and global variables. It incorporates 1026 functions and 1243 methods in Python and Java from GitHub and 253 examples from the Rosalind Project. BioCoder incorporates a fuzz-testing framework for evaluation, and we have applied it to evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#25991;&#26412;&#25968;&#25454;&#25903;&#25345;&#30005;&#23376;&#20581;&#24247;&#25968;&#25454;&#30340;&#22240;&#26524;&#25512;&#26029;&#65292;&#22312;&#20020;&#24202;&#30740;&#31350;&#20013;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#24739;&#32773;&#20449;&#24687;&#65292;&#20026;&#21305;&#37197;&#20998;&#26512;&#24341;&#20837;&#20102;&#25991;&#26412;&#65292;&#25913;&#21892;&#20102;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#30340;&#25928;&#26524;&#65292;&#24182;&#22686;&#24378;&#20102;&#21305;&#37197;&#36807;&#31243;&#30340;&#21512;&#29702;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03687</link><description>&lt;p&gt;
&#21033;&#29992;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#30005;&#23376;&#21307;&#30103;&#35760;&#24405;&#30340;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Leveraging text data for causal inference using electronic health records. (arXiv:2307.03687v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03687
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#25991;&#26412;&#25968;&#25454;&#25903;&#25345;&#30005;&#23376;&#20581;&#24247;&#25968;&#25454;&#30340;&#22240;&#26524;&#25512;&#26029;&#65292;&#22312;&#20020;&#24202;&#30740;&#31350;&#20013;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#24739;&#32773;&#20449;&#24687;&#65292;&#20026;&#21305;&#37197;&#20998;&#26512;&#24341;&#20837;&#20102;&#25991;&#26412;&#65292;&#25913;&#21892;&#20102;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#30340;&#25928;&#26524;&#65292;&#24182;&#22686;&#24378;&#20102;&#21305;&#37197;&#36807;&#31243;&#30340;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#26159;&#21307;&#30103;&#25968;&#25454;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#21547;&#20102;&#26377;&#20851;&#24739;&#32773;&#29305;&#24449;&#21644;&#27835;&#30103;&#30340;&#23453;&#36149;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#36890;&#24120;&#22312;&#32467;&#26500;&#21270;&#22270;&#34920;&#25968;&#25454;&#20013;&#32570;&#22833;&#12290;&#23613;&#31649;&#22914;&#27492;&#20016;&#23500;&#65292;&#20294;&#30001;&#20110;&#20854;&#22797;&#26434;&#24615;&#65292;&#23427;&#24456;&#23569;&#22312;&#20020;&#24202;&#30740;&#31350;&#20013;&#20351;&#29992;&#12290;&#21033;&#29992;&#22823;&#37327;&#24739;&#32773;&#35760;&#24405;&#21644;&#27835;&#30103;&#21382;&#21490;&#30340;&#25968;&#25454;&#24211;&#65292;&#20197;&#21450;&#38506;&#25252;&#21307;&#29983;&#21644;&#25252;&#22763;&#30340;&#24191;&#27867;&#31508;&#35760;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#30005;&#23376;&#20581;&#24247;&#25968;&#25454;&#20013;&#21033;&#29992;&#25991;&#26412;&#25968;&#25454;&#25903;&#25345;&#22240;&#26524;&#25512;&#26029;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#20174;&#26500;&#24605;&#21644;&#35774;&#35745;&#21040;&#20998;&#26512;&#21644;&#35299;&#37322;&#65292;&#20165;&#38656;&#24456;&#23569;&#30340;&#39069;&#22806;&#24037;&#20316;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#20351;&#29992;&#37197;&#23545;&#21305;&#37197;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#31181;&#26041;&#24335;&#23558;&#25991;&#26412;&#32435;&#20837;&#32463;&#20856;&#37197;&#23545;&#20998;&#26512;&#20013;&#65306;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#34917;&#20805;&#22810;&#37325;&#25554;&#34917;&#31243;&#24207;&#65292;&#25913;&#21892;&#20102;&#23545;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#30340;&#25554;&#34917;&#20540;&#30340;&#20934;&#30830;&#24615;&#65307;&#36890;&#36807;&#22312;&#21305;&#37197;&#38454;&#27573;&#20013;&#32435;&#20837;&#25991;&#26412;&#65292;&#22686;&#24378;&#20102;&#21305;&#37197;&#36807;&#31243;&#30340;&#21512;&#29702;&#24615;&#65307;&#36890;&#36807;&#23545;&#25991;&#26412;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#65292;&#25105;&#20204;&#21487;&#20197;&#20272;&#35745;...
&lt;/p&gt;
&lt;p&gt;
Text is a ubiquitous component of medical data, containing valuable information about patient characteristics and care that are often missing from structured chart data. Despite this richness, it is rarely used in clinical research, owing partly to its complexity. Using a large database of patient records and treatment histories accompanied by extensive notes by attendant physicians and nurses, we show how text data can be used to support causal inference with electronic health data in all stages, from conception and design to analysis and interpretation, with minimal additional effort. We focus on studies using matching for causal inference. We augment a classic matching analysis by incorporating text in three ways: by using text to supplement a multiple imputation procedure, we improve the fidelity of imputed values to handle missing data; by incorporating text in the matching stage, we strengthen the plausibility of the matching procedure; and by conditioning on text, we can estimat
&lt;/p&gt;</description></item></channel></rss>