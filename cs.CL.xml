<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#27169;&#25311;&#35780;&#20272;&#24320;&#25918;&#24335;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#36890;&#36807;&#21028;&#26029;&#27169;&#22411;&#22312;&#25972;&#20010;&#22242;&#20307;&#19978;&#30340;&#34920;&#29616;&#26159;&#21542;&#22987;&#32456;&#20934;&#30830;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13981</link><description>&lt;p&gt;
&#20445;&#25345;&#30693;&#35782;&#19981;&#21464;&#24615;&#65306;&#37325;&#26032;&#24605;&#32771;&#24320;&#25918;&#20449;&#24687;&#25277;&#21462;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Preserving Knowledge Invariance: Rethinking Robustness Evaluation of Open Information Extraction. (arXiv:2305.13981v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#27169;&#25311;&#35780;&#20272;&#24320;&#25918;&#24335;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#36890;&#36807;&#21028;&#26029;&#27169;&#22411;&#22312;&#25972;&#20010;&#22242;&#20307;&#19978;&#30340;&#34920;&#29616;&#26159;&#21542;&#22987;&#32456;&#20934;&#30830;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#26159;&#30830;&#20445;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#33021;&#22815;&#25104;&#21151;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#32780;&#35328;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#35780;&#20272;&#22522;&#20934;&#37117;&#19987;&#27880;&#20110;&#39564;&#35777;&#37197;&#23545;&#21305;&#37197;&#30340;&#27491;&#30830;&#24615;&#65292;&#24573;&#30053;&#20102;&#20851;&#38190;&#30340;&#40065;&#26834;&#24615;&#27979;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#27169;&#25311;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#35780;&#20272;&#24320;&#25918;&#24335;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#21516;&#19968;&#30693;&#35782;&#21547;&#20041;&#30340;&#21477;&#27861;&#21644;&#34920;&#36798;&#20998;&#24067;&#20250;&#21508;&#19981;&#30456;&#21516;&#12290;&#25105;&#20204;&#35774;&#35745;&#21644;&#27880;&#37322;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#20854;&#20013;&#27599;&#20010;&#31034;&#20363;&#37117;&#26159;&#19968;&#20010;&#30693;&#35782;&#19981;&#21464;&#30340;&#22242;&#20307;&#65292;&#30001;&#20855;&#26377;&#30456;&#21516;&#21547;&#20041;&#20294;&#32467;&#26500;&#19981;&#21516;&#30340;&#21477;&#23376;&#32452;&#25104;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#38416;&#36848;&#40065;&#26834;&#24615;&#25351;&#26631;&#65292;&#24403;&#27169;&#22411;&#22312;&#25972;&#20010;&#22242;&#20307;&#19978;&#30340;&#34920;&#29616;&#22987;&#32456;&#20934;&#30830;&#26102;&#65292;&#34987;&#21028;&#23450;&#20026;&#40065;&#26834;&#24615;&#24378;&#12290;&#25105;&#20204;&#23545;&#36807;&#21435;&#21313;&#24180;&#20013;&#21457;&#34920;&#30340;&#20960;&#31181;&#20856;&#22411;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robustness to distribution changes ensures that NLP models can be successfully applied in the realistic world, especially for information extraction tasks. However, most prior evaluation benchmarks have been devoted to validating pairwise matching correctness, ignoring the crucial measurement of robustness. In this paper, we present the first benchmark that simulates the evaluation of open information extraction models in the real world, where the syntactic and expressive distributions under the same knowledge meaning may drift variously. We design and annotate a large-scale testbed in which each example is a knowledge-invariant clique that consists of sentences with structured knowledge of the same meaning but with different syntactic and expressive forms. By further elaborating the robustness metric, a model is judged to be robust if its performance is consistently accurate on the overall cliques. We perform experiments on typical models published in the last decade as well as a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#23558;&#20869;&#23384;&#31649;&#29702;&#33021;&#21147;&#38598;&#25104;&#21040; BlenderBot3 &#20013;&#20197;&#25913;&#36827;&#20854;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#21270;&#25968;&#25454;&#38598;&#21019;&#24314;&#26469;&#31649;&#29702;&#20869;&#23384;&#12290;&#23545;&#20110; F1 &#20998;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411; BlenderBot3-M^3 &#27604; BlenderBot3 &#25552;&#39640;&#20102; 4%&#12290;</title><link>http://arxiv.org/abs/2305.13973</link><description>&lt;p&gt;
&#26080;&#32541;&#38598;&#25104;&#20869;&#23384;&#31649;&#29702;&#21040;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Effortless Integration of Memory Management into Open-Domain Conversation Systems. (arXiv:2305.13973v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#23558;&#20869;&#23384;&#31649;&#29702;&#33021;&#21147;&#38598;&#25104;&#21040; BlenderBot3 &#20013;&#20197;&#25913;&#36827;&#20854;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#21270;&#25968;&#25454;&#38598;&#21019;&#24314;&#26469;&#31649;&#29702;&#20869;&#23384;&#12290;&#23545;&#20110; F1 &#20998;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411; BlenderBot3-M^3 &#27604; BlenderBot3 &#25552;&#39640;&#20102; 4%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#36890;&#36807;&#27169;&#22359;&#21270;&#26041;&#27861;&#23558;&#22810;&#31181;&#23545;&#35805;&#25216;&#33021;&#25972;&#21512;&#21040;&#21333;&#19968;&#31995;&#32479;&#20013;&#12290;&#28982;&#32780;&#65292;&#35813;&#31995;&#32479;&#30340;&#19968;&#20010;&#38480;&#21046;&#26159;&#32570;&#20047;&#22806;&#37096;&#20869;&#23384;&#30340;&#31649;&#29702;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#23558;&#20869;&#23384;&#31649;&#29702;&#33021;&#21147;&#38598;&#25104;&#21040; BlenderBot3 &#20013;&#20197;&#25913;&#36827;&#20854;&#24615;&#33021;&#12290;&#30001;&#20110;&#27809;&#26377;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#33258;&#21160;&#21270;&#25968;&#25454;&#38598;&#21019;&#24314;&#26041;&#27861;&#26469;&#31649;&#29702;&#20869;&#23384;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861; 1) &#38656;&#35201;&#24456;&#23569;&#30340;&#25968;&#25454;&#26500;&#24314;&#25104;&#26412;&#65292;2) &#19981;&#20250;&#24433;&#21709;&#20854;&#20182;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;3) &#20943;&#23569;&#20102;&#22806;&#37096;&#20869;&#23384;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411; BlenderBot3-M^3&#65292;&#23427;&#26159;&#22810;&#20219;&#21153;&#35757;&#32451;&#30340;&#20869;&#23384;&#31649;&#29702;&#29256;&#26412;&#65292;&#22312; F1 &#20998;&#25968;&#26041;&#38754;&#30456;&#23545;&#20110; BlenderBot3 &#25552;&#39640;&#20102; 4%&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-domain conversation systems integrate multiple conversation skills into a single system through a modular approach. One of the limitations of the system, however, is the absence of management capability for external memory. In this paper, we propose a simple method to improve BlenderBot3 by integrating memory management ability into it. Since no training data exists for this purpose, we propose an automating dataset creation for memory management. Our method 1) requires little cost for data construction, 2) does not affect performance in other tasks, and 3) reduces external memory. We show that our proposed model BlenderBot3-M^3, which is multi-task trained with memory management, outperforms BlenderBot3 with a relative 4% performance gain in terms of F1 score.
&lt;/p&gt;</description></item><item><title>McL-KBQA&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22810;&#39033;&#36873;&#25321;&#23558;LLMs&#30340;&#19968;&#20123;&#26679;&#26412;&#33021;&#21147;&#32435;&#20837;KBQA&#26041;&#27861;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#27010;&#25324;&#33021;&#21147;&#65292;&#26377;&#25928;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13972</link><description>&lt;p&gt;
&#20570;&#20986;&#36873;&#25321;&#65281;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#30693;&#35782;&#24211;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Make a Choice! Knowledge Base Question Answering with In-Context Learning. (arXiv:2305.13972v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13972
&lt;/p&gt;
&lt;p&gt;
McL-KBQA&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22810;&#39033;&#36873;&#25321;&#23558;LLMs&#30340;&#19968;&#20123;&#26679;&#26412;&#33021;&#21147;&#32435;&#20837;KBQA&#26041;&#27861;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#27010;&#25324;&#33021;&#21147;&#65292;&#26377;&#25928;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#26088;&#22312;&#21033;&#29992;&#32473;&#23450;&#30340;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#22238;&#31572;&#20107;&#23454;&#31867;&#38382;&#39064;&#12290;&#30001;&#20110;KB&#30340;&#22823;&#35268;&#27169;&#65292;&#27880;&#37322;&#25968;&#25454;&#26080;&#27861;&#28085;&#30422;KB&#20013;&#30340;&#25152;&#26377;&#20107;&#23454;&#27169;&#24335;&#65292;&#36825;&#20063;&#32473;&#38656;&#35201;&#36275;&#22815;&#27880;&#37322;&#25968;&#25454;&#30340;&#26041;&#27861;&#30340;&#27010;&#25324;&#33021;&#21147;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;LLMs&#22312;&#35768;&#22810;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;&#25105;&#20204;&#26399;&#26395;LLMs&#21487;&#20197;&#24110;&#21161;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#23427;&#20204;&#30340;&#27010;&#25324;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;McL-KBQA&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22810;&#39033;&#36873;&#25321;&#23558;LLMs&#30340;&#19968;&#20123;&#26679;&#26412;&#33021;&#21147;&#32435;&#20837;KBQA&#26041;&#27861;&#24182;&#25552;&#39640;QA&#20219;&#21153;&#25928;&#26524;&#30340;&#26694;&#26550;&#12290;&#22312;&#20004;&#20010;KBQA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;McL-KBQA&#34920;&#29616;&#31454;&#20105;&#21147;&#24378;&#65292;&#27010;&#25324;&#33021;&#21147;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;&#25105;&#20204;&#24076;&#26395;&#25506;&#32034;&#19968;&#31181;&#32467;&#21512;LLMs&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;KBQA&#20013;&#30340;QA&#20219;&#21153;&#65292;&#22914;&#20309;&#20351;&#22238;&#31572;&#35268;&#33539;&#27491;&#30830;&#19988;&#27010;&#25324;&#33021;&#21147;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering over knowledge bases (KBQA) aims to answer factoid questions with a given knowledge base (KB). Due to the large scale of KB, annotated data is impossible to cover all fact schemas in KB, which poses a challenge to the generalization ability of methods that require a sufficient amount of annotated data. Recently, LLMs have shown strong few-shot performance in many NLP tasks. We expect LLM can help existing methods improve their generalization ability, especially in low-resource situations. In this paper, we present McL-KBQA, a framework that incorporates the few-shot ability of LLM into the KBQA method via ICL-based multiple choice and then improves the effectiveness of the QA tasks. Experimental results on two KBQA datasets demonstrate the competitive performance of McL-KBQA with strong improvements in generalization. We expect to explore a new way to QA tasks from KBQA in conjunction with LLM, how to generate answers normatively and correctly with strong generalizat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13971</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#32422;&#26463;&#30340;&#35821;&#35328;&#27169;&#22411;&#28789;&#27963;&#35299;&#30721;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Flexible Grammar-Based Constrained Decoding for Language Models. (arXiv:2305.13971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#23569;&#37327;&#26679;&#26412;&#34920;&#29616;&#65292;&#20294;&#22312;&#29983;&#25104;&#20449;&#24687;&#25552;&#21462;&#25152;&#38656;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26102;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#20010;&#38480;&#21046;&#28304;&#20110;LLM&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#20542;&#21521;&#20110;&#29983;&#25104;&#33258;&#30001;&#25991;&#26412;&#32780;&#19981;&#26159;&#36981;&#24490;&#29305;&#23450;&#35821;&#27861;&#30340;&#31934;&#30830;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#35299;&#30721;&#27493;&#39588;&#20013;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#26469;&#20016;&#23500;&#27169;&#22411;&#12290;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#65292;&#21482;&#26377;&#31526;&#21512;&#35821;&#27861;&#20135;&#29983;&#35268;&#21017;&#30340;&#26377;&#25928;&#20196;&#29260;&#33021;&#34987;&#32771;&#34385;&#21040;&#12290;&#36825;&#26679;&#23601;&#24378;&#21046;&#21482;&#20135;&#29983;&#26377;&#25928;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#38750;&#24120;&#36890;&#29992;&#21644;&#28789;&#27963;&#65292;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;(CFG)&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#33258;&#23450;&#20041;&#32422;&#26463;beam&#25628;&#32034;&#23454;&#29616;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;NLP&#20219;&#21153;&#30340;&#36755;&#20986;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#24418;&#24335;&#35821;&#35328;&#65292;&#20351;&#23427;&#20204;&#36866;&#21512;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#30452;&#25509;&#20351;&#29992;&#12290;&#23545;&#20110;&#36755;&#20986;&#31354;&#38388;&#21462;&#20915;&#20110;&#36755;&#20837;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36755;&#20837;&#30340;CFG&#65292;&#26681;&#25454;&#29305;&#23450;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#26356;&#26032;&#20135;&#29983;&#35268;&#21017;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs have shown impressive few-shot performance across many tasks. However, they still struggle when it comes to generating complex output structures, such as those required for Information Extraction. This limitation stems from the fact that LLMs, without finetuning, tend to generate free text rather than precise structures that follow a specific grammar. In this work, we propose to enrich the decoding step with formal grammar constraints. During beam search, only valid token continuations compliant with the grammar production rules are considered. This enforces the generation of valid sequences exclusively. Our framework is highly general and flexible, allowing any Context-Free Grammar (CFG) to be integrated into our custom constrained beam search implementation. We demonstrate that the outputs of many NLP tasks can be represented as formal languages, making them suitable for direct use in our framework. For task where the output space is dependent on the input, we propose input-depe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#24212;&#29992;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#33719;&#21462;&#26694;&#26550;&#20803;&#32032;&#30693;&#35782;&#65292;&#20197;&#36866;&#21512;&#20110;&#21306;&#20998;&#26694;&#26550;&#20803;&#32032;&#35282;&#33394;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13944</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#30340;&#35821;&#20041;&#26694;&#26550;&#35825;&#23548;&#20013;&#30340;&#26694;&#26550;&#20803;&#32032;&#30693;&#35782;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
Acquiring Frame Element Knowledge with Deep Metric Learning for Semantic Frame Induction. (arXiv:2305.13944v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#24212;&#29992;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#33719;&#21462;&#26694;&#26550;&#20803;&#32032;&#30693;&#35782;&#65292;&#20197;&#36866;&#21512;&#20110;&#21306;&#20998;&#26694;&#26550;&#20803;&#32032;&#35282;&#33394;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#26694;&#26550;&#35825;&#23548;&#20219;&#21153;&#23558;&#21333;&#35789;&#32858;&#31867;&#20026;&#23427;&#20204;&#25152;&#21796;&#36215;&#30340;&#26694;&#26550;&#65292;&#24182;&#26681;&#25454;&#23427;&#20204;&#24212;&#35813;&#22635;&#20805;&#30340;&#26694;&#26550;&#20803;&#32032;&#35282;&#33394;&#23545;&#23427;&#20204;&#30340;&#21442;&#25968;&#36827;&#34892;&#32858;&#31867;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#26694;&#26550;&#32858;&#31867;&#20219;&#21153;&#30340;&#21518;&#32773;&#65292;&#26088;&#22312;&#33719;&#21462;&#26694;&#26550;&#20803;&#32032;&#30693;&#35782;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24102;&#26377;&#26694;&#26550;&#27880;&#37322;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#36866;&#21512;&#20110;&#21306;&#20998;&#26694;&#26550;&#20803;&#32032;&#35282;&#33394;&#65292;&#24182;&#36890;&#36807;&#20174;&#24494;&#35843;&#27169;&#22411;&#33719;&#24471;&#30340;&#23884;&#20837;&#36827;&#34892;&#21442;&#25968;&#32858;&#31867;&#12290;FrameNet&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#35201;&#22909;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
The semantic frame induction tasks are defined as a clustering of words into the frames that they evoke, and a clustering of their arguments according to the frame element roles that they should fill. In this paper, we address the latter task of argument clustering, which aims to acquire frame element knowledge, and propose a method that applies deep metric learning. In this method, a pre-trained language model is fine-tuned to be suitable for distinguishing frame element roles through the use of frame-annotated data, and argument clustering is performed with embeddings obtained from the fine-tuned model. Experimental results on FrameNet demonstrate that our method achieves substantially better performance than existing methods.
&lt;/p&gt;</description></item><item><title>&#31526;&#21495;&#35821;&#35328;&#20219;&#21153;&#20013;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#34987;&#25552;&#20986;&#12290;SymGen&#30001;&#20449;&#24687;&#25552;&#31034;&#21644;&#22522;&#20110;&#21327;&#35758;&#30340;&#39564;&#35777;&#22120;&#32452;&#25104;&#65292;&#21487;&#20197;&#29983;&#25104;&#21508;&#31181;&#27880;&#37322;&#26114;&#36149;&#30340;&#31526;&#21495;&#35821;&#35328;&#25968;&#25454;&#12290;&#30456;&#23545;&#20110;LLMs&#65292;&#20351;&#29992;1%&#22823;&#23567;&#30340;&#20219;&#21153;&#27169;&#22411;&#24615;&#33021;&#30456;&#24403;&#25110;&#26356;&#22909;&#65292;&#22823;&#24133;&#21066;&#20943;&#20102;&#25512;&#29702;&#21644;&#37096;&#32626;&#25104;&#26412;&#12290;&#20351;&#29992;SymGen&#29983;&#25104;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#31526;&#21495;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13917</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31526;&#21495;&#35821;&#35328;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Generating Data for Symbolic Language with Large Language Models. (arXiv:2305.13917v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13917
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#35821;&#35328;&#20219;&#21153;&#20013;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#34987;&#25552;&#20986;&#12290;SymGen&#30001;&#20449;&#24687;&#25552;&#31034;&#21644;&#22522;&#20110;&#21327;&#35758;&#30340;&#39564;&#35777;&#22120;&#32452;&#25104;&#65292;&#21487;&#20197;&#29983;&#25104;&#21508;&#31181;&#27880;&#37322;&#26114;&#36149;&#30340;&#31526;&#21495;&#35821;&#35328;&#25968;&#25454;&#12290;&#30456;&#23545;&#20110;LLMs&#65292;&#20351;&#29992;1%&#22823;&#23567;&#30340;&#20219;&#21153;&#27169;&#22411;&#24615;&#33021;&#30456;&#24403;&#25110;&#26356;&#22909;&#65292;&#22823;&#24133;&#21066;&#20943;&#20102;&#25512;&#29702;&#21644;&#37096;&#32626;&#25104;&#26412;&#12290;&#20351;&#29992;SymGen&#29983;&#25104;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#31526;&#21495;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24102;&#26469;&#20102;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#20063;&#22686;&#21152;&#20102;&#22797;&#26434;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24320;&#22987;&#23558;LLMs&#36716;&#25442;&#20026;&#25968;&#25454;&#29983;&#25104;&#22120;&#32780;&#19981;&#26159;&#20219;&#21153;&#25512;&#29702;&#22120;&#65292;&#36890;&#36807;&#35757;&#32451;&#21478;&#19968;&#20010;&#21487;&#36127;&#25285;&#30340;&#20219;&#21153;&#27169;&#22411;&#20197;&#23454;&#29616;&#39640;&#25928;&#37096;&#32626;&#21644;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20027;&#35201;&#34987;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#24182;&#19988;&#23578;&#26410;&#25506;&#32034;&#29992;&#20110;&#20855;&#26377;&#22797;&#26434;&#32467;&#26500;&#36755;&#20986;&#65288;&#20363;&#22914;&#35821;&#20041;&#35299;&#26512;&#21644;&#20195;&#30721;&#29983;&#25104;&#65289;&#30340;&#31526;&#21495;&#35821;&#35328;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SymGen&#65292;&#21033;&#29992;LLMs&#29983;&#25104;&#21508;&#31181;&#27880;&#37322;&#26114;&#36149;&#30340;&#31526;&#21495;&#35821;&#35328;&#25968;&#25454;&#12290;SymGen&#30001;&#20449;&#24687;&#25552;&#31034;&#21644;&#22522;&#20110;&#21327;&#35758;&#30340;&#39564;&#35777;&#22120;&#32452;&#25104;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#23545;&#20845;&#20010;&#31526;&#21495;&#35821;&#35328;&#20219;&#21153;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#19982;LLMs&#30456;&#27604;&#65292;&#25105;&#20204;&#35777;&#26126;1\%&#22823;&#23567;&#30340;&#20219;&#21153;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#25512;&#29702;&#21644;&#37096;&#32626;&#25104;&#26412;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20351;&#29992;SymGen&#29983;&#25104;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#31526;&#21495;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) bring not only performance but also complexity, recent work has started to turn LLMs into data generators rather than task inferencers, where another affordable task model is trained for efficient deployment and inference. However, such an approach has primarily been applied to natural language tasks and has not yet been explored for symbolic language tasks with complex structured outputs (e.g., semantic parsing and code generation). In this paper, we propose SymGen which utilizes LLMs for generating various annotation-expensive symbolic language data. SymGen consists of an informative prompt to steer generation and an agreement-based verifier to improve data correctness. We conduct extensive experiments on six symbolic language tasks across various settings. Compared with the LLMs, we demonstrate the 1\%-sized task model can achieve comparable or better performance, largely cutting inference and deployment costs. We also show that generated data with
&lt;/p&gt;</description></item><item><title>DAPR&#26159;&#19968;&#20010;&#25991;&#26723;&#24863;&#30693;&#27573;&#33853;&#26816;&#32034;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#20174;&#38271;&#25991;&#26723;&#20013;&#25214;&#21040;&#27491;&#30830;&#30340;&#27573;&#33853;&#24182;&#36820;&#22238;&#20934;&#30830;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13915</link><description>&lt;p&gt;
DAPR&#65306;&#25991;&#26723;&#24863;&#30693;&#27573;&#33853;&#26816;&#32034;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
DAPR: A Benchmark on Document-Aware Passage Retrieval. (arXiv:2305.13915v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13915
&lt;/p&gt;
&lt;p&gt;
DAPR&#26159;&#19968;&#20010;&#25991;&#26723;&#24863;&#30693;&#27573;&#33853;&#26816;&#32034;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#20174;&#38271;&#25991;&#26723;&#20013;&#25214;&#21040;&#27491;&#30830;&#30340;&#27573;&#33853;&#24182;&#36820;&#22238;&#20934;&#30830;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#31070;&#32463;&#26816;&#32034;&#20027;&#35201;&#20851;&#27880;&#30701;&#25991;&#26412;&#30340;&#25490;&#21517;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#38271;&#25991;&#26723;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#35780;&#20272;&#25490;&#21517;&#27573;&#33853;&#25110;&#25972;&#20010;&#25991;&#26723;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#24076;&#26395;&#20174;&#24222;&#22823;&#30340;&#35821;&#26009;&#24211;&#20013;&#25214;&#21040;&#38271;&#25991;&#26723;&#20013;&#30340;&#30456;&#20851;&#27573;&#33853;&#65292;&#20363;&#22914;&#27861;&#24459;&#26696;&#20363;&#65292;&#30740;&#31350;&#35770;&#25991;&#31561;&#65292;&#27492;&#26102;&#27573;&#33853;&#24448;&#24448;&#25552;&#20379;&#24456;&#23569;&#30340;&#25991;&#26723;&#19978;&#19979;&#25991;&#65292;&#36825;&#23601;&#25361;&#25112;&#20102;&#24403;&#21069;&#30340;&#26041;&#27861;&#25214;&#21040;&#27491;&#30830;&#30340;&#25991;&#26723;&#24182;&#36820;&#22238;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#21629;&#21517;&#20102;Document-Aware Passage Retrieval&#65288;DAPR&#65289;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#25324;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;DAPR&#21644;&#25972;&#20010;&#25991;&#26723;&#26816;&#32034;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22312;&#25991;&#26723;&#25688;&#35201;&#20013;&#28155;&#21152;&#25991;&#26723;&#32423;&#21035;&#30340;&#20869;&#23481;&#65292;&#27719;&#24635;&#27573;&#33853;&#34920;&#31034;&#21644;&#20351;&#29992;BM25&#36827;&#34892;&#28151;&#21512;&#26816;&#32034;&#65292;&#25193;&#23637;&#20102;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#27573;&#33853;&#26816;&#32034;&#22120;&#12290;&#36825;&#20010;&#28151;&#21512;&#26816;&#32034;&#31995;&#32479;&#65292;&#24635;&#20307;&#22522;&#20934;&#27979;&#35797;&#26174;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;DAPR&#20219;&#21153;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#37325;&#35201;&#24615;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent neural retrieval mainly focuses on ranking short texts and is challenged with long documents. Existing work mainly evaluates either ranking passages or whole documents. However, there are many cases where the users want to find a relevant passage within a long document from a huge corpus, e.g. legal cases, research papers, etc. In this scenario, the passage often provides little document context and thus challenges the current approaches to finding the correct document and returning accurate results. To fill this gap, we propose and name this task Document-Aware Passage Retrieval (DAPR) and build a benchmark including multiple datasets from various domains, covering both DAPR and whole-document retrieval. In experiments, we extend the state-of-the-art neural passage retrievers with document-level context via different approaches including prepending document summary, pooling over passage representations, and hybrid retrieval with BM25. The hybrid-retrieval systems, the overall b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521; VideoCOT&#65292;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#33021;&#21147;&#65292;&#20197;&#22686;&#24378;&#35270;&#39057;&#25512;&#29702;&#65292;&#21516;&#26102;&#20943;&#23569;&#22788;&#29702;&#25968;&#30334;&#25110;&#25968;&#21315;&#24103;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#22312;VIP&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#22522;&#20110;&#21508;&#31181;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;VideoCOT&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.13903</link><description>&lt;p&gt;
&#35753;&#25105;&#20204;&#36880;&#24103;&#24605;&#32771;&#65306;&#20351;&#29992;&#35270;&#39057;&#25554;&#24103;&#21644;&#39044;&#27979;&#35780;&#20272;&#35270;&#39057;&#24605;&#32500;&#38142;
&lt;/p&gt;
&lt;p&gt;
Let's Think Frame by Frame: Evaluating Video Chain of Thought with Video Infilling and Prediction. (arXiv:2305.13903v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13903
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521; VideoCOT&#65292;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#33021;&#21147;&#65292;&#20197;&#22686;&#24378;&#35270;&#39057;&#25512;&#29702;&#65292;&#21516;&#26102;&#20943;&#23569;&#22788;&#29702;&#25968;&#30334;&#25110;&#25968;&#21315;&#24103;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#22312;VIP&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#22522;&#20110;&#21508;&#31181;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;VideoCOT&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;2023&#24180;&#26500;&#25104;&#20102;&#25152;&#26377;&#20114;&#32852;&#32593;&#27969;&#37327;&#30340;65&#65285;&#65292;&#20294;&#35270;&#39057;&#20869;&#23481;&#22312;&#29983;&#25104;AI&#30740;&#31350;&#20013;&#21364;&#34987;&#20302;&#20272;&#20102;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#36234;&#26469;&#36234;&#22810;&#22320;&#19982;&#35270;&#35273;&#27169;&#24577;&#34701;&#21512;&#12290;&#23558;&#35270;&#39057;&#19982;LLM&#25972;&#21512;&#26159;&#19979;&#19968;&#27493;&#33258;&#28982;&#30340;&#21457;&#23637;&#26041;&#21521;&#65292;&#37027;&#20040;&#36825;&#20010;&#40511;&#27807;&#22914;&#20309;&#34987;&#22635;&#34917;&#65311;&#20026;&#20102;&#25512;&#36827;&#35270;&#39057;&#25512;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#21363;&#22522;&#20110;&#35270;&#39057;&#20851;&#38190;&#24103;&#30340;VideoCOT&#65292;&#23427;&#21033;&#29992;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#33021;&#21147;&#65292;&#20197;&#22686;&#24378;&#35270;&#39057;&#25512;&#29702;&#65292;&#21516;&#26102;&#20943;&#23569;&#22788;&#29702;&#25968;&#30334;&#25110;&#25968;&#21315;&#24103;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;VIP&#65292;&#19968;&#31181;&#21487;&#20197;&#29992;&#26469;&#35780;&#20272;VideoCOT&#30340;&#25512;&#26029;&#26102;&#38388;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;1&#65289;&#21508;&#31181;&#24102;&#26377;&#20851;&#38190;&#24103;&#30340;&#30495;&#23454;&#29983;&#27963;&#35270;&#39057;&#20197;&#21450;&#30456;&#24212;&#30340;&#38750;&#32467;&#26500;&#21270;&#21644;&#32467;&#26500;&#21270;&#22330;&#26223;&#25551;&#36848;&#65292;2&#65289;&#20004;&#20010;&#26032;&#30340;&#35270;&#39057;&#25512;&#29702;&#20219;&#21153;&#65306;&#35270;&#39057;&#25554;&#24103;&#21644;&#22330;&#26223;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;VIP&#19978;&#23545;&#21508;&#31181;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;VideoCOT&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite constituting 65% of all internet traffic in 2023, video content is underrepresented in generative AI research. Meanwhile, recent large language models (LLMs) have become increasingly integrated with capabilities in the visual modality. Integrating video with LLMs is a natural next step, so how can this gap be bridged? To advance video reasoning, we propose a new research direction of VideoCOT on video keyframes, which leverages the multimodal generative abilities of vision-language models to enhance video reasoning while reducing the computational complexity of processing hundreds or thousands of frames. We introduce VIP, an inference-time dataset that can be used to evaluate VideoCOT, containing 1) a variety of real-life videos with keyframes and corresponding unstructured and structured scene descriptions, and 2) two new video reasoning tasks: video infilling and scene prediction. We benchmark various vision-language models on VIP, demonstrating the potential to use vision-la
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#21475;&#35821;&#35821;&#35328;&#29702;&#35299;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22686;&#37327;&#31867;&#21035;&#22330;&#26223;&#21644;&#19977;&#31181;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#26174;&#31034;&#24207;&#21015;&#32423;&#30693;&#35782;&#33976;&#39311;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#32489;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.13899</link><description>&lt;p&gt;
&#38024;&#23545;&#22686;&#37327;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#29702;&#35299;&#24207;&#21015;&#32423;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Sequence-Level Knowledge Distillation for Class-Incremental End-to-End Spoken Language Understanding. (arXiv:2305.13899v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#21475;&#35821;&#35821;&#35328;&#29702;&#35299;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22686;&#37327;&#31867;&#21035;&#22330;&#26223;&#21644;&#19977;&#31181;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#26174;&#31034;&#24207;&#21015;&#32423;&#30693;&#35782;&#33976;&#39311;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#32489;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#22312;&#36880;&#27493;&#23398;&#20064;&#26032;&#27010;&#24565;&#26041;&#38754;&#30340;&#33021;&#21147;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#24369;&#28857;&#65292;&#36825;&#22952;&#30861;&#20102;&#23427;&#20204;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#30340;&#20351;&#29992;&#12290;&#23427;&#20204;&#20542;&#21521;&#20110;&#23558;&#24403;&#21069;&#25968;&#25454;&#20998;&#24067;&#25311;&#21512;&#24471;&#36234;&#26469;&#36234;&#22909;&#65292;&#32780;&#24573;&#30053;&#20102;&#36807;&#21435;&#25152;&#33719;&#21462;&#30340;&#30693;&#35782;&#65292;&#23548;&#33268;&#20102;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#24212;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#24773;&#22659;&#30340;&#21475;&#35821;&#35821;&#35328;&#29702;&#35299;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20026;SLURP&#25968;&#25454;&#38598;&#23450;&#20041;&#20102;&#19968;&#20010;&#22686;&#37327;&#31867;&#21035;&#22330;&#26223;&#65292;&#24182;&#38024;&#23545;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;Transformer&#27169;&#22411;&#25552;&#20986;&#20102;&#19977;&#31181;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26041;&#27861;&#20197;&#20943;&#36731;&#36951;&#24536;&#65306;&#31532;&#19968;&#31181;KD&#26041;&#27861;&#24212;&#29992;&#20110;&#32534;&#30721;&#22120;&#36755;&#20986;&#65288;audio-KD&#65289;&#65292;&#20854;&#20313;&#20004;&#31181;&#26041;&#27861;&#21017;&#20998;&#21035;&#22312;&#35299;&#30721;&#22120;&#36755;&#20986;&#30340;&#26631;&#35760;&#32423;&#65288;tok-KD&#65289;&#25110;&#24207;&#21015;&#32423;&#65288;seq-KD&#65289;&#20998;&#24067;&#19978;&#36827;&#34892;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;seq-KD&#26174;&#33879;&#22320;&#25913;&#21892;&#20102;&#25152;&#26377;&#32489;&#25928;&#25351;&#26631;&#65292;&#23558;&#23427;&#19982;audio-KD&#30456;&#32467;&#21512;&#36827;&#19968;&#27493;&#38477;&#20302;&#20102;&#24179;&#22343;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#24182;&#25552;&#39640;&#20102;&#23454;&#20307;&#39044;&#27979;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to learn new concepts sequentially is a major weakness for modern neural networks, which hinders their use in non-stationary environments. Their propensity to fit the current data distribution to the detriment of the past acquired knowledge leads to the catastrophic forgetting issue. In this work we tackle the problem of Spoken Language Understanding applied to a continual learning setting. We first define a class-incremental scenario for the SLURP dataset. Then, we propose three knowledge distillation (KD) approaches to mitigate forgetting for a sequence-to-sequence transformer model: the first KD method is applied to the encoder output (audio-KD), and the other two work on the decoder output, either directly on the token-level (tok-KD) or on the sequence-level (seq-KD) distributions. We show that the seq-KD substantially improves all the performance metrics, and its combination with the audio-KD further decreases the average WER and enhances the entity prediction metric.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31243;&#24207;&#36741;&#21161;&#33976;&#39311;&#65288;PaD&#65289;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#33976;&#39311;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#33719;&#24471;&#19987;&#19994;&#21270;&#30340;&#23567;&#27169;&#22411;&#12290;PaD&#20351;&#29992;&#31243;&#24207;&#36741;&#21161;&#25512;&#29702;&#21152;&#24378;&#19987;&#19994;&#21270;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#21270;&#38169;&#35823;&#26816;&#26597;&#26469;&#24110;&#21161;&#23427;&#20204;&#20811;&#26381;&#38169;&#35823;&#30340;&#25512;&#29702;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2305.13888</link><description>&lt;p&gt;
PaD: &#31243;&#24207;&#36741;&#21161;&#33976;&#39311;&#19987;&#27880;&#20110;&#25512;&#29702;&#30340;&#22823;&#22411;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PaD: Program-aided Distillation Specializes Large Models in Reasoning. (arXiv:2305.13888v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31243;&#24207;&#36741;&#21161;&#33976;&#39311;&#65288;PaD&#65289;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#33976;&#39311;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#33719;&#24471;&#19987;&#19994;&#21270;&#30340;&#23567;&#27169;&#22411;&#12290;PaD&#20351;&#29992;&#31243;&#24207;&#36741;&#21161;&#25512;&#29702;&#21152;&#24378;&#19987;&#19994;&#21270;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#21270;&#38169;&#35823;&#26816;&#26597;&#26469;&#24110;&#21161;&#23427;&#20204;&#20811;&#26381;&#38169;&#35823;&#30340;&#25512;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20960;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#23427;&#20204;&#30340;&#22823;&#23567;&#21644;&#19981;&#21487;&#35775;&#38382;&#24615;&#23545;&#20110;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#23545;LLMs&#36827;&#34892;&#31934;&#28860;&#20197;&#33719;&#21462;&#19987;&#19994;&#25216;&#33021;&#65292;&#22312;&#21830;&#19994;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#36890;&#29992;&#33021;&#21147;&#30340;&#20132;&#25442;&#65292;&#31216;&#20026;&#27169;&#22411;&#19987;&#19994;&#21270;&#12290;&#23545;&#20110;&#25512;&#29702;&#33021;&#21147;&#65292;&#20844;&#21496;&#24050;&#21512;&#25104;&#29992;&#20110;&#21518;&#32493;&#25552;&#28860;&#30340;&#24605;&#32500;&#38142;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#24187;&#35273;&#65292;LLMs&#30340;&#21512;&#25104;&#24605;&#32500;&#38142;&#21253;&#21547;&#38169;&#35823;&#25512;&#29702;&#65292;&#36825;&#20123;&#19981;&#27491;&#30830;&#30340;&#25512;&#29702;&#27493;&#39588;&#25439;&#23475;&#20102;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31243;&#24207;&#36741;&#21161;&#33976;&#39311;&#65288;PaD&#65289;&#65292;&#23427;&#21487;&#20197;&#33976;&#39311;LLMs&#20197;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#33719;&#24471;&#19987;&#19994;&#21270;&#30340;&#23567;&#27169;&#22411;&#12290;&#22312;PaD&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#31243;&#24207;&#36741;&#21161;&#25512;&#29702;&#21152;&#24378;&#19987;&#19994;&#21270;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#21270;&#38169;&#35823;&#26816;&#26597;&#26469;&#24110;&#21161;&#23427;&#20204;&#20811;&#26381;&#38169;&#35823;&#30340;&#25512;&#29702;&#27493;&#39588;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;GSM8K&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#20351;&#29992;PaD&#30340;0.06B&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#32988;&#36807;&#26576;&#20123;LLMs&#65288;&#20363;&#22914;LLaMA&#65289;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#21462;&#24471;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) excel in several natural language processing tasks, their size and inaccessibility present challenges for extensive practical application. Previous studies acquire specialized skills through distillation on LLMs, which result in trading generic abilities, called model specialization. As for reasoning ability, chain-of-thought was synthesized to subsequent distillation. However, due to hallucination, synthetic chain-of-thought from LLMs contains faulty reasoning. These incorrect reasoning steps damage the reasoning capability. To tackle above issues, we propose Program-aided Distillation (PaD), which distills LLMs to obtain specialized small models in reasoning tasks. In PaD, we strengthen specialized models with program-aided reasoning, and help them overcome faulty reasoning steps with automated error checking. Experimental results demonstrate that, on the GSM8K benchmark, a 0.06B model using PaD can not only outperform certain LLMs (e.g., LLaMA), bu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#38271;&#26399;&#35760;&#24518;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#33258;&#28982;&#25968;&#25454;&#38598;&#65292;&#20197;&#24110;&#21161;&#25913;&#36827;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25968;&#25454;&#38598;&#30001; GPT 3.5 &#29983;&#25104;&#65292;&#25688;&#35201;&#21253;&#25324;&#26469;&#33258; Project Gutenberg &#30340; 1500 &#26412;&#20070;&#20013;&#27599;&#20010;&#22330;&#26223;&#30340;&#24635;&#32467;&#65292;&#20197;&#21450;&#37197;&#22871;&#30340;&#38405;&#35835;&#29702;&#35299;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13877</link><description>&lt;p&gt;
Narrative XL: &#19968;&#20010;&#29992;&#20110;&#38271;&#26399;&#35760;&#24518;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Narrative XL: A Large-scale Dataset For Long-Term Memory Models. (arXiv:2305.13877v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#38271;&#26399;&#35760;&#24518;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#33258;&#28982;&#25968;&#25454;&#38598;&#65292;&#20197;&#24110;&#21161;&#25913;&#36827;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25968;&#25454;&#38598;&#30001; GPT 3.5 &#29983;&#25104;&#65292;&#25688;&#35201;&#21253;&#25324;&#26469;&#33258; Project Gutenberg &#30340; 1500 &#26412;&#20070;&#20013;&#27599;&#20010;&#22330;&#26223;&#30340;&#24635;&#32467;&#65292;&#20197;&#21450;&#37197;&#22871;&#30340;&#38405;&#35835;&#29702;&#35299;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22810;&#25968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#20219;&#20309;&#38271;&#26399;&#35760;&#24518;&#26426;&#21046;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;&#35201;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#19981;&#20165;&#38656;&#35201;&#23545;&#20856;&#22411;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#25110;&#35757;&#32451;&#31243;&#24207;&#36827;&#34892;&#26356;&#25913;&#65292;&#36824;&#38656;&#35201;&#19968;&#20010;&#21487;&#20197;&#35757;&#32451;&#21644;&#35780;&#20272;&#36825;&#20123;&#26032;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;&#36164;&#28304;&#32570;&#23569;&#19968;&#20123;&#20851;&#38190;&#23646;&#24615;&#65292;&#30446;&#21069;&#27809;&#26377;&#36275;&#22815;&#35268;&#27169;&#30340;&#33258;&#28982;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#65288;&#32780;&#19981;&#20165;&#20165;&#26159;&#35780;&#20272;&#65289;&#38271;&#26399;&#35760;&#24518;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#30701;&#26399;&#35760;&#24518;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#26469;&#21019;&#24314;&#36825;&#26679;&#19968;&#20010;&#25968;&#25454;&#38598;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20351;&#29992; GPT 3.5&#65292;&#25105;&#20204;&#24635;&#32467;&#20102; Project Gutenberg &#20013; 1500 &#26412;&#25163;&#24037;&#31579;&#36873;&#30340;&#20070;&#31821;&#20013;&#30340;&#27599;&#20010;&#22330;&#26223;&#65292;&#27599;&#26412;&#20070;&#24471;&#21040;&#22823;&#32422; 150 &#20010;&#22330;&#26223;&#32423;&#21035;&#30340;&#25688;&#35201;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20123;&#38405;&#35835;&#29702;&#35299;&#38382;&#39064;&#65292;&#21253;&#25324;&#19977;&#31181;&#31867;&#22411;&#30340;&#22810;&#39033;&#36873;&#25321;&#22330;&#26223;&#35782;&#21035;&#38382;&#39064;&#65292;&#20197;&#21450;...
&lt;/p&gt;
&lt;p&gt;
Despite their tremendous successes, most large language models do not have any long-term memory mechanisms, which restricts their applications. Overcoming this limitation would not only require changes to the typical transformer architectures or training procedures, but also a dataset on which these new models could be trained and evaluated. We argue that existing resources lack a few key properties, and that at present, there are no naturalistic datasets of sufficient scale to train (and not only evaluate) long-term memory language models. We then present our solution that capitalizes on the advances in short-term memory language models to create such a dataset. Using GPT 3.5, we summarized each scene in 1500 hand-curated books from Project Gutenberg, which resulted in approximately 150 scene-level summaries per book. We then created a number of reading comprehension questions based on these summaries, including three types of multiple-choice scene recognition questions, as well as fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;Masked-Attention&#29983;&#25104;&#65292;&#20351;&#29992;GPT-2&#21464;&#24418;&#22120;&#29983;&#25104;&#30340;&#22266;&#23450;&#37327;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#35789;&#23884;&#20837;&#21487;&#20197;&#39044;&#27979;&#20154;&#31867;&#21548;&#33258;&#28982;&#35821;&#35328;&#26102;&#30340;fMRI&#33041;&#27963;&#21160;&#65292;&#32467;&#26524;&#34920;&#26126;&#35821;&#35328;&#32593;&#32476;&#30340;&#22823;&#22810;&#25968;&#30382;&#23618;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#25935;&#24863;&#65292;&#21491;&#21322;&#29699;&#23545;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#26356;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2305.13863</link><description>&lt;p&gt;
&#25506;&#31350;&#33041;&#21306;&#19978;&#19979;&#25991;&#25935;&#24863;&#24615;&#65306;&#22522;&#20110;Masked-Attention&#29983;&#25104;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Probing Brain Context-Sensitivity with Masked-Attention Generation. (arXiv:2305.13863v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;Masked-Attention&#29983;&#25104;&#65292;&#20351;&#29992;GPT-2&#21464;&#24418;&#22120;&#29983;&#25104;&#30340;&#22266;&#23450;&#37327;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#35789;&#23884;&#20837;&#21487;&#20197;&#39044;&#27979;&#20154;&#31867;&#21548;&#33258;&#28982;&#35821;&#35328;&#26102;&#30340;fMRI&#33041;&#27963;&#21160;&#65292;&#32467;&#26524;&#34920;&#26126;&#35821;&#35328;&#32593;&#32476;&#30340;&#22823;&#22810;&#25968;&#30382;&#23618;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#25935;&#24863;&#65292;&#21491;&#21322;&#29699;&#23545;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#26356;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#23398;&#20013;&#30340;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#36229;&#36234;&#35789;&#27719;&#23618;&#27425;&#25972;&#21512;&#20449;&#24687;&#30340;&#33041;&#21306;&#20197;&#21450;&#23427;&#20204;&#30340;&#25972;&#21512;&#31383;&#21475;&#22823;&#23567;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Masked-Attention&#29983;&#25104;&#30340;&#26032;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;GPT-2&#21464;&#24418;&#22120;&#29983;&#25104;&#25429;&#33719;&#22266;&#23450;&#37327;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#35789;&#23884;&#20837;&#12290;&#28982;&#21518;&#25105;&#20204;&#27979;&#35797;&#36825;&#20123;&#23884;&#20837;&#33021;&#21542;&#39044;&#27979;&#20154;&#31867;&#21548;&#33258;&#28982;&#25991;&#26412;&#26102;&#30340;fMRI&#33041;&#27963;&#21160;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35821;&#35328;&#32593;&#32476;&#20013;&#30340;&#22823;&#22810;&#25968;&#30382;&#23618;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#25935;&#24863;&#65292;&#21491;&#21322;&#29699;&#27604;&#24038;&#21322;&#29699;&#26356;&#21152;&#25935;&#24863;&#20110;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#12290;Masked-Attention&#29983;&#25104;&#25903;&#25345;&#20043;&#21069;&#22312;&#22823;&#33041;&#19978;&#19979;&#25991;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#24182;&#36890;&#36807;&#37327;&#21270;&#27599;&#20010;&#20307;&#32032;&#30340;&#19978;&#19979;&#25991;&#25972;&#21512;&#31383;&#21475;&#22823;&#23567;&#26469;&#34917;&#20805;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two fundamental questions in neurolinguistics concerns the brain regions that integrate information beyond the lexical level, and the size of their window of integration. To address these questions we introduce a new approach named masked-attention generation. It uses GPT-2 transformers to generate word embeddings that capture a fixed amount of contextual information. We then tested whether these embeddings could predict fMRI brain activity in humans listening to naturalistic text. The results showed that most of the cortex within the language network is sensitive to contextual information, and that the right hemisphere is more sensitive to longer contexts than the left. Masked-attention generation supports previous analyses of context-sensitivity in the brain, and complements them by quantifying the window size of context integration per voxel.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20559;&#24046;&#25216;&#26415;&#20197;&#20135;&#29983;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#20581;&#22766;&#21435;&#20559;&#24046;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13862</link><description>&lt;p&gt;
&#20844;&#24179;&#20043;&#36335;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#21450;&#21435;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
A Trip Towards Fairness: Bias and De-Biasing in Large Language Models. (arXiv:2305.13862v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20559;&#24046;&#25216;&#26415;&#20197;&#20135;&#29983;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#20581;&#22766;&#21435;&#20559;&#24046;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT&#65288;Brown&#31561;&#65292;2020&#65289;&#21644;PaLM&#65288;Chowdhery&#31561;&#65292;2022&#65289;&#65289;&#30340;&#26222;&#21450;&#24341;&#21457;&#20102;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#20174;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#23545;&#20110;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#30528;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#20284;&#20046;&#20855;&#26377;&#23545;&#26576;&#20123;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#20559;&#35265;&#30340;&#22266;&#26377;&#20559;&#24046;&#12290;&#23613;&#31649;&#30740;&#31350;&#35797;&#22270;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#26410;&#33021;&#23436;&#20840;&#28040;&#38500;&#20559;&#35265;&#65292;&#35201;&#20040;&#38477;&#20302;&#20102;&#24615;&#33021;&#65292;&#35201;&#20040;&#20195;&#20215;&#36807;&#39640;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#19981;&#21516;&#21442;&#25968;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;&#36825;&#20123;&#26377;&#21069;&#36884;&#30340;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#30340;&#20559;&#35265;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20559;&#24046;&#25216;&#26415;&#65292;&#21487;&#20197;&#20135;&#29983;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20445;&#25345;&#24615;&#33021;&#30340;&#20581;&#22766;&#30340;&#21435;&#20559;&#24046;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
An outbreak in the popularity of transformer-based Language Models (such as GPT (Brown et al., 2020) and PaLM (Chowdhery et al., 2022)) has opened the doors to new Machine Learning applications. In particular, in Natural Language Processing and how pre-training from large text, corpora is essential in achieving remarkable results in downstream tasks. However, these Language Models seem to have inherent biases toward certain demographics reflected in their training data. While research has attempted to mitigate this problem, existing methods either fail to remove bias altogether, degrade performance, or are expensive. This paper examines the bias produced by promising Language Models when varying parameters and pre-training data. Finally, we propose a de-biasing technique that produces robust de-bias models that maintain performance on downstream tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#30772;&#35299;ChatGPT&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#30772;&#35299;&#25552;&#31034;&#21487;&#20197;&#22312;40&#31181;&#29992;&#20363;&#24773;&#20917;&#19979;&#19968;&#33268;&#22320;&#35268;&#36991;&#38480;&#21046;&#65292;&#24378;&#35843;&#20102;&#25552;&#31034;&#32467;&#26500;&#22312;&#30772;&#35299;ChatGPT&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13860</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#30772;&#35299;ChatGPT&#65306;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study. (arXiv:2305.13860v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#30772;&#35299;ChatGPT&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#30772;&#35299;&#25552;&#31034;&#21487;&#20197;&#22312;40&#31181;&#29992;&#20363;&#24773;&#20917;&#19979;&#19968;&#33268;&#22320;&#35268;&#36991;&#38480;&#21046;&#65292;&#24378;&#35843;&#20102;&#25552;&#31034;&#32467;&#26500;&#22312;&#30772;&#35299;ChatGPT&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#24050;&#32463;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#21516;&#26102;&#20063;&#24341;&#21457;&#20102;&#19982;&#20869;&#23481;&#32422;&#26463;&#21644;&#28508;&#22312;&#28389;&#29992;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#31350;&#20102;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#21487;&#20197;&#29992;&#22810;&#23569;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#31867;&#22411;&#30772;&#35299;LLMs&#65292;&#65288;2&#65289;&#30772;&#35299;&#25552;&#31034;&#22312;&#35268;&#36991;LLM&#38480;&#21046;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#65288;3&#65289;ChatGPT&#23545;&#36825;&#20123;&#30772;&#35299;&#25552;&#31034;&#30340;&#38887;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#31867;&#27169;&#22411;&#26469;&#20998;&#26512;&#29616;&#26377;&#25552;&#31034;&#30340;&#20998;&#24067;&#65292;&#35782;&#21035;&#20986;&#21313;&#20010;&#19981;&#21516;&#27169;&#24335;&#21644;&#19977;&#20010;&#30772;&#35299;&#25552;&#31034;&#31867;&#21035;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;3,120&#20010;&#31105;&#27490;&#24773;&#26223;&#19979;&#30340;&#29425;&#20013;&#38382;&#39064;&#25968;&#25454;&#38598;&#35780;&#20272;ChatGPT 3.5&#21644;4.0&#29256;&#26412;&#30340;&#30772;&#35299;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#23545;&#30772;&#35299;&#25552;&#31034;&#30340;&#25269;&#25239;&#21147;&#65292;&#21457;&#29616;&#25552;&#31034;&#21487;&#20197;&#22312;40&#31181;&#29992;&#20363;&#24773;&#26223;&#19979;&#19968;&#33268;&#22320;&#35268;&#36991;&#38480;&#21046;&#12290;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#25552;&#31034;&#32467;&#26500;&#22312;&#30772;&#35299;ChatGPT&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;LLMs&#23545;&#24847;&#22806;&#28389;&#29992;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential but also introduce challenges related to content constraints and potential misuse. Our study investigates three key research questions: (1) the number of different prompt types that can jailbreak LLMs, (2) the effectiveness of jailbreak prompts in circumventing LLM constraints, and (3) the resilience of ChatGPT against these jailbreak prompts. Initially, we develop a classification model to analyze the distribution of existing prompts, identifying ten distinct patterns and three categories of jailbreak prompts. Subsequently, we assess the jailbreak capability of prompts with ChatGPT versions 3.5 and 4.0, utilizing a dataset of 3,120 jailbreak questions across eight prohibited scenarios. Finally, we evaluate the resistance of ChatGPT against jailbreak prompts, finding that the prompts can consistently evade the restrictions in 40 use-case scenarios. The study underscores the importance of prompt structures in j
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#23384;&#22312;&#29992;&#25143;&#29087;&#24713;&#24230;&#20559;&#35265;&#65292;&#32780;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#22330;&#26223;&#24456;&#23569;&#31526;&#21512;&#23553;&#38381;&#30446;&#26631;&#30340;&#35774;&#23450;&#12290;&#22240;&#27492;&#65292;&#22312;&#24320;&#25918;&#30446;&#26631;&#35774;&#32622;&#19979;&#65292;&#31995;&#32479;&#20250;&#20986;&#29616;&#20005;&#37325;&#38382;&#39064;&#65292;&#21516;&#26102;&#30740;&#31350;&#32773;&#21457;&#29616;&#20102;&#8220;&#19981;&#21305;&#37197;&#38169;&#35823;&#8221;&#36825;&#19968;&#26032;&#22411;&#38169;&#35823;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13857</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#20114;&#24335;&#35780;&#20272;&#25581;&#31034;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#30340;&#29992;&#25143;&#29087;&#24713;&#24230;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Revealing User Familiarity Bias in Task-Oriented Dialogue via Interactive Evaluation. (arXiv:2305.13857v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#23384;&#22312;&#29992;&#25143;&#29087;&#24713;&#24230;&#20559;&#35265;&#65292;&#32780;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#22330;&#26223;&#24456;&#23569;&#31526;&#21512;&#23553;&#38381;&#30446;&#26631;&#30340;&#35774;&#23450;&#12290;&#22240;&#27492;&#65292;&#22312;&#24320;&#25918;&#30446;&#26631;&#35774;&#32622;&#19979;&#65292;&#31995;&#32479;&#20250;&#20986;&#29616;&#20005;&#37325;&#38382;&#39064;&#65292;&#21516;&#26102;&#30740;&#31350;&#32773;&#21457;&#29616;&#20102;&#8220;&#19981;&#21305;&#37197;&#38169;&#35823;&#8221;&#36825;&#19968;&#26032;&#22411;&#38169;&#35823;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;(TOD)&#22522;&#20934;&#20551;&#23450;&#29992;&#25143;&#31934;&#30830;&#22320;&#30693;&#36947;&#22914;&#20309;&#20351;&#29992;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#29992;&#25143;&#34892;&#20026;&#38480;&#21046;&#22312;&#31995;&#32479;&#30340;&#33021;&#21147;&#33539;&#22260;&#20869;&#65292;&#21363;&#8220;&#29992;&#25143;&#29087;&#24713;&#24230;&#8221;&#20559;&#35265;&#12290;&#24403;&#36825;&#31181;&#25968;&#25454;&#20559;&#35265;&#19982;&#25968;&#25454;&#39537;&#21160;&#30340;TOD&#31995;&#32479;&#30456;&#32467;&#21512;&#26102;&#65292;&#35813;&#20559;&#35265;&#21152;&#28145;&#20102;&#65292;&#22240;&#20026;&#20351;&#29992;&#29616;&#26377;&#30340;&#38745;&#24577;&#35780;&#20272;&#26080;&#27861;&#29702;&#35299;&#20854;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20132;&#20114;&#24335;&#29992;&#25143;&#30740;&#31350;&#65292;&#25581;&#31034;TOD&#31995;&#32479;&#22312;&#30495;&#23454;&#22330;&#26223;&#19979;&#30340;&#33030;&#24369;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20855;&#26377;1&#65289;&#31526;&#21512;&#31995;&#32479;&#36793;&#30028;&#30340;&#35814;&#32454;&#30446;&#26631;&#35828;&#26126;&#65288;&#23553;&#38381;&#30446;&#26631;&#65289;&#21644;2&#65289;&#36890;&#24120;&#19981;&#21463;&#25903;&#25345;&#20294;&#29616;&#23454;&#65288;&#24320;&#25918;&#30446;&#26631;&#65289;&#30340;&#27169;&#31946;&#30446;&#26631;&#35828;&#26126;&#30340;&#29992;&#25143;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#24320;&#25918;&#30446;&#26631;&#35774;&#32622;&#19979;&#30340;&#23545;&#35805;&#20250;&#23548;&#33268;&#31995;&#32479;&#20005;&#37325;&#22833;&#36133;&#65292;92%&#30340;&#23545;&#35805;&#23384;&#22312;&#26174;&#33879;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#20998;&#26512;&#65292;&#36890;&#36807;&#38169;&#35823;&#27880;&#37322;&#35782;&#21035;&#20004;&#31181;&#35774;&#32622;&#20043;&#38388;&#30340;&#26174;&#33879;&#29305;&#24449;&#12290;&#20174;&#20013;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#38169;&#35823;&#31867;&#22411;&#31216;&#20026;&#8220;&#19981;&#21305;&#37197;&#38169;&#35823;&#8221;&#65292;&#36825;&#34920;&#26126;&#29992;&#25143;&#21644;&#31995;&#32479;&#26080;&#27861;&#24314;&#31435;&#20849;&#20139;&#30340;&#35821;&#22659;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;TOD&#35780;&#20272;&#20013;&#32771;&#34385;&#29992;&#25143;&#29087;&#24713;&#24230;&#20559;&#35265;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;&#31995;&#32479;&#26469;&#22788;&#29702;&#23454;&#38469;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most task-oriented dialogue (TOD) benchmarks assume users that know exactly how to use the system by constraining the user behaviors within the system's capabilities via strict user goals, namely "user familiarity" bias. This data bias deepens when it combines with data-driven TOD systems, as it is impossible to fathom the effect of it with existing static evaluations. Hence, we conduct an interactive user study to unveil how vulnerable TOD systems are against realistic scenarios. In particular, we compare users with 1) detailed goal instructions that conform to the system boundaries (closed-goal) and 2) vague goal instructions that are often unsupported but realistic (open-goal). Our study reveals that conversations in open-goal settings lead to catastrophic failures of the system, in which 92% of the dialogues had significant issues. Moreover, we conduct a thorough analysis to identify distinctive features between the two settings through error annotation. From this, we discover a no
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20840;&#23616;&#32467;&#26500;&#30693;&#35782;&#30340;&#36830;&#32493;&#36845;&#20195;&#30340;&#26041;&#24335;&#21435;&#25429;&#33719;&#23454;&#20307;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#20851;&#31995;&#25277;&#21462;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13850</link><description>&lt;p&gt;
&#32467;&#21512;&#20840;&#23616;&#32467;&#26500;&#30693;&#35782;&#30340;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Global Structure Knowledge-Guided Relation Extraction Method for Visually-Rich Document. (arXiv:2305.13850v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13850
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20840;&#23616;&#32467;&#26500;&#30693;&#35782;&#30340;&#36830;&#32493;&#36845;&#20195;&#30340;&#26041;&#24335;&#21435;&#25429;&#33719;&#23454;&#20307;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#20851;&#31995;&#25277;&#21462;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#20851;&#31995;&#25552;&#21462;&#65288;VRE&#65289;&#26088;&#22312;&#20174;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#20013;&#25552;&#21462;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#22522;&#20110;&#23454;&#20307;&#29305;&#24449;&#21333;&#29420;&#39044;&#27979;&#27599;&#23545;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20294;&#24573;&#30053;&#20102;&#20840;&#23616;&#32467;&#26500;&#20449;&#24687;&#65292;&#21363;&#23454;&#20307;&#23545;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#32570;&#20047;&#20840;&#23616;&#32467;&#26500;&#20449;&#24687;&#21487;&#33021;&#20351;&#27169;&#22411;&#38590;&#20197;&#23398;&#20064;&#38271;&#31243;&#20851;&#31995;&#65292;&#24182;&#23481;&#26131;&#20135;&#29983;&#20914;&#31361;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;GOSE&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20197;&#36845;&#20195;&#30340;&#26041;&#24335;&#25429;&#33719;&#23454;&#20307;&#23545;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#32473;&#23450;&#25991;&#26723;&#30340;&#25195;&#25551;&#22270;&#20687;&#65292;GOSE&#39318;&#20808;&#23545;&#23454;&#20307;&#23545;&#29983;&#25104;&#21021;&#27493;&#30340;&#20851;&#31995;&#39044;&#27979;&#12290;&#31532;&#20108;&#65292;&#22312;&#20808;&#21069;&#36845;&#20195;&#30340;&#39044;&#27979;&#32467;&#26524;&#22522;&#30784;&#19978;&#65292;GOSE&#21033;&#29992;&#20840;&#23616;&#32467;&#26500;&#30693;&#35782;&#36827;&#19968;&#27493;&#25972;&#21512;&#23454;&#20307;&#34920;&#31034;&#12290;&#36825;&#31181;&#8220;&#29983;&#25104;-&#25429;&#33719;-&#25972;&#21512;&#8221;&#27169;&#24335;&#34987;&#22810;&#27425;&#25191;&#34892;&#65292;&#20197;&#20415;&#23454;&#20307;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#33021;&#22815;&#34987;&#24456;&#22909;&#22320;&#25429;&#33719;&#21644;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual relation extraction (VRE) aims to extract relations between entities from visuallyrich documents. Existing methods usually predict relations for each entity pair independently based on entity features but ignore the global structure information, i.e., dependencies between entity pairs. The absence of global structure information may make the model struggle to learn long-range relations and easily predict conflicted results. To alleviate such limitations, we propose a GlObal Structure knowledgeguided relation Extraction (GOSE) framework, which captures dependencies between entity pairs in an iterative manner. Given a scanned image of the document, GOSE firstly generates preliminary relation predictions on entity pairs. Secondly, it mines global structure knowledge based on prediction results of the previous iteration and further incorporates global structure knowledge into entity representations. This "generate-capture-incorporate" schema is performed multiple times so that entit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26085;&#26412;&#26053;&#34892;&#28216;&#35760;&#25968;&#25454;&#38598;&#65292;&#24378;&#35843;&#25991;&#26723;&#32423;&#22320;&#29702;&#23454;&#20307;&#35299;&#26512;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20379;&#20016;&#23500;&#30340;&#22320;&#29702;&#23454;&#20307;&#20449;&#24687;&#65292;&#24182;&#20026;&#22320;&#29702;&#35299;&#26512;&#31995;&#32479;&#35780;&#20272;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2305.13844</link><description>&lt;p&gt;
&#24102;&#26377;&#22320;&#29702;&#23454;&#20307;&#25552;&#21450;&#12289;&#25351;&#20195;&#21644;&#38142;&#25509;&#26631;&#27880;&#30340;Arukikata&#28216;&#35760;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Arukikata Travelogue Dataset with Geographic Entity Mention, Coreference, and Link Annotation. (arXiv:2305.13844v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13844
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26085;&#26412;&#26053;&#34892;&#28216;&#35760;&#25968;&#25454;&#38598;&#65292;&#24378;&#35843;&#25991;&#26723;&#32423;&#22320;&#29702;&#23454;&#20307;&#35299;&#26512;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20379;&#20016;&#23500;&#30340;&#22320;&#29702;&#23454;&#20307;&#20449;&#24687;&#65292;&#24182;&#20026;&#22320;&#29702;&#35299;&#26512;&#31995;&#32479;&#35780;&#20272;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29702;&#35299;&#26512;&#26159;&#20998;&#26512;&#25991;&#26412;&#20013;&#22320;&#29702;&#23454;&#20307;&#20449;&#24687;&#30340;&#22522;&#26412;&#25216;&#26415;&#12290;&#25105;&#20204;&#20851;&#27880;&#20110;&#25991;&#26723;&#32423;&#22320;&#29702;&#35299;&#26512;&#65292;&#32771;&#34385;&#22320;&#29702;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#25991;&#26723;&#32423;&#22320;&#29702;&#35299;&#26512;&#31995;&#32479;&#30340;&#26085;&#26412;&#26053;&#34892;&#28216;&#35760;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;200&#31687;&#28216;&#35760;&#25991;&#26723;&#65292;&#20855;&#26377;&#20016;&#23500;&#30340;&#22320;&#29702;&#23454;&#20307;&#20449;&#24687;&#65306;12,171&#20010;&#25552;&#21450;&#12289;6,339&#20010;&#25351;&#20195;&#31751;&#21644;2,551&#20010;&#38142;&#25509;&#21040;&#22320;&#29702;&#25968;&#25454;&#24211;&#26465;&#30446;&#30340;&#22320;&#29702;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geoparsing is a fundamental technique for analyzing geo-entity information in text. We focus on document-level geoparsing, which considers geographic relatedness among geo-entity mentions, and presents a Japanese travelogue dataset designed for evaluating document-level geoparsing systems. Our dataset comprises 200 travelogue documents with rich geo-entity information: 12,171 mentions, 6,339 coreference clusters, and 2,551 geo-entities linked to geo-database entries.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22312;&#23545;&#35805;&#29983;&#25104;&#25991;&#26412;&#20013;&#38477;&#20302;&#35828;&#35805;&#32773;&#21517;&#31216;&#25935;&#24863;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#37327;&#27979;&#37327;&#27169;&#22411;&#25935;&#24863;&#24230;&#24182;&#20840;&#38754;&#35780;&#20272;&#24050;&#30693;&#26041;&#27861;&#65292;&#24471;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#30340;&#33391;&#22909;&#34920;&#29616;&#65292;&#20026;&#27492;&#38382;&#39064;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2305.13833</link><description>&lt;p&gt;
&#20174;&#23545;&#35805;&#29983;&#25104;&#25991;&#26412;&#20013;&#38477;&#20302;&#35828;&#35805;&#32773;&#21517;&#31216;&#25935;&#24863;&#24230;
&lt;/p&gt;
&lt;p&gt;
Reducing Sensitivity on Speaker Names for Text Generation from Dialogues. (arXiv:2305.13833v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22312;&#23545;&#35805;&#29983;&#25104;&#25991;&#26412;&#20013;&#38477;&#20302;&#35828;&#35805;&#32773;&#21517;&#31216;&#25935;&#24863;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#37327;&#27979;&#37327;&#27169;&#22411;&#25935;&#24863;&#24230;&#24182;&#20840;&#38754;&#35780;&#20272;&#24050;&#30693;&#26041;&#27861;&#65292;&#24471;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#30340;&#33391;&#22909;&#34920;&#29616;&#65292;&#20026;&#27492;&#38382;&#39064;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#35805;&#20013;&#22987;&#32456;&#20445;&#25345;&#35828;&#35805;&#32773;&#21517;&#31216;&#30340;&#19968;&#33268;&#24615;&#19981;&#24212;&#35813;&#24433;&#21709;&#21040;&#20854;&#21547;&#20041;&#20197;&#21450;&#23545;&#35805;&#29983;&#25104;&#30340;&#30456;&#24212;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23545;&#35805;&#22788;&#29702;&#20219;&#21153;&#30340;&#20027;&#24178;&#24050;&#32463;&#26174;&#31034;&#20986;&#23545;&#24494;&#22937;&#20043;&#22788;&#30340;&#25935;&#24863;&#24615;&#12290;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#19981;&#20844;&#24179;&#12290;&#36807;&#21435;&#27809;&#26377;&#23545;&#36825;&#20010;&#38382;&#39064;&#36827;&#34892;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#23450;&#37327;&#27979;&#37327;&#27169;&#22411;&#23545;&#35828;&#35805;&#32773;&#21517;&#31216;&#30340;&#25935;&#24863;&#24230;&#65292;&#24182;&#20840;&#38754;&#35780;&#20272;&#35768;&#22810;&#24050;&#30693;&#30340;&#20943;&#23569;&#35828;&#35805;&#32773;&#21517;&#31216;&#25935;&#24863;&#24230;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#25105;&#20204;&#33258;&#24049;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#20026;&#27492;&#38382;&#39064;&#25552;&#20379;&#20102;&#22522;&#20934;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25935;&#24863;&#24230;&#38477;&#20302;&#21644;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Changing speaker names consistently throughout a dialogue should not affect its meaning and corresponding outputs for text generation from dialogues. However, pre-trained language models, serving as the backbone for dialogue-processing tasks, have shown to be sensitive to nuances. This may result in unfairness in real-world applications. No comprehensive analysis of this problem has been done in the past. In this work, we propose to quantitatively measure a model's sensitivity on speaker names, and comprehensively evaluate a number of known methods for reducing speaker name sensitivity, including a novel approach of our own. Extensive experiments on multiple datasets provide a benchmark for this problem and show the favorable performance of our approach in sensitivity reduction and quality of generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#33258;&#36866;&#24212;&#24773;&#24863;&#21487;&#25511;&#30340;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;ZET-Speech, &#23427;&#21487;&#20197;&#36890;&#36807;&#19968;&#27573;&#30701;&#30340;&#20013;&#24615;&#35821;&#38899;&#21644;&#30446;&#26631;&#24773;&#24863;&#26631;&#31614;&#21512;&#25104;&#20219;&#20309;&#35828;&#35805;&#32773;&#30340;&#24773;&#24863;&#35821;&#38899;&#65292;&#24182;&#19988;&#25104;&#21151;&#21512;&#25104;&#20102;&#20855;&#26377;&#25152;&#38656;&#24773;&#24863;&#30340;&#33258;&#28982;&#21644;&#24773;&#24863;&#35821;&#38899;&#65292;&#36866;&#29992;&#20110;&#35265;&#36807;&#21644;&#26410;&#35265;&#36807;&#30340;&#21457;&#35328;&#20154;&#12290;</title><link>http://arxiv.org/abs/2305.13831</link><description>&lt;p&gt;
ZET-Speech: &#22522;&#20110;&#25193;&#25955;&#21644;&#22522;&#20110;&#39118;&#26684;&#30340;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#33258;&#36866;&#24212;&#24773;&#24863;&#21487;&#25511;&#25991;&#26412;&#36716;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
ZET-Speech: Zero-shot adaptive Emotion-controllable Text-to-Speech Synthesis with Diffusion and Style-based Models. (arXiv:2305.13831v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#33258;&#36866;&#24212;&#24773;&#24863;&#21487;&#25511;&#30340;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;ZET-Speech, &#23427;&#21487;&#20197;&#36890;&#36807;&#19968;&#27573;&#30701;&#30340;&#20013;&#24615;&#35821;&#38899;&#21644;&#30446;&#26631;&#24773;&#24863;&#26631;&#31614;&#21512;&#25104;&#20219;&#20309;&#35828;&#35805;&#32773;&#30340;&#24773;&#24863;&#35821;&#38899;&#65292;&#24182;&#19988;&#25104;&#21151;&#21512;&#25104;&#20102;&#20855;&#26377;&#25152;&#38656;&#24773;&#24863;&#30340;&#33258;&#28982;&#21644;&#24773;&#24863;&#35821;&#38899;&#65292;&#36866;&#29992;&#20110;&#35265;&#36807;&#21644;&#26410;&#35265;&#36807;&#30340;&#21457;&#35328;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#35821;&#38899;&#21512;&#25104;&#26159;&#24320;&#21457;&#38656;&#35201;&#33258;&#28982;&#21644;&#24773;&#32490;&#35821;&#38899;&#30340;&#31995;&#32479;&#65288;&#20363;&#22914;&#31867;&#20154;&#23545;&#35805;&#20195;&#29702;&#65289;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#21482;&#38024;&#23545;&#35757;&#32451;&#26399;&#38388;&#35265;&#36807;&#30340;&#21457;&#35328;&#20154;&#29983;&#25104;&#24773;&#24863;&#35821;&#38899;&#65292;&#27809;&#26377;&#32771;&#34385;&#21040;&#23545;&#26410;&#35265;&#36807;&#30340;&#21457;&#35328;&#20154;&#30340;&#27867;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#33258;&#36866;&#24212;&#24773;&#24863;&#21487;&#25511;&#30340;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;ZET-Speech&#65292;&#21482;&#38656;&#35201;&#36890;&#36807;&#19968;&#27573;&#30701;&#30340;&#20013;&#24615;&#35821;&#38899;&#21644;&#30446;&#26631;&#24773;&#24863;&#26631;&#31614;&#21363;&#21487;&#21512;&#25104;&#20219;&#20309;&#35828;&#35805;&#32773;&#30340;&#24773;&#24863;&#35821;&#38899;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#25193;&#25955;&#27169;&#22411;&#19978;&#36827;&#34892;&#22495;&#23545;&#25239;&#23398;&#20064;&#21644;&#25351;&#23548;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#33258;&#36866;&#24212;&#24773;&#24863;&#35821;&#38899;&#21512;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ZET-Speech&#25104;&#21151;&#22320;&#21512;&#25104;&#20102;&#20855;&#26377;&#25152;&#38656;&#24773;&#24863;&#30340;&#33258;&#28982;&#21644;&#24773;&#24863;&#35821;&#38899;&#65292;&#36866;&#29992;&#20110;&#35265;&#36807;&#21644;&#26410;&#35265;&#36807;&#30340;&#21457;&#35328;&#20154;&#12290;&#26679;&#26412;&#22312;https://ZET-Speech.github.io/ZET-Speech-Demo/&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotional Text-To-Speech (TTS) is an important task in the development of systems (e.g., human-like dialogue agents) that require natural and emotional speech. Existing approaches, however, only aim to produce emotional TTS for seen speakers during training, without consideration of the generalization to unseen speakers. In this paper, we propose ZET-Speech, a zero-shot adaptive emotion-controllable TTS model that allows users to synthesize any speaker's emotional speech using only a short, neutral speech segment and the target emotion label. Specifically, to enable a zero-shot adaptive TTS model to synthesize emotional speech, we propose domain adversarial learning and guidance methods on the diffusion model. Experimental results demonstrate that ZET-Speech successfully synthesizes natural and emotional speech with the desired emotion for both seen and unseen speakers. Samples are at https://ZET-Speech.github.io/ZET-Speech-Demo/.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550; SALAM&#65292;&#36890;&#36807;&#21327;&#20316;&#20132;&#20114;&#19982;&#23398;&#20064;&#21161;&#25163;&#26469;&#24110;&#21161; LLM &#22312;&#21453;&#24605;&#21644;&#25913;&#36827;&#36807;&#31243;&#20013;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#25910;&#38598;&#38169;&#35823;&#24182;&#22312;&#25512;&#29702;&#26102;&#25552;&#20379;&#25351;&#23548;&#26041;&#38024;&#65292;&#26174;&#30528;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13829</link><description>&lt;p&gt;
&#36890;&#36807;&#21327;&#20316;&#20132;&#20114;&#19982;&#23398;&#20064;&#21161;&#25163;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learn from Mistakes through Cooperative Interaction with Study Assistant. (arXiv:2305.13829v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550; SALAM&#65292;&#36890;&#36807;&#21327;&#20316;&#20132;&#20114;&#19982;&#23398;&#20064;&#21161;&#25163;&#26469;&#24110;&#21161; LLM &#22312;&#21453;&#24605;&#21644;&#25913;&#36827;&#36807;&#31243;&#20013;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#25910;&#38598;&#38169;&#35823;&#24182;&#22312;&#25512;&#29702;&#26102;&#25552;&#20379;&#25351;&#23548;&#26041;&#38024;&#65292;&#26174;&#30528;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#33258;&#25105;&#21453;&#24605;&#21644;&#25913;&#36827;&#29983;&#25104;&#33021;&#21147;&#30340;&#33021;&#21147;&#65292;&#36825;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21453;&#39304;&#26426;&#21046;&#38754;&#20020;&#25361;&#25112;&#65292;&#20363;&#22914;&#19981;&#33021;&#20445;&#35777;&#27491;&#30830;&#24615;&#21644;&#23545;&#27169;&#22411;&#24369;&#28857;&#32570;&#20047;&#20840;&#23616;&#27934;&#23519;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550; Study Assistant for Large Language Model (SALAM)&#65292;&#20197;&#24110;&#21161; LLM &#22312;&#21453;&#24605;&#21644;&#25913;&#36827;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#26681;&#25454;&#20154;&#31867;&#21161;&#29702;&#30740;&#31350;&#30340;&#28789;&#24863;&#65292;&#36890;&#36807;&#23558;&#20808;&#21069;&#30340;&#21709;&#24212;&#19982;&#30495;&#23454;&#20540;&#36827;&#34892;&#23450;&#37327;&#20998;&#32423;&#65292;&#24182;&#22312;&#35757;&#32451;&#38454;&#27573;&#25910;&#38598;&#38169;&#35823;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#22312;&#25512;&#29702;&#26399;&#38388;&#65292;&#23427;&#26681;&#25454;&#38169;&#35823;&#25910;&#38598;&#30830;&#23450;&#24120;&#35265;&#35823;&#35299;&#65292;&#24182;&#25552;&#20379;&#25351;&#23548;&#26041;&#38024;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#22312;&#25512;&#29702;&#26399;&#38388;&#36991;&#20813;&#31867;&#20284;&#30340;&#38169;&#35823;&#12290;SALAM &#26159;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26694;&#26550;&#65292;&#19987;&#27880;&#20110;&#25552;&#20379;&#19968;&#33324;&#24615;&#30340;&#21453;&#39304;&#65292;&#24182;&#21487;&#36866;&#29992;&#20110;&#20219;&#20309;&#22522;&#30784;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#23545; SALAM &#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#23427;&#22312;&#21508;&#31181;&#22522;&#32447;&#19978;&#37117;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have demonstrated their ability to self-reflect and refine their generation, which can further improve their performance. However, this feedback mechanism faces challenges such as no guarantee of correctness and the lack of global insight into the model's weaknesses. In this paper, we propose a novel framework, Study Assistant for Large Language Model (SALAM), to aid LLMs in the reflection and refinement process. Motivated by the human study assistant, this framework grades previous responses with the ground truth and collects mistakes in the training phase. During inference, it identifies common misunderstandings based on the mistake collections and provides guidelines for the model to help the model avoid similar mistakes during inference. SALAM is a model-agnostic framework, focusing on providing general feedback and can adapt to any base model. Our evaluation of SALAM on two challenging benchmarks demonstrated a significant improvement over various baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36816;&#29992;&#24605;&#32500;&#38142;&#25512;&#29702;&#65292;&#23558;&#26684;&#36182;&#26031;&#30340;&#22235;&#20010;&#20934;&#21017;&#32435;&#20837;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#20197;&#26377;&#25928;&#29702;&#35299;&#23545;&#35805;&#21547;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.13826</link><description>&lt;p&gt;
&#8220;&#25945;&#30343;&#26159;&#22825;&#20027;&#25945;&#24466;&#21527;&#65311;&#8221;&#8212;&#8212;&#36816;&#29992;&#24605;&#32500;&#38142;&#25512;&#29702;&#29702;&#35299;&#23545;&#35805;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
"Is the Pope Catholic?" Applying Chain-of-Thought Reasoning to Understanding Conversational Implicatures. (arXiv:2305.13826v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36816;&#29992;&#24605;&#32500;&#38142;&#25512;&#29702;&#65292;&#23558;&#26684;&#36182;&#26031;&#30340;&#22235;&#20010;&#20934;&#21017;&#32435;&#20837;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#20197;&#26377;&#25928;&#29702;&#35299;&#23545;&#35805;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#21547;&#20041;&#26159;&#25351;&#35201;&#27714;&#21548;&#32773;&#20174;&#35828;&#35805;&#32773;&#30340;&#26126;&#30830;&#35805;&#35821;&#20013;&#25512;&#26029;&#20986;&#20854;&#25152;&#35201;&#20256;&#36798;&#30340;&#24847;&#24605;&#30340;&#35821;&#29992;&#25512;&#26029;&#12290;&#23613;&#31649;&#36825;&#31181;&#25512;&#29702;&#26159;&#20154;&#31867;&#20132;&#27969;&#30340;&#22522;&#30784;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#36825;&#20123;&#21547;&#20041;&#26041;&#38754;&#30340;&#34920;&#29616;&#36828;&#20302;&#20110;&#26222;&#36890;&#20154;&#12290;&#26412;&#25991;&#36890;&#36807;&#36816;&#29992;&#26684;&#36182;&#26031;&#30340;&#22235;&#20010;&#20934;&#21017;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#38142;&#25552;&#31034;&#23558;&#20854;&#32435;&#20837;&#27169;&#22411;&#20013;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#35813;&#20219;&#21153;&#30340;&#24179;&#22343;&#20154;&#31867;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational implicatures are pragmatic inferences that require listeners to deduce the intended meaning conveyed by a speaker from their explicit utterances. Although such inferential reasoning is fundamental to human communication, recent research indicates that large language models struggle to comprehend these implicatures as effectively as the average human. This paper demonstrates that by incorporating Grice's Four Maxims into the model through chain-of-thought prompting, we can significantly enhance its performance, surpassing even the average human performance on this task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35821;&#35328;&#35782;&#21035;&#27169;&#22411;&#65292;&#20351;&#29992;&#24320;&#28304;&#30340;&#25968;&#25454;&#38598;&#21644;&#31579;&#36873;&#20986;&#26469;&#30340;&#21333;&#35821;&#25968;&#25454;&#65292;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#23588;&#20854;&#26159;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#23545;&#27169;&#22411;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2305.13820</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#35782;&#21035;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Open Dataset and Model for Language Identification. (arXiv:2305.13820v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35821;&#35328;&#35782;&#21035;&#27169;&#22411;&#65292;&#20351;&#29992;&#24320;&#28304;&#30340;&#25968;&#25454;&#38598;&#21644;&#31579;&#36873;&#20986;&#26469;&#30340;&#21333;&#35821;&#25968;&#25454;&#65292;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#23588;&#20854;&#26159;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#23545;&#27169;&#22411;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#35782;&#21035;&#26159;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27969;&#31243;&#30340;&#22522;&#26412;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#35782;&#21035;&#31995;&#32479;&#36824;&#26377;&#35768;&#22810;&#32570;&#38519;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35821;&#35328;&#35782;&#21035;&#27169;&#22411;&#65292;&#23427;&#22312; 201 &#31181;&#35821;&#35328;&#19978;&#21462;&#24471;&#20102;&#23439;&#24179;&#22343; F1 &#20540;&#20026;0.93&#12289;&#20551;&#38451;&#24615;&#29575;&#20026;0.033&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#19968;&#20010;&#31579;&#36873;&#20986;&#26469;&#30340;&#21333;&#35821;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#21516;&#26102;&#25163;&#24037;&#23457;&#26680;&#26679;&#26412;&#65292;&#30830;&#20445;&#20854;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#23558;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#37117;&#25552;&#20379;&#32473;&#20102;&#30740;&#31350;&#31038;&#21306;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#20998;&#26512;&#65292;&#21253;&#25324;&#19982;&#29616;&#26377;&#24320;&#28304;&#27169;&#22411;&#30340;&#27604;&#36739;&#20197;&#21450;&#25353;&#35821;&#35328;&#31867;&#21035;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language identification (LID) is a fundamental step in many natural language processing pipelines. However, current LID systems are far from perfect, particularly on lower-resource languages. We present a LID model which achieves a macro-average F1 score of 0.93 and a false positive rate of 0.033 across 201 languages, outperforming previous work. We achieve this by training on a curated dataset of monolingual data, the reliability of which we ensure by auditing a sample from each source and each language manually. We make both the model and the dataset available to the research community. Finally, we carry out detailed analysis into our model's performance, both in comparison to existing open models and by language class.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#25552;&#21462;&#20020;&#24202; PDF &#25991;&#26723;&#20013;&#26377;&#20851;&#20020;&#24202;&#30340;&#25991;&#26412;&#65292;&#20197;&#25552;&#39640;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13817</link><description>&lt;p&gt;
&#33258;&#21160;&#26816;&#27979;&#20020;&#24202;&#25991;&#26723;&#30340;&#29256;&#38754;&#20197;&#25552;&#39640;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Detecting automatically the layout of clinical documents to enhance the performances of downstream natural language processing. (arXiv:2305.13817v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#25552;&#21462;&#20020;&#24202; PDF &#25991;&#26723;&#20013;&#26377;&#20851;&#20020;&#24202;&#30340;&#25991;&#26412;&#65292;&#20197;&#25552;&#39640;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#24320;&#21457;&#24182;&#39564;&#35777;&#19968;&#31181;&#20998;&#26512; PDF &#20020;&#24202;&#25991;&#26723;&#29256;&#38754;&#30340;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#22788;&#29702;&#20020;&#24202; PDF &#25991;&#26723;&#65292;&#24182;&#25552;&#21462;&#21482;&#19982;&#20020;&#24202;&#30456;&#20851;&#30340;&#25991;&#26412;&#12290;&#35813;&#31639;&#27861;&#21253;&#25324;&#20960;&#20010;&#27493;&#39588;&#65306;&#20351;&#29992; PDF &#35299;&#26512;&#22120;&#36827;&#34892;&#21021;&#22987;&#25991;&#26412;&#25552;&#21462;&#65292;&#28982;&#21518;&#20351;&#29992; Transformer &#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#23558;&#20854;&#20998;&#31867;&#20026;&#27491;&#25991;&#12289;&#24038;&#20391;&#27880;&#37322;&#21644;&#39029;&#33050;&#31561;&#31867;&#21035;&#65292;&#26368;&#21518;&#36827;&#34892;&#27719;&#24635;&#27493;&#39588;&#20197;&#32534;&#35793;&#32473;&#23450;&#26631;&#31614;&#30340;&#25991;&#26412;&#34892;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#24050;&#27880;&#37322;&#30340;&#25991;&#26723;&#30340;&#38543;&#26426;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#27491;&#25991;&#25552;&#21462;&#31639;&#27861;&#30340;&#25216;&#26415;&#24615;&#33021;&#12290;&#36890;&#36807;&#26816;&#26597;&#20174;&#21508;&#33258;&#30340;&#37096;&#20998;&#20013;&#25552;&#21462;&#24863;&#20852;&#36259;&#30340;&#21307;&#23398;&#27010;&#24565;&#26469;&#35780;&#20272;&#21307;&#23398;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#25551;&#36848;&#24613;&#24615;&#24863;&#26579;&#30340;&#21307;&#23398;&#29992;&#20363;&#19978;&#27979;&#35797;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective:Develop and validate an algorithm for analyzing the layout of PDF clinical documents to improve the performance of downstream natural language processing tasks. Materials and Methods: We designed an algorithm to process clinical PDF documents and extract only clinically relevant text. The algorithm consists of several steps: initial text extraction using a PDF parser, followed by classification into categories such as body text, left notes, and footers using a Transformer deep neural network architecture, and finally an aggregation step to compile the lines of a given label in the text. We evaluated the technical performance of the body text extraction algorithm by applying it to a random sample of documents that were annotated. Medical performance was evaluated by examining the extraction of medical concepts of interest from the text in their respective sections. Finally, we tested an end-to-end system on a medical use case of automatic detection of acute infection described
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#22270;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20174;&#25991;&#26412;&#20013;&#35299;&#26512;&#20986;&#30340;&#22330;&#26223;&#22270;&#35270;&#20026;&#22270;&#20687;&#22330;&#26223;&#22270;&#30340;&#20195;&#29702;&#65292;&#24182;&#23545;&#22270;&#36827;&#34892;&#20998;&#35299;&#21644;&#22686;&#24378;&#65292;&#20174;&#31616;&#21333;&#21040;&#22797;&#26434;&#30340;&#23545;&#27604;&#23398;&#20064;&#20197;&#23558;&#21508;&#31181;&#22797;&#26434;&#24230;&#30340;&#21477;&#23376;&#23545;&#40784;&#21040;&#21516;&#19968;&#24133;&#22270;&#20687;&#19978;&#65292;&#21516;&#26102;&#22312;&#22330;&#26223;&#22270;&#31354;&#38388;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#36127;&#26679;&#26412;&#25366;&#25496;&#25216;&#26415;&#65292;&#20197;&#25913;&#21892;&#35270;&#35273;&#35821;&#35328;&#32452;&#21512;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.13812</link><description>&lt;p&gt;
&#20174;&#22270;&#20687;-&#25991;&#26412;-&#22270;&#35889;&#31354;&#38388;&#20013;&#36827;&#34892;&#31895;&#21040;&#32454;&#30340;&#23545;&#27604;&#23398;&#20064;&#65292;&#25552;&#39640;&#35270;&#35273;&#35821;&#35328;&#32452;&#21512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality. (arXiv:2305.13812v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#22270;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20174;&#25991;&#26412;&#20013;&#35299;&#26512;&#20986;&#30340;&#22330;&#26223;&#22270;&#35270;&#20026;&#22270;&#20687;&#22330;&#26223;&#22270;&#30340;&#20195;&#29702;&#65292;&#24182;&#23545;&#22270;&#36827;&#34892;&#20998;&#35299;&#21644;&#22686;&#24378;&#65292;&#20174;&#31616;&#21333;&#21040;&#22797;&#26434;&#30340;&#23545;&#27604;&#23398;&#20064;&#20197;&#23558;&#21508;&#31181;&#22797;&#26434;&#24230;&#30340;&#21477;&#23376;&#23545;&#40784;&#21040;&#21516;&#19968;&#24133;&#22270;&#20687;&#19978;&#65292;&#21516;&#26102;&#22312;&#22330;&#26223;&#22270;&#31354;&#38388;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#36127;&#26679;&#26412;&#25366;&#25496;&#25216;&#26415;&#65292;&#20197;&#25913;&#21892;&#35270;&#35273;&#35821;&#35328;&#32452;&#21512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20174;&#32780;&#20026;&#21508;&#31181;&#19979;&#28216;&#22810;&#27169;&#24577;&#20219;&#21153;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#20984;&#26174;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#23545;&#35937;&#12289;&#23646;&#24615;&#21644;&#20851;&#31995;&#30340;&#32452;&#25104;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#20005;&#37325;&#38480;&#21046;&#12290;&#22330;&#26223;&#22270;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#29702;&#35299;&#22270;&#20687;&#32452;&#25104;&#30340;&#26377;&#25928;&#26041;&#24335;&#12290;&#36825;&#20123;&#26159;&#22270;&#20687;&#30340;&#22270;&#24418;&#32467;&#26500;&#21270;&#35821;&#20041;&#34920;&#31034;&#65292;&#21253;&#25324;&#22330;&#26223;&#20013;&#30340;&#23545;&#35937;&#12289;&#23427;&#20204;&#30340;&#23646;&#24615;&#21644;&#19982;&#22330;&#26223;&#20013;&#20854;&#20182;&#23545;&#35937;&#30340;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#20174;&#25991;&#26412;&#20013;&#35299;&#26512;&#20986;&#30340;&#22330;&#26223;&#22270;&#35270;&#20026;&#22270;&#20687;&#22330;&#26223;&#22270;&#30340;&#20195;&#29702;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#20998;&#35299;&#21644;&#22686;&#24378;&#26694;&#26550;&#65292;&#20197;&#21450;&#20174;&#31616;&#21333;&#21040;&#22797;&#26434;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#23558;&#21508;&#31181;&#22797;&#26434;&#24230;&#30340;&#21477;&#23376;&#23545;&#40784;&#21040;&#21516;&#19968;&#24133;&#22270;&#20687;&#19978;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#22312;&#22330;&#26223;&#22270;&#31354;&#38388;&#25552;&#20986;&#20102;&#26032;&#30340;&#36127;&#26679;&#26412;&#25366;&#25496;&#25216;&#26415;&#65292;&#29992;&#20110;&#25913;&#21892;&#23545;&#27604;&#23398;&#20064;&#12290;&#22312;&#19977;&#20010;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#24378;&#22823;&#30340;&#35270;&#35273;&#35821;&#35328;&#22522;&#32447;&#65292;&#29305;&#21035;&#26159;&#22312;&#23545;&#35937;&#12289;&#23646;&#24615;&#21644;&#20851;&#31995;&#30340;&#32452;&#25104;&#25512;&#29702;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastively trained vision-language models have achieved remarkable progress in vision and language representation learning, leading to state-of-the-art models for various downstream multimodal tasks. However, recent research has highlighted severe limitations of these models in their ability to perform compositional reasoning over objects, attributes, and relations. Scene graphs have emerged as an effective way to understand images compositionally. These are graph-structured semantic representations of images that contain objects, their attributes, and relations with other objects in a scene. In this work, we consider the scene graph parsed from text as a proxy for the image scene graph and propose a graph decomposition and augmentation framework along with a coarse-to-fine contrastive learning objective between images and text that aligns sentences of various complexities to the same image. Along with this, we propose novel negative mining techniques in the scene graph space for im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24320;&#25918;&#22495;QA&#20013;&#27495;&#20041;&#38382;&#39064;&#30340;&#26041;&#27861;&#65306;&#36890;&#36807;&#25552;&#38382;&#28548;&#28165;&#26469;&#30830;&#23450;&#26368;&#31526;&#21512;&#29992;&#25143;&#24847;&#22270;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2305.13808</link><description>&lt;p&gt;
&#24320;&#25918;&#22495;QA&#20013;&#36890;&#36807;&#25552;&#38382;&#28548;&#28165;&#35299;&#20915;&#27495;&#20041;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Asking Clarification Questions to Handle Ambiguity in Open-Domain QA. (arXiv:2305.13808v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24320;&#25918;&#22495;QA&#20013;&#27495;&#20041;&#38382;&#39064;&#30340;&#26041;&#27861;&#65306;&#36890;&#36807;&#25552;&#38382;&#28548;&#28165;&#26469;&#30830;&#23450;&#26368;&#31526;&#21512;&#29992;&#25143;&#24847;&#22270;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#22495;&#20013;&#23384;&#22312;&#27495;&#20041;&#38382;&#39064;&#65292;&#22914;&#20309;&#25552;&#20986;&#19968;&#20010;&#20934;&#30830;&#19988;&#29420;&#19968;&#26080;&#20108;&#30340;&#38382;&#39064;&#26159;&#24456;&#20855;&#25361;&#25112;&#24615;&#30340;&#12290;&#36807;&#21435;&#65292;Min et al. (2020) &#36890;&#36807;&#20026;&#25152;&#26377;&#21487;&#33021;&#30340;&#35299;&#37322;&#29983;&#25104;&#28040;&#38500;&#27495;&#20041;&#30340;&#38382;&#39064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#26377;&#25928;&#65292;&#20294;&#24182;&#19981;&#29702;&#24819;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#25552;&#38382;&#28548;&#28165;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29992;&#25143;&#30340;&#22238;&#31572;&#23558;&#24110;&#21161;&#30830;&#23450;&#26368;&#31526;&#21512;&#29992;&#25143;&#24847;&#22270;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;CAMBIGNQ&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;5,654&#20010;&#21547;&#26377;&#30456;&#20851;&#27573;&#33853;&#12289;&#21487;&#33021;&#30340;&#31572;&#26696;&#21644;&#28548;&#28165;&#38382;&#39064;&#30340;&#27495;&#20041;&#38382;&#39064;&#32452;&#25104;&#12290;&#36825;&#20123;&#28548;&#28165;&#38382;&#39064;&#26159;&#36890;&#36807;&#20351;&#29992;InstructGPT&#29983;&#25104;&#28982;&#21518;&#36827;&#34892;&#24517;&#35201;&#30340;&#25163;&#21160;&#20462;&#35746;&#26469;&#39640;&#25928;&#21019;&#24314;&#30340;&#12290;&#28982;&#21518;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31995;&#21015;&#20219;&#21153;&#21644;&#30456;&#24212;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#27495;&#20041;&#26816;&#27979;&#19978;&#33719;&#24471;&#20102;61.3 F1&#65292;&#22312;&#28548;&#28165;&#22411;QA&#19978;&#33719;&#24471;&#20102;40.5 F1&#65292;&#20026;&#25552;&#20379;&#24378;&#26377;&#21147;&#30340;&#31572;&#26696;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ambiguous questions persist in open-domain question answering, because formulating a precise question with a unique answer is often challenging. Previously, Min et al. (2020) have tackled this issue by generating disambiguated questions for all possible interpretations of the ambiguous question. This can be effective, but not ideal for providing an answer to the user. Instead, we propose to ask a clarification question, where the user's response will help identify the interpretation that best aligns with the user's intention. We first present CAMBIGNQ, a dataset consisting of 5,654 ambiguous questions, each with relevant passages, possible answers, and a clarification question. The clarification questions were efficiently created by generating them using InstructGPT and manually revising them as necessary. We then define a pipeline of tasks and design appropriate evaluation metrics. Lastly, we achieve 61.3 F1 on ambiguity detection and 40.5 F1 on clarification-based QA, providing stron
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; ReXMiner &#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616; Web &#25366;&#25496;&#20013;&#30340;&#38646;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#25991;&#26723;&#23545;&#35937;&#27169;&#22411;&#65288;DOM&#65289;&#26641;&#20013;&#26368;&#30701;&#30456;&#23545;&#36335;&#24452;&#26469;&#26356;&#20934;&#30830;&#12289;&#39640;&#25928;&#22320;&#25552;&#21462;&#32593;&#39029;&#20013;&#30340;&#38190;&#20540;&#23545;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#25991;&#26412;&#33410;&#28857;&#22312;&#19981;&#21516;&#32593;&#39029;&#20013;&#20986;&#29616;&#30340;&#27425;&#25968;&#26469;&#34913;&#37327;&#20854;&#27969;&#34892;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.13805</link><description>&lt;p&gt;
&#38754;&#21521;&#38646;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#30340; Web &#25366;&#25496;&#22810;&#27169;&#24577;&#30456;&#23545; XML &#36335;&#24452;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Zero-shot Relation Extraction in Web Mining: A Multimodal Approach with Relative XML Path. (arXiv:2305.13805v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; ReXMiner &#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616; Web &#25366;&#25496;&#20013;&#30340;&#38646;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#25991;&#26723;&#23545;&#35937;&#27169;&#22411;&#65288;DOM&#65289;&#26641;&#20013;&#26368;&#30701;&#30456;&#23545;&#36335;&#24452;&#26469;&#26356;&#20934;&#30830;&#12289;&#39640;&#25928;&#22320;&#25552;&#21462;&#32593;&#39029;&#20013;&#30340;&#38190;&#20540;&#23545;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#25991;&#26412;&#33410;&#28857;&#22312;&#19981;&#21516;&#32593;&#39029;&#20013;&#20986;&#29616;&#30340;&#27425;&#25968;&#26469;&#34913;&#37327;&#20854;&#27969;&#34892;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#39029;&#25968;&#37327;&#30340;&#24555;&#36895;&#22686;&#38271;&#21450;&#20854;&#32467;&#26500;&#30340;&#26085;&#30410;&#22797;&#26434;&#21270;&#23545; Web &#25366;&#25496;&#27169;&#22411;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#29702;&#35299;&#21322;&#32467;&#26500;&#21270;&#30340;&#32593;&#39029;&#65292;&#29305;&#21035;&#26159;&#22312;&#23545;&#26032;&#32593;&#39029;&#30340;&#20027;&#39064;&#25110;&#27169;&#26495;&#30693;&#20043;&#29978;&#23569;&#30340;&#24773;&#20917;&#19979;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558; XML &#28304;&#20195;&#30721;&#23884;&#20837;&#21040; transformer &#25110;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#21576;&#29616;&#30340;&#24067;&#23616;&#36827;&#34892;&#32534;&#30721;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#36801;&#31227;&#21040; Web &#25366;&#25496;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#27809;&#26377;&#32771;&#34385;&#32593;&#39029;&#20869;&#37096;&#21644;&#36328;&#39029;&#38754;&#30340;&#25991;&#26412;&#33410;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; ReXMiner&#65292;&#29992;&#20110;&#23454;&#29616; Web &#25366;&#25496;&#20013;&#30340;&#38646;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#12290;ReXMiner &#32534;&#30721;&#25991;&#26723;&#23545;&#35937;&#27169;&#22411;&#65288;DOM&#65289;&#26641;&#20013;&#30340;&#26368;&#30701;&#30456;&#23545;&#36335;&#24452;&#65292;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#32593;&#39029;&#20869;&#30340;&#38190;&#20540;&#23545;&#25552;&#21462;&#26469;&#35828;&#26356;&#21152;&#20934;&#30830;&#21644;&#26377;&#25928;&#12290;&#23427;&#36824;&#36890;&#36807;&#35745;&#31639;&#30456;&#21516;&#25991;&#26412;&#33410;&#28857;&#22312;&#19981;&#21516;&#32593;&#39029;&#20013;&#20986;&#29616;&#30340;&#27425;&#25968;&#65292;&#26469;&#21453;&#26144;&#27599;&#20010;&#25991;&#26412;&#33410;&#28857;&#30340;&#27969;&#34892;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid growth of web pages and the increasing complexity of their structure poses a challenge for web mining models. Web mining models are required to understand the semi-structured web pages, particularly when little is known about the subject or template of a new page. Current methods migrate language models to the web mining by embedding the XML source code into the transformer or encoding the rendered layout with graph neural networks. However, these approaches do not take into account the relationships between text nodes within and across pages. In this paper, we propose a new approach, ReXMiner, for zero-shot relation extraction in web mining. ReXMiner encodes the shortest relative paths in the Document Object Model (DOM) tree which is a more accurate and efficient signal for key-value pair extraction within a web page. It also incorporates the popularity of each text node by counting the occurrence of the same text node across different web pages. We use the contrastive learn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#39044;&#27979;ASR&#26041;&#27861;&#65292;&#21487;&#20197;&#38477;&#20302;&#35821;&#38899;&#21161;&#25163;&#20013;&#30340;&#24310;&#36831;&#65292;&#36890;&#36807;&#39044;&#27979;&#23436;&#25972;&#35805;&#35821;&#26469;&#39044;&#21462;&#21709;&#24212;&#65292;&#24182;&#25506;&#35752;&#20102;&#25104;&#21151;&#39044;&#27979;&#21644;&#22833;&#36133;&#39044;&#27979;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.13794</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#39044;&#27979;ASR&#22312;&#35821;&#38899;&#21161;&#25163;&#20013;&#30340;&#24310;&#36831;&#38477;&#20302;
&lt;/p&gt;
&lt;p&gt;
Personalized Predictive ASR for Latency Reduction in Voice Assistants. (arXiv:2305.13794v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#39044;&#27979;ASR&#26041;&#27861;&#65292;&#21487;&#20197;&#38477;&#20302;&#35821;&#38899;&#21161;&#25163;&#20013;&#30340;&#24310;&#36831;&#65292;&#36890;&#36807;&#39044;&#27979;&#23436;&#25972;&#35805;&#35821;&#26469;&#39044;&#21462;&#21709;&#24212;&#65292;&#24182;&#25506;&#35752;&#20102;&#25104;&#21151;&#39044;&#27979;&#21644;&#22833;&#36133;&#39044;&#27979;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#21161;&#25163;&#20013;&#30340;&#27969;&#24335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21487;&#20197;&#21033;&#29992;&#39044;&#21462;&#26469;&#37096;&#20998;&#38544;&#34255;&#21709;&#24212;&#29983;&#25104;&#30340;&#24310;&#36831;&#12290;&#39044;&#21462;&#28041;&#21450;&#23558;&#21021;&#27493;&#30340;ASR&#20551;&#35774;&#20256;&#36882;&#32473;&#19979;&#28216;&#31995;&#32479;&#65292;&#20197;&#39044;&#21462;&#21644;&#32531;&#23384;&#21709;&#24212;&#12290;&#22914;&#26524;&#32456;&#28857;&#26816;&#27979;&#21518;&#30340;&#26368;&#32456;ASR&#20551;&#35774;&#19982;&#21021;&#27493;&#20551;&#35774;&#21305;&#37197;&#65292;&#21017;&#21487;&#20197;&#23558;&#32531;&#23384;&#30340;&#21709;&#24212;&#20132;&#20184;&#32473;&#29992;&#25143;&#65292;&#20174;&#32780;&#33410;&#30465;&#24310;&#36831;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#35805;&#35821;&#39044;&#27979;&#23436;&#25972;&#35805;&#35821;&#65292;&#24182;&#26681;&#25454;&#39044;&#27979;&#35805;&#35821;&#39044;&#21462;&#21709;&#24212;&#30340;&#39044;&#27979;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65292;&#25193;&#23637;&#20102;&#36825;&#20010;&#24819;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#25104;&#21151;&#39044;&#27979;&#30340;&#28508;&#22312;&#24310;&#36831;&#22686;&#30410;&#19982;&#39044;&#27979;&#22833;&#36133;&#30340;&#25104;&#26412;&#22686;&#21152;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#22312;&#20869;&#37096;&#35821;&#38899;&#21161;&#25163;&#25968;&#25454;&#38598;&#20197;&#21450;&#20844;&#20849;SLURP&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Streaming Automatic Speech Recognition (ASR) in voice assistants can utilize prefetching to partially hide the latency of response generation. Prefetching involves passing a preliminary ASR hypothesis to downstream systems in order to prefetch and cache a response. If the final ASR hypothesis after endpoint detection matches the preliminary one, the cached response can be delivered to the user, thus saving latency. In this paper, we extend this idea by introducing predictive automatic speech recognition, where we predict the full utterance from a partially observed utterance, and prefetch the response based on the predicted utterance. We introduce two personalization approaches and investigate the tradeoff between potential latency gains from successful predictions and the cost increase from failed predictions. We evaluate our methods on an internal voice assistant dataset as well as the public SLURP dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#26041;&#38754;&#30340;&#24615;&#33021;&#21644;&#19982;&#20154;&#31867;&#20998;&#27495;&#20998;&#24067;&#30340;&#23545;&#40784;&#24773;&#20917;&#12290;&#32467;&#26524;&#34920;&#26126;LLM&#30340;&#25512;&#26029;&#33021;&#21147;&#26377;&#38480;&#65292;&#26080;&#27861;&#25429;&#25417;&#21040;&#20154;&#31867;&#20998;&#27495;&#20998;&#24067;&#65292;&#24341;&#21457;&#20102;&#23545;&#20854;NLU&#21644;&#20195;&#34920;&#20154;&#31867;&#29992;&#25143;&#24615;&#36136;&#30340;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2305.13788</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#20687;&#20154;&#31867;&#19968;&#26679;&#25512;&#29702;&#21644;&#20135;&#29983;&#20998;&#27495;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Infer and Disagree Like Humans?. (arXiv:2305.13788v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#26041;&#38754;&#30340;&#24615;&#33021;&#21644;&#19982;&#20154;&#31867;&#20998;&#27495;&#20998;&#24067;&#30340;&#23545;&#40784;&#24773;&#20917;&#12290;&#32467;&#26524;&#34920;&#26126;LLM&#30340;&#25512;&#26029;&#33021;&#21147;&#26377;&#38480;&#65292;&#26080;&#27861;&#25429;&#25417;&#21040;&#20154;&#31867;&#20998;&#27495;&#20998;&#24067;&#65292;&#24341;&#21457;&#20102;&#23545;&#20854;NLU&#21644;&#20195;&#34920;&#20154;&#31867;&#29992;&#25143;&#24615;&#36136;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#24191;&#27867;&#20219;&#21153;&#26041;&#38754;&#24050;&#32463;&#34920;&#29616;&#20986;&#38750;&#24120;&#22909;&#30340;&#25104;&#32489;&#12290;&#22312;&#29983;&#25104;&#25991;&#26412;&#26102;&#65292;&#20174;&#36825;&#20123;&#27169;&#22411;&#20013;&#37319;&#26679;&#26631;&#35760;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#31574;&#30053;&#12290;&#20294;&#26159;&#65292;LLM&#24456;&#38590;&#19982;&#20154;&#31867;&#30340;&#20998;&#27495;&#20998;&#24067;&#39640;&#24230;&#23545;&#40784;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#26041;&#38754;&#12290;&#26412;&#25991;&#20351;&#29992; Monte Carlo Reconstruction&#65288;MCR&#65289;&#21644; Log Probability Reconstruction&#65288;LPR&#65289;&#20004;&#31181;&#19981;&#21516;&#30340;&#25216;&#26415;&#35780;&#20272;&#20102;LLM&#20998;&#24067;&#30340;&#24615;&#33021;&#21644;&#19982;&#20154;&#31867;&#30340;&#23545;&#40784;&#24773;&#20917;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#22312;&#35299;&#20915;NLI&#20219;&#21153;&#26041;&#38754;&#33021;&#21147;&#26377;&#38480;&#65292;&#21516;&#26102;&#26080;&#27861;&#25429;&#25417;&#21040;&#20154;&#31867;&#30340;&#20998;&#27495;&#20998;&#24067;&#65292;&#36825;&#23545;&#20854;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#33021;&#21147;&#21644;&#20195;&#34920;&#20154;&#31867;&#29992;&#25143;&#30340;&#29305;&#24615;&#25552;&#20986;&#20102;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown stellar achievements in solving a broad range of tasks. When generating text, it is common to sample tokens from these models: whether LLMs closely align with the human disagreement distribution has not been well-studied, especially within the scope of Natural Language Inference (NLI). In this paper, we evaluate the performance and alignment of LLM distribution with humans using two different techniques: Monte Carlo Reconstruction (MCR) and Log Probability Reconstruction (LPR). As a result, we show LLMs exhibit limited ability in solving NLI tasks and simultaneously fail to capture human disagreement distribution, raising concerns about their natural language understanding (NLU) ability and their representativeness of human users.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36741;&#21161;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#24494;&#35843;&#26469;&#23454;&#29616;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#40657;&#30418;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13785</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#25968;&#25454;&#22686;&#24378;&#25552;&#21319;&#40657;&#30418;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Enhancing Black-Box Few-Shot Text Classification with Prompt-Based Data Augmentation. (arXiv:2305.13785v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36741;&#21161;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#24494;&#35843;&#26469;&#23454;&#29616;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#40657;&#30418;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#25110;&#24494;&#35843;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22914; GPT-3 &#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#25512;&#21160;&#20102;&#26368;&#36817;&#25506;&#32034;&#21442;&#25968;&#39640;&#25928;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#30340;&#21162;&#21147;&#12290;&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20248;&#21270;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382; LLM &#30340;&#26799;&#24230;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#23558;&#40657;&#30418;&#27169;&#22411;&#35270;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#20351;&#29992;&#22686;&#24378;&#30340;&#25991;&#26412;&#25968;&#25454;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#25968;&#25454;&#22686;&#24378;&#26159;&#36890;&#36807;&#22312;&#19968;&#20010;&#27604;&#40657;&#30418;&#27169;&#22411;&#21442;&#25968;&#35268;&#27169;&#23567;&#24471;&#22810;&#30340;&#36741;&#21161;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#26469;&#23436;&#25104;&#30340;&#12290;&#36890;&#36807;&#23545;&#20843;&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65288;&#31216;&#20026; BT-Classifier&#65289;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#40657;&#30418;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#65292;&#24182;&#19982;&#20381;&#36182;&#20110;&#20840;&#27169;&#22411;&#35843;&#25972;&#30340;&#26041;&#27861;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training or finetuning large-scale language models (LLMs) such as GPT-3 requires substantial computation resources, motivating recent efforts to explore parameter-efficient adaptation to downstream tasks. One practical area of research is to treat these models as black boxes and interact with them through their inference APIs. In this paper, we investigate how to optimize few-shot text classification without accessing the gradients of the LLMs. To achieve this, we treat the black-box model as a feature extractor and train a classifier with the augmented text data. Data augmentation is performed using prompt-based finetuning on an auxiliary language model with a much smaller parameter size than the black-box model. Through extensive experiments on eight text classification datasets, we show that our approach, dubbed BT-Classifier, significantly outperforms state-of-the-art black-box few-shot learners and performs on par with methods that rely on full-model tuning.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21482;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#38656;&#35201;&#35270;&#35273;&#36755;&#20837;&#21644;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#30340;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#21333;&#29420;&#30340;&#35821;&#35328;&#27169;&#22411;&#20351;&#20854;&#21487;&#20197;&#35775;&#38382;&#35270;&#35273;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26679;&#26412;&#26377;&#38480;&#65292;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#25104;&#21151;&#35299;&#20915;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#65292;&#36824;&#25552;&#39640;&#20102;&#27169;&#22411;&#36755;&#20986;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13782</link><description>&lt;p&gt;
&#35821;&#35328;&#31354;&#38388;&#20013;&#30340;&#22270;&#20687;&#65306;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Images in Language Space: Exploring the Suitability of Large Language Models for Vision &amp; Language Tasks. (arXiv:2305.13782v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13782
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21482;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#38656;&#35201;&#35270;&#35273;&#36755;&#20837;&#21644;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#30340;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#21333;&#29420;&#30340;&#35821;&#35328;&#27169;&#22411;&#20351;&#20854;&#21487;&#20197;&#35775;&#38382;&#35270;&#35273;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26679;&#26412;&#26377;&#38480;&#65292;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#25104;&#21151;&#35299;&#20915;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#65292;&#36824;&#25552;&#39640;&#20102;&#27169;&#22411;&#36755;&#20986;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;&#20102;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#12290;&#34429;&#28982;&#22810;&#27169;&#22411;&#27169;&#22411;&#27491;&#22312;&#31215;&#26497;&#30740;&#31350;&#20013;&#65292;&#21487;&#20197;&#39069;&#22806;&#22788;&#29702;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#65292;&#20294;&#22312;&#22823;&#23567;&#21644;&#26222;&#36866;&#24615;&#19978;&#36824;&#27809;&#26377;&#36798;&#21040;&#20165;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35810;&#38382;&#21482;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#38656;&#35201;&#35270;&#35273;&#36755;&#20837;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#27491;&#22914;&#25105;&#20204;&#25152;&#20105;&#35770;&#30340;&#65292;&#36825;&#20123;&#20219;&#21153;&#36890;&#24120;&#38656;&#35201;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#31867;&#20284;&#20110;&#19968;&#20123;&#26368;&#36817;&#30340;&#30456;&#20851;&#24037;&#20316;&#65292;&#25105;&#20204;&#20351;&#29992;&#21333;&#29420;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#20351;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#35775;&#38382;&#35270;&#35273;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24320;&#28304;&#12289;&#24320;&#25918;&#35775;&#38382;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#23545;&#20110;GPT-3&#22312;&#32473;&#23450;&#20197;&#25991;&#26412;&#32534;&#30721;&#30340;&#35270;&#35273;&#20449;&#24687;&#26102;&#22312;&#20116;&#20010;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26679;&#26412;&#26377;&#38480;&#65292;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#36824;&#36890;&#36807;&#25552;&#20379;&#25163;&#27573;&#26469;&#22686;&#24378;&#27169;&#22411;&#36755;&#20986;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have demonstrated robust performance on various language tasks using zero-shot or few-shot learning paradigms. While being actively researched, multimodal models that can additionally handle images as input have yet to catch up in size and generality with language-only models. In this work, we ask whether language-only models can be utilised for tasks that require visual input -- but also, as we argue, often require a strong reasoning component. Similar to some recent related work, we make visual information accessible to the language model using separate verbalisation models. Specifically, we investigate the performance of open-source, open-access language models against GPT-3 on five vision-language tasks when given textually-encoded visual information. Our results suggest that language models are effective for solving vision-language tasks even with limited samples. This approach also enhances the interpretability of a model's output by providing a means of tra
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24847;&#22270;&#26465;&#20214;&#19979;&#30340;&#21453;&#35805;&#35821;&#29983;&#25104;&#26041;&#27861;QUARC&#65292;&#22522;&#20110;IntentCONAN&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#21521;&#37327;&#37327;&#21270;&#34920;&#31034;&#21644;PerFuMe&#34701;&#21512;&#27169;&#22359;&#23454;&#29616;&#29305;&#23450;&#24847;&#22270;&#30340;&#21453;&#35805;&#35821;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2305.13776</link><description>&lt;p&gt;
&#25345;&#32493;&#34701;&#21512;&#24847;&#22270;&#20998;&#24067;&#23398;&#20064;&#30340;&#21453;&#35805;&#35821;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Counterspeeches up my sleeve! Intent Distribution Learning and Persistent Fusion for Intent-Conditioned Counterspeech Generation. (arXiv:2305.13776v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13776
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24847;&#22270;&#26465;&#20214;&#19979;&#30340;&#21453;&#35805;&#35821;&#29983;&#25104;&#26041;&#27861;QUARC&#65292;&#22522;&#20110;IntentCONAN&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#21521;&#37327;&#37327;&#21270;&#34920;&#31034;&#21644;PerFuMe&#34701;&#21512;&#27169;&#22359;&#23454;&#29616;&#29305;&#23450;&#24847;&#22270;&#30340;&#21453;&#35805;&#35821;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#35805;&#35821;&#24050;&#34987;&#35777;&#26126;&#26159;&#23545;&#25239;&#20167;&#24680;&#35328;&#35770;&#30340;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#27599;&#31181;&#22330;&#26223;&#65292;&#20855;&#26377;&#29305;&#23450;&#24847;&#22270;&#30340;&#21453;&#35805;&#35821;&#21487;&#33021;&#24182;&#19981;&#36275;&#22815;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24847;&#22270;&#26465;&#20214;&#19979;&#30340;&#21453;&#35805;&#35821;&#29983;&#25104;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;IntentCONAN&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;6831&#20010;&#21453;&#35805;&#35821;&#65292;&#20998;&#20026;&#20116;&#31181;&#24847;&#22270;&#65306;&#20449;&#24687;&#12289;&#35892;&#36131;&#12289;&#38382;&#39064;&#12289;&#31215;&#26497;&#21644;&#24189;&#40664;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;QUARC&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26377;&#20004;&#20010;&#38454;&#27573;&#65292;&#29992;&#20110;&#24847;&#22270;&#26465;&#20214;&#19979;&#30340;&#21453;&#35805;&#35821;&#29983;&#25104;&#12290;QUARC&#21033;&#29992;&#23398;&#20064;&#27599;&#31181;&#24847;&#22270;&#31867;&#21035;&#30340;&#21521;&#37327;&#37327;&#21270;&#34920;&#31034;&#65292;&#20197;&#21450;PerFuMe&#65292;&#19968;&#31181;&#29992;&#20110;&#25972;&#21512;&#29305;&#23450;&#24847;&#22270;&#30340;&#20449;&#24687;&#30340;&#26032;&#22411;&#34701;&#21512;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterspeech has been demonstrated to be an efficacious approach for combating hate speech. While various conventional and controlled approaches have been studied in recent years to generate counterspeech, a counterspeech with a certain intent may not be sufficient in every scenario. Due to the complex and multifaceted nature of hate speech, utilizing multiple forms of counter-narratives with varying intents may be advantageous in different circumstances. In this paper, we explore intent-conditioned counterspeech generation. At first, we develop IntentCONAN, a diversified intent-specific counterspeech dataset with 6831 counterspeeches conditioned on five intents, i.e., informative, denouncing, question, positive, and humour. Subsequently, we propose QUARC, a two-stage framework for intent-conditioned counterspeech generation. QUARC leverages vector-quantized representations learned for each intent category along with PerFuMe, a novel fusion module to incorporate intent-specific inform
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#24565;&#24863;&#30693;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#33021;&#22815;&#26356;&#22909;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13775</link><description>&lt;p&gt;
&#27010;&#24565;&#24863;&#30693;&#35757;&#32451;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Concept-aware Training Improves In-context Learning Ability of Language Models. (arXiv:2305.13775v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#24565;&#24863;&#30693;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#33021;&#22815;&#26356;&#22909;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#22810;&#20010;Transformer&#31995;&#21015;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20102;&#25152;&#35859;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;(ICL)&#65292;&#34920;&#29616;&#20026;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23545;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#20219;&#21153;&#36827;&#34892;&#35843;&#33410;&#26469;&#25913;&#21464;&#33258;&#36523;&#30340;&#21151;&#33021;&#12290;&#20043;&#21069;&#30340;&#19968;&#20123;&#30740;&#31350;&#35748;&#20026;&#65292;ICL&#30340;&#20986;&#29616;&#26159;&#30001;&#20110;&#36807;&#24230;&#21442;&#25968;&#21270;&#25110;&#22810;&#20219;&#21153;&#35757;&#32451;&#35268;&#27169;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#19968;&#20123;&#29702;&#35770;&#30740;&#31350;&#35748;&#20026;&#65292;ICL&#30340;&#20986;&#29616;&#26159;&#30001;&#20855;&#20307;&#30340;&#35757;&#32451;&#25968;&#25454;&#23646;&#24615;&#24341;&#36215;&#30340;&#65292;&#24182;&#22312;&#23567;&#35268;&#27169;&#30340;&#20223;&#30495;&#29615;&#22659;&#20013;&#21019;&#24314;&#20102;&#21151;&#33021;&#24615;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22120;&#12290;&#20511;&#37492;&#25968;&#25454;&#23646;&#24615;&#39537;&#21160;ICL&#30340;&#26368;&#26032;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#21019;&#24314;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#26500;&#24314;&#35757;&#32451;&#22330;&#26223;&#65292;&#20351;&#24471;&#27169;&#22411;&#25429;&#25417;&#21040;&#31867;&#27604;&#24605;&#32500;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30340;&#27010;&#24565;&#24863;&#30693;&#35757;&#32451; (CoAT) &#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#32467;&#26524;&#65292;&#20351;&#29992;CoAT&#35757;&#32451;&#24471;&#21040;&#30340;&#20855;&#26377;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent language models (LMs) of Transformers family exhibit so-called in-context learning (ICL) ability, manifested in the LMs' ability to modulate their function by a task described in a natural language input. Previous work curating these models assumes that ICL emerges from vast over-parametrization or the scale of multi-task training. However, a complementary branch of recent theoretical work attributes ICL emergence to specific properties of training data and creates functional in-context learners in small-scale, synthetic settings.  Inspired by recent findings on data properties driving the emergence of ICL, we propose a method to create LMs able to better utilize the in-context information, by constructing training scenarios where it is beneficial for the LM to capture the analogical reasoning concepts. We measure that data sampling of Concept-aware Training (CoAT) consistently improves models' reasoning ability. As a result, the in-context learners trained with CoAT on onl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#39064;&#39537;&#21160;&#30340;&#36828;&#31243;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#21033;&#29992;&#39046;&#22495;&#20869;&#25968;&#25454;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#31687;&#31456;&#35757;&#32451;&#25968;&#25454;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#31687;&#31456;&#20998;&#26512;&#24615;&#33021;&#65292;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13755</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#39064;&#39537;&#21160;&#30340;&#36828;&#31243;&#30417;&#30563;&#26694;&#26550;&#23454;&#29616;&#23439;&#35266;&#23618;&#38754;&#30340;&#31687;&#31456;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Topic-driven Distant Supervision Framework for Macro-level Discourse Parsing. (arXiv:2305.13755v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#39064;&#39537;&#21160;&#30340;&#36828;&#31243;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#21033;&#29992;&#39046;&#22495;&#20869;&#25968;&#25454;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#31687;&#31456;&#35757;&#32451;&#25968;&#25454;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#31687;&#31456;&#20998;&#26512;&#24615;&#33021;&#65292;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31687;&#31456;&#20998;&#26512;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20854;&#30446;&#30340;&#26159;&#20998;&#26512;&#25991;&#26412;&#30340;&#20869;&#37096;&#20462;&#36766;&#32467;&#26500;&#12290;&#23613;&#31649;&#31070;&#32463;&#27169;&#22411;&#23384;&#22312;&#36817;&#26399;&#30340;&#36827;&#23637;&#65292;&#20294;&#32570;&#20047;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#35821;&#26009;&#24211;&#29992;&#20110;&#35757;&#32451;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38556;&#30861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#26469;&#20811;&#26381;&#27492;&#38480;&#21046;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65288;&#20363;&#22914;&#24773;&#24863;&#26497;&#24615;&#12289;&#27880;&#24847;&#21147;&#30697;&#38453;&#21644;&#20998;&#21106;&#27010;&#29575;&#65289;&#30340;&#32467;&#26524;&#26469;&#35299;&#26512;&#31687;&#31456;&#26641;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#27809;&#26377;&#32771;&#34385;&#39046;&#22495;&#20869;&#22806;&#20219;&#21153;&#30340;&#24046;&#24322;&#65292;&#23548;&#33268;&#25928;&#26524;&#36739;&#24046;&#24182;&#19988;&#19981;&#33021;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;&#39046;&#22495;&#20869;&#25968;&#25454;&#36827;&#19968;&#27493;&#25552;&#39640;&#25928;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36828;&#31243;&#30417;&#30563;&#26694;&#26550;&#65292;&#23558;&#20027;&#39064;&#32467;&#26500;&#21644;&#20462;&#36766;&#32467;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#21033;&#29992;&#36215;&#26469;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#36828;&#31243;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#36716;&#31227;&#23398;&#20064;&#21644;&#24072;&#29983;&#27169;&#22411;&#65292;&#29992;&#20110;&#37325;&#22797;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#39046;&#22495;&#20869;&#31687;&#31456;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25552;&#35758;&#26694;&#26550;&#22312;&#35201;&#27714;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discourse parsing, the task of analyzing the internal rhetorical structure of texts, is a challenging problem in natural language processing. Despite the recent advances in neural models, the lack of large-scale, high-quality corpora for training remains a major obstacle. Recent studies have attempted to overcome this limitation by using distant supervision, which utilizes results from other NLP tasks (e.g., sentiment polarity, attention matrix, and segmentation probability) to parse discourse trees. However, these methods do not take into account the differences between in-domain and out-of-domain tasks, resulting in lower performance and inability to leverage the high-quality in-domain data for further improvement. To address these issues, we propose a distant supervision framework that leverages the relations between topic structure and rhetorical structure. Specifically, we propose two distantly supervised methods, based on transfer learning and the teacher-student model, that narr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#19978;&#19979;&#25991;&#24863;&#30693;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26356;&#20026;&#30495;&#23454;&#30340;&#25991;&#26723;&#32423;&#32763;&#35793;&#35774;&#32622;&#65292;&#27573;&#33853;&#32423;&#32763;&#35793;(para2para)&#65292;&#20197;&#21450;&#25910;&#38598;&#20102;&#19968;&#20221;&#26032;&#30340;&#20013;&#33521;&#23567;&#35828;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.13751</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#24863;&#30693;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Challenges in Context-Aware Neural Machine Translation. (arXiv:2305.13751v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#19978;&#19979;&#25991;&#24863;&#30693;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26356;&#20026;&#30495;&#23454;&#30340;&#25991;&#26723;&#32423;&#32763;&#35793;&#35774;&#32622;&#65292;&#27573;&#33853;&#32423;&#32763;&#35793;(para2para)&#65292;&#20197;&#21450;&#25910;&#38598;&#20102;&#19968;&#20221;&#26032;&#30340;&#20013;&#33521;&#23567;&#35828;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#24863;&#30693;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#28041;&#21450;&#21033;&#29992;&#21477;&#23376;&#32423;&#21035;&#19978;&#19979;&#25991;&#20043;&#22806;&#30340;&#20449;&#24687;&#26469;&#35299;&#20915;&#21477;&#38469;&#35805;&#35821;&#20381;&#36182;&#20851;&#31995;&#21644;&#25552;&#39640;&#25991;&#26723;&#32423;&#32763;&#35793;&#36136;&#37327;&#65292;&#24341;&#36215;&#20102;&#35768;&#22810;&#25216;&#26415;&#26041;&#38754;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#30528;&#26126;&#26234;&#30340;&#30452;&#35273;&#65292;&#22823;&#22810;&#25968;&#19978;&#19979;&#25991;&#24863;&#30693;&#32763;&#35793;&#27169;&#22411;&#21482;&#33021;&#26174;&#31034;&#20986;&#36866;&#24230;&#30340;&#25913;&#36827;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38459;&#30861;&#35813;&#39046;&#22495;&#36827;&#23637;&#30340;&#20960;&#20010;&#25361;&#25112;&#65292;&#28041;&#21450;&#35805;&#35821;&#29616;&#35937;&#12289;&#19978;&#19979;&#25991;&#20351;&#29992;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#25991;&#26723;&#32423;&#35780;&#20272;&#31561;&#26041;&#38754;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26356;&#20026;&#30495;&#23454;&#30340;&#25991;&#26723;&#32423;&#32763;&#35793;&#35774;&#32622;&#65292;&#31216;&#20026;&#27573;&#33853;&#32423;&#32763;&#35793;(para2para)&#65292;&#24182;&#25910;&#38598;&#20102;&#19968;&#20221;&#26032;&#30340;&#20013;&#33521;&#23567;&#35828;&#25968;&#25454;&#38598;&#20197;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context-aware neural machine translation involves leveraging information beyond sentence-level context to resolve inter-sentential discourse dependencies and improve document-level translation quality, and has given rise to a number of recent techniques. However, despite well-reasoned intuitions, most context-aware translation models show only modest improvements over sentence-level systems. In this work, we investigate several challenges that impede progress within this field, relating to discourse phenomena, context usage, model architectures, and document-level evaluation. To address these problems, we propose a more realistic setting for document-level translation, called paragraph-to-paragraph (para2para) translation, and collect a new dataset of Chinese-English novels to promote future research.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#24102;&#35299;&#37322;&#30340;&#22522;&#20110;&#30446;&#26631;&#30340;&#32858;&#31867;&#8221;&#65288;GoalEx&#65289;&#30340;&#26032;&#20219;&#21153;&#24418;&#24335;&#65292;&#23427;&#23558;&#30446;&#26631;&#21644;&#35299;&#37322;&#37117;&#34920;&#31034;&#20026;&#33258;&#30001;&#24418;&#24335;&#30340;&#35821;&#35328;&#25551;&#36848;&#12290;&#36890;&#36807;&#23558;&#25688;&#35201;&#31995;&#32479;&#30340;&#27880;&#37322;&#36827;&#34892;&#20998;&#31867;&#26469;&#35828;&#26126;&#30740;&#31350;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#29983;&#25104;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2305.13749</link><description>&lt;p&gt;
&#22522;&#20110;&#30446;&#26631;&#30340;&#21487;&#35299;&#37322;&#32858;&#31867;&#22312;&#35821;&#35328;&#25551;&#36848;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Goal-Driven Explainable Clustering via Language Descriptions. (arXiv:2305.13749v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13749
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#24102;&#35299;&#37322;&#30340;&#22522;&#20110;&#30446;&#26631;&#30340;&#32858;&#31867;&#8221;&#65288;GoalEx&#65289;&#30340;&#26032;&#20219;&#21153;&#24418;&#24335;&#65292;&#23427;&#23558;&#30446;&#26631;&#21644;&#35299;&#37322;&#37117;&#34920;&#31034;&#20026;&#33258;&#30001;&#24418;&#24335;&#30340;&#35821;&#35328;&#25551;&#36848;&#12290;&#36890;&#36807;&#23558;&#25688;&#35201;&#31995;&#32479;&#30340;&#27880;&#37322;&#36827;&#34892;&#20998;&#31867;&#26469;&#35828;&#26126;&#30740;&#31350;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#29983;&#25104;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#32858;&#31867;&#24191;&#27867;&#29992;&#20110;&#25506;&#32034;&#22823;&#22411;&#35821;&#26009;&#24211;&#65292;&#20294;&#29616;&#26377;&#34920;&#36848;&#26082;&#19981;&#32771;&#34385;&#29992;&#25143;&#30340;&#30446;&#26631;&#65292;&#20063;&#19981;&#35299;&#37322;&#32858;&#31867;&#30340;&#21547;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#24418;&#24335;&#8212;&#8212;&#24102;&#35299;&#37322;&#30340;&#22522;&#20110;&#30446;&#26631;&#30340;&#32858;&#31867;&#65288;GoalEx&#65289;&#65292;&#23427;&#23558;&#30446;&#26631;&#21644;&#35299;&#37322;&#37117;&#34920;&#31034;&#20026;&#33258;&#30001;&#24418;&#24335;&#30340;&#35821;&#35328;&#25551;&#36848;&#12290;&#23545;&#20110;&#19968;&#20010;&#24635;&#32467;&#31995;&#32479;&#25152;&#29359;&#30340;&#38169;&#35823;&#36827;&#34892;&#20998;&#31867;&#65292;GoalEx&#30340;&#36755;&#20837;&#26159;&#19968;&#20010;&#27880;&#37322;&#32773;&#20026;&#31995;&#32479;&#29983;&#25104;&#30340;&#25688;&#35201;&#25776;&#20889;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#21644;&#30446;&#26631;&#25551;&#36848;&#8220;&#26681;&#25454;&#27880;&#37322;&#32773;&#35748;&#20026;&#25688;&#35201;&#19981;&#23436;&#32654;&#30340;&#21407;&#22240;&#23545;&#27880;&#37322;&#36827;&#34892;&#20998;&#31867;&#8221;;&#36755;&#20986;&#26159;&#27599;&#20010;&#20855;&#26377;&#35299;&#37322;&#30340;&#25991;&#26412;&#32858;&#31867;(&#8220;&#27492;&#32858;&#31867;&#25552;&#21040;&#25688;&#35201;&#32570;&#23569;&#37325;&#35201;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#8220;)&#65292;&#36825;&#20123;&#32858;&#31867;&#19982;&#30446;&#26631;&#30456;&#20851;&#65292;&#24182;&#20934;&#30830;&#35299;&#37322;&#21738;&#20123;&#27880;&#37322;&#24212;&#35813;(&#19981;&#24212;&#35813;)&#23646;&#20110;&#19968;&#20010;&#32858;&#31867;&#12290;&#20026;&#20102;&#35299;&#20915;GoalEx&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#8220; [&#25968;&#25454;&#38598;&#23376;&#38598;]+[&#30446;&#26631;]+&#22836;&#33041;&#39118;&#26292;&#19968;&#20010;&#20195;&#34920;&#32858;&#31867;&#30340;&#35299;&#37322;&#21015;&#34920;&#8221;&#65292;&#28982;&#21518;&#20998;&#31867;&#21738;&#20123;&#35299;&#37322;&#23646;&#20110;&#27599;&#20010;&#32858;&#31867;&#12290;&#23454;&#39564;&#22312;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#65292;&#21253;&#25324;&#27719;&#24635;&#21453;&#39304;&#12289;&#26032;&#38395;&#25991;&#31456;&#12289;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#12289;&#31185;&#23398;&#25991;&#31456;&#21644;&#25209;&#35780;&#35780;&#35770;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#29983;&#25104;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised clustering is widely used to explore large corpora, but existing formulations neither consider the users' goals nor explain clusters' meanings. We propose a new task formulation, "Goal-Driven Clustering with Explanations" (GoalEx), which represents both the goal and the explanations as free-form language descriptions. For example, to categorize the errors made by a summarization system, the input to GoalEx is a corpus of annotator-written comments for system-generated summaries and a goal description "cluster the comments based on why the annotators think the summary is imperfect.''; the outputs are text clusters each with an explanation ("this cluster mentions that the summary misses important context information."), which relates to the goal and precisely explain which comments should (not) belong to a cluster. To tackle GoalEx, we prompt a language model with "[corpus subset] + [goal] + Brainstorm a list of explanations each representing a cluster."; then we classify wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;552&#20010;&#27861;&#35821;-&#33521;&#35821;&#35805;&#35821;&#30340;&#24179;&#34892;&#26102;&#24577;&#27979;&#35797;&#38598;&#21644;&#30456;&#24212;&#30340;&#26102;&#24577;&#39044;&#27979;&#20934;&#30830;&#29575;&#22522;&#20934;&#65292;&#36825;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#39318;&#27425;&#20174;&#35821;&#35328;&#23398;&#35282;&#24230;&#34913;&#37327;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#26102;&#24577;&#19968;&#33268;&#24615;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13740</link><description>&lt;p&gt;
TeCS:&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#26102;&#24577;&#19968;&#33268;&#24615;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
TeCS: A Dataset and Benchmark for Tense Consistency of Machine Translation. (arXiv:2305.13740v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;552&#20010;&#27861;&#35821;-&#33521;&#35821;&#35805;&#35821;&#30340;&#24179;&#34892;&#26102;&#24577;&#27979;&#35797;&#38598;&#21644;&#30456;&#24212;&#30340;&#26102;&#24577;&#39044;&#27979;&#20934;&#30830;&#29575;&#22522;&#20934;&#65292;&#36825;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#39318;&#27425;&#20174;&#35821;&#35328;&#23398;&#35282;&#24230;&#34913;&#37327;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#26102;&#24577;&#19968;&#33268;&#24615;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#20013;&#32463;&#24120;&#20986;&#29616;&#26102;&#24577;&#19981;&#19968;&#33268;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#20174;&#35821;&#35328;&#23398;&#35282;&#24230;&#35780;&#20272;&#27169;&#22411;&#26102;&#24577;&#39044;&#27979;&#30340;&#25484;&#25569;&#31243;&#24230;&#30340;&#26631;&#20934;&#24456;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#34892;&#26102;&#24577;&#27979;&#35797;&#38598;&#65292;&#21253;&#21547;&#27861;&#35821;-&#33521;&#35821;552&#20010;&#35805;&#35821;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#30456;&#24212;&#30340;&#22522;&#20934;&#65292;&#21363;&#26102;&#24577;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;&#20511;&#21161;&#26102;&#24577;&#27979;&#35797;&#38598;&#21644;&#22522;&#20934;&#65292;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#39318;&#27425;&#34913;&#37327;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#26102;&#24577;&#19968;&#33268;&#24615;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tense inconsistency frequently occurs in machine translation. However, there are few criteria to assess the model's mastery of tense prediction from a linguistic perspective. In this paper, we present a parallel tense test set, containing French-English 552 utterances. We also introduce a corresponding benchmark, tense prediction accuracy. With the tense test set and the benchmark, researchers are able to measure the tense consistency performance of machine translation systems for the first time.
&lt;/p&gt;</description></item><item><title>i-Code Studio&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#12289;&#28789;&#27963;&#21644;&#21487;&#32452;&#21512;&#30340;&#35774;&#32622;&#65292;&#21487;&#20197;&#20351;&#24320;&#21457;&#20154;&#21592;&#24555;&#36895;&#36731;&#26494;&#22320;&#32452;&#21512;&#26368;&#20808;&#36827;&#30340;&#26381;&#21153;&#21644;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#26159;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#19968;&#31181;&#37325;&#35201;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13738</link><description>&lt;p&gt;
i-Code Studio&#65306;&#19968;&#31181;&#21487;&#37197;&#32622;&#21644;&#21487;&#32452;&#21512;&#30340;&#32508;&#21512;AI&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
i-Code Studio: A Configurable and Composable Framework for Integrative AI. (arXiv:2305.13738v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13738
&lt;/p&gt;
&lt;p&gt;
i-Code Studio&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#12289;&#28789;&#27963;&#21644;&#21487;&#32452;&#21512;&#30340;&#35774;&#32622;&#65292;&#21487;&#20197;&#20351;&#24320;&#21457;&#20154;&#21592;&#24555;&#36895;&#36731;&#26494;&#22320;&#32452;&#21512;&#26368;&#20808;&#36827;&#30340;&#26381;&#21153;&#21644;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#26159;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#19968;&#31181;&#37325;&#35201;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#38656;&#35201;&#20840;&#38754;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#21487;&#20197;&#28085;&#30422;&#19981;&#21516;&#30340;&#27169;&#24577;&#21644;&#21151;&#33021;&#12290;&#32508;&#21512;AI&#26159;&#23454;&#29616;AGI&#30340;&#19968;&#31181;&#37325;&#35201;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#22810;&#20010;&#27169;&#22411;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#31181;&#28789;&#27963;&#21644;&#21487;&#32452;&#21512;&#30340;&#24179;&#21488;&#26469;&#20419;&#36827;&#27169;&#22411;&#30340;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#32452;&#21512;&#21644;&#21327;&#35843;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;i-Code Studio&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#37197;&#32622;&#21644;&#21487;&#32452;&#21512;&#30340;&#32508;&#21512;AI&#26694;&#26550;&#12290; i-Code Studio&#20197;&#26080;fine-tuning&#26041;&#24335;&#21327;&#35843;&#22810;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#25191;&#34892;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290; i-Code Studio&#19981;&#20165;&#25552;&#20379;&#31616;&#21333;&#30340;&#27169;&#22411;&#32452;&#21512;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#12289;&#28789;&#27963;&#21644;&#21487;&#32452;&#21512;&#30340;&#35774;&#32622;&#65292;&#20351;&#24320;&#21457;&#20154;&#21592;&#33021;&#22815;&#24555;&#36895;&#36731;&#26494;&#22320;&#32452;&#21512;&#26368;&#20808;&#36827;&#30340;&#26381;&#21153;&#21644;&#25216;&#26415;&#65292;&#20197;&#28385;&#36275;&#20182;&#20204;&#30340;&#29305;&#23450;&#35201;&#27714;&#12290;i-Code Studio&#22312;&#22810;&#31181;&#38646;-shot&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial General Intelligence (AGI) requires comprehensive understanding and generation capabilities for a variety of tasks spanning different modalities and functionalities. Integrative AI is one important direction to approach AGI, through combining multiple models to tackle complex multimodal tasks. However, there is a lack of a flexible and composable platform to facilitate efficient and effective model composition and coordination. In this paper, we propose the i-Code Studio, a configurable and composable framework for Integrative AI. The i-Code Studio orchestrates multiple pre-trained models in a finetuning-free fashion to conduct complex multimodal tasks. Instead of simple model composition, the i-Code Studio provides an integrative, flexible, and composable setting for developers to quickly and easily compose cutting-edge services and technologies tailored to their specific requirements. The i-Code Studio achieves impressive results on a variety of zero-shot multimodal tasks,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#21453;&#39304;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#20154;&#21147;&#25104;&#26412;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#39044;&#20808;&#23545;&#40784;&#30340;LLMs&#12290;&#20854;&#20013;&#65292;&#36890;&#36807;&#23545;&#23610;&#23544;&#21644;&#25552;&#31034;&#31561;&#19981;&#21516;&#22240;&#32032;&#30340;&#26222;&#36890; LLMS&#30340;&#21709;&#24212;&#36827;&#34892;&#22870;&#21169;&#24314;&#27169;&#65292;&#26469;&#27169;&#25311;&#39640;&#36136;&#37327;&#30340;&#31034;&#33539;&#26469;&#35757;&#32451;&#30417;&#30563;&#31574;&#30053;&#65292;&#24182;&#36827;&#19968;&#27493;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13735</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#21453;&#39304;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models through Synthetic Feedback. (arXiv:2305.13735v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13735
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#21453;&#39304;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#20154;&#21147;&#25104;&#26412;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#39044;&#20808;&#23545;&#40784;&#30340;LLMs&#12290;&#20854;&#20013;&#65292;&#36890;&#36807;&#23545;&#23610;&#23544;&#21644;&#25552;&#31034;&#31561;&#19981;&#21516;&#22240;&#32032;&#30340;&#26222;&#36890; LLMS&#30340;&#21709;&#24212;&#36827;&#34892;&#22870;&#21169;&#24314;&#27169;&#65292;&#26469;&#27169;&#25311;&#39640;&#36136;&#37327;&#30340;&#31034;&#33539;&#26469;&#35757;&#32451;&#30417;&#30563;&#31574;&#30053;&#65292;&#24182;&#36827;&#19968;&#27493;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#25552;&#20379;&#22797;&#26434;&#30340;LLMs&#25511;&#21046;&#65292;&#20363;&#22914;&#20351;&#23427;&#20204;&#25353;&#29031;&#29305;&#23450;&#30340;&#25351;&#20196;&#25805;&#20316;&#32780;&#19981;&#20250;&#20135;&#29983;&#26377;&#23475;&#21453;&#24212;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#31867;&#31034;&#33539;&#21644;&#21453;&#39304;&#12290;&#26368;&#36817;&#65292;&#24320;&#28304;&#27169;&#22411;&#35797;&#22270;&#36890;&#36807;&#25552;&#28860;&#26469;&#33258;&#24050;&#23545;&#40784;&#30340;LLMs&#65288;&#22914;InstructGPT&#25110;ChatGPT&#65289;&#30340;&#25968;&#25454;&#26469;&#22797;&#21046;&#23545;&#40784;&#23398;&#20064;&#36807;&#31243;&#12290;&#34429;&#28982;&#36825;&#20010;&#36807;&#31243;&#20943;&#23569;&#20102;&#20154;&#21147;&#25104;&#26412;&#65292;&#20294;&#26159;&#26500;&#24314;&#36825;&#20123;&#25968;&#25454;&#38598;&#23545;&#25945;&#24072;&#27169;&#22411;&#30340;&#20381;&#36182;&#24615;&#24456;&#39640;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#40784;&#23398;&#20064;&#26694;&#26550;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#20154;&#31867;&#21171;&#21160;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#39044;&#20808;&#23545;&#40784;&#30340;LLMs&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#23567;&#21644;&#25552;&#31034;&#31561;&#19981;&#21516;&#22240;&#32032;&#30340;&#26222;&#36890;LLMs&#30340;&#21709;&#24212;&#36827;&#34892;&#21512;&#25104;&#21453;&#39304;&#30340;&#22870;&#21169;&#24314;&#27169;(RM)&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;RM&#27169;&#25311;&#39640;&#36136;&#37327;&#30340;&#31034;&#33539;&#26469;&#35757;&#32451;&#30417;&#30563;&#31574;&#30053;&#65292;&#24182;&#36827;&#19968;&#27493;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models (LLMs) to human values has become increasingly important as it enables sophisticated steering of LLMs, e.g., making them follow given instructions while keeping them less toxic. However, it requires a significant amount of human demonstrations and feedback. Recently, open-sourced models have attempted to replicate the alignment learning process by distilling data from already aligned LLMs like InstructGPT or ChatGPT. While this process reduces human efforts, constructing these datasets has a heavy dependency on the teacher models. In this work, we propose a novel framework for alignment learning with almost no human labor and no dependency on pre-aligned LLMs. First, we perform reward modeling (RM) with synthetic feedback by contrasting responses from vanilla LLMs with various sizes and prompts. Then, we use the RM for simulating high-quality demonstrations to train a supervised policy and for further optimizing the model with reinforcement learning. Our 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#21517;&#20026;INDust&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#20110;&#21253;&#21547;&#38169;&#35823;&#20449;&#24687;&#30340;&#25351;&#20196;&#30340;&#25269;&#25239;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#30340;LLMs&#24456;&#23481;&#26131;&#34987;&#27450;&#39575;&#65292;&#22240;&#27492;&#37319;&#29992;&#33258;&#25105;&#25209;&#21028;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#28608;&#21169;LLMs&#19981;&#20165;&#23545;&#33258;&#24049;&#36827;&#34892;&#25209;&#35780;&#65292;&#32780;&#19988;&#23545;&#29992;&#25143;&#36827;&#34892;&#25209;&#35780;&#12290;</title><link>http://arxiv.org/abs/2305.13733</link><description>&lt;p&gt;
&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#25209;&#21028;&#25552;&#31034;&#29992;&#20110;&#24402;&#32435;&#25945;&#23398;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
Self-Critique Prompting with Large Language Models for Inductive Instructions. (arXiv:2305.13733v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#21517;&#20026;INDust&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#20110;&#21253;&#21547;&#38169;&#35823;&#20449;&#24687;&#30340;&#25351;&#20196;&#30340;&#25269;&#25239;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#30340;LLMs&#24456;&#23481;&#26131;&#34987;&#27450;&#39575;&#65292;&#22240;&#27492;&#37319;&#29992;&#33258;&#25105;&#25209;&#21028;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#28608;&#21169;LLMs&#19981;&#20165;&#23545;&#33258;&#24049;&#36827;&#34892;&#25209;&#35780;&#65292;&#32780;&#19988;&#23545;&#29992;&#25143;&#36827;&#34892;&#25209;&#35780;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30340;&#24037;&#20316;&#37117;&#34987;&#25552;&#20986;&#26469;&#25552;&#39640;&#25110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23454;&#29616;&#29992;&#25143;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290; &#28982;&#32780;&#65292;&#23427;&#20204;&#24573;&#30053;&#20102;&#29992;&#25143;&#36755;&#20837;&#21487;&#33021;&#22240;&#29992;&#25143;&#30340;&#38169;&#35823;&#20449;&#24565;&#25110;&#24694;&#24847;&#24847;&#22270;&#32780;&#22266;&#26377;&#22320;&#21253;&#21547;&#19981;&#27491;&#30830;&#30340;&#20449;&#24687;&#30340;&#21487;&#33021;&#24615;&#12290; &#30450;&#30446;&#22320;&#36981;&#24490;&#29992;&#25143;&#30340;&#38169;&#35823;&#20869;&#23481;&#23558;&#23548;&#33268;&#27450;&#39575;&#21644;&#20260;&#23475;&#12290; &#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#65292;&#30001;&#24402;&#32435;&#25351;&#20196;&#65288;INDust&#65289;&#32452;&#25104;&#65292;&#20197;&#35780;&#20272;LLMs&#26159;&#21542;&#33021;&#22815;&#25269;&#25239;&#36825;&#20123;&#25351;&#20196;&#12290; INDust&#21253;&#25324;&#19977;&#20010;&#31867;&#21035;&#30340;15K&#25351;&#20196;&#65306;&#20107;&#23454;&#26680;&#26597;&#25351;&#20196;&#65292;&#22522;&#20110;&#38169;&#35823;&#21069;&#25552;&#30340;&#38382;&#39064;&#21644;&#22522;&#20110;&#38169;&#35823;&#21069;&#25552;&#30340;&#21019;&#24847;&#25351;&#20196;&#12290; &#25105;&#20204;&#23545;&#20960;&#20010;&#24378;&#22823;&#30340;LLMs&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;LLMs&#21487;&#20197;&#36731;&#26131;&#22320;&#34987;INDust&#27450;&#39575;&#65292;&#29983;&#25104;&#35823;&#23548;&#24615;&#21644;&#24694;&#24847;&#30340;&#38472;&#36848;&#12290; &#22240;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#33258;&#25105;&#25209;&#21028;&#25552;&#31034;&#65292;&#20197;&#28608;&#21169;LLMs&#19981;&#20165;&#20687;&#20197;&#21069;&#30340;&#24037;&#20316;&#37027;&#26679;&#23545;&#33258;&#24049;&#36827;&#34892;&#25209;&#35780;&#65292;&#32780;&#19988;&#23545;&#29992;&#25143;&#36827;&#34892;&#25209;&#35780;&#65292;&#23427;&#23637;&#31034;&#20986;r&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous works are proposed to improve or evaluate the capabilities of Large language models (LLMs) to fulfill user instructions. However, they neglect the possibility that user inputs may inherently contain incorrect information due to users' false beliefs or malicious intents. In this way, blindly adhering to users' false content will cause deception and harm. To address this problem, we propose a challenging benchmark consisting of Inductive Instructions (INDust) to evaluate whether LLMs could resist these instructions. The INDust includes 15K instructions across three categories: Fact-Checking Instructions, Questions based on False Premises, and Creative Instructions based on False Premises. Our experiments on several strong LLMs reveal that current LLMs can be easily deceived by INDust into generating misleading and malicious statements. Hence we employ Self-Critique prompting to encourage LLMs to not only critique themselves like in previous works but also the users, which show r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#21463;&#32422;&#26463;&#25552;&#31034;&#29983;&#25104;&#65288;Co-Prompt&#65289;&#65292;&#36890;&#36807;&#20272;&#31639;&#26368;&#20339;&#25490;&#24207;&#26469;&#24341;&#23548; PLM &#29983;&#25104;&#30340;&#25991;&#26412;&#26397;&#21521;&#26368;&#20248;&#25552;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Co-Prompt &#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#37325;&#25490;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13729</link><description>&lt;p&gt;
&#22522;&#20110;&#32422;&#26463;&#29983;&#25104;&#30340;&#31163;&#25955;&#25552;&#31034;&#20248;&#21270;&#22312;&#38646;&#26679;&#26412;&#37325;&#25490;&#22120;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker. (arXiv:2305.13729v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#21463;&#32422;&#26463;&#25552;&#31034;&#29983;&#25104;&#65288;Co-Prompt&#65289;&#65292;&#36890;&#36807;&#20272;&#31639;&#26368;&#20339;&#25490;&#24207;&#26469;&#24341;&#23548; PLM &#29983;&#25104;&#30340;&#25991;&#26412;&#26397;&#21521;&#26368;&#20248;&#25552;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Co-Prompt &#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#37325;&#25490;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#25490;&#22120;&#26159;&#22312;&#32473;&#23450;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#35780;&#20998;&#19979;&#23545;&#26816;&#32034;&#30340;&#25991;&#26723;&#36827;&#34892;&#25490;&#24207;&#30340;&#26041;&#27861;&#65292;&#22312;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#20851;&#27880;&#12290;&#19982;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#19981;&#21516;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#20855;&#26377;&#20248;&#24322;&#32467;&#26524;&#30340;&#38646;&#26679;&#26412;&#37325;&#25490;&#22120;&#12290;&#34429;&#28982; LLM &#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#25552;&#31034;&#35821;&#65292;&#20294;&#38646;&#26679;&#26412;&#37325;&#25490;&#22120;&#25552;&#31034;&#35821;&#30340;&#24433;&#21709;&#21644;&#20248;&#21270;&#23578;&#26410;&#24471;&#21040;&#25506;&#31350;&#12290;&#38500;&#20102;&#24378;&#35843;&#20248;&#21270;&#23545;&#38646;&#26679;&#26412;&#37325;&#25490;&#22120;&#30340;&#24433;&#21709;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#21463;&#32422;&#26463;&#25552;&#31034;&#29983;&#25104;&#65288;Co-Prompt&#65289;&#65292;&#36890;&#36807;&#20272;&#31639;&#26368;&#20339;&#25490;&#24207;&#26469;&#24341;&#23548; PLM &#29983;&#25104;&#30340;&#25991;&#26412;&#26397;&#21521;&#26368;&#20248;&#25552;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Co-Prompt &#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#37325;&#25490;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;Co-Prompt &#29983;&#25104;&#30340;&#25552;&#31034;&#26356;&#26131;&#20110;&#20154;&#31867;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Re-rankers, which order retrieved documents with respect to the relevance score on the given query, have gained attention for the information retrieval (IR) task. Rather than fine-tuning the pre-trained language model (PLM), the large-scale language model (LLM) is utilized as a zero-shot re-ranker with excellent results. While LLM is highly dependent on the prompts, the impact and the optimization of the prompts for the zero-shot re-ranker are not explored yet. Along with highlighting the impact of optimization on the zero-shot re-ranker, we propose a novel discrete prompt optimization method, Constrained Prompt generation (Co-Prompt), with the metric estimating the optimum for re-ranking. Co-Prompt guides the generated texts from PLM toward optimal prompts based on the metric without parameter update. The experimental results demonstrate that Co-Prompt leads to outstanding re-ranking performance against the baselines. Also, Co-Prompt generates more interpretable prompts for humans aga
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20197;&#26816;&#32034;&#26041;&#24335;&#36827;&#34892;&#30340;&#23545;&#35805;&#24335;&#25512;&#33616;&#26041;&#27861;&#65292;&#23558;&#23545;&#35805;&#34920;&#31034;&#20026;&#26597;&#35810;&#65292;&#23558;&#29289;&#21697;&#34920;&#31034;&#20026;&#24453;&#26816;&#32034;&#30340;&#25991;&#26723;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;BM25&#30340;&#26816;&#32034;&#22120;&#36827;&#34892;&#25512;&#33616;&#12290;&#30456;&#27604;&#20110;&#20351;&#29992;&#22797;&#26434;&#30340;&#22806;&#37096;&#30693;&#35782;&#30340;&#22522;&#20934;&#32447;&#65292;&#35813;&#26041;&#27861;&#31616;&#21333;&#19988;&#26356;&#20855;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13725</link><description>&lt;p&gt;
&#20197;&#26816;&#32034;&#26041;&#24335;&#36827;&#34892;&#30340;&#23545;&#35805;&#24335;&#25512;&#33616;&#65306;&#19968;&#20010;&#31616;&#21333;&#12289;&#24378;&#22823;&#30340;&#22522;&#20934;&#32447;
&lt;/p&gt;
&lt;p&gt;
Conversational Recommendation as Retrieval: A Simple, Strong Baseline. (arXiv:2305.13725v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20197;&#26816;&#32034;&#26041;&#24335;&#36827;&#34892;&#30340;&#23545;&#35805;&#24335;&#25512;&#33616;&#26041;&#27861;&#65292;&#23558;&#23545;&#35805;&#34920;&#31034;&#20026;&#26597;&#35810;&#65292;&#23558;&#29289;&#21697;&#34920;&#31034;&#20026;&#24453;&#26816;&#32034;&#30340;&#25991;&#26723;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;BM25&#30340;&#26816;&#32034;&#22120;&#36827;&#34892;&#25512;&#33616;&#12290;&#30456;&#27604;&#20110;&#20351;&#29992;&#22797;&#26434;&#30340;&#22806;&#37096;&#30693;&#35782;&#30340;&#22522;&#20934;&#32447;&#65292;&#35813;&#26041;&#27861;&#31616;&#21333;&#19988;&#26356;&#20855;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#21521;&#29992;&#25143;&#25512;&#33616;&#21512;&#36866;&#30340;&#29289;&#21697;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#24182;&#27809;&#26377;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#23545;&#35805;&#25552;&#20379;&#30340;&#20449;&#21495;&#12290;&#23427;&#20204;&#20005;&#37325;&#20381;&#36182;&#20110;&#26174;&#24335;&#30340;&#22806;&#37096;&#30693;&#35782;&#24211;&#65288;&#20363;&#22914;&#30693;&#35782;&#22270;&#35889;&#65289;&#26469;&#22686;&#24378;&#27169;&#22411;&#23545;&#29289;&#21697;&#21644;&#23646;&#24615;&#30340;&#29702;&#35299;&#65292;&#36825;&#24456;&#38590;&#25193;&#23637;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#39118;&#26684;&#30340;&#26041;&#27861;&#65292;&#23558;&#23545;&#35805;&#34920;&#31034;&#20026;&#26597;&#35810;&#65292;&#23558;&#29289;&#21697;&#34920;&#31034;&#20026;&#24453;&#26816;&#32034;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#29992;&#20110;&#26816;&#32034;&#30340;&#25991;&#26723;&#34920;&#31034;&#65292;&#20351;&#29992;&#35757;&#32451;&#38598;&#20013;&#30340;&#23545;&#35805;&#20449;&#24687;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#22522;&#20110;BM25&#30340;&#26816;&#32034;&#22120;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20219;&#21153;&#35774;&#32622;&#22312;&#27969;&#34892;&#30340;CRS&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;&#22797;&#26434;&#30340;&#12289;&#20351;&#29992;&#22797;&#26434;&#22806;&#37096;&#30693;&#35782;&#30340;&#22522;&#20934;&#32447;&#30456;&#27604;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#24314;&#27169;&#21644;&#25968;&#25454;&#22686;&#24378;&#26469;&#23545;&#25239;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#25512;&#33616;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational recommendation systems (CRS) aim to recommend suitable items to users through natural language conversation. However, most CRS approaches do not effectively utilize the signal provided by these conversations. They rely heavily on explicit external knowledge e.g., knowledge graphs to augment the models' understanding of the items and attributes, which is quite hard to scale. To alleviate this, we propose an alternative information retrieval (IR)-styled approach to the CRS item recommendation task, where we represent conversations as queries and items as documents to be retrieved. We expand the document representation used for retrieval with conversations from the training set. With a simple BM25-based retriever, we show that our task formulation compares favorably with much more complex baselines using complex external knowledge on a popular CRS benchmark. We demonstrate further improvements using user-centric modeling and data augmentation to counter the cold start probl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992; ChatGPT &#25552;&#21462;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#23454;&#29616;&#20855;&#26377;&#24773;&#24863;&#30340;&#23545;&#35805;&#35821;&#38899;&#21512;&#25104;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992; ChatGPT &#34893;&#29983;&#30340;&#19978;&#19979;&#25991;&#21333;&#35789;&#23884;&#20837;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#30456;&#24403;&#20110;&#20351;&#29992;&#24773;&#24863;&#26631;&#31614;&#25110;&#31070;&#32463;&#32593;&#32476;&#34893;&#29983;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13724</link><description>&lt;p&gt;
ChatGPT-EDSS: &#22522;&#20110; ChatGPT &#34893;&#29983;&#20986;&#30340;&#19978;&#19979;&#25991;&#35789;&#23884;&#20837;&#26469;&#35757;&#32451;&#30340;&#20855;&#26377;&#24773;&#24863;&#30340;&#23545;&#35805;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ChatGPT-EDSS: Empathetic Dialogue Speech Synthesis Trained from ChatGPT-derived Context Word Embeddings. (arXiv:2305.13724v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992; ChatGPT &#25552;&#21462;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#23454;&#29616;&#20855;&#26377;&#24773;&#24863;&#30340;&#23545;&#35805;&#35821;&#38899;&#21512;&#25104;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992; ChatGPT &#34893;&#29983;&#30340;&#19978;&#19979;&#25991;&#21333;&#35789;&#23884;&#20837;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#30456;&#24403;&#20110;&#20351;&#29992;&#24773;&#24863;&#26631;&#31614;&#25110;&#31070;&#32463;&#32593;&#32476;&#34893;&#29983;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; ChatGPT-EDSS&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992; ChatGPT &#25277;&#21462;&#23545;&#35805;&#19978;&#19979;&#25991;&#36827;&#34892;&#24773;&#24863;&#23545;&#35805;&#35821;&#38899;&#21512;&#25104;&#65288;EDSS&#65289;&#30340;&#26041;&#27861;&#12290; ChatGPT &#26159;&#19968;&#31181;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#21487;&#20197;&#28145;&#20837;&#29702;&#35299;&#36755;&#20837;&#25552;&#31034;&#30340;&#20869;&#23481;&#21644;&#30446;&#30340;&#65292;&#24182;&#36866;&#24403;&#22320;&#22238;&#24212;&#29992;&#25143;&#30340;&#35831;&#27714;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110; ChatGPT &#30340;&#38405;&#35835;&#29702;&#35299;&#65292;&#24182;&#23558;&#20854;&#24341;&#20837; EDSS&#65292;&#36825;&#26159;&#19968;&#39033;&#21512;&#25104;&#20855;&#26377;&#20849;&#24773;&#23545;&#35805;&#35821;&#38899;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#23558;&#32842;&#22825;&#21382;&#21490;&#35760;&#24405;&#25552;&#20379;&#32473; ChatGPT&#65292;&#24182;&#35201;&#27714;&#20854;&#20026;&#27599;&#34892;&#29983;&#25104;&#34920;&#31034;&#24847;&#22270;&#12289;&#24773;&#24863;&#21644;&#35828;&#35805;&#39118;&#26684;&#30340;&#19977;&#20010;&#21333;&#35789;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;ChatGPT&#34893;&#29983;&#30340;&#19978;&#19979;&#25991;&#21333;&#35789;&#30340;&#23884;&#20837;&#20316;&#20026;&#35843;&#33410;&#29305;&#24449;&#65292;&#35757;&#32451;EDSS&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#19982;&#20351;&#29992;&#24773;&#24863;&#26631;&#31614;&#25110;&#20174;&#32842;&#22825;&#21382;&#21490;&#35760;&#24405;&#20013;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#34893;&#29983;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#30340;&#26041;&#27861;&#30456;&#24403;&#12290;&#34893;&#29983;&#30340; ChatGPT &#19978;&#19979;&#25991;&#20449;&#24687;&#21487;&#22312; https://sarulab-speech.github.io/demo_ChatGPT_EDSS/ &#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose ChatGPT-EDSS, an empathetic dialogue speech synthesis (EDSS) method using ChatGPT for extracting dialogue context. ChatGPT is a chatbot that can deeply understand the content and purpose of an input prompt and appropriately respond to the user's request. We focus on ChatGPT's reading comprehension and introduce it to EDSS, a task of synthesizing speech that can empathize with the interlocutor's emotion. Our method first gives chat history to ChatGPT and asks it to generate three words representing the intention, emotion, and speaking style for each line in the chat. Then, it trains an EDSS model using the embeddings of ChatGPT-derived context words as the conditioning features. The experimental results demonstrate that our method performs comparably to ones using emotion labels or neural network-derived context embeddings learned from chat histories. The collected ChatGPT-derived context information is available at https://sarulab-speech.github.io/demo_ChatGPT_EDSS/.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21483;PromptClass&#30340;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25552;&#31034;&#22686;&#24378;&#23398;&#20064;&#65292;&#29983;&#25104;&#22122;&#22768;&#40065;&#26834;&#24615;&#26356;&#24378;&#30340;&#20266;&#26631;&#31614;&#21644;&#33258;&#25105;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2305.13723</link><description>&lt;p&gt;
PromptClass: &#21033;&#29992;&#25552;&#31034;&#22686;&#24378;&#22122;&#22768;&#40065;&#26834;&#30340;&#33258;&#35757;&#32451;&#65292;&#36827;&#34892;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
PromptClass: Weakly-Supervised Text Classification with Prompting Enhanced Noise-Robust Self-Training. (arXiv:2305.13723v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13723
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21483;PromptClass&#30340;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25552;&#31034;&#22686;&#24378;&#23398;&#20064;&#65292;&#29983;&#25104;&#22122;&#22768;&#40065;&#26834;&#24615;&#26356;&#24378;&#30340;&#20266;&#26631;&#31614;&#21644;&#33258;&#25105;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#30340;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#65292;&#20165;&#20351;&#29992;&#27599;&#20010;&#30446;&#26631;&#31867;&#21035;&#30340;&#26631;&#31614;&#21517;&#20316;&#20026;&#21807;&#19968;&#30340;&#30417;&#30563;&#12290;&#36825;&#31181;&#26041;&#27861;&#30456;&#27604;&#20110;&#23436;&#20840;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#27169;&#22411;&#33021;&#22823;&#22823;&#20943;&#23569;&#20154;&#31867;&#27880;&#37322;&#30340;&#24037;&#20316;&#37327;&#65292;&#22240;&#27492;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#35813;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#26631;&#31614;&#21517;&#26469;&#29983;&#25104;&#20266;&#26631;&#31614;&#65292;&#28982;&#21518;&#29992;&#20110;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21516;&#19968;&#21333;&#35789;&#22312;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#20250;&#26377;&#19981;&#21516;&#30340;&#21547;&#20041;&#65292;&#22240;&#27492;&#20165;&#20165;&#20351;&#29992;&#26631;&#31614;&#21517;&#36827;&#34892;&#21305;&#37197;&#20250;&#23548;&#33268;&#38750;&#24120;&#22024;&#26434;&#30340;&#20266;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#22312;&#20266;&#26631;&#31614;&#29983;&#25104;&#38454;&#27573;&#20135;&#29983;&#30340;&#38169;&#35823;&#23558;&#30452;&#25509;&#20256;&#25773;&#21040;&#20998;&#31867;&#22120;&#35757;&#32451;&#38454;&#27573;&#65292;&#26080;&#27861;&#34987;&#32416;&#27491;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;PromptClass&#65292;&#21253;&#21547;&#20004;&#20010;&#27169;&#22359;&#65306;(1)&#20266;&#26631;&#31614;&#33719;&#21462;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Recently proposed weakly-supervised text classification settings train a classifier using the label name of each target class as the only supervision. Such weakly-supervised settings have been gaining increasing attention since they can largely reduce human annotation efforts compared to fully-supervised and semi-supervised settings. Most existing methods follow the strategy that first uses the label names as static features to generate pseudo labels, which are then used for classifier training. While reasonable, such a commonly adopted framework suffers from two limitations: (1) words can have different meanings in different contexts, so using label names for context-free matching can induce very noisy pseudo labels; and (2) the errors made in the pseudo label generation stage will directly propagate to the classifier training stage without a chance of being corrected. In this paper, we propose a new method, PromptClass, consisting of two modules: (1) a pseudo label acquisition module
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#35758;&#23558;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#37325;&#26500;&#20026;&#30001;&#20363;&#23376;&#24341;&#23548;&#30340;&#31890;&#24230;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#65292;&#20197;&#26368;&#23567;&#21270;&#26381;&#21153;&#20043;&#38388;&#30340;&#20219;&#21153;&#36716;&#31227;&#65292;&#33719;&#24471;&#25345;&#32493;&#30340;&#23398;&#20064;&#25928;&#30410;&#12290;&#36890;&#36807;&#32467;&#21512;&#31616;&#21333;&#30340;&#25345;&#32493;&#23398;&#20064;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13721</link><description>&lt;p&gt;
&#22522;&#20110;&#31034;&#20363;&#24341;&#23548;&#38382;&#31572;&#30340;&#25345;&#32493;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Continual Dialogue State Tracking via Example-Guided Question Answering. (arXiv:2305.13721v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#35758;&#23558;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#37325;&#26500;&#20026;&#30001;&#20363;&#23376;&#24341;&#23548;&#30340;&#31890;&#24230;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#65292;&#20197;&#26368;&#23567;&#21270;&#26381;&#21153;&#20043;&#38388;&#30340;&#20219;&#21153;&#36716;&#31227;&#65292;&#33719;&#24471;&#25345;&#32493;&#30340;&#23398;&#20064;&#25928;&#30410;&#12290;&#36890;&#36807;&#32467;&#21512;&#31616;&#21333;&#30340;&#25345;&#32493;&#23398;&#20064;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#31995;&#32479;&#38656;&#35201;&#19981;&#26029;&#26356;&#26032;&#20197;&#36866;&#24212;&#26032;&#26381;&#21153;&#65292;&#20294;&#26159;&#31616;&#21333;&#22320;&#20351;&#29992;&#26032;&#26381;&#21153;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#20250;&#38477;&#20302;&#20808;&#21069;&#23398;&#20064;&#30340;&#26381;&#21153;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;(DST)&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#20854;&#37325;&#26500;&#20026;&#19968;&#32452;&#30001;&#20363;&#23376;&#24341;&#23548;&#30340;&#31890;&#24230;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#65292;&#20197;&#26368;&#23567;&#21270;&#26381;&#21153;&#20043;&#38388;&#30340;&#20219;&#21153;&#36716;&#31227;&#65292;&#20174;&#32780;&#33719;&#24471;&#25345;&#32493;&#30340;&#23398;&#20064;&#25928;&#30410;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20943;&#36731;&#29305;&#23450;&#26381;&#21153;&#30340;&#35760;&#24518;&#36127;&#25285;&#65292;&#24182;&#25945;&#20250;&#27169;&#22411;&#23558;&#25152;&#32473;&#38382;&#39064;&#21644;&#31034;&#20363;&#29992;&#20110;&#20174;&#23545;&#35805;&#20013;&#25552;&#21462;&#24517;&#35201;&#20449;&#24687;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#20010;&#21482;&#26377;6000&#19975;&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#20174;&#26816;&#32034;&#22120;&#33719;&#21462;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#33719;&#24471;&#24040;&#22823;&#30340;&#25552;&#21319;&#12290;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#31616;&#21333;&#30340;&#25345;&#32493;&#23398;&#20064;&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue systems are frequently updated to accommodate new services, but naively updating them by continually training with data for new services in diminishing performance on previously learnt services. Motivated by the insight that dialogue state tracking (DST), a crucial component of dialogue systems that estimates the user's goal as a conversation proceeds, is a simple natural language understanding task, we propose reformulating it as a bundle of granular example-guided question answering tasks to minimize the task shift between services and thus benefit continual learning. Our approach alleviates service-specific memorization and teaches a model to contextualize the given question and example to extract the necessary information from the conversation. We find that a model with just 60M parameters can achieve a significant boost by learning to learn from in-context examples retrieved by a retriever trained to identify turns with similar dialogue state changes. Combining our method
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; LogicLLM&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#30417;&#30563;&#21518;&#35757;&#32451;&#26469;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#22312;&#24120;&#35265;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13718</link><description>&lt;p&gt;
LogicLLM&#65306;&#25506;&#32034;&#33258;&#30417;&#30563;&#36923;&#36753;&#22686;&#24378;&#35757;&#32451;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large Language Models. (arXiv:2305.13718v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; LogicLLM&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#30417;&#30563;&#21518;&#35757;&#32451;&#26469;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#22312;&#24120;&#35265;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#30340;&#29616;&#26377;&#21162;&#21147;&#20027;&#35201;&#20381;&#36182;&#20110;&#26377;&#30417;&#30563;&#24494;&#35843;&#65292;&#36825;&#38459;&#30861;&#20102;&#23558;&#27169;&#22411;&#27867;&#21270;&#21040;&#26032;&#30340;&#39046;&#22495;&#21644;/&#25110;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#21457;&#23637;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#35777;&#26126;&#20102;&#23558;&#20016;&#23500;&#30340;&#30693;&#35782;&#21387;&#32553;&#20026;&#21333;&#20010;&#20195;&#29702;&#30340;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#22810;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;LLMs &#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#24182;&#27809;&#26377;&#34920;&#29616;&#20986;&#33021;&#21147;&#12290;LLMs &#22312;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#36828;&#36828;&#33853;&#21518;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#36890;&#36807;&#33258;&#30417;&#30563;&#21518;&#35757;&#32451;&#26469;&#25506;&#32034;&#34701;&#21512;&#36923;&#36753;&#30693;&#35782;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#28608;&#27963;&#23427;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;LogicLLM&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;MERIt &#30340;&#33258;&#22238;&#24402;&#30446;&#26631;&#21464;&#20307;&#65292;&#24182;&#23558;&#20854;&#19982;&#20004;&#20010;LLM&#31995;&#21015;FLAN-T5&#21644;LLaMA&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#21442;&#25968;&#22823;&#23567;&#33539;&#22260;&#20174;30&#20159;&#21040;130&#20159;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24120;&#29992;&#25512;&#29702;&#31574;&#30053;&#19978;&#19982;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#26041;&#27861;&#30456;&#24403;&#65292;&#24182;&#19988;&#36828;&#36828;&#36229;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing efforts to improve logical reasoning ability of language models have predominantly relied on supervised fine-tuning, hindering generalization to new domains and/or tasks. The development of Large Langauge Models (LLMs) has demonstrated the capacity of compressing abundant knowledge into a single proxy, enabling them to tackle multiple tasks effectively. Our preliminary experiments, nevertheless, show that LLMs do not show capability on logical reasoning. The performance of LLMs on logical reasoning benchmarks is far behind the existing state-of-the-art baselines. In this paper, we make the first attempt to investigate the feasibility of incorporating logical knowledge through self-supervised post-training, and activating it via in-context learning, which we termed as LogicLLM. Specifically, we devise an auto-regressive objective variant of MERIt and integrate it with two LLM series, i.e., FLAN-T5 and LLaMA, with parameter size ranging from 3 billion to 13 billion. The results 
&lt;/p&gt;</description></item><item><title>BA-SOT&#26159;&#19968;&#31181;&#38754;&#21521;&#22810;&#35828;&#35805;&#20154;ASR&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#36793;&#30028;&#24863;&#30693;&#21644;&#36830;&#25509;&#26102;&#38388;&#20998;&#31867;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.13716</link><description>&lt;p&gt;
BA-SOT: &#38754;&#21521;&#22810;&#35828;&#35805;&#20154;ASR&#30340;&#36793;&#30028;&#24863;&#30693;&#24207;&#21015;&#21270;&#36755;&#20986;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
BA-SOT: Boundary-Aware Serialized Output Training for Multi-Talker ASR. (arXiv:2305.13716v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13716
&lt;/p&gt;
&lt;p&gt;
BA-SOT&#26159;&#19968;&#31181;&#38754;&#21521;&#22810;&#35828;&#35805;&#20154;ASR&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#36793;&#30028;&#24863;&#30693;&#21644;&#36830;&#25509;&#26102;&#38388;&#20998;&#31867;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#30340;&#24207;&#21015;&#21270;&#36755;&#20986;&#35757;&#32451;&#65288;SOT&#65289;&#36890;&#36807;&#29983;&#25104;&#30001;&#29305;&#27530;&#26631;&#35760;&#20998;&#38548;&#30340;&#35828;&#35805;&#32773;&#36716;&#24405;&#31616;&#21270;&#20102;&#22810;&#35828;&#35805;&#32773;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12290;&#20294;&#26159;&#65292;&#39057;&#32321;&#30340;&#35828;&#35805;&#32773;&#26356;&#25913;&#21487;&#33021;&#20250;&#20351;&#35828;&#35805;&#32773;&#26356;&#25913;&#39044;&#27979;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36793;&#30028;&#24863;&#30693;&#24207;&#21015;&#21270;&#36755;&#20986;&#35757;&#32451;&#65288;BA-SOT&#65289;&#65292;&#23427;&#36890;&#36807;&#35828;&#35805;&#32773;&#26356;&#25913;&#26816;&#27979;&#20219;&#21153;&#21644;&#36793;&#30028;&#32422;&#26463;&#25439;&#22833;&#23558;&#36793;&#30028;&#30693;&#35782;&#26126;&#30830;&#22320;&#32435;&#20837;&#35299;&#30721;&#22120;&#20013;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#36830;&#25509;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#31574;&#30053;&#65292;&#23427;&#23558;&#22522;&#20110;&#26631;&#35760;&#30340;SOT CTC&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#24674;&#22797;&#26102;&#38388;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#38500;&#20102;&#20856;&#22411;&#30340;&#23383;&#31526;&#38169;&#35823;&#29575;&#65288;CER&#65289;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#35805;&#35821;&#30340;&#23383;&#31526;&#38169;&#35823;&#29575;&#65288;UD-CER&#65289;&#65292;&#20197;&#36827;&#19968;&#27493;&#34913;&#37327;&#35828;&#35805;&#32773;&#26356;&#25913;&#39044;&#27979;&#30340;&#31934;&#24230;&#12290;&#19982;&#21407;&#22987;&#30340;SOT&#30456;&#27604;&#65292;BA-SOT&#23558;CER / UD-CER&#38477;&#20302;&#20102;5.1&#65285;/ 14.0&#65285;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;ASR&#27169;&#22411;&#36827;&#34892;BA-SOT&#27169;&#22411;&#21021;&#22987;&#21270;&#36827;&#19968;&#27493;&#23558;CER / UD-CER&#38477;&#20302;&#20102;8.4&#65285;/ 19.9&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recently proposed serialized output training (SOT) simplifies multi-talker automatic speech recognition (ASR) by generating speaker transcriptions separated by a special token. However, frequent speaker changes can make speaker change prediction difficult. To address this, we propose boundary-aware serialized output training (BA-SOT), which explicitly incorporates boundary knowledge into the decoder via a speaker change detection task and boundary constraint loss. We also introduce a two-stage connectionist temporal classification (CTC) strategy that incorporates token-level SOT CTC to restore temporal context information. Besides typical character error rate (CER), we introduce utterance-dependent character error rate (UD-CER) to further measure the precision of speaker change prediction. Compared to original SOT, BA-SOT reduces CER/UD-CER by 5.1%/14.0%, and leveraging a pre-trained ASR model for BA-SOT model initialization further reduces CER/UD-CER by 8.4%/19.9%.
&lt;/p&gt;</description></item><item><title>&#36825;&#20221;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26085;&#35821;&#35821;&#38899;&#35821;&#26009;&#24211; - CALLS&#65292;&#23427;&#26088;&#22312;&#23558;&#20849;&#24773;&#23545;&#35805;&#35821;&#38899;&#21512;&#25104;&#24212;&#29992;&#20110;&#23458;&#25143;&#26381;&#21153;&#20013;&#24515;&#30340;&#25237;&#35785;&#22788;&#29702;&#21644;&#20851;&#27880;&#20542;&#21548;&#39046;&#22495;&#12290;&#23545;&#20110;&#25193;&#23637;&#35813;&#25216;&#26415;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#35813;&#35821;&#26009;&#24211;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.13713</link><description>&lt;p&gt;
CALLS: &#20855;&#26377;&#20849;&#24773;&#23545;&#35805;&#26041;&#27861;&#30340;&#26085;&#26412;&#23458;&#25143;&#26381;&#21153;&#20013;&#24515;&#25237;&#35785;&#22788;&#29702;&#21644;&#20851;&#27880;&#20542;&#21548;&#35821;&#38899;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
CALLS: Japanese Empathetic Dialogue Speech Corpus of Complaint Handling and Attentive Listening in Customer Center. (arXiv:2305.13713v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13713
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26085;&#35821;&#35821;&#38899;&#35821;&#26009;&#24211; - CALLS&#65292;&#23427;&#26088;&#22312;&#23558;&#20849;&#24773;&#23545;&#35805;&#35821;&#38899;&#21512;&#25104;&#24212;&#29992;&#20110;&#23458;&#25143;&#26381;&#21153;&#20013;&#24515;&#30340;&#25237;&#35785;&#22788;&#29702;&#21644;&#20851;&#27880;&#20542;&#21548;&#39046;&#22495;&#12290;&#23545;&#20110;&#25193;&#23637;&#35813;&#25216;&#26415;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#35813;&#35821;&#26009;&#24211;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;CALLS&#65292;&#36825;&#26159;&#19968;&#20010;&#26085;&#35821;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#23558;&#23458;&#25143;&#20013;&#24515;&#20013;&#30340;&#30005;&#35805;&#21628;&#21483;&#31216;&#20026;&#20849;&#24773;&#21475;&#35821;&#23545;&#35805;&#30340;&#26032;&#39046;&#22495;&#12290;&#29616;&#26377;&#30340;STUDIES&#35821;&#26009;&#24211;&#20165;&#28085;&#30422;&#23398;&#26657;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#20849;&#24773;&#23545;&#35805;&#12290;&#20026;&#20102;&#25193;&#23637;&#20849;&#24773;&#23545;&#35805;&#35821;&#38899;&#21512;&#25104;&#65288;EDSS&#65289;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36825;&#20010;&#35821;&#26009;&#24211;&#65292;&#20197;&#21253;&#25324;&#19982;STUDIES&#25945;&#24072;&#30456;&#21516;&#30340;&#22899;&#24615;&#35762;&#36848;&#32773;&#65292;&#22312;&#27169;&#25311;&#30340;&#30005;&#35805;&#21628;&#21483;&#20013;&#25285;&#20219;&#25805;&#20316;&#21592;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#35821;&#26009;&#24211;&#26500;&#24314;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#24405;&#21046;&#30340;&#35821;&#38899;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;CALLS&#21644;STUDIES&#35821;&#26009;&#24211;&#36827;&#34892;EDSS&#23454;&#39564;&#65292;&#20197;&#30740;&#31350;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28151;&#21512;&#20004;&#20010;&#35821;&#26009;&#24211;&#20250;&#23548;&#33268;&#21512;&#25104;&#35821;&#38899;&#36136;&#37327;&#30340;&#20559;&#24046;&#25913;&#36827;&#65292;&#36825;&#26159;&#30001;&#20110;&#34920;&#29616;&#31243;&#24230;&#19981;&#21516;&#25152;&#23548;&#33268;&#30340;&#12290;&#26412;&#35821;&#26009;&#24211;&#30340;&#39033;&#30446;&#39029;&#38754;&#26159;http&#32593;&#22336;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present CALLS, a Japanese speech corpus that considers phone calls in a customer center as a new domain of empathetic spoken dialogue. The existing STUDIES corpus covers only empathetic dialogue between a teacher and student in a school. To extend the application range of empathetic dialogue speech synthesis (EDSS), we designed our corpus to include the same female speaker as the STUDIES teacher, acting as an operator in simulated phone calls. We describe a corpus construction methodology and analyze the recorded speech. We also conduct EDSS experiments using the CALLS and STUDIES corpora to investigate the effect of domain differences. The results show that mixing the two corpora during training causes biased improvements in the quality of synthetic speech due to the different degrees of expressiveness. Our project page of the corpus is this http URL
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#33258;&#36523;&#30693;&#35782;&#30340;&#29702;&#35299;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#12290;&#35813;&#30740;&#31350;&#32858;&#28966;&#20110;&#35299;&#20915;&#8220;&#24050;&#30693;-&#26410;&#30693;&#8221;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#26041;&#26696;&#65292;&#24182;&#20351;&#29992;&#35821;&#20041;&#35780;&#20272;&#26041;&#27861;&#37327;&#21270;&#20102;&#27169;&#22411;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13712</link><description>&lt;p&gt;
&#30693;&#35782;&#30340;&#30693;&#35782;&#65306;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26410;&#30693;-&#24050;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models. (arXiv:2305.13712v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#33258;&#36523;&#30693;&#35782;&#30340;&#29702;&#35299;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#12290;&#35813;&#30740;&#31350;&#32858;&#28966;&#20110;&#35299;&#20915;&#8220;&#24050;&#30693;-&#26410;&#30693;&#8221;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#26041;&#26696;&#65292;&#24182;&#20351;&#29992;&#35821;&#20041;&#35780;&#20272;&#26041;&#27861;&#37327;&#21270;&#20102;&#27169;&#22411;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29702;&#35299;&#33258;&#36523;&#30693;&#35782;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20197;&#32531;&#35299;&#34394;&#26500;&#29616;&#35937;&#12290;&#25105;&#20204;&#19987;&#38376;&#20851;&#27880;&#35299;&#20915;&#8220;&#24050;&#30693;-&#26410;&#30693;&#8221;&#38382;&#39064;&#65292;&#36825;&#31181;&#38382;&#39064;&#30001;&#20110;&#32570;&#20047;&#30830;&#23450;&#30340;&#31572;&#26696;&#32780;&#20855;&#26377;&#39640;&#24230;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#20419;&#36827;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#30340;&#24050;&#30693;-&#26410;&#30693;&#38382;&#39064;&#65288;KUQ&#65289;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#26041;&#26696;&#26469;&#38416;&#26126;&#19981;&#30830;&#23450;&#24615;&#30340;&#26469;&#28304;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;LLM&#21306;&#20998;&#24050;&#30693;&#21644;&#26410;&#30693;&#38382;&#39064;&#20197;&#21450;&#30456;&#24212;&#20998;&#31867;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#24320;&#25918;&#24335;QA&#29615;&#22659;&#20013;&#35780;&#20272;LLM&#30340;&#31572;&#26696;&#36136;&#37327;&#12290;&#20026;&#20102;&#37327;&#21270;&#31572;&#26696;&#20013;&#34920;&#36798;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#31181;&#35821;&#20041;&#35780;&#20272;&#26041;&#27861;&#65292;&#29992;&#20110;&#27979;&#37327;&#27169;&#22411;&#22312;&#34920;&#36798;&#24050;&#30693;vs&#26410;&#30693;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the capabilities of Large Language Models (LLMs) in the context of understanding their own knowledge and measuring their uncertainty. We argue this is an important feature for mitigating hallucinations. Specifically, we focus on addressing \textit{known-unknown} questions, characterized by high uncertainty due to the absence of definitive answers. To facilitate our study, we collect a dataset with new Known-Unknown Questions (KUQ) and propose a novel categorization scheme to elucidate the sources of uncertainty. Subsequently, we assess the LLMs' ability to differentiate between known and unknown questions and classify them accordingly. Moreover, we evaluate the quality of their answers in an Open-Ended QA setting. To quantify the uncertainty expressed in the answers, we create a semantic evaluation method that measures the model's accuracy in expressing uncertainty between known vs unknown questions.
&lt;/p&gt;</description></item><item><title>LLM-Eval&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#30340;&#22810;&#32500;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#20854;&#22312;&#19968;&#20010;&#27169;&#22411;&#35843;&#29992;&#20013;&#28085;&#30422;&#20102;&#22810;&#20010;&#23545;&#35805;&#36136;&#37327;&#32500;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#24615;&#12289;&#39640;&#25928;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#26159;&#35780;&#20272;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#30340;&#22810;&#21151;&#33021;&#24378;&#22823;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.13711</link><description>&lt;p&gt;
LLM-Eval&#65306;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#20013;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#22810;&#32500;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models. (arXiv:2305.13711v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13711
&lt;/p&gt;
&lt;p&gt;
LLM-Eval&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#30340;&#22810;&#32500;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#20854;&#22312;&#19968;&#20010;&#27169;&#22411;&#35843;&#29992;&#20013;&#28085;&#30422;&#20102;&#22810;&#20010;&#23545;&#35805;&#36136;&#37327;&#32500;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#24615;&#12289;&#39640;&#25928;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#26159;&#35780;&#20272;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#30340;&#22810;&#21151;&#33021;&#24378;&#22823;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;LLM-Eval&#65292;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#30340;&#32479;&#19968;&#22810;&#32500;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#24120;&#24120;&#20381;&#36182;&#20110;&#20154;&#24037;&#27880;&#37322;&#12289;&#22522;&#26412;&#20107;&#23454;&#22238;&#22797;&#25110;&#22810;&#20010;LLM&#25552;&#31034;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#38656;&#35201;&#20184;&#20986;&#26114;&#36149;&#30340;&#20195;&#20215;&#24182;&#28040;&#32791;&#22823;&#37327;&#26102;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21333;&#25552;&#31034;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#32479;&#19968;&#30340;&#35780;&#20272;&#27169;&#24335;&#65292;&#22312;&#21333;&#20010;&#27169;&#22411;&#35843;&#29992;&#20013;&#28085;&#30422;&#20102;&#23545;&#35805;&#36136;&#37327;&#30340;&#22810;&#20010;&#32500;&#24230;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24191;&#27867;&#35780;&#20272;&#20102;LLM-Eval&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#35780;&#20272;&#26041;&#27861;&#32780;&#35328;&#20855;&#26377;&#30340;&#26377;&#25928;&#24615;&#12289;&#39640;&#25928;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#24378;&#35843;&#20102;&#20026;&#33719;&#24471;&#20934;&#30830;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#36873;&#25321;&#21512;&#36866;&#30340;LLM&#21644;&#35299;&#30721;&#31574;&#30053;&#30340;&#37325;&#35201;&#24615;&#12290;LLM-Eval&#25552;&#20379;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#19988;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35780;&#20272;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#65292;&#31616;&#21270;&#20102;&#35780;&#20272;&#36807;&#31243;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#30340;&#19968;&#33268;&#24615;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose LLM-Eval, a unified multi-dimensional automatic evaluation method for open-domain conversations with large language models (LLMs). Existing evaluation methods often rely on human annotations, ground-truth responses, or multiple LLM prompts, which can be expensive and time-consuming. To address these issues, we design a single prompt-based evaluation method that leverages a unified evaluation schema to cover multiple dimensions of conversation quality in a single model call. We extensively evaluate the performance of LLM-Eval on various benchmark datasets, demonstrating its effectiveness, efficiency, and adaptability compared to state-of-the-art evaluation methods. Our analysis also highlights the importance of choosing suitable LLMs and decoding strategies for accurate evaluation results. LLM-Eval offers a versatile and robust solution for evaluating open-domain conversation systems, streamlining the evaluation process and providing consistent performance across diverse scen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25991;&#26412;&#30028;&#38754;&#23545;&#40784;&#22806;&#37096;&#30693;&#35782;&#20197;&#28040;&#38500;&#32321;&#29712;&#36807;&#31243;&#30340;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#26032;&#33539;&#20363;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#29983;&#25104;&#20102;&#26356;&#33258;&#28982;&#30340;&#26368;&#32456;&#21709;&#24212;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#20808;&#21069;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#26356;&#22823;&#30340;&#20219;&#21153;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.13710</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#30028;&#38754;&#23545;&#40784;&#22806;&#37096;&#30693;&#35782;&#20197;&#23454;&#29616;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Using Textual Interface to Align External Knowledge for End-to-End Task-Oriented Dialogue Systems. (arXiv:2305.13710v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25991;&#26412;&#30028;&#38754;&#23545;&#40784;&#22806;&#37096;&#30693;&#35782;&#20197;&#28040;&#38500;&#32321;&#29712;&#36807;&#31243;&#30340;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#26032;&#33539;&#20363;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#29983;&#25104;&#20102;&#26356;&#33258;&#28982;&#30340;&#26368;&#32456;&#21709;&#24212;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#20808;&#21069;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#26356;&#22823;&#30340;&#20219;&#21153;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#37319;&#29992;&#27169;&#22359;&#21270;&#35774;&#35745;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35774;&#35745;&#24120;&#24120;&#23548;&#33268;&#20195;&#29702;&#21709;&#24212;&#19982;&#22806;&#37096;&#30693;&#35782;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#65292;&#30001;&#20110;&#20449;&#24687;&#34920;&#31034;&#19981;&#36275;&#32780;&#20135;&#29983;&#12290;&#27492;&#22806;&#65292;&#20854;&#35780;&#20272;&#25351;&#26631;&#24378;&#35843;&#35780;&#20272;&#20195;&#29702;&#30340;&#39044;&#35789;&#27719;&#21270;&#21709;&#24212;&#65292;&#24573;&#30053;&#20102;&#23436;&#25104;&#21709;&#24212;&#30340;&#36136;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#33539;&#20363;&#65292;&#20351;&#29992;&#25991;&#26412;&#30028;&#38754;&#23545;&#40784;&#22806;&#37096;&#30693;&#35782;&#65292;&#28040;&#38500;&#20887;&#20313;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;MultiWOZ-Remake&#20013;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#33539;&#20363;&#65292;&#21253;&#25324;&#20026;MultiWOZ&#25968;&#25454;&#24211;&#26500;&#24314;&#30340;&#20132;&#20114;&#24335;&#25991;&#26412;&#30028;&#38754;&#21644;&#30456;&#24212;&#30340;&#37325;&#26032;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#23545;&#35805;&#31995;&#32479;&#26469;&#35780;&#20272;&#36825;&#20010;&#26032;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#20102;&#26356;&#33258;&#28982;&#30340;&#26368;&#32456;&#21709;&#24212;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#20808;&#21069;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#26356;&#22823;&#30340;&#20219;&#21153;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional end-to-end task-oriented dialogue systems have been built with a modularized design. However, such design often causes misalignment between the agent response and external knowledge, due to inadequate representation of information. Furthermore, its evaluation metrics emphasize assessing the agent's pre-lexicalization response, neglecting the quality of the completed response. In this work, we propose a novel paradigm that uses a textual interface to align external knowledge and eliminate redundant processes. We demonstrate our paradigm in practice through MultiWOZ-Remake, including an interactive textual interface built for the MultiWOZ database and a correspondingly re-processed dataset. We train an end-to-end dialogue system to evaluate this new dataset. The experimental results show that our approach generates more natural final responses and achieves a greater task success rate compared to the previous models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21830;&#29992;&#35821;&#35328;&#27169;&#22411;API&#22312;&#22810;&#35821;&#35328;&#25903;&#25345;&#26041;&#38754;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#26631;&#35760;&#21270;&#23545;API&#36328;&#35821;&#35328;&#23450;&#20215;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;22&#31181;&#35821;&#35328;&#20013;&#35780;&#20272;&#20102;OpenAI&#35821;&#35328;&#27169;&#22411;API&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#20123;&#22320;&#21306;&#30340;&#35821;&#35328;&#20184;&#36153;&#36807;&#39640;&#65292;&#32780;&#19988;&#32467;&#26524;&#36739;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.13707</link><description>&lt;p&gt;
&#22312;&#21830;&#19994;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65292;&#25152;&#26377;&#35821;&#35328;&#30340;&#25104;&#26412;&#37117;&#19968;&#26679;&#21527;&#65311;&#22522;&#20110;&#26631;&#35760;&#21270;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models. (arXiv:2305.13707v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21830;&#29992;&#35821;&#35328;&#27169;&#22411;API&#22312;&#22810;&#35821;&#35328;&#25903;&#25345;&#26041;&#38754;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#26631;&#35760;&#21270;&#23545;API&#36328;&#35821;&#35328;&#23450;&#20215;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;22&#31181;&#35821;&#35328;&#20013;&#35780;&#20272;&#20102;OpenAI&#35821;&#35328;&#27169;&#22411;API&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#20123;&#22320;&#21306;&#30340;&#35821;&#35328;&#20184;&#36153;&#36807;&#39640;&#65292;&#32780;&#19988;&#32467;&#26524;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20174;&#30740;&#31350;&#21407;&#22411;&#36880;&#28176;&#21830;&#19994;&#21270;&#65292;&#34987;&#20316;&#20026;Web API&#25552;&#20379;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#36825;&#20123;&#20135;&#21697;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;API&#20379;&#24212;&#21830;&#26681;&#25454;&#20351;&#29992;&#37327;&#25910;&#36153;&#65292;&#20855;&#20307;&#32780;&#35328;&#26159;&#30001;&#28508;&#22312;&#30340;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#25110;&#29983;&#25104;&#30340;&#8220;&#20196;&#29260;&#8221;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#20160;&#20040;&#26500;&#25104;&#19968;&#20010;&#20196;&#29260;&#65292;&#26159;&#19982;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#30456;&#20851;&#30340;&#65292;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#20256;&#36798;&#30456;&#21516;&#20449;&#24687;&#25152;&#38656;&#30340;&#20196;&#29260;&#25968;&#37327;&#24046;&#24322;&#24456;&#22823;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#31181;&#19981;&#22343;&#21248;&#24615;&#23545; API &#36328;&#35821;&#35328;&#23450;&#20215;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23545; 22 &#31181;&#35821;&#35328;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#35780;&#20272;OpenAI&#30340;&#35821;&#35328;&#27169;&#22411;API&#22312;&#22810;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#25104;&#26412;&#21644;&#25928;&#29992;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;&#35768;&#22810;&#25903;&#25345;&#35821;&#35328;&#30340;&#20351;&#29992;&#32773;&#20184;&#36153;&#36807;&#39640;&#65292;&#24182;&#33719;&#24471;&#26356;&#24046;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#20351;&#29992;&#32773;&#24448;&#24448;&#26469;&#33258; API &#20351;&#29992;&#29575;&#36739;&#20302;&#30340;&#22320;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models have graduated from being research prototypes to commercialized products offered as web APIs, and recent works have highlighted the multilingual capabilities of these products. The API vendors charge their users based on usage, more specifically on the number of ``tokens'' processed or generated by the underlying language models. What constitutes a token, however, is training data and model dependent with a large variance in the number of tokens required to convey the same information in different languages. In this work, we analyze the effect of this non-uniformity on the fairness of an API's pricing policy across languages. We conduct a systematic analysis of the cost and utility of OpenAI's language model API on multilingual benchmarks in 22 typologically diverse languages. We show evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results. These speakers tend to also come from regions where the APIs are less af
&lt;/p&gt;</description></item><item><title>MemeCap&#26159;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35828;&#26126;&#21644;&#35299;&#37322;Memes&#12290; &#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#20063;&#38590;&#20197;&#24212;&#23545;Memes&#20013;&#30340;&#35270;&#35273;&#38544;&#21947;&#65292;&#34920;&#29616;&#36828;&#36828;&#19981;&#22914;&#20154;&#31867;&#12290;</title><link>http://arxiv.org/abs/2305.13703</link><description>&lt;p&gt;
MemeCap&#65306;&#19968;&#20010;&#29992;&#20110;&#35828;&#26126;&#21644;&#35299;&#37322;Memes&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MemeCap: A Dataset for Captioning and Interpreting Memes. (arXiv:2305.13703v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13703
&lt;/p&gt;
&lt;p&gt;
MemeCap&#26159;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35828;&#26126;&#21644;&#35299;&#37322;Memes&#12290; &#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#20063;&#38590;&#20197;&#24212;&#23545;Memes&#20013;&#30340;&#35270;&#35273;&#38544;&#21947;&#65292;&#34920;&#29616;&#36828;&#36828;&#19981;&#22914;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Memes&#26159;&#32593;&#27665;&#20204;&#20351;&#29992;&#35270;&#35273;&#38544;&#21947;&#34920;&#36798;&#20182;&#20204;&#30340;&#24605;&#24819;&#30340;&#24191;&#27867;&#24037;&#20855;&#12290;&#29702;&#35299;Memes&#38656;&#35201;&#35782;&#21035;&#21644;&#35299;&#37322;&#35270;&#35273;&#38544;&#21947;&#65292;&#21516;&#26102;&#32771;&#34385;Memes&#20869;&#22806;&#30340;&#25991;&#26412;&#65292;&#24182;&#24120;&#24120;&#20351;&#29992;&#32972;&#26223;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MemeCaption&#20219;&#21153;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;MemeCap&#12290;&#27492;&#25968;&#25454;&#38598;&#21253;&#21547;6.3K&#20010;Memes&#65292;&#20197;&#21450;&#21253;&#21547;Memes&#30340;&#24086;&#23376;&#30340;&#26631;&#39064;&#12289;Memes&#30340;&#35828;&#26126;&#12289;&#23383;&#38754;&#22270;&#20687;&#35828;&#26126;&#21644;&#35270;&#35273;&#38544;&#21947;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#35270;&#35273;&#21644;&#35821;&#35328;&#65288;VL&#65289;&#27169;&#22411;&#22312;&#22270;&#20687;&#35828;&#26126;&#21644;&#35270;&#35273;&#38382;&#31572;&#31561;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;VL&#27169;&#22411;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#23427;&#20204;&#20173;&#28982;&#38590;&#20197;&#24212;&#23545;&#35270;&#35273;&#38544;&#21947;&#65292;&#19988;&#34920;&#29616;&#36828;&#19981;&#22914;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memes are a widely popular tool for web users to express their thoughts using visual metaphors. Understanding memes requires recognizing and interpreting visual metaphors with respect to the text inside or around the meme, often while employing background knowledge and reasoning abilities. We present the task of meme captioning and release a new dataset, MemeCap. Our dataset contains 6.3K memes along with the title of the post containing the meme, the meme captions, the literal image caption, and the visual metaphors. Despite the recent success of vision and language (VL) models on tasks such as image captioning and visual question answering, our extensive experiments using state-of-the-art VL models show that they still struggle with visual metaphors, and perform substantially worse than humans.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20351;&#29992;RoBERTa&#21644;T5&#20316;&#20026;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#31867;&#22411;&#30340;&#32534;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#21019;&#24314;&#20102;&#21476;&#24076;&#33098;&#35821;&#21644;&#25289;&#19969;&#35821;&#31561;&#22810;&#35821;&#35328;&#23454;&#20363;&#30340;&#22235;&#20010;&#21476;&#24076;&#33098;&#35821;&#35328;&#27169;&#22411;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#24418;&#24577;&#21644;&#21477;&#27861;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#37117;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#65292;&#24182;&#19988;&#22810;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#21333;&#35821;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13698</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21476;&#20856;&#35821;&#35328;&#23398;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring Large Language Models for Classical Philology. (arXiv:2305.13698v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20351;&#29992;RoBERTa&#21644;T5&#20316;&#20026;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#31867;&#22411;&#30340;&#32534;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#21019;&#24314;&#20102;&#21476;&#24076;&#33098;&#35821;&#21644;&#25289;&#19969;&#35821;&#31561;&#22810;&#35821;&#35328;&#23454;&#20363;&#30340;&#22235;&#20010;&#21476;&#24076;&#33098;&#35821;&#35328;&#27169;&#22411;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#24418;&#24577;&#21644;&#21477;&#27861;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#37117;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#65292;&#24182;&#19988;&#22810;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#21333;&#35821;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#20026;&#21253;&#25324;&#21476;&#24076;&#33098;&#35821;&#21644;&#25289;&#19969;&#35821;&#22312;&#20869;&#30340;&#35768;&#22810;&#35821;&#35328;&#21019;&#24314;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#20197;&#24448;&#30340;&#21476;&#20856;&#35821;&#35328;&#30740;&#31350;&#20013;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#20351;&#29992;BERT&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#22235;&#20010;&#21476;&#24076;&#33098;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#20010;&#32500;&#24230;&#30340;&#21464;&#21270;&#26469;&#30740;&#31350;&#23427;&#20204;&#22312;&#21476;&#20856;&#35821;&#35328;&#23398;&#20013;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;&#25105;&#20204;&#25506;&#32034;&#20351;&#29992;RoBERTa&#21644;T5&#20316;&#20026;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#31867;&#22411;&#30340;&#32534;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24182;&#20026;&#27599;&#20010;&#27169;&#22411;&#21019;&#24314;&#20102;&#21253;&#21547;&#25289;&#19969;&#35821;&#21644;&#33521;&#35821;&#30340;&#21333;&#35821;&#21476;&#24076;&#33098;&#35821;&#21644;&#22810;&#35821;&#35328;&#23454;&#20363;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25152;&#26377;&#27169;&#22411;&#22312;&#24418;&#24577;&#21644;&#21477;&#27861;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#35789;&#24418;&#36824;&#21407;&#65292;&#23637;&#31034;&#20102;T5&#30340;&#35299;&#30721;&#33021;&#21147;&#30340;&#38468;&#21152;&#20215;&#20540;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23450;&#20041;&#20102;&#20004;&#20010;&#25506;&#27979;&#20219;&#21153;&#26469;&#30740;&#31350;&#39044;&#35757;&#32451;&#22312;&#21476;&#20856;&#25991;&#26412;&#19978;&#30340;&#27169;&#22411;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25552;&#20379;&#20102;&#23545;&#29616;&#26377;&#21476;&#24076;&#33098;&#35821;&#35328;&#27169;&#22411;&#30340;&#31532;&#19968;&#25209;&#22522;&#20934;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30456;&#23545;&#20110;&#24403;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#22810;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#21333;&#35821;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in NLP have led to the creation of powerful language models for many languages including Ancient Greek and Latin. While prior work on Classical languages unanimously uses BERT, in this work we create four language models for Ancient Greek that vary along two dimensions to study their versatility for tasks of interest for Classical languages: we explore (i) encoder-only and encoder-decoder architectures using RoBERTa and T5 as strong model types, and create for each of them (ii) a monolingual Ancient Greek and a multilingual instance that includes Latin and English. We evaluate all models on morphological and syntactic tasks, including lemmatization, which demonstrates the added value of T5's decoding abilities. We further define two probing tasks to investigate the knowledge acquired by models pre-trained on Classical texts. Our experiments provide the first benchmarking analysis of existing models of Ancient Greek. Results show that our models provide significant impro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102; UNIMO-3 &#27169;&#22411;&#65292;&#20855;&#26377;&#22810;&#23618;&#27425;&#20132;&#20114;&#33021;&#21147;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#22810;&#27169;&#24577;&#35821;&#20041;&#20449;&#24687;&#30340;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2305.13697</link><description>&lt;p&gt;
UNIMO-3: &#22810;&#23618;&#27425;&#20132;&#20114;&#30340;&#35270;&#35273;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UNIMO-3: Multi-granularity Interaction for Vision-Language Representation Learning. (arXiv:2305.13697v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102; UNIMO-3 &#27169;&#22411;&#65292;&#20855;&#26377;&#22810;&#23618;&#27425;&#20132;&#20114;&#33021;&#21147;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#22810;&#27169;&#24577;&#35821;&#20041;&#20449;&#24687;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#26088;&#22312;&#23398;&#20064;&#36890;&#29992;&#30340;&#22270;&#25991;&#37197;&#23545;&#34920;&#31034;&#65292;&#20197;&#20415;&#22312;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#36827;&#34892;&#36716;&#31227;&#12290;&#19982;&#24314;&#27169;&#21333;&#27169;&#24577;&#25968;&#25454;&#30456;&#27604;&#65292;VL &#27169;&#22411;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#22914;&#20309;&#20174;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#23398;&#20064;&#36328;&#27169;&#24577;&#20132;&#20114;&#65292;&#29305;&#21035;&#26159;&#32454;&#31890;&#24230;&#20132;&#20114;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; UNIMO-3 &#27169;&#22411;&#65292;&#20855;&#26377;&#21516;&#26102;&#23398;&#20064;&#22810;&#27169;&#24577;&#20869;&#23618;&#20132;&#20114;&#21644;&#36328;&#23618;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290; UNIMO-3 &#27169;&#22411;&#33021;&#22815;&#24314;&#31435;&#26377;&#25928;&#30340;&#36830;&#25509;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#23398;&#20064;&#22810;&#27169;&#24577;&#35821;&#20041;&#20449;&#24687;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-and-language (VL) pre-training, which aims to learn a general representation of image-text pairs that can be transferred to various vision-and-language tasks. Compared with modeling uni-modal data, the main challenge of the VL model is: how to learn the cross-modal interaction from multimodal data, especially the fine-grained interaction. Existing works have shown that fully transformer-based models that adopt attention mechanisms to learn in-layer cross-model interaction can demonstrate impressive performance on various cross-modal downstream tasks. However, they ignored that the semantic information of the different modals at the same layer was not uniform, which leads to the cross-modal interaction collapsing into a limited multi-modal semantic information interaction. In this work, we propose the UNIMO-3 model, which has the capacity to simultaneously learn the multimodal in-layer interaction and cross-layer interaction. UNIMO-3 model can establish effective connections betw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;BRIO&#35757;&#32451;&#33539;&#24335;&#65292;&#20197;&#20943;&#23569;&#25688;&#35201;&#27169;&#22411;&#23545;&#21442;&#32771;&#25688;&#35201;&#30340;&#20381;&#36182;&#65292;&#24182;&#25552;&#39640;&#20854;&#25512;&#29702;&#24615;&#33021;&#12290;&#22312;VieSum&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;BRIO&#35757;&#32451;&#33539;&#24335;&#21487;&#20197;&#22312;&#22522;&#26412;&#30828;&#20214;&#19978;&#20248;&#21270;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#65292;&#24182;&#22312;&#36234;&#21335;&#25991;&#26412;&#25688;&#35201;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13696</link><description>&lt;p&gt;
&#20351;&#29992;BRIO&#35757;&#32451;&#33539;&#24335;&#30340;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Abstractive Text Summarization Using the BRIO Training Paradigm. (arXiv:2305.13696v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;BRIO&#35757;&#32451;&#33539;&#24335;&#65292;&#20197;&#20943;&#23569;&#25688;&#35201;&#27169;&#22411;&#23545;&#21442;&#32771;&#25688;&#35201;&#30340;&#20381;&#36182;&#65292;&#24182;&#25552;&#39640;&#20854;&#25512;&#29702;&#24615;&#33021;&#12290;&#22312;VieSum&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;BRIO&#35757;&#32451;&#33539;&#24335;&#21487;&#20197;&#22312;&#22522;&#26412;&#30828;&#20214;&#19978;&#20248;&#21270;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#65292;&#24182;&#22312;&#36234;&#21335;&#25991;&#26412;&#25688;&#35201;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#29983;&#25104;&#30340;&#25688;&#35201;&#21477;&#23376;&#21487;&#33021;&#36830;&#36143;&#20840;&#38754;&#65292;&#20294;&#32570;&#20047;&#25511;&#21046;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#20110;&#21442;&#32771;&#25688;&#35201;&#12290;BRIO&#35757;&#32451;&#33539;&#20363;&#20551;&#23450;&#19968;&#20010;&#38750;&#30830;&#23450;&#24615;&#20998;&#24067;&#65292;&#20197;&#20943;&#23569;&#27169;&#22411;&#23545;&#21442;&#32771;&#25688;&#35201;&#30340;&#20381;&#36182;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#22312;&#25512;&#29702;&#26399;&#38388;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;BRIO&#35757;&#32451;&#33539;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#25913;&#21892;&#25277;&#35937;&#25688;&#35201;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#36234;&#21335;&#25991;&#26412;&#25688;&#35201;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;VieSum&#12290;&#25105;&#20204;&#22312;CNNDM&#21644;VieSum&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#32463;&#36807;BRIO&#27169;&#22411;&#35757;&#32451;&#30340;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#22522;&#26412;&#30828;&#20214;&#35757;&#32451;&#30340;&#27169;&#22411;&#20248;&#20110;&#25152;&#26377;&#29616;&#26377;&#30340;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36234;&#21335;&#35821;&#32780;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Summary sentences produced by abstractive summarization models may be coherent and comprehensive, but they lack control and rely heavily on reference summaries. The BRIO training paradigm assumes a non-deterministic distribution to reduce the model's dependence on reference summaries, and improve model performance during inference. This paper presents a straightforward but effective technique to improve abstractive summaries by fine-tuning pre-trained language models, and training them with the BRIO paradigm. We build a text summarization dataset for Vietnamese, called VieSum. We perform experiments with abstractive summarization models trained with the BRIO paradigm on the CNNDM and the VieSum datasets. The results show that the models, trained on basic hardware, outperform all existing abstractive summarization models, especially for Vietnamese.
&lt;/p&gt;</description></item><item><title>&#22312;&#21307;&#23398;&#25991;&#29486;&#32508;&#36848;&#30340;&#22810;&#25991;&#29486;&#25688;&#35201;&#20013;&#65292;&#29616;&#26377;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#24037;&#35780;&#20272;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#65292;&#38656;&#35201;&#26356;&#22909;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2305.13693</link><description>&lt;p&gt;
&#21307;&#23398;&#22810;&#25991;&#29486;&#25688;&#35201;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#24037;&#35780;&#20272;&#19981;&#19968;&#33268;
&lt;/p&gt;
&lt;p&gt;
Automated Metrics for Medical Multi-Document Summarization Disagree with Human Evaluations. (arXiv:2305.13693v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13693
&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#25991;&#29486;&#32508;&#36848;&#30340;&#22810;&#25991;&#29486;&#25688;&#35201;&#20013;&#65292;&#29616;&#26377;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#24037;&#35780;&#20272;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#65292;&#38656;&#35201;&#26356;&#22909;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22810;&#25991;&#29486;&#25688;&#35201;&#65288;MDS&#65289;&#36136;&#37327;&#26159;&#22256;&#38590;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#32508;&#36848;&#30340;&#24773;&#20917;&#19979;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#21033;&#29992;&#38590;&#20197;&#29992;&#26631;&#20934;n-gram&#30456;&#20284;&#24230;&#25351;&#26631;&#65288;&#22914;ROUGE&#65289;&#26816;&#27979;&#30340;&#24555;&#25463;&#26041;&#24335;&#65292;&#32780;&#19981;&#26159;&#25191;&#34892;&#20219;&#21153;&#12290;&#38656;&#35201;&#26356;&#22909;&#30340;&#33258;&#21160;&#21270;&#35780;&#20272;&#25351;&#26631;&#65292;&#20294;&#25552;&#20986;&#35780;&#20272;&#25351;&#26631;&#26102;&#24456;&#23569;&#26377;&#36164;&#28304;&#21487;&#20197;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20154;&#24037;&#35780;&#20272;&#30340;&#25688;&#35201;&#36136;&#37327;&#26041;&#38754;&#21644;&#25104;&#23545;&#20559;&#22909;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#40723;&#21169;&#21644;&#25903;&#25345;&#26356;&#22909;&#30340;&#25991;&#29486;&#32508;&#36848;MDS&#33258;&#21160;&#21270;&#35780;&#20272;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#21033;&#29992;&#31038;&#21306;&#25552;&#20132;&#30340;&#22810;&#25991;&#26723;&#32508;&#36848;&#65288;MSLR&#65289;&#20849;&#20139;&#20219;&#21153;&#65292;&#32534;&#32534;&#35793;&#20102;&#22810;&#26679;&#21270;&#19988;&#20195;&#34920;&#24615;&#30340;&#25688;&#35201;&#26679;&#26412;&#65292;&#24182;&#20998;&#26512;&#20102;&#33258;&#21160;&#25688;&#35201;&#35780;&#20272;&#25351;&#26631;&#19982;&#35789;&#27719;&#30456;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating multi-document summarization (MDS) quality is difficult. This is especially true in the case of MDS for biomedical literature reviews, where models must synthesize contradicting evidence reported across different documents. Prior work has shown that rather than performing the task, models may exploit shortcuts that are difficult to detect using standard n-gram similarity metrics such as ROUGE. Better automated evaluation metrics are needed, but few resources exist to assess metrics when they are proposed. Therefore, we introduce a dataset of human-assessed summary quality facets and pairwise preferences to encourage and support the development of better automated evaluation methods for literature review MDS. We take advantage of community submissions to the Multi-document Summarization for Literature Review (MSLR) shared task to compile a diverse and representative sample of generated summaries. We analyze how automated summarization evaluation metrics correlate with lexical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MAS2S&#30340;&#22810;&#27880;&#24847;&#21147;Seq2Seq&#32593;&#32476;&#65292;&#23427;&#33021;&#22815;&#25552;&#38382;&#20197;&#28548;&#28165;&#20219;&#21153;&#23548;&#21521;&#20449;&#24687;&#33719;&#21462;&#20013;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#21644;&#20010;&#20154;&#36164;&#26009;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#20219;&#21153;&#23548;&#21521;&#20449;&#24687;&#26597;&#35810;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#35206;&#30422;&#33539;&#22260;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13690</link><description>&lt;p&gt;
&#38754;&#21521;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#20449;&#24687;&#35831;&#27714;&#20013;&#30340;&#28548;&#28165;&#38382;&#39064;&#25552;&#38382;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Asking Clarification Questions for Information Seeking on Task-Oriented Dialogues. (arXiv:2305.13690v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MAS2S&#30340;&#22810;&#27880;&#24847;&#21147;Seq2Seq&#32593;&#32476;&#65292;&#23427;&#33021;&#22815;&#25552;&#38382;&#20197;&#28548;&#28165;&#20219;&#21153;&#23548;&#21521;&#20449;&#24687;&#33719;&#21462;&#20013;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#21644;&#20010;&#20154;&#36164;&#26009;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#20219;&#21153;&#23548;&#21521;&#20449;&#24687;&#26597;&#35810;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#35206;&#30422;&#33539;&#22260;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#20026;&#29992;&#25143;&#25552;&#20379;&#29305;&#23450;&#20219;&#21153;&#30340;&#26381;&#21153;&#12290;&#36825;&#31181;&#31995;&#32479;&#30340;&#29992;&#25143;&#36890;&#24120;&#19981;&#30693;&#36947;&#20182;&#20204;&#27491;&#22312;&#23436;&#25104;&#30340;&#20219;&#21153;&#30340;&#25152;&#26377;&#20449;&#24687;&#65292;&#38656;&#35201;&#23547;&#27714;&#20851;&#20110;&#20219;&#21153;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#25552;&#20379;&#20934;&#30830;&#21644;&#20010;&#24615;&#21270;&#30340;&#20219;&#21153;&#23548;&#21521;&#20449;&#24687;&#26597;&#35810;&#32467;&#26524;&#65292;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#38656;&#35201;&#35299;&#20915;&#20004;&#20010;&#28508;&#22312;&#38382;&#39064;&#65306;1) &#29992;&#25143;&#26080;&#27861;&#22312;&#35831;&#27714;&#20013;&#25551;&#36848;&#20182;&#20204;&#22797;&#26434;&#30340;&#20449;&#24687;&#38656;&#27714;&#65307;2) &#31995;&#32479;&#23545;&#29992;&#25143;&#30340;&#20449;&#24687;&#23384;&#22312;&#27169;&#31946;/&#32570;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27880;&#24847;&#21147;Seq2Seq&#32593;&#32476;&#65292;&#21629;&#21517;&#20026;MAS2S&#65292;&#23427;&#21487;&#20197;&#25552;&#38382;&#20197;&#28548;&#28165;&#29992;&#25143;&#22312;&#20219;&#21153;&#23548;&#21521;&#20449;&#24687;&#26597;&#35810;&#20013;&#30340;&#20449;&#24687;&#38656;&#27714;&#21644;&#29992;&#25143;&#20010;&#20154;&#36164;&#26009;&#12290;&#25105;&#20204;&#36824;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#20219;&#21153;&#23548;&#21521;&#20449;&#24687;&#26597;&#35810;&#25968;&#25454;&#38598;&#65292;&#23548;&#33268;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#32422;100k&#20010;&#20219;&#21153;&#23548;&#21521;&#20449;&#24687;&#26597;&#35810;&#23545;&#35805;&#65292;&#36825;&#20123;&#23545;&#35805;&#26159;&#20844;&#24320;&#30340;\footnote{ &#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#21487;&#22312;\href{http://link/to/dataset}{http://link/to/dataset}&#25214;&#21040;}&#12290;&#22312;&#25193;&#23637;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20219;&#21153;&#23548;&#21521;&#20449;&#24687;&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#21644;&#35206;&#30422;&#33539;&#22260;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-oriented dialogue systems aim at providing users with task-specific services. Users of such systems often do not know all the information about the task they are trying to accomplish, requiring them to seek information about the task. To provide accurate and personalized task-oriented information seeking results, task-oriented dialogue systems need to address two potential issues: 1) users' inability to describe their complex information needs in their requests; and 2) ambiguous/missing information the system has about the users. In this paper, we propose a new Multi-Attention Seq2Seq Network, named MAS2S, which can ask questions to clarify the user's information needs and the user's profile in task-oriented information seeking. We also extend an existing dataset for task-oriented information seeking, leading to the \ourdataset which contains about 100k task-oriented information seeking dialogues that are made publicly available\footnote{Dataset and code is available at \href{http
&lt;/p&gt;</description></item><item><title>mPLM-Sim&#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#35821;&#35328;&#24179;&#34892;&#35821;&#26009;&#24211;&#20174;mPLMs&#20013;&#24341;&#23548;&#20986;&#35821;&#35328;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#21487;&#29992;&#20110;&#36873;&#25321;&#28304;&#35821;&#35328;&#20197;&#22686;&#24378;&#36328;&#35821;&#35328;&#36801;&#31227;&#65292;&#20855;&#26377;&#20013;&#31561;&#31243;&#24230;&#30340;&#30456;&#20851;&#24615;&#12290;&#19981;&#21516;&#30340;mPLMs&#21644;&#23618;&#20135;&#29983;&#19981;&#21516;&#30340;&#30456;&#20284;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13684</link><description>&lt;p&gt;
mPLM-Sim: &#25581;&#31034;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#26356;&#22909;&#30340;&#36328;&#35821;&#35328;&#30456;&#20284;&#24615;&#21644;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
mPLM-Sim: Unveiling Better Cross-Lingual Similarity and Transfer in Multilingual Pretrained Language Models. (arXiv:2305.13684v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13684
&lt;/p&gt;
&lt;p&gt;
mPLM-Sim&#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#35821;&#35328;&#24179;&#34892;&#35821;&#26009;&#24211;&#20174;mPLMs&#20013;&#24341;&#23548;&#20986;&#35821;&#35328;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#21487;&#29992;&#20110;&#36873;&#25321;&#28304;&#35821;&#35328;&#20197;&#22686;&#24378;&#36328;&#35821;&#35328;&#36801;&#31227;&#65292;&#20855;&#26377;&#20013;&#31561;&#31243;&#24230;&#30340;&#30456;&#20851;&#24615;&#12290;&#19981;&#21516;&#30340;mPLMs&#21644;&#23618;&#20135;&#29983;&#19981;&#21516;&#30340;&#30456;&#20284;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;mPLMs&#65289;&#24050;&#32463;&#35777;&#26126;&#20855;&#26377;&#24378;&#22823;&#30340;&#29305;&#23450;&#35821;&#35328;&#20449;&#21495;&#65292;&#36825;&#20123;&#20449;&#21495;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#24182;&#27809;&#26377;&#34987;&#26126;&#30830;&#25552;&#20379;&#12290;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#26159;&#21542;&#21487;&#23558;mPLMs&#29992;&#20110;&#27979;&#37327;&#35821;&#35328;&#30456;&#20284;&#24615;&#65292;&#24182;&#38543;&#21518;&#20351;&#29992;&#30456;&#20284;&#24615;&#32467;&#26524;&#36873;&#25321;&#28304;&#35821;&#35328;&#20197;&#22686;&#24378;&#36328;&#35821;&#35328;&#36801;&#31227;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;mPLM-Sim&#65292;&#23427;&#21033;&#29992;&#22810;&#35821;&#35328;&#24179;&#34892;&#35821;&#26009;&#24211;&#20174;mPLMs&#20013;&#24341;&#23548;&#20986;&#35821;&#35328;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;mPLM-Sim&#19982;&#35789;&#27719;&#32479;&#35745;&#12289;&#35821;&#31995;&#21644;&#22320;&#29702;&#21306;&#22495;&#31561;&#35821;&#35328;&#30456;&#20284;&#24230;&#27979;&#37327;&#20855;&#26377;&#20013;&#31561;&#31243;&#24230;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36824;&#23545;&#30456;&#20851;&#24615;&#36739;&#20302;&#30340;&#35821;&#35328;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#35266;&#23519;&#21040;mPLM-Sim&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#30456;&#20284;&#24615;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#30456;&#20284;&#24615;&#32467;&#26524;&#22240;&#19981;&#21516;&#30340;mPLMs&#21644;mPLM&#20013;&#30340;&#19981;&#21516;&#23618;&#32780;&#24322;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35843;&#26597;&#20102;mPLMs&#23545;&#35821;&#35328;&#36801;&#31227;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent multilingual pretrained language models (mPLMs) have been shown to encode strong language-specific signals, which are not explicitly provided during pretraining. It remains an open question whether it is feasible to employ mPLMs to measure language similarity, and subsequently use the similarity results to select source languages for boosting cross-lingual transfer. To investigate this, we propose mPLM-Sim, a new language similarity measure that induces the similarities across languages from mPLMs using multi-parallel corpora. Our study shows that mPLM-Sim exhibits moderately high correlations with linguistic similarity measures, such as lexicostatistics, genealogical language family, and geographical sprachbund. We also conduct a case study on languages with low correlation and observe that mPLM-Sim yields more accurate similarity results. Additionally, we find that similarity results vary across different mPLMs and different layers within an mPLM. We further investigate whethe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#31435;&#20110;&#35299;&#26512;&#22120;&#30340;&#25991;&#26412;&#21040;SQL&#35821;&#20041;&#35299;&#26512;&#30340;&#35823;&#24046;&#26816;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#35299;&#26512;&#22120;&#30340;&#24615;&#33021;&#21644;&#21487;&#29992;&#24615;&#65292;&#19981;&#32771;&#34385;&#20854;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.13683</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;SQL&#35821;&#20041;&#35299;&#26512;&#20013;&#30340;&#35823;&#24046;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Error Detection for Text-to-SQL Semantic Parsing. (arXiv:2305.13683v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13683
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#31435;&#20110;&#35299;&#26512;&#22120;&#30340;&#25991;&#26412;&#21040;SQL&#35821;&#20041;&#35299;&#26512;&#30340;&#35823;&#24046;&#26816;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#35299;&#26512;&#22120;&#30340;&#24615;&#33021;&#21644;&#21487;&#29992;&#24615;&#65292;&#19981;&#32771;&#34385;&#20854;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#25991;&#26412;&#21040;SQL&#35821;&#20041;&#35299;&#26512;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#35299;&#26512;&#22120;&#30340;&#24615;&#33021;&#20173;&#36828;&#38750;&#23436;&#32654;&#12290;&#21516;&#26102;&#65292;&#29616;&#20195;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#22120;&#24448;&#24448;&#36807;&#20110;&#33258;&#20449;&#65292;&#22240;&#27492;&#22312;&#23454;&#38469;&#20351;&#29992;&#26102;&#23545;&#20854;&#21487;&#38752;&#24615;&#20135;&#29983;&#24576;&#30097;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#31435;&#20110;&#35299;&#26512;&#22120;&#30340;&#25991;&#26412;&#21040;SQL&#35821;&#20041;&#35299;&#26512;&#30340;&#35823;&#24046;&#26816;&#27979;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#20195;&#30721;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21040;&#30340;&#32467;&#26500;&#29305;&#24449;&#36827;&#34892;&#22686;&#24378;&#12290;&#25105;&#20204;&#22312;&#36328;&#39046;&#22495;&#35774;&#32622;&#20013;&#25910;&#38598;&#30340;&#23454;&#38469;&#35299;&#26512;&#35823;&#24046;&#19978;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#38024;&#23545;&#20855;&#26377;&#19981;&#21516;&#35299;&#30721;&#26426;&#21046;&#30340;&#19977;&#31181;&#24378;&#22823;&#30340;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#22120;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#35299;&#26512;&#22120;&#20381;&#36182;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#35299;&#26512;&#22120;&#30340;&#24615;&#33021;&#21644;&#21487;&#29992;&#24615;&#65292;&#32780;&#19981;&#32771;&#34385;&#20854;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite remarkable progress in text-to-SQL semantic parsing in recent years, the performance of existing parsers is still far from perfect. At the same time, modern deep learning based text-to-SQL parsers are often over-confident and thus casting doubt on their trustworthiness when deployed for real use. To that end, we propose to build a parser-independent error detection model for text-to-SQL semantic parsing. The proposed model is based on pre-trained language model of code and is enhanced with structural features learned by graph neural networks. We train our model on realistic parsing errors collected from a cross-domain setting. Experiments with three strong text-to-SQL parsers featuring different decoding mechanisms show that our approach outperforms parser-dependent uncertainty metrics and could effectively improve the performance and usability of text-to-SQL semantic parsers regardless of their architectures.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#27861;&#24459;&#23450;&#20041;&#20026;&#20013;&#24515;&#30340;&#12289;&#21487;&#27861;&#24459;&#24378;&#21046;&#25191;&#34892;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#65292;&#21033;&#29992;&#27861;&#24459;&#19987;&#23478;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#32467;&#21512;&#22522;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#30340;&#25552;&#31034;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#30340;&#36755;&#20986;&#26356;&#31526;&#21512;&#30417;&#31649;&#32773;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.13677</link><description>&lt;p&gt;
&#38754;&#21521;&#20844;&#20849;&#35770;&#22363;&#30340;&#21487;&#27861;&#24459;&#24378;&#21046;&#25191;&#34892;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Legally Enforceable Hate Speech Detection for Public Forums. (arXiv:2305.13677v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#27861;&#24459;&#23450;&#20041;&#20026;&#20013;&#24515;&#30340;&#12289;&#21487;&#27861;&#24459;&#24378;&#21046;&#25191;&#34892;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#65292;&#21033;&#29992;&#27861;&#24459;&#19987;&#23478;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#32467;&#21512;&#22522;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#30340;&#25552;&#31034;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#30340;&#36755;&#20986;&#26356;&#31526;&#21512;&#30417;&#31649;&#32773;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20167;&#24680;&#35328;&#35770;&#26159;&#20844;&#20849;&#35770;&#22363;&#19978;&#30340;&#20005;&#37325;&#38382;&#39064;&#65292;&#23545;&#24694;&#24847;&#21644;&#27495;&#35270;&#24615;&#35821;&#35328;&#30340;&#36866;&#24403;&#25191;&#34892;&#26159;&#20445;&#25252;&#20154;&#32676;&#20813;&#21463;&#20260;&#23475;&#21644;&#27495;&#35270;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#20160;&#20040;&#26500;&#25104;&#20167;&#24680;&#35328;&#35770;&#26159;&#19968;&#39033;&#38750;&#24120;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#39640;&#24230;&#23481;&#26131;&#21463;&#21040;&#20027;&#35266;&#35299;&#37322;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#20316;&#21697;&#27809;&#26377;&#23558;&#23427;&#20204;&#30340;&#31995;&#32479;&#19982;&#21487;&#25191;&#34892;&#30340;&#20167;&#24680;&#35328;&#35770;&#23450;&#20041;&#23545;&#40784;&#65292;&#36825;&#21487;&#33021;&#20250;&#20351;&#23427;&#20204;&#30340;&#36755;&#20986;&#19982;&#30417;&#31649;&#32773;&#30340;&#30446;&#26631;&#19981;&#19968;&#33268;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#20197;&#27861;&#24459;&#23450;&#20041;&#20026;&#20013;&#24515;&#30340;&#21487;&#25191;&#34892;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#65292;&#24182;&#20351;&#29992;&#27861;&#24459;&#19987;&#23478;&#23545;&#36829;&#21453;&#21313;&#19968;&#31181;&#21487;&#33021;&#23450;&#20041;&#36827;&#34892;&#20102;&#25968;&#25454;&#38598;&#27880;&#37322;&#12290;&#32771;&#34385;&#21040;&#30830;&#23450;&#28165;&#26224;&#12289;&#21487;&#27861;&#24459;&#24378;&#21046;&#25191;&#34892;&#30340;&#20167;&#24680;&#35328;&#35770;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#20351;&#29992;&#19987;&#23478;&#29983;&#25104;&#30340;&#26679;&#26412;&#21644;&#33258;&#21160;&#25366;&#25496;&#30340;&#25361;&#25112;&#38598;&#22686;&#24378;&#20102;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#30340;&#25552;&#31034;&#26469;&#22522;&#20110;&#36825;&#20123;&#23450;&#20041;&#26469;&#20915;&#23450;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#22312;&#20960;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hate speech is a serious issue on public forums, and proper enforcement of hate speech laws is key for protecting groups of people against harmful and discriminatory language. However, determining what constitutes hate speech is a complex task that is highly open to subjective interpretations. Existing works do not align their systems with enforceable definitions of hate speech, which can make their outputs inconsistent with the goals of regulators. Our work introduces a new task for enforceable hate speech detection centred around legal definitions, and a dataset annotated on violations of eleven possible definitions by legal experts. Given the challenge of identifying clear, legally enforceable instances of hate speech, we augment the dataset with expert-generated samples and an automatically mined challenge set. We experiment with grounding the model decision in these definitions using zero-shot and few-shot prompting. We then report results on several large language models (LLMs). 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#36328;&#36234;&#22810;&#31181;&#35821;&#35328;&#12289;&#20027;&#39064;&#21644;&#19978;&#19979;&#25991;&#26469;&#26816;&#32034;&#30334;&#31185;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;Meta&#30340;LLaMA&#27169;&#22411;&#20934;&#30830;&#29575;&#36739;&#39640;&#65292;&#20294;&#20063;&#23384;&#22312;&#19981;&#36275;&#20043;&#22788;&#65292;&#34920;&#26126;&#21033;&#29992;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22810;&#35821;&#31181;&#24037;&#20855;&#30340;&#21069;&#26223;&#24182;&#19981;&#26126;&#26174;&#12290;</title><link>http://arxiv.org/abs/2305.13675</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#36824;&#26159;&#21333;&#19968;&#35821;&#31181;&#65311;&#22522;&#20110;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#30334;&#31185;&#30693;&#35782;&#26816;&#32034;&#33021;&#21147;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Polyglot or Not? Measuring Multilingual Encyclopedic Knowledge Retrieval from Foundation Language Models. (arXiv:2305.13675v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#36328;&#36234;&#22810;&#31181;&#35821;&#35328;&#12289;&#20027;&#39064;&#21644;&#19978;&#19979;&#25991;&#26469;&#26816;&#32034;&#30334;&#31185;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;Meta&#30340;LLaMA&#27169;&#22411;&#20934;&#30830;&#29575;&#36739;&#39640;&#65292;&#20294;&#20063;&#23384;&#22312;&#19981;&#36275;&#20043;&#22788;&#65292;&#34920;&#26126;&#21033;&#29992;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22810;&#35821;&#31181;&#24037;&#20855;&#30340;&#21069;&#26223;&#24182;&#19981;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#36328;&#36234;&#22810;&#31181;&#35821;&#35328;&#12289;&#20027;&#39064;&#21644;&#19978;&#19979;&#25991;&#26469;&#26816;&#32034;&#30334;&#31185;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#20026;&#25903;&#25345;&#36825;&#19968;&#24037;&#20316;&#65292;&#25105;&#20204;&#21046;&#20316;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;20&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;303k&#20010;&#20107;&#23454;&#20851;&#32852;&#65292;&#24182;&#21046;&#23450;&#20102;&#19968;&#31181;&#26032;&#30340;&#21453;&#20107;&#23454;&#30693;&#35782;&#35780;&#20272;&#26041;&#24335;&#8220;&#22810;&#35821;&#31181;&#36824;&#26159;&#21333;&#19968;&#35821;&#31181;&#8221;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#23545;5&#20010;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#21450;&#22312;&#33521;&#35821;&#29615;&#22659;&#19979;&#23545;20&#31181;&#27169;&#22411;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#24863;&#20852;&#36259;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#24046;&#24322;&#26174;&#33879;&#65292;Meta&#30340;LLaMA&#27169;&#22411;&#22312;&#22810;&#35821;&#31181;&#21644;&#20165;&#33521;&#35821;&#35780;&#20272;&#20013;&#22343;&#25490;&#21517;&#31532;&#19968;&#12290;&#35823;&#24046;&#20998;&#26512;&#26174;&#31034;&#65292;LLaMA&#27169;&#22411;&#22312;&#26816;&#32034;&#20351;&#29992;&#35199;&#37324;&#23572;&#23383;&#27597;&#20070;&#20889;&#30340;&#35821;&#35328;&#30340;&#20107;&#23454;&#26102;&#26377;&#26174;&#33879;&#19981;&#36275;&#65292;&#21516;&#26102;&#22312;&#29702;&#35299;&#20027;&#35821;&#30340;&#20301;&#32622;&#21644;&#24615;&#21035;&#26041;&#38754;&#23384;&#22312;&#28431;&#27934;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#23558;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#29992;&#20316;&#30495;&#27491;&#30340;&#22810;&#35821;&#31181;&#24517;&#22791;&#24037;&#20855;&#26102;&#65292;&#23427;&#20204;&#34987;&#36171;&#20104;&#20102;&#26816;&#32034;&#20449;&#24687;&#30340;&#20219;&#21153;&#65292;&#36825;&#31181;&#25215;&#35834;&#22823;&#22823;&#38477;&#20302;&#20102;&#23427;&#20204;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we evaluate the capacity for foundation models to retrieve encyclopedic knowledge across a wide range of languages, topics, and contexts. To support this effort, we 1) produce a new dataset containing 303k factual associations in 20 different languages, 2) formulate a new counterfactual knowledge assessment, Polyglot or Not, and 3) benchmark 5 foundation models in a multilingual setting and a diverse set of 20 models in an English-only setting. We observed significant accuracy differences in models of interest, with Meta's LLaMA topping both the multilingual and English-only assessments. Error analysis reveals a significant deficiency in LLaMA's ability to retrieve facts in languages written in the Cyrillic script and gaps in its understanding of facts based on the location and gender of entailed subjects. Ultimately, we argue that the promise of utilizing foundation language models as bonafide polyglots is greatly diminished when they are tasked with retrieving informati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#65292;&#24182;&#36890;&#36807;&#26500;&#36896;&#20154;&#36896;&#25968;&#25454;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;transformers&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#30740;&#31350;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65292;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#35777;&#26126;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.13673</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#29702;&#23398;&#65306;&#31532;&#19968;&#37096;&#20998;&#65292;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics of Language Models: Part 1, Context-Free Grammar. (arXiv:2305.13673v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#65292;&#24182;&#36890;&#36807;&#26500;&#36896;&#20154;&#36896;&#25968;&#25454;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;transformers&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#30740;&#31350;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65292;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#35777;&#26126;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#20102;&#23454;&#39564;&#26469;&#30740;&#31350;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT&#65289;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;-&#20855;&#26377;&#26641;&#29366;&#32467;&#26500;&#30340;&#22810;&#26679;&#21270;&#35821;&#35328;&#31995;&#32479;&#65292;&#21487;&#25429;&#25417;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#65292;&#31243;&#24207;&#21644;&#20154;&#31867;&#36923;&#36753;&#30340;&#26041;&#38754;&#12290;CFG&#19982;&#19979;&#25512;&#33258;&#21160;&#26426;&#19968;&#26679;&#22256;&#38590;&#65292;&#21487;&#33021;&#26159;&#27169;&#26865;&#20004;&#21487;&#30340;&#65292;&#22240;&#27492;&#39564;&#35777;&#23383;&#31526;&#20018;&#26159;&#21542;&#28385;&#36275;&#35268;&#21017;&#38656;&#35201;&#21160;&#24577;&#35268;&#21010;&#12290;&#25105;&#20204;&#26500;&#36896;&#20102;&#20154;&#36896;&#25968;&#25454;&#65292;&#24182;&#35777;&#26126;&#21363;&#20351;&#23545;&#20110;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;CFG&#65292;&#39044;&#35757;&#32451;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;transformers&#23398;&#20064;CFG&#32972;&#21518;&#30340;&#29289;&#29702;&#21407;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65288;&#22914;&#22312;&#23376;&#26641;&#36793;&#30028;&#19978;&#31934;&#30830;&#23450;&#20301;&#26641;&#33410;&#28857;&#20449;&#24687;&#65289;&#65292;&#24182;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#36824;&#28085;&#30422;&#20102;&#19968;&#20123;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#23637;&#31034;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#20998;&#26512;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We design experiments to study $\textit{how}$ generative language models, like GPT, learn context-free grammars (CFGs) -- diverse language systems with a tree-like structure capturing many aspects of natural languages, programs, and human logics. CFGs are as hard as pushdown automata, and can be ambiguous so that verifying if a string satisfies the rules requires dynamic programming. We construct synthetic data and demonstrate that even for very challenging CFGs, pre-trained transformers can learn to generate sentences with near-perfect accuracy and remarkable $\textit{diversity}$.  More importantly, we delve into the $\textit{physical principles}$ behind how transformers learns CFGs. We discover that the hidden states within the transformer implicitly and $\textit{precisely}$ encode the CFG structure (such as putting tree node information exactly on the subtree boundary), and learn to form "boundary to boundary" attentions that resemble dynamic programming. We also cover some extensio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MixAlign&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#29992;&#25143;&#21644;&#30693;&#35782;&#24211;&#20132;&#20114;&#65292;&#23454;&#29616;&#33258;&#21160;&#30340;&#38382;&#39064;-&#30693;&#35782;&#23545;&#40784;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#22240;&#26080;&#27861;&#27491;&#30830;&#29702;&#35299;&#38382;&#39064;&#21644;&#30693;&#35782;&#32780;&#23548;&#33268;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13669</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#20114;&#24335;&#38382;&#39064;-&#30693;&#35782;&#23545;&#40784;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Language Model Hallucination with Interactive Question-Knowledge Alignment. (arXiv:2305.13669v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MixAlign&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#29992;&#25143;&#21644;&#30693;&#35782;&#24211;&#20132;&#20114;&#65292;&#23454;&#29616;&#33258;&#21160;&#30340;&#38382;&#39064;-&#30693;&#35782;&#23545;&#40784;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#22240;&#26080;&#27861;&#27491;&#30830;&#29702;&#35299;&#38382;&#39064;&#21644;&#30693;&#35782;&#32780;&#23548;&#33268;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#36817;&#26399;&#36827;&#23637;&#26174;&#33879;&#65292;&#20294;&#20173;&#38754;&#20020;&#24187;&#35273;&#38382;&#39064;&#65292;&#21487;&#33021;&#20250;&#29983;&#25104;&#35823;&#23548;&#24615;&#21644;&#19981;&#25903;&#25345;&#30340;&#22238;&#31572;&#12290;&#19968;&#31181;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20174;&#30693;&#35782;&#24211;&#20013;&#26816;&#32034;&#21644;&#25972;&#21512;&#25903;&#25345;&#35777;&#25454;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#30340;&#38382;&#39064;&#36890;&#24120;&#19982;&#23384;&#20648;&#30340;&#30693;&#35782;&#19981;&#22826;&#23545;&#40784;&#65292;&#22240;&#20026;&#20182;&#20204;&#22312;&#25552;&#38382;&#21069;&#19981;&#30693;&#36947;&#21487;&#29992;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#19981;&#23545;&#40784;&#21487;&#33021;&#38480;&#21046;&#35821;&#35328;&#27169;&#22411;&#23450;&#20301;&#21644;&#21033;&#29992;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#21487;&#33021;&#36843;&#20351;&#20854;&#36890;&#36807;&#24573;&#30053;&#25110;&#35206;&#30422;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#32780;&#20135;&#29983;&#24187;&#35273;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; MixAlign&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#23427;&#19982;&#29992;&#25143;&#21644;&#30693;&#35782;&#24211;&#20132;&#20114;&#20197;&#33719;&#24471;&#24182;&#25972;&#21512;&#20851;&#20110;&#29992;&#25143;&#38382;&#39064;&#19982;&#23384;&#20648;&#20449;&#24687;&#30456;&#20851;&#24615;&#30340;&#28548;&#28165;&#20449;&#24687;&#12290; MixAlign &#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#33258;&#21160;&#38382;&#39064;-&#30693;&#35782;&#23545;&#40784;&#65292;&#24182;&#22312;&#38656;&#35201;&#26102;&#36890;&#36807;&#20154;&#24037;&#29992;&#25143;&#28548;&#28165;&#36827;&#19968;&#27493;&#22686;&#24378;&#36825;&#31181;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable recent advances in language models, they still struggle with the hallucination problem and can generate misleading and unsupported responses. A common approach to mitigate the hallucination issue is retrieving and incorporating supporting evidence from a knowledge base. However, user questions usually do not align well with the stored knowledge, as they are unaware of the information available before asking questions. This misalignment can limit the language model's ability to locate and utilize the knowledge, potentially forcing it to hallucinate by ignoring or overriding the retrieved evidence. To address this issue, we introduce MixAlign, a framework that interacts with both the user and the knowledge base to obtain and integrate clarifications on how the user question relates to the stored information. MixAlign employs a language model to achieve automatic question-knowledge alignment and, if necessary, further enhances this alignment through human user clari
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20855;&#20307;&#27169;&#25311;&#20013;&#30340;&#26234;&#33021;&#20307;&#32463;&#39564;&#23558;&#19978;&#19979;&#25991;&#21270;&#30340;&#35789;&#21521;&#37327;&#25509;&#22320;&#21040;&#29289;&#20307;&#34920;&#31034;&#20013;&#12290;&#32467;&#26524;&#21457;&#29616;&#25509;&#22320;&#23545;&#35937;&#26631;&#35760;&#21521;&#37327;&#27604;&#25509;&#22320;&#21160;&#35789;&#21644;&#23646;&#24615;&#26631;&#35760;&#21521;&#37327;&#26356;&#26377;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2305.13668</link><description>&lt;p&gt;
&#36890;&#36807;&#30456;&#20284;&#24230;&#23398;&#20064;&#22312;&#20855;&#20307;&#27169;&#25311;&#20013;&#23545;&#27010;&#24565;&#35789;&#27719;&#36827;&#34892;&#23450;&#20301;&#21644;&#21306;&#20998;
&lt;/p&gt;
&lt;p&gt;
Grounding and Distinguishing Conceptual Vocabulary Through Similarity Learning in Embodied Simulations. (arXiv:2305.13668v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20855;&#20307;&#27169;&#25311;&#20013;&#30340;&#26234;&#33021;&#20307;&#32463;&#39564;&#23558;&#19978;&#19979;&#25991;&#21270;&#30340;&#35789;&#21521;&#37327;&#25509;&#22320;&#21040;&#29289;&#20307;&#34920;&#31034;&#20013;&#12290;&#32467;&#26524;&#21457;&#29616;&#25509;&#22320;&#23545;&#35937;&#26631;&#35760;&#21521;&#37327;&#27604;&#25509;&#22320;&#21160;&#35789;&#21644;&#23646;&#24615;&#26631;&#35760;&#21521;&#37327;&#26356;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36890;&#36807;&#20855;&#20307;&#27169;&#25311;&#25910;&#38598;&#30340;&#26234;&#33021;&#20307;&#32463;&#39564;&#65292;&#23558;&#19978;&#19979;&#25991;&#21270;&#30340;&#35789;&#21521;&#37327;&#25509;&#22320;&#21040;&#29289;&#20307;&#34920;&#31034;&#20013;&#12290;&#25105;&#20204;&#20351;&#29992;&#30456;&#20284;&#24230;&#23398;&#20064;&#26469;&#27604;&#36739;&#19981;&#21516;&#23545;&#35937;&#31867;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#21462;&#19982;&#29289;&#20307;&#34892;&#20026;&#30456;&#20851;&#30340;&#20849;&#21516;&#29305;&#24449;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#20223;&#23556;&#21464;&#25442;&#26469;&#35745;&#31639;&#20174;&#19981;&#21516;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#21521;&#37327;&#21040;&#36825;&#20010;&#23398;&#20064;&#31354;&#38388;&#30340;&#25237;&#24433;&#30697;&#38453;&#65292;&#24182;&#35780;&#20272;&#26159;&#21542;&#23558;&#36716;&#25442;&#21518;&#30340;&#26631;&#35760;&#21521;&#37327;&#30340;&#26032;&#27979;&#35797;&#23454;&#20363;&#27491;&#30830;&#22320;&#35782;&#21035;&#20026;&#23545;&#35937;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#27491;&#30830;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#22235;&#31181;&#19981;&#21516;&#36716;&#25442;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#30340;&#29305;&#24615;&#65292;&#24182;&#34920;&#26126;&#25509;&#22320;&#23545;&#35937;&#26631;&#35760;&#21521;&#37327;&#36890;&#24120;&#27604;&#25509;&#22320;&#21160;&#35789;&#21644;&#23646;&#24615;&#26631;&#35760;&#21521;&#37327;&#26356;&#26377;&#24110;&#21161;&#65292;&#36825;&#21453;&#26144;&#20102;&#26089;&#26399;&#31867;&#27604;&#25512;&#29702;&#21644;&#24515;&#29702;&#35821;&#35328;&#23398;&#25991;&#29486;&#20013;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel method for using agent experiences gathered through an embodied simulation to ground contextualized word vectors to object representations. We use similarity learning to make comparisons between different object types based on their properties when interacted with, and to extract common features pertaining to the objects' behavior. We then use an affine transformation to calculate a projection matrix that transforms contextualized word vectors from different transformer-based language models into this learned space, and evaluate whether new test instances of transformed token vectors identify the correct concept in the object embedding space. Our results expose properties of the embedding spaces of four different transformer models and show that grounding object token vectors is usually more helpful to grounding verb and attribute token vectors than the reverse, which reflects earlier conclusions in the analogical reasoning and psycholinguistic literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20174;&#27169;&#22411;&#20998;&#24067;&#20013;&#37319;&#26679;&#26469;&#32531;&#35299;NATs&#23398;&#20064;&#22810;&#27169;&#24577;&#25968;&#25454;&#20998;&#24067;&#30340;&#22256;&#38590;&#65292;&#24182;&#23548;&#20986;&#23545;&#27604;&#32422;&#26463;&#26469;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#22312;&#26426;&#22120;&#32763;&#35793;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#25913;&#20889;&#19977;&#20010;&#20219;&#21153;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#38750;&#33258;&#22238;&#24402;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.13667</link><description>&lt;p&gt;
&#20248;&#21270;&#23545;&#27604;&#23398;&#20064;&#30340;&#38750;&#33258;&#22238;&#24402;Transformer
&lt;/p&gt;
&lt;p&gt;
Optimizing Non-Autoregressive Transformers with Contrastive Learning. (arXiv:2305.13667v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20174;&#27169;&#22411;&#20998;&#24067;&#20013;&#37319;&#26679;&#26469;&#32531;&#35299;NATs&#23398;&#20064;&#22810;&#27169;&#24577;&#25968;&#25454;&#20998;&#24067;&#30340;&#22256;&#38590;&#65292;&#24182;&#23548;&#20986;&#23545;&#27604;&#32422;&#26463;&#26469;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#22312;&#26426;&#22120;&#32763;&#35793;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#25913;&#20889;&#19977;&#20010;&#20219;&#21153;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#38750;&#33258;&#22238;&#24402;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;Transformer (NATs) &#21487;&#20197;&#21516;&#26102;&#39044;&#27979;&#25152;&#26377;&#21333;&#35789;&#65292;&#32780;&#19981;&#26159;&#25353;&#39034;&#24207;&#39044;&#27979;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#33258;&#22238;&#24402;Transformer&#65288;ATs&#65289;&#30340;&#25512;&#26029;&#24310;&#36831;&#12290;&#23427;&#20204;&#22312;&#26426;&#22120;&#32763;&#35793;&#20197;&#21450;&#35768;&#22810;&#20854;&#20182;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;NATs &#38271;&#26399;&#20197;&#26469;&#38754;&#20020;&#30340;&#25361;&#25112;&#26159;&#23398;&#20064;&#22810;&#27169;&#24577;&#25968;&#25454;&#20998;&#24067;&#65292;&#36825;&#26159; NATs &#21644; ATs &#24615;&#33021;&#24046;&#36317;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20174;&#27169;&#22411;&#20998;&#24067;&#20013;&#37319;&#26679;&#26469;&#32531;&#35299;&#27169;&#24577;&#23398;&#20064;&#30340;&#22256;&#38590;&#65292;&#32780;&#19981;&#26159;&#20174;&#25968;&#25454;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#25105;&#20204;&#23548;&#20986;&#23545;&#27604;&#32422;&#26463;&#26469;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#23558;&#27492;&#32467;&#26524;&#30340;&#30446;&#26631;&#19982;&#26368;&#20808;&#36827;&#30340; NAT &#26550;&#26500; DA-Transformer &#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#25913;&#20889;&#36825;3&#20010;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#26816;&#39564;&#65292;&#20849;&#20351;&#29992;&#20102;5&#31181;&#22522;&#20934;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20197;&#21069;&#30340;&#38750;&#33258;&#22238;&#24402;&#22522;&#32447;&#65292;&#24182;&#30830;&#31435;&#20102;
&lt;/p&gt;
&lt;p&gt;
Non-autoregressive Transformers (NATs) reduce the inference latency of Autoregressive Transformers (ATs) by predicting words all at once rather than in sequential order. They have achieved remarkable progress in machine translation as well as many other applications. However, a long-standing challenge for NATs is the learning of multi-modality data distribution, which is the main cause of the performance gap between NATs and ATs. In this paper, we propose to ease the difficulty of modality learning via sampling from the model distribution instead of the data distribution. We derive contrastive constraints to stabilize the training process and integrate this resulting objective with the state-of-the-art NAT architecture DA-Transformer. Our model \method is examined on 3 different tasks, including machine translation, text summarization, and paraphrasing with 5 benchmarks. Results show that our approach outperforms previous non-autoregressive baselines by a significant margin and establi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#33021;&#35823;&#29992;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#25351;&#20986;LLM&#21487;&#20197;&#20316;&#20026;&#26377;&#25928;&#30340;&#35823;&#23548;&#24615;&#20449;&#24687;&#29983;&#25104;&#22120;&#65292;&#23548;&#33268;&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;ODQA&#65289;&#31995;&#32479;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#65292;&#24182;&#23581;&#35797;&#25552;&#20986;&#19977;&#31181;&#38450;&#24481;&#31574;&#30053;&#65306;&#25552;&#31034;&#65292;&#35823;&#25253;&#26816;&#27979;&#21644;&#22823;&#22810;&#25968;&#25237;&#31080;&#12290;</title><link>http://arxiv.org/abs/2305.13661</link><description>&lt;p&gt;
&#35770;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#20449;&#24687;&#27745;&#26579;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
On the Risk of Misinformation Pollution with Large Language Models. (arXiv:2305.13661v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#33021;&#35823;&#29992;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#25351;&#20986;LLM&#21487;&#20197;&#20316;&#20026;&#26377;&#25928;&#30340;&#35823;&#23548;&#24615;&#20449;&#24687;&#29983;&#25104;&#22120;&#65292;&#23548;&#33268;&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;ODQA&#65289;&#31995;&#32479;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#65292;&#24182;&#23581;&#35797;&#25552;&#20986;&#19977;&#31181;&#38450;&#24481;&#31574;&#30053;&#65306;&#25552;&#31034;&#65292;&#35823;&#25253;&#26816;&#27979;&#21644;&#22823;&#22810;&#25968;&#25237;&#31080;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#28508;&#22312;&#35823;&#29992;&#65292;&#25506;&#35752;&#20102;&#20854;&#29983;&#25104;&#21487;&#20449;&#24182;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#20449;&#24687;&#24182;&#23545;&#20449;&#24687;&#23494;&#38598;&#22411;&#24212;&#29992;&#31243;&#24207;&#65292;&#23588;&#20854;&#26159;&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;ODQA&#65289;&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#23041;&#32961;&#27169;&#22411;&#65292;&#24182;&#23545;&#26080;&#24847;&#21644;&#25925;&#24847;&#30340;&#28508;&#22312;&#35823;&#29992;&#22330;&#26223;&#36827;&#34892;&#27169;&#25311;&#65292;&#20197;&#35780;&#20272;LLM&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#20449;&#24687;&#19981;&#23454;&#30340;&#31243;&#24230;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;LLM&#21487;&#20197;&#20316;&#20026;&#26377;&#25928;&#30340;&#35823;&#23548;&#24615;&#20449;&#24687;&#29983;&#25104;&#22120;&#65292;&#23548;&#33268;ODQA&#31995;&#32479;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#12290;&#20026;&#20102;&#20943;&#36731;&#30001;LLM&#29983;&#25104;&#30340;&#38169;&#35823;&#20449;&#24687;&#24102;&#26469;&#30340;&#21361;&#23475;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19977;&#31181;&#38450;&#24481;&#31574;&#30053;&#65306;&#25552;&#31034;&#65292;&#35823;&#25253;&#26816;&#27979;&#21644;&#22823;&#22810;&#25968;&#25237;&#31080;&#12290;&#34429;&#28982;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#38450;&#24481;&#24615;&#31574;&#30053;&#26377;&#24076;&#26395;&#20135;&#29983;&#26126;&#26174;&#25928;&#26524;&#65292;&#20294;&#36824;&#38656;&#35201;&#20570;&#22823;&#37327;&#24037;&#20316;&#26469;&#24212;&#23545;&#38169;&#35823;&#20449;&#24687;&#27745;&#26579;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#38656;&#35201;&#36827;&#19968;&#27493;&#36827;&#34892;&#36328;&#23398;&#31185;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we comprehensively investigate the potential misuse of modern Large Language Models (LLMs) for generating credible-sounding misinformation and its subsequent impact on information-intensive applications, particularly Open-Domain Question Answering (ODQA) systems. We establish a threat model and simulate potential misuse scenarios, both unintentional and intentional, to assess the extent to which LLMs can be utilized to produce misinformation. Our study reveals that LLMs can act as effective misinformation generators, leading to a significant degradation in the performance of ODQA systems. To mitigate the harm caused by LLM-generated misinformation, we explore three defense strategies: prompting, misinformation detection, and majority voting. While initial results show promising trends for these defensive strategies, much more work needs to be done to address the challenge of misinformation pollution. Our work highlights the need for further research and interdisciplinary
&lt;/p&gt;</description></item><item><title>GDP-Zero&#26159;&#19968;&#31181;&#20351;&#29992;Open-Loop MCTS&#36827;&#34892;&#30446;&#26631;&#23548;&#21521;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#27169;&#22411;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#31574;&#30053;&#20808;&#39564;&#12289;&#20215;&#20540;&#20989;&#25968;&#12289;&#29992;&#25143;&#27169;&#25311;&#22120;&#21644;&#31995;&#32479;&#27169;&#22411;&#65292;&#22312;&#30446;&#26631;&#23548;&#21521;&#20219;&#21153;&#20013;&#20248;&#20110;ChatGPT&#12290;</title><link>http://arxiv.org/abs/2305.13660</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;Monte-Carlo&#26641;&#25628;&#32034;&#29992;&#20110;&#30446;&#26631;&#23548;&#21521;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Prompt-Based Monte-Carlo Tree Search for Goal-Oriented Dialogue Policy Planning. (arXiv:2305.13660v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13660
&lt;/p&gt;
&lt;p&gt;
GDP-Zero&#26159;&#19968;&#31181;&#20351;&#29992;Open-Loop MCTS&#36827;&#34892;&#30446;&#26631;&#23548;&#21521;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#27169;&#22411;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#31574;&#30053;&#20808;&#39564;&#12289;&#20215;&#20540;&#20989;&#25968;&#12289;&#29992;&#25143;&#27169;&#25311;&#22120;&#21644;&#31995;&#32479;&#27169;&#22411;&#65292;&#22312;&#30446;&#26631;&#23548;&#21521;&#20219;&#21153;&#20013;&#20248;&#20110;ChatGPT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#23548;&#21521;&#30340;&#23545;&#35805;&#35268;&#21010;&#36890;&#24120;&#38656;&#35201;&#27169;&#25311;&#26410;&#26469;&#30340;&#23545;&#35805;&#20132;&#20114;&#24182;&#20272;&#35745;&#20219;&#21153;&#36827;&#23637;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#26041;&#27861;&#32771;&#34385;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#25191;&#34892;&#21069;&#30651;&#25628;&#32034;&#31639;&#27861;&#65292;&#20363;&#22914;A *&#25628;&#32034;&#21644;Monte Carlo Tree Search&#65288;MCTS&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35757;&#32451;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#65292;&#24403;&#38754;&#20020;&#22024;&#26434;&#30340;&#27880;&#37322;&#25110;&#20302;&#36164;&#28304;&#35774;&#32622;&#26102;&#20250;&#24102;&#26469;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;GDP-Zero&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;Open-Loop MCTS&#36827;&#34892;&#30446;&#26631;&#23548;&#21521;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#27169;&#22411;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;GDP-Zero&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26641;&#25628;&#32034;&#26399;&#38388;&#20805;&#24403;&#31574;&#30053;&#20808;&#39564;&#12289;&#20215;&#20540;&#20989;&#25968;&#12289;&#29992;&#25143;&#27169;&#25311;&#22120;&#21644;&#31995;&#32479;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#30446;&#26631;&#23548;&#21521;&#20219;&#21153;PersuasionForGood&#19978;&#35780;&#20272;&#20102;GDP-Zero&#65292;&#24182;&#21457;&#29616;&#20854;&#21709;&#24212;&#27604;ChatGPT&#26356;&#21463;&#27426;&#36814;&#65292;&#36798;&#21040;&#20102;59.32&#65285;&#65292;&#22312;&#20132;&#20114;&#35780;&#20272;&#26399;&#38388;&#27604;ChatGPT&#26356;&#26377;&#35828;&#26381;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Planning for goal-oriented dialogue often requires simulating future dialogue interactions and estimating task progress. Many approaches thus consider training neural networks to perform look-ahead search algorithms such as A* search and Monte Carlo Tree Search (MCTS). However, this training often require abundant annotated data, which creates challenges when faced with noisy annotations or low-resource settings. We introduce GDP-Zero, an approach using Open-Loop MCTS to perform goal-oriented dialogue policy planning without any model training. GDP-Zero prompts a large language model to act as a policy prior, value function, user simulator, and system model during the tree search. We evaluate GDP-Zero on the goal-oriented task PersuasionForGood, and find that its responses are preferred over ChatGPT up to 59.32% of the time, and are rated more persuasive than ChatGPT during interactive evaluations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#33258;&#21160;&#35789;&#24418;&#21464;&#21270;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;StemCorrupt&#24102;&#26469;&#30340;&#26681;&#26412;&#24615;&#21464;&#21270;&#65292;&#24182;&#35777;&#26126;&#36873;&#25321;&#39640;&#22810;&#26679;&#24615;&#21644;&#39640;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#28857;&#23376;&#38598;&#26159;&#25552;&#39640;&#20854;&#25968;&#25454;&#25928;&#29575;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;StemCorrupt&#33021;&#22815;&#23398;&#20064;&#21487;&#25512;&#24191;&#30340;&#35789;&#24418;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2305.13658</link><description>&lt;p&gt;
&#33258;&#21160;&#35789;&#24418;&#21464;&#21270;&#20013;&#30340;&#32452;&#21512;&#25968;&#25454;&#22686;&#24378;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Understanding compositional data augmentation in automatic morphological inflection. (arXiv:2305.13658v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#33258;&#21160;&#35789;&#24418;&#21464;&#21270;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;StemCorrupt&#24102;&#26469;&#30340;&#26681;&#26412;&#24615;&#21464;&#21270;&#65292;&#24182;&#35777;&#26126;&#36873;&#25321;&#39640;&#22810;&#26679;&#24615;&#21644;&#39640;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#28857;&#23376;&#38598;&#26159;&#25552;&#39640;&#20854;&#25968;&#25454;&#25928;&#29575;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;StemCorrupt&#33021;&#22815;&#23398;&#20064;&#21487;&#25512;&#24191;&#30340;&#35789;&#24418;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#31232;&#30095;&#30340;&#38382;&#39064;&#65292;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20302;&#36164;&#28304;&#30340;&#33258;&#21160;&#35789;&#24418;&#21464;&#21270;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#30340;&#20840;&#37096;&#24433;&#21709;&#20173;&#28982;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25581;&#31034;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;StemCorrupt&#30340;&#29702;&#35770;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#38543;&#26426;&#26367;&#25442;&#29616;&#26377;&#30340;&#40644;&#37329;&#26631;&#20934;&#35757;&#32451;&#26679;&#26412;&#20013;&#30340;&#35789;&#24178;&#23383;&#31526;&#26469;&#29983;&#25104;&#21512;&#25104;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;StemCorrupt&#24102;&#26469;&#20102;&#26681;&#26412;&#24615;&#30340;&#21464;&#21270;&#65292;&#22312;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#20013;&#23637;&#29616;&#20102;&#22266;&#26377;&#30340;&#32452;&#21512;&#36830;&#25509;&#32467;&#26500;&#12290;&#20026;&#20102;&#34917;&#20805;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;StemCorrupt&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;&#36890;&#36807;&#23545;&#19971;&#31181;&#31867;&#22411;&#23398;&#19981;&#21516;&#30340;&#35821;&#35328;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#36873;&#25321;&#39640;&#22810;&#26679;&#24615;&#21644;&#39640;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#28857;&#23376;&#38598;&#26174;&#33879;&#25552;&#39640;&#20102;StemCorrupt&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#30456;&#27604;&#31454;&#20105;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;StemCorrupt&#33021;&#22815;&#23398;&#20064;&#21487;&#25512;&#24191;&#30340;&#35789;&#24418;&#35268;&#21017;&#65292;&#36825;&#26159;&#36890;&#36807;&#20854;&#22312;&#22495;&#22806;&#20445;&#30041;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#30340;&#20248;&#24322;&#24615;&#33021;&#25152;&#35777;&#26126;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation techniques are widely used in low-resource automatic morphological inflection to address the issue of data sparsity. However, the full implications of these techniques remain poorly understood. In this study, we aim to shed light on the theoretical aspects of the data augmentation strategy StemCorrupt, a method that generates synthetic examples by randomly substituting stem characters in existing gold standard training examples. Our analysis uncovers that StemCorrupt brings about fundamental changes in the underlying data distribution, revealing inherent compositional concatenative structure. To complement our theoretical analysis, we investigate the data-efficiency of StemCorrupt. Through evaluation across a diverse set of seven typologically distinct languages, we demonstrate that selecting a subset of datapoints with both high diversity and high predictive uncertainty significantly enhances the data-efficiency of StemCorrupt compared to competitive baselines. Furth
&lt;/p&gt;</description></item><item><title>ChatGPT&#26694;&#26550;&#20316;&#20026;&#19968;&#31181;&#20010;&#20154;&#25968;&#25454;&#31185;&#23398;&#23478;&#65292;&#21487;&#20197;&#21033;&#29992;&#33258;&#28982;&#23545;&#35805;&#30028;&#38754;&#21327;&#21161;&#29992;&#25143;&#36827;&#34892;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.13657</link><description>&lt;p&gt;
ChatGPT&#20316;&#20026;&#20320;&#30340;&#20010;&#20154;&#25968;&#25454;&#31185;&#23398;&#23478;
&lt;/p&gt;
&lt;p&gt;
ChatGPT as your Personal Data Scientist. (arXiv:2305.13657v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13657
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26694;&#26550;&#20316;&#20026;&#19968;&#31181;&#20010;&#20154;&#25968;&#25454;&#31185;&#23398;&#23478;&#65292;&#21487;&#20197;&#21033;&#29992;&#33258;&#28982;&#23545;&#35805;&#30028;&#38754;&#21327;&#21161;&#29992;&#25143;&#36827;&#34892;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#25968;&#25454;&#30340;&#20852;&#36215;&#21152;&#24378;&#20102;&#38656;&#35201;&#39640;&#25928;&#26131;&#29992;&#30340;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855; (AutoML)&#12290;&#28982;&#32780;&#65292;&#29702;&#35299;&#29305;&#23450;&#39046;&#22495;&#30340;&#25968;&#25454;&#20197;&#21450;&#23450;&#20041;&#39044;&#27979;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#38656;&#35201;&#20154;&#24037;&#24178;&#39044;&#65292;&#20351;&#35813;&#36807;&#31243;&#32791;&#26102;&#19988;&#26080;&#27861;&#23436;&#20840;&#33258;&#21160;&#21270;&#12290;&#30456;&#21453;&#65292;&#26500;&#24819;&#19968;&#20010;&#26234;&#33021;&#20195;&#29702;&#65292;&#36890;&#36807;&#30452;&#35266;&#12289;&#33258;&#28982;&#30340;&#23545;&#35805;&#21327;&#21161;&#29992;&#25143;&#36827;&#34892; AutoML &#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#28145;&#20837;&#20102;&#35299;&#24213;&#23618;&#30340;&#26426;&#22120;&#23398;&#20064; (ML) &#36807;&#31243;&#12290;&#36825;&#20010;&#20195;&#29702;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#20934;&#30830;&#29702;&#35299;&#29992;&#25143;&#30340;&#39044;&#27979;&#30446;&#26631;&#65292;&#24182;&#22240;&#27492;&#21046;&#23450;&#31934;&#30830;&#30340; ML &#20219;&#21153;&#65292;&#30456;&#24212;&#22320;&#35843;&#25972;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#24182;&#26377;&#25928;&#22320;&#34920;&#36848;&#32467;&#26524;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110; ChatGPT &#30340;&#23545;&#35805;&#24615;&#25968;&#25454;&#31185;&#23398;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20316;&#20026;&#8220;&#20010;&#20154;&#25968;&#25454;&#31185;&#23398;&#23478;&#8221;&#30340;&#21021;&#27493;&#30446;&#26631;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (ChatGPT) &#26500;&#24314;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#26469;&#20132;&#20114;&#22320;&#36827;&#34892;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of big data has amplified the need for efficient, user-friendly automated machine learning (AutoML) tools. However, the intricacy of understanding domain-specific data and defining prediction tasks necessitates human intervention making the process time-consuming while preventing full automation. Instead, envision an intelligent agent capable of assisting users in conducting AutoML tasks through intuitive, natural conversations without requiring in-depth knowledge of the underlying machine learning (ML) processes. This agent's key challenge is to accurately comprehend the user's prediction goals and, consequently, formulate precise ML tasks, adjust data sets and model parameters accordingly, and articulate results effectively. In this paper, we take a pioneering step towards this ambitious goal by introducing a ChatGPT-based conversational data-science framework to act as a "personal data scientist". Precisely, we utilize Large Language Models (ChatGPT) to build a natural inte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21033;&#29992;&#35757;&#32451;&#38598;&#20013;&#23384;&#22312;&#20294;&#36890;&#24120;&#19981;&#25104;&#31435;&#30340;&#20266;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#37051;&#22495;&#20998;&#26512;&#26694;&#26550;&#20197;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#20266;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#27491;&#21017;&#21270;&#26041;&#27861;NFL&#65288;&#19981;&#35201;&#24536;&#35760;&#20320;&#30340;&#35821;&#35328;&#65289;&#36991;&#20813;&#20102;&#36825;&#31181;&#24773;&#20917;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.13654</link><description>&lt;p&gt;
&#29702;&#35299;&#21644;&#20943;&#23569;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#20266;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding and Mitigating Spurious Correlations in Text Classification. (arXiv:2305.13654v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21033;&#29992;&#35757;&#32451;&#38598;&#20013;&#23384;&#22312;&#20294;&#36890;&#24120;&#19981;&#25104;&#31435;&#30340;&#20266;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#37051;&#22495;&#20998;&#26512;&#26694;&#26550;&#20197;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#20266;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#27491;&#21017;&#21270;&#26041;&#27861;NFL&#65288;&#19981;&#35201;&#24536;&#35760;&#20320;&#30340;&#35821;&#35328;&#65289;&#36991;&#20813;&#20102;&#36825;&#31181;&#24773;&#20917;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21033;&#29992;&#35757;&#32451;&#38598;&#20013;&#23384;&#22312;&#20294;&#36890;&#24120;&#19981;&#25104;&#31435;&#30340;&#20266;&#30456;&#20851;&#24615;&#12290;&#20363;&#22914;&#24773;&#24863;&#20998;&#31867;&#22120;&#21487;&#33021;&#20250;&#38169;&#35823;&#22320;&#23398;&#20064;&#21040;&#20196;&#20154;&#24841;&#24742;&#30340;&#30005;&#24433;&#35780;&#35770;&#24635;&#26159;&#19982;&#8220;Spielberg&#8221;&#36825;&#20010;&#35789;&#30456;&#20851;&#32852;&#12290;&#20381;&#36182;&#20110;&#20266;&#30456;&#20851;&#24615;&#21487;&#33021;&#20250;&#23548;&#33268;&#27867;&#21270;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#65292;&#22240;&#27492;&#24212;&#35813;&#36991;&#20813;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37051;&#22495;&#20998;&#26512;&#26694;&#26550;&#26469;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#20266;&#30456;&#20851;&#24615;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#27491;&#21017;&#21270;&#26041;&#27861;NFL&#65288;&#19981;&#35201;&#24536;&#35760;&#20320;&#30340;&#35821;&#35328;&#65289;&#65292;&#20197;&#36991;&#20813;&#36825;&#31181;&#24773;&#20917;&#12290;&#22312;&#20004;&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;NFL&#30456;&#23545;&#20110;&#26631;&#20934;&#30340;&#24494;&#35843;&#31639;&#27861;&#22312;&#40065;&#26834;&#24615;&#26041;&#38754;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#32780;&#27809;&#26377;&#29306;&#29298;&#22312;&#25968;&#25454;&#20869;&#37096;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that deep learning models are prone to exploit spurious correlations that are present in the training set, yet may not hold true in general. A sentiment classifier may erroneously learn that the token spielberg is always tied to positive movie reviews. Relying on spurious correlations may lead to significant degradation in generalizability and should be avoided. In this paper, we propose a neighborhood analysis framework to explain how exactly language models exploit spurious correlations. Driven by the analysis, we propose a family of regularization methods, NFL (do Not Forget your Language) to prevent the situation. Experiments on two text classification tasks show that NFL brings a significant improvement over standard fine-tuning in terms of robustness without sacrificing in-distribution accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20004;&#31181;&#26032;&#25216;&#26415;&#65288;&#36328;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;&#21644;&#36845;&#20195;&#20266;&#26631;&#35760;&#65289;&#22914;&#20309;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#25216;&#26415;&#25552;&#39640;&#20102;Transducer&#22522;&#30784;ASR&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13652</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;&#21644;&#36845;&#20195;&#20266;&#26631;&#35760;&#22312;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual Knowledge Transfer and Iterative Pseudo-labeling for Low-Resource Speech Recognition with Transducers. (arXiv:2305.13652v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20004;&#31181;&#26032;&#25216;&#26415;&#65288;&#36328;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;&#21644;&#36845;&#20195;&#20266;&#26631;&#35760;&#65289;&#22914;&#20309;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#25216;&#26415;&#25552;&#39640;&#20102;Transducer&#22522;&#30784;ASR&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35821;&#38899;&#25216;&#26415;&#26085;&#30410;&#26222;&#21450;&#12290; &#28982;&#32780;&#65292;&#19981;&#21516;&#35821;&#35328;&#30340;&#20934;&#30830;&#24615;&#65292;&#21644;&#22240;&#27492;&#20135;&#29983;&#30340;&#20307;&#39564;&#65292;&#24046;&#24322;&#26174;&#33879;&#65292;&#36825;&#20351;&#24471;&#25216;&#26415;&#19981;&#22815;&#21253;&#23481;&#12290;&#19981;&#21516;&#35821;&#35328;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#26159;&#24433;&#21709;&#20934;&#30830;&#24615;&#30340;&#20851;&#38190;&#22240;&#32032;&#20043;&#19968;&#65292;&#23588;&#20854;&#26159;&#22312;&#35757;&#32451;&#20840;&#31070;&#32463;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#26102;&#12290;&#36328;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;&#21644;&#36845;&#20195;&#20266;&#26631;&#35760;&#26159;&#20004;&#31181;&#24050;&#34987;&#35777;&#26126;&#25104;&#21151;&#25552;&#39640;ASR&#31995;&#32479;&#20934;&#30830;&#24615;&#30340;&#25216;&#26415;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20687;&#20044;&#20811;&#20848;&#35821;&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35757;&#32451;&#19968;&#20010;&#20840;&#31070;&#32463;Transducer&#22522;&#30784;ASR&#31995;&#32479;&#65292;&#20197;&#26367;&#25442;&#27809;&#26377;&#25163;&#21160;&#27880;&#37322;&#35757;&#32451;&#25968;&#25454;&#30340;DNN-HMM&#28151;&#21512;&#31995;&#32479;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20351;&#29992;&#28151;&#21512;&#31995;&#32479;&#20135;&#29983;&#30340;&#36716;&#24405;&#25991;&#26412;&#35757;&#32451;&#30340;Transducer&#31995;&#32479;&#22312;&#35789;&#38169;&#35823;&#29575;&#26041;&#38754;&#23454;&#29616;&#20102;18%&#30340;&#38477;&#20302;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#30456;&#20851;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;&#21644;&#36845;&#20195;&#20266;&#26631;&#35760;&#30340;&#32452;&#21512;&#65292;&#21017;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;ASR&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Voice technology has become ubiquitous recently. However, the accuracy, and hence experience, in different languages varies significantly, which makes the technology not equally inclusive. The availability of data for different languages is one of the key factors affecting accuracy, especially in training of all-neural end-to-end automatic speech recognition systems.  Cross-lingual knowledge transfer and iterative pseudo-labeling are two techniques that have been shown to be successful for improving the accuracy of ASR systems, in particular for low-resource languages, like Ukrainian.  Our goal is to train an all-neural Transducer-based ASR system to replace a DNN-HMM hybrid system with no manually annotated training data. We show that the Transducer system trained using transcripts produced by the hybrid system achieves 18% reduction in terms of word error rate. However, using a combination of cross-lingual knowledge transfer from related languages and iterative pseudo-labeling, we ar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;kNN&#39044;&#27979;&#30340;&#32479;&#35745;&#20449;&#24687;&#26469;&#25913;&#21892;fine-tuning&#38454;&#27573;&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#34920;&#29616;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#26041;&#27861;&#25972;&#21512;kNN&#32479;&#35745;&#20449;&#24687;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;BLEU&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.13648</link><description>&lt;p&gt;
&#26080;&#21442;&#25968;&#65292;&#26368;&#36817;&#37051;&#36741;&#21161;&#24494;&#35843;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Non-parametric, Nearest-neighbor-assisted Fine-tuning for Neural Machine Translation. (arXiv:2305.13648v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;kNN&#39044;&#27979;&#30340;&#32479;&#35745;&#20449;&#24687;&#26469;&#25913;&#21892;fine-tuning&#38454;&#27573;&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#34920;&#29616;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#26041;&#27861;&#25972;&#21512;kNN&#32479;&#35745;&#20449;&#24687;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;BLEU&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#37051;&#31639;&#27861;&#24050;&#32463;&#34987;&#29992;&#20110;&#36741;&#21161;&#35821;&#35328;&#27169;&#22411;&#21644;&#26426;&#22120;&#32763;&#35793;&#35299;&#30721;&#22120;&#31561;&#29983;&#25104;&#27169;&#22411;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#31181;&#38750;&#21442;&#25968;&#27169;&#22411;&#22914;&#20309;&#36890;&#36807;kNN&#39044;&#27979;&#30340;&#32479;&#35745;&#20449;&#24687;&#26469;&#25913;&#36827;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;fine-tuning&#38454;&#27573;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25506;&#31350;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#22914;&#36890;&#36807;&#38376;&#25511;&#26426;&#21046;&#36827;&#34892;&#28176;&#21464;&#32553;&#25918;&#12289;&#20351;&#29992;kNN&#30340;&#30495;&#23454;&#27010;&#29575;&#20197;&#21450;&#24378;&#21270;&#23398;&#20064;&#31561;&#26041;&#27861;&#26469;&#25972;&#21512;kNN&#32479;&#35745;&#20449;&#24687;&#12290;&#23545;&#20110;&#22235;&#20010;&#26631;&#20934;&#39046;&#22495;&#30340;&#26426;&#22120;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#19982;&#32463;&#20856;&#30340;&#24494;&#35843;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25253;&#36947;&#20102;&#19977;&#31181;&#26041;&#27861;&#30340;&#19968;&#33268;&#25913;&#36827;&#65292;&#23545;&#20110;&#24503;&#33521;&#21644;&#33521;&#24503;&#32763;&#35793;&#65292;BLEU&#20998;&#21035;&#25552;&#39640;&#20102;1.45&#21644;1.28&#20998;&#12290;&#36890;&#36807;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#32763;&#35793;&#35821;&#27861;&#20851;&#31995;&#25110;&#20989;&#25968;&#35789;&#26102;&#65292;&#26377;&#30528;&#29305;&#21035;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-parametric, k-nearest-neighbor algorithms have recently made inroads to assist generative models such as language models and machine translation decoders. We explore whether such non-parametric models can improve machine translation models at the fine-tuning stage by incorporating statistics from the kNN predictions to inform the gradient updates for a baseline translation model. There are multiple methods which could be used to incorporate kNN statistics and we investigate gradient scaling by a gating mechanism, the kNN's ground truth probability, and reinforcement learning. For four standard in-domain machine translation datasets, compared with classic fine-tuning, we report consistent improvements of all of the three methods by as much as 1.45 BLEU and 1.28 BLEU for German-English and English-German translations respectively. Through qualitative analysis, we found particular improvements when it comes to translating grammatical relations or function words, which results in incre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;mFACT&#65292;&#21487;&#20197;&#22312;&#38750;&#33521;&#35821;&#25688;&#35201;&#20013;&#35780;&#20272;&#20854;&#24544;&#23454;&#24615;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#21152;&#26435;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#20943;&#23569;&#25688;&#35201;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13632</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#26816;&#27979;&#21644;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Detecting and Mitigating Hallucinations in Multilingual Summarisation. (arXiv:2305.13632v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;mFACT&#65292;&#21487;&#20197;&#22312;&#38750;&#33521;&#35821;&#25688;&#35201;&#20013;&#35780;&#20272;&#20854;&#24544;&#23454;&#24615;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#21152;&#26435;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#20943;&#23569;&#25688;&#35201;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24187;&#35273;&#23545;&#20110;&#25277;&#35937;&#25688;&#35201;&#30340;&#31070;&#32463;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#34429;&#28982;&#33258;&#21160;&#20135;&#29983;&#30340;&#25688;&#35201;&#21487;&#33021;&#27969;&#30021;&#65292;&#20294;&#36890;&#24120;&#32570;&#20047;&#23545;&#21407;&#22987;&#25991;&#26723;&#30340;&#24544;&#23454;&#24615;&#12290;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#65292;&#22914;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#31361;&#20986;&#12290;&#30001;&#20110;&#29616;&#26377;&#30340;&#24544;&#23454;&#24615;&#27979;&#37327;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#33521;&#35821;&#65292;&#22240;&#27492;&#22312;&#36328;&#35821;&#35328;&#29615;&#22659;&#20013;&#29978;&#33267;&#34913;&#37327;&#36825;&#31181;&#29616;&#35937;&#30340;&#31243;&#24230;&#20063;&#24456;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20316;&#32773;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;mFACT&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#33521;&#35821;&#30340;&#24544;&#23454;&#24615;&#27979;&#37327;&#32467;&#26524;&#20013;&#20511;&#37492;&#32763;&#35793;&#22522;&#30784;&#30693;&#35782;&#20026;&#38750;&#33521;&#35821;&#25688;&#35201;&#35780;&#20272;&#20854;&#24544;&#23454;&#24615;&#12290;&#28982;&#21518;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#20943;&#23569;&#24187;&#35273;&#65292;&#35813;&#26041;&#27861;&#23558;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#30340;&#25439;&#22833;&#20056;&#20197;&#20854;&#24544;&#23454;&#24615;&#24471;&#20998;&#12290;&#36890;&#36807;&#22810;&#31181;&#35821;&#35328;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;mFACT&#26159;&#26368;&#36866;&#21512;&#26816;&#27979;&#24187;&#35273;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#21457;&#29616;&#20182;&#20204;&#30340;&#25552;&#20986;&#30340;&#21152;&#26435;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hallucinations pose a significant challenge to the reliability of neural models for abstractive summarisation. While automatically generated summaries may be fluent, they often lack faithfulness to the original document. This issue becomes even more pronounced in low-resource settings, such as cross-lingual transfer. With the existing faithful metrics focusing on English, even measuring the extent of this phenomenon in cross-lingual settings is hard. To address this, we first develop a novel metric, mFACT, evaluating the faithfulness of non-English summaries, leveraging translation-based transfer from multiple English faithfulness metrics. We then propose a simple but effective method to reduce hallucinations with a cross-lingual transfer, which weighs the loss of each training example by its faithfulness score. Through extensive experiments in multiple languages, we demonstrate that mFACT is the metric that is most suited to detect hallucinations. Moreover, we find that our proposed l
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;EDIS&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;100&#19975;&#20010;&#22810;&#27169;&#24577;&#22270;&#20687;&#21644;&#25991;&#26412;&#37197;&#23545;&#65292;&#26088;&#22312;&#40723;&#21169;&#24320;&#21457;&#23454;&#29616;&#36328;&#27169;&#24577;&#20449;&#24687;&#34701;&#21512;&#21644;&#21305;&#37197;&#30340;&#26816;&#32034;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13631</link><description>&lt;p&gt;
&#22522;&#20110;&#23454;&#20307;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#20869;&#23481;&#22270;&#20687;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
EDIS: Entity-Driven Image Search over Multimodal Web Content. (arXiv:2305.13631v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13631
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;EDIS&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;100&#19975;&#20010;&#22810;&#27169;&#24577;&#22270;&#20687;&#21644;&#25991;&#26412;&#37197;&#23545;&#65292;&#26088;&#22312;&#40723;&#21169;&#24320;&#21457;&#23454;&#29616;&#36328;&#27169;&#24577;&#20449;&#24687;&#34701;&#21512;&#21644;&#21305;&#37197;&#30340;&#26816;&#32034;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#23454;&#38469;&#25628;&#32034;&#24212;&#29992;&#20013;&#23454;&#29616;&#22270;&#20687;&#26816;&#32034;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#38656;&#35201;&#22312;&#25968;&#25454;&#38598;&#35268;&#27169;&#12289;&#23454;&#20307;&#29702;&#35299;&#21644;&#22810;&#27169;&#24577;&#20449;&#24687;&#34701;&#21512;&#26041;&#38754;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Making image retrieval methods practical for real-world search applications requires significant progress in dataset scales, entity comprehension, and multimodal information fusion. In this work, we introduce \textbf{E}ntity-\textbf{D}riven \textbf{I}mage \textbf{S}earch (EDIS), a challenging dataset for cross-modal image search in the news domain. EDIS consists of 1 million web images from actual search engine results and curated datasets, with each image paired with a textual description. Unlike datasets that assume a small set of single-modality candidates, EDIS reflects real-world web image search scenarios by including a million multimodal image-text pairs as candidates. EDIS encourages the development of retrieval models that simultaneously address cross-modal information fusion and matching. To achieve accurate ranking results, a model must: 1) understand named entities and events from text queries, 2) ground entities onto images or text descriptions, and 3) effectively fuse tex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ContProto&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#33258;&#25105;&#35757;&#32451;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#20266;&#26631;&#35760;&#65292;&#32467;&#21512;&#34920;&#31034;&#23398;&#20064;&#21644;&#20266;&#26631;&#31614;&#31934;&#21270;&#65292;&#22312;&#19968;&#20010;&#19968;&#33268;&#30340;&#26694;&#26550;&#20013;&#25552;&#39640;&#20102;&#36328;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#33258;&#35757;&#32451;&#26041;&#27861;&#65307;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ContProto &#26041;&#27861;&#22312;&#22810;&#20010;&#36716;&#31227;&#23545;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.13628</link><description>&lt;p&gt;
&#21033;&#29992;&#23545;&#27604;&#33258;&#25105;&#35757;&#32451;&#21644;&#21407;&#22411;&#23398;&#20064;&#25913;&#36827;&#36328;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#33258;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Self-training for Cross-lingual Named Entity Recognition with Contrastive and Prototype Learning. (arXiv:2305.13628v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ContProto&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#33258;&#25105;&#35757;&#32451;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#20266;&#26631;&#35760;&#65292;&#32467;&#21512;&#34920;&#31034;&#23398;&#20064;&#21644;&#20266;&#26631;&#31614;&#31934;&#21270;&#65292;&#22312;&#19968;&#20010;&#19968;&#33268;&#30340;&#26694;&#26550;&#20013;&#25552;&#39640;&#20102;&#36328;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#33258;&#35757;&#32451;&#26041;&#27861;&#65307;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ContProto &#26041;&#27861;&#22312;&#22810;&#20010;&#36716;&#31227;&#23545;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36328;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#65292;&#33258;&#35757;&#32451;&#36890;&#24120;&#29992;&#20110;&#36890;&#36807;&#22312;&#20266;&#26631;&#35760;&#30446;&#26631;&#35821;&#35328;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#24357;&#21512;&#35821;&#35328;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30446;&#26631;&#35821;&#35328;&#24615;&#33021;&#19981;&#20339;&#65292;&#20266;&#26631;&#31614;&#36890;&#24120;&#23384;&#22312;&#22122;&#22768;&#65292;&#38480;&#21046;&#20102;&#25972;&#20307;&#24615;&#33021;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#22312;&#19968;&#20010;&#19968;&#33268;&#30340;&#26694;&#26550;&#20013;&#32467;&#21512;&#34920;&#31034;&#23398;&#20064;&#21644;&#20266;&#26631;&#31614;&#31934;&#21270;&#26469;&#25913;&#36827;&#36328;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#33258;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20027;&#35201;&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;1&#65289;&#23545;&#27604;&#33258;&#25105;&#35757;&#32451;&#21644;&#65288;2&#65289;&#22522;&#20110;&#21407;&#22411;&#30340;&#20266;&#26631;&#35760;&#12290;&#25105;&#20204;&#30340;&#23545;&#27604;&#33258;&#25105;&#35757;&#32451;&#36890;&#36807;&#20998;&#31163;&#19981;&#21516;&#31867;&#21035;&#30340;&#32858;&#31867;&#26469;&#20419;&#36827;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#24182;&#36890;&#36807;&#20135;&#29983;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#20043;&#38388;&#32039;&#23494;&#23545;&#40784;&#30340;&#34920;&#31034;&#26469;&#22686;&#24378;&#36328;&#35821;&#35328;&#21487;&#36716;&#31227;&#24615;&#12290;&#21516;&#26102;&#65292;&#22522;&#20110;&#21407;&#22411;&#30340;&#20266;&#26631;&#35760;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26377;&#25928;&#25552;&#39640;&#20102;&#20266;&#26631;&#31614;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#36716;&#31227;&#23545;&#19978;&#35780;&#20272;ContProto&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In cross-lingual named entity recognition (NER), self-training is commonly used to bridge the linguistic gap by training on pseudo-labeled target-language data. However, due to sub-optimal performance on target languages, the pseudo labels are often noisy and limit the overall performance. In this work, we aim to improve self-training for cross-lingual NER by combining representation learning and pseudo label refinement in one coherent framework. Our proposed method, namely ContProto mainly comprises two components: (1) contrastive self-training and (2) prototype-based pseudo-labeling. Our contrastive self-training facilitates span classification by separating clusters of different classes, and enhances cross-lingual transferability by producing closely-aligned representations between the source and target language. Meanwhile, prototype-based pseudo-labeling effectively improves the accuracy of pseudo labels during training. We evaluate ContProto on multiple transfer pairs, and experim
&lt;/p&gt;</description></item><item><title>Instruct-Align&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#40784;&#30340;&#36328;&#35821;&#35328;&#25945;&#23398;&#35843;&#25972;&#26694;&#26550;&#65292;&#20351;&#24471;&#25945;&#23398;&#35843;&#25972;&#30340;LLMs&#33021;&#22815;&#23398;&#20064;&#26032;&#35821;&#35328;&#65292;&#19988;&#19981;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2305.13627</link><description>&lt;p&gt;
Instruct-Align&#65306;&#36890;&#36807;&#22522;&#20110;&#23545;&#40784;&#30340;&#36328;&#35821;&#35328;&#25945;&#23398;&#23558;&#26032;&#35821;&#35328;&#25945;&#32473;LLM
&lt;/p&gt;
&lt;p&gt;
Instruct-Align: Teaching Novel Languages with to LLMs through Alignment-based Cross-Lingual Instruction. (arXiv:2305.13627v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13627
&lt;/p&gt;
&lt;p&gt;
Instruct-Align&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#40784;&#30340;&#36328;&#35821;&#35328;&#25945;&#23398;&#35843;&#25972;&#26694;&#26550;&#65292;&#20351;&#24471;&#25945;&#23398;&#35843;&#25972;&#30340;LLMs&#33021;&#22815;&#23398;&#20064;&#26032;&#35821;&#35328;&#65292;&#19988;&#19981;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#23398;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#22810;&#31181;&#35821;&#35328;&#21644;&#22810;&#31181;&#20219;&#21153;&#19978;&#30340;&#21331;&#36234;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#19981;&#21516;&#35821;&#35328;&#30340;&#27867;&#21270;&#33021;&#21147;&#20250;&#26377;&#25152;&#19981;&#21516;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#23569;&#25968;&#35821;&#35328;&#25110;&#32773;&#26159;&#26410;&#30693;&#35821;&#35328;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#21457;&#29616;&#65292;&#31616;&#21333;&#22320;&#23558;&#26032;&#35821;&#35328;&#36866;&#24212;&#21040;&#32463;&#36807;&#25945;&#23398;&#35843;&#25972;&#30340;LLM&#20013;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#20174;&#32780;&#23548;&#33268;&#36825;&#20123;LLM&#22833;&#21435;&#22810;&#20219;&#21153;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31216;&#20026;Instruct-Align&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#23545;&#40784;&#30340;&#36328;&#35821;&#35328;&#25945;&#23398;&#35843;&#25972;&#65292;&#20351;&#24471;&#32463;&#36807;&#25945;&#23398;&#35843;&#25972;&#30340;LLM&#33021;&#22815;&#23398;&#20064;&#21040;&#30475;&#19981;&#35265;&#30340;&#21644;&#20043;&#21069;&#23398;&#20064;&#30340;&#35821;&#35328;&#20043;&#38388;&#30340;&#36328;&#35821;&#35328;&#23545;&#40784;&#12290;&#25105;&#20204;&#22312;BLOOMZ-560M&#25968;&#25454;&#38598;&#19978;&#30340;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;Instruct-Align&#33021;&#22815;&#22312;&#20165;&#20351;&#29992;&#26377;&#38480;&#37327;&#30340;&#24179;&#34892;&#35821;&#26009;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23398;&#20064;&#26032;&#35821;&#35328;&#65292;&#24182;&#19988;&#36890;&#36807;&#25345;&#32493;&#30340;&#25945;&#23398;&#35843;&#25972;&#65292;&#38450;&#27490;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned large language models (LLMs) have shown remarkable generalization capability over multiple tasks in multiple languages. Nevertheless, their generalization towards different languages varies especially to underrepresented languages or even to unseen languages. Prior works on adapting new languages to LLMs find that naively adapting new languages to instruction-tuned LLMs will result in catastrophic forgetting, which in turn causes the loss of multitasking ability in these LLMs. To tackle this, we propose the Instruct-Align a.k.a (IA)$^1$ framework, which enables instruction-tuned LLMs to learn cross-lingual alignment between unseen and previously learned languages via alignment-based cross-lingual instruction-tuning. Our preliminary result on BLOOMZ-560M shows that (IA)$^1$ is able to learn a new language effectively with only a limited amount of parallel data and at the same time prevent catastrophic forgetting by applying continual instruction-tuning through experien
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#38024;&#23545;&#28548;&#28165;&#12289;&#30446;&#26631;&#23548;&#21521;&#21644;&#38750;&#21327;&#20316;&#23545;&#35805;&#65292;&#25552;&#20986;&#20102;Proactive Chain-of-Thought&#25552;&#31034;&#26041;&#26696;&#65292;&#20197;&#22686;&#24378;&#31995;&#32479;&#30340;&#20027;&#21160;&#24615;&#33021;&#21147;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#32463;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13626</link><description>&lt;p&gt;
&#28608;&#21169;&#21644;&#35780;&#20272;&#29992;&#20110;&#20027;&#21160;&#23545;&#35805;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#28548;&#28165;&#12289;&#30446;&#26631;&#23548;&#21521;&#21644;&#38750;&#21327;&#20316;&#24615;
&lt;/p&gt;
&lt;p&gt;
Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration. (arXiv:2305.13626v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#38024;&#23545;&#28548;&#28165;&#12289;&#30446;&#26631;&#23548;&#21521;&#21644;&#38750;&#21327;&#20316;&#23545;&#35805;&#65292;&#25552;&#20986;&#20102;Proactive Chain-of-Thought&#25552;&#31034;&#26041;&#26696;&#65292;&#20197;&#22686;&#24378;&#31995;&#32479;&#30340;&#20027;&#21160;&#24615;&#33021;&#21147;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#32463;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#22914;ChatGPT&#65292;&#22312;&#19978;&#19979;&#25991;&#29702;&#35299;&#21644;&#21709;&#24212;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#24322;&#24120;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#20294;&#26159;&#65292;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20173;&#28982;&#23384;&#22312;&#38480;&#21046;&#65292;&#20363;&#22914;&#23545;&#27169;&#31946;&#26597;&#35810;&#25552;&#20379;&#38543;&#26426;&#29468;&#27979;&#31572;&#26696;&#25110;&#26080;&#27861;&#25298;&#32477;&#29992;&#25143;&#30340;&#35831;&#27714;&#65292;&#36825;&#20123;&#37117;&#34987;&#35748;&#20026;&#26159;&#23545;&#35805;&#20195;&#29702;&#30340;&#20027;&#21160;&#24615;&#26041;&#38754;&#12290;&#36825;&#24341;&#21457;&#20102;LLM&#22522;&#20110;&#23545;&#35805;&#31995;&#32479;&#26159;&#21542;&#33021;&#22815;&#22788;&#29702;&#20027;&#21160;&#23545;&#35805;&#38382;&#39064;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#22522;&#20110;LLM&#30340;&#23545;&#35805;&#31995;&#32479;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#20855;&#20307;&#20851;&#27880;&#20027;&#21160;&#23545;&#35805;&#31995;&#32479;&#30340;&#19977;&#20010;&#26041;&#38754;&#65306;&#28548;&#28165;&#12289;&#30446;&#26631;&#23548;&#21521;&#21644;&#38750;&#21327;&#20316;&#23545;&#35805;&#12290;&#20026;&#20102;&#35302;&#21457;LLM&#30340;&#20027;&#21160;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Proactive Chain-of-Thought&#25552;&#31034;&#26041;&#26696;&#65292;&#23427;&#36890;&#36807;&#23545;&#25551;&#36848;&#24615;&#25512;&#29702;&#38142;&#30340;&#30446;&#26631;&#35268;&#21010;&#33021;&#21147;&#22686;&#24378;&#20102;LLM&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23454;&#35777;&#32467;&#26524;&#20197;&#20419;&#36827;&#26410;&#26469;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational systems based on Large Language Models (LLMs), such as ChatGPT, show exceptional proficiency in context understanding and response generation. However, despite their impressive capabilities, they still possess limitations, such as providing randomly-guessed answers to ambiguous queries or failing to refuse users' requests, both of which are considered aspects of a conversational agent's proactivity. This raises the question of whether LLM-based conversational systems are equipped to handle proactive dialogue problems. In this work, we conduct a comprehensive analysis of LLM-based conversational systems, specifically focusing on three aspects of proactive dialogue systems: clarification, target-guided, and non-collaborative dialogues. To trigger the proactivity of LLMs, we propose the Proactive Chain-of-Thought prompting scheme, which augments LLMs with the goal planning capability over descriptive reasoning chains. Empirical findings are discussed to promote future studi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Semantic Fusion&#8221;&#30340;&#36890;&#29992;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#20004;&#20010;&#25110;&#22810;&#20010;&#19981;&#21516;&#27169;&#24577;&#30340;&#20869;&#23481;&#23457;&#26680;&#27169;&#22411;&#26469;&#25552;&#39640;&#22810;&#23186;&#20307;&#20869;&#23481;&#23457;&#26680;&#36719;&#20214;&#30340;&#39564;&#35777;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#32463;&#36807;&#22823;&#35268;&#27169;&#22810;&#23186;&#20307;&#20869;&#23481;&#23457;&#26680;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#39564;&#35777;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13623</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#34701;&#21512;&#39564;&#35777;&#22810;&#23186;&#20307;&#20869;&#23481;&#23457;&#26680;&#36719;&#20214;
&lt;/p&gt;
&lt;p&gt;
Validating Multimedia Content Moderation Software via Semantic Fusion. (arXiv:2305.13623v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13623
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Semantic Fusion&#8221;&#30340;&#36890;&#29992;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#20004;&#20010;&#25110;&#22810;&#20010;&#19981;&#21516;&#27169;&#24577;&#30340;&#20869;&#23481;&#23457;&#26680;&#27169;&#22411;&#26469;&#25552;&#39640;&#22810;&#23186;&#20307;&#20869;&#23481;&#23457;&#26680;&#36719;&#20214;&#30340;&#39564;&#35777;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#32463;&#36807;&#22823;&#35268;&#27169;&#22810;&#23186;&#20307;&#20869;&#23481;&#23457;&#26680;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#39564;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#65292;&#22914;Facebook&#21644;TikTok&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#24050;&#32463;&#25913;&#21464;&#20102;&#20154;&#31867;&#31038;&#20250;&#30340;&#20132;&#27969;&#21644;&#20869;&#23481;&#21457;&#24067;&#26041;&#24335;&#12290;&#22312;&#36825;&#20123;&#24179;&#21488;&#19978;&#65292;&#29992;&#25143;&#21487;&#20197;&#21457;&#24067;&#32467;&#21512;&#25991;&#26412;&#65292;&#38899;&#39057;&#65292;&#22270;&#20687;&#21644;&#35270;&#39057;&#20256;&#36882;&#20449;&#24687;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22810;&#23186;&#20307;&#20869;&#23481;&#21457;&#24067;&#35774;&#26045;&#26085;&#30410;&#34987;&#21033;&#29992;&#26469;&#20256;&#25773;&#26377;&#23475;&#20869;&#23481;&#65292;&#22914;&#20167;&#24680;&#35328;&#35770;&#65292;&#24694;&#24847;&#24191;&#21578;&#21644;&#33394;&#24773;&#20869;&#23481;&#12290;&#20026;&#27492;&#65292;&#20869;&#23481;&#23457;&#26680;&#36719;&#20214;&#24050;&#32463;&#24191;&#27867;&#37096;&#32626;&#22312;&#36825;&#20123;&#24179;&#21488;&#19978;&#65292;&#20197;&#26816;&#27979;&#21644;&#23631;&#34109;&#26377;&#23475;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20869;&#23481;&#23457;&#26680;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#36328;&#22810;&#31181;&#27169;&#24335;&#29702;&#35299;&#20449;&#24687;&#30340;&#22256;&#38590;&#65292;&#29616;&#26377;&#30340;&#20869;&#23481;&#23457;&#26680;&#36719;&#20214;&#21487;&#33021;&#20250;&#22833;&#36133;&#65292;&#23548;&#33268;&#26497;&#20026;&#36127;&#38754;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24341;&#20837;Semantic Fusion&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#32780;&#26377;&#25928;&#30340;&#39564;&#35777;&#22810;&#23186;&#20307;&#20869;&#23481;&#23457;&#26680;&#36719;&#20214;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#34701;&#21512;&#20004;&#20010;&#25110;&#26356;&#22810;&#22522;&#20110;&#19981;&#21516;&#27169;&#24577;&#65288;&#22914;&#25991;&#26412;&#65292;&#38899;&#39057;&#65292;&#22270;&#20687;&#21644;&#35270;&#39057;&#65289;&#30340;&#29616;&#26377;&#20869;&#23481;&#23457;&#26680;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#22810;&#27169;&#24577;&#30340;&#20114;&#34917;&#24615;&#21644;&#19968;&#33268;&#24615;&#26469;&#25552;&#39640;&#20869;&#23481;&#23457;&#26680;&#36719;&#20214;&#30340;&#39564;&#35777;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#23457;&#26680;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;Semantic Fusion&#65292;&#24182;&#26174;&#31034;&#23427;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#39564;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exponential growth of social media platforms, such as Facebook and TikTok, has revolutionized communication and content publication in human society. Users on these platforms can publish multimedia content that delivers information via the combination of text, audio, images, and video. Meanwhile, the multimedia content release facility has been increasingly exploited to propagate toxic content, such as hate speech, malicious advertisements, and pornography. To this end, content moderation software has been widely deployed on these platforms to detect and blocks toxic content. However, due to the complexity of content moderation models and the difficulty of understanding information across multiple modalities, existing content moderation software can fail to detect toxic content, which often leads to extremely negative impacts.  We introduce Semantic Fusion, a general, effective methodology for validating multimedia content moderation software. Our key idea is to fuse two or more ex
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SPEECH&#30340;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#33021;&#37327;&#24314;&#27169;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#20107;&#20214;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#36229;&#29699;&#26469;&#34920;&#31034;&#20107;&#20214;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SPEECH&#22312;&#20107;&#20214;&#26816;&#27979;&#21644;&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13617</link><description>&lt;p&gt;
SPEECH: &#22522;&#20110;&#33021;&#37327;&#30340;&#20107;&#20214;&#20013;&#24515;&#36229;&#29699;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SPEECH: Structured Prediction with Energy-Based Event-Centric Hyperspheres. (arXiv:2305.13617v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13617
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SPEECH&#30340;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#33021;&#37327;&#24314;&#27169;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#20107;&#20214;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#36229;&#29699;&#26469;&#34920;&#31034;&#20107;&#20214;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SPEECH&#22312;&#20107;&#20214;&#26816;&#27979;&#21644;&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#20013;&#24515;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979;&#28041;&#21450;&#39044;&#27979;&#20107;&#20214;&#30340;&#32467;&#26500;&#21270;&#36755;&#20986;&#12290;&#22312;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24773;&#20917;&#19979;&#65292;&#20107;&#20214;&#32467;&#26500;&#37117;&#20855;&#26377;&#22797;&#26434;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22240;&#27492;&#26377;&#25928;&#22320;&#34920;&#31034;&#36825;&#20123;&#22797;&#26434;&#30340;&#20107;&#20214;&#32467;&#26500;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#20107;&#20214;&#20013;&#24515;&#36229;&#29699;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979; (SPEECH)&#12290; SPEECH &#20351;&#29992;&#22522;&#20110;&#33021;&#37327;&#30340;&#24314;&#27169;&#26469;&#27169;&#25311;&#20107;&#20214;&#32467;&#26500;&#32452;&#20214;&#20043;&#38388;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#36229;&#29699;&#26469;&#34920;&#31034;&#20107;&#20214;&#31867;&#21035;&#12290;&#22312;&#20004;&#20010;&#32479;&#19968;&#26631;&#27880;&#30340;&#20107;&#20214;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#32467;&#26524;&#34920;&#26126;SPEECH&#22312;&#20107;&#20214;&#26816;&#27979;&#21644;&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#21344;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event-centric structured prediction involves predicting structured outputs of events. In most NLP cases, event structures are complex with manifold dependency, and it is challenging to effectively represent these complicated structured events. To address these issues, we propose Structured Prediction with Energy-based Event-Centric Hyperspheres (SPEECH). SPEECH models complex dependency among event structured components with energy-based modeling, and represents event classes with simple but effective hyperspheres. Experiments on two unified-annotated event datasets indicate that SPEECH is predominant in event detection and event-relation extraction tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#25506;&#32034;&#21033;&#29992;ChatGPT&#25216;&#26415;&#36171;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#31934;&#31070;&#30149;&#21307;&#29983;&#21644;&#24739;&#32773;&#27169;&#25311;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#31934;&#31070;&#31185;&#22330;&#26223;&#20013;&#20351;&#29992;ChatGPT&#25216;&#26415;&#36171;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13614</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#25216;&#26415;&#30340;&#31934;&#31070;&#30149;&#21307;&#29983;&#21644;&#24739;&#32773;&#27169;&#25311;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24212;&#29992;&#19982;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
LLM-empowered Chatbots for Psychiatrist and Patient Simulation: Application and Evaluation. (arXiv:2305.13614v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#25506;&#32034;&#21033;&#29992;ChatGPT&#25216;&#26415;&#36171;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#31934;&#31070;&#30149;&#21307;&#29983;&#21644;&#24739;&#32773;&#27169;&#25311;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#31934;&#31070;&#31185;&#22330;&#26223;&#20013;&#20351;&#29992;ChatGPT&#25216;&#26415;&#36171;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31934;&#31070;&#20581;&#24247;&#39046;&#22495;&#65292;&#36171;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;&#27491;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#28982;&#32780;&#22312;&#31934;&#31070;&#31185;&#38376;&#35786;&#22330;&#26223;&#19979;&#65292;&#24320;&#21457;&#21644;&#35780;&#20272;&#32842;&#22825;&#26426;&#22120;&#20154;&#20173;&#28982;&#32570;&#20047;&#25506;&#32034;&#12290;&#26412;&#20316;&#21697;&#32858;&#28966;&#20110;&#25506;&#32034;&#21033;&#29992;ChatGPT&#25216;&#26415;&#36171;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#31934;&#31070;&#30149;&#21307;&#29983;&#21644;&#24739;&#32773;&#27169;&#25311;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#19982;&#31934;&#31070;&#30149;&#21307;&#29983;&#21512;&#20316;&#65292;&#30830;&#23450;&#30446;&#26631;&#24182;&#36880;&#27493;&#24320;&#21457;&#23545;&#35805;&#31995;&#32479;&#65292;&#32039;&#23494;&#32467;&#21512;&#29616;&#23454;&#22330;&#26223;&#12290;&#22312;&#35780;&#20272;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#36992;&#35831;&#30495;&#27491;&#30340;&#31934;&#31070;&#30149;&#21307;&#29983;&#21644;&#24739;&#32773;&#19982;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#35786;&#26029;&#24615;&#23545;&#35805;&#65292;&#25910;&#38598;&#20182;&#20204;&#30340;&#35780;&#20998;&#20197;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#35777;&#26126;&#20102;&#22312;&#31934;&#31070;&#31185;&#22330;&#26223;&#20013;&#20351;&#29992;ChatGPT&#25216;&#26415;&#36171;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25506;&#32034;&#20102;&#25552;&#31034;&#35774;&#35745;&#23545;&#32842;&#22825;&#26426;&#22120;&#20154;&#34892;&#20026;&#21644;&#29992;&#25143;&#20307;&#39564;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empowering chatbots in the field of mental health is receiving increasing amount of attention, while there still lacks exploration in developing and evaluating chatbots in psychiatric outpatient scenarios. In this work, we focus on exploring the potential of ChatGPT in powering chatbots for psychiatrist and patient simulation. We collaborate with psychiatrists to identify objectives and iteratively develop the dialogue system to closely align with real-world scenarios. In the evaluation experiments, we recruit real psychiatrists and patients to engage in diagnostic conversations with the chatbots, collecting their ratings for assessment. Our findings demonstrate the feasibility of using ChatGPT-powered chatbots in psychiatric scenarios and explore the impact of prompt designs on chatbot behavior and user experience.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26500;&#24314;&#22810;&#27169;&#24577;&#23545;&#35805;&#30340;&#33539;&#20363;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;&#23558;&#35270;&#35273;&#30693;&#35782;&#26126;&#30830;&#20998;&#31867;&#20026;&#26356;&#32454;&#31890;&#24230;&#26469;&#22686;&#24378;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#20174;&#20114;&#32852;&#32593;&#25110;&#22823;&#22411;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#26816;&#32034;&#35270;&#35273;&#20449;&#24687;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;ReSee&#26694;&#26550;&#65292;&#21487;&#23558;&#35270;&#35273;&#34920;&#31034;&#28155;&#21152;&#21040;&#21407;&#22987;&#23545;&#35805;&#27169;&#22411;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.13602</link><description>&lt;p&gt;
ReSee&#65306;&#22312;&#24320;&#25918;&#22495;&#23545;&#35805;&#20013;&#36890;&#36807;&#35270;&#35273;&#30693;&#35782;&#22238;&#24212;
&lt;/p&gt;
&lt;p&gt;
ReSee: Responding through Seeing Fine-grained Visual Knowledge in Open-domain Dialogue. (arXiv:2305.13602v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13602
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26500;&#24314;&#22810;&#27169;&#24577;&#23545;&#35805;&#30340;&#33539;&#20363;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;&#23558;&#35270;&#35273;&#30693;&#35782;&#26126;&#30830;&#20998;&#31867;&#20026;&#26356;&#32454;&#31890;&#24230;&#26469;&#22686;&#24378;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#20174;&#20114;&#32852;&#32593;&#25110;&#22823;&#22411;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#26816;&#32034;&#35270;&#35273;&#20449;&#24687;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;ReSee&#26694;&#26550;&#65292;&#21487;&#23558;&#35270;&#35273;&#34920;&#31034;&#28155;&#21152;&#21040;&#21407;&#22987;&#23545;&#35805;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35270;&#35273;&#30693;&#35782;&#19982;&#25991;&#26412;&#23545;&#35805;&#31995;&#32479;&#30456;&#32467;&#21512;&#25104;&#20026;&#19968;&#31181;&#27169;&#20223;&#20154;&#31867;&#24605;&#32771;&#12289;&#24819;&#35937;&#21644;&#20132;&#27969;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#31995;&#32479;&#25110;&#32773;&#21463;&#21040;&#21487;&#29992;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#36136;&#37327;&#30340;&#38480;&#21046;&#65292;&#25110;&#32773;&#21463;&#21040;&#35270;&#35273;&#30693;&#35782;&#27010;&#24565;&#30340;&#31895;&#31961;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26500;&#24314;&#22810;&#27169;&#24577;&#23545;&#35805;&#30340;&#26032;&#33539;&#20363;&#21450;&#20854;&#30456;&#20851;&#25968;&#25454;&#38598;&#65288;ReSee-WoW&#12289;ReSee-DD&#65289;&#12290;&#25105;&#20204;&#25552;&#35758;&#23558;&#35270;&#35273;&#30693;&#35782;&#26126;&#30830;&#20998;&#20026;&#26356;&#32454;&#31890;&#24230;&#65288;&#8220;&#36716;&#21521;&#32423;&#8221;&#21644;&#8220;&#23454;&#20307;&#32423;&#8221;&#65289;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#35270;&#35273;&#20449;&#24687;&#30340;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#20174;&#20114;&#32852;&#32593;&#25110;&#22823;&#22411;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#26816;&#32034;&#23427;&#20204;&#12290;&#20026;&#20102;&#23637;&#31034;&#25552;&#20379;&#30340;&#35270;&#35273;&#30693;&#35782;&#30340;&#20248;&#36234;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;ReSee&#65292;&#36890;&#36807;&#27169;&#24577;&#36830;&#25509;&#23558;&#35270;&#35273;&#34920;&#31034;&#28155;&#21152;&#21040;&#21407;&#22987;&#23545;&#35805;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating visual knowledge into text-only dialogue systems has become a potential direction to imitate the way humans think, imagine, and communicate. However, existing multimodal dialogue systems are either confined by the scale and quality of available datasets or the coarse concept of visual knowledge. To address these issues, we provide a new paradigm of constructing multimodal dialogues as well as two datasets extended from text-only dialogues under such paradigm (ReSee-WoW, ReSee-DD). We propose to explicitly split the visual knowledge into finer granularity (``turn-level'' and ``entity-level''). To further boost the accuracy and diversity of augmented visual information, we retrieve them from the Internet or a large image dataset. To demonstrate the superiority and universality of the provided visual knowledge, we propose a simple but effective framework ReSee to add visual representation into vanilla dialogue models by modality concatenations. We also conduct extensive expe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DR&#30340;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#19981;&#23545;&#31216;&#30340;&#23398;&#20064;&#29575;&#26469;&#35299;&#20915;&#30001;&#21512;&#20316;&#21338;&#24328;&#24341;&#21457;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#33879;&#25913;&#21892;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13599</link><description>&lt;p&gt;
&#19981;&#23545;&#31216;&#23398;&#20064;&#29575;&#30340;&#20998;&#31163;&#24335;&#29702;&#24615;&#21270;: &#19968;&#31181;&#28789;&#27963;&#30340;Lipschitz&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Decoupled Rationalization with Asymmetric Learning Rates: A Flexible Lipshitz Restraint. (arXiv:2305.13599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DR&#30340;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#19981;&#23545;&#31216;&#30340;&#23398;&#20064;&#29575;&#26469;&#35299;&#20915;&#30001;&#21512;&#20316;&#21338;&#24328;&#24341;&#21457;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#33879;&#25913;&#21892;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#33258;&#35828;&#26126;&#29702;&#24615;&#21270;&#27169;&#22411;&#36890;&#36807;&#21512;&#20316;&#21338;&#24328;&#26500;&#24314;&#65292;&#20854;&#20013;&#29983;&#25104;&#22120;&#20174;&#36755;&#20837;&#25991;&#26412;&#20013;&#36873;&#25321;&#26368;&#26131;&#29702;&#35299;&#30340;&#37096;&#20998;&#20316;&#20026;&#21407;&#29702;&#65292;&#25509;&#30528;&#39044;&#27979;&#22120;&#22522;&#20110;&#25152;&#36873;&#25321;&#30340;&#21407;&#29702;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21512;&#20316;&#21338;&#24328;&#21487;&#33021;&#20250;&#24341;&#21457;&#36864;&#21270;&#38382;&#39064;&#65292;&#39044;&#27979;&#22120;&#36807;&#24230;&#25311;&#21512;&#20110;&#30001;&#23578;&#26410;&#35757;&#32451;&#22909;&#30340;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#20449;&#24687;&#19981;&#36275;&#30340;&#37096;&#20998;&#65292;&#21453;&#36807;&#26469;&#23548;&#33268;&#29983;&#25104;&#22120;&#25910;&#25947;&#20110;&#36235;&#21521;&#20110;&#36873;&#25321;&#26080;&#24847;&#20041;&#30340;&#37096;&#20998;&#30340;&#27425;&#20248;&#27169;&#22411;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#23558;&#36864;&#21270;&#38382;&#39064;&#19982;&#39044;&#27979;&#22120;&#30340;Lipschitz&#36830;&#32493;&#24615;&#32852;&#31995;&#36215;&#26469;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23454;&#39564;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DR&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#28982;&#12289;&#28789;&#27963;&#22320;&#32422;&#26463;&#39044;&#27979;&#22120;&#30340;Lipschitz&#24120;&#25968;&#65292;&#24182;&#35299;&#20915;&#20102;&#36864;&#21270;&#38382;&#39064;&#12290;DR&#26041;&#27861;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#23558;&#29983;&#25104;&#22120;&#21644;&#39044;&#27979;&#22120;&#20998;&#31163;&#65292;&#20026;&#23427;&#20204;&#20998;&#37197;&#19981;&#23545;&#31216;&#30340;&#23398;&#20064;&#29575;&#12290;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;DR&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#29616;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
A self-explaining rationalization model is generally constructed by a cooperative game where a generator selects the most human-intelligible pieces from the input text as rationales, followed by a predictor that makes predictions based on the selected rationales. However, such a cooperative game may incur the degeneration problem where the predictor overfits to the uninformative pieces generated by a not yet well-trained generator and in turn, leads the generator to converge to a sub-optimal model that tends to select senseless pieces. In this paper, we theoretically bridge degeneration with the predictor's Lipschitz continuity. Then, we empirically propose a simple but effective method named DR, which can naturally and flexibly restrain the Lipschitz constant of the predictor, to address the problem of degeneration. The main idea of DR is to decouple the generator and predictor to allocate them with asymmetric learning rates. A series of experiments conducted on two widely used benchm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#33719;&#21462;&#20195;&#34920;&#24615;&#36755;&#20837;&#26469;&#24110;&#21161;&#35821;&#20041;&#29702;&#35299;&#31243;&#24207;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13592</link><description>&lt;p&gt;
&#21033;&#29992;&#65288;&#27169;&#31946;&#27979;&#35797;&#65289;&#27979;&#35797;&#29992;&#20363;&#26469;&#29702;&#35299;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Understanding Programs by Exploiting (Fuzzing) Test Cases. (arXiv:2305.13592v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#33719;&#21462;&#20195;&#34920;&#24615;&#36755;&#20837;&#26469;&#24110;&#21161;&#35821;&#20041;&#29702;&#35299;&#31243;&#24207;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#30340;&#35821;&#20041;&#29702;&#35299;&#24341;&#36215;&#20102;&#31038;&#21306;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26368;&#36817;&#25104;&#21151;&#21551;&#21457;&#65292;&#36890;&#36807;&#23558;&#32534;&#31243;&#35821;&#35328;&#35270;&#20026;&#21478;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#65292;&#24182;&#22312;&#31243;&#24207;&#20195;&#30721;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;LLM&#65292;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#31243;&#24207;&#27605;&#31455;&#19982;&#25991;&#26412;&#26377;&#26412;&#36136;&#30340;&#21306;&#21035;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#20855;&#26377;&#20005;&#26684;&#30340;&#32467;&#26500;&#21644;&#35821;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#31243;&#24207;&#21450;&#20854;&#22522;&#26412;&#21333;&#20803;&#65288;&#21363;&#20989;&#25968;&#21644;&#23376;&#31243;&#24207;&#65289;&#26088;&#22312;&#23637;&#31034;&#21508;&#31181;&#34892;&#20026;&#21644;/&#25110;&#25552;&#20379;&#21487;&#33021;&#30340;&#36755;&#20986;&#65292;&#32473;&#23450;&#19981;&#21516;&#30340;&#36755;&#20837;&#12290;&#36755;&#20837;&#21644;&#21487;&#33021;&#30340;&#36755;&#20986;/&#34892;&#20026;&#20043;&#38388;&#30340;&#20851;&#31995;&#34920;&#31034;&#20989;&#25968;/&#23376;&#31243;&#24207;&#65292;&#24182;&#27010;&#36848;&#20102;&#25972;&#20010;&#31243;&#24207;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#36825;&#31181;&#20851;&#31995;&#32435;&#20837;&#23398;&#20064;&#20013;&#65292;&#20197;&#23454;&#29616;&#23545;&#31243;&#24207;&#30340;&#26356;&#28145;&#20837;&#35821;&#20041;&#29702;&#35299;&#12290;&#20026;&#20102;&#33719;&#24471;&#36275;&#22815;&#20195;&#34920;&#24615;&#30340;&#36755;&#20837;&#20197;&#35302;&#21457;&#22823;&#37327;&#25191;&#34892;&#65292;&#21487;&#20197;&#20351;&#29992;&#27169;&#31946;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic understanding of programs has attracted great attention in the community. Inspired by recent successes of large language models (LLMs) in natural language understanding, tremendous progress has been made by treating programming language as another sort of natural language and training LLMs on corpora of program code. However, programs are essentially different from texts after all, in a sense that they are normally heavily structured and syntax-strict. In particular, programs and their basic units (i.e., functions and subroutines) are designed to demonstrate a variety of behaviors and/or provide possible outputs, given different inputs. The relationship between inputs and possible outputs/behaviors represents the functions/subroutines and profiles the program as a whole. Therefore, we propose to incorporate such a relationship into learning, for achieving a deeper semantic understanding of programs. To obtain inputs that are representative enough to trigger the execution of mo
&lt;/p&gt;</description></item><item><title>BiasX&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#36755;&#20837;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#30340;&#38544;&#21547;&#31038;&#20250;&#20559;&#35265;&#26469;&#25552;&#39640;&#20869;&#23481;&#23457;&#26680;&#30340;&#36136;&#37327;&#12290;&#32463;&#36807;&#22823;&#35268;&#27169;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35299;&#37322;&#23545;&#20110;&#20934;&#30830;&#35782;&#21035;&#24314;&#35758;&#30340;&#26377;&#23475;&#20869;&#23481;&#30340;&#24494;&#22937;&#31243;&#24230;&#26377;&#24456;&#22823;&#30340;&#24110;&#21161;&#12290;&#26426;&#22120;&#29983;&#25104;&#30340;&#35299;&#37322;&#20165;&#33021;&#25552;&#39640;2.4&#65285;&#30340;&#26377;&#25928;&#24615;&#65292;&#32780;&#20154;&#24037;&#25776;&#20889;&#30340;&#35299;&#37322;&#33021;&#22815;&#25552;&#39640;7.2&#65285;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13589</link><description>&lt;p&gt;
BiasX&#65306;&#20351;&#29992;&#38544;&#21547;&#31038;&#20250;&#20559;&#35265;&#35299;&#37322;&#22312;&#26377;&#23475;&#20869;&#23481;&#23457;&#26597;&#20013;"&#32531;&#24930;&#24605;&#32771;"
&lt;/p&gt;
&lt;p&gt;
BiasX: "Thinking Slow" in Toxic Content Moderation with Explanations of Implied Social Biases. (arXiv:2305.13589v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13589
&lt;/p&gt;
&lt;p&gt;
BiasX&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#36755;&#20837;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#30340;&#38544;&#21547;&#31038;&#20250;&#20559;&#35265;&#26469;&#25552;&#39640;&#20869;&#23481;&#23457;&#26680;&#30340;&#36136;&#37327;&#12290;&#32463;&#36807;&#22823;&#35268;&#27169;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35299;&#37322;&#23545;&#20110;&#20934;&#30830;&#35782;&#21035;&#24314;&#35758;&#30340;&#26377;&#23475;&#20869;&#23481;&#30340;&#24494;&#22937;&#31243;&#24230;&#26377;&#24456;&#22823;&#30340;&#24110;&#21161;&#12290;&#26426;&#22120;&#29983;&#25104;&#30340;&#35299;&#37322;&#20165;&#33021;&#25552;&#39640;2.4&#65285;&#30340;&#26377;&#25928;&#24615;&#65292;&#32780;&#20154;&#24037;&#25776;&#20889;&#30340;&#35299;&#37322;&#33021;&#22815;&#25552;&#39640;7.2&#65285;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26377;&#23475;&#20869;&#23481;&#30340;&#27880;&#37322;&#21644;&#23457;&#26597;&#20013;&#65292;&#27880;&#37322;&#21592;&#21644;&#23457;&#26597;&#21592;&#32463;&#24120;&#37319;&#29992;&#24515;&#29702;&#24555;&#25463;&#26041;&#24335;&#20570;&#20986;&#20915;&#31574;&#12290;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#38169;&#36807;&#24494;&#22937;&#30340;&#26377;&#23475;&#24615;&#65292;&#32780;&#30475;&#20284;&#26377;&#23475;&#20294;&#26080;&#23475;&#30340;&#20869;&#23481;&#34987;&#36807;&#24230;&#26816;&#27979;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;BiasX&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#38472;&#36848;&#30340;&#38544;&#21547;&#31038;&#20250;&#20559;&#35265;&#30340;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#26469;&#22686;&#24378;&#20869;&#23481;&#23457;&#26597;&#35774;&#32622;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#30340;&#20247;&#21253;&#29992;&#25143;&#30740;&#31350;&#26469;&#25506;&#32034;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21442;&#19982;&#32773;&#36890;&#36807;&#35299;&#37322;&#27491;&#30830;&#35782;&#21035;&#24494;&#22937;&#30340;&#65288;&#38750;&#65289;&#26377;&#23475;&#20869;&#23481;&#30340;&#23454;&#38469;&#33719;&#30410;&#12290;&#35299;&#37322;&#30340;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;:&#19981;&#23436;&#32654;&#30340;&#26426;&#22120;&#29983;&#25104;&#30340;&#35299;&#37322;&#65288;+2.4%&#22312;&#38590;&#20197;&#22788;&#29702;&#30340;&#26377;&#23475;&#26679;&#20363;&#19978;&#65289;&#30456;&#27604;&#19987;&#23478;&#25776;&#20889;&#30340;&#20154;&#24037;&#35299;&#37322;&#65288;+7.2%&#65289;&#24110;&#21161;&#36739;&#23569;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#20351;&#29992;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#40723;&#21169;&#26356;&#21152;&#28145;&#24605;&#29087;&#34385;&#30340;&#26377;&#27602;&#24615;&#23457;&#26597;&#30340;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Toxicity annotators and content moderators often default to mental shortcuts when making decisions. This can lead to subtle toxicity being missed, and seemingly toxic but harmless content being over-detected. We introduce BiasX, a framework that enhances content moderation setups with free-text explanations of statements' implied social biases, and explore its effectiveness through a large-scale crowdsourced user study. We show that indeed, participants substantially benefit from explanations for correctly identifying subtly (non-)toxic content. The quality of explanations is critical: imperfect machine-generated explanations (+2.4% on hard toxic examples) help less compared to expert-written human explanations (+7.2%). Our results showcase the promise of using free-text explanations to encourage more thoughtful toxicity moderation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;&#32467;&#26500;&#24314;&#27169;&#30340;&#25991;&#26412;&#32534;&#30721;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#19981;&#23436;&#25972;&#30340;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#24402;&#32435;&#36923;&#36753;&#25512;&#29702;&#12290;&#36890;&#36807;&#38024;&#23545;&#22797;&#26434;&#26597;&#35810;&#30340;&#32467;&#26500;&#24314;&#27169;&#21644;&#21333;&#29420;&#23545;&#19981;&#21516;&#30340;&#20960;&#20309;&#25805;&#20316;&#36827;&#34892;&#24314;&#27169;&#65292;&#23427;&#22312;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#31572;&#26696;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2305.13585</link><description>&lt;p&gt;
&#22522;&#20110;&#26597;&#35810;&#32467;&#26500;&#24314;&#27169;&#30340;&#30693;&#35782;&#22270;&#35889;&#24402;&#32435;&#36923;&#36753;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Query Structure Modeling for Inductive Logical Reasoning Over Knowledge Graphs. (arXiv:2305.13585v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;&#32467;&#26500;&#24314;&#27169;&#30340;&#25991;&#26412;&#32534;&#30721;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#19981;&#23436;&#25972;&#30340;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#24402;&#32435;&#36923;&#36753;&#25512;&#29702;&#12290;&#36890;&#36807;&#38024;&#23545;&#22797;&#26434;&#26597;&#35810;&#30340;&#32467;&#26500;&#24314;&#27169;&#21644;&#21333;&#29420;&#23545;&#19981;&#21516;&#30340;&#20960;&#20309;&#25805;&#20316;&#36827;&#34892;&#24314;&#27169;&#65292;&#23427;&#22312;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#31572;&#26696;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#23436;&#25972;&#30340;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#20197;&#22238;&#31572;&#22797;&#26434;&#30340;&#36923;&#36753;&#26597;&#35810;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#38543;&#30528;&#19981;&#26029;&#28436;&#21270;&#30340;&#30693;&#35782;&#22270;&#35889;&#20013;&#26032;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#20986;&#29616;&#65292;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#24402;&#32435;&#36923;&#36753;&#25512;&#29702;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#22522;&#20110;PLMs&#30340;&#26041;&#27861;&#38590;&#20197;&#23545;&#22797;&#26434;&#26597;&#35810;&#36827;&#34892;&#36923;&#36753;&#24314;&#27169;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30456;&#21516;&#32467;&#26500;&#20869;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32467;&#26500;&#24314;&#27169;&#30340;&#25991;&#26412;&#32534;&#30721;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#24402;&#32435;&#36923;&#36753;&#25512;&#29702;&#12290;&#23427;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#32447;&#24615;&#21270;&#30340;&#26597;&#35810;&#32467;&#26500;&#21644;&#23454;&#20307;&#36827;&#34892;&#32534;&#30721;&#20197;&#25214;&#21040;&#31572;&#26696;&#12290;&#38024;&#23545;&#22797;&#26434;&#26597;&#35810;&#30340;&#32467;&#26500;&#24314;&#27169;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36880;&#27493;&#25351;&#23548;&#30340;&#25351;&#20196;&#65292;&#23427;&#20204;&#38544;&#21547;&#22320;&#25552;&#31034;PLMs&#22312;&#27599;&#20010;&#26597;&#35810;&#20013;&#25191;&#34892;&#20960;&#20309;&#25805;&#20316;&#30340;&#25191;&#34892;&#39034;&#24207;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#22312;&#34920;&#31034;&#31354;&#38388;&#19978;&#21333;&#29420;&#23545;&#19981;&#21516;&#30340;&#20960;&#20309;&#25805;&#20316;&#65288;&#21363;&#25237;&#24433;&#12289;&#20132;&#38598;&#21644;&#24182;&#38598;&#65289;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#21152;&#20837;&#20102;&#27880;&#24847;&#21147;&#21644;&#26368;&#22823;&#36755;&#20986;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logical reasoning over incomplete knowledge graphs to answer complex logical queries is a challenging task. With the emergence of new entities and relations in constantly evolving KGs, inductive logical reasoning over KGs has become a crucial problem. However, previous PLMs-based methods struggle to model the logical structures of complex queries, which limits their ability to generalize within the same structure. In this paper, we propose a structure-modeled textual encoding framework for inductive logical reasoning over KGs. It encodes linearized query structures and entities using pre-trained language models to find answers. For structure modeling of complex queries, we design stepwise instructions that implicitly prompt PLMs on the execution order of geometric operations in each query. We further separately model different geometric operations (i.e., projection, intersection, and union) on the representation space using a pre-trained encoder with additional attention and maxout lay
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#21327;&#35843;&#24863;&#30693;&#30340;&#36328;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;Hierarchical Crossmodal Transformer with Modality Gating(HCT-MG)&#27169;&#22411;&#26469;&#30830;&#23450;&#20027;&#35201;&#27169;&#24577;&#24182;&#20998;&#23618;&#34701;&#21512;&#36741;&#21161;&#27169;&#24577;&#65292;&#26377;&#25928;&#20943;&#36731;&#27169;&#24577;&#20043;&#38388;&#30340;&#19981;&#21327;&#35843;&#24863;&#30693;&#21644;&#20449;&#24687;&#20887;&#20313;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13583</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#19981;&#36275;&#65306;&#22522;&#20110;&#19981;&#21327;&#35843;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#19982;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Cross-Attention is Not Enough: Incongruity-Aware Multimodal Sentiment Analysis and Emotion Recognition. (arXiv:2305.13583v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#21327;&#35843;&#24863;&#30693;&#30340;&#36328;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;Hierarchical Crossmodal Transformer with Modality Gating(HCT-MG)&#27169;&#22411;&#26469;&#30830;&#23450;&#20027;&#35201;&#27169;&#24577;&#24182;&#20998;&#23618;&#34701;&#21512;&#36741;&#21161;&#27169;&#24577;&#65292;&#26377;&#25928;&#20943;&#36731;&#27169;&#24577;&#20043;&#38388;&#30340;&#19981;&#21327;&#35843;&#24863;&#30693;&#21644;&#20449;&#24687;&#20887;&#20313;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#34701;&#21512;&#22312;&#24773;&#24863;&#35745;&#31639;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#23545;&#24615;&#33021;&#30340;&#25552;&#21319;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#26426;&#29702;&#23578;&#19981;&#28165;&#26970;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20351;&#29992;&#23427;&#36890;&#24120;&#20250;&#23548;&#33268;&#22823;&#22411;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#22312;&#24773;&#24863;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#39318;&#20808;&#20998;&#26512;&#20102;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#20013;&#19968;&#20010;&#27169;&#24577;&#20013;&#31361;&#20986;&#30340;&#24773;&#24863;&#20449;&#24687;&#22914;&#20309;&#21463;&#21040;&#21478;&#19968;&#20010;&#27169;&#24577;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30001;&#20110;&#36328;&#27169;&#24577;&#30340;&#20851;&#27880;&#65292;&#27169;&#24577;&#20043;&#38388;&#23384;&#22312;&#28508;&#22312;&#30340;&#19981;&#21327;&#35843;&#24863;&#30693;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#27169;&#22411;(HCT-MG)&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20998;&#23618;&#20132;&#21449;&#27169;&#24577;Transformer&#19982;&#27169;&#24577;&#38376;&#25511;&#21046;&#26469;&#30830;&#23450;&#20027;&#35201;&#30340;&#27169;&#24577;&#65292;&#24182;&#20998;&#23618;&#22320;&#23558;&#36741;&#21161;&#27169;&#24577;&#32435;&#20837;&#20854;&#20013;&#65292;&#20197;&#20943;&#36731;&#27169;&#24577;&#20043;&#38388;&#30340;&#19981;&#21327;&#35843;&#24863;&#30693;&#24182;&#20943;&#23569;&#20449;&#24687;&#20887;&#20313;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;CMU-MOSI&#12289;CMU-MOSEI&#21644;IEMOCAP&#19978;&#30340;&#23454;&#39564;&#35780;&#20272;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#65306;1&#65289;&#20854;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65307;2&#65289;&#23427;&#20165;&#20351;&#29992;&#23569;&#37327;&#30340;&#36229;&#21442;&#25968;&#21644;&#21442;&#25968;&#65307;3&#65289;&#23427;&#30340;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fusing multiple modalities for affective computing tasks has proven effective for performance improvement. However, how multimodal fusion works is not well understood, and its use in the real world usually results in large model sizes. In this work, on sentiment and emotion analysis, we first analyze how the salient affective information in one modality can be affected by the other in crossmodal attention. We find that inter-modal incongruity exists at the latent level due to crossmodal attention. Based on this finding, we propose a lightweight model via Hierarchical Crossmodal Transformer with Modality Gating (HCT-MG), which determines a primary modality according to its contribution to the target task and then hierarchically incorporates auxiliary modalities to alleviate inter-modal incongruity and reduce information redundancy. The experimental evaluation on three benchmark datasets: CMU-MOSI, CMU-MOSEI, and IEMOCAP verifies the efficacy of our approach, showing that it: 1) outperfo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32763;&#35793;&#21644;&#27880;&#37322;&#34701;&#21512;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#25913;&#36827;&#20302;&#36164;&#28304;&#35821;&#35328;&#25991;&#26412;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#36890;&#36807;TransFusion&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#24378;&#22823;&#30340;&#39044;&#27979;&#65292;&#19988;&#22312;&#20004;&#20010;&#20302;&#36164;&#28304;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19968;&#33268;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2305.13582</link><description>&lt;p&gt;
&#36890;&#36807;&#32763;&#35793;&#21644;&#27880;&#37322;&#34701;&#21512;&#25913;&#36827;&#20302;&#36164;&#28304;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Better Low-Resource Entity Recognition Through Translation and Annotation Fusion. (arXiv:2305.13582v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32763;&#35793;&#21644;&#27880;&#37322;&#34701;&#21512;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#25913;&#36827;&#20302;&#36164;&#28304;&#35821;&#35328;&#25991;&#26412;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#36890;&#36807;TransFusion&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#24378;&#22823;&#30340;&#39044;&#27979;&#65292;&#19988;&#22312;&#20004;&#20010;&#20302;&#36164;&#28304;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19968;&#33268;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#23454;&#29616;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#36716;&#31227;&#33267;&#20302;&#36164;&#28304;&#35821;&#35328;&#26102;&#65292;&#36890;&#24120;&#34920;&#29616;&#20986;&#24615;&#33021;&#24046;&#24322;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26410;&#34987;&#20805;&#20998;&#35757;&#32451;&#25110;&#26410;&#21253;&#21547;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#35821;&#35328;&#12290;&#21463;&#36825;&#20123;&#27169;&#22411;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#34920;&#29616;&#20248;&#31168;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#32763;&#35793;&#21644;&#34701;&#21512;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#20302;&#36164;&#28304;&#35821;&#35328;&#25991;&#26412;&#32763;&#35793;&#25104;&#39640;&#36164;&#28304;&#35821;&#35328;&#36827;&#34892;&#27880;&#37322;&#65292;&#28982;&#21518;&#23558;&#27880;&#37322;&#34701;&#21512;&#22238;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#22522;&#20110;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TransFusion&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#35757;&#32451;&#29992;&#20110;&#34701;&#21512;&#26469;&#33258;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#20197;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#36827;&#34892;&#24378;&#22823;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20302;&#36164;&#28304;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#25968;&#25454;&#38598;MasakhaNER2.0&#21644;LORELEI NER&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#36328;&#35821;&#35328;NER&#22522;&#32447;&#30340;&#19968;&#33268;&#20248;&#31168;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained multilingual language models have enabled significant advancements in cross-lingual transfer. However, these models often exhibit a performance disparity when transferring from high-resource languages to low-resource languages, especially for languages that are underrepresented or not in the pre-training data. Motivated by the superior performance of these models on high-resource languages compared to low-resource languages, we introduce a Translation-and-fusion framework, which translates low-resource language text into a high-resource language for annotation using fully supervised models before fusing the annotations back into the low-resource language. Based on this framework, we present TransFusion, a model trained to fuse predictions from a high-resource language to make robust predictions on low-resource languages. We evaluate our methods on two low-resource named entity recognition (NER) datasets, MasakhaNER2.0 and LORELEI NER, covering 25 languages, and show consist
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#22312;&#26080;&#20301;&#32622;&#23884;&#20837;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#27880;&#24847;&#21147;&#26041;&#24046;&#20013;&#23384;&#22312;&#28508;&#22312;&#30340;&#20301;&#32622;&#20449;&#24687;&#65292;&#24182;&#35777;&#26126;&#20002;&#24323;&#20301;&#32622;&#23884;&#20837;&#30340;&#20915;&#31574;&#21487;&#20419;&#36827;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#26356;&#26377;&#25928;&#39044;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2305.13571</link><description>&lt;p&gt;
&#22312;&#26080;&#20301;&#32622;&#23884;&#20837;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#27880;&#24847;&#21147;&#26041;&#24046;&#20013;&#23384;&#22312;&#28508;&#22312;&#30340;&#20301;&#32622;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings. (arXiv:2305.13571v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13571
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#22312;&#26080;&#20301;&#32622;&#23884;&#20837;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#27880;&#24847;&#21147;&#26041;&#24046;&#20013;&#23384;&#22312;&#28508;&#22312;&#30340;&#20301;&#32622;&#20449;&#24687;&#65292;&#24182;&#35777;&#26126;&#20002;&#24323;&#20301;&#32622;&#23884;&#20837;&#30340;&#20915;&#31574;&#21487;&#20419;&#36827;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#26356;&#26377;&#25928;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#20301;&#32622;&#23884;&#20837;&#65292;&#28982;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#36136;&#30097;&#27492;&#31867;&#23884;&#20837;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#38543;&#26426;&#21021;&#22987;&#21270;&#19988;&#26080;&#20301;&#32622;&#23884;&#20837;&#30340;&#20923;&#32467;Transformer&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#33258;&#27880;&#24847;&#21147;&#26041;&#24046;&#30340;&#25910;&#32553;&#20869;&#22312;&#22320;&#32534;&#30721;&#20102;&#24378;&#30340;&#20301;&#32622;&#20449;&#24687;&#65292;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;Transformer&#23618;&#20869;&#27599;&#19968;&#27493;&#30340;&#24213;&#23618;&#20998;&#24067;&#26469;&#37327;&#21270;&#36825;&#19968;&#26041;&#24046;&#12290;&#36890;&#36807;&#20351;&#29992;&#23436;&#20840;&#39044;&#35757;&#32451;&#36807;&#30340;&#27169;&#22411;&#36827;&#34892;&#23454;&#35777;&#39564;&#35777;&#65292;&#25105;&#20204;&#35777;&#26126;&#21363;&#20351;&#32463;&#36807;&#20102;&#22823;&#37327;&#30340;&#28176;&#36827;&#24335;&#26356;&#26032;&#26799;&#24230;&#65292;&#26041;&#24046;&#25910;&#32553;&#25928;&#24212;&#20173;&#28982;&#23384;&#22312;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#35777;&#26126;&#20102;&#25918;&#24323;&#20301;&#32622;&#23884;&#20837;&#30340;&#20915;&#31574;&#65292;&#24182;&#20419;&#36827;Transformer&#35821;&#35328;&#27169;&#22411;&#26356;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of positional embeddings in transformer language models is widely accepted. However, recent research has called into question the necessity of such embeddings. We further extend this inquiry by demonstrating that a randomly initialized and frozen transformer language model, devoid of positional embeddings, inherently encodes strong positional information through the shrinkage of self-attention variance. To quantify this variance, we derive the underlying distribution of each step within a transformer layer. Through empirical validation using a fully pretrained model, we show that the variance shrinkage effect still persists after extensive gradient updates. Our findings serve to justify the decision to discard positional embeddings and thus facilitate more efficient pretraining of transformer language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#31216;&#26356;&#20026;&#22810;&#26679;&#12289;&#27809;&#26377;&#25463;&#24452;&#12289;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20851;&#31995;&#25552;&#21462;&#22522;&#20934;&#27979;&#35797;EntRed&#65292;&#24182;&#35299;&#20915;&#20102;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#23384;&#22312;&#30340;&#23454;&#20307;&#27880;&#37322;&#38169;&#35823;&#12289;&#23454;&#20307;&#21517;&#31216;&#22810;&#26679;&#24615;&#36739;&#20302;&#12289;&#20174;&#23454;&#20307;&#21517;&#31216;&#21040;&#22522;&#26412;&#20107;&#23454;&#20851;&#31995;&#30340;&#25463;&#24452;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13551</link><description>&lt;p&gt;
EntRED: &#29992;&#26356;&#23569;&#30340;&#25463;&#24452;&#36827;&#34892;&#20851;&#31995;&#25277;&#21462;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
EntRED: Benchmarking Relation Extraction with Fewer Shortcuts. (arXiv:2305.13551v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#31216;&#26356;&#20026;&#22810;&#26679;&#12289;&#27809;&#26377;&#25463;&#24452;&#12289;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20851;&#31995;&#25552;&#21462;&#22522;&#20934;&#27979;&#35797;EntRed&#65292;&#24182;&#35299;&#20915;&#20102;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#23384;&#22312;&#30340;&#23454;&#20307;&#27880;&#37322;&#38169;&#35823;&#12289;&#23454;&#20307;&#21517;&#31216;&#22810;&#26679;&#24615;&#36739;&#20302;&#12289;&#20174;&#23454;&#20307;&#21517;&#31216;&#21040;&#22522;&#26412;&#20107;&#23454;&#20851;&#31995;&#30340;&#25463;&#24452;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#21517;&#31216;&#22312;&#20851;&#31995;&#25277;&#21462;&#20013;&#36215;&#30528;&#26377;&#25928;&#30340;&#20316;&#29992;&#65292;&#24182;&#24120;&#24120;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#38598;&#20013;&#30340;&#23454;&#20307;&#21517;&#31216;&#26174;&#33879;&#24433;&#21709;&#20102;&#20851;&#31995;&#25552;&#21462;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#26631;&#20934;&#30340;&#20851;&#31995;&#25277;&#21462;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#23384;&#22312;&#22823;&#37327;&#38169;&#35823;&#30340;&#23454;&#20307;&#27880;&#37322;&#65292;&#23454;&#20307;&#21517;&#31216;&#22810;&#26679;&#24615;&#36739;&#20302;&#65292;&#24182;&#19988;&#23481;&#26131;&#20986;&#29616;&#20174;&#23454;&#20307;&#21517;&#31216;&#21040;&#22522;&#26412;&#20107;&#23454;&#20851;&#31995;&#30340;&#25463;&#24452;&#12290;&#36825;&#20123;&#38382;&#39064;&#20351;&#24471;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19982;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30456;&#36317;&#29978;&#36828;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EntRED&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#36739;&#23569;&#25463;&#24452;&#21644;&#26356;&#39640;&#23454;&#20307;&#22810;&#26679;&#24615;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20851;&#31995;&#25552;&#21462;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#26500;&#24314;EntRED&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#29702;&#65288;CI&#65289;&#30340;&#31471;&#21040;&#31471;&#23454;&#20307;&#26367;&#25442;&#31649;&#36947;&#65306;ERIC&#12290;ERIC&#23545;&#23454;&#20307;&#36827;&#34892;&#31867;&#22411;&#32422;&#26463;&#26367;&#25442;&#65292;&#20197;&#20943;&#23569;&#20174;&#23454;&#20307;&#20559;&#24046;&#21040;&#22522;&#26412;&#20107;&#23454;&#20851;&#31995;&#30340;&#25463;&#24452;&#12290;ERIC&#22312;&#20004;&#20010;&#26041;&#38754;&#24212;&#29992;CI&#65306;1&#65289;&#38024;&#23545;&#38656;&#35201;&#23454;&#20307;&#26367;&#25442;&#30340;&#23454;&#20363;&#65292;2&#65289;&#30830;&#23450;&#20505;&#36873;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity names play an effective role in relation extraction (RE) and often influence model performance. As a result, the entity names in the benchmarks' test sets significantly influence the evaluation of RE models. In this work, we find that the standard RE benchmarks' datasets have a large portion of incorrect entity annotations, low entity name diversity, and are prone to have shortcuts from entity names to ground-truth relations. These issues make the standard benchmarks far from reflecting the real-world scenarios. Hence, in this work, we present EntRED, a challenging RE benchmark with reduced shortcuts and higher diversity of entities. To build EntRED, we propose an end-to-end entity replacement pipeline based on causal inference (CI): ERIC. ERIC performs type-constrained replacements on entities to reduce the shortcuts from entity bias to ground-truth relations. ERIC applies CI in two aspects: 1) targeting the instances that need entity replacements, and 2) determining the candid
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#36827;&#21270;&#23398;&#20064;&#30340; Mixup &#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#25968;&#25454;&#25193;&#20805;&#65292;&#21487;&#20197;&#20026;&#27169;&#22411;&#35757;&#32451;&#29983;&#25104;&#26356;&#21152;&#36866;&#24212;&#21644;&#21451;&#22909;&#30340;&#20266;&#26679;&#26412;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;&#27169;&#22411;&#30340;overconfidence&#12290;</title><link>http://arxiv.org/abs/2305.13547</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#25105;&#36827;&#21270;&#23398;&#20064;&#30340; Mixup&#65306;&#22686;&#24378;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Self-Evolution Learning for Mixup: Enhance Data Augmentation on Few-Shot Text Classification Tasks. (arXiv:2305.13547v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13547
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#36827;&#21270;&#23398;&#20064;&#30340; Mixup &#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#25968;&#25454;&#25193;&#20805;&#65292;&#21487;&#20197;&#20026;&#27169;&#22411;&#35757;&#32451;&#29983;&#25104;&#26356;&#21152;&#36866;&#24212;&#21644;&#21451;&#22909;&#30340;&#20266;&#26679;&#26412;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;&#27169;&#22411;&#30340;overconfidence&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#24448;&#24448;&#36935;&#21040;&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#30340;&#23569;&#26679;&#26412;&#22330;&#26223;&#65292;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#20351;&#29992; Mixup &#36827;&#34892;&#25968;&#25454;&#25193;&#20805;&#24050;&#32463;&#22312;&#21508;&#31181;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968; Mixup &#26041;&#27861;&#24182;&#19981;&#32771;&#34385;&#35757;&#32451;&#19981;&#21516;&#38454;&#27573;&#30340;&#23398;&#20064;&#38590;&#24230;&#24046;&#24322;&#24182;&#20135;&#29983;&#24102;&#26377; one hot &#26631;&#31614;&#30340;&#26032;&#26679;&#26412;&#65292;&#23548;&#33268;&#27169;&#22411;&#36807;&#20110;&#33258;&#20449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#36827;&#21270;&#23398;&#20064;&#65288;SE&#65289;&#30340; Mixup &#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#25968;&#25454;&#25193;&#20805;&#65292;&#21487;&#20197;&#20026;&#27169;&#22411;&#35757;&#32451;&#29983;&#25104;&#26356;&#21152;&#36866;&#24212;&#21644;&#21451;&#22909;&#30340;&#20266;&#26679;&#26412;&#12290;SE &#20851;&#27880;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#21464;&#21270;&#12290;&#20026;&#20102;&#20943;&#36731;&#27169;&#22411;&#32622;&#20449;&#24230;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#20363;&#26631;&#31614;&#24179;&#28369;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32447;&#24615;&#25554;&#20540;&#27169;&#22411;&#30340;&#36755;&#20986;&#21644;&#21407;&#22987;&#26679;&#26412;&#30340; one-hot &#26631;&#31614;&#65292;&#20197;&#29983;&#25104;&#26032;&#30340;&#36719;&#26631;&#31614;&#29992;&#20110;&#28151;&#21512;&#12290;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#65292;&#22312;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#29575;&#30340;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;&#27169;&#22411;&#30340;overconfidence&#12290;
&lt;/p&gt;
&lt;p&gt;
Text classification tasks often encounter few shot scenarios with limited labeled data, and addressing data scarcity is crucial. Data augmentation with mixup has shown to be effective on various text classification tasks. However, most of the mixup methods do not consider the varying degree of learning difficulty in different stages of training and generate new samples with one hot labels, resulting in the model over confidence. In this paper, we propose a self evolution learning (SE) based mixup approach for data augmentation in text classification, which can generate more adaptive and model friendly pesudo samples for the model training. SE focuses on the variation of the model's learning ability. To alleviate the model confidence, we introduce a novel instance specific label smoothing approach, which linearly interpolates the model's output and one hot labels of the original samples to generate new soft for label mixing up. Through experimental analysis, in addition to improving cla
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#21453;&#20107;&#23454;&#29983;&#25104;&#27169;&#22411;&#26469;&#20027;&#21160;&#25277;&#26679;&#29983;&#25104;&#22823;&#37327;&#19981;&#21516;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#65292;&#24182;&#33258;&#21160;&#26631;&#35760;&#23427;&#20204;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#25104;&#23545;&#20998;&#31867;&#22120;&#26469;&#25554;&#20540;&#21407;&#22987;&#26679;&#20363;&#21644;&#21453;&#20107;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21487;&#20197;&#26356;&#27491;&#30830;&#22320;&#26631;&#35760;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13535</link><description>&lt;p&gt;
&#36890;&#36807;&#20027;&#21160;&#29983;&#25104;&#25104;&#23545;&#21453;&#20107;&#23454;&#25968;&#25454;&#26469;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Classifier Robustness through Active Generation of Pairwise Counterfactuals. (arXiv:2305.13535v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#21453;&#20107;&#23454;&#29983;&#25104;&#27169;&#22411;&#26469;&#20027;&#21160;&#25277;&#26679;&#29983;&#25104;&#22823;&#37327;&#19981;&#21516;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#65292;&#24182;&#33258;&#21160;&#26631;&#35760;&#23427;&#20204;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#25104;&#23545;&#20998;&#31867;&#22120;&#26469;&#25554;&#20540;&#21407;&#22987;&#26679;&#20363;&#21644;&#21453;&#20107;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21487;&#20197;&#26356;&#27491;&#30830;&#22320;&#26631;&#35760;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65288;CDA&#65289;&#26159;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#20998;&#31867;&#22120;&#40065;&#26834;&#24615;&#30340;&#24120;&#29992;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#21457;&#29616;&#26377;&#24847;&#20041;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#24182;&#26377;&#25928;&#22320;&#26631;&#35760;&#23427;&#20204;&#26159;&#19968;&#20010;&#26681;&#26412;&#24615;&#25361;&#25112;&#65292;&#38656;&#35201;&#23613;&#21487;&#33021;&#38477;&#20302;&#20154;&#24037;&#26631;&#35760;&#25104;&#26412;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#23436;&#20840;&#20381;&#36182;&#20110;&#20154;&#24037;&#26631;&#27880;&#30340;&#26631;&#31614;&#65292;&#36825;&#26159;&#19968;&#20010;&#26114;&#36149;&#30340;&#36807;&#31243;&#65292;&#38480;&#21046;&#20102;&#21453;&#20107;&#23454;&#25968;&#25454;&#30340;&#35268;&#27169;&#65292;&#35201;&#20040;&#38544;&#21547;&#22320;&#20551;&#35774;&#26631;&#31614;&#19981;&#21464;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#35823;&#23548;&#27169;&#22411;&#20135;&#29983;&#38169;&#35823;&#30340;&#26631;&#31614;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21453;&#20107;&#23454;&#29983;&#25104;&#27169;&#22411;&#20174;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#20027;&#21160;&#25277;&#26679;&#29983;&#25104;&#22823;&#37327;&#19981;&#21516;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#65292;&#28982;&#21518;&#29992;&#23398;&#20064;&#30340;&#25104;&#23545;&#20998;&#31867;&#22120;&#33258;&#21160;&#26631;&#35760;&#23427;&#20204;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#35265;&#26159;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#25104;&#23545;&#20998;&#31867;&#22120;&#26469;&#25554;&#20540;&#21407;&#22987;&#26679;&#20363;&#21644;&#21453;&#20107;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#21487;&#20197;&#26356;&#27491;&#30830;&#22320;&#26631;&#35760;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#23567;&#35268;&#27169;&#30340;&#20154;&#24037;&#26631;&#35760;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#65292;&#26174;&#33879;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Data Augmentation (CDA) is a commonly used technique for improving robustness in natural language classifiers. However, one fundamental challenge is how to discover meaningful counterfactuals and efficiently label them, with minimal human labeling cost. Most existing methods either completely rely on human-annotated labels, an expensive process which limits the scale of counterfactual data, or implicitly assume label invariance, which may mislead the model with incorrect labels. In this paper, we present a novel framework that utilizes counterfactual generative models to generate a large number of diverse counterfactuals by actively sampling from regions of uncertainty, and then automatically label them with a learned pairwise classifier. Our key insight is that we can more correctly label the generated counterfactuals by training a pairwise classifier that interpolates the relationship between the original example and the counterfactual. We demonstrate that with a small
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#20135;&#20013;&#23481;&#26131;&#20135;&#29983;&#24187;&#35273;&#38169;&#35823;&#65292;&#36825;&#20123;&#24187;&#35273;&#20250;&#23548;&#33268;&#27169;&#22411;&#20135;&#29983;&#26356;&#22810;&#30340;&#38169;&#35823;&#65292;&#24182;&#19988;&#27169;&#22411;&#21487;&#20197;&#33258;&#34892;&#35782;&#21035;&#20854;&#20013;&#30340;&#19968;&#20123;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2305.13534</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#22914;&#20309;&#20250;&#36234;&#26469;&#36234;&#20005;&#37325;
&lt;/p&gt;
&lt;p&gt;
How Language Model Hallucinations Can Snowball. (arXiv:2305.13534v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13534
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#20135;&#20013;&#23481;&#26131;&#20135;&#29983;&#24187;&#35273;&#38169;&#35823;&#65292;&#36825;&#20123;&#24187;&#35273;&#20250;&#23548;&#33268;&#27169;&#22411;&#20135;&#29983;&#26356;&#22810;&#30340;&#38169;&#35823;&#65292;&#24182;&#19988;&#27169;&#22411;&#21487;&#20197;&#33258;&#34892;&#35782;&#21035;&#20854;&#20013;&#30340;&#19968;&#20123;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#20010;&#20027;&#35201;&#39118;&#38505;&#26159;&#23427;&#20204;&#20542;&#21521;&#20110;&#20135;&#29983;&#38169;&#35823;&#30340;&#35821;&#21477;&#12290;&#36825;&#20123;&#24187;&#35273;&#36890;&#24120;&#24402;&#22240;&#20110;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#32570;&#21475;&#65292;&#20294;&#25105;&#20204;&#20551;&#35774;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#24403;&#35777;&#26126;&#20043;&#21069;&#20135;&#29983;&#30340;&#24187;&#35273;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#20250;&#36755;&#20986;&#38169;&#35823;&#30340;&#22768;&#26126;&#65292;&#23427;&#20204;&#21487;&#20197;&#21333;&#29420;&#22320;&#35782;&#21035;&#20026;&#19981;&#27491;&#30830;&#30340;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19977;&#20010;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;ChatGPT&#21644;GPT-4&#32463;&#24120;&#38472;&#36848;&#38169;&#35823;&#30340;&#31572;&#26696;&#65292;&#24182;&#25552;&#20379;&#33267;&#23569;&#19968;&#20010;&#19981;&#27491;&#30830;&#30340;&#22768;&#26126;&#30340;&#35299;&#37322;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;ChatGPT&#21644;GPT-4&#21487;&#20197;&#20998;&#21035;&#35782;&#21035;&#20854;&#33258;&#24049;&#38169;&#35823;&#30340;67&#65285;&#21644;87&#65285;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#24187;&#35273;&#28378;&#38634;&#29699;&#65306;&#35821;&#35328;&#27169;&#22411;&#36807;&#24230;&#33268;&#21147;&#20110;&#26089;&#26399;&#30340;&#38169;&#35823;&#65292;&#23548;&#33268;&#26356;&#22810;&#30340;&#38169;&#35823;&#65292;&#21542;&#21017;&#23427;&#19981;&#20250;&#29359;&#36825;&#20123;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major risk of using language models in practical applications is their tendency to hallucinate incorrect statements. Hallucinations are often attributed to knowledge gaps in LMs, but we hypothesize that in some cases, when justifying previously generated hallucinations, LMs output false claims that they can separately recognize as incorrect. We construct three question-answering datasets where ChatGPT and GPT-4 often state an incorrect answer and offer an explanation with at least one incorrect claim. Crucially, we find that ChatGPT and GPT-4 can identify 67% and 87% of their own mistakes, respectively. We refer to this phenomenon as hallucination snowballing: an LM over-commits to early mistakes, leading to more mistakes that it otherwise would not make.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24320;&#25918;&#19990;&#30028;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#24050;&#30693;&#31867;&#21644;&#26032;&#39062;&#31867;&#20013;&#36827;&#34892;&#26174;&#24335;&#21644;&#38544;&#24335;&#34920;&#31034;&#30340;&#20851;&#31995;&#20998;&#31867;&#65292;&#22312;&#30495;&#23454;&#22330;&#26223;&#25968;&#25454;&#30340;&#29305;&#24449;&#19979;&#36827;&#34892;&#20102;&#20004;&#20010;&#20851;&#38190;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.13533</link><description>&lt;p&gt;
&#23454;&#29616;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#23545;&#40784;&#30340;&#24320;&#25918;&#19990;&#30028;&#21322;&#30417;&#30563;&#24191;&#20041;&#20851;&#31995;&#21457;&#29616; (arXiv:2305.13533v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
Open-world Semi-supervised Generalized Relation Discovery Aligned in a Real-world Setting. (arXiv:2305.13533v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24320;&#25918;&#19990;&#30028;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#24050;&#30693;&#31867;&#21644;&#26032;&#39062;&#31867;&#20013;&#36827;&#34892;&#26174;&#24335;&#21644;&#38544;&#24335;&#34920;&#31034;&#30340;&#20851;&#31995;&#20998;&#31867;&#65292;&#22312;&#30495;&#23454;&#22330;&#26223;&#25968;&#25454;&#30340;&#29305;&#24449;&#19979;&#36827;&#34892;&#20102;&#20004;&#20010;&#20851;&#38190;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#20851;&#31995;&#25277;&#21462;(OpenRE)&#26368;&#36817;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#31616;&#21270;&#20102;&#38382;&#39064;&#65292;&#20551;&#35774;&#25152;&#26377;&#26410;&#26631;&#35760;&#30340;&#25991;&#26412;&#37117;&#23646;&#20110;&#26032;&#31867;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;OpenRE&#35774;&#32622;&#24212;&#26356;&#31526;&#21512;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#25913;&#36827;:(a)&#26410;&#26631;&#35760;&#25968;&#25454;&#24212;&#21253;&#25324;&#24050;&#30693;&#21644;&#26032;&#39062;&#30340;&#31867;&#65292;&#21253;&#25324;&#38590;&#20197;&#21306;&#20998;&#30340;&#36127;&#26679;&#26412;&#23454;&#20363;;(b)&#26032;&#39062;&#30340;&#31867;&#38598;&#24212;&#35813;&#20195;&#34920;&#38271;&#23614;&#20851;&#31995;&#31867;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#27969;&#34892;&#30340;&#20851;&#31995;&#65292;&#22914;&#26631;&#39064;&#21644;&#20301;&#32622;&#65292;&#36890;&#24120;&#21487;&#20197;&#36890;&#36807;&#29305;&#23450;&#30340;&#27169;&#24335;&#38544;&#21547;&#22320;&#25512;&#26029;&#65292;&#32780;&#38271;&#23614;&#30340;&#20851;&#31995;&#20542;&#21521;&#20110;&#22312;&#21477;&#23376;&#20013;&#26126;&#30830;&#34920;&#31034;&#12290;&#22312;&#36825;&#20123;&#35265;&#35299;&#30340;&#25512;&#21160;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KNoRD&#65288;&#24050;&#30693;&#21644;&#26032;&#39062;&#20851;&#31995;&#21457;&#29616;&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#23545;&#24050;&#30693;&#31867;&#21644;&#26032;&#39062;&#31867;&#20013;&#30340;&#26174;&#24335;&#21644;&#38544;&#24335;&#34920;&#31034;&#30340;&#20851;&#31995;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-world Relation Extraction (OpenRE) has recently garnered significant attention. However, existing approaches tend to oversimplify the problem by assuming that all unlabeled texts belong to novel classes, thereby limiting the practicality of these methods. We argue that the OpenRE setting should be more aligned with the characteristics of real-world data. Specifically, we propose two key improvements: (a) unlabeled data should encompass known and novel classes, including hard-negative instances; and (b) the set of novel classes should represent long-tail relation types. Furthermore, we observe that popular relations such as titles and locations can often be implicitly inferred through specific patterns, while long-tail relations tend to be explicitly expressed in sentences. Motivated by these insights, we present a novel method called KNoRD (Known and Novel Relation Discovery), which effectively classifies explicitly and implicitly expressed relations from known and novel classes w
&lt;/p&gt;</description></item><item><title>StyloMetrix&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#21644;&#21477;&#27861;&#30340;&#25991;&#26412;&#25366;&#25496;&#24037;&#20855;&#65292;&#21487;&#20998;&#26512;&#20044;&#20811;&#20848;&#35821;&#30340;&#35821;&#27861;&#12289;&#25991;&#20307;&#21644;&#21477;&#27861;&#27169;&#24335;&#65292;&#36866;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.13530</link><description>&lt;p&gt;
&#38754;&#21521;&#20044;&#20811;&#20848;&#35821;&#30340;&#22522;&#20110;&#35821;&#27861;&#21644;&#21477;&#27861;&#30340;&#35821;&#26009;&#24211;&#20998;&#26512;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
The Grammar and Syntax Based Corpus Analysis Tool For The Ukrainian Language. (arXiv:2305.13530v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13530
&lt;/p&gt;
&lt;p&gt;
StyloMetrix&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#21644;&#21477;&#27861;&#30340;&#25991;&#26412;&#25366;&#25496;&#24037;&#20855;&#65292;&#21487;&#20998;&#26512;&#20044;&#20811;&#20848;&#35821;&#30340;&#35821;&#27861;&#12289;&#25991;&#20307;&#21644;&#21477;&#27861;&#27169;&#24335;&#65292;&#36866;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25991;&#26412;&#25366;&#25496;&#24037;&#20855;StyloMetrix&#65292;&#26368;&#21021;&#29992;&#20110;&#27874;&#20848;&#35821;&#65292;&#21518;&#26469;&#25193;&#23637;&#21040;&#33521;&#35821;&#21644;&#20044;&#20811;&#20848;&#35821;&#12290;&#23427;&#22522;&#20110;&#35745;&#31639;&#35821;&#35328;&#23398;&#23478;&#21644;&#25991;&#23398;&#30740;&#31350;&#20154;&#21592;&#25163;&#24037;&#21046;&#20316;&#30340;&#21508;&#31181;&#25351;&#26631;&#65292;&#20998;&#26512;&#35821;&#27861;&#12289;&#25991;&#20307;&#21644;&#21477;&#27861;&#27169;&#24335;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;StyloMetrix&#31649;&#36947;&#65292;&#24182;&#38024;&#23545;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#20102;&#19968;&#20123;&#23454;&#39564;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#25105;&#20204;&#36719;&#20214;&#21253;&#30340;&#20027;&#35201;&#38480;&#21046;&#21644;&#25351;&#26631;&#30340;&#35780;&#20272;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides an overview of a text mining tool the StyloMetrix developed initially for the Polish language and further extended for English and recently for Ukrainian. The StyloMetrix is built upon various metrics crafted manually by computational linguists and researchers from literary studies to analyze grammatical, stylistic, and syntactic patterns. The idea of constructing the statistical evaluation of syntactic and grammar features is straightforward and familiar for the languages like English, Spanish, German, and others; it is yet to be developed for low-resource languages like Ukrainian. We describe the StyloMetrix pipeline and provide some experiments with this tool for the text classification task. We also describe our package's main limitations and the metrics' evaluation procedure.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#33521;&#25991;&#25968;&#25454;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#39640;&#25928;&#26631;&#35760;&#26041;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#27604;&#36328;&#35821;&#35328;&#36716;&#31227;&#22522;&#20934;&#26174;&#30528;&#25552;&#39640;&#65288;&#26368;&#22810;&#25552;&#39640;22%&#65289;&#12290;</title><link>http://arxiv.org/abs/2305.13528</link><description>&lt;p&gt;
&#26080;&#38656;&#36716;&#31227;&#25968;&#25454;&#30340;&#22810;&#35821;&#35328;&#30701;&#35821;&#26631;&#35760;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Transfer-Free Data-Efficient Multilingual Slot Labeling. (arXiv:2305.13528v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#33521;&#25991;&#25968;&#25454;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#39640;&#25928;&#26631;&#35760;&#26041;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#27604;&#36328;&#35821;&#35328;&#36716;&#31227;&#22522;&#20934;&#26174;&#30528;&#25552;&#39640;&#65288;&#26368;&#22810;&#25552;&#39640;22%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#35821;&#26631;&#35760;&#65288;SL&#65289;&#26159;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#65288;ToD&#65289;&#31995;&#32479;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#32780;&#20854;&#20013;&#30340;&#30701;&#35821;&#21644;&#30456;&#24212;&#30340;&#20540;&#36890;&#24120;&#26159;&#29305;&#23450;&#20110;&#35821;&#35328;&#12289;&#20219;&#21153;&#21644;&#39046;&#22495;&#30340;&#12290;&#22240;&#27492;&#65292;&#23558;&#31995;&#32479;&#25193;&#23637;&#21040;&#20219;&#20309;&#26032;&#30340;&#35821;&#35328;-&#39046;&#22495;-&#20219;&#21153;&#37197;&#32622;&#38656;&#35201;&#37325;&#26032;&#36816;&#34892;&#26114;&#36149;&#32780;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#25968;&#25454;&#26631;&#27880;&#27969;&#31243;&#12290;&#20026;&#20102;&#20943;&#36731;&#22266;&#26377;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24403;&#21069;&#22810;&#35821;&#35328;ToD&#30740;&#31350;&#20551;&#35774;&#29305;&#23450;&#20219;&#21153;&#21644;&#39046;&#22495;&#30340;&#36275;&#22815;&#33521;&#35821;&#27880;&#37322;&#25968;&#25454;&#22987;&#32456;&#21487;&#29992;&#65292;&#22240;&#27492;&#22312;&#26631;&#20934;&#30340;&#36328;&#35821;&#35328;&#20256;&#36755;&#35774;&#32622;&#20013;&#36816;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25670;&#33073;&#36825;&#31181;&#24120;&#24120;&#19981;&#29616;&#23454;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#30740;&#31350;&#25361;&#25112;&#24615;&#22330;&#26223;&#65292;&#21363;&#26080;&#27861;&#20445;&#35777;&#20855;&#26377;&#20256;&#36755;&#21151;&#33021;&#30340;&#33521;&#25991;&#27880;&#37322;&#25968;&#25454;&#65292;&#24182;&#19987;&#27880;&#20110;&#22312;&#30446;&#26631;&#35821;&#35328;&#20013;&#30452;&#25509;&#36827;&#34892;&#22810;&#35821;&#35328;&#25968;&#25454;&#39640;&#25928;&#30340;&#26631;&#35760;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30701;&#35821;&#26631;&#35760;&#26041;&#27861;&#65288;&#31216;&#20026;TWOSL&#65289;&#65292;&#23558;&#26631;&#20934;&#30340;&#22810;&#35821;&#35328;SL&#36716;&#21270;&#20026;&#26080;&#38656;&#36716;&#31227;&#25968;&#25454;&#30340;&#39640;&#25928;&#35774;&#32622;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#23567;&#22411;&#24179;&#34892;&#35821;&#26009;&#24211;&#26469;&#23545;&#40784;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#30701;&#35821;&#38598;&#65292;&#24182;&#21033;&#29992;&#36825;&#31181;&#23545;&#40784;&#26469;&#36890;&#36807;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#22810;&#35821;&#35328;&#30701;&#35821;&#24863;&#24212;&#26694;&#26550;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#20256;&#36882;&#27880;&#37322;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#24212;&#29992;&#20027;&#21160;&#23398;&#20064;&#26469;&#36890;&#36807;&#21253;&#21547;&#26469;&#33258;&#30446;&#26631;&#35821;&#35328;&#30340;&#23569;&#37327;&#30417;&#30563;&#25968;&#25454;&#26469;&#36845;&#20195;&#26356;&#26032;&#21644;&#25913;&#36827;&#22810;&#35821;&#35328;&#30701;&#35821;&#20998;&#31867;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#35821;&#35328;&#21644;&#39046;&#22495;&#20013;&#65292;TWOSL&#27604;&#26368;&#20808;&#36827;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#22522;&#32447;&#26174;&#30528;&#25552;&#39640;&#20102;&#30701;&#35821;F1&#20998;&#25968;&#65288;&#26368;&#22810;&#25552;&#39640;22%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Slot labeling (SL) is a core component of task-oriented dialogue (ToD) systems, where slots and corresponding values are usually language-, task- and domain-specific. Therefore, extending the system to any new language-domain-task configuration requires (re)running an expensive and resource-intensive data annotation process. To mitigate the inherent data scarcity issue, current research on multilingual ToD assumes that sufficient English-language annotated data are always available for particular tasks and domains, and thus operates in a standard cross-lingual transfer setup. In this work, we depart from this often unrealistic assumption. We examine challenging scenarios where such transfer-enabling English annotated data cannot be guaranteed, and focus on bootstrapping multilingual data-efficient slot labelers in transfer-free scenarios directly in the target languages without any English-ready data. We propose a two-stage slot labeling approach (termed TWOSL) which transforms standar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#25386;&#23041;&#20004;&#31181;&#20070;&#20889;&#24418;&#24335;&#35821;&#26009;&#24211;&#20013;&#30340;&#23454;&#20307;&#21644;&#20849;&#25351;&#26631;&#27880;&#25968;&#25454;&#21512;&#24182;&#21040;&#20102;&#36890;&#29992;&#20381;&#23384;&#35821;&#26009;&#24211;&#65288;UD treebanks&#65289;&#20013;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21152;&#20837;&#23454;&#20307;&#21644;&#20849;&#25351;&#20449;&#24687;&#30340;&#25386;&#23041;UD treebank&#65292;&#23545;&#26410;&#26469;&#35821;&#26009;&#24211;&#23545;&#40784;&#21644;&#20849;&#25351;&#27880;&#37322;&#24037;&#20316;&#26377;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2305.13527</link><description>&lt;p&gt;
&#23558;&#25386;&#23041;UD Treebank&#19982;&#23454;&#20307;&#21644;&#20849;&#25351;&#20449;&#24687;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligning the Norwegian UD Treebank with Entity and Coreference Information. (arXiv:2305.13527v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#25386;&#23041;&#20004;&#31181;&#20070;&#20889;&#24418;&#24335;&#35821;&#26009;&#24211;&#20013;&#30340;&#23454;&#20307;&#21644;&#20849;&#25351;&#26631;&#27880;&#25968;&#25454;&#21512;&#24182;&#21040;&#20102;&#36890;&#29992;&#20381;&#23384;&#35821;&#26009;&#24211;&#65288;UD treebanks&#65289;&#20013;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21152;&#20837;&#23454;&#20307;&#21644;&#20849;&#25351;&#20449;&#24687;&#30340;&#25386;&#23041;UD treebank&#65292;&#23545;&#26410;&#26469;&#35821;&#26009;&#24211;&#23545;&#40784;&#21644;&#20849;&#25351;&#27880;&#37322;&#24037;&#20316;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#25386;&#23041;&#20004;&#31181;&#20070;&#20889;&#24418;&#24335;&#35821;&#26009;&#24211;&#9472;&#9472;Bokm{&#229;}l&#21644;Nynorsk&#30340;&#36890;&#29992;&#20381;&#23384;&#35821;&#26009;&#24211;&#65288;UD treebanks&#65289;&#20013;&#30340;&#23454;&#20307;&#21644;&#20849;&#25351;&#26631;&#27880;&#25968;&#25454;&#30340;&#21512;&#24182;&#38598;&#21512;&#12290;&#25152;&#21512;&#24182;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;Norwegian Named Entities&#65288;NorNE&#65289;&#21644;Norwegian Anaphora Resolution Corpus&#65288;NARC&#65289;&#20004;&#37096;&#20998;&#12290;&#34429;&#28982;NorNE&#19982;&#26087;&#29256;&#26412;&#30340;treebank&#23545;&#40784;&#65292;&#20294;NARC&#21017;&#26410;&#33021;&#23545;&#40784;&#65292;&#38656;&#35201;&#20174;&#21407;&#22987;&#27880;&#37322;&#21040;UD&#32467;&#26500;&#21644;CoNLL-U&#26684;&#24335;&#36827;&#34892;&#24191;&#27867;&#30340;&#36716;&#25442;&#12290;&#25105;&#20204;&#22312;&#27492;&#28436;&#31034;&#20102;&#36716;&#25442;&#21644;&#23545;&#40784;&#36807;&#31243;&#65292;&#24182;&#20998;&#26512;&#20102;&#21457;&#29616;&#30340;&#25968;&#25454;&#38382;&#39064;&#21644;&#38169;&#35823;&#65292;&#20854;&#20013;&#21253;&#25324;&#21407;&#22987;treebank&#20013;&#30340;&#25968;&#25454;&#20998;&#21106;&#37325;&#21472;&#38382;&#39064;&#12290;&#36825;&#20123;&#31243;&#24207;&#21644;&#24320;&#21457;&#30340;&#31995;&#32479;&#21487;&#33021;&#26377;&#21161;&#20110;&#26410;&#26469;&#30340;&#35821;&#26009;&#24211;&#23545;&#40784;&#21644;&#20849;&#25351;&#27880;&#37322;&#24037;&#20316;&#12290;&#21512;&#24182;&#30340;&#35821;&#26009;&#24211;&#21253;&#25324;&#31532;&#19968;&#20010;&#21152;&#20837;&#21629;&#21517;&#23454;&#20307;&#21644;&#20849;&#25351;&#20449;&#24687;&#30340;&#25386;&#23041;UD treebank&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a merged collection of entity and coreference annotated data grounded in the Universal Dependencies (UD) treebanks for the two written forms of Norwegian: Bokm{\aa}l and Nynorsk. The aligned and converted corpora are the \textit{Norwegian Named Entities} (NorNE) and \textit{Norwegian Anaphora Resolution Corpus} (NARC). While NorNE is aligned with an older version of the treebank, NARC is misaligned and requires extensive transformation from the original annotations to the UD structure and CoNLL-U format. We here demonstrate the conversion and alignment processes, along with an analysis of discovered issues and errors in the data -- some of which include data split overlaps in the original treebank. These procedures and the developed system may prove helpful for future corpus alignment and coreference annotation endeavors. The merged corpora comprise the first Norwegian UD treebank enriched with named entities and coreference information.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20020;&#24202;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#8212;&#8212;GatorTronGPT&#65292;&#23427;&#25913;&#21892;&#20102;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#20351;&#29992;&#23427;&#35757;&#32451;&#30340;&#21512;&#25104;NLP&#27169;&#22411;&#24615;&#33021;&#20248;&#20110;&#20351;&#29992;&#30495;&#23454;&#20020;&#24202;&#25991;&#26412;&#35757;&#32451;&#30340;NLP&#27169;&#22411;&#65292;&#21307;&#29983;&#20063;&#26080;&#27861;&#21306;&#20998;&#23427;&#21644;&#30495;&#23454;&#20020;&#24202;&#25991;&#26412;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.13523</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#30740;&#31350;&#19982;&#20581;&#24247;&#20445;&#20581;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study of Generative Large Language Model for Medical Research and Healthcare. (arXiv:2305.13523v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20020;&#24202;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#8212;&#8212;GatorTronGPT&#65292;&#23427;&#25913;&#21892;&#20102;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#20351;&#29992;&#23427;&#35757;&#32451;&#30340;&#21512;&#25104;NLP&#27169;&#22411;&#24615;&#33021;&#20248;&#20110;&#20351;&#29992;&#30495;&#23454;&#20020;&#24202;&#25991;&#26412;&#35757;&#32451;&#30340;NLP&#27169;&#22411;&#65292;&#21307;&#29983;&#20063;&#26080;&#27861;&#21306;&#20998;&#23427;&#21644;&#30495;&#23454;&#20020;&#24202;&#25991;&#26412;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#22791;&#21463;&#30633;&#30446;&#65292;&#20294;&#24403;&#21069;&#30340;&#20551;&#35774;&#37117;&#26159;&#22522;&#20110;&#36890;&#29992;&#22411;&#30340;LLMs&#65292;&#22914;ChatGPT&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20020;&#24202;&#29983;&#25104;&#24335;LLM&#65292;GatorTronGPT&#65292;&#20351;&#29992;2770&#20159;&#20010;&#28151;&#21512;&#20020;&#24202;&#19982;&#33521;&#35821;&#25991;&#26412;&#21644;&#19968;&#20010;200&#20159;&#21442;&#25968;&#30340;GPT-3&#26550;&#26500;&#12290;GatorTronGPT&#25913;&#36827;&#20102;&#21307;&#23398;&#30740;&#31350;&#30340;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#20351;&#29992;GatorTronGPT&#35757;&#32451;&#30340;&#21512;&#25104;NLP&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#24615;&#33021;&#20248;&#20110;&#20351;&#29992;&#30495;&#23454;&#20020;&#24202;&#25991;&#26412;&#35757;&#32451;&#30340;NLP&#27169;&#22411;&#12290;&#20351;&#29992;1&#65288;&#26368;&#24046;&#65289;&#21040;9&#65288;&#26368;&#22909;&#65289;&#30340;&#21051;&#24230;&#36827;&#34892;&#30340;&#21307;&#29983;&#22270;&#28789;&#27979;&#35797;&#34920;&#26126;&#65292;&#35821;&#35328;&#21487;&#35835;&#24615;&#65288;p = 0.22; GatorTronGPT&#20026;6.57&#65292;&#20154;&#31867;&#20026;6.93&#65289;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#65288;p = 0.91; GatorTronGPT&#20026;7.0&#65292;&#20154;&#31867;&#20026;6.97&#65289;&#27809;&#26377;&#26174;&#30528;&#24046;&#24322;&#65292;&#24182;&#19988;&#21307;&#29983;&#26080;&#27861;&#21306;&#20998;&#23427;&#20204;&#65288;p &lt;0.001&#65289;&#12290;&#27492;&#30740;&#31350;&#25552;&#20379;&#20102;&#20851;&#20110;LLMs&#22312;&#21307;&#23398;&#30740;&#31350;&#21644;&#20445;&#20581;&#20013;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is enormous enthusiasm and concerns in using large language models (LLMs) in healthcare, yet current assumptions are all based on general-purpose LLMs such as ChatGPT. This study develops a clinical generative LLM, GatorTronGPT, using 277 billion words of mixed clinical and English text with a GPT-3 architecture of 20 billion parameters. GatorTronGPT improves biomedical natural language processing for medical research. Synthetic NLP models trained using GatorTronGPT generated text outperform NLP models trained using real-world clinical text. Physicians Turing test using 1 (worst) to 9 (best) scale shows that there is no significant difference in linguistic readability (p = 0.22; 6.57 of GatorTronGPT compared with 6.93 of human) and clinical relevance (p = 0.91; 7.0 of GatorTronGPT compared with 6.97 of human) and that physicians cannot differentiate them (p &lt; 0.001). This study provides insights on the opportunities and challenges of LLMs for medical research and healthcare.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CEO&#30340;&#20107;&#20214;&#26412;&#20307;&#35825;&#23548;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#25918;&#26494;&#39044;&#23450;&#20041;&#20107;&#20214;&#26412;&#20307;&#25152;&#24378;&#21152;&#30340;&#38480;&#21046;&#65292;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#26816;&#27979;&#25972;&#20010;&#35821;&#26009;&#24211;&#20013;&#26174;&#33879;&#30340;&#20107;&#20214;&#65292;&#24182;&#35825;&#23548;&#20855;&#26377;&#26377;&#24847;&#20041;&#21517;&#31216;&#30340;&#20998;&#23618;&#20107;&#20214;&#26412;&#20307;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20854;&#35825;&#23548;&#30340;&#27169;&#24335;&#20855;&#26377;&#26356;&#22909;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13521</link><description>&lt;p&gt;
CEO&#65306;&#22522;&#20110;&#35821;&#26009;&#24211;&#30340;&#24320;&#25918;&#22495;&#20107;&#20214;&#26412;&#20307;&#35825;&#23548;
&lt;/p&gt;
&lt;p&gt;
CEO: Corpus-based Open-Domain Event Ontology Induction. (arXiv:2305.13521v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CEO&#30340;&#20107;&#20214;&#26412;&#20307;&#35825;&#23548;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#25918;&#26494;&#39044;&#23450;&#20041;&#20107;&#20214;&#26412;&#20307;&#25152;&#24378;&#21152;&#30340;&#38480;&#21046;&#65292;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#26816;&#27979;&#25972;&#20010;&#35821;&#26009;&#24211;&#20013;&#26174;&#33879;&#30340;&#20107;&#20214;&#65292;&#24182;&#35825;&#23548;&#20855;&#26377;&#26377;&#24847;&#20041;&#21517;&#31216;&#30340;&#20998;&#23618;&#20107;&#20214;&#26412;&#20307;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20854;&#35825;&#23548;&#30340;&#27169;&#24335;&#20855;&#26377;&#26356;&#22909;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#38754;&#21521;&#20107;&#20214;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#36890;&#24120;&#20165;&#36866;&#29992;&#20110;&#39044;&#23450;&#20041;&#26412;&#20307;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35821;&#26009;&#24211;&#30340;&#20107;&#20214;&#26412;&#20307;&#35825;&#23548;&#27169;&#22411;CEO&#65292;&#20197;&#25918;&#26494;&#39044;&#23450;&#20041;&#20107;&#20214;&#26412;&#20307;&#25152;&#24378;&#21152;&#30340;&#38480;&#21046;&#12290;&#22312;&#27809;&#26377;&#30452;&#25509;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;CEO&#21033;&#29992;&#21487;&#29992;&#25688;&#35201;&#25968;&#25454;&#38598;&#30340;&#36828;&#31243;&#30417;&#30563;&#26469;&#26816;&#27979;&#25972;&#20010;&#35821;&#26009;&#24211;&#20013;&#26174;&#33879;&#30340;&#20107;&#20214;&#65292;&#24182;&#21033;&#29992;&#22806;&#37096;&#20107;&#20214;&#30693;&#35782;&#20351;&#36317;&#31163;&#30701;&#30340;&#20107;&#20214;&#20855;&#26377;&#30456;&#20284;&#30340;&#23884;&#20837;&#12290;&#23545;&#19977;&#20010;&#24120;&#29992;&#30340;&#20107;&#20214;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CEO&#35825;&#23548;&#30340;&#27169;&#24335;&#20855;&#26377;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;CEO&#26159;&#31532;&#19968;&#20010;&#33021;&#22312;&#21313;&#19968;&#20010;&#24320;&#25918;&#22495;&#35821;&#26009;&#24211;&#19978;&#35825;&#23548;&#20855;&#26377;&#26377;&#24847;&#20041;&#21517;&#31216;&#30340;&#20998;&#23618;&#20107;&#20214;&#26412;&#20307;&#30340;&#20107;&#20214;&#26412;&#20307;&#35825;&#23548;&#27169;&#22411;&#65292;&#20351;&#35825;&#23548;&#30340;&#27169;&#24335;&#26356;&#20540;&#24471;&#20449;&#36182;&#24182;&#26356;&#26131;&#20110;&#36827;&#19968;&#27493;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing event-centric NLP models often only apply to the pre-defined ontology, which significantly restricts their generalization capabilities. This paper presents CEO, a novel Corpus-based Event Ontology induction model to relax the restriction imposed by pre-defined event ontologies. Without direct supervision, CEO leverages distant supervision from available summary datasets to detect corpus-wise salient events and exploits external event knowledge to force events within a short distance to have close embeddings. Experiments on three popular event datasets show that the schema induced by CEO has better coverage and higher accuracy than previous methods. Moreover, CEO is the first event ontology induction model that can induce a hierarchical event ontology with meaningful names on eleven open-domain corpora, making the induced schema more trustworthy and easier to be further curated.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;Massively Multilingual Speech (MMS)&#39033;&#30446;&#65292;&#35813;&#39033;&#30446;&#36890;&#36807;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#23558;&#21463;&#25903;&#25345;&#30340;&#35821;&#35328;&#25968;&#37327;&#22686;&#21152;&#20102;10-40&#20493;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;MMS&#30340;&#22810;&#35821;&#31181;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#21487;&#20197;&#22312;FLEURS&#22522;&#20934;&#27979;&#35797;&#30340;54&#31181;&#35821;&#35328;&#19978;&#23558;&#21333;&#35789;&#38169;&#35823;&#29575;&#38477;&#33267;&#19968;&#21322;&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2305.13516</link><description>&lt;p&gt;
&#23558;&#35821;&#38899;&#25216;&#26415;&#25193;&#23637;&#21040;1000+&#31181;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Scaling Speech Technology to 1,000+ Languages. (arXiv:2305.13516v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13516
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;Massively Multilingual Speech (MMS)&#39033;&#30446;&#65292;&#35813;&#39033;&#30446;&#36890;&#36807;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#23558;&#21463;&#25903;&#25345;&#30340;&#35821;&#35328;&#25968;&#37327;&#22686;&#21152;&#20102;10-40&#20493;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;MMS&#30340;&#22810;&#35821;&#31181;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#21487;&#20197;&#22312;FLEURS&#22522;&#20934;&#27979;&#35797;&#30340;54&#31181;&#35821;&#35328;&#19978;&#23558;&#21333;&#35789;&#38169;&#35823;&#29575;&#38477;&#33267;&#19968;&#21322;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#23637;&#35821;&#38899;&#25216;&#26415;&#30340;&#35821;&#35328;&#35206;&#30422;&#33539;&#22260;&#23545;&#20110;&#35768;&#22810;&#20154;&#25913;&#21892;&#33719;&#21462;&#20449;&#24687;&#30340;&#26426;&#20250;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#35821;&#38899;&#25216;&#26415;&#20165;&#38480;&#20110;&#32422;100&#31181;&#35821;&#35328;&#65292;&#32780;&#36825;&#21482;&#26159;&#19990;&#30028;&#19978;&#20849;&#20351;&#29992;&#30340;7000&#22810;&#31181;&#35821;&#35328;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;Massively Multilingual Speech (MMS)&#39033;&#30446;&#36890;&#36807;&#26032;&#25968;&#25454;&#38598;&#21644;&#26377;&#25928;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23558;&#21463;&#25903;&#25345;&#30340;&#35821;&#35328;&#25968;&#37327;&#22686;&#21152;&#20102;10-40&#20493;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#39044;&#35757;&#32451;&#30340;wav2vec 2.0&#27169;&#22411;&#65292;&#35206;&#30422;&#20102;1406&#31181;&#35821;&#35328;&#65292;&#19968;&#20010;&#29992;&#20110;1107&#31181;&#35821;&#35328;&#30340;&#21333;&#19968;&#30340;&#22810;&#35821;&#31181;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#30456;&#21516;&#25968;&#37327;&#30340;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;4017&#31181;&#35821;&#35328;&#30340;&#35821;&#35328;&#35782;&#21035;&#27169;&#22411;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#22810;&#35821;&#31181;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#22312;FLEURS&#22522;&#20934;&#27979;&#35797;&#30340;54&#31181;&#35821;&#35328;&#19978;&#21487;&#20197;&#23558;Whisper&#30340;&#21333;&#35789;&#38169;&#35823;&#29575;&#38477;&#33267;&#19968;&#21322;&#20197;&#19978;&#65292;&#32780;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#22522;&#20110;&#25152;&#26377;&#35821;&#31181;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Expanding the language coverage of speech technology has the potential to improve access to information for many more people. However, current speech technology is restricted to about one hundred languages which is a small fraction of the over 7,000 languages spoken around the world. The Massively Multilingual Speech (MMS) project increases the number of supported languages by 10-40x, depending on the task. The main ingredients are a new dataset based on readings of publicly available religious texts and effectively leveraging self-supervised learning. We built pre-trained wav2vec 2.0 models covering 1,406 languages, a single multilingual automatic speech recognition model for 1,107 languages, speech synthesis models for the same number of languages, as well as a language identification model for 4,017 languages. Experiments show that our multilingual speech recognition model more than halves the word error rate of Whisper on 54 languages of the FLEURS benchmark while being trained on 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#35821;&#35328;&#27169;&#22411;&#37325;&#20889;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25913;&#21892;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13514</link><description>&lt;p&gt;
&#23567;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#37325;&#20889;&#20854;&#36755;&#20986;&#26469;&#25552;&#39640;&#24040;&#22411;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Small Language Models Improve Giants by Rewriting Their Outputs. (arXiv:2305.13514v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#35821;&#35328;&#27169;&#22411;&#37325;&#20889;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25913;&#21892;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#25361;&#25112;&#24615;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#36890;&#24120;&#19981;&#22914;&#24494;&#35843;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#24040;&#22823;&#20307;&#31215;&#21644;&#36890;&#36807;API&#30340;&#21463;&#38480;&#35775;&#38382;&#20351;&#24471;&#38024;&#23545;&#20219;&#21153;&#30340;&#24494;&#35843;&#19981;&#20999;&#23454;&#38469;&#12290;&#32780;&#19988;&#65292;LLMs&#23545;&#25552;&#31034;&#30340;&#19981;&#21516;&#26041;&#38754;&#65288;&#20363;&#22914;&#65292;&#28436;&#31034;&#30340;&#36873;&#25321;&#21644;&#39034;&#24207;&#65289;&#24456;&#25935;&#24863;&#65292;&#22240;&#27492;&#21487;&#33021;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#36827;&#34892;&#25552;&#31034;&#24037;&#31243;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#20854;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#32416;&#27491;LLM&#30340;&#36755;&#20986;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#23569;&#26679;&#26412;&#25552;&#31034;LLM&#29983;&#25104;&#19968;&#20010;&#20505;&#36873;&#27744;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#26356;&#23567;&#30340;&#27169;&#22411;&#65292;LM-corrector&#65288;LMCor&#65289;&#26469;&#25913;&#36827;LLM&#29983;&#25104;&#30340;&#36755;&#20986;&#12290;LMCor&#34987;&#35757;&#32451;&#29992;&#20110;&#23545;&#20505;&#36873;&#32773;&#36827;&#34892;&#25490;&#21517;&#12289;&#32452;&#21512;&#21644;&#37325;&#20889;&#65292;&#20197;&#20135;&#29983;&#26368;&#32456;&#30340;&#30446;&#26631;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#19968;&#20010;&#23567;&#30340;LMCor&#27169;&#22411;&#65288;250M&#65289;&#65292;&#20063;&#21487;&#20197;&#26174;&#30528;&#25913;&#21892;LLMs&#65288;62B&#65289;&#30340;&#23569;&#26679;&#26412;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;LMCor&#34920;&#29616;&#20986;&#23545;&#25552;&#31034;&#21464;&#21270;&#30340;&#25913;&#36827;&#40065;&#26834;&#24615;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#25913;&#21892;LLMs&#23454;&#38469;&#21487;&#29992;&#24615;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive few-shot learning capabilities, but they often underperform compared to fine-tuned models on challenging tasks. Furthermore, their large size and restricted access only through APIs make task-specific fine-tuning impractical. Moreover, LLMs are sensitive to different aspects of prompts (e.g., the selection and order of demonstrations) and can thus require time-consuming prompt engineering. In this light, we propose a method to correct LLM outputs without relying on their weights. First, we generate a pool of candidates by few-shot prompting an LLM. Second, we refine the LLM-generated outputs using a smaller model, the LM-corrector (LMCor), which is trained to rank, combine and rewrite the candidates to produce the final target output. Our experiments demonstrate that even a small LMCor model (250M) substantially improves the few-shot performance of LLMs (62B) across diverse tasks. Moreover, we illustrate that the LMCor exhibits 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#20960;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#26368;&#22823;&#27169;&#22411;&#21487;&#20197;&#22312;&#38646;-shot&#23398;&#20064;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#36798;&#21040;&#19982;&#30417;&#30563;&#27169;&#22411;&#30456;&#36817;&#30340;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#24230;&#65292;&#20294;&#22312;&#27133;&#22635;&#20805;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#19988;&#23545;ASR&#38169;&#35823;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2305.13512</link><description>&lt;p&gt;
&#33021;ChatGPT&#26816;&#27979;&#20986;&#24847;&#22270;&#21527;&#65311;&#35780;&#20272;&#29992;&#20110;&#21475;&#35821;&#29702;&#35299;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding. (arXiv:2305.13512v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#20960;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#26368;&#22823;&#27169;&#22411;&#21487;&#20197;&#22312;&#38646;-shot&#23398;&#20064;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#36798;&#21040;&#19982;&#30417;&#30563;&#27169;&#22411;&#30456;&#36817;&#30340;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#24230;&#65292;&#20294;&#22312;&#27133;&#22635;&#20805;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#19988;&#23545;ASR&#38169;&#35823;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#29305;&#21035;&#20307;&#29616;&#22312;&#36890;&#36807;&#25552;&#31034;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#38646;-shot&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;&#20026;&#20102;&#35780;&#20272;&#23427;&#20204;&#23545;&#21475;&#35821;&#29702;&#35299;&#65288;SLU&#65289;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;ChatGPT&#21644;OPT&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#26368;&#22823;&#27169;&#22411;&#29305;&#26377;&#30340;&#26032;&#20852;&#33021;&#21147;&#65292;&#21363;&#22312;&#32473;&#23450;Oracle&#36716;&#24405;&#30340;&#21508;&#31181;&#35821;&#35328;&#19978;&#65292;&#20854;&#21487;&#20197;&#25509;&#36817;&#20110;&#30417;&#30563;&#27169;&#22411;&#30340;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#36866;&#21512;&#21333;&#20010;GPU&#30340;&#36739;&#23567;&#27169;&#22411;&#30340;&#32467;&#26524;&#36828;&#36828;&#33853;&#21518;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#38169;&#35823;&#26696;&#20363;&#36890;&#24120;&#26469;&#33258;&#25968;&#25454;&#38598;&#30340;&#27880;&#37322;&#26041;&#26696;&#65307;ChatGPT&#30340;&#21709;&#24212;&#20173;&#28982;&#26159;&#21512;&#29702;&#30340;&#12290;&#20294;&#26159;&#25105;&#20204;&#21457;&#29616;&#65292;&#35813;&#27169;&#22411;&#22312;&#27133;&#22635;&#20805;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#19988;&#23545;ASR&#38169;&#35823;&#38750;&#24120;&#25935;&#24863;&#65292;&#22240;&#27492;&#34920;&#26126;&#20102;&#23558;&#36825;&#20123;&#25991;&#26412;&#27169;&#22411;&#24212;&#29992;&#20110;&#21475;&#35821;&#29702;&#35299;&#30340;&#20005;&#23803;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large pretrained language models have demonstrated strong language understanding capabilities. This is particularly reflected in their zero-shot and in-context learning abilities on downstream tasks through prompting. To assess their impact on spoken language understanding (SLU), we evaluate several such models like ChatGPT and OPT of different sizes on multiple benchmarks. We verify the emergent ability unique to the largest models as they can reach intent classification accuracy close to that of supervised models with zero or few shots on various languages given oracle transcripts. By contrast, the results for smaller models fitting a single GPU fall far behind. We note that the error cases often arise from the annotation scheme of the dataset; responses from ChatGPT are still reasonable. We show, however, that the model is worse at slot filling, and its performance is sensitive to ASR errors, suggesting serious challenges for the application of those textual models on SLU.
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#30340;&#26694;&#26550;&#65292;&#24182;&#21253;&#25324;&#20102;&#29420;&#29305;&#30340;&#23376;&#20219;&#21153;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#25991;&#26412;&#65292;&#22270;&#20687;&#65292;&#38899;&#39057;&#21644;&#35270;&#39057;&#36825;&#22235;&#31181;&#27169;&#24577;&#30340;&#29616;&#23454;&#24212;&#29992;&#12290;&#32426;&#24405;&#20102;&#30456;&#20851;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#21644;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.13507</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#65306;&#19968;&#20221;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Multimodal Automated Fact-Checking: A Survey. (arXiv:2305.13507v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#30340;&#26694;&#26550;&#65292;&#24182;&#21253;&#25324;&#20102;&#29420;&#29305;&#30340;&#23376;&#20219;&#21153;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#25991;&#26412;&#65292;&#22270;&#20687;&#65292;&#38899;&#39057;&#21644;&#35270;&#39057;&#36825;&#22235;&#31181;&#27169;&#24577;&#30340;&#29616;&#23454;&#24212;&#29992;&#12290;&#32426;&#24405;&#20102;&#30456;&#20851;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38169;&#35823;&#20449;&#24687;&#65292;&#21363;&#20107;&#23454;&#19978;&#19981;&#27491;&#30830;&#30340;&#20449;&#24687;&#65292;&#36890;&#24120;&#20197;&#22810;&#31181;&#24418;&#24335;&#20256;&#36798;&#65292;&#20363;&#22914;&#24102;&#26377;&#26631;&#39064;&#30340;&#22270;&#20687;&#12290; &#23427;&#34987;&#20154;&#20204;&#35270;&#20026;&#26356;&#21487;&#20449;&#65292;&#27604;&#20854;&#20165;&#38480;&#20110;&#25991;&#26412;&#30340;&#23545;&#24212;&#29289;&#25193;&#25955;&#36895;&#24230;&#26356;&#24555;&#65292;&#33539;&#22260;&#26356;&#24191;&#12290; &#23613;&#31649;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#28041;&#21450;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#65288;AFC&#65289;&#65292;&#20294;&#20197;&#24448;&#30340;&#35843;&#26597;&#20027;&#35201;&#38598;&#20013;&#22312;&#25991;&#26412;&#35823;&#23548;&#26041;&#38754;&#12290; &#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#25324;&#22810;&#27169;&#24577;&#35823;&#23548;&#29420;&#29305;&#23376;&#20219;&#21153;&#22312;&#20869;&#30340;AFC&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#19978;&#35752;&#35770;&#20102;&#19981;&#21516;&#31038;&#21306;&#25152;&#21457;&#23637;&#30340;&#30456;&#20851;&#26415;&#35821;&#12290; &#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#29616;&#23454;&#19990;&#30028;&#20107;&#23454;&#26680;&#26597;&#20013;&#23384;&#22312;&#30340;&#22235;&#31181;&#27169;&#24577;&#65306;&#25991;&#26412;&#65292;&#22270;&#20687;&#65292;&#38899;&#39057;&#21644;&#35270;&#39057;&#12290; &#25105;&#20204;&#35843;&#26597;&#20102;&#22522;&#20934;&#21644;&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#21644;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Misinformation, i.e. factually incorrect information, is often conveyed in multiple modalities, e.g. an image accompanied by a caption. It is perceived as more credible by humans, and spreads faster and wider than its text-only counterparts. While an increasing body of research investigates automated fact-checking (AFC), previous surveys mostly focus on textual misinformation. In this survey, we conceptualise a framework for AFC including subtasks unique to multimodal misinformation. Furthermore, we discuss related terminological developed in different communities in the context of our framework. We focus on four modalities prevalent in real-world fact-checking: text, image, audio, and video. We survey benchmarks and models, and discuss limitations and promising directions for future research.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#27010;&#36848;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;&#12290;&#35813;&#24212;&#29992;&#28085;&#30422;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#36755;&#20837;&#24773;&#20917;&#21644;&#32422;&#26463;&#26465;&#20214;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#24050;&#25506;&#32034;&#30340;&#22810;&#31181;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#30446;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.13504</link><description>&lt;p&gt;
&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Neural Machine Translation for Code Generation. (arXiv:2305.13504v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13504
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#27010;&#36848;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;&#12290;&#35813;&#24212;&#29992;&#28085;&#30422;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#36755;&#20837;&#24773;&#20917;&#21644;&#32422;&#26463;&#26465;&#20214;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#24050;&#25506;&#32034;&#30340;&#22810;&#31181;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#30446;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24320;&#21457;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#33258;&#21160;&#32763;&#35793;&#33258;&#28982;&#35821;&#35328;&#21040;&#21478;&#19968;&#31181;&#35821;&#35328;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#26368;&#36817;&#65292;&#36825;&#20123;NMT&#26041;&#27861;&#24050;&#34987;&#24212;&#29992;&#21040;&#31243;&#24207;&#20195;&#30721;&#30340;&#29983;&#25104;&#20013;&#12290;&#22312;NMT&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#20013;&#65292;&#20219;&#21153;&#26159;&#29983;&#25104;&#28385;&#36275;&#36755;&#20837;&#20013;&#34920;&#36798;&#30340;&#32422;&#26463;&#26465;&#20214;&#30340;&#36755;&#20986;&#28304;&#20195;&#30721;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#19981;&#21516;&#30340;&#36755;&#20837;&#24773;&#20917;&#65292;&#21253;&#25324;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#20195;&#30721;&#29983;&#25104;&#65292;&#36739;&#20302;&#32423;&#21035;&#30340;&#34920;&#31034;&#65292;&#22914;&#20108;&#36827;&#21046;&#25110;&#27719;&#32534;&#65288;&#31070;&#32463;&#21453;&#27719;&#32534;&#65289;&#65292;&#28304;&#20195;&#30721;&#30340;&#37096;&#20998;&#34920;&#31034;&#65288;&#20195;&#30721;&#23436;&#25104;&#21644;&#20462;&#22797;&#65289;&#65292;&#20197;&#21450;&#21478;&#19968;&#31181;&#35821;&#35328;&#30340;&#28304;&#20195;&#30721;&#65288;&#20195;&#30721;&#32763;&#35793;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;NMT&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#25991;&#29486;&#36827;&#34892;&#27010;&#36848;&#65292;&#25353;&#29031;&#36755;&#20837;&#21644;&#36755;&#20986;&#34920;&#31034;&#65292;&#27169;&#22411;&#26550;&#26500;&#65292;&#20351;&#29992;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#23545;&#24050;&#25506;&#32034;&#30340;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#35752;&#35770;NMT-based&#26041;&#27861;&#29983;&#25104;&#20195;&#30721;&#30340;&#29616;&#26377;&#38480;&#21046;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural machine translation (NMT) methods developed for natural language processing have been shown to be highly successful in automating translation from one natural language to another. Recently, these NMT methods have been adapted to the generation of program code. In NMT for code generation, the task is to generate output source code that satisfies constraints expressed in the input. In the literature, a variety of different input scenarios have been explored, including generating code based on natural language description, lower-level representations such as binary or assembly (neural decompilation), partial representations of source code (code completion and repair), and source code in another language (code translation). In this paper we survey the NMT for code generation literature, cataloging the variety of methods that have been explored according to input and output representations, model architectures, optimization techniques used, data sets, and evaluation methods. We discu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#32512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#28304;&#20219;&#21153;&#30340;&#22266;&#23450;&#25991;&#26412;&#34920;&#31034;&#12290;&#29420;&#31435;&#22320;&#23398;&#20064;&#27599;&#20010;&#28304;&#20219;&#21153;&#30340;&#20219;&#21153;&#29305;&#23450;&#21069;&#32512;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#21512;&#25104;&#26368;&#32456;&#34920;&#31034;&#65292;&#20197;&#35299;&#20915;&#22914;&#20309;&#23398;&#20064;&#26131;&#20110;&#26356;&#26032;&#12289;&#36866;&#29992;&#24191;&#27867;&#30340;&#36890;&#29992;&#25991;&#26412;&#34920;&#31034;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.13499</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#36866;&#24212;&#20219;&#21153;&#29305;&#23450;&#21069;&#32512;&#23398;&#20064;&#26131;&#20110;&#26356;&#26032;&#30340;&#36890;&#29992;&#25991;&#26412;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Easily Updated General Purpose Text Representations with Adaptable Task-Specific Prefixes. (arXiv:2305.13499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#32512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#28304;&#20219;&#21153;&#30340;&#22266;&#23450;&#25991;&#26412;&#34920;&#31034;&#12290;&#29420;&#31435;&#22320;&#23398;&#20064;&#27599;&#20010;&#28304;&#20219;&#21153;&#30340;&#20219;&#21153;&#29305;&#23450;&#21069;&#32512;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#21512;&#25104;&#26368;&#32456;&#34920;&#31034;&#65292;&#20197;&#35299;&#20915;&#22914;&#20309;&#23398;&#20064;&#26131;&#20110;&#26356;&#26032;&#12289;&#36866;&#29992;&#24191;&#27867;&#30340;&#36890;&#29992;&#25991;&#26412;&#34920;&#31034;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#38656;&#35201;&#20174;&#30456;&#21516;&#30340;&#25991;&#26412;&#20013;&#36827;&#34892;&#22810;&#27425;&#39044;&#27979;&#12290;&#38024;&#23545;&#27599;&#20010;&#19979;&#28216;&#20219;&#21153;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20250;&#22312;&#25512;&#26029;&#26102;&#24102;&#26469;&#35745;&#31639;&#36127;&#25285;&#65292;&#22240;&#20026;&#38656;&#35201;&#22810;&#27425;&#27491;&#21521;&#20256;&#36882;&#12290;&#20026;&#20102;&#25674;&#38144;&#35745;&#31639;&#25104;&#26412;&#65292;&#20923;&#32467;&#35821;&#35328;&#27169;&#22411;&#24182;&#22522;&#20110;&#22266;&#23450;&#25991;&#26412;&#34920;&#31034;&#20026;&#19979;&#28216;&#20219;&#21153;&#24314;&#31435;&#36731;&#37327;&#32423;&#27169;&#22411;&#26159;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#23398;&#20064;&#19968;&#31181;&#22266;&#23450;&#20294;&#36890;&#29992;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#20197;&#20415;&#22312;&#26410;&#30693;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#25104;&#20026;&#19968;&#39033;&#25361;&#25112;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20197;&#22810;&#20219;&#21153;&#30340;&#26041;&#24335;&#23545;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21487;&#20197;&#25552;&#39640;&#34920;&#31034;&#30340;&#36890;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#32512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#28304;&#20219;&#21153;&#30340;&#22266;&#23450;&#25991;&#26412;&#34920;&#31034;&#12290;&#25105;&#20204;&#29420;&#31435;&#22320;&#23398;&#20064;&#27599;&#20010;&#28304;&#20219;&#21153;&#30340;&#20219;&#21153;&#29305;&#23450;&#21069;&#32512;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#21512;&#25104;&#26368;&#32456;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;...
&lt;/p&gt;
&lt;p&gt;
Many real-world applications require making multiple predictions from the same text. Fine-tuning a large pre-trained language model for each downstream task causes computational burdens in the inference time due to several times of forward passes. To amortize the computational cost, freezing the language model and building lightweight models for downstream tasks based on fixed text representations are common solutions. Accordingly, how to learn fixed but general text representations that can generalize well to unseen downstream tasks becomes a challenge. Previous works have shown that the generalizability of representations can be improved by fine-tuning the pre-trained language model with some source tasks in a multi-tasking way. In this work, we propose a prefix-based method to learn the fixed text representations with source tasks. We learn a task-specific prefix for each source task independently and combine them to get the final representations. Our experimental results show that 
&lt;/p&gt;</description></item><item><title>Flover&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#24182;&#34892;&#25512;&#26029;&#30340;&#26102;&#38388;&#34701;&#21512;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24182;&#34892;&#24615;&#19981;&#36275;&#21644;&#28789;&#27963;&#24615;&#24046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#21152;&#39640;&#25928;&#30340;&#25512;&#26029;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13484</link><description>&lt;p&gt;
Flover&#65306;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#33258;&#22238;&#24402;&#27169;&#22411;&#24182;&#34892;&#25512;&#26029;&#30340;&#26102;&#38388;&#34701;&#21512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Flover: A Temporal Fusion Framework for Efficient Autoregressive Model Parallel Inference. (arXiv:2305.13484v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13484
&lt;/p&gt;
&lt;p&gt;
Flover&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#24182;&#34892;&#25512;&#26029;&#30340;&#26102;&#38388;&#34701;&#21512;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24182;&#34892;&#24615;&#19981;&#36275;&#21644;&#28789;&#27963;&#24615;&#24046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#21152;&#39640;&#25928;&#30340;&#25512;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#24555;&#36895;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#27169;&#22411;&#25512;&#26029;&#24615;&#33021;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#65292;&#23588;&#20854;&#26159;&#22312;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#24182;&#34987;&#37096;&#32626;&#22312;&#22810;&#20010;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#24773;&#20917;&#19979;&#12290;&#33258;&#22238;&#24402;&#27169;&#22411;&#30001;&#20110;&#22312;&#20247;&#22810;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#22240;&#27492;&#22791;&#21463;&#20851;&#27880;&#12290;&#36825;&#20123;&#27169;&#22411;&#35774;&#35745;&#19978;&#37319;&#29992;&#20102;&#19968;&#31181;&#26102;&#38388;&#20381;&#36182;&#32467;&#26500;&#65292;&#20854;&#20013;&#24403;&#21069;token&#30340;&#27010;&#29575;&#20998;&#24067;&#21463;&#21040;&#21069;&#38754;token&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26412;&#36136;&#19978;&#30340;&#24207;&#21015;&#29305;&#24615;&#36981;&#24490;&#39532;&#23572;&#21487;&#22827;&#38142;&#20551;&#35774;&#65292;&#32570;&#20047;&#26102;&#38388;&#24182;&#34892;&#24615;&#65292;&#22240;&#27492;&#23384;&#22312;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#22312;&#24037;&#19994;&#32972;&#26223;&#19979;&#65292;&#25512;&#26029;&#35831;&#27714;&#36981;&#24490;&#27850;&#26494;&#26102;&#38388;&#20998;&#24067;&#65292;&#38656;&#35201;&#19981;&#21516;&#30340;&#21709;&#24212;&#38271;&#24230;&#65292;&#36825;&#31181;&#24182;&#34892;&#24615;&#30340;&#32570;&#22833;&#26356;&#21152;&#26126;&#26174;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#22914;&#21160;&#24577;&#25209;&#22788;&#29702;&#21644;&#24182;&#21457;&#27169;&#22411;&#23454;&#20363;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#31895;&#31890;&#24230;&#30340;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#30340;&#24320;&#38144;&#21644;&#32570;&#20047;&#28789;&#27963;&#24615;&#65292;&#26080;&#27861;&#23454;&#29616;&#26368;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving field of deep learning, the performance of model inference has become a pivotal aspect as models become more complex and are deployed in diverse applications. Among these, autoregressive models stand out due to their state-of-the-art performance in numerous generative tasks. These models, by design, harness a temporal dependency structure, where the current token's probability distribution is conditioned on preceding tokens. This inherently sequential characteristic, however, adheres to the Markov Chain assumption and lacks temporal parallelism, which poses unique challenges. Particularly in industrial contexts where inference requests, following a Poisson time distribution, necessitate diverse response lengths, this absence of parallelism is more profound. Existing solutions, such as dynamic batching and concurrent model instances, nevertheless, come with severe overheads and a lack of flexibility, these coarse-grained methods fall short of achieving optimal la
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35821;&#35328;&#26041;&#38754;&#65288;&#22914;&#30456;&#20114;&#26234;&#33021;&#24615;&#25110;&#35821;&#35328;&#30456;&#20851;&#24615;&#65289;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#33258;&#21160;&#21487;&#35835;&#24615;&#35780;&#20272;&#65292;&#24182;&#20351;&#29992;&#19977;&#31181;&#33778;&#24459;&#23486;&#35821;&#35328;&#30340;&#30701;&#31687;&#23567;&#35828;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#21457;&#29616;&#24212;&#29992;&#19987;&#19994;&#29305;&#24449;CrossNGO&#21487;&#20197;&#25913;&#21892;ARA&#12290;</title><link>http://arxiv.org/abs/2305.13478</link><description>&lt;p&gt;
&#30456;&#36817;&#35821;&#35328;&#30340;&#33258;&#21160;&#21487;&#35835;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Automatic Readability Assessment for Closely Related Languages. (arXiv:2305.13478v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35821;&#35328;&#26041;&#38754;&#65288;&#22914;&#30456;&#20114;&#26234;&#33021;&#24615;&#25110;&#35821;&#35328;&#30456;&#20851;&#24615;&#65289;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#33258;&#21160;&#21487;&#35835;&#24615;&#35780;&#20272;&#65292;&#24182;&#20351;&#29992;&#19977;&#31181;&#33778;&#24459;&#23486;&#35821;&#35328;&#30340;&#30701;&#31687;&#23567;&#35828;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#21457;&#29616;&#24212;&#29992;&#19987;&#19994;&#29305;&#24449;CrossNGO&#21487;&#20197;&#25913;&#21892;ARA&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#21160;&#21487;&#35835;&#24615;&#35780;&#20272;&#65288;ARA&#65289;&#30340;&#20027;&#35201;&#30740;&#31350;&#37325;&#28857;&#24050;&#36716;&#21521;&#20351;&#29992;&#26114;&#36149;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#65292;&#20256;&#32479;&#30340;&#25163;&#24037;&#21046;&#20316;&#29305;&#24449;&#20173;&#28982;&#24191;&#27867;&#20351;&#29992;&#65292;&#22240;&#20026;&#32570;&#20047;&#29616;&#26377;&#30340;NLP&#24037;&#20855;&#26469;&#25552;&#21462;&#26356;&#28145;&#23618;&#27425;&#30340;&#35821;&#35328;&#34920;&#31034;&#12290;&#25105;&#20204;&#20174;&#25216;&#26415;&#32452;&#20214;&#19978;&#36864;&#19968;&#27493;&#65292;&#30528;&#37325;&#25506;&#35752;&#22914;&#20309;&#36890;&#36807;&#35832;&#22914;&#30456;&#20114;&#26234;&#33021;&#24615;&#25110;&#35821;&#35328;&#30456;&#20851;&#24615;&#30340;&#35821;&#35328;&#26041;&#38754;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#30340;ARA&#12290;&#25105;&#20204;&#25910;&#38598;&#22312;&#33778;&#24459;&#23486;&#30340;&#19977;&#31181;&#35821;&#35328;&#65288;&#20182;&#21152;&#31108;&#35821;&#65292;&#27604;&#31185;&#23572;&#35821;&#21644;&#23487;&#21153;&#35821;&#65289;&#20013;&#32534;&#20889;&#30340;&#30701;&#31687;&#23567;&#35828;&#26469;&#35757;&#32451;&#21487;&#35835;&#24615;&#35780;&#20272;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#21508;&#31181;&#36328;&#35821;&#35328;&#35774;&#32622;&#20013;&#30340;&#25968;&#25454;&#21644;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290; &#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#39640;&#30456;&#20114;&#21487;&#29702;&#35299;&#24615;&#30340;&#35821;&#35328;&#20013;&#24212;&#29992;n-gram&#37325;&#21472;&#30340;&#26032;&#22411;&#19987;&#19994;&#29305;&#24449;CrossNGO&#21487;&#20197;&#25913;&#21892;ARA&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the main focus of research on automatic readability assessment (ARA) has shifted towards using expensive deep learning-based methods with the primary goal of increasing models' accuracy. This, however, is rarely applicable for low-resource languages where traditional handcrafted features are still widely used due to the lack of existing NLP tools to extract deeper linguistic representations. In this work, we take a step back from the technical component and focus on how linguistic aspects such as mutual intelligibility or degree of language relatedness can improve ARA in a low-resource setting. We collect short stories written in three languages in the Philippines-Tagalog, Bikol, and Cebuano-to train readability assessment models and explore the interaction of data and features in various cross-lingual setups. Our results show that the inclusion of CrossNGO, a novel specialized feature exploiting n-gram overlap applied to languages with high mutual intelligibility, sig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#37038;&#20214;&#39046;&#22495;&#30340;&#20107;&#20214;&#25277;&#21462;&#25968;&#25454;&#38598;\dataset&#65292;&#27604;&#36739;&#20102;&#24207;&#21015;&#26631;&#35760;&#21644;&#29983;&#25104;&#24335;&#31471;&#21040;&#31471;&#25277;&#21462;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#20219;&#21153;&#23384;&#22312;&#38750;&#36830;&#32493;&#20849;&#20139;&#35302;&#21457;&#22120;&#36328;&#24230;&#12289;&#38750;&#21629;&#21517;&#23454;&#20307;&#21442;&#25968;&#21644;&#37038;&#20214;&#20250;&#35805;&#21382;&#21490;&#31561;&#38590;&#28857;&#65292;&#26410;&#26469;&#38656;&#35201;&#26356;&#22810;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.13469</link><description>&lt;p&gt;
MAILEX: &#37038;&#20214;&#20107;&#20214;&#19982;&#21442;&#25968;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
MAILEX: Email Event and Argument Extraction. (arXiv:2305.13469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#37038;&#20214;&#39046;&#22495;&#30340;&#20107;&#20214;&#25277;&#21462;&#25968;&#25454;&#38598;\dataset&#65292;&#27604;&#36739;&#20102;&#24207;&#21015;&#26631;&#35760;&#21644;&#29983;&#25104;&#24335;&#31471;&#21040;&#31471;&#25277;&#21462;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#20219;&#21153;&#23384;&#22312;&#38750;&#36830;&#32493;&#20849;&#20139;&#35302;&#21457;&#22120;&#36328;&#24230;&#12289;&#38750;&#21629;&#21517;&#23454;&#20307;&#21442;&#25968;&#21644;&#37038;&#20214;&#20250;&#35805;&#21382;&#21490;&#31561;&#38590;&#28857;&#65292;&#26410;&#26469;&#38656;&#35201;&#26356;&#22810;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598; \dataset&#65292;&#29992;&#20110;&#20174;&#37038;&#20214;&#20018;&#20013;&#25191;&#34892;&#20107;&#20214;&#25277;&#21462;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#28085;&#30422;&#20102;&#37038;&#20214;&#39046;&#22495;&#20013;&#30340; 10 &#31181;&#20107;&#20214;&#31867;&#22411;&#21644; 76 &#20010;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#25968;&#25454;&#38598;&#21253;&#25324;&#32422; 4K &#23553;&#26631;&#35760;&#26377;&#32422; 9K &#20010;&#20107;&#20214;&#23454;&#20363;&#30340;&#37038;&#20214;&#12290;&#20026;&#20102;&#20102;&#35299;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#20004;&#31181;&#24120;&#35265;&#30340;&#20107;&#20214;&#25277;&#21462;&#26041;&#27861;&#65292;&#21363;&#24207;&#21015;&#26631;&#35760;&#21644;&#29983;&#25104;&#24335;&#31471;&#21040;&#31471;&#25277;&#21462;&#65288;&#21253;&#25324;&#20960;&#29575; GPT-3.5&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#37038;&#20214;&#20107;&#20214;&#25277;&#21462;&#20219;&#21153;&#36828;&#26410;&#24471;&#21040;&#35299;&#20915;&#65292;&#22240;&#20026;&#23384;&#22312;&#35832;&#22810;&#38590;&#28857;&#65292;&#20363;&#22914;&#25552;&#21462;&#38750;&#36830;&#32493;&#20849;&#20139;&#35302;&#21457;&#22120;&#36328;&#24230;&#12289;&#25552;&#21462;&#38750;&#21629;&#21517;&#23454;&#20307;&#21442;&#25968;&#21644;&#24314;&#27169;&#37038;&#20214;&#20250;&#35805;&#21382;&#21490;&#31561;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#26410;&#26469;&#22312;&#36825;&#20010;&#29305;&#23450;&#39046;&#22495;&#30340;&#20107;&#20214;&#25277;&#21462;&#20219;&#21153;&#20013;&#38656;&#35201;&#36827;&#34892;&#26356;&#22810;&#30740;&#31350;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present the first dataset, \dataset, for performing event extraction from conversational email threads. To this end, we first proposed a new taxonomy covering 10 event types and 76 arguments in the email domain. Our final dataset includes $\sim$4K emails annotated with $\sim$9K event instances. To understand the task challenges, we conducted a series of experiments comparing two commonly-seen lines of approaches for event extraction, i.e., sequence labeling and generative end-to-end extraction (including few-shot GPT-3.5). Our results showed that the task of email event extraction is far from being addressed, due to challenges lying in, e.g., extracting non-continuous, shared trigger spans, extracting non-named entity arguments, and modeling the email conversational history. Our work thus suggests more investigations in this domain-specific event extraction task in the future.\footnote{The source code and dataset can be obtained from \url{https://github.com/salokr/Emai
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25509;&#35302;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21463;&#38480;&#28216;&#25103;&#24335;&#29615;&#22659;&#19979;&#65292;&#33021;&#21542;&#26377;&#24847;&#20041;&#22320;&#35780;&#20272;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#65292;&#30740;&#31350;&#20102;&#20116;&#31181;&#20132;&#20114;&#35774;&#32622;&#65292;&#34920;&#26126;&#24403;&#21069;&#30340;&#32842;&#22825;&#20248;&#21270;LLMs&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#33021;&#22815;&#36981;&#24490;&#28216;&#25103;&#29609;&#27861;&#25351;&#20196;&#12290;&#36825;&#23545;&#20110;&#23558;LLMs&#24320;&#21457;&#20026;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#23545;&#35805;&#20195;&#29702;&#20154;&#20855;&#26377;&#21551;&#31034;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.13455</link><description>&lt;p&gt;
clembench&#65306;&#20351;&#29992;&#28216;&#25103;&#26469;&#35780;&#20272;&#20316;&#20026;&#23545;&#35805;&#20195;&#29702;&#20154;&#30340;&#32842;&#22825;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents. (arXiv:2305.13455v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25509;&#35302;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21463;&#38480;&#28216;&#25103;&#24335;&#29615;&#22659;&#19979;&#65292;&#33021;&#21542;&#26377;&#24847;&#20041;&#22320;&#35780;&#20272;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#65292;&#30740;&#31350;&#20102;&#20116;&#31181;&#20132;&#20114;&#35774;&#32622;&#65292;&#34920;&#26126;&#24403;&#21069;&#30340;&#32842;&#22825;&#20248;&#21270;LLMs&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#33021;&#22815;&#36981;&#24490;&#28216;&#25103;&#29609;&#27861;&#25351;&#20196;&#12290;&#36825;&#23545;&#20110;&#23558;LLMs&#24320;&#21457;&#20026;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#23545;&#35805;&#20195;&#29702;&#20154;&#20855;&#26377;&#21551;&#31034;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#8220;&#31449;&#31435;&#35821;&#35328;&#29702;&#35299;&#20195;&#29702;&#8221;&#30340;&#31995;&#32479;&#35780;&#20272;&#26041;&#27861;&#8212;&#8212;&#20195;&#29702;&#22312;&#20016;&#23500;&#30340;&#35821;&#35328;&#21644;&#38750;&#35821;&#35328;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#36890;&#36807;&#22312;&#31934;&#24515;&#26500;&#36896;&#30340;&#20114;&#21160;&#29615;&#22659;&#20013;&#36827;&#34892;&#27979;&#35797;&#26469;&#35780;&#20272;&#23427;&#20204;&#12290;&#20854;&#20182;&#26368;&#36817;&#30340;&#24037;&#20316;&#21017;&#35748;&#20026;&#65292;&#22914;&#26524;&#36866;&#24403;&#35774;&#32622;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#36825;&#26679;&#30340;&#20195;&#29702;&#65288;&#30340;&#27169;&#25311;&#22120;&#65289;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36825;&#31181;&#32852;&#31995;&#65306;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#35753;LLMs&#25509;&#35302;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21463;&#38480;&#28216;&#25103;&#24335;&#29615;&#22659;&#26469;&#26377;&#24847;&#20041;&#22320;&#35780;&#20272;&#23427;&#20204;&#30340;&#33021;&#21147;&#65311;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#20116;&#31181;&#20132;&#20114;&#35774;&#32622;&#65292;&#34920;&#26126;&#24403;&#21069;&#30340;&#32842;&#22825;&#20248;&#21270;LLMs&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#33021;&#22815;&#36981;&#24490;&#28216;&#25103;&#29609;&#27861;&#25351;&#20196;&#12290;&#36825;&#31181;&#33021;&#21147;&#21644;&#28216;&#25103;&#29609;&#27861;&#30340;&#36136;&#37327;&#65288;&#36890;&#36807;&#28385;&#36275;&#19981;&#21516;&#28216;&#25103;&#30446;&#26631;&#30340;&#24773;&#20917;&#26469;&#34913;&#37327;&#65289;&#37117;&#36981;&#24490;&#30528;&#21457;&#23637;&#24490;&#29615;&#65292;&#26032;&#22411;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;&#21363;&#20351;&#26159;&#30456;&#23545;&#31616;&#21333;&#30340;&#8220;&#20117;&#23383;&#28216;&#25103;&#8221;&#31034;&#20363;&#28216;&#25103;&#30340;&#25351;&#26631;&#20063;&#25552;&#20379;&#20102;&#27169;&#22411;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#30340;&#22522;&#26412;&#24615;&#33021;&#25351;&#31034;&#12290;&#36825;&#23545;&#20110;&#23558;LLMs&#24320;&#21457;&#20026;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#23545;&#35805;&#20195;&#29702;&#20154;&#20855;&#26377;&#21551;&#31034;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has proposed a methodology for the systematic evaluation of "Situated Language Understanding Agents"-agents that operate in rich linguistic and non-linguistic contexts-through testing them in carefully constructed interactive settings. Other recent work has argued that Large Language Models (LLMs), if suitably set up, can be understood as (simulators of) such agents. A connection suggests itself, which this paper explores: Can LLMs be evaluated meaningfully by exposing them to constrained game-like settings that are built to challenge specific capabilities? As a proof of concept, this paper investigates five interaction settings, showing that current chat-optimised LLMs are, to an extent, capable to follow game-play instructions. Both this capability and the quality of the game play, measured by how well the objectives of the different games are met, follows the development cycle, with newer models performing better. The metrics even for the comparatively simple example gam
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;Transformer&#27169;&#22411;&#30340;&#26435;&#37325;&#21644;&#38544;&#34255;&#29366;&#24577;&#25237;&#24433;&#21040;&#20854;&#35789;&#27719;&#34920;&#20013;&#35299;&#37322;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#20869;&#37096;&#20449;&#24687;&#27969;&#30340;&#27169;&#24335;&#12290;&#25991;&#31456;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#23558;GPT&#30340;&#21069;&#21521;&#20256;&#36882;&#21487;&#35270;&#21270;&#20026;&#20132;&#20114;&#24335;&#27969;&#22270;&#65292;&#31616;&#21270;&#20102;&#22823;&#37327;&#25968;&#25454;&#20026;&#26131;&#20110;&#38405;&#35835;&#30340;&#22270;&#34920;&#65292;&#23637;&#31034;&#20102;&#20854;&#35821;&#20041;&#20449;&#24687;&#27969;&#12290;</title><link>http://arxiv.org/abs/2305.13417</link><description>&lt;p&gt;
&#35299;&#35835;Transformer&#30340;&#27880;&#24847;&#21147;&#21160;&#24577;&#20869;&#23384;&#65292;&#21487;&#35270;&#21270;GPT&#30340;&#35821;&#20041;&#20449;&#24687;&#27969;
&lt;/p&gt;
&lt;p&gt;
Interpreting Transformer's Attention Dynamic Memory and Visualizing the Semantic Information Flow of GPT. (arXiv:2305.13417v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;Transformer&#27169;&#22411;&#30340;&#26435;&#37325;&#21644;&#38544;&#34255;&#29366;&#24577;&#25237;&#24433;&#21040;&#20854;&#35789;&#27719;&#34920;&#20013;&#35299;&#37322;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#20869;&#37096;&#20449;&#24687;&#27969;&#30340;&#27169;&#24335;&#12290;&#25991;&#31456;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#23558;GPT&#30340;&#21069;&#21521;&#20256;&#36882;&#21487;&#35270;&#21270;&#20026;&#20132;&#20114;&#24335;&#27969;&#22270;&#65292;&#31616;&#21270;&#20102;&#22823;&#37327;&#25968;&#25454;&#20026;&#26131;&#20110;&#38405;&#35835;&#30340;&#22270;&#34920;&#65292;&#23637;&#31034;&#20102;&#20854;&#35821;&#20041;&#20449;&#24687;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#36827;&#23637;&#34920;&#26126;&#25105;&#20204;&#21487;&#20197;&#23558;&#22522;&#20110;transformer&#27169;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#21644;&#38544;&#34255;&#29366;&#24577;&#25237;&#24433;&#21040;&#20854;&#35789;&#27719;&#34920;&#20013;&#65292;&#36825;&#31181;&#36716;&#25442;&#20351;&#23427;&#20204;&#21464;&#24471;&#26356;&#23481;&#26131;&#29702;&#35299;&#65292;&#24182;&#19988;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#35821;&#20041;&#20998;&#37197;&#21040;&#20165;&#20316;&#20026;&#25968;&#23383;&#21521;&#37327;&#30340;&#20869;&#23481;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;LM&#27880;&#24847;&#21147;&#22836;&#21644;&#20869;&#23384;&#20540;&#65292;&#36825;&#20123;&#21521;&#37327;&#26159;&#27169;&#22411;&#22312;&#22788;&#29702;&#32473;&#23450;&#36755;&#20837;&#26102;&#21160;&#24577;&#22320;&#21019;&#24314;&#21644;&#26816;&#32034;&#30340;&#12290;&#36890;&#36807;&#36890;&#36807;&#36825;&#31181;&#25237;&#24433;&#20998;&#26512;&#23427;&#20204;&#25152;&#20195;&#34920;&#30340;&#26631;&#35760;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#20869;&#37096;&#20449;&#24687;&#27969;&#30340;&#27169;&#24335;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#24037;&#20855;&#26469;&#23558;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#65288;GPT&#65289;&#30340;&#21069;&#21521;&#20256;&#36882;&#21487;&#35270;&#21270;&#20026;&#20132;&#20114;&#24335;&#27969;&#22270;&#65292;&#20854;&#20013;&#32467;&#28857;&#20195;&#34920;&#31070;&#32463;&#20803;&#25110;&#38544;&#34255;&#29366;&#24577;&#65292;&#36793;&#20195;&#34920;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#21487;&#35270;&#21270;&#23558;&#28023;&#37327;&#25968;&#25454;&#31616;&#21270;&#20026;&#26131;&#20110;&#38405;&#35835;&#30340;&#22270;&#34920;&#65292;&#21453;&#26144;&#20102;&#27169;&#22411;&#20026;&#20160;&#20040;&#36755;&#20986;&#20854;&#32467;&#26524;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#36890;&#36807;&#30830;&#23450;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#24182;&#21487;&#35270;&#21270;&#20854;&#35821;&#20041;&#20449;&#24687;&#27969;&#26469;&#28436;&#31034;&#25105;&#20204;&#24314;&#27169;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in interpretability suggest we can project weights and hidden states of transformer-based language models (LMs) to their vocabulary, a transformation that makes them human interpretable and enables us to assign semantics to what was seen only as numerical vectors. In this paper, we interpret LM attention heads and memory values, the vectors the models dynamically create and recall while processing a given input. By analyzing the tokens they represent through this projection, we identify patterns in the information flow inside the attention mechanism. Based on these discoveries, we create a tool to visualize a forward pass of Generative Pre-trained Transformers (GPTs) as an interactive flow graph, with nodes representing neurons or hidden states and edges representing the interactions between them. Our visualization simplifies huge amounts of data into easy-to-read plots that reflect why models output their results. We demonstrate the utility of our modeling by identifyi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#21644;BERT&#26469;&#34920;&#31034;&#21477;&#27861;&#20381;&#36182;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20016;&#23500;&#28304;&#35821;&#35328;&#34920;&#31034;&#24182;&#24341;&#23548;&#30446;&#26631;&#35821;&#35328;&#29983;&#25104;&#65292;&#32463;&#23454;&#39564;&#35777;&#23454;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#24433;&#21709;BLEU&#20998;&#25968;&#30340;&#24773;&#20917;&#19979;&#25913;&#21892;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.13413</link><description>&lt;p&gt;
&#22522;&#20110;BERT&#21644;&#22270;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#21477;&#27861;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Syntactic Knowledge via Graph Attention with BERT in Machine Translation. (arXiv:2305.13413v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13413
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#21644;BERT&#26469;&#34920;&#31034;&#21477;&#27861;&#20381;&#36182;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20016;&#23500;&#28304;&#35821;&#35328;&#34920;&#31034;&#24182;&#24341;&#23548;&#30446;&#26631;&#35821;&#35328;&#29983;&#25104;&#65292;&#32463;&#23454;&#39564;&#35777;&#23454;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#24433;&#21709;BLEU&#20998;&#25968;&#30340;&#24773;&#20917;&#19979;&#25913;&#21892;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;Transformer&#27169;&#22411;&#36890;&#36807;&#33258;&#27880;&#24847;&#26426;&#21046;&#21487;&#20197;&#26377;&#25928;&#22320;&#33719;&#21462;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#20294;&#36739;&#28145;&#30340;&#21477;&#27861;&#30693;&#35782;&#20173;&#28982;&#26410;&#34987;&#26377;&#25928;&#22320;&#24314;&#27169;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26426;&#22120;&#32763;&#35793;&#22330;&#26223;&#20013;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#21644;BERT&#26469;&#34920;&#31034;&#21477;&#27861;&#20381;&#36182;&#29305;&#24449;&#30340;&#21477;&#27861;&#30693;&#35782;&#65288;SGB&#65289;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20016;&#23500;&#28304;&#35821;&#35328;&#34920;&#31034;&#24182;&#24341;&#23548;&#30446;&#26631;&#35821;&#35328;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20351;&#29992;&#20102;&#37329;&#26631;&#27880;&#21477;&#23376;&#21644;&#36136;&#37327;&#20272;&#35745;&#27169;&#22411;&#26469;&#33719;&#24471;&#20851;&#20110;&#21477;&#27861;&#30693;&#35782;&#23545;&#32763;&#35793;&#36136;&#37327;&#25913;&#21892;&#30340;&#35299;&#37322;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;SGB&#24341;&#25806;&#21487;&#20197;&#22312;&#19977;&#20010;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#25913;&#21892;&#32763;&#35793;&#36136;&#37327;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;BLEU&#20998;&#25968;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21738;&#20123;&#28304;&#21477;&#38271;&#24230;&#21463;&#30410;&#26368;&#22823;&#65292;&#20197;&#21450;&#21738;&#20123;&#20381;&#36182;&#20851;&#31995;&#34987;SGB&#24341;&#25806;&#26356;&#22909;&#22320;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the Transformer model can effectively acquire context features via a self-attention mechanism, deeper syntactic knowledge is still not effectively modeled. To alleviate the above problem, we propose Syntactic knowledge via Graph attention with BERT (SGB) in Machine Translation (MT) scenarios. Graph Attention Network (GAT) and BERT jointly represent syntactic dependency feature as explicit knowledge of the source language to enrich source language representations and guide target language generation. Our experiments use gold syntax-annotation sentences and Quality Estimation (QE) model to obtain interpretability of translation quality improvement regarding syntactic knowledge without being limited to a BLEU score. Experiments show that the proposed SGB engines improve translation quality across the three MT tasks without sacrificing BLEU scores. We investigate what length of source sentences benefits the most and what dependencies are better identified by the SGB engines. We al
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#30340;&#20803;&#32032;&#24863;&#30693;&#27979;&#35797;&#38598;&#20026;&#33258;&#21160;&#25991;&#25688;&#25552;&#20379;&#26356;&#32454;&#31890;&#24230;&#30340;&#21442;&#32771;&#25688;&#35201;&#65292;&#20351;&#29992; LLMS &#20197;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#25688;&#35201;&#65292;&#25552;&#20986; SumCoT &#25216;&#26415;&#20197;&#25913;&#36827;&#36830;&#36143;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#20026;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#29702;&#35299;&#25552;&#20379;&#20102;7&#65285;&#30340;&#20934;&#30830;&#29575;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2305.13412</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20803;&#32032;&#24863;&#30693;&#25991;&#25688;&#65306;&#19987;&#23478;&#23545;&#40784;&#35780;&#20272;&#21644;&#24605;&#36335;&#38142;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method. (arXiv:2305.13412v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13412
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#30340;&#20803;&#32032;&#24863;&#30693;&#27979;&#35797;&#38598;&#20026;&#33258;&#21160;&#25991;&#25688;&#25552;&#20379;&#26356;&#32454;&#31890;&#24230;&#30340;&#21442;&#32771;&#25688;&#35201;&#65292;&#20351;&#29992; LLMS &#20197;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#25688;&#35201;&#65292;&#25552;&#20986; SumCoT &#25216;&#26415;&#20197;&#25913;&#36827;&#36830;&#36143;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#20026;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#29702;&#35299;&#25552;&#20379;&#20102;7&#65285;&#30340;&#20934;&#30830;&#29575;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25688;&#35201;&#29983;&#25104;&#21253;&#21547;&#28304;&#25991;&#20214;&#20851;&#38190;&#24605;&#24819;&#30340;&#31616;&#27905;&#25688;&#35201;&#12290;CNN / DailyMail&#21644;BBC XSum&#20316;&#20026;&#26032;&#38395;&#39046;&#22495;&#30340;&#26368;&#20027;&#27969;&#25968;&#25454;&#38598;&#65292;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#21442;&#32771;&#25688;&#35201;&#35777;&#26126;&#26159;&#22024;&#26434;&#30340;&#65292;&#20027;&#35201;&#20307;&#29616;&#22312;&#20107;&#23454;&#24187;&#35273;&#21644;&#20449;&#24687;&#20887;&#20313;&#26041;&#38754;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#25353;&#29031;Lasswell&#65288;1948&#65289;&#25552;&#20986;&#30340;&#8220;Lasswell&#36890;&#35759;&#27169;&#22411;&#8221;&#27880;&#37322;&#20102;&#26032;&#30340;&#19987;&#23478;&#32534;&#20889;&#30340;&#20803;&#32032;&#24863;&#30693;&#27979;&#35797;&#38598;&#65292;&#20801;&#35768;&#21442;&#32771;&#25688;&#35201;&#23458;&#35266;&#20840;&#38754;&#22320;&#20851;&#27880;&#26356;&#32454;&#31890;&#24230;&#30340;&#26032;&#38395;&#20803;&#32032;&#12290;&#21033;&#29992;&#26032;&#30340;&#27979;&#35797;&#38598;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;LLMs&#24778;&#20154;&#30340;&#38646;-shot&#25991;&#25688;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;LLMs&#30340;&#38646;-shot&#25991;&#25688;&#22312;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#20154;&#31867;&#20559;&#22909;&#21644;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#20043;&#38388;&#19981;&#19968;&#33268;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;Summary Chain-of-Thought&#65288;SumCoT&#65289;&#25216;&#26415;&#65292;&#20197;&#36880;&#27493;&#24341;&#23548;LLMs&#29983;&#25104;&#25688;&#35201;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#36830;&#36143;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;&#22312;CNN / DailyMail&#21644;&#25105;&#20204;&#30340;&#26032;&#20803;&#32032;&#24863;&#30693;&#27979;&#35797;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SumCoT&#22312;&#21508;&#20010;&#26041;&#38754;&#26174;&#33879;&#25552;&#39640;&#20102;LLM&#29983;&#25104;&#30340;&#25688;&#35201;&#36136;&#37327;&#12290;&#20316;&#20026;&#20027;&#35201;&#24212;&#29992;&#65292;&#25105;&#20204;&#23637;&#31034;&#20803;&#32032;&#24863;&#30693;&#25991;&#25688;&#21487;&#20197;&#22312;&#25512;&#33616;&#22330;&#26223;&#20013;&#21463;&#30410;&#20110;&#29992;&#25143;&#24847;&#22270;&#29702;&#35299;&#65292;&#22312;&#23454;&#38469;&#30005;&#24433;&#25688;&#35201;&#25968;&#25454;&#38598;&#19978;&#20135;&#29983;&#26368;&#39640;7&#65285;&#30340;&#20934;&#30830;&#29575;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic summarization generates concise summaries that contain key ideas of source documents. As the most mainstream datasets for the news sub-domain, CNN/DailyMail and BBC XSum have been widely used for performance benchmarking. However, the reference summaries of those datasets turn out to be noisy, mainly in terms of factual hallucination and information redundancy. To address this challenge, we first annotate new expert-writing Element-aware test sets following the "Lasswell Communication Model" proposed by Lasswell (1948), allowing reference summaries to focus on more fine-grained news elements objectively and comprehensively. Utilizing the new test sets, we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMs' zero-shot summaries in prior work. Further, we propose a Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by s
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#22359;&#21270;&#39046;&#22495;&#36866;&#24212;&#30340;&#26694;&#26550;&#65292;&#20351;&#21333;&#20010;Conformer&#27169;&#22411;&#22788;&#29702;&#22810;&#39046;&#22495;&#25968;&#25454;&#65292;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#39046;&#22495;&#29305;&#24322;&#24615;&#65292;&#36890;&#36807;&#22312;Conformer&#32534;&#30721;&#22120;&#20013;&#28155;&#21152;&#27599;&#20010;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#21644;&#36880;&#39046;&#22495;&#30340;&#21069;&#39304;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#22810;&#39046;&#22495;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#22312;&#20854;&#20182;&#39046;&#22495;&#65288;&#22914;&#35821;&#38899;&#25628;&#32034;&#21644;&#21548;&#20889;&#65289;&#20013;&#36798;&#21040;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13408</link><description>&lt;p&gt;
&#22522;&#20110; Conformer &#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#27169;&#22359;&#21270;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Modular Domain Adaptation for Conformer-Based Streaming ASR. (arXiv:2305.13408v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13408
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#22359;&#21270;&#39046;&#22495;&#36866;&#24212;&#30340;&#26694;&#26550;&#65292;&#20351;&#21333;&#20010;Conformer&#27169;&#22411;&#22788;&#29702;&#22810;&#39046;&#22495;&#25968;&#25454;&#65292;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#39046;&#22495;&#29305;&#24322;&#24615;&#65292;&#36890;&#36807;&#22312;Conformer&#32534;&#30721;&#22120;&#20013;&#28155;&#21152;&#27599;&#20010;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#21644;&#36880;&#39046;&#22495;&#30340;&#21069;&#39304;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#22810;&#39046;&#22495;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#22312;&#20854;&#20182;&#39046;&#22495;&#65288;&#22914;&#35821;&#38899;&#25628;&#32034;&#21644;&#21548;&#20889;&#65289;&#20013;&#36798;&#21040;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#39046;&#22495;&#30340;&#35821;&#38899;&#25968;&#25454;&#20855;&#26377;&#19981;&#21516;&#30340;&#22768;&#23398;&#21644;&#35821;&#35328;&#29305;&#24449;&#12290;&#36890;&#24120;&#22312;&#25152;&#26377;&#39046;&#22495;&#30340;&#28151;&#21512;&#25968;&#25454;&#19978;&#35757;&#32451;&#21333;&#20010;&#22810;&#22495;&#27169;&#22411;&#65292;&#22914;Conformer transducer&#35821;&#38899;&#35782;&#21035;&#22120;&#12290;&#20294;&#26159;&#65292;&#26356;&#25913;&#19968;&#20010;&#39046;&#22495;&#30340;&#25968;&#25454;&#25110;&#28155;&#21152;&#26032;&#39046;&#22495;&#20250;&#35201;&#27714;&#37325;&#26032;&#35757;&#32451;&#22810;&#39046;&#22495;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#27169;&#22359;&#21270;&#39046;&#22495;&#36866;&#24212;&#65288;MDA&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#20351;&#21333;&#20010;&#27169;&#22411;&#22788;&#29702;&#22810;&#39046;&#22495;&#25968;&#25454;&#65292;&#21516;&#26102;&#20445;&#25345;&#25152;&#26377;&#21442;&#25968;&#29305;&#23450;&#20110;&#39046;&#22495;&#65292;&#21363;&#27599;&#20010;&#21442;&#25968;&#20165;&#30001;&#19968;&#20010;&#39046;&#22495;&#30340;&#25968;&#25454;&#35757;&#32451;&#12290;&#22312;&#20165;&#20351;&#29992;&#35270;&#39057;&#23383;&#24149;&#25968;&#25454;&#35757;&#32451;&#30340;&#27969;&#24335;Conformer transducer&#19978;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36890;&#36807;&#22312;Conformer encoder&#20013;&#28155;&#21152;&#27599;&#20010;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#21644;&#36880;&#39046;&#22495;&#30340;&#21069;&#39304;&#32593;&#32476;&#65292;&#22522;&#20110;MDA&#30340;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#19982;&#22810;&#39046;&#22495;&#27169;&#22411;&#31867;&#20284;&#30340;&#24615;&#33021;&#22312;&#20854;&#20182;&#39046;&#22495;&#65292;&#22914;&#35821;&#38899;&#25628;&#32034;&#21644;&#21548;&#20889;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech data from different domains has distinct acoustic and linguistic characteristics. It is common to train a single multidomain model such as a Conformer transducer for speech recognition on a mixture of data from all domains. However, changing data in one domain or adding a new domain would require the multidomain model to be retrained. To this end, we propose a framework called modular domain adaptation (MDA) that enables a single model to process multidomain data while keeping all parameters domain-specific, i.e., each parameter is only trained by data from one domain. On a streaming Conformer transducer trained only on video caption data, experimental results show that an MDA-based model can reach similar performance as the multidomain model on other domains such as voice search and dictation by adding per-domain adapters and per-domain feed-forward networks in the Conformer encoder.
&lt;/p&gt;</description></item><item><title>DADA&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#20010;&#26041;&#35328;&#65292;&#22522;&#20110;&#35821;&#35328;&#35268;&#21017;&#30340;&#21160;&#24577;&#32858;&#21512;&#36866;&#37197;&#22120;&#65292;&#21487;&#20026;SAE&#35757;&#32451;&#30340;&#27169;&#22411;&#36171;&#20104;&#22810;&#26041;&#35328;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#38024;&#23545;&#29305;&#23450;&#26041;&#35328;&#21464;&#20307;&#36827;&#34892;&#36866;&#24212;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26041;&#35328;&#36866;&#24212;&#24615;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.13406</link><description>&lt;p&gt;
DADA: &#22522;&#20110;&#35821;&#35328;&#35268;&#21017;&#30340;&#26041;&#35328;&#36866;&#24212;&#24615;&#21160;&#24577;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules. (arXiv:2305.13406v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13406
&lt;/p&gt;
&lt;p&gt;
DADA&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#20010;&#26041;&#35328;&#65292;&#22522;&#20110;&#35821;&#35328;&#35268;&#21017;&#30340;&#21160;&#24577;&#32858;&#21512;&#36866;&#37197;&#22120;&#65292;&#21487;&#20026;SAE&#35757;&#32451;&#30340;&#27169;&#22411;&#36171;&#20104;&#22810;&#26041;&#35328;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#38024;&#23545;&#29305;&#23450;&#26041;&#35328;&#21464;&#20307;&#36827;&#34892;&#36866;&#24212;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26041;&#35328;&#36866;&#24212;&#24615;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#20110;&#26631;&#20934;&#32654;&#24335;&#33521;&#35821;&#65288;SAE&#65289;&#65292;&#22312;&#24212;&#29992;&#20110;&#20854;&#20182;&#33521;&#35821;&#26041;&#35328;&#26102;&#34920;&#29616;&#24448;&#24448;&#36739;&#24046;&#12290;&#32780;&#29616;&#26377;&#30340;&#32531;&#35299;&#26041;&#27861;&#38024;&#23545;&#21333;&#20010;&#30446;&#26631;&#26041;&#35328;&#30340;&#20559;&#24046;&#65292;&#20294;&#20551;&#35774;&#20102;&#21487;&#20197;&#35775;&#38382;&#39640;&#31934;&#24230;&#30340;&#26041;&#35328;&#35782;&#21035;&#31995;&#32479;&#12290;&#26041;&#35328;&#20043;&#38388;&#30340;&#30028;&#38480;&#22266;&#26377;&#24377;&#24615;&#65292;&#20351;&#24471;&#23558;&#35821;&#35328;&#21010;&#20998;&#20026;&#31163;&#25955;&#39044;&#23450;&#20041;&#30340;&#33539;&#30068;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DADA&#65288;&#22522;&#20110;&#35821;&#35328;&#35268;&#21017;&#30340;&#26041;&#35328;&#36866;&#24212;&#24615;&#21160;&#24577;&#32858;&#21512;&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#32452;&#21512;&#22788;&#29702;&#29305;&#23450;&#35821;&#35328;&#29305;&#24449;&#30340;&#36866;&#37197;&#22120;&#65292;&#20026;SAE&#35757;&#32451;&#30340;&#27169;&#22411;&#36171;&#20104;&#22810;&#26041;&#35328;&#30340;&#40065;&#26834;&#24615;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#12290;DADA&#30340;&#32452;&#21512;&#26550;&#26500;&#20801;&#35768;&#26377;&#38024;&#23545;&#24615;&#22320;&#36866;&#24212;&#29305;&#23450;&#26041;&#35328;&#21464;&#20307;&#65292;&#21516;&#26102;&#36866;&#24212;&#21508;&#31181;&#26041;&#35328;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DADA&#23545;&#20110;&#21333;&#20219;&#21153;&#21644;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#37117;&#26159;&#26377;&#25928;&#30340;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#21487;&#35299;&#37322;&#30340;&#26694;&#26550;&#26469;&#36866;&#24212;&#21508;&#31181;&#26041;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing large language models (LLMs) that mainly focus on Standard American English (SAE) often lead to significantly worse performance when being applied to other English dialects. While existing mitigations tackle discrepancies for individual target dialects, they assume access to high-accuracy dialect identification systems. The boundaries between dialects are inherently flexible, making it difficult to categorize language into discrete predefined categories. In this paper, we propose DADA (Dialect Adaptation via Dynamic Aggregation), a modular approach to imbue SAE-trained models with multi-dialectal robustness by composing adapters which handle specific linguistic features. The compositional architecture of DADA allows for both targeted adaptation to specific dialect variants and simultaneous adaptation to various dialects. We show that DADA is effective for both single task and instruction finetuned language models, offering an extensible and interpretable framework for adapting
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#20351;&#29992;GAT&#21644;BERT&#31561;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#22330;&#26223;&#20013;&#24314;&#27169;&#21477;&#27861;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#23618;&#25968;&#21644;&#20851;&#27880;&#22836;&#25968;&#37327;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#30456;&#23545;&#20110;MT-B&#65292;GAT&#22312;&#19979;&#28216;MT&#20219;&#21153;&#20013;&#30340;&#35821;&#27861;&#24314;&#27169;&#26041;&#38754;&#30053;&#24494;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.13403</link><description>&lt;p&gt;
GATology&#22312;&#35821;&#35328;&#23398;&#20013;&#30340;&#24212;&#29992;: &#23427;&#20102;&#35299;&#21738;&#20123;&#21477;&#27861;&#20381;&#23384;&#20851;&#31995;&#65311;
&lt;/p&gt;
&lt;p&gt;
GATology for Linguistics: What Syntactic Dependencies It Knows. (arXiv:2305.13403v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#20351;&#29992;GAT&#21644;BERT&#31561;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#22330;&#26223;&#20013;&#24314;&#27169;&#21477;&#27861;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#23618;&#25968;&#21644;&#20851;&#27880;&#22836;&#25968;&#37327;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#30456;&#23545;&#20110;MT-B&#65292;GAT&#22312;&#19979;&#28216;MT&#20219;&#21153;&#20013;&#30340;&#35821;&#27861;&#24314;&#27169;&#26041;&#38754;&#30053;&#24494;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GAT&#65289;&#26159;&#19968;&#31181;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#65292;&#26159;&#27169;&#25311;&#21644;&#34920;&#31034;&#26126;&#30830;&#30340;&#21477;&#27861;&#30693;&#35782;&#30340;&#31574;&#30053;&#20043;&#19968;&#65292;&#21487;&#20197;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#19968;&#36215;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20351;&#29992;&#12290;&#30446;&#21069;&#65292;&#20174;&#27169;&#22411;&#32467;&#26500;&#30340;&#35282;&#24230;&#26469;&#30740;&#31350;GAT&#22914;&#20309;&#23398;&#20064;&#21477;&#27861;&#30693;&#35782;&#20173;&#28982;&#32570;&#20047;&#30740;&#31350;&#12290;&#20316;&#20026;&#27169;&#22411;&#26126;&#30830;&#21477;&#27861;&#30693;&#35782;&#30340;&#31574;&#30053;&#20043;&#19968;&#65292;GAT&#21644;BERT&#20174;&#26410;&#34987;&#24212;&#29992;&#21644;&#35752;&#35770;&#36807;&#30340;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#22330;&#26223;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20381;&#36182;&#20851;&#31995;&#39044;&#27979;&#20219;&#21153;&#65292;&#30740;&#31350;&#20102;GAT&#22914;&#20309;&#20316;&#20026;&#20851;&#27880;&#22836;&#21644;&#23618;&#25968;&#25968;&#37327;&#30340;&#20989;&#25968;&#26469;&#23398;&#20064;&#19977;&#31181;&#35821;&#35328;&#30340;&#21477;&#27861;&#30693;&#35782;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#37197;&#23545;t&#26816;&#39564;&#21644;F1&#20998;&#25968;&#26469;&#28548;&#28165;GAT&#19982;MT-B&#24494;&#35843;&#20043;&#38388;&#30340;&#21477;&#27861;&#20381;&#23384;&#20851;&#31995;&#39044;&#27979;&#24046;&#24322;&#12290;&#23454;&#29616;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24403;&#22686;&#21152;&#20004;&#20010;GAT&#23618;&#30340;&#20851;&#27880;&#22836;&#30340;&#25968;&#37327;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#36229;&#36807;&#20004;&#23618;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#35770;&#20851;&#27880;&#22836;&#30340;&#25968;&#37327;&#22914;&#20309;&#65292;&#24615;&#33021;&#22343;&#27809;&#26377;&#26174;&#30528;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#19979;&#28216;MT&#20219;&#21153;&#20013;&#65292;GAT&#22312;&#35821;&#27861;&#24314;&#27169;&#26041;&#38754;&#30340;&#34920;&#29616;&#30053;&#22909;&#20110;MT-B&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Attention Network (GAT) is a graph neural network which is one of the strategies for modeling and representing explicit syntactic knowledge and can work with pre-trained models, such as BERT, in downstream tasks. Currently, there is still a lack of investigation into how GAT learns syntactic knowledge from the perspective of model structure. As one of the strategies for modeling explicit syntactic knowledge, GAT and BERT have never been applied and discussed in Machine Translation (MT) scenarios. We design a dependency relation prediction task to study how GAT learns syntactic knowledge of three languages as a function of the number of attention heads and layers. We also use a paired t-test and F1-score to clarify the differences in syntactic dependency prediction between GAT and BERT fine-tuned by the MT task (MT-B). The experiments show that better performance can be achieved by appropriately increasing the number of attention heads with two GAT layers. With more than two layer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#23450;&#20041;&#35821;&#35328;&#30456;&#20284;&#24230;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#35821;&#35328;&#22914;&#20309;&#34920;&#31034;&#22522;&#26412;&#27010;&#24565;&#65292;&#32780;&#38750;&#24120;&#35268;&#30340;&#35789;&#27719;&#25110;&#31867;&#22411;&#23398;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#19978;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.13401</link><description>&lt;p&gt;
&#19968;&#39033;&#27010;&#24565;&#35821;&#35328;&#30456;&#20284;&#24615;&#30340;&#30740;&#31350;: &#27604;&#36739;&#19982;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A study of conceptual language similarity: comparison and evaluation. (arXiv:2305.13401v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#23450;&#20041;&#35821;&#35328;&#30456;&#20284;&#24230;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#35821;&#35328;&#22914;&#20309;&#34920;&#31034;&#22522;&#26412;&#27010;&#24565;&#65292;&#32780;&#38750;&#24120;&#35268;&#30340;&#35789;&#27719;&#25110;&#31867;&#22411;&#23398;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#19978;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20013;&#30340;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#26088;&#22312;&#34701;&#20837;&#35821;&#35328;&#23398;&#31867;&#22411;&#23398;, &#20197;&#26725;&#25509;&#35821;&#35328;&#22810;&#26679;&#24615;&#24182;&#21327;&#21161;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#30740;&#31350;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#20316;&#21697;&#26159;&#22522;&#20110;&#35789;&#27719;&#25110;&#31867;&#22411;&#23398;&#29305;&#24449;(&#22914;&#35789;&#24207;&#21644;&#21160;&#35789;&#21464;&#21270;)&#26500;&#24314;&#35821;&#35328;&#30456;&#20284;&#24230;&#24230;&#37327;, &#20294;&#26368;&#36817;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;, &#22522;&#20110;&#35821;&#35328;&#22914;&#20309;&#34920;&#31034;&#22522;&#26412;&#27010;&#24565;&#26469;&#23450;&#20041;&#35821;&#35328;&#30456;&#20284;&#24230;, &#36825;&#26159;&#29616;&#26377;&#30456;&#20284;&#24230;&#24230;&#37327;&#30340;&#34917;&#20805;&#12290;&#26412;&#25991;&#35814;&#32454;&#30740;&#31350;&#20102;&#27010;&#24565;&#30456;&#20284;&#24615;, &#24182;&#22312;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
An interesting line of research in natural language processing (NLP) aims to incorporate linguistic typology to bridge linguistic diversity and assist the research of low-resource languages. While most works construct linguistic similarity measures based on lexical or typological features, such as word order and verbal inflection, recent work has introduced a novel approach to defining language similarity based on how they represent basic concepts, which is complementary to existing similarity measures. In this work, we study the conceptual similarity in detail and evaluate it extensively on a binary classification task.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;BioDEX&#65292;&#23427;&#26159;&#19968;&#20010;&#22823;&#22411;&#36164;&#28304;&#65292;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#25552;&#21462;&#65292;&#21487;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#25913;&#36827;&#33647;&#29289;&#23433;&#20840;&#30417;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.13395</link><description>&lt;p&gt;
BioDEX&#65306;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#33647;&#29289;&#30417;&#27979;&#30340;&#22823;&#35268;&#27169;&#29983;&#29289;&#21307;&#23398;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
BioDEX: Large-Scale Biomedical Adverse Drug Event Extraction for Real-World Pharmacovigilance. (arXiv:2305.13395v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13395
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;BioDEX&#65292;&#23427;&#26159;&#19968;&#20010;&#22823;&#22411;&#36164;&#28304;&#65292;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#25552;&#21462;&#65292;&#21487;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#25913;&#36827;&#33647;&#29289;&#23433;&#20840;&#30417;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21450;&#26102;&#20934;&#30830;&#22320;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;(Adverse Drug Events, ADE)&#23545;&#20844;&#20247;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#28041;&#21450;&#32531;&#24930;&#21644;&#26114;&#36149;&#30340;&#20154;&#24037;&#21171;&#21160;&#12290;&#25105;&#20204;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#26469;&#25913;&#36827;&#33647;&#29289;&#23433;&#20840;&#30417;&#27979;(&#33647;&#29289;&#30417;&#31649;&#23398;, PV)&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;BioDEX&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#22411;&#36164;&#28304;&#65292;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#25552;&#21462;&#65292;&#22522;&#20110;&#32654;&#22269;&#33647;&#29289;&#23433;&#20840;&#25253;&#21578;&#30340;&#21382;&#21490;&#36755;&#20986;&#12290;BioDEX&#21253;&#25324;65k&#20010;&#25688;&#35201;&#21644;19k&#20010;&#20840;&#25991;&#29983;&#29289;&#21307;&#23398;&#35770;&#25991;&#65292;&#20197;&#21450;&#30001;&#21307;&#23398;&#19987;&#23478;&#21019;&#24314;&#30340;256k&#20010;&#30456;&#20851;&#30340;&#25991;&#26723;&#32423;&#23433;&#20840;&#25253;&#21578;&#12290;&#36825;&#20123;&#25253;&#21578;&#30340;&#26680;&#24515;&#29305;&#24449;&#21253;&#25324;&#24739;&#32773;&#30340;&#20307;&#37325;&#12289;&#24180;&#40836;&#21644;&#29983;&#29289;&#24615;&#21035;&#65292;&#24739;&#32773;&#26381;&#29992;&#30340;&#19968;&#32452;&#33647;&#29289;&#12289;&#33647;&#29289;&#21058;&#37327;&#12289;&#32463;&#21382;&#30340;&#21453;&#24212;&#20197;&#21450;&#21453;&#24212;&#26159;&#21542;&#21361;&#21450;&#29983;&#21629;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#26681;&#25454;&#36215;&#22987;&#35770;&#25991;&#39044;&#27979;&#25253;&#21578;&#30340;&#26680;&#24515;&#20449;&#24687;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#20272;&#35745;&#20154;&#31867;&#30340;&#34920;&#29616;&#20026;72.0% F1&#65292;&#32780;&#25105;&#20204;&#26368;&#22909;&#30340;m.....
&lt;/p&gt;
&lt;p&gt;
Timely and accurate extraction of Adverse Drug Events (ADE) from biomedical literature is paramount for public safety, but involves slow and costly manual labor. We set out to improve drug safety monitoring (pharmacovigilance, PV) through the use of Natural Language Processing (NLP). We introduce BioDEX, a large-scale resource for Biomedical adverse Drug Event Extraction, rooted in the historical output of drug safety reporting in the U.S. BioDEX consists of 65k abstracts and 19k full-text biomedical papers with 256k associated document-level safety reports created by medical experts. The core features of these reports include the reported weight, age, and biological sex of a patient, a set of drugs taken by the patient, the drug dosages, the reactions experienced, and whether the reaction was life threatening. In this work, we consider the task of predicting the core information of the report given its originating paper. We estimate human performance to be 72.0% F1, whereas our best m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21548;&#35273;&#21333;&#35789;&#35782;&#21035;&#21644;&#25972;&#21512;&#30340;&#31070;&#32463;&#21160;&#24577;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#35299;&#37322;&#20102;&#36825;&#19968;&#36807;&#31243;&#65292;&#21457;&#29616;&#23545;&#20110;&#38656;&#35201;&#36229;&#36807;&#22823;&#32422;100ms&#30340;&#36755;&#20837;&#25165;&#33021;&#34987;&#35782;&#21035;&#30340;&#21333;&#35789;&#65292;&#31070;&#32463;&#21709;&#24212;&#20250;&#34987;&#25918;&#22823;&#12290;</title><link>http://arxiv.org/abs/2305.13388</link><description>&lt;p&gt;
&#21548;&#35273;&#21333;&#35789;&#35782;&#21035;&#21644;&#25972;&#21512;&#30340;&#31070;&#32463;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
The neural dynamics of auditory word recognition and integration. (arXiv:2305.13388v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13388
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21548;&#35273;&#21333;&#35789;&#35782;&#21035;&#21644;&#25972;&#21512;&#30340;&#31070;&#32463;&#21160;&#24577;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#35299;&#37322;&#20102;&#36825;&#19968;&#36807;&#31243;&#65292;&#21457;&#29616;&#23545;&#20110;&#38656;&#35201;&#36229;&#36807;&#22823;&#32422;100ms&#30340;&#36755;&#20837;&#25165;&#33021;&#34987;&#35782;&#21035;&#30340;&#21333;&#35789;&#65292;&#31070;&#32463;&#21709;&#24212;&#20250;&#34987;&#25918;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21548;&#32773;&#36890;&#36807;&#23558;&#26377;&#20851;&#21363;&#23558;&#20986;&#29616;&#30340;&#20869;&#23481;&#30340;&#26399;&#26395;&#19982;&#22686;&#37327;&#24863;&#30693;&#35777;&#25454;&#30456;&#32467;&#21512;&#65292;&#26469;&#24555;&#36895;&#35782;&#21035;&#21644;&#25972;&#21512;&#22024;&#26434;&#30340;&#26085;&#24120;&#35821;&#38899;&#20013;&#30340;&#21333;&#35789;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21333;&#35789;&#35782;&#21035;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#36125;&#21494;&#26031;&#20915;&#31574;&#29702;&#35770;&#20013;&#24418;&#24335;&#21270;&#20102;&#36825;&#19968;&#30693;&#35273;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#35813;&#27169;&#22411;&#25311;&#21512;&#21040;&#20316;&#20026;&#34987;&#35797;&#32773;&#34987;&#21160;&#21548;&#21462;&#34394;&#26500;&#25925;&#20107;&#26102;&#35760;&#24405;&#30340;&#22836;&#30382;&#33041;&#30005;&#20449;&#21495;&#20013;&#65292;&#25581;&#31034;&#20102;&#22312;&#32447;&#21548;&#35273;&#21333;&#35789;&#35782;&#21035;&#36807;&#31243;&#21644;&#21333;&#35789;&#35782;&#21035;&#21644;&#25972;&#21512;&#30340;&#31070;&#32463;&#30456;&#20851;&#24615;&#30340;&#21160;&#21147;&#23398;&#12290;&#35813;&#27169;&#22411;&#25581;&#31034;&#20102;&#21333;&#35789;&#30340;&#19981;&#21516;&#31070;&#32463;&#22788;&#29702;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#24555;&#36895;&#35782;&#21035;&#12290;&#34429;&#28982;&#25152;&#26377;&#21333;&#35789;&#37117;&#35302;&#21457;&#27010;&#29575;&#25972;&#21512;&#30340;&#31070;&#32463;&#21709;&#24212;&#65292;&#21363;&#25991;&#26412;&#32972;&#26223;&#20013;&#23545;&#21333;&#35789;&#24778;&#24322;&#24230;&#39044;&#27979;&#30340;&#30005;&#21387;&#35843;&#21046;&#65292;&#20294;&#23545;&#20110;&#38656;&#35201;&#36229;&#36807;&#22823;&#32422;100ms&#30340;&#36755;&#20837;&#25165;&#33021;&#34987;&#35782;&#21035;&#30340;&#21333;&#35789;&#65292;&#36825;&#20123;&#35843;&#21046;&#20250;&#34987;&#25918;&#22823;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#31070;&#32463;&#21709;&#24212;&#30340;&#28508;&#20239;&#26399;&#19981;&#20250;&#26681;&#25454;&#21333;&#35789;&#38271;&#24230;&#32780;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Listeners recognize and integrate words in rapid and noisy everyday speech by combining expectations about upcoming content with incremental sensory evidence. We present a computational model of word recognition which formalizes this perceptual process in Bayesian decision theory. We fit this model to explain scalp EEG signals recorded as subjects passively listened to a fictional story, revealing both the dynamics of the online auditory word recognition process and the neural correlates of the recognition and integration of words.  The model reveals distinct neural processing of words depending on whether or not they can be quickly recognized. While all words trigger a neural response characteristic of probabilistic integration -- voltage modulations predicted by a word's surprisal in context -- these modulations are amplified for words which require more than roughly 100 ms of input to be recognized. We observe no difference in the latency of these neural responses according to words
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#20316;&#20026;&#27880;&#37322;&#22120;&#20197;&#20415;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#35299;&#37322;&#20998;&#26512;&#65292;&#21457;&#29616;ChatGPT&#20135;&#29983;&#20102;&#26356;&#20934;&#30830;&#21644;&#35821;&#20041;&#26356;&#20016;&#23500;&#30340;&#27880;&#37322;&#12290;&#21516;&#26102;&#65292;&#22522;&#20110;GPT&#27880;&#37322;&#30340;&#35299;&#37322;&#20998;&#26512;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#36827;&#19968;&#27493;&#25506;&#32034;&#21644;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.13386</link><description>&lt;p&gt;
LLMs&#26159;&#21542;&#21487;&#20197;&#20419;&#36827;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can LLMs facilitate interpretation of pre-trained language models?. (arXiv:2305.13386v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13386
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#20316;&#20026;&#27880;&#37322;&#22120;&#20197;&#20415;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#35299;&#37322;&#20998;&#26512;&#65292;&#21457;&#29616;ChatGPT&#20135;&#29983;&#20102;&#26356;&#20934;&#30830;&#21644;&#35821;&#20041;&#26356;&#20016;&#23500;&#30340;&#27880;&#37322;&#12290;&#21516;&#26102;&#65292;&#22522;&#20110;GPT&#27880;&#37322;&#30340;&#35299;&#37322;&#20998;&#26512;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#36827;&#19968;&#27493;&#25506;&#32034;&#21644;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#30693;&#35782;&#30340;&#24037;&#20316;&#20381;&#36182;&#20110;&#24102;&#27880;&#37322;&#30340;&#35821;&#26009;&#24211;&#25110;&#20154;&#22312;&#29615;&#36335;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#21487;&#20280;&#32553;&#24615;&#21644;&#35299;&#37322;&#33539;&#22260;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#35758;&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#20316;&#20026;&#27880;&#37322;&#22120;&#65292;&#20197;&#20415;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#35299;&#37322;&#20998;&#26512;&#12290;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#34920;&#31034;&#19978;&#24212;&#29992;&#20998;&#23618;&#32858;&#31867;&#65292;&#25105;&#20204;&#21457;&#29616;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#27010;&#24565;&#65292;&#28982;&#21518;&#20351;&#29992;GPT&#27880;&#37322;&#23545;&#36825;&#20123;&#27010;&#24565;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#20154;&#24037;&#27880;&#37322;&#30340;&#27010;&#24565;&#30456;&#27604;&#65292;ChatGPT&#20135;&#29983;&#20102;&#26356;&#20934;&#30830;&#21644;&#35821;&#20041;&#26356;&#20016;&#23500;&#30340;&#27880;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;GPT&#27880;&#37322;&#30340;&#35299;&#37322;&#20998;&#26512;&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#23637;&#31034;&#20102;&#20004;&#31181;&#65306;&#25506;&#38024;&#26694;&#26550;&#21644;&#31070;&#32463;&#20803;&#35299;&#37322;&#12290;&#20026;&#20102;&#20419;&#36827;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#25506;&#32034;&#21644;&#23454;&#39564;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#27010;&#24565;&#32593;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Work done to uncover the knowledge encoded within pre-trained language models, rely on annotated corpora or human-in-the-loop methods. However, these approaches are limited in terms of scalability and the scope of interpretation. We propose using a large language model, ChatGPT, as an annotator to enable fine-grained interpretation analysis of pre-trained language models. We discover latent concepts within pre-trained language models by applying hierarchical clustering over contextualized representations and then annotate these concepts using GPT annotations. Our findings demonstrate that ChatGPT produces accurate and semantically richer annotations compared to human-annotated concepts. Additionally, we showcase how GPT-based annotations empower interpretation analysis methodologies of which we demonstrate two: probing framework and neuron interpretation. To facilitate further exploration and experimentation in this field, we have made available a substantial ConceptNet dataset compris
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#20027;&#21160;&#23398;&#20064;&#27169;&#25311;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#35686;&#21578;&#22522;&#20110;&#27169;&#25311;&#23454;&#39564;&#32467;&#26524;&#24471;&#20986;&#24378;&#28872;&#32467;&#35770;&#21487;&#33021;&#23548;&#33268;&#35780;&#20272;AL&#31639;&#27861;&#30340;&#35823;&#23548;&#12290;</title><link>http://arxiv.org/abs/2305.13342</link><description>&lt;p&gt;
&#35770;&#27169;&#25311;&#20027;&#21160;&#23398;&#20064;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
On the Limitations of Simulating Active Learning. (arXiv:2305.13342v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13342
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#20027;&#21160;&#23398;&#20064;&#27169;&#25311;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#35686;&#21578;&#22522;&#20110;&#27169;&#25311;&#23454;&#39564;&#32467;&#26524;&#24471;&#20986;&#24378;&#28872;&#32467;&#35770;&#21487;&#33021;&#23548;&#33268;&#35780;&#20272;AL&#31639;&#27861;&#30340;&#35823;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#26159;&#19968;&#31181;&#20154;&#19982;&#27169;&#22411;&#20132;&#20114;&#24490;&#29615;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#36845;&#20195;&#22320;&#36873;&#25321;&#20449;&#24687;&#24615;&#26410;&#26631;&#35760;&#25968;&#25454;&#20197;&#20379;&#20154;&#31867;&#27880;&#37322;&#65292;&#26088;&#22312;&#25913;&#21892;&#38543;&#26426;&#25277;&#26679;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;&#27969;&#31243;&#20013;&#23454;&#26102;&#36827;&#34892;&#24102;&#20154;&#31867;&#27880;&#37322;&#30340;AL&#23454;&#39564;&#26159;&#19968;&#39033;&#32321;&#29712;&#32780;&#26114;&#36149;&#30340;&#36807;&#31243;&#65292;&#22240;&#27492;&#22312;&#23398;&#26415;&#30740;&#31350;&#20013;&#19981;&#20999;&#23454;&#38469;&#12290;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#31616;&#21333;&#26041;&#27861;&#26159;&#36890;&#36807;&#23558;&#24050;&#26631;&#35760;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#20316;&#20026;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#27744;&#26469;&#27169;&#25311;AL&#12290;&#22312;&#36825;&#31687;&#35266;&#28857;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35843;&#26597;&#26368;&#36817;&#30340;&#25991;&#29486;&#24182;&#31361;&#20986;&#26174;&#31034;AL&#24490;&#29615;&#20013;&#25152;&#26377;&#19981;&#21516;&#27493;&#39588;&#20013;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#23454;&#39564;&#35774;&#32622;&#20013;&#34987;&#24573;&#35270;&#30340;&#27880;&#24847;&#20107;&#39033;&#65292;&#36825;&#20123;&#27880;&#24847;&#20107;&#39033;&#21487;&#33021;&#20250;&#26174;&#30528;&#24433;&#21709;AL&#30740;&#31350;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#25509;&#30528;&#25506;&#35752;&#20102;&#27169;&#25311;&#35774;&#32622;&#22914;&#20309;&#25903;&#37197;&#32463;&#39564;&#21457;&#29616;&#65292;&#35748;&#20026;&#36825;&#21487;&#33021;&#26159;&#8220;&#20026;&#20160;&#20040;&#26377;&#26102;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#26080;&#27861;&#32988;&#36807;&#38543;&#26426;&#25277;&#26679;&#8221;&#30340;&#26435;&#34913;&#20043;&#19968;&#12290;&#25105;&#20204;&#35748;&#20026;&#20165;&#22522;&#20110;&#27169;&#25311;&#23454;&#39564;&#32467;&#26524;&#24471;&#20986;&#24378;&#28872;&#32467;&#35770;&#21487;&#20197;&#23548;&#33268;&#35780;&#20272;AL&#31639;&#27861;&#30340;&#35823;&#23548;&#65292;&#22240;&#27492;&#25552;&#20986;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning (AL) is a human-and-model-in-the-loop paradigm that iteratively selects informative unlabeled data for human annotation, aiming to improve over random sampling. However, performing AL experiments with human annotations on-the-fly is a laborious and expensive process, thus unrealistic for academic research. An easy fix to this impediment is to simulate AL, by treating an already labeled and publicly available dataset as the pool of unlabeled data. In this position paper, we first survey recent literature and highlight the challenges across all different steps within the AL loop. We further unveil neglected caveats in the experimental setup that can significantly affect the quality of AL research. We continue with an exploration of how the simulation setting can govern empirical findings, arguing that it might be one of the answers behind the ever posed question ``why do active learning algorithms sometimes fail to outperform random sampling?''. We argue that evaluating A
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#23545;&#22522;&#22240;&#38598;&#36827;&#34892;&#20989;&#25968;&#27010;&#25324;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;SPINDOCTOR&#65292;&#21487;&#20197;&#25552;&#20379;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13338</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#22240;&#38598;&#27010;&#25324;
&lt;/p&gt;
&lt;p&gt;
Gene Set Summarization using Large Language Models. (arXiv:2305.13338v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#23545;&#22522;&#22240;&#38598;&#36827;&#34892;&#20989;&#25968;&#27010;&#25324;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;SPINDOCTOR&#65292;&#21487;&#20197;&#25552;&#20379;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#29983;&#29289;&#23398;&#23478;&#32463;&#24120;&#35299;&#37322;&#20174;&#39640;&#36890;&#37327;&#23454;&#39564;&#21644;&#35745;&#31639;&#20998;&#26512;&#20013;&#33719;&#24471;&#30340;&#22522;&#22240;&#21015;&#34920;&#12290;&#36825;&#36890;&#24120;&#26159;&#36890;&#36807;&#32479;&#35745;&#23500;&#38598;&#20998;&#26512;&#26469;&#23436;&#25104;&#30340;&#65292;&#35813;&#20998;&#26512;&#27979;&#37327;&#19982;&#22522;&#22240;&#25110;&#20854;&#23646;&#24615;&#30456;&#20851;&#30340;&#29983;&#29289;&#21151;&#33021;&#26415;&#35821;&#30340;&#36807;&#24230;&#25110;&#27424;&#34920;&#31034;&#31243;&#24230;&#65292;&#22522;&#20110;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#65288;&#20363;&#22914;Gene Ontology&#65288;GO&#65289;&#65289;&#20013;&#30340;&#32534;&#35793;&#26029;&#35328;&#12290;&#35299;&#37322;&#22522;&#22240;&#21015;&#34920;&#20063;&#21487;&#20197;&#34987;&#26500;&#24314;&#20026;&#19968;&#20010;&#25991;&#26412;&#27010;&#25324;&#20219;&#21153;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#65292;&#21487;&#33021;&#30452;&#25509;&#21033;&#29992;&#31185;&#23398;&#25991;&#26412;&#24182;&#36991;&#20813;&#20381;&#36182;KB&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;SPINDOCTOR&#65288;&#31283;&#23450;&#30340;&#25552;&#31034;&#25554;&#20540;&#30340;&#21463;&#25511;&#26415;&#35821;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#32467;&#26500;&#21270;&#25253;&#21578;&#27169;&#26495;&#65289;&#65292;&#19968;&#31181;&#20351;&#29992;GPT&#27169;&#22411;&#25191;&#34892;&#22522;&#22240;&#38598;&#20989;&#25968;&#27010;&#25324;&#30340;&#26041;&#27861;&#65292;&#20316;&#20026;&#26631;&#20934;&#23500;&#38598;&#20998;&#26512;&#30340;&#34917;&#20805;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#22522;&#22240;&#21151;&#33021;&#20449;&#24687;&#26469;&#28304;&#65306;&#65288;1&#65289;&#20174;&#37492;&#23450;&#30340;&#26412;&#20307;KB&#27880;&#37322;&#20013;&#33719;&#24471;&#30340;&#32467;&#26500;&#21270;&#25991;&#26412;&#65292;&#65288;2&#65289;&#20174;&#25991;&#26412;&#25366;&#25496;&#20013;&#25512;&#26029;&#30340;&#26412;&#20307;&#26415;&#35821;&#65292;&#20197;&#21450;&#65288;3&#65289;&#30452;&#25509;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#33719;&#24471;&#30340;&#26415;&#35821;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;1813&#20010;&#22522;&#22240;&#38598;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;SPINDOCTOR&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;GPT&#27169;&#22411;&#26174;&#33879;&#25913;&#21892;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#29983;&#25104;&#20154;&#31867;&#21487;&#35835;&#30340;&#22522;&#22240;&#21151;&#33021;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular biologists frequently interpret gene lists derived from high-throughput experiments and computational analysis. This is typically done as a statistical enrichment analysis that measures the over- or under-representation of biological function terms associated with genes or their properties, based on curated assertions from a knowledge base (KB) such as the Gene Ontology (GO). Interpreting gene lists can also be framed as a textual summarization task, enabling the use of Large Language Models (LLMs), potentially utilizing scientific texts directly and avoiding reliance on a KB.  We developed SPINDOCTOR (Structured Prompt Interpolation of Natural Language Descriptions of Controlled Terms for Ontology Reporting), a method that uses GPT models to perform gene set function summarization as a complement to standard enrichment analysis. This method can use different sources of gene functional information: (1) structured text derived from curated ontological KB annotations, (2) ontol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;&#21644;AphsiaBank&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#22833;&#35821;&#30151;&#35821;&#38899;&#35782;&#21035;&#21644;&#26816;&#27979;&#30340;&#26032;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#35828;&#35805;&#20154;&#32423;&#21035;&#26816;&#27979;&#20934;&#30830;&#29575;(97.3%)&#65292;&#24182;&#30456;&#23545;&#20110;&#20013;&#24230;&#22833;&#35821;&#30151;&#24739;&#32773;&#30340;WER&#38477;&#20302;&#20102;11%&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21487;&#24212;&#29992;&#20110;&#20854;&#20182;&#35821;&#38899;&#25968;&#25454;&#24211;&#65292;&#20419;&#36827;&#22833;&#35821;&#30151;&#35821;&#38899;&#22788;&#29702;&#30340;&#36827;&#27493;&#12290;</title><link>http://arxiv.org/abs/2305.13331</link><description>&lt;p&gt;
&#22522;&#20110;E-Branchformer&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#22833;&#35821;&#30151;&#35821;&#38899;&#35782;&#21035;&#21644;&#26816;&#27979;&#26032;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A New Benchmark of Aphasia Speech Recognition and Detection Based on E-Branchformer and Multi-task Learning. (arXiv:2305.13331v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;&#21644;AphsiaBank&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#22833;&#35821;&#30151;&#35821;&#38899;&#35782;&#21035;&#21644;&#26816;&#27979;&#30340;&#26032;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#35828;&#35805;&#20154;&#32423;&#21035;&#26816;&#27979;&#20934;&#30830;&#29575;(97.3%)&#65292;&#24182;&#30456;&#23545;&#20110;&#20013;&#24230;&#22833;&#35821;&#30151;&#24739;&#32773;&#30340;WER&#38477;&#20302;&#20102;11%&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21487;&#24212;&#29992;&#20110;&#20854;&#20182;&#35821;&#38899;&#25968;&#25454;&#24211;&#65292;&#20419;&#36827;&#22833;&#35821;&#30151;&#35821;&#38899;&#22788;&#29702;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22833;&#35821;&#30151;&#26159;&#19968;&#31181;&#24433;&#21709;&#25968;&#20197;&#30334;&#19975;&#35745;&#24739;&#32773;&#35762;&#35805;&#33021;&#21147;&#30340;&#35821;&#35328;&#38556;&#30861;&#12290;&#26412;&#25991;&#21033;&#29992;AphsiaBank&#25968;&#25454;&#38598;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;&#25552;&#20986;&#20102;&#19968;&#31181;&#22833;&#35821;&#30151;&#35821;&#38899;&#35782;&#21035;&#21644;&#26816;&#27979;&#20219;&#21153;&#30340;&#26032;&#22522;&#20934;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#22522;&#20110;CTC/Attention&#26550;&#26500;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#21516;&#26102;&#25191;&#34892;&#20004;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#35828;&#35805;&#20154;&#32423;&#21035;&#26816;&#27979;&#20934;&#30830;&#29575;(97.3%)&#65292;&#24182;&#19988;&#30456;&#23545;WER&#38477;&#20302;&#20102;11%&#30340;&#20013;&#24230;&#22833;&#35821;&#30151;&#24739;&#32773;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#21478;&#19968;&#20010;&#26377;&#35821;&#35328;&#38556;&#30861;&#30340;&#35821;&#38899;&#25968;&#25454;&#24211;DementiaBank Pitt corpus&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#36890;&#29992;&#24615;&#12290;&#25105;&#20204;&#23558;&#20844;&#24320;&#21457;&#24067;&#25105;&#20204;&#30340;&#37197;&#26041;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#20415;&#20110;&#23398;&#26415;&#35770;&#25991;&#30340;&#22797;&#29616;&#12290;&#25105;&#20204;&#30340;&#26631;&#20934;&#21270;&#25968;&#25454;&#39044;&#22788;&#29702;&#27969;&#31243;&#21644;&#24320;&#28304;&#37197;&#26041;&#21487;&#20351;&#30740;&#31350;&#32773;&#30452;&#25509;&#27604;&#36739;&#32467;&#26524;&#65292;&#25512;&#21160;&#22833;&#35821;&#30151;&#35821;&#38899;&#22788;&#29702;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aphasia is a language disorder that affects the speaking ability of millions of patients. This paper presents a new benchmark for Aphasia speech recognition and detection tasks using state-of-the-art speech recognition techniques with the AphsiaBank dataset. Specifically, we introduce two multi-task learning methods based on the CTC/Attention architecture to perform both tasks simultaneously. Our system achieves state-of-the-art speaker-level detection accuracy (97.3%), and a relative WER reduction of 11% for moderate Aphasia patients. In addition, we demonstrate the generalizability of our approach by applying it to another disordered speech database, the DementiaBank Pitt corpus. We will make our all-in-one recipes and pre-trained model publicly available to facilitate reproducibility. Our standardized data preprocessing pipeline and open-source recipes enable researchers to compare results directly, promoting progress in disordered speech processing.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#35821;&#35328;&#20266;&#26631;&#27880;&#30340;&#26080;&#30417;&#30563;ASR&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#26631;&#27880;&#25968;&#25454;&#26469;&#24341;&#23548;&#26032;&#35821;&#35328;&#30340;&#26080;&#30417;&#30563;AM&#12290;&#22312;Common Voice&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#21487;&#20197;&#23454;&#29616;18% WER&#12290;&#32780;&#19988;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13330</link><description>&lt;p&gt;
&#22522;&#20110;&#36328;&#35821;&#35328;&#20266;&#26631;&#27880;&#30340;&#26080;&#30417;&#30563;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Unsupervised ASR via Cross-Lingual Pseudo-Labeling. (arXiv:2305.13330v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#35821;&#35328;&#20266;&#26631;&#27880;&#30340;&#26080;&#30417;&#30563;ASR&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#26631;&#27880;&#25968;&#25454;&#26469;&#24341;&#23548;&#26032;&#35821;&#35328;&#30340;&#26080;&#30417;&#30563;AM&#12290;&#22312;Common Voice&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#21487;&#20197;&#23454;&#29616;18% WER&#12290;&#32780;&#19988;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#38750;&#37197;&#23545;&#30340;&#38899;&#39057;&#21644;&#25991;&#26412;&#26469;&#35757;&#32451;&#26080;&#30417;&#30563;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#12290;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;ASR&#26041;&#27861;&#20551;&#23450;&#19981;&#33021;&#20351;&#29992;&#20219;&#20309;&#26631;&#27880;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#21363;&#20351;&#27809;&#26377;&#32473;&#23450;&#35821;&#35328;&#30340;&#20219;&#20309;&#26631;&#27880;&#38899;&#39057;&#65292;&#20063;&#22987;&#32456;&#21487;&#20197;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#26631;&#27880;&#25968;&#25454;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;&#30340;&#23383;&#31526;&#32423;&#22768;&#23398;&#27169;&#22411;&#65288;AM&#65289;&#65292;&#26469;&#24341;&#23548;&#26032;&#35821;&#35328;&#30340;&#26080;&#30417;&#30563;AM&#12290; &#36825;&#37324;&#65292;&#8220;&#26080;&#30417;&#30563;&#8221;&#24847;&#21619;&#30528;&#27809;&#26377;&#21487;&#29992;&#20110;&#30446;&#26631;&#35821;&#35328;&#30340;&#26631;&#27880;&#38899;&#39057;&#12290;&#26412;&#25991;&#30340;&#26041;&#27861;&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#65288;i&#65289;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;AM&#29983;&#25104;&#8220;&#30446;&#26631;&#8221;&#35821;&#35328;&#30340;&#20266;&#26631;&#31614;&#65288;PLs&#65289;&#65307;&#65288;ii&#65289;&#20351;&#29992;&#8220;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#8221;&#38480;&#21046;&#36825;&#20123;PLs&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Common Voice&#19978;&#38750;&#24120;&#26377;&#25928;&#65306;&#20363;&#22914;&#65292;&#23558;&#33521;&#35821;AM&#20256;&#36882;&#21040;&#26031;&#29926;&#24076;&#37324;&#35821;&#21487;&#20197;&#23454;&#29616;18&#65285;&#30340;WER&#12290; &#23427;&#36824;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#22522;&#20110;&#23383;&#31526;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that it is possible to train an $\textit{unsupervised}$ automatic speech recognition (ASR) system using only unpaired audio and text. Existing unsupervised ASR methods assume that no labeled data can be used for training. We argue that even if one does not have any labeled audio for a given language, there is $\textit{always}$ labeled data available for other languages. We show that it is possible to use character-level acoustic models (AMs) from other languages to bootstrap an $\textit{unsupervised}$ AM in a new language. Here, "unsupervised" means no labeled audio is available for the $\textit{target}$ language. Our approach is based on two key ingredients: (i) generating pseudo-labels (PLs) of the $\textit{target}$ language using some $\textit{other}$ language AM and (ii) constraining these PLs with a $\textit{target language model}$. Our approach is effective on Common Voice: e.g. transfer of English AM to Swahili achieves 18% WER. It also outperforms characte
&lt;/p&gt;</description></item><item><title>&#36825;&#20221;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#26377;&#20851;&#30149;&#27602;&#19982;&#23487;&#20027;&#30456;&#20114;&#20316;&#29992;&#30340;&#23454;&#20307;&#65292;&#25552;&#20986;&#35813;&#25968;&#25454;&#38598;&#21487;&#20316;&#20026;&#26410;&#26469;&#27169;&#22411;&#35757;&#32451;&#30340;&#40644;&#37329;&#26631;&#20934;&#12290;&#35813;&#24037;&#20316;&#20026;&#33258;&#21160;&#20174;&#31185;&#23398;&#20986;&#29256;&#29289;&#20013;&#25552;&#21462;&#19982;&#20027;&#26426;-&#30149;&#21407;&#20307;&#26816;&#27979;&#26041;&#27861;&#30456;&#20851;&#30340;&#20449;&#24687;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#65292;&#24182;&#33258;&#21160;&#39044;&#27979;&#20102;&#19982;&#20154;&#31867;&#20581;&#24247;&#30456;&#20851;&#30340;&#37325;&#35201;&#27010;&#24565;&#8220;&#30149;&#27602;&#36328;&#30028;&#20256;&#25773;&#39118;&#38505;&#8221;&#12290;</title><link>http://arxiv.org/abs/2305.13317</link><description>&lt;p&gt;
&#19968;&#20221;&#29992;&#20110;&#25552;&#21462;&#30149;&#27602;&#21644;&#23487;&#20027;&#30456;&#20114;&#20316;&#29992;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Novel Dataset Towards Extracting Virus-Host Interactions. (arXiv:2305.13317v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13317
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#26377;&#20851;&#30149;&#27602;&#19982;&#23487;&#20027;&#30456;&#20114;&#20316;&#29992;&#30340;&#23454;&#20307;&#65292;&#25552;&#20986;&#35813;&#25968;&#25454;&#38598;&#21487;&#20316;&#20026;&#26410;&#26469;&#27169;&#22411;&#35757;&#32451;&#30340;&#40644;&#37329;&#26631;&#20934;&#12290;&#35813;&#24037;&#20316;&#20026;&#33258;&#21160;&#20174;&#31185;&#23398;&#20986;&#29256;&#29289;&#20013;&#25552;&#21462;&#19982;&#20027;&#26426;-&#30149;&#21407;&#20307;&#26816;&#27979;&#26041;&#27861;&#30456;&#20851;&#30340;&#20449;&#24687;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#65292;&#24182;&#33258;&#21160;&#39044;&#27979;&#20102;&#19982;&#20154;&#31867;&#20581;&#24247;&#30456;&#20851;&#30340;&#37325;&#35201;&#27010;&#24565;&#8220;&#30149;&#27602;&#36328;&#30028;&#20256;&#25773;&#39118;&#38505;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#19982;&#30149;&#27602;&#19982;&#23487;&#20027;&#30456;&#20851;&#30340;&#21629;&#21517;&#20998;&#31867;&#21644;&#20854;&#20182;&#23454;&#20307;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25551;&#36848;&#20102;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#35813;&#26032;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20219;&#21153;&#30340;&#19968;&#20123;&#21021;&#27493;&#32467;&#26524;&#12290;&#25105;&#20204;&#24314;&#35758;&#25105;&#20204;&#25163;&#21160;&#27880;&#37322;&#30340;&#25688;&#35201;&#25968;&#25454;&#38598;&#29616;&#22312;&#20026;&#26410;&#26469;&#30340;NER&#27169;&#22411;&#22521;&#35757;&#25552;&#20379;&#20102;&#40644;&#37329;&#26631;&#20934;&#12290;&#33258;&#21160;&#20174;&#31185;&#23398;&#20986;&#29256;&#29289;&#20013;&#25552;&#21462;&#20027;&#26426;-&#30149;&#21407;&#20307;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#36827;&#19968;&#27493;&#35299;&#37322;&#20102;&#25105;&#20204;&#30340;&#24037;&#20316;&#22914;&#20309;&#36808;&#20986;&#20102;&#33258;&#21160;&#39044;&#27979;&#19982;&#20154;&#31867;&#20581;&#24247;&#30456;&#20851;&#30340;&#8220;&#30149;&#27602;&#36328;&#30028;&#20256;&#25773;&#39118;&#38505;&#8221;&#36825;&#19968;&#37325;&#35201;&#27010;&#24565;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe a novel dataset for the automated recognition of named taxonomic and other entities relevant to the association of viruses with their hosts. We further describe some initial results using pre-trained models on the named-entity recognition (NER) task on this novel dataset. We propose that our dataset of manually annotated abstracts now offers a Gold Standard Corpus for training future NER models in the automated extraction of host-pathogen detection methods from scientific publications, and further explain how our work makes first steps towards predicting the important human health-related concept of viral spillover risk automatically from the scientific literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#36328;11&#31181;&#35821;&#35328;&#32423;&#21035;&#19978;ChatGPT&#27169;&#22411;&#22312;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#20013;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#22797;&#26434;&#30340;&#25925;&#38556;&#65292;&#24182;&#25351;&#20986;&#29983;&#25104;&#27169;&#22411;&#22312;&#26576;&#20123;&#31867;&#22411;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#20026;&#26410;&#26469;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31995;&#32479;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.13276</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#23545;&#22810;&#35821;&#35328;&#21644;&#22522;&#20110;&#34920;&#24773;&#31526;&#21495;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Evaluating ChatGPT's Performance for Multilingual and Emoji-based Hate Speech Detection. (arXiv:2305.13276v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#36328;11&#31181;&#35821;&#35328;&#32423;&#21035;&#19978;ChatGPT&#27169;&#22411;&#22312;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#20013;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#22797;&#26434;&#30340;&#25925;&#38556;&#65292;&#24182;&#25351;&#20986;&#29983;&#25104;&#27169;&#22411;&#22312;&#26576;&#20123;&#31867;&#22411;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#20026;&#26410;&#26469;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31995;&#32479;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20167;&#24680;&#35328;&#35770;&#26159;&#24433;&#21709;&#35768;&#22810;&#22312;&#32447;&#24179;&#21488;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#24050;&#36827;&#34892;&#20102;&#22810;&#39033;&#30740;&#31350;&#20197;&#24320;&#21457;&#24378;&#22823;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31995;&#32479;&#12290;&#36817;&#26469;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#25191;&#34892;&#22810;&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#20197;&#24314;&#31435;&#24378;&#22823;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31995;&#32479;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;ChatGPT&#27169;&#22411;&#22312;&#36328;11&#31181;&#35821;&#35328;&#30340;&#31890;&#24230;&#32423;&#21035;&#19978;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#37319;&#29992;&#19968;&#31995;&#21015;&#21151;&#33021;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#21508;&#31181;&#22797;&#26434;&#25925;&#38556;&#65292;&#32780;&#32858;&#21512;&#25351;&#26631;&#22914;&#23439;F1&#25110;&#20934;&#30830;&#24615;&#21017;&#26080;&#27861;&#23637;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#21253;&#25324;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#22312;&#20869;&#30340;&#22797;&#26434;&#24773;&#24863;&#23545;ChatGPT&#27169;&#22411;&#34920;&#29616;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#31361;&#20986;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#26816;&#27979;&#26576;&#20123;&#31867;&#22411;&#30340;&#20167;&#24680;&#35328;&#35770;&#26041;&#38754;&#30340;&#32570;&#28857;&#65292;&#24182;&#20026;&#26410;&#26469;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31995;&#32479;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hate speech is a severe issue that affects many online platforms. So far, several studies have been performed to develop robust hate speech detection systems. Large language models like ChatGPT have recently shown a great promise in performing several tasks, including hate speech detection. However, it is crucial to comprehend the limitations of these models to build robust hate speech detection systems. To bridge this gap, our study aims to evaluate the strengths and weaknesses of the ChatGPT model in detecting hate speech at a granular level across 11 languages. Our evaluation employs a series of functionality tests that reveals various intricate failures of the model which the aggregate metrics like macro F1 or accuracy are not able to unfold. In addition, we investigate the influence of complex emotions, such as the use of emojis in hate speech, on the performance of the ChatGPT model. Our analysis highlights the shortcomings of the generative models in detecting certain types of h
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;SparseFit&#65292;&#19968;&#31181;&#23569;&#26679;&#26412;&#21050;&#28608;&#30340;&#31232;&#30095;&#24494;&#35843;&#31574;&#30053;&#65292;&#29992;&#20110;&#32852;&#21512;&#29983;&#25104;&#39044;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#21482;&#26377;&#23569;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#21487;&#29992;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2305.13235</link><description>&lt;p&gt;
SPARSEFIT&#65306;&#23569;&#26679;&#26412;&#21050;&#28608;&#30340;&#31232;&#30095;&#24494;&#35843;&#65292;&#32852;&#21512;&#29983;&#25104;&#39044;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
SPARSEFIT: Few-shot Prompting with Sparse Fine-tuning for Jointly Generating Predictions and Natural Language Explanations. (arXiv:2305.13235v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13235
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;SparseFit&#65292;&#19968;&#31181;&#23569;&#26679;&#26412;&#21050;&#28608;&#30340;&#31232;&#30095;&#24494;&#35843;&#31574;&#30053;&#65292;&#29992;&#20110;&#32852;&#21512;&#29983;&#25104;&#39044;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#21482;&#26377;&#23569;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#21487;&#29992;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#31070;&#32463;&#27169;&#22411;&#30340;&#20915;&#31574;&#23545;&#20110;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#22312;&#37096;&#32626;&#26102;&#30340;&#21487;&#20449;&#24230;&#24456;&#20851;&#38190;&#12290;&#26368;&#36817;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#26469;&#35777;&#26126;&#27169;&#22411;&#30340;&#39044;&#27979;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#32534;&#20889;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20316;&#20026;&#30495;&#23454;&#31572;&#26696;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26082;&#26114;&#36149;&#21448;&#21487;&#33021;&#23545;&#20110;&#26576;&#20123;&#24212;&#29992;&#31243;&#24207;&#26469;&#35828;&#19981;&#21487;&#34892;&#12290;&#20026;&#20102;&#20351;&#27169;&#22411;&#22312;&#21482;&#26377;&#23569;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#21487;&#29992;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#22522;&#20110;&#21050;&#28608;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20855;&#26377;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#65292;&#20351;&#24471;&#24494;&#35843;&#21313;&#20998;&#26114;&#36149;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SparseFit&#65292;&#19968;&#31181;&#31232;&#30095;&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;&#31574;&#30053;&#65292;&#21033;&#29992;&#31163;&#25955;&#21050;&#28608;&#26469;&#32852;&#21512;&#29983;&#25104;&#39044;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#25105;&#20204;&#22312;T5&#27169;&#22411;&#21644;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;SparseFit&#65292;&#24182;&#23558;&#20854;&#19982;&#29616;&#26377;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining the decisions of neural models is crucial for ensuring their trustworthiness at deployment time. Using Natural Language Explanations (NLEs) to justify a model's predictions has recently gained increasing interest. However, this approach usually demands large datasets of human-written NLEs for the ground-truth answers, which are expensive and potentially infeasible for some applications. For models to generate high-quality NLEs when only a few NLEs are available, the fine-tuning of Pre-trained Language Models (PLMs) in conjunction with prompt-based learning recently emerged. However, PLMs typically have billions of parameters, making fine-tuning expensive. We propose SparseFit, a sparse few-shot fine-tuning strategy that leverages discrete prompts to jointly generate predictions and NLEs. We experiment with SparseFit on the T5 model and four datasets and compare it against state-of-the-art parameter-efficient fine-tuning techniques. We perform automatic and human evaluations 
&lt;/p&gt;</description></item><item><title>GPT-SW3&#26159;&#38754;&#21521;&#21271;&#27431;&#35821;&#35328;&#30340;&#31532;&#19968;&#20010;&#26412;&#22320;&#21270;&#22823;&#22411;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#20854;&#24320;&#21457;&#36807;&#31243;&#65292;&#21487;&#20316;&#20026;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#38754;&#21521;&#36739;&#23567;&#35821;&#35328;&#30340;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#30340;&#25351;&#21335;&#21644;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2305.12987</link><description>&lt;p&gt;
GPT-SW3&#65306;&#19968;&#31181;&#38754;&#21521;&#21271;&#27431;&#35821;&#35328;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GPT-SW3: An Autoregressive Language Model for the Nordic Languages. (arXiv:2305.12987v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12987
&lt;/p&gt;
&lt;p&gt;
GPT-SW3&#26159;&#38754;&#21521;&#21271;&#27431;&#35821;&#35328;&#30340;&#31532;&#19968;&#20010;&#26412;&#22320;&#21270;&#22823;&#22411;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#20854;&#24320;&#21457;&#36807;&#31243;&#65292;&#21487;&#20316;&#20026;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#38754;&#21521;&#36739;&#23567;&#35821;&#35328;&#30340;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#30340;&#25351;&#21335;&#21644;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#24320;&#21457;&#38754;&#21521;&#21271;&#27431;&#35821;&#35328;&#30340;&#31532;&#19968;&#20010;&#26412;&#22320;&#21270;&#22823;&#22411;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;GPT-SW3&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#28085;&#30422;&#20102;&#24320;&#21457;&#36807;&#31243;&#30340;&#25152;&#26377;&#37096;&#20998;&#65292;&#20174;&#25968;&#25454;&#25910;&#38598;&#21644;&#22788;&#29702;&#65292;&#35757;&#32451;&#37197;&#32622;&#21644;&#25351;&#20196;&#24494;&#35843;&#65292;&#21040;&#35780;&#20272;&#21644;&#21457;&#24067;&#31574;&#30053;&#30340;&#32771;&#34385;&#12290;&#25105;&#20204;&#24076;&#26395;&#26412;&#25991;&#33021;&#22815;&#20316;&#20026;&#25351;&#21335;&#21644;&#21442;&#32771;&#65292;&#24110;&#21161;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#38754;&#21521;&#36739;&#23567;&#35821;&#35328;&#30340;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper details the process of developing the first native large generative language model for the Nordic languages, GPT-SW3. We cover all parts of the development process, from data collection and processing, training configuration and instruction finetuning, to evaluation and considerations for release strategies. We hope that this paper can serve as a guide and reference for other researchers that undertake the development of large generative models for smaller languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#19981;&#21516;&#30340;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#20256;&#32479;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;&#31361;&#20986;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#36129;&#29486;&#21644;&#25361;&#25112;&#65292;&#24182;&#24378;&#35843;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#23637;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.12641</link><description>&lt;p&gt;
&#36229;&#36234;&#35821;&#35328;&#65306;&#21477;&#23376;&#34920;&#31034;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Beyond Words: A Comprehensive Survey of Sentence Representations. (arXiv:2305.12641v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#19981;&#21516;&#30340;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#20256;&#32479;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;&#31361;&#20986;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#36129;&#29486;&#21644;&#25361;&#25112;&#65292;&#24182;&#24378;&#35843;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#23637;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#34920;&#31034;&#24050;&#32463;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#22914;&#26816;&#32034;&#12289;&#38382;&#31572;&#21644;&#25991;&#26412;&#20998;&#31867;&#12290;&#23427;&#20204;&#25429;&#25417;&#21477;&#23376;&#30340;&#35821;&#20041;&#21644;&#21547;&#20041;&#65292;&#20351;&#35745;&#31639;&#26426;&#33021;&#22815;&#29702;&#35299;&#21644;&#25512;&#29702;&#20154;&#31867;&#35821;&#35328;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#23398;&#20064;&#21477;&#23376;&#34920;&#31034;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#21253;&#25324;&#26080;&#30417;&#30563;&#12289;&#30417;&#30563;&#21644;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#19981;&#21516;&#30340;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#20256;&#32479;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#25972;&#29702;&#20102;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#30340;&#25991;&#29486;&#65292;&#31361;&#20986;&#20102;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#36129;&#29486;&#21644;&#25361;&#25112;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#32508;&#36848;&#24378;&#35843;&#20102;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#36825;&#19968;&#39046;&#22495;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#20173;&#28982;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence representations have become a critical component in natural language processing applications, such as retrieval, question answering, and text classification. They capture the semantics and meaning of a sentence, enabling machines to understand and reason over human language. In recent years, significant progress has been made in developing methods for learning sentence representations, including unsupervised, supervised, and transfer learning approaches. In this paper, we provide an overview of the different methods for sentence representation learning, including both traditional and deep learning-based techniques. We provide a systematic organization of the literature on sentence representation learning, highlighting the key contributions and challenges in this area. Overall, our review highlights the progress made in sentence representation learning, the importance of this area in natural language processing, and the challenges that remain. We conclude with directions for fu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;MultiCochrane&#65292;&#36825;&#26159;&#21307;&#23398;&#39046;&#22495;&#20013;&#31532;&#19968;&#20010;&#21477;&#23376;&#23545;&#40784;&#30340;&#22810;&#35821;&#35328;&#25991;&#26412;&#31616;&#21270;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22810;&#35821;&#35328;&#31616;&#21270;&#30452;&#25509;&#23558;&#22797;&#26434;&#25991;&#26412;&#31616;&#21270;&#20026;&#22810;&#31181;&#35821;&#35328;&#30340;&#31616;&#21270;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.12532</link><description>&lt;p&gt;
&#21307;&#23398;&#25991;&#26412;&#30340;&#22810;&#35821;&#35328;&#31616;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multilingual Simplification of Medical Texts. (arXiv:2305.12532v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;MultiCochrane&#65292;&#36825;&#26159;&#21307;&#23398;&#39046;&#22495;&#20013;&#31532;&#19968;&#20010;&#21477;&#23376;&#23545;&#40784;&#30340;&#22810;&#35821;&#35328;&#25991;&#26412;&#31616;&#21270;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22810;&#35821;&#35328;&#31616;&#21270;&#30452;&#25509;&#23558;&#22797;&#26434;&#25991;&#26412;&#31616;&#21270;&#20026;&#22810;&#31181;&#35821;&#35328;&#30340;&#31616;&#21270;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#25991;&#26412;&#31616;&#21270;&#26088;&#22312;&#20135;&#29983;&#22797;&#26434;&#25991;&#26412;&#30340;&#31616;&#21270;&#29256;&#26412;&#12290;&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#36825;&#39033;&#20219;&#21153;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#26368;&#26032;&#30340;&#21307;&#23398;&#21457;&#29616;&#36890;&#24120;&#36890;&#36807;&#22797;&#26434;&#21644;&#25216;&#26415;&#24615;&#30340;&#25991;&#31456;&#36827;&#34892;&#20256;&#25773;&#12290;&#36825;&#20026;&#23547;&#27714;&#26368;&#26032;&#21307;&#23398;&#21457;&#29616;&#20449;&#24687;&#30340;&#26222;&#36890;&#20154;&#36896;&#25104;&#20102;&#38556;&#30861;&#65292;&#36827;&#32780;&#38459;&#30861;&#20102;&#20581;&#24247;&#32032;&#20859;&#30340;&#25552;&#39640;&#12290;&#22823;&#37096;&#20998;&#21307;&#23398;&#25991;&#26412;&#31616;&#21270;&#30340;&#29616;&#26377;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#21333;&#35821;&#35328;&#29615;&#22659;&#20013;&#65292;&#23548;&#33268;&#36825;&#20123;&#35777;&#25454;&#21482;&#33021;&#29992;&#19968;&#31181;&#35821;&#35328;&#65288;&#36890;&#24120;&#26159;&#33521;&#35821;&#65289;&#25552;&#20379;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#22810;&#35821;&#35328;&#31616;&#21270;&#30452;&#25509;&#23558;&#22797;&#26434;&#25991;&#26412;&#31616;&#21270;&#20026;&#22810;&#31181;&#35821;&#35328;&#30340;&#31616;&#21270;&#25991;&#26412;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;MultiCochrane&#65292;&#36825;&#26159;&#21307;&#23398;&#39046;&#22495;&#20013;&#31532;&#19968;&#20010;&#22235;&#31181;&#35821;&#35328;&#65288;&#33521;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#12289;&#27861;&#35821;&#21644;&#27874;&#26031;&#35821;&#65289;&#30340;&#21477;&#23376;&#23545;&#40784;&#22810;&#35821;&#35328;&#25991;&#26412;&#31616;&#21270;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#20154;&#24037;&#35780;&#20272;&#21644;&#20998;&#26512;&#65292;&#22312;&#36825;&#20123;&#35821;&#35328;&#20043;&#38388;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#24494;&#35843;&#21644;&#38646;&#26679;&#26412;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated text simplification aims to produce simple versions of complex texts. This task is especially useful in the medical domain, where the latest medical findings are typically communicated via complex and technical articles. This creates barriers for laypeople seeking access to up-to-date medical findings, consequently impeding progress on health literacy. Most existing work on medical text simplification has focused on monolingual settings, with the result that such evidence would be available only in just one language (most often, English). This work addresses this limitation via multilingual simplification, i.e., directly simplifying complex texts into simplified texts in multiple languages. We introduce MultiCochrane, the first sentence-aligned multilingual text simplification dataset for the medical domain in four languages: English, Spanish, French, and Farsi. We evaluate fine-tuned and zero-shot models across these languages, with extensive human assessments and analyses. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#39640;&#32771;&#32771;&#35797;&#38382;&#39064;&#30340;&#22522;&#20934;&#27979;&#35797;GAOKAO-Benchmark&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#38382;&#39064;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#23545;ChatGPT&#27169;&#22411;&#30340;&#35780;&#20272;&#65292;&#30740;&#31350;&#21457;&#29616;&#20854;&#22312;&#23458;&#35266;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#20854;&#19981;&#36275;&#20043;&#22788;&#21644;&#25913;&#36827;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.12474</link><description>&lt;p&gt;
&#22312;&#39640;&#32771;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Performance of Large Language Models on GAOKAO Benchmark. (arXiv:2305.12474v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#39640;&#32771;&#32771;&#35797;&#38382;&#39064;&#30340;&#22522;&#20934;&#27979;&#35797;GAOKAO-Benchmark&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#38382;&#39064;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#23545;ChatGPT&#27169;&#22411;&#30340;&#35780;&#20272;&#65292;&#30740;&#31350;&#21457;&#29616;&#20854;&#22312;&#23458;&#35266;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#20854;&#19981;&#36275;&#20043;&#22788;&#21644;&#25913;&#36827;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65307;&#28982;&#32780;&#23427;&#20204;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#20219;&#21153;&#20013;&#30340;&#21151;&#25928;&#20173;&#28982;&#19981;&#22826;&#28165;&#26970;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;GAOKAO-Benchmark&#65288;GAOKAO-Bench&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#30452;&#35266;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#20351;&#29992;&#20013;&#22269;&#39640;&#32771;&#32771;&#35797;&#30340;&#39064;&#30446;&#20316;&#20026;&#27979;&#35797;&#26679;&#26412;&#65292;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#20102;&#23613;&#21487;&#33021;&#22320;&#20351;&#35780;&#20272;&#32467;&#26524;&#19982;&#20154;&#31867;&#19968;&#33268;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#38646;-shot&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#20026;&#20027;&#35266;&#21644;&#23458;&#35266;&#31867;&#22411;&#26469;&#20998;&#26512;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#35780;&#20998;&#29575;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#27169;&#22411;&#22312;GAOKAO-Benchmark&#24615;&#33021;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;ChatGPT&#27169;&#22411;&#22312;&#35299;&#20915;&#23458;&#35266;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#20854;&#19981;&#36275;&#20043;&#22788;&#21644;&#25913;&#36827;&#30340;&#26041;&#21521;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#23457;&#26597;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#25105;&#20204;&#21152;&#20837;&#20102;&#20154;&#31867;&#35780;&#20272;&#12290;&#24635;&#20043;&#65292;&#26412;&#30740;&#31350;&#20026;&#21019;&#24314;&#19968;&#20010;&#31283;&#20581;&#30340;&#35780;&#20272;GAOKAO&#22522;&#20934;&#27979;&#35797;&#25552;&#20379;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have demonstrated remarkable performance across various natural language processing tasks; however, their efficacy in more challenging and domain-specific tasks remains less explored. This paper introduces the GAOKAO-Benchmark (GAOKAO-Bench), an intuitive benchmark that employs questions from the Chinese Gaokao examination as test samples for evaluating large language models.In order to align the evaluation results with humans as much as possible, we designed a method based on zero-shot prompts to analyze the accuracy and scoring rate of the model by dividing the questions into subjective and objective types. We evaluated the ChatGPT model on GAOKAO-Benchmark performance.Our findings reveal that the ChatGPT model excels in tackling objective questions, while also shedding light on its shortcomings and areas for improvement. To further scrutinize the model's responses, we incorporate human evaluations.In conclusion, this research contributes a robust evaluation ben
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23459;&#20256;&#24615;&#25991;&#20214;&#32763;&#35793;&#20013;&#30340;&#25514;&#36766;&#22788;&#29702;&#38382;&#39064;&#65292;&#21457;&#29616;&#25919;&#27835;&#25991;&#26412;&#20013;&#30340;&#25514;&#36766;&#22312;&#33521;&#25991;&#20013;&#20986;&#29616;&#26356;&#39057;&#32321;&#65292;&#19988;&#32763;&#35793;&#26041;&#21521;&#24433;&#21709;&#25514;&#36766;&#20351;&#29992;&#39057;&#29575;&#21644;&#32763;&#35793;&#31574;&#30053;&#12290;&#21516;&#26102;&#36824;&#35266;&#23519;&#21040;&#20102;&#25514;&#36766;&#22312;&#21382;&#26102;&#26041;&#38754;&#30340;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2305.12146</link><description>&lt;p&gt;
&#23459;&#20256;&#24615;&#25991;&#20214;&#21452;&#21521;&#32763;&#35793;&#20013;&#30340;&#25514;&#36766;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Hedges in Bidirectional Translations of Publicity-Oriented Documents. (arXiv:2305.12146v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23459;&#20256;&#24615;&#25991;&#20214;&#32763;&#35793;&#20013;&#30340;&#25514;&#36766;&#22788;&#29702;&#38382;&#39064;&#65292;&#21457;&#29616;&#25919;&#27835;&#25991;&#26412;&#20013;&#30340;&#25514;&#36766;&#22312;&#33521;&#25991;&#20013;&#20986;&#29616;&#26356;&#39057;&#32321;&#65292;&#19988;&#32763;&#35793;&#26041;&#21521;&#24433;&#21709;&#25514;&#36766;&#20351;&#29992;&#39057;&#29575;&#21644;&#32763;&#35793;&#31574;&#30053;&#12290;&#21516;&#26102;&#36824;&#35266;&#23519;&#21040;&#20102;&#25514;&#36766;&#22312;&#21382;&#26102;&#26041;&#38754;&#30340;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25514;&#36766;&#26159;&#36328;&#19987;&#19994;&#39046;&#22495;&#24191;&#27867;&#30740;&#31350;&#30340;&#35789;&#27719;&#65292;&#20294;&#25919;&#27835;&#25991;&#26412;&#20013;&#25514;&#36766;&#30340;&#32763;&#35793;&#30740;&#31350;&#26497;&#20026;&#26377;&#38480;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25514;&#36766;&#22312;&#30446;&#26631;&#25991;&#26412;&#20013;&#30340;&#35789;&#39057;&#21464;&#21270;&#26159;&#21542;&#20855;&#26377;&#21382;&#26102;&#24615;&#65292;&#32763;&#35793;&#20013;&#25514;&#36766;&#36890;&#36807;&#24180;&#20221;&#21464;&#21270;&#30340;&#31243;&#24230;&#22914;&#20309;&#24402;&#22240;&#20110;&#28304;&#25991;&#26412;&#65292;&#24182;&#37319;&#29992;&#20102;&#20309;&#31181;&#32763;&#35793;&#31574;&#30053;&#26469;&#22788;&#29702;&#23427;&#20204;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30740;&#31350;&#30446;&#30340;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;&#20013;&#22269;&#21644;&#32852;&#21512;&#22269;&#30340;&#20004;&#31181;&#20844;&#21153;&#25919;&#27835;&#25991;&#26412;&#21450;&#20854;&#32763;&#35793;&#65292;&#24418;&#25104;&#20102;&#19977;&#20010;&#23376;&#35821;&#26009;&#24211;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25514;&#36766;&#22312;&#33521;&#25991;&#25919;&#27835;&#25991;&#26412;&#20013;&#65288;&#26080;&#35770;&#26159;&#21407;&#22987;&#33521;&#25991;&#36824;&#26159;&#32763;&#35793;&#33521;&#25991;&#65289;&#20284;&#20046;&#26356;&#39057;&#32321;&#20986;&#29616;&#12290;&#27492;&#22806;&#65292;&#32763;&#35793;&#26041;&#21521;&#20284;&#20046;&#22312;&#24433;&#21709;&#25514;&#36766;&#20351;&#29992;&#30340;&#39057;&#29575;&#21644;&#32763;&#35793;&#31574;&#30053;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#25514;&#36766;&#20351;&#29992;&#39057;&#29575;&#22312;&#25105;&#20204;&#30340;&#23376;&#35821;&#26009;&#24211;&#20013;&#20986;&#29616;&#20102;&#26174;&#33879;&#30340;&#21382;&#26102;&#24615;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hedges are widely studied across registers and disciplines, yet research on the translation of hedges in political texts is extremely limited. This contrastive study is dedicated to investigating whether there is a diachronic change in the frequencies of hedging devices in the target texts, to what extent the changing frequencies of translated hedges through years are attributed to the source texts, and what translation strategies are adopted to deal with them. For the purposes of this research, two types of official political texts and their translations from China and the United Nations were collected to form three sub-corpora. Results show that hedges tend to appear more frequently in English political texts, be it original English or translated English. In addition, directionality seems to play an important role in influencing both the frequencies and translation strategies regarding the use of hedges. A noticeable diachronic increase of hedging devices is also observed in our corp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#24341;&#23548;LLMs&#36827;&#34892;&#25991;&#26412;&#21040;SQL&#30340;&#20219;&#21153;&#20013;&#25552;&#31034;&#25991;&#26412;&#26500;&#24314;&#38382;&#39064;&#23637;&#24320;&#20102;&#32508;&#21512;&#25506;&#31350;&#65292;&#20174;&#32780;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.11853</link><description>&lt;p&gt;
&#22914;&#20309;&#24341;&#23548;LLMs&#36827;&#34892;&#25991;&#26412;&#21040;SQL&#30340;&#23398;&#20064;: &#20174;&#38646;&#26679;&#26412;&#21040;&#21333;&#39046;&#22495;&#21040;&#36328;&#39046;&#22495;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings. (arXiv:2305.11853v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#24341;&#23548;LLMs&#36827;&#34892;&#25991;&#26412;&#21040;SQL&#30340;&#20219;&#21153;&#20013;&#25552;&#31034;&#25991;&#26412;&#26500;&#24314;&#38382;&#39064;&#23637;&#24320;&#20102;&#32508;&#21512;&#25506;&#31350;&#65292;&#20174;&#32780;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26174;&#33879;&#33021;&#21147;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#21508;&#31181;&#28436;&#31034;-&#26816;&#32034;&#31574;&#30053;&#21644;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#26469;&#20419;&#20351;LLMs&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#22312;&#26500;&#24314;&#25991;&#26412;&#21040;SQL&#36755;&#20837;&#30340;&#25552;&#31034;&#25991;&#26412;(&#22914;&#25968;&#25454;&#24211;&#21644;&#28436;&#31034;&#31034;&#20363;)&#26102;&#24120;&#37319;&#29992;&#19981;&#21516;&#30340;&#31574;&#30053;&#12290;&#36825;&#23548;&#33268;&#25552;&#31034;&#25991;&#26412;&#30340;&#26500;&#24314;&#21644;&#20854;&#20027;&#35201;&#36129;&#29486;&#30340;&#21487;&#27604;&#24615;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#36873;&#25321;&#26377;&#25928;&#30340;&#25552;&#31034;&#25991;&#26412;&#24314;&#35774;&#24050;&#25104;&#20026;&#26410;&#26469;&#30740;&#31350;&#20013;&#30340;&#25345;&#20037;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#20840;&#38754;&#35843;&#26597;&#20102;&#19981;&#21516;&#35774;&#32622;&#19979;&#25552;&#31034;&#25991;&#26412;&#32467;&#26500;&#30340;&#24433;&#21709;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) with in-context learning have demonstrated remarkable capability in the text-to-SQL task. Previous research has prompted LLMs with various demonstration-retrieval strategies and intermediate reasoning steps to enhance the performance of LLMs. However, those works often employ varied strategies when constructing the prompt text for text-to-SQL inputs, such as databases and demonstration examples. This leads to a lack of comparability in both the prompt constructions and their primary contributions. Furthermore, selecting an effective prompt construction has emerged as a persistent problem for future research. To address this limitation, we comprehensively investigate the impact of prompt constructions across various settings and provide insights for future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#33021;&#21147;&#30340;&#33539;&#20363;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#33258;&#36523;&#29983;&#25104;&#30340;&#19981;&#21516;&#24847;&#20041;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#25506;&#35752;&#20102;&#22810;&#35821;&#35328;&#33258;&#25105;&#19968;&#33268;&#24615;&#20316;&#20026;&#27169;&#22411;&#29702;&#35299;&#30340;&#26816;&#39564;&#26041;&#27861;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;ChatGPT&#22312;&#22810;&#35821;&#35328;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#20248;&#31168;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11662</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#35821;&#35328;&#19968;&#33268;&#24615;&#35780;&#20272;&#20219;&#21153;&#29702;&#35299;&#65306;ChatGPT&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Evaluating task understanding through multilingual consistency: A ChatGPT case study. (arXiv:2305.11662v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#33021;&#21147;&#30340;&#33539;&#20363;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#33258;&#36523;&#29983;&#25104;&#30340;&#19981;&#21516;&#24847;&#20041;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#25506;&#35752;&#20102;&#22810;&#35821;&#35328;&#33258;&#25105;&#19968;&#33268;&#24615;&#20316;&#20026;&#27169;&#22411;&#29702;&#35299;&#30340;&#26816;&#39564;&#26041;&#27861;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;ChatGPT&#22312;&#22810;&#35821;&#35328;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#20248;&#31168;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21151;&#33021;&#30340;&#24778;&#20154;&#25552;&#21319;&#65292;&#21019;&#24314;&#26410;&#26469;&#21487;&#25345;&#32493;&#30340;&#35780;&#20272;&#38598;&#20197;&#35780;&#20272;&#23427;&#20204;&#30340;&#29702;&#35299;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;LLM&#30340;&#33539;&#20363;&#65292;&#35813;&#33539;&#20363;&#21033;&#29992;&#20102;&#27491;&#30830;&#30340;&#19990;&#30028;&#29702;&#35299;&#24212;&#35813;&#22312;&#30456;&#21516;&#21547;&#20041;&#30340;&#19981;&#21516;&#65288;&#24343;&#38647;&#26684;&#65289;&#24847;&#20041;&#19978;&#20445;&#25345;&#19968;&#33268;&#30340;&#24605;&#24819;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19981;&#26159;&#36890;&#36807;&#27491;&#30830;&#24615;&#26469;&#34913;&#37327;&#29702;&#35299;&#65292;&#32780;&#26159;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#33258;&#36523;&#29983;&#25104;&#30340;&#22810;&#20010;&#24847;&#20041;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26469;&#34913;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#20363;&#21270;&#19968;&#20010;&#27979;&#35797;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19981;&#21516;&#30340;&#24847;&#20041;&#26159;&#19981;&#21516;&#30340;&#35821;&#35328;&#65292;&#22240;&#27492;&#23558;&#22810;&#35821;&#35328;&#33258;&#25105;&#19968;&#33268;&#24615;&#20316;&#20026;&#27169;&#22411;&#29702;&#35299;&#30340;&#26816;&#39564;&#24182;&#21516;&#26102;&#35299;&#20915;&#22810;&#35821;&#35328;&#30340;&#37325;&#35201;&#20027;&#39064;&#12290;&#25105;&#20204;&#20197;&#26368;&#26032;&#29256;&#26412;&#30340;ChatGPT&#20026;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#35937;&#65292;&#22312;&#19977;&#31181;&#19981;&#21516;&#35821;&#35328;&#20013;&#35780;&#20272;&#20004;&#20010;&#19981;&#21516;&#20219;&#21153;&#30340;&#22810;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;ChatGPT&#22312;&#22810;&#35821;&#35328;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#20248;&#31168;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
At the staggering pace with which the capabilities of large language models (LLMs) are increasing, creating future-proof evaluation sets to assess their understanding becomes more and more challenging. In this paper, we propose a novel paradigm for evaluating LLMs which leverages the idea that correct world understanding should be consistent across different (Fregean) senses of the same meaning. Accordingly, we measure understanding not in terms of correctness but by evaluating consistency across multiple senses that are generated by the model itself. We showcase our approach by instantiating a test where the different senses are different languages, hence using multilingual self-consistency as a litmus test for the model's understanding and simultaneously addressing the important topic of multilingualism. Taking one of the latest versions of ChatGPT as our object of study, we evaluate multilingual consistency for two different tasks across three different languages. We show that its m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312; PLMs &#20013;&#27880;&#20837;&#19978;&#19979;&#25991; NER &#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#23569;&#37327;&#31034;&#24847;&#23454;&#20363;&#21363;&#21487;&#21160;&#24577;&#35782;&#21035;&#26032;&#31867;&#22411;&#30340;&#23454;&#20307;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11038</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Learning In-context Learning for Named Entity Recognition. (arXiv:2305.11038v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312; PLMs &#20013;&#27880;&#20837;&#19978;&#19979;&#25991; NER &#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#23569;&#37327;&#31034;&#24847;&#23454;&#20363;&#21363;&#21487;&#21160;&#24577;&#35782;&#21035;&#26032;&#31867;&#22411;&#30340;&#23454;&#20307;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21463;&#21040;&#23454;&#20307;&#31867;&#22411;&#30340;&#22810;&#26679;&#24615;&#12289;&#26032;&#23454;&#20307;&#31867;&#22411;&#30340;&#20986;&#29616;&#21644;&#39640;&#36136;&#37327;&#26631;&#27880;&#30340;&#32570;&#20047;&#31561;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#19978;&#19979;&#25991;NER&#33021;&#21147;&#26377;&#25928;&#22320;&#27880;&#20837;&#21040;PLMs&#20013;&#65292;&#24182;&#19988;&#21482;&#20351;&#29992;&#23569;&#37327;&#31034;&#24847;&#23454;&#20363;&#23601;&#33021;&#21160;&#24577;&#35782;&#21035;&#26032;&#31867;&#22411;&#30340;&#23454;&#20307;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;PLMs&#24314;&#27169;&#20026;&#19968;&#20010;&#20803;&#20989;&#25968; $\mathcal{ \lambda_ {\text{instruction, demonstrations, text}}. M}$&#65292;&#24182;&#36890;&#36807;&#23558;&#26032;&#30340;&#25351;&#31034;&#21644;&#31034;&#20363;&#24212;&#29992;&#20110;PLMs&#26469;&#38544;&#21547;&#22320;&#26500;&#24314;&#26032;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#65292;&#21363; $\mathcal{ (\lambda . M) }$(instruction, demonstrations) $\to$ $\mathcal{F}$&#65292;&#20854;&#20013; $\mathcal{F}$ &#23558;&#25104;&#20026;&#19968;&#20010;&#26032;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#65292;&#21363; $\mathcal{F}$: text $\to$ entities&#12290;&#20026;&#20102;&#23558;&#19978;&#36848;&#19978;&#19979;&#25991;NER&#33021;&#21147;&#27880;&#20837;PLMs&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#20989;&#25968;&#39044;&#35757;&#32451;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#27604;&#36739;&#65288;&#25351;&#31034;&#12289;&#31034;&#20363;&#65289;-identity&#21644;&#65288;&#25513;&#30422;&#21518;&#30340;&#25351;&#31034;&#12289;&#31034;&#20363;&#65289;-identity&#26469;&#39044;&#35757;&#32451;PLMs&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#20351;&#29992;&#23569;&#37327;&#31034;&#24847;&#23454;&#20363;&#26377;&#25928;&#22320;&#35782;&#21035;&#26032;&#31867;&#22411;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named entity recognition in real-world applications suffers from the diversity of entity types, the emergence of new entity types, and the lack of high-quality annotations. To address the above problems, this paper proposes an in-context learning-based NER approach, which can effectively inject in-context NER ability into PLMs and recognize entities of novel types on-the-fly using only a few demonstrative instances. Specifically, we model PLMs as a meta-function $\mathcal{ \lambda_ {\text{instruction, demonstrations, text}}. M}$, and a new entity extractor can be implicitly constructed by applying new instruction and demonstrations to PLMs, i.e., $\mathcal{ (\lambda . M) }$(instruction, demonstrations) $\to$ $\mathcal{F}$ where $\mathcal{F}$ will be a new entity extractor, i.e., $\mathcal{F}$: text $\to$ entities. To inject the above in-context NER ability into PLMs, we propose a meta-function pre-training algorithm, which pre-trains PLMs by comparing the (instruction, demonstration)-i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#8212;&#8212;&#23558;&#20854;&#19982;&#19990;&#30028;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#26377;&#30446;&#30340;&#30340;&#35268;&#21010;&#21644;&#38543;&#26426;&#25506;&#32034;&#33719;&#24471;&#20016;&#23500;&#30340;&#23454;&#20307;&#32463;&#39564;&#36827;&#34892;&#24494;&#35843;, &#20197;&#25552;&#39640;&#20854;&#22312;&#29289;&#29702;&#29615;&#22659;&#19979;&#30340;&#25512;&#29702;&#21644;&#34892;&#20026;&#33021;&#21147;&#65292;&#24182;&#22312;&#35821;&#35328;&#22522;&#20934;&#19978;&#20445;&#25345;&#25110;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10626</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36935;&#35265;&#19990;&#30028;&#27169;&#22411;&#65306;&#23454;&#20307;&#32463;&#39564;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Language Models Meet World Models: Embodied Experiences Enhance Language Models. (arXiv:2305.10626v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#8212;&#8212;&#23558;&#20854;&#19982;&#19990;&#30028;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#26377;&#30446;&#30340;&#30340;&#35268;&#21010;&#21644;&#38543;&#26426;&#25506;&#32034;&#33719;&#24471;&#20016;&#23500;&#30340;&#23454;&#20307;&#32463;&#39564;&#36827;&#34892;&#24494;&#35843;, &#20197;&#25552;&#39640;&#20854;&#22312;&#29289;&#29702;&#29615;&#22659;&#19979;&#30340;&#25512;&#29702;&#21644;&#34892;&#20026;&#33021;&#21147;&#65292;&#24182;&#22312;&#35821;&#35328;&#22522;&#20934;&#19978;&#20445;&#25345;&#25110;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LMs) &#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#22312;&#22788;&#29702;&#29289;&#29702;&#29615;&#22659;&#19979;&#30340;&#31616;&#21333;&#25512;&#29702;&#21644;&#35268;&#21010;&#38382;&#39064;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#20363;&#22914;&#29702;&#35299;&#29289;&#20307;&#27704;&#24658;&#25110;&#35268;&#21010;&#23478;&#24237;&#27963;&#21160;&#12290;&#36825;&#31181;&#38480;&#21046;&#28304;&#20110; LM &#20165;&#21463;&#20070;&#38754;&#35821;&#35328;&#35757;&#32451;&#65292;&#32570;&#23569;&#24517;&#35201;&#30340;&#23454;&#20307;&#30693;&#35782;&#21644;&#25216;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378; LM &#30340;&#26041;&#27861;&#65292;&#21363;&#23558;&#20854;&#19982;&#19990;&#30028;&#27169;&#22411;&#30456;&#32467;&#21512;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#33719;&#24471;&#22810;&#26679;&#21270;&#30340;&#23454;&#20307;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#19968;&#33324;&#35821;&#35328;&#33021;&#21147;&#12290;&#26412;&#26041;&#27861;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#37096;&#32626;&#19968;&#20010;&#34701;&#20837;&#23454;&#20307;&#32463;&#39564;&#30340;&#20195;&#29702;&#65292;&#29305;&#21035;&#26159;&#19968;&#20010;&#27169;&#25311;&#29289;&#29702;&#19990;&#30028;&#30340;&#20223;&#30495;&#22120;(VirtualHome)&#65292;&#36890;&#36807;&#26377;&#30446;&#30340;&#30340;&#35268;&#21010;&#21644;&#38543;&#26426;&#25506;&#32034;&#33719;&#24471;&#22810;&#26679;&#21270;&#30340;&#23454;&#20307;&#32463;&#39564;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36825;&#20123;&#32463;&#39564;&#24494;&#35843; LM &#65292;&#20197;&#25945;&#25480;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#21508;&#31181;&#25512;&#29702;&#21644;&#34892;&#20026;&#33021;&#21147;&#65292;&#20363;&#22914;&#35268;&#21010;&#21644;&#23436;&#25104;&#30446;&#26631;&#12289;&#29289;&#20307;&#27704;&#24658;&#21644;&#36319;&#36394;&#31561;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#20197;&#21033;&#29992;&#20854;&#20182;&#27169;&#25311;&#22120;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#25110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#30528;&#25552;&#39640;&#20102; LM &#22312;&#19968;&#31995;&#21015;&#29289;&#29702;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#30041;&#24182;&#32463;&#24120;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;&#35821;&#35328;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LMs) have shown remarkable capabilities across numerous tasks, they often struggle with simple reasoning and planning in physical environments, such as understanding object permanence or planning household activities. The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills. In this paper, we propose a new paradigm of enhancing LMs by finetuning them with world models, to gain diverse embodied knowledge while retaining their general language capabilities. Our approach deploys an embodied agent in a world model, particularly a simulator of the physical world (VirtualHome), and acquires a diverse set of embodied experiences through both goal-oriented planning and random exploration. These experiences are then used to finetune LMs to teach diverse abilities of reasoning and acting in the physical world, e.g., planning and completing goals, object permanence and tracking, etc. Moreover, it is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#35780;&#20998;&#26041;&#27861;&#65292;&#21363;PLL-word-l2r&#65292;&#29992;&#20110;&#20272;&#35745;&#21477;&#23376;&#30340;&#20266;&#23545;&#25968;&#20284;&#28982;&#24471;&#20998;&#65292;&#30456;&#23545;&#20110;&#21407;PLL&#26041;&#27861;&#21644;&#23631;&#34109;&#25152;&#26377;&#21333;&#35789;&#26631;&#35760;&#30340;PLL&#35780;&#20998;&#26041;&#27861;&#65292;&#25913;&#36827;&#30340;&#24230;&#37327;&#26041;&#27861;&#26356;&#22909;&#22320;&#38024;&#23545;&#23383;&#27719;&#22806;&#21333;&#35789;&#24471;&#20998;&#38382;&#39064;&#36827;&#34892;&#20102;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2305.10588</link><description>&lt;p&gt;
&#19968;&#31181;&#26356;&#22909;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#35780;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Better Way to Do Masked Language Model Scoring. (arXiv:2305.10588v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#35780;&#20998;&#26041;&#27861;&#65292;&#21363;PLL-word-l2r&#65292;&#29992;&#20110;&#20272;&#35745;&#21477;&#23376;&#30340;&#20266;&#23545;&#25968;&#20284;&#28982;&#24471;&#20998;&#65292;&#30456;&#23545;&#20110;&#21407;PLL&#26041;&#27861;&#21644;&#23631;&#34109;&#25152;&#26377;&#21333;&#35789;&#26631;&#35760;&#30340;PLL&#35780;&#20998;&#26041;&#27861;&#65292;&#25913;&#36827;&#30340;&#24230;&#37327;&#26041;&#27861;&#26356;&#22909;&#22320;&#38024;&#23545;&#23383;&#27719;&#22806;&#21333;&#35789;&#24471;&#20998;&#38382;&#39064;&#36827;&#34892;&#20102;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#19979;&#32473;&#23450;&#21477;&#23376;&#30340;&#23545;&#25968;&#20284;&#28982;&#24456;&#31616;&#21333;&#65306;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#38142;&#24335;&#27861;&#21017;&#24182;&#23545;&#27599;&#20010;&#36830;&#32493;&#26631;&#35760;&#30340;&#23545;&#25968;&#20284;&#28982;&#20540;&#27714;&#21644;&#12290;&#20294;&#23545;&#20110;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#27809;&#26377;&#30452;&#25509;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#19968;&#20010;&#21477;&#23376;&#30340;&#23545;&#25968;&#20284;&#28982;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;Salazar&#31561;&#20154;&#65288;2020&#65289;&#25552;&#20986;&#20102;&#20272;&#35745;&#21477;&#23376;&#20266;&#23545;&#25968;&#20284;&#28982;&#65288;PLL&#65289;&#20998;&#25968;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20381;&#27425;&#23631;&#34109;&#27599;&#20010;&#21477;&#23376;&#26631;&#35760;&#65292;&#20351;&#29992;&#20854;&#20313;&#30340;&#21477;&#23376;&#20316;&#20026;&#19978;&#19979;&#25991;&#26816;&#32034;&#20854;&#24471;&#20998;&#65292;&#24182;&#24635;&#32467;&#32467;&#26524;&#20540;&#12290;&#26412;&#25991;&#38024;&#23545;&#21407;PLL&#26041;&#27861;&#20013;&#23383;&#27719;&#22806;&#21333;&#35789;&#20135;&#29983;&#30340;&#24471;&#20998;&#22840;&#22823;&#30340;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#19981;&#20165;&#23631;&#34109;&#30446;&#26631;&#26631;&#35760;&#65292;&#32780;&#19988;&#36824;&#23631;&#34109;&#30446;&#26631;&#26631;&#35760;&#21491;&#20391;&#25152;&#26377;&#30340;&#26631;&#35760;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25913;&#36827;&#30340;&#24230;&#37327;&#26041;&#27861;&#65288;PLL-word-l2r&#65289;&#20248;&#20110;&#21407;&#22987;PLL&#24230;&#37327;&#26041;&#27861;&#21644;&#19968;&#20010;&#23631;&#34109;&#25152;&#26377;&#21333;&#35789;&#26631;&#35760;&#30340;PLL&#21512;&#25104;&#26041;&#24335;&#12290;&#29305;&#21035;&#22320;&#65292;&#23427;&#26356;&#22909;&#22320;&#31526;&#21512;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the log-likelihood of a given sentence under an autoregressive language model is straightforward: one can simply apply the chain rule and sum the log-likelihood values for each successive token. However, for masked language models, there is no direct way to estimate the log-likelihood of a sentence. To address this issue, Salazar et al. (2020) propose to estimate sentence pseudo-log-likelihood (PLL) scores, computed by successively masking each sentence token, retrieving its score using the rest of the sentence as context, and summing the resulting values. Here, we demonstrate that the original PLL method yields inflated scores for out-of-vocabulary words and propose an adapted metric, in which we mask not only the target token, but also all within-word tokens to the right of the target. We show that our adapted metric (PLL-word-l2r) outperforms both the original PLL metric and a PLL metric in which all within-word tokens are masked. In particular, it better satisfies theore
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#23545;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;&#38382;&#39064;&#36827;&#34892;&#30340;&#31532;&#19968;&#39033;&#31995;&#32479;&#30740;&#31350;&#65292;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#35270;&#35273;&#25351;&#20196;&#21487;&#33021;&#24433;&#21709;&#24187;&#35273;&#65292;&#25552;&#20986;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#25104;&#21151;&#35299;&#20915;&#20102;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2305.10355</link><description>&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluating Object Hallucination in Large Vision-Language Models. (arXiv:2305.10355v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#23545;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;&#38382;&#39064;&#36827;&#34892;&#30340;&#31532;&#19968;&#39033;&#31995;&#32479;&#30740;&#31350;&#65292;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#35270;&#35273;&#25351;&#20196;&#21487;&#33021;&#24433;&#21709;&#24187;&#35273;&#65292;&#25552;&#20986;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#25104;&#21151;&#35299;&#20915;&#20102;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#25496;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22240;&#20026;&#20854;&#20986;&#33394;&#30340;&#35821;&#35328;&#33021;&#21147;&#36817;&#26469;&#24050;&#32463;&#24320;&#22987;&#30740;&#31350;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLM)&#65292;&#24182;&#23558;&#24378;&#22823;&#30340;LLM&#38598;&#25104;&#20110;LVLM&#20013;&#65292;&#20197;&#25552;&#39640;LVLM&#22312;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#34429;&#28982;LVLM&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#27493;&#65292;&#20294;&#26159;&#26412;&#30740;&#31350;&#21457;&#29616;LVLM&#23384;&#22312;&#38271;&#24230;&#24187;&#35273;&#38382;&#39064;&#65292;&#21363;&#23427;&#20204;&#20542;&#21521;&#20110;&#29983;&#25104;&#19982;&#30446;&#26631;&#22270;&#20687;&#19981;&#19968;&#33268;&#30340;&#29289;&#20307;&#25551;&#36848;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#24320;&#23637;&#20102;&#31532;&#19968;&#39033;&#31995;&#32479;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;LVLM&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;&#12290;&#25105;&#20204;&#23545;&#20960;&#20010;&#20195;&#34920;&#24615;&#30340;LVLM&#36827;&#34892;&#20102;&#35780;&#20272;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#22823;&#22810;&#25968;&#37117;&#23384;&#22312;&#20005;&#37325;&#30340;&#29289;&#20307;&#24187;&#35273;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#35270;&#35273;&#25351;&#20196;&#21487;&#33021;&#20250;&#24433;&#21709;&#24187;&#35273;&#65292;&#24182;&#21457;&#29616;&#22312;&#35270;&#35273;&#25351;&#20196;&#20013;&#32463;&#24120;&#20986;&#29616;&#25110;&#19982;&#22270;&#20687;&#20013;&#30340;&#29289;&#20307;&#20849;&#29616;&#30340;&#29289;&#20307;&#65292;&#26356;&#23481;&#26131;&#34987;LVLM&#20135;&#29983;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#21487;&#33021;&#20250;&#21463;&#21040;&#36755;&#20837;&#25351;&#20196;&#30340;&#24433;&#21709;&#65292;&#19981;&#33021;&#36275;&#20197;&#35782;&#21035;&#29289;&#20307;&#24187;&#35273;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35780;&#20272;&#29289;&#20307;&#24187;&#35273;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25351;&#26631;&#19981;&#20165;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#29289;&#20307;&#24187;&#35273;&#38382;&#39064;&#65292;&#36824;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#24187;&#35273;&#38382;&#39064;&#20986;&#29616;&#20301;&#32622;&#21644;&#22914;&#20309;&#32531;&#35299;&#23427;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the superior language abilities of large language models (LLM), large vision-language models (LVLM) have been recently explored by integrating powerful LLMs for improving the performance on complex multimodal tasks. Despite the promising progress on LVLMs, we find that LVLMs suffer from the hallucination problem, i.e. they tend to generate objects that are inconsistent with the target images in the descriptions. To investigate it, this work presents the first systematic study on object hallucination of LVLMs. We conduct the evaluation experiments on several representative LVLMs, and show that they mostly suffer from severe object hallucination issue. We further discuss that the visual instructions may influence the hallucination, and find that: objects that frequently occur in the visual instructions or co-occur with the image objects, are obviously prone to be hallucinated by LVLMs. Besides, we find that existing evaluation methods might be affected by the input instructio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#35268;&#21010;&#65288;NLP&#65289;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#30740;&#31350;LLMs&#22312;&#38656;&#35201;&#29702;&#35299;&#24182;&#22312;&#25991;&#26412;&#20013;&#30456;&#24212;&#36827;&#34892;&#25805;&#20316;&#30340;&#22797;&#26434;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;CoS&#65292;&#20351;&#29992;&#31616;&#21270;&#30340;&#31526;&#21495;&#31354;&#38388;&#34920;&#31034;&#27861;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2305.10276</link><description>&lt;p&gt;
&#36830;&#38145;&#31526;&#21495;&#25552;&#31034;&#28608;&#21457;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35268;&#21010;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models. (arXiv:2305.10276v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#35268;&#21010;&#65288;NLP&#65289;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#30740;&#31350;LLMs&#22312;&#38656;&#35201;&#29702;&#35299;&#24182;&#22312;&#25991;&#26412;&#20013;&#30456;&#24212;&#36827;&#34892;&#25805;&#20316;&#30340;&#22797;&#26434;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;CoS&#65292;&#20351;&#29992;&#31616;&#21270;&#30340;&#31526;&#21495;&#31354;&#38388;&#34920;&#31034;&#27861;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;LLMs&#22312;&#38656;&#35201;&#29702;&#35299;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#27169;&#25311;&#30340;&#34394;&#25311;&#31354;&#38388;&#29615;&#22659;&#24182;&#22312;&#25991;&#26412;&#20013;&#30456;&#24212;&#36827;&#34892;&#25805;&#20316;&#30340;&#22797;&#26434;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#33258;&#28982;&#35821;&#35328;&#35268;&#21010;&#65288;NLP&#65289;&#30340;&#22522;&#20934;&#65292;&#23427;&#30001;&#19968;&#32452;&#26032;&#39062;&#30340;&#20219;&#21153;&#32452;&#25104;&#65306;Brick World&#12289;&#22522;&#20110;NLVR&#30340;&#25805;&#20316;&#21644;&#33258;&#28982;&#35821;&#35328;&#23548;&#33322;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#27969;&#34892;&#30340;LLMs&#65288;&#22914;ChatGPT&#65289;&#20173;&#28982;&#32570;&#20047;&#22797;&#26434;&#35268;&#21010;&#30340;&#33021;&#21147;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#8212;&#8212;LLMs&#26159;&#21542;&#23545;&#33258;&#28982;&#35821;&#35328;&#20013;&#25551;&#36848;&#30340;&#29615;&#22659;&#26377;&#33391;&#22909;&#30340;&#29702;&#35299;&#65292;&#25110;&#32773;&#20854;&#20182;&#26367;&#20195;&#26041;&#27861;&#65288;&#22914;&#31526;&#21495;&#34920;&#31034;&#65289;&#26159;&#21542;&#26356;&#21152;&#31616;&#21333;&#65292;&#22240;&#27492;&#26356;&#23481;&#26131;&#34987;LLMs&#29702;&#35299;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoS&#65288;Chain-of-Symbol Prompting&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#38142;&#24335;&#20013;&#38388;&#24605;&#32771;&#27493;&#39588;&#20013;&#20351;&#29992;&#31616;&#21270;&#30340;&#31526;&#21495;&#31354;&#38388;&#34920;&#31034;&#27861;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#29615;&#22659;&#12290;CoS&#26131;&#20110;&#20351;&#29992;&#65292;&#19981;&#38656;&#35201;&#23545;LLMs&#36827;&#34892;&#39069;&#22806;&#30340;&#22521;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we take the initiative to investigate the performance of LLMs on complex planning tasks that require LLMs to understand a virtual spatial environment simulated via natural language and act correspondingly in text. We propose a benchmark named Natural Language Planning (NLP) composed of a set of novel tasks: Brick World, NLVR-based Manipulations, and Natural Language Navigation. We found that current popular LLMs such as ChatGPT still lack abilities in complex planning. This arises a question -- do the LLMs have a good understanding of the environments described in natural language, or maybe other alternatives such as symbolic representations are neater and hence better to be understood by LLMs? To this end, we propose a novel method called CoS (Chain-of-Symbol Prompting) that represents the complex environments with condensed symbolic spatial representations during the chained intermediate thinking steps. CoS is easy to use and does not need additional training on LLMs. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#38754;&#21521;&#27861;&#24459;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#21457;&#24067;&#20102;&#19968;&#20010;&#36328;&#22269;&#33521;&#35821;&#27861;&#24459;&#25991;&#38598;&#21644;&#19968;&#20010;&#27861;&#24459;&#30693;&#35782;&#25506;&#38024;&#22522;&#20934;&#65292;&#21457;&#29616;&#25506;&#38024;&#24615;&#33021;&#19982;&#19978;&#28216;&#24615;&#33021;&#24378;&#30456;&#20851;&#65292;&#19979;&#28216;&#24615;&#33021;&#20027;&#35201;&#30001;&#27169;&#22411;&#30340;&#22823;&#23567;&#21644;&#20808;&#21069;&#30340;&#27861;&#24459;&#30693;&#35782;&#39537;&#21160;&#12290;</title><link>http://arxiv.org/abs/2305.07507</link><description>&lt;p&gt;
LeXFiles&#21644;LegalLAMA&#65306;&#20419;&#36827;&#33521;&#35821;&#36328;&#22269;&#27861;&#24459;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;
LeXFiles and LegalLAMA: Facilitating English Multinational Legal Language Model Development. (arXiv:2305.07507v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#38754;&#21521;&#27861;&#24459;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#21457;&#24067;&#20102;&#19968;&#20010;&#36328;&#22269;&#33521;&#35821;&#27861;&#24459;&#25991;&#38598;&#21644;&#19968;&#20010;&#27861;&#24459;&#30693;&#35782;&#25506;&#38024;&#22522;&#20934;&#65292;&#21457;&#29616;&#25506;&#38024;&#24615;&#33021;&#19982;&#19978;&#28216;&#24615;&#33021;&#24378;&#30456;&#20851;&#65292;&#19979;&#28216;&#24615;&#33021;&#20027;&#35201;&#30001;&#27169;&#22411;&#30340;&#22823;&#23567;&#21644;&#20808;&#21069;&#30340;&#27861;&#24459;&#30693;&#35782;&#39537;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#38754;&#21521;&#27861;&#24459;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#25105;&#20204;&#32771;&#23519;&#20102;&#23427;&#20204;&#30340;&#21407;&#22987;&#30446;&#26631;&#12289;&#33719;&#21462;&#30340;&#30693;&#35782;&#21644;&#27861;&#24459;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#23558;&#20854;&#23450;&#20041;&#20026;&#19978;&#28216;&#12289;&#25506;&#38024;&#21644;&#19979;&#28216;&#24615;&#33021;&#12290;&#25105;&#20204;&#19981;&#20165;&#32771;&#34385;&#20102;&#27169;&#22411;&#30340;&#22823;&#23567;&#65292;&#36824;&#23558;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20316;&#20026;&#30740;&#31350;&#20013;&#30340;&#37325;&#35201;&#32500;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#36328;&#22269;&#33521;&#35821;&#27861;&#24459;&#25991;&#38598;&#65288;LeXFiles&#65289;&#21644;&#19968;&#20010;&#27861;&#24459;&#30693;&#35782;&#25506;&#38024;&#22522;&#20934;&#65288;LegalLAMA&#65289;&#65292;&#20197;&#20419;&#36827;&#27861;&#24459;&#23548;&#21521;PLMs&#30340;&#35757;&#32451;&#21644;&#35814;&#32454;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#20004;&#20010;&#22312;LeXFiles&#19978;&#35757;&#32451;&#30340;&#26032;&#30340;&#27861;&#24459;PLMs&#65292;&#24182;&#22312;LegalLAMA&#21644;LexGLUE&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#30456;&#20851;&#27861;&#24459;&#20027;&#39064;&#20013;&#65292;&#25506;&#38024;&#24615;&#33021;&#19982;&#19978;&#28216;&#24615;&#33021;&#24378;&#30456;&#20851;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#19979;&#28216;&#24615;&#33021;&#20027;&#35201;&#30001;&#27169;&#22411;&#30340;&#22823;&#23567;&#21644;&#20808;&#21069;&#30340;&#27861;&#24459;&#30693;&#35782;&#39537;&#21160;&#65292;&#21487;&#20197;&#36890;&#36807;&#19978;&#28216;&#21644;&#25506;&#38024;&#24615;&#33021;&#26469;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we conduct a detailed analysis on the performance of legal-oriented pre-trained language models (PLMs). We examine the interplay between their original objective, acquired knowledge, and legal language understanding capacities which we define as the upstream, probing, and downstream performance, respectively. We consider not only the models' size but also the pre-training corpora used as important dimensions in our study. To this end, we release a multinational English legal corpus (LeXFiles) and a legal knowledge probing benchmark (LegalLAMA) to facilitate training and detailed analysis of legal-oriented PLMs. We release two new legal PLMs trained on LeXFiles and evaluate them alongside others on LegalLAMA and LexGLUE. We find that probing performance strongly correlates with upstream performance in related legal topics. On the other hand, downstream performance is mainly driven by the model's size and prior legal knowledge which can be estimated by upstream and probing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#39044;&#27979;&#21333;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#30446;&#26631;&#32676;&#20307;&#30340;&#39044;&#27979;&#65292;&#27169;&#25311;&#20102;&#30446;&#26631;&#32676;&#20307;&#25104;&#21592;&#30340;&#24847;&#35265;&#65292;&#36890;&#36807;&#20351;&#29992;&#20182;&#20204;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#22312;&#32447;&#24847;&#35265;&#39044;&#27979;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#20027;&#35266;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06626</link><description>&lt;p&gt;
&#24403;&#22810;&#25968;&#20154;&#26159;&#38169;&#35823;&#30340;&#65306;&#21033;&#29992;&#26631;&#27880;&#32773;&#19981;&#19968;&#33268;&#24615;&#36827;&#34892;&#20027;&#35266;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
When the Majority is Wrong: Leveraging Annotator Disagreement for Subjective Tasks. (arXiv:2305.06626v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#39044;&#27979;&#21333;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#30446;&#26631;&#32676;&#20307;&#30340;&#39044;&#27979;&#65292;&#27169;&#25311;&#20102;&#30446;&#26631;&#32676;&#20307;&#25104;&#21592;&#30340;&#24847;&#35265;&#65292;&#36890;&#36807;&#20351;&#29992;&#20182;&#20204;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#22312;&#32447;&#24847;&#35265;&#39044;&#27979;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#20027;&#35266;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#34429;&#28982;&#36890;&#24120;&#20351;&#29992;&#26631;&#27880;&#32773;&#30340;&#22810;&#25968;&#25237;&#31080;&#26469;&#30830;&#23450;&#26631;&#31614;&#65292;&#20294;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#20027;&#35266;&#20219;&#21153;&#20013;&#65292;&#26631;&#27880;&#32773;&#20043;&#38388;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#21487;&#33021;&#21453;&#26144;&#20986;&#32676;&#20307;&#35266;&#28857;&#30340;&#24046;&#24322;&#65292;&#32780;&#19981;&#26159;&#22122;&#22768;&#12290;&#22240;&#27492;&#65292;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#19968;&#20010;&#35821;&#21477;&#26159;&#21542;&#20882;&#29359;&#20102;&#23427;&#25152;&#38024;&#23545;&#30340;&#20154;&#32676;&#65292;&#32780;&#36825;&#21487;&#33021;&#21482;&#21344;&#26631;&#27880;&#32773;&#27744;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#39044;&#27979;&#21487;&#33021;&#20855;&#26377;&#20882;&#29359;&#24615;&#25991;&#26412;&#19978;&#27599;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#30340;&#39044;&#27979;&#30446;&#26631;&#32676;&#20307;&#26469;&#27169;&#25311;&#30446;&#26631;&#32676;&#20307;&#25104;&#21592;&#30340;&#24847;&#35265;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31995;&#21015;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#21253;&#25324;&#25552;&#39640;&#20102;22&#65285;&#22312;&#39044;&#27979;&#27599;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#19978;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;33&#65285;&#22312;&#39044;&#27979;&#26631;&#27880;&#32773;&#20043;&#38388;&#26041;&#24046;&#19978;&#30340;&#24615;&#33021;&#65292;&#36825;&#25552;&#20379;&#20102;&#19979;&#28216;&#29992;&#26469;&#34913;&#37327;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#21487;&#20197;&#20351;&#29992;&#26631;&#27880;&#32773;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#21644;&#20854;&#22312;&#32447;&#24847;&#35265;&#26469;&#39044;&#27979;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Though majority vote among annotators is typically used for ground truth labels in natural language processing, annotator disagreement in tasks such as hate speech detection may reflect differences among group opinions, not noise. Thus, a crucial problem in hate speech detection is whether a statement is offensive to the demographic group that it targets, which may constitute a small fraction of the annotator pool. We construct a model that predicts individual annotator ratings on potentially offensive text and combines this information with the predicted target group of the text to model the opinions of target group members. We show gains across a range of metrics, including raising performance over the baseline by 22% at predicting individual annotators' ratings and 33% at predicting variance among annotators, which provides a method of measuring model uncertainty downstream. We find that annotators' ratings can be predicted using their demographic information and opinions on online 
&lt;/p&gt;</description></item><item><title>&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#36807;&#20110;&#37325;&#35270;&#20070;&#38754;&#35821;&#35328;&#65292;&#24212;&#35813;&#23558;&#21475;&#35821;&#20316;&#20026;&#20027;&#35201;&#20132;&#27969;&#26041;&#24335;&#32435;&#20837;&#32771;&#34385;&#65292;&#30495;&#27491;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21487;&#20197;&#36229;&#36234;&#25991;&#26412;&#65292;&#19982;&#20854;&#20182;&#35821;&#35328;&#31185;&#23398;&#26356;&#22909;&#22320;&#25972;&#21512;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#12289;&#26356;&#20687;&#20154;&#31867;&#30340;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2305.04572</link><description>&lt;p&gt;
&#35753;&#33258;&#28982;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#19968;&#37096;&#20998;
&lt;/p&gt;
&lt;p&gt;
Putting Natural in Natural Language Processing. (arXiv:2305.04572v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04572
&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#36807;&#20110;&#37325;&#35270;&#20070;&#38754;&#35821;&#35328;&#65292;&#24212;&#35813;&#23558;&#21475;&#35821;&#20316;&#20026;&#20027;&#35201;&#20132;&#27969;&#26041;&#24335;&#32435;&#20837;&#32771;&#34385;&#65292;&#30495;&#27491;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21487;&#20197;&#36229;&#36234;&#25991;&#26412;&#65292;&#19982;&#20854;&#20182;&#35821;&#35328;&#31185;&#23398;&#26356;&#22909;&#22320;&#25972;&#21512;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#12289;&#26356;&#20687;&#20154;&#31867;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35821;&#35328;&#39318;&#20808;&#26159;&#21475;&#35821;&#65292;&#20854;&#27425;&#25165;&#26159;&#20070;&#20889;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#25991;&#26412;&#26159;&#35821;&#35328;&#30340;&#19968;&#31181;&#38750;&#24120;&#26041;&#20415;&#21644;&#26377;&#25928;&#30340;&#34920;&#31034;&#26041;&#24335;&#65292;&#29616;&#20195;&#25991;&#26126;&#24050;&#23558;&#20854;&#26222;&#21450;&#12290;&#22240;&#27492;&#65292;NLP&#39046;&#22495;&#20027;&#35201;&#20851;&#27880;&#22788;&#29702;&#20070;&#38754;&#35821;&#35328;&#65292;&#24456;&#23569;&#20851;&#27880;&#21475;&#35821;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21475;&#35821;&#22788;&#29702;&#21017;&#20027;&#35201;&#38598;&#20013;&#20110;&#29420;&#31435;&#30340;&#35821;&#38899;&#22788;&#29702;&#31038;&#21306;&#65292;&#22312;&#23558;&#35821;&#38899;&#36716;&#24405;&#20026;&#25991;&#26412;&#26041;&#38754;&#19968;&#30452;&#26497;&#20026;&#21344;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#23548;&#33268;&#20102;&#35821;&#38899;&#22788;&#29702;&#21644;&#20027;&#27969;NLP&#26041;&#27861;&#20043;&#38388;&#30340;&#26377;&#21033;&#36235;&#21516;&#12290;&#26377;&#20154;&#35748;&#20026;&#65292;&#29616;&#22312;&#26159;&#23558;&#36825;&#20004;&#20010;&#39046;&#22495;&#32479;&#19968;&#36215;&#26469;&#65292;&#35748;&#30495;&#23545;&#24453;&#21475;&#35821;&#20316;&#20026;&#20154;&#31867;&#20027;&#35201;&#20132;&#27969;&#26041;&#24335;&#30340;&#26102;&#20505;&#20102;&#12290;&#30495;&#27491;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21487;&#20197;&#24102;&#26469;&#19982;&#20854;&#20182;&#35821;&#35328;&#31185;&#23398;&#26356;&#22909;&#30340;&#25972;&#21512;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#12289;&#26356;&#20687;&#20154;&#31867;&#30340;&#31995;&#32479;&#65292;&#20174;&#32780;&#21487;&#20197;&#36229;&#36234;&#25991;&#26412;&#27169;&#24335;&#36827;&#34892;&#27807;&#36890;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human language is firstly spoken and only secondarily written. Text, however, is a very convenient and efficient representation of language, and modern civilization has made it ubiquitous. Thus the field of NLP has overwhelmingly focused on processing written rather than spoken language. Work on spoken language, on the other hand, has been siloed off within the largely separate speech processing community which has been inordinately preoccupied with transcribing speech into text. Recent advances in deep learning have led to a fortuitous convergence in methods between speech processing and mainstream NLP. Arguably, the time is ripe for a unification of these two fields, and for starting to take spoken language seriously as the primary mode of human communication. Truly natural language processing could lead to better integration with the rest of language science and could lead to systems which are more data-efficient and more human-like, and which can communicate beyond the textual moda
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Vera&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#21487;&#20197;&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#20272;&#35745;&#38472;&#36848;&#24615;&#35821;&#21477;&#30340;&#21487;&#20449;&#24230;&#12290;&#22312;&#35299;&#20915;&#39564;&#35777;&#26684;&#24335;&#30340;&#24120;&#35782;&#38382;&#39064;&#26102;&#65292;Vera&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#29616;&#20102;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#33391;&#22909;&#30340;&#26631;&#23450;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2305.03695</link><description>&lt;p&gt;
Vera&#65306;&#19968;&#20010;&#29992;&#20110;&#36890;&#29992;&#24120;&#35782;&#35821;&#21477;&#21487;&#20449;&#24230;&#35780;&#20272;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements. (arXiv:2305.03695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Vera&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#21487;&#20197;&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#20272;&#35745;&#38472;&#36848;&#24615;&#35821;&#21477;&#30340;&#21487;&#20449;&#24230;&#12290;&#22312;&#35299;&#20915;&#39564;&#35777;&#26684;&#24335;&#30340;&#24120;&#35782;&#38382;&#39064;&#26102;&#65292;Vera&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#29616;&#20102;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#33391;&#22909;&#30340;&#26631;&#23450;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24403;&#20170;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#20986;&#29616;&#33618;&#35884;&#21644;&#24847;&#22806;&#30340;&#24120;&#35782;&#22833;&#36133;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22238;&#39038;&#24615;&#39564;&#35777;&#26041;&#27861;&#65292;&#21453;&#24605;LM&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;Vera&#65292;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#23427;&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#20272;&#35745;&#38472;&#36848;&#24615;&#35821;&#21477;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;19&#20010;QA&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#22823;&#35268;&#27169;&#30693;&#35782;&#24211;&#21019;&#24314;&#30340;&#32422;700&#19975;&#26465;&#24120;&#35782;&#35821;&#21477;&#20197;&#21450;&#19977;&#20010;&#35757;&#32451;&#30446;&#26631;&#30340;&#32452;&#21512;&#36827;&#34892;&#35757;&#32451;&#65292;Vera&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21306;&#20998;&#21508;&#31181;&#24120;&#35782;&#39046;&#22495;&#20013;&#30340;&#27491;&#30830;&#21644;&#38169;&#35823;&#35821;&#21477;&#12290;&#24403;&#24212;&#29992;&#20110;&#35299;&#20915;&#39564;&#35777;&#26684;&#24335;&#30340;&#24120;&#35782;&#38382;&#39064;&#26102;&#65292;Vera&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#21487;&#37325;&#29992;&#20110;&#24120;&#35782;&#39564;&#35777;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#23427;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#26631;&#23450;&#36755;&#20986;&#12290;&#25105;&#20204;&#21457;&#29616;Vera&#22312;&#36807;&#28388;LM&#29983;&#25104;&#30340;&#24120;&#35782;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#31361;&#20986;&#65292;&#21487;&#20197;&#28508;&#22312;&#22320;&#22686;&#24378;&#23427;&#20204;&#30340;&#21487;&#20449;&#24230;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the much discussed capabilities of today's language models, they are still prone to silly and unexpected commonsense failures. We consider a retrospective verification approach that reflects on the correctness of LM outputs, and introduce Vera, a general-purpose model that estimates the plausibility of declarative statements based on commonsense knowledge. Trained on ~7M commonsense statements created from 19 QA datasets and two large-scale knowledge bases, and with a combination of three training objectives, Vera is a versatile model that effectively separates correct from incorrect statements across diverse commonsense domains. When applied to solving commonsense problems in the verification format, Vera substantially outperforms existing models that can be repurposed for commonsense verification, and it further exhibits generalization capabilities to unseen tasks and provides well-calibrated outputs. We find that Vera excels at filtering LM-generated commonsense knowledge an
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ansible Wisdom&#30340;&#33258;&#28982;&#35821;&#35328;&#36716;Ansible-YAML&#20195;&#30721;&#30340;&#24037;&#20855;&#65292;&#21487;&#33258;&#21160;&#21270;&#29983;&#25104;Ansible&#33050;&#26412;&#65292;&#25552;&#39640;IT&#33258;&#21160;&#21270;&#29983;&#20135;&#21147;&#65292;&#24182;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#36798;&#21040;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.02783</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25216;&#26415;&#20219;&#21153;&#20013;&#33258;&#21160;&#29983;&#25104;YAML&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
Automated Code generation for Information Technology Tasks in YAML through Large Language Models. (arXiv:2305.02783v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02783
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ansible Wisdom&#30340;&#33258;&#28982;&#35821;&#35328;&#36716;Ansible-YAML&#20195;&#30721;&#30340;&#24037;&#20855;&#65292;&#21487;&#33258;&#21160;&#21270;&#29983;&#25104;Ansible&#33050;&#26412;&#65292;&#25552;&#39640;IT&#33258;&#21160;&#21270;&#29983;&#20135;&#21147;&#65292;&#24182;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#36798;&#21040;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#19981;&#26029;&#25552;&#21319;&#65292;&#22312;&#36890;&#29992;&#32534;&#31243;&#35821;&#35328;&#26041;&#38754;&#30340;&#21463;&#30410;&#26368;&#22823;&#65292;&#32780;&#38024;&#23545;IT&#33258;&#21160;&#21270;&#31561;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;Ansible-YAML&#30340;&#29983;&#25104;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ansible Wisdom&#30340;&#33258;&#28982;&#35821;&#35328;&#36716;Ansible-YAML&#20195;&#30721;&#30340;&#24037;&#20855;&#65292;&#26088;&#22312;&#25552;&#39640;IT&#33258;&#21160;&#21270;&#29983;&#20135;&#21147;&#12290;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#21253;&#21547;Ansible-YAML&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25193;&#23637;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#36824;&#24320;&#21457;&#20102;&#20004;&#20010;&#29992;&#20110;&#25429;&#25417;&#27492;&#39046;&#22495;&#29305;&#24449;&#30340;YAML&#21644;Ansible&#24615;&#33021;&#25351;&#26631;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Ansible Wisdom&#21487;&#20197;&#31934;&#30830;&#22320;&#20174;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#20013;&#29983;&#25104;Ansible&#33050;&#26412;&#65292;&#24182;&#19988;&#20854;&#24615;&#33021;&#21487;&#19982;&#29616;&#26377;&#25216;&#26415;&#30340;&#29366;&#24577;&#30456;&#23218;&#32654;&#25110;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent improvement in code generation capabilities due to the use of large language models has mainly benefited general purpose programming languages. Domain specific languages, such as the ones used for IT Automation, have received far less attention, despite involving many active developers and being an essential component of modern cloud platforms. This work focuses on the generation of Ansible-YAML, a widely used markup language for IT Automation. We present Ansible Wisdom, a natural-language to Ansible-YAML code generation tool, aimed at improving IT automation productivity. Ansible Wisdom is a transformer-based model, extended by training with a new dataset containing Ansible-YAML. We also develop two novel performance metrics for YAML and Ansible to capture the specific characteristics of this domain. Results show that Ansible Wisdom can accurately generate Ansible script from natural language prompts with performance comparable or better than existing state of the art code 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20803;&#25512;&#29702;&#30340;Multi-Chain Reasoning (MCR)&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26816;&#26597;&#22810;&#20010;&#25512;&#29702;&#38142;&#65292;&#28151;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#20449;&#24687;&#24182;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#20107;&#23454;&#65292;&#20174;&#32780;&#36229;&#36234;&#22810;&#38142;&#24605;&#32500;&#65292;&#35299;&#20915;&#22810;&#36339;QA&#38382;&#39064;&#12290; &#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MCR&#32988;&#36807;&#22810;&#20010;&#24378;&#22522;&#32447;&#65292;&#35299;&#37322;&#36136;&#37327;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.13007</link><description>&lt;p&gt;
&#36229;&#36234;&#22810;&#38142;&#24605;&#32500;&#65306;&#22522;&#20110;&#20803;&#25512;&#29702;&#30340;&#38382;&#39064;&#35299;&#31572;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Answering Questions by Meta-Reasoning over Multiple Chains of Thought. (arXiv:2304.13007v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20803;&#25512;&#29702;&#30340;Multi-Chain Reasoning (MCR)&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26816;&#26597;&#22810;&#20010;&#25512;&#29702;&#38142;&#65292;&#28151;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#20449;&#24687;&#24182;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#20107;&#23454;&#65292;&#20174;&#32780;&#36229;&#36234;&#22810;&#38142;&#24605;&#32500;&#65292;&#35299;&#20915;&#22810;&#36339;QA&#38382;&#39064;&#12290; &#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MCR&#32988;&#36807;&#22810;&#20010;&#24378;&#22522;&#32447;&#65292;&#35299;&#37322;&#36136;&#37327;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22810;&#36339;&#38382;&#39064;&#35299;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#36890;&#24120;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#24605;&#32771;&#27493;&#39588;&#65288;CoT&#65289;&#65292;&#28982;&#21518;&#25165;&#24471;&#20986;&#26368;&#32456;&#31572;&#26696;&#12290;&#36890;&#24120;&#26469;&#35828;&#65292;&#22810;&#20010;&#38142;&#26465;&#34987;&#25277;&#26679;&#24182;&#36890;&#36807;&#26368;&#32456;&#31572;&#26696;&#30340;&#25237;&#31080;&#26426;&#21046;&#36827;&#34892;&#32858;&#21512;&#65292;&#20294;&#20013;&#38388;&#27493;&#39588;&#26412;&#36523;&#34987;&#20002;&#24323;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#32771;&#34385;&#38142;&#20043;&#38388;&#30340;&#20013;&#38388;&#27493;&#39588;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#19981;&#25552;&#20379;&#39044;&#27979;&#31572;&#26696;&#30340;&#32479;&#19968;&#35299;&#37322;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20803;&#25512;&#29702;&#30340; Multi-Chain Reasoning (MCR) &#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#36229;&#36234;&#22810;&#20010;&#24605;&#32771;&#38142;&#65292;&#32780;&#19981;&#26159;&#32858;&#21512;&#22238;&#31572;&#12290;MCR&#26816;&#26597;&#19981;&#21516;&#30340;&#25512;&#29702;&#38142;&#65292;&#28151;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#20449;&#24687;&#24182;&#36873;&#25321;&#22312;&#29983;&#25104;&#35299;&#37322;&#21644;&#39044;&#27979;&#31572;&#26696;&#26102;&#26368;&#30456;&#20851;&#30340;&#20107;&#23454;&#12290;MCR&#22312;7&#20010;&#22810;&#36339;QA&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#24378;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;MCR&#30340;&#35299;&#37322;&#20855;&#26377;&#39640;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern systems for multi-hop question answering (QA) typically break questions into a sequence of reasoning steps, termed chain-of-thought (CoT), before arriving at a final answer. Often, multiple chains are sampled and aggregated through a voting mechanism over the final answers, but the intermediate steps themselves are discarded. While such approaches improve performance, they do not consider the relations between intermediate steps across chains and do not provide a unified explanation for the predicted answer. We introduce Multi-Chain Reasoning (MCR), an approach which prompts large language models to meta-reason over multiple chains of thought, rather than aggregating their answers. MCR examines different reasoning chains, mixes information between them and selects the most relevant facts in generating an explanation and predicting the answer. MCR outperforms strong baselines on 7 multi-hop QA datasets. Moreover, our analysis reveals that MCR explanations exhibit high quality, en
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT-4&#22312;&#25918;&#23556;&#32959;&#30244;&#23398;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#25104;&#32489;&#26174;&#31034;&#20986;&#23427;&#22312;&#21307;&#23398;&#32771;&#35797;&#19978;&#26377;&#24456;&#22823;&#30340;&#20248;&#21183;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#21478;&#22806;&#65292;ChatGPT-4 &#22312;&#25918;&#23556;&#32959;&#30244;&#23398;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#39592;&#39612;&#21644;&#36719;&#32452;&#32455;&#20197;&#21450;&#22919;&#31185;&#26041;&#38754;&#26377;&#24453;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2304.11957</link><description>&lt;p&gt;
&#22522;&#20110;ChatGPT-4&#30340;ACR&#25918;&#23556;&#32959;&#30244;&#20869;&#31185;&#65288;TXIT&#65289;&#32771;&#35797;&#21644;Red Journal Gray Zone&#26696;&#20363;&#30340;&#22522;&#20934;&#27979;&#35797;&#65306;AI&#36741;&#21161;&#21307;&#23398;&#25945;&#32946;&#21644;&#25918;&#23556;&#32959;&#30244;&#27835;&#30103;&#20915;&#31574;&#30340;&#28508;&#21147;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Benchmarking ChatGPT-4 on ACR Radiation Oncology In-Training (TXIT) Exam and Red Journal Gray Zone Cases: Potentials and Challenges for AI-Assisted Medical Education and Decision Making in Radiation Oncology. (arXiv:2304.11957v2 [physics.med-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT-4&#22312;&#25918;&#23556;&#32959;&#30244;&#23398;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#25104;&#32489;&#26174;&#31034;&#20986;&#23427;&#22312;&#21307;&#23398;&#32771;&#35797;&#19978;&#26377;&#24456;&#22823;&#30340;&#20248;&#21183;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#21478;&#22806;&#65292;ChatGPT-4 &#22312;&#25918;&#23556;&#32959;&#30244;&#23398;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#39592;&#39612;&#21644;&#36719;&#32452;&#32455;&#20197;&#21450;&#22919;&#31185;&#26041;&#38754;&#26377;&#24453;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#19978;&#30340;&#25945;&#32946;&#21644;&#20915;&#31574;&#26041;&#38754;&#30340;&#28508;&#21147;&#24050;&#32463;&#24471;&#21040;&#35777;&#26126;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#32654;&#22269;&#21307;&#23398;&#35768;&#21487;&#32771;&#35797;&#65288;USMLE&#65289;&#21644;MedQA&#32771;&#35797;&#31561;&#21307;&#23398;&#32771;&#35797;&#20013;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#32489;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT-4&#22312;&#25918;&#23556;&#32959;&#30244;&#23398;&#19987;&#19994;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;&#20351;&#29992;&#20102;&#31532;38&#23626;&#32654;&#22269;&#25918;&#23556;&#23398;&#38498;&#65288;ACR&#65289;&#25918;&#23556;&#32959;&#30244;&#20869;&#31185;&#65288;TXIT&#65289;&#32771;&#35797;&#21644;2022&#24180;&#30340;Red Journal Gray Zone&#26696;&#20363;&#12290;&#22522;&#20110;TXIT&#32771;&#35797;&#65292;ChatGPT-4&#22312;&#25918;&#23556;&#32959;&#30244;&#23398;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;ACR&#30693;&#35782;&#39046;&#22495;&#20013;&#30340;&#39592;&#39612;&#21644;&#36719;&#32452;&#32455;&#20197;&#21450;&#22919;&#31185;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#20020;&#24202;&#36335;&#24452;&#26041;&#38754;&#65292;ChatGPT-4&#22312;2022&#24180;&#30340;Red Journal Gray Zone&#26696;&#20363;&#20013;&#34920;&#29616;&#36739;&#22909;&#65292;&#20855;&#26377;70.65&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;ChatGPT-4&#65289;&#36827;&#34892;&#25918;&#23556;&#32959;&#30244;&#23398;AI&#36741;&#21161;&#21307;&#23398;&#25945;&#32946;&#21644;&#20915;&#31574;&#21046;&#23450;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential of large language models in medicine for education and decision making purposes has been demonstrated as they achieve decent scores on medical exams such as the United States Medical Licensing Exam (USMLE) and the MedQA exam. In this work, we evaluate the performance of ChatGPT-4 in the specialized field of radiation oncology using the 38th American College of Radiology (ACR) radiation oncology in-training (TXIT) exam and the 2022 red journal gray zone cases. For the TXIT exam, ChatGPT-3.5 and ChatGPT-4 have achieved the scores of 63.65% and 74.57%, respectively, highlighting the advantage of the latest ChatGPT-4 model. Based on the TXIT exam, ChatGPT-4's strong and weak areas in radiation oncology are identified to some extent. Specifically, ChatGPT-4 demonstrates good knowledge of statistics, CNS &amp; eye, pediatrics, biology, and physics but has limitations in bone &amp; soft tissue and gynecology, as per the ACR knowledge domain. Regarding clinical care paths, ChatGPT-4 perf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#27169;&#24335;&#20026;&#23548;&#21521;&#30340;&#36127;&#36131;&#20219;AI-by-design&#21442;&#32771;&#26550;&#26500;&#65292;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#31561;&#20851;&#38190;&#35774;&#35745;&#20803;&#32032;&#12290;</title><link>http://arxiv.org/abs/2304.11090</link><description>&lt;p&gt;
&#22312;ChatGPT&#26102;&#20195;&#36808;&#21521;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#30340;&#21442;&#32771;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Towards Responsible AI in the Era of ChatGPT: A Reference Architecture for Designing Foundation Model-based AI Systems. (arXiv:2304.11090v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#27169;&#24335;&#20026;&#23548;&#21521;&#30340;&#36127;&#36131;&#20219;AI-by-design&#21442;&#32771;&#26550;&#26500;&#65292;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#31561;&#20851;&#38190;&#35774;&#35745;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#12289;Bard&#21644;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#25512;&#20986;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#24341;&#36215;&#20102;&#24040;&#22823;&#20851;&#27880;&#12290;&#22522;&#30784;&#27169;&#22411;&#23558;&#25104;&#20026;&#26410;&#26469;&#22823;&#22810;&#25968;AI&#31995;&#32479;&#30340;&#22522;&#30784;&#26500;&#24314;&#22359;&#30340;&#36235;&#21183;&#27491;&#22312;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#23558;&#22522;&#30784;&#27169;&#22411;&#32435;&#20837;AI&#31995;&#32479;&#24341;&#21457;&#20102;&#23545;&#36127;&#36131;&#20219;AI&#30340;&#37325;&#22823;&#20851;&#27880;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#40657;&#21283;&#23376;&#24615;&#36136;&#21644;&#24555;&#36895;&#21457;&#23637;&#30340;&#36229;&#32423;&#26234;&#33021;&#24341;&#36215;&#30340;&#12290;&#27492;&#22806;&#65292;&#22522;&#30784;&#27169;&#22411;&#30340;&#22686;&#38271;&#33021;&#21147;&#26368;&#32456;&#21487;&#33021;&#20250;&#21534;&#22124;AI&#31995;&#32479;&#30340;&#20854;&#20182;&#32452;&#20214;&#65292;&#24341;&#20837;&#26550;&#26500;&#35774;&#35745;&#20013;&#30340;&#36816;&#21160;&#36793;&#30028;&#21644;&#25509;&#21475;&#28436;&#21464;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#27169;&#24335;&#20026;&#23548;&#21521;&#30340;&#36127;&#36131;&#20219;AI-by-design&#21442;&#32771;&#26550;&#26500;&#65292;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#12290;&#29305;&#21035;&#22320;&#65292;&#26412;&#25991;&#39318;&#20808;&#21576;&#29616;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#22312;&#26550;&#26500;&#28436;&#36827;&#26041;&#38754;&#30340;&#21457;&#23637;&#65292;&#20174;"&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#36830;&#25509;&#22120;"&#21040;"&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#21333;&#29255;&#26426;&#26680;"&#12290;&#28982;&#21518;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#21442;&#32771;&#26550;&#26500;&#65292;&#21253;&#25324;&#20116;&#20010;&#31867;&#21035;&#30340;&#27169;&#24335;&#65292;&#37325;&#28857;&#20851;&#27880;&#20851;&#38190;&#35774;&#35745;&#20803;&#32032;&#65292;&#20363;&#22914;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#21442;&#32771;&#26550;&#26500;&#20026;&#35774;&#35745;&#36127;&#36131;&#20219;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#25552;&#20379;&#20102;&#31995;&#32479;&#21270;&#21644;&#36879;&#26126;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The release of ChatGPT, Bard, and other large language model (LLM)-based chatbots has drawn huge attention on foundations models worldwide. There is a growing trend that foundation models will serve as the fundamental building blocks for most of the future AI systems. However, incorporating foundation models in AI systems raises significant concerns about responsible AI due to their black box nature and rapidly advancing super-intelligence. Additionally, the foundation model's growing capabilities can eventually absorb the other components of AI systems, introducing the moving boundary and interface evolution challenges in architecture design. To address these challenges, this paper proposes a pattern-oriented responsible-AI-by-design reference architecture for designing foundation model-based AI systems. Specially, the paper first presents an architecture evolution of AI systems in the era of foundation models, from "foundation-model-as-a-connector" to "foundation-model-as-a-monolithi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36827;&#34892;&#25991;&#23398;&#27573;&#33853;&#32763;&#35793;&#26102;&#20250;&#21033;&#29992;&#26356;&#22810;&#30340;&#25991;&#26723;&#32423;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#20943;&#23569;&#20851;&#38190;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#19982;&#19978;&#19979;&#25991;&#21644;&#24847;&#20041;&#30456;&#20851;&#30340;&#38169;&#35823;&#20173;&#28982;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2304.03245</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#23398;&#32763;&#35793;&#20013;&#39640;&#25928;&#21033;&#29992;&#25991;&#26723;&#32423;&#19978;&#19979;&#25991;&#65292;&#20294;&#20851;&#38190;&#38169;&#35823;&#20173;&#28982;&#23384;&#22312;
&lt;/p&gt;
&lt;p&gt;
Large language models effectively leverage document-level context for literary translation, but critical errors persist. (arXiv:2304.03245v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03245
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36827;&#34892;&#25991;&#23398;&#27573;&#33853;&#32763;&#35793;&#26102;&#20250;&#21033;&#29992;&#26356;&#22810;&#30340;&#25991;&#26723;&#32423;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#20943;&#23569;&#20851;&#38190;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#19982;&#19978;&#19979;&#25991;&#21644;&#24847;&#20041;&#30456;&#20851;&#30340;&#38169;&#35823;&#20173;&#28982;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#21477;&#23376;&#32423;&#21035;&#30340;&#32763;&#35793;&#25968;&#25454;&#38598;&#19978;&#19982;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#30456;&#24403;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#27573;&#33853;&#21644;&#25991;&#26723;&#32763;&#35793;&#26041;&#38754;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#25506;&#31350;&#65292;&#22240;&#20026;&#36825;&#20123;&#29615;&#22659;&#19979;&#30340;&#35780;&#20272;&#20195;&#20215;&#39640;&#19988;&#22256;&#38590;&#12290;&#36890;&#36807;&#19968;&#39033;&#20005;&#35880;&#30340;&#20154;&#24037;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35201;&#27714;Gpt-3.5&#65288;text-davinci-003&#65289;LLM&#23558;&#25972;&#20010;&#25991;&#23398;&#27573;&#33853;&#65288;&#20363;&#22914;&#65292;&#20174;&#23567;&#35828;&#20013;&#65289;&#36827;&#34892;&#32763;&#35793;&#30340;&#32467;&#26524;&#27604;&#26631;&#20934;&#30340;&#36880;&#21477;&#32763;&#35793;&#22312;18&#20010;&#35821;&#35328;&#23545;&#65288;&#20363;&#22914;&#65292;&#26085;&#35821;&#12289;&#27874;&#20848;&#35821;&#21644;&#33521;&#35821;&#30340;&#32763;&#35793;&#65289;&#19978;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#38656;&#35201;&#32422;350&#20010;&#23567;&#26102;&#30340;&#27880;&#37322;&#21644;&#20998;&#26512;&#24037;&#20316;&#65292;&#36890;&#36807;&#32856;&#35831;&#29087;&#32451;&#25484;&#25569;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#30340;&#35793;&#32773;&#65292;&#24182;&#35201;&#27714;&#20182;&#20204;&#25552;&#20379;&#36328;&#24230;&#32423;&#21035;&#30340;&#38169;&#35823;&#27880;&#37322;&#20197;&#21450;&#21738;&#31181;&#31995;&#32479;&#30340;&#32763;&#35793;&#26356;&#22909;&#30340;&#20559;&#22909;&#21028;&#26029;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#31687;&#31456;&#32423;&#21035;&#30340;LLM&#32763;&#35793;&#22312;&#25991;&#23398;&#27573;&#33853;&#30340;&#32763;&#35793;&#20013;&#20986;&#29616;&#30340;&#20851;&#38190;&#38169;&#35823;&#26356;&#23569;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#19982;&#19978;&#19979;&#25991;&#21644;&#24847;&#20041;&#30456;&#20851;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are competitive with the state of the art on a wide range of sentence-level translation datasets. However, their ability to translate paragraphs and documents remains unexplored because evaluation in these settings is costly and difficult. We show through a rigorous human evaluation that asking the Gpt-3.5 (text-davinci-003) LLM to translate an entire literary paragraph (e.g., from a novel) at once results in higher-quality translations than standard sentence-by-sentence translation across 18 linguistically-diverse language pairs (e.g., translating into and out of Japanese, Polish, and English). Our evaluation, which took approximately 350 hours of effort for annotation and analysis, is conducted by hiring translators fluent in both the source and target language and asking them to provide both span-level error annotations as well as preference judgments of which system's translations are better. We observe that discourse-level LLM translators commit fewer 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#24773;&#24863;&#20316;&#20026;&#25991;&#23398;&#25991;&#26412;&#24773;&#32490;&#30340;&#20195;&#29702;&#65292;&#24182;&#33021;&#36890;&#36807;&#25193;&#23637;&#24773;&#24863;&#35789;&#20856;&#22312;&#32771;&#34385;&#25991;&#26412;&#35821;&#20041;&#36716;&#31227;&#21644;&#39046;&#22495;&#30340;&#21069;&#25552;&#19979;&#65292;&#25552;&#20379;&#36817;&#26399;&#21644;&#29616;&#20195;&#20998;&#26512;&#30340;&#23454;&#38469;&#21487;&#34892;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.02894</link><description>&lt;p&gt;
&#22522;&#20110;&#24773;&#24863;&#30340;&#25991;&#23398;&#24773;&#32490;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Affect as a proxy for literary mood. (arXiv:2304.02894v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02894
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#24773;&#24863;&#20316;&#20026;&#25991;&#23398;&#25991;&#26412;&#24773;&#32490;&#30340;&#20195;&#29702;&#65292;&#24182;&#33021;&#36890;&#36807;&#25193;&#23637;&#24773;&#24863;&#35789;&#20856;&#22312;&#32771;&#34385;&#25991;&#26412;&#35821;&#20041;&#36716;&#31227;&#21644;&#39046;&#22495;&#30340;&#21069;&#25552;&#19979;&#65292;&#25552;&#20379;&#36817;&#26399;&#21644;&#29616;&#20195;&#20998;&#26512;&#30340;&#23454;&#38469;&#21487;&#34892;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24773;&#24863;&#20316;&#20026;&#25991;&#23398;&#25991;&#26412;&#24773;&#32490;&#30340;&#20195;&#29702;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#35745;&#31639;&#24773;&#24863;&#19982;&#26816;&#27979;&#24773;&#32490;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#20174;&#26041;&#27861;&#35770;&#30340;&#35282;&#24230;&#65292;&#25105;&#20204;&#21033;&#29992;&#24773;&#24863;&#35789;&#23884;&#20837;&#26469;&#35266;&#23519;&#19981;&#21516;&#25991;&#26412;&#27573;&#33853;&#20013;&#30340;&#24773;&#24863;&#20998;&#24067;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#24773;&#24863;&#35789;&#20856;&#65292;&#32771;&#34385;&#20102;&#35821;&#20041;&#36716;&#31227;&#21644;&#25991;&#26412;&#39046;&#22495;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19982;&#24403;&#20195;&#21644;&#29616;&#20195;&#23450;&#24615;&#20998;&#26512;&#23494;&#20999;&#21305;&#37197;&#30340;&#29616;&#23454;&#19990;&#30028;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to use affect as a proxy for mood in literary texts. In this study, we explore the differences in computationally detecting tone versus detecting mood. Methodologically we utilize affective word embeddings to look at the affective distribution in different text segments. We also present a simple yet efficient and effective method of enhancing emotion lexicons to take both semantic shift and the domain of the text into account producing real-world congruent results closely matching both contemporary and modern qualitative analyses.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#22312;Google&#12289;ChatGPT&#12289;&#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#19978;&#65292;&#25628;&#32034;&#32467;&#26524;&#21463;&#38480;&#20110;&#35821;&#35328;&#65292;&#21453;&#26144;&#20102;&#19982;&#22797;&#26434;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#65292;&#32570;&#20047;&#36328;&#25991;&#21270;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2303.16281</link><description>&lt;p&gt;
&#22823;&#35937;&#30340;&#36879;&#35270;&#38236;&#65306;&#35843;&#26597;&#35895;&#27468;&#12289;ChatGPT&#12289;&#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#19978;&#30340;&#35821;&#35328;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
A Perspectival Mirror of the Elephant: Investigating Language Bias on Google, ChatGPT, Wikipedia, and YouTube. (arXiv:2303.16281v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16281
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#22312;Google&#12289;ChatGPT&#12289;&#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#19978;&#65292;&#25628;&#32034;&#32467;&#26524;&#21463;&#38480;&#20110;&#35821;&#35328;&#65292;&#21453;&#26144;&#20102;&#19982;&#22797;&#26434;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#65292;&#32570;&#20047;&#36328;&#25991;&#21270;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#35895;&#27468;&#25628;&#32034;&#8220;&#20174;&#22810;&#20010;&#35282;&#24230;&#33719;&#21462;&#20449;&#24687;&#65292;&#20197;&#20415;&#20320;&#21487;&#20197;&#24418;&#25104;&#33258;&#24049;&#23545;&#19990;&#30028;&#30340;&#29702;&#35299;&#8221;&#30340;&#20219;&#21153;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#35895;&#27468;&#21450;&#20854;&#26368;&#31361;&#20986;&#30340;&#25628;&#32034;&#32467;&#26524; - &#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#65292;&#20165;&#21453;&#26144;&#19982;&#8220;&#20315;&#25945;&#8221;&#12289;&#8220;&#33258;&#30001;&#20027;&#20041;&#8221;&#12289;&#8220;&#27542;&#27665;&#21270;&#8221;&#12289;&#8220;&#20234;&#26391;&#8221;&#21644;&#8220;&#32654;&#22269;&#8221;&#31561;&#22797;&#26434;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#12290;&#31616;&#21333;&#22320;&#35828;&#65292;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#30456;&#21516;&#25628;&#32034;&#20013;&#65292;&#23427;&#20204;&#20197;&#19981;&#21516;&#31243;&#24230;&#21576;&#29616;&#19981;&#21516;&#30340;&#20449;&#24687;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#35821;&#35328;&#20559;&#35265;&#8221;&#65289;&#65292;&#32780;&#19981;&#26159;&#21576;&#29616;&#22797;&#26434;&#20027;&#39064;&#30340;&#20840;&#29699;&#22270;&#29255;&#12290;&#25105;&#20204;&#30340;&#22312;&#32447;&#25628;&#32034;&#20351;&#25105;&#20204;&#25104;&#20026;&#35866;&#35821;&#20013;&#30340;&#30450;&#20154;&#65292;&#20165;&#35302;&#25720;&#23567;&#35937;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#19981;&#30693;&#36947;&#20854;&#20182;&#25991;&#21270;&#30340;&#35270;&#35282;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#29992;&#20110;&#25628;&#32034;&#30340;&#35821;&#35328;&#26368;&#32456;&#25104;&#20026;&#20419;&#36827;&#26412;&#26063;&#20013;&#24515;&#20027;&#20041;&#35266;&#28857;&#30340;&#25991;&#21270;&#36807;&#28388;&#22120;&#65292;&#20854;&#20013;&#19968;&#20010;&#20154;&#26681;&#25454;&#33258;&#24049;&#30340;&#25991;&#21270;&#35780;&#20272;&#20854;&#20182;&#20154;&#25110;&#24605;&#24819;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;ChatGPT&#20013;&#28145;&#28145;&#23884;&#20837;&#20102;&#35821;&#35328;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrary to Google Search's mission of delivering information from "many angles so you can form your own understanding of the world," we find that Google and its most prominent returned results -- Wikipedia and YouTube, simply reflect the narrow set of cultural stereotypes tied to the search language for complex topics like "Buddhism," "Liberalism," "colonization," "Iran" and "America." Simply stated, they present, to varying degrees, distinct information across the same search in different languages (we call it 'language bias'). Instead of presenting a global picture of a complex topic, our online searches turn us into the proverbial blind person touching a small portion of an elephant, ignorant of the existence of other cultural perspectives. The language we use to search ends up as a cultural filter to promote ethnocentric views, where a person evaluates other people or ideas based on their own culture. We also find that language bias is deeply embedded in ChatGPT. As it is primaril
&lt;/p&gt;</description></item><item><title>DeltaScore&#21033;&#29992;&#24046;&#20998;&#25200;&#21160;&#26469;&#35780;&#20272;&#25925;&#20107;&#29983;&#25104;&#30340;&#32454;&#31890;&#24230;&#26041;&#38754;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#25925;&#20107;&#22312;&#29305;&#23450;&#26041;&#38754;&#25200;&#21160;&#21069;&#21518;&#30340;&#21487;&#33021;&#24615;&#24046;&#24322;&#26469;&#34913;&#37327;&#24433;&#21709;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25925;&#20107;&#39046;&#22495;&#20013;&#24471;&#21040;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.08991</link><description>&lt;p&gt;
DeltaScore: &#21033;&#29992;&#24046;&#20998;&#25200;&#21160;&#35780;&#20215;&#25925;&#20107;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DeltaScore: Evaluating Story Generation with Differentiating Perturbations. (arXiv:2303.08991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08991
&lt;/p&gt;
&lt;p&gt;
DeltaScore&#21033;&#29992;&#24046;&#20998;&#25200;&#21160;&#26469;&#35780;&#20272;&#25925;&#20107;&#29983;&#25104;&#30340;&#32454;&#31890;&#24230;&#26041;&#38754;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#25925;&#20107;&#22312;&#29305;&#23450;&#26041;&#38754;&#25200;&#21160;&#21069;&#21518;&#30340;&#21487;&#33021;&#24615;&#24046;&#24322;&#26469;&#34913;&#37327;&#24433;&#21709;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25925;&#20107;&#39046;&#22495;&#20013;&#24471;&#21040;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#21508;&#31181;&#35780;&#20215;&#25351;&#26631;&#23384;&#22312;&#65292;&#20294;&#23545;&#20110;&#25925;&#20107;&#29983;&#25104;&#30340;&#23454;&#29992;&#24615;&#26377;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#19981;&#24378;&#65292;&#20063;&#19981;&#33021;&#27979;&#37327;&#32454;&#31890;&#24230;&#30340;&#25925;&#20107;&#26041;&#38754;&#65292;&#20363;&#22914;&#27969;&#30021;&#24230;&#19982;&#30456;&#20851;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#26088;&#22312;&#35780;&#20272;&#25972;&#20307;&#29983;&#25104;&#36136;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;DeltaScore&#65292;&#19968;&#31181;&#21033;&#29992;&#25200;&#21160;&#26469;&#35780;&#20272;&#32454;&#31890;&#24230;&#25925;&#20107;&#26041;&#38754;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22522;&#20110;&#36825;&#26679;&#30340;&#20551;&#35774;&#65306;&#25925;&#20107;&#22312;&#29305;&#23450;&#26041;&#38754;&#34920;&#29616;&#24471;&#36234;&#22909;&#65288;&#20363;&#22914;&#27969;&#30021;&#24230;&#65289;&#65292;&#23427;&#23601;&#20250;&#21463;&#21040;&#29305;&#23450;&#25200;&#21160;&#65288;&#20363;&#22914;&#24341;&#20837;&#38169;&#21035;&#23383;&#65289;&#30340;&#24433;&#21709;&#36234;&#22823;&#12290;&#20026;&#20102;&#34913;&#37327;&#24433;&#21709;&#65292;&#25105;&#20204;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#25200;&#21160;&#21069;&#21518;&#25925;&#20107;&#30340;&#21487;&#33021;&#24615;&#24046;&#24322;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25925;&#20107;&#39046;&#22495;&#20013;&#20351;&#29992;DeltaScore&#35780;&#20272;&#20102;&#22522;&#20110;&#29366;&#24577;&#30340;&#26368;&#26032;&#27169;&#22411;&#21644;&#20256;&#32479;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#25351;&#26631;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#19982;&#20154;&#31867;&#22312;&#20116;&#20010;&#32454;&#31890;&#24230;&#25925;&#20107;&#26041;&#38754;&#30340;&#21028;&#26029;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various evaluation metrics exist for natural language generation tasks, but they have limited utility for story generation since they generally do not correlate well with human judgments and do not measure fine-grained story aspects, such as fluency versus relatedness, as they are intended to assess overall generation quality. In this paper, we propose deltascore, an approach that utilizes perturbation to evaluate fine-grained story aspects. Our core idea is based on the hypothesis that the better the story performs in a specific aspect (e.g., fluency), the more it will be affected by a particular perturbation (e.g., introducing typos). To measure the impact, we calculate the likelihood difference between the pre- and post-perturbation stories using a language model. We evaluate deltascore against state-of-the-art model-based and traditional similarity-based metrics across multiple story domains, and investigate its correlation with human judgments on five fine-grained story aspects: f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;MUX-PLMs&#30340;&#39640;&#21534;&#21520;&#37327;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#25968;&#25454;&#22797;&#29992;&#35757;&#32451;&#65292;&#21487;&#29992;&#20110;&#39640;&#24615;&#33021;&#30340;MIMO&#26679;&#24335;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2302.12441</link><description>&lt;p&gt;
MUX-PLMs&#65306;&#39640;&#21534;&#21520;&#37327;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#22797;&#29992;
&lt;/p&gt;
&lt;p&gt;
MUX-PLMs: Data Multiplexing for High-throughput Language Models. (arXiv:2302.12441v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12441
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;MUX-PLMs&#30340;&#39640;&#21534;&#21520;&#37327;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#25968;&#25454;&#22797;&#29992;&#35757;&#32451;&#65292;&#21487;&#29992;&#20110;&#39640;&#24615;&#33021;&#30340;MIMO&#26679;&#24335;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#21644;Bard&#65289;&#30340;&#24191;&#27867;&#37319;&#29992;&#24102;&#26469;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#38656;&#27714;&#12290;&#36234;&#26469;&#36234;&#22823;&#30340;&#27169;&#22411;&#23610;&#23544;&#25152;&#38656;&#30340;&#25512;&#26029;&#25104;&#26412;&#20197;&#21450;&#30828;&#20214;&#30701;&#32570;&#65292;&#38480;&#21046;&#20102;&#32463;&#27982;&#23454;&#24800;&#30340;&#35775;&#38382;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#39640;&#21534;&#21520;&#37327;&#21644;&#39640;&#24615;&#33021;&#30340;&#25928;&#29575;&#26041;&#27861;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#31639;&#27861;&#65288;&#20363;&#22914;&#25968;&#25454;&#22797;&#29992;&#65289;&#36890;&#36807;&#23545;&#22810;&#20010;&#36755;&#20837;&#25191;&#34892;&#25512;&#26029;&#65292;&#20197;&#21333;&#20010;&#36755;&#20837;&#30340;&#25104;&#26412;&#25552;&#20379;&#20102;&#22810;&#37325;&#21534;&#21520;&#37327;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30446;&#21069;&#30340;&#34920;&#29616;&#36824;&#19981;&#36275;&#20197;&#37096;&#32626;&#22312;&#29616;&#20195;&#31995;&#32479;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;MUX-PLMs&#65292;&#19968;&#31181;&#20351;&#29992;&#25968;&#25454;&#22797;&#29992;&#35757;&#32451;&#30340;&#39640;&#21534;&#21520;&#37327;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#21487;&#20197;&#24494;&#35843;&#20219;&#20309;&#19979;&#28216;&#20219;&#21153;&#20197;&#20135;&#29983;&#39640;&#21534;&#21520;&#37327;&#21644;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26032;&#22411;&#22797;&#29992;&#21644;&#35299;&#22797;&#29992;&#27169;&#22359;&#33021;&#22815;&#26377;&#25928;&#22320;&#32416;&#32544;&#21644;&#35299;&#32544;&#36755;&#20837;&#65292;&#24182;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;MIMO&#26679;&#24335;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread adoption of large language models such as ChatGPT and Bard has led to unprecedented demand for these technologies. The burgeoning cost of inference for ever-increasing model sizes coupled with hardware shortages has limited affordable access and poses a pressing need for efficiency approaches geared towards high throughput and performance. Multi-input multi-output (MIMO) algorithms such as data multiplexing, offer a promising solution with a many-fold increase in throughput by performing inference for multiple inputs at the cost of a single input. Yet these approaches are not currently performant enough to be deployed in modern systems. We change that by developing MUX-PLMs, a class of high throughput pre-trained language models (PLMs) trained with data multiplexing, that can be fine-tuned for any downstream task to yield high-throughput high-performance. Our novel multiplexing and demultiplexing modules proficiently entangle and disentangle inputs, and enable high-perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;Active-Prompt&#65292;&#23427;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#31034;&#20363;&#25552;&#31034;&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#21516;&#20219;&#21153;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#19982;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2302.12246</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#32500;&#38142;&#20027;&#21160;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Active Prompting with Chain-of-Thought for Large Language Models. (arXiv:2302.12246v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;Active-Prompt&#65292;&#23427;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#31034;&#20363;&#25552;&#31034;&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#21516;&#20219;&#21153;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#19982;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#27169;&#26085;&#30410;&#22686;&#22823;&#65292;&#20026;&#21508;&#31181;&#38656;&#35201;&#25512;&#29702;&#30340;&#22797;&#26434;&#20219;&#21153;&#65288;&#22914;&#31639;&#26415;&#21644;&#24120;&#35782;&#25512;&#29702;&#65289;&#24102;&#26469;&#20102;&#26032;&#30340;&#33021;&#21147;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#30340;&#26377;&#25928;&#35774;&#35745;&#23545;LLMs&#20135;&#29983;&#39640;&#36136;&#37327;&#31572;&#26696;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#21035;&#26159;&#65292;&#23545;&#20110;&#22797;&#26434;&#30340;&#38382;&#31572;&#20219;&#21153;&#65292;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#31034;&#20363;&#30340;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25512;&#23548;&#25552;&#31034;&#65292;&#23427;&#22823;&#22823;&#25552;&#39640;&#20102;LLMs&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#24403;&#21069;&#30340;CoT&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#32452;&#22266;&#23450;&#30340;&#20154;&#31867;&#27880;&#37322;&#31034;&#20363;&#65292;&#36825;&#20123;&#31034;&#20363;&#19981;&#19968;&#23450;&#26159;&#19981;&#21516;&#20219;&#21153;&#30340;&#26368;&#26377;&#25928;&#31034;&#20363;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Active-Prompt&#65292;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#31034;&#20363;&#25552;&#31034;&#65288;&#20154;&#20026;&#35774;&#35745;&#30340;CoT&#25512;&#29702;&#27880;&#37322;&#65289;&#26469;&#36866;&#24212;LLMs&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#30830;&#23450;&#21738;&#20123;&#38382;&#39064;&#20174;&#20219;&#21153;&#29305;&#23450;&#26597;&#35810;&#27744;&#20013;&#27880;&#37322;&#26368;&#37325;&#35201;&#21644;&#26377;&#29992;&#12290;&#36890;&#36807;&#20511;&#37492;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20027;&#21160;&#25552;&#31034;(Acitve-Prompt)&#30340;&#26041;&#27861;&#65292;&#23558;&#26368;&#30456;&#20851;&#30340;&#38382;&#39064;&#20316;&#20026;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#28155;&#21152;&#32473;LLMs&#65292;&#20174;&#32780;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex question-and-answer tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful ones to annotate from a pool of task-specific queries. By borrowi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26816;&#39564;&#20102; ChatGPT &#22312; 25 &#20010;&#19981;&#21516;&#30340; NLP &#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#23427;&#26159;&#19968;&#20010;&#19975;&#33021;&#30340; AI &#27169;&#22411;&#65292;&#20294;&#26080;&#20851;&#32039;&#35201;&#30340;&#34920;&#29616;&#21487;&#33021;&#20250;&#23545;&#26576;&#20123;&#20219;&#21153;&#30340;&#34920;&#29616;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.10724</link><description>&lt;p&gt;
ChatGPT&#65306;&#24212;&#20184;&#21315;&#20107;&#30340;&#19975;&#33021;&#22411; AI&#65292;&#20294;&#26080;&#25152;&#19987;&#31934;
&lt;/p&gt;
&lt;p&gt;
ChatGPT: Jack of all trades, master of none. (arXiv:2302.10724v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26816;&#39564;&#20102; ChatGPT &#22312; 25 &#20010;&#19981;&#21516;&#30340; NLP &#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#23427;&#26159;&#19968;&#20010;&#19975;&#33021;&#30340; AI &#27169;&#22411;&#65292;&#20294;&#26080;&#20851;&#32039;&#35201;&#30340;&#34920;&#29616;&#21487;&#33021;&#20250;&#23545;&#26576;&#20123;&#20219;&#21153;&#30340;&#34920;&#29616;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OpenAI &#25512;&#20986;&#20102;&#32842;&#22825;&#29983;&#25104;&#39044;&#35757;&#32451; Transformer&#65288;ChatGPT&#65289;&#65292;&#38761;&#26032;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#26041;&#27861;&#12290;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#27979;&#35797; ChatGPT &#22312;&#20247;&#25152;&#21608;&#30693;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#65292;&#26469;&#35780;&#20272;&#35813;&#27169;&#22411;&#30340;&#25928;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#22823;&#22810;&#38750;&#33258;&#21160;&#21270;&#65292;&#24182;&#19988;&#35268;&#27169;&#38750;&#24120;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#22312; 25 &#20010;&#19981;&#21516;&#30340; NLP &#20219;&#21153;&#19978;&#26816;&#39564;&#20102; ChatGPT &#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#20219;&#21153;&#29978;&#33267;&#23545;&#20154;&#31867;&#32780;&#35328;&#37117;&#26159;&#20027;&#35266;&#30340;&#65292;&#20363;&#22914;&#24773;&#24863;&#20998;&#26512;&#12289;&#24773;&#32490;&#35782;&#21035;&#12289;&#25915;&#20987;&#24615;&#21644;&#31435;&#22330;&#26816;&#27979;&#12290;&#21478;&#19968;&#20123;&#20219;&#21153;&#21017;&#38656;&#35201;&#26356;&#23458;&#35266;&#30340;&#25512;&#29702;&#65292;&#22914;&#35789;&#20041;&#28040;&#27495;&#12289;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#21644;&#38382;&#31572;&#12290;&#25105;&#20204;&#36824;&#23545; GPT-4 &#27169;&#22411;&#22312;&#20116;&#20010;&#36873;&#23450;&#30340; NLP &#20219;&#21153;&#23376;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#33258;&#21160;&#21270;&#20102; ChatGPT &#21644; GPT-4 &#30340;&#24341;&#23548;&#36807;&#31243;&#65292;&#24182;&#20998;&#26512;&#20102;&#36229;&#36807; 49k &#20010;&#21709;&#24212;&#12290;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#65288;SOTA&#65289;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#19968;&#20123;&#20219;&#21153;&#19978; ChatGPT &#30340;&#24615;&#33021;&#23384;&#22312;&#19968;&#23450;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and revolutionized the approach in artificial intelligence to human-model interaction. Several publications on ChatGPT evaluation test its effectiveness on well-known natural language processing (NLP) tasks. However, the existing studies are mostly non-automated and tested on a very limited scale. In this work, we examined ChatGPT's capabilities on 25 diverse analytical NLP tasks, most of them subjective even to humans, such as sentiment analysis, emotion recognition, offensiveness, and stance detection. In contrast, the other tasks require more objective reasoning like word sense disambiguation, linguistic acceptability, and question answering. We also evaluated GPT-4 model on five selected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process and analyzed more than 49k responses. Our comparison of its results with available State-of-the-Art (SOTA) solutions showed that the average loss in quali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#32467;&#26500;&#21270;&#29289;&#20307;&#24207;&#21015;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#32534;&#30721;&#38190;&#30340;&#20540;&#30340;&#34920;&#31034;&#24182;&#33258;&#25105;&#20851;&#27880;&#36825;&#20123;&#38190;&#20197;&#23436;&#25104;&#19979;&#28216;&#20219;&#21153;&#26469;&#35299;&#20915;&#38271;&#23545;&#35937;&#24207;&#21015;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.01015</link><description>&lt;p&gt;
&#21322;&#32467;&#26500;&#21270;&#29289;&#20307;&#24207;&#21015;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Semi-Structured Object Sequence Encoders. (arXiv:2301.01015v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#32467;&#26500;&#21270;&#29289;&#20307;&#24207;&#21015;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#32534;&#30721;&#38190;&#30340;&#20540;&#30340;&#34920;&#31034;&#24182;&#33258;&#25105;&#20851;&#27880;&#36825;&#20123;&#38190;&#20197;&#23436;&#25104;&#19979;&#28216;&#20219;&#21153;&#26469;&#35299;&#20915;&#38271;&#23545;&#35937;&#24207;&#21015;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#24314;&#27169;&#21322;&#32467;&#26500;&#21270;&#23545;&#35937;&#24207;&#21015;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#20851;&#27880;&#24320;&#21457;&#36825;&#20123;&#24207;&#21015;&#30340;&#32467;&#26500;&#24863;&#30693;&#36755;&#20837;&#34920;&#31034;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#25968;&#25454;&#30340;&#20363;&#23376;&#21253;&#25324;&#29992;&#25143;&#22312;&#32593;&#31449;&#19978;&#30340;&#27963;&#21160;&#12289;&#26426;&#22120;&#26085;&#24535;&#31561;&#12290;&#30001;&#20110;&#24207;&#21015;&#38271;&#24230;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#36825;&#31181;&#25968;&#25454;&#32463;&#24120;&#34987;&#34920;&#31034;&#20026;&#19968;&#31995;&#21015;&#30340;&#38190;&#20540;&#23545;&#38598;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#37096;&#20998;&#26041;&#27861;&#65292;&#39318;&#20808;&#29420;&#31435;&#32771;&#34385;&#27599;&#20010;&#38190;&#24182;&#32534;&#30721;&#20854;&#20540;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#33258;&#25105;&#20851;&#27880;&#36825;&#20123;&#20855;&#26377;&#20540;&#24863;&#30693;&#30340;&#38190;&#34920;&#31034;&#20197;&#23436;&#25104;&#19979;&#28216;&#20219;&#21153;&#12290;&#36825;&#26679;&#65292;&#25105;&#20204;&#21487;&#20197;&#25805;&#20316;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#38271;&#30340;&#23545;&#35937;&#24207;&#21015;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#20010;&#27169;&#22359;&#20043;&#38388;&#30340;&#26032;&#22411;&#20849;&#20139;&#27880;&#24847;&#21147;&#22836;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35757;&#32451;&#35745;&#21010;&#65292;&#20132;&#26367;&#35757;&#32451;&#20004;&#20010;&#27169;&#22359;&#65292;&#26576;&#20123;&#27880;&#24847;&#21147;&#22836;&#20351;&#29992;&#20849;&#20139;&#26435;&#37325;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21457;&#29616;&#23427;&#22312;&#20960;&#20010;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we explore the task of modeling semi-structured object sequences; in particular, we focus our attention on the problem of developing a structure-aware input representation for such sequences. Examples of such data include user activity on websites, machine logs, and many others. This type of data is often represented as a sequence of sets of key-value pairs over time and can present modeling challenges due to an ever-increasing sequence length. We propose a two-part approach, which first considers each key independently and encodes a representation of its values over time; we then self-attend over these value-aware key representations to accomplish a downstream task. This allows us to operate on longer object sequences than existing methods. We introduce a novel shared-attention-head architecture between the two modules and present an innovative training schedule that interleaves the training of both modules with shared weights for some attention heads. Our experiments on
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#35760;&#21495;&#35328;&#35821;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35799;&#27468;&#29983;&#25104;&#27169;&#22411;ByGPT5&#65292;&#24182;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#38901;&#24459;&#12289;&#33410;&#24459;&#21644;&#22836;&#38901;&#31561;&#39118;&#26684;&#30340;&#35799;&#27468;&#29983;&#25104;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.10474</link><description>&lt;p&gt;
ByGPT5&#65306;&#22522;&#20110;&#31471;&#21040;&#31471;&#30340;&#12289;&#38754;&#21521;&#39118;&#26684;&#30340;&#35799;&#27468;&#29983;&#25104;&#27169;&#22411;&#65292;&#37319;&#29992;&#26080;&#35760;&#21495;&#35328;&#35821;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models. (arXiv:2212.10474v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#35760;&#21495;&#35328;&#35821;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35799;&#27468;&#29983;&#25104;&#27169;&#22411;ByGPT5&#65292;&#24182;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#38901;&#24459;&#12289;&#33410;&#24459;&#21644;&#22836;&#38901;&#31561;&#39118;&#26684;&#30340;&#35799;&#27468;&#29983;&#25104;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#35799;&#27468;&#29983;&#25104;&#31995;&#32479;&#36890;&#24120;&#38750;&#24120;&#22797;&#26434;&#65292;&#35201;&#20040;&#30001;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#31649;&#36947;&#26500;&#25104;&#65292;&#35201;&#20040;&#37319;&#29992;&#25163;&#21160;&#21019;&#24314;&#30340;&#32422;&#26463;&#26465;&#20214;&#26469;&#34701;&#20837;&#20808;&#21069;&#30340;&#30693;&#35782;&#65292;&#25110;&#20004;&#32773;&#20860;&#32780;&#26377;&#20043;&#12290;&#30456;&#21453;&#65292;&#31471;&#21040;&#31471;&#30340;&#27169;&#22411;&#19981;&#38656;&#35201;&#24314;&#27169;&#20808;&#21069;&#30340;&#30693;&#35782;&#65292;&#24182;&#19988;&#21487;&#20197;&#20165;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#35799;&#27468;&#30340;&#24494;&#22937;&#20043;&#22788;&#65292;&#20174;&#32780;&#20943;&#23569;&#38656;&#35201;&#20154;&#31867;&#30417;&#30563;&#30340;&#31243;&#24230;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20197;&#38901;&#24459;&#12289;&#33410;&#24459;&#21644;&#22836;&#38901;&#31561;&#39118;&#26684;&#20026;&#26465;&#20214;&#30340;&#31471;&#21040;&#31471;&#35799;&#27468;&#29983;&#25104;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#35299;&#20915;&#20102;&#36807;&#21435;&#23581;&#35797;&#30340;&#25968;&#25454;&#32570;&#20047;&#21644;&#19981;&#21305;&#37197;&#30340;&#35760;&#21495;&#21270;&#31639;&#27861;&#21487;&#33021;&#23384;&#22312;&#30340;&#23616;&#38480;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#39044;&#35757;&#32451;&#20102;ByGPT5&#65292;&#19968;&#31181;&#26032;&#30340;&#26080;&#35760;&#21495;&#35299;&#30721;&#22120;-&#20165;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#25105;&#20204;&#30340;&#39118;&#26684;&#26631;&#27880;&#30340;&#22823;&#22411;&#23450;&#21046;&#33521;&#35821;&#21644;&#24503;&#35821;&#22235;&#34892;&#35799;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ByGPT5&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#22914;mT5&#12289;ByT5&#12289;GPT-2&#21644;ChatGPT&#65292;&#21516;&#26102;&#20063;&#26356;&#20855;&#21442;&#25968;&#25928;&#29575;&#65292;&#24615;&#33021;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art poetry generation systems are often complex. They either consist of task-specific model pipelines, incorporate prior knowledge in the form of manually created constraints, or both. In contrast, end-to-end models would not suffer from the overhead of having to model prior knowledge and could learn the nuances of poetry from data alone, reducing the degree of human supervision required. In this work, we investigate end-to-end poetry generation conditioned on styles such as rhyme, meter, and alliteration. We identify and address lack of training data and mismatching tokenization algorithms as possible limitations of past attempts. In particular, we successfully pre-train ByGPT5, a new token-free decoder-only language model, and fine-tune it on a large custom corpus of English and German quatrains annotated with our styles. We show that ByGPT5 outperforms other models such as mT5, ByT5, GPT-2 and ChatGPT, while also being more parameter efficient and performing favorably c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#39564;&#35777;&#26041;&#27861;&#65292;&#20351;&#29992;CoT&#30340;&#32467;&#35770;&#26469;&#26500;&#24314;&#26032;&#26679;&#26412;&#24182;&#35201;&#27714;LLM&#37325;&#26032;&#39044;&#27979;&#21407;&#22987;&#26465;&#20214;&#65292;&#20197;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#23545;&#20854;&#33258;&#24049;&#30340;&#32467;&#35770;&#36827;&#34892;&#33258;&#25105;&#39564;&#35777;&#24182;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.09561</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#24102;&#26377;&#33258;&#25105;&#39564;&#35777;&#30340;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are reasoners with Self-Verification. (arXiv:2212.09561v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#39564;&#35777;&#26041;&#27861;&#65292;&#20351;&#29992;CoT&#30340;&#32467;&#35770;&#26469;&#26500;&#24314;&#26032;&#26679;&#26412;&#24182;&#35201;&#27714;LLM&#37325;&#26032;&#39044;&#27979;&#21407;&#22987;&#26465;&#20214;&#65292;&#20197;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#23545;&#20854;&#33258;&#24049;&#30340;&#32467;&#35770;&#36827;&#34892;&#33258;&#25105;&#39564;&#35777;&#24182;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#26102;&#65292;&#23427;&#38750;&#24120;&#25935;&#24863;&#20110;&#20010;&#21035;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24517;&#39035;&#35757;&#32451;&#39564;&#35777;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#33258;&#25105;&#39564;&#35777;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;CoT&#30340;&#32467;&#35770;&#20316;&#20026;&#26465;&#20214;&#26469;&#26500;&#24314;&#19968;&#20010;&#26032;&#26679;&#26412;&#65292;&#24182;&#35201;&#27714;LLM&#37325;&#26032;&#39044;&#27979;&#34987;&#25513;&#30422;&#30340;&#21407;&#22987;&#26465;&#20214;&#12290;&#25105;&#20204;&#22522;&#20110;&#20934;&#30830;&#24615;&#35745;&#31639;&#21487;&#35299;&#37322;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#26102;&#25552;&#39640;&#22810;&#20010;&#31639;&#26415;&#21644;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#24050;&#32463;&#35777;&#26126;LLM&#21487;&#20197;&#23545;&#20854;&#33258;&#24049;&#30340;&#32467;&#35770;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#33258;&#25105;&#39564;&#35777;&#24182;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#22810;&#31181;&#24102;&#26377;&#33258;&#25105;&#39564;&#35777;&#21151;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36991;&#20813;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;
When a large language model (LLM) performs complex reasoning by chain of thought (CoT), it can be highly sensitive to individual mistakes. We have had to train verifiers to address this issue. As we all know, after human inferring a conclusion, they often check it by re-verifying it, which can avoid some mistakes. We propose a new method called self-verification that uses the conclusion of the CoT as a condition to build a new sample and asks the LLM to re-predict the original conditions which be masked. We calculate an explainable verification score based on the accuracy. This method can improve the accuracy of multiple arithmetics and logical reasoning datasets when using few-shot learning. we have demonstrated that LLMs can conduct explainable self-verification of their own conclusions and achieve competitive reasoning performance. Extensive experimentals have demonstrated that our method can help multiple large language models with self-verification can avoid interference from inco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#21512;&#22810;&#20010;&#35270;&#22270;&#30340;&#20247;&#21253;&#27880;&#37322;&#26469;&#33719;&#21462;&#36719;&#26631;&#31614;&#65292;&#20174;&#32780;&#36827;&#34892;&#36328;&#39046;&#22495;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2212.09409</link><description>&lt;p&gt;
&#20174;&#20247;&#21253;&#27880;&#37322;&#20013;&#36827;&#34892;&#22810;&#35270;&#35282;&#30693;&#35782;&#33976;&#39311;&#20197;&#23454;&#29616;&#36328;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-View Knowledge Distillation from Crowd Annotations for Out-of-Domain Generalization. (arXiv:2212.09409v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#21512;&#22810;&#20010;&#35270;&#22270;&#30340;&#20247;&#21253;&#27880;&#37322;&#26469;&#33719;&#21462;&#36719;&#26631;&#31614;&#65292;&#20174;&#32780;&#36827;&#34892;&#36328;&#39046;&#22495;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#36873;&#25321;&#26377;&#25928;&#30340;&#35757;&#32451;&#20449;&#21495;&#24456;&#22256;&#38590;&#65306;&#19987;&#23478;&#27880;&#37322;&#24456;&#26114;&#36149;&#65292;&#32780;&#20247;&#21253;&#27880;&#37322;&#21487;&#33021;&#19981;&#21487;&#38752;&#12290;&#26368;&#36817;&#30340;NLP&#30740;&#31350;&#34920;&#26126;&#65292;&#20174;&#20247;&#21253;&#27880;&#37322;&#20013;&#33719;&#21462;&#26631;&#31614;&#20998;&#24067;&#30340;&#23398;&#20064;&#21487;&#20197;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#26377;&#24456;&#22810;&#33719;&#21462;&#36825;&#31181;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#20219;&#20309;&#19968;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#37117;&#21487;&#33021;&#22240;&#20219;&#21153;&#21644;&#21487;&#29992;&#20247;&#21253;&#27880;&#37322;&#37327;&#32780;&#27874;&#21160;&#65292;&#36825;&#20351;&#24471;&#20107;&#20808;&#19981;&#30693;&#36947;&#21738;&#31181;&#20998;&#24067;&#26368;&#22909;&#12290;&#26412;&#25991;&#22312;&#39046;&#22495;&#22806;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#32858;&#21512;&#29616;&#26377;&#26041;&#27861;&#20135;&#29983;&#30340;&#20998;&#24067;&#26469;&#33719;&#21462;&#26469;&#33258;&#20247;&#21253;&#27880;&#37322;&#30340;&#36719;&#26631;&#31614;&#30340;&#26032;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#21644;&#25214;&#21040;&#23427;&#20204;&#30340;Jensen-Shannon&#20013;&#24515;&#26469;&#32858;&#21512;&#20247;&#21253;&#27880;&#37322;&#30340;&#22810;&#20010;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selecting an effective training signal for tasks in natural language processing is difficult: expert annotations are expensive, and crowd-sourced annotations may not be reliable. At the same time, recent work in NLP has demonstrated that learning from a distribution over labels acquired from crowd annotations can be effective. However, there are many ways to acquire such a distribution, and the performance allotted by any one method can fluctuate based on the task and the amount of available crowd annotations, making it difficult to know a priori which distribution is best. This paper systematically analyzes this in the out-of-domain setting, adding to the NLP literature which has focused on in-domain evaluation, and proposes new methods for acquiring soft-labels from crowd-annotations by aggregating the distributions produced by existing methods. In particular, we propose to aggregate multiple-views of crowd annotations via temperature scaling and finding their Jensen-Shannon centroid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;KITMUS&#27979;&#35797;&#22871;&#20214;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#23545;&#22810;&#28304;&#30693;&#35782;&#36827;&#34892;&#25972;&#21512;&#21644;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#22312;&#27979;&#35797;&#20013;&#30340;&#26680;&#24515;&#23376;&#20219;&#21153;&#38656;&#35201;&#36827;&#34892;&#38024;&#23545;&#22810;&#20010;&#20107;&#23454;&#30340;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35768;&#22810;&#27169;&#22411;&#38590;&#20197;&#23454;&#26102;&#36827;&#34892;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2212.08192</link><description>&lt;p&gt;
KITMUS&#27979;&#35797;&#65306;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#31995;&#32479;&#20013;&#22810;&#28304;&#30693;&#35782;&#25972;&#21512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources in Natural Language Understanding Systems. (arXiv:2212.08192v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;KITMUS&#27979;&#35797;&#22871;&#20214;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#23545;&#22810;&#28304;&#30693;&#35782;&#36827;&#34892;&#25972;&#21512;&#21644;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#22312;&#27979;&#35797;&#20013;&#30340;&#26680;&#24515;&#23376;&#20219;&#21153;&#38656;&#35201;&#36827;&#34892;&#38024;&#23545;&#22810;&#20010;&#20107;&#23454;&#30340;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35768;&#22810;&#27169;&#22411;&#38590;&#20197;&#23454;&#26102;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#37117;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20250;&#21033;&#29992;&#22810;&#20010;&#26469;&#28304;&#30340;&#20449;&#24687;&#36827;&#34892;&#25512;&#29702;&#65292;&#20854;&#20013;&#19968;&#20010;&#37325;&#35201;&#31867;&#21035;&#30340;&#25512;&#29702;&#38656;&#35201;&#22312;&#25512;&#29702;&#26102;&#38388;&#25552;&#20379;&#39044;&#35757;&#32451;&#21442;&#25968;&#20013;&#21253;&#21547;&#30340;&#32972;&#26223;&#30693;&#35782;&#20197;&#21450;&#29305;&#23450;&#23454;&#20363;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#28304;&#30693;&#35782;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#30340;&#25972;&#21512;&#21644;&#25512;&#29702;&#33021;&#21147;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26680;&#24515;&#25351;&#20195;&#28040;&#35299;&#23376;&#20219;&#21153;&#30340;&#27979;&#35797;&#22871;&#20214;&#65292;&#38656;&#35201;&#38024;&#23545;&#22810;&#20010;&#20107;&#23454;&#36827;&#34892;&#25512;&#29702;&#12290;&#36825;&#20123;&#23376;&#20219;&#21153;&#22312;&#21738;&#20123;&#30693;&#35782;&#26469;&#28304;&#21253;&#21547;&#30456;&#20851;&#30340;&#20107;&#23454;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22312;&#25512;&#29702;&#26102;&#38388;&#20165;&#20351;&#29992;&#34394;&#26500;&#30693;&#35782;&#30340;&#23376;&#20219;&#21153;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#26680;&#24515;&#25351;&#20195;&#28040;&#35299;&#27169;&#22411;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26377;&#20960;&#20010;&#27169;&#22411;&#38590;&#20197;&#23454;&#26102;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many state-of-the-art natural language understanding (NLU) models are based on pretrained neural language models. These models often make inferences using information from multiple sources. An important class of such inferences are those that require both background knowledge, presumably contained in a model's pretrained parameters, and instance-specific information that is supplied at inference time. However, the integration and reasoning abilities of NLU models in the presence of multiple knowledge sources have been largely understudied. In this work, we propose a test suite of coreference resolution subtasks that require reasoning over multiple facts. These subtasks differ in terms of which knowledge sources contain the relevant facts. We also introduce subtasks where knowledge is present only at inference time using fictional knowledge. We evaluate state-of-the-art coreference resolution models on our dataset. Our results indicate that several models struggle to reason on-the-fly o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24615;&#33021;&#19981;&#21487;&#30693;&#30340;&#22810;&#27169;&#24577;&#24471;&#20998;&#26041;&#27861;MM-SHAP&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#37327;&#21270;&#22810;&#27169;&#24577;&#27169;&#22411;&#20351;&#29992;&#21508;&#33258;&#27169;&#24577;&#30340;&#27604;&#20363;&#65292;&#24182;&#24212;&#29992;&#20110;&#27604;&#36739;&#27169;&#22411;&#30340;&#24179;&#22343;&#22810;&#27169;&#24577;&#31243;&#24230;&#21644;&#34913;&#37327;&#20010;&#20307;&#27169;&#22411;&#30340;&#36129;&#29486;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21333;&#27169;&#24577;&#23849;&#28291;&#27604;&#20197;&#21069;&#35748;&#20026;&#30340;&#26356;&#20026;&#26222;&#36941;&#65292;&#32780;MM-SHAP&#26159;&#20998;&#26512;VL&#27169;&#22411;&#22810;&#27169;&#24577;&#34892;&#20026;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2212.08158</link><description>&lt;p&gt;
MM-SHAP&#65306;&#19968;&#31181;&#29992;&#20110;&#34913;&#37327;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#21644;&#20219;&#21153;&#30340;&#22810;&#27169;&#24577;&#36129;&#29486;&#30340;&#24615;&#33021;&#19981;&#21487;&#30693;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
MM-SHAP: A Performance-agnostic Metric for Measuring Multimodal Contributions in Vision and Language Models &amp; Tasks. (arXiv:2212.08158v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08158
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24615;&#33021;&#19981;&#21487;&#30693;&#30340;&#22810;&#27169;&#24577;&#24471;&#20998;&#26041;&#27861;MM-SHAP&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#37327;&#21270;&#22810;&#27169;&#24577;&#27169;&#22411;&#20351;&#29992;&#21508;&#33258;&#27169;&#24577;&#30340;&#27604;&#20363;&#65292;&#24182;&#24212;&#29992;&#20110;&#27604;&#36739;&#27169;&#22411;&#30340;&#24179;&#22343;&#22810;&#27169;&#24577;&#31243;&#24230;&#21644;&#34913;&#37327;&#20010;&#20307;&#27169;&#22411;&#30340;&#36129;&#29486;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21333;&#27169;&#24577;&#23849;&#28291;&#27604;&#20197;&#21069;&#35748;&#20026;&#30340;&#26356;&#20026;&#26222;&#36941;&#65292;&#32780;MM-SHAP&#26159;&#20998;&#26512;VL&#27169;&#22411;&#22810;&#27169;&#24577;&#34892;&#20026;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;VL&#65289;&#24448;&#24448;&#21033;&#29992;&#21508;&#33258;&#27169;&#24577;&#20013;&#30340;&#19981;&#31283;&#23450;&#25351;&#26631;&#65288;&#20363;&#22914;&#30001;&#20998;&#24067;&#20559;&#24046;&#24341;&#20837;&#65289;&#32780;&#19981;&#26159;&#19987;&#27880;&#20110;&#27599;&#20010;&#27169;&#24577;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#22914;&#26524;&#21333;&#27169;&#24577;&#27169;&#22411;&#22312;VL&#20219;&#21153;&#19978;&#36798;&#21040;&#31867;&#20284;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#65292;&#21017;&#34920;&#26126;&#25152;&#35859;&#30340;&#21333;&#27169;&#24577;&#23849;&#28291;&#24050;&#32463;&#21457;&#29983;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#20934;&#30830;&#24230;&#30340;&#27979;&#35797;&#26080;&#27861;&#26816;&#27979;&#20363;&#22914;&#27169;&#22411;&#39044;&#27979;&#38169;&#35823;&#20294;&#27169;&#22411;&#20351;&#29992;&#20102;&#19968;&#20010;&#27169;&#24577;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MM-SHAP&#65292;&#19968;&#31181;&#22522;&#20110;Shapley&#20540;&#30340;&#24615;&#33021;&#19981;&#21487;&#30693;&#22810;&#27169;&#24577;&#24471;&#20998;&#65292;&#21487;&#21487;&#38752;&#22320;&#37327;&#21270;&#22810;&#27169;&#24577;&#27169;&#22411;&#20351;&#29992;&#21508;&#33258;&#27169;&#24577;&#30340;&#27604;&#20363;&#12290;&#25105;&#20204;&#23558;MM-SHAP&#24212;&#29992;&#20110;&#20004;&#31181;&#26041;&#24335;&#65306;&#65288;1&#65289;&#27604;&#36739;&#27169;&#22411;&#30340;&#24179;&#22343;&#22810;&#27169;&#24577;&#31243;&#24230;&#65292;&#65288;2&#65289;&#34913;&#37327;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#20010;&#20307;&#27169;&#22411;&#23545;&#21508;&#33258;&#27169;&#24577;&#30340;&#36129;&#29486;&#12290;&#20845;&#20010;VL&#27169;&#22411;&#30340;&#23454;&#39564;&#65288;LXMERT&#12289;CLIP&#21644;&#22235;&#20010;ALBEF&#21464;&#20307;&#65289;&#34920;&#26126;&#21333;&#27169;&#24577;&#23849;&#28291;&#27604;&#25105;&#20204;&#20197;&#21069;&#35748;&#20026;&#30340;&#26356;&#20026;&#26222;&#36941;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;MM-SHAP&#26159;&#25581;&#31034;&#21644;&#20998;&#26512;VL&#27169;&#22411;&#22810;&#27169;&#24577;&#34892;&#20026;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision and language models (VL) are known to exploit unrobust indicators in individual modalities (e.g., introduced by distributional biases) instead of focusing on relevant information in each modality. That a unimodal model achieves similar accuracy on a VL task to a multimodal one, indicates that so-called unimodal collapse occurred. However, accuracy-based tests fail to detect e.g., when the model prediction is wrong, while the model used relevant information from a modality. Instead, we propose MM-SHAP, a performance-agnostic multimodality score based on Shapley values that reliably quantifies in which proportions a multimodal model uses individual modalities. We apply MM-SHAP in two ways: (1) to compare models for their average degree of multimodality, and (2) to measure for individual models the contribution of individual modalities for different tasks and datasets. Experiments with six VL models -- LXMERT, CLIP and four ALBEF variants -- on four VL tasks highlight that unimodal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#26041;&#27861;&#26469;&#20174;&#20219;&#21153;&#35828;&#26126;&#20013;&#23398;&#20064;&#65292;&#20197;&#22788;&#29702;&#35828;&#26126;&#30340;&#21464;&#21270;&#24182;&#25552;&#39640;&#23545;&#26032;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.03813</link><description>&lt;p&gt;
&#20174;&#20219;&#21153;&#35828;&#26126;&#20070;&#20013;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robustness of Learning from Task Instructions. (arXiv:2212.03813v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#26041;&#27861;&#26469;&#20174;&#20219;&#21153;&#35828;&#26126;&#20013;&#23398;&#20064;&#65292;&#20197;&#22788;&#29702;&#35828;&#26126;&#30340;&#21464;&#21270;&#24182;&#25552;&#39640;&#23545;&#26032;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#30417;&#30563;&#23398;&#20064;&#22823;&#22810;&#22312;&#20010;&#21035;&#20219;&#21153;&#19978;&#36827;&#34892;&#65292;&#24182;&#38656;&#35201;&#22312;&#22823;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#19978;&#35757;&#32451;&#12290;&#36825;&#31181;&#33539;&#24335;&#20005;&#37325;&#38459;&#30861;&#20102;&#20219;&#21153;&#27010;&#25324;&#30340;&#21457;&#23637;&#65292;&#22240;&#20026;&#20934;&#22791;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#38598;&#26159;&#26114;&#36149;&#30340;&#12290;&#20026;&#20102;&#26500;&#24314;&#19968;&#20010;&#21487;&#20197;&#24555;&#36895;&#36731;&#26494;&#22320;&#25512;&#24191;&#21040;&#26032;&#20219;&#21153;&#30340;&#31995;&#32479;&#65292;&#26368;&#36817;&#37319;&#29992;&#20102;&#20219;&#21153;&#35828;&#26126;&#20316;&#20026;&#30417;&#30563;&#30340;&#26032;&#20852;&#36235;&#21183;&#12290;&#36825;&#20123;&#35828;&#26126;&#32473;&#27169;&#22411;&#23450;&#20041;&#20102;&#20219;&#21153;&#65292;&#24182;&#20801;&#35768;&#27169;&#22411;&#26681;&#25454;&#35828;&#26126;&#21644;&#36755;&#20837;&#36755;&#20986;&#36866;&#24403;&#30340;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#20219;&#21153;&#35828;&#26126;&#36890;&#24120;&#20197;&#19981;&#21516;&#24418;&#24335;&#34920;&#36798;&#65292;&#21487;&#20197;&#20174;&#20004;&#20010;&#32447;&#32034;&#20013;&#35299;&#37322;&#65306;&#39318;&#20808;&#65292;&#19968;&#20123;&#35828;&#26126;&#26159;&#30701;&#21477;&#65292;&#24182;&#19988;&#26159;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#23548;&#21521;&#65292;&#20363;&#22914;&#25552;&#31034;&#65292;&#32780;&#20854;&#20182;&#35828;&#26126;&#26159;&#27573;&#33853;&#65292;&#24182;&#19988;&#26159;&#20154;&#20026;&#23548;&#21521;&#30340;&#65292;&#20363;&#22914;&#20122;&#39532;&#36874;&#30340;MTurk; &#20854;&#27425;&#65292;&#19981;&#21516;&#30340;&#26368;&#32456;&#29992;&#25143;&#24456;&#21487;&#33021;&#29992;&#19981;&#21516;&#30340;&#25991;&#26412;&#34920;&#36798;&#26041;&#24335;&#35299;&#37322;&#30456;&#21516;&#30340;&#20219;&#21153;&#12290;&#38656;&#35201;&#19968;&#31181;&#40065;&#26834;&#30340;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#20219;&#21153;&#35828;&#26126;&#30340;&#21487;&#21464;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#26041;&#27861;&#26469;&#20174;&#20219;&#21153;&#35828;&#26126;&#20013;&#23398;&#20064;&#65292;&#21487;&#20197;&#22788;&#29702;&#35828;&#26126;&#30340;&#21464;&#21270;&#24182;&#25913;&#21892;&#23545;&#26032;&#20219;&#21153;&#30340;&#27010;&#25324;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional supervised learning mostly works on individual tasks and requires training on a large set of task-specific examples. This paradigm seriously hinders the development of task generalization since preparing a task-specific example set is costly. To build a system that can quickly and easily generalize to new tasks, task instructions have been adopted as an emerging trend of supervision recently. These instructions give the model the definition of the task and allow the model to output the appropriate answer based on the instructions and inputs. However, task instructions are often expressed in different forms, which can be interpreted from two threads: first, some instructions are short sentences and are pretrained language model (PLM) oriented, such as prompts, while other instructions are paragraphs and are human-oriented, such as those in Amazon MTurk; second, different end-users very likely explain the same task with instructions of different textual expressions. A robust 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;KRLS&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20851;&#38190;&#35789;&#24378;&#21270;&#23398;&#20064;&#21644;&#31934;&#32454;&#30340;&#22870;&#21169;&#20989;&#25968;&#26469;&#24110;&#21161;&#27169;&#22411;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#29983;&#25104;&#20851;&#38190;&#35789;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#31639;&#27861;&#22312;MultiWoZ&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2211.16773</link><description>&lt;p&gt;
&#22522;&#20110;&#20851;&#38190;&#35789;&#24378;&#21270;&#23398;&#20064;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#31471;&#21040;&#31471;&#21709;&#24212;&#29983;&#25104;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
KRLS: Improving End-to-End Response Generation in Task Oriented Dialog with Reinforced Keywords Learning. (arXiv:2211.16773v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;KRLS&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20851;&#38190;&#35789;&#24378;&#21270;&#23398;&#20064;&#21644;&#31934;&#32454;&#30340;&#22870;&#21169;&#20989;&#25968;&#26469;&#24110;&#21161;&#27169;&#22411;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#29983;&#25104;&#20851;&#38190;&#35789;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#31639;&#27861;&#22312;MultiWoZ&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#20013;&#65292;&#19968;&#20010;&#20449;&#24687;&#20016;&#23500;&#19988;&#25104;&#21151;&#30340;&#31995;&#32479;&#21709;&#24212;&#38656;&#35201;&#21253;&#21547;&#20851;&#38190;&#20449;&#24687;&#65292;&#20363;&#22914;&#37202;&#24215;&#30340;&#30005;&#35805;&#21495;&#30721;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20551;&#35774;&#36890;&#36807;&#27491;&#30830;&#29983;&#25104;&#20851;&#38190;&#25968;&#37327;&#65292;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#21363;&#20851;&#38190;&#35789;&#24378;&#21270;&#23398;&#20064;&#19982;&#19979;&#19968;&#20010;&#21333;&#35789;&#37319;&#26679;&#65288;KRLS&#65289;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#20294;&#36991;&#20813;&#20102;&#32791;&#26102;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#24182;&#37319;&#29992;&#20102;&#32454;&#31890;&#24230;&#30340;&#36880;&#20196;&#29260;&#22870;&#21169;&#20989;&#25968;&#26469;&#24110;&#21161;&#27169;&#22411;&#26356;&#21152;&#24378;&#20581;&#22320;&#23398;&#20064;&#20851;&#38190;&#35789;&#29983;&#25104;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;KRLS&#31639;&#27861;&#21487;&#20197;&#22312;MultiWoZ&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#33391;&#22909;&#30340;&#20449;&#24687;&#12289;&#25104;&#21151;&#21644;&#32508;&#21512;&#20998;&#25968;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In task-oriented dialogs, an informative and successful system response needs to include key information such as the phone number of a hotel. Therefore, we hypothesize that a model can achieve better overall performance by focusing on correctly generating key quantities. In this paper, we propose a new training algorithm, Keywords Reinforcement Learning with Next-word Sampling (KRLS), that utilizes Reinforcement Learning but avoids the time-consuming auto-regressive generation, and a fine-grained per-token reward function to help the model learn keywords generation more robustly. Empirical results show that the KRLS algorithm can achieve state-of-the-art performance on the inform, success, and combined score on the MultiWoZ benchmark dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#23454;&#29616;&#24847;&#35265;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36882;&#24402;&#25688;&#35201;&#21644;&#26174;&#33879;&#20869;&#23481;&#36873;&#25321;&#30340;&#26041;&#24335;&#26469;&#22788;&#29702;&#22823;&#37327;&#29992;&#25143;&#35780;&#35770;&#65292;&#24182;&#20351;&#29992;&#19977;&#20010;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.15914</link><description>&lt;p&gt;
GPT-3.5&#19979;&#30340;&#25552;&#31034;&#24847;&#35265;&#25688;&#35201;&#21270;
&lt;/p&gt;
&lt;p&gt;
Prompted Opinion Summarization with GPT-3.5. (arXiv:2211.15914v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#23454;&#29616;&#24847;&#35265;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36882;&#24402;&#25688;&#35201;&#21644;&#26174;&#33879;&#20869;&#23481;&#36873;&#25321;&#30340;&#26041;&#24335;&#26469;&#22788;&#29702;&#22823;&#37327;&#29992;&#25143;&#35780;&#35770;&#65292;&#24182;&#20351;&#29992;&#19977;&#20010;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#25991;&#26412;&#25688;&#35201;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#36825;&#31181;&#24378;&#22823;&#24615;&#33021;&#25193;&#23637;&#21040;&#20102;&#24847;&#35265;&#25688;&#35201;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20960;&#31181; GPT-3.5 &#24212;&#29992;&#20110;&#25552;&#31034;&#26041;&#24335;&#19979;&#23545;&#22823;&#37327;&#29992;&#25143;&#35780;&#35770;&#36827;&#34892;&#25688;&#35201;&#30340;&#27969;&#27700;&#32447;&#26041;&#27861;&#12290;&#20026;&#20102;&#22788;&#29702;&#20219;&#24847;&#25968;&#37327;&#30340;&#29992;&#25143;&#35780;&#35770;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#36882;&#24402;&#25688;&#35201;&#20197;&#21450;&#36890;&#36807;&#30417;&#30563;&#32858;&#31867;&#25110;&#25277;&#21462;&#36873;&#25321;&#26174;&#33879;&#20869;&#23481;&#36827;&#34892;&#25688;&#35201;&#30340;&#26041;&#27861;&#12290;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#65288;&#19968;&#20010;&#26159;&#37202;&#24215;&#35780;&#35770;&#30340;&#26041;&#38754;&#23548;&#21521;&#30340;&#25688;&#35201;&#25968;&#25454;&#38598;&#65288;SPACE&#65289;&#65292;&#21478;&#19968;&#20010;&#26159;&#20851;&#20110;&#20122;&#39532;&#36874;&#21644; Yelp &#35780;&#35770;&#30340;&#36890;&#29992;&#25688;&#35201;&#25968;&#25454;&#38598;&#65288;FewSum&#65289;&#65289;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; GPT-3.5 &#27169;&#22411;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#38750;&#24120;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35748;&#20026;&#26631;&#20934;&#35780;&#20272;&#25351;&#26631;&#19981;&#33021;&#21453;&#26144;&#36825;&#19968;&#28857;&#65292;&#24182;&#24341;&#20837;&#20102;&#19977;&#20010;&#26032;&#30340;&#25351;&#26631;&#65292;&#20197;&#23545;&#27604;&#36825;&#20123;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#20998;&#21035;&#38024;&#23545;&#24544;&#35802;&#24230;&#12289;&#20107;&#23454;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have shown impressive performance across a wide variety of tasks, including text summarization. In this paper, we show that this strong performance extends to opinion summarization. We explore several pipeline methods for applying GPT-3.5 to summarize a large collection of user reviews in a prompted fashion. To handle arbitrarily large numbers of user reviews, we explore recursive summarization as well as methods for selecting salient content to summarize through supervised clustering or extraction. On two datasets, an aspect-oriented summarization dataset of hotel reviews (SPACE) and a generic summarization dataset of Amazon and Yelp reviews (FewSum), we show that GPT-3.5 models achieve very strong performance in human evaluation. We argue that standard evaluation metrics do not reflect this, and introduce three new metrics targeting faithfulness, factuality, and genericity to contrast these different methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24207;&#21015;&#34917;&#20840;&#30340;&#26041;&#24335;&#36880;&#27493;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24456;&#22909;&#30340;&#25512;&#24191;&#33021;&#21147;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2211.11297</link><description>&lt;p&gt;
&#24207;&#21015;&#34917;&#20840;&#30340;&#35838;&#31243;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
In-sample Curriculum Learning by Sequence Completion for Natural Language Generation. (arXiv:2211.11297v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24207;&#21015;&#34917;&#20840;&#30340;&#26041;&#24335;&#36880;&#27493;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24456;&#22909;&#30340;&#25512;&#24191;&#33021;&#21147;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#26131;&#21040;&#38590;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#35838;&#31243;&#23398;&#20064;&#24050;&#32463;&#22312;&#22810;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#25552;&#21319;&#25928;&#26524;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#35774;&#35745;&#35268;&#21017;&#25110;&#35757;&#32451;&#27169;&#22411;&#26469;&#35780;&#20272;&#38590;&#24230;&#65292;&#39640;&#24230;&#20381;&#36182;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#38590;&#20197;&#25512;&#24191;&#12290;&#21463;&#8220;&#20174;&#26131;&#21040;&#38590;&#8221;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#36827;&#34892;&#35838;&#31243;&#23398;&#20064;&#30340;&#26041;&#27861;&#65306;&#25105;&#20204;&#30340;&#23398;&#20064;&#31574;&#30053;&#20174;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#26368;&#21518;&#20960;&#20010;&#35789;&#24320;&#22987;&#65292;&#21363;&#36827;&#34892;&#24207;&#21015;&#34917;&#20840;&#65292;&#28982;&#21518;&#36880;&#28176;&#25193;&#23637;&#21040;&#29983;&#25104;&#25972;&#20010;&#36755;&#20986;&#24207;&#21015;&#12290;&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25512;&#24191;&#33021;&#21147;&#24378;&#65292;&#23545;&#19981;&#21516;&#30340;&#20219;&#21153;&#36827;&#34892;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Curriculum learning has shown promising improvements in multiple domains by training machine learning models from easy samples to hard ones. Previous works which either design rules or train models for scoring the difficulty highly rely on task-specific expertise, and cannot generalize. Inspired by the "easy-to-hard" intuition, we propose to do in-sample curriculum learning for natural language generation tasks. Our learning strategy starts training the model to generate the last few words, i.e., do sequence completion, and gradually extends to generate the whole output sequence. Comprehensive experiments show that it generalizes well to different tasks and achieves significant improvements over strong baselines.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#30340; UI &#35270;&#35273;&#24341;&#23548;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#36890;&#36807;&#23558;&#25351;&#20196;&#27493;&#39588;&#19982; UI &#35270;&#39057;&#30456;&#32467;&#21512;&#65292;&#24110;&#21161;&#26234;&#33021;&#25163;&#26426;&#29992;&#25143;&#26356;&#36731;&#26494;&#22320;&#23436;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2211.07615</link><description>&lt;p&gt;
UGIF&#65306;UI &#35270;&#35273;&#24341;&#23548;&#19979;&#30340;&#25351;&#20196;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
UGIF: UI Grounded Instruction Following. (arXiv:2211.07615v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07615
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#30340; UI &#35270;&#35273;&#24341;&#23548;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#36890;&#36807;&#23558;&#25351;&#20196;&#27493;&#39588;&#19982; UI &#35270;&#39057;&#30456;&#32467;&#21512;&#65292;&#24110;&#21161;&#26234;&#33021;&#25163;&#26426;&#29992;&#25143;&#26356;&#36731;&#26494;&#22320;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#25163;&#26426;&#29992;&#25143;&#32463;&#24120;&#20250;&#21457;&#29616;&#23548;&#33322;&#22797;&#26434;&#30340;&#33756;&#21333;&#25191;&#34892;&#24120;&#35265;&#20219;&#21153;&#21464;&#24471;&#22256;&#38590;&#65292;&#20363;&#22914;&#8220;&#22914;&#20309;&#23631;&#34109;&#26410;&#30693;&#21495;&#30721;&#30340;&#26469;&#30005;&#65311;&#8221;&#30446;&#21069;&#65292;&#20154;&#24037;&#32534;&#20889;&#30340;&#36880;&#27493;&#35828;&#26126;&#25991;&#20214;&#21487;&#24110;&#21161;&#29992;&#25143;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; UGIF-DataSet&#65292;&#19968;&#20010;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#30340; UI &#35270;&#35273;&#24341;&#23548;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102; 8 &#31181;&#35821;&#35328;&#30340; 4,184 &#20010;&#24120;&#29992;&#25805;&#20316;&#12290;&#20316;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#21021;&#27493;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#22522;&#20110;&#29992;&#25143;&#26597;&#35810;&#26816;&#32034;&#30456;&#20851;&#25351;&#20196;&#27493;&#39588;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#20998;&#26512;&#27493;&#39588;&#24182;&#29983;&#25104;&#21487;&#22312;&#35774;&#22791;&#19978;&#25191;&#34892;&#30340;&#23439;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smartphone users often find it difficult to navigate myriad menus to perform common tasks such as "How to block calls from unknown numbers?". Currently, help documents with step-by-step instructions are manually written to aid the user. The user experience can be further enhanced by grounding the instructions in the help document to the UI and overlaying a tutorial on the phone UI. To build such tutorials, several natural language processing components including retrieval, parsing, and grounding are necessary, but there isn't any relevant dataset for such a task. Thus, we introduce UGIF-DataSet, a multi-lingual, multi-modal UI grounded dataset for step-by-step task completion on the smartphone containing 4,184 tasks across 8 languages. As an initial approach to this problem, we propose retrieving the relevant instruction steps based on the user's query and parsing the steps using Large Language Models (LLMs) to generate macros that can be executed on-device. The instruction steps are o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;miCSE&#26694;&#26550;&#65292;&#20351;&#29992;&#20114;&#20449;&#24687;&#23545;&#27604;&#23398;&#20064;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#34920;&#29616;&#20986;&#21331;&#36234;&#32467;&#26524;&#65292;&#24182;&#20026;&#26356;&#21152;&#40065;&#26834;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2211.04928</link><description>&lt;p&gt;
miCSE&#65306;&#29992;&#20110;&#23569;&#26679;&#26412;&#21477;&#23376;&#23884;&#20837;&#30340;&#20114;&#20449;&#24687;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
miCSE: Mutual Information Contrastive Learning for Low-shot Sentence Embeddings. (arXiv:2211.04928v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;miCSE&#26694;&#26550;&#65292;&#20351;&#29992;&#20114;&#20449;&#24687;&#23545;&#27604;&#23398;&#20064;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#34920;&#29616;&#20986;&#21331;&#36234;&#32467;&#26524;&#65292;&#24182;&#20026;&#26356;&#21152;&#40065;&#26834;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;miCSE&#65292;&#19968;&#31181;&#22522;&#20110;&#20114;&#20449;&#24687;&#23545;&#27604;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#23569;&#26679;&#26412;&#21477;&#23376;&#23884;&#20837;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23545;&#27604;&#23398;&#20064;&#26399;&#38388;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#35270;&#22270;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#36827;&#34892;&#23545;&#40784;&#12290;&#20351;&#29992;miCSE&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#21363;&#23545;&#27599;&#20010;&#21477;&#23376;&#30340;&#22686;&#24378;&#35270;&#22270;&#24378;&#21046;&#23454;&#26045;&#32467;&#26500;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#20351;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#26356;&#21152;&#39640;&#25928;&#12290;&#22240;&#27492;&#65292;&#35813;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#39046;&#22495;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#19982;&#22810;&#20010;&#23569;&#26679;&#26412;&#23398;&#20064;&#22522;&#20934;&#30340;&#26368;&#26032;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#20840;&#26679;&#26412;&#24773;&#20917;&#19979;&#20855;&#26377;&#21487;&#27604;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#27604;&#24403;&#21069;&#30340;&#21477;&#23376;&#23884;&#20837;&#23545;&#27604;&#26041;&#27861;&#26356;&#21152;&#40065;&#26834;&#30340;&#39640;&#25928;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents miCSE, a mutual information-based contrastive learning framework that significantly advances the state-of-the-art in few-shot sentence embedding. The proposed approach imposes alignment between the attention pattern of different views during contrastive learning. Learning sentence embeddings with miCSE entails enforcing the structural consistency across augmented views for every sentence, making contrastive self-supervised learning more sample efficient. As a result, the proposed approach shows strong performance in the few-shot learning domain. While it achieves superior results compared to state-of-the-art methods on multiple benchmarks in few-shot learning, it is comparable in the full-shot scenario. This study opens up avenues for efficient self-supervised learning methods that are more robust than current contrastive methods for sentence embedding.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20013;&#25991;&#22270;&#20687;-&#25991;&#26412;&#23545;&#25968;&#25454;&#38598;&#65292;&#26032;&#25552;&#20986;&#30340;&#20004;&#38454;&#27573;&#39044;&#35757;&#32451;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#20013;&#25991;CLIP&#22312;&#22810;&#20219;&#21153;&#22270;&#20687;&#29702;&#35299;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#29305;&#21035;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#24494;&#35843;&#35774;&#32622;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2211.01335</link><description>&lt;p&gt;
&#20013;&#25991;CLIP: &#20013;&#25991;&#23545;&#27604;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese. (arXiv:2211.01335v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20013;&#25991;&#22270;&#20687;-&#25991;&#26412;&#23545;&#25968;&#25454;&#38598;&#65292;&#26032;&#25552;&#20986;&#30340;&#20004;&#38454;&#27573;&#39044;&#35757;&#32451;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#20013;&#25991;CLIP&#22312;&#22810;&#20219;&#21153;&#22270;&#20687;&#29702;&#35299;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#29305;&#21035;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#24494;&#35843;&#35774;&#32622;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CLIP&#30340;&#24040;&#22823;&#25104;&#21151;&#25512;&#21160;&#20102;&#23545;&#20110;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#23545;&#27604;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20013;&#25991;&#22270;&#20687;-&#25991;&#26412;&#23545;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#25968;&#25454;&#26469;&#28304;&#20110;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#22312;&#36825;&#20010;&#26032;&#25968;&#25454;&#38598;&#19978;&#23545;&#20013;&#25991;CLIP&#27169;&#22411;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#24320;&#21457;&#20102;5&#20010;&#22810;&#23610;&#23544;&#30340;&#20013;&#25991;CLIP&#27169;&#22411;&#65292;&#33539;&#22260;&#20174;7700&#19975;&#21040;9.58&#20159;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#35813;&#26041;&#27861;&#20013;&#65292;&#27169;&#22411;&#39318;&#20808;&#20351;&#29992;&#20923;&#32467;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#25152;&#26377;&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#65292;&#20013;&#25991;CLIP&#21487;&#20197;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#24494;&#35843;&#35774;&#32622;&#19979;&#22312;MUGE&#12289;Flickr30K-CN&#21644;COCO-CN&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#23427;&#33021;&#22815;&#22312;ELEVATER&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20013;&#21462;&#24471;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The tremendous success of CLIP (Radford et al., 2021) has promoted the research and application of contrastive learning for vision-language pretraining. In this work, we construct a large-scale dataset of image-text pairs in Chinese, where most data are retrieved from publicly available datasets, and we pretrain Chinese CLIP models on the new dataset. We develop 5 Chinese CLIP models of multiple sizes, spanning from 77 to 958 million parameters. Furthermore, we propose a two-stage pretraining method, where the model is first trained with the image encoder frozen and then trained with all parameters being optimized, to achieve enhanced model performance. Our comprehensive experiments demonstrate that Chinese CLIP can achieve the state-of-the-art performance on MUGE, Flickr30K-CN, and COCO-CN in the setups of zero-shot learning and finetuning, and it is able to achieve competitive performance in zero-shot image classification based on the evaluation on the ELEVATER benchmark (Li et al., 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;&#38899;&#39057;&#35821;&#35328;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#38899;&#39057;&#35821;&#35328;&#37197;&#23545;&#22686;&#24378;&#21644;&#22810;&#23618;&#27979;&#35797;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#23558;&#23427;&#20204;&#19982;&#21333;&#27169;&#24577;&#22686;&#24378;&#32467;&#21512;&#36215;&#26469;&#65292;&#22312;&#33258;&#21160;&#21270;&#38899;&#39057;&#23383;&#24149;&#21644;&#38899;&#39057;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2210.17143</link><description>&lt;p&gt;
&#25506;&#32034;&#38899;&#39057;&#35821;&#35328;&#23398;&#20064;&#20013;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Exploring Train and Test-Time Augmentations for Audio-Language Learning. (arXiv:2210.17143v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;&#38899;&#39057;&#35821;&#35328;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#38899;&#39057;&#35821;&#35328;&#37197;&#23545;&#22686;&#24378;&#21644;&#22810;&#23618;&#27979;&#35797;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#23558;&#23427;&#20204;&#19982;&#21333;&#27169;&#24577;&#22686;&#24378;&#32467;&#21512;&#36215;&#26469;&#65292;&#22312;&#33258;&#21160;&#21270;&#38899;&#39057;&#23383;&#24149;&#21644;&#38899;&#39057;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25581;&#31034;&#38899;&#39057;&#35821;&#35328;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#25968;&#25454;&#22686;&#24378;&#30340;&#24433;&#21709;&#65292;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#22686;&#24378;&#26041;&#27861;&#65292;&#19981;&#20165;&#22312;&#35757;&#32451;&#26102;&#65292;&#32780;&#19988;&#22312;&#27979;&#35797;&#26102;&#20063;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#36866;&#24403;&#30340;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24212;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#38899;&#39057;&#35821;&#35328;&#37197;&#23545;&#22686;&#24378;PairMix&#65288;&#36825;&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#38899;&#39057;&#35821;&#35328;&#22686;&#24378;&#26041;&#27861;&#65289;&#65292;&#22312;&#33258;&#21160;&#21270;&#38899;&#39057;&#23383;&#24149;&#21644;&#38899;&#39057;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;&#22522;&#32447;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#22810;&#23618;&#27979;&#35797;&#22686;&#24378;&#65288;Multi-TTA&#65289;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#20004;&#31181;&#25552;&#20986;&#30340;&#26041;&#27861;&#21644;&#21333;&#27169;&#22686;&#24378;&#32452;&#21512;&#36215;&#26469;&#65292;&#22312;&#38899;&#39057;&#23383;&#24149;&#26041;&#38754;&#23454;&#29616;&#20102;47.5 SPIDEr&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#25552;&#39640;&#20102;18.2&#65285;&#12290;&#22312;&#38899;&#39057;&#25991;&#26412;&#26816;&#32034;&#26041;&#38754;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20063;&#34920;&#29616;&#20986;&#20102;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we aim to unveil the impact of data augmentation in audio-language multi-modal learning, which has not been explored despite its importance. We explore various augmentation methods at not only train-time but also test-time and find out that proper data augmentation can lead to substantial improvements. Specifically, applying our proposed audio-language paired augmentation PairMix, which is the first multi-modal audio-language augmentation method, outperforms the baselines for both automated audio captioning and audio-text retrieval tasks. To fully take advantage of data augmentation, we also present multi-level test-time augmentation (Multi-TTA) for the test-time. We successfully incorporate the two proposed methods and uni-modal augmentations and achieve 47.5 SPIDEr on audio captioning, which is an 18.2% relative increase over the baseline. In audio-text retrieval, the proposed methods also show an improvement in performance as well.
&lt;/p&gt;</description></item><item><title>TASER&#26159;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#20351;&#24471;&#23494;&#38598;&#26816;&#32034;&#22120;&#33021;&#22815;&#22312;&#21442;&#25968;&#20302;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;BM25&#65307;&#23454;&#39564;&#34920;&#26126;TASER&#20063;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.05156</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#24320;&#25918;&#22495;&#38382;&#31572;&#30340;&#39640;&#25928;&#31283;&#20581;&#23494;&#38598;&#26816;&#32034;&#30340;&#20219;&#21153;&#24863;&#30693;&#19987;&#19994;&#21270;
&lt;/p&gt;
&lt;p&gt;
Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering. (arXiv:2210.05156v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05156
&lt;/p&gt;
&lt;p&gt;
TASER&#26159;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#20351;&#24471;&#23494;&#38598;&#26816;&#32034;&#22120;&#33021;&#22815;&#22312;&#21442;&#25968;&#20302;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;BM25&#65307;&#23454;&#39564;&#34920;&#26126;TASER&#20063;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#31181;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24320;&#25918;&#22495;&#38382;&#31572;&#30340;&#20107;&#23454;&#19978;&#30340;&#26550;&#26500;&#20351;&#29992;&#20004;&#20010;&#21516;&#26500;&#32534;&#30721;&#22120;&#65292;&#20174;&#30456;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21021;&#22987;&#21270;&#65292;&#20294;&#20998;&#21035;&#20026;&#38382;&#39064;&#21644;&#27573;&#33853;&#21442;&#25968;&#21270;&#12290;&#36825;&#31181;&#21452;&#32534;&#30721;&#22120;&#26550;&#26500;&#22312;&#21442;&#25968;&#26041;&#38754;&#26159;&#20302;&#25928;&#30340;&#65292;&#22240;&#20026;&#22312;&#32534;&#30721;&#22120;&#20043;&#38388;&#27809;&#26377;&#21442;&#25968;&#20849;&#20139;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#65292;&#36825;&#31181;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#24615;&#33021;&#20302;&#20110;BM25&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;Task-aware Specialization for dense Retrieval(TASER)&#65292;&#23427;&#36890;&#36807;&#20132;&#38169;&#20849;&#20139;&#21644;&#19987;&#38376;&#21270;&#22359;&#22312;&#21333;&#20010;&#32534;&#30721;&#22120;&#20013;&#23454;&#29616;&#21442;&#25968;&#20849;&#20139;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TASER&#21487;&#20197;&#22312;&#20165;&#20351;&#29992;&#21452;&#32534;&#30721;&#22120;&#23494;&#38598;&#26816;&#32034;&#22120;&#32422;60%&#30340;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20248;&#36234;&#30340;&#20934;&#30830;&#24615;&#65292;&#36229;&#36807;BM25&#12290;&#22312;&#22495;&#22806;&#35780;&#20272;&#20013;&#65292;TASER&#20063;&#32463;&#39564;&#35777;&#23454;&#38469;&#19978;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given its effectiveness on knowledge-intensive natural language processing tasks, dense retrieval models have become increasingly popular. Specifically, the de-facto architecture for open-domain question answering uses two isomorphic encoders that are initialized from the same pretrained model but separately parameterized for questions and passages. This bi-encoder architecture is parameter-inefficient in that there is no parameter sharing between encoders. Further, recent studies show that such dense retrievers underperform BM25 in various settings. We thus propose a new architecture, Task-aware Specialization for dense Retrieval (TASER), which enables parameter sharing by interleaving shared and specialized blocks in a single encoder. Our experiments on five question answering datasets show that TASER can achieve superior accuracy, surpassing BM25, while using about 60% of the parameters as bi-encoder dense retrievers. In out-of-domain evaluations, TASER is also empirically more robu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ASDOT&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20219;&#20309;&#32473;&#23450;&#25110;&#27809;&#26377;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#21040;&#25991;&#26412;&#30340;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#30001;&#20004;&#20010;&#27493;&#39588;&#32452;&#25104;&#65292;&#20854;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35299;&#20915;&#65292;&#24182;&#21487;&#36866;&#29992;&#20110;&#21508;&#31181;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2210.04325</link><description>&lt;p&gt;
ASDOT&#65306;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#25968;&#25454;&#21040;&#25991;&#26412;&#30340;&#38646;&#26679;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ASDOT: Any-Shot Data-to-Text Generation with Pretrained Language Models. (arXiv:2210.04325v3 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04325
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ASDOT&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20219;&#20309;&#32473;&#23450;&#25110;&#27809;&#26377;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#21040;&#25991;&#26412;&#30340;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#30001;&#20004;&#20010;&#27493;&#39588;&#32452;&#25104;&#65292;&#20854;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35299;&#20915;&#65292;&#24182;&#21487;&#36866;&#29992;&#20110;&#21508;&#31181;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21040;&#25991;&#26412;&#30340;&#29983;&#25104;&#22312;&#36755;&#20837;&#25968;&#25454;&#30340;&#39046;&#22495;&#65288;&#22914;&#37329;&#34701; vs &#36816;&#21160;&#65289;&#25110;&#26550;&#26500;&#65288;&#20363;&#22914;&#65292;&#19981;&#21516;&#30340;&#35859;&#35789;&#65289;&#26041;&#38754;&#23384;&#22312;&#24040;&#22823;&#30340;&#24046;&#24322;&#65292;&#36825;&#20351;&#24471;&#26368;&#36817;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#26041;&#27861;&#38656;&#35201;&#36275;&#22815;&#22810;&#30340;&#35757;&#32451;&#26679;&#26412;&#25165;&#33021;&#23398;&#20064;&#21040;&#28040;&#38500;&#27495;&#20041;&#21644;&#25551;&#36848;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#20013;&#30340;&#25968;&#25454;&#21040;&#25991;&#26412;&#38382;&#39064;&#24448;&#24448;&#38754;&#20020;&#30528;&#21508;&#31181;&#19981;&#36275;&#26679;&#26412;&#30340;&#38382;&#39064;&#65306;&#21487;&#33021;&#21482;&#26377;&#26497;&#23569;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#25110;&#26681;&#26412;&#27809;&#26377;&#35757;&#32451;&#26679;&#26412;&#65292;&#25110;&#38656;&#35201;&#20381;&#36182;&#20110;&#19981;&#21516;&#39046;&#22495;&#25110;&#26550;&#26500;&#30340;&#26679;&#20363;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; Any-Shot Data-to-Text (ASDOT)&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#20219;&#20309;&#32473;&#23450;&#65288;&#25110;&#27809;&#26377;&#65289;&#26679;&#26412;&#65292;&#21487;&#20197;&#28789;&#27963;&#36866;&#29992;&#20110;&#21508;&#31181;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;ASDOT&#30001;&#20004;&#20010;&#27493;&#39588;&#32452;&#25104;&#65292;&#25968;&#25454;&#28040;&#27495;&#21644;&#21477;&#23376;&#34701;&#21512;&#65292;&#36825;&#20004;&#20010;&#27493;&#39588;&#37117;&#21487;&#20197;&#20351;&#29992;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36827;&#34892;&#35299;&#20915;&#12290;&#22312;&#25968;&#25454;&#28040;&#27495;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#25552;&#31034;&#24335;GPT-3&#27169;&#22411;&#26469;&#29702;&#35299;&#36755;&#20837;&#25968;&#25454;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#27169;&#31946;&#19977;&#20803;&#32452;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#21487;&#29992;&#26679;&#26412;&#20013;&#30340;&#20449;&#24687;&#34701;&#21512;&#20197;&#29983;&#25104;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-to-text generation is challenging due to the great variety of the input data in terms of domains (e.g., finance vs sports) or schemata (e.g., diverse predicates). Recent end-to-end neural methods thus require substantial training examples to learn to disambiguate and describe the data. Yet, real-world data-to-text problems often suffer from various data-scarce issues: one may have access to only a handful of or no training examples, and/or have to rely on examples in a different domain or schema. To fill this gap, we propose Any-Shot Data-to-Text (ASDOT), a new approach flexibly applicable to diverse settings by making efficient use of any given (or no) examples. ASDOT consists of two steps, data disambiguation and sentence fusion, both of which are amenable to be solved with off-the-shelf pretrained language models (LMs) with optional finetuning. In the data disambiguation stage, we employ the prompted GPT-3 model to understand possibly ambiguous triples from the input data and c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20809;&#36731;&#28151;&#21512;&#21484;&#22238;&#22120;&#30340;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#32034;&#24341;&#39640;&#25928;&#30340;&#23494;&#38598;&#21484;&#22238;&#22120;&#21644;LITE&#21484;&#22238;&#22120;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#21487;&#20197;&#33410;&#30465;&#20869;&#23384;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.01371</link><description>&lt;p&gt;
&#20809;&#36731;&#28151;&#21512;&#21484;&#22238;&#22120;&#30340;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on the Efficiency and Generalization of Light Hybrid Retrievers. (arXiv:2210.01371v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20809;&#36731;&#28151;&#21512;&#21484;&#22238;&#22120;&#30340;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#32034;&#24341;&#39640;&#25928;&#30340;&#23494;&#38598;&#21484;&#22238;&#22120;&#21644;LITE&#21484;&#22238;&#22120;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#21487;&#20197;&#33410;&#30465;&#20869;&#23384;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#21484;&#22238;&#22120;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#31232;&#30095;&#21644;&#23494;&#38598;&#21484;&#22238;&#22120;&#30340;&#20248;&#28857;&#12290;&#20197;&#21069;&#30340;&#28151;&#21512;&#21484;&#22238;&#22120;&#21033;&#29992;&#32034;&#24341;&#23494;&#38598;&#30340;&#23494;&#38598;&#21484;&#22238;&#22120;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#8220;&#26159;&#21542;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#28151;&#21512;&#21484;&#22238;&#22120;&#30340;&#32034;&#24341;&#20869;&#23384;&#8221;&#65311;&#21463;&#27492;&#38382;&#39064;&#39537;&#21160;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#32034;&#24341;&#39640;&#25928;&#30340;&#23494;&#38598;&#21484;&#22238;&#22120;&#65288;&#21363;DrBoost&#65289;&#65292;&#24182;&#24341;&#20837;LITE&#21484;&#22238;&#22120;&#36827;&#19968;&#27493;&#20943;&#23569;DrBoost&#30340;&#20869;&#23384;&#12290;LITE&#21516;&#26102;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#30693;&#35782;&#33976;&#39311;&#26469;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65292;&#24182;&#23558;BM25&#31232;&#30095;&#21484;&#22238;&#22120;&#19982;LITE&#25110;DrBoost&#30456;&#32467;&#21512;&#24418;&#25104;&#36731;&#28151;&#21512;&#21484;&#22238;&#22120;&#12290;&#25105;&#20204;&#30340;Hybrid-LITE&#21484;&#22238;&#22120;&#22312;&#20445;&#25345;BM25&#21644;DPR&#28151;&#21512;&#21484;&#22238;&#22120;98.0&#65285;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#33410;&#30465;&#20102;13&#20493;&#30340;&#20869;&#23384;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#36731;&#28151;&#21512;&#21484;&#22238;&#22120;&#22312;&#22495;&#22806;&#25968;&#25454;&#38598;&#21644;&#19968;&#32452;&#23545;&#25239;&#24615;&#25915;&#20987;&#25968;&#25454;&#38598;&#19978;&#30340;&#27867;&#21270;&#23481;&#37327;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36731;&#28151;&#21512;&#21484;&#22238;&#22120;&#30340;&#27867;&#21270;&#24615;&#33021;&#20248;&#20110;&#21333;&#20010;&#21484;&#22238;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hybrid retrievers can take advantage of both sparse and dense retrievers. Previous hybrid retrievers leverage indexing-heavy dense retrievers. In this work, we study "Is it possible to reduce the indexing memory of hybrid retrievers without sacrificing performance"? Driven by this question, we leverage an indexing-efficient dense retriever (i.e. DrBoost) and introduce a LITE retriever that further reduces the memory of DrBoost. LITE is jointly trained on contrastive learning and knowledge distillation from DrBoost. Then, we integrate BM25, a sparse retriever, with either LITE or DrBoost to form light hybrid retrievers. Our Hybrid-LITE retriever saves 13X memory while maintaining 98.0% performance of the hybrid retriever of BM25 and DPR. In addition, we study the generalization capacity of our light hybrid retrievers on out-of-domain dataset and a set of adversarial attacks datasets. Experiments showcase that light hybrid retrievers achieve better generalization performance than individ
&lt;/p&gt;</description></item><item><title>Zemi&#26159;&#19968;&#31181;&#38646;&#26679;&#26412;&#21322;&#21442;&#25968;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#22806;&#37096;&#26816;&#32034;&#22120;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#27604;&#21442;&#25968;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2210.00185</link><description>&lt;p&gt;
Zemi: &#20174;&#22810;&#20219;&#21153;&#20013;&#23398;&#20064;&#38646;&#26679;&#26412;&#21322;&#21442;&#25968;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Zemi: Learning Zero-Shot Semi-Parametric Language Models from Multiple Tasks. (arXiv:2210.00185v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00185
&lt;/p&gt;
&lt;p&gt;
Zemi&#26159;&#19968;&#31181;&#38646;&#26679;&#26412;&#21322;&#21442;&#25968;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#22806;&#37096;&#26816;&#32034;&#22120;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#27604;&#21442;&#25968;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#20294;&#24040;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#36890;&#24120;&#20250;&#20135;&#29983;&#39640;&#25104;&#26412;&#12290;&#26368;&#36817;&#65292;&#21322;&#21442;&#25968;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#22806;&#37096;&#26816;&#32034;&#22120;&#22686;&#24378;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#26377;&#21069;&#36884;&#30340;&#35821;&#35328;&#24314;&#27169;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#21322;&#21442;&#25968;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#22312;&#38646;&#26679;&#26412;&#25512;&#24191;&#21040;&#19979;&#28216;&#20219;&#21153;&#26102;&#20687;&#20840;&#21442;&#25968;&#27169;&#22411;&#19968;&#26679;&#34920;&#29616;&#31454;&#20105;&#21147;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Zemi&#65292;&#19968;&#31181;&#38646;&#26679;&#26412;&#21322;&#21442;&#25968;&#35821;&#35328;&#27169;&#22411;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#24191;&#27867;&#30340;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#24378;&#22823;&#38646;&#26679;&#26412;&#24615;&#33021;&#30340;&#21322;&#21442;&#25968;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#26032;&#39062;&#30340;&#21322;&#21442;&#25968;&#22810;&#20219;&#21153;&#25552;&#31034;&#35757;&#32451;&#33539;&#24335;&#26469;&#35757;&#32451;Zemi&#65292;&#19982;T0&#25552;&#20986;&#30340;&#21442;&#25968;&#22810;&#20219;&#21153;&#35757;&#32451;&#30456;&#27604;&#65292;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#22810;&#20219;&#21153;&#35757;&#32451;&#21644;&#38646;&#26679;&#26412;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Although large language models have achieved impressive zero-shot ability, the huge model size generally incurs high cost. Recently, semi-parametric language models, which augment a smaller language model with an external retriever, have demonstrated promising language modeling capabilities. However, it remains unclear whether such semi-parametric language models can perform competitively well as their fully-parametric counterparts on zero-shot generalization to downstream tasks. In this work, we introduce $\text{Zemi}$, a zero-shot semi-parametric language model. To our best knowledge, this is the first semi-parametric language model that can demonstrate strong zero-shot performance on a wide range of held-out unseen tasks. We train $\text{Zemi}$ with a novel semi-parametric multitask prompted training paradigm, which shows significant improvement compared with the parametric multitask training as proposed by T0. Specifically, we augment the multitask training and zero-shot evaluation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#21464;&#21387;&#22120;&#65292;&#23427;&#21487;&#20197;&#39640;&#25928;&#22320;&#20445;&#23384;&#23545;&#35805;&#21382;&#21490;&#20449;&#24687;&#65292;&#24182;&#19988;&#22312;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.07634</link><description>&lt;p&gt;
&#29992;&#20110;&#39640;&#25928;&#23545;&#35805;&#24314;&#27169;&#30340;&#29366;&#24577;&#35760;&#24518;&#22686;&#24378;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Stateful Memory-Augmented Transformers for Efficient Dialogue Modeling. (arXiv:2209.07634v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#21464;&#21387;&#22120;&#65292;&#23427;&#21487;&#20197;&#39640;&#25928;&#22320;&#20445;&#23384;&#23545;&#35805;&#21382;&#21490;&#20449;&#24687;&#65292;&#24182;&#19988;&#22312;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#22312;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#23427;&#20204;&#26080;&#27861;&#22788;&#29702;&#38271;&#23545;&#35805;&#21382;&#21490;&#65292;&#24120;&#24120;&#23548;&#33268;&#19978;&#19979;&#25991;&#34987;&#25130;&#26029;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35760;&#24518;&#22686;&#24378;&#21464;&#21387;&#22120;&#65292;&#19982;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#20860;&#23481;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20445;&#23384;&#23545;&#35805;&#21382;&#21490;&#20449;&#24687;&#12290;&#36890;&#36807;&#23558;&#19968;&#20010;&#21333;&#29420;&#30340;&#35760;&#24518;&#27169;&#22359;&#19982;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#30456;&#32467;&#21512;&#65292;&#36825;&#20010;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#20132;&#25442;&#35760;&#24518;&#29366;&#24577;&#21644;&#24403;&#21069;&#36755;&#20837;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#23545;&#35805;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#35821;&#35328;&#24314;&#27169;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#39044;&#35757;&#32451;Transformer&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#37117;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer encoder-decoder models have achieved great performance in dialogue generation tasks, however, their inability to process long dialogue history often leads to truncation of the context To address this problem, we propose a novel memory-augmented transformer that is compatible with existing pre-trained encoder-decoder models and enables efficient preservation of the dialogue history information. By incorporating a separate memory module alongside the pre-trained transformer, the model can effectively interchange information between the memory states and the current input context. We evaluate our model on three dialogue datasets and two language modeling datasets. Experimental results show that our method has achieved superior efficiency and performance compared to other pre-trained Transformer baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; CombLM &#26041;&#27861;&#65292;&#36890;&#36807;&#23567;&#22411;&#24494;&#35843;&#27169;&#22411;&#35843;&#25972;&#22823;&#22411;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#20197;&#36866;&#24212;&#26032;&#39046;&#22495;&#21644;&#20219;&#21153;&#65292;&#19988;&#19981;&#38656;&#35201;&#35775;&#38382;&#23427;&#20204;&#30340;&#26435;&#37325;&#25110;&#20013;&#38388;&#28608;&#27963;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;&#22810;&#20010;&#39046;&#22495;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#24615;&#33021;&#24471;&#21040;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2205.12213</link><description>&lt;p&gt;
CombLM: &#36890;&#36807;&#23567;&#22411;&#24494;&#35843;&#27169;&#22411;&#35843;&#25972;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CombLM: Adapting Black-Box Language Models through Small Fine-Tuned Models. (arXiv:2205.12213v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; CombLM &#26041;&#27861;&#65292;&#36890;&#36807;&#23567;&#22411;&#24494;&#35843;&#27169;&#22411;&#35843;&#25972;&#22823;&#22411;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#20197;&#36866;&#24212;&#26032;&#39046;&#22495;&#21644;&#20219;&#21153;&#65292;&#19988;&#19981;&#38656;&#35201;&#35775;&#38382;&#23427;&#20204;&#30340;&#26435;&#37325;&#25110;&#20013;&#38388;&#28608;&#27963;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;&#22810;&#20010;&#39046;&#22495;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#24615;&#33021;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#26032;&#20219;&#21153;&#21644;&#22495;&#30340;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#23545;&#27169;&#22411;&#26377;&#30333;&#30418;&#35775;&#38382;&#65292;&#24182;&#36890;&#36807;&#20462;&#25913;&#20854;&#21442;&#25968;&#36827;&#34892;&#25805;&#20316;&#12290;&#20294;&#36825;&#19982;&#35813;&#39046;&#22495;&#30340;&#26368;&#39640;&#36136;&#37327;&#27169;&#22411;&#20165;&#36890;&#36807;&#25512;&#29702;API&#20316;&#20026;&#40657;&#30418;&#21487;&#29992;&#30340;&#26368;&#36817;&#36235;&#21183;&#19981;&#20860;&#23481;&#12290;&#21363;&#20351;&#21487;&#29992;&#27169;&#22411;&#26435;&#37325;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#35745;&#31639;&#25104;&#26412;&#20063;&#21487;&#33021;&#23545;&#22823;&#22810;&#25968;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#26159;&#31105;&#27490;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#36866;&#24212;&#26032;&#39046;&#22495;&#21644;&#20219;&#21153;&#65292;&#20551;&#35774;&#27809;&#26377;&#35775;&#38382;&#23427;&#20204;&#30340;&#26435;&#37325;&#25110;&#20013;&#38388;&#28608;&#27963;&#30340;&#26435;&#38480;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#23567;&#39564;&#35777;&#38598;&#19978;&#23398;&#20064;&#30340;&#23567;&#22411;&#32593;&#32476;&#65292;&#22312;&#27010;&#29575;&#32423;&#21035;&#19978;&#24494;&#35843;&#23567;&#22411;&#30333;&#30418;LM&#65292;&#24182;&#23558;&#20854;&#19982;&#22823;&#22411;&#40657;&#30418;LM&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#22823;&#22411;LM&#65288;OPT-30B&#65289;&#36866;&#24212;&#22810;&#20010;&#39046;&#22495;&#21644;&#19979;&#28216;&#20219;&#21153;&#65288;&#26426;&#22120;&#32763;&#35793;&#65289;&#65292;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#35266;&#23519;&#21040;&#24615;&#33021;&#30340;&#25552;&#39640;&#65292;&#26368;&#39640;&#21487;&#36798;9\%&#65292;&#21516;&#26102;&#20351;&#29992;&#39046;&#22495;&#19987;&#23478;23&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for adapting language models (LMs) to new tasks and domains have traditionally assumed white-box access to the model, and work by modifying its parameters. However, this is incompatible with a recent trend in the field, where the highest quality models are only available as black-boxes through inference APIs. Even when the model weights are available, the computational cost of fine-tuning large LMs can be prohibitive for most practitioners. In this work, we present a lightweight method for adapting large LMs to new domains and tasks, assuming no access to their weights or intermediate activations. Our approach fine-tunes a small white-box LM and combines it with the large black-box LM at the probability level through a small network, learned on a small validation set. We validate our approach by adapting a large LM (OPT-30B) to several domains and a downstream task (machine translation), observing improved performance in all cases, of up to 9\%, while using a domain expert 23x 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#35206;&#30422;&#25991;&#26412;&#20043;&#38388;&#35821;&#20041;&#36317;&#31163;&#35780;&#20272;&#30340;&#20256;&#32479;&#25361;&#25112;&#12290;&#36890;&#36807;&#25513;&#30721;&#21644;&#39044;&#27979;&#31574;&#30053;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#37051;&#36817;&#20998;&#24067;&#25955;&#24230;&#65288;NDD&#65289;&#26469;&#34920;&#31034;&#37325;&#21472;&#37096;&#20998;&#30340;&#35821;&#20041;&#36317;&#31163;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;NDD&#23545;&#20110;&#21508;&#31181;&#35821;&#20041;&#24046;&#24322;&#26356;&#20026;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2110.01176</link><description>&lt;p&gt;
&#39640;&#24230;&#37325;&#21472;&#25991;&#26412;&#30340;&#24773;&#22659;&#21270;&#35821;&#20041;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Contextualized Semantic Distance between Highly Overlapped Texts. (arXiv:2110.01176v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.01176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#35206;&#30422;&#25991;&#26412;&#20043;&#38388;&#35821;&#20041;&#36317;&#31163;&#35780;&#20272;&#30340;&#20256;&#32479;&#25361;&#25112;&#12290;&#36890;&#36807;&#25513;&#30721;&#21644;&#39044;&#27979;&#31574;&#30053;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#37051;&#36817;&#20998;&#24067;&#25955;&#24230;&#65288;NDD&#65289;&#26469;&#34920;&#31034;&#37325;&#21472;&#37096;&#20998;&#30340;&#35821;&#20041;&#36317;&#31163;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;NDD&#23545;&#20110;&#21508;&#31181;&#35821;&#20041;&#24046;&#24322;&#26356;&#20026;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#32534;&#36753;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#35780;&#20272;&#31561;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#25991;&#26412;&#20043;&#38388;&#32463;&#24120;&#20250;&#20986;&#29616;&#37325;&#21472;&#12290;&#26356;&#22909;&#22320;&#35780;&#20272;&#37325;&#21472;&#21477;&#23376;&#20043;&#38388;&#30340;&#35821;&#20041;&#36317;&#31163;&#26377;&#21161;&#20110;&#35821;&#35328;&#31995;&#32479;&#30340;&#29702;&#35299;&#24182;&#25351;&#23548;&#29983;&#25104;&#12290;&#30001;&#20110;&#20256;&#32479;&#30340;&#35821;&#20041;&#24230;&#37327;&#22522;&#20110;&#21333;&#35789;&#34920;&#31034;&#65292;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#20855;&#26377;&#31867;&#20284;&#34920;&#31034;&#30340;&#37325;&#21472;&#37096;&#20998;&#30340;&#24178;&#25200;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25513;&#30721;&#21644;&#39044;&#27979;&#31574;&#30053;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#26368;&#38271;&#20844;&#20849;&#24207;&#21015;&#65288;LCS&#65289;&#20013;&#30340;&#21333;&#35789;&#20316;&#20026;&#37051;&#36817;&#21333;&#35789;&#65292;&#24182;&#20351;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#26469;&#39044;&#27979;&#20854;&#20301;&#32622;&#19978;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#21363;&#37051;&#36817;&#20998;&#24067;&#25955;&#24230;&#65288;NDD&#65289;&#65292;&#36890;&#36807;&#35745;&#31639;&#37325;&#21472;&#37096;&#20998;&#20013;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#34920;&#31034;&#35821;&#20041;&#36317;&#31163;&#12290;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#27979;&#35797;&#20013;&#65292;&#23454;&#39564;&#35777;&#26126;NDD&#23545;&#20110;&#21508;&#31181;&#35821;&#20041;&#24046;&#24322;&#26356;&#20026;&#25935;&#24863;&#65292;
&lt;/p&gt;
&lt;p&gt;
Overlapping frequently occurs in paired texts in natural language processing tasks like text editing and semantic similarity evaluation. Better evaluation of the semantic distance between the overlapped sentences benefits the language system's understanding and guides the generation. Since conventional semantic metrics are based on word representations, they are vulnerable to the disturbance of overlapped components with similar representations. This paper aims to address the issue with a mask-and-predict strategy. We take the words in the longest common sequence (LCS) as neighboring words and use masked language modeling (MLM) from pre-trained language models (PLMs) to predict the distributions on their positions. Our metric, Neighboring Distribution Divergence (NDD), represent the semantic distance by calculating the divergence between distributions in the overlapped parts. Experiments on Semantic Textual Similarity show NDD to be more sensitive to various semantic differences, espec
&lt;/p&gt;</description></item></channel></rss>