<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>PointLLM&#26159;&#19968;&#31181;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#28857;&#20113;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#28857;&#20113;&#32534;&#30721;&#22120;&#21644;&#24378;&#22823;&#30340;LLM&#23558;&#20960;&#20309;&#12289;&#22806;&#35266;&#21644;&#35821;&#35328;&#20449;&#24687;&#34701;&#21512;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#25351;&#23548;&#29983;&#25104;&#29615;&#22659;&#19978;&#24688;&#24403;&#30340;&#21709;&#24212;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25910;&#38598;&#22823;&#35268;&#27169;&#30340;&#28857;-&#25991;&#26412;&#25351;&#20196;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16911</link><description>&lt;p&gt;
PointLLM&#65306;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#28857;&#20113;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
PointLLM: Empowering Large Language Models to Understand Point Clouds. (arXiv:2308.16911v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16911
&lt;/p&gt;
&lt;p&gt;
PointLLM&#26159;&#19968;&#31181;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#28857;&#20113;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#28857;&#20113;&#32534;&#30721;&#22120;&#21644;&#24378;&#22823;&#30340;LLM&#23558;&#20960;&#20309;&#12289;&#22806;&#35266;&#21644;&#35821;&#35328;&#20449;&#24687;&#34701;&#21512;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#25351;&#23548;&#29983;&#25104;&#29615;&#22659;&#19978;&#24688;&#24403;&#30340;&#21709;&#24212;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25910;&#38598;&#22823;&#35268;&#27169;&#30340;&#28857;-&#25991;&#26412;&#25351;&#20196;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#36827;&#23637;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#65292;&#20294;&#22312;3D&#29702;&#35299;&#39046;&#22495;&#20173;&#26377;&#24453;&#23436;&#20840;&#21457;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PointLLM&#65292;&#36825;&#26159;&#19968;&#39033;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#30340;&#21021;&#27493;&#24037;&#20316;&#65292;&#20351;LLM&#33021;&#22815;&#29702;&#35299;&#28857;&#20113;&#65292;&#24182;&#25552;&#20379;&#20102;&#36229;&#36234;2D&#35270;&#35273;&#25968;&#25454;&#30340;&#26032;&#36884;&#24452;&#12290;PointLLM&#36890;&#36807;&#20154;&#31867;&#25351;&#23548;&#22788;&#29702;&#24102;&#26377;&#39068;&#33394;&#30340;&#29289;&#20307;&#28857;&#20113;&#65292;&#24182;&#29983;&#25104;&#29615;&#22659;&#19978;&#24688;&#24403;&#30340;&#21709;&#24212;&#65292;&#23637;&#31034;&#20102;&#20854;&#23545;&#28857;&#20113;&#21644;&#24120;&#35782;&#30340;&#25484;&#25569;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#20010;&#28857;&#20113;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#24378;&#22823;&#30340;LLM&#65292;&#26377;&#25928;&#22320;&#34701;&#21512;&#20102;&#20960;&#20309;&#12289;&#22806;&#35266;&#21644;&#35821;&#35328;&#20449;&#24687;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;66&#19975;&#20010;&#31616;&#21333;&#21644;7&#19975;&#20010;&#22797;&#26434;&#30340;&#28857;-&#25991;&#26412;&#25351;&#20196;&#23545;&#65292;&#20197;&#23454;&#29616;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#65306;&#39318;&#20808;&#23545;&#40784;&#28508;&#22312;&#31354;&#38388;&#65292;&#28982;&#21518;&#23545;&#32479;&#19968;&#27169;&#22411;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#12290;&#20026;&#20102;&#20005;&#26684;&#35780;&#20272;&#25105;&#20204;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#35780;&#20272;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The unprecedented advancements in Large Language Models (LLMs) have created a profound impact on natural language processing but are yet to fully embrace the realm of 3D understanding. This paper introduces PointLLM, a preliminary effort to fill this gap, thereby enabling LLMs to understand point clouds and offering a new avenue beyond 2D visual data. PointLLM processes colored object point clouds with human instructions and generates contextually appropriate responses, illustrating its grasp of point clouds and common sense. Specifically, it leverages a point cloud encoder with a powerful LLM to effectively fuse geometric, appearance, and linguistic information. We collect a novel dataset comprising 660K simple and 70K complex point-text instruction pairs to enable a two-stage training strategy: initially aligning latent spaces and subsequently instruction-tuning the unified model. To rigorously evaluate our model's perceptual abilities and its generalization capabilities, we establis
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24314;&#31435;&#20102;&#33258;&#27880;&#24847;&#21147;&#21644;&#30828;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#20043;&#38388;&#30340;&#27491;&#24335;&#31561;&#20215;&#20851;&#31995;&#65292;&#36890;&#36807;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#20248;&#21270;&#20960;&#20309;&#26469;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#36716;&#25442;&#22120;&#30340;&#38544;&#24335;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2308.16898</link><description>&lt;p&gt;
Transformers&#20316;&#20026;&#25903;&#25345;&#21521;&#37327;&#26426;
&lt;/p&gt;
&lt;p&gt;
Transformers as Support Vector Machines. (arXiv:2308.16898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16898
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24314;&#31435;&#20102;&#33258;&#27880;&#24847;&#21147;&#21644;&#30828;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#20043;&#38388;&#30340;&#27491;&#24335;&#31561;&#20215;&#20851;&#31995;&#65292;&#36890;&#36807;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#20248;&#21270;&#20960;&#20309;&#26469;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#36716;&#25442;&#22120;&#30340;&#38544;&#24335;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;"Attention Is All You Need"&#20013;&#24341;&#20837;&#36716;&#25442;&#22120;&#26550;&#26500;&#20197;&#26469;&#65292;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#12290;&#36716;&#25442;&#22120;&#20013;&#30340;&#27880;&#24847;&#21147;&#23618;&#25509;&#21463;&#36755;&#20837;&#20196;&#29260;&#24207;&#21015;$X$&#24182;&#36890;&#36807;&#35745;&#31639;softmax$(XQK^\top X^\top)$&#30340;&#25104;&#23545;&#30456;&#20284;&#24615;&#20351;&#23427;&#20204;&#30456;&#20114;&#20316;&#29992;&#65292;&#20854;&#20013;$(K,Q)$&#26159;&#21487;&#35757;&#32451;&#30340;&#38190;-&#26597;&#35810;&#21442;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#33258;&#27880;&#24847;&#21147;&#20248;&#21270;&#20960;&#20309;&#21644;&#19968;&#20010;&#30828;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#20043;&#38388;&#30340;&#27491;&#24335;&#31561;&#20215;&#20851;&#31995;&#65292;&#36890;&#36807;&#23545;&#20196;&#29260;&#23545;&#30340;&#22806;&#31215;&#26045;&#21152;&#32447;&#24615;&#32422;&#26463;&#65292;&#23558;&#26368;&#20339;&#36755;&#20837;&#20196;&#29260;&#19982;&#38750;&#26368;&#20339;&#20196;&#29260;&#20998;&#31163;&#12290;&#36825;&#20010;&#24418;&#24335;&#20027;&#20041;&#20351;&#25105;&#20204;&#33021;&#22815;&#34920;&#24449;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#21333;&#23618;&#36716;&#25442;&#22120;&#30340;&#38544;&#24335;&#20559;&#24046;&#65306;(1)&#20248;&#21270;&#27880;&#24847;&#21147;&#23618;&#65292;&#20351;&#29992;&#21487;&#21464;&#27491;&#21017;&#21270;&#21442;&#25968;$(K,Q)$&#65292;&#25910;&#25947;&#30340;&#26041;&#21521;&#26159;&#19968;&#20010;&#26368;&#23567;&#21270;&#32508;&#21512;&#21442;&#25968;$W=KQ^\top$&#30340;&#26680;&#33539;&#25968;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#35299;&#20915;&#26041;&#26696;&#12290;&#32780;&#30452;&#25509;&#20351;&#29992;$W$&#36827;&#34892;&#21442;&#25968;&#21270;&#21017;&#26368;&#23567;&#21270;&#19968;&#20010;Frobenius&#33539;&#25968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since its inception in "Attention Is All You Need", transformer architecture has led to revolutionary advancements in NLP. The attention layer within the transformer admits a sequence of input tokens $X$ and makes them interact through pairwise similarities computed as softmax$(XQK^\top X^\top)$, where $(K,Q)$ are the trainable key-query parameters. In this work, we establish a formal equivalence between the optimization geometry of self-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs. This formalism allows us to characterize the implicit bias of 1-layer transformers optimized with gradient descent: (1) Optimizing the attention layer with vanishing regularization, parameterized by $(K,Q)$, converges in direction to an SVM solution minimizing the nuclear norm of the combined parameter $W=KQ^\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius norm objective. 
&lt;/p&gt;</description></item><item><title>TouchStone&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#65292;&#20351;&#29992;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35780;&#22996;&#26469;&#20840;&#38754;&#35780;&#20272;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#21508;&#31181;&#33021;&#21147;&#12290;&#36890;&#36807;&#26500;&#24314;&#32508;&#21512;&#30340;&#35270;&#35273;&#23545;&#35805;&#25968;&#25454;&#38598;&#21644;&#25972;&#21512;&#22270;&#20687;&#27880;&#37322;&#65292;&#35780;&#20272;&#21253;&#25324;&#35782;&#21035;&#12289;&#29702;&#35299;&#12289;&#23545;&#35805;&#21644;&#21465;&#20107;&#31561;&#22810;&#20010;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16890</link><description>&lt;p&gt;
TouchStone: &#29992;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TouchStone: Evaluating Vision-Language Models by Language Models. (arXiv:2308.16890v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16890
&lt;/p&gt;
&lt;p&gt;
TouchStone&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#65292;&#20351;&#29992;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35780;&#22996;&#26469;&#20840;&#38754;&#35780;&#20272;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#21508;&#31181;&#33021;&#21147;&#12290;&#36890;&#36807;&#26500;&#24314;&#32508;&#21512;&#30340;&#35270;&#35273;&#23545;&#35805;&#25968;&#25454;&#38598;&#21644;&#25972;&#21512;&#22270;&#20687;&#27880;&#37322;&#65292;&#35780;&#20272;&#21253;&#25324;&#35782;&#21035;&#12289;&#29702;&#35299;&#12289;&#23545;&#35805;&#21644;&#21465;&#20107;&#31561;&#22810;&#20010;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#25509;&#25910;&#22120;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30456;&#36830;&#25509;&#65292;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#24863;&#30693;&#12289;&#29702;&#35299;&#21644;&#22788;&#29702;&#35270;&#35273;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#35780;&#20272;&#20027;&#35201;&#20851;&#27880;&#35782;&#21035;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#32570;&#20047;&#23545;&#23545;&#35805;&#33021;&#21147;&#21644;&#35270;&#35273;&#21465;&#20107;&#33021;&#21147;&#30340;&#30452;&#25509;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#65292;&#20351;&#29992;&#24378;&#22823;&#30340;LLMs&#20316;&#20026;&#35780;&#22996;&#26469;&#20840;&#38754;&#35780;&#20272;LVLMs&#30340;&#21508;&#31181;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#24320;&#25918;&#19990;&#30028;&#22270;&#20687;&#21644;&#38382;&#39064;&#30340;&#32508;&#21512;&#35270;&#35273;&#23545;&#35805;&#25968;&#25454;&#38598;TouchStone&#65292;&#28085;&#30422;&#20102;&#20116;&#20010;&#20027;&#35201;&#33021;&#21147;&#21644;27&#20010;&#23376;&#20219;&#21153;&#12290;&#35813;&#25968;&#25454;&#38598;&#19981;&#20165;&#28085;&#30422;&#20102;&#22522;&#30784;&#30340;&#35782;&#21035;&#21644;&#29702;&#35299;&#65292;&#36824;&#25193;&#23637;&#21040;&#25991;&#23398;&#21019;&#20316;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#25972;&#21512;&#35814;&#32454;&#30340;&#22270;&#20687;&#27880;&#37322;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#23558;&#22810;&#27169;&#24577;&#36755;&#20837;&#20869;&#23481;&#36716;&#21270;&#20026;LLMs&#21487;&#20197;&#29702;&#35299;&#30340;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large vision-language models (LVLMs) have recently witnessed rapid advancements, exhibiting a remarkable capacity for perceiving, understanding, and processing visual information by connecting visual receptor with large language models (LLMs). However, current assessments mainly focus on recognizing and reasoning abilities, lacking direct evaluation of conversational skills and neglecting visual storytelling abilities. In this paper, we propose an evaluation method that uses strong LLMs as judges to comprehensively evaluate the various abilities of LVLMs. Firstly, we construct a comprehensive visual dialogue dataset TouchStone, consisting of open-world images and questions, covering five major categories of abilities and 27 subtasks. This dataset not only covers fundamental recognition and comprehension but also extends to literary creation. Secondly, by integrating detailed image annotations we effectively transform the multimodal input content into a form understandable by LLMs. This
&lt;/p&gt;</description></item><item><title>Belebele&#26159;&#19968;&#20010;&#21253;&#21547;122&#31181;&#35821;&#35328;&#21464;&#20307;&#30340;&#22810;&#36873;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#27169;&#22411;&#22312;&#39640;&#12289;&#20013;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23567;&#22411;&#22810;&#35821;&#35328;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2308.16884</link><description>&lt;p&gt;
Belebele&#22522;&#20934;&#25968;&#25454;&#38598;&#65306;122&#31181;&#35821;&#35328;&#21464;&#20307;&#30340;&#24182;&#34892;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants. (arXiv:2308.16884v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16884
&lt;/p&gt;
&lt;p&gt;
Belebele&#26159;&#19968;&#20010;&#21253;&#21547;122&#31181;&#35821;&#35328;&#21464;&#20307;&#30340;&#22810;&#36873;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#27169;&#22411;&#22312;&#39640;&#12289;&#20013;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23567;&#22411;&#22810;&#35821;&#35328;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Belebele&#65292;&#19968;&#20010;&#21253;&#21547;122&#31181;&#35821;&#35328;&#21464;&#20307;&#30340;&#22810;&#36873;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;MRC&#65289;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#26497;&#22823;&#22320;&#25193;&#23637;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#22522;&#20934;&#30340;&#35821;&#35328;&#35206;&#30422;&#33539;&#22260;&#65292;&#20351;&#24471;&#21487;&#20197;&#35780;&#20272;&#25991;&#26412;&#27169;&#22411;&#22312;&#39640;&#12289;&#20013;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#24615;&#33021;&#12290;&#27599;&#20010;&#38382;&#39064;&#37117;&#22522;&#20110;Flores-200&#25968;&#25454;&#38598;&#20013;&#30340;&#19968;&#20010;&#30701;&#31687;&#25991;&#31456;&#65292;&#24182;&#25552;&#20379;&#20102;&#22235;&#20010;&#22810;&#36873;&#31572;&#26696;&#12290;&#38382;&#39064;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#65292;&#20197;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#27700;&#24179;&#30340;&#27169;&#22411;&#12290;&#21333;&#29420;&#30340;&#33521;&#35821;&#25968;&#25454;&#38598;&#24050;&#32463;&#36275;&#22815;&#22256;&#38590;&#65292;&#21487;&#20197;&#25361;&#25112;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#30001;&#20110;&#23436;&#20840;&#24182;&#34892;&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#30452;&#25509;&#27604;&#36739;&#25152;&#26377;&#35821;&#35328;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35780;&#20272;&#22810;&#35821;&#35328;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#65288;MLMs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24191;&#27867;&#30340;&#32467;&#26524;&#65292;&#24182;&#21457;&#29616;&#23613;&#31649;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;LLMs&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#20294;&#23567;&#22411;MLMs&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#23545;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Belebele, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in high-, medium-, and low-resource languages. Each question is based on a short passage from the Flores-200 dataset and has four multiple-choice answers. The questions were carefully curated to discriminate between models with different levels of general language comprehension. The English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. We use this dataset to evaluate the capabilities of multilingual masked language models (MLMs) and large language models (LLMs). We present extensive results and find that despite significant cross-lingual transfer in English-centric LLMs, much small
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Gender-GAP Pipeline&#65292;&#19968;&#20010;&#29992;&#20110;55&#31181;&#35821;&#35328;&#20013;&#24615;&#21035;&#34920;&#24449;&#30340;&#33258;&#21160;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#35821;&#35328;&#24615;&#21035;&#20154;&#31216;&#21517;&#35789;&#35789;&#27719;&#34920;&#23545;&#25991;&#26412;&#36827;&#34892;&#37327;&#21270;&#26469;&#25253;&#21578;&#25968;&#25454;&#20013;&#30340;&#24615;&#21035;&#34920;&#24449;&#12290;&#22312;WMT&#35757;&#32451;&#25968;&#25454;&#21644;&#26032;&#38395;&#20219;&#21153;&#30340;&#24320;&#21457;&#25968;&#25454;&#20013;&#34920;&#26126;&#24403;&#21069;&#25968;&#25454;&#20559;&#21521;&#30007;&#24615;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.16871</link><description>&lt;p&gt;
The Gender-GAP Pipeline: &#19968;&#20010;&#29992;&#20110;55&#31181;&#35821;&#35328;&#20013;&#24615;&#21035;&#34920;&#24449;&#30340;&#24615;&#21035;&#24863;&#30693;&#22810;&#35821;&#35328;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
The Gender-GAP Pipeline: A Gender-Aware Polyglot Pipeline for Gender Characterisation in 55 Languages. (arXiv:2308.16871v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Gender-GAP Pipeline&#65292;&#19968;&#20010;&#29992;&#20110;55&#31181;&#35821;&#35328;&#20013;&#24615;&#21035;&#34920;&#24449;&#30340;&#33258;&#21160;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#35821;&#35328;&#24615;&#21035;&#20154;&#31216;&#21517;&#35789;&#35789;&#27719;&#34920;&#23545;&#25991;&#26412;&#36827;&#34892;&#37327;&#21270;&#26469;&#25253;&#21578;&#25968;&#25454;&#20013;&#30340;&#24615;&#21035;&#34920;&#24449;&#12290;&#22312;WMT&#35757;&#32451;&#25968;&#25454;&#21644;&#26032;&#38395;&#20219;&#21153;&#30340;&#24320;&#21457;&#25968;&#25454;&#20013;&#34920;&#26126;&#24403;&#21069;&#25968;&#25454;&#20559;&#21521;&#30007;&#24615;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#29983;&#25104;&#31995;&#32479;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#24456;&#38590;&#34987;&#32531;&#35299;&#12290;&#20854;&#20013;&#19968;&#20010;&#21487;&#33021;&#23548;&#33268;&#36825;&#31181;&#20559;&#35265;&#30340;&#21407;&#22240;&#26159;&#35757;&#32451;&#21644;&#35780;&#20272;&#25968;&#25454;&#20013;&#30340;&#24615;&#21035;&#34920;&#24449;&#19981;&#24179;&#34913;&#12290;&#23613;&#31649;&#26368;&#36817;&#22312;&#35760;&#24405;&#36825;&#20010;&#38382;&#39064;&#21644;&#35797;&#22270;&#32531;&#35299;&#23427;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#25105;&#20204;&#20173;&#28982;&#32570;&#20047;&#20849;&#20139;&#30340;&#26041;&#27861;&#35770;&#21644;&#24037;&#20855;&#65292;&#20197;&#25253;&#21578;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#30340;&#24615;&#21035;&#34920;&#24449;&#12290;&#36825;&#31181;&#23450;&#37327;&#25253;&#21578;&#23558;&#20351;&#36827;&#19968;&#27493;&#32531;&#35299;&#25104;&#20026;&#21487;&#33021;&#65292;&#20363;&#22914;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;Gender-GAP Pipeline&#65288;&#29992;&#20110;&#24615;&#21035;&#24863;&#30693;&#30340;&#22810;&#35821;&#35328;&#27969;&#27700;&#32447;&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#33258;&#21160;&#27969;&#31243;&#65292;&#29992;&#20110;&#23545;55&#31181;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#24615;&#21035;&#34920;&#24449;&#12290;&#35813;&#27969;&#27700;&#32447;&#20351;&#29992;&#19968;&#20010;&#22810;&#35821;&#35328;&#24615;&#21035;&#20154;&#31216;&#21517;&#35789;&#35789;&#27719;&#34920;&#26469;&#37327;&#21270;&#25991;&#26412;&#20013;&#30340;&#24615;&#21035;&#34920;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#26469;&#25253;&#21578;WMT&#35757;&#32451;&#25968;&#25454;&#21644;&#26032;&#38395;&#20219;&#21153;&#30340;&#24320;&#21457;&#25968;&#25454;&#20013;&#30340;&#24615;&#21035;&#34920;&#24449;&#65292;&#35777;&#23454;&#24403;&#21069;&#25968;&#25454;&#20559;&#21521;&#30007;&#24615;&#34920;&#24449;&#12290;&#25317;&#26377;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#21487;&#33021;&#20250;&#38388;&#25509;&#22320;&#20248;&#21270;&#25105;&#20204;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gender biases in language generation systems are challenging to mitigate. One possible source for these biases is gender representation disparities in the training and evaluation data. Despite recent progress in documenting this problem and many attempts at mitigating it, we still lack shared methodology and tooling to report gender representation in large datasets. Such quantitative reporting will enable further mitigation, e.g., via data augmentation. This paper describes the Gender-GAP Pipeline (for Gender-Aware Polyglot Pipeline), an automatic pipeline to characterize gender representation in large-scale datasets for 55 languages. The pipeline uses a multilingual lexicon of gendered person-nouns to quantify the gender representation in text. We showcase it to report gender representation in WMT training data and development data for the News task, confirming that current data is skewed towards masculine representation. Having unbalanced datasets may indirectly optimize our systems 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#32534;&#31243;&#35821;&#35328;&#21487;&#20197;&#22312;&#25351;&#20196;&#35843;&#20248;&#38454;&#27573;&#30456;&#20114;&#20419;&#36827;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#24444;&#27492;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16824</link><description>&lt;p&gt;
&#32534;&#31243;&#35821;&#35328;&#33021;&#36890;&#36807;&#25351;&#20196;&#35843;&#20248;&#30456;&#20114;&#25552;&#21319;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Programming Languages Boost Each Other via Instruction Tuning?. (arXiv:2308.16824v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16824
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#32534;&#31243;&#35821;&#35328;&#21487;&#20197;&#22312;&#25351;&#20196;&#35843;&#20248;&#38454;&#27573;&#30456;&#20114;&#20419;&#36827;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#24444;&#27492;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#31867;&#31243;&#24207;&#21592;&#25484;&#25569;&#20102;&#19968;&#31181;&#32534;&#31243;&#35821;&#35328;&#21518;&#65292;&#23398;&#20064;&#19968;&#31181;&#26032;&#30340;&#32534;&#31243;&#35821;&#35328;&#20250;&#26356;&#23481;&#26131;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#25506;&#35752;&#20102;&#22312;&#20195;&#30721;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#24494;&#35843;&#38454;&#27573;&#20013;&#65292;&#32534;&#31243;&#35821;&#35328;&#26159;&#21542;&#33021;&#22815;&#36890;&#36807;&#30456;&#20114;&#25552;&#21319;&#26469;&#22686;&#24378;&#24444;&#27492;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;StarCoder&#19978;&#23545;8&#31181;&#27969;&#34892;&#30340;&#32534;&#31243;&#35821;&#35328;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65288;Python&#65292;JavaScript&#65292;TypeScript&#65292;C&#65292;C ++&#65292;Java&#65292;Go&#65292;HTML&#65289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#32534;&#31243;&#35821;&#35328;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24444;&#27492;&#30340;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#36890;&#36807;&#22312;Python&#19978;&#35757;&#32451;&#30340;CodeM-Python 15B&#21487;&#20197;&#20351;Java&#30340;pass@1&#29575;&#32477;&#23545;&#22686;&#21152;&#20102;17.95&#65285;&#12290;&#26356;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#22312;HTML&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;CodeM-HTML 7B&#21487;&#20197;&#20351;Java&#30340;pass@1&#29575;&#32477;&#23545;&#22686;&#21152;&#20102;15.24&#65285;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#24050;&#32463;&#21457;&#24067;&#22312;https://github.com/NL2Code/CodeM&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
When human programmers have mastered a programming language, it would be easier when they learn a new programming language. In this report, we focus on exploring whether programming languages can boost each other during the instruction fine-tuning phase of code large language models. We conduct extensive experiments of 8 popular programming languages (Python, JavaScript, TypeScript, C, C++, Java, Go, HTML) on StarCoder. Results demonstrate that programming languages can significantly improve each other. For example, CodeM-Python 15B trained on Python is able to increase Java by an absolute 17.95% pass@1 on HumanEval-X. More surprisingly, we found that CodeM-HTML 7B trained on the HTML corpus can improve Java by an absolute 15.24% pass@1. Our training data is released at https://github.com/NL2Code/CodeM.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#24403;&#21069;&#35780;&#20272;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#26032;&#24314;&#31435;&#30340;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#33539;&#24335;&#65292;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#12289;&#22810;&#35821;&#35328;&#30340;&#23545;&#35805;&#35780;&#20272;&#25351;&#26631;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20010;&#26694;&#26550;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;DSTC11 Track 4&#20013;&#30340;&#31283;&#20581;&#21644;&#22810;&#35821;&#35328;&#20219;&#21153;&#20013;&#25490;&#21517;&#31532;&#19968;&#12290;</title><link>http://arxiv.org/abs/2308.16797</link><description>&lt;p&gt;
&#31616;&#21333;&#30340;LLM&#25552;&#31034;&#26159;&#31283;&#20581;&#19988;&#22810;&#35821;&#35328;&#23545;&#35805;&#35780;&#20215;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Simple LLM Prompting is State-of-the-Art for Robust and Multilingual Dialogue Evaluation. (arXiv:2308.16797v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#24403;&#21069;&#35780;&#20272;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#26032;&#24314;&#31435;&#30340;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#33539;&#24335;&#65292;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#12289;&#22810;&#35821;&#35328;&#30340;&#23545;&#35805;&#35780;&#20272;&#25351;&#26631;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20010;&#26694;&#26550;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;DSTC11 Track 4&#20013;&#30340;&#31283;&#20581;&#21644;&#22810;&#35821;&#35328;&#20219;&#21153;&#20013;&#25490;&#21517;&#31532;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#33258;&#21160;&#23545;&#35805;&#35780;&#20215;&#25351;&#26631;&#30340;&#24320;&#21457;&#19978;&#24050;&#32463;&#20184;&#20986;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#20294;&#23545;&#35780;&#20215;&#38750;&#33521;&#35821;&#23545;&#35805;&#30340;&#24605;&#32771;&#21364;&#24456;&#23569;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#30830;&#20445;&#25351;&#26631;&#23545;&#35821;&#20041;&#30456;&#20284;&#30340;&#22238;&#31572;&#19981;&#21464;&#20063;&#26159;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#31283;&#20581;&#24615;&#21644;&#22810;&#35821;&#35328;&#24615;&#23545;&#35805;&#35780;&#20272;&#25351;&#26631;&#30340;&#26399;&#26395;&#23646;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#24403;&#21069;&#35780;&#20272;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#26032;&#24314;&#31435;&#30340;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#33539;&#24335;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#20197;&#24179;&#22343;&#26031;&#30382;&#23572;&#26364;&#30456;&#20851;&#24471;&#20998;&#21019;&#36896;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;DSTC11 Track 4&#8220;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#30340;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#8221;&#20013;&#30340;&#31283;&#20581;&#21644;&#22810;&#35821;&#35328;&#20219;&#21153;&#20013;&#25490;&#21517;&#31532;&#19968;&#65292;&#35777;&#26126;&#20102;&#25552;&#31034;LLM&#30340;&#35780;&#20272;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant research effort in the development of automatic dialogue evaluation metrics, little thought is given to evaluating dialogues other than in English. At the same time, ensuring metrics are invariant to semantically similar responses is also an overlooked topic. In order to achieve the desired properties of robustness and multilinguality for dialogue evaluation metrics, we propose a novel framework that takes advantage of the strengths of current evaluation models with the newly-established paradigm of prompting Large Language Models (LLMs). Empirical results show our framework achieves state of the art results in terms of mean Spearman correlation scores across several benchmarks and ranks first place on both the Robust and Multilingual tasks of the DSTC11 Track 4 "Automatic Evaluation Metrics for Open-Domain Dialogue Systems", proving the evaluation capabilities of prompted LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#22686;&#24378;&#29616;&#26377;&#33521;&#35821;&#23545;&#35805;&#25968;&#25454;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#35821;&#35328;&#23545;&#35805;&#35780;&#20272;&#30340;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;&#28304;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#30340;&#24378;&#22522;&#20934;&#26041;&#27861;&#20248;&#20110;&#30452;&#25509;&#20351;&#29992;&#32763;&#35793;&#25968;&#25454;&#24494;&#35843;&#39044;&#35757;&#32451;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#26420;&#32032;&#26041;&#27861;&#12290;&#26368;&#20339;&#26041;&#27861;&#26159;&#21033;&#29992;&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#25351;&#26631;&#31934;&#24515;&#31579;&#36873;&#32763;&#35793;&#25968;&#25454;&#65292;&#25490;&#38500;&#20302;&#36136;&#37327;&#30340;&#32763;&#35793;&#12290;</title><link>http://arxiv.org/abs/2308.16795</link><description>&lt;p&gt;
&#23454;&#29616;&#22810;&#35821;&#35328;&#33258;&#21160;&#23545;&#35805;&#35780;&#20272;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Multilingual Automatic Dialogue Evaluation. (arXiv:2308.16795v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#22686;&#24378;&#29616;&#26377;&#33521;&#35821;&#23545;&#35805;&#25968;&#25454;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#35821;&#35328;&#23545;&#35805;&#35780;&#20272;&#30340;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;&#28304;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#30340;&#24378;&#22522;&#20934;&#26041;&#27861;&#20248;&#20110;&#30452;&#25509;&#20351;&#29992;&#32763;&#35793;&#25968;&#25454;&#24494;&#35843;&#39044;&#35757;&#32451;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#26420;&#32032;&#26041;&#27861;&#12290;&#26368;&#20339;&#26041;&#27861;&#26159;&#21033;&#29992;&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#25351;&#26631;&#31934;&#24515;&#31579;&#36873;&#32763;&#35793;&#25968;&#25454;&#65292;&#25490;&#38500;&#20302;&#36136;&#37327;&#30340;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#23545;&#35805;&#35780;&#20272;&#25351;&#26631;&#21457;&#23637;&#30340;&#20027;&#35201;&#38480;&#21046;&#22240;&#32032;&#26159;&#32570;&#20047;&#22810;&#35821;&#35328;&#25968;&#25454;&#21644;&#23569;&#37327;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#23545;&#35805;&#31995;&#32479;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36825;&#31181;&#25968;&#25454;&#19981;&#36275;&#30340;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24182;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;&#33521;&#35821;&#23545;&#35805;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#30452;&#25509;&#20351;&#29992;&#32763;&#35793;&#25968;&#25454;&#26469;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#32534;&#30721;&#27169;&#22411;&#30340;&#26420;&#32032;&#26041;&#27861;&#26080;&#27861;&#36229;&#36807;&#21482;&#20351;&#29992;&#28304;&#25968;&#25454;&#24494;&#35843;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22522;&#20934;&#12290;&#30456;&#21453;&#65292;&#26368;&#20339;&#26041;&#27861;&#26159;&#36890;&#36807;&#20351;&#29992;&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#25351;&#26631;&#26469;&#31934;&#24515;&#31579;&#36873;&#32763;&#35793;&#25968;&#25454;&#65292;&#25490;&#38500;&#20302;&#36136;&#37327;&#30340;&#32763;&#35793;&#65292;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main limiting factor in the development of robust multilingual dialogue evaluation metrics is the lack of multilingual data and the limited availability of open sourced multilingual dialogue systems. In this work, we propose a workaround for this lack of data by leveraging a strong multilingual pretrained LLM and augmenting existing English dialogue data using Machine Translation. We empirically show that the naive approach of finetuning a pretrained multilingual encoder model with translated data is insufficient to outperform the strong baseline of finetuning a multilingual model with only source data. Instead, the best approach consists in the careful curation of translated data using MT Quality Estimation metrics, excluding low quality translations that hinder its performance.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#21644;&#35268;&#21017;&#30340;&#25552;&#31034;&#35843;&#25972;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;PLM&#22312;&#21171;&#21160;&#21147;&#24066;&#22330;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#27169;&#22411;&#23618;&#65292;&#25163;&#21160;&#27880;&#37322;&#21644;&#25968;&#25454;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2308.16770</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25351;&#20196;&#30340;&#24494;&#35843;&#21644;&#35268;&#21017;&#30340;&#25552;&#31034;&#24494;&#35843;&#22686;&#24378;&#21171;&#21160;&#21147;&#24066;&#22330;&#20219;&#21153;&#30340;PLM&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Enhancing PLM Performance on Labour Market Tasks via Instruction-based Finetuning and Prompt-tuning with Rules. (arXiv:2308.16770v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16770
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#21644;&#35268;&#21017;&#30340;&#25552;&#31034;&#35843;&#25972;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;PLM&#22312;&#21171;&#21160;&#21147;&#24066;&#22330;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#27169;&#22411;&#23618;&#65292;&#25163;&#21160;&#27880;&#37322;&#21644;&#25968;&#25454;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#25968;&#23383;&#21270;&#22686;&#38271;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#12289;&#25945;&#32946;&#32773;&#21644;&#20225;&#19994;&#33021;&#22815;&#20998;&#26512;&#21644;&#26356;&#22909;&#22320;&#29702;&#35299;&#21171;&#21160;&#21147;&#24066;&#22330;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21171;&#21160;&#21147;&#24066;&#22330;&#36164;&#28304;&#25968;&#37327;&#24222;&#22823;&#65292;&#20294;&#24448;&#24448;&#26159;&#38750;&#32467;&#26500;&#21270;&#30340;&#65292;&#22240;&#27492;&#65292;&#23545;&#20110;&#23454;&#20307;&#30340;&#35782;&#21035;&#12289;&#20851;&#32852;&#21644;&#25552;&#21462;&#30340;&#26041;&#27861;&#30740;&#31350;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#36861;&#27714;&#26356;&#22909;&#30340;&#21171;&#21160;&#21147;&#24066;&#22330;&#34920;&#29616;&#30340;&#32972;&#26223;&#19979;&#65292;&#36164;&#28304;&#38480;&#21046;&#21644;&#22823;&#35268;&#27169;&#26631;&#27880;&#25968;&#25454;&#19981;&#21487;&#29992;&#23548;&#33268;&#20154;&#31867;&#39046;&#22495;&#19987;&#23478;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;&#21171;&#21160;&#21147;&#24066;&#22330;&#29305;&#23450;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;PTR&#21644;&#27809;&#26377;&#31034;&#20363;&#30340;&#25351;&#20196;&#35843;&#25972;&#31561;&#25104;&#26412;&#25928;&#30410;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;PLM&#22312;&#21171;&#21160;&#21147;&#24066;&#22330;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#24341;&#20837;&#39069;&#22806;&#30340;&#27169;&#22411;&#23618;&#12289;&#25163;&#21160;&#27880;&#37322;&#21644;&#25968;&#25454;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increased digitization of the labour market has given researchers, educators, and companies the means to analyze and better understand the labour market. However, labour market resources, although available in high volumes, tend to be unstructured, and as such, research towards methodologies for the identification, linking, and extraction of entities becomes more and more important. Against the backdrop of this quest for better labour market representations, resource constraints and the unavailability of large-scale annotated data cause a reliance on human domain experts. We demonstrate the effectiveness of prompt-based tuning of pre-trained language models (PLM) in labour market specific applications. Our results indicate that cost-efficient methods such as PTR and instruction tuning without exemplars can significantly increase the performance of PLMs on downstream labour market applications without introducing additional model layers, manual annotations, and data augmentation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Ladder-of-Thought&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#26469;&#25552;&#21319;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#23567;&#22411;&#27169;&#22411;&#22312;&#24212;&#29992;&#20808;&#21069;&#20869;&#37096;&#30693;&#35782;&#26102;&#24615;&#33021;&#25552;&#21319;&#19981;&#26126;&#26174;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#22823;&#35268;&#27169;&#27169;&#22411;&#22312;&#25928;&#29575;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.16763</link><description>&lt;p&gt;
Ladder-of-Thought: &#20351;&#29992;&#30693;&#35782;&#20316;&#20026;&#38454;&#26799;&#25552;&#21319;&#31435;&#22330;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Ladder-of-Thought: Using Knowledge as Steps to Elevate Stance Detection. (arXiv:2308.16763v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16763
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Ladder-of-Thought&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#26469;&#25552;&#21319;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#23567;&#22411;&#27169;&#22411;&#22312;&#24212;&#29992;&#20808;&#21069;&#20869;&#37096;&#30693;&#35782;&#26102;&#24615;&#33021;&#25552;&#21319;&#19981;&#26126;&#26174;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#22823;&#35268;&#27169;&#27169;&#22411;&#22312;&#25928;&#29575;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24605;&#32500;&#38142;&#24335;&#25552;&#20379;&#65288;CoT&#65289;&#36890;&#36807;&#29983;&#25104;&#20013;&#38388;&#30340;&#25512;&#29702;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22686;&#24378;&#20027;&#35201;&#26377;&#30410;&#20110;&#22823;&#35268;&#27169;&#27169;&#22411;&#65292;&#22312;&#30452;&#25509;&#24212;&#29992;CoT&#26102;&#23567;&#22411;LLM&#30340;&#24615;&#33021;&#25913;&#36827;&#19981;&#26126;&#26174;&#12290;&#23613;&#31649;LLM&#20855;&#26377;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;CoT&#20027;&#35201;&#20381;&#36182;&#20110;&#20854;&#39044;&#20808;&#35757;&#32451;&#30340;&#20869;&#37096;&#30693;&#35782;&#65292;&#20808;&#21069;&#26410;&#30693;&#20110;&#27169;&#22411;&#30340;&#22806;&#37096;&#30693;&#35782;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#12290;&#22312;&#31435;&#22330;&#26816;&#27979;&#31561;&#20219;&#21153;&#20013;&#65292;&#22806;&#37096;&#32972;&#26223;&#30693;&#35782;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#36825;&#31181;&#36951;&#28431;&#21464;&#24471;&#26356;&#21152;&#26126;&#26174;&#12290;&#27492;&#22806;&#65292;LLM&#30340;&#22823;&#35268;&#27169;&#26550;&#26500;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#19981;&#21487;&#36991;&#20813;&#22320;&#23384;&#22312;&#25928;&#29575;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#31435;&#22330;&#26816;&#27979;&#30340;&#24605;&#32500;&#38454;&#26799;&#65288;LoT&#65289;&#12290;LoT&#22522;&#20110;&#21452;&#38454;&#27573;&#32423;&#32852;&#20248;&#21270;&#26694;&#26550;&#65292;&#25351;&#23548;&#27169;&#22411;&#25972;&#21512;&#39640;&#36136;&#37327;&#30340;&#22806;&#37096;&#30693;&#35782;&#65292;&#22686;&#24378;&#20013;&#38388;&#27493;&#39588;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought Prompting (CoT) reinforces the reasoning capabilities of Large Language Models (LLMs) through the generation of intermediate rationales. However, these enhancements predominantly benefit large-scale models, leaving small LMs without significant performance improvements when directly applying CoT. Despite the advanced reasoning capabilities of LLMs, CoT relies primarily on their pre-trained internal knowledge. The external knowledge that is previously unknown to the model remains unexploited. This omission becomes pronounced in tasks such as stance detection, where the external background knowledge plays a pivotal role. Additionally, the large-scale architecture of LLMs inevitably present efficiency challenges during deployment. To address these challenges, we introduce the Ladder-of-Thought (LoT) for stance detection. Grounded in a dual-phase Cascaded Optimization framework, LoT directs the model to incorporate high-quality external knowledge, enhancing the intermediat
&lt;/p&gt;</description></item><item><title>CReHate&#36890;&#36807;&#36328;&#25991;&#21270;&#37325;&#26032;&#27880;&#37322;&#33521;&#35821;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#30340;&#20010;&#20307;&#23545;&#20167;&#24680;&#35328;&#35770;&#30340;&#19981;&#21516;&#30475;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20855;&#26377;&#25991;&#21270;&#25935;&#24863;&#24615;&#30340;&#20998;&#31867;&#22120;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#37325;&#26032;&#35780;&#20272;NLP&#30740;&#31350;&#22312;&#20167;&#24680;&#35328;&#35770;&#39046;&#22495;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16705</link><description>&lt;p&gt;
CReHate: &#36328;&#25991;&#21270;&#37325;&#26032;&#27880;&#37322;&#33521;&#35821;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CReHate: Cross-cultural Re-annotation of English Hate Speech Dataset. (arXiv:2308.16705v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16705
&lt;/p&gt;
&lt;p&gt;
CReHate&#36890;&#36807;&#36328;&#25991;&#21270;&#37325;&#26032;&#27880;&#37322;&#33521;&#35821;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#30340;&#20010;&#20307;&#23545;&#20167;&#24680;&#35328;&#35770;&#30340;&#19981;&#21516;&#30475;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20855;&#26377;&#25991;&#21270;&#25935;&#24863;&#24615;&#30340;&#20998;&#31867;&#22120;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#37325;&#26032;&#35780;&#20272;NLP&#30740;&#31350;&#22312;&#20167;&#24680;&#35328;&#35770;&#39046;&#22495;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33521;&#35821;&#25968;&#25454;&#38598;&#20027;&#35201;&#21453;&#26144;&#20102;&#29305;&#23450;&#22269;&#23478;&#30340;&#35266;&#28857;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#25991;&#21270;&#20559;&#24046;&#12290;&#36825;&#22312;&#21463;&#20027;&#35266;&#24615;&#24433;&#21709;&#36739;&#22823;&#30340;&#20219;&#21153;&#65292;&#22914;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#29305;&#21035;&#26377;&#38382;&#39064;&#12290;&#20026;&#20102;&#28145;&#20837;&#20102;&#35299;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#30340;&#20010;&#20307;&#22914;&#20309;&#29702;&#35299;&#20167;&#24680;&#35328;&#35770;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CReHate&#65292;&#23545;&#25277;&#26679;&#30340;SBIC&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#36328;&#25991;&#21270;&#37325;&#26032;&#27880;&#37322;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#26469;&#33258;&#20116;&#20010;&#19981;&#21516;&#22269;&#23478;&#30340;&#27880;&#37322;&#65306;&#28595;&#22823;&#21033;&#20122;&#12289;&#26032;&#21152;&#22369;&#12289;&#21335;&#38750;&#12289;&#33521;&#22269;&#21644;&#32654;&#22269;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#21457;&#29616;&#22522;&#20110;&#22269;&#31821;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#21482;&#26377;59.4%&#30340;&#26679;&#26412;&#22312;&#25152;&#26377;&#22269;&#23478;&#20043;&#38388;&#36798;&#25104;&#20849;&#35782;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#24341;&#20837;&#20102;&#19968;&#31181;&#20855;&#26377;&#25991;&#21270;&#25935;&#24863;&#24615;&#30340;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#65292;&#33021;&#22815;&#25429;&#25417;&#19981;&#21516;&#22269;&#31821;&#30340;&#35266;&#28857;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20167;&#24680;&#35328;&#35770;&#30340;&#32454;&#24494;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
English datasets predominantly reflect the perspectives of certain nationalities, which can lead to cultural biases in models and datasets. This is particularly problematic in tasks heavily influenced by subjectivity, such as hate speech detection. To delve into how individuals from different countries perceive hate speech, we introduce CReHate, a cross-cultural re-annotation of the sampled SBIC dataset. This dataset includes annotations from five distinct countries: Australia, Singapore, South Africa, the United Kingdom, and the United States. Our thorough statistical analysis highlights significant differences based on nationality, with only 59.4% of the samples achieving consensus among all countries. We also introduce a culturally sensitive hate speech classifier via transfer learning, adept at capturing perspectives of different nationalities. These findings underscore the need to re-evaluate certain aspects of NLP research, especially with regard to the nuanced nature of hate spe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#35821;&#38899;&#20998;&#35789;&#22120;SpeechTokenizer&#65292;&#36890;&#36807;&#32479;&#19968;&#35821;&#20041;&#21644;&#22768;&#23398;&#26631;&#35760;&#24182;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#23618;&#32423;&#19978;&#35299;&#32806;&#35821;&#38899;&#20449;&#24687;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65288;USLM&#65289;&#12290;</title><link>http://arxiv.org/abs/2308.16692</link><description>&lt;p&gt;
SpeechTokenizer&#65306;&#38754;&#21521;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#35821;&#38899;&#20998;&#35789;&#22120;
&lt;/p&gt;
&lt;p&gt;
SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models. (arXiv:2308.16692v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16692
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#35821;&#38899;&#20998;&#35789;&#22120;SpeechTokenizer&#65292;&#36890;&#36807;&#32479;&#19968;&#35821;&#20041;&#21644;&#22768;&#23398;&#26631;&#35760;&#24182;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#23618;&#32423;&#19978;&#35299;&#32806;&#35821;&#38899;&#20449;&#24687;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65288;USLM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#31163;&#25955;&#30340;&#35821;&#38899;&#34920;&#31034;&#65292;&#21487;&#20197;&#20998;&#20026;&#35821;&#20041;&#26631;&#35760;&#21644;&#22768;&#23398;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35821;&#38899;&#26631;&#35760;&#24182;&#38750;&#19987;&#20026;&#35821;&#38899;&#35821;&#35328;&#24314;&#27169;&#32780;&#35774;&#35745;&#12290;&#20026;&#20102;&#35780;&#20272;&#35821;&#38899;&#26631;&#35760;&#22312;&#26500;&#24314;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#36866;&#24212;&#24615;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#31532;&#19968;&#20010;&#22522;&#20934;&#26631;&#20934;&#65292;&#21363;SLMTokBench&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#35821;&#20041;&#26631;&#35760;&#36824;&#26159;&#22768;&#23398;&#26631;&#35760;&#37117;&#19981;&#36866;&#21512;&#36825;&#20010;&#30446;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SpeechTokenizer&#65292;&#19968;&#31181;&#38754;&#21521;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#35821;&#38899;&#20998;&#35789;&#22120;&#12290;SpeechTokenizer&#37319;&#29992;&#20855;&#26377;&#27531;&#24046;&#21521;&#37327;&#37327;&#21270;&#65288;RVQ&#65289;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#12290;&#36890;&#36807;&#32479;&#19968;&#35821;&#20041;&#21644;&#22768;&#23398;&#26631;&#35760;&#65292;SpeechTokenizer&#22312;&#19981;&#21516;&#30340;RVQ&#23618;&#32423;&#19978;&#20197;&#23618;&#27425;&#26041;&#24335;&#35299;&#32806;&#35821;&#38899;&#20449;&#24687;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21033;&#29992;SpeechTokenizer&#30340;&#32479;&#19968;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65288;USLM&#65289;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SpeechTokenizer&#22312;&#35821;&#38899;&#37325;&#24314;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#20998;&#31867;&#31185;&#23398;&#25991;&#31456;&#30340;&#26041;&#27861;&#65292;&#20027;&#35201;&#24212;&#29992;&#20110;&#30524;&#31185;&#23398;&#39046;&#22495;&#65292;&#20294;&#21487;&#25193;&#23637;&#21040;&#20854;&#20182;&#39046;&#22495;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;LLM&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;LLMs&#22312;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#33021;&#26377;&#25928;&#22320;&#23545;&#22823;&#37327;&#30524;&#31185;&#23398;&#35770;&#25991;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2308.16688</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#31185;&#23398;&#25991;&#31456;&#30340;&#20998;&#31867;&#21644;&#36235;&#21183;&#20998;&#26512;&#65306;&#20197;&#30524;&#31185;&#23398;&#20026;&#24212;&#29992;&#23454;&#20363;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Models to Automate Category and Trend Analysis of Scientific Articles: An Application in Ophthalmology. (arXiv:2308.16688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#20998;&#31867;&#31185;&#23398;&#25991;&#31456;&#30340;&#26041;&#27861;&#65292;&#20027;&#35201;&#24212;&#29992;&#20110;&#30524;&#31185;&#23398;&#39046;&#22495;&#65292;&#20294;&#21487;&#25193;&#23637;&#21040;&#20854;&#20182;&#39046;&#22495;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;LLM&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;LLMs&#22312;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#33021;&#26377;&#25928;&#22320;&#23545;&#22823;&#37327;&#30524;&#31185;&#23398;&#35770;&#25991;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#21160;&#36827;&#34892;&#25991;&#31456;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#20027;&#35201;&#20851;&#27880;&#30524;&#31185;&#39046;&#22495;&#65292;&#20294;&#35813;&#27169;&#22411;&#21487;&#25193;&#23637;&#21040;&#20854;&#20182;&#39046;&#22495;&#12290;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65288;NLP&#65289;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21253;&#25324;&#39640;&#32423;LLM&#65292;&#29992;&#20110;&#22788;&#29702;&#21644;&#20998;&#26512;&#31185;&#23398;&#35770;&#25991;&#30340;&#25991;&#26412;&#20869;&#23481;&#12290;&#22312;LLM&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#38646;&#26679;&#26412;&#23398;&#20064;&#65288;ZSL&#65289;LLM&#27169;&#22411;&#65292;&#24182;&#19982;&#21452;&#21521;&#21644;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#65288;BART&#65289;&#21450;&#20854;&#21464;&#31181;&#65292;&#20197;&#21450;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#20174;&#21464;&#25442;&#22120;&#65288;BERT&#65289;&#21450;&#20854;&#21464;&#31181;&#65288;&#22914;distilBERT&#65292;SciBERT&#65292;PubmedBERT&#65292;BioBERT&#65289;&#36827;&#34892;&#27604;&#36739;&#12290;&#20998;&#31867;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27809;&#26377;&#20154;&#20026;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#65292;LLM&#22312;&#23545;&#22823;&#37327;&#30524;&#31185;&#23398;&#35770;&#25991;&#36827;&#34892;&#20998;&#31867;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: In this paper, we present an automated method for article classification, leveraging the power of Large Language Models (LLM). The primary focus is on the field of ophthalmology, but the model is extendable to other fields. Methods: We have developed a model based on Natural Language Processing (NLP) techniques, including advanced LLMs, to process and analyze the textual content of scientific papers. Specifically, we have employed zero-shot learning (ZSL) LLM models and compared against Bidirectional and Auto-Regressive Transformers (BART) and its variants, and Bidirectional Encoder Representations from Transformers (BERT), and its variant such as distilBERT, SciBERT, PubmedBERT, BioBERT. Results: The classification results demonstrate the effectiveness of LLMs in categorizing large number of ophthalmology papers without human intervention. Results: To evalute the LLMs, we compiled a dataset (RenD) of 1000 ocular disease-related articles, which were expertly annotated by a pan
&lt;/p&gt;</description></item><item><title>DictaBERT&#26159;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;BERT&#27169;&#22411;&#65292;&#38024;&#23545;&#29616;&#20195;&#24076;&#20271;&#26469;&#35821;&#65292;&#22312;&#22823;&#22810;&#25968;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#23427;&#36824;&#25552;&#20379;&#20102;&#20004;&#20010;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#29256;&#26412;&#65292;&#21487;&#29992;&#20110;&#24076;&#20271;&#26469;&#35821;&#25991;&#26412;&#20998;&#26512;&#20013;&#30340;&#21069;&#32512;&#20998;&#21106;&#21644;&#24418;&#24577;&#26631;&#27880;&#20219;&#21153;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#21457;&#24067;&#26088;&#22312;&#20419;&#36827;&#24076;&#20271;&#26469;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.16687</link><description>&lt;p&gt;
DictaBERT: &#19968;&#27454;&#29992;&#20110;&#29616;&#20195;&#24076;&#20271;&#26469;&#35821;&#30340;&#26368;&#20808;&#36827;BERT&#22871;&#20214;&#30340;&#32763;&#35793;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
DictaBERT: A State-of-the-Art BERT Suite for Modern Hebrew. (arXiv:2308.16687v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16687
&lt;/p&gt;
&lt;p&gt;
DictaBERT&#26159;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;BERT&#27169;&#22411;&#65292;&#38024;&#23545;&#29616;&#20195;&#24076;&#20271;&#26469;&#35821;&#65292;&#22312;&#22823;&#22810;&#25968;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#23427;&#36824;&#25552;&#20379;&#20102;&#20004;&#20010;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#29256;&#26412;&#65292;&#21487;&#29992;&#20110;&#24076;&#20271;&#26469;&#35821;&#25991;&#26412;&#20998;&#26512;&#20013;&#30340;&#21069;&#32512;&#20998;&#21106;&#21644;&#24418;&#24577;&#26631;&#27880;&#20219;&#21153;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#21457;&#24067;&#26088;&#22312;&#20419;&#36827;&#24076;&#20271;&#26469;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DictaBERT&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#29616;&#20195;&#24076;&#20271;&#26469;&#35821;&#30340;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;BERT&#27169;&#22411;&#65292;&#22312;&#22823;&#22810;&#25968;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#20004;&#20010;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#29256;&#26412;&#65292;&#26088;&#22312;&#25191;&#34892;&#24076;&#20271;&#26469;&#35821;&#25991;&#26412;&#20998;&#26512;&#30340;&#20004;&#20010;&#29305;&#23450;&#22522;&#26412;&#20219;&#21153;&#65306;&#21069;&#32512;&#20998;&#21106;&#21644;&#24418;&#24577;&#26631;&#27880;&#12290;&#36825;&#20123;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#20801;&#35768;&#20219;&#20309;&#24320;&#21457;&#20154;&#21592;&#21482;&#38656;&#35843;&#29992;HuggingFace&#27169;&#22411;&#19968;&#27425;&#21363;&#21487;&#23545;&#24076;&#20271;&#26469;&#35821;&#21477;&#23376;&#36827;&#34892;&#21069;&#32512;&#20998;&#21106;&#21644;&#24418;&#24577;&#26631;&#27880;&#65292;&#26080;&#38656;&#38598;&#25104;&#20219;&#20309;&#39069;&#22806;&#30340;&#24211;&#25110;&#20195;&#30721;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#35757;&#32451;&#30340;&#32454;&#33410;&#20197;&#21450;&#22312;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#27169;&#22411;&#19982;&#23637;&#31034;&#20854;&#20351;&#29992;&#30340;&#31034;&#20363;&#20195;&#30721;&#19968;&#36215;&#21457;&#24067;&#32473;&#31038;&#21306;&#12290;&#25105;&#20204;&#21457;&#24067;&#36825;&#20123;&#27169;&#22411;&#26159;&#20026;&#20102;&#24110;&#21161;&#36827;&#19968;&#27493;&#20419;&#36827;&#24076;&#20271;&#26469;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present DictaBERT, a new state-of-the-art pre-trained BERT model for modern Hebrew, outperforming existing models on most benchmarks. Additionally, we release two fine-tuned versions of the model, designed to perform two specific foundational tasks in the analysis of Hebrew texts: prefix segmentation and morphological tagging. These fine-tuned models allow any developer to perform prefix segmentation and morphological tagging of a Hebrew sentence with a single call to a HuggingFace model, without the need to integrate any additional libraries or code. In this paper we describe the details of the training as well and the results on the different benchmarks. We release the models to the community, along with sample code demonstrating their use. We release these models as part of our goal to help further research and development in Hebrew NLP.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#12290;&#26694;&#26550;&#21253;&#25324;&#35821;&#27861;&#21644;&#38169;&#35823;&#20462;&#27491;&#12289;&#20107;&#23454;&#25552;&#21462;&#21644;&#25968;&#25454;&#38598;&#29983;&#25104;&#19977;&#20010;&#25361;&#25112;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;LLMs&#22312;&#38646;-shot&#25552;&#31034;&#19979;&#36741;&#21161;&#30693;&#35782;&#22270;&#35889;&#29983;&#25104;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2308.16622</link><description>&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#24037;&#31243;&#20013;&#24320;&#21457;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Developing a Scalable Benchmark for Assessing Large Language Models in Knowledge Graph Engineering. (arXiv:2308.16622v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#12290;&#26694;&#26550;&#21253;&#25324;&#35821;&#27861;&#21644;&#38169;&#35823;&#20462;&#27491;&#12289;&#20107;&#23454;&#25552;&#21462;&#21644;&#25968;&#25454;&#38598;&#29983;&#25104;&#19977;&#20010;&#25361;&#25112;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;LLMs&#22312;&#38646;-shot&#25552;&#31034;&#19979;&#36741;&#21161;&#30693;&#35782;&#22270;&#35889;&#29983;&#25104;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#35780;&#20272;&#21644;&#30417;&#27979;&#20854;&#24615;&#33021;&#30340;&#36843;&#20999;&#38656;&#27714;&#28014;&#20986;&#27700;&#38754;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#30693;&#35782;&#22270;&#35889;&#24037;&#31243;&#65288;KGE&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#25361;&#25112;&#65292;&#28041;&#21450;&#35821;&#27861;&#21644;&#38169;&#35823;&#20462;&#27491;&#12289;&#20107;&#23454;&#25552;&#21462;&#21644;&#25968;&#25454;&#38598;&#29983;&#25104;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23613;&#31649;LLMs&#26159;&#26377;&#29992;&#30340;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#23578;&#19981;&#33021;&#22312;&#38646;-shot&#25552;&#31034;&#19979;&#36741;&#21161;&#30693;&#35782;&#22270;&#35889;&#29983;&#25104;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;LLM-KG-Bench&#26694;&#26550;&#25552;&#20379;&#20102;LLM&#22238;&#31572;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#23384;&#20648;&#65292;&#20197;&#21450;&#32479;&#35745;&#25968;&#25454;&#21644;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#25903;&#25345;&#25552;&#31034;&#24037;&#31243;&#21644;&#27169;&#22411;&#24615;&#33021;&#30340;&#36319;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the field of Large Language Models (LLMs) evolves at an accelerated pace, the critical need to assess and monitor their performance emerges. We introduce a benchmarking framework focused on knowledge graph engineering (KGE) accompanied by three challenges addressing syntax and error correction, facts extraction and dataset generation. We show that while being a useful tool, LLMs are yet unfit to assist in knowledge graph generation with zero-shot prompting. Consequently, our LLM-KG-Bench framework provides automatic evaluation and storage of LLM responses as well as statistical data and visualization tools to support tracking of prompt engineering and model performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#33258;&#21457;&#39118;&#26684;&#35821;&#38899;&#21644;&#33258;&#21457;&#34892;&#20026;&#26631;&#31614;&#30340;&#25968;&#37327;&#65292;&#20197;&#23454;&#29616;&#33258;&#21457;&#39118;&#26684;&#24314;&#27169;&#29992;&#20110;&#23545;&#35805;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#33258;&#21457;&#39118;&#26684;&#30340;&#35821;&#38899;&#20013;&#24314;&#27169;&#33258;&#21457;&#34892;&#20026;&#65292;&#24182;&#20174;&#25991;&#26412;&#20013;&#39044;&#27979;&#21512;&#29702;&#30340;&#33258;&#21457;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2308.16593</link><description>&lt;p&gt;
&#36890;&#36807;&#21322;&#30417;&#30563;&#39044;&#35757;&#32451;&#23454;&#29616;&#33258;&#21457;&#39118;&#26684;&#24314;&#27169;&#29992;&#20110;&#23545;&#35805;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Towards Spontaneous Style Modeling with Semi-supervised Pre-training for Conversational Text-to-Speech Synthesis. (arXiv:2308.16593v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#33258;&#21457;&#39118;&#26684;&#35821;&#38899;&#21644;&#33258;&#21457;&#34892;&#20026;&#26631;&#31614;&#30340;&#25968;&#37327;&#65292;&#20197;&#23454;&#29616;&#33258;&#21457;&#39118;&#26684;&#24314;&#27169;&#29992;&#20110;&#23545;&#35805;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#33258;&#21457;&#39118;&#26684;&#30340;&#35821;&#38899;&#20013;&#24314;&#27169;&#33258;&#21457;&#34892;&#20026;&#65292;&#24182;&#20174;&#25991;&#26412;&#20013;&#39044;&#27979;&#21512;&#29702;&#30340;&#33258;&#21457;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20013;&#32463;&#24120;&#21457;&#29983;&#30340;&#33258;&#21457;&#34892;&#20026;&#20351;&#24471;&#35821;&#38899;&#21548;&#36215;&#26469;&#26356;&#21152;&#20687;&#20154;&#31867;&#65292;&#32780;&#19981;&#26159;&#20687;&#26391;&#35835;&#12290;&#28982;&#32780;&#65292;&#21512;&#25104;&#33258;&#21457;&#39118;&#26684;&#30340;&#35821;&#38899;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#33258;&#21457;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#26631;&#35760;&#33258;&#21457;&#34892;&#20026;&#30340;&#25104;&#26412;&#36739;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#22686;&#21152;&#33258;&#21457;&#39118;&#26684;&#35821;&#38899;&#21644;&#33258;&#21457;&#34892;&#20026;&#26631;&#31614;&#30340;&#25968;&#37327;&#12290;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#36807;&#31243;&#20013;&#65292;&#32771;&#34385;&#20102;&#25991;&#26412;&#21644;&#35821;&#38899;&#20449;&#24687;&#65292;&#20197;&#20415;&#22312;&#35821;&#38899;&#20013;&#26816;&#27979;&#33258;&#21457;&#34892;&#20026;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#35821;&#35328;&#24863;&#30693;&#32534;&#30721;&#22120;&#26469;&#24314;&#27169;&#23545;&#35805;&#20013;&#27599;&#20010;&#21477;&#23376;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#34920;&#36798;&#24615;&#35821;&#38899;&#21512;&#25104;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#65292;&#33021;&#22815;&#22312;&#33258;&#21457;&#39118;&#26684;&#30340;&#35821;&#38899;&#20013;&#24314;&#27169;&#33258;&#21457;&#34892;&#20026;&#65292;&#24182;&#20174;&#25991;&#26412;&#20013;&#39044;&#27979;&#21512;&#29702;&#30340;&#33258;&#21457;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spontaneous behavior that often occurs in conversations makes speech more human-like compared to reading-style. However, synthesizing spontaneous-style speech is challenging due to the lack of high-quality spontaneous datasets and the high cost of labeling spontaneous behavior. In this paper, we propose a semi-supervised pre-training method to increase the amount of spontaneous-style speech and spontaneous behavioral labels. In the process of semi-supervised learning, both text and speech information are considered for detecting spontaneous behaviors labels in speech. Moreover, a linguistic-aware encoder is used to model the relationship between each sentence in the conversation. Experimental results indicate that our proposed method achieves superior expressive speech synthesis performance with the ability to model spontaneous behavior in spontaneous-style speech and predict reasonable spontaneous behavior from text.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26641;&#24418;&#32467;&#26500;&#8212;&#8212;&#35821;&#20041;&#26641;&#65292;&#29992;&#20110;&#35299;&#37322;&#24773;&#24863;&#32452;&#21512;&#12290;&#23427;&#36890;&#36807;&#25551;&#36848;&#19981;&#21516;&#35821;&#20041;&#35282;&#33394;&#19978;&#30340;&#32452;&#21512;&#35268;&#21017;&#26469;&#23454;&#29616;&#24773;&#24863;&#32452;&#21512;&#30340;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#20869;&#37096;&#31639;&#27861;&#36827;&#34892;&#36793;&#38469;&#21270;&#21644;&#23398;&#20064;&#65292;&#20197;&#20248;&#21270;&#20998;&#31867;&#24615;&#33021;&#12290;&#23450;&#37327;&#21644;&#23450;&#24615;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#20248;&#21270;&#30340;&#20998;&#31867;&#24615;&#33021;&#21644;&#23545;&#24773;&#24863;&#32452;&#21512;&#35821;&#20041;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16588</link><description>&lt;p&gt;
&#29992;&#28508;&#22312;&#35821;&#20041;&#26641;&#35299;&#37322;&#24773;&#24863;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Interpreting Sentiment Composition with Latent Semantic Tree. (arXiv:2308.16588v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26641;&#24418;&#32467;&#26500;&#8212;&#8212;&#35821;&#20041;&#26641;&#65292;&#29992;&#20110;&#35299;&#37322;&#24773;&#24863;&#32452;&#21512;&#12290;&#23427;&#36890;&#36807;&#25551;&#36848;&#19981;&#21516;&#35821;&#20041;&#35282;&#33394;&#19978;&#30340;&#32452;&#21512;&#35268;&#21017;&#26469;&#23454;&#29616;&#24773;&#24863;&#32452;&#21512;&#30340;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#20869;&#37096;&#31639;&#27861;&#36827;&#34892;&#36793;&#38469;&#21270;&#21644;&#23398;&#20064;&#65292;&#20197;&#20248;&#21270;&#20998;&#31867;&#24615;&#33021;&#12290;&#23450;&#37327;&#21644;&#23450;&#24615;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#20248;&#21270;&#30340;&#20998;&#31867;&#24615;&#33021;&#21644;&#23545;&#24773;&#24863;&#32452;&#21512;&#35821;&#20041;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#24773;&#24863;&#20998;&#26512;&#30340;&#20851;&#38190;&#65292;&#24773;&#24863;&#32452;&#21512;&#32771;&#34385;&#36890;&#36807;&#23376;&#25104;&#20998;&#30340;&#20998;&#31867;&#21644;&#24212;&#29992;&#20110;&#23427;&#20204;&#30340;&#35268;&#21017;&#23545;&#25104;&#20998;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20197;&#21069;&#24191;&#27867;&#30740;&#31350;&#36807;&#30340;&#21253;&#25324;&#26410;&#26631;&#35760;&#21644;&#24773;&#24863;&#26641;&#22312;&#20869;&#30340;&#20998;&#23618;&#26641;&#24418;&#32467;&#26500;&#22312;&#26412;&#36136;&#19978;&#26159;&#27425;&#20248;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#20041;&#26641;&#65292;&#19968;&#31181;&#33021;&#22815;&#20197;&#31995;&#32479;&#30340;&#26041;&#24335;&#35299;&#37322;&#24773;&#24863;&#32452;&#21512;&#30340;&#26032;&#26641;&#24418;&#32467;&#26500;&#12290;&#35821;&#20041;&#26641;&#26159;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#65288;CFG&#65289;&#30340;&#19968;&#20010;&#27966;&#29983;&#65292;&#25551;&#36848;&#20102;&#19981;&#21516;&#35821;&#20041;&#35282;&#33394;&#19978;&#30340;&#29305;&#23450;&#32452;&#21512;&#35268;&#21017;&#65292;&#20854;&#35774;&#35745;&#32463;&#36807;&#20102;&#20043;&#21069;&#30340;&#35821;&#35328;&#23398;&#32467;&#35770;&#30340;&#31934;&#24515;&#32771;&#34385;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24120;&#35268;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#23545;&#35821;&#20041;&#26641;&#30340;&#27880;&#37322;&#65292;&#35821;&#20041;&#26641;&#26159;&#19968;&#20010;&#28508;&#22312;&#21464;&#37327;&#12290;&#22240;&#27492;&#65292;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#20869;&#37096;&#31639;&#27861;&#23545;&#20854;&#36827;&#34892;&#36793;&#38469;&#21270;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20248;&#21270;&#20998;&#31867;&#24615;&#33021;&#12290;&#23450;&#37327;&#21644;&#23450;&#24615;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#23454;&#29616;&#20102;&#20248;&#21270;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#36824;&#33021;&#22815;&#35299;&#37322;&#24773;&#24863;&#32452;&#21512;&#30340;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the key to sentiment analysis, sentiment composition considers the classification of a constituent via classifications of its contained sub-constituents and rules operated on them. Such compositionality has been widely studied previously in the form of hierarchical trees including untagged and sentiment ones, which are intrinsically suboptimal in our view. To address this, we propose semantic tree, a new tree form capable of interpreting the sentiment composition in a principled way. Semantic tree is a derivation of a context-free grammar (CFG) describing the specific composition rules on difference semantic roles, which is designed carefully following previous linguistic conclusions. However, semantic tree is a latent variable since there is no its annotation in regular datasets. Thus, in our method, it is marginalized out via inside algorithm and learned to optimize the classification performance. Quantitative and qualitative results demonstrate that our method not only achieves b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23398;&#20064;&#28508;&#22312;&#32534;&#30721;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#32534;&#30721;&#26469;&#36716;&#25442;&#21477;&#23376;&#65292;&#33021;&#22815;&#32479;&#19968;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#19968;&#31181;&#35299;&#37322;&#20808;&#21069;&#25552;&#20986;&#30340;&#25216;&#26415;&#30340;&#21407;&#29702;&#24615;&#35266;&#28857;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#24378;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#26356;&#22909;&#25110;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.16584</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#23454;&#29616;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Text Style Transfer with Deep Generative Models. (arXiv:2308.16584v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16584
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23398;&#20064;&#28508;&#22312;&#32534;&#30721;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#32534;&#30721;&#26469;&#36716;&#25442;&#21477;&#23376;&#65292;&#33021;&#22815;&#32479;&#19968;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#19968;&#31181;&#35299;&#37322;&#20808;&#21069;&#25552;&#20986;&#30340;&#25216;&#26415;&#30340;&#21407;&#29702;&#24615;&#35266;&#28857;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#24378;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#26356;&#22909;&#25110;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#23454;&#29616;&#26080;&#30417;&#30563;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#23558;&#38750;&#24179;&#34892;&#35821;&#26009;&#20013;&#30340;&#27599;&#20010;&#21477;&#23376;-&#26631;&#31614;&#23545;&#24314;&#27169;&#20026;&#19968;&#20010;&#37096;&#20998;&#35266;&#27979;&#30340;&#23436;&#25972;&#22235;&#20803;&#32452;&#65292;&#35813;&#22235;&#20803;&#32452;&#36824;&#21253;&#21547;&#34920;&#31034;&#20869;&#23481;&#21644;&#39118;&#26684;&#30340;&#20004;&#20010;&#28508;&#22312;&#32534;&#30721;&#12290;&#36825;&#20123;&#32534;&#30721;&#36890;&#36807;&#21033;&#29992;&#35266;&#27979;&#25968;&#25454;&#20869;&#37096;&#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#36827;&#34892;&#23398;&#20064;&#12290;&#28982;&#21518;&#36890;&#36807;&#25805;&#32437;&#36825;&#20123;&#32534;&#30721;&#26469;&#23454;&#29616;&#21477;&#23376;&#30340;&#36716;&#25442;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#23558;&#20197;&#21069;&#30340;&#23884;&#20837;&#21644;&#21407;&#22411;&#26041;&#27861;&#32479;&#19968;&#20026;&#20004;&#20010;&#29305;&#27530;&#24418;&#24335;&#12290;&#23427;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#21407;&#29702;&#24615;&#30340;&#35266;&#28857;&#26469;&#35299;&#37322;&#39046;&#22495;&#20013;&#20808;&#21069;&#25552;&#20986;&#30340;&#25216;&#26415;&#65292;&#22914;&#23545;&#40784;&#32534;&#30721;&#22120;&#21644;&#23545;&#25239;&#35757;&#32451;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20960;&#20010;&#24378;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26356;&#22909;&#25110;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a general framework for unsupervised text style transfer with deep generative models. The framework models each sentence-label pair in the non-parallel corpus as partially observed from a complete quadruplet which additionally contains two latent codes representing the content and style, respectively. These codes are learned by exploiting dependencies inside the observed data. Then a sentence is transferred by manipulating them. Our framework is able to unify previous embedding and prototype methods as two special forms. It also provides a principled perspective to explain previously proposed techniques in the field such as aligned encoder and adversarial training. We further conduct experiments on three benchmarks. Both automatic and human evaluation results show that our methods achieve better or competitive results compared to several strong baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#22810;&#32423;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#21253;&#25324;&#36328;&#21477;&#23376;&#21644;&#20869;&#37096;&#21477;&#23376;&#30340;&#35821;&#35328;&#20449;&#24687;&#65292;&#25552;&#39640;&#26222;&#36890;&#35805;&#38901;&#24459;&#32467;&#26500;&#39044;&#27979;&#30340;&#24615;&#33021;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.16577</link><description>&lt;p&gt;
&#29992;&#22810;&#32423;&#19978;&#19979;&#25991;&#20449;&#24687;&#25552;&#21319;&#26222;&#36890;&#35805;&#38901;&#24459;&#32467;&#26500;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Mandarin Prosodic Structure Prediction with Multi-level Contextual Information. (arXiv:2308.16577v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#22810;&#32423;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#21253;&#25324;&#36328;&#21477;&#23376;&#21644;&#20869;&#37096;&#21477;&#23376;&#30340;&#35821;&#35328;&#20449;&#24687;&#65292;&#25552;&#39640;&#26222;&#36890;&#35805;&#38901;&#24459;&#32467;&#26500;&#39044;&#27979;&#30340;&#24615;&#33021;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#65288;TTS&#65289;&#32780;&#35328;&#65292;&#38901;&#24459;&#32467;&#26500;&#39044;&#27979;&#65288;PSP&#65289;&#22312;&#29983;&#25104;&#33258;&#28982;&#21644;&#21487;&#29702;&#35299;&#30340;&#35821;&#38899;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#36328;&#21477;&#23376;&#30340;&#35821;&#35328;&#20449;&#24687;&#21487;&#20197;&#24433;&#21709;&#30446;&#26631;&#21477;&#23376;&#30340;&#35821;&#38899;&#35299;&#37322;&#65292;&#20294;&#20043;&#21069;&#20851;&#20110;PSP&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20165;&#21033;&#29992;&#24403;&#21069;&#21477;&#23376;&#30340;&#20869;&#37096;&#35821;&#35328;&#20449;&#24687;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#36328;&#21477;&#23376;&#30340;&#35821;&#35328;&#20449;&#24687;&#26469;&#25552;&#39640;PSP&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23618;&#27425;&#32534;&#30721;&#22120;&#20174;&#36755;&#20837;&#25991;&#26412;&#30340;&#23383;&#31526;&#32423;&#12289;&#21477;&#23376;&#32423;&#21644;&#35805;&#35821;&#32423;&#25552;&#21462;&#22810;&#32423;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#21253;&#25324;&#36328;&#21477;&#23376;&#21644;&#20869;&#37096;&#21477;&#23376;&#30340;&#35821;&#35328;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#35299;&#30721;&#22120;&#20174;&#22810;&#32423;&#19978;&#19979;&#25991;&#20449;&#24687;&#20013;&#39044;&#27979;&#38901;&#24459;&#36793;&#30028;&#12290;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23458;&#35266;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#38901;&#24459;&#35789;&#65288;PW&#65289;&#12289;&#38901;&#24459;&#30701;&#35821;&#65288;PPH&#65289;&#21644;&#35821;&#35843;&#30701;&#35821;&#65288;IPH&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
For text-to-speech (TTS) synthesis, prosodic structure prediction (PSP) plays an important role in producing natural and intelligible speech. Although inter-utterance linguistic information can influence the speech interpretation of the target utterance, previous works on PSP mainly focus on utilizing intrautterance linguistic information of the current utterance only. This work proposes to use inter-utterance linguistic information to improve the performance of PSP. Multi-level contextual information, which includes both inter-utterance and intrautterance linguistic information, is extracted by a hierarchical encoder from character level, utterance level and discourse level of the input text. Then a multi-task learning (MTL) decoder predicts prosodic boundaries from multi-level contextual information. Objective evaluation results on two datasets show that our method achieves better F1 scores in predicting prosodic word (PW), prosodic phrase (PPH) and intonational phrase (IPH). It demo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#30740;&#31350;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#20559;&#35265;&#23545;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#24433;&#21709;&#30340;&#21338;&#22763;&#35770;&#25991;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20559;&#35265;&#23545;&#26816;&#27979;&#20219;&#21153;&#30340;&#24433;&#21709;&#21253;&#25324;&#21487;&#35299;&#37322;&#24615;&#12289;&#20882;&#29359;&#24615;&#21051;&#26495;&#21360;&#35937;&#21644;&#20844;&#24179;&#24615;&#19977;&#20010;&#26041;&#38754;&#12290;&#20026;&#20102;&#26377;&#25928;&#35299;&#20915;&#24403;&#21069;&#22312;&#27979;&#37327;&#21644;&#20943;&#36731;&#20559;&#35265;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#38656;&#35201;&#23558;&#31038;&#20250;&#31185;&#23398;&#32435;&#20837;&#21040;&#30740;&#31350;&#20013;&#12290;</title><link>http://arxiv.org/abs/2308.16549</link><description>&lt;p&gt;
&#35770;&#25991;&#25688;&#35201;&#8212;&#8212;&#35770;&#36848;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#23545;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Thesis Distillation: Investigating The Impact of Bias in NLP Models on Hate Speech Detection. (arXiv:2308.16549v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#30740;&#31350;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#20559;&#35265;&#23545;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#24433;&#21709;&#30340;&#21338;&#22763;&#35770;&#25991;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20559;&#35265;&#23545;&#26816;&#27979;&#20219;&#21153;&#30340;&#24433;&#21709;&#21253;&#25324;&#21487;&#35299;&#37322;&#24615;&#12289;&#20882;&#29359;&#24615;&#21051;&#26495;&#21360;&#35937;&#21644;&#20844;&#24179;&#24615;&#19977;&#20010;&#26041;&#38754;&#12290;&#20026;&#20102;&#26377;&#25928;&#35299;&#20915;&#24403;&#21069;&#22312;&#27979;&#37327;&#21644;&#20943;&#36731;&#20559;&#35265;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#38656;&#35201;&#23558;&#31038;&#20250;&#31185;&#23398;&#32435;&#20837;&#21040;&#30740;&#31350;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#25105;&#30340;&#21338;&#22763;&#35770;&#25991;&#24037;&#20316;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20174;&#21487;&#35299;&#37322;&#24615;&#12289;&#20882;&#29359;&#24615;&#21051;&#26495;&#21360;&#35937;&#21644;&#20844;&#24179;&#24615;&#19977;&#20010;&#26041;&#38754;&#25506;&#35752;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#20559;&#35265;&#23545;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#25105;&#35752;&#35770;&#20102;&#35770;&#25991;&#30340;&#20027;&#35201;&#35201;&#28857;&#20197;&#21450;&#23427;&#20204;&#23545;&#26356;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#30340;&#30410;&#22788;&#12290;&#26368;&#21518;&#65292;&#25105;&#35752;&#35770;&#20102;&#37325;&#35201;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#30340;&#35770;&#25991;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#20174;&#36825;&#19977;&#20010;&#26041;&#38754;&#24433;&#21709;&#20102;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#12290;&#38500;&#38750;&#25105;&#20204;&#24320;&#22987;&#23558;&#31038;&#20250;&#31185;&#23398;&#32435;&#20837;&#21040;&#30740;&#31350;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#20013;&#65292;&#21542;&#21017;&#25105;&#20204;&#23558;&#26080;&#27861;&#26377;&#25928;&#22320;&#20811;&#26381;&#30446;&#21069;&#22312;&#27979;&#37327;&#21644;&#20943;&#36731;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20559;&#35265;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is a summary of the work in my PhD thesis. In which, I investigate the impact of bias in NLP models on the task of hate speech detection from three perspectives: explainability, offensive stereotyping bias, and fairness. I discuss the main takeaways from my thesis and how they can benefit the broader NLP community. Finally, I discuss important future research directions. The findings of my thesis suggest that bias in NLP models impacts the task of hate speech detection from all three perspectives. And that unless we start incorporating social sciences in studying bias in NLP models, we will not effectively overcome the current limitations of measuring and mitigating bias in NLP models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26102;&#21464;&#20934;&#23553;&#38381;&#30456;&#20301;&#20998;&#26512;&#36827;&#34892;&#35821;&#38899;&#20449;&#21495;&#20849;&#25391;&#23792;&#30340;&#20934;&#30830;&#20272;&#35745;&#21644;&#36861;&#36394;&#65292;&#36890;&#36807;&#23558;&#20272;&#35745;&#21644;&#36861;&#36394;&#20004;&#20010;&#38454;&#27573;&#21512;&#24182;&#20026;&#19968;&#20010;&#21333;&#19968;&#38454;&#27573;&#65292;&#25552;&#39640;&#20102;&#20849;&#25391;&#23792;&#20272;&#35745;&#21644;&#36861;&#36394;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16540</link><description>&lt;p&gt;
&#38024;&#23545;&#35821;&#38899;&#20449;&#21495;&#20934;&#30830;&#36861;&#36394;&#20849;&#25391;&#23792;&#30340;&#26102;&#21464;&#20934;&#23553;&#38381;&#30456;&#20301;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Time-Varying Quasi-Closed-Phase Analysis for Accurate Formant Tracking in Speech Signals. (arXiv:2308.16540v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26102;&#21464;&#20934;&#23553;&#38381;&#30456;&#20301;&#20998;&#26512;&#36827;&#34892;&#35821;&#38899;&#20449;&#21495;&#20849;&#25391;&#23792;&#30340;&#20934;&#30830;&#20272;&#35745;&#21644;&#36861;&#36394;&#65292;&#36890;&#36807;&#23558;&#20272;&#35745;&#21644;&#36861;&#36394;&#20004;&#20010;&#38454;&#27573;&#21512;&#24182;&#20026;&#19968;&#20010;&#21333;&#19968;&#38454;&#27573;&#65292;&#25552;&#39640;&#20102;&#20849;&#25391;&#23792;&#20272;&#35745;&#21644;&#36861;&#36394;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26102;&#21464;&#20934;&#23553;&#38381;&#30456;&#20301;&#65288;TVQCP&#65289;&#20998;&#26512;&#36827;&#34892;&#35821;&#38899;&#20449;&#21495;&#20849;&#25391;&#23792;&#30340;&#20934;&#30830;&#20272;&#35745;&#21644;&#36861;&#36394;&#30340;&#26032;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#20849;&#25391;&#23792;&#36861;&#36394;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#20004;&#38454;&#27573;&#30340;&#20272;&#35745;&#21644;&#36861;&#36394;&#31574;&#30053;&#65292;&#39318;&#20808;&#21033;&#29992;&#30701;&#26102;&#20998;&#26512;&#65288;&#20363;&#22914;10-50&#27627;&#31186;&#65289;&#20272;&#35745;&#24471;&#21040;&#21021;&#27493;&#30340;&#20849;&#25391;&#23792;&#20505;&#36873;&#38598;&#65292;&#28982;&#21518;&#26681;&#25454;&#21160;&#24577;&#35268;&#21010;&#25110;&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#36861;&#36394;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#20027;&#35201;&#32570;&#28857;&#20043;&#19968;&#26159;&#65292;&#26080;&#35770;&#36861;&#36394;&#38454;&#27573;&#22914;&#20309;&#20248;&#31168;&#65292;&#37117;&#26080;&#27861;&#25552;&#39640;&#31532;&#19968;&#38454;&#27573;&#30340;&#20849;&#25391;&#23792;&#20272;&#35745;&#20934;&#30830;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;TVQCP&#26041;&#27861;&#23558;&#20272;&#35745;&#21644;&#36861;&#36394;&#20004;&#20010;&#38454;&#27573;&#21512;&#24182;&#20026;&#19968;&#20010;&#21333;&#19968;&#38454;&#27573;&#30340;&#20849;&#25391;&#23792;&#36861;&#36394;&#26041;&#27861;&#12290;TVQCP&#20998;&#26512;&#32467;&#21512;&#20102;&#19977;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#20849;&#25391;&#23792;&#20272;&#35745;&#21644;&#36861;&#36394;&#30340;&#20934;&#30830;&#24615;&#65306;&#65288;1&#65289;&#21033;&#29992;&#26102;&#22495;&#21152;&#26435;&#30340;&#20934;&#23553;&#38381;&#30456;&#20301;&#20998;&#26512;&#65292;&#20197;&#20943;&#23569;&#28608;&#21169;&#28304;&#30340;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a new method for the accurate estimation and tracking of formants in speech signals using time-varying quasi-closed-phase (TVQCP) analysis. Conventional formant tracking methods typically adopt a two-stage estimate-and-track strategy wherein an initial set of formant candidates are estimated using short-time analysis (e.g., 10--50 ms), followed by a tracking stage based on dynamic programming or a linear state-space model. One of the main disadvantages of these approaches is that the tracking stage, however good it may be, cannot improve upon the formant estimation accuracy of the first stage. The proposed TVQCP method provides a single-stage formant tracking that combines the estimation and tracking stages into one. TVQCP analysis combines three approaches to improve formant estimation and tracking: (1) it uses temporally weighted quasi-closed-phase analysis to derive closed-phase estimates of the vocal tract with reduced interference from the excitation sour
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21307;&#29983;&#21451;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#26234;&#33021;&#25968;&#25454;&#25552;&#21462;&#22120;&#65292;&#36890;&#36807;&#21322;&#33258;&#21160;&#21270;&#30340;&#26041;&#24335;&#26469;&#21152;&#36895;&#21644;&#25913;&#21892;&#20020;&#24202;&#35797;&#39564;&#26399;&#38388;&#30340;&#25968;&#25454;&#25910;&#38598;&#12290;&#19982;&#20256;&#32479;&#30340;&#25163;&#21160;&#25968;&#25454;&#25910;&#38598;&#30456;&#27604;&#65292;&#26234;&#33021;&#25968;&#25454;&#25552;&#21462;&#22120;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#22635;&#20889;&#26102;&#38388;&#65292;&#24182;&#25552;&#20379;&#26356;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#65292;&#36991;&#20813;&#20102;&#20154;&#20026;&#38169;&#35823;&#21644;&#25968;&#25454;&#37325;&#22797;&#36755;&#20837;&#12290;</title><link>http://arxiv.org/abs/2308.16537</link><description>&lt;p&gt;
The Smart Data Extractor&#65292;&#19968;&#31181;&#21307;&#29983;&#21451;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#20020;&#24202;&#35797;&#39564;&#26399;&#38388;&#21152;&#36895;&#21644;&#25913;&#21892;&#25968;&#25454;&#25910;&#38598;
&lt;/p&gt;
&lt;p&gt;
The Smart Data Extractor, a Clinician Friendly Solution to Accelerate and Improve the Data Collection During Clinical Trials. (arXiv:2308.16537v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21307;&#29983;&#21451;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#26234;&#33021;&#25968;&#25454;&#25552;&#21462;&#22120;&#65292;&#36890;&#36807;&#21322;&#33258;&#21160;&#21270;&#30340;&#26041;&#24335;&#26469;&#21152;&#36895;&#21644;&#25913;&#21892;&#20020;&#24202;&#35797;&#39564;&#26399;&#38388;&#30340;&#25968;&#25454;&#25910;&#38598;&#12290;&#19982;&#20256;&#32479;&#30340;&#25163;&#21160;&#25968;&#25454;&#25910;&#38598;&#30456;&#27604;&#65292;&#26234;&#33021;&#25968;&#25454;&#25552;&#21462;&#22120;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#22635;&#20889;&#26102;&#38388;&#65292;&#24182;&#25552;&#20379;&#26356;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#65292;&#36991;&#20813;&#20102;&#20154;&#20026;&#38169;&#35823;&#21644;&#25968;&#25454;&#37325;&#22797;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#30740;&#31350;&#20013;&#65292;&#20256;&#32479;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#24335;&#65292;&#21363;&#26597;&#30475;&#30149;&#20154;&#26723;&#26696;&#65292;&#24050;&#32463;&#34987;&#35777;&#26126;&#20250;&#24341;&#36215;&#20559;&#20506;&#12289;&#38169;&#35823;&#12289;&#20154;&#21147;&#21644;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#33021;&#22815;&#25552;&#21462;&#21508;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#21253;&#25324;&#31508;&#35760;&#12290;&#26234;&#33021;&#25968;&#25454;&#25552;&#21462;&#22120;&#36890;&#36807;&#36981;&#24490;&#35268;&#21017;&#26469;&#39044;&#22635;&#20020;&#24202;&#30740;&#31350;&#34920;&#26684;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20132;&#21449;&#27979;&#35797;&#23454;&#39564;&#65292;&#23558;&#21322;&#33258;&#21160;&#21270;&#25968;&#25454;&#25910;&#38598;&#19982;&#25163;&#21160;&#25968;&#25454;&#25910;&#38598;&#36827;&#34892;&#27604;&#36739;&#12290;&#23545;&#20110;79&#21517;&#24739;&#32773;&#65292;&#38656;&#35201;&#25910;&#38598;20&#20010;&#30446;&#26631;&#39033;&#30446;&#12290;&#25163;&#21160;&#25968;&#25454;&#25910;&#38598;&#23436;&#25104;&#19968;&#20010;&#34920;&#26684;&#30340;&#24179;&#22343;&#26102;&#38388;&#20026;6&#20998;81&#31186;&#65292;&#32780;&#26234;&#33021;&#25968;&#25454;&#25552;&#21462;&#22120;&#20026;3&#20998;22&#31186;&#12290;&#25163;&#21160;&#25968;&#25454;&#25910;&#38598;&#20013;&#30340;&#38169;&#35823;&#25968;&#37327;&#65288;&#25972;&#20010;&#38431;&#21015;&#20849;163&#20010;&#65289;&#20063;&#27604;&#26234;&#33021;&#25968;&#25454;&#25552;&#21462;&#22120;&#65288;&#25972;&#20010;&#38431;&#21015;&#20849;46&#20010;&#65289;&#22810;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#12289;&#29702;&#35299;&#21644;&#28789;&#27963;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#22635;&#20889;&#20020;&#24202;&#30740;&#31350;&#34920;&#26684;&#12290;&#23427;&#20943;&#23569;&#20102;&#20154;&#21147;&#25237;&#20837;&#65292;&#25552;&#20379;&#26356;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#65292;&#36991;&#20813;&#20102;&#25968;&#25454;&#37325;&#22797;&#36755;&#20837;&#21644;&#30130;&#21171;&#24341;&#36215;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
In medical research, the traditional way to collect data, i.e. browsing patient files, has been proven to induce bias, errors, human labor and costs. We propose a semi-automated system able to extract every type of data, including notes. The Smart Data Extractor pre-populates clinic research forms by following rules. We performed a cross-testing experiment to compare semi-automated to manual data collection. 20 target items had to be collected for 79 patients. The average time to complete one form was 6'81'' for manual data collection and 3'22'' with the Smart Data Extractor. There were also more mistakes during manual data collection (163 for the whole cohort) than with the Smart Data Extractor (46 for the whole cohort). We present an easy to use, understandable and agile solution to fill out clinical research forms. It reduces human effort and provides higher quality data, avoiding data re-entry and fatigue induced errors.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;Winograd Schema&#22312;&#19978;&#19979;&#25991;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#37327;&#23376;&#29289;&#29702;&#23454;&#39564;&#27169;&#22411;&#26469;&#35299;&#20915;Winograd&#27169;&#24335;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.16498</link><description>&lt;p&gt;
&#24191;&#20041;Winograd Schema&#21450;&#20854;&#19978;&#19979;&#25991;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generalised Winograd Schema and its Contextuality. (arXiv:2308.16498v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;Winograd Schema&#22312;&#19978;&#19979;&#25991;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#37327;&#23376;&#29289;&#29702;&#23454;&#39564;&#27169;&#22411;&#26469;&#35299;&#20915;Winograd&#27169;&#24335;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#27495;&#20041;&#20250;&#24341;&#36215;&#23545;&#35299;&#37322;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#36825;&#20123;&#20998;&#24067;&#36890;&#24120;&#28041;&#21450;&#21040;&#22810;&#20010;&#27169;&#26865;&#20004;&#21487;&#30340;&#35789;&#27719;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#25104;&#20026;&#36866;&#21512;&#37327;&#23376;&#19978;&#19979;&#25991;&#24615;&#25311;&#35774;&#27169;&#22411;&#30340;&#30740;&#31350;&#20027;&#39064;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19978;&#19979;&#25991;&#24615;&#30340;&#19981;&#21516;&#23450;&#37327;&#24230;&#37327;&#19982;&#24515;&#29702;&#35821;&#35328;&#23398;&#30740;&#31350;&#20013;&#30340;&#35789;&#20041;&#27495;&#20041;&#26377;&#24456;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#25351;&#20195;&#30340;&#27495;&#20041;&#65292;&#24182;&#30740;&#31350;&#20102;Winograd&#27169;&#24335;&#25361;&#25112;&#65288;WSC&#65289;&#65292;&#36825;&#26159;Levesque&#22312;2011&#24180;&#25552;&#20986;&#30340;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#26234;&#33021;&#30340;&#27979;&#35797;&#12290;WSC&#21253;&#21547;&#19968;&#31995;&#21015;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#38656;&#35201;&#22312;&#25353;&#29031;Winograd&#27169;&#24335;&#26500;&#36896;&#30340;&#21477;&#23376;&#20013;&#28040;&#38500;&#20195;&#35789;&#30340;&#27495;&#20041;&#65292;&#36825;&#23545;&#26426;&#22120;&#26469;&#35828;&#24456;&#38590;&#30830;&#23450;&#27491;&#30830;&#30340;&#20195;&#35789;&#25351;&#21521;&#65292;&#20294;&#23545;&#20154;&#31867;&#29702;&#35299;&#26469;&#35828;&#21364;&#24456;&#30452;&#35266;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#20284;&#22320;&#23558;Winograd&#27169;&#24335;&#24314;&#27169;&#20026;&#37327;&#23376;&#29289;&#29702;&#23454;&#39564;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ambiguities in natural language give rise to probability distributions over interpretations. The distributions are often over multiple ambiguous words at a time; a multiplicity which makes them a suitable topic for sheaf-theoretic models of quantum contextuality. Previous research showed that different quantitative measures of contextuality correlate well with Psycholinguistic research on lexical ambiguities. In this work, we focus on coreference ambiguities and investigate the Winograd Schema Challenge (WSC), a test proposed by Levesque in 2011 to evaluate the intelligence of machines. The WSC consists of a collection of multiple-choice questions that require disambiguating pronouns in sentences structured according to the Winograd schema, in a way that makes it difficult for machines to determine the correct referents but remains intuitive for human comprehension. In this study, we propose an approach that analogously models the Winograd schema as an experiment in quantum physics. Ho
&lt;/p&gt;</description></item><item><title>Transformer&#21387;&#32553;&#36890;&#36807;&#23376;&#31354;&#38388;&#25237;&#24433;&#65292;&#22312;&#20943;&#23567;&#27169;&#22411;&#38544;&#34255;&#22823;&#23567;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#36739;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#20943;&#23569;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#26041;&#27861;&#20860;&#23481;&#12290;</title><link>http://arxiv.org/abs/2308.16475</link><description>&lt;p&gt;
Transformer&#21387;&#32553;&#36890;&#36807;&#23376;&#31354;&#38388;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
Transformer Compression via Subspace Projection. (arXiv:2308.16475v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16475
&lt;/p&gt;
&lt;p&gt;
Transformer&#21387;&#32553;&#36890;&#36807;&#23376;&#31354;&#38388;&#25237;&#24433;&#65292;&#22312;&#20943;&#23567;&#27169;&#22411;&#38544;&#34255;&#22823;&#23567;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#36739;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#20943;&#23569;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#26041;&#27861;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TCSP&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#20943;&#23569;&#27169;&#22411;&#30340;&#38544;&#34255;&#22823;&#23567;&#26469;&#21387;&#32553;Transformer&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#25972;&#20010;&#36716;&#25442;&#27169;&#22411;&#25237;&#24433;&#21040;&#19968;&#20010;&#23376;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#20351;&#27169;&#22411;&#20013;&#30340;&#26435;&#37325;&#30697;&#38453;&#19982;&#20943;&#23567;&#32500;&#24230;&#31354;&#38388;&#20013;&#30340;&#29305;&#24449;&#20043;&#38388;&#21487;&#20197;&#36827;&#34892;&#30697;&#38453;&#25805;&#20316;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#24314;&#31435;&#36825;&#20010;&#23376;&#31354;&#38388;&#65292;&#25105;&#20204;&#23558;&#26469;&#33258;&#19981;&#21516;&#23618;&#27425;&#30340;&#37319;&#26679;&#25968;&#25454;&#23454;&#20363;&#30340;&#29305;&#24449;&#30697;&#38453;&#20998;&#35299;&#20026;&#19968;&#20010;&#25237;&#24433;&#30697;&#38453;&#12290;&#20026;&#20102;&#35780;&#20272;&#25928;&#26524;&#65292;&#25105;&#20204;&#22312;GLUE&#21644;SQuAD&#22522;&#20934;&#27979;&#35797;&#19978;&#24212;&#29992;TCSP&#26469;&#21387;&#32553;T5&#21644;BERT&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TCSP&#22312;&#20445;&#35777;&#26368;&#22810;1.6%&#30340;&#20934;&#30830;&#24230;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;44%&#30340;&#21387;&#32553;&#27604;&#65292;&#36229;&#36807;&#25110;&#32773;&#36798;&#21040;&#20102;&#20808;&#21069;&#30340;&#21387;&#32553;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;TCSP&#36824;&#19982;&#20854;&#20182;&#30446;&#26631;&#36807;&#28388;&#22120;&#21644;&#27880;&#24847;&#21147;&#22836;&#22823;&#23567;&#21387;&#32553;&#30340;&#26041;&#27861;&#30456;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose TCSP, a novel method for compressing a transformer model by focusing on reducing the hidden size of the model. By projecting the whole transform model into a subspace, we enable matrix operations between the weight matrices in the model and features in a reduced-dimensional space, leading to significant reductions in model parameters and computing resources. To establish this subspace, we decompose the feature matrix, derived from different layers of sampled data instances, into a projection matrix. For evaluation, TCSP is applied to compress T5 and BERT models on the GLUE and SQuAD benchmarks. Experimental results demonstrate that TCSP achieves a compression ratio of 44\% with at most 1.6\% degradation in accuracy, surpassing or matching prior compression methods. Furthermore, TCSP exhibits compatibility with other methods targeting filter and attention head size compression.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#22810;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#23436;&#25104;&#30456;&#21516;&#30340;&#23376;&#20219;&#21153;&#65292;&#36890;&#36807;&#32452;&#21512;&#22810;&#20010;&#27169;&#22411;&#30340;&#32467;&#26524;&#33719;&#24471;&#26368;&#20339;&#30340;&#23376;&#20219;&#21153;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.16474</link><description>&lt;p&gt;
&#25552;&#21319;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23376;&#20219;&#21153;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Enhancing Subtask Performance of Multi-modal Large Language Model. (arXiv:2308.16474v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16474
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#22810;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#23436;&#25104;&#30456;&#21516;&#30340;&#23376;&#20219;&#21153;&#65292;&#36890;&#36807;&#32452;&#21512;&#22810;&#20010;&#27169;&#22411;&#30340;&#32467;&#26524;&#33719;&#24471;&#26368;&#20339;&#30340;&#23376;&#20219;&#21153;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#26159;&#25351;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25193;&#23637;&#32780;&#26469;&#30340;&#27169;&#22411;&#65292;&#20855;&#22791;&#22788;&#29702;&#21644;&#25512;&#29702;&#22810;&#27169;&#24335;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#24403;&#21069;&#30340;MLLM&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;LLM&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#23376;&#20219;&#21153;&#65292;&#28982;&#21518;&#20351;&#29992;&#21508;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#23436;&#25104;&#29305;&#23450;&#30340;&#23376;&#20219;&#21153;&#65292;&#24182;&#26368;&#32456;&#21033;&#29992;LLM&#25972;&#21512;&#27599;&#20010;&#23376;&#20219;&#21153;&#30340;&#32467;&#26524;&#26469;&#33719;&#24471;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#22788;&#29702;&#22823;&#22411;&#39033;&#30446;&#26102;&#65292;&#24120;&#24120;&#23558;&#39033;&#30446;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#23376;&#39033;&#30446;&#65292;&#24182;&#30001;&#19981;&#21516;&#30340;&#22242;&#38431;&#25552;&#20379;&#30456;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#25110;&#32467;&#26524;&#12290;&#39033;&#30446;&#25152;&#26377;&#32773;&#38543;&#21518;&#20915;&#23450;&#20351;&#29992;&#21738;&#20010;&#35299;&#20915;&#26041;&#26696;&#25110;&#32467;&#26524;&#65292;&#20197;&#30830;&#20445;&#27599;&#20010;&#23376;&#20219;&#21153;&#21644;&#25972;&#20010;&#39033;&#30446;&#33021;&#22815;&#36798;&#21040;&#26368;&#20339;&#32467;&#26524;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#32771;&#34385;&#36873;&#25321;&#22810;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#23436;&#25104;&#30456;&#21516;&#30340;&#23376;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#22810;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#32467;&#26524;&#36827;&#34892;&#32452;&#21512;&#65292;&#33719;&#24471;&#26368;&#20339;&#30340;&#23376;&#20219;&#21153;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Large Language Model (MLLM) refers to a model expanded from a Large Language Model (LLM) that possesses the capability to handle and infer multi-modal data. Current MLLMs typically begin by using LLMs to decompose tasks into multiple subtasks, then employing individual pre-trained models to complete specific subtasks, and ultimately utilizing LLMs to integrate the results of each subtasks to obtain the results of the task. In real-world scenarios, when dealing with large projects, it is common practice to break down the project into smaller sub-projects, with different teams providing corresponding solutions or results. The project owner then decides which solution or result to use, ensuring the best possible outcome for each subtask and, consequently, for the entire project. Inspired by this, this study considers selecting multiple pre-trained models to complete the same subtask. By combining the results from multiple pre-trained models, the optimal subtask result is obtai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#30340;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;DSAA-2023&#31454;&#36187;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#35780;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.16469</link><description>&lt;p&gt;
&#23558;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#30340;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Link Prediction for Wikipedia Articles as a Natural Language Inference Task. (arXiv:2308.16469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#30340;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;DSAA-2023&#31454;&#36187;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#23545;&#20110;&#33258;&#21160;&#29702;&#35299;&#22823;&#22411;&#30693;&#35782;&#24211;&#30340;&#32467;&#26500;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;&#25968;&#25454;&#31185;&#23398;&#21644;&#39640;&#32423;&#20998;&#26512;2023&#24180;&#31454;&#36187;&#8220;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#38142;&#25509;&#39044;&#27979;&#8221;&#65288;DSAA-2023&#31454;&#36187;&#65289;&#20013;&#29992;&#21253;&#21547;948,233&#20010;&#35757;&#32451;&#26679;&#26412;&#21644;238,265&#20010;&#29992;&#20110;&#20844;&#20849;&#27979;&#35797;&#30340;&#35821;&#26009;&#24211;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#30340;&#31995;&#32479;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#23558;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#30340;&#38142;&#25509;&#39044;&#27979;&#38382;&#39064;&#24314;&#27169;&#20026;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#65288;NLI&#65289;&#30340;&#26041;&#27861;&#12290;&#21463;&#21040;&#36817;&#26399;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#29702;&#35299;&#26041;&#38754;&#30340;&#36827;&#23637;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#38142;&#25509;&#39044;&#27979;&#20316;&#20026;&#19968;&#20010;NLI&#20219;&#21153;&#65292;&#20854;&#20013;&#23558;&#20004;&#20010;&#25991;&#31456;&#20043;&#38388;&#30340;&#38142;&#25509;&#23384;&#22312;&#35270;&#20026;&#21069;&#25552;&#65292;&#20219;&#21153;&#26159;&#22522;&#20110;&#25991;&#31456;&#20013;&#21576;&#29616;&#30340;&#20449;&#24687;&#26469;&#30830;&#23450;&#35813;&#21069;&#25552;&#26159;&#21542;&#25104;&#31435;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#26159;&#22522;&#20110;&#29992;&#20110;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#38142;&#25509;&#39044;&#27979;&#30340;&#21477;&#23545;&#20998;&#31867;&#30340;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#23454;&#29616;&#20102;0.99996&#30340;Macro F1-score&#21644;1.00000&#30340;Macro F1-score&#12290;
&lt;/p&gt;
&lt;p&gt;
Link prediction task is vital to automatically understanding the structure of large knowledge bases. In this paper, we present our system to solve this task at the Data Science and Advanced Analytics 2023 Competition "Efficient and Effective Link Prediction" (DSAA-2023 Competition) with a corpus containing 948,233 training and 238,265 for public testing. This paper introduces an approach to link prediction in Wikipedia articles by formulating it as a natural language inference (NLI) task. Drawing inspiration from recent advancements in natural language processing and understanding, we cast link prediction as an NLI task, wherein the presence of a link between two articles is treated as a premise, and the task is to determine whether this premise holds based on the information presented in the articles. We implemented our system based on the Sentence Pair Classification for Link Prediction for the Wikipedia Articles task. Our system achieved 0.99996 Macro F1-score and 1.00000 Macro F1-s
&lt;/p&gt;</description></item><item><title>Sparkles&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#36394;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#23454;&#29616;&#22810;&#22270;&#23545;&#35805;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SparklesDialogue&#25968;&#25454;&#38598;&#21644;SparklesEval&#22522;&#20934;&#26469;&#25903;&#25345;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;SparklesChat&#22312;&#29702;&#35299;&#22810;&#22270;&#23545;&#35805;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16463</link><description>&lt;p&gt;
Sparkles: &#35299;&#38145;&#22810;&#22270;&#32842;&#22825;&#20197;&#23454;&#29616;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#36394;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models. (arXiv:2308.16463v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16463
&lt;/p&gt;
&lt;p&gt;
Sparkles&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#36394;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#23454;&#29616;&#22810;&#22270;&#23545;&#35805;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SparklesDialogue&#25968;&#25454;&#38598;&#21644;SparklesEval&#22522;&#20934;&#26469;&#25903;&#25345;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;SparklesChat&#22312;&#29702;&#35299;&#22810;&#22270;&#23545;&#35805;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20351;&#29992;&#25351;&#20196;&#36319;&#36394;&#25968;&#25454;&#26469;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#24615;&#33021;&#12290;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#36394;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#36825;&#20123;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27169;&#22411;&#65288;&#22914;MiniGPT-4&#65289;&#22312;&#28041;&#21450;&#22810;&#20010;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#23545;&#35805;&#36830;&#36143;&#24615;&#38754;&#20020;&#25361;&#25112;&#12290;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#32570;&#20047;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#36825;&#19968;&#20851;&#38190;&#24212;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SparklesChat&#65292;&#19968;&#20010;&#29992;&#20110;&#22810;&#22270;&#23545;&#35805;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#36394;&#27169;&#22411;&#12290;&#20026;&#20102;&#25903;&#25345;&#35757;&#32451;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SparklesDialogue&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#21333;&#35789;&#32423;&#20132;&#38169;&#22810;&#22270;&#20687;&#21644;&#25991;&#26412;&#20132;&#20114;&#32780;&#23450;&#21046;&#30340;&#26426;&#22120;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;SparklesEval&#65292;&#19968;&#20010;&#20511;&#21161;GPT&#36741;&#21161;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#23450;&#37327;&#35780;&#20272;&#27169;&#22411;&#22312;&#22810;&#20010;&#22270;&#20687;&#21644;&#23545;&#35805;&#36718;&#27425;&#20013;&#30340;&#23545;&#35805;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;SparklesChat&#22312;&#29702;&#35299;&#22810;&#22270;&#23545;&#35805;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models exhibit enhanced zero-shot performance on various tasks when fine-tuned with instruction-following data. Multimodal instruction-following models extend these capabilities by integrating both text and images. However, existing models such as MiniGPT-4 face challenges in maintaining dialogue coherence in scenarios involving multiple images. A primary reason is the lack of a specialized dataset for this critical application. To bridge these gaps, we present SparklesChat, a multimodal instruction-following model for open-ended dialogues across multiple images. To support the training, we introduce SparklesDialogue, the first machine-generated dialogue dataset tailored for word-level interleaved multi-image and text interactions. Furthermore, we construct SparklesEval, a GPT-assisted benchmark for quantitatively assessing a model's conversational competence across multiple images and dialogue turns. Our experiments validate the effectiveness of SparklesChat in understa
&lt;/p&gt;</description></item><item><title>BioCoder&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.16458</link><description>&lt;p&gt;
BioCoder: &#19968;&#31181;&#24102;&#26377;&#19978;&#19979;&#25991;&#35821;&#29992;&#30693;&#35782;&#30340;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. (arXiv:2308.16458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16458
&lt;/p&gt;
&lt;p&gt;
BioCoder&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#26174;&#33879;&#25913;&#36827;&#20102;&#20195;&#30721;&#29983;&#25104;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#30340;&#25193;&#22823;&#65292;&#38656;&#35201;&#36755;&#20986;&#26469;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#30340;&#38656;&#27714;&#20063;&#36234;&#26469;&#36234;&#22810;&#12290;&#27492;&#22806;&#65292;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#65292;&#29983;&#25104;&#21151;&#33021;&#31243;&#24207;&#30001;&#20110;&#39046;&#22495;&#30693;&#35782;&#37327;&#22823;&#12289;&#38656;&#35201;&#22797;&#26434;&#30340;&#25968;&#25454;&#25805;&#20316;&#21644;&#22797;&#26434;&#30340;&#21151;&#33021;&#20381;&#36182;&#20851;&#31995;&#32780;&#38754;&#20020;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BioCoder&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#29616;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#12290;&#19982;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#26377;&#20851;&#65292;BioCoder&#28085;&#30422;&#20102;&#21487;&#33021;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#12290;&#23427;&#21253;&#25324;&#26469;&#33258;GitHub&#30340;1026&#20010;Python&#21644;Java&#20989;&#25968;&#21644;1243&#20010;&#26041;&#27861;&#65292;&#20197;&#21450;&#26469;&#33258;Rosalind&#39033;&#30446;&#30340;253&#20010;&#31034;&#20363;&#12290;BioCoder&#36824;&#32467;&#21512;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#65292;&#25105;&#20204;&#24050;&#32463;&#24212;&#29992;&#23427;&#26469;&#35780;&#20272;&#35768;&#22810;&#27169;&#22411;&#65292;&#21253;&#25324;InCoder&#12289;CodeGen&#12289;CodeGen2&#12289;SantaCoder&#12289;StarCoder&#12289;StarCoder+&#12289;InstructCodeT&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models like ChatGPT have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks. Moreover, in bioinformatics, generating functional programs poses additional notable challenges due to the amount of domain knowledge, the need for complicated data operations, and intricate functional dependencies between the operations. Here, we present BioCoder, a benchmark developed to evaluate existing pre-trained models in generating bioinformatics code. In relation to function-code generation, BioCoder covers potential package dependencies, class declarations, and global variables. It incorporates 1026 functions and 1243 methods in Python and Java from GitHub and 253 examples from the Rosalind Project. BioCoder incorporates a fuzz-testing framework for evaluation, and we have applied it to evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38750;&#27969;&#24335;&#21040;&#27969;&#24335;ASR&#32534;&#30721;&#22120;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#23618;&#33976;&#39311;&#21644;&#24341;&#20837;&#36741;&#21161;&#30340;&#38750;&#27969;&#24335;&#23618;&#65292;&#20197;&#21450;&#29305;&#23450;&#30340;&#33976;&#39311;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#27969;&#24335;ASR&#30340;&#35789;&#38169;&#35823;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.16415</link><description>&lt;p&gt;
&#20174;&#38750;&#27969;&#24335;&#21040;&#27969;&#24335;ASR&#32534;&#30721;&#22120;&#30340;&#30693;&#35782;&#33976;&#39311;&#65292;&#20351;&#29992;&#36741;&#21161;&#30340;&#38750;&#27969;&#24335;&#23618;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation from Non-streaming to Streaming ASR Encoder using Auxiliary Non-streaming Layer. (arXiv:2308.16415v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38750;&#27969;&#24335;&#21040;&#27969;&#24335;ASR&#32534;&#30721;&#22120;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#23618;&#33976;&#39311;&#21644;&#24341;&#20837;&#36741;&#21161;&#30340;&#38750;&#27969;&#24335;&#23618;&#65292;&#20197;&#21450;&#29305;&#23450;&#30340;&#33976;&#39311;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#27969;&#24335;ASR&#30340;&#35789;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#30001;&#20110;&#26080;&#27861;&#35775;&#38382;&#26410;&#26469;&#30340;&#19978;&#19979;&#25991;&#65292;&#23548;&#33268;&#24615;&#33021;&#27604;&#38750;&#27969;&#24335;&#27169;&#22411;&#24046;&#12290;&#20026;&#20102;&#25552;&#39640;&#27969;&#24335;ASR&#30340;&#24615;&#33021;&#65292;&#24050;&#32463;&#30740;&#31350;&#20102;&#20174;&#38750;&#27969;&#24335;&#21040;&#27969;&#24335;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#65292;&#20027;&#35201;&#20851;&#27880;&#36755;&#20986;&#26631;&#35760;&#27010;&#29575;&#30340;&#23545;&#40784;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25945;&#24072;&#32534;&#30721;&#22120;&#21040;&#23398;&#29983;&#32534;&#30721;&#22120;&#30340;&#36880;&#23618;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#12290;&#20026;&#20102;&#30830;&#20445;&#20351;&#29992;&#30456;&#21516;&#30340;&#19978;&#19979;&#25991;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#25105;&#20204;&#22312;&#23398;&#29983;&#27169;&#22411;&#20013;&#25554;&#20837;&#36741;&#21161;&#30340;&#38750;&#27969;&#24335;&#20998;&#25903;&#65292;&#24182;&#20174;&#38750;&#27969;&#24335;&#25945;&#24072;&#23618;&#21521;&#38750;&#27969;&#24335;&#36741;&#21161;&#23618;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;&#33976;&#39311;&#25439;&#22833;&#65292;&#21033;&#29992;&#33258;&#22238;&#24402;&#39044;&#27979;&#32534;&#30721;&#65288;APC&#65289;&#26426;&#21046;&#65292;&#40723;&#21169;&#27969;&#24335;&#27169;&#22411;&#39044;&#27979;&#30475;&#19981;&#35265;&#30340;&#26410;&#26469;&#19978;&#19979;&#25991;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20043;&#21069;&#30340;&#26631;&#35760;&#27010;&#29575;&#33976;&#39311;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#35789;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Streaming automatic speech recognition (ASR) models are restricted from accessing future context, which results in worse performance compared to the non-streaming models. To improve the performance of streaming ASR, knowledge distillation (KD) from the non-streaming to streaming model has been studied, mainly focusing on aligning the output token probabilities. In this paper, we propose a layer-to-layer KD from the teacher encoder to the student encoder. To ensure that features are extracted using the same context, we insert auxiliary non-streaming branches to the student and perform KD from the non-streaming teacher layer to the non-streaming auxiliary layer. We design a special KD loss that leverages the autoregressive predictive coding (APC) mechanism to encourage the streaming model to predict unseen future contexts. Experimental results show that the proposed method can significantly reduce the word error rate compared to previous token probability distillation methods.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;AffectVisDial&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;50,000&#20010;&#22522;&#20110;&#35270;&#35273;&#30340;&#23545;&#35805;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#24773;&#24863;&#35270;&#35273;&#23545;&#35805;&#27169;&#22411;&#26469;&#35299;&#20915;&#22522;&#20110;&#23545;&#35805;&#30340;&#38382;&#31572;&#12289;&#24773;&#24863;&#39044;&#27979;&#21644;&#24773;&#24863;&#35299;&#37322;&#20219;&#21153;&#65292;&#23637;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#24773;&#24863;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16349</link><description>&lt;p&gt;
&#24773;&#24863;&#35270;&#35273;&#23545;&#35805;&#65306;&#22522;&#20110;&#35270;&#35273;&#23545;&#35805;&#29702;&#35299;&#24773;&#24863;&#24418;&#25104;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Affective Visual Dialog: A Large-Scale Benchmark for Emotional Reasoning Based on Visually Grounded Conversations. (arXiv:2308.16349v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16349
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;AffectVisDial&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;50,000&#20010;&#22522;&#20110;&#35270;&#35273;&#30340;&#23545;&#35805;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#24773;&#24863;&#35270;&#35273;&#23545;&#35805;&#27169;&#22411;&#26469;&#35299;&#20915;&#22522;&#20110;&#23545;&#35805;&#30340;&#38382;&#31572;&#12289;&#24773;&#24863;&#39044;&#27979;&#21644;&#24773;&#24863;&#35299;&#37322;&#20219;&#21153;&#65292;&#23637;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#24773;&#24863;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#24773;&#24863;&#35270;&#35273;&#23545;&#35805;&#65292;&#20316;&#20026;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#65292;&#29992;&#20110;&#30740;&#31350;&#29702;&#35299;&#22312;&#22522;&#20110;&#35270;&#35273;&#23545;&#35805;&#20013;&#24773;&#24863;&#24418;&#25104;&#30340;&#36807;&#31243;&#12290;&#36825;&#39033;&#20219;&#21153;&#28041;&#21450;&#19977;&#39033;&#25216;&#33021;&#65306;&#65288;1&#65289;&#22522;&#20110;&#23545;&#35805;&#30340;&#38382;&#31572;&#65292;&#65288;2&#65289;&#22522;&#20110;&#23545;&#35805;&#30340;&#24773;&#24863;&#39044;&#27979;&#65292;&#20197;&#21450;&#65288;3&#65289;&#22522;&#20110;&#23545;&#35805;&#29983;&#25104;&#24773;&#24863;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;AffectVisDial&#65292;&#21253;&#21547;50,000&#20010;10&#36718;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#23545;&#35805;&#65292;&#36824;&#21253;&#25324;&#24635;&#32467;&#30340;&#24773;&#24863;&#24402;&#22240;&#21644;&#22522;&#20110;&#23545;&#35805;&#30340;&#24773;&#24863;&#35299;&#37322;&#65292;&#24635;&#20849;&#38656;&#35201;27180&#20010;&#24037;&#20316;&#23567;&#26102;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#25910;&#38598;&#35813;&#25968;&#25454;&#38598;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#24182;&#20171;&#32461;&#20102;&#19982;&#23545;&#35805;&#21442;&#19982;&#32773;&#30456;&#20851;&#30340;&#25552;&#38382;&#32773;&#21644;&#22238;&#31572;&#32773;&#20219;&#21153;&#12290;&#25105;&#20204;&#35757;&#32451;&#21644;&#23637;&#31034;&#20102;&#26469;&#33258;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#22362;&#23454;&#30340;&#24773;&#24863;&#35270;&#35273;&#23545;&#35805;&#22522;&#32447;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#27169;&#22411;&#29983;&#25104;&#30340;&#22238;&#31572;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#24773;&#24863;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Affective Visual Dialog, an emotion explanation and reasoning task as a testbed for research on understanding the formation of emotions in visually grounded conversations. The task involves three skills: (1) Dialog-based Question Answering (2) Dialog-based Emotion Prediction and (3) Affective emotion explanation generation based on the dialog. Our key contribution is the collection of a large-scale dataset, dubbed AffectVisDial, consisting of 50K 10-turn visually grounded dialogs as well as concluding emotion attributions and dialog-informed textual emotion explanations, resulting in a total of 27,180 working hours. We explain our design decisions in collecting the dataset and introduce the questioner and answerer tasks that are associated with the participants in the conversation. We train and demonstrate solid Affective Visual Dialog baselines adapted from state-of-the-art models. Remarkably, the responses generated by our models show promising emotional reasoning abilit
&lt;/p&gt;</description></item><item><title>ToddlerBERTa&#26159;&#19968;&#20010;&#31867;&#20284;&#20110;BabyBERTa&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23613;&#31649;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#23427;&#23637;&#31034;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;RoBERTa-base&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2308.16336</link><description>&lt;p&gt;
ToddlerBERTa: &#21033;&#29992;BabyBERTa&#36827;&#34892;&#35821;&#27861;&#23398;&#20064;&#21644;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ToddlerBERTa: Exploiting BabyBERTa for Grammar Learning and Language Understanding. (arXiv:2308.16336v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16336
&lt;/p&gt;
&lt;p&gt;
ToddlerBERTa&#26159;&#19968;&#20010;&#31867;&#20284;&#20110;BabyBERTa&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23613;&#31649;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#23427;&#23637;&#31034;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;RoBERTa-base&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ToddlerBERTa&#65292;&#36825;&#26159;&#19968;&#20010;&#31867;&#20284;&#20110;BabyBERTa&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20116;&#31181;&#19981;&#21516;&#30340;&#20855;&#26377;&#19981;&#21516;&#36229;&#21442;&#25968;&#30340;&#27169;&#22411;&#26469;&#25506;&#32034;&#20854;&#33021;&#21147;&#12290;&#22312;BLiMP&#65292;SuperGLUE&#65292;MSGS&#21644;BabyLM&#25361;&#25112;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36739;&#23567;&#30340;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#36739;&#22823;&#30340;&#27169;&#22411;&#22312;&#22823;&#37327;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#23613;&#31649;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;ToddlerBERTa&#23637;&#31034;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#24615;&#33021;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;RoBERTa-base&#30456;&#23218;&#32654;&#12290;&#35813;&#27169;&#22411;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#21363;&#20351;&#26159;&#22312;&#21333;&#21477;&#39044;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#19982;&#21033;&#29992;&#26356;&#24191;&#27867;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22522;&#32447;&#31454;&#20105;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#36229;&#21442;&#25968;&#36873;&#25321;&#21644;&#25968;&#25454;&#21033;&#29992;&#25552;&#20379;&#20102;&#27934;&#23519;&#65292;&#24182;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ToddlerBERTa, a BabyBERTa-like language model, exploring its capabilities through five different models with varied hyperparameters. Evaluating on BLiMP, SuperGLUE, MSGS, and a Supplement benchmark from the BabyLM challenge, we find that smaller models can excel in specific tasks, while larger models perform well with substantial data. Despite training on a smaller dataset, ToddlerBERTa demonstrates commendable performance, rivalling the state-of-the-art RoBERTa-base. The model showcases robust language understanding, even with single-sentence pretraining, and competes with baselines that leverage broader contextual information. Our work provides insights into hyperparameter choices, and data utilization, contributing to the advancement of language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65288;&#30693;&#35782;&#22270;&#35889;LLM&#65289;&#65292;&#20197;&#25552;&#39640;&#19977;&#20803;&#32452;&#20998;&#31867;&#21644;&#20851;&#31995;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.13916</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Exploring Large Language Models for Knowledge Graph Completion. (arXiv:2308.13916v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65288;&#30693;&#35782;&#22270;&#35889;LLM&#65289;&#65292;&#20197;&#25552;&#39640;&#19977;&#20803;&#32452;&#20998;&#31867;&#21644;&#20851;&#31995;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22312;&#20247;&#22810;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#32463;&#24120;&#38754;&#20020;&#19981;&#23436;&#25972;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#19977;&#20803;&#32452;&#35270;&#20026;&#25991;&#26412;&#24207;&#21015;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#30693;&#35782;&#22270;&#35889;LLM&#65288;KG-LLM&#65289;&#65292;&#26469;&#23545;&#36825;&#20123;&#19977;&#20803;&#32452;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#21033;&#29992;&#19977;&#20803;&#32452;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#25551;&#36848;&#20316;&#20026;&#25552;&#31034;&#65292;&#24182;&#21033;&#29992;&#21709;&#24212;&#36827;&#34892;&#39044;&#27979;&#12290;&#23545;&#21508;&#31181;&#22522;&#20934;&#30693;&#35782;&#22270;&#35889;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20803;&#32452;&#20998;&#31867;&#21644;&#20851;&#31995;&#39044;&#27979;&#31561;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#24494;&#35843;&#30456;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;LLaMA-7B&#65292;ChatGLM-6B&#65289;&#20248;&#20110;&#26368;&#26032;&#30340;ChatGPT&#21644;GPT-4&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs play a vital role in numerous artificial intelligence tasks, yet they frequently face the issue of incompleteness. In this study, we explore utilizing Large Language Models (LLM) for knowledge graph completion. We consider triples in knowledge graphs as text sequences and introduce an innovative framework called Knowledge Graph LLM (KG-LLM) to model these triples. Our technique employs entity and relation descriptions of a triple as prompts and utilizes the response for predictions. Experiments on various benchmark knowledge graphs demonstrate that our method attains state-of-the-art performance in tasks such as triple classification and relation prediction. We also find that fine-tuning relatively smaller models (e.g., LLaMA-7B, ChatGLM-6B) outperforms recent ChatGPT and GPT-4.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DocPrompt&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DocPrompt&#27169;&#22411;&#32463;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#21518;&#22312;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#20132;&#20184;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#38477;&#20302;&#20102;&#27880;&#37322;&#25104;&#26412;&#21644;&#21171;&#21160;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.10959</link><description>&lt;p&gt;
DocPrompt: &#22823;&#35268;&#27169;&#36830;&#32493;&#39044;&#35757;&#32451;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25991;&#26723;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
DocPrompt: Large-scale continue pretrain for zero-shot and few-shot document question answering. (arXiv:2308.10959v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DocPrompt&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DocPrompt&#27169;&#22411;&#32463;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#21518;&#22312;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#20132;&#20184;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#38477;&#20302;&#20102;&#27880;&#37322;&#25104;&#26412;&#21644;&#21171;&#21160;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DocPrompt&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24369;&#30417;&#30563;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12289;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#35299;&#27169;&#22411;&#21644;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#32463;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#30340;DocPrompt&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#25991;&#26723;&#38382;&#31572;&#23458;&#25143;&#39033;&#30446;&#30340;&#20132;&#20184;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#38477;&#20302;&#20102;&#27880;&#37322;&#25104;&#26412;&#21644;&#21171;&#21160;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#28436;&#31034;&#21487;&#20197;&#22312;https://huggingface.co/spaces/PaddlePaddle/ERNIE-Layout&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose Docprompt for document question answering tasks with powerful zero-shot and few-shot performance. We proposed a novel weakly supervised data generation method, a novel multl-stage training method and a novel understanding model &amp; generation model ensemble method. Experiment results show that the Docprompt model after continue pretrain significantly outperforms the existing strong baseline models on document question answering tasks. This method greatly improves the delivery efficiency and model performance of document question answering customer projects, reducing annotation costs and labor costs. Our demo can be found at https://huggingface.co/spaces/PaddlePaddle/ERNIE-Layout.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#27604;&#36739;&#20102;ChatGPT&#21644;&#20154;&#31867;&#22312;&#35789;&#27719;&#21644;&#35789;&#27719;&#20016;&#23500;&#24230;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;ChatGPT&#31561;&#24037;&#20855;&#20250;&#23545;&#35789;&#27719;&#20351;&#29992;&#21644;&#35789;&#27719;&#20016;&#23500;&#24230;&#20135;&#29983;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#35821;&#35328;&#28436;&#21464;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.07462</link><description>&lt;p&gt;
&#29609;&#24324;&#25991;&#23383;&#65306;&#27604;&#36739;ChatGPT&#21644;&#20154;&#31867;&#30340;&#35789;&#27719;&#21644;&#35789;&#27719;&#20016;&#23500;&#24230;
&lt;/p&gt;
&lt;p&gt;
Playing with Words: Comparing the Vocabulary and Lexical Richness of ChatGPT and Humans. (arXiv:2308.07462v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07462
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#27604;&#36739;&#20102;ChatGPT&#21644;&#20154;&#31867;&#22312;&#35789;&#27719;&#21644;&#35789;&#27719;&#20016;&#23500;&#24230;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;ChatGPT&#31561;&#24037;&#20855;&#20250;&#23545;&#35789;&#27719;&#20351;&#29992;&#21644;&#35789;&#27719;&#20016;&#23500;&#24230;&#20135;&#29983;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#35821;&#35328;&#28436;&#21464;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT&#65289;&#21644;ChatGPT&#31561;&#24037;&#20855;&#30340;&#24341;&#20837;&#24341;&#21457;&#20102;&#19968;&#22330;&#38761;&#21629;&#65292;&#21487;&#20197;&#25913;&#21464;&#25991;&#26412;&#29983;&#25104;&#30340;&#26041;&#24335;&#12290;&#36825;&#23545;&#35835;&#32773;&#30340;&#35821;&#35328;&#33021;&#21147;&#20197;&#21450;&#26032;&#22411;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#22521;&#35757;&#26159;&#21542;&#20250;&#20135;&#29983;&#24433;&#21709;&#20855;&#26377;&#35768;&#22810;&#21547;&#20041;&#65311;&#23427;&#26159;&#21542;&#20250;&#24433;&#21709;&#35821;&#35328;&#30340;&#28436;&#21464;&#65311;&#25105;&#20204;&#20851;&#27880;&#35821;&#35328;&#30340;&#19968;&#20010;&#29305;&#23450;&#26041;&#38754;&#65306;&#35789;&#35821;&#65307;&#22312;&#32534;&#20889;&#32473;&#23450;&#25991;&#26412;&#26102;&#65292;&#20351;&#29992;ChatGPT&#31561;&#24037;&#20855;&#20250;&#22686;&#21152;&#25110;&#20943;&#23569;&#20351;&#29992;&#30340;&#35789;&#27719;&#37327;&#25110;&#35789;&#27719;&#20016;&#23500;&#24230;&#65288;&#29702;&#35299;&#20026;&#20070;&#38754;&#25110;&#21475;&#22836;&#34920;&#36798;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#35789;&#27719;&#25968;&#37327;&#65289;&#65311;&#36825;&#23545;&#35789;&#35821;&#26377;&#24433;&#21709;&#65292;&#22240;&#20026;&#26410;&#21253;&#21547;&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#20013;&#30340;&#35789;&#35821;&#24448;&#24448;&#20250;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#21463;&#27426;&#36814;&#65292;&#24182;&#26368;&#32456;&#21487;&#33021;&#28040;&#22833;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;ChatGPT&#21644;&#20154;&#31867;&#30340;&#35789;&#27719;&#21644;&#35789;&#27719;&#20016;&#23500;&#24230;&#36827;&#34892;&#20102;&#21021;&#27493;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of Artificial Intelligence (AI) generative language models such as GPT (Generative Pre-trained Transformer) and tools such as ChatGPT has triggered a revolution that can transform how text is generated. This has many implications, for example, as AI-generated text becomes a significant fraction of the text in many disciplines, would this have an effect on the language capabilities of readers and also on the training of newer AI tools? Would it affect the evolution of languages? Focusing on one specific aspect of the language: words; will the use of tools such as ChatGPT increase or reduce the vocabulary used or the lexical richness (understood as the number of different words used in a written or oral production) when writing a given text? This has implications for words, as those not included in AI-generated content will tend to be less and less popular and may eventually be lost. In this work, we perform an initial comparison of the vocabulary and lexical richness of
&lt;/p&gt;</description></item><item><title>Sensi-BERT&#26159;&#19968;&#31181;&#38754;&#21521;&#25935;&#24863;&#24230;&#39537;&#21160;&#30340;&#21442;&#25968;&#39640;&#25928;BERT&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#25935;&#24863;&#24230;&#20998;&#26512;&#21644;&#35009;&#21098;&#21442;&#25968;&#24352;&#37327;&#65292;&#21487;&#29983;&#25104;&#36866;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#39640;&#24230;&#21442;&#25968;&#39640;&#25928;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.11764</link><description>&lt;p&gt;
Sensi-BERT: &#38754;&#21521;&#25935;&#24863;&#24230;&#39537;&#21160;&#30340;&#21442;&#25968;&#39640;&#25928;BERT&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Sensi-BERT: Towards Sensitivity Driven Fine-Tuning for Parameter-Efficient BERT. (arXiv:2307.11764v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11764
&lt;/p&gt;
&lt;p&gt;
Sensi-BERT&#26159;&#19968;&#31181;&#38754;&#21521;&#25935;&#24863;&#24230;&#39537;&#21160;&#30340;&#21442;&#25968;&#39640;&#25928;BERT&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#25935;&#24863;&#24230;&#20998;&#26512;&#21644;&#35009;&#21098;&#21442;&#25968;&#24352;&#37327;&#65292;&#21487;&#29983;&#25104;&#36866;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#39640;&#24230;&#21442;&#25968;&#39640;&#25928;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#22312;&#25991;&#26412;&#20998;&#31867;&#21644;&#38382;&#31572;&#31561;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#25913;&#36827;&#34920;&#29616;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36880;&#28176;&#21463;&#21040;&#20851;&#27880;&#65292;&#21482;&#38656;&#36827;&#34892;&#24456;&#23569;&#27425;&#25968;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#20854;&#24222;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#24120;&#24120;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#24212;&#29992;&#12290;&#29616;&#26377;&#30340;&#21442;&#25968;&#39640;&#25928;BERT&#27169;&#22411;&#35299;&#20915;&#26041;&#26696;&#22823;&#22810;&#20381;&#36182;&#20110;&#35745;&#31639;&#23494;&#38598;&#30340;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#24182;&#19988;&#24120;&#24120;&#20381;&#36182;&#20110;&#39069;&#22806;&#30340;&#35745;&#31639;&#23494;&#38598;&#22411;&#27169;&#22411;&#26469;&#24357;&#34917;&#24615;&#33021;&#24046;&#36317;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Sensi-BERT&#65292;&#19968;&#31181;&#25935;&#24863;&#24230;&#39537;&#21160;&#30340;BERT&#27169;&#22411;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;BERT&#27169;&#22411;&#65292;&#29983;&#25104;&#36866;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#39640;&#24230;&#21442;&#25968;&#39640;&#25928;&#30340;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36827;&#34892;&#25935;&#24863;&#24230;&#20998;&#26512;&#20197;&#23545;&#27599;&#20010;&#21333;&#29420;&#30340;&#21442;&#25968;&#24352;&#37327;&#36827;&#34892;&#25490;&#24207;&#65292;&#28982;&#21518;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#26681;&#25454;&#32473;&#23450;&#30340;&#21442;&#25968;&#25110;FLOPs&#39044;&#31639;&#36827;&#34892;&#30456;&#24212;&#30340;&#35009;&#21098;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;Sensi-BERT&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained language models have recently gained significant traction due to their improved performance on various down-stream tasks like text classification and question answering, requiring only few epochs of fine-tuning. However, their large model sizes often prohibit their applications on resource-constrained edge devices. Existing solutions of yielding parameter-efficient BERT models largely rely on compute-exhaustive training and fine-tuning. Moreover, they often rely on additional compute heavy models to mitigate the performance gap. In this paper, we present Sensi-BERT, a sensitivity driven efficient fine-tuning of BERT models that can take an off-the-shelf pre-trained BERT model and yield highly parameter-efficient models for downstream tasks. In particular, we perform sensitivity analysis to rank each individual parameter tensor, that then is used to trim them accordingly during fine-tuning for a given parameter or FLOPs budget. Our experiments show the efficacy of Sens
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19977;&#33410;&#27425;&#30340;&#23450;&#24615;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#20154;&#31867;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#20889;&#36807;&#31243;&#20013;&#30340;&#21512;&#20316;&#27169;&#24335;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#20154;&#26426;&#20849;&#21019;&#36807;&#31243;&#65306;&#26500;&#24605;&#12289;&#21551;&#21457;&#21644;&#23454;&#26045;&#12290;&#22312;&#36825;&#20010;&#21512;&#20316;&#36807;&#31243;&#20013;&#65292;&#20154;&#31867;&#25198;&#28436;&#30528;&#20027;&#23548;&#35282;&#33394;&#12290;</title><link>http://arxiv.org/abs/2307.10811</link><description>&lt;p&gt;
"&#24863;&#35273;&#20687;&#26377;&#31532;&#20108;&#20010;&#24605;&#32500;": &#25506;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#21019;&#24847;&#21487;&#20889;&#24615;&#39044;&#20889;&#30340;&#20154;&#26426;&#20849;&#21019;
&lt;/p&gt;
&lt;p&gt;
"It Felt Like Having a Second Mind": Investigating Human-AI Co-creativity in Prewriting with Large Language Models. (arXiv:2307.10811v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10811
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19977;&#33410;&#27425;&#30340;&#23450;&#24615;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#20154;&#31867;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#20889;&#36807;&#31243;&#20013;&#30340;&#21512;&#20316;&#27169;&#24335;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#20154;&#26426;&#20849;&#21019;&#36807;&#31243;&#65306;&#26500;&#24605;&#12289;&#21551;&#21457;&#21644;&#23454;&#26045;&#12290;&#22312;&#36825;&#20010;&#21512;&#20316;&#36807;&#31243;&#20013;&#65292;&#20154;&#31867;&#25198;&#28436;&#30528;&#20027;&#23548;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#20889;&#26159;&#22312;&#31532;&#19968;&#31295;&#20043;&#21069;&#21457;&#29616;&#21644;&#21457;&#23637;&#24605;&#24819;&#30340;&#36807;&#31243;&#65292;&#23427;&#38656;&#35201;&#21457;&#25955;&#24615;&#24605;&#32500;&#65292;&#36890;&#24120;&#28041;&#21450;&#21040;&#26080;&#32467;&#26500;&#30340;&#31574;&#30053;&#65292;&#22914;&#22270;&#34920;&#12289;&#27010;&#36848;&#21644;&#33258;&#30001;&#20889;&#20316;&#31561;&#12290;&#34429;&#28982;&#24050;&#32463;&#35777;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#26159;&#26377;&#29992;&#30340;&#65292;&#21253;&#25324;&#21019;&#24847;&#20889;&#20316;&#65292;&#20294;&#23545;&#29992;&#25143;&#22914;&#20309;&#19982;LLMs&#21512;&#20316;&#26469;&#25903;&#25345;&#39044;&#20889;&#30340;&#26041;&#24335;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#31181;&#21019;&#36896;&#24615;&#36807;&#31243;&#20013;&#65292;LLMs&#30340;&#39318;&#36873;&#21512;&#20316;&#35282;&#33394;&#21644;&#20027;&#21160;&#24615;&#20063;&#19981;&#26126;&#30830;&#12290;&#20026;&#20102;&#30740;&#31350;&#20154;&#31867;&#19982;LLMs&#22312;&#39044;&#20889;&#36807;&#31243;&#20013;&#30340;&#21512;&#20316;&#27169;&#24335;&#21644;&#21160;&#21147;&#23398;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#19977;&#33410;&#27425;&#30340;&#23450;&#24615;&#30740;&#31350;&#65292;&#19982;15&#20301;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#20004;&#20010;&#21019;&#36896;&#24615;&#20219;&#21153;&#65306;&#20889;&#25925;&#20107;&#21644;&#20889;&#21475;&#21495;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21512;&#20316;&#30340;&#39044;&#20889;&#36807;&#31243;&#20013;&#65292;&#20284;&#20046;&#23384;&#22312;&#30528;&#19968;&#20010;&#19977;&#38454;&#27573;&#36845;&#20195;&#30340;&#20154;&#26426;&#20849;&#21019;&#36807;&#31243;&#65292;&#21253;&#25324;&#26500;&#24605;&#12289;&#21551;&#21457;&#21644;&#23454;&#26045;&#38454;&#27573;&#12290;&#36825;&#20010;&#21512;&#20316;&#36807;&#31243;&#20197;&#20154;&#31867;&#22312;&#20027;&#23548;&#35282;&#33394;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prewriting is the process of discovering and developing ideas before a first draft, which requires divergent thinking and often implies unstructured strategies such as diagramming, outlining, free-writing, etc. Although large language models (LLMs) have been demonstrated to be useful for a variety of tasks including creative writing, little is known about how users would collaborate with LLMs to support prewriting. The preferred collaborative role and initiative of LLMs during such a creativity process is also unclear. To investigate human-LLM collaboration patterns and dynamics during prewriting, we conducted a three-session qualitative study with 15 participants in two creative tasks: story writing and slogan writing. The findings indicated that during collaborative prewriting, there appears to be a three-stage iterative Human-AI Co-creativity process that includes Ideation, Illumination, and Implementation stages. This collaborative process champions the human in a dominant role, in
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120; (mDT) &#26159;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#20167;&#24680;&#35328;&#35770;&#30340;&#26032;&#39062;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;&#20165;&#20351;&#29992;&#25991;&#26412;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;mDT&#36890;&#36807;&#25972;&#20307;&#20998;&#26512;&#25991;&#26412;&#21644;&#22270;&#20687;&#65292;&#32467;&#21512;&#22270;&#21464;&#25442;&#22120;&#25429;&#25417;&#35780;&#35770;&#21608;&#22260;&#25972;&#20010;&#35752;&#35770;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#20132;&#32455;&#34701;&#21512;&#23618;&#23558;&#25991;&#26412;&#21644;&#22270;&#20687;&#23884;&#20837;&#36827;&#34892;&#32452;&#21512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25429;&#25417;&#23545;&#35805;&#30340;&#25972;&#20307;&#35270;&#22270;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#26816;&#27979;&#21453;&#31038;&#20250;&#34892;&#20026;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09312</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120;&#65306;&#25972;&#21512;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#21464;&#25442;&#22120;&#20197;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media. (arXiv:2307.09312v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09312
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120; (mDT) &#26159;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#20167;&#24680;&#35328;&#35770;&#30340;&#26032;&#39062;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;&#20165;&#20351;&#29992;&#25991;&#26412;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;mDT&#36890;&#36807;&#25972;&#20307;&#20998;&#26512;&#25991;&#26412;&#21644;&#22270;&#20687;&#65292;&#32467;&#21512;&#22270;&#21464;&#25442;&#22120;&#25429;&#25417;&#35780;&#35770;&#21608;&#22260;&#25972;&#20010;&#35752;&#35770;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#20132;&#32455;&#34701;&#21512;&#23618;&#23558;&#25991;&#26412;&#21644;&#22270;&#20687;&#23884;&#20837;&#36827;&#34892;&#32452;&#21512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25429;&#25417;&#23545;&#35805;&#30340;&#25972;&#20307;&#35270;&#22270;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#26816;&#27979;&#21453;&#31038;&#20250;&#34892;&#20026;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22522;&#20110;&#22270;&#30340;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#21517;&#20026;&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120;&#65288;mDT&#65289;&#65292;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;&#19982;&#20256;&#32479;&#30340;&#20165;&#20351;&#29992;&#25991;&#26412;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#26631;&#35760;&#35780;&#35770;&#20026;&#20167;&#24680;&#35328;&#35770;&#30340;&#26041;&#27861;&#22260;&#32469;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#25972;&#20307;&#20998;&#26512;&#23637;&#24320;&#12290;&#36825;&#26159;&#36890;&#36807;&#21033;&#29992;&#22270;&#21464;&#25442;&#22120;&#26469;&#25429;&#25417;&#35780;&#35770;&#21608;&#22260;&#25972;&#20010;&#35752;&#35770;&#20013;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#65292;&#24182;&#37319;&#29992;&#20132;&#32455;&#34701;&#21512;&#23618;&#26469;&#32452;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#23884;&#20837;&#65292;&#32780;&#19981;&#26159;&#21333;&#29420;&#22788;&#29702;&#19981;&#21516;&#30340;&#27169;&#24577;&#12290;&#25105;&#20204;&#23558;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20165;&#22788;&#29702;&#25991;&#26412;&#30340;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#65292;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#28040;&#34701;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#26395;&#20102;&#22810;&#27169;&#24577;&#35299;&#20915;&#26041;&#26696;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#25552;&#20379;&#31038;&#20250;&#20215;&#20540;&#30340;&#26410;&#26469;&#24037;&#20316;&#65292;&#24182;&#35748;&#20026;&#25429;&#25417;&#23545;&#35805;&#30340;&#25972;&#20307;&#35270;&#22270;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26816;&#27979;&#21453;&#31038;&#20250;&#34892;&#20026;&#30340;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the Multi-Modal Discussion Transformer (mDT), a novel multi-modal graph-based transformer model for detecting hate speech in online social networks. In contrast to traditional text-only methods, our approach to labelling a comment as hate speech centers around the holistic analysis of text and images. This is done by leveraging graph transformers to capture the contextual relationships in the entire discussion that surrounds a comment, with interwoven fusion layers to combine text and image embeddings instead of processing different modalities separately. We compare the performance of our model to baselines that only process text; we also conduct extensive ablation studies. We conclude with future work for multimodal solutions to deliver social value in online contexts, arguing that capturing a holistic view of a conversation greatly advances the effort to detect anti-social behavior.
&lt;/p&gt;</description></item><item><title>CARE-MI&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#22269;&#23381;&#23156;&#25252;&#29702;&#39046;&#22495;LLM&#34394;&#20551;&#20449;&#24687;&#30340;&#22522;&#20934;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#25552;&#20379;&#20102;&#26500;&#24314;&#38271;&#31687;&#29983;&#25104;&#35780;&#20272;&#22522;&#20934;&#30340;&#21019;&#26032;&#33539;&#24335;&#12290;</title><link>http://arxiv.org/abs/2307.01458</link><description>&lt;p&gt;
CARE-MI: &#20013;&#22269;&#23381;&#23156;&#25252;&#29702;&#39046;&#22495;&#30340;&#34394;&#20551;&#20449;&#24687;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care. (arXiv:2307.01458v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01458
&lt;/p&gt;
&lt;p&gt;
CARE-MI&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#22269;&#23381;&#23156;&#25252;&#29702;&#39046;&#22495;LLM&#34394;&#20551;&#20449;&#24687;&#30340;&#22522;&#20934;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#25552;&#20379;&#20102;&#26500;&#24314;&#38271;&#31687;&#29983;&#25104;&#35780;&#20272;&#22522;&#20934;&#30340;&#21019;&#26032;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#23637;&#23548;&#33268;&#20102;&#23558;LLM&#24212;&#29992;&#20110;&#29616;&#23454;&#22330;&#26223;&#30340;&#26032;&#36235;&#21183;&#12290;&#23613;&#31649;&#26368;&#26032;&#30340;LLM&#22312;&#19982;&#20154;&#31867;&#20114;&#21160;&#26102;&#20196;&#20154;&#24778;&#21497;&#22320;&#27969;&#21033;&#65292;&#20294;&#23427;&#20204;&#22312;&#29983;&#25104;&#38169;&#35823;&#20107;&#23454;&#38472;&#36848;&#26102;&#20250;&#24847;&#22806;&#20135;&#29983;&#34394;&#20551;&#20449;&#24687;&#38382;&#39064;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#26377;&#23475;&#21518;&#26524;&#65292;&#23588;&#20854;&#26159;&#22312;&#25935;&#24863;&#29615;&#22659;&#19979;&#65292;&#27604;&#22914;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#35780;&#20272;LLM&#38271;&#31687;&#29983;&#25104;&#20013;&#30340;&#34394;&#20551;&#20449;&#24687;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#30693;&#35782;&#23494;&#38598;&#22411;&#20027;&#39064;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;LLM&#22312;&#19981;&#21516;&#35821;&#35328;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#34394;&#20551;&#20449;&#24687;&#35780;&#20272;&#20027;&#35201;&#22312;&#33521;&#35821;&#20013;&#36827;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;CARE-MI&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#34394;&#20551;&#20449;&#24687;&#22312;&#65306;1&#65289;&#19968;&#20010;&#25935;&#24863;&#20027;&#39064;&#65292;&#20855;&#20307;&#26159;&#23381;&#23156;&#25252;&#29702;&#39046;&#22495;&#65307;&#21644;2&#65289;&#19968;&#31181;&#38750;&#33521;&#35821;&#35821;&#35328;&#65292;&#21363;&#20013;&#25991;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#26500;&#24314;&#38271;&#31687;&#29983;&#25104;&#35780;&#20272;&#22522;&#20934;&#65292;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
The recent advances in NLP, have led to a new trend of applying LLMs to real-world scenarios. While the latest LLMs are astonishingly fluent when interacting with humans, they suffer from the misinformation problem by unintentionally generating factually false statements. This can lead to harmful consequences, especially when produced within sensitive contexts, such as healthcare. Yet few previous works have focused on evaluating misinformation in the long-form generation of LLMs, especially for knowledge-intensive topics. Moreover, although LLMs have been shown to perform well in different languages, misinformation evaluation has been mostly conducted in English. To this end, we present a benchmark, CARE-MI, for evaluating LLM misinformation in: 1) a sensitive topic, specifically the maternity and infant care domain; and 2) a language other than English, namely Chinese. Most importantly, we provide an innovative paradigm for building long-form generation evaluation benchmarks that can
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#21644;&#32858;&#21512;&#20505;&#36873;&#24230;&#37327;&#26469;&#20248;&#21270;&#38598;&#21512;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#39640;&#30456;&#20284;&#24230;&#35780;&#20272;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;&#38598;&#21512;&#23545;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2307.00925</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
Automatic Design of Semantic Similarity Ensembles Using Grammatical Evolution. (arXiv:2307.00925v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#21644;&#32858;&#21512;&#20505;&#36873;&#24230;&#37327;&#26469;&#20248;&#21270;&#38598;&#21512;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#39640;&#30456;&#20284;&#24230;&#35780;&#20272;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;&#38598;&#21512;&#23545;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22810;&#31181;&#19982;&#35745;&#31639;&#26426;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#21333;&#19968;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#36866;&#29992;&#20110;&#25152;&#26377;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#20351;&#29992;&#38598;&#21512;&#31574;&#30053;&#26469;&#30830;&#20445;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;&#30340;&#26041;&#27861;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#39318;&#27425;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#26469;&#33258;&#21160;&#36873;&#25321;&#21644;&#32858;&#21512;&#19968;&#32452;&#20505;&#36873;&#24230;&#37327;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#26368;&#22823;&#21270;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#30340;&#38598;&#21512;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#38598;&#21512;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#30456;&#20284;&#24230;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26082;&#23637;&#31034;&#20102;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#26469;&#33258;&#21160;&#27604;&#36739;&#25991;&#26412;&#30340;&#28508;&#21147;&#65292;&#20063;&#35777;&#26126;&#20102;&#20351;&#29992;&#38598;&#21512;&#23545;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic similarity measures are widely used in natural language processing to catalyze various computer-related tasks. However, no single semantic similarity measure is the most appropriate for all tasks, and researchers often use ensemble strategies to ensure performance. This research work proposes a method for automatically designing semantic similarity ensembles. In fact, our proposed method uses grammatical evolution, for the first time, to automatically select and aggregate measures from a pool of candidates to create an ensemble that maximizes correlation to human judgment. The method is evaluated on several benchmark datasets and compared to state-of-the-art ensembles, showing that it can significantly improve similarity assessment accuracy and outperform existing methods in some cases. As a result, our research demonstrates the potential of using grammatical evolution to automatically compare text and prove the benefits of using ensembles for semantic similarity tasks. The so
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#28857;&#23545;&#28857;&#20114;&#20449;&#24687;&#30340;&#27169;&#22411;-&#26080;&#20851;&#26041;&#27861;&#65292;&#29992;&#20110;&#34913;&#37327;&#23545;&#35805;&#31995;&#32479;&#19982;&#29992;&#25143;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#36890;&#36807;&#26367;&#25442;&#35780;&#20998;&#22120;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15245</link><description>&lt;p&gt;
C-PMI: &#26465;&#20214;&#28857;&#23545;&#28857;&#20114;&#20449;&#24687;&#29992;&#20110;&#23545;&#35805;&#35780;&#20272;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
C-PMI: Conditional Pointwise Mutual Information for Turn-level Dialogue Evaluation. (arXiv:2306.15245v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#28857;&#23545;&#28857;&#20114;&#20449;&#24687;&#30340;&#27169;&#22411;-&#26080;&#20851;&#26041;&#27861;&#65292;&#29992;&#20110;&#34913;&#37327;&#23545;&#35805;&#31995;&#32479;&#19982;&#29992;&#25143;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#36890;&#36807;&#26367;&#25442;&#35780;&#20998;&#22120;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;chatbot&#30340;&#26080;&#21442;&#32771;&#32423;&#23545;&#35805;&#35780;&#20272;&#25351;&#26631;&#19981;&#36275;&#20197;&#25429;&#25417;&#29992;&#25143;&#19982;&#31995;&#32479;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#36890;&#24120;&#19982;&#20154;&#31867;&#35780;&#20272;&#30340;&#30456;&#20851;&#24615;&#36739;&#24046;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#21033;&#29992;&#26465;&#20214;&#28857;&#23545;&#28857;&#20114;&#20449;&#24687;&#65288;C-PMI&#65289;&#26469;&#24230;&#37327;&#31995;&#32479;&#21644;&#29992;&#25143;&#20043;&#38388;&#22522;&#20110;&#32473;&#23450;&#35780;&#20272;&#32500;&#24230;&#30340;&#23545;&#35805;&#20132;&#20114;&#12290;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;FED&#23545;&#35805;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#35780;&#20272;&#31995;&#32479;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#23558;&#22522;&#20110;&#36127;&#23545;&#25968;&#20284;&#28982;&#30340;&#35780;&#20998;&#22120;&#26367;&#25442;&#20026;&#25105;&#20204;&#25552;&#20986;&#30340;C-PMI&#35780;&#20998;&#22120;&#65292;&#25105;&#20204;&#22312;FED&#35780;&#20272;&#25351;&#26631;&#19978;&#30340;Spearman&#30456;&#20851;&#24615;&#24179;&#22343;&#30456;&#23545;&#25552;&#39640;&#20102;60.5%&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#21457;&#24067;&#22312;https://github.com/renll/C-PMI&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing reference-free turn-level evaluation metrics for chatbots inadequately capture the interaction between the user and the system. Consequently, they often correlate poorly with human evaluations. To address this issue, we propose a novel model-agnostic approach that leverages Conditional Pointwise Mutual Information (C-PMI) to measure the turn-level interaction between the system and the user based on a given evaluation dimension. Experimental results on the widely used FED dialogue evaluation dataset demonstrate that our approach significantly improves the correlation with human judgment compared with existing evaluation systems. By replacing the negative log-likelihood-based scorer with our proposed C-PMI scorer, we achieve a relative 60.5% higher Spearman correlation on average for the FED evaluation metric. Our code is publicly available at https://github.com/renll/C-PMI.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#25216;&#26415;&#26469;&#25552;&#39640;&#38750;&#33258;&#22238;&#24402;&#32763;&#35793;&#27169;&#22411;&#30340;&#32763;&#35793;&#36136;&#37327;&#65292;&#22312;&#20445;&#25345;&#26174;&#30528;&#25512;&#29702;&#36895;&#24230;&#21152;&#36895;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12289;&#37319;&#29992;MASK&#25554;&#20837;&#26041;&#26696;&#36827;&#34892;&#19978;&#37319;&#26679;&#12289;&#20197;&#21450;&#37319;&#29992;&#23884;&#20837;&#33976;&#39311;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.06345</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12289;&#23884;&#20837;&#33976;&#39311;&#21644;&#19978;&#37319;&#26679;&#31574;&#30053;&#25913;&#21892;&#38750;&#33258;&#22238;&#24402;&#32763;&#35793;&#36136;&#37327;&#65288;arXiv:2306.06345v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
Improving Non-autoregressive Translation Quality with Pretrained Language Model, Embedding Distillation and Upsampling Strategy for CTC. (arXiv:2306.06345v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#25216;&#26415;&#26469;&#25552;&#39640;&#38750;&#33258;&#22238;&#24402;&#32763;&#35793;&#27169;&#22411;&#30340;&#32763;&#35793;&#36136;&#37327;&#65292;&#22312;&#20445;&#25345;&#26174;&#30528;&#25512;&#29702;&#36895;&#24230;&#21152;&#36895;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12289;&#37319;&#29992;MASK&#25554;&#20837;&#26041;&#26696;&#36827;&#34892;&#19978;&#37319;&#26679;&#12289;&#20197;&#21450;&#37319;&#29992;&#23884;&#20837;&#33976;&#39311;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#26041;&#27861;&#26088;&#22312;&#25552;&#39640;&#32763;&#35793;&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#21487;&#20197;&#19968;&#27425;&#27491;&#21521;&#20256;&#36882;&#29983;&#25104;&#36755;&#20986;&#30340;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#22312;&#32763;&#35793;&#36136;&#37327;&#19978;&#26377;&#26174;&#33879;&#30340;&#19979;&#38477;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#21019;&#26032;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#38750;&#33258;&#22238;&#24402;&#32763;&#35793;&#27169;&#22411;&#30340;&#32763;&#35793;&#36136;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#25512;&#29702;&#36895;&#24230;&#30340;&#26174;&#33879;&#21152;&#36895;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;CTC&#25439;&#22833;&#24494;&#35843;&#39044;&#35757;&#32451;&#22810;&#35821;&#35328;&#27169;&#22411;&#26469;&#26377;&#25928;&#22320;&#35757;&#32451;NAT&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;MASK&#25554;&#20837;&#26041;&#26696;&#36827;&#34892;&#19978;&#37319;&#26679;&#65292;&#32780;&#19981;&#26159;&#20196;&#29260;&#22797;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#33976;&#39311;&#26041;&#27861;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#22522;&#32447;&#33258;&#22238;&#24402;&#27169;&#22411;&#65288;Transformer base&#65289;&#65292;&#21253;&#25324;WMT'14 DE $\leftrightarrow$ EN&#12289;WMT'16 RO $\leftrightarrow$ EN&#21644;IWSLT'14 DE $\leftrightarrow$ EN&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-autoregressive approaches aim to improve the inference speed of translation models, particularly those that generate output in a one-pass forward manner. However, these approaches often suffer from a significant drop in translation quality compared to autoregressive models. This paper introduces a series of innovative techniques to enhance the translation quality of Non-Autoregressive Translation (NAT) models while maintaining a substantial acceleration in inference speed. We propose fine-tuning Pretrained Multilingual Language Models (PMLMs) with the CTC loss to train NAT models effectively. Furthermore, we adopt the MASK insertion scheme for up-sampling instead of token duplication, and we present an embedding distillation method to further enhance performance. In our experiments, our model outperforms the baseline autoregressive model (Transformer \textit{base}) on multiple datasets, including WMT'14 DE$\leftrightarrow$EN, WMT'16 RO$\leftrightarrow$EN, and IWSLT'14 DE$\leftright
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;LLM&#39537;&#21160;&#30340;&#29983;&#25104;&#24335;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;GENRE&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#20041;&#30693;&#35782;&#20016;&#23500;&#26032;&#38395;&#25968;&#25454;&#65292;&#36890;&#36807;&#20174;&#27169;&#22411;&#35774;&#35745;&#36716;&#31227;&#21040;&#25552;&#31034;&#35774;&#35745;&#25552;&#20379;&#28789;&#27963;&#32780;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#26032;&#38395;&#29983;&#25104;&#12289;&#29992;&#25143;&#30011;&#20687;&#21644;&#26032;&#38395;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.06566</link><description>&lt;p&gt;
LLM&#39537;&#21160;&#30340;&#29983;&#25104;&#24335;&#26032;&#38395;&#25512;&#33616;&#21021;&#25506;
&lt;/p&gt;
&lt;p&gt;
A First Look at LLM-Powered Generative News Recommendation. (arXiv:2305.06566v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;LLM&#39537;&#21160;&#30340;&#29983;&#25104;&#24335;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;GENRE&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#20041;&#30693;&#35782;&#20016;&#23500;&#26032;&#38395;&#25968;&#25454;&#65292;&#36890;&#36807;&#20174;&#27169;&#22411;&#35774;&#35745;&#36716;&#31227;&#21040;&#25552;&#31034;&#35774;&#35745;&#25552;&#20379;&#28789;&#27963;&#32780;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#26032;&#38395;&#29983;&#25104;&#12289;&#29992;&#25143;&#30011;&#20687;&#21644;&#26032;&#38395;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#30340;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#24050;&#25104;&#20026;&#29992;&#25143;&#27983;&#35272;&#28023;&#37327;&#22312;&#32447;&#26032;&#38395;&#20869;&#23481;&#25152;&#24517;&#38656;&#30340;&#24037;&#20855;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#30528;&#20919;&#21551;&#21160;&#38382;&#39064;&#12289;&#29992;&#25143;&#30011;&#20687;&#24314;&#27169;&#21644;&#26032;&#38395;&#20869;&#23481;&#29702;&#35299;&#31561;&#37325;&#22823;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#36890;&#36807;&#27169;&#22411;&#35774;&#35745;&#36981;&#24490;&#19968;&#31181;&#19981;&#28789;&#27963;&#30340;&#20363;&#34892;&#31243;&#24207;&#26469;&#35299;&#20915;&#29305;&#23450;&#30340;&#25361;&#25112;&#65292;&#20294;&#22312;&#29702;&#35299;&#26032;&#38395;&#20869;&#23481;&#21644;&#25429;&#25417;&#29992;&#25143;&#20852;&#36259;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GENRE&#65292;&#19968;&#31181;LLM&#39537;&#21160;&#30340;&#29983;&#25104;&#24335;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#35821;&#20041;&#30693;&#35782;&#26469;&#20016;&#23500;&#26032;&#38395;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20174;&#27169;&#22411;&#35774;&#35745;&#36716;&#31227;&#21040;&#25552;&#31034;&#35774;&#35745;&#26469;&#25552;&#20379;&#19968;&#31181;&#28789;&#27963;&#32780;&#32479;&#19968;&#30340;&#26032;&#38395;&#25512;&#33616;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;GENRE&#22312;&#20010;&#24615;&#21270;&#26032;&#38395;&#29983;&#25104;&#12289;&#29992;&#25143;&#30011;&#20687;&#21644;&#26032;&#38395;&#25688;&#35201;&#20013;&#30340;&#24212;&#29992;&#12290;&#20351;&#29992;&#21508;&#31181;&#27969;&#34892;&#30340;&#25512;&#33616;&#27169;&#22411;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;GENRE&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized news recommendation systems have become essential tools for users to navigate the vast amount of online news content, yet existing news recommenders face significant challenges such as the cold-start problem, user profile modeling, and news content understanding. Previous works have typically followed an inflexible routine to address a particular challenge through model design, but are limited in their ability to understand news content and capture user interests. In this paper, we introduce GENRE, an LLM-powered generative news recommendation framework, which leverages pretrained semantic knowledge from large language models to enrich news data. Our aim is to provide a flexible and unified solution for news recommendation by moving from model design to prompt design. We showcase the use of GENRE for personalized news generation, user profiling, and news summarization. Extensive experiments with various popular recommendation models demonstrate the effectiveness of GENRE. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24544;&#23454;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20174;&#27604;&#25945;&#24072;&#27169;&#22411;&#22823;&#25968;&#20493;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#19968;&#20010;&#23567;&#30340;&#12289;&#33258;&#25105;&#19968;&#33268;&#30340;&#24605;&#36335;&#20018;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#35777;&#26126;&#20915;&#31574;&#24182;&#25552;&#39640;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.01879</link><description>&lt;p&gt;
SCOTT: &#33258;&#25105;&#19968;&#33268;&#24615;&#24605;&#36335;&#20018;&#25552;&#28860;
&lt;/p&gt;
&lt;p&gt;
SCOTT: Self-Consistent Chain-of-Thought Distillation. (arXiv:2305.01879v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24544;&#23454;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20174;&#27604;&#25945;&#24072;&#27169;&#22411;&#22823;&#25968;&#20493;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#19968;&#20010;&#23567;&#30340;&#12289;&#33258;&#25105;&#19968;&#33268;&#30340;&#24605;&#36335;&#20018;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#35777;&#26126;&#20915;&#31574;&#24182;&#25552;&#39640;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#20986;&#19968;&#23450;&#35268;&#27169;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#36890;&#36807;&#19968;&#31995;&#21015;&#36830;&#32493;&#30340;&#24605;&#32771;&#36807;&#31243;&#33719;&#24471;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#30340;&#31361;&#20986;&#33021;&#21147;&#12290;&#34429;&#28982;&#24605;&#36335;&#20018;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#20165;&#22312;&#36275;&#22815;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#25165;&#33021;&#35266;&#23519;&#21040;&#36825;&#31181;&#25910;&#30410;&#12290;&#26356;&#20196;&#20154;&#25285;&#24551;&#30340;&#26159;&#65292;&#29983;&#25104;&#30340;&#29702;&#30001;&#24456;&#23569;&#20445;&#35777;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#20445;&#25345;&#19968;&#33268;&#25110;&#32773;&#24544;&#23454;&#22320;&#35777;&#26126;&#20915;&#31574;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24544;&#23454;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20174;&#27604;&#25945;&#24072;&#27169;&#22411;&#22823;&#25968;&#20493;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#19968;&#20010;&#23567;&#30340;&#12289;&#33258;&#25105;&#19968;&#33268;&#30340;&#24605;&#36335;&#20018;&#27169;&#22411;&#12290;&#20026;&#20102;&#24418;&#25104;&#26356;&#22909;&#30340;&#30417;&#30563;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#27604;&#35299;&#30721;&#24341;&#23548;&#25945;&#24072;&#27169;&#22411;&#20135;&#29983;&#25903;&#25345;&#27491;&#30830;&#31572;&#26696;&#30340;&#29702;&#30001;&#65292;&#36825;&#40723;&#21169;&#25945;&#24072;&#27169;&#22411;&#29983;&#25104;&#30340;token&#21482;&#22312;&#32771;&#34385;&#21040;&#31572;&#26696;&#26102;&#25165;&#26356;&#21152;&#21487;&#20449;&#12290;&#20026;&#20102;&#20445;&#35777;&#24544;&#23454;&#30340;&#33976;&#39311;&#65292;&#25105;&#20204;&#20351;&#29992;&#25945;&#24072;&#29983;&#25104;&#30340;&#29702;&#30001;&#26469;&#23398;&#20064;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#21453;&#20107;&#23454;&#25512;&#29702;&#30446;&#26631;&#65292;&#21363;&#26681;&#25454;&#20855;&#26377;&#33258;&#25105;&#19968;&#33268;&#24615;&#19988;&#24544;&#23454;&#20110;&#25945;&#24072;&#39044;&#27979;&#30340;&#24605;&#36335;&#20018;&#29702;&#30001;&#39044;&#27979;&#20915;&#31574;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#25277;&#35937;&#25688;&#35201;&#22522;&#20934;&#27979;&#35797;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23398;&#29983;&#27169;&#22411;&#20013;&#30340;&#33258;&#25105;&#19968;&#33268;&#24615;&#26377;&#21161;&#20110;&#35777;&#26126;&#20915;&#31574;&#24182;&#25552;&#39640;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LMs) beyond a certain scale, demonstrate the emergent capability of generating free-text rationales for their predictions via chain-of-thought (CoT) prompting. While CoT can yield dramatically improved performance, such gains are only observed for sufficiently large LMs. Even more concerning, there is little guarantee that the generated rationales are consistent with LM's predictions or faithfully justify the decisions. In this work, we propose a faithful knowledge distillation method to learn a small, self-consistent CoT model from a teacher model that is orders of magnitude larger. To form better supervision, we elicit rationales supporting the gold answers from a large LM (teacher) by contrastive decoding, which encourages the teacher to generate tokens that become more plausible only when the answer is considered. To ensure faithful distillation, we use the teacher-generated rationales to learn a student LM with a counterfactual reasoning objective, which pre
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;OLISIA&#65292;&#19968;&#20010;&#21475;&#35821;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#32423;&#32852;&#31995;&#32479;&#65292;&#20351;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;DST&#27169;&#22411;&#65292;&#37319;&#29992;&#20960;&#20010;&#36866;&#24212;&#24615;&#31574;&#30053;&#26469;&#25552;&#39640;&#31283;&#20581;&#24615;&#65292;&#24182;&#22312;DSTC11 Track3&#20013;&#21462;&#24471;&#31532;&#19968;&#21517;&#30340;&#22909;&#25104;&#32489;&#12290;</title><link>http://arxiv.org/abs/2304.11073</link><description>&lt;p&gt;
OLISIA: &#19968;&#20010;&#29992;&#20110;&#21475;&#35821;&#21270;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#32423;&#32852;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
OLISIA: a Cascade System for Spoken Dialogue State Tracking. (arXiv:2304.11073v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11073
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;OLISIA&#65292;&#19968;&#20010;&#21475;&#35821;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#32423;&#32852;&#31995;&#32479;&#65292;&#20351;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;DST&#27169;&#22411;&#65292;&#37319;&#29992;&#20960;&#20010;&#36866;&#24212;&#24615;&#31574;&#30053;&#26469;&#25552;&#39640;&#31283;&#20581;&#24615;&#65292;&#24182;&#22312;DSTC11 Track3&#20013;&#21462;&#24471;&#31532;&#19968;&#21517;&#30340;&#22909;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#29366;&#24577;&#36319;&#36394; (DST) &#26159;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#20851;&#20110;&#35813;&#20219;&#21153;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#20110;&#32842;&#22825;&#26102;&#30340;&#35821;&#26009;&#24211;&#65292;&#24573;&#30053;&#20102;&#21475;&#35821;&#21644;&#20070;&#38754;&#35821;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; OLISIA&#65292;&#36825;&#26159;&#19968;&#20010;&#32423;&#32852;&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035; (ASR) &#27169;&#22411;&#21644; DST &#27169;&#22411;&#12290;&#25105;&#20204;&#22312; ASR &#21644; DST &#27169;&#22359;&#20013;&#24341;&#20837;&#20102;&#20960;&#20010;&#36866;&#24212;&#24615;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#23545;&#21475;&#35821;&#23545;&#35805;&#30340;&#25972;&#21512;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#32463;&#36807;&#36825;&#20123;&#31574;&#30053;&#30340;&#35843;&#25972;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312; DSTC11 Track 3 &#20013;&#25490;&#21517;&#31532;&#19968;&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20272;&#21475;&#35821; DST &#24615;&#33021;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#32467;&#26524;&#20998;&#26512;&#65292;&#21457;&#29616;&#35268;&#33539;&#21270; ASR &#30340;&#36755;&#20986;&#21644;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#35843;&#25972; DST &#30340;&#36755;&#20837;&#65292;&#20197;&#21450;&#22686;&#21152;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22823;&#23567;&#65292;&#37117;&#22312;&#38477;&#20302;&#20070;&#38754;&#21644;&#21475;&#35821;&#23545;&#35805;&#20043;&#38388;&#24615;&#33021;&#24046;&#24322;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Though Dialogue State Tracking (DST) is a core component of spoken dialogue systems, recent work on this task mostly deals with chat corpora, disregarding the discrepancies between spoken and written language.In this paper, we propose OLISIA, a cascade system which integrates an Automatic Speech Recognition (ASR) model and a DST model. We introduce several adaptations in the ASR and DST modules to improve integration and robustness to spoken conversations.With these adaptations, our system ranked first in DSTC11 Track 3, a benchmark to evaluate spoken DST. We conduct an in-depth analysis of the results and find that normalizing the ASR outputs and adapting the DST inputs through data augmentation, along with increasing the pre-trained models size all play an important role in reducing the performance discrepancy between written and spoken conversations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20851;&#20110;&#20351;&#29992;Transformer&#26550;&#26500;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LaMDA&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#35828;&#27861;&#12290;&#20316;&#32773;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#19981;&#21487;&#33021;&#20855;&#26377;&#24847;&#35782;&#65292;&#32780;LaMDA&#27809;&#26377;&#27604;&#20854;&#20182;&#31867;&#20284;&#27169;&#22411;&#26356;&#20855;&#20808;&#36827;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.11483</link><description>&lt;p&gt;
Deanthropomorphising NLP&#65306;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#24847;&#35782;&#21040;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Deanthropomorphising NLP: Can a Language Model Be Conscious?. (arXiv:2211.11483v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20851;&#20110;&#20351;&#29992;Transformer&#26550;&#26500;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LaMDA&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#35828;&#27861;&#12290;&#20316;&#32773;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#19981;&#21487;&#33021;&#20855;&#26377;&#24847;&#35782;&#65292;&#32780;LaMDA&#27809;&#26377;&#27604;&#20854;&#20182;&#31867;&#20284;&#27169;&#22411;&#26356;&#20855;&#20808;&#36827;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23545;&#26368;&#36817;&#26377;&#20851;&#20351;&#29992;Transformer&#27169;&#22411;&#26550;&#26500;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LaMDA&#20855;&#26377;&#24847;&#35782;&#30340;&#35828;&#27861;&#36827;&#34892;&#35752;&#35770;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26679;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#21487;&#33021;&#20855;&#26377;&#24847;&#35782;&#65292;&#32780;LaMDA&#24182;&#27809;&#26377;&#27604;&#20854;&#20182;&#31867;&#20284;&#27169;&#22411;&#26356;&#20855;&#20808;&#36827;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#32508;&#21512;&#20449;&#24687;&#29702;&#35770;&#23545;Transformer&#26550;&#26500;&#36827;&#34892;&#20998;&#26512;&#26469;&#35777;&#26126;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#26377;&#24847;&#35782;&#30340;&#35828;&#27861;&#26159;NLP&#25253;&#36947;&#20013;&#20351;&#29992;&#25311;&#20154;&#21270;&#35821;&#35328;&#30340;&#26356;&#24191;&#27867;&#20542;&#21521;&#30340;&#19968;&#37096;&#20998;&#12290;&#26080;&#35770;&#36825;&#20123;&#35828;&#27861;&#30340;&#30495;&#23454;&#24615;&#22914;&#20309;&#65292;&#25105;&#20204;&#35748;&#20026;&#29616;&#22312;&#26159;&#35780;&#20272;&#35821;&#35328;&#24314;&#27169;&#36827;&#23637;&#24182;&#32771;&#34385;&#35813;&#20219;&#21153;&#30340;&#20262;&#29702;&#24433;&#21709;&#30340;&#36866;&#24403;&#26102;&#26426;&#12290;&#20026;&#20102;&#20351;&#26412;&#25991;&#26377;&#21161;&#20110;NLP&#31038;&#21306;&#20197;&#22806;&#30340;&#35835;&#32773;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20123;NLP&#22522;&#30784;&#30693;&#35782;&#30340;&#20171;&#32461;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work is intended as a voice in the discussion over the recent claims that LaMDA, a pretrained language model based on the Transformer model architecture, is sentient. This claim, if confirmed, would have serious ramifications in the Natural Language Processing (NLP) community due to wide-spread use of similar models. However, here we take the position that such a language model cannot be sentient, or conscious, and that LaMDA in particular exhibits no advances over other similar models that would qualify it. We justify this by analysing the Transformer architecture through Integrated Information Theory. We see the claims of consciousness as part of a wider tendency to use anthropomorphic language in NLP reporting. Regardless of the veracity of the claims, we consider this an opportune moment to take stock of progress in language modelling and consider the ethical implications of the task. In order to make this work helpful for readers outside the NLP community, we also present the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#19981;&#21516;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#35270;&#35273;&#25512;&#29702;&#33021;&#21147;&#21644;&#31038;&#20250;&#20559;&#35265;&#12290;&#23613;&#31649;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#23545;&#35937;&#35745;&#25968;&#21644;&#31354;&#38388;&#20851;&#31995;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#20173;&#23384;&#22312;&#19982;&#19978;&#30028;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#24615;&#21035;&#21644;&#32932;&#33394;&#26041;&#38754;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2202.04053</link><description>&lt;p&gt;
DALL-Eval: &#25506;&#31350;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#31038;&#20250;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generation Models. (arXiv:2202.04053v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.04053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#19981;&#21516;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#35270;&#35273;&#25512;&#29702;&#33021;&#21147;&#21644;&#31038;&#20250;&#20559;&#35265;&#12290;&#23613;&#31649;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#23545;&#35937;&#35745;&#25968;&#21644;&#31354;&#38388;&#20851;&#31995;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#20173;&#23384;&#22312;&#19982;&#19978;&#30028;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#24615;&#21035;&#21644;&#32932;&#33394;&#26041;&#38754;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;DALL-E&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#30340;&#36716;&#25442;&#35821;&#35328;&#27169;&#22411;&#21450;&#20854;&#21464;&#31181;&#65292;&#21253;&#25324;&#25193;&#25955;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#36924;&#30495;&#30340;&#22270;&#20687;&#29983;&#25104;&#32467;&#26524;&#65292;&#23545;&#20110;&#22914;&#20309;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#36824;&#27809;&#26377;&#36827;&#34892;&#35814;&#32454;&#30340;&#20998;&#26512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#35270;&#35273;&#25512;&#29702;&#33021;&#21147;&#21644;&#31038;&#20250;&#20559;&#35265;&#65292;&#28085;&#30422;&#20102;&#22810;&#27169;&#24577;&#36716;&#25442;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#19977;&#31181;&#35270;&#35273;&#25512;&#29702;&#33021;&#21147;&#65306;&#23545;&#35937;&#35782;&#21035;&#65292;&#23545;&#35937;&#35745;&#25968;&#21644;&#31354;&#38388;&#20851;&#31995;&#29702;&#35299;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PaintSkills&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#34913;&#37327;&#36825;&#20123;&#33021;&#21147;&#30340;&#32452;&#21512;&#24335;&#35786;&#26029;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#20855;&#26377;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#22312;&#23545;&#35937;&#35745;&#25968;&#21644;&#31354;&#38388;&#20851;&#31995;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#65292;&#26368;&#36817;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#19978;&#30028;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#24456;&#22823;&#24046;&#36317;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#24615;&#21035;&#21644;&#32932;&#33394;&#20559;&#35265;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#21035;&#21644;&#32932;&#33394;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, DALL-E, a multimodal transformer language model, and its variants, including diffusion models, have shown high-quality text-to-image generation capabilities. However, despite the realistic image generation results, there has not been a detailed analysis of how to evaluate such models. In this work, we investigate the visual reasoning capabilities and social biases of different text-to-image models, covering both multimodal transformer language models and diffusion models. First, we measure three visual reasoning skills: object recognition, object counting, and spatial relation understanding. For this, we propose PaintSkills, a compositional diagnostic evaluation dataset that measures these skills. Despite the high-fidelity image generation capability, a large gap exists between the performance of recent models and the upper bound accuracy in object counting and spatial relation understanding skills. Second, we assess the gender and skin tone biases by measuring the gender/ski
&lt;/p&gt;</description></item></channel></rss>