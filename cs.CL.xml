<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#20559;&#35265;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#29983;&#25104;&#23545;&#35805;&#20013;&#20844;&#24179;&#25991;&#26412;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#20844;&#24179;&#25991;&#26412;&#29983;&#25104;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#35777;&#26126;&#20102;&#23398;&#20064;&#20844;&#24179;&#21487;&#20197;&#24402;&#32467;&#20026;&#25913;&#21892;&#20154;&#31867;&#26679;&#24335;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.04303</link><description>&lt;p&gt;
&#20174;&#20559;&#35265;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#29983;&#25104;&#23545;&#35805;&#20013;&#20844;&#24179;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Equitable Text in Dialogue from Biased Training Data. (arXiv:2307.04303v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04303
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#20559;&#35265;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#29983;&#25104;&#23545;&#35805;&#20013;&#20844;&#24179;&#25991;&#26412;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#20844;&#24179;&#25991;&#26412;&#29983;&#25104;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#35777;&#26126;&#20102;&#23398;&#20064;&#20844;&#24179;&#21487;&#20197;&#24402;&#32467;&#20026;&#25913;&#21892;&#20154;&#31867;&#26679;&#24335;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#31995;&#32479;&#22312;&#20915;&#31574;&#36807;&#31243;&#21644;&#29983;&#25104;&#22238;&#22797;&#20013;&#27880;&#37325;&#20844;&#24179;&#21407;&#21017;&#23545;&#20110;&#29992;&#25143;&#30340;&#21442;&#19982;&#12289;&#28385;&#24847;&#24230;&#21644;&#20219;&#21153;&#23436;&#25104;&#33267;&#20851;&#37325;&#35201;&#12290;&#32570;&#20047;&#20844;&#24179;&#21644;&#21253;&#23481;&#21407;&#21017;&#21487;&#33021;&#38459;&#30861;&#20849;&#21516;&#22522;&#30784;&#30340;&#24418;&#25104;&#65292;&#20174;&#32780;&#23545;&#31995;&#32479;&#30340;&#25972;&#20307;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23545;&#35805;&#20013;&#20844;&#24179;&#25991;&#26412;&#29983;&#25104;&#30340;&#32508;&#21512;&#30740;&#31350;&#36824;&#19981;&#20840;&#38754;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#35745;&#31639;&#23398;&#20064;&#30340;&#29702;&#35770;&#26469;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25991;&#26412;&#29983;&#25104;&#20013;&#20844;&#24179;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#23398;&#20064;&#20154;&#31867;&#26679;&#24335;&#21644;&#23398;&#20064;&#20844;&#24179;&#20043;&#38388;&#30340;&#27491;&#24335;&#20851;&#32852;&#65306;&#25913;&#36827;&#20844;&#24179;&#30340;&#31639;&#27861;&#26368;&#32456;&#24402;&#32467;&#20026;&#25913;&#21892;&#20154;&#31867;&#26679;&#24335;&#30340;&#31639;&#27861;&#65288;&#22312;&#25193;&#20805;&#25968;&#25454;&#19978;&#65289;&#12290;&#22522;&#20110;&#36825;&#20010;&#29702;&#35299;&#65292;&#25105;&#20204;&#36824;&#21046;&#23450;&#20102;&#21512;&#29702;&#30340;&#26465;&#20214;&#65292;&#20351;&#24471;&#25991;&#26412;&#29983;&#25104;&#31639;&#27861;&#33021;&#22815;&#23398;&#20064;&#29983;&#25104;&#20844;&#24179;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ingrained principles of fairness in a dialogue system's decision-making process and generated responses are crucial for user engagement, satisfaction, and task achievement. Absence of equitable and inclusive principles can hinder the formation of common ground, which in turn negatively impacts the overall performance of the system. For example, misusing pronouns in a user interaction may cause ambiguity about the intended subject. Yet, there is no comprehensive study of equitable text generation in dialogue. Aptly, in this work, we use theories of computational learning to study this problem. We provide formal definitions of equity in text generation, and further, prove formal connections between learning human-likeness and learning equity: algorithms for improving equity ultimately reduce to algorithms for improving human-likeness (on augmented data). With this insight, we also formulate reasonable conditions under which text generation algorithms can learn to generate equitable t
&lt;/p&gt;</description></item><item><title>HistRED&#26159;&#19968;&#20010;&#21382;&#21490;&#25991;&#26723;&#32423;&#21035;&#30340;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#21452;&#35821;&#26631;&#27880;&#65292;&#25903;&#25345;&#19981;&#21516;&#38271;&#24230;&#30340;&#23376;&#25991;&#26412;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#21452;&#35821;&#27169;&#22411;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2307.04285</link><description>&lt;p&gt;
HistRED&#65306;&#19968;&#20010;&#21382;&#21490;&#25991;&#26723;&#32423;&#21035;&#30340;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
HistRED: A Historical Document-Level Relation Extraction Dataset. (arXiv:2307.04285v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04285
&lt;/p&gt;
&lt;p&gt;
HistRED&#26159;&#19968;&#20010;&#21382;&#21490;&#25991;&#26723;&#32423;&#21035;&#30340;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#21452;&#35821;&#26631;&#27880;&#65292;&#25903;&#25345;&#19981;&#21516;&#38271;&#24230;&#30340;&#23376;&#25991;&#26412;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#21452;&#35821;&#27169;&#22411;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#20219;&#21153;&#22312;&#21508;&#20010;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20294;&#22312;&#21382;&#21490;&#32972;&#26223;&#19979;&#30340;&#30740;&#31350;&#36824;&#24456;&#23569;&#12290;&#20026;&#20102;&#25512;&#21160;&#21382;&#21490;RE&#30740;&#31350;&#65292;&#25105;&#20204;&#20174;&#12298;&#24310;&#34892;&#37636;&#12299;&#26500;&#24314;&#20102;HistRED&#12290;&#12298;&#24310;&#34892;&#37636;&#12299;&#26159;&#26368;&#21021;&#29992;&#27721;&#23383;&#20889;&#25104;&#30340;&#35760;&#24405;&#65292;&#21518;&#26469;&#34987;&#32763;&#35793;&#25104;&#38889;&#25991;&#12290;HistRED&#25552;&#20379;&#20102;&#21452;&#35821;&#26631;&#27880;&#65292;&#21487;&#20197;&#23545;&#38889;&#25991;&#21644;&#27721;&#23383;&#25991;&#26412;&#36827;&#34892;RE&#12290;&#27492;&#22806;&#65292;HistRED&#25903;&#25345;&#19981;&#21516;&#38271;&#24230;&#30340;&#33258;&#21253;&#21547;&#23376;&#25991;&#26412;&#65292;&#20174;&#21477;&#23376;&#32423;&#21035;&#21040;&#25991;&#26723;&#32423;&#21035;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#22810;&#26679;&#30340;&#19978;&#19979;&#25991;&#35774;&#32622;&#26469;&#35780;&#20272;&#20854;RE&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#26377;&#29992;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#35821;RE&#27169;&#22411;&#65292;&#21033;&#29992;&#38889;&#25991;&#21644;&#27721;&#23383;&#19978;&#19979;&#25991;&#26469;&#39044;&#27979;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;HistRED&#19978;&#34920;&#29616;&#20248;&#20110;&#21333;&#35821;&#22522;&#32447;&#27169;&#22411;&#65292;&#26174;&#31034;&#20986;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the extensive applications of relation extraction (RE) tasks in various domains, little has been explored in the historical context, which contains promising data across hundreds and thousands of years. To promote the historical RE research, we present HistRED constructed from Yeonhaengnok. Yeonhaengnok is a collection of records originally written in Hanja, the classical Chinese writing, which has later been translated into Korean. HistRED provides bilingual annotations such that RE can be performed on Korean and Hanja texts. In addition, HistRED supports various self-contained subtexts with different lengths, from a sentence level to a document level, supporting diverse context settings for researchers to evaluate the robustness of their RE models. To demonstrate the usefulness of our dataset, we propose a bilingual RE model that leverages both Korean and Hanja contexts to predict relations between entities. Our model outperforms monolingual baselines on HistRED, showing that
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20998;&#35770;&#35777;&#24615;&#20889;&#20316;&#25991;&#31456;&#30340;&#35828;&#26381;&#21147;&#36136;&#37327;&#65292;&#24182;&#35745;&#21010;&#36827;&#19968;&#27493;&#30740;&#31350;&#20854;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#25552;&#20379;&#21487;&#25805;&#20316;&#30340;&#21453;&#39304;&#32473;&#23398;&#29983;&#12290;</title><link>http://arxiv.org/abs/2307.04276</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#30340;&#35770;&#35777;&#24615;&#20889;&#20316;&#25991;&#31456;&#35780;&#20998;&#65306;DeBERT&#25945;&#23398;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Automated Essay Scoring in Argumentative Writing: DeBERTeachingAssistant. (arXiv:2307.04276v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20998;&#35770;&#35777;&#24615;&#20889;&#20316;&#25991;&#31456;&#30340;&#35828;&#26381;&#21147;&#36136;&#37327;&#65292;&#24182;&#35745;&#21010;&#36827;&#19968;&#27493;&#30740;&#31350;&#20854;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#25552;&#20379;&#21487;&#25805;&#20316;&#30340;&#21453;&#39304;&#32473;&#23398;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#30340;&#25991;&#31456;&#35780;&#20998;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#30740;&#31350;&#21644;&#20135;&#19994;&#38382;&#39064;&#36229;&#36807;50&#24180;&#12290;&#30001;&#20110;&#20854;&#26126;&#30830;&#30340;&#25945;&#32946;&#20215;&#20540;&#20197;&#21450;&#23545;&#25945;&#32946;&#32773;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#33410;&#30465;&#26102;&#38388;&#24037;&#20855;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#23427;&#21560;&#24341;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#30340;&#22823;&#37327;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20855;&#36890;&#24120;&#21482;&#20851;&#27880;&#20110;&#26816;&#27979;&#33391;&#22909;&#30340;&#35821;&#27861;&#12289;&#25340;&#20889;&#38169;&#35823;&#21644;&#32452;&#32455;&#36136;&#37327;&#65292;&#20294;&#24448;&#24448;&#22312;&#35780;&#20272;&#20013;&#26410;&#33021;&#23558;&#35828;&#26381;&#21147;&#29305;&#24449;&#32435;&#20837;&#32771;&#34385;&#12290;&#25552;&#20379;&#21487;&#25805;&#20316;&#30340;&#21453;&#39304;&#20197;&#25913;&#36827;&#23398;&#29983;&#35770;&#35777;&#30340;&#21147;&#37327;&#30340;&#36131;&#20219;&#23436;&#20840;&#33853;&#22312;&#25945;&#24072;&#32937;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#20197;&#36229;&#36807;&#20154;&#31867;&#20934;&#30830;&#24230;&#23545;&#35770;&#35777;&#24615;&#20889;&#20316;&#30340;&#35805;&#35821;&#35201;&#32032;&#36827;&#34892;&#27880;&#37322;&#20854;&#35828;&#26381;&#21147;&#36136;&#37327;&#65292;&#24182;&#25193;&#23637;&#20102;&#23545;&#25105;&#20204;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#26410;&#26469;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated Essay scoring has been explored as a research and industry problem for over 50 years. It has drawn a lot of attention from the NLP community because of its clear educational value as a research area that can engender the creation of valuable time-saving tools for educators around the world. Yet, these tools are generally focused on detecting good grammar, spelling mistakes, and organization quality but tend to fail at incorporating persuasiveness features in their final assessment. The responsibility to give actionable feedback to the student to improve the strength of their arguments is left solely on the teacher's shoulders. In this work, we present a transformer-based architecture capable of achieving above-human accuracy in annotating argumentative writing discourse elements for their persuasiveness quality and we expand on planned future work investigating the explainability of our model so that actionable feedback can be offered to the student and thus potentially enabl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#20934;&#30830;&#30340;&#25945;&#24072;&#22238;&#31572;&#20013;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25945;&#24072;-&#23398;&#29983;&#32842;&#22825;&#23460;&#35821;&#26009;&#24211;&#23376;&#38598;&#19978;&#65292;GPT-4&#30340;&#25928;&#26524;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#30740;&#31350;&#36824;&#25351;&#20986;&#26679;&#26412;&#25277;&#26679;&#21644;&#20195;&#34920;&#24615;&#31561;&#25968;&#25454;&#38598;&#29305;&#24449;&#21487;&#33021;&#24433;&#21709;&#29983;&#25104;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.04274</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#20934;&#30830;&#30340;&#25945;&#24072;&#22238;&#31572;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessing the efficacy of large language models in generating accurate teacher responses. (arXiv:2307.04274v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#20934;&#30830;&#30340;&#25945;&#24072;&#22238;&#31572;&#20013;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25945;&#24072;-&#23398;&#29983;&#32842;&#22825;&#23460;&#35821;&#26009;&#24211;&#23376;&#38598;&#19978;&#65292;GPT-4&#30340;&#25928;&#26524;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#30740;&#31350;&#36824;&#25351;&#20986;&#26679;&#26412;&#25277;&#26679;&#21644;&#20195;&#34920;&#24615;&#31561;&#25968;&#25454;&#38598;&#29305;&#24449;&#21487;&#33021;&#24433;&#21709;&#29983;&#25104;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
(Tack&#31561;&#20154;&#65292;2023&#24180;)&#22312;&#31532;18&#23626;&#21019;&#26032;&#20351;&#29992;NLP&#26500;&#24314;&#25945;&#32946;&#24212;&#29992;&#30740;&#35752;&#20250;&#19978;&#32452;&#32455;&#20102;&#20849;&#20139;&#20219;&#21153;&#65292;&#21363;&#25945;&#32946;&#23545;&#35805;&#20013;&#25945;&#24072;&#35821;&#35328;&#30340;&#29983;&#25104;&#12290;&#26412;&#30740;&#31350;&#25353;&#29031;&#20849;&#20139;&#20219;&#21153;&#30340;&#32467;&#26500;&#65292;&#35797;&#22270;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21521;&#23398;&#29983;&#25552;&#20379;&#20449;&#24687;&#24615;&#21644;&#26377;&#30410;&#27934;&#23519;&#21147;&#26041;&#38754;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20174;&#32780;&#27169;&#25311;&#30693;&#35782;&#28170;&#21338;&#30340;&#25945;&#24072;&#30340;&#35282;&#33394;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#20960;&#31181;&#22522;&#20934;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;GPT-4&#65288;&#23569;&#26679;&#26412;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#12289;&#24494;&#35843;&#30340;GPT-2&#21644;&#24494;&#35843;&#30340;DialoGPT&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20248;&#21270;&#25945;&#23398;&#36136;&#37327;&#65292;&#25105;&#20204;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#23545;Flan-T5&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#22312;&#25945;&#24072;-&#23398;&#29983;&#32842;&#22825;&#23460;&#35821;&#26009;&#24211;&#23376;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;BERTScore&#21644;DialogRPT&#24230;&#37327;&#65292;GPT-4&#27604;&#20854;&#20182;&#24494;&#35843;&#27169;&#22411;&#26356;&#20026;&#26377;&#25928;&#12290;&#25105;&#20204;&#20551;&#35774;&#26679;&#26412;&#25277;&#26679;&#21644;&#20195;&#34920;&#24615;&#31561;&#22810;&#20010;&#25968;&#25454;&#38598;&#29305;&#24449;&#21487;&#33021;&#24433;&#21709;&#29983;&#25104;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
(Tack et al., 2023) organized the shared task hosted by the 18th Workshop on Innovative Use of NLP for Building Educational Applications on generation of teacher language in educational dialogues. Following the structure of the shared task, in this study, we attempt to assess the generative abilities of large language models in providing informative and helpful insights to students, thereby simulating the role of a knowledgeable teacher. To this end, we present an extensive evaluation of several benchmarking generative models, including GPT-4 (few-shot, in-context learning), fine-tuned GPT-2, and fine-tuned DialoGPT. Additionally, to optimize for pedagogical quality, we fine-tuned the Flan-T5 model using reinforcement learning. Our experimental findings on the Teacher-Student Chatroom Corpus subset indicate the efficacy of GPT-4 over other fine-tuned models, measured using BERTScore and DialogRPT.  We hypothesize that several dataset characteristics, including sampling, representativen
&lt;/p&gt;</description></item><item><title>ChatGPT&#26159;&#30001;OpenAI&#21019;&#24314;&#30340;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#38761;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#20844;&#20247;&#22823;&#35268;&#27169;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#20114;&#21160;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#20063;&#24341;&#21457;&#20102;&#31867;&#20284;&#25216;&#26415;&#30340;&#30740;&#31350;&#20852;&#36259;&#21644;&#24212;&#29992;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2307.04251</link><description>&lt;p&gt;
ChatGPT&#22312;&#29983;&#25104;&#24335;AI&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#24212;&#29992;&#65306;&#19968;&#20221;&#31616;&#27905;&#30340;&#35843;&#26597;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey. (arXiv:2307.04251v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04251
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#30001;OpenAI&#21019;&#24314;&#30340;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#38761;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#20844;&#20247;&#22823;&#35268;&#27169;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#20114;&#21160;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#20063;&#24341;&#21457;&#20102;&#31867;&#20284;&#25216;&#26415;&#30340;&#30740;&#31350;&#20852;&#36259;&#21644;&#24212;&#29992;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#30001;OpenAI&#21019;&#24314;&#30340;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#32463;&#36807;&#31934;&#24515;&#35757;&#32451;&#24182;&#20351;&#29992;&#20102;&#22823;&#37327;&#25968;&#25454;&#12290;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#38761;&#65292;&#24182;&#25512;&#21160;&#20102;LLM&#33021;&#21147;&#30340;&#36793;&#30028;&#12290;ChatGPT&#22312;&#22823;&#35268;&#27169;&#33539;&#22260;&#20869;&#23454;&#29616;&#20102;&#26222;&#36941;&#20844;&#20247;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#30340;&#20114;&#21160;&#65292;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#23427;&#36824;&#24341;&#21457;&#20102;&#24320;&#21457;&#31867;&#20284;&#25216;&#26415;&#21644;&#30740;&#31350;&#20854;&#24212;&#29992;&#21644;&#24433;&#21709;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23545;ChatGPT&#21450;&#20854;&#28436;&#21270;&#30340;&#24403;&#21069;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#31616;&#26126;&#35843;&#26597;&#12290;&#25105;&#20204;&#21516;&#26102;&#32771;&#34385;&#20102;ChatGPT&#30340;&#29627;&#29827;&#30418;&#21644;&#40657;&#30418;&#35270;&#35282;&#65292;&#21253;&#25324;&#25216;&#26415;&#30340;&#32452;&#25104;&#37096;&#20998;&#21644;&#22522;&#26412;&#35201;&#32032;&#65292;&#20197;&#21450;&#20854;&#24212;&#29992;&#12289;&#24433;&#21709;&#21644;&#24433;&#21709;&#12290;&#29627;&#29827;&#30418;&#26041;&#27861;&#30528;&#37325;&#20110;&#29702;&#35299;&#25216;&#26415;&#30340;&#20869;&#37096;&#36816;&#20316;&#65292;&#32780;&#40657;&#30418;&#26041;&#27861;&#23558;&#20854;&#35270;&#20026;&#19968;&#20010;&#22797;&#26434;&#31995;&#32479;&#65292;&#22240;&#27492;&#30740;&#31350;&#20854;&#36755;&#20837;&#65292;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a large language model (LLM) created by OpenAI that has been carefully trained on a large amount of data. It has revolutionized the field of natural language processing (NLP) and has pushed the boundaries of LLM capabilities. ChatGPT has played a pivotal role in enabling widespread public interaction with generative artificial intelligence (GAI) on a large scale. It has also sparked research interest in developing similar technologies and investigating their applications and implications. In this paper, our primary goal is to provide a concise survey on the current lines of research on ChatGPT and its evolution. We considered both the glass box and black box views of ChatGPT, encompassing the components and foundational elements of the technology, as well as its applications, impacts, and implications. The glass box approach focuses on understanding the inner workings of the technology, and the black box approach embraces it as a complex system, and thus examines its inputs,
&lt;/p&gt;</description></item><item><title>SAS&#35270;&#39057;&#38382;&#31572;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#35299;&#20915;&#20102;&#35270;&#39057;&#38382;&#31572;&#20013;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;</title><link>http://arxiv.org/abs/2307.04192</link><description>&lt;p&gt;
SAS&#35270;&#39057;&#38382;&#31572;&#65306;&#33258;&#36866;&#24212;&#37319;&#26679;&#29992;&#20110;&#39640;&#25928;&#35270;&#39057;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering. (arXiv:2307.04192v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04192
&lt;/p&gt;
&lt;p&gt;
SAS&#35270;&#39057;&#38382;&#31572;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#35299;&#20915;&#20102;&#35270;&#39057;&#38382;&#31572;&#20013;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#38382;&#31572;&#26159;&#35270;&#39057;&#29702;&#35299;&#39046;&#22495;&#30340;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(VLMs)&#37197;&#22791;&#20102;&#35270;&#39057;&#21464;&#25442;&#22120;(Video Transformers)&#65292;&#23454;&#29616;&#20102;&#26102;&#38388;&#24314;&#27169;&#24182;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#65292;&#20294;&#20195;&#20215;&#26159;&#24040;&#22823;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#22240;&#27492;&#22312;&#23454;&#26102;&#24212;&#29992;&#22330;&#26223;&#20013;&#36807;&#20110;&#26114;&#36149;&#12290;&#19968;&#31181;&#32463;&#27982;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#21482;&#23545;&#35270;&#39057;&#30340;&#19968;&#23567;&#37096;&#20998;&#24103;&#36827;&#34892;&#37319;&#26679;&#65292;&#26469;&#20195;&#34920;&#35270;&#39057;&#30340;&#20027;&#35201;&#20869;&#23481;&#65292;&#24182;&#22312;&#36825;&#20123;&#37319;&#26679;&#24103;&#19978;&#35843;&#25972;&#22270;&#20687;-&#25991;&#26412;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#35270;&#39057;&#29702;&#35299;&#27169;&#22411;&#36890;&#24120;&#38543;&#26426;&#37319;&#26679;&#19968;&#32452;&#24103;&#25110;&#29255;&#27573;&#65292;&#32780;&#19981;&#32771;&#34385;&#23427;&#20204;&#30340;&#20869;&#37096;&#20851;&#32852;&#24615;&#21644;&#19982;&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#26080;&#30446;&#26631;&#30340;&#37319;&#26679;&#21487;&#33021;&#20250;&#36951;&#28431;&#21487;&#20197;&#25512;&#23548;&#20986;&#27491;&#30830;&#31572;&#26696;&#30340;&#20851;&#38190;&#24103;&#65292;&#22312;&#37319;&#26679;&#31232;&#30095;&#31243;&#24230;&#22686;&#21152;&#26102;&#65292;&#24773;&#20917;&#20250;&#21464;&#24471;&#26356;&#31967;&#65292;&#32780;&#38543;&#30528;&#35270;&#39057;&#38271;&#24230;&#30340;&#22686;&#21152;&#65292;&#37319;&#26679;&#31232;&#30095;&#31243;&#24230;&#20063;&#20250;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#24103;&#37319;&#26679;&#31574;&#30053;&#65292;&#20998;&#21035;&#26159;
&lt;/p&gt;
&lt;p&gt;
Video question--answering is a fundamental task in the field of video understanding. Although current vision--language models (VLMs) equipped with Video Transformers have enabled temporal modeling and yielded superior results, they are at the cost of huge computational power and thus too expensive to deploy in real-time application scenarios. An economical workaround only samples a small portion of frames to represent the main content of that video and tune an image--text model on these sampled frames. Recent video understanding models usually randomly sample a set of frames or clips, regardless of internal correlations between their visual contents, nor their relevance to the problem. We argue that such kinds of aimless sampling may omit the key frames from which the correct answer can be deduced, and the situation gets worse when the sampling sparsity increases, which always happens as the video lengths increase. To mitigate this issue, we propose two frame sampling strategies, namel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#22312;ASR&#38169;&#35823;&#20462;&#27491;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;ChatGPT&#20026;&#20363;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#38646;-shot&#25110;1-shot&#35774;&#32622;&#19979;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;ChatGPT&#27169;&#22411;&#36827;&#34892;&#38169;&#35823;&#20462;&#27491;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;ASR&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.04172</link><description>&lt;p&gt;
&#22823;&#22411;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#25191;&#34892;ASR&#38169;&#35823;&#20462;&#27491;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Generative Large Language Models Perform ASR Error Correction?. (arXiv:2307.04172v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#22312;ASR&#38169;&#35823;&#20462;&#27491;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;ChatGPT&#20026;&#20363;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#38646;-shot&#25110;1-shot&#35774;&#32622;&#19979;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;ChatGPT&#27169;&#22411;&#36827;&#34892;&#38169;&#35823;&#20462;&#27491;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;ASR&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ASR&#38169;&#35823;&#20462;&#27491;&#22312;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#21518;&#22788;&#29702;&#20013;&#32487;&#32493;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#24213;&#23618;ASR&#31995;&#32479;&#30340;&#35299;&#30721;&#32467;&#26524;&#21644;&#21442;&#32771;&#25991;&#26412;&#36827;&#34892;&#26377;&#30417;&#30563;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#38750;&#24120;&#32791;&#36153;&#36164;&#28304;&#65292;&#32780;&#19988;&#38656;&#35201;&#22312;&#20999;&#25442;&#24213;&#23618;ASR&#27169;&#22411;&#26102;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#20351;&#24471;&#23427;&#20204;&#33021;&#22815;&#20197;&#38646;-shot&#30340;&#26041;&#24335;&#25191;&#34892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#26412;&#25991;&#20197;ChatGPT&#20026;&#20363;&#65292;&#30740;&#31350;&#20854;&#22312;&#38646;-shot&#25110;1-shot&#35774;&#32622;&#19979;&#36827;&#34892;ASR&#38169;&#35823;&#20462;&#27491;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;ASR N-best&#21015;&#34920;&#20316;&#20026;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#24182;&#25552;&#20986;&#20102;&#26080;&#32422;&#26463;&#38169;&#35823;&#20462;&#27491;&#21644;N-best&#32422;&#26463;&#38169;&#35823;&#20462;&#27491;&#30340;&#26041;&#27861;&#12290;&#22312;Conformer-Transducer&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#30340;Whisper&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#24378;&#22823;&#30340;ChatGPT&#27169;&#22411;&#36827;&#34892;&#38169;&#35823;&#20462;&#27491;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;ASR&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
ASR error correction continues to serve as an important part of post-processing for speech recognition systems. Traditionally, these models are trained with supervised training using the decoding results of the underlying ASR system and the reference text. This approach is computationally intensive and the model needs to be re-trained when switching the underlying ASR model. Recent years have seen the development of large language models and their ability to perform natural language processing tasks in a zero-shot manner. In this paper, we take ChatGPT as an example to examine its ability to perform ASR error correction in the zero-shot or 1-shot settings. We use the ASR N-best list as model input and propose unconstrained error correction and N-best constrained error correction methods. Results on a Conformer-Transducer model and the pre-trained Whisper model show that we can largely improve the ASR system performance with error correction using the powerful ChatGPT model.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20174;Reddit&#30340;r/Dreams&#23376;&#29256;&#22359;&#20013;&#21457;&#29616;&#26790;&#22659;&#20869;&#23481;&#12290;&#36890;&#36807;&#27979;&#35797;&#25968;&#25454;&#65292;&#24471;&#21040;&#20102;&#21253;&#21547;217&#20010;&#20027;&#39064;&#30340;&#26368;&#24191;&#27867;&#30340;&#26790;&#22659;&#20027;&#39064;&#38598;&#21512;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#19981;&#21463;&#35268;&#27169;&#38480;&#21046;&#65292;&#24182;&#33021;&#21457;&#29616;&#19981;&#21516;&#26790;&#22659;&#31867;&#22411;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2307.04167</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#28151;&#21512;&#26041;&#27861;&#20174;Reddit&#21457;&#29616;&#26790;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Dream Content Discovery from Reddit with an Unsupervised Mixed-Method Approach. (arXiv:2307.04167v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20174;Reddit&#30340;r/Dreams&#23376;&#29256;&#22359;&#20013;&#21457;&#29616;&#26790;&#22659;&#20869;&#23481;&#12290;&#36890;&#36807;&#27979;&#35797;&#25968;&#25454;&#65292;&#24471;&#21040;&#20102;&#21253;&#21547;217&#20010;&#20027;&#39064;&#30340;&#26368;&#24191;&#27867;&#30340;&#26790;&#22659;&#20027;&#39064;&#38598;&#21512;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#19981;&#21463;&#35268;&#27169;&#38480;&#21046;&#65292;&#24182;&#33021;&#21457;&#29616;&#19981;&#21516;&#26790;&#22659;&#31867;&#22411;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26790;&#26159;&#20154;&#31867;&#20307;&#39564;&#30340;&#19968;&#20010;&#22522;&#26412;&#20294;&#19981;&#23436;&#20840;&#29702;&#35299;&#30340;&#37096;&#20998;&#65292;&#21487;&#20197;&#25581;&#31034;&#25105;&#20204;&#30340;&#24605;&#32500;&#27169;&#24335;&#12290;&#20256;&#32479;&#30340;&#26790;&#22659;&#20998;&#26512;&#26041;&#27861;&#34429;&#28982;&#21463;&#27426;&#36814;&#19988;&#26377;130&#22810;&#31181;&#29420;&#29305;&#30340;&#35780;&#20998;&#31995;&#32479;&#65292;&#20294;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20256;&#32479;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#22238;&#39038;&#24615;&#35843;&#26597;&#25110;&#23454;&#39564;&#23460;&#30740;&#31350;&#65292;&#38590;&#20197;&#22823;&#35268;&#27169;&#24212;&#29992;&#25110;&#23637;&#31034;&#19981;&#21516;&#26790;&#22659;&#20027;&#39064;&#20043;&#38388;&#30340;&#37325;&#35201;&#24615;&#21644;&#32852;&#31995;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26469;&#35782;&#21035;&#33258;&#30001;&#24418;&#24335;&#30340;&#26790;&#22659;&#25253;&#21578;&#20013;&#30340;&#20027;&#39064;&#12290;&#25105;&#20204;&#22312;Reddit&#30340;r/Dreams&#23376;&#29256;&#22359;&#19978;&#23545;44,213&#20221;&#26790;&#22659;&#25253;&#21578;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21457;&#29616;&#20102;217&#20010;&#20027;&#39064;&#65292;&#20998;&#20026;22&#20010;&#26356;&#22823;&#30340;&#20027;&#39064;&#65306;&#21040;&#30446;&#21069;&#20026;&#27490;&#26368;&#20840;&#38754;&#30340;&#26790;&#22659;&#20027;&#39064;&#25910;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;Hall&#21644;van de Castle&#35780;&#20998;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20027;&#39064;&#12290;&#19982;&#20256;&#32479;&#35780;&#20998;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21457;&#29616;&#19981;&#21516;&#31867;&#22411;&#26790;&#22659;&#65288;&#22914;&#22121;&#26790;&#25110;&#21453;&#22797;&#20986;&#29616;&#30340;&#26790;&#22659;&#65289;&#20013;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dreaming is a fundamental but not fully understood part of human experience that can shed light on our thought patterns. Traditional dream analysis practices, while popular and aided by over 130 unique scales and rating systems, have limitations. Mostly based on retrospective surveys or lab studies, they struggle to be applied on a large scale or to show the importance and connections between different dream themes. To overcome these issues, we developed a new, data-driven mixed-method approach for identifying topics in free-form dream reports through natural language processing. We tested this method on 44,213 dream reports from Reddit's r/Dreams subreddit, where we found 217 topics, grouped into 22 larger themes: the most extensive collection of dream topics to date. We validated our topics by comparing it to the widely-used Hall and van de Castle scale. Going beyond traditional scales, our method can find unique patterns in different dream types (like nightmares or recurring dreams)
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23454;&#29616;&#36328;&#35821;&#35328;&#35821;&#35843;&#36716;&#25442;&#20197;&#29992;&#20110;&#23545;&#35805;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25910;&#38598;&#21452;&#35821;&#35762;&#35805;&#32773;&#29992;&#21478;&#19968;&#31181;&#35821;&#35328;&#37325;&#26032;&#28436;&#32462;&#23545;&#35805;&#30340;&#35805;&#35821;&#65292;&#24182;&#37327;&#21270;&#20102;&#36328;&#35821;&#35328;&#38901;&#24459;&#24046;&#24322;&#65292;&#35780;&#20272;&#20102;&#22522;&#20934;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20026;&#26410;&#26469;&#30340;&#36328;&#35821;&#35328;&#38901;&#24459;&#30740;&#31350;&#21644;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#35774;&#35745;&#25552;&#20379;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2307.04123</link><description>&lt;p&gt;
&#23454;&#29616;&#36328;&#35821;&#35328;&#35821;&#35843;&#36716;&#25442;&#20197;&#29992;&#20110;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Towards cross-language prosody transfer for dialog. (arXiv:2307.04123v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23454;&#29616;&#36328;&#35821;&#35328;&#35821;&#35843;&#36716;&#25442;&#20197;&#29992;&#20110;&#23545;&#35805;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25910;&#38598;&#21452;&#35821;&#35762;&#35805;&#32773;&#29992;&#21478;&#19968;&#31181;&#35821;&#35328;&#37325;&#26032;&#28436;&#32462;&#23545;&#35805;&#30340;&#35805;&#35821;&#65292;&#24182;&#37327;&#21270;&#20102;&#36328;&#35821;&#35328;&#38901;&#24459;&#24046;&#24322;&#65292;&#35780;&#20272;&#20102;&#22522;&#20934;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20026;&#26410;&#26469;&#30340;&#36328;&#35821;&#35328;&#38901;&#24459;&#30740;&#31350;&#21644;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#35774;&#35745;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#22312;&#23545;&#35805;&#30446;&#30340;&#19979;&#30340;&#24212;&#29992;&#25903;&#25345;&#19981;&#36275;&#12290;&#29305;&#21035;&#26159;&#30001;&#20110;&#35821;&#35843;&#36716;&#25442;&#19981;&#24403;&#65292;&#35762;&#35805;&#32773;&#24847;&#22270;&#21644;&#31435;&#22330;&#30340;&#32454;&#24494;&#24046;&#21035;&#21487;&#33021;&#20250;&#20002;&#22833;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#25454;&#25910;&#38598;&#21327;&#35758;&#65292;&#20351;&#21452;&#35821;&#35762;&#35805;&#32773;&#29992;&#21478;&#19968;&#31181;&#35821;&#35328;&#37325;&#26032;&#28436;&#32462;&#26089;&#21069;&#23545;&#35805;&#30340;&#35805;&#35821;&#65292;&#24182;&#21033;&#29992;&#27492;&#26041;&#27861;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;1871&#20010;&#37197;&#23545;&#35805;&#35821;&#30340;&#33521;&#35821;-&#35199;&#29677;&#29273;&#35821;&#35821;&#26009;&#24211;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20110;&#27431;&#27663;&#36317;&#31163;&#30340;&#38901;&#24459;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#38901;&#24459;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#38901;&#24459;&#24046;&#24322;&#65292;&#37327;&#21270;&#20102;&#19977;&#20010;&#31616;&#21333;&#22522;&#20934;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#30830;&#23450;&#20102;&#38656;&#35201;&#26356;&#24378;&#22823;&#24314;&#27169;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#23558;&#26377;&#21161;&#20110;&#26410;&#26469;&#30340;&#36328;&#35821;&#35328;&#38901;&#24459;&#30740;&#31350;&#21644;&#35774;&#35745;&#33021;&#22815;&#26377;&#25928;&#36827;&#34892;&#38901;&#24459;&#36716;&#25442;&#30340;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech-to-speech translation systems today do not adequately support use for dialog purposes. In particular, nuances of speaker intent and stance can be lost due to improper prosody transfer. We present an exploration of what needs to be done to overcome this. First, we developed a data collection protocol in which bilingual speakers re-enact utterances from an earlier conversation in their other language, and used this to collect an English-Spanish corpus, so far comprising 1871 matched utterance pairs. Second, we developed a simple prosodic dissimilarity metric based on Euclidean distance over a broad set of prosodic features. We then used these to investigate cross-language prosodic differences, measure the likely utility of three simple baseline models, and identify phenomena which will require more powerful modeling. Our findings should inform future research on cross-language prosody and the design of speech-to-speech translation systems capable of effective prosody transfer.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#23569;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#65292;&#20351;&#29992;&#25991;&#26412;&#23884;&#20837;&#21644;&#24230;&#37327;&#27169;&#22359;&#26469;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#24615;&#33021;&#21644;&#21487;&#36801;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.04114</link><description>&lt;p&gt;
&#12298;FILM:&#22914;&#20309;&#35753;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20174;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#21463;&#30410;?&#12299;
&lt;/p&gt;
&lt;p&gt;
FILM: How can Few-Shot Image Classification Benefit from Pre-Trained Language Models?. (arXiv:2307.04114v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#23569;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#65292;&#20351;&#29992;&#25991;&#26412;&#23884;&#20837;&#21644;&#24230;&#37327;&#27169;&#22359;&#26469;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#24615;&#33021;&#21644;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#26088;&#22312;&#35757;&#32451;&#33021;&#22815;&#21033;&#29992;&#23569;&#37327;&#26679;&#26412;&#25512;&#24191;&#21040;&#26032;&#31867;&#21035;&#30340;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#19968;&#31995;&#21015;&#24037;&#20316;&#25552;&#20986;&#21033;&#29992;&#21487;&#35775;&#38382;&#30340;&#31867;&#21035;&#21517;&#31216;&#35821;&#20041;&#20449;&#24687;&#26469;&#22686;&#24378;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#25913;&#36827;&#26631;&#20934;&#23569;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#35270;&#35273;&#21407;&#22411;&#21644;&#29305;&#24449;&#25552;&#21462;&#22120;&#31561;&#29616;&#26377;&#27169;&#22359;&#65292;&#38480;&#21046;&#20102;&#35821;&#20041;&#20449;&#24687;&#30340;&#20805;&#20998;&#21033;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#23569;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#12290;&#20026;&#20102;&#35299;&#20915;&#35270;&#35273;&#29305;&#24449;&#21644;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#23545;&#40784;&#25361;&#25112;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#26694;&#26550;&#30340;&#25991;&#26412;&#20998;&#25903;&#65292;&#24182;&#24341;&#20837;&#20102;&#24230;&#37327;&#27169;&#22359;&#26469;&#25512;&#24191;&#20313;&#24358;&#30456;&#20284;&#24230;&#12290;&#20026;&#20102;&#26356;&#22909;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#25105;&#20204;&#35753;&#24230;&#37327;&#27169;&#22359;&#36866;&#24212;&#19981;&#21516;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#65292;&#24182;&#37319;&#29992;MAML&#36827;&#34892;&#21452;&#23618;&#20248;&#21270;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning aims to train models that can be generalized to novel classes with only a few samples. Recently, a line of works are proposed to enhance few-shot learning with accessible semantic information from class names. However, these works focus on improving existing modules such as visual prototypes and feature extractors of the standard few-shot learning framework. This limits the full potential use of semantic information. In this paper, we propose a novel few-shot learning framework that uses pre-trained language models based on contrastive learning. To address the challenge of alignment between visual features and textual embeddings obtained from text-based pre-trained language model, we carefully design the textual branch of our framework and introduce a metric module to generalize the cosine similarity. For better transferability, we let the metric module adapt to different few-shot tasks and adopt MAML to train the model via bi-level optimization. Moreover, we conduct 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#35821;&#20041;&#35299;&#26512;&#26041;&#27861;&#65292;&#21033;&#29992;&#26368;&#20248;&#36755;&#36816;&#26469;&#26174;&#24335;&#22320;&#26368;&#23567;&#21270;&#27010;&#29575;&#28508;&#21464;&#37327;&#20043;&#38388;&#30340;&#36328;&#35821;&#20041;&#24046;&#24322;&#12290;&#36890;&#36807;&#36825;&#31181;&#30452;&#25509;&#24341;&#23548;&#65292;&#21487;&#20197;&#20351;&#29992;&#36739;&#23569;&#30340;&#26679;&#20363;&#21644;&#35757;&#32451;&#26469;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#35299;&#26512;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#23569;&#26679;&#26412;&#36328;&#35821;&#35328;&#21046;&#24230;&#19979;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#22312;&#27809;&#26377;&#24182;&#34892;&#36755;&#20837;&#32763;&#35793;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#19988;&#26356;&#22909;&#22320;&#25429;&#25417;&#36328;&#35821;&#35328;&#32467;&#26500;&#20197;&#25913;&#21892;&#35821;&#20041;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.04096</link><description>&lt;p&gt;
&#36328;&#35821;&#20041;&#35299;&#26512;&#30340;&#26368;&#20248;&#36755;&#36816;&#21518;&#39564;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport Posterior Alignment for Cross-lingual Semantic Parsing. (arXiv:2307.04096v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04096
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#35821;&#20041;&#35299;&#26512;&#26041;&#27861;&#65292;&#21033;&#29992;&#26368;&#20248;&#36755;&#36816;&#26469;&#26174;&#24335;&#22320;&#26368;&#23567;&#21270;&#27010;&#29575;&#28508;&#21464;&#37327;&#20043;&#38388;&#30340;&#36328;&#35821;&#20041;&#24046;&#24322;&#12290;&#36890;&#36807;&#36825;&#31181;&#30452;&#25509;&#24341;&#23548;&#65292;&#21487;&#20197;&#20351;&#29992;&#36739;&#23569;&#30340;&#26679;&#20363;&#21644;&#35757;&#32451;&#26469;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#35299;&#26512;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#23569;&#26679;&#26412;&#36328;&#35821;&#35328;&#21046;&#24230;&#19979;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#22312;&#27809;&#26377;&#24182;&#34892;&#36755;&#20837;&#32763;&#35793;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#19988;&#26356;&#22909;&#22320;&#25429;&#25417;&#36328;&#35821;&#35328;&#32467;&#26500;&#20197;&#25913;&#21892;&#35821;&#20041;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#20041;&#35299;&#26512;&#36890;&#36807;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#65288;&#20363;&#22914;&#33521;&#35821;&#65289;&#36716;&#31227;&#35299;&#26512;&#33021;&#21147;&#21040;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#32771;&#34385;&#20102;&#38134;&#26631;&#20934;&#25968;&#25454;&#22686;&#24378;&#25110;&#38646;&#26679;&#26412;&#26041;&#27861;&#65292;&#28982;&#32780;&#21033;&#29992;&#23569;&#26679;&#26412;&#40644;&#37329;&#25968;&#25454;&#30340;&#26041;&#27861;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26174;&#24335;&#22320;&#26368;&#23567;&#21270;&#27010;&#29575;&#28508;&#21464;&#37327;&#20043;&#38388;&#30340;&#36328;&#35821;&#20041;&#24046;&#24322;&#26469;&#36827;&#34892;&#36328;&#35821;&#20041;&#35299;&#26512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#30452;&#25509;&#24341;&#23548;&#22914;&#20309;&#22312;&#20351;&#29992;&#36739;&#23569;&#30340;&#26679;&#20363;&#21644;&#36739;&#23569;&#30340;&#35757;&#32451;&#19979;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#35299;&#26512;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;MTOP&#21644;MultiATIS++SQL&#65289;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#22312;&#23569;&#26679;&#26412;&#36328;&#35821;&#35328;&#21046;&#24230;&#19979;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28040;&#34701;&#30740;&#31350;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21363;&#20351;&#22312;&#27809;&#26377;&#24182;&#34892;&#36755;&#20837;&#32763;&#35793;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#25552;&#39640;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#26356;&#22909;&#22320;&#25429;&#25417;&#36328;&#35821;&#35328;&#32467;&#26500;&#22312;&#28508;&#31354;&#38388;&#20013;&#20197;&#25913;&#21892;&#35821;&#20041;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual semantic parsing transfers parsing capability from a high-resource language (e.g., English) to low-resource languages with scarce training data. Previous work has primarily considered silver-standard data augmentation or zero-shot methods, however, exploiting few-shot gold data is comparatively unexplored. We propose a new approach to cross-lingual semantic parsing by explicitly minimizing cross-lingual divergence between probabilistic latent variables using Optimal Transport. We demonstrate how this direct guidance improves parsing from natural languages using fewer examples and less training. We evaluate our method on two datasets, MTOP and MultiATIS++SQL, establishing state-of-the-art results under a few-shot cross-lingual regime. Ablation studies further reveal that our method improves performance even without parallel input translations. In addition, we show that our model better captures cross-lingual structure in the latent space to improve semantic representation 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#30693;&#35782;&#22270;&#33258;&#21160;&#21019;&#24314;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20105;&#35770;&#30340;&#35821;&#20041;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38480;&#21046;&#26368;&#30701;&#36335;&#24452;&#36941;&#21382;&#65292;&#26377;&#25928;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#36777;&#35770;&#26696;&#20363;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#32654;&#22269;&#31454;&#36187;&#36777;&#35770;&#20013;&#65292;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25913;&#36827;&#20102;&#24050;&#26377;&#25968;&#25454;&#38598;DebateSum&#65292;&#24182;&#36129;&#29486;&#20102;&#26032;&#30340;&#20363;&#23376;&#21644;&#26377;&#29992;&#30340;&#20803;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;txtai&#35821;&#20041;&#25628;&#32034;&#21644;&#30693;&#35782;&#22270;&#24037;&#20855;&#38142;&#65292;&#21019;&#24314;&#21644;&#36129;&#29486;&#20102;9&#20010;&#35821;&#20041;&#30693;&#35782;&#22270;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35780;&#20272;&#26041;&#27861;&#26469;&#30830;&#23450;&#21738;&#20010;&#30693;&#35782;&#22270;&#26356;&#36866;&#21512;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.04090</link><description>&lt;p&gt;
DebateKG: &#29992;&#35821;&#20041;&#30693;&#35782;&#22270;&#33258;&#21160;&#21019;&#24314;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge Graphs. (arXiv:2307.04090v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#30693;&#35782;&#22270;&#33258;&#21160;&#21019;&#24314;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20105;&#35770;&#30340;&#35821;&#20041;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38480;&#21046;&#26368;&#30701;&#36335;&#24452;&#36941;&#21382;&#65292;&#26377;&#25928;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#36777;&#35770;&#26696;&#20363;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#32654;&#22269;&#31454;&#36187;&#36777;&#35770;&#20013;&#65292;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25913;&#36827;&#20102;&#24050;&#26377;&#25968;&#25454;&#38598;DebateSum&#65292;&#24182;&#36129;&#29486;&#20102;&#26032;&#30340;&#20363;&#23376;&#21644;&#26377;&#29992;&#30340;&#20803;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;txtai&#35821;&#20041;&#25628;&#32034;&#21644;&#30693;&#35782;&#22270;&#24037;&#20855;&#38142;&#65292;&#21019;&#24314;&#21644;&#36129;&#29486;&#20102;9&#20010;&#35821;&#20041;&#30693;&#35782;&#22270;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35780;&#20272;&#26041;&#27861;&#26469;&#30830;&#23450;&#21738;&#20010;&#30693;&#35782;&#22270;&#26356;&#36866;&#21512;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30456;&#20851;&#24037;&#20316;&#34920;&#26126;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#22312;&#35299;&#20915;&#31454;&#36187;&#36777;&#35770;&#20013;&#30340;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#24615;&#12290;&#31454;&#36187;&#36777;&#35770;&#20013;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#26159;&#36777;&#25163;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#36777;&#35770;&#26696;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#38480;&#21046;&#26368;&#30701;&#36335;&#24452;&#36941;&#21382;&#22312;&#20105;&#35770;&#30340;&#35821;&#20041;&#30693;&#35782;&#22270;&#19978;&#26500;&#24314;&#26377;&#25928;&#30340;&#36777;&#35770;&#26696;&#20363;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21517;&#20026;DebateSum&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30740;&#31350;&#20102;&#36825;&#31181;&#28508;&#21147;&#65292;&#35813;&#25968;&#25454;&#38598;&#38024;&#23545;&#30340;&#26159;&#19968;&#31181;&#21517;&#20026;&#25919;&#31574;&#36777;&#35770;&#30340;&#32654;&#22269;&#31454;&#36187;&#36777;&#35770;&#31867;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#21521;&#25968;&#25454;&#38598;&#20013;&#24341;&#20837;53180&#20010;&#26032;&#30340;&#20363;&#23376;&#65292;&#24182;&#20026;&#27599;&#20010;&#20363;&#23376;&#25552;&#20379;&#36827;&#19968;&#27493;&#26377;&#29992;&#30340;&#20803;&#25968;&#25454;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;DebateSum&#12290;&#25105;&#20204;&#21033;&#29992;txtai&#35821;&#20041;&#25628;&#32034;&#21644;&#30693;&#35782;&#22270;&#24037;&#20855;&#38142;&#22522;&#20110;&#36825;&#20010;&#25968;&#25454;&#38598;&#20135;&#29983;&#24182;&#36129;&#29486;&#20102;9&#20010;&#35821;&#20041;&#30693;&#35782;&#22270;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#30830;&#23450;&#22312;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#29983;&#25104;&#30340;&#32972;&#26223;&#19979;&#21738;&#20010;&#30693;&#35782;&#22270;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work within the Argument Mining community has shown the applicability of Natural Language Processing systems for solving problems found within competitive debate. One of the most important tasks within competitive debate is for debaters to create high quality debate cases. We show that effective debate cases can be constructed using constrained shortest path traversals on Argumentative Semantic Knowledge Graphs. We study this potential in the context of a type of American Competitive Debate, called Policy Debate, which already has a large scale dataset targeting it called DebateSum. We significantly improve upon DebateSum by introducing 53180 new examples, as well as further useful metadata for every example, to the dataset. We leverage the txtai semantic search and knowledge graph toolchain to produce and contribute 9 semantic knowledge graphs built on this dataset. We create a unique method for evaluating which knowledge graphs are better in the context of producing policy deb
&lt;/p&gt;</description></item><item><title>&#21452;&#21521;&#27880;&#24847;&#21147;&#27169;&#22411;&#20855;&#26377;&#28151;&#21512;&#19987;&#23478;&#26435;&#37325;&#65292;&#31867;&#20284;&#20110;&#36830;&#32493;&#35789;&#34955;&#27169;&#22411;&#65288;CBOW&#65289;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#23427;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.04057</link><description>&lt;p&gt;
&#21452;&#21521;&#27880;&#24847;&#21147;&#20316;&#20026;&#36830;&#32493;&#35789;&#19987;&#23478;&#30340;&#28151;&#21512;&#29289;
&lt;/p&gt;
&lt;p&gt;
Bidirectional Attention as a Mixture of Continuous Word Experts. (arXiv:2307.04057v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04057
&lt;/p&gt;
&lt;p&gt;
&#21452;&#21521;&#27880;&#24847;&#21147;&#27169;&#22411;&#20855;&#26377;&#28151;&#21512;&#19987;&#23478;&#26435;&#37325;&#65292;&#31867;&#20284;&#20110;&#36830;&#32493;&#35789;&#34955;&#27169;&#22411;&#65288;CBOW&#65289;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#23427;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#21521;&#27880;&#24847;&#21147;&#30001;&#20301;&#32622;&#32534;&#30721;&#21644;&#23631;&#34109;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#30446;&#26631;&#32452;&#25104;&#30340;&#33258;&#27880;&#24847;&#21147;&#26500;&#25104;&#65292;&#24050;&#25104;&#20026;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;&#23613;&#31649;&#23427;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;&#23427;&#30340;&#32479;&#35745;&#22522;&#30784;&#65306;&#21452;&#21521;&#27880;&#24847;&#21147;&#38544;&#21547;&#22320;&#25311;&#21512;&#20102;&#20160;&#20040;&#32479;&#35745;&#27169;&#22411;&#65311;&#23427;&#19982;&#38750;&#27880;&#24847;&#26426;&#21046;&#30340;&#20808;&#39537;&#26377;&#20309;&#19981;&#21516;&#65311;&#26412;&#25991;&#25506;&#35752;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#37325;&#26032;&#21442;&#25968;&#21270;&#21518;&#65292;&#25311;&#21512;&#21333;&#23618;&#21333;&#22836;&#21452;&#21521;&#27880;&#24847;&#21147;&#31561;&#20110;&#25311;&#21512;&#20855;&#26377;&#28151;&#21512;&#19987;&#23478;&#26435;&#37325;&#30340;&#36830;&#32493;&#35789;&#34955;&#65288;CBOW&#65289;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20855;&#26377;&#22810;&#20010;&#22836;&#21644;&#22810;&#20010;&#23618;&#30340;&#21452;&#21521;&#27880;&#24847;&#21147;&#31561;&#20215;&#20110;&#22534;&#21472;&#30340;MoEs&#21644;MoEs&#30340;&#28151;&#21512;&#12290;&#36825;&#20010;&#32479;&#35745;&#35266;&#28857;&#25581;&#31034;&#20102;MoE&#22312;&#21452;&#21521;&#27880;&#24847;&#21147;&#20013;&#30340;&#29420;&#29305;&#29992;&#36884;&#65292;&#36825;&#19982;&#20854;&#22312;&#22788;&#29702;&#24322;&#26500;&#24615;&#26041;&#38754;&#30340;&#23454;&#38469;&#26377;&#25928;&#24615;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bidirectional attention $\unicode{x2013}$ composed of self-attention with positional encodings and the masked language model (MLM) objective $\unicode{x2013}$ has emerged as a key component of modern large language models (LLMs). Despite its empirical success, few studies have examined its statistical underpinnings: What statistical model is bidirectional attention implicitly fitting? What sets it apart from its non-attention predecessors? We explore these questions in this paper. The key observation is that fitting a single-layer single-head bidirectional attention, upon reparameterization, is equivalent to fitting a continuous bag of words (CBOW) model with mixture-of-experts (MoE) weights. Further, bidirectional attention with multiple heads and multiple layers is equivalent to stacked MoEs and a mixture of MoEs, respectively. This statistical viewpoint reveals the distinct use of MoE in bidirectional attention, which aligns with its practical effectiveness in handling heterogeneous
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#26032;&#21152;&#22369;&#30340;&#22312;&#32447;&#29615;&#22659;&#20013;&#22914;&#20309;&#35299;&#35835;&#29238;&#20146;&#36523;&#20221;&#65292;&#21457;&#29616;&#29238;&#20146;&#24182;&#27809;&#26377;&#34987;&#35270;&#20026;&#23478;&#24237;&#21333;&#20301;&#30340;&#26680;&#24515;&#12290;</title><link>http://arxiv.org/abs/2307.04053</link><description>&lt;p&gt;
&#26032;&#21152;&#22369;&#30340;&#22312;&#32447;&#29615;&#22659;&#20013;&#22914;&#20309;&#35299;&#35835;&#29238;&#20146;&#36523;&#20221;&#65311;
&lt;/p&gt;
&lt;p&gt;
How is Fatherhood Framed Online in Singapore?. (arXiv:2307.04053v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04053
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#26032;&#21152;&#22369;&#30340;&#22312;&#32447;&#29615;&#22659;&#20013;&#22914;&#20309;&#35299;&#35835;&#29238;&#20146;&#36523;&#20221;&#65292;&#21457;&#29616;&#29238;&#20146;&#24182;&#27809;&#26377;&#34987;&#35270;&#20026;&#23478;&#24237;&#21333;&#20301;&#30340;&#26680;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#21152;&#22369;&#20851;&#20110;&#29238;&#20146;&#36523;&#20221;&#30340;&#35752;&#35770;&#28608;&#22686;&#65292;&#34920;&#26126;&#26377;&#24517;&#35201;&#25506;&#32034;&#29238;&#20146;&#36523;&#20221;&#30340;&#26694;&#26550;&#65292;&#20197;&#20419;&#36827;&#26032;&#21152;&#22369;&#30340;&#29238;&#20146;&#25919;&#31574;&#21046;&#23450;&#12290;&#20840;&#38754;&#21644;&#26377;&#25928;&#30340;&#29238;&#20146;&#25919;&#31574;&#21487;&#20197;&#20943;&#23569;&#23545;&#25104;&#20026;&#29238;&#27597;&#30340;&#36127;&#38754;&#35780;&#20215;&#21644;&#24656;&#24807;&#65292;&#23545;&#25913;&#21892;&#22269;&#23478;&#30340;&#20986;&#29983;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;15,705&#31687;&#25991;&#31456;&#21644;56,221&#20010;&#24086;&#23376;&#65292;&#30740;&#31350;&#20102;&#22312;&#26032;&#21152;&#22369;&#30340;&#21508;&#31181;&#22312;&#32447;&#24179;&#21488;&#65288;&#26032;&#38395;&#23186;&#20307;&#12289;&#32946;&#20799;&#35770;&#22363;&#12289;Twitter&#65289;&#19978;&#20851;&#20110;&#29238;&#20146;&#36523;&#20221;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26469;&#29702;&#35299;&#36825;&#20123;&#24046;&#24322;&#12290;&#23613;&#31649;&#22312;&#26032;&#21152;&#22369;&#30340;&#22312;&#32447;&#29615;&#22659;&#20013;&#26377;&#22810;&#31181;&#23545;&#29238;&#20146;&#36523;&#20221;&#30340;&#35299;&#35835;&#65292;&#20294;&#20284;&#20046;&#29238;&#20146;&#24182;&#27809;&#26377;&#34987;&#35270;&#20026;&#26032;&#21152;&#22369;&#23478;&#24237;&#21333;&#20301;&#30340;&#26680;&#24515;&#12290;&#25105;&#20204;&#24037;&#20316;&#30340;&#19968;&#20010;&#20248;&#21183;&#26159;&#24212;&#29992;&#20102;&#19981;&#21516;&#30340;&#25216;&#26415;&#30456;&#20114;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of discussion about fatherhood in Singapore attests to its significance, indicating the need for an exploration of how fatherhood is framed, aiding policy-making around fatherhood in Singapore. Sound and holistic policy around fatherhood in Singapore may reduce stigma and apprehension around being a parent, critical to improving the nations flagging birth rate. We analyzed 15,705 articles and 56,221 posts to study how fatherhood is framed in Singapore across a range of online platforms (news outlets, parenting forums, Twitter). We used NLP techniques to understand these differences. While fatherhood was framed in a range of ways on the Singaporean online environment, it did not seem that fathers were framed as central to the Singaporean family unit. A strength of our work is how the different techniques we have applied validate each other.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#36328;&#35821;&#35328;&#25688;&#35201;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20855;&#26377;&#25913;&#36827;&#27880;&#37322;&#30340;&#22522;&#20934;&#12290;&#36890;&#36807;&#32771;&#34385;&#28304;&#36755;&#20837;&#19978;&#19979;&#25991;&#65292;&#35813;&#22522;&#20934;&#26356;&#21487;&#38752;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#26041;&#27861;&#65292;&#22312;ConvSumX&#19978;&#34920;&#29616;&#20986;&#36739;&#24378;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.04018</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#36328;&#35821;&#35328;&#25688;&#35201;&#65306;&#19968;&#20010;&#22522;&#20110;&#35821;&#26009;&#24211;&#30340;&#30740;&#31350;&#21644;&#19968;&#20010;&#26032;&#30340;&#20855;&#26377;&#25913;&#36827;&#27880;&#37322;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Revisiting Cross-Lingual Summarization: A Corpus-based Study and A New Benchmark with Improved Annotation. (arXiv:2307.04018v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#36328;&#35821;&#35328;&#25688;&#35201;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20855;&#26377;&#25913;&#36827;&#27880;&#37322;&#30340;&#22522;&#20934;&#12290;&#36890;&#36807;&#32771;&#34385;&#28304;&#36755;&#20837;&#19978;&#19979;&#25991;&#65292;&#35813;&#22522;&#20934;&#26356;&#21487;&#38752;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#26041;&#27861;&#65292;&#22312;ConvSumX&#19978;&#34920;&#29616;&#20986;&#36739;&#24378;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#36328;&#35821;&#35328;&#25688;&#35201;&#65288;CLS&#65289;&#24037;&#20316;&#36890;&#36807;&#31616;&#21333;&#30452;&#25509;&#22320;&#23558;&#39044;&#20808;&#27880;&#37322;&#30340;&#25688;&#35201;&#20174;&#19968;&#31181;&#35821;&#35328;&#32763;&#35793;&#20026;&#21478;&#19968;&#31181;&#35821;&#35328;&#26469;&#26500;&#24314;CLS&#35821;&#26009;&#24211;&#65292;&#36825;&#21487;&#33021;&#21253;&#21547;&#25688;&#35201;&#21644;&#32763;&#35793;&#36807;&#31243;&#20013;&#30340;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ConvSumX&#65292;&#19968;&#20010;&#36328;&#35821;&#35328;&#23545;&#35805;&#25688;&#35201;&#22522;&#20934;&#65292;&#36890;&#36807;&#19968;&#20010;&#26032;&#30340;&#27880;&#37322;&#26550;&#26500;&#65292;&#26126;&#30830;&#32771;&#34385;&#20102;&#28304;&#36755;&#20837;&#19978;&#19979;&#25991;&#12290;ConvSumX&#21253;&#25324;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#30340;&#29616;&#23454;&#22330;&#26223;&#65292;&#27599;&#20010;&#23376;&#20219;&#21153;&#28085;&#30422;&#20102;&#19977;&#20010;&#35821;&#35328;&#26041;&#21521;&#12290;&#25105;&#20204;&#23545;ConvSumX&#21644;3&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25163;&#24037;&#27880;&#37322;&#30340;CLS&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#23454;&#35777;&#21457;&#29616;ConvSumX&#23545;&#36755;&#20837;&#25991;&#26412;&#26356;&#21487;&#38752;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#30456;&#21516;&#30340;&#30452;&#35273;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#26041;&#27861;&#65292;&#23558;&#23545;&#35805;&#21644;&#25688;&#35201;&#20316;&#20026;&#36755;&#20837;&#26469;&#27169;&#25311;&#20154;&#31867;&#27880;&#37322;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20004;&#27493;&#26041;&#27861;&#22312;ConvSumX&#19978;&#22312;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#19979;&#22343;&#36229;&#36234;&#20102;&#24378;&#22522;&#32447;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Most existing cross-lingual summarization (CLS) work constructs CLS corpora by simply and directly translating pre-annotated summaries from one language to another, which can contain errors from both summarization and translation processes. To address this issue, we propose ConvSumX, a cross-lingual conversation summarization benchmark, through a new annotation schema that explicitly considers source input context. ConvSumX consists of 2 sub-tasks under different real-world scenarios, with each covering 3 language directions. We conduct thorough analysis on ConvSumX and 3 widely-used manually annotated CLS corpora and empirically find that ConvSumX is more faithful towards input text. Additionally, based on the same intuition, we propose a 2-Step method, which takes both conversation and summary as input to simulate human annotation process. Experimental results show that 2-Step method surpasses strong baselines on ConvSumX under both automatic and human evaluation. Analysis shows that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20801;&#35768;&#29992;&#25143;&#22312;&#21475;&#36848;&#36807;&#31243;&#20013;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#32534;&#36753;&#21629;&#20196;&#30340;&#21487;&#34892;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;TERTiUS&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2307.04008</link><description>&lt;p&gt;
&#36808;&#21521;&#20132;&#20114;&#24335;&#21475;&#36848;
&lt;/p&gt;
&lt;p&gt;
Toward Interactive Dictation. (arXiv:2307.04008v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20801;&#35768;&#29992;&#25143;&#22312;&#21475;&#36848;&#36807;&#31243;&#20013;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#32534;&#36753;&#21629;&#20196;&#30340;&#21487;&#34892;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;TERTiUS&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#21475;&#36848;&#26159;&#19968;&#20010;&#26085;&#30410;&#37325;&#35201;&#30340;&#25991;&#26412;&#36755;&#20837;&#26041;&#24335;&#12290;&#29616;&#26377;&#30340;&#20801;&#35768;&#21475;&#36848;&#21644;&#35821;&#38899;&#32534;&#36753;&#30340;&#31995;&#32479;&#23558;&#23427;&#20204;&#30340;&#21629;&#20196;&#35821;&#35328;&#38480;&#21046;&#22312;&#30001;&#35302;&#21457;&#35789;&#35843;&#29992;&#30340;&#24179;&#38754;&#27169;&#26495;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20801;&#35768;&#29992;&#25143;&#20013;&#26029;&#21475;&#36848;&#24182;&#20351;&#29992;&#24320;&#25918;&#24335;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#21475;&#36848;&#21629;&#20196;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;TERTiUS&#65292;&#29992;&#20110;&#23545;&#36825;&#31181;&#31995;&#32479;&#36827;&#34892;&#23454;&#39564;&#12290;&#20026;&#20102;&#23454;&#26102;&#25903;&#25345;&#36825;&#31181;&#28789;&#27963;&#24615;&#65292;&#31995;&#32479;&#24517;&#39035;&#23558;&#36830;&#32493;&#30340;&#35821;&#38899;&#29255;&#27573;&#36880;&#27493;&#20998;&#27573;&#24182;&#20998;&#31867;&#20026;&#21475;&#36848;&#25110;&#21629;&#20196;&#65292;&#24182;&#35299;&#37322;&#21629;&#20196;&#29255;&#27573;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#39044;&#27979;&#32534;&#36753;&#21518;&#30340;&#25991;&#26412;&#65292;&#25110;&#32773;&#39044;&#27979;&#19968;&#20010;&#23567;&#30340;&#25991;&#26412;&#32534;&#36753;&#31243;&#24207;&#12290;&#23454;&#39564;&#26174;&#31034;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#20043;&#38388;&#23384;&#22312;&#22825;&#28982;&#30340;&#26435;&#34913;&#65306;&#36739;&#23567;&#30340;&#27169;&#22411;&#20197;1.3&#31186;&#30340;&#24310;&#36831;&#36798;&#21040;30&#65285;&#30340;&#26368;&#32456;&#20934;&#30830;&#29575;&#65292;&#32780;&#36739;&#22823;&#30340;&#27169;&#22411;&#20197;7&#31186;&#30340;&#24310;&#36831;&#36798;&#21040;55&#65285;&#30340;&#26368;&#32456;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Voice dictation is an increasingly important text input modality. Existing systems that allow both dictation and editing-by-voice restrict their command language to flat templates invoked by trigger words. In this work, we study the feasibility of allowing users to interrupt their dictation with spoken editing commands in open-ended natural language. We introduce a new task and dataset, TERTiUS, to experiment with such systems. To support this flexibility in real-time, a system must incrementally segment and classify spans of speech as either dictation or command, and interpret the spans that are commands. We experiment with using large pre-trained language models to predict the edited text, or alternatively, to predict a small text-editing program. Experiments show a natural trade-off between model accuracy and latency: a smaller model achieves 30% end-state accuracy with 1.3 seconds of latency, while a larger model achieves 55% end-state accuracy with 7 seconds of latency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39564;&#35777;&#20302;&#32622;&#20449;&#24230;&#29983;&#25104;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;LLMs&#30340;&#24187;&#35273;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#29983;&#25104;&#30340;&#20505;&#36873;&#39033;&#36827;&#34892;&#39564;&#35777;&#65292;&#24182;&#32531;&#35299;&#24187;&#35273;&#65292;&#25913;&#36827;&#20102;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03987</link><description>&lt;p&gt;
&#23396;&#25484;&#38590;&#40483;&#65306;&#36890;&#36807;&#39564;&#35777;&#20302;&#32622;&#20449;&#24230;&#29983;&#25104;&#26816;&#27979;&#21644;&#20943;&#36731;LLMs&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation. (arXiv:2307.03987v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39564;&#35777;&#20302;&#32622;&#20449;&#24230;&#29983;&#25104;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;LLMs&#30340;&#24187;&#35273;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#29983;&#25104;&#30340;&#20505;&#36873;&#39033;&#36827;&#34892;&#39564;&#35777;&#65292;&#24182;&#32531;&#35299;&#24187;&#35273;&#65292;&#25913;&#36827;&#20102;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24320;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#27969;&#30021;&#36830;&#36143;&#30340;&#25991;&#26412;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#24120;&#24120;&#20250;&#21457;&#29983;&#8220;&#24187;&#35273;&#8221;&#65292;&#20005;&#37325;&#24433;&#21709;&#20854;&#21487;&#38752;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#26816;&#27979;&#21644;&#20943;&#36731;&#29983;&#25104;&#36807;&#31243;&#20013;&#24187;&#35273;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#27169;&#22411;&#30340;&#36923;&#36753;&#36755;&#20986;&#20540;&#35782;&#21035;&#21487;&#33021;&#30340;&#24187;&#35273;&#20505;&#36873;&#39033;&#65292;&#36890;&#36807;&#39564;&#35777;&#36807;&#31243;&#26816;&#26597;&#20854;&#27491;&#30830;&#24615;&#65292;&#20943;&#36731;&#26816;&#27979;&#21040;&#30340;&#24187;&#35273;&#65292;&#28982;&#21518;&#32487;&#32493;&#29983;&#25104;&#36807;&#31243;&#12290;&#36890;&#36807;&#19982;&#8220;&#25991;&#31456;&#29983;&#25104;&#20219;&#21153;&#8221;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26816;&#27979;&#21644;&#20943;&#36731;&#25216;&#26415;&#30340;&#20010;&#20307;&#26377;&#25928;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26816;&#27979;&#25216;&#26415;&#30340;&#21484;&#22238;&#29575;&#36798;&#21040;&#20102;88&#65285;&#65292;&#32531;&#35299;&#25216;&#26415;&#25104;&#21151;&#32531;&#35299;&#20102;57.6&#65285;&#34987;&#27491;&#30830;&#26816;&#27979;&#21040;&#30340;&#24187;&#35273;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20943;&#36731;&#25216;&#26415;&#19981;&#20250;&#24341;&#20837;&#26032;&#30340;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently developed large language models have achieved remarkable success in generating fluent and coherent text. However, these models often tend to 'hallucinate' which critically hampers their reliability. In this work, we address this crucial problem and propose an approach that actively detects and mitigates hallucinations during the generation process. Specifically, we first identify the candidates of potential hallucination leveraging the model's logit output values, check their correctness through a validation procedure, mitigate the detected hallucinations, and then continue with the generation process. Through extensive experiments with the 'article generation task', we first demonstrate the individual efficacy of our detection and mitigation techniques. Specifically, the detection technique achieves a recall of 88% and the mitigation technique successfully mitigates 57.6% of the correctly detected hallucinations. Importantly, our mitigation technique does not introduce new ha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23384;&#22312;&#36807;&#24230;&#20462;&#27491;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#22312;&#35780;&#20272;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.03972</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Capability of Large-scale Language Models on Chinese Grammatical Error Correction Task. (arXiv:2307.03972v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23384;&#22312;&#36807;&#24230;&#20462;&#27491;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#22312;&#35780;&#20272;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#26368;&#36817;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33521;&#25991;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#20219;&#21153;&#20013;&#26410;&#33021;&#36798;&#21040;&#36229;&#36234;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#33391;&#22909;&#32467;&#26524;&#12290;&#26412;&#25253;&#21578;&#26088;&#22312;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20379;&#25351;&#23548;&#12290;&#25105;&#20204;&#22312;4&#20010;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#20102;3&#20010;&#19981;&#21516;&#27169;&#22411;&#35268;&#27169;&#30340;LLMs&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#22312;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#19978;&#30340;&#34920;&#29616;&#19981;&#21450;&#20043;&#21069;&#30340;&#26368;&#20339;&#27169;&#22411;&#65292;&#22240;&#20026;&#23384;&#22312;&#36807;&#24230;&#20462;&#27491;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;LLMs&#22312;&#35780;&#20272;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#26102;&#30340;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;LLMs&#22312;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#20219;&#21153;&#19978;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale language models (LLMs) has shown remarkable capability in various of Natural Language Processing (NLP) tasks and attracted lots of attention recently. However, some studies indicated that large language models fail to achieve promising result beyond the state-of-the-art models in English grammatical error correction (GEC) tasks. In this report, we aim to explore the how large language models perform on Chinese grammatical error correction tasks and provide guidance for future work. We conduct experiments with 3 different LLMs of different model scale on 4 Chinese GEC dataset. Our experimental results indicate that the performances of LLMs on automatic evaluation metrics falls short of the previous sota models because of the problem of over-correction. Furthermore, we also discover notable variations in the performance of LLMs when evaluated on different data distributions. Our findings demonstrates that further investigation is required for the application of LLMs on Chines
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36827;&#34892;&#20102;&#21021;&#27493;&#35780;&#20272;&#65292;&#25506;&#32034;&#20102;ChatGPT&#22312;&#25991;&#26412;&#20013;&#35782;&#21035;&#20154;&#26684;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#38754;&#21521;&#23618;&#32423;&#30340;&#25552;&#31034;&#31574;&#30053;&#12290;&#35770;&#25991;&#27604;&#36739;&#20102;ChatGPT&#21644;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#12289;RoBERTa&#22312;&#20004;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.03952</link><description>&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#20154;&#26684;&#35782;&#21035;&#22120;&#21527;&#65311;&#21021;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT a Good Personality Recognizer? A Preliminary Study. (arXiv:2307.03952v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03952
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36827;&#34892;&#20102;&#21021;&#27493;&#35780;&#20272;&#65292;&#25506;&#32034;&#20102;ChatGPT&#22312;&#25991;&#26412;&#20013;&#35782;&#21035;&#20154;&#26684;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#38754;&#21521;&#23618;&#32423;&#30340;&#25552;&#31034;&#31574;&#30053;&#12290;&#35770;&#25991;&#27604;&#36739;&#20102;ChatGPT&#21644;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#12289;RoBERTa&#22312;&#20004;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#26684;&#34987;&#35270;&#20026;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#20010;&#20154;&#22240;&#32032;&#65292;&#34987;&#32435;&#20837;&#20102;&#35768;&#22810;&#20219;&#21153;&#65292;&#22914;&#24773;&#24863;&#20998;&#26512;&#21644;&#20135;&#21697;&#25512;&#33616;&#12290;&#36825;&#23548;&#33268;&#20102;&#23545;&#22522;&#20110;&#25991;&#26412;&#30340;&#20154;&#26684;&#35782;&#21035;&#20219;&#21153;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#26681;&#25454;&#32473;&#23450;&#30340;&#25991;&#26412;&#35782;&#21035;&#20010;&#20307;&#30340;&#20154;&#26684;&#12290;&#32771;&#34385;&#21040;ChatGPT&#36817;&#26399;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#25105;&#20204;&#23545;ChatGPT&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#20154;&#26684;&#35782;&#21035;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#21021;&#27493;&#35780;&#20272;&#65292;&#20197;&#29983;&#25104;&#26377;&#25928;&#30340;&#20154;&#26684;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#65292;&#25506;&#32034;ChatGPT&#20174;&#32473;&#23450;&#30340;&#25991;&#26412;&#20013;&#35782;&#21035;&#20154;&#26684;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#25105;&#20204;&#35774;&#35745;&#30340;&#38754;&#21521;&#23618;&#32423;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#29992;&#20110;&#25351;&#23548;ChatGPT&#22312;&#25351;&#23450;&#23618;&#27425;&#20998;&#26512;&#32473;&#23450;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#23558;ChatGPT&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#19982;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#12289;&#24494;&#35843;&#30340;RoBERTa&#20197;&#21450;&#30456;&#24212;&#30340;&#26368;&#26032;&#20219;&#21153;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, personality has been regarded as a valuable personal factor being incorporated into numerous tasks such as sentiment analysis and product recommendation. This has led to widespread attention to text-based personality recognition task, which aims to identify an individual's personality based on given text. Considering that ChatGPT has recently exhibited remarkable abilities on various natural language processing tasks, we provide a preliminary evaluation of ChatGPT on text-based personality recognition task for generating effective personality data. Concretely, we employ a variety of prompting strategies to explore ChatGPT's ability in recognizing personality from given text, especially the level-oriented prompting strategy we designed for guiding ChatGPT in analyzing given text at a specified level. We compare the performance of ChatGPT on two representative real-world datasets with traditional neural network, fine-tuned RoBERTa, and corresponding state-of-the-art task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#34987;&#36951;&#24536;&#26435;&#65288;RTBF&#65289;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25552;&#20379;&#20102;&#23454;&#26045;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.03941</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#34987;&#36951;&#24536;&#26435;&#65306;&#28085;&#20041;&#12289;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions. (arXiv:2307.03941v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#34987;&#36951;&#24536;&#26435;&#65288;RTBF&#65289;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25552;&#20379;&#20102;&#23454;&#26045;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34987;&#36951;&#24536;&#26435;&#65288;RTBF&#65289;&#26368;&#21021;&#26159;&#30001;&#35895;&#27468;&#35199;&#29677;&#29273;&#19982;&#22467;&#20811;&#26031;&#20869;&#22612;&#32034;&#22996;&#21592;&#20250;(Mario Costeja Gonz\'alez)&#20043;&#38388;&#30340;&#23448;&#21496;&#32467;&#26524;&#32780;&#30830;&#31435;&#30340;&#65292;&#24182;&#19988;&#21518;&#26469;&#34987;&#20316;&#20026;&#27431;&#27954;&#32852;&#30431;&#19968;&#33324;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#65288;GDPR&#65289;&#19979;&#30340;&#21024;&#38500;&#26435;&#12290;RTBF&#20801;&#35768;&#20010;&#20154;&#21521;&#32452;&#32455;&#35831;&#27714;&#21024;&#38500;&#20010;&#20154;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#25628;&#32034;&#24341;&#25806;&#65292;&#20010;&#20154;&#21487;&#20197;&#21521;&#32452;&#32455;&#21457;&#36865;&#35831;&#27714;&#65292;&#25490;&#38500;&#20182;&#20204;&#30340;&#20449;&#24687;&#22312;&#26597;&#35810;&#32467;&#26524;&#20013;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#21644;&#20854;&#22312;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#65292;LLM&#21551;&#29992;&#30340;&#36719;&#20214;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#20294;&#23427;&#20204;&#24182;&#27809;&#26377;&#34987;&#25490;&#38500;&#22312;RTBF&#20043;&#22806;&#12290;&#30456;&#27604;&#25628;&#32034;&#24341;&#25806;&#20351;&#29992;&#30340;&#32034;&#24341;&#26041;&#27861;&#65292;LLMs&#20197;&#19968;&#31181;&#23436;&#20840;&#19981;&#21516;&#30340;&#26041;&#24335;&#23384;&#20648;&#21644;&#22788;&#29702;&#20449;&#24687;&#65292;&#36825;&#20026;&#31526;&#21512;RTBF&#25552;&#20986;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#23454;&#26045;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#20197;&#31526;&#21512;RTBF&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Right to be Forgotten (RTBF) was first established as the result of the ruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja Gonz\'alez, and was later included as the Right to Erasure under the General Data Protection Regulation (GDPR) of European Union to allow individuals the right to request personal data be deleted by organizations. Specifically for search engines, individuals can send requests to organizations to exclude their information from the query results. With the recent development of Large Language Models (LLMs) and their use in chatbots, LLM-enabled software systems have become popular. But they are not excluded from the RTBF. Compared with the indexing approach used by search engines, LLMs store, and process information in a completely different way. This poses new challenges for compliance with the RTBF. In this paper, we explore these challenges and provide our insights on how to implement technical solutions for the RTBF, including the use of machine unle
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;Speech-LLaMA&#65292;&#23558;&#22768;&#23398;&#20449;&#24687;&#26377;&#25928;&#22320;&#34701;&#20837;&#22522;&#20110;&#25991;&#26412;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#36890;&#36807;&#20351;&#29992;&#36830;&#25509;&#20027;&#20041;&#26102;&#24207;&#20998;&#31867;&#21644;&#31616;&#21333;&#30340;&#38899;&#39057;&#32534;&#30721;&#22120;&#65292;&#23558;&#21387;&#32553;&#30340;&#22768;&#23398;&#29305;&#24449;&#26144;&#23556;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36830;&#32493;&#35821;&#20041;&#31354;&#38388;&#20013;&#65292;&#23454;&#29616;&#20102;&#35821;&#38899;&#21040;&#25991;&#26412;&#20219;&#21153;&#20013;&#30340;&#23454;&#36136;&#24615;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2307.03917</link><description>&lt;p&gt;
&#20851;&#20110;&#20165;&#35299;&#30721;&#22120;&#26550;&#26500;&#22312;&#35821;&#38899;&#21040;&#25991;&#26412;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On decoder-only architecture for speech-to-text and large language model integration. (arXiv:2307.03917v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03917
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;Speech-LLaMA&#65292;&#23558;&#22768;&#23398;&#20449;&#24687;&#26377;&#25928;&#22320;&#34701;&#20837;&#22522;&#20110;&#25991;&#26412;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#36890;&#36807;&#20351;&#29992;&#36830;&#25509;&#20027;&#20041;&#26102;&#24207;&#20998;&#31867;&#21644;&#31616;&#21333;&#30340;&#38899;&#39057;&#32534;&#30721;&#22120;&#65292;&#23558;&#21387;&#32553;&#30340;&#22768;&#23398;&#29305;&#24449;&#26144;&#23556;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36830;&#32493;&#35821;&#20041;&#31354;&#38388;&#20013;&#65292;&#23454;&#29616;&#20102;&#35821;&#38899;&#21040;&#25991;&#26412;&#20219;&#21153;&#20013;&#30340;&#23454;&#36136;&#24615;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#33021;&#22815;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#23454;&#29616;&#26356;&#22909;&#30340;&#20154;&#26426;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#23558;&#35821;&#38899;&#20449;&#21495;&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#25506;&#32034;&#12290;&#21516;&#26102;&#65292;&#20851;&#20110;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#30340;&#8220;&#20165;&#35299;&#30721;&#22120;&#8221;&#26550;&#26500;&#20063;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Speech-LLaMA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#23558;&#22768;&#23398;&#20449;&#24687;&#34701;&#20837;&#22522;&#20110;&#25991;&#26412;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#36830;&#25509;&#20027;&#20041;&#26102;&#24207;&#20998;&#31867;&#21644;&#31616;&#21333;&#30340;&#38899;&#39057;&#32534;&#30721;&#22120;&#65292;&#23558;&#21387;&#32553;&#30340;&#22768;&#23398;&#29305;&#24449;&#26144;&#23556;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36830;&#32493;&#35821;&#20041;&#31354;&#38388;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#20165;&#35299;&#30721;&#22120;&#26550;&#26500;&#22312;&#35821;&#38899;&#21040;&#25991;&#26412;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#35821;&#38899;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#35757;&#32451;&#19968;&#20010;&#36739;&#23567;&#35268;&#27169;&#12289;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;Speech-LLaMA&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#22810;&#35821;&#35328;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#19982;&#24378;&#22522;&#20934;&#30456;&#27604;&#26377;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved remarkable success in the field of natural language processing, enabling better human-computer interaction using natural language. However, the seamless integration of speech signals into LLMs has not been explored well. The "decoder-only" architecture has also not been well studied for speech processing tasks. In this research, we introduce Speech-LLaMA, a novel approach that effectively incorporates acoustic information into text-based large language models. Our method leverages Connectionist Temporal Classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the LLM. In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speech-LLaMA model from speech-text paired data alone. We conduct experiments on multilingual speech-to-text translation tasks and demonstrate a significant improvement over strong baseli
&lt;/p&gt;</description></item><item><title>ScriptWorld&#26159;&#19968;&#20010;&#22522;&#20110;&#25991;&#26412;&#30340;&#29615;&#22659;&#65292;&#29992;&#20110;&#25945;&#25480;&#26234;&#33021;&#20307;&#20851;&#20110;&#30495;&#23454;&#19990;&#30028;&#26085;&#24120;&#20219;&#21153;&#21644;&#24120;&#35782;&#30693;&#35782;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#37319;&#29992;&#33050;&#26412;&#25968;&#25454;&#38598;&#35774;&#35745;&#30340;&#20114;&#21160;&#24335;&#22522;&#20110;&#25991;&#26412;&#30340;&#28216;&#25103;&#26694;&#26550;&#65292;&#22312;10&#20010;&#26085;&#24120;&#27963;&#21160;&#20013;&#25552;&#20379;&#20102;&#28216;&#25103;&#29615;&#22659;&#65292;&#24182;&#36890;&#36807;&#23545;&#29615;&#22659;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#24341;&#20837;&#33050;&#26412;&#24335;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#21487;&#20197;&#26377;&#25928;&#25552;&#20379;&#24120;&#35782;&#30693;&#35782;&#24182;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03906</link><description>&lt;p&gt;
ScriptWorld: &#29992;&#20110;&#23398;&#20064;&#36807;&#31243;&#24615;&#30693;&#35782;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
ScriptWorld: Text Based Environment For Learning Procedural Knowledge. (arXiv:2307.03906v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03906
&lt;/p&gt;
&lt;p&gt;
ScriptWorld&#26159;&#19968;&#20010;&#22522;&#20110;&#25991;&#26412;&#30340;&#29615;&#22659;&#65292;&#29992;&#20110;&#25945;&#25480;&#26234;&#33021;&#20307;&#20851;&#20110;&#30495;&#23454;&#19990;&#30028;&#26085;&#24120;&#20219;&#21153;&#21644;&#24120;&#35782;&#30693;&#35782;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#37319;&#29992;&#33050;&#26412;&#25968;&#25454;&#38598;&#35774;&#35745;&#30340;&#20114;&#21160;&#24335;&#22522;&#20110;&#25991;&#26412;&#30340;&#28216;&#25103;&#26694;&#26550;&#65292;&#22312;10&#20010;&#26085;&#24120;&#27963;&#21160;&#20013;&#25552;&#20379;&#20102;&#28216;&#25103;&#29615;&#22659;&#65292;&#24182;&#36890;&#36807;&#23545;&#29615;&#22659;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#24341;&#20837;&#33050;&#26412;&#24335;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#21487;&#20197;&#26377;&#25928;&#25552;&#20379;&#24120;&#35782;&#30693;&#35782;&#24182;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#28216;&#25103;&#20026;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26234;&#33021;&#20307;&#24320;&#21457;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#24120;&#35782;&#30693;&#35782;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#29616;&#26377;&#30340;&#25991;&#26412;&#29615;&#22659;&#36890;&#24120;&#20381;&#36182;&#20110;&#34394;&#26500;&#30340;&#24773;&#26223;&#21644;&#35282;&#33394;&#26469;&#21019;&#24314;&#28216;&#25103;&#26694;&#26550;&#65292;&#19982;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#30456;&#24046;&#29978;&#36828;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ScriptWorld&#65306;&#19968;&#20010;&#22522;&#20110;&#25991;&#26412;&#30340;&#29615;&#22659;&#65292;&#29992;&#20110;&#25945;&#25480;&#26234;&#33021;&#20307;&#20851;&#20110;&#30495;&#23454;&#19990;&#30028;&#26085;&#24120;&#20219;&#21153;&#21644;&#24120;&#35782;&#30693;&#35782;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#37319;&#29992;&#33050;&#26412;&#25968;&#25454;&#38598;&#35774;&#35745;&#30340;&#20114;&#21160;&#24335;&#22522;&#20110;&#25991;&#26412;&#30340;&#28216;&#25103;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;10&#20010;&#26085;&#24120;&#27963;&#21160;&#30340;&#28216;&#25103;&#29615;&#22659;&#65292;&#24182;&#23545;&#25152;&#25552;&#20986;&#30340;&#29615;&#22659;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#20934;&#27169;&#22411;/&#26234;&#33021;&#20307;&#26469;&#29609;ScriptWorld&#20013;&#30340;&#28216;&#25103;&#12290;&#20026;&#20102;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#30340;&#20316;&#29992;&#65292;&#25105;&#20204;&#21033;&#29992;&#20174;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#29305;&#24449;&#26469;&#36741;&#21161;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;ScriptWorld&#20013;&#24341;&#20837;&#33050;&#26412;&#24335;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#21487;&#20197;&#26377;&#25928;&#25552;&#20379;&#24120;&#35782;&#30693;&#35782;&#21644;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-based games provide a framework for developing natural language understanding and commonsense knowledge about the world in reinforcement learning based agents. Existing text-based environments often rely on fictional situations and characters to create a gaming framework and are far from real-world scenarios. In this paper, we introduce ScriptWorld: a text-based environment for teaching agents about real-world daily chores and hence imparting commonsense knowledge. To the best of our knowledge, it is the first interactive text-based gaming framework that consists of daily real-world human activities designed using scripts dataset. We provide gaming environments for 10 daily activities and perform a detailed analysis of the proposed environment. We develop RL-based baseline models/agents to play the games in Scriptworld. To understand the role of language models in such environments, we leverage features obtained from pre-trained language models in the RL agents. Our experiments sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36845;&#20195;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#22238;&#31572;&#27169;&#31946;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#19968;&#20010;&#31572;&#26696;&#27169;&#22411;&#21644;&#19968;&#20010;&#25552;&#31034;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#36880;&#27493;&#36861;&#36394;&#38405;&#35835;&#36807;&#31243;&#24182;&#35302;&#21457;&#31572;&#26696;&#27169;&#22411;&#26469;&#29983;&#25104;&#29420;&#29305;&#19988;&#30456;&#20851;&#30340;&#31572;&#26696;&#12290;&#36890;&#36807;&#24320;&#21457;&#20219;&#21153;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2307.03897</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#25552;&#31034;&#22238;&#31572;&#27169;&#31946;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Answering Ambiguous Questions via Iterative Prompting. (arXiv:2307.03897v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36845;&#20195;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#22238;&#31572;&#27169;&#31946;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#19968;&#20010;&#31572;&#26696;&#27169;&#22411;&#21644;&#19968;&#20010;&#25552;&#31034;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#36880;&#27493;&#36861;&#36394;&#38405;&#35835;&#36807;&#31243;&#24182;&#35302;&#21457;&#31572;&#26696;&#27169;&#22411;&#26469;&#29983;&#25104;&#29420;&#29305;&#19988;&#30456;&#20851;&#30340;&#31572;&#26696;&#12290;&#36890;&#36807;&#24320;&#21457;&#20219;&#21153;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#31572;&#20013;&#65292;&#30001;&#20110;&#38382;&#39064;&#30340;&#27169;&#31946;&#24615;&#65292;&#21487;&#33021;&#23384;&#22312;&#22810;&#20010;&#21512;&#29702;&#30340;&#31572;&#26696;&#12290;&#20026;&#20102;&#23545;&#19968;&#20010;&#27169;&#31946;&#30340;&#38382;&#39064;&#25552;&#20379;&#21487;&#34892;&#30340;&#31572;&#26696;&#65292;&#19968;&#31181;&#26041;&#27861;&#26159;&#30452;&#25509;&#39044;&#27979;&#25152;&#26377;&#26377;&#25928;&#30340;&#31572;&#26696;&#65292;&#20294;&#36825;&#21487;&#33021;&#38590;&#20197;&#24179;&#34913;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#25910;&#38598;&#20505;&#36873;&#31572;&#26696;&#24182;&#23545;&#23427;&#20204;&#36827;&#34892;&#27719;&#24635;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#20195;&#20215;&#39640;&#65292;&#24182;&#19988;&#21487;&#33021;&#24573;&#30053;&#31572;&#26696;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AmbigPrompt&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#22238;&#31572;&#27169;&#31946;&#38382;&#39064;&#30340;&#32570;&#38519;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20197;&#36845;&#20195;&#30340;&#26041;&#24335;&#23558;&#19968;&#20010;&#31572;&#26696;&#27169;&#22411;&#19982;&#19968;&#20010;&#25552;&#31034;&#27169;&#22411;&#38598;&#25104;&#22312;&#19968;&#36215;&#12290;&#25552;&#31034;&#27169;&#22411;&#36880;&#27493;&#36861;&#36394;&#38405;&#35835;&#36807;&#31243;&#65292;&#24182;&#36880;&#28176;&#35302;&#21457;&#31572;&#26696;&#27169;&#22411;&#26469;&#29983;&#25104;&#29420;&#29305;&#19988;&#30456;&#20851;&#30340;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#31572;&#26696;&#27169;&#22411;&#21644;&#25552;&#31034;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31181;&#20219;&#21153;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In open-domain question answering, due to the ambiguity of questions, multiple plausible answers may exist. To provide feasible answers to an ambiguous question, one approach is to directly predict all valid answers, but this can struggle with balancing relevance and diversity. An alternative is to gather candidate answers and aggregate them, but this method can be computationally costly and may neglect dependencies among answers. In this paper, we present AmbigPrompt to address the imperfections of existing approaches to answering ambiguous questions. Specifically, we integrate an answering model with a prompting model in an iterative manner. The prompting model adaptively tracks the reading process and progressively triggers the answering model to compose distinct and relevant answers. Additionally, we develop a task-specific post-pretraining approach for both the answering model and the prompting model, which greatly improves the performance of our framework. Empirical studies on tw
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#35805;&#35821;&#23884;&#20837;&#25216;&#26415;&#24320;&#21457;&#31038;&#21306;&#25512;&#33616;&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#32676;&#20307;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#31038;&#21306;&#30340;&#35805;&#35821;&#20449;&#24687;&#65292;&#37319;&#29992;&#22522;&#20110;&#20869;&#23481;&#21644;&#21327;&#21516;&#36807;&#28388;&#25216;&#26415;&#65292;&#25552;&#21319;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03892</link><description>&lt;p&gt;
&#23558;&#24515;&#29702;&#20581;&#24247;&#35805;&#35821;&#23884;&#20837;&#31038;&#21306;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Embedding Mental Health Discourse for Community Recommendation. (arXiv:2307.03892v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#35805;&#35821;&#23884;&#20837;&#25216;&#26415;&#24320;&#21457;&#31038;&#21306;&#25512;&#33616;&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#32676;&#20307;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#31038;&#21306;&#30340;&#35805;&#35821;&#20449;&#24687;&#65292;&#37319;&#29992;&#22522;&#20110;&#20869;&#23481;&#21644;&#21327;&#21516;&#36807;&#28388;&#25216;&#26415;&#65292;&#25552;&#21319;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#35805;&#35821;&#23884;&#20837;&#25216;&#26415;&#26469;&#24320;&#21457;&#19968;&#20010;&#31038;&#21306;&#25512;&#33616;&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#32676;&#20307;&#12290;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#19982;&#28385;&#36275;&#20854;&#29305;&#23450;&#20852;&#36259;&#30340;&#31038;&#21306;&#21311;&#21517;&#36830;&#25509;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#32447;&#31038;&#21306;&#25968;&#37327;&#24222;&#22823;&#65292;&#29992;&#25143;&#21487;&#33021;&#38590;&#20197;&#25214;&#21040;&#30456;&#20851;&#32676;&#32452;&#26469;&#35299;&#20915;&#20182;&#20204;&#30340;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20351;&#29992;&#23884;&#20837;&#25216;&#26415;&#25506;&#32034;&#26469;&#33258;&#19981;&#21516;subreddit&#31038;&#21306;&#30340;&#35805;&#35821;&#20449;&#24687;&#30340;&#25972;&#21512;&#65292;&#20197;&#24320;&#21457;&#19968;&#20010;&#26377;&#25928;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#20869;&#23481;&#21644;&#21327;&#21516;&#36807;&#28388;&#30340;&#25216;&#26415;&#26469;&#25552;&#21319;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#21333;&#29420;&#20351;&#29992;&#27599;&#31181;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#25512;&#33616;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our paper investigates the use of discourse embedding techniques to develop a community recommendation system that focuses on mental health support groups on social media. Social media platforms provide a means for users to anonymously connect with communities that cater to their specific interests. However, with the vast number of online communities available, users may face difficulties in identifying relevant groups to address their mental health concerns. To address this challenge, we explore the integration of discourse information from various subreddit communities using embedding techniques to develop an effective recommendation system. Our approach involves the use of content-based and collaborative filtering techniques to enhance the performance of the recommendation system. Our findings indicate that the proposed approach outperforms the use of each technique separately and provides interpretability in the recommendation process.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#35299;&#37322;&#21644;&#35299;&#35835;&#20379;&#24212;&#38142;&#20248;&#21270;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;&#20182;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#25509;&#21463;&#26222;&#36890;&#25991;&#26412;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#20851;&#20110;&#24213;&#23618;&#20248;&#21270;&#32467;&#26524;&#30340;&#27934;&#23519;&#12290;&#36890;&#36807;&#23450;&#37327;&#22238;&#31572;&#20551;&#35774;&#24773;&#20917;&#65292;&#35813;&#26694;&#26550;&#22312;&#19981;&#25918;&#24323;&#26368;&#20808;&#36827;&#30340;&#32452;&#21512;&#20248;&#21270;&#25216;&#26415;&#30340;&#24773;&#20917;&#19979;&#24110;&#21161;&#20225;&#19994;&#36816;&#33829;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#20449;&#20219;&#20248;&#21270;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.03875</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20379;&#24212;&#38142;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Supply Chain Optimization. (arXiv:2307.03875v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03875
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#35299;&#37322;&#21644;&#35299;&#35835;&#20379;&#24212;&#38142;&#20248;&#21270;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;&#20182;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#25509;&#21463;&#26222;&#36890;&#25991;&#26412;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#20851;&#20110;&#24213;&#23618;&#20248;&#21270;&#32467;&#26524;&#30340;&#27934;&#23519;&#12290;&#36890;&#36807;&#23450;&#37327;&#22238;&#31572;&#20551;&#35774;&#24773;&#20917;&#65292;&#35813;&#26694;&#26550;&#22312;&#19981;&#25918;&#24323;&#26368;&#20808;&#36827;&#30340;&#32452;&#21512;&#20248;&#21270;&#25216;&#26415;&#30340;&#24773;&#20917;&#19979;&#24110;&#21161;&#20225;&#19994;&#36816;&#33829;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#20449;&#20219;&#20248;&#21270;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#20379;&#24212;&#38142;&#25805;&#20316;&#28041;&#21450;&#21508;&#31181;&#22797;&#26434;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#20379;&#24212;&#38142;&#21463;&#30410;&#20110;&#35745;&#31639;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#20174;&#25163;&#21160;&#22788;&#29702;&#36807;&#28193;&#21040;&#33258;&#21160;&#21270;&#21644;&#25104;&#26412;&#25928;&#30410;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#20225;&#19994;&#36816;&#33829;&#32773;&#20173;&#28982;&#38656;&#35201;&#33457;&#36153;&#22823;&#37327;&#31934;&#21147;&#26469;&#35299;&#37322;&#21644;&#35299;&#35835;&#20248;&#21270;&#32467;&#26524;&#32473;&#30456;&#20851;&#20154;&#22763;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26368;&#36817;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#39072;&#35206;&#24615;&#25216;&#26415;&#22914;&#20309;&#24110;&#21161;&#24357;&#21512;&#20379;&#24212;&#38142;&#33258;&#21160;&#21270;&#21644;&#20154;&#31867;&#29702;&#35299;&#19982;&#20449;&#20219;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;\name{}&#30340;&#26694;&#26550;&#65292;&#23427;&#25509;&#21463;&#26222;&#36890;&#25991;&#26412;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#20851;&#20110;&#24213;&#23618;&#20248;&#21270;&#32467;&#26524;&#30340;&#27934;&#23519;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24182;&#27809;&#26377;&#25918;&#24323;&#26368;&#20808;&#36827;&#30340;&#32452;&#21512;&#20248;&#21270;&#25216;&#26415;&#65292;&#32780;&#26159;&#21033;&#29992;&#23427;&#26469;&#23450;&#37327;&#22320;&#22238;&#31572;&#20551;&#35774;&#24773;&#20917;&#65288;&#20363;&#22914;&#65292;&#22914;&#26524;&#25105;&#20204;&#20351;&#29992;&#20379;&#24212;&#21830;B&#32780;&#19981;&#26159;&#20379;&#24212;&#21830;A&#65292;&#25104;&#26412;&#20250;&#22914;&#20309;&#21464;&#21270;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supply chain operations traditionally involve a variety of complex decision making problems. Over the last few decades, supply chains greatly benefited from advances in computation, which allowed the transition from manual processing to automation and cost-effective optimization. Nonetheless, business operators still need to spend substantial efforts in \emph{explaining} and interpreting the optimization outcomes to stakeholders. Motivated by the recent advances in Large Language Models (LLMs), we study how this disruptive technology can help bridge the gap between supply chain automation and human comprehension and trust thereof. We design \name{} -- a framework that accepts as input queries in plain text, and outputs insights about the underlying optimization outcomes. Our framework does not forgo the state-of-the-art combinatorial optimization technology, but rather leverages it to quantitatively answer what-if scenarios (e.g., how would the cost change if we used supplier B instead
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#22312;&#38271;&#31687;&#21307;&#30103;&#25991;&#26723;&#19978;&#36827;&#34892;&#35777;&#25454;/&#29702;&#30001;&#25552;&#21462;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#33021;&#22815;&#20026;&#35745;&#31639;&#26426;&#36741;&#21161;&#32534;&#30721;&#31995;&#32479;&#25552;&#20379;&#20934;&#30830;&#21487;&#38752;&#30340;&#25903;&#25345;&#35777;&#25454;&#65292;&#23545;&#20110;&#25913;&#36827;&#21307;&#30103;&#32534;&#30721;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.03859</link><description>&lt;p&gt;
MDACE: MIMIC&#25991;&#26723;&#30340;&#24102;&#20195;&#30721;&#35777;&#25454;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
MDACE: MIMIC Documents Annotated with Code Evidence. (arXiv:2307.03859v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03859
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#22312;&#38271;&#31687;&#21307;&#30103;&#25991;&#26723;&#19978;&#36827;&#34892;&#35777;&#25454;/&#29702;&#30001;&#25552;&#21462;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#33021;&#22815;&#20026;&#35745;&#31639;&#26426;&#36741;&#21161;&#32534;&#30721;&#31995;&#32479;&#25552;&#20379;&#20934;&#30830;&#21487;&#38752;&#30340;&#25903;&#25345;&#35777;&#25454;&#65292;&#23545;&#20110;&#25913;&#36827;&#21307;&#30103;&#32534;&#30721;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#22312;&#38271;&#31687;&#21307;&#30103;&#25991;&#26723;&#19978;&#36827;&#34892;&#35777;&#25454;/&#29702;&#30001;&#25552;&#21462;&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#22312;&#26497;&#31471;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#12290;&#20854;&#20013;&#19968;&#20010;&#20219;&#21153;&#26159;&#35745;&#31639;&#26426;&#36741;&#21161;&#32534;&#30721;&#65288;CAC&#65289;&#65292;&#36817;&#24180;&#26469;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;CAC&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#39044;&#27979;&#24739;&#32773;&#23601;&#35786;&#30340;&#19968;&#32452;&#26368;&#32456;&#20195;&#30721;&#26159;&#19981;&#22815;&#30340;&#65292;&#22240;&#20026;CAC&#31995;&#32479;&#38656;&#35201;&#25552;&#20379;&#25903;&#25345;&#30340;&#25991;&#26412;&#35777;&#25454;&#26469;&#35777;&#26126;&#35745;&#36153;&#20195;&#30721;&#30340;&#21512;&#29702;&#24615;&#12290;&#33021;&#22815;&#20026;&#27599;&#20010;&#20195;&#30721;&#20135;&#29983;&#20934;&#30830;&#21487;&#38752;&#30340;&#25903;&#25345;&#35777;&#25454;&#30340;&#27169;&#22411;&#23558;&#26377;&#24040;&#22823;&#30340;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#20154;&#24037;&#27880;&#37322;&#30340;&#20195;&#30721;&#35777;&#25454;&#35821;&#26009;&#24211;&#38750;&#24120;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MDACE&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#20195;&#30721;&#35777;&#25454;&#25968;&#25454;&#38598;&#65292;&#23427;&#26159;&#22522;&#20110;MIMIC-III&#20020;&#24202;&#35760;&#24405;&#30340;&#19968;&#20010;&#23376;&#38598;&#26500;&#24314;&#30340;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;&#19987;&#19994;&#21307;&#30103;&#32534;&#30721;&#20154;&#21592;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#21253;&#21547;302&#20221;&#20303;&#38498;&#30149;&#21382;&#30340;3,934&#20010;&#35777;&#25454;&#29255;&#27573;&#21644;52&#20221;&#36153;&#29992;&#30149;&#21382;&#30340;5,563&#20010;&#35777;&#25454;&#29255;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a dataset for evidence/rationale extraction on an extreme multi-label classification task over long medical documents. One such task is Computer-Assisted Coding (CAC) which has improved significantly in recent years, thanks to advances in machine learning technologies. Yet simply predicting a set of final codes for a patient encounter is insufficient as CAC systems are required to provide supporting textual evidence to justify the billing codes. A model able to produce accurate and reliable supporting evidence for each code would be a tremendous benefit. However, a human annotated code evidence corpus is extremely difficult to create because it requires specialized knowledge. In this paper, we introduce MDACE, the first publicly available code evidence dataset, which is built on a subset of the MIMIC-III clinical records. The dataset -- annotated by professional medical coders -- consists of 302 Inpatient charts with 3,934 evidence spans and 52 Profee charts with 5,563 evi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RADAR&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#23454;&#29616;&#20102;&#40065;&#26834;&#30340;AI&#25991;&#26412;&#26816;&#27979;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#23545;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25913;&#20889;&#19981;&#20855;&#22791;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.03838</link><description>&lt;p&gt;
RADAR: &#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#23454;&#29616;&#40065;&#26834;&#30340;AI&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
RADAR: Robust AI-Text Detection via Adversarial Learning. (arXiv:2307.03838v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RADAR&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#23454;&#29616;&#20102;&#40065;&#26834;&#30340;AI&#25991;&#26412;&#26816;&#27979;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#23545;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25913;&#20889;&#19981;&#20855;&#22791;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20197;&#21450;ChatGPT&#31867;&#24212;&#29992;&#30340;&#26222;&#21450;&#24050;&#32463;&#27169;&#31946;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#39640;&#36136;&#37327;&#25991;&#26412;&#29983;&#25104;&#30340;&#30028;&#38480;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#23545;&#25105;&#20204;&#30340;&#25216;&#26415;&#21644;&#31038;&#20250;&#39044;&#26399;&#30340;&#38761;&#21629;&#24615;&#21464;&#21270;&#22806;&#65292;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#65288;AI&#25991;&#26412;&#65289;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#22256;&#38590;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#28389;&#29992;&#21644;&#20844;&#24179;&#24615;&#25361;&#25112;&#65292;&#20363;&#22914;&#34394;&#20551;&#20869;&#23481;&#29983;&#25104;&#65292;&#25220;&#34989;&#20197;&#21450;&#23545;&#26080;&#36764;&#20316;&#32773;&#30340;&#38169;&#35823;&#25351;&#25511;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#24403;&#21069;&#30340;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#23545;&#22522;&#20110;LLM&#30340;&#25913;&#20889;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#20294;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;RADAR&#30340;&#26032;&#26694;&#26550;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#20849;&#21516;&#35757;&#32451;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#12290;RADAR&#22522;&#20110;&#19968;&#20010;&#25913;&#20889;&#22120;&#21644;&#19968;&#20010;&#26816;&#27979;&#22120;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#12290;&#25913;&#20889;&#22120;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#36924;&#30495;&#30340;&#20869;&#23481;&#20197;&#35268;&#36991;AI&#25991;&#26412;&#26816;&#27979;&#12290;RADAR&#20351;&#29992;&#26469;&#33258;&#26816;&#27979;&#22120;&#30340;&#21453;&#39304;&#26469;&#26356;&#26032;&#25913;&#20889;&#22120;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusation of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a Robust AI-text Detector via Adversarial leaRning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic contents to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice vers
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#20013;&#65292;&#23558;&#35821;&#35328;&#34920;&#31034;&#20316;&#20026;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#23545;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#20182;&#20204;&#36890;&#36807;&#20132;&#26367;&#20351;&#29992;&#21477;&#27861;&#21644;&#35821;&#20041;&#22270;&#26469;&#25193;&#20805;Transformer&#26550;&#26500;&#65292;&#24182;&#21457;&#29616;&#35821;&#35328;&#34920;&#31034;&#21487;&#20197;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#19988;&#35821;&#20041;&#34920;&#31034;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#20851;&#31995;&#25277;&#21462;&#19978;&#26356;&#26377;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2307.03823</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#30340;&#35821;&#35328;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Linguistic representations for fewer-shot relation extraction across domains. (arXiv:2307.03823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03823
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#20013;&#65292;&#23558;&#35821;&#35328;&#34920;&#31034;&#20316;&#20026;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#23545;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#20182;&#20204;&#36890;&#36807;&#20132;&#26367;&#20351;&#29992;&#21477;&#27861;&#21644;&#35821;&#20041;&#22270;&#26469;&#25193;&#20805;Transformer&#26550;&#26500;&#65292;&#24182;&#21457;&#29616;&#35821;&#35328;&#34920;&#31034;&#21487;&#20197;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#19988;&#35821;&#20041;&#34920;&#31034;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#20851;&#31995;&#25277;&#21462;&#19978;&#26356;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#23558;&#35821;&#35328;&#34920;&#31034;&#20316;&#20026;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#21644;&#25903;&#25345;&#20449;&#24687;&#21152;&#20837;&#21040;&#22810;&#20010;NLP&#20219;&#21153;&#30340;&#39046;&#22495;&#20869;&#24615;&#33021;&#20013;&#21487;&#20197;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20960;&#27425;&#36801;&#31227;&#23398;&#20064;&#29615;&#22659;&#20013;&#25506;&#32034;&#35821;&#35328;&#34920;&#31034;&#23545;&#36328;&#39046;&#22495;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#23545;&#36825;&#20123;&#30740;&#31350;&#36827;&#34892;&#20102;&#25193;&#23637;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#26159;&#65292;&#35821;&#35328;&#34920;&#31034;&#26159;&#21542;&#36890;&#36807;&#25552;&#20379;&#20316;&#20026;&#36328;&#39046;&#22495;&#20013;&#24515;&#30340;&#29305;&#24449;&#26469;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#20851;&#27880;&#22312;&#20004;&#20010;&#39046;&#22495;&#65288;&#28921;&#39274;&#21644;&#26448;&#26009;&#31185;&#23398;&#65289;&#20013;&#30340;&#19977;&#20010;&#31243;&#24207;&#24615;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#30340;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20132;&#26367;&#22320;&#20351;&#29992;&#36890;&#36807;&#20813;&#36153;&#25552;&#20379;&#30340;&#29616;&#25104;&#24037;&#20855;&#26500;&#24314;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#22270;&#26469;&#25193;&#20805;&#27969;&#34892;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#32771;&#23519;&#20102;&#23427;&#20204;&#22312;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#35843;&#26597;&#20102;&#26089;&#26399;&#21457;&#29616;&#65292;&#20363;&#22914;&#35821;&#20041;&#34920;&#31034;&#27604;&#21477;&#27861;&#34920;&#31034;&#26356;&#26377;&#24110;&#21161;&#65292;&#26159;&#21542;&#36866;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#30340;&#20851;&#31995;&#25277;&#21462;&#12290;&#25105;&#20204;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Recent work has demonstrated the positive impact of incorporating linguistic representations as additional context and scaffolding on the in-domain performance of several NLP tasks. We extend this work by exploring the impact of linguistic representations on cross-domain performance in a few-shot transfer setting. An important question is whether linguistic representations enhance generalizability by providing features that function as cross-domain pivots. We focus on the task of relation extraction on three datasets of procedural text in two domains, cooking and materials science. Our approach augments a popular transformer-based architecture by alternately incorporating syntactic and semantic graphs constructed by freely available off-the-shelf tools. We examine their utility for enhancing generalization, and investigate whether earlier findings, e.g. that semantic representations can be more helpful than syntactic ones, extend to relation extraction in multiple domains. We find that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#27874;&#26031;&#35821;Twitter&#35805;&#35821;&#30340;&#35745;&#31639;&#20998;&#26512;&#65292;&#20272;&#31639;&#20102;&#39532;&#33678;&#183;&#38463;&#31859;&#23612;&#21435;&#19990;&#21518;&#20234;&#26391;&#31038;&#20250;&#23545;&#24615;&#21035;&#24179;&#31561;&#31435;&#22330;&#30340;&#36716;&#21464;&#12290;&#30740;&#31350;&#37319;&#29992;&#21442;&#19982;&#24335;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#28041;&#21450;&#20234;&#26391;&#22899;&#24615;&#22312;&#26631;&#27880;&#36807;&#31243;&#20013;&#30340;&#31215;&#26497;&#35282;&#33394;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#39532;&#33678;&#183;&#38463;&#31859;&#23612;&#30340;&#27515;&#20129;&#24341;&#21457;&#20102;&#27874;&#26031;&#35821;&#35805;&#35821;&#30340;&#26497;&#21270;&#65292;&#31215;&#26497;&#25512;&#25991;&#25968;&#37327;&#30053;&#22810;&#20110;&#36127;&#38754;&#25512;&#25991;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#35266;&#23519;&#21040;&#25919;&#24220;&#21644;&#25239;&#35758;&#32773;&#22312;Twitter&#19978;&#30340;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2307.03764</link><description>&lt;p&gt;
&#23545;&#20110;&#22899;&#24615;&#32780;&#35328;&#65292;&#29983;&#27963;&#21644;&#33258;&#30001;&#65306;&#22522;&#20110;&#21442;&#19982;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#35770;&#25991;&#30740;&#31350;&#20234;&#26391;&#24615;&#21035;&#26007;&#20105;&#20013;&#30340;&#20998;&#27700;&#23725;&#26102;&#21051;
&lt;/p&gt;
&lt;p&gt;
For Women, Life, Freedom: A Participatory AI-Based Social Web Analysis of a Watershed Moment in Iran's Gender Struggles. (arXiv:2307.03764v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#27874;&#26031;&#35821;Twitter&#35805;&#35821;&#30340;&#35745;&#31639;&#20998;&#26512;&#65292;&#20272;&#31639;&#20102;&#39532;&#33678;&#183;&#38463;&#31859;&#23612;&#21435;&#19990;&#21518;&#20234;&#26391;&#31038;&#20250;&#23545;&#24615;&#21035;&#24179;&#31561;&#31435;&#22330;&#30340;&#36716;&#21464;&#12290;&#30740;&#31350;&#37319;&#29992;&#21442;&#19982;&#24335;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#28041;&#21450;&#20234;&#26391;&#22899;&#24615;&#22312;&#26631;&#27880;&#36807;&#31243;&#20013;&#30340;&#31215;&#26497;&#35282;&#33394;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#39532;&#33678;&#183;&#38463;&#31859;&#23612;&#30340;&#27515;&#20129;&#24341;&#21457;&#20102;&#27874;&#26031;&#35821;&#35805;&#35821;&#30340;&#26497;&#21270;&#65292;&#31215;&#26497;&#25512;&#25991;&#25968;&#37327;&#30053;&#22810;&#20110;&#36127;&#38754;&#25512;&#25991;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#35266;&#23519;&#21040;&#25919;&#24220;&#21644;&#25239;&#35758;&#32773;&#22312;Twitter&#19978;&#30340;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#27874;&#26031;&#35821;Twitter&#35805;&#35821;&#30340;&#35745;&#31639;&#20998;&#26512;&#65292;&#26088;&#22312;&#20272;&#31639;&#39532;&#33678;&#183;&#38463;&#31859;&#23612;&#22312;&#35686;&#26041;&#25304;&#30041;&#26399;&#38388;&#21435;&#19990;&#21518;&#23545;&#24615;&#21035;&#24179;&#31561;&#31435;&#22330;&#30340;&#36716;&#21464;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20027;&#21160;&#23398;&#20064;&#27969;&#31243;&#26469;&#35757;&#32451;&#19968;&#20010;&#31435;&#22330;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#22312;&#20110;&#20234;&#26391;&#22899;&#24615;&#20197;&#20027;&#21160;&#30340;&#35282;&#33394;&#21442;&#19982;&#22312;&#26500;&#24314;&#36825;&#20010;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#30340;&#26631;&#27880;&#12290;&#25105;&#20204;&#30340;&#26631;&#27880;&#32773;&#19981;&#20165;&#25552;&#20379;&#26631;&#31614;&#65292;&#36824;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#20851;&#38190;&#35789;&#20197;&#36827;&#34892;&#26356;&#26377;&#24847;&#20041;&#30340;&#35821;&#26009;&#24211;&#21019;&#24314;&#65292;&#24182;&#25552;&#20379;&#31616;&#30701;&#30340;&#31034;&#20363;&#25991;&#26723;&#20197;&#36827;&#34892;&#24341;&#23548;&#24335;&#25277;&#26679;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#39532;&#33678;&#183;&#38463;&#31859;&#23612;&#30340;&#27515;&#20129;&#24341;&#21457;&#20102;&#26497;&#21270;&#30340;&#27874;&#26031;&#35821;&#35805;&#35821;&#65292;&#23545;&#24615;&#21035;&#24179;&#31561;&#30340;&#36127;&#38754;&#21644;&#31215;&#26497;&#25512;&#25991;&#25968;&#37327;&#37117;&#22686;&#21152;&#20102;&#12290;&#31215;&#26497;&#25512;&#25991;&#30340;&#22686;&#21152;&#30053;&#22823;&#20110;&#36127;&#38754;&#25512;&#25991;&#30340;&#22686;&#21152;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#23601;&#36134;&#21495;&#21019;&#24314;&#26102;&#38388;&#32780;&#35328;&#65292;&#19982;&#25919;&#24220;&#23545;&#40784;&#30340;Twitter&#36134;&#21495;&#21644;&#25903;&#25345;&#25239;&#35758;&#30340;Twitter&#36134;&#21495;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a computational analysis of the Persian language Twitter discourse with the aim to estimate the shift in stance toward gender equality following the death of Mahsa Amini in police custody. We present an ensemble active learning pipeline to train a stance classifier. Our novelty lies in the involvement of Iranian women in an active role as annotators in building this AI system. Our annotators not only provide labels, but they also suggest valuable keywords for more meaningful corpus creation as well as provide short example documents for a guided sampling step. Our analyses indicate that Mahsa Amini's death triggered polarized Persian language discourse where both fractions of negative and positive tweets toward gender equality increased. The increase in positive tweets was slightly greater than the increase in negative tweets. We also observe that with respect to account creation time, between the state-aligned Twitter accounts and pro-protest Twitter accounts
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#21512;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#24182;&#25351;&#20986;&#20102;&#20854;&#35780;&#20272;&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#65307;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#24212;&#35813;&#20855;&#22791;&#30340;&#33021;&#21147;&#65292;&#24182;&#35752;&#35770;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#32570;&#22833;&#20043;&#22788;&#65292;&#21363;&#30693;&#19982;&#34892;&#30340;&#32479;&#19968;&#12290;</title><link>http://arxiv.org/abs/2307.03762</link><description>&lt;p&gt;
&#36142;&#23384;&#22312;&#35745;&#31639;&#26426;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#32570;&#22833;&#20043;&#22788;
&lt;/p&gt;
&lt;p&gt;
Brain in a Vat: On Missing Pieces Towards Artificial General Intelligence in Large Language Models. (arXiv:2307.03762v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03762
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#21512;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#24182;&#25351;&#20986;&#20102;&#20854;&#35780;&#20272;&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#65307;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#24212;&#35813;&#20855;&#22791;&#30340;&#33021;&#21147;&#65292;&#24182;&#35752;&#35770;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#32570;&#22833;&#20043;&#22788;&#65292;&#21363;&#30693;&#19982;&#34892;&#30340;&#32479;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35266;&#28857;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#32508;&#21512;&#35780;&#20272;&#20102;&#20351;&#29992;&#26631;&#20934;&#21270;&#27979;&#35797;&#21644;&#20197;&#33021;&#21147;&#20026;&#23548;&#21521;&#30340;&#22522;&#20934;&#27979;&#35797;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#29616;&#26377;&#35780;&#20272;&#12290;&#25105;&#20204;&#25351;&#20986;&#20102;&#24403;&#21069;&#35780;&#20272;&#26041;&#27861;&#23384;&#22312;&#30340;&#20960;&#20010;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#22840;&#22823;&#20102;LLMs&#30340;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#24212;&#35813;&#20855;&#22791;&#30340;&#33021;&#21147;&#65292;&#36229;&#36234;&#20102;LLMs&#30340;&#33021;&#21147;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26234;&#33021;&#20195;&#29702;&#30340;&#22235;&#20010;&#29305;&#24449;&#65306;1&#65289;&#20182;&#20204;&#21487;&#20197;&#25191;&#34892;&#26080;&#38480;&#30340;&#20219;&#21153;&#65307;2&#65289;&#20182;&#20204;&#21487;&#20197;&#22312;&#29305;&#23450;&#29615;&#22659;&#20013;&#29983;&#25104;&#26032;&#30340;&#20219;&#21153;&#65307;3&#65289;&#20182;&#20204;&#22522;&#20110;&#25903;&#25745;&#20219;&#21153;&#29983;&#25104;&#30340;&#20215;&#20540;&#20307;&#31995;&#36827;&#34892;&#25805;&#20316;&#65307;4&#65289;&#20182;&#20204;&#25317;&#26377;&#21453;&#26144;&#29616;&#23454;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#36825;&#24433;&#21709;&#30528;&#20182;&#20204;&#19982;&#19990;&#30028;&#30340;&#20114;&#21160;&#12290;&#22312;&#27492;&#35266;&#28857;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#32570;&#22833;&#20043;&#22788;&#65292;&#21363;&#30693;&#19982;&#34892;&#30340;&#32479;&#19968;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19982;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#29289;&#20307;&#31215;&#26497;&#20114;&#21160;&#21487;&#20197;&#20026;&#24418;&#25104;&#27010;&#24565;&#24615;&#34920;&#31034;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this perspective paper, we first comprehensively review existing evaluations of Large Language Models (LLMs) using both standardized tests and ability-oriented benchmarks. We pinpoint several problems with current evaluation methods that tend to overstate the capabilities of LLMs. We then articulate what artificial general intelligence should encompass beyond the capabilities of LLMs. We propose four characteristics of generally intelligent agents: 1) they can perform unlimited tasks; 2) they can generate new tasks within a context; 3) they operate based on a value system that underpins task generation; and 4) they have a world model reflecting reality, which shapes their interaction with the world. Building on this viewpoint, we highlight the missing pieces in artificial general intelligence, that is, the unity of knowing and acting. We argue that active engagement with objects in the real world delivers more robust signals for forming conceptual representations. Additionally, know
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#31354;&#30333;&#65292;&#36890;&#36807;&#30740;&#31350;11&#31181;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;surprisal&#19982;&#38405;&#35835;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#27979;&#35797;&#20102;Surprisal&#29702;&#35770;&#30340;&#19977;&#20010;&#39044;&#27979;&#65292;&#24182;&#21457;&#29616;&#20102;&#20854;&#20182;&#35821;&#35328;&#29305;&#24449;&#23545;&#38405;&#35835;&#26102;&#38388;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.03667</link><description>&lt;p&gt;
&#22312;11&#31181;&#35821;&#35328;&#20013;&#27979;&#35797;Surprisal&#29702;&#35770;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Testing the Predictions of Surprisal Theory in 11 Languages. (arXiv:2307.03667v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#31354;&#30333;&#65292;&#36890;&#36807;&#30740;&#31350;11&#31181;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;surprisal&#19982;&#38405;&#35835;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#27979;&#35797;&#20102;Surprisal&#29702;&#35770;&#30340;&#19977;&#20010;&#39044;&#27979;&#65292;&#24182;&#21457;&#29616;&#20102;&#20854;&#20182;&#35821;&#35328;&#29305;&#24449;&#23545;&#38405;&#35835;&#26102;&#38388;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#29702;&#35821;&#35328;&#23398;&#30340;&#19968;&#20010;&#22522;&#26412;&#32467;&#26524;&#26159;&#65292;&#21487;&#39044;&#27979;&#24615;&#36739;&#20302;&#30340;&#35789;&#35821;&#38656;&#35201;&#26356;&#38271;&#26102;&#38388;&#26469;&#22788;&#29702;&#12290;Surprisal&#29702;&#35770;&#65288;Hale, 2001; Levy, 2008&#65289;&#26159;&#23545;&#36825;&#19968;&#21457;&#29616;&#30340;&#19968;&#20010;&#29702;&#35770;&#35299;&#37322;&#65292;&#23427;&#23558;&#19968;&#20010;&#35789;&#30340;&#21487;&#39044;&#27979;&#24615;&#37327;&#21270;&#20026;&#20854;surprisal&#65292;&#21363;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#65292;&#20854;&#36127;&#23545;&#25968;&#27010;&#29575;&#12290;&#34429;&#28982;&#26377;&#22823;&#37327;&#30340;&#35777;&#25454;&#25903;&#25345;Surprisal&#29702;&#35770;&#30340;&#39044;&#27979;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#19968;&#20010;&#38750;&#24120;&#26377;&#38480;&#30340;&#25968;&#25454;&#33539;&#22260;&#20869;&#65292;&#21363;&#20197;&#33521;&#35821;&#20026;&#27597;&#35821;&#30340;&#20154;&#38405;&#35835;&#33521;&#35821;&#25991;&#26412;&#12290;&#20107;&#23454;&#19978;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#20840;&#38754;&#30340;&#22810;&#35821;&#35328;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#22312;&#20116;&#20010;&#35821;&#35328;&#23478;&#26063;&#20013;&#20998;&#24067;&#30340;&#21313;&#19968;&#31181;&#19981;&#21516;&#35821;&#35328;&#20013;surprisal&#19982;&#38405;&#35835;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#22635;&#34917;&#24403;&#21069;&#25991;&#29486;&#20013;&#30340;&#36825;&#19968;&#31354;&#30333;&#12290;&#36890;&#36807;&#20174;&#21333;&#35821;&#21644;&#22810;&#35821;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#25512;&#23548;&#20272;&#35745;&#20540;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#19982;surprisal&#29702;&#35770;&#30456;&#20851;&#30340;&#19977;&#20010;&#39044;&#27979;&#65306;(i) surprisal&#26159;&#21542;&#33021;&#22815;&#39044;&#27979;&#38405;&#35835;&#26102;&#38388;&#65307;(ii) &#39044;&#26399;surprisal&#65292;&#21363;&#19978;&#19979;&#25991;&#29109;&#65292;&#26159;&#21542;&#24433;&#21709;&#38405;&#35835;&#26102;&#38388;&#65307;(iii) &#19982;surprisal&#30456;&#20851;&#30340;&#20854;&#20182;&#35821;&#35328;&#29305;&#24449;&#26159;&#21542;&#21487;&#20197;&#35299;&#37322;&#38405;&#35835;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental result in psycholinguistics is that less predictable words take a longer time to process. One theoretical explanation for this finding is Surprisal Theory (Hale, 2001; Levy, 2008), which quantifies a word's predictability as its surprisal, i.e. its negative log-probability given a context. While evidence supporting the predictions of Surprisal Theory have been replicated widely, most have focused on a very narrow slice of data: native English speakers reading English texts. Indeed, no comprehensive multilingual analysis exists. We address this gap in the current literature by investigating the relationship between surprisal and reading times in eleven different languages, distributed across five language families. Deriving estimates from language models trained on monolingual and multilingual corpora, we test three predictions associated with surprisal theory: (i) whether surprisal is predictive of reading times; (ii) whether expected surprisal, i.e. contextual entropy, i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26368;&#23567;&#39118;&#38505;&#35757;&#32451;&#31995;&#32479;&#24615;&#22320;&#20998;&#26512;&#21644;&#27604;&#36739;&#20102;&#21508;&#31181;&#33258;&#21160;&#24230;&#37327;&#65292;&#24182;&#21457;&#29616;&#20102;BLEURT&#21644;BARTScore&#31561;&#24230;&#37327;&#20013;&#23384;&#22312;&#30340;&#36890;&#29992;&#23545;&#25239;&#32763;&#35793;&#29616;&#35937;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#40065;&#26834;&#24615;&#32570;&#38519;&#20027;&#35201;&#30001;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#20998;&#24067;&#20559;&#24046;&#21644;&#24230;&#37327;&#33539;&#24335;&#30340;&#20542;&#21521;&#24341;&#36215;&#12290;&#36890;&#36807;&#24341;&#20837;&#26631;&#35760;&#32423;&#32422;&#26463;&#65292;&#21487;&#20197;&#25552;&#39640;&#24230;&#37327;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03131</link><description>&lt;p&gt;
BLEURT&#20855;&#26377;&#36890;&#29992;&#32763;&#35793;&#33021;&#21147;&#65306;&#22522;&#20110;&#26368;&#23567;&#39118;&#38505;&#35757;&#32451;&#30340;&#33258;&#21160;&#24230;&#37327;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
BLEURT Has Universal Translations: An Analysis of Automatic Metrics by Minimum Risk Training. (arXiv:2307.03131v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26368;&#23567;&#39118;&#38505;&#35757;&#32451;&#31995;&#32479;&#24615;&#22320;&#20998;&#26512;&#21644;&#27604;&#36739;&#20102;&#21508;&#31181;&#33258;&#21160;&#24230;&#37327;&#65292;&#24182;&#21457;&#29616;&#20102;BLEURT&#21644;BARTScore&#31561;&#24230;&#37327;&#20013;&#23384;&#22312;&#30340;&#36890;&#29992;&#23545;&#25239;&#32763;&#35793;&#29616;&#35937;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#40065;&#26834;&#24615;&#32570;&#38519;&#20027;&#35201;&#30001;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#20998;&#24067;&#20559;&#24046;&#21644;&#24230;&#37327;&#33539;&#24335;&#30340;&#20542;&#21521;&#24341;&#36215;&#12290;&#36890;&#36807;&#24341;&#20837;&#26631;&#35760;&#32423;&#32422;&#26463;&#65292;&#21487;&#20197;&#25552;&#39640;&#24230;&#37327;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24230;&#37327;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;n-gram&#24230;&#37327;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#26368;&#36817;&#20986;&#29616;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24230;&#37327;&#30340;&#21457;&#23637;&#28526;&#27969;&#65292;&#37325;&#28857;&#22312;&#20110;&#27979;&#37327;&#21477;&#23376;&#35821;&#20041;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31070;&#32463;&#24230;&#37327;&#34429;&#28982;&#19982;&#20154;&#24037;&#35780;&#20272;&#30456;&#20851;&#24615;&#26356;&#39640;&#65292;&#20294;&#24120;&#24120;&#34987;&#35748;&#20026;&#26159;&#24102;&#26377;&#28508;&#22312;&#20559;&#35265;&#19988;&#38590;&#20197;&#26816;&#27979;&#30340;&#40657;&#30418;&#23376;&#12290;&#26412;&#30740;&#31350;&#20174;&#35757;&#32451;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#25351;&#23548;&#35282;&#24230;&#65292;&#31995;&#32479;&#20998;&#26512;&#21644;&#27604;&#36739;&#20102;&#22810;&#31181;&#20027;&#27969;&#21644;&#21069;&#27839;&#30340;&#33258;&#21160;&#24230;&#37327;&#12290;&#36890;&#36807;&#26368;&#23567;&#39118;&#38505;&#35757;&#32451;&#65288;MRT&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;&#26576;&#20123;&#24230;&#37327;&#23384;&#22312;&#40065;&#26834;&#24615;&#32570;&#38519;&#65292;&#20363;&#22914;BLEURT&#21644;BARTScore&#20013;&#23384;&#22312;&#36890;&#29992;&#23545;&#25239;&#32763;&#35793;&#29616;&#35937;&#12290;&#28145;&#20837;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#20123;&#40065;&#26834;&#24615;&#32570;&#38519;&#20027;&#35201;&#26377;&#20004;&#20010;&#21407;&#22240;&#65306;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#20998;&#24067;&#20559;&#24046;&#21644;&#24230;&#37327;&#33539;&#24335;&#30340;&#20542;&#21521;&#12290;&#36890;&#36807;&#24341;&#20837;&#26631;&#35760;&#32423;&#32422;&#26463;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#24230;&#37327;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic metrics play a crucial role in machine translation. Despite the widespread use of n-gram-based metrics, there has been a recent surge in the development of pre-trained model-based metrics that focus on measuring sentence semantics. However, these neural metrics, while achieving higher correlations with human evaluations, are often considered to be black boxes with potential biases that are difficult to detect. In this study, we systematically analyze and compare various mainstream and cutting-edge automatic metrics from the perspective of their guidance for training machine translation systems. Through Minimum Risk Training (MRT), we find that certain metrics exhibit robustness defects, such as the presence of universal adversarial translations in BLEURT and BARTScore. In-depth analysis suggests two main causes of these robustness deficits: distribution biases in the training datasets, and the tendency of the metric paradigm. By incorporating token-level constraints, we enhan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#35299;&#37322;&#20013;&#65292;&#27867;&#21270;&#20102;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#29702;&#35299;&#26799;&#24230;&#22270;&#30340;&#21487;&#35299;&#37322;&#32479;&#35745;&#25968;&#25454;&#65292;&#22914;&#26368;&#39640;&#21152;&#26435;&#36335;&#24452;&#21644;&#29109;&#12290;&#20316;&#32773;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#21644;&#24212;&#29992;&#20110;BERT&#30340;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03056</link><description>&lt;p&gt;
&#27867;&#21270;&#21453;&#21521;&#20256;&#25773;&#29992;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generalizing Backpropagation for Gradient-Based Interpretability. (arXiv:2307.03056v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#35299;&#37322;&#20013;&#65292;&#27867;&#21270;&#20102;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#29702;&#35299;&#26799;&#24230;&#22270;&#30340;&#21487;&#35299;&#37322;&#32479;&#35745;&#25968;&#25454;&#65292;&#22914;&#26368;&#39640;&#21152;&#26435;&#36335;&#24452;&#21644;&#29109;&#12290;&#20316;&#32773;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#21644;&#24212;&#29992;&#20110;BERT&#30340;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#34892;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20381;&#36182;&#20110;&#35745;&#31639;&#27169;&#22411;&#36755;&#20986;&#23545;&#36755;&#20837;&#30340;&#26799;&#24230;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#25351;&#31034;&#21738;&#20123;&#36755;&#20837;&#29305;&#24449;&#21487;&#33021;&#23545;&#27169;&#22411;&#30340;&#39044;&#27979;&#24456;&#37325;&#35201;&#65292;&#20294;&#23427;&#20204;&#23545;&#27169;&#22411;&#26412;&#36523;&#30340;&#20869;&#37096;&#24037;&#20316;&#20102;&#35299;&#29978;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#30340;&#26799;&#24230;&#35745;&#31639;&#26159;&#20351;&#29992;&#21322;&#29615;&#30340;&#26356;&#19968;&#33324;&#24418;&#24335;&#30340;&#29305;&#20363;&#12290;&#36825;&#31181;&#35266;&#23519;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#27867;&#21270;&#65292;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#26799;&#24230;&#22270;&#30340;&#20854;&#20182;&#21487;&#35299;&#37322;&#32479;&#35745;&#25968;&#25454;&#65292;&#20363;&#22914;&#26368;&#39640;&#21152;&#26435;&#36335;&#24452;&#21644;&#29109;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#20010;&#27867;&#21270;&#31639;&#27861;&#65292;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#23427;&#35745;&#31639;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#30740;&#31350;BERT&#22312;&#20027;&#35859;&#25968;&#19968;&#33268;&#24615;&#20219;&#21153;&#65288;SVA&#65289;&#19978;&#30340;&#34892;&#20026;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#27169;&#22411;&#32452;&#20214;&#19978;&#36890;&#36807;&#30340;&#26799;&#24230;&#27969;&#37327;&#21453;&#26144;&#20102;&#20854;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many popular feature-attribution methods for interpreting deep neural networks rely on computing the gradients of a model's output with respect to its inputs. While these methods can indicate which input features may be important for the model's prediction, they reveal little about the inner workings of the model itself. In this paper, we observe that the gradient computation of a model is a special case of a more general formulation using semirings. This observation allows us to generalize the backpropagation algorithm to efficiently compute other interpretable statistics about the gradient graph of a neural network, such as the highest-weighted path and entropy. We implement this generalized algorithm, evaluate it on synthetic datasets to better understand the statistics it computes, and apply it to study BERT's behavior on the subject-verb number agreement task (SVA). With this method, we (a) validate that the amount of gradient flow through a component of a model reflects its impor
&lt;/p&gt;</description></item><item><title>ValiTex&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#39564;&#35777;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#23398;&#32773;&#20204;&#22522;&#20110;&#25991;&#26412;&#25968;&#25454;&#26469;&#24230;&#37327;&#31038;&#20250;&#31185;&#23398;&#26500;&#24314;&#12290;&#23427;&#20511;&#37492;&#20102;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#20256;&#32479;&#65292;&#36890;&#36807;&#27010;&#24565;&#27169;&#22411;&#21644;&#21160;&#24577;&#26816;&#26597;&#34920;&#25552;&#20379;&#20102;&#39564;&#35777;&#30340;&#32467;&#26500;&#21644;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2307.02863</link><description>&lt;p&gt;
ValiTex -- &#19968;&#31181;&#29992;&#20110;&#35745;&#31639;&#25991;&#26412;&#30340;&#31038;&#20250;&#31185;&#23398;&#26500;&#24314;&#24230;&#37327;&#30340;&#32479;&#19968;&#39564;&#35777;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ValiTex -- a uniform validation framework for computational text-based measures of social science constructs. (arXiv:2307.02863v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02863
&lt;/p&gt;
&lt;p&gt;
ValiTex&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#39564;&#35777;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#23398;&#32773;&#20204;&#22522;&#20110;&#25991;&#26412;&#25968;&#25454;&#26469;&#24230;&#37327;&#31038;&#20250;&#31185;&#23398;&#26500;&#24314;&#12290;&#23427;&#20511;&#37492;&#20102;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#20256;&#32479;&#65292;&#36890;&#36807;&#27010;&#24565;&#27169;&#22411;&#21644;&#21160;&#24577;&#26816;&#26597;&#34920;&#25552;&#20379;&#20102;&#39564;&#35777;&#30340;&#32467;&#26500;&#21644;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22914;&#20309;&#39564;&#35777;&#35745;&#31639;&#25991;&#26412;&#30340;&#31038;&#20250;&#31185;&#23398;&#26500;&#24314;&#24230;&#37327;&#30340;&#25351;&#23548;&#26159;&#20998;&#25955;&#30340;&#12290;&#34429;&#28982;&#23398;&#32773;&#20204;&#26222;&#36941;&#35748;&#35782;&#21040;&#39564;&#35777;&#20182;&#20204;&#30340;&#25991;&#26412;&#24230;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#20182;&#20204;&#36890;&#24120;&#32570;&#20047;&#20849;&#21516;&#30340;&#26415;&#35821;&#21644;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#36827;&#34892;&#39564;&#35777;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;ValiTex&#30340;&#26032;&#39564;&#35777;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#23398;&#32773;&#20204;&#22522;&#20110;&#25991;&#26412;&#25968;&#25454;&#26469;&#24230;&#37327;&#31038;&#20250;&#31185;&#23398;&#26500;&#24314;&#12290;&#35813;&#26694;&#26550;&#20511;&#37492;&#20102;&#24515;&#29702;&#27979;&#37327;&#23398;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#20256;&#32479;&#65292;&#21516;&#26102;&#25193;&#23637;&#20102;&#26694;&#26550;&#20197;&#36866;&#29992;&#20110;&#35745;&#31639;&#25991;&#26412;&#20998;&#26512;&#30340;&#30446;&#30340;&#12290;ValiTex&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#19968;&#20010;&#26159;&#27010;&#24565;&#27169;&#22411;&#65292;&#19968;&#20010;&#26159;&#21160;&#24577;&#26816;&#26597;&#34920;&#12290;&#27010;&#24565;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#32467;&#26500;&#65292;&#21487;&#20197;&#25351;&#23548;&#39564;&#35777;&#30340;&#19981;&#21516;&#38454;&#27573;&#65292;&#21160;&#24577;&#26816;&#26597;&#34920;&#23450;&#20041;&#20102;&#20855;&#20307;&#30340;&#39564;&#35777;&#27493;&#39588;&#65292;&#24182;&#25552;&#20379;&#20102;&#21738;&#20123;&#27493;&#39588;&#21487;&#33021;&#34987;&#35748;&#20026;&#26159;&#25512;&#33616;&#30340;&#65288;&#21363;&#25552;&#20379;&#30456;&#20851;&#21644;&#24517;&#35201;&#30340;&#39564;&#35777;&#35777;&#25454;&#65289;&#25110;&#21487;&#36873;&#30340;&#65288;&#21363;&#23545;&#25552;&#20379;&#39069;&#22806;&#20449;&#24687;&#26377;&#29992;&#30340;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Guidance on how to validate computational text-based measures of social science constructs is fragmented. Whereas scholars are generally acknowledging the importance of validating their text-based measures, they often lack common terminology and a unified framework to do so. This paper introduces a new validation framework called ValiTex, designed to assist scholars to measure social science constructs based on textual data. The framework draws on a long-established tradition within psychometrics while extending the framework for the purpose of computational text analysis. ValiTex consists of two components, a conceptual model, and a dynamic checklist. Whereas the conceptual model provides a general structure along distinct phases on how to approach validation, the dynamic checklist defines specific validation steps and provides guidance on which steps might be considered recommendable (i.e., providing relevant and necessary validation evidence) or optional (i.e., useful for providing 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#26410;&#33021;&#25104;&#21151;&#23398;&#20064;&#21040;&#25991;&#21270;&#36866;&#23452;&#24773;&#32490;&#30340;&#24494;&#22937;&#24046;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#32416;&#27491;&#36825;&#19968;&#38382;&#39064;&#30340;&#21487;&#33021;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.01370</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#24182;&#38750;&#22810;&#20803;&#25991;&#21270;&#30340;: &#20197;&#24773;&#32490;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multilingual Language Models are not Multicultural: A Case Study in Emotion. (arXiv:2307.01370v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01370
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#26410;&#33021;&#25104;&#21151;&#23398;&#20064;&#21040;&#25991;&#21270;&#36866;&#23452;&#24773;&#32490;&#30340;&#24494;&#22937;&#24046;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#32416;&#27491;&#36825;&#19968;&#38382;&#39064;&#30340;&#21487;&#33021;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#22312;&#19990;&#30028;&#21508;&#22320;&#30340;&#32463;&#21382;&#21644;&#34920;&#36798;&#26041;&#24335;&#19981;&#21516;&#12290;&#20026;&#20102;&#22312;&#38656;&#35201;&#24773;&#24863;&#25935;&#24863;&#24615;&#30340;&#22810;&#35821;&#35328;&#20219;&#21153;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LMs)&#65292;LMs&#24517;&#39035;&#21453;&#26144;&#24773;&#32490;&#19978;&#30340;&#25991;&#21270;&#24046;&#24322;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;2023&#24180;&#24191;&#27867;&#20351;&#29992;&#30340;&#22810;&#35821;&#35328;LMs&#26159;&#21542;&#21453;&#26144;&#20102;&#36328;&#25991;&#21270;&#21644;&#36328;&#35821;&#35328;&#24773;&#24863;&#34920;&#36798;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#20174;LMs(&#22914;XLM-RoBERTa)&#33719;&#24471;&#30340;&#23884;&#20837;&#26159;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#65292;&#29983;&#25104;&#22411;LMs(&#22914;ChatGPT)&#22312;&#22238;&#24212;&#20854;&#20182;&#35821;&#35328;&#30340;&#25552;&#31034;&#26102;&#20063;&#20307;&#29616;&#20102;&#35199;&#26041;&#35268;&#33539;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#35821;&#35328;LMs&#24182;&#27809;&#26377;&#25104;&#21151;&#22320;&#23398;&#20064;&#25991;&#21270;&#19978;&#36866;&#24403;&#30340;&#24773;&#32490;&#32454;&#24494;&#24046;&#21035;&#65292;&#24182;&#24378;&#35843;&#20102;&#21487;&#33021;&#30340;&#30740;&#31350;&#26041;&#21521;&#20197;&#32416;&#27491;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotions are experienced and expressed differently across the world. In order to use Large Language Models (LMs) for multilingual tasks that require emotional sensitivity, LMs must reflect this cultural variation in emotion. In this study, we investigate whether the widely-used multilingual LMs in 2023 reflect differences in emotional expressions across cultures and languages. We find that embeddings obtained from LMs (e.g., XLM-RoBERTa) are Anglocentric, and generative LMs (e.g., ChatGPT) reflect Western norms, even when responding to prompts in other languages. Our results show that multilingual LMs do not successfully learn the culturally appropriate nuances of emotion and we highlight possible research directions towards correcting this.
&lt;/p&gt;</description></item><item><title>vONTSS&#26159;&#19968;&#31181;&#22522;&#20110;vMF&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#23427;&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#25903;&#25345;&#26080;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#27604;&#26368;&#36817;&#30340;NTM&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2307.01226</link><description>&lt;p&gt;
vONTSS&#65306;&#22522;&#20110;vMF&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
vONTSS: vMF based semi-supervised neural topic modeling with optimal transport. (arXiv:2307.01226v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01226
&lt;/p&gt;
&lt;p&gt;
vONTSS&#26159;&#19968;&#31181;&#22522;&#20110;vMF&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#23427;&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#25903;&#25345;&#26080;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#27604;&#26368;&#36817;&#30340;NTM&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21463;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21551;&#21457;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;NTM&#65289;&#24341;&#36215;&#20102;&#24456;&#22810;&#30740;&#31350;&#20852;&#36259;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#25972;&#21512;&#20154;&#31867;&#30693;&#35782;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;vONTSS&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;von Mises-Fisher&#65288;vMF&#65289;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#26368;&#20248;&#20256;&#36755;&#12290;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#20013;&#65292;&#24403;&#25552;&#20379;&#27599;&#20010;&#20027;&#39064;&#30340;&#23569;&#37327;&#20851;&#38190;&#35789;&#26102;&#65292;vONTSS&#29983;&#25104;&#28508;&#22312;&#20027;&#39064;&#24182;&#20248;&#21270;&#20027;&#39064;-&#20851;&#38190;&#35789;&#36136;&#37327;&#21644;&#20027;&#39064;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#21322;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;vONTSS&#36824;&#25903;&#25345;&#26080;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#12290;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#22312;&#22810;&#20010;&#26041;&#38754;&#20248;&#20110;&#26368;&#36817;&#30340;NTM&#65306;vONTSS&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#39640;&#24230;&#32858;&#31867;&#21644;&#36830;&#36143;&#30340;&#20027;&#39064;&#12290;&#23427;&#20063;&#27604;&#29616;&#26377;-&#25163;&#27861;&#24555;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Neural Topic Models (NTM), inspired by variational autoencoders, have attracted a lot of research interest; however, these methods have limited applications in the real world due to the challenge of incorporating human knowledge. This work presents a semi-supervised neural topic modeling method, vONTSS, which uses von Mises-Fisher (vMF) based variational autoencoders and optimal transport. When a few keywords per topic are provided, vONTSS in the semi-supervised setting generates potential topics and optimizes topic-keyword quality and topic classification. Experiments show that vONTSS outperforms existing semi-supervised topic modeling methods in classification accuracy and diversity. vONTSS also supports unsupervised topic modeling. Quantitative and qualitative experiments show that vONTSS in the unsupervised setting outperforms recent NTMs on multiple aspects: vONTSS discovers highly clustered and coherent topics on benchmark datasets. It is also much faster than the state
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#26032;&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;GenRec&#65292;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#25512;&#33616;&#39033;&#32780;&#19981;&#26159;&#35745;&#31639;&#25490;&#21517;&#20998;&#25968;&#65292;&#21033;&#29992;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#29702;&#35299;&#33021;&#21147;&#26469;&#29983;&#25104;&#30456;&#20851;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2307.00457</link><description>&lt;p&gt;
GenRec:&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#24335;&#25512;&#33616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
GenRec: Large Language Model for Generative Recommendation. (arXiv:2307.00457v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#26032;&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;GenRec&#65292;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#25512;&#33616;&#39033;&#32780;&#19981;&#26159;&#35745;&#31639;&#25490;&#21517;&#20998;&#25968;&#65292;&#21033;&#29992;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#29702;&#35299;&#33021;&#21147;&#26469;&#29983;&#25104;&#30456;&#20851;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Large Language Model&#65292;LLM)&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#25104;&#24335;&#25512;&#33616;&#33539;&#24335;&#19979;&#65292;&#23427;&#20204;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#25991;&#26412;&#25968;&#25454;&#30340;&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26469;&#36827;&#34892;&#25512;&#33616;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#33616;&#31995;&#32479;(GenRec)&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#25512;&#33616;&#39033;&#65292;&#32780;&#19981;&#26159;&#20687;&#20256;&#32479;&#30340;&#21028;&#21035;&#24335;&#25512;&#33616;&#31995;&#32479;&#19968;&#26679;&#36880;&#20010;&#35745;&#31639;&#27599;&#20010;&#20505;&#36873;&#39033;&#30340;&#25490;&#21517;&#20998;&#25968;&#12290;GenRec&#21033;&#29992;LLM&#30340;&#29702;&#35299;&#33021;&#21147;&#26469;&#35299;&#37322;&#19978;&#19979;&#25991;&#12289;&#23398;&#20064;&#29992;&#25143;&#20559;&#22909;&#24182;&#29983;&#25104;&#30456;&#20851;&#25512;&#33616;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#20016;&#23500;&#30693;&#35782;&#26469;&#23436;&#25104;&#25512;&#33616;&#20219;&#21153;&#12290;&#25105;&#20204;&#39318;&#20808;&#21046;&#23450;&#20102;&#19987;&#38376;&#30340;&#25552;&#31034;&#65292;&#20197;&#22686;&#24378;LLM&#29702;&#35299;&#25512;&#33616;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, large language models (LLM) have emerged as powerful tools for diverse natural language processing tasks. However, their potential for recommender systems under the generative recommendation paradigm remains relatively unexplored. This paper presents an innovative approach to recommendation systems using large language models (LLMs) based on text data. In this paper, we present a novel LLM for generative recommendation (GenRec) that utilized the expressive power of LLM to directly generate the target item to recommend, rather than calculating ranking score for each candidate item one by one as in traditional discriminative recommendation. GenRec uses LLM's understanding ability to interpret context, learn user preferences, and generate relevant recommendation. Our proposed approach leverages the vast knowledge encoded in large language models to accomplish recommendation tasks. We first we formulate specialized prompts to enhance the ability of LLM to comprehend recomm
&lt;/p&gt;</description></item><item><title>&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#23545;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#30340;&#26631;&#35760;&#20998;&#21106;&#26041;&#24335;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36825;&#23545;&#20110;&#25913;&#36827;&#19979;&#28216;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2306.17649</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#23545;&#19981;&#29702;&#24819;&#30340;&#26631;&#35760;&#20998;&#21106;&#26041;&#24335;&#20855;&#26377;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Biomedical Language Models are Robust to Sub-optimal Tokenization. (arXiv:2306.17649v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17649
&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#23545;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#30340;&#26631;&#35760;&#20998;&#21106;&#26041;&#24335;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36825;&#23545;&#20110;&#25913;&#36827;&#19979;&#28216;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#19968;&#33324;&#30340;&#33521;&#35821;&#30456;&#21453;&#65292;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#20013;&#30340;&#35768;&#22810;&#27010;&#24565;&#26159;&#30001;&#29983;&#29289;&#21307;&#23398;&#19987;&#19994;&#20154;&#21592;&#35774;&#35745;&#30340;&#65292;&#30446;&#30340;&#26159;&#35201;&#31934;&#30830;&#19988;&#31616;&#26126;&#12290;&#36890;&#24120;&#36890;&#36807;&#23558;&#26377;&#24847;&#20041;&#30340;&#29983;&#29289;&#21307;&#23398;&#35789;&#32032;&#36830;&#25509;&#36215;&#26469;&#21019;&#24314;&#26032;&#30340;&#35821;&#20041;&#21333;&#20301;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#20195;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411; (LMs) &#26159;&#20351;&#29992;&#20174;&#22823;&#35268;&#27169;&#29983;&#29289;&#21307;&#23398;&#35821;&#26009;&#24211;&#32479;&#35745;&#20013;&#23548;&#20986;&#30340;&#26631;&#20934;&#39046;&#22495;&#29305;&#23450;&#26631;&#35760;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#65292;&#32780;&#27809;&#26377;&#26126;&#30830;&#21033;&#29992;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#30340;&#31896;&#38468;&#24615;&#29305;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#21457;&#29616;&#26631;&#20934;&#36890;&#29992;&#39046;&#22495;&#21644;&#29983;&#29289;&#21307;&#23398;&#26631;&#35760;&#22120;&#22312;&#23558;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#20998;&#21106;&#25104;&#26377;&#24847;&#20041;&#30340;&#32452;&#25104;&#37096;&#20998;&#26041;&#38754;&#33021;&#21147;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20551;&#35774;&#20351;&#29992;&#19968;&#31181;&#26356;&#20934;&#30830;&#20998;&#21106;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#30340;&#26631;&#35760;&#22120;&#23558;&#20351;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#28041;&#21450;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#30340;&#20219;&#21153;&#65292;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035; (NER) &#21644;&#23454;&#20307;&#38142;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
As opposed to general English, many concepts in biomedical terminology have been designed in recent history by biomedical professionals with the goal of being precise and concise. This is often achieved by concatenating meaningful biomedical morphemes to create new semantic units. Nevertheless, most modern biomedical language models (LMs) are pre-trained using standard domain-specific tokenizers derived from large scale biomedical corpus statistics without explicitly leveraging the agglutinating nature of biomedical language. In this work, we first find that standard open-domain and biomedical tokenizers are largely unable to segment biomedical terms into meaningful components. Therefore, we hypothesize that using a tokenizer which segments biomedical terminology more accurately would enable biomedical LMs to improve their performance on downstream biomedical NLP tasks, especially ones which involve biomedical terms directly such as named entity recognition (NER) and entity linking. Su
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#29983;&#25104;&#36830;&#32493;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#30340;&#26041;&#27861;&#65288;TESGAN&#65289;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;GAN&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#38480;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#30340;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#21462;&#20195;&#31163;&#25955;&#30340;&#26631;&#35760;&#65292;&#20351;&#24471;&#29983;&#25104;&#22120;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#26799;&#24230;&#26102;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2306.17181</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#26080;&#30417;&#30563;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#29992;&#20110;&#25991;&#26412;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis. (arXiv:2306.17181v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#29983;&#25104;&#36830;&#32493;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#30340;&#26041;&#27861;&#65288;TESGAN&#65289;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;GAN&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#38480;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#30340;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#21462;&#20195;&#31163;&#25955;&#30340;&#26631;&#35760;&#65292;&#20351;&#24471;&#29983;&#25104;&#22120;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#26799;&#24230;&#26102;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#21512;&#25104;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#30340;&#31454;&#20105;&#26469;&#21019;&#24314;&#36924;&#30495;&#30340;&#25968;&#25454;&#12290;&#23613;&#31649;GAN&#22312;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#23384;&#22312;&#22266;&#26377;&#30340;&#38480;&#21046;&#12290;&#22240;&#20026;&#33258;&#28982;&#35821;&#35328;&#30001;&#31163;&#25955;&#30340;&#26631;&#35760;&#32452;&#25104;&#65292;&#29983;&#25104;&#22120;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#26799;&#24230;&#26102;&#36935;&#21040;&#22256;&#38590;&#65307;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#25991;&#26412;-GAN&#30740;&#31350;&#20351;&#29992;&#22870;&#21169;&#31995;&#32479;&#20197;&#38543;&#26426;&#26631;&#35760;&#20026;&#22522;&#30784;&#29983;&#25104;&#21477;&#23376;&#12290;&#22240;&#27492;&#65292;&#20808;&#21069;&#30740;&#31350;&#20013;&#30340;&#29983;&#25104;&#22120;&#22312;&#23545;&#25239;&#35757;&#32451;&#20043;&#21069;&#20197;&#33258;&#22238;&#24402;&#26041;&#24335;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#23548;&#33268;&#21512;&#25104;&#30340;&#21477;&#23376;&#37325;&#22797;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#31867;&#20284;&#21407;&#22987;GAN&#30340;&#26694;&#26550;&#26469;&#21512;&#25104;&#21477;&#23376;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;TESGAN&#65289;&#65292;&#23427;&#29983;&#25104;&#36830;&#32493;&#30340;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#26469;&#35299;&#20915;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GAN) is a model for data synthesis, which creates plausible data through the competition of generator and discriminator. Although GAN application to image synthesis is extensively studied, it has inherent limitations to natural language generation. Because natural language is composed of discrete tokens, a generator has difficulty updating its gradient through backpropagation; therefore, most text-GAN studies generate sentences starting with a random token based on a reward system. Thus, the generators of previous studies are pre-trained in an autoregressive way before adversarial training, causing data memorization that synthesized sentences reproduce the training data. In this paper, we synthesize sentences using a framework similar to the original GAN. More specifically, we propose Text Embedding Space Generative Adversarial Networks (TESGAN) which generate continuous text embedding spaces instead of discrete tokens to solve the gradient backpropagat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#24037;&#20855;&#30340;&#21151;&#33021;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#26816;&#27979;&#24037;&#20855;&#26082;&#19981;&#20934;&#30830;&#20063;&#19981;&#21487;&#38752;&#12290;</title><link>http://arxiv.org/abs/2306.15666</link><description>&lt;p&gt;
AI-&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#30340;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Testing of Detection Tools for AI-Generated Text. (arXiv:2306.15666v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#24037;&#20855;&#30340;&#21151;&#33021;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#26816;&#27979;&#24037;&#20855;&#26082;&#19981;&#20934;&#30830;&#20063;&#19981;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#24378;&#35843;&#20102;&#22312;&#23398;&#26415;&#29615;&#22659;&#20013;&#19981;&#20844;&#24179;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#21152;&#24378;&#20102;&#23547;&#25214;&#26816;&#27979;&#27492;&#31867;&#20869;&#23481;&#35299;&#20915;&#26041;&#26696;&#30340;&#21162;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#24037;&#20855;&#30340;&#19968;&#33324;&#21151;&#33021;&#65292;&#24182;&#26681;&#25454;&#20934;&#30830;&#24615;&#21644;&#38169;&#35823;&#31867;&#22411;&#20998;&#26512;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;&#20197;&#19979;&#30740;&#31350;&#38382;&#39064;&#65306;&#29616;&#26377;&#30340;&#26816;&#27979;&#24037;&#20855;&#26159;&#21542;&#33021;&#21487;&#38752;&#22320;&#21306;&#20998;&#20154;&#31867;&#32534;&#20889;&#30340;&#25991;&#26412;&#21644;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#20197;&#21450;&#26426;&#22120;&#32763;&#35793;&#21644;&#20869;&#23481;&#28151;&#28102;&#25216;&#26415;&#26159;&#21542;&#24433;&#21709;&#23545;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#12290;&#30740;&#31350;&#28085;&#30422;&#20102;12&#31181;&#20844;&#24320;&#21487;&#29992;&#30340;&#24037;&#20855;&#21644;&#20004;&#20010;&#24191;&#27867;&#24212;&#29992;&#20110;&#23398;&#26415;&#29615;&#22659;&#20013;&#30340;&#21830;&#19994;&#31995;&#32479;&#65288;Turnitin&#21644;PlagiarismCheck&#65289;&#12290;&#30740;&#31350;&#20154;&#21592;&#24471;&#20986;&#32467;&#35770;&#65292;&#29616;&#26377;&#30340;&#26816;&#27979;&#24037;&#20855;&#26082;&#19981;&#20934;&#30830;&#20063;&#19981;&#21487;&#38752;&#65292;&#24182;&#19988;&#23384;&#22312;&#20027;&#35201;&#30340;&#21487;&#36776;&#35782;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in generative pre-trained transformer large language models have emphasised the potential risks of unfair use of artificial intelligence (AI) generated content in an academic environment and intensified efforts in searching for solutions to detect such content. The paper examines the general functionality of detection tools for artificial intelligence generated text and evaluates them based on accuracy and error type analysis. Specifically, the study seeks to answer research questions about whether existing detection tools can reliably differentiate between human-written text and ChatGPT-generated text, and whether machine translation and content obfuscation techniques affect the detection of AIgenerated text. The research covers 12 publicly available tools and two commercial systems (Turnitin and PlagiarismCheck) that are widely used in the academic setting. The researchers conclude that the available detection tools are neither accurate nor reliable and have a main bi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#31232;&#30095;&#27169;&#22359;&#28608;&#27963; (SMA) &#26426;&#21046;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#24207;&#21015;&#24314;&#27169;&#12290;&#36825;&#31181;&#26426;&#21046;&#21487;&#20197;&#21160;&#24577;&#22320;&#31232;&#30095;&#28608;&#27963;&#24207;&#21015;&#20803;&#32032;&#30340;&#23376;&#27169;&#22359;&#65292;&#20943;&#23569;&#35745;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2306.11197</link><description>&lt;p&gt;
&#31232;&#30095;&#27169;&#22359;&#28608;&#27963;&#29992;&#20110;&#39640;&#25928;&#30340;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Sparse Modular Activation for Efficient Sequence Modeling. (arXiv:2306.11197v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#31232;&#30095;&#27169;&#22359;&#28608;&#27963; (SMA) &#26426;&#21046;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#24207;&#21015;&#24314;&#27169;&#12290;&#36825;&#31181;&#26426;&#21046;&#21487;&#20197;&#21160;&#24577;&#22320;&#31232;&#30095;&#28608;&#27963;&#24207;&#21015;&#20803;&#32032;&#30340;&#23376;&#27169;&#22359;&#65292;&#20943;&#23569;&#35745;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411; (SSM) &#22312;&#21508;&#31181;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#24378;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#25928;&#22320;&#32534;&#30721;&#20102;&#24490;&#29615;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#22312;&#26356;&#32508;&#21512;&#30340;&#20219;&#21153;&#20013;&#65292;&#22914;&#35821;&#35328;&#24314;&#27169;&#21644;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#20173;&#28982;&#20248;&#20110;SSM&#12290;&#21516;&#26102;&#20351;&#29992;SSM&#21644;&#33258;&#27880;&#24847;&#21147;&#30340;&#28151;&#21512;&#27169;&#22411;&#36890;&#24120;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#20294;&#24403;&#21069;&#26041;&#27861;&#23558;&#27880;&#24847;&#21147;&#27169;&#22359;&#38745;&#24577;&#19988;&#22343;&#21248;&#22320;&#24212;&#29992;&#20110;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#25152;&#26377;&#20803;&#32032;&#65292;&#23548;&#33268;&#20102;&#36136;&#37327;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#27425;&#20248;&#26435;&#34913;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31232;&#30095;&#27169;&#22359;&#28608;&#27963; (SMA)&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#26426;&#21046;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#20197;&#21487;&#24494;&#20998;&#30340;&#26041;&#24335;&#31232;&#30095;&#22320;&#21160;&#24577;&#28608;&#27963;&#24207;&#21015;&#20803;&#32032;&#30340;&#23376;&#27169;&#22359;&#12290;&#36890;&#36807;&#20801;&#35768;&#27599;&#20010;&#20803;&#32032;&#36339;&#36807;&#38750;&#28608;&#27963;&#30340;&#23376;&#27169;&#22359;&#65292;SMA&#21487;&#20197;&#22312;&#24207;&#21015;&#24314;&#27169;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#38477;&#20302;&#35745;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#12290;&#20316;&#20026;SMA&#30340;&#19968;&#20010;&#29305;&#23450;&#23454;&#20363;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural a
&lt;/p&gt;</description></item><item><title>ChatGPT&#20351;&#29992;&#22312;&#36234;&#21335;&#39640;&#20013;&#27605;&#19994;&#32771;&#35797;&#20013;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#27700;&#24179;&#65292;&#23637;&#31034;&#20102;AI&#21160;&#21147;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.09170</link><description>&lt;p&gt;
ChatGPT &#33021;&#36890;&#36807;&#36234;&#21335;&#39640;&#20013;&#27605;&#19994;&#32771;&#35797;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT pass the Vietnamese National High School Graduation Examination?. (arXiv:2306.09170v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09170
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20351;&#29992;&#22312;&#36234;&#21335;&#39640;&#20013;&#27605;&#19994;&#32771;&#35797;&#20013;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#27700;&#24179;&#65292;&#23637;&#31034;&#20102;AI&#21160;&#21147;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25991;&#31456;&#24378;&#35843;&#20102; AI &#21160;&#21147;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#25945;&#32946;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#20171;&#32461;&#20102;&#20351;&#29992; ChatGPT&#65292;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23436;&#25104;&#36234;&#21335;&#39640;&#20013;&#27605;&#19994;&#32771;&#35797;&#65288;VNHSGE&#65289;&#30340;&#32467;&#26524;&#12290;&#30740;&#31350;&#25968;&#25454;&#38598;&#21253;&#25324;&#25991;&#23398;&#27979;&#35797;&#26696;&#20363;&#20013;&#30340; 30 &#31687;&#25991;&#31456;&#21644;&#35774;&#35745;&#32473;&#20854;&#20182;&#31185;&#30446;&#30340; 1,700 &#36947;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;&#32467;&#26524;&#26174;&#31034; ChatGPT &#33021;&#22815;&#36890;&#36807;&#32771;&#35797;&#65292;&#24182;&#24471;&#21040;&#20102;&#24179;&#22343; 6-7 &#20998;&#30340;&#25104;&#32489;&#65292;&#23637;&#31034;&#20102;&#36825;&#39033;&#25216;&#26415;&#38761;&#21629;&#25945;&#32946;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290; ChatGPT &#34920;&#29616;&#30340;&#20998;&#26512;&#26174;&#31034;&#20854;&#22312;&#21253;&#25324;&#25968;&#23398;&#12289;&#33521;&#35821;&#12289;&#29289;&#29702;&#12289;&#21270;&#23398;&#12289;&#29983;&#29289;&#12289;&#21382;&#21490;&#12289;&#22320;&#29702;&#12289;&#20844;&#27665;&#25945;&#32946;&#21644;&#25991;&#23398;&#22312;&#20869;&#30340;&#19968;&#31995;&#21015;&#23398;&#31185;&#20013;&#37117;&#24456;&#29087;&#32451;&#65292;&#36825;&#34920;&#26126;&#20854;&#25903;&#25345;&#23398;&#20064;&#32773;&#20855;&#26377;&#28508;&#21147;&#12290;&#20294;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350; ChatGPT &#22312;&#26356;&#22797;&#26434;&#30340;&#32771;&#35797;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#20197;&#21450;&#20854;&#25903;&#25345;&#19981;&#21516;&#32972;&#26223;&#23398;&#20064;&#32773;&#30340;&#28508;&#21147;&#12290;&#38543;&#30528;&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;AI &#21160;&#21147;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#26159;&#24191;&#27867;&#32780;&#26377;&#21069;&#26223;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research article highlights the potential of AI-powered chatbots in education and presents the results of using ChatGPT, a large language model, to complete the Vietnamese National High School Graduation Examination (VNHSGE). The study dataset included 30 essays in the literature test case and 1,700 multiple-choice questions designed for other subjects. The results showed that ChatGPT was able to pass the examination with an average score of 6-7, demonstrating the technology's potential to revolutionize the educational landscape. The analysis of ChatGPT performance revealed its proficiency in a range of subjects, including mathematics, English, physics, chemistry, biology, history, geography, civic education, and literature, which suggests its potential to provide effective support for learners. However, further research is needed to assess ChatGPT performance on more complex exam questions and its potential to support learners in different contexts. As technology continues to evo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GEmo-CLAP&#27169;&#22411;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#32467;&#21512;&#20102;&#24615;&#21035;&#23646;&#24615;&#20449;&#24687;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;IEMOCAP&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07848</link><description>&lt;p&gt;
GEmo-CLAP: &#38754;&#21521;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#24615;&#21035;&#23646;&#24615;&#22686;&#24378;&#23545;&#27604;&#35821;&#38899;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Speech Emotion Recognition. (arXiv:2306.07848v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GEmo-CLAP&#27169;&#22411;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#32467;&#21512;&#20102;&#24615;&#21035;&#23646;&#24615;&#20449;&#24687;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;IEMOCAP&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#38899;-&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;CLAP&#65289;&#26368;&#36817;&#22312;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#21151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GEmo-CLAP&#30340;&#39640;&#25928;&#24615;&#21035;&#23646;&#24615;&#22686;&#24378;CLAP&#27169;&#22411;&#65292;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#21508;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26500;&#24314;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24773;&#24863;CLAP&#27169;&#22411;&#65288;&#31216;&#20026;Emo-CLAP&#65289;&#65292;&#29992;&#20110;SER&#12290;&#28982;&#21518;&#65292;&#32771;&#34385;&#21040;&#22312;&#35821;&#38899;&#24773;&#24863;&#24314;&#27169;&#20013;&#24615;&#21035;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20004;&#31181;GEmo-CLAP&#26041;&#27861;&#65292;&#26469;&#25972;&#21512;&#35821;&#38899;&#20449;&#21495;&#30340;&#24773;&#24863;&#21644;&#24615;&#21035;&#20449;&#24687;&#65292;&#24418;&#25104;&#26356;&#21512;&#29702;&#30340;&#30446;&#26631;&#12290;&#22312;IEMOCAP&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20004;&#31181;GEmo-CLAP&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;Emo-CLAP&#27169;&#22411;&#65288;&#20351;&#29992;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65289;&#65292;&#21516;&#26102;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Audio Pretraining (CLAP) has recently exhibited impressive success in diverse fields. In this paper, we propose GEmo-CLAP, a kind of efficient gender-attribute-enhanced CLAP model for speech emotion recognition (SER). Specifically, we first build an effective emotion CLAP model termed Emo-CLAP for SER, utilizing various self-supervised learning based pre-trained models. Then, considering the importance of the gender attribute in speech emotion modeling, two GEmo-CLAP approaches are further proposed to integrate the emotion and gender information of speech signals, forming more reasonable objectives. Extensive experiments conducted on the IEMOCAP corpus demonstrate that our proposed two GEmo-CLAP approaches consistently outperform the baseline Emo-CLAP with different pre-trained models, while also achieving superior recognition performance compared with other state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21333;&#35789;&#39034;&#24207;&#20013;&#30340;&#20449;&#24687;&#23494;&#24230;&#32479;&#19968;&#21387;&#21147;&#65292;&#36890;&#36807;&#23545;10&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#36827;&#34892;&#35745;&#31639;&#27169;&#22411;&#27979;&#35797;&#65292;&#21457;&#29616;&#22312;SVO&#35821;&#35328;&#20013;&#65292;&#30495;&#23454;&#35789;&#24207;&#20855;&#26377;&#26356;&#22823;&#30340;&#32479;&#19968;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03734</link><description>&lt;p&gt;
&#21333;&#35789;&#39034;&#24207;&#20013;&#30340;&#36328;&#35821;&#35328;&#20449;&#24687;&#23494;&#24230;&#32479;&#19968;&#21387;&#21147;
&lt;/p&gt;
&lt;p&gt;
A Cross-Linguistic Pressure for Uniform Information Density in Word Order. (arXiv:2306.03734v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21333;&#35789;&#39034;&#24207;&#20013;&#30340;&#20449;&#24687;&#23494;&#24230;&#32479;&#19968;&#21387;&#21147;&#65292;&#36890;&#36807;&#23545;10&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#36827;&#34892;&#35745;&#31639;&#27169;&#22411;&#27979;&#35797;&#65292;&#21457;&#29616;&#22312;SVO&#35821;&#35328;&#20013;&#65292;&#30495;&#23454;&#35789;&#24207;&#20855;&#26377;&#26356;&#22823;&#30340;&#32479;&#19968;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#22312;&#22522;&#26412;&#35789;&#24207;&#21644;&#35789;&#24207;&#28789;&#27963;&#24615;&#26041;&#38754;&#24046;&#24322;&#24456;&#22823;&#65292;&#20294;&#23427;&#20204;&#30340;&#35789;&#24207;&#20173;&#28982;&#36981;&#24490;&#20849;&#20139;&#30340;&#36328;&#35821;&#35328;&#32479;&#35745;&#27169;&#24335;&#65292;&#36825;&#36890;&#24120;&#24402;&#22240;&#20110;&#21151;&#33021;&#21387;&#21147;&#12290;&#22312;&#35782;&#21035;&#36825;&#20123;&#21387;&#21147;&#30340;&#21162;&#21147;&#20013;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#27604;&#36739;&#20102;&#30495;&#23454;&#21644;&#21453;&#20107;&#23454;&#30340;&#35789;&#24207;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20123;&#35843;&#26597;&#20013;&#65292;&#26377;&#19968;&#20010;&#21151;&#33021;&#21387;&#21147;&#34987;&#24573;&#35270;&#20102;&#65292;&#21363;&#32479;&#19968;&#20449;&#24687;&#23494;&#24230;&#20551;&#35774;&#65292;&#35813;&#20551;&#35774;&#35748;&#20026;&#20449;&#24687;&#24212;&#35813;&#22312;&#35805;&#35821;&#20013;&#22343;&#21248;&#20998;&#24067;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35810;&#38382;&#26159;&#21542;&#36328;&#35821;&#35328;&#22320;&#23384;&#22312;&#20449;&#24687;&#23494;&#24230;&#32479;&#19968;&#30340;&#21387;&#21147;&#21487;&#33021;&#20250;&#24433;&#21709;&#35789;&#24207;&#27169;&#24335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#35745;&#31639;&#27169;&#22411;&#27979;&#35797;&#30495;&#23454;&#39034;&#24207;&#26159;&#21542;&#27604;&#21453;&#20107;&#23454;&#39034;&#24207;&#23548;&#33268;&#26356;&#22823;&#30340;&#20449;&#24687;&#32479;&#19968;&#24615;&#12290;&#22312;&#25105;&#20204;&#23545;10&#31181;&#31867;&#22411;&#22810;&#26679;&#30340;&#35821;&#35328;&#36827;&#34892;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#65306;&#65288;i&#65289;&#22312;SVO&#35821;&#35328;&#20013;&#65292;&#30495;&#23454;&#30340;&#35789;&#24207;&#19968;&#33268;&#27604;&#21453;&#21521;&#30340;&#35789;&#24207;&#20855;&#26377;&#26356;&#22823;&#30340;&#32479;&#19968;&#24615;&#65292;&#65288;ii&#65289;&#21482;&#26377;&#35821;&#35328;&#19978;&#19981;&#21512;&#29702;&#30340;&#21453;&#20107;&#23454;&#39034;&#24207;...
&lt;/p&gt;
&lt;p&gt;
While natural languages differ widely in both canonical word order and word order flexibility, their word orders still follow shared cross-linguistic statistical patterns, often attributed to functional pressures. In the effort to identify these pressures, prior work has compared real and counterfactual word orders. Yet one functional pressure has been overlooked in such investigations: the uniform information density (UID) hypothesis, which holds that information should be spread evenly throughout an utterance. Here, we ask whether a pressure for UID may have influenced word order patterns cross-linguistically. To this end, we use computational models to test whether real orders lead to greater information uniformity than counterfactual orders. In our empirical study of 10 typologically diverse languages, we find that: (i) among SVO languages, real word orders consistently have greater uniformity than reverse word orders, and (ii) only linguistically implausible counterfactual orders 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffusEmp&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#22810;&#32423;&#25511;&#21046;&#20449;&#21495;&#26469;&#29983;&#25104;&#26356;&#21152;&#32454;&#33268;&#21644;&#20010;&#24615;&#21270;&#30340;&#20849;&#24773;&#22238;&#24212;&#12290;</title><link>http://arxiv.org/abs/2306.01657</link><description>&lt;p&gt;
DiffusEmp:&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#22810;&#32423;&#25511;&#21046;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DiffusEmp: A Diffusion Model-Based Framework with Multi-Grained Control for Empathetic Response Generation. (arXiv:2306.01657v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffusEmp&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#22810;&#32423;&#25511;&#21046;&#20449;&#21495;&#26469;&#29983;&#25104;&#26356;&#21152;&#32454;&#33268;&#21644;&#20010;&#24615;&#21270;&#30340;&#20849;&#24773;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#24773;&#26159;&#24320;&#25918;&#24335;&#20132;&#27969;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#22240;&#32032;&#65292;&#23427;&#33258;&#28982;&#22320;&#23637;&#31034;&#20102;&#19968;&#20010;&#20154;&#23545;&#20182;&#20154;&#30340;&#20851;&#24515;&#21644;&#29702;&#35299;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#29983;&#25104;&#20849;&#24773;&#21709;&#24212;&#30340;&#26041;&#27861;&#65292;&#20294;&#29616;&#26377;&#30340;&#20316;&#21697;&#24448;&#24448;&#23548;&#33268;&#21333;&#35843;&#30340;&#20849;&#24773;&#65292;&#21363;&#25351;&#36890;&#29992;&#21644;&#23433;&#20840;&#30340;&#34920;&#36798;&#26041;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#26174;&#24335;&#25511;&#21046;&#26469;&#24341;&#23548;&#20849;&#24773;&#34920;&#36798;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;DiffusEmp&#26694;&#26550;&#65292;&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#32479;&#19968;&#20102;&#23545;&#35805;&#19978;&#19979;&#25991;&#21644;&#23646;&#24615;&#23548;&#21521;&#25511;&#21046;&#20449;&#21495;&#30340;&#21033;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24341;&#20837;&#36890;&#20449;&#26426;&#21046;&#12289;&#24847;&#22270;&#21644;&#35821;&#20041;&#26694;&#26550;&#20316;&#20026;&#22810;&#23618;&#27425;&#20449;&#21495;&#65292;&#21487;&#20174;&#31895;&#31961;&#21040;&#32454;&#33268;&#22320;&#25511;&#21046;&#20849;&#24773;&#23454;&#29616;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#23631;&#34109;&#31574;&#30053;&#65292;&#20197;&#21453;&#26144;&#22810;&#23618;&#27425;&#20449;&#21495;&#19982;&#21709;&#24212;&#20196;&#29260;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#25193;&#25955;&#27169;&#22411;&#20013;&#20197;&#24433;&#21709;&#29983;&#25104;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;EmpatheticDialogue&#19978;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20248;&#20110;&#20854;&#20182;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empathy is a crucial factor in open-domain conversations, which naturally shows one's caring and understanding to others. Though several methods have been proposed to generate empathetic responses, existing works often lead to monotonous empathy that refers to generic and safe expressions. In this paper, we propose to use explicit control to guide the empathy expression and design a framework DiffusEmp based on conditional diffusion language model to unify the utilization of dialogue context and attribute-oriented control signals. Specifically, communication mechanism, intent, and semantic frame are imported as multi-grained signals that control the empathy realization from coarse to fine levels. We then design a specific masking strategy to reflect the relationship between multi-grained signals and response tokens, and integrate it into the diffusion model to influence the generative process. Experimental results on a benchmark dataset EmpatheticDialogue show that our framework outper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#24335;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;SACL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#31867;&#21035;&#20998;&#24067;&#32467;&#26500;&#34920;&#31034;&#65292;&#36890;&#36807;&#32852;&#21512;&#31867;&#21035;&#20998;&#24067;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#26377;&#25928;&#21033;&#29992;&#26631;&#31614;&#32423;&#29305;&#24615;&#19968;&#33268;&#24615;&#24182;&#20445;&#30041;&#32454;&#31890;&#24230;&#30340;&#31867;&#20869;&#29305;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#20013;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.01505</link><description>&lt;p&gt;
&#22312;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#20013;&#20351;&#29992;&#30417;&#30563;&#24335;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations. (arXiv:2306.01505v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#24335;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;SACL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#31867;&#21035;&#20998;&#24067;&#32467;&#26500;&#34920;&#31034;&#65292;&#36890;&#36807;&#32852;&#21512;&#31867;&#21035;&#20998;&#24067;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#26377;&#25928;&#21033;&#29992;&#26631;&#31614;&#32423;&#29305;&#24615;&#19968;&#33268;&#24615;&#24182;&#20445;&#30041;&#32454;&#31890;&#24230;&#30340;&#31867;&#20869;&#29305;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#20013;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#35782;&#21035;&#22312;&#23545;&#35805;&#20013;&#26159;&#25552;&#21462;&#27867;&#21270;&#21644;&#31283;&#20581;&#34920;&#31034;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#24335;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;SACL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#31867;&#21035;&#20998;&#24067;&#32467;&#26500;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#23545;&#27604;&#24863;&#30693;&#23545;&#25239;&#24615;&#35757;&#32451;&#20197;&#29983;&#25104;&#26368;&#22351;&#24773;&#20917;&#30340;&#26679;&#26412;&#65292;&#24182;&#22312;&#21407;&#22987;&#21644;&#23545;&#25239;&#26679;&#26412;&#19978;&#20351;&#29992;&#32852;&#21512;&#31867;&#21035;&#20998;&#24067;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#12290;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#26631;&#31614;&#32423;&#29305;&#24615;&#19968;&#33268;&#24615;&#24182;&#20445;&#30041;&#32454;&#31890;&#24230;&#30340;&#31867;&#20869;&#29305;&#24615;&#12290;&#20026;&#20102;&#36991;&#20813;&#23545;&#19978;&#19979;&#25991;&#30456;&#20851;&#25968;&#25454;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#23545;&#25239;&#24615;&#35757;&#32451;&#31574;&#30053;&#65292;&#20174;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#26356;&#22810;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#24182;&#22686;&#24378;&#27169;&#22411;&#23545;&#19978;&#19979;&#25991;&#30340;&#23481;&#38169;&#24615;&#12290;&#22312;&#35813;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#24207;&#21015;&#30340;&#26041;&#27861;SACL-LSTM&#65292;&#29992;&#20110;&#23398;&#20064;&#38024;&#23545;ERC&#30340;&#26631;&#31614;&#19968;&#33268;&#21644;&#19978;&#19979;&#25991;&#31283;&#20581;&#30340;&#24773;&#24863;&#29305;&#24449;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SACL-LSTM&#22312;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting generalized and robust representations is a major challenge in emotion recognition in conversations (ERC). To address this, we propose a supervised adversarial contrastive learning (SACL) framework for learning class-spread structured representations. The framework applies contrast-aware adversarial training to generate worst-case samples and uses a joint class-spread contrastive learning objective on both original and adversarial samples. It can effectively utilize label-level feature consistency and retain fine-grained intra-class features. To avoid the negative impact of adversarial perturbations on context-dependent data, we design a contextual adversarial training strategy to learn more diverse features from context and enhance the model's context robustness. We develop a sequence-based method SACL-LSTM under this framework, to learn label-consistent and context-robust emotional features for ERC. Experiments on three datasets demonstrate that SACL-LSTM achieves state-of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#27880;&#24847;&#21147;&#22836;&#21098;&#26525;&#26041;&#27861;&#21644;Straight-Through Estimator&#65292;&#29992;&#20110;&#21152;&#36895;&#27169;&#22411;&#21098;&#26525;&#65292;&#20197;&#35299;&#20915;&#33258;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22823;&#20869;&#23384;&#21644;&#24378;&#35745;&#31639;&#38656;&#27714;&#38382;&#39064;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#20219;&#21153;&#20013;&#21462;&#24471;&#24456;&#22909;&#30340;&#24615;&#33021;&#19988;&#21442;&#25968;&#20943;&#23569;&#19988;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.01385</link><description>&lt;p&gt;
&#38754;&#21521;&#20219;&#21153;&#26080;&#20851;&#30340;&#35821;&#38899;&#34920;&#31034;&#27169;&#22411;&#32467;&#26500;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Task-Agnostic Structured Pruning of Speech Representation Models. (arXiv:2306.01385v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#27880;&#24847;&#21147;&#22836;&#21098;&#26525;&#26041;&#27861;&#21644;Straight-Through Estimator&#65292;&#29992;&#20110;&#21152;&#36895;&#27169;&#22411;&#21098;&#26525;&#65292;&#20197;&#35299;&#20915;&#33258;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22823;&#20869;&#23384;&#21644;&#24378;&#35745;&#31639;&#38656;&#27714;&#38382;&#39064;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#20219;&#21153;&#20013;&#21462;&#24471;&#24456;&#22909;&#30340;&#24615;&#33021;&#19988;&#21442;&#25968;&#20943;&#23569;&#19988;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22914;Wav2vec2&#12289;Hubert&#21644;WavLM&#24050;&#34987;&#35777;&#26126;&#33021;&#26174;&#33879;&#25552;&#39640;&#35768;&#22810;&#35821;&#38899;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24222;&#22823;&#30340;&#20869;&#23384;&#21644;&#24378;&#22823;&#30340;&#35745;&#31639;&#38656;&#27714;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24037;&#19994;&#24212;&#29992;&#12290;&#32467;&#26500;&#21270;&#21098;&#26525;&#26159;&#19968;&#31181;&#30828;&#20214;&#21451;&#22909;&#30340;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#20294;&#36890;&#24120;&#20250;&#23548;&#33268;&#26356;&#22823;&#30340;&#31934;&#24230;&#25439;&#22833;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#27880;&#24847;&#21147;&#22836;&#21098;&#26525;&#26041;&#27861;&#26469;&#24357;&#34917;&#24615;&#33021;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;L0&#35268;&#21017;&#21270;&#20013;&#24341;&#20837;&#20102;Straight-Through Estimator&#26469;&#36827;&#19968;&#27493;&#21152;&#36895;&#21098;&#26525;&#27169;&#22411;&#12290;&#22312;SUPERB&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36798;&#21040;&#19982;&#23494;&#38598;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#24179;&#22343;&#20248;&#20110;Wav2vec 2.0&#22522;&#20934;&#27169;&#22411;&#65292;&#21516;&#26102;&#21442;&#25968;&#20943;&#23569;72&#65285;&#65292;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised pre-trained models such as Wav2vec2, Hubert, and WavLM have been shown to significantly improve many speech tasks. However, their large memory and strong computational requirements hinder their industrial applicability. Structured pruning is a hardware-friendly model compression technique but usually results in a larger loss of accuracy. In this paper, we propose a fine-grained attention head pruning method to compensate for the performance degradation. In addition, we also introduce the straight through estimator into the L0 regularization to further accelerate the pruned model. Experiments on the SUPERB benchmark show that our model can achieve comparable performance to the dense model in multiple tasks and outperforms the Wav2vec 2.0 base model on average, with 72% fewer parameters and 2 times faster inference speed.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#24863;&#30693;&#24352;&#37327;&#21387;&#32553;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#27169;&#22411;&#22823;&#23567;&#65292;&#31639;&#26415;&#36816;&#31639;&#21644;&#26368;&#32456;&#36816;&#34892;&#26102;&#24310;&#12290;&#35813;&#26041;&#27861;&#32463;&#36807;&#36880;&#23618;&#33976;&#39311;&#21518;&#22312;&#25991;&#26412;&#34164;&#21547;&#21644;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01076</link><description>&lt;p&gt;
&#37327;&#21270;&#24863;&#30693;&#21644;&#24352;&#37327;&#21387;&#32553;&#35757;&#32451;&#65306;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
Quantization-Aware and Tensor-Compressed Training of Transformers for Natural Language Understanding. (arXiv:2306.01076v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#24863;&#30693;&#24352;&#37327;&#21387;&#32553;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#27169;&#22411;&#22823;&#23567;&#65292;&#31639;&#26415;&#36816;&#31639;&#21644;&#26368;&#32456;&#36816;&#34892;&#26102;&#24310;&#12290;&#35813;&#26041;&#27861;&#32463;&#36807;&#36880;&#23618;&#33976;&#39311;&#21518;&#22312;&#25991;&#26412;&#34164;&#21547;&#21644;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#30340;Transformer&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24222;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#38459;&#27490;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#39640;&#24615;&#33021;Transformer&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#24863;&#30693;&#24352;&#37327;&#21387;&#32553;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#27169;&#22411;&#22823;&#23567;&#65292;&#31639;&#26415;&#36816;&#31639;&#21644;&#26368;&#32456;&#36816;&#34892;&#26102;&#24310;&#12290;&#25105;&#20204;&#23558;Transformer&#30340;&#23884;&#20837;&#21644;&#32447;&#24615;&#23618;&#21387;&#32553;&#20026;&#23567;&#22411;&#20302;&#31209;&#24352;&#37327;&#26680;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#12290;&#37319;&#29992;&#21487;&#23398;&#20064;&#27604;&#20363;&#22240;&#23376;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26469;&#36827;&#19968;&#27493;&#33719;&#24471;&#24352;&#37327;&#21387;&#32553;&#27169;&#22411;&#30340;&#20302;&#31934;&#24230;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#31471;&#21040;&#31471;&#35757;&#32451;&#21644;&#33976;&#39311;&#35757;&#32451;&#12290;&#20026;&#20102;&#25552;&#39640;&#25910;&#25947;&#24615;&#65292;&#37319;&#29992;&#36880;&#23618;&#33976;&#39311;&#26041;&#27861;&#20174;&#39044;&#35757;&#32451;Transformer&#20013;&#25552;&#21462;&#20986;&#19968;&#20010;&#32463;&#36807;&#37327;&#21270;&#21644;&#24352;&#37327;&#21387;&#32553;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#22312;&#20004;&#20010;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#21363;&#25991;&#26412;&#34164;&#21547;&#21644;&#24773;&#24863;&#20998;&#26512;&#20013;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuned transformer models have shown superior performances in many natural language tasks. However, the large model size prohibits deploying high-performance transformer models on resource-constrained devices. This paper proposes a quantization-aware tensor-compressed training approach to reduce the model size, arithmetic operations, and ultimately runtime latency of transformer-based models. We compress the embedding and linear layers of transformers into small low-rank tensor cores, which significantly reduces model parameters. A quantization-aware training with learnable scale factors is used to further obtain low-precision representations of the tensor-compressed models. The developed approach can be used for both end-to-end training and distillation-based training. To improve the convergence, a layer-by-layer distillation is applied to distill a quantized and tensor-compressed student model from a pre-trained transformer. The performance is demonstrated in two natural language
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#25361;&#25112;&#12289;&#26041;&#27861;&#35770;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#23450;&#21046;&#25216;&#26415;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#65292;&#24182;&#24191;&#27867;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.18703</link><description>&lt;p&gt;
&#36229;&#36234;&#19968;&#20010;&#27169;&#22411;&#36866;&#29992;&#20110;&#25152;&#26377;&#39046;&#22495;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Beyond One-Model-Fits-All: A Survey of Domain Specialization for Large Language Models. (arXiv:2305.18703v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#25361;&#25112;&#12289;&#26041;&#27861;&#35770;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#23450;&#21046;&#25216;&#26415;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#65292;&#24182;&#24191;&#27867;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22823;&#22823;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#20026;&#24191;&#27867;&#24212;&#29992;&#25552;&#20379;&#20102;&#39640;&#24230;&#23454;&#29992;&#12289;&#20219;&#21153;&#26080;&#20851;&#30340;&#22522;&#30784;&#12290;LLMs &#20316;&#20026;&#36890;&#29992;&#20219;&#21153;&#27714;&#35299;&#22120;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20419;&#20351;&#20154;&#20204;&#23558;&#20854;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#12289;&#37329;&#34701;&#21644;&#25945;&#32946;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#21161;&#25163;&#29978;&#33267;&#26367;&#20195;&#29305;&#23450;&#39046;&#22495;&#30340;&#19987;&#23478;&#21644;&#24037;&#20855;&#12290;&#20294;&#26159;&#65292;&#23558;LLMs&#30452;&#25509;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#20013;&#30340;&#22797;&#26434;&#38382;&#39064;&#20250;&#36935;&#21040;&#35768;&#22810;&#22256;&#38590;&#65292;&#21253;&#25324;&#39046;&#22495;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12289;&#39046;&#22495;&#30693;&#35782;&#30340;&#22797;&#26434;&#24615;&#12289;&#39046;&#22495;&#30446;&#26631;&#30340;&#29420;&#29305;&#24615;&#20197;&#21450;&#32422;&#26463;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#31181;&#24046;&#36317;&#65292;&#26368;&#36817;&#20960;&#24180;&#36827;&#34892;&#20102;&#24613;&#21095;&#22686;&#21152;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#33268;&#21147;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#65292;&#28982;&#32780;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#23578;&#26410;&#34987;&#31995;&#32479;&#22320;&#24635;&#32467;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23545;LLMs&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#25361;&#25112;&#12289;&#26041;&#27861;&#35770;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#23450;&#21046;&#25216;&#26415;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#65292;&#24182;&#24191;&#27867;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. The great promise of LLMs as general task solvers motivated people to extend their functionality largely beyond just a ``chatbot'', and use it as an assistant or even replacement for domain experts and tools in specific domains such as healthcare, finance, and education. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). To fill such a gap, explosively-increase research, and practices have been conducted in very recent years on the domain specialization of LLMs, which, howe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#25216;&#26415;&#65292;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#19982;&#39044;&#27979;&#20934;&#30830;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.18404</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#31572;&#26696;&#30830;&#35748;&#39044;&#27979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Conformal Prediction with Large Language Models for Multi-Choice Question Answering. (arXiv:2305.18404v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#25216;&#26415;&#65292;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#19982;&#39044;&#27979;&#20934;&#30830;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#24320;&#21457;&#65292;&#23545;&#23427;&#20204;&#36827;&#34892;&#20581;&#22766;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25216;&#26415;&#23558;&#25104;&#20026;&#23427;&#20204;&#22312;&#39640;&#39118;&#38505;&#22330;&#26223;&#19979;&#23433;&#20840;&#37096;&#32626;&#30340;&#20851;&#38190;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#25216;&#26415;&#65292;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#19982;&#39044;&#27979;&#20934;&#30830;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;&#36825;&#31181;&#35266;&#23519;&#23545;&#20110;&#19979;&#28216;&#24212;&#29992;&#65292;&#22914;&#36873;&#25321;&#24615;&#20998;&#31867;&#21644;&#36807;&#28388;&#20302;&#36136;&#37327;&#39044;&#27979;&#65292;&#21487;&#33021;&#20250;&#26377;&#29992;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#31526;&#21512;&#24615;&#39044;&#27979;&#23545;&#20110;&#36229;&#20986;&#20027;&#39064;&#30340;&#38382;&#39064;&#30340;&#20132;&#25442;&#24615;&#20551;&#35774;&#65292;&#36825;&#21487;&#33021;&#26159;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#30340;&#26356;&#20026;&#29616;&#23454;&#30340;&#22330;&#26223;&#12290;&#26412;&#30740;&#31350;&#20026;&#22312;&#38656;&#35201;&#21487;&#38752;&#20445;&#35777;&#38169;&#35823;&#29575;&#30340;&#23433;&#20840;&#20851;&#38190;&#24773;&#20917;&#19979;&#26356;&#21152;&#20540;&#24471;&#20449;&#36182;&#21644;&#21487;&#38752;&#22320;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models continue to be widely developed, robust uncertainty quantification techniques will become crucial for their safe deployment in high-stakes scenarios. In this work, we explore how conformal prediction can be used to provide uncertainty quantification in language models for the specific task of multiple-choice question-answering. We find that the uncertainty estimates from conformal prediction are tightly correlated with prediction accuracy. This observation can be useful for downstream applications such as selective classification and filtering out low-quality predictions. We also investigate the exchangeability assumption required by conformal prediction to out-of-subject questions, which may be a more realistic scenario for many practical applications. Our work contributes towards more trustworthy and reliable usage of large language models in safety-critical situations, where robust guarantees of error rate are required.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20197;&#20195;&#29702;&#24615;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#27861;&#21644;&#35821;&#20041;&#20132;&#20114;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#29305;&#23450;&#25968;&#25454;&#38598;&#27979;&#35797;&#20102;&#19981;&#21516;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-3 text-davinci-003&#22312;&#36825;&#20123;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#27604;&#20854;&#20182;&#27979;&#35797;&#30340;&#27169;&#22411;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.18185</link><description>&lt;p&gt;
&#35821;&#27861;&#21644;&#35821;&#20041;&#22312;&#8220;&#20013;&#38388;&#8221;&#30456;&#36935;&#65306;&#36890;&#36807;&#20195;&#29702;&#24615;&#25506;&#31350;&#35821;&#27861;-&#35821;&#20041;&#30028;&#38754;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Syntax and Semantics Meet in the "Middle": Probing the Syntax-Semantics Interface of LMs Through Agentivity. (arXiv:2305.18185v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20197;&#20195;&#29702;&#24615;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#27861;&#21644;&#35821;&#20041;&#20132;&#20114;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#29305;&#23450;&#25968;&#25454;&#38598;&#27979;&#35797;&#20102;&#19981;&#21516;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-3 text-davinci-003&#22312;&#36825;&#20123;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#27604;&#20854;&#20182;&#27979;&#35797;&#30340;&#27169;&#22411;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#20419;&#20351;&#30740;&#31350;&#20154;&#21592;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#19978;&#26816;&#39564;&#23427;&#20204;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#30740;&#31350;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#36328;&#21333;&#35789;&#21644;&#26356;&#22823;&#21477;&#27861;&#24418;&#24335;&#30340;&#24847;&#20041;&#20132;&#20114;&#26041;&#38754;&#20960;&#20046;&#27809;&#26377;&#30740;&#31350;&#12290;&#25105;&#20204;&#20197;&#20195;&#29702;&#24615;&#20316;&#20026;&#35821;&#20041;&#27010;&#24565;&#65292;&#20316;&#20026;&#25506;&#31350;&#36825;&#31181;&#20132;&#20114;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#21033;&#29992;&#37096;&#20998;&#21487;&#36873;&#30340;&#33521;&#35821;&#21450;&#29289;&#21160;&#35789;&#30340;&#29420;&#29305;&#35821;&#35328;&#23646;&#24615;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#34987;&#29992;&#26469;&#38024;&#23545;&#19981;&#21516;&#22823;&#23567;&#30340;&#19977;&#31181;&#27169;&#22411;&#31867;&#21035;&#36827;&#34892;&#27979;&#35797;&#65292;&#20197;&#26597;&#30475;&#23427;&#20204;&#26159;&#21542;&#23545;&#35789;&#27719;&#32423;&#21035;&#30340;&#20195;&#29702;&#24615;&#25935;&#24863;&#65292;&#20197;&#21450;&#23427;&#20204;&#26159;&#21542;&#33021;&#22312;&#29305;&#23450;&#30340;&#21477;&#27861;&#19978;&#19979;&#25991;&#20013;&#36866;&#24403;&#22320;&#24212;&#29992;&#36825;&#20123;&#35789;&#32423;&#20808;&#39564;&#30693;&#35782;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;GPT-3 text-davinci-003&#22312;&#25152;&#26377;&#23454;&#39564;&#20013;&#34920;&#29616;&#38750;&#24120;&#20986;&#33394;&#65292;&#36828;&#36828;&#36229;&#36807;&#20854;&#20182;&#27979;&#35797;&#30340;&#27169;&#22411;&#12290;&#23454;&#38469;&#19978;&#65292;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#27604;&#65292;&#32467;&#26524;&#29978;&#33267;&#26356;&#22909;&#22320;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models have prompted researchers to examine their abilities across a variety of linguistic tasks, but little has been done to investigate how models handle the interactions in meaning across words and larger syntactic forms -- i.e. phenomena at the intersection of syntax and semantics. We present the semantic notion of agentivity as a case study for probing such interactions. We created a novel evaluation dataset by utilitizing the unique linguistic properties of a subset of optionally transitive English verbs. This dataset was used to prompt varying sizes of three model classes to see if they are sensitive to agentivity at the lexical level, and if they can appropriately employ these word-level priors given a specific syntactic context. Overall, GPT-3 text-davinci-003 performs extremely well across all experiments, outperforming all other models tested by far. In fact, the results are even better correlated with human judgements than both syntactic an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#31163;&#25955;&#21333;&#20803;&#20316;&#20026;&#20013;&#38388;&#25351;&#23548;&#65292;&#20197;&#25552;&#39640;&#26080;&#25991;&#26412;&#35821;&#38899;&#29702;&#35299;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36807;&#20102;&#22522;&#20934;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#21333;&#20803;&#25351;&#23548;&#26377;&#21161;&#20110;&#23567;&#26679;&#26412;&#23398;&#20064;&#21644;&#25552;&#21319;&#27169;&#22411;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.18096</link><description>&lt;p&gt;
&#29992;&#31163;&#25955;&#21333;&#20803;&#20316;&#20026;&#20013;&#38388;&#30446;&#26631;&#25552;&#39640;&#26080;&#25991;&#26412;&#35821;&#38899;&#29702;&#35299;&#25216;&#26415;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Textless Spoken Language Understanding with Discrete Units as Intermediate Target. (arXiv:2305.18096v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#31163;&#25955;&#21333;&#20803;&#20316;&#20026;&#20013;&#38388;&#25351;&#23548;&#65292;&#20197;&#25552;&#39640;&#26080;&#25991;&#26412;&#35821;&#38899;&#29702;&#35299;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36807;&#20102;&#22522;&#20934;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#21333;&#20803;&#25351;&#23548;&#26377;&#21161;&#20110;&#23567;&#26679;&#26412;&#23398;&#20064;&#21644;&#25552;&#21319;&#27169;&#22411;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#29702;&#35299;&#65288;SLU&#65289;&#26159;&#19968;&#39033;&#26088;&#22312;&#20174;&#21475;&#35821;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#30340;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#37197;&#23545;&#30340;&#35821;&#38899;&#36716;&#25991;&#26412;&#25968;&#25454;&#65292;&#22914;&#39044;&#35757;&#32451;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#25110;&#37197;&#23545;&#25991;&#26412;&#20316;&#20026;&#20013;&#38388;&#30446;&#26631;&#65292;&#22312;&#31471;&#21040;&#31471;SLU&#19978;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#37197;&#23545;&#30340;&#25991;&#26412;&#36716;&#24405;&#23545;&#20110;&#26080;&#20070;&#38754;&#35821;&#35328;&#26469;&#35828;&#26159;&#26114;&#36149;&#19988;&#19981;&#23454;&#38469;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26080;&#25991;&#26412;SLU&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#32780;&#19981;&#20351;&#29992;&#37197;&#23545;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#26080;&#25991;&#26412;SLU&#32570;&#20047;&#20013;&#38388;&#30446;&#26631;&#21644;&#35757;&#32451;&#25351;&#23548;&#24120;&#24120;&#23548;&#33268;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#21463;&#21040;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#20013;&#20869;&#23481;&#20998;&#35299;&#31163;&#25955;&#21333;&#20803;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#31163;&#25955;&#21333;&#20803;&#20316;&#20026;&#20013;&#38388;&#25351;&#23548;&#65292;&#20197;&#25552;&#39640;&#26080;&#25991;&#26412;SLU&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20116;&#20010;SLU&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#22522;&#20934;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#21333;&#20803;&#25351;&#23548;&#26377;&#21161;&#20110;&#23567;&#26679;&#26412;&#23398;&#20064;&#65292;&#24182;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spoken Language Understanding (SLU) is a task that aims to extract semantic information from spoken utterances. Previous research has made progress in end-to-end SLU by using paired speech-text data, such as pre-trained Automatic Speech Recognition (ASR) models or paired text as intermediate targets. However, acquiring paired transcripts is expensive and impractical for unwritten languages. On the other hand, Textless SLU extracts semantic information from speech without utilizing paired transcripts. However, the absence of intermediate targets and training guidance for textless SLU often results in suboptimal performance. In this work, inspired by the content-disentangled discrete units from self-supervised speech models, we proposed to use discrete units as intermediate guidance to improve textless SLU performance. Our method surpasses the baseline method on five SLU benchmark corpora. Additionally, we find that unit guidance facilitates few-shot learning and enhances the model's abi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#20445;&#38505;&#38382;&#31572;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#20445;&#38505;&#25919;&#31574;&#25163;&#20876;&#20013;&#25552;&#21462;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#26469;&#22686;&#24378;LLMs&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#39046;&#22495;&#36866;&#24212;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07230</link><description>&lt;p&gt;
&#24403;&#36229;&#32423;&#35821;&#35328;&#27169;&#22411;&#19981;&#36275;&#20197;&#28385;&#36275;&#19994;&#21153;&#38656;&#27714;&#65306;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
When Giant Language Brains Just Aren't Enough! Domain Pizzazz with Knowledge Sparkle Dust. (arXiv:2305.07230v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#20445;&#38505;&#38382;&#31572;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#20445;&#38505;&#25919;&#31574;&#25163;&#20876;&#20013;&#25552;&#21462;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#26469;&#22686;&#24378;LLMs&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#39046;&#22495;&#36866;&#24212;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#26174;&#33879;&#22320;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;GPT&#27169;&#22411;&#22788;&#20110;&#39046;&#20808;&#22320;&#20301;&#12290;&#34429;&#28982;&#23427;&#20204;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20196;&#20154;&#24778;&#21497;&#65292;&#20294;&#23558;LLMs&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#19994;&#21153;&#22330;&#26223;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#65292;&#24182;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#39564;&#20998;&#26512;&#65292;&#26088;&#22312;&#24357;&#21512;&#23558;LLMs&#36866;&#24212;&#20110;&#23454;&#38469;&#20351;&#29992;&#24773;&#20917;&#30340;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36873;&#25321;&#20445;&#38505;&#38382;&#31572;&#65288;QA&#65289;&#20219;&#21153;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#25512;&#29702;&#30340;&#25361;&#25112;&#12290;&#22522;&#20110;&#35813;&#20219;&#21153;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#20381;&#36182;&#20110;&#20174;&#20445;&#38505;&#25919;&#31574;&#25163;&#20876;&#20013;&#25552;&#21462;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#20351;LLMs&#33021;&#22815;&#29702;&#35299;&#20445;&#38505;&#30340;&#26032;&#27010;&#24565;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#12290;&#23454;&#38469;QA&#23545;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#20174;&#25919;&#31574;&#25163;&#20876;&#20013;&#25552;&#21462;&#30340;&#30693;&#35782;&#26174;&#33879;&#25552;&#39640;&#20102;GPT-3.5&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;50.4&#65285;&#12290;&#20998;&#26512;&#36824;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#20844;&#24320;&#35780;&#20272;&#26631;&#20934;&#21487;&#33021;&#19981;&#36275;&#20197;&#35780;&#20272;LLMs&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have significantly advanced the field of natural language processing, with GPT models at the forefront. While their remarkable performance spans a range of tasks, adapting LLMs for real-world business scenarios still poses challenges warranting further investigation. This paper presents an empirical analysis aimed at bridging the gap in adapting LLMs to practical use cases. To do that, we select the question answering (QA) task of insurance as a case study due to its challenge of reasoning. Based on the task we design a new model relied on LLMs which are empowered by domain-specific knowledge extracted from insurance policy rulebooks. The domain-specific knowledge helps LLMs to understand new concepts of insurance for domain adaptation. Preliminary results on real QA pairs show that knowledge enhancement from policy rulebooks significantly improves the reasoning ability of GPT-3.5 of 50.4% in terms of accuracy. The analysis also indicates that existing publ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06569</link><description>&lt;p&gt;
&#22914;&#20309;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#32034;&#24341;&#39033;&#30446;ID
&lt;/p&gt;
&lt;p&gt;
How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#23558;&#25512;&#33616;&#20219;&#21153;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#25512;&#33616;&#12290;&#23427;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#24314;&#35758;&#30340;&#39033;&#30446;&#32780;&#19981;&#26159;&#35745;&#31639;&#20256;&#32479;&#25512;&#33616;&#27169;&#22411;&#20013;&#27599;&#20010;&#20505;&#36873;&#39033;&#30446;&#30340;&#25490;&#21517;&#24471;&#20998;&#65292;&#31616;&#21270;&#20102;&#25512;&#33616;&#31649;&#36947;&#65292;&#36991;&#20813;&#20102;&#22810;&#27573;&#36807;&#28388;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#36991;&#20813;&#22312;&#20915;&#23450;&#35201;&#25512;&#33616;&#21738;&#20123;&#39033;&#30446;&#26102;&#29983;&#25104;&#36807;&#38271;&#30340;&#25991;&#26412;&#65292;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#21019;&#24314;LLM&#20860;&#23481;&#30340;&#39033;&#30446;ID&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#65292;&#20197;P5&#20026;&#20195;&#34920;&#30340;&#20027;&#24178;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#32034;&#24341;&#26041;&#27861;&#22797;&#21046;&#20854;&#32467;&#26524;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#20960;&#31181;&#24494;&#19981;&#36275;&#36947;&#30340;&#39033;&#30446;&#32034;&#24341;&#26041;&#27861;&#65288;&#22914;&#29420;&#31435;&#32034;&#24341;&#12289;&#26631;&#39064;&#32034;&#24341;&#21644;&#38543;&#26426;&#32034;&#24341;&#65289;&#30340;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32034;&#24341;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#32034;&#24341;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#32034;&#24341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text when deciding which item(s) to recommend, creating LLM-compatible item IDs is essential for recommendation foundation models. In this study, we systematically examine the item indexing problem for recommendation foundation models, using P5 as the representative backbone model and replicating its results with various indexing methods. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as independent indexing, title indexing, and random inde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;Transformer&#30340;&#26041;&#27861;&#65292;&#21363;REPORT&#65292;&#33021;&#22815;&#21516;&#26102;&#32858;&#21512;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#65292;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20869;&#22312;&#29305;&#24615;&#12290;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#20851;&#31995;&#35821;&#20041;&#65292;&#24182;&#33021;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#23436;&#20840;&#24402;&#32435;&#30340;&#35774;&#32622;&#20013;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.00215</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;Transformer&#30340;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#24402;&#32435;&#20851;&#31995;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers. (arXiv:2304.00215v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;Transformer&#30340;&#26041;&#27861;&#65292;&#21363;REPORT&#65292;&#33021;&#22815;&#21516;&#26102;&#32858;&#21512;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#65292;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20869;&#22312;&#29305;&#24615;&#12290;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#20851;&#31995;&#35821;&#20041;&#65292;&#24182;&#33021;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#23436;&#20840;&#24402;&#32435;&#30340;&#35774;&#32622;&#20013;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#20851;&#31995;&#39044;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#29616;&#26377;&#30340;&#23884;&#20837;&#24335;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#36716;&#23548;&#35774;&#32622;&#65292;&#32570;&#20047;&#24402;&#32435;&#33021;&#21147;&#65292;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#30340;&#23454;&#20307;&#19978;&#36827;&#34892;&#25512;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32479;&#19968;&#30340;&#20998;&#23618;Transformer&#26694;&#26550;&#65292;&#21363;REPORT&#65292;&#21516;&#26102;&#32858;&#21512;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#65292;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20869;&#22312;&#29305;&#24615;&#65292;&#36825;&#31181;&#26041;&#27861;&#23436;&#20840;&#20381;&#36182;&#20110;&#20851;&#31995;&#35821;&#20041;&#65292;&#24182;&#33021;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#23436;&#20840;&#24402;&#32435;&#30340;&#35774;&#32622;&#20013;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;REPORT&#34920;&#29616;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#65292;&#29978;&#33267;&#22312;&#20004;&#20010;&#23436;&#20840;&#24402;&#32435;&#30340;&#25968;&#25454;&#38598;&#30340;&#20843;&#20010;&#29256;&#26412;&#23376;&#38598;&#19978;&#20063;&#26159;&#22914;&#27492;&#12290;&#27492;&#22806;&#65292;REPORT&#33021;&#22815;&#23558;&#25512;&#29702;&#25512;&#24191;&#21040;&#35757;&#32451;&#21644;&#25512;&#29702;&#20013;&#27809;&#26377;&#20844;&#20849;&#23454;&#20307;&#30340;&#26032;&#23454;&#20307;&#19978;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation prediction on knowledge graphs (KGs) is a key research topic. Dominant embedding-based methods mainly focus on the transductive setting and lack the inductive ability to generalize to new entities for inference. Existing methods for inductive reasoning mostly mine the connections between entities, i.e., relational paths, without considering the nature of head and tail entities contained in the relational context. This paper proposes a novel method that captures both connections between entities and the intrinsic nature of entities, by simultaneously aggregating RElational Paths and cOntext with a unified hieRarchical Transformer framework, namely REPORT. REPORT relies solely on relation semantics and can naturally generalize to the fully-inductive setting, where KGs for training and inference have no common entities. In the experiments, REPORT performs consistently better than all baselines on almost all the eight version subsets of two fully-inductive datasets. Moreover. REPO
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#26426;&#22120;&#21019;&#36896;&#24615;&#30340;&#38590;&#28857;&#21644;&#26131;&#28857;&#65292;&#24182;&#37325;&#28857;&#20998;&#26512;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.00008</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Creativity of Large Language Models. (arXiv:2304.00008v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00008
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#26426;&#22120;&#21019;&#36896;&#24615;&#30340;&#38590;&#28857;&#21644;&#26131;&#28857;&#65292;&#24182;&#37325;&#28857;&#20998;&#26512;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27491;&#22312;&#39072;&#35206;&#20154;&#24037;&#26234;&#33021;&#30340;&#22810;&#20010;&#39046;&#22495;&#12290;&#20854;&#20013;&#26368;&#26174;&#33879;&#30340;&#24212;&#29992;&#20043;&#19968;&#26159;&#21019;&#20316;&#65292;&#20363;&#22914;&#35799;&#27468;&#25110;&#25925;&#20107;&#65306;&#29983;&#25104;&#30340;&#36755;&#20986;&#36890;&#24120;&#20855;&#26377;&#24778;&#20154;&#30340;&#36136;&#37327;&#12290;&#20294;&#26159;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#65306;LLMs&#30495;&#30340;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#21019;&#36896;&#24615;&#30340;&#21527;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#21019;&#36896;&#24615;&#29702;&#35770;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;LLMs&#30340;&#21457;&#23637;&#65292;&#25506;&#35752;&#20102;&#20851;&#38190;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#19982;LLMs&#30456;&#20851;&#30340;&#26426;&#22120;&#21019;&#36896;&#24615;&#26041;&#38754;&#30830;&#23450;&#20102;&#19968;&#32452;&#8220;&#26131;&#8221;&#21644;&#8220;&#38590;&#8221;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are revolutionizing several areas of Artificial Intelligence. One of the most remarkable applications is creative writing, e.g., poetry or storytelling: the generated outputs are often of astonishing quality. However, a natural question arise: can LLMs really be considered creative? In this article we firstly analyze the development of LLMs under the lens of creativity theories, investigating the key open questions and challenges. Then, we identify a set of "easy" and "hard" problems in machine creativity, discussing them in relation to LLMs. Finally, we analyze the societal impact of these technologies with a particular focus on the creative industries.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#26102;&#22914;&#20309;&#20351;&#29992;&#23398;&#29983;t-&#20998;&#24067;&#26041;&#27861;&#35780;&#20272;&#35780;&#20215;&#32773;&#38388;&#21487;&#38752;&#24615;&#65292;&#24182;&#36890;&#36807;&#23450;&#37327;&#20998;&#26512;&#23637;&#31034;&#20102;&#24341;&#20837;&#26356;&#22810;&#35266;&#23519;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#35780;&#20272;&#32622;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.04526</link><description>&lt;p&gt;
&#23398;&#29983;t-&#20998;&#24067;&#65306;&#22312;&#35266;&#23519;&#31232;&#32570;&#26102;&#27979;&#37327;&#35780;&#20215;&#32773;&#38388;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Student's t-Distribution: On Measuring the Inter-Rater Reliability When the Observations are Scarce. (arXiv:2303.04526v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04526
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#26102;&#22914;&#20309;&#20351;&#29992;&#23398;&#29983;t-&#20998;&#24067;&#26041;&#27861;&#35780;&#20272;&#35780;&#20215;&#32773;&#38388;&#21487;&#38752;&#24615;&#65292;&#24182;&#36890;&#36807;&#23450;&#37327;&#20998;&#26512;&#23637;&#31034;&#20102;&#24341;&#20837;&#26356;&#22810;&#35266;&#23519;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#35780;&#20272;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#25105;&#20204;&#24635;&#26159;&#20381;&#36182;&#20110;&#20154;&#31867;&#21028;&#26029;&#20316;&#20026;&#40644;&#37329;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26576;&#20123;&#35780;&#20272;&#20219;&#21153;&#65288;&#22914;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#65289;&#30340;&#35780;&#20215;&#32773;&#38388;&#21487;&#38752;&#24615;&#65288;IRR&#65289;&#27700;&#24179;&#22914;&#20309;&#26356;&#22909;&#35780;&#20272;&#19968;&#30452;&#23384;&#22312;&#20105;&#35758;&#65292;&#29305;&#21035;&#26159;&#24403;&#25968;&#25454;&#26679;&#26412;&#65288;&#35266;&#23519;&#65289;&#38750;&#24120;&#31232;&#32570;&#26102;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#20171;&#32461;&#20102;&#24403;&#21482;&#26377;&#19968;&#20010;&#25968;&#25454;&#65288;&#35780;&#20272;&#65289;&#28857;&#21487;&#29992;&#26102;&#22914;&#20309;&#20272;&#35745;&#27979;&#37327;&#20540;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20855;&#20307;&#20363;&#23376;&#20171;&#32461;&#20102;&#20004;&#20010;&#20154;&#29983;&#25104;&#30340;&#35266;&#23519;&#35780;&#20998;&#65292;&#24341;&#20837;&#20102;"&#23398;&#29983;t-&#20998;&#24067;"&#26041;&#27861;&#65292;&#24182;&#35299;&#37322;&#20102;&#22914;&#20309;&#20351;&#29992;&#23427;&#20165;&#20351;&#29992;&#36825;&#20004;&#20010;&#25968;&#25454;&#28857;&#26469;&#27979;&#37327;IRR&#24471;&#20998;&#65292;&#20197;&#21450;&#36136;&#37327;&#35780;&#20272;&#30340;&#32622;&#20449;&#21306;&#38388;&#65288;CI&#65289;&#12290;&#25105;&#20204;&#23450;&#37327;&#20998;&#26512;&#20102;&#36890;&#36807;&#24341;&#20837;&#26356;&#22810;&#35266;&#23519;&#21363;&#20351;&#21482;&#26377;&#19968;&#20010;&#39069;&#22806;&#35266;&#23519;&#22914;&#20309;&#22823;&#22823;&#25552;&#39640;&#35780;&#20272;&#32622;&#20449;&#24230;&#12290;&#25105;&#20204;&#40723;&#21169;&#30740;&#31350;&#32773;&#20204;&#36827;&#34892;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In natural language processing (NLP) we always rely on human judgement as the golden quality evaluation method. However, there has been an ongoing debate on how to better evaluate inter-rater reliability (IRR) levels for certain evaluation tasks, such as translation quality evaluation (TQE), especially when the data samples (observations) are very scarce. In this work, we first introduce the study on how to estimate the confidence interval for the measurement value when only one data (evaluation) point is available. Then, this leads to our example with two human-generated observational scores, for which, we introduce ``Student's \textit{t}-Distribution'' method and explain how to use it to measure the IRR score using only these two data points, as well as the confidence intervals (CIs) of the quality evaluation. We give quantitative analysis on how the evaluation confidence can be greatly improved by introducing more observations, even if only one extra observation. We encourage resear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38376;&#25511;&#35821;&#35328;&#19987;&#23478;&#21644;&#35838;&#31243;&#35757;&#32451;&#26469;&#22686;&#24378;&#22810;&#35821;&#35328;ASR&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#29992;&#25143;&#25552;&#20379;LID&#36755;&#20837;&#12290;&#22312;&#21452;&#35821;&#20219;&#21153;&#20013;&#65292;&#19982;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#65292;&#24179;&#22343;&#35789;&#38169;&#35823;&#29575;&#20998;&#21035;&#38477;&#20302;&#20102;12.5&#65285;&#21644;7.3&#65285;&#65292;&#24182;&#19988;&#19982;&#20351;&#29992;oracle LID&#35757;&#32451;&#30340;&#19978;&#38480;&#27169;&#22411;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2303.00786</link><description>&lt;p&gt;
&#20351;&#29992;&#38376;&#25511;&#35821;&#35328;&#19987;&#23478;&#21644;&#35838;&#31243;&#35757;&#32451;&#26500;&#24314;&#39640;&#20934;&#30830;&#24230;&#30340;&#22810;&#35821;&#35328;ASR
&lt;/p&gt;
&lt;p&gt;
Building High-accuracy Multilingual ASR with Gated Language Experts and Curriculum Training. (arXiv:2303.00786v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38376;&#25511;&#35821;&#35328;&#19987;&#23478;&#21644;&#35838;&#31243;&#35757;&#32451;&#26469;&#22686;&#24378;&#22810;&#35821;&#35328;ASR&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#29992;&#25143;&#25552;&#20379;LID&#36755;&#20837;&#12290;&#22312;&#21452;&#35821;&#20219;&#21153;&#20013;&#65292;&#19982;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#65292;&#24179;&#22343;&#35789;&#38169;&#35823;&#29575;&#20998;&#21035;&#38477;&#20302;&#20102;12.5&#65285;&#21644;7.3&#65285;&#65292;&#24182;&#19988;&#19982;&#20351;&#29992;oracle LID&#35757;&#32451;&#30340;&#19978;&#38480;&#27169;&#22411;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#38376;&#25511;&#35821;&#35328;&#19987;&#23478;&#21644;&#35838;&#31243;&#35757;&#32451;&#26469;&#22686;&#24378;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#20256;&#23548;&#27169;&#22411;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#26080;&#38656;&#29992;&#25143;&#25552;&#20379;&#35821;&#35328;&#35782;&#21035;&#65288;LID&#65289;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#38376;&#25511;&#26426;&#21046;&#21644;LID&#25439;&#22833;&#65292;&#20351;&#21464;&#21387;&#22120;&#19987;&#23478;&#23398;&#20064;&#35821;&#35328;&#29305;&#23450;&#20449;&#24687;&#12290;&#36890;&#36807;&#23558;&#38376;&#25511;&#21464;&#21387;&#22120;&#19987;&#23478;&#19982;&#20849;&#20139;&#21464;&#21387;&#22120;&#23618;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#22359;&#65292;&#24182;&#21033;&#29992;&#32447;&#24615;&#19987;&#23478;&#26377;&#25928;&#22320;&#35268;&#33539;&#21270;&#32852;&#21512;&#32593;&#32476;&#12290;&#35838;&#31243;&#35757;&#32451;&#26041;&#26696;&#21033;&#29992;LID&#24341;&#23548;&#38376;&#25511;&#19987;&#23478;&#25913;&#36827;&#21508;&#33258;&#30340;&#35821;&#35328;&#24615;&#33021;&#12290;&#22312;&#28041;&#21450;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#30340;&#21452;&#35821;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#19982;&#22522;&#32447;&#30340;&#21452;&#35821;&#21644;&#21333;&#35821;&#27169;&#22411;&#30456;&#27604;&#65292;&#24179;&#22343;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#20998;&#21035;&#38477;&#20302;&#20102;12.5&#65285;&#21644;7.3&#65285;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#20351;&#29992;oracle LID&#35757;&#32451;&#21644;&#25512;&#26029;&#30340;&#19978;&#38480;&#27169;&#22411;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose gated language experts and curriculum training to enhance multilingual transformer transducer models without requiring language identification (LID) input from users during inference. Our method incorporates a gating mechanism and LID loss, enabling transformer experts to learn language-specific information. By combining gated transformer experts with shared transformer layers, we construct multilingual transformer blocks and utilize linear experts to effectively regularize the joint network. The curriculum training scheme leverages LID to guide the gated experts in improving their respective language performance. Experimental results on a bilingual task involving English and Spanish demonstrate significant improvements, with average relative word error reductions of 12.5% and 7.3% compared to the baseline bilingual and monolingual models, respectively. Notably, our method achieves performance comparable to the upper-bound model trained and inferred with oracle LID. Extendin
&lt;/p&gt;</description></item><item><title>TalkTheWalk &#26159;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#31934;&#24515;&#31574;&#21010;&#30340;&#39033;&#30446;&#25910;&#34255;&#20013;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#26469;&#21512;&#25104;&#36924;&#30495;&#39640;&#36136;&#37327;&#30340;&#20250;&#35805;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#26500;&#24314;&#20250;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#25910;&#38598;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2301.11489</link><description>&lt;p&gt;
Talk the Walk: &#38024;&#23545;&#20250;&#35805;&#24335;&#38899;&#20048;&#25512;&#33616;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Talk the Walk: Synthetic Data Generation for Conversational Music Recommendation. (arXiv:2301.11489v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11489
&lt;/p&gt;
&lt;p&gt;
TalkTheWalk &#26159;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#31934;&#24515;&#31574;&#21010;&#30340;&#39033;&#30446;&#25910;&#34255;&#20013;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#26469;&#21512;&#25104;&#36924;&#30495;&#39640;&#36136;&#37327;&#30340;&#20250;&#35805;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#26500;&#24314;&#20250;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#25910;&#38598;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#24191;&#27867;&#23384;&#22312;&#65292;&#20294;&#29992;&#25143;&#24448;&#24448;&#24456;&#38590;&#22312;&#25512;&#33616;&#36136;&#37327;&#36739;&#24046;&#26102;&#36827;&#34892;&#25511;&#21046;&#21644;&#35843;&#25972;&#12290;&#36825;&#20419;&#20351;&#20102;&#20250;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;(CRSs)&#30340;&#21457;&#23637;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#25552;&#20379;&#23545;&#25512;&#33616;&#30340;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#20250;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#38656;&#35201;&#21253;&#21547;&#29992;&#25143;&#35805;&#35821;&#21644;&#28085;&#30422;&#22810;&#26679;&#21270;&#20559;&#22909;&#33539;&#22260;&#30340;&#39033;&#30446;&#30340;&#20250;&#35805;&#35757;&#32451;&#25968;&#25454;&#12290;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#22914;&#20247;&#21253;&#65292;&#36825;&#26679;&#30340;&#25968;&#25454;&#25910;&#38598;&#36215;&#26469;&#38750;&#24120;&#22256;&#38590;&#12290;&#25105;&#20204;&#22312;&#39033;&#30446;&#38598;&#25512;&#33616;&#30340;&#32972;&#26223;&#19979;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#27880;&#24847;&#21040;&#36825;&#20010;&#20219;&#21153;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#20851;&#27880;&#65292;&#21160;&#26426;&#22312;&#20110;&#38899;&#20048;&#12289;&#26032;&#38395;&#21644;&#39135;&#35889;&#25512;&#33616;&#31561;&#20351;&#29992;&#26696;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;TalkTheWalk&#65292;&#36890;&#36807;&#21033;&#29992;&#24191;&#27867;&#21487;&#33719;&#24471;&#30340;&#31934;&#24515;&#31574;&#21010;&#30340;&#39033;&#30446;&#25910;&#34255;&#20013;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#26469;&#21512;&#25104;&#36924;&#30495;&#39640;&#36136;&#37327;&#30340;&#20250;&#35805;&#25968;&#25454;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20854;&#36716;&#21270;&#20026;&#30456;&#24212;&#30340;&#39033;&#30446;&#38598;&#31574;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation systems are ubiquitous yet often difficult for users to control and adjust when recommendation quality is poor. This has motivated the development of conversational recommendation systems (CRSs), with control over recommendations provided through natural language feedback. However, building conversational recommendation systems requires conversational training data involving user utterances paired with items that cover a diverse range of preferences. Such data has proved challenging to collect scalably using conventional methods like crowdsourcing. We address it in the context of item-set recommendation, noting the increasing attention to this task motivated by use cases like music, news and recipe recommendation. We present a new technique, TalkTheWalk, that synthesizes realistic high-quality conversational data by leveraging domain expertise encoded in widely available curated item collections, showing how these can be transformed into corresponding item set curation c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;CausalDialogue&#25968;&#25454;&#38598;&#24182;&#25552;&#20986;&#20102;ExMATE&#26041;&#27861;&#26469;&#27169;&#25311;&#23545;&#35805;&#20013;&#35805;&#35821;&#32423;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#34920;&#26126;&#20256;&#32479;&#30340;&#25439;&#22833;&#20989;&#25968;&#38590;&#20197;&#25972;&#21512;&#26377;&#21521;&#26080;&#29615;&#22270;&#32467;&#26500;&#12290;&#30740;&#31350;&#32467;&#26524;&#20026;&#35757;&#32451;&#31070;&#32463;&#23545;&#35805;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#24378;&#22240;&#26524;&#20851;&#31995;&#24433;&#21709;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.10515</link><description>&lt;p&gt;
CausalDialogue: &#23545;&#35805;&#20013;&#35805;&#35821;&#32423;&#22240;&#26524;&#20851;&#31995;&#30340;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
CausalDialogue: Modeling Utterance-level Causality in Conversations. (arXiv:2212.10515v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;CausalDialogue&#25968;&#25454;&#38598;&#24182;&#25552;&#20986;&#20102;ExMATE&#26041;&#27861;&#26469;&#27169;&#25311;&#23545;&#35805;&#20013;&#35805;&#35821;&#32423;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#34920;&#26126;&#20256;&#32479;&#30340;&#25439;&#22833;&#20989;&#25968;&#38590;&#20197;&#25972;&#21512;&#26377;&#21521;&#26080;&#29615;&#22270;&#32467;&#26500;&#12290;&#30740;&#31350;&#32467;&#26524;&#20026;&#35757;&#32451;&#31070;&#32463;&#23545;&#35805;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#24378;&#22240;&#26524;&#20851;&#31995;&#24433;&#21709;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23427;&#20204;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#31070;&#32463;&#23545;&#35805;&#27169;&#22411;&#23578;&#26410;&#23637;&#29616;&#20986;&#19982;&#20154;&#31867;&#33258;&#28982;&#20132;&#27969;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#29992;&#25143;&#35805;&#35821;&#35270;&#20026;&#22240;&#26524;&#20851;&#31995;&#65292;&#23558;&#29983;&#25104;&#30340;&#22238;&#24212;&#35270;&#20026;&#25928;&#26524;&#65292;&#24182;&#35748;&#35782;&#21040;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#25913;&#21464;&#24212;&#20135;&#29983;&#19981;&#21516;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25506;&#32034;&#36825;&#20010;&#27010;&#24565;&#65292;&#25105;&#20204;&#36890;&#36807;&#20247;&#21253;&#32534;&#21046;&#20102;&#19968;&#20010;&#21517;&#20026;CausalDialogue&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#25193;&#23637;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#19968;&#20010;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#32467;&#26500;&#20013;&#30340;&#22810;&#20010;&#22240;&#26524;&#23545;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;&#20256;&#32479;&#30340;&#25439;&#22833;&#20989;&#25968;&#38590;&#20197;&#26377;&#25928;&#22320;&#25972;&#21512;DAG&#32467;&#26500;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#35805;&#35821;&#32423;&#22240;&#26524;&#20851;&#31995;&#22312;&#35757;&#32451;&#31070;&#32463;&#23545;&#35805;&#27169;&#22411;&#20013;&#24433;&#21709;&#21147;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#25351;&#25968;&#26368;&#22823;&#24179;&#22343;&#27835;&#30103;&#25928;&#24212;(ExMATE)&#12290;&#20026;&#20102;&#35780;&#20272;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#32771;&#34385;&#22240;&#26524;&#20851;&#31995;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#21033;&#29992;&#19981;&#21516;&#30340;&#27169;&#22411;&#12289;&#25512;&#29702;&#21644;&#35757;&#32451;&#26041;&#27861;&#22312;CausalDialogue&#25968;&#25454;&#38598;&#19978;&#26500;&#24314;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their widespread adoption, neural conversation models have yet to exhibit natural chat capabilities with humans. In this research, we examine user utterances as causes and generated responses as effects, recognizing that changes in a cause should produce a different effect. To further explore this concept, we have compiled and expanded upon a new dataset called CausalDialogue through crowd-sourcing. This dataset includes multiple cause-effect pairs within a directed acyclic graph (DAG) structure. Our analysis reveals that traditional loss functions struggle to effectively incorporate the DAG structure, leading us to propose a causality-enhanced method called Exponential Maximum Average Treatment Effect (ExMATE) to enhance the impact of causality at the utterance level in training neural conversation models. To evaluate the needs of considering causality in dialogue generation, we built a comprehensive benchmark on CausalDialogue dataset using different models, inference, and tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25991;&#26412;&#31616;&#21270;&#30340;&#21487;&#23398;&#20064;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;LENS&#65292;&#36890;&#36807;&#24341;&#20837;SimpeEval&#35821;&#26009;&#24211;&#20316;&#20026;&#20154;&#31867;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#19982;&#29616;&#26377;&#24230;&#37327;&#26631;&#20934;&#30456;&#27604;&#65292;LENS&#23545;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#26356;&#22909;&#65292;&#20026;&#35780;&#20272;&#25991;&#26412;&#31616;&#21270;&#30340;&#26410;&#26469;&#36827;&#23637;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2212.09739</link><description>&lt;p&gt;
LENS&#65306;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#25991;&#26412;&#31616;&#21270;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
LENS: A Learnable Evaluation Metric for Text Simplification. (arXiv:2212.09739v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25991;&#26412;&#31616;&#21270;&#30340;&#21487;&#23398;&#20064;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;LENS&#65292;&#36890;&#36807;&#24341;&#20837;SimpeEval&#35821;&#26009;&#24211;&#20316;&#20026;&#20154;&#31867;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#19982;&#29616;&#26377;&#24230;&#37327;&#26631;&#20934;&#30456;&#27604;&#65292;LENS&#23545;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#26356;&#22909;&#65292;&#20026;&#35780;&#20272;&#25991;&#26412;&#31616;&#21270;&#30340;&#26410;&#26469;&#36827;&#23637;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21487;&#23398;&#20064;&#24230;&#37327;&#26631;&#20934;&#24050;&#25104;&#20026;&#33258;&#21160;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#25991;&#26412;&#31616;&#21270;&#30340;&#20154;&#31867;&#35780;&#20272;&#25968;&#25454;&#38598;&#21482;&#26377;&#19968;&#20123;&#22522;&#20110;&#21333;&#19968;&#25110;&#36807;&#26102;&#27169;&#22411;&#30340;&#26377;&#38480;&#27880;&#37322;&#65292;&#20351;&#23427;&#20204;&#19981;&#33021;&#36866;&#29992;&#20110;&#36825;&#31181;&#26041;&#27861;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SimpEval&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#25324;&#65306;SimpEval_past&#65292;&#21253;&#25324;&#23545;24&#20010;&#36807;&#21435;&#31995;&#32479;2.4K&#31616;&#21270;&#30340;12K&#20154;&#31867;&#35780;&#20998;&#65292;&#20197;&#21450;SimpEval_2022&#65292;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31616;&#21270;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;&#23545;360&#20010;&#31616;&#21270;&#65292;&#21253;&#25324;GPT-3.5&#29983;&#25104;&#25991;&#26412;&#30340;1K&#20154;&#31867;&#35780;&#20998;&#12290;&#22312;SimpEval&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25991;&#26412;&#31616;&#21270;&#30340;&#21487;&#23398;&#20064;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;LENS&#12290;&#24191;&#27867;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;LENS&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#26356;&#22909;&#65292;&#20026;&#35780;&#20272;&#25991;&#26412;&#31616;&#21270;&#30340;&#26410;&#26469;&#36827;&#23637;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;Rank&#21644;Rate&#65292;&#19968;&#31181;&#20154;&#31867;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#31616;&#21270;&#36827;&#34892;&#25490;&#21517;&#21644;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training learnable metrics using modern language models has recently emerged as a promising method for the automatic evaluation of machine translation. However, existing human evaluation datasets for text simplification have limited annotations that are based on unitary or outdated models, making them unsuitable for this approach. To address these issues, we introduce the SimpEval corpus that contains: SimpEval_past, comprising 12K human ratings on 2.4K simplifications of 24 past systems, and SimpEval_2022, a challenging simplification benchmark consisting of over 1K human ratings of 360 simplifications including GPT-3.5 generated text. Training on SimpEval, we present LENS, a Learnable Evaluation Metric for Text Simplification. Extensive empirical results show that LENS correlates much better with human judgment than existing metrics, paving the way for future progress in the evaluation of text simplification. We also introduce Rank and Rate, a human evaluation framework that rates si
&lt;/p&gt;</description></item><item><title>Transformers&#30456;&#23545;&#20110;&#24490;&#29615;&#27169;&#22411;&#26356;&#20559;&#21521;&#20110;&#23398;&#20064;&#20855;&#26377;&#20302;&#25935;&#24863;&#24230;&#30340;&#20989;&#25968;&#65292;&#23588;&#20854;&#22312;&#31232;&#30095;&#24067;&#23572;&#20989;&#25968;&#19978;&#65292;Transformers&#33021;&#22815;&#23454;&#29616;&#36817;&#20046;&#23436;&#32654;&#30340;&#27867;&#21270;&#65292;&#32780;LSTMs&#21017;&#34920;&#29616;&#20986;&#36807;&#25311;&#21512;&#21644;&#36739;&#20302;&#30340;&#27867;&#21270;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.12316</link><description>&lt;p&gt;
Transformers&#20013;&#30340;&#31616;&#27905;&#20559;&#35265;&#21450;&#20854;&#23398;&#20064;&#31232;&#30095;&#24067;&#23572;&#20989;&#25968;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Simplicity Bias in Transformers and their Ability to Learn Sparse Boolean Functions. (arXiv:2211.12316v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12316
&lt;/p&gt;
&lt;p&gt;
Transformers&#30456;&#23545;&#20110;&#24490;&#29615;&#27169;&#22411;&#26356;&#20559;&#21521;&#20110;&#23398;&#20064;&#20855;&#26377;&#20302;&#25935;&#24863;&#24230;&#30340;&#20989;&#25968;&#65292;&#23588;&#20854;&#22312;&#31232;&#30095;&#24067;&#23572;&#20989;&#25968;&#19978;&#65292;Transformers&#33021;&#22815;&#23454;&#29616;&#36817;&#20046;&#23436;&#32654;&#30340;&#27867;&#21270;&#65292;&#32780;LSTMs&#21017;&#34920;&#29616;&#20986;&#36807;&#25311;&#21512;&#21644;&#36739;&#20302;&#30340;&#27867;&#21270;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Transformers&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24191;&#27867;&#30340;&#25104;&#21151;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#24490;&#29615;&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#20204;&#22312;&#24314;&#27169;&#20960;&#31181;&#24418;&#24335;&#35821;&#35328;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#20026;&#20160;&#20040;Transformers&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#23427;&#20204;&#26159;&#21542;&#20855;&#26377;&#20219;&#20309;&#33021;&#20351;&#23427;&#20204;&#27604;&#24490;&#29615;&#27169;&#22411;&#26356;&#22909;&#22320;&#27867;&#21270;&#30340;&#23646;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#24067;&#23572;&#20989;&#25968;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#35777;&#26126;&#20197;&#19979;&#20869;&#23481;&#65306;(i) &#38543;&#26426;Transformers&#30456;&#23545;&#26356;&#20559;&#21521;&#20110;&#20855;&#26377;&#20302;&#25935;&#24863;&#24230;&#30340;&#20989;&#25968;&#12290;(ii) &#24403;&#35757;&#32451;&#24067;&#23572;&#20989;&#25968;&#26102;&#65292;Transformers&#21644;LSTMs&#37117;&#20248;&#20808;&#23398;&#20064;&#20855;&#26377;&#20302;&#25935;&#24863;&#24230;&#30340;&#20989;&#25968;&#65292;&#26368;&#32456;Transformers&#25910;&#25947;&#21040;&#20855;&#26377;&#26356;&#20302;&#25935;&#24863;&#24230;&#30340;&#20989;&#25968;&#12290;(iii) &#22312;&#20855;&#26377;&#20302;&#25935;&#24863;&#24230;&#30340;&#31232;&#30095;&#24067;&#23572;&#20989;&#25968;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;Transformers&#22312;&#23384;&#22312;&#22122;&#38899;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#36817;&#20046;&#23436;&#32654;&#22320;&#27867;&#21270;&#65292;&#32780;LSTMs&#36807;&#25311;&#21512;&#24182;&#19988;&#27867;&#21270;&#31934;&#24230;&#36739;&#20302;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#21487;&#37327;&#21270;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the widespread success of Transformers on NLP tasks, recent works have found that they struggle to model several formal languages when compared to recurrent models. This raises the question of why Transformers perform well in practice and whether they have any properties that enable them to generalize better than recurrent models. In this work, we conduct an extensive empirical study on Boolean functions to demonstrate the following: (i) Random Transformers are relatively more biased towards functions of low sensitivity. (ii) When trained on Boolean functions, both Transformers and LSTMs prioritize learning functions of low sensitivity, with Transformers ultimately converging to functions of lower sensitivity. (iii) On sparse Boolean functions which have low sensitivity, we find that Transformers generalize near perfectly even in the presence of noisy labels whereas LSTMs overfit and achieve poor generalization accuracy. Overall, our results provide strong quantifiable evidence
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23569;&#26679;&#26412;&#20154;&#26426;&#20132;&#20114;&#35757;&#32451;&#31639;&#27861;Nano&#65292;&#29992;&#20110;&#25353;&#20219;&#24847;&#20998;&#24067;&#65288;&#23450;&#37327;&#21644;&#26410;&#23450;&#37327;&#65289;&#29983;&#25104;&#25991;&#26412;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;Nano&#22312;&#21333;&#19968;&#20027;&#39064;/&#23646;&#24615;&#20197;&#21450;&#23450;&#37327;&#20998;&#24067;&#25511;&#21046;&#26041;&#38754;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.05750</link><description>&lt;p&gt;
Nano: &#23884;&#22871;&#30340;&#20154;&#26426;&#20132;&#20114;&#22870;&#21169;&#23398;&#20064;&#29992;&#20110;&#23569;&#26679;&#26412;&#35821;&#35328;&#27169;&#22411;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Nano: Nested Human-in-the-Loop Reward Learning for Few-shot Language Model Control. (arXiv:2211.05750v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23569;&#26679;&#26412;&#20154;&#26426;&#20132;&#20114;&#35757;&#32451;&#31639;&#27861;Nano&#65292;&#29992;&#20110;&#25353;&#20219;&#24847;&#20998;&#24067;&#65288;&#23450;&#37327;&#21644;&#26410;&#23450;&#37327;&#65289;&#29983;&#25104;&#25991;&#26412;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;Nano&#22312;&#21333;&#19968;&#20027;&#39064;/&#23646;&#24615;&#20197;&#21450;&#23450;&#37327;&#20998;&#24067;&#25511;&#21046;&#26041;&#38754;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#23637;&#31034;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;&#20219;&#21153;&#32463;&#24120;&#38656;&#35201;&#25511;&#21046;&#29983;&#25104;&#25991;&#26412;&#30340;&#20998;&#24067;&#65292;&#20197;&#20943;&#36731;&#20559;&#35265;&#12289;&#20419;&#36827;&#20844;&#24179;&#24615;&#21644;&#23454;&#29616;&#20010;&#24615;&#21270;&#12290;&#29616;&#26377;&#30340;&#25991;&#26412;&#20998;&#24067;&#25511;&#21046;&#25216;&#26415;&#21482;&#36866;&#29992;&#20110;&#23450;&#37327;&#20998;&#24067;&#65292;&#36825;&#35201;&#27714;&#39044;&#20808;&#23450;&#20041;&#30340;&#31867;&#21035;&#12289;&#20998;&#24067;&#27604;&#20363;&#25110;&#31526;&#21512;&#25152;&#38656;&#20998;&#24067;&#30340;&#29616;&#26377;&#35821;&#26009;&#24211;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#37325;&#35201;&#30340;&#20998;&#24067;&#65292;&#22914;&#20010;&#20154;&#20559;&#22909;&#65292;&#26159;&#26410;&#23450;&#37327;&#21270;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;Nano&#65292;&#19968;&#20010;&#23569;&#26679;&#26412;&#20154;&#26426;&#20132;&#20114;&#35757;&#32451;&#31639;&#27861;&#65292;&#19981;&#26029;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65292;&#26469;&#35299;&#20915;&#25353;&#20219;&#24847;&#20998;&#24067;&#65288;&#23450;&#37327;&#21644;&#26410;&#23450;&#37327;&#65289;&#29983;&#25104;&#25991;&#26412;&#30340;&#38382;&#39064;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;Nano&#22312;&#21333;&#19968;&#20027;&#39064;/&#23646;&#24615;&#20197;&#21450;&#23450;&#37327;&#20998;&#24067;&#25511;&#21046;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;Nano&#33021;&#22815;&#23398;&#20064;&#26410;&#23450;&#37327;&#21270;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models have demonstrated extraordinary capabilities in language generation. However, real-world tasks often require controlling the distribution of generated text in order to mitigate bias, promote fairness, and achieve personalization. Existing techniques for controlling the distribution of generated text only work with quantified distributions, which require pre-defined categories, proportions of the distribution, or an existing corpus following the desired distributions. However, many important distributions, such as personal preferences, are unquantified. In this work, we tackle the problem of generating text following arbitrary distributions (quantified and unquantified) by proposing Nano, a few-shot human-in-the-loop training algorithm that continuously learns from human feedback. Nano achieves state-of-the-art results on single topic/attribute as well as quantified distribution control compared to previous works. We also show that Nano is able to learn unquan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Mars&#65292;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#24314;&#27169;&#19978;&#19979;&#25991;&#21644;&#29366;&#24577;&#34920;&#31034;&#30340;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#35821;&#20041;&#29366;&#24577;&#34920;&#31034;&#19981;&#21516;&#30340;&#23545;&#35805;&#19978;&#19979;&#25991;&#34920;&#31034;&#23545;&#20110;&#22810;&#36718;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#26356;&#20026;&#26377;&#30410;&#65292;&#19988;Mars&#22312;&#24120;&#29992;&#35780;&#27979;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.08917</link><description>&lt;p&gt;
Mars: &#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#24314;&#27169;&#19978;&#19979;&#25991;&#21644;&#29366;&#24577;&#34920;&#31034;&#30340;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Mars: Modeling Context &amp; State Representations with Contrastive Learning for End-to-End Task-Oriented Dialog. (arXiv:2210.08917v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Mars&#65292;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#24314;&#27169;&#19978;&#19979;&#25991;&#21644;&#29366;&#24577;&#34920;&#31034;&#30340;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#35821;&#20041;&#29366;&#24577;&#34920;&#31034;&#19981;&#21516;&#30340;&#23545;&#35805;&#19978;&#19979;&#25991;&#34920;&#31034;&#23545;&#20110;&#22810;&#36718;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#26356;&#20026;&#26377;&#30410;&#65292;&#19988;Mars&#22312;&#24120;&#29992;&#35780;&#27979;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#23558;&#23545;&#35805;&#19978;&#19979;&#25991;&#36716;&#25442;&#20026;&#20449;&#24565;&#29366;&#24577;&#21644;&#21160;&#20316;&#29366;&#24577;&#65292;&#28982;&#21518;&#29983;&#25104;&#31995;&#32479;&#21709;&#24212;&#12290;&#31995;&#32479;&#21709;&#24212;&#30340;&#24615;&#33021;&#26174;&#33879;&#21463;&#21040;&#20449;&#24565;&#29366;&#24577;&#21644;&#21160;&#20316;&#29366;&#24577;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#39318;&#20808;&#25506;&#32034;&#20102;&#23545;&#35805;&#19978;&#19979;&#25991;&#34920;&#31034;&#23545;&#25552;&#39640;&#20449;&#24565;&#29366;&#24577;&#21644;&#21160;&#20316;&#29366;&#24577;&#36136;&#37327;&#30340;&#30410;&#22788;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#22686;&#24378;&#29983;&#25104;&#30340;&#21709;&#24212;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Mars&#65292;&#19968;&#31181;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#65292;&#21033;&#29992;&#20004;&#31181;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#24314;&#27169;&#23545;&#35805;&#19978;&#19979;&#25991;&#21644;&#20449;&#24565;/&#21160;&#20316;&#29366;&#24577;&#34920;&#31034;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#35821;&#20041;&#29366;&#24577;&#34920;&#31034;&#26356;&#19981;&#21516;&#30340;&#23545;&#35805;&#19978;&#19979;&#25991;&#34920;&#31034;&#23545;&#22810;&#36718;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#26356;&#26377;&#30410;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;Mars&#22312;MultiWOZ 2.0&#12289;CamRest676&#21644;CrossWOZ&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional end-to-end task-oriented dialog systems first convert dialog context into belief state and action state before generating the system response. The system response performance is significantly affected by the quality of the belief state and action state. We first explore what dialog context representation is beneficial to improving the quality of the belief state and action state, which further enhances the generated response quality. To tackle our exploration, we propose Mars, an end-to-end task-oriented dialog system with two contrastive learning strategies to model the relationship between dialog context and belief/action state representations. Empirical results show dialog context representations, which are more different from semantic state representations, are more conducive to multi-turn task-oriented dialog. Moreover, our proposed Mars achieves state-of-the-art performance on the MultiWOZ 2.0, CamRest676, and CrossWOZ.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22810;&#23186;&#20307;&#29983;&#25104;&#24335;&#33050;&#26412;&#23398;&#20064;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#36319;&#36394;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#20013;&#30340;&#21382;&#21490;&#29366;&#24577;&#26469;&#29983;&#25104;&#21518;&#32493;&#27493;&#39588;&#65292;&#33021;&#23545;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#20855;&#26377;&#24402;&#32435;&#33021;&#21147;&#24182;&#20855;&#26377;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.12306</link><description>&lt;p&gt;
&#22810;&#23186;&#20307;&#29983;&#25104;&#24335;&#33050;&#26412;&#23398;&#20064;&#29992;&#20110;&#20219;&#21153;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Multimedia Generative Script Learning for Task Planning. (arXiv:2208.12306v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12306
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22810;&#23186;&#20307;&#29983;&#25104;&#24335;&#33050;&#26412;&#23398;&#20064;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#36319;&#36394;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#20013;&#30340;&#21382;&#21490;&#29366;&#24577;&#26469;&#29983;&#25104;&#21518;&#32493;&#27493;&#39588;&#65292;&#33021;&#23545;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#20855;&#26377;&#24402;&#32435;&#33021;&#21147;&#24182;&#20855;&#26377;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#23548;&#21521;&#30340;&#29983;&#25104;&#24335;&#33050;&#26412;&#23398;&#20064;&#26088;&#22312;&#22522;&#20110;&#30446;&#26631;&#29983;&#25104;&#21518;&#32493;&#27493;&#39588;&#65292;&#36825;&#26159;&#24110;&#21161;&#26426;&#22120;&#20154;&#25191;&#34892;&#26085;&#24120;&#29983;&#27963;&#20013;&#20856;&#22411;&#27963;&#21160;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#26524;&#21382;&#21490;&#29366;&#24577;&#19981;&#20165;&#30001;&#32473;&#20154;&#30340;&#35821;&#35328;&#25351;&#31034;&#25429;&#33719;&#65292;&#32780;&#19988;&#36824;&#36890;&#36807;&#30456;&#20276;&#30340;&#22270;&#20687;&#25552;&#20379;&#20102;&#38468;&#21152;&#20449;&#24687;&#65292;&#37027;&#20040;&#27492;&#20219;&#21153;&#30340;&#34920;&#29616;&#21487;&#20197;&#25913;&#21892;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#22810;&#23186;&#20307;&#29983;&#25104;&#24335;&#33050;&#26412;&#23398;&#20064;&#65292;&#20197;&#36890;&#36807;&#36319;&#36394;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#20013;&#30340;&#21382;&#21490;&#29366;&#24577;&#26469;&#29983;&#25104;&#21518;&#32493;&#27493;&#39588;&#65292;&#24182;&#25552;&#20379;&#20102;&#21253;&#21547;2,338&#20010;&#20219;&#21153;&#21644;31,496&#20010;&#27493;&#39588;&#21450;&#20854;&#25551;&#36848;&#24615;&#22270;&#20687;&#30340;&#31532;&#19968;&#20010;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#21487;&#35270;&#29366;&#24577;&#21487;&#36319;&#36394;&#30340;&#33050;&#26412;&#65292;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#20855;&#26377;&#24402;&#32435;&#33021;&#21147;&#65292;&#24182;&#19988;&#20854;&#27493;&#39588;&#20855;&#26377;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#22810;&#23186;&#20307;&#36873;&#25321;&#24615;&#32534;&#30721;&#22120;&#23545;&#35270;&#35273;&#29366;&#24577;&#21464;&#21270;&#36827;&#34892;&#32534;&#30721;&#65292;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#35299;&#30721;&#22120;&#20256;&#36882;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#20219;&#21153;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#25277;&#26679;&#21644;&#27874;&#26463;&#25628;&#32034;&#35299;&#30721;&#29983;&#25104;&#22810;&#26679;&#30340;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Goal-oriented generative script learning aims to generate subsequent steps based on a goal, which is an essential task to assist robots in performing stereotypical activities of daily life. We show that the performance of this task can be improved if historical states are not just captured by the linguistic instructions given to people, but are augmented with the additional information provided by accompanying images. Therefore, we propose a new task, Multimedia Generative Script Learning, to generate subsequent steps by tracking historical states in both text and vision modalities, as well as presenting the first benchmark containing 2,338 tasks and 31,496 steps with descriptive images. We aim to generate scripts that are visual-state trackable, inductive for unseen tasks, and diverse in their individual steps. We propose to encode visual state changes through a multimedia selective encoder, transferring knowledge from previously observed tasks using a retrieval-augmented decoder, and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#27979;&#35797;&#26041;&#27861;&#65292;&#31216;&#20026;&#22270;&#28789;&#23454;&#39564;&#65288;TE&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#34892;&#20026;&#27169;&#25311;&#33021;&#21147;&#12290;&#35770;&#25991;&#36890;&#36807;&#27169;&#25311;&#32463;&#27982;&#23398;&#12289;&#24515;&#29702;&#35821;&#35328;&#23398;&#21644;&#31038;&#20250;&#24515;&#29702;&#23398;&#23454;&#39564;&#65292;&#25104;&#21151;&#22797;&#21046;&#20102;&#24050;&#26377;&#30740;&#31350;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#31181;&#8220;&#36229;&#20934;&#30830;&#24230;&#25197;&#26354;&#8221;&#30340;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2208.10264</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#22810;&#20010;&#20154;&#24182;&#22797;&#21046;&#20154;&#31867;&#23454;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies. (arXiv:2208.10264v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10264
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#27979;&#35797;&#26041;&#27861;&#65292;&#31216;&#20026;&#22270;&#28789;&#23454;&#39564;&#65288;TE&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#34892;&#20026;&#27169;&#25311;&#33021;&#21147;&#12290;&#35770;&#25991;&#36890;&#36807;&#27169;&#25311;&#32463;&#27982;&#23398;&#12289;&#24515;&#29702;&#35821;&#35328;&#23398;&#21644;&#31038;&#20250;&#24515;&#29702;&#23398;&#23454;&#39564;&#65292;&#25104;&#21151;&#22797;&#21046;&#20102;&#24050;&#26377;&#30740;&#31350;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#31181;&#8220;&#36229;&#20934;&#30830;&#24230;&#25197;&#26354;&#8221;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#22270;&#28789;&#23454;&#39564;&#65288;TE&#65289;&#30340;&#26032;&#22411;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#32473;&#23450;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT&#27169;&#22411;&#65289;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#20197;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;TE&#36824;&#21487;&#20197;&#25581;&#31034;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#25311;&#29305;&#23450;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#30340;&#19968;&#33268;&#24615;&#25197;&#26354;&#12290;&#19982;&#22270;&#28789;&#27979;&#35797;&#19981;&#21516;&#65292;&#22270;&#28789;&#23454;&#39564;&#38656;&#35201;&#27169;&#25311;&#20195;&#34920;&#24615;&#21442;&#19982;&#20154;&#31867;&#21463;&#35797;&#32773;&#30740;&#31350;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#36827;&#34892;TEs&#65292;&#35797;&#22270;&#22797;&#21046;&#20043;&#21069;&#30740;&#31350;&#20013;&#30830;&#31435;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#27169;&#25311;TEs&#30340;&#26041;&#27861;&#65292;&#24182;&#20030;&#20363;&#35828;&#26126;&#20854;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22810;&#22823;&#31243;&#24230;&#19978;&#22797;&#21046;&#32463;&#20856;&#32463;&#27982;&#23398;&#12289;&#24515;&#29702;&#35821;&#35328;&#23398;&#21644;&#31038;&#20250;&#24515;&#29702;&#23398;&#23454;&#39564;&#30340;&#33021;&#21147;&#65306;&#26368;&#32456;&#28216;&#25103;&#12289;&#22253;&#36335;&#21477;&#23376;&#12289;&#31859;&#23572;&#26684;&#25289;&#22982;&#30005;&#20987;&#23454;&#39564;&#21644;&#20247;&#20154;&#30340;&#26234;&#24935;&#12290;&#22312;&#21069;&#19977;&#20010;TEs&#20013;&#65292;&#20351;&#29992;&#26368;&#26032;&#27169;&#22411;&#25104;&#21151;&#22797;&#21046;&#20102;&#29616;&#26377;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#32780;&#26368;&#21518;&#19968;&#20010;TE&#25581;&#31034;&#20102;&#23384;&#22312;&#20110;&#26368;&#26032;&#27169;&#22411;&#20013;&#30340;&#8220;&#36229;&#20934;&#30830;&#24230;&#25197;&#26354;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new type of test, called a Turing Experiment (TE), for evaluating to what extent a given language model, such as GPT models, can simulate different aspects of human behavior. A TE can also reveal consistent distortions in a language model's simulation of a specific human behavior. Unlike the Turing Test, which involves simulating a single arbitrary individual, a TE requires simulating a representative sample of participants in human subject research. We carry out TEs that attempt to replicate well-established findings from prior studies. We design a methodology for simulating TEs and illustrate its use to compare how well different language models are able to reproduce classic economic, psycholinguistic, and social psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings were replicated using recent models, while the last TE reveals a "hyper-accuracy distortion" present in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#30005;&#35270;&#21095;&#26412;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;Multilingual Multiparty Coref&#65288;MMC&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35821;&#35328;&#22810;&#26041;&#23545;&#35805;&#20013;&#30340;&#20849;&#25351;&#35299;&#26512;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#40644;&#37329;&#26631;&#27880;&#65292;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#21019;&#24314;&#20102;&#38134;&#20849;&#25351;&#35299;&#26512;&#25968;&#25454;&#65292;&#24182;&#21457;&#29616;MMC&#22312;&#22810;&#26041;&#20849;&#25351;&#35299;&#26512;&#30340;&#35206;&#30422;&#33539;&#22260;&#19978;&#27604;&#20043;&#21069;&#30340;&#25968;&#25454;&#38598;&#26356;&#24191;&#27867;&#12290;&#22312;&#38134;&#26631;&#27880;&#25968;&#25454;&#19978;&#65292;&#26080;&#35770;&#26159;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#36824;&#26159;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#37117;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#26377;&#25928;&#22320;&#27169;&#25311;&#20102;&#38646;&#23556;&#36328;&#35821;&#35328;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2208.01307</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#22810;&#26041;&#23545;&#35805;&#20013;&#30340;&#20849;&#25351;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Multilingual Coreference Resolution in Multiparty Dialogue. (arXiv:2208.01307v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.01307
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#30005;&#35270;&#21095;&#26412;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;Multilingual Multiparty Coref&#65288;MMC&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35821;&#35328;&#22810;&#26041;&#23545;&#35805;&#20013;&#30340;&#20849;&#25351;&#35299;&#26512;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#40644;&#37329;&#26631;&#27880;&#65292;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#21019;&#24314;&#20102;&#38134;&#20849;&#25351;&#35299;&#26512;&#25968;&#25454;&#65292;&#24182;&#21457;&#29616;MMC&#22312;&#22810;&#26041;&#20849;&#25351;&#35299;&#26512;&#30340;&#35206;&#30422;&#33539;&#22260;&#19978;&#27604;&#20043;&#21069;&#30340;&#25968;&#25454;&#38598;&#26356;&#24191;&#27867;&#12290;&#22312;&#38134;&#26631;&#27880;&#25968;&#25454;&#19978;&#65292;&#26080;&#35770;&#26159;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#36824;&#26159;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#37117;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#26377;&#25928;&#22320;&#27169;&#25311;&#20102;&#38646;&#23556;&#36328;&#35821;&#35328;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22810;&#26041;&#23545;&#35805;&#23454;&#20307;&#20849;&#25351;&#35299;&#26512;&#25968;&#25454;&#38598;&#23578;&#19981;&#23436;&#21892;&#65292;&#24182;&#19988;&#36824;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#23578;&#24453;&#35299;&#20915;&#12290;&#25105;&#20204;&#22522;&#20110;&#30005;&#35270;&#21095;&#26412;&#21019;&#36896;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;Multilingual Multiparty Coref&#65288;MMC&#65289;&#65292;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#12290;&#30001;&#20110;&#22810;&#31181;&#35821;&#35328;&#20013;&#23384;&#22312;&#36136;&#37327;&#36739;&#39640;&#30340;&#23383;&#24149;&#65292;&#25105;&#20204;&#25552;&#20986;&#37325;&#22797;&#20351;&#29992;&#26631;&#27880;&#26469;&#21019;&#24314;&#20854;&#20182;&#35821;&#35328;&#65288;&#20013;&#25991;&#21644;&#27874;&#26031;&#35821;&#65289;&#19978;&#30340;&#38134;&#20849;&#25351;&#35299;&#26512;&#25968;&#25454;&#65292;&#36890;&#36807;&#26631;&#27880;&#20256;&#36882;&#26469;&#23454;&#29616;&#12290;&#22312;&#40644;&#37329;&#26631;&#27880;&#65288;&#33521;&#25991;&#65289;&#25968;&#25454;&#19978;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;MMC&#19978;&#34920;&#29616;&#30456;&#23545;&#36739;&#24046;&#65292;&#36825;&#34920;&#26126;MMC&#22312;&#22810;&#26041;&#20849;&#25351;&#35299;&#26512;&#30340;&#35206;&#30422;&#33539;&#22260;&#19978;&#27604;&#20043;&#21069;&#30340;&#25968;&#25454;&#38598;&#26356;&#24191;&#27867;&#12290;&#22312;&#38134;&#26631;&#27880;&#25968;&#25454;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#26080;&#35770;&#26159;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#36824;&#26159;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#37117;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#36825;&#26377;&#25928;&#22320;&#27169;&#25311;&#20102;&#38646;&#23556;&#36328;&#35821;&#35328;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing multiparty dialogue datasets for entity coreference resolution are nascent, and many challenges are still unaddressed. We create a large-scale dataset, Multilingual Multiparty Coref (MMC), for this task based on TV transcripts. Due to the availability of gold-quality subtitles in multiple languages, we propose reusing the annotations to create silver coreference resolution data in other languages (Chinese and Farsi) via annotation projection. On the gold (English) data, off-the-shelf models perform relatively poorly on MMC, suggesting that MMC has broader coverage of multiparty coreference than prior datasets. On the silver data, we find success both using it for data augmentation and training from scratch, which effectively simulates the zero-shot cross-lingual setting.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;KenSwQuAD - &#19968;&#20221;&#36866;&#29992;&#20110;&#26031;&#29926;&#24076;&#37324;&#35821;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#20197;&#28385;&#36275;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#26426;&#22120;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#30340;&#38656;&#27714;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#20174;&#26031;&#29926;&#24076;&#37324;&#35821;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#21407;&#22987;&#25925;&#20107;&#25991;&#26412;&#20013;&#36827;&#34892;&#27880;&#37322;&#24471;&#21040;&#30340;&#65292;&#21253;&#21547;7,526&#20010;QA&#23545;&#12290;</title><link>http://arxiv.org/abs/2205.02364</link><description>&lt;p&gt;
KenSwQuAD - &#19968;&#20221;&#36866;&#29992;&#20110;&#26031;&#29926;&#24076;&#37324;&#35821;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
KenSwQuAD -- A Question Answering Dataset for Swahili Low Resource Language. (arXiv:2205.02364v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02364
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;KenSwQuAD - &#19968;&#20221;&#36866;&#29992;&#20110;&#26031;&#29926;&#24076;&#37324;&#35821;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#20197;&#28385;&#36275;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#26426;&#22120;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#30340;&#38656;&#27714;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#20174;&#26031;&#29926;&#24076;&#37324;&#35821;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#21407;&#22987;&#25925;&#20107;&#25991;&#26412;&#20013;&#36827;&#34892;&#27880;&#37322;&#24471;&#21040;&#30340;&#65292;&#21253;&#21547;7,526&#20010;QA&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#21160;&#26426;&#26159;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#24320;&#21457;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#24320;&#21457;&#20102;&#26031;&#29926;&#24076;&#37324;&#35821;&#38382;&#31572;&#25968;&#25454;&#38598;KenSwQuAD&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#20174;&#26031;&#29926;&#24076;&#37324;&#35821;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#21407;&#22987;&#25925;&#20107;&#25991;&#26412;&#20013;&#36827;&#34892;&#27880;&#37322;&#30340;&#65292;&#35813;&#35821;&#35328;&#20027;&#35201;&#22312;&#19996;&#38750;&#21644;&#19990;&#30028;&#20854;&#20182;&#22320;&#26041;&#20351;&#29992;&#12290;&#38382;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#23545;&#20110;&#26426;&#22120;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#30340;&#20219;&#21153;&#65288;&#22914;&#20114;&#32852;&#32593;&#25628;&#32034;&#21644;&#23545;&#35805;&#31995;&#32479;&#65289;&#38750;&#24120;&#37325;&#35201;&#12290;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#65292;&#22914;&#26412;&#30740;&#31350;&#24320;&#21457;&#30340;&#40644;&#37329;&#26631;&#20934;&#38382;&#31572;&#38598;&#12290;&#26412;&#39033;&#30446;&#32856;&#35831;&#20102;&#27880;&#35299;&#21592;&#20174;Kencorpus&#39033;&#30446;&#25910;&#38598;&#30340;&#26031;&#29926;&#24076;&#37324;&#35821;&#25991;&#26412;&#20013;&#21046;&#23450;QA&#23545;&#12290;&#35813;&#39033;&#30446;&#23545;&#24635;&#20849;2,585&#20010;&#25991;&#26412;&#36827;&#34892;&#20102;1,445&#20010;&#25991;&#26412;&#30340;&#27880;&#37322;&#65292;&#27599;&#20010;&#27880;&#37322;&#33267;&#23569;&#26377;5&#20010;QA&#23545;&#65292;&#26368;&#32456;&#24418;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;7,526&#20010;QA&#23545;&#30340;&#25968;&#25454;&#38598;&#12290;&#23545;12.5%&#30340;&#27880;&#37322;&#25991;&#26412;&#36827;&#34892;&#30340;&#36136;&#37327;&#20445;&#35777;&#30830;&#35748;&#20102;QA&#23545;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The need for Question Answering datasets in low resource languages is the motivation of this research, leading to the development of Kencorpus Swahili Question Answering Dataset, KenSwQuAD. This dataset is annotated from raw story texts of Swahili low resource language, which is a predominantly spoken in Eastern African and in other parts of the world. Question Answering (QA) datasets are important for machine comprehension of natural language for tasks such as internet search and dialog systems. Machine learning systems need training data such as the gold standard Question Answering set developed in this research. The research engaged annotators to formulate QA pairs from Swahili texts collected by the Kencorpus project, a Kenyan languages corpus. The project annotated 1,445 texts from the total 2,585 texts with at least 5 QA pairs each, resulting into a final dataset of 7,526 QA pairs. A quality assurance set of 12.5% of the annotated texts confirmed that the QA pairs were all correc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#32593;&#32476;&#20998;&#26512;&#26469;&#39044;&#27979;&#37329;&#34701;&#24066;&#22330;&#65292;&#22522;&#20110;&#26032;&#30340;&#25991;&#26412;&#25968;&#25454;&#25351;&#26631;&#65292;&#35780;&#20272;&#32463;&#27982;&#30456;&#20851;&#20851;&#38190;&#35789;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24212;&#29992;&#20110;&#24847;&#22823;&#21033;&#32929;&#31080;&#21644;&#20538;&#21048;&#24066;&#22330;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#25351;&#26631;&#33021;&#22815;&#24456;&#22909;&#22320;&#25429;&#25417;&#21040;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#19981;&#21516;&#38454;&#27573;&#65292;&#24182;&#19988;&#23545;&#20110;&#20538;&#21048;&#24066;&#22330;&#25968;&#25454;&#21644;&#32929;&#31080;&#24066;&#22330;&#27874;&#21160;&#24615;&#37117;&#23384;&#22312;&#26126;&#26174;&#30340;&#21487;&#39044;&#27979;&#24615;&#12290;</title><link>http://arxiv.org/abs/2009.04975</link><description>&lt;p&gt;
&#22312;COVID-19&#21361;&#26426;&#20013;&#20351;&#29992;&#35821;&#20041;&#32593;&#32476;&#20998;&#26512;&#39044;&#27979;&#37329;&#34701;&#24066;&#22330;
&lt;/p&gt;
&lt;p&gt;
Forecasting financial markets with semantic network analysis in the COVID-19 crisis. (arXiv:2009.04975v4 [q-fin.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.04975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#32593;&#32476;&#20998;&#26512;&#26469;&#39044;&#27979;&#37329;&#34701;&#24066;&#22330;&#65292;&#22522;&#20110;&#26032;&#30340;&#25991;&#26412;&#25968;&#25454;&#25351;&#26631;&#65292;&#35780;&#20272;&#32463;&#27982;&#30456;&#20851;&#20851;&#38190;&#35789;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24212;&#29992;&#20110;&#24847;&#22823;&#21033;&#32929;&#31080;&#21644;&#20538;&#21048;&#24066;&#22330;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#25351;&#26631;&#33021;&#22815;&#24456;&#22909;&#22320;&#25429;&#25417;&#21040;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#19981;&#21516;&#38454;&#27573;&#65292;&#24182;&#19988;&#23545;&#20110;&#20538;&#21048;&#24066;&#22330;&#25968;&#25454;&#21644;&#32929;&#31080;&#24066;&#22330;&#27874;&#21160;&#24615;&#37117;&#23384;&#22312;&#26126;&#26174;&#30340;&#21487;&#39044;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#25968;&#25454;&#25351;&#26631;&#26469;&#39044;&#27979;&#32929;&#31080;&#24066;&#22330;&#25968;&#25454;&#12290;&#35813;&#25351;&#26631;&#24212;&#29992;&#20110;&#22823;&#37327;&#26032;&#38395;&#20013;&#65292;&#35780;&#20272;&#25991;&#26412;&#20013;&#20986;&#29616;&#30340;&#19968;&#20010;&#25110;&#22810;&#20010;&#19982;&#32463;&#27982;&#30456;&#20851;&#30340;&#20851;&#38190;&#35789;&#30340;&#37325;&#35201;&#24615;&#12290;&#35813;&#25351;&#26631;&#26681;&#25454;&#20851;&#38190;&#35789;&#30340;&#20351;&#29992;&#39057;&#29575;&#21644;&#35821;&#20041;&#32593;&#32476;&#20301;&#32622;&#35780;&#20272;&#20854;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#24847;&#22823;&#21033;&#26032;&#38395;&#65292;&#24182;&#26500;&#24314;&#25351;&#25968;&#26469;&#39044;&#27979;&#24847;&#22823;&#21033;&#32929;&#31080;&#21644;&#20538;&#21048;&#24066;&#22330;&#30340;&#22238;&#25253;&#29575;&#21644;&#27874;&#21160;&#24615;&#65292;&#21253;&#25324;COVID-19&#21361;&#26426;&#26399;&#38388;&#30340;&#25968;&#25454;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#25351;&#26631;&#24456;&#22909;&#22320;&#25429;&#25417;&#21040;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#19981;&#21516;&#38454;&#27573;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20538;&#21048;&#24066;&#22330;&#25968;&#25454;&#65292;&#26080;&#35770;&#26159;&#22238;&#25253;&#29575;&#36824;&#26159;&#27874;&#21160;&#24615;&#65292;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#21040;&#26399;&#26085;&#20197;&#21450;&#32929;&#31080;&#24066;&#22330;&#27874;&#21160;&#24615;&#26041;&#38754;&#37117;&#23384;&#22312;&#26126;&#26174;&#30340;&#21487;&#39044;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper uses a new textual data index for predicting stock market data. The index is applied to a large set of news to evaluate the importance of one or more general economic-related keywords appearing in the text. The index assesses the importance of the economic-related keywords, based on their frequency of use and semantic network position. We apply it to the Italian press and construct indices to predict Italian stock and bond market returns and volatilities in a recent sample period, including the COVID-19 crisis. The evidence shows that the index captures the different phases of financial time series well. Moreover, results indicate strong evidence of predictability for bond market data, both returns and volatilities, short and long maturities, and stock market volatility.
&lt;/p&gt;</description></item></channel></rss>