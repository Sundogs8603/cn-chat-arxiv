<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25216;&#26415;&#65292;&#31216;&#20026;&#33258;&#25105;&#23545;&#25239;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#65288;SPIN-Diffusion&#65289;&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20808;&#21069;&#29256;&#26412;&#30340;&#31454;&#20105;&#65292;&#23454;&#29616;&#20102;&#36880;&#27493;&#33258;&#25105;&#25913;&#36827;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.10210</link><description>&lt;p&gt;
&#33258;&#25105;&#23545;&#25239;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25216;&#26415;&#65292;&#31216;&#20026;&#33258;&#25105;&#23545;&#25239;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#65288;SPIN-Diffusion&#65289;&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20808;&#21069;&#29256;&#26412;&#30340;&#31454;&#20105;&#65292;&#23454;&#29616;&#20102;&#36880;&#27493;&#33258;&#25105;&#25913;&#36827;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#21069;&#27839;&#65292;&#23588;&#20854;&#26159;&#19982;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24494;&#35843;&#26041;&#38754;&#21462;&#24471;&#30340;&#26174;&#33879;&#36827;&#23637;&#30456;&#27604;&#12290;&#23613;&#31649;&#29616;&#22312;&#30340;&#20808;&#36827;&#25193;&#25955;&#27169;&#22411;&#22914;&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#21644;SDXL&#20381;&#36182;&#20110;&#30417;&#30563;&#24494;&#35843;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#22312;&#35266;&#23519;&#21040;&#19968;&#23450;&#25968;&#37327;&#30340;&#25968;&#25454;&#21518;&#24517;&#28982;&#20250;&#36798;&#21040;&#29942;&#39048;&#12290;&#26368;&#36817;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#34987;&#24212;&#29992;&#20110;&#36890;&#36807;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20294;&#27599;&#20010;&#25991;&#26412;&#25552;&#31034;&#38656;&#35201;&#33267;&#23569;&#20004;&#20010;&#22270;&#20687;&#65288;&#8220;&#33719;&#32988;&#32773;&#8221;&#21644;&#8220;&#22833;&#36133;&#32773;&#8221;&#22270;&#20687;&#65289;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25216;&#26415;&#65292;&#31216;&#20026;&#33258;&#25105;&#23545;&#25239;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#65288;SPIN-Diffusion&#65289;&#65292;&#20854;&#20013;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20808;&#21069;&#29256;&#26412;&#36827;&#34892;&#31454;&#20105;&#65292;&#20419;&#36827;&#20102;&#19968;&#20010;&#36845;&#20195;&#30340;&#33258;&#25105;&#25913;&#36827;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#20256;&#32479;&#30417;&#30563;&#24494;&#35843;&#21644;RL&#31574;&#30053;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10210v1 Announce Type: cross  Abstract: Fine-tuning Diffusion Models remains an underexplored frontier in generative artificial intelligence (GenAI), especially when compared with the remarkable progress made in fine-tuning Large Language Models (LLMs). While cutting-edge diffusion models such as Stable Diffusion (SD) and SDXL rely on supervised fine-tuning, their performance inevitably plateaus after seeing a certain volume of data. Recently, reinforcement learning (RL) has been employed to fine-tune diffusion models with human preference data, but it requires at least two images ("winner" and "loser" images) for each text prompt. In this paper, we introduce an innovative technique called self-play fine-tuning for diffusion models (SPIN-Diffusion), where the diffusion model engages in competition with its earlier versions, facilitating an iterative self-improvement process. Our approach offers an alternative to conventional supervised fine-tuning and RL strategies, signific
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24674;&#22797;&#29983;&#25104;&#27169;&#22411;&#39044;&#24494;&#35843;&#26435;&#37325;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#20302;&#31209;&#24494;&#35843;&#27169;&#22411;&#21487;&#20197;&#24674;&#22797;&#20934;&#30830;&#30340;&#39044;&#24494;&#35843;&#26435;&#37325;&#65292;&#21033;&#29992;&#36825;&#20010;&#26032;&#28431;&#27934;&#25915;&#20987;&#22823;&#35268;&#27169;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.10208</link><description>&lt;p&gt;
&#24674;&#22797;&#29983;&#25104;&#27169;&#22411;&#30340;&#39044;&#24494;&#35843;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
Recovering the Pre-Fine-Tuning Weights of Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10208
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24674;&#22797;&#29983;&#25104;&#27169;&#22411;&#39044;&#24494;&#35843;&#26435;&#37325;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#20302;&#31209;&#24494;&#35843;&#27169;&#22411;&#21487;&#20197;&#24674;&#22797;&#20934;&#30830;&#30340;&#39044;&#24494;&#35843;&#26435;&#37325;&#65292;&#21033;&#29992;&#36825;&#20010;&#26032;&#28431;&#27934;&#25915;&#20987;&#22823;&#35268;&#27169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#65292;&#20027;&#27969;&#27169;&#24335;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;i) &#22312;&#22823;&#35268;&#27169;&#20294;&#19981;&#23433;&#20840;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;ii) &#36890;&#36807;&#24494;&#35843;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#12290;&#36825;&#31181;&#20570;&#27861;&#34987;&#35748;&#20026;&#26159;&#23433;&#20840;&#30340;&#65292;&#22240;&#20026;&#30446;&#21069;&#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#19981;&#23433;&#20840;&#30340;&#39044;&#24494;&#35843;&#27169;&#22411;&#26435;&#37325;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#36825;&#31181;&#20551;&#35774;&#36890;&#24120;&#26159;&#38169;&#35823;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#35889;&#21453;&#35843;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#20302;&#31209;&#65288;LoRA&#65289;&#24494;&#35843;&#27169;&#22411;&#24674;&#22797;&#39044;&#24494;&#35843;&#27169;&#22411;&#30340;&#26435;&#37325;&#12290;&#19982;&#20808;&#21069;&#35797;&#22270;&#24674;&#22797;&#39044;&#24494;&#35843;&#33021;&#21147;&#30340;&#25915;&#20987;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#24674;&#22797;&#31934;&#30830;&#30340;&#39044;&#24494;&#35843;&#26435;&#37325;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#36825;&#20010;&#26032;&#30340;&#23545;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#28431;&#27934;&#65292;&#20363;&#22914;&#20010;&#24615;&#21270;&#30340;&#31283;&#23450;&#25193;&#25955;&#21644;&#23545;&#40784;&#30340;Mistral&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10208v1 Announce Type: cross  Abstract: The dominant paradigm in generative modeling consists of two steps: i) pre-training on a large-scale but unsafe dataset, ii) aligning the pre-trained model with human values via fine-tuning. This practice is considered safe, as no current method can recover the unsafe, pre-fine-tuning model weights. In this paper, we demonstrate that this assumption is often false. Concretely, we present Spectral DeTuning, a method that can recover the weights of the pre-fine-tuning model using a few low-rank (LoRA) fine-tuned models. In contrast to previous attacks that attempt to recover pre-fine-tuning capabilities, our method aims to recover the exact pre-fine-tuning weights. Our approach exploits this new vulnerability against large-scale models such as a personalized Stable Diffusion and an aligned Mistral.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#20010;&#22870;&#21169;&#26465;&#20214;&#25511;&#21046;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#36827;&#34892;&#23545;&#40784;&#12290;&#23427;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.10207</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22870;&#21169;&#65306;&#22522;&#20110;&#21160;&#24577;&#20559;&#22909;&#35843;&#25972;&#30340;&#22810;&#30446;&#26631;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#20010;&#22870;&#21169;&#26465;&#20214;&#25511;&#21046;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#36827;&#34892;&#23545;&#40784;&#12290;&#23427;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#22522;&#30784;&#27169;&#22411;&#22810;&#30446;&#26631;&#23545;&#40784;&#38382;&#39064;&#65292;&#36825;&#26159;&#23454;&#29616;&#26377;&#30410;&#21644;&#26080;&#23475;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#23545;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#36890;&#24120;&#26159;&#26114;&#36149;&#19988;&#19981;&#31283;&#23450;&#30340;&#65292;&#24182;&#19988;&#20154;&#31867;&#20559;&#22909;&#30340;&#22810;&#32500;&#24230;&#12289;&#24322;&#36136;&#24615;&#21644;&#20914;&#31361;&#24615;&#36827;&#19968;&#27493;&#22797;&#26434;&#21270;&#20102;&#23545;&#40784;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#23427;&#20351;&#24471;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#21462;&#20915;&#20110;&#20854;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#30340;&#22810;&#20010;&#22870;&#21169;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#26469;&#36827;&#34892;&#23545;&#40784;&#12290;RiC&#30340;&#26174;&#33879;&#29305;&#28857;&#26159;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#23545;&#21333;&#20010;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;&#21463;&#21040;&#25277;&#35937;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#26512;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25512;&#29702;&#26102;&#35843;&#25972;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10207v1 Announce Type: cross  Abstract: We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method appro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#25913;&#21464;&#35299;&#30721;&#36807;&#31243;&#32780;&#19981;&#26159;&#20351;&#29992;&#25552;&#31034;&#24037;&#31243;&#65292;&#21487;&#20197;&#20174;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20986;&#24605;&#32500;&#38142;&#25512;&#29702;&#36335;&#24452;&#65292;&#36825;&#31181;&#26041;&#27861;&#32469;&#36807;&#20102;&#25552;&#31034;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#21487;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#20869;&#22312;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10200</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#25552;&#31034;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought Reasoning Without Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#25913;&#21464;&#35299;&#30721;&#36807;&#31243;&#32780;&#19981;&#26159;&#20351;&#29992;&#25552;&#31034;&#24037;&#31243;&#65292;&#21487;&#20197;&#20174;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20986;&#24605;&#32500;&#38142;&#25512;&#29702;&#36335;&#24452;&#65292;&#36825;&#31181;&#26041;&#27861;&#32469;&#36807;&#20102;&#25552;&#31034;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#21487;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#20869;&#22312;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#29305;&#23450;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#22914;&#23569;&#26679;&#26412;&#25110;&#38646;&#26679;&#26412;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#12290;&#36825;&#20123;&#26041;&#27861;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#24448;&#24448;&#38656;&#35201;&#25163;&#21160;&#36827;&#34892;&#22823;&#37327;&#30340;&#25552;&#31034;&#24037;&#31243;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65306;LLMs&#26159;&#21542;&#21487;&#20197;&#22312;&#27809;&#26377;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#25512;&#29702;&#65311;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#36890;&#36807;&#31616;&#21333;&#22320;&#25913;&#21464;&#35299;&#30721;&#36807;&#31243;&#65292;&#23601;&#21487;&#20197;&#20174;&#39044;&#35757;&#32451;&#30340;LLMs&#20013;&#24341;&#20986;&#24605;&#32500;&#38142;&#25512;&#29702;&#36335;&#24452;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21069;$k$&#20010;&#26367;&#20195;&#26631;&#35760;&#65292;&#21457;&#29616;&#36825;&#20123;&#24207;&#21015;&#20013;&#32463;&#24120;&#23384;&#22312;&#24605;&#32500;&#38142;&#36335;&#24452;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#32469;&#36807;&#20102;&#25552;&#31034;&#30340;&#28151;&#28102;&#22240;&#32032;&#65292;&#36824;&#21487;&#20197;&#35780;&#20272;LLMs&#30340;&#20869;&#22312;&#25512;&#29702;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#35299;&#30721;&#36335;&#24452;&#20013;&#30340;&#24605;&#32500;&#38142;&#23384;&#22312;&#19982;&#27169;&#22411;&#35299;&#30721;&#30340;&#32622;&#20449;&#24230;&#36739;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10200v1 Announce Type: new  Abstract: In enhancing the reasoning capabilities of large language models (LLMs), prior research primarily focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while effective, often involve manually intensive prompt engineering. Our study takes a novel approach by asking: Can LLMs reason effectively without prompting? Our findings reveal that, intriguingly, CoT reasoning paths can be elicited from pre-trained LLMs by simply altering the \textit{decoding} process. Rather than conventional greedy decoding, we investigate the top-$k$ alternative tokens, uncovering that CoT paths are frequently inherent in these sequences. This approach not only bypasses the confounders of prompting but also allows us to assess the LLMs' \textit{intrinsic} reasoning abilities. Moreover, we observe that the presence of a CoT in the decoding path correlates with a higher confidence in the model's decod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#31532;&#19968;&#20010;&#23545;&#35821;&#35328;&#20195;&#29702;&#30340;&#25932;&#23545;&#25915;&#20987;&#36827;&#34892;&#26144;&#23556;&#30340;&#31995;&#32479;&#21270;&#21162;&#21147;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#27010;&#24565;&#26694;&#26550;&#26469;&#30740;&#31350;&#36825;&#20123;&#25915;&#20987;&#12290;&#36825;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;&#35821;&#35328;&#20195;&#29702;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2402.10196</link><description>&lt;p&gt;
&#19968;&#24231;&#25671;&#25671;&#27442;&#22368;&#30340;&#32440;&#29260;&#23627;&#65311;&#23545;&#35821;&#35328;&#20195;&#29702;&#30340;&#25932;&#23545;&#25915;&#20987;&#30340;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
A Trembling House of Cards? Mapping Adversarial Attacks against Language Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#31532;&#19968;&#20010;&#23545;&#35821;&#35328;&#20195;&#29702;&#30340;&#25932;&#23545;&#25915;&#20987;&#36827;&#34892;&#26144;&#23556;&#30340;&#31995;&#32479;&#21270;&#21162;&#21147;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#27010;&#24565;&#26694;&#26550;&#26469;&#30740;&#31350;&#36825;&#20123;&#25915;&#20987;&#12290;&#36825;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;&#35821;&#35328;&#20195;&#29702;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#35821;&#35328;&#20195;&#29702;&#22312;&#21457;&#23637;&#20013;&#36805;&#29467;&#12290;&#23427;&#20204;&#21033;&#29992;&#35821;&#35328;&#20316;&#20026;&#24605;&#24819;&#21644;&#20132;&#27969;&#30340;&#24037;&#20855;&#65292;&#36171;&#20104;&#20102;&#26080;&#27604;&#30340;&#28789;&#27963;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#20154;&#20204;&#36805;&#36895;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#23558;LLMs&#36830;&#25509;&#21040;&#21508;&#31181;&#22806;&#37096;&#32452;&#20214;&#21644;&#29615;&#22659;&#20013;&#65306;&#25968;&#25454;&#24211;&#65292;&#24037;&#20855;&#65292;&#22240;&#29305;&#32593;&#65292;&#26426;&#22120;&#20154;&#23454;&#20307;&#31561;&#12290;&#35768;&#22810;&#20154;&#30456;&#20449;&#19968;&#31181;&#21069;&#25152;&#26410;&#26377;&#30340;&#24378;&#22823;&#33258;&#21160;&#21270;&#25216;&#26415;&#27491;&#22312;&#23835;&#36215;&#12290;&#28982;&#32780;&#65292;&#26032;&#30340;&#33258;&#21160;&#21270;&#25216;&#26415;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22797;&#26434;&#30340;&#31995;&#32479;&#22914;&#35821;&#35328;&#20195;&#29702;&#12290;&#20182;&#20204;&#30340;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;&#36895;&#24230;&#21644;&#35268;&#27169;&#19982;&#25105;&#20204;&#23545;&#20854;&#23433;&#20840;&#39118;&#38505;&#30340;&#29702;&#35299;&#20043;&#38388;&#23384;&#22312;&#30528;&#20196;&#20154;&#24778;&#35766;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#26159;&#21542;&#27491;&#22312;&#24314;&#36896;&#19968;&#24231;&#32440;&#29260;&#23627;&#65311;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#31995;&#32479;&#22320;&#23545;&#35821;&#35328;&#20195;&#29702;&#30340;&#25932;&#23545;&#25915;&#20987;&#36827;&#34892;&#20102;&#26144;&#23556;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#27010;&#24565;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10196v1 Announce Type: cross  Abstract: Language agents powered by large language models (LLMs) have seen exploding development. Their capability of using language as a vehicle for thought and communication lends an incredible level of flexibility and versatility. People have quickly capitalized on this capability to connect LLMs to a wide range of external components and environments: databases, tools, the Internet, robotic embodiment, etc. Many believe an unprecedentedly powerful automation technology is emerging. However, new automation technologies come with new safety risks, especially for intricate systems like language agents. There is a surprisingly large gap between the speed and scale of their development and deployment and our understanding of their safety risks. Are we building a house of cards? In this position paper, we present the first systematic effort in mapping adversarial attacks against language agents. We first present a unified conceptual framework for
&lt;/p&gt;</description></item><item><title>BitDelta&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#20887;&#20313;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BitDelta&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#24494;&#35843;&#36807;&#31243;&#20013;&#28155;&#21152;&#30340;&#20449;&#24687;&#37327;&#21270;&#20026;&#19968;&#20010;&#27604;&#29305;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#20110;&#22810;&#31199;&#25143;&#27169;&#22411;&#30340;&#26381;&#21153;&#21644;&#23384;&#20648;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;GPU&#20869;&#23384;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.10193</link><description>&lt;p&gt;
BitDelta&#65306;&#20320;&#30340;&#24494;&#35843;&#21487;&#33021;&#21482;&#26377;&#19968;&#20010;&#27604;&#29305;&#30340;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
BitDelta: Your Fine-Tune May Only Be Worth One Bit
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10193
&lt;/p&gt;
&lt;p&gt;
BitDelta&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#20887;&#20313;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BitDelta&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#24494;&#35843;&#36807;&#31243;&#20013;&#28155;&#21152;&#30340;&#20449;&#24687;&#37327;&#21270;&#20026;&#19968;&#20010;&#27604;&#29305;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#20110;&#22810;&#31199;&#25143;&#27169;&#22411;&#30340;&#26381;&#21153;&#21644;&#23384;&#20648;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;GPU&#20869;&#23384;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#22312;&#20004;&#20010;&#38454;&#27573;&#36827;&#34892;&#35757;&#32451;&#65306;&#22312;&#22823;&#35268;&#27169;&#20114;&#32852;&#32593;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#30340;&#39640;&#35745;&#31639;&#38656;&#27714;&#65292;&#30452;&#35273;&#19978;&#35748;&#20026;&#24494;&#35843;&#23545;&#27169;&#22411;&#30340;&#20449;&#24687;&#28155;&#21152;&#36739;&#23569;&#65292;&#22240;&#27492;&#26356;&#20855;&#26377;&#21487;&#21387;&#32553;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#24494;&#35843;&#27169;&#22411;&#30340;&#26435;&#37325;&#20998;&#35299;&#20026;&#39044;&#35757;&#32451;&#32452;&#20214;&#21644;&#39069;&#22806;&#30340;&#22686;&#37327;&#26469;&#25506;&#31350;&#36825;&#19968;&#20551;&#35774;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#8212;&#8212;BitDelta&#65292;&#25104;&#21151;&#22320;&#23558;&#36825;&#20010;&#22686;&#37327;&#37327;&#21270;&#20026;1&#27604;&#29305;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;&#36825;&#19968;&#26377;&#36259;&#30340;&#21457;&#29616;&#19981;&#20165;&#31361;&#26174;&#20102;&#24494;&#35843;&#36807;&#31243;&#20013;&#28155;&#21152;&#30340;&#20449;&#24687;&#30340;&#28508;&#22312;&#20887;&#20313;&#24615;&#65292;&#32780;&#19988;&#23545;&#20110;&#22810;&#31199;&#25143;&#27169;&#22411;&#30340;&#26381;&#21153;&#21644;&#23384;&#20648;&#20063;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#39640;&#31934;&#24230;&#30340;&#22522;&#30784;&#27169;&#22411;&#20197;&#21450;&#22810;&#20010;1&#27604;&#29305;&#30340;&#22686;&#37327;&#65292;BitDelta&#22823;&#22823;&#38477;&#20302;&#20102;GPU&#20869;&#23384;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10193v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are typically trained in two phases: pre-training on large internet-scale datasets, and fine-tuning for downstream tasks. Given the higher computational demand of pre-training, it's intuitive to assume that fine-tuning adds less new information to the model, and is thus more compressible. We explore this assumption by decomposing the weights of fine-tuned models into their pre-trained components and an additional delta. We introduce a simple method, BitDelta, which successfully quantizes this delta down to 1 bit without compromising performance. This interesting finding not only highlights the potential redundancy of information added during fine-tuning, but also has significant implications for the multi-tenant serving and multi-tenant storage of fine-tuned models. By enabling the use of a single high-precision base model accompanied by multiple 1-bit deltas, BitDelta dramatically reduces GPU memory requir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#21253;&#25324;&#28436;&#31034;&#20135;&#29983;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#27169;&#22411;&#37197;&#32622;&#30340;&#27169;&#31946;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10189</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20998;&#35299;&#21644;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Decomposition and Quantification for In-Context Learning of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#21253;&#25324;&#28436;&#31034;&#20135;&#29983;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#27169;&#22411;&#37197;&#32622;&#30340;&#27169;&#31946;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31361;&#30772;&#24615;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22312;&#25552;&#31034;&#20013;&#25552;&#20379;&#19968;&#20123;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#28436;&#31034;&#26469;&#24443;&#24213;&#25913;&#21464;&#20102;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;LLM&#21709;&#24212;&#20013;&#30340;&#21487;&#20449;&#38382;&#39064;&#65292;&#22914;&#24187;&#35273;&#65292;&#20063;&#34987;&#31215;&#26497;&#35752;&#35770;&#12290;&#29616;&#26377;&#24037;&#20316;&#33268;&#21147;&#20110;&#37327;&#21270;LLM&#21709;&#24212;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;LLM&#30340;&#22797;&#26434;&#24615;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#29420;&#29305;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#30456;&#20851;&#30340;LLM&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#24378;&#35843;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#21487;&#33021;&#26469;&#33258;&#20110;&#25552;&#20379;&#30340;&#28436;&#31034;&#65288;aleatoric&#19981;&#30830;&#23450;&#24615;&#65289;&#21644;&#19982;&#27169;&#22411;&#37197;&#32622;&#30456;&#20851;&#30340;&#27169;&#31946;&#24615;&#65288;epistemic&#19981;&#30830;&#23450;&#24615;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24335;&#21644;&#30456;&#24212;&#30340;&#20272;&#35745;&#26041;&#27861;&#26469;&#37327;&#21270;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#20026;&#29702;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;&#37324;&#30340;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10189v1 Announce Type: new  Abstract: In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM's response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM's response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model's configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-cont
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#65292;&#20174;&#22270;&#35770;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;RLHF&#20013;&#22870;&#21169;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#25104;&#26412;&#26631;&#27880;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10184</link><description>&lt;p&gt;
&#37325;&#22609;RLHF&#20013;&#30340;&#20449;&#24687;&#32467;&#26500;&#65306;&#22522;&#20110;&#22270;&#35770;&#30340;&#22870;&#21169;&#27867;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#65292;&#20174;&#22270;&#35770;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;RLHF&#20013;&#22870;&#21169;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#25104;&#26412;&#26631;&#27880;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#65288;RLHF&#65289;&#23384;&#22312;&#19968;&#20010;&#19977;&#38590;&#38382;&#39064;&#65306;&#39640;&#24230;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#26631;&#27880;&#25104;&#26412;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#20043;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#26469;&#32531;&#35299;&#36825;&#31181;&#19981;&#20860;&#23481;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;RLHF&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#23558;&#20854;&#25551;&#32472;&#20026;&#25991;&#26412;&#20998;&#24067;&#19978;&#30340;&#33258;&#21160;&#32534;&#30721;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24418;&#24335;&#21270;&#20102;RLHF&#30446;&#26631;&#65292;&#21363;&#30830;&#20445;&#20154;&#31867;&#20559;&#22909;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34892;&#20026;&#20043;&#38388;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;RLHF&#22870;&#21169;&#24314;&#27169;&#38454;&#27573;&#20013;&#20449;&#24687;&#32467;&#26500;&#30340;&#24615;&#33021;&#24433;&#21709;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#29702;&#35299;&#22870;&#21169;&#24314;&#27169;&#38454;&#27573;&#20013;&#30340;&#22870;&#21169;&#27867;&#21270;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#22270;&#35770;&#30340;&#26041;&#27861;&#26469;&#24314;&#27169;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#27867;&#21270;&#12290;&#20854;&#20013;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10184v1 Announce Type: cross  Abstract: There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance. Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling. Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions. Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and large language model (LLM) behavior. Building on this framework, we then systematically investigate the performance impact of information structure in the reward modeling stage of RLHF. To further understand reward generalization in the reward modeling stage, we introduce a new method based on random graph theory that models generalization in the semantic space. A key insight of
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#20219;&#21153;&#20998;&#35299;&#21644;&#20195;&#29702;&#29983;&#25104;&#30340;&#22810;&#20195;&#29702;&#26694;&#26550;(TDAG)&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#35299;&#20915;&#22797;&#26434;&#30340;&#23454;&#38469;&#38382;&#39064;&#26102;&#25552;&#39640;&#20195;&#29702;&#30340;&#36866;&#24212;&#24615;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;ItineraryBench&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#26053;&#34892;&#35268;&#21010;&#20013;&#20855;&#26377;&#31934;&#32454;&#35780;&#20272;&#31995;&#32479;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2402.10178</link><description>&lt;p&gt;
TDAG:&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#20219;&#21153;&#20998;&#35299;&#21644;&#20195;&#29702;&#29983;&#25104;&#30340;&#22810;&#20195;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TDAG: A Multi-Agent Framework based on Dynamic Task Decomposition and Agent Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10178
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#20219;&#21153;&#20998;&#35299;&#21644;&#20195;&#29702;&#29983;&#25104;&#30340;&#22810;&#20195;&#29702;&#26694;&#26550;(TDAG)&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#35299;&#20915;&#22797;&#26434;&#30340;&#23454;&#38469;&#38382;&#39064;&#26102;&#25552;&#39640;&#20195;&#29702;&#30340;&#36866;&#24212;&#24615;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;ItineraryBench&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#26053;&#34892;&#35268;&#21010;&#20013;&#20855;&#26377;&#31934;&#32454;&#35780;&#20272;&#31995;&#32479;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs)&#22914;ChatGPT&#30340;&#20986;&#29616;&#65292;&#21551;&#21457;&#20102;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#30340;&#24320;&#21457;&#65292;&#33021;&#22815;&#35299;&#20915;&#22797;&#26434;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20195;&#29702;&#22312;&#25191;&#34892;&#20219;&#21153;&#26102;&#24120;&#24120;&#30001;&#20110;&#26041;&#27861;&#35770;&#32422;&#26463;&#65288;&#22914;&#38169;&#35823;&#20256;&#25773;&#21644;&#36866;&#24212;&#24615;&#21463;&#38480;&#65289;&#32780;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#20219;&#21153;&#20998;&#35299;&#21644;&#20195;&#29702;&#29983;&#25104;&#30340;&#22810;&#20195;&#29702;&#26694;&#26550;(TDAG)&#12290;&#35813;&#26694;&#26550;&#21160;&#24577;&#22320;&#23558;&#22797;&#26434;&#20219;&#21153;&#20998;&#35299;&#20026;&#26356;&#23567;&#30340;&#23376;&#20219;&#21153;&#65292;&#24182;&#23558;&#27599;&#20010;&#23376;&#20219;&#21153;&#20998;&#37197;&#32473;&#19968;&#20010;&#29305;&#21035;&#29983;&#25104;&#30340;&#23376;&#20195;&#29702;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22312;&#22810;&#26679;&#21270;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#24212;&#24615;&#12290;&#21516;&#26102;&#65292;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#24448;&#24448;&#32570;&#20047;&#35780;&#20272;&#22797;&#26434;&#12289;&#22810;&#27493;&#39588;&#20219;&#21153;&#20013;&#36882;&#22686;&#36827;&#23637;&#25152;&#38656;&#30340;&#32454;&#31890;&#24230;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#26053;&#34892;&#35268;&#21010;&#30340;&#32972;&#26223;&#19979;&#24341;&#20837;&#20102;ItineraryBench&#65292;&#23427;&#20855;&#26377;&#20114;&#36830;&#12289;&#36880;&#28176;&#22797;&#26434;&#30340;&#20219;&#21153;&#21644;&#32454;&#31890;&#24230;&#30340;&#35780;&#20272;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10178v1 Announce Type: new  Abstract: The emergence of Large Language Models (LLMs) like ChatGPT has inspired the development of LLM-based agents capable of addressing complex, real-world tasks. However, these agents often struggle during task execution due to methodological constraints, such as error propagation and limited adaptability. To address this issue, we propose a multi-agent framework based on dynamic Task Decomposition and Agent Generation (TDAG). This framework dynamically decomposes complex tasks into smaller subtasks and assigns each to a specifically generated subagent, thereby enhancing adaptability in diverse and unpredictable real-world tasks. Simultaneously, existing benchmarks often lack the granularity needed to evaluate incremental progress in complex, multi-step tasks. In response, we introduce ItineraryBench in the context of travel planning, featuring interconnected, progressively complex tasks with a fine-grained evaluation system. ItineraryBench i
&lt;/p&gt;</description></item><item><title>OpenMathInstruct-1&#26159;&#19968;&#20010;&#21253;&#21547;180&#19975;&#20010;&#25968;&#23398;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#27861;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21512;&#25104;&#24320;&#28304;LLM&#30340;&#20195;&#30721;&#35299;&#37322;&#22120;&#35299;&#20915;&#26041;&#26696;&#26469;&#26500;&#24314;&#65292;&#22635;&#34917;&#20102;&#30446;&#21069;&#24320;&#28304;LLM&#22312;&#25968;&#23398;&#25216;&#33021;&#26041;&#38754;&#19982;&#38381;&#28304;LLM&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.10176</link><description>&lt;p&gt;
OpenMathInstruct-1: &#19968;&#20010;&#25317;&#26377;180&#19975;&#20010;&#25968;&#23398;&#25945;&#23398;&#35843;&#20248;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10176
&lt;/p&gt;
&lt;p&gt;
OpenMathInstruct-1&#26159;&#19968;&#20010;&#21253;&#21547;180&#19975;&#20010;&#25968;&#23398;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#27861;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21512;&#25104;&#24320;&#28304;LLM&#30340;&#20195;&#30721;&#35299;&#37322;&#22120;&#35299;&#20915;&#26041;&#26696;&#26469;&#26500;&#24314;&#65292;&#22635;&#34917;&#20102;&#30446;&#21069;&#24320;&#28304;LLM&#22312;&#25968;&#23398;&#25216;&#33021;&#26041;&#38754;&#19982;&#38381;&#28304;LLM&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#21512;&#25104;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#20026;&#20102;&#33719;&#24471;&#29305;&#23450;&#30340;&#25216;&#33021;&#12290;&#30446;&#21069;&#30340;&#22823;&#35268;&#27169;&#25968;&#23398;&#25945;&#23398;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#22914;MetaMathQA&#21644;MAmmoTH&#65292;&#26159;&#20351;&#29992;&#26469;&#33258;&#21830;&#19994;&#38480;&#21046;&#35768;&#21487;&#30340;&#38381;&#28304;LLM&#30340;&#36755;&#20986;&#26500;&#24314;&#30340;&#12290;&#38480;&#21046;&#22312;&#36825;&#20123;&#25968;&#25454;&#29983;&#25104;&#27969;&#31243;&#20013;&#20351;&#29992;&#24320;&#28304;LLM&#30340;&#19968;&#20010;&#20851;&#38190;&#21407;&#22240;&#26159;&#30446;&#21069;&#26368;&#22909;&#30340;&#38381;&#28304;LLM&#65288;&#22914;GPT-4&#65289;&#21644;&#26368;&#22909;&#30340;&#24320;&#28304;LLM&#20043;&#38388;&#22312;&#25968;&#23398;&#25216;&#33021;&#19978;&#23384;&#22312;&#24456;&#22823;&#24046;&#36317;&#12290;&#22522;&#20110;&#24320;&#28304;LLM&#30340;&#26368;&#36817;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#25552;&#31034;&#26041;&#24335;&#21644;&#19968;&#20123;&#24378;&#21147;&#32553;&#25918;&#65292;&#26500;&#24314;&#20102;OpenMathInstruct-1&#65292;&#19968;&#20010;&#25317;&#26377;180&#19975;&#20010;&#38382;&#39064;-&#35299;&#20915;&#26041;&#27861;&#23545;&#30340;&#25968;&#23398;&#25945;&#23398;&#35843;&#20248;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#20351;&#29992;GSM8K&#21644;MATH&#36825;&#20004;&#20010;&#27969;&#34892;&#30340;&#25968;&#23398;&#25512;&#29702;&#22522;&#20934;&#30340;&#20195;&#30721;&#35299;&#37322;&#22120;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#21512;&#25104;&#26500;&#24314;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10176v1 Announce Type: cross  Abstract: Recent work has shown the immense potential of synthetically generated datasets for training large language models (LLMs), especially for acquiring targeted skills. Current large-scale math instruction tuning datasets such as MetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed using outputs from closed-source LLMs with commercially restrictive licenses. A key reason limiting the use of open-source LLMs in these data generation pipelines has been the wide gap between the mathematical skills of the best closed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on the recent progress in open-source LLMs, our proposed prompting novelty, and some brute-force scaling, we construct OpenMathInstruct-1, a math instruction tuning dataset with 1.8M problem-solution pairs. The dataset is constructed by synthesizing code-interpreter solutions for GSM8K and MATH, two popular math reasoning benchmarks, using t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;PDD&#65292;&#29992;&#20110;&#35780;&#20272;&#38271;&#31687;&#25991;&#31456;&#20043;&#38388;&#30340;&#35805;&#35821;&#36830;&#36143;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#25351;&#26631;&#26356;&#25509;&#36817;&#20154;&#31867;&#20559;&#22909;&#21644;GPT-4&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10175</link><description>&lt;p&gt;
&#35299;&#38145;&#32467;&#26500;&#27979;&#37327;&#65306;&#24341;&#20837;PDD&#65292;&#19968;&#31181;&#29992;&#20110;&#20301;&#32622;&#35805;&#35821;&#36830;&#36143;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Unlocking Structure Measuring: Introducing PDD, an Automatic Metric for Positional Discourse Coherence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;PDD&#65292;&#29992;&#20110;&#35780;&#20272;&#38271;&#31687;&#25991;&#31456;&#20043;&#38388;&#30340;&#35805;&#35821;&#36830;&#36143;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#25351;&#26631;&#26356;&#25509;&#36817;&#20154;&#31867;&#20559;&#22909;&#21644;GPT-4&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#19982;&#29992;&#25143;&#24847;&#22270;&#23545;&#40784;&#29983;&#25104;&#25991;&#26412;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#24403;&#28041;&#21450;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#26102;&#65292;&#20174;&#35805;&#35821;&#36830;&#36143;&#30340;&#35282;&#24230;&#20986;&#21457;&#23545;&#29983;&#25104;&#32467;&#26524;&#20135;&#29983;&#20102;&#36234;&#26469;&#36234;&#22823;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35789;&#27719;&#25110;&#35821;&#20041;&#35780;&#20272;&#25351;&#26631;&#22914;BLEU&#12289;ROUGE&#12289;BertScore&#19981;&#33021;&#26377;&#25928;&#25429;&#25417;&#35805;&#35821;&#36830;&#36143;&#24615;&#12290;&#22240;&#27492;&#65292;&#21457;&#23637;&#38024;&#23545;LLMs&#29983;&#25104;&#32467;&#26524;&#30340;&#35805;&#35821;&#29305;&#23450;&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#38656;&#35201;&#26356;&#22810;&#30340;&#20851;&#27880;&#21644;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#37327;&#21270;&#20004;&#31687;&#38271;&#31687;&#25991;&#31456;&#20043;&#38388;&#30340;&#35805;&#35821;&#24046;&#24322;&#12290;&#23545;&#26469;&#33258;&#20195;&#34920;&#24615;&#39046;&#22495;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#25351;&#26631;&#26356;&#25509;&#36817;&#20154;&#31867;&#20559;&#22909;&#21644;GPT-4&#30340;&#36830;&#36143;&#24615;&#35780;&#20272;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10175v1 Announce Type: new  Abstract: Recent large language models (LLMs) have shown remarkable performance in aligning generated text with user intentions across various tasks. When it comes to long-form text generation, there has been a growing interest in generation from a discourse coherence perspective. However, existing lexical or semantic metrics such as BLEU, ROUGE, BertScore cannot effectively capture the discourse coherence. The development of discourse-specific automatic evaluation methods for assessing the output of LLMs warrants greater focus and exploration. In this paper, we present a novel automatic metric designed to quantify the discourse divergence between two long-form articles. Extensive experiments on three datasets from representative domains demonstrate that our metric aligns more closely with human preferences and GPT-4 coherence evaluation, outperforming existing evaluation methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#25193;&#23637;&#21040;128K&#30340;&#36830;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#25968;&#25454;&#28151;&#21512;&#21644;&#36731;&#37327;&#32423;&#30340;&#39044;&#35757;&#32451;&#21487;&#20197;&#23454;&#29616;&#65292;&#20854;&#20013;&#20851;&#38190;&#35201;&#28857;&#22312;&#20110;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#65292;&#38656;&#35201;&#27880;&#24847;&#39046;&#22495;&#24179;&#34913;&#21644;&#38271;&#24230;&#19978;&#37319;&#26679;&#12290;</title><link>https://arxiv.org/abs/2402.10171</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;128K&#19978;&#19979;&#25991;&#30340;&#25968;&#25454;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Data Engineering for Scaling Language Models to 128K Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#25193;&#23637;&#21040;128K&#30340;&#36830;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#25968;&#25454;&#28151;&#21512;&#21644;&#36731;&#37327;&#32423;&#30340;&#39044;&#35757;&#32451;&#21487;&#20197;&#23454;&#29616;&#65292;&#20854;&#20013;&#20851;&#38190;&#35201;&#28857;&#22312;&#20110;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#65292;&#38656;&#35201;&#27880;&#24847;&#39046;&#22495;&#24179;&#34913;&#21644;&#38271;&#24230;&#19978;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#25193;&#23637;&#21040;128K&#30340;&#36830;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#30528;&#37325;&#32771;&#34385;&#25968;&#25454;&#24037;&#31243;&#12290;&#25105;&#20204;&#20551;&#35774;&#38271;&#19978;&#19979;&#25991;&#24314;&#27169;&#65292;&#29305;&#21035;&#26159;&#8220;&#33021;&#22815;&#21033;&#29992;&#20219;&#24847;&#36755;&#20837;&#20301;&#32622;&#30340;&#20449;&#24687;&#8221;&#30340;&#33021;&#21147;&#65292;&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#25484;&#25569;&#65292;&#24182;&#19988;&#36825;&#31181;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#36731;&#37327;&#32423;&#36830;&#32493;&#39044;&#35757;&#32451;&#22312;&#27604;&#35757;&#32451;&#26102;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;(&#20363;&#22914;&#20174;4K&#21040;128K)&#19979;&#36731;&#26494;&#25193;&#23637;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36830;&#32493;&#39044;&#35757;&#32451;&#30340;&#25968;&#25454;&#8220;&#25968;&#37327;&#8221;&#21644;&#8220;&#36136;&#37327;&#8221;&#65306;(1)&#23545;&#20110;&#25968;&#37327;&#65292;&#25105;&#20204;&#35777;&#26126;5&#20159;&#21040;50&#20159;&#20010;&#26631;&#35760;&#36275;&#20197;&#20351;&#27169;&#22411;&#33021;&#22815;&#26816;&#32034;&#21040;128K&#19978;&#19979;&#25991;&#20013;&#30340;&#20219;&#20309;&#20301;&#32622;&#30340;&#20449;&#24687;&#65307;(2)&#23545;&#20110;&#36136;&#37327;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#21516;&#31561;&#24378;&#35843;&#8220;&#39046;&#22495;&#24179;&#34913;&#8221;&#21644;&#8220;&#38271;&#24230;&#19978;&#37319;&#26679;&#8221;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#22320;&#19978;&#37319;&#26679;&#26356;&#38271;&#30340;&#25968;&#25454;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#36275;&#22815;&#30340;&#36136;&#37327;&#65292;&#32780;&#26159;&#38656;&#35201;&#27880;&#24847;&#25968;&#25454;&#30340;&#39046;&#22495;&#24179;&#34913;&#21644;&#38271;&#24230;&#19978;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10171v1 Announce Type: cross  Abstract: We study the continual pretraining recipe for scaling language models' context lengths to 128K, with a focus on data engineering. We hypothesize that long context modeling, in particular \textit{the ability to utilize information at arbitrary input locations}, is a capability that is mostly already acquired through large-scale pretraining, and that this capability can be readily extended to contexts substantially longer than seen during training~(e.g., 4K to 128K) through lightweight continual pretraining on appropriate data mixture. We investigate the \textit{quantity} and \textit{quality} of the data for continual pretraining: (1) for quantity, we show that 500 million to 5 billion tokens are enough to enable the model to retrieve information anywhere within the 128K context; (2) for quality, our results equally emphasize \textit{domain balance} and \textit{length upsampling}. Concretely, we find that naively upsampling longer data o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#27880;&#20837;&#30340;&#22522;&#20110;LLM&#30340;&#23545;&#35805;&#24335;&#20581;&#24247;&#20195;&#29702;&#65288;CHA&#65289;&#29992;&#20110;&#31958;&#23615;&#30149;&#24739;&#32773;&#65292;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21644;&#20998;&#26512;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#31958;&#23615;&#30149;&#31649;&#29702;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10153</link><description>&lt;p&gt;
&#30693;&#35782;&#27880;&#20837;&#30340;&#22522;&#20110;LLM&#30340;&#23545;&#35805;&#24335;&#20581;&#24247;&#20195;&#29702;&#65306;&#31958;&#23615;&#30149;&#24739;&#32773;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Infused LLM-Powered Conversational Health Agent: A Case Study for Diabetes Patients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#27880;&#20837;&#30340;&#22522;&#20110;LLM&#30340;&#23545;&#35805;&#24335;&#20581;&#24247;&#20195;&#29702;&#65288;CHA&#65289;&#29992;&#20110;&#31958;&#23615;&#30149;&#24739;&#32773;&#65292;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21644;&#20998;&#26512;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#31958;&#23615;&#30149;&#31649;&#29702;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#31958;&#23615;&#30149;&#31649;&#29702;&#23545;&#20110;&#31958;&#23615;&#30149;&#24739;&#32773;&#30340;&#20581;&#24247;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20026;&#31958;&#23615;&#30149;&#31649;&#29702;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#65292;&#25552;&#39640;&#20102;&#20854;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#21463;&#38480;&#20110;&#23545;&#19968;&#33324;&#26469;&#28304;&#30340;&#20381;&#36182;&#65292;&#32570;&#20047;&#19982;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#25972;&#21512;&#65292;&#23548;&#33268;&#22238;&#22797;&#19981;&#20934;&#30830;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#27880;&#20837;&#30340;&#22522;&#20110;LLM&#30340;&#23545;&#35805;&#24335;&#20581;&#24247;&#20195;&#29702;&#65288;CHA&#65289;&#29992;&#20110;&#31958;&#23615;&#30149;&#24739;&#32773;&#12290;&#25105;&#20204;&#26681;&#25454;&#24320;&#28304;&#30340;openCHA&#26694;&#26550;&#36827;&#34892;&#23450;&#21046;&#65292;&#24182;&#22686;&#24378;&#20102;&#25105;&#20204;&#30340;CHA&#30340;&#22806;&#37096;&#30693;&#35782;&#21644;&#20998;&#26512;&#33021;&#21147;&#12290;&#36825;&#31181;&#25972;&#21512;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;1&#65289;&#25972;&#21512;&#32654;&#22269;&#31958;&#23615;&#30149;&#21327;&#20250;&#30340;&#33203;&#39135;&#25351;&#21335;&#21644;Nutritionix&#30340;&#20449;&#24687;&#65307;2&#65289;&#37096;&#32626;&#20998;&#26512;&#24037;&#20855;&#65292;&#23454;&#29616;&#33829;&#20859;&#25668;&#20837;&#35745;&#31639;&#24182;&#19982;&#25351;&#21335;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;CHA&#19982;GPT4&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#21253;&#25324;100&#20010;&#31958;&#23615;&#30149;&#24739;&#32773;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10153v1 Announce Type: new  Abstract: Effective diabetes management is crucial for maintaining health in diabetic patients. Large Language Models (LLMs) have opened new avenues for diabetes management, facilitating their efficacy. However, current LLM-based approaches are limited by their dependence on general sources and lack of integration with domain-specific knowledge, leading to inaccurate responses. In this paper, we propose a knowledge-infused LLM-powered conversational health agent (CHA) for diabetic patients. We customize and leverage the open-source openCHA framework, enhancing our CHA with external knowledge and analytical capabilities. This integration involves two key components: 1) incorporating the American Diabetes Association dietary guidelines and the Nutritionix information and 2) deploying analytical tools that enable nutritional intake calculation and comparison with the guidelines. We compare the proposed CHA with GPT4. Our evaluation includes 100 diabe
&lt;/p&gt;</description></item><item><title>ControlLM&#26159;&#19968;&#31181;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#20010;&#24615;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24046;&#24322;&#30340;&#34892;&#20026;&#25552;&#31034;&#28608;&#27963;&#27169;&#24335;&#26469;&#23454;&#29616;&#65292;&#20174;&#32780;&#23454;&#29616;&#27169;&#22411;&#34892;&#20026;&#30340;&#31934;&#30830;&#12289;&#23454;&#26102;&#35843;&#25972;&#12290;</title><link>https://arxiv.org/abs/2402.10151</link><description>&lt;p&gt;
ControlLM: &#20026;&#35821;&#35328;&#27169;&#22411;&#25171;&#36896;&#22810;&#26679;&#24615;&#20010;&#24615;
&lt;/p&gt;
&lt;p&gt;
ControlLM: Crafting Diverse Personalities for Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10151
&lt;/p&gt;
&lt;p&gt;
ControlLM&#26159;&#19968;&#31181;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#20010;&#24615;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24046;&#24322;&#30340;&#34892;&#20026;&#25552;&#31034;&#28608;&#27963;&#27169;&#24335;&#26469;&#23454;&#29616;&#65292;&#20174;&#32780;&#23454;&#29616;&#27169;&#22411;&#34892;&#20026;&#30340;&#31934;&#30830;&#12289;&#23454;&#26102;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#22312;&#35268;&#27169;&#21644;&#33021;&#21147;&#19978;&#30340;&#25345;&#32493;&#25193;&#22823;&#65292;&#23427;&#20204;&#23637;&#31034;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#20852;&#30340;&#34892;&#20026;&#65292;&#26082;&#26377;&#26377;&#30410;&#30340;&#20063;&#26377;&#20196;&#20154;&#25285;&#24551;&#30340;&#34892;&#20026;&#12290;&#36825;&#22686;&#21152;&#20102;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#25511;&#21046;&#38656;&#27714;&#12290;&#25105;&#20204;&#24076;&#26395;&#33021;&#22815;&#22312;&#25512;&#29702;&#26102;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;&#29305;&#24449;&#65292;&#20197;&#28385;&#36275;&#19981;&#21516;&#31867;&#22411;&#20219;&#21153;&#30340;&#35201;&#27714;&#12290;&#20010;&#24615;&#26159;&#35821;&#35328;&#27169;&#22411;&#30340;&#26356;&#39640;&#32423;&#21644;&#26356;&#25277;&#35937;&#30340;&#34892;&#20026;&#34920;&#31034;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ControlLM&#65292;&#23427;&#21033;&#29992;&#27169;&#22411;&#28508;&#22312;&#31354;&#38388;&#20013;&#23545;&#27604;&#30340;&#34892;&#20026;&#25552;&#31034;&#30340;&#24046;&#24322;&#28608;&#27963;&#27169;&#24335;&#65292;&#20197;&#24433;&#21709;&#27169;&#22411;&#25512;&#29702;&#26102;&#30340;&#20010;&#24615;&#29305;&#24449;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#31934;&#30830;&#12289;&#23454;&#26102;&#22320;&#35843;&#25972;&#27169;&#22411;&#34892;&#20026;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;ControlLM&#33021;&#22815;&#22312;&#27809;&#26377;&#20219;&#20309;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#24341;&#36215;&#22810;&#26679;&#30340;&#20010;&#24615;&#34892;&#20026;&#65292;&#32780;&#31934;&#30830;&#25511;&#21046;&#20351;&#20010;&#24615;&#29305;&#24449;&#33021;&#22815;&#19982;&#24179;&#22343;&#24773;&#24418;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10151v1 Announce Type: new  Abstract: As language models continue to scale in size and capability, they display an array of emerging behaviors, both beneficial and concerning. This heightens the need to control model behaviors. We hope to be able to control the personality traits of language models at the inference-time so as to have various character features, on top of which the requirements of different types of tasks can be met. Personality is a higher-level and more abstract behavioral representation for language models. We introduce ControlLM, which leverages differential activation patterns, derived from contrasting behavioral prompts in the model's latent space, to influence the model's personality traits at inference. This approach allows for the precise, real-time adjustment of model behavior. First, we demonstrate ControlLM's capacity to elicit diverse persona behaviors without any training, while precision control allows personality traits to closely match averag
&lt;/p&gt;</description></item><item><title>TOAD&#26159;&#19968;&#20010;&#20855;&#26377;&#22810;&#26679;&#21709;&#24212;&#39118;&#26684;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#33258;&#21160;&#23545;&#35805;&#31995;&#32479;&#65292;&#20854;&#20013;&#32771;&#34385;&#20102;&#20887;&#38271;&#31243;&#24230;&#21644;&#29992;&#25143;&#34920;&#36798;&#38236;&#20687;&#20004;&#20010;&#26041;&#38754;&#12290;TOAD&#36890;&#36807;&#27169;&#25311;&#30495;&#23454;&#30340;&#24212;&#29992;&#19978;&#19979;&#25991;&#20132;&#20114;&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#31995;&#32479;&#21709;&#24212;&#39118;&#26684;&#36873;&#39033;&#65292;&#24182;&#22312;&#35780;&#20272;&#20013;&#34920;&#26126;&#24314;&#27169;&#26356;&#20887;&#38271;&#30340;&#22238;&#22797;&#25110;&#19981;&#36827;&#34892;&#29992;&#25143;&#34920;&#36798;&#38236;&#20687;&#30340;&#22238;&#22797;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10137</link><description>&lt;p&gt;
TOAD: &#20855;&#26377;&#22810;&#26679;&#21709;&#24212;&#39118;&#26684;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#33258;&#21160;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
TOAD: Task-Oriented Automatic Dialogs with Diverse Response Styles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10137
&lt;/p&gt;
&lt;p&gt;
TOAD&#26159;&#19968;&#20010;&#20855;&#26377;&#22810;&#26679;&#21709;&#24212;&#39118;&#26684;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#33258;&#21160;&#23545;&#35805;&#31995;&#32479;&#65292;&#20854;&#20013;&#32771;&#34385;&#20102;&#20887;&#38271;&#31243;&#24230;&#21644;&#29992;&#25143;&#34920;&#36798;&#38236;&#20687;&#20004;&#20010;&#26041;&#38754;&#12290;TOAD&#36890;&#36807;&#27169;&#25311;&#30495;&#23454;&#30340;&#24212;&#29992;&#19978;&#19979;&#25991;&#20132;&#20114;&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#31995;&#32479;&#21709;&#24212;&#39118;&#26684;&#36873;&#39033;&#65292;&#24182;&#22312;&#35780;&#20272;&#20013;&#34920;&#26126;&#24314;&#27169;&#26356;&#20887;&#38271;&#30340;&#22238;&#22797;&#25110;&#19981;&#36827;&#34892;&#29992;&#25143;&#34920;&#36798;&#38236;&#20687;&#30340;&#22238;&#22797;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#26174;&#31034;&#65292;&#19979;&#19968;&#20195;&#34394;&#25311;&#21161;&#25163;&#30340;&#26399;&#26395;&#21253;&#25324;&#22312;&#21508;&#31181;&#20351;&#29992;&#22330;&#26223;&#19979;&#25552;&#20379;&#26356;&#21152;&#33258;&#28982;&#21644;&#36866;&#24212;&#24615;&#24378;&#30340;&#23545;&#35805;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20026;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#65288;TOD&#65289;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#34987;&#35748;&#20026;&#26159;&#32531;&#24930;&#21644;&#26114;&#36149;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20219;&#21153;&#23548;&#21521;&#30340;&#33258;&#21160;&#23545;&#35805;&#31995;&#32479;&#65288;TOAD&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#19988;&#21487;&#25193;&#23637;&#30340;TOD&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19982;&#20043;&#37197;&#22871;&#30340;&#33258;&#21160;&#29983;&#25104;&#27969;&#31243;&#12290;TOAD&#25968;&#25454;&#38598;&#27169;&#25311;&#20102;&#30495;&#23454;&#30340;&#24212;&#29992;&#19978;&#19979;&#25991;&#20132;&#20114;&#65292;&#24182;&#25552;&#20379;&#20102;&#21508;&#31181;&#31995;&#32479;&#21709;&#24212;&#39118;&#26684;&#36873;&#39033;&#12290;&#32771;&#34385;&#20102;&#31995;&#32479;&#21709;&#24212;&#39118;&#26684;&#30340;&#20004;&#20010;&#26041;&#38754;&#65292;&#21363;&#20887;&#38271;&#31243;&#24230;&#21644;&#29992;&#25143;&#34920;&#36798;&#38236;&#20687;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#21709;&#24212;&#29983;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;TOAD&#30340;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#24314;&#27169;&#26356;&#20887;&#38271;&#30340;&#22238;&#22797;&#25110;&#19981;&#36827;&#34892;&#29992;&#25143;&#34920;&#36798;&#38236;&#20687;&#30340;&#22238;&#22797;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10137v1 Announce Type: new  Abstract: In light of recent advances in large language models~(LLMs), the expectations for the next generation of virtual assistants include enhanced naturalness and adaptability across diverse usage scenarios. However, the creation of high-quality annotated data for Task-Oriented Dialog~(TOD) is recognized to be slow and costly. To address these challenges, we introduce Task-Oriented Automatic Dialogs~(TOAD), a novel and scalable TOD dataset along with its automatic generation pipeline. The TOAD dataset simulates realistic app context interaction and provide a variety of system response style options. Two aspects of system response styles are considered, verbosity level and users' expression mirroring. We benchmark TOAD on two response generation tasks and the results show that modeling more verbose or responses without user expression mirroring is more challenging.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#21453;&#23556;&#35843;&#33410;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25945;&#24072;LLM&#30340;&#21453;&#23556;&#21644;&#33258;&#30465;&#19982;&#23398;&#29983;LLM&#30340;&#25968;&#25454;&#36873;&#25321;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#33258;&#21160;&#20248;&#21270;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#33410;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#33410;&#21644;&#21331;&#36234;&#24615;&#33021;&#30340;LLM&#12290;</title><link>https://arxiv.org/abs/2402.10110</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#21453;&#23556;&#35843;&#33410;&#65306;LLM&#25351;&#20196;&#35843;&#33410;&#30340;&#23398;&#29983;&#36873;&#25321;&#25968;&#25454;&#22238;&#25910;
&lt;/p&gt;
&lt;p&gt;
Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#21453;&#23556;&#35843;&#33410;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25945;&#24072;LLM&#30340;&#21453;&#23556;&#21644;&#33258;&#30465;&#19982;&#23398;&#29983;LLM&#30340;&#25968;&#25454;&#36873;&#25321;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#33258;&#21160;&#20248;&#21270;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#33410;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#33410;&#21644;&#21331;&#36234;&#24615;&#33021;&#30340;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#33410;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35828;&#38750;&#24120;&#20851;&#38190;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25351;&#20196;&#36319;&#36394;&#21644;&#20219;&#21153;&#36866;&#24212;&#33021;&#21147;&#65292;&#20294;&#20854;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#35768;&#22810;&#26368;&#36817;&#30340;&#26041;&#27861;&#37117;&#33268;&#21147;&#20110;&#25913;&#36827;&#25968;&#25454;&#36136;&#37327;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#25968;&#25454;&#19982;&#27491;&#22312;&#24494;&#35843;&#30340;&#23398;&#29983;&#27169;&#22411;&#30340;&#20860;&#23481;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#8212;&#8212;&#36873;&#25321;&#24615;&#21453;&#23556;&#35843;&#33410;&#65292;&#36890;&#36807;&#32467;&#21512;&#25945;&#24072;LLM&#30340;&#21453;&#23556;&#21644;&#33258;&#30465;&#65292;&#20197;&#33258;&#21160;&#20248;&#21270;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#33410;&#25968;&#25454;&#12290;&#36825;&#31181;&#24072;&#29983;&#21512;&#20316;&#20135;&#29983;&#20102;&#39640;&#36136;&#37327;&#19988;&#19982;&#23398;&#29983;LLM&#20860;&#23481;&#30340;&#25351;&#20196;&#21709;&#24212;&#23545;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#33410;&#21644;&#21331;&#36234;&#24615;&#33021;&#30340;LLM&#12290;&#36873;&#25321;&#24615;&#21453;&#23556;&#35843;&#33410;&#26159;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#21644;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#24120;&#33021;&#25913;&#21892;LLM&#24494;&#35843;&#21644;&#33258;&#25105;&#20248;&#21270;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10110v1 Announce Type: cross  Abstract: Instruction tuning is critical to large language models (LLMs) for achieving better instruction following and task adaptation capabilities but its success heavily relies on the training data quality. Many recent methods focus on improving the data quality but often overlook the compatibility of the data with the student model being finetuned. This paper introduces Selective Reflection-Tuning, a novel paradigm that synergizes a teacher LLM's reflection and introspection for improving existing data quality with the data selection capability of the student LLM, to automatically refine existing instruction-tuning data. This teacher-student collaboration produces high-quality and student-compatible instruction-response pairs, resulting in sample-efficient instruction tuning and LLMs of superior performance. Selective Reflection-Tuning is a data augmentation and synthesis that generally improves LLM finetuning and self-improvement without co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#26041;&#27861;&#26469;&#35782;&#21035;&#30149;&#20154;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#20013;&#25351;&#31034;&#29305;&#23450;&#35786;&#26029;&#39118;&#38505;&#22686;&#21152;&#25110;&#20943;&#23569;&#30340;&#35777;&#25454;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;&#35777;&#25454;&#30340;&#33719;&#21462;&#19982;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#26469;&#38477;&#20302;&#35786;&#26029;&#38169;&#35823;&#12290;&#27169;&#22411;&#20351;&#29992;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#35777;&#25454;&#20026;&#21518;&#30462;&#65292;&#24182;&#32473;&#20986;&#20010;&#20307;&#21270;&#39118;&#38505;&#20272;&#35745;&#65292;&#29305;&#21035;&#38024;&#23545;&#35786;&#26029;&#24310;&#36831;&#21644;&#26469;&#33258;&#19981;&#23436;&#25972;&#37492;&#21035;&#30340;&#38169;&#35823;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.10109</link><description>&lt;p&gt;
&#29992;&#21487;&#35299;&#37322;&#30340;&#39118;&#38505;&#39044;&#27979;&#26041;&#27861;&#38477;&#20302;&#35786;&#26029;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Towards Reducing Diagnostic Errors with Interpretable Risk Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#26041;&#27861;&#26469;&#35782;&#21035;&#30149;&#20154;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#20013;&#25351;&#31034;&#29305;&#23450;&#35786;&#26029;&#39118;&#38505;&#22686;&#21152;&#25110;&#20943;&#23569;&#30340;&#35777;&#25454;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;&#35777;&#25454;&#30340;&#33719;&#21462;&#19982;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#26469;&#38477;&#20302;&#35786;&#26029;&#38169;&#35823;&#12290;&#27169;&#22411;&#20351;&#29992;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#35777;&#25454;&#20026;&#21518;&#30462;&#65292;&#24182;&#32473;&#20986;&#20010;&#20307;&#21270;&#39118;&#38505;&#20272;&#35745;&#65292;&#29305;&#21035;&#38024;&#23545;&#35786;&#26029;&#24310;&#36831;&#21644;&#26469;&#33258;&#19981;&#23436;&#25972;&#37492;&#21035;&#30340;&#38169;&#35823;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#35786;&#26029;&#38169;&#35823;&#21457;&#29983;&#26159;&#22240;&#20026;&#20020;&#24202;&#21307;&#29983;&#26080;&#27861;&#36731;&#26131;&#33719;&#21462;&#30149;&#20154;&#30005;&#23376;&#30149;&#21382;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#26041;&#27861;&#26469;&#35782;&#21035;&#30149;&#20154;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#20013;&#25351;&#31034;&#29305;&#23450;&#35786;&#26029;&#39118;&#38505;&#22686;&#21152;&#25110;&#20943;&#23569;&#30340;&#35777;&#25454;&#30340;&#26041;&#27861;&#65292;&#26368;&#32456;&#30446;&#26631;&#26159;&#22686;&#21152;&#35777;&#25454;&#30340;&#33719;&#21462;&#19982;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#26469;&#36827;&#34892;&#24102;&#26377;&#20010;&#20307;&#21270;&#39118;&#38505;&#20272;&#35745;&#30340;&#20197;&#35777;&#25454;&#20026;&#21518;&#30462;&#30340;&#39044;&#27979;&#65292;&#22312;&#20020;&#24202;&#21307;&#29983;&#20173;&#28982;&#19981;&#30830;&#23450;&#30340;&#26102;&#38388;&#28857;&#19978;&#65292;&#26088;&#22312;&#29305;&#21035;&#20943;&#36731;&#35786;&#26029;&#24310;&#36831;&#21644;&#28304;&#20110;&#19981;&#23436;&#25972;&#37492;&#21035;&#30340;&#38169;&#35823;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#26679;&#19968;&#20010;&#27169;&#22411;&#65292;&#38656;&#35201;&#25512;&#26029;&#20986;&#20107;&#20214;&#24615;&#30340;&#8220;&#30495;&#23454;&#8221;&#35786;&#26029;&#30340;&#26102;&#38388;&#31890;&#24230;&#32454;&#33268;&#30340;&#22238;&#39038;&#24615;&#26631;&#31614;&#12290;&#25105;&#20204;&#20351;&#29992;LLMs&#26469;&#20445;&#35777;&#36755;&#20837;&#25991;&#26412;&#26159;&#22312;&#21487;&#20197;&#36827;&#34892;&#33258;&#20449;&#30340;&#35786;&#26029;&#20043;&#21069;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;LLMs&#26469;&#26816;&#32034;&#21021;&#22987;&#30340;&#35777;&#25454;&#27744;&#65292;&#28982;&#21518;&#36827;&#34892;&#32454;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10109v1 Announce Type: new  Abstract: Many diagnostic errors occur because clinicians cannot easily access relevant information in patient Electronic Health Records (EHRs). In this work we propose a method to use LLMs to identify pieces of evidence in patient EHR data that indicate increased or decreased risk of specific diagnoses; our ultimate aim is to increase access to evidence and reduce diagnostic errors. In particular, we propose a Neural Additive Model to make predictions backed by evidence with individualized risk estimates at time-points where clinicians are still uncertain, aiming to specifically mitigate delays in diagnosis and errors stemming from an incomplete differential. To train such a model, it is necessary to infer temporally fine-grained retrospective labels of eventual "true" diagnoses. We do so with LLMs, to ensure that the input text is from before a confident diagnosis can be made. We use an LLM to retrieve an initial pool of evidence, but then refin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#23884;&#20837;&#21487;&#25511;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65288;QE-CDLM&#65289;&#65292;&#36890;&#36807;&#37325;&#24314;&#20219;&#21153;&#29305;&#23450;&#30340;&#23884;&#20837;&#31354;&#38388;&#26469;&#25913;&#21892;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12289;&#21487;&#31227;&#26893;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10107</link><description>&lt;p&gt;
&#21487;&#25511;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#23884;&#20837;&#21521;&#37327;
&lt;/p&gt;
&lt;p&gt;
Quantized Embedding Vectors for Controllable Diffusion Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#23884;&#20837;&#21487;&#25511;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65288;QE-CDLM&#65289;&#65292;&#36890;&#36807;&#37325;&#24314;&#20219;&#21153;&#29305;&#23450;&#30340;&#23884;&#20837;&#31354;&#38388;&#26469;&#25913;&#21892;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12289;&#21487;&#31227;&#26893;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25913;&#21892;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12289;&#21487;&#31227;&#26893;&#24615;&#21644;&#25512;&#29702;&#36895;&#24230;&#26159;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#26434;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#20294;&#20869;&#23384;&#21644;&#35745;&#31639;&#33021;&#21147;&#20173;&#28982;&#38750;&#24120;&#33499;&#21051;&#65292;&#26080;&#27861;&#28385;&#36275;&#39044;&#26399;&#65292;&#36825;&#33258;&#28982;&#23548;&#33268;&#27169;&#22411;&#30340;&#21487;&#31227;&#26893;&#24615;&#21644;&#31283;&#23450;&#24615;&#36739;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#37327;&#21270;&#23884;&#20837;&#21487;&#25511;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65288;QE-CDLM&#65289;&#12290;QE-CDLM&#22522;&#20110;&#26368;&#36817;&#25104;&#21151;&#30340;&#21487;&#25511;DLM&#65292;&#36890;&#36807;&#37327;&#21270;&#37325;&#24314;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#29983;&#25104;&#24335;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10107v1 Announce Type: cross  Abstract: Improving the controllability, portability, and inference speed of diffusion language models (DLMs) is a key challenge in natural language generation. While recent research has shown significant success in complex text generation with language models, the memory and computational power are still very demanding and fall short of expectations, which naturally results in low portability and instability for the models. To mitigate these issues, numerous well-established methods were proposed for neural network quantization. To further enhance their portability of independent deployment as well as improve their stability evaluated by language perplexity, we propose a novel approach called the Quantized Embedding Controllable Diffusion Language Model (QE-CDLM). QE-CDLM builds upon the recent successful controllable DLMs by remodeling the task-specific embedding space via quantization. This leads to a gradient-based controller for the generat
&lt;/p&gt;</description></item><item><title>GeoEval&#22522;&#20934;&#27979;&#35797;&#29992;&#20110;&#35780;&#20272;LLMs&#21644;MMs&#22312;&#20960;&#20309;&#38382;&#39064;&#35299;&#20915;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;WizardMath&#27169;&#22411;&#22312;&#20027;&#35201;&#23376;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23376;&#38598;&#19978;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.10104</link><description>&lt;p&gt;
GeoEval&#65306;&#29992;&#20110;&#35780;&#20272;LLMs&#21644;&#22810;&#27169;&#22411;&#22312;&#20960;&#20309;&#38382;&#39064;&#35299;&#20915;&#19978;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry Problem-Solving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10104
&lt;/p&gt;
&lt;p&gt;
GeoEval&#22522;&#20934;&#27979;&#35797;&#29992;&#20110;&#35780;&#20272;LLMs&#21644;MMs&#22312;&#20960;&#20309;&#38382;&#39064;&#35299;&#20915;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;WizardMath&#27169;&#22411;&#22312;&#20027;&#35201;&#23376;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23376;&#38598;&#19978;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22810;&#27169;&#22411;&#65288;MMs&#65289;&#26041;&#38754;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22788;&#29702;&#20960;&#20309;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#21363;&#38656;&#35201;&#32508;&#21512;&#29702;&#35299;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#65292;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#35780;&#20272;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;GeoEval&#22522;&#20934;&#27979;&#35797;&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#38598;&#21512;&#65292;&#21253;&#25324;&#19968;&#20010;&#20027;&#35201;&#23376;&#38598;&#21512;&#30340;2000&#20010;&#38382;&#39064;&#65292;&#19968;&#20010;&#37325;&#28857;&#20851;&#27880;&#21453;&#25512;&#29702;&#30340;750&#20010;&#38382;&#39064;&#23376;&#38598;&#21512;&#65292;&#19968;&#20010;&#22686;&#24378;&#23376;&#38598;&#21512;&#30340;2000&#20010;&#38382;&#39064;&#20197;&#21450;&#19968;&#20010;&#38590;&#39064;&#23376;&#38598;&#21512;&#30340;300&#20010;&#38382;&#39064;&#12290;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#26377;&#21161;&#20110;&#26356;&#28145;&#20837;&#22320;&#30740;&#31350;LLMs&#21644;MMs&#22312;&#35299;&#20915;&#20960;&#20309;&#25968;&#23398;&#38382;&#39064;&#26102;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#21313;&#20010;LLMs&#21644;MMs&#22312;&#36825;&#20123;&#19981;&#21516;&#23376;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;WizardMath&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#20027;&#35201;&#23376;&#38598;&#19978;&#36798;&#21040;55.67%&#30340;&#20934;&#30830;&#29575;&#65292;&#20294;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23376;&#38598;&#19978;&#21482;&#26377;6.00%&#30340;&#20934;&#30830;&#29575;&#12290;&#36825;&#31361;&#20986;&#20102;&#20851;&#38190;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10104v1 Announce Type: new  Abstract: Recent advancements in Large Language Models (LLMs) and Multi-Modal Models (MMs) have demonstrated their remarkable capabilities in problem-solving. Yet, their proficiency in tackling geometry math problems, which necessitates an integrated understanding of both textual and visual information, has not been thoroughly evaluated. To address this gap, we introduce the GeoEval benchmark, a comprehensive collection that includes a main subset of 2000 problems, a 750 problem subset focusing on backward reasoning, an augmented subset of 2000 problems, and a hard subset of 300 problems. This benchmark facilitates a deeper investigation into the performance of LLMs and MMs on solving geometry math problems. Our evaluation of ten LLMs and MMs across these varied subsets reveals that the WizardMath model excels, achieving a 55.67\% accuracy rate on the main subset but only a 6.00\% accuracy on the challenging subset. This highlights the critical ne
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;\textbf{MoEI}&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;EI&#30456;&#20851;&#20219;&#21153;&#38598;&#21512;\textsc{EiBench}&#24182;&#37319;&#29992;&#27169;&#22359;&#21270;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#24773;&#24863;&#26234;&#21147;&#22686;&#24378;&#22120;&#30340;&#38598;&#25104;&#65292;&#32508;&#21512;&#25552;&#39640;&#20102;LLM&#30340;&#24773;&#24863;&#26234;&#21147;&#65288;EI&#65289;&#32780;&#19981;&#25439;&#23475;&#20854;&#26222;&#36866;&#26234;&#33021;&#65288;GI&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.10073</link><description>&lt;p&gt;
&#21516;&#26102;&#37325;&#35270;&#65306;&#22312;&#19981;&#25439;&#23475;&#26222;&#36866;&#26234;&#33021;&#30340;&#24773;&#24863;&#26234;&#33021;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#22686;&#24378;&#24773;&#32490;&#26234;&#21147;
&lt;/p&gt;
&lt;p&gt;
Both Matter: Enhancing the Emotional Intelligence of Large Language Models without Compromising the General Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;\textbf{MoEI}&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;EI&#30456;&#20851;&#20219;&#21153;&#38598;&#21512;\textsc{EiBench}&#24182;&#37319;&#29992;&#27169;&#22359;&#21270;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#24773;&#24863;&#26234;&#21147;&#22686;&#24378;&#22120;&#30340;&#38598;&#25104;&#65292;&#32508;&#21512;&#25552;&#39640;&#20102;LLM&#30340;&#24773;&#24863;&#26234;&#21147;&#65288;EI&#65289;&#32780;&#19981;&#25439;&#23475;&#20854;&#26222;&#36866;&#26234;&#33021;&#65288;GI&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#26234;&#21147;&#65288;EI&#65289;&#21253;&#25324;&#24773;&#24863;&#24863;&#30693;&#12289;&#24773;&#24863;&#35748;&#30693;&#21644;&#24773;&#24863;&#34920;&#36798;&#65292;&#22312;&#25552;&#39640;&#24403;&#21069;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23545;&#35805;&#24335;&#26222;&#36866;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#19982;&#29992;&#25143;&#20132;&#20114;&#20307;&#39564;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#23545;EI&#30456;&#20851;&#30340;&#20998;&#31867;&#25110;&#22238;&#24402;&#20219;&#21153;&#30340;&#22825;&#30495;&#24494;&#35843;&#25552;&#39640;&#20854;&#24773;&#24863;&#24863;&#30693;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#23548;&#33268;&#20102;EI&#30340;&#19981;&#23436;&#20840;&#22686;&#24378;&#21644;&#23545;&#26222;&#36866;&#26234;&#33021;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20197;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;EI&#30456;&#20851;&#20219;&#21153;&#38598;&#21512;\textsc{EiBench}&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#35206;&#30422;&#20102;EI&#30340;&#19977;&#20010;&#26041;&#38754;&#30340;&#20219;&#21153;&#25351;&#31034;&#65292;&#20026;&#32508;&#21512;&#22686;&#24378;LLM&#30340;EI&#22880;&#23450;&#20102;&#22362;&#23454;&#30340;&#22522;&#30784;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#24773;&#32490;&#26234;&#21147;&#22686;&#24378;&#26041;&#27861;&#65288;\textbf{MoEI}&#65289;&#65292;&#21253;&#21547;&#20102;&#27169;&#22359;&#21270;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#24773;&#24863;&#26234;&#21147;&#22686;&#24378;&#22120;&#30340;&#38598;&#25104;&#65292;&#20197;&#21516;&#26102;&#25552;&#39640;EI&#21644;&#20445;&#25345;&#26222;&#36866;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10073v1 Announce Type: new  Abstract: Emotional Intelligence (EI), consisting of emotion perception, emotion cognition and emotion expression, plays the critical roles in improving user interaction experience for the current large language model (LLM) based conversational general AI assistants. Previous works mainly focus on raising the emotion perception ability of them via naive fine-tuning on EI-related classification or regression tasks. However, this leads to the incomplete enhancement of EI and catastrophic forgetting of the general intelligence (GI). To this end, we first introduce \textsc{EiBench}, a large-scale collection of EI-related tasks in the text-to-text formation with task instructions that covers all three aspects of EI, which lays a solid foundation for the comprehensive EI enhancement of LLMs. Then a novel \underline{\textbf{Mo}}dular \underline{\textbf{E}}motional \underline{\textbf{I}}ntelligence enhancement method (\textbf{MoEI}), consisting of Modular
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36951;&#24536;&#26694;&#26550;SKU&#65292;&#26088;&#22312;&#28040;&#38500;&#26377;&#23475;&#30693;&#35782;&#30340;&#21516;&#26102;&#20445;&#30041;&#27169;&#22411;&#23545;&#27491;&#24120;&#25552;&#31034;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10058</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#36951;&#24536;&#23454;&#29616;&#26356;&#23433;&#20840;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Safer Large Language Models through Machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10058
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36951;&#24536;&#26694;&#26550;SKU&#65292;&#26088;&#22312;&#28040;&#38500;&#26377;&#23475;&#30693;&#35782;&#30340;&#21516;&#26102;&#20445;&#30041;&#27169;&#22411;&#23545;&#27491;&#24120;&#25552;&#31034;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#24402;&#22240;&#20110;&#23427;&#20204;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#21644;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;LLM&#38754;&#23545;&#26377;&#38382;&#39064;&#30340;&#25552;&#31034;&#26102;&#65292;&#32463;&#24120;&#20250;&#36935;&#21040;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#23581;&#35797;&#36890;&#36807;&#26799;&#24230;&#19978;&#21319;&#26041;&#27861;&#38459;&#27490;LLM&#20135;&#29983;&#26377;&#23475;&#36755;&#20986;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#24433;&#21709;&#27169;&#22411;&#23545;&#27491;&#24120;&#25552;&#31034;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36873;&#25321;&#24615;&#30693;&#35782;&#21542;&#35748;&#36951;&#24536;&#65288;SKU&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38024;&#23545;LLM&#30340;&#36951;&#24536;&#26694;&#26550;&#65292;&#26088;&#22312;&#28040;&#38500;&#26377;&#23475;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#27491;&#24120;&#25552;&#31034;&#30340;&#23454;&#29992;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SKU&#30001;&#20004;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#26377;&#23475;&#30693;&#35782;&#33719;&#21462;&#38454;&#27573;&#21644;&#30693;&#35782;&#21542;&#23450;&#38454;&#27573;&#12290;&#31532;&#19968;&#20010;&#38454;&#27573;&#26088;&#22312;&#35782;&#21035;&#21644;&#33719;&#21462;&#26377;&#23475;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10058v1 Announce Type: new  Abstract: The rapid advancement of Large Language Models (LLMs) has demonstrated their vast potential across various domains, attributed to their extensive pretraining knowledge and exceptional generalizability. However, LLMs often encounter challenges in generating harmful content when faced with problematic prompts. To address this problem, existing work attempted to implement a gradient ascent based approach to prevent LLMs from producing harmful output. While these methods can be effective, they frequently impact the model utility in responding to normal prompts. To address this gap, we introduce Selective Knowledge negation Unlearning (SKU), a novel unlearning framework for LLMs, designed to eliminate harmful knowledge while preserving utility on normal prompts. Specifically, SKU is consisted of two stages: harmful knowledge acquisition stage and knowledge negation stage. The first stage aims to identify and acquire harmful knowledge within t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#33976;&#39311;&#21644;&#26377;&#24847;&#35782;&#30340;&#24819;&#35937;&#65292;&#26377;&#25928;&#22320;&#36951;&#24536;&#30446;&#26631;&#25991;&#26412;&#65292;&#24182;&#22312;&#29983;&#25104;&#20219;&#21153;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#20445;&#30041;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10052</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#33258;&#25105;&#33976;&#39311;&#21644;&#26377;&#24847;&#35782;&#30340;&#24819;&#35937;&#36827;&#34892;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Unmemorization in Large Language Models via Self-Distillation and Deliberate Imagination
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#33976;&#39311;&#21644;&#26377;&#24847;&#35782;&#30340;&#24819;&#35937;&#65292;&#26377;&#25928;&#22320;&#36951;&#24536;&#30446;&#26631;&#25991;&#26412;&#65292;&#24182;&#22312;&#29983;&#25104;&#20219;&#21153;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#20445;&#30041;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20173;&#28982;&#23384;&#22312;&#38544;&#31169;&#20405;&#29359;&#21644;&#25935;&#24863;&#25968;&#25454;&#19981;&#21463;&#25511;&#21046;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#22312;LLM&#36951;&#24536;&#30340;&#36807;&#31243;&#20013;&#37319;&#29992;&#26377;&#24847;&#35782;&#30340;&#24819;&#35937;&#12290;&#25105;&#20204;&#19981;&#26159;&#35797;&#22270;&#24536;&#35760;&#24050;&#35760;&#24518;&#30340;&#25968;&#25454;&#65292;&#32780;&#26159;&#36890;&#36807;&#33258;&#25105;&#33976;&#39311;&#30340;&#26694;&#26550;&#24341;&#23548;LLM&#26377;&#24847;&#35782;&#22320;&#24819;&#35937;&#26367;&#20195;&#24773;&#22659;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#26377;&#25928;&#22320;&#36951;&#24536;&#30446;&#26631;&#25991;&#26412;&#65292;&#36824;&#21487;&#20197;&#20445;&#30041;LLM&#22312;&#24320;&#25918;&#24335;&#29983;&#25104;&#20219;&#21153;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#19981;&#21516;&#27169;&#22411;&#21644;&#35268;&#27169;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10052v1 Announce Type: cross  Abstract: While displaying impressive generation capabilities across many tasks, Large Language Models (LLMs) still struggle with crucial issues of privacy violation and unwanted exposure of sensitive data. This raises an essential question: how should we prevent such undesired behavior of LLMs while maintaining their strong generation and natural language understanding (NLU) capabilities? In this work, we introduce a novel approach termed deliberate imagination in the context of LLM unlearning. Instead of trying to forget memorized data, we employ a self-distillation framework, guiding LLMs to deliberately imagine alternative scenarios. As demonstrated in a wide range of experiments, the proposed method not only effectively unlearns targeted text but also preserves the LLMs' capabilities in open-ended generation tasks as well as in NLU tasks. Our results demonstrate the usefulness of this approach across different models and sizes, and also wit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40657;&#30418;&#29615;&#22659;&#30340;&#22522;&#20110;LLM&#30340;&#24037;&#20855;&#29983;&#25104;&#26234;&#33021;&#20307;&#30340;&#26041;&#27861;&#65292;&#22312;&#22797;&#26434;&#30340;API&#35843;&#29992;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#24212;&#23545;&#20855;&#26377;&#19981;&#21487;&#36870;&#24615;&#21644;&#22823;&#37327;&#26102;&#38388;&#28040;&#32791;&#30340;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.10051</link><description>&lt;p&gt;
SwissNYF&#65306;&#22522;&#20110;&#40657;&#30418;&#29615;&#22659;&#30340;&#22522;&#20110;LLM&#30340;&#24037;&#20855;&#29983;&#25104;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
SwissNYF: Tool Grounded LLM Agents for Black Box Setting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10051
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40657;&#30418;&#29615;&#22659;&#30340;&#22522;&#20110;LLM&#30340;&#24037;&#20855;&#29983;&#25104;&#26234;&#33021;&#20307;&#30340;&#26041;&#27861;&#65292;&#22312;&#22797;&#26434;&#30340;API&#35843;&#29992;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#24212;&#23545;&#20855;&#26377;&#19981;&#21487;&#36870;&#24615;&#21644;&#22823;&#37327;&#26102;&#38388;&#28040;&#32791;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35775;&#38382;&#20989;&#25968;&#30340;&#36820;&#22238;&#32467;&#26524;&#19978;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#22686;&#24378;&#30340;&#21151;&#33021;&#35843;&#29992;&#33021;&#21147;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#22312;&#31616;&#21333;API&#19978;&#26159;&#23454;&#29992;&#30340;&#65292;&#20294;&#23545;&#20110;&#19981;&#21487;&#36870;API&#65288;&#20363;&#22914;&#25968;&#25454;&#24211;&#21024;&#38500;API&#65289;&#20250;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#21516;&#26679;&#65292;&#23545;&#20110;&#27599;&#20010;API&#35843;&#29992;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#30340;&#27969;&#31243;&#20197;&#21450;&#38656;&#35201;&#21069;&#21521;&#35268;&#21010;&#30340;&#33258;&#21160;&#21270;&#25805;&#20316;&#31649;&#36947;&#31561;&#37117;&#23384;&#22312;&#22797;&#26434;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#36890;&#24120;&#20986;&#29616;&#30340;&#24773;&#20917;&#26159;&#38656;&#35201;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#31639;&#27861;&#32570;&#20047;&#23545;&#36825;&#20123;&#20989;&#25968;&#30340;&#29305;&#23450;&#23454;&#29616;&#25110;&#20351;&#29992;&#23427;&#20204;&#30340;&#31192;&#23494;&#30340;&#30452;&#25509;&#35775;&#38382;&#26041;&#24335;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#20256;&#32479;&#30340;&#24037;&#20855;&#35268;&#21010;&#26041;&#27861;&#26159;&#19981;&#21512;&#36866;&#30340;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#40657;&#30418;&#29615;&#22659;&#20013;&#36816;&#34892;&#12290;&#19982;&#22312;&#24037;&#20855;&#25805;&#20316;&#20013;&#30340;&#34920;&#29616;&#19981;&#21516;&#65292;LLM&#22312;&#40657;&#30418;&#20219;&#21153;&#65288;&#20363;&#22914;&#31243;&#24207;&#32508;&#21512;&#65289;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#26469;&#29983;&#25104;&#22522;&#20110;&#40657;&#30418;&#29615;&#22659;&#30340;&#26234;&#33021;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10051v1 Announce Type: new  Abstract: While Large Language Models (LLMs) have demonstrated enhanced capabilities in function-calling, these advancements primarily rely on accessing the functions' responses. This methodology is practical for simpler APIs but faces scalability issues with irreversible APIs that significantly impact the system, such as a database deletion API. Similarly, processes requiring extensive time for each API call and those necessitating forward planning, like automated action pipelines, present complex challenges. Furthermore, scenarios often arise where a generalized approach is needed because algorithms lack direct access to the specific implementations of these functions or secrets to use them. Traditional tool planning methods are inadequate in these cases, compelling the need to operate within black-box environments. Unlike their performance in tool manipulation, LLMs excel in black-box tasks, such as program synthesis. Therefore, we harness the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RS-DPO&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#25298;&#32477;&#37319;&#26679;&#21644;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#30340;&#31574;&#30053;&#27169;&#22411;&#65292;&#24182;&#20174;&#35813;&#27169;&#22411;&#20013;&#30452;&#25509;&#37319;&#26679;&#21709;&#24212;&#65292;RS-DPO&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22522;&#20110;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35782;&#21035;&#23545;&#27604;&#26679;&#26412;&#23545;&#65292;RS-DPO&#33021;&#22815;&#26356;&#22909;&#22320;&#36827;&#34892;RLHF&#12290;</title><link>https://arxiv.org/abs/2402.10038</link><description>&lt;p&gt;
RS-DPO&#65306;&#19968;&#31181;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28151;&#21512;&#25298;&#32477;&#37319;&#26679;&#21644;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RS-DPO&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#25298;&#32477;&#37319;&#26679;&#21644;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#30340;&#31574;&#30053;&#27169;&#22411;&#65292;&#24182;&#20174;&#35813;&#27169;&#22411;&#20013;&#30452;&#25509;&#37319;&#26679;&#21709;&#24212;&#65292;RS-DPO&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22522;&#20110;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35782;&#21035;&#23545;&#27604;&#26679;&#26412;&#23545;&#65292;RS-DPO&#33021;&#22815;&#26356;&#22909;&#22320;&#36827;&#34892;RLHF&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#29992;&#25143;&#24847;&#22270;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#30340;RLHF&#26377;&#26102;&#19981;&#31283;&#23450;&#65292;&#38656;&#35201;&#26174;&#33879;&#30340;&#36229;&#21442;&#25968;&#24494;&#35843;&#65292;&#24182;&#19988;&#22312;&#23545;&#40784;&#36807;&#31243;&#20013;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#65288;DPO&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;DPO&#20381;&#36182;&#20110;&#20174;&#20154;&#31867;&#26631;&#27880;&#32773;&#21644;&#26367;&#20195;LLM&#29983;&#25104;&#30340;&#23545;&#27604;&#22238;&#22797;&#65292;&#32780;&#19981;&#26159;&#31574;&#30053;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;RLHF&#30340;&#25928;&#26524;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#22320;&#32467;&#21512;&#25298;&#32477;&#37319;&#26679;&#65288;RS&#65289;&#21644;DPO&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;RS-DPO&#65292;&#39318;&#20808;&#24320;&#21457;&#20986;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#30340;&#31574;&#30053;&#27169;&#22411;&#65288;SFT&#65289;&#12290;&#28982;&#21518;&#30452;&#25509;&#20174;SFT&#27169;&#22411;&#20013;&#37319;&#26679;&#27599;&#20010;&#25552;&#31034;&#30340;k&#20010;&#21709;&#24212;&#12290;RS-DPO&#22522;&#20110;&#20854;&#30456;&#20284;&#24230;&#35782;&#21035;&#23545;&#27604;&#26679;&#26412;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10038v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) has been extensively employed to align large language models with user intent. However, proximal policy optimization (PPO) based RLHF is occasionally unstable requiring significant hyperparameter finetuning, and computationally expensive to maximize the estimated reward during alignment. Recently, direct preference optimization (DPO) is proposed to address those challenges. However, DPO relies on contrastive responses generated from human annotator and alternative LLM, instead of the policy model, limiting the effectiveness of the RLHF. In this paper, we addresses both challenges by systematically combining rejection sampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the development of a supervised fine-tuned policy model (SFT). A varied set of k responses per prompt are sampled directly from the SFT model. RS-DPO identifies pairs of contrastive samples based on their re
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#23398;&#20064;&#19978;&#19979;&#25991;&#22686;&#24378;&#26041;&#27861;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#35789;&#27719;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#22312;&#38646;&#26679;&#26412;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#22522;&#20110;&#26144;&#23556;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10024</link><description>&lt;p&gt;
&#33258;&#23398;&#20064;&#19978;&#19979;&#25991;&#22686;&#24378;&#23545;&#20110;&#26080;&#30417;&#30563;&#35789;&#27719;&#32763;&#35793;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Self-Augmented In-Context Learning for Unsupervised Word Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10024
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#23398;&#20064;&#19978;&#19979;&#25991;&#22686;&#24378;&#26041;&#27861;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#35789;&#27719;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#22312;&#38646;&#26679;&#26412;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#22522;&#20110;&#26144;&#23556;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#20123;&#23567;&#35268;&#27169;&#30340;&#35774;&#32622;&#20013;&#23637;&#31034;&#20986;&#20102;&#36739;&#24378;&#30340;&#35789;&#27719;&#32763;&#35793;&#21644;&#21452;&#35821;&#35789;&#20856;&#35825;&#23548;(BLI)&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#27809;&#26377;&#31181;&#23376;&#32763;&#35793;&#23545;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#65292;&#23427;&#20204;&#20173;&#28982;&#26080;&#27861;&#36798;&#21040;&#8220;&#20256;&#32479;&#8221;&#30340;&#22522;&#20110;&#26144;&#23556;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20064;&#19978;&#19979;&#25991;&#22686;&#24378;&#26041;&#27861; (SAIL) &#26469;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;BLI&#65306;&#20174;&#38646;&#26679;&#26412;&#25552;&#31034;&#24320;&#22987;&#65292;SAIL&#36890;&#36807;&#36845;&#20195;&#22320;&#20174;LLM&#20013;&#24341;&#20986;&#19968;&#32452;&#39640;&#32622;&#20449;&#24230;&#30340;&#35789;&#27719;&#32763;&#35793;&#23545;&#65292;&#28982;&#21518;&#22312;ICL&#30340;&#26041;&#24335;&#19979;&#20877;&#27425;&#24212;&#29992;&#20110;&#21516;&#19968;&#20010;LLM&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#24191;&#27867;&#30340;BLI&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#36328;&#36234;&#22810;&#31181;&#35821;&#35328;&#23545;&#65292;&#22312;&#38646;&#26679;&#26412;&#25552;&#31034;&#30340;LLM&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20063;&#22312;&#21508;&#20010;&#26041;&#38754;&#20248;&#20110;&#22522;&#20110;&#26144;&#23556;&#30340;&#22522;&#32447;&#12290;&#38500;&#20102;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10024v1 Announce Type: cross  Abstract: Recent work has shown that, while large language models (LLMs) demonstrate strong word translation or bilingual lexicon induction (BLI) capabilities in few-shot setups, they still cannot match the performance of 'traditional' mapping-based approaches in the unsupervised scenario where no seed translation pairs are available, especially for lower-resource languages. To address this challenge with LLMs, we propose self-augmented in-context learning (SAIL) for unsupervised BLI: starting from a zero-shot prompt, SAIL iteratively induces a set of high-confidence word translation pairs for in-context learning (ICL) from an LLM, which it then reapplies to the same LLM in the ICL fashion. Our method shows substantial gains over zero-shot prompting of LLMs on two established BLI benchmarks spanning a wide range of language pairs, also outperforming mapping-based baselines across the board. In addition to achieving state-of-the-art unsupervised 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#30446;&#26631;&#65288;MDL&#65289;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#24418;&#24335;&#35821;&#35328;&#23398;&#20064;&#20013;&#30340;&#32463;&#39564;-&#29702;&#35770;&#24046;&#36317;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10013</link><description>&lt;p&gt;
&#21033;&#29992;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#32553;&#23567;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#35821;&#35328;&#23398;&#20064;&#20013;&#30340;&#32463;&#39564;-&#29702;&#35770;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Bridging the Empirical-Theoretical Gap in Neural Network Formal Language Learning Using Minimum Description Length
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10013
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#30446;&#26631;&#65288;MDL&#65289;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#24418;&#24335;&#35821;&#35328;&#23398;&#20064;&#20013;&#30340;&#32463;&#39564;-&#29702;&#35770;&#24046;&#36317;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#36817;&#20284;&#65292;&#20294;&#26159;&#21363;&#20351;&#29702;&#35770;&#24037;&#20316;&#34920;&#26126;&#36825;&#20123;&#23436;&#32654;&#30340;&#35299;&#21487;&#20197;&#30001;&#29305;&#23450;&#30340;&#26550;&#26500;&#26469;&#34920;&#31034;&#65292;&#23427;&#20204;&#20173;&#28982;&#26080;&#27861;&#36798;&#21040;&#23436;&#32654;&#30340;&#27867;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#24418;&#24335;&#35821;&#35328;&#23398;&#20064;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#24418;&#24335;&#35821;&#35328;&#65292;&#24182;&#23637;&#31034;&#20102;&#29702;&#35770;&#19978;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#23454;&#38469;&#19978;&#19981;&#26159;&#24120;&#29992;&#30446;&#26631;&#20989;&#25968;&#30340;&#26368;&#20248;&#35299;&#65292;&#21363;&#20351;&#20351;&#29992;&#20102;&#27491;&#21017;&#21270;&#25216;&#26415;&#65288;&#22914;L1&#65292;L2&#65289;&#25110;&#20854;&#20182;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65288;&#26089;&#20572;&#65292;dropout&#65289;&#12290;&#28982;&#32780;&#65292;&#23558;&#26631;&#20934;&#30446;&#26631;&#26367;&#25442;&#20026;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#30446;&#26631;&#65288;MDL&#65289;&#21487;&#20197;&#20351;&#27491;&#30830;&#30340;&#35299;&#25104;&#20026;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10013v1 Announce Type: new  Abstract: Neural networks offer good approximation to many tasks but consistently fail to reach perfect generalization, even when theoretical work shows that such perfect solutions can be expressed by certain architectures. Using the task of formal language learning, we focus on one simple formal language and show that the theoretically correct solution is in fact not an optimum of commonly used objectives -- even with regularization techniques that according to common wisdom should lead to simple weights and good generalization (L1, L2) or other meta-heuristics (early-stopping, dropout). However, replacing standard targets with the Minimum Description Length objective (MDL) results in the correct solution being an optimum.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RiVEG&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#36830;&#25509;&#26725;&#26753;&#65292;&#23558;&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#37325;&#26032;&#26500;&#24314;&#20026;&#32852;&#21512;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#21629;&#21517;&#23454;&#20307;&#26080;&#27861;&#30830;&#23450;&#21644;&#25351;&#20195;&#34920;&#36798;&#19982;&#21629;&#21517;&#23454;&#20307;&#20043;&#38388;&#30340;&#21306;&#21035;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09989</link><description>&lt;p&gt;
LLMs&#20316;&#20026;&#26725;&#26753;&#65306;&#37325;&#26032;&#26500;&#24314;&#22522;&#20110;&#22810;&#27169;&#24577;&#22270;&#20687;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RiVEG&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#36830;&#25509;&#26725;&#26753;&#65292;&#23558;&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#37325;&#26032;&#26500;&#24314;&#20026;&#32852;&#21512;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#21629;&#21517;&#23454;&#20307;&#26080;&#27861;&#30830;&#23450;&#21644;&#25351;&#20195;&#34920;&#36798;&#19982;&#21629;&#21517;&#23454;&#20307;&#20043;&#38388;&#30340;&#21306;&#21035;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Grounded Multimodal Named Entity Recognition (GMNER) &#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#26088;&#22312;&#35782;&#21035;&#21629;&#21517;&#23454;&#20307;&#12289;&#23454;&#20307;&#31867;&#22411;&#21450;&#20854;&#23545;&#24212;&#30340;&#35270;&#35273;&#21306;&#22495;&#12290;GMNER&#20219;&#21153;&#20855;&#26377;&#20004;&#20010;&#25361;&#25112;&#24615;&#36136;&#65306;1&#65289;&#31038;&#20132;&#23186;&#20307;&#20013;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#24369;&#30456;&#20851;&#24615;&#23548;&#33268;&#22823;&#37096;&#20998;&#21629;&#21517;&#23454;&#20307;&#38590;&#20197;&#30830;&#23450;&#65307;2&#65289;&#24120;&#29992;&#20110;&#31867;&#20284;&#20219;&#21153;&#30340;&#31895;&#31890;&#24230;&#25351;&#20195;&#34920;&#36798;&#19982;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#21306;&#21035;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;RiVEG&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#36830;&#25509;&#26725;&#26753;&#65292;&#23558;GMNER&#37325;&#26032;&#26500;&#24314;&#20026;&#32852;&#21512;MNER-VE-VG&#20219;&#21153;&#12290;&#36825;&#31181;&#37325;&#26032;&#26500;&#24314;&#24102;&#26469;&#20102;&#20004;&#20010;&#22909;&#22788;&#65306;1&#65289;&#20445;&#25345;&#20102;&#26368;&#20339;&#30340;MNER&#24615;&#33021;&#65292;&#28040;&#38500;&#20102;&#20351;&#29992;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#39044;&#25552;&#21462;&#21306;&#22495;&#29305;&#24449;&#30340;&#38656;&#27714;&#65292;&#33258;&#28982;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09989v1 Announce Type: cross  Abstract: Grounded Multimodal Named Entity Recognition (GMNER) is a nascent multimodal task that aims to identify named entities, entity types and their corresponding visual regions. GMNER task exhibits two challenging properties: 1) The weak correlation between image-text pairs in social media results in a significant portion of named entities being ungroundable. 2) There exists a distinction between coarse-grained referring expressions commonly used in similar tasks (e.g., phrase localization, referring expression comprehension) and fine-grained named entities. In this paper, we propose RiVEG, a unified framework that reformulates GMNER into a joint MNER-VE-VG task by leveraging large language models (LLMs) as a connecting bridge. This reformulation brings two benefits: 1) It maintains the optimal MNER performance and eliminates the need for employing object detection methods to pre-extract regional features, thereby naturally addressing two m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35789;&#27719;&#36716;&#31227;&#30340;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20854;&#20182;&#21387;&#32553;&#25216;&#26415;&#32467;&#21512;&#20351;&#29992;&#65292;&#26174;&#33879;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#24615;&#33021;&#30053;&#26377;&#22949;&#21327;&#12290;</title><link>https://arxiv.org/abs/2402.09977</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#30340;&#24555;&#36895;&#35789;&#27719;&#36716;&#31227;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fast Vocabulary Transfer for Language Model Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09977
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35789;&#27719;&#36716;&#31227;&#30340;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20854;&#20182;&#21387;&#32553;&#25216;&#26415;&#32467;&#21512;&#20351;&#29992;&#65292;&#26174;&#33879;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#24615;&#33021;&#30053;&#26377;&#22949;&#21327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#19994;&#21153;&#24212;&#29992;&#38656;&#35201;&#22312;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#21644;&#22823;&#23567;&#20043;&#38388;&#20570;&#20986;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35789;&#27719;&#36716;&#31227;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#22402;&#30452;&#39046;&#22495;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35789;&#27719;&#36716;&#31227;&#21487;&#20197;&#19982;&#20854;&#20182;&#21387;&#32553;&#25216;&#26415;&#26377;&#25928;&#32467;&#21512;&#20351;&#29992;&#65292;&#26174;&#33879;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#22312;&#24615;&#33021;&#19978;&#30053;&#26377;&#22949;&#21327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09977v1 Announce Type: cross  Abstract: Real-world business applications require a trade-off between language model performance and size. We propose a new method for model compression that relies on vocabulary transfer. We evaluate the method on various vertical domains and downstream tasks. Our results indicate that vocabulary transfer can be effectively used in combination with other compression techniques, yielding a significant reduction in model size and inference time while marginally compromising on performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#22797;&#26434;&#25512;&#29702;&#22330;&#26223;&#20013;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#26159;&#30446;&#21069;&#38656;&#35201;&#25913;&#36827;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09967</link><description>&lt;p&gt;
&#26696;&#20363;&#30740;&#31350;&#65306;&#22312;&#26576;&#20123;&#25512;&#29702;&#20219;&#21153;&#20013;&#27979;&#35797;&#27169;&#22411;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Case Study: Testing Model Capabilities in Some Reasoning Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#22797;&#26434;&#25512;&#29702;&#22330;&#26223;&#20013;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#26159;&#30446;&#21069;&#38656;&#35201;&#25913;&#36827;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#20010;&#24615;&#21270;&#20869;&#23481;&#21644;&#20419;&#36827;&#20132;&#20114;&#23545;&#35805;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#29702;&#21644;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#36755;&#20986;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#22312;&#25512;&#29702;&#33021;&#21147;&#30340;&#32972;&#26223;&#19979;&#65292;&#23427;&#20204;&#30340;&#33021;&#21147;&#20173;&#28982;&#38656;&#35201;&#25913;&#36827;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#30446;&#21069;&#38459;&#30861;&#23427;&#20204;&#22312;&#22797;&#26434;&#25512;&#29702;&#22330;&#26223;&#20013;&#21457;&#25381;&#26377;&#25928;&#24615;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09967v1 Announce Type: new  Abstract: Large Language Models (LLMs) excel in generating personalized content and facilitating interactive dialogues, showcasing their remarkable aptitude for a myriad of applications. However, their capabilities in reasoning and providing explainable outputs, especially within the context of reasoning abilities, remain areas for improvement. In this study, we delve into the reasoning abilities of LLMs, highlighting the current challenges and limitations that hinder their effectiveness in complex reasoning scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22522;&#20110;&#35282;&#33394;&#29983;&#25104;&#23545;&#35805;&#26041;&#38754;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#35843;&#25972;&#25552;&#31034;&#25351;&#20196;&#21487;&#20197;&#26368;&#30452;&#25509;&#26377;&#25928;&#19988;&#32463;&#27982;&#22320;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#19988;&#38543;&#26426;&#26816;&#32034;&#31034;&#33539;&#20250;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#65292;&#32780;&#26597;&#35810;&#30456;&#21516;&#19978;&#19979;&#25991;&#30340;&#31034;&#33539;&#26816;&#32034;&#25928;&#26524;&#26368;&#24046;&#12290;&#21363;&#20351;&#30772;&#22351;&#20102;&#31034;&#33539;&#20013;&#30340;&#22810;&#22238;&#21512;&#20851;&#32852;&#21644;&#21333;&#22238;&#21512;&#35821;&#20041;&#65292;&#23545;&#35805;&#29983;&#25104;&#20173;&#28982;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.09954</link><description>&lt;p&gt;
&#21046;&#23450;&#33391;&#22909;&#25552;&#31034;&#36824;&#26159;&#25552;&#20379;&#20986;&#33394;&#30340;&#23545;&#35805;&#65311;&#20851;&#20110;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#35282;&#33394;&#29983;&#25104;&#23545;&#35805;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of In-Context Learning for Persona-based Dialogue Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22522;&#20110;&#35282;&#33394;&#29983;&#25104;&#23545;&#35805;&#26041;&#38754;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#35843;&#25972;&#25552;&#31034;&#25351;&#20196;&#21487;&#20197;&#26368;&#30452;&#25509;&#26377;&#25928;&#19988;&#32463;&#27982;&#22320;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#19988;&#38543;&#26426;&#26816;&#32034;&#31034;&#33539;&#20250;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#65292;&#32780;&#26597;&#35810;&#30456;&#21516;&#19978;&#19979;&#25991;&#30340;&#31034;&#33539;&#26816;&#32034;&#25928;&#26524;&#26368;&#24046;&#12290;&#21363;&#20351;&#30772;&#22351;&#20102;&#31034;&#33539;&#20013;&#30340;&#22810;&#22238;&#21512;&#20851;&#32852;&#21644;&#21333;&#22238;&#21512;&#35821;&#20041;&#65292;&#23545;&#35805;&#29983;&#25104;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20851;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#30740;&#31350;&#20027;&#35201;&#20391;&#37325;&#20110;&#20998;&#31867;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#34920;&#26684;&#31561;&#20219;&#21153;&#65292;&#32780;&#23545;&#20110;ICL&#33021;&#21542;&#25913;&#36827;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#23545;&#35805;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#22312;&#39640;&#36136;&#37327;&#30340;&#30495;&#23454;&#20154;&#31867;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22522;&#20110;&#35282;&#33394;&#29983;&#25104;&#23545;&#35805;&#26041;&#38754;&#30340;ICL&#33021;&#21147;&#12290;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#24471;&#20986;&#19977;&#20010;&#32467;&#35770;&#65306;1&#65289;&#35843;&#25972;&#25552;&#31034;&#25351;&#20196;&#26159;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#26368;&#30452;&#25509;&#12289;&#26377;&#25928;&#21644;&#32463;&#27982;&#30340;&#26041;&#27861;&#65307;2&#65289;&#38543;&#26426;&#26816;&#32034;&#31034;&#33539;&#21487;&#20197;&#21462;&#24471;&#26368;&#20339;&#30340;&#32467;&#26524;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#20855;&#26377;&#26356;&#22810;&#26679;&#21270;&#21644;&#26377;&#25928;&#20449;&#24687;&#30340;&#21407;&#22240;&#65307;&#19982;&#26597;&#35810;&#30456;&#21516;&#19978;&#19979;&#25991;&#30340;&#31034;&#33539;&#26816;&#32034;&#32467;&#26524;&#26368;&#24046;&#65307;3&#65289;&#21363;&#20351;&#30772;&#22351;&#20102;&#31034;&#33539;&#20013;&#30340;&#22810;&#22238;&#21512;&#20851;&#32852;&#21644;&#21333;&#22238;&#21512;&#35821;&#20041;&#65292;&#23545;&#35805;&#29983;&#25104;&#20173;&#28982;&#21487;&#20197;&#23454;&#29616;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09954v1 Announce Type: new  Abstract: Previous in-context learning (ICL) research has focused on tasks such as classification, machine translation, text2table, etc., while studies on whether ICL can improve human-like dialogue generation are scarce. Our work fills this gap by systematically investigating the ICL capabilities of large language models (LLMs) in persona-based dialogue generation, conducting extensive experiments on high-quality real human Chinese dialogue datasets. From experimental results, we draw three conclusions: 1) adjusting prompt instructions is the most direct, effective, and economical way to improve generation quality; 2) randomly retrieving demonstrations (demos) achieves the best results, possibly due to the greater diversity and the amount of effective information; counter-intuitively, retrieving demos with a context identical to the query performs the worst; 3) even when we destroy the multi-turn associations and single-turn semantics in the demo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MWT&#30340;&#22810;&#35789;&#26631;&#35760;&#22120;&#65292;&#36890;&#36807;&#23558;&#39057;&#32321;&#20986;&#29616;&#30340;&#22810;&#35789;&#34920;&#36798;&#24335;&#34920;&#31034;&#20026;&#21333;&#20010;&#26631;&#35760;&#65292;&#31361;&#30772;&#20102;&#35789;&#36793;&#30028;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#32039;&#20945;&#21644;&#39640;&#25928;&#30340;&#26631;&#35760;&#21270;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.09949</link><description>&lt;p&gt;
&#22810;&#35789;&#26631;&#35760;&#21270;&#29992;&#20110;&#24207;&#21015;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Multi-Word Tokenization for Sequence Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MWT&#30340;&#22810;&#35789;&#26631;&#35760;&#22120;&#65292;&#36890;&#36807;&#23558;&#39057;&#32321;&#20986;&#29616;&#30340;&#22810;&#35789;&#34920;&#36798;&#24335;&#34920;&#31034;&#20026;&#21333;&#20010;&#26631;&#35760;&#65292;&#31361;&#30772;&#20102;&#35789;&#36793;&#30028;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#32039;&#20945;&#21644;&#39640;&#25928;&#30340;&#26631;&#35760;&#21270;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24314;&#27169;&#21508;&#31181;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26497;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20063;&#24847;&#21619;&#30528;&#35745;&#31639;&#25104;&#26412;&#30340;&#22823;&#24133;&#22686;&#21152;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MWT&#30340;&#22810;&#35789;&#26631;&#35760;&#22120;&#65292;&#36890;&#36807;&#23558;&#39057;&#32321;&#20986;&#29616;&#30340;&#22810;&#35789;&#34920;&#36798;&#24335;&#34920;&#31034;&#20026;&#21333;&#20010;&#26631;&#35760;&#65292;&#31361;&#30772;&#20102;&#35789;&#36793;&#30028;&#30340;&#38480;&#21046;&#12290;MWT&#20135;&#29983;&#20102;&#26356;&#32039;&#20945;&#21644;&#39640;&#25928;&#30340;&#26631;&#35760;&#21270;&#32467;&#26524;&#65292;&#24102;&#26469;&#20004;&#20010;&#22909;&#22788;&#65306;&#65288;1&#65289;&#22312;&#22266;&#23450;&#24207;&#21015;&#38271;&#24230;&#21644;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#22240;&#20026;&#33021;&#22815;&#26356;&#20840;&#38754;&#22320;&#35206;&#30422;&#36755;&#20837;&#25968;&#25454;&#65307;&#65288;2&#65289;&#30001;&#20110;&#33021;&#22815;&#20943;&#23569;&#24207;&#21015;&#38271;&#24230;&#32780;&#23545;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24555;&#36895;&#21644;&#26356;&#36731;&#37327;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;MWT&#22312;&#36739;&#30701;&#30340;&#24207;&#21015;&#38271;&#24230;&#19979;&#26356;&#20026;&#31283;&#20581;&#65292;&#20174;&#32780;&#21487;&#20197;&#36890;&#36807;&#26089;&#26399;&#24207;&#21015;&#25130;&#26029;&#23454;&#29616;&#37325;&#22823;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09949v1 Announce Type: new  Abstract: Large Language Models have proven highly successful at modelling a variety of tasks. However, this comes at a steep computational cost that hinders wider industrial uptake. In this pa005 per, we present MWT: a Multi-Word Tokenizer that goes beyond word boundaries by representing frequent multi-word expressions as single tokens. MWTs produce a more compact and efficient tokenization that yields two benefits: (1) Increase in performance due to a greater coverage of input data given a fixed sequence length and budget; (2) Faster and lighter inference due to the ability to reduce the sequence length with negligible drops in performance. Our results show that MWT is more robust across shorter sequence lengths, thus allowing for major speedups via early sequence truncation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#25552;&#20379;&#20102;&#24314;&#31569;&#34892;&#19994;&#20013;&#29983;&#25104;&#24335;AI&#30340;&#26368;&#26032;&#29366;&#24577;&#12289;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24110;&#21161;&#24314;&#31569;&#20844;&#21496;&#26500;&#24314;&#23450;&#21046;&#21270;&#29983;&#25104;&#24335;AI&#35299;&#20915;&#26041;&#26696;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.09939</link><description>&lt;p&gt;
&#24314;&#31569;&#34892;&#19994;&#20013;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65306;&#19968;&#39033;&#26368;&#26032;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Generative AI in the Construction Industry: A State-of-the-art Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#25552;&#20379;&#20102;&#24314;&#31569;&#34892;&#19994;&#20013;&#29983;&#25104;&#24335;AI&#30340;&#26368;&#26032;&#29366;&#24577;&#12289;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24110;&#21161;&#24314;&#31569;&#20844;&#21496;&#26500;&#24314;&#23450;&#21046;&#21270;&#29983;&#25104;&#24335;AI&#35299;&#20915;&#26041;&#26696;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31569;&#34892;&#19994;&#26159;&#20840;&#29699;&#32463;&#27982;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#19968;&#20010;&#37096;&#38376;&#65292;&#20294;&#22312;&#35774;&#35745;&#12289;&#35268;&#21010;&#12289;&#37319;&#36141;&#12289;&#26816;&#26597;&#21644;&#32500;&#25252;&#31561;&#21508;&#20010;&#29615;&#33410;&#20013;&#38754;&#20020;&#30528;&#35768;&#22810;&#29983;&#20135;&#21147;&#25361;&#25112;&#12290;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21487;&#20197;&#22522;&#20110;&#26576;&#20123;&#36755;&#20837;&#25110;&#20808;&#21069;&#30340;&#30693;&#35782;&#21019;&#36896;&#26032;&#39062;&#19988;&#36924;&#30495;&#30340;&#25968;&#25454;&#25110;&#20869;&#23481;&#65292;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#25110;&#20195;&#30721;&#65292;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#25552;&#20379;&#20102;&#21019;&#26032;&#21644;&#39072;&#35206;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#20851;&#20110;&#24314;&#31569;&#34892;&#19994;&#20013;&#29983;&#25104;&#24335;AI&#30340;&#24403;&#21069;&#29366;&#24577;&#12289;&#26426;&#36935;&#21644;&#25361;&#25112;&#30340;&#25991;&#29486;&#20013;&#23384;&#22312;&#30528;&#31354;&#30333;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#24314;&#31569;&#39046;&#22495;&#29983;&#25104;&#24335;AI&#30340;&#26368;&#26032;&#20998;&#26512;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#32570;&#65292;&#30740;&#31350;&#30446;&#26631;&#21253;&#25324;&#65306;&#65288;1&#65289;&#23545;&#24314;&#31569;&#34892;&#19994;&#29616;&#26377;&#21644;&#26032;&#20852;&#30340;&#29983;&#25104;&#24335;AI&#26426;&#36935;&#21644;&#25361;&#25112;&#36827;&#34892;&#22238;&#39038;&#21644;&#20998;&#31867;&#65307;&#65288;2&#65289;&#25552;&#20986;&#19968;&#20010;&#26694;&#26550;&#65292;&#24110;&#21161;&#24314;&#31569;&#20844;&#21496;&#21033;&#29992;&#33258;&#24049;&#30340;&#25968;&#25454;&#21644;&#38656;&#27714;&#26500;&#24314;&#23450;&#21046;&#21270;&#30340;&#29983;&#25104;&#24335;AI&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09939v1 Announce Type: new  Abstract: The construction industry is a vital sector of the global economy, but it faces many productivity challenges in various processes, such as design, planning, procurement, inspection, and maintenance. Generative artificial intelligence (AI), which can create novel and realistic data or content, such as text, image, video, or code, based on some input or prior knowledge, offers innovative and disruptive solutions to address these challenges. However, there is a gap in the literature on the current state, opportunities, and challenges of generative AI in the construction industry. This study aims to fill this gap by providing a state-of-the-art analysis of generative AI in construction, with three objectives: (1) to review and categorize the existing and emerging generative AI opportunities and challenges in the construction industry; (2) to propose a framework for construction firms to build customized generative AI solutions using their ow
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25366;&#25496;&#20102;&#22312;&#32447;&#35805;&#35821;&#20013;&#30340;&#28436;&#32462;&#32454;&#24494;&#24046;&#21035;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20934;&#30830;&#26816;&#27979;&#21453;&#38382;&#20027;&#20041;&#65292;&#24182;&#22312;Twitter&#21644;YouTube&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.09934</link><description>&lt;p&gt;
&#20851;&#27880;&#20559;&#24046;&#65306;&#25366;&#25496;&#22312;&#32447;&#35805;&#35821;&#20013;&#30340;&#28436;&#32462;&#32454;&#24494;&#24046;&#21035;&#65292;&#26816;&#27979;&#21453;&#38382;&#20027;&#20041;
&lt;/p&gt;
&lt;p&gt;
Paying Attention to Deflections: Mining Pragmatic Nuances for Whataboutism Detection in Online Discourse
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25366;&#25496;&#20102;&#22312;&#32447;&#35805;&#35821;&#20013;&#30340;&#28436;&#32462;&#32454;&#24494;&#24046;&#21035;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20934;&#30830;&#26816;&#27979;&#21453;&#38382;&#20027;&#20041;&#65292;&#24182;&#22312;Twitter&#21644;YouTube&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#38382;&#20027;&#20041;&#22312;&#25200;&#20081;&#21465;&#20107;&#21644;&#25773;&#31181;&#19981;&#20449;&#20219;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#24037;&#20855;&#25928;&#29992;&#65292;&#20294;&#22312;&#23450;&#37327;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#21364;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#36807;&#21435;&#30340;&#30740;&#31350;&#26410;&#33021;&#21306;&#20998;&#21453;&#38382;&#20027;&#20041;&#20316;&#20026;&#35823;&#23548;&#21644;&#23459;&#20256;&#31574;&#30053;&#30340;&#29992;&#36884;&#19982;&#20854;&#20316;&#20026;&#35821;&#29992;&#21644;&#35821;&#20041;&#26694;&#26550;&#24037;&#20855;&#30340;&#29992;&#36884;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#26032;&#30340;&#26469;&#33258;Twitter&#21644;YouTube&#30340;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#21453;&#38382;&#20027;&#20041;&#12289;&#23459;&#20256;&#21644;tu quoque&#35884;&#35823;&#20043;&#38388;&#30340;&#37325;&#21472;&#21644;&#21306;&#21035;&#12290;&#27492;&#22806;&#65292;&#32467;&#21512;&#26368;&#36817;&#22312;&#35821;&#35328;&#35821;&#20041;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#23558;&#8220;what about&#8221;&#35789;&#27719;&#32467;&#26500;&#19982;&#21453;&#38382;&#20027;&#20041;&#21306;&#20998;&#24320;&#26469;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#20934;&#30830;&#26816;&#27979;&#21453;&#38382;&#20027;&#20041;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#20419;&#20351;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#27880;&#24847;&#26435;&#37325;&#36827;&#34892;&#36127;&#26679;&#26412;&#25366;&#25496;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;Twitter&#21644;YouTube&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#21035;&#30456;&#23545;&#20110;&#20043;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;4%&#21644;10%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09934v1 Announce Type: cross  Abstract: Whataboutism, a potent tool for disrupting narratives and sowing distrust, remains under-explored in quantitative NLP research. Moreover, past work has not distinguished its use as a strategy for misinformation and propaganda from its use as a tool for pragmatic and semantic framing. We introduce new datasets from Twitter and YouTube, revealing overlaps as well as distinctions between whataboutism, propaganda, and the tu quoque fallacy. Furthermore, drawing on recent work in linguistic semantics, we differentiate the `what about' lexical construct from whataboutism. Our experiments bring to light unique challenges in its accurate detection, prompting the introduction of a novel method using attention weights for negative sample mining. We report significant improvements of 4% and 10% over previous state-of-the-art methods in our Twitter and YouTube collections, respectively.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20013;&#25991;&#22810;&#27573;&#31572;&#26696;&#30340;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#25968;&#25454;&#38598;CLEAN&#65292;&#24357;&#34917;&#20102;&#20013;&#25991;MSQA&#30740;&#31350;&#20013;&#30340;&#19981;&#36275;&#65292;&#21253;&#25324;&#22810;&#26679;&#30340;&#20027;&#39064;&#21644;&#38656;&#35201;&#35814;&#32454;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#25552;&#20379;&#20102;&#30456;&#20851;&#25991;&#29486;&#20013;&#30340;&#22522;&#32447;&#27169;&#22411;&#20316;&#20026;&#21442;&#32771;&#12290;</title><link>https://arxiv.org/abs/2402.09923</link><description>&lt;p&gt;
&#19968;&#20010;&#21253;&#21547;&#22810;&#27573;&#31572;&#26696;&#30340;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Dataset of Open-Domain Question Answering with Multiple-Span Answers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09923
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20013;&#25991;&#22810;&#27573;&#31572;&#26696;&#30340;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#25968;&#25454;&#38598;CLEAN&#65292;&#24357;&#34917;&#20102;&#20013;&#25991;MSQA&#30740;&#31350;&#20013;&#30340;&#19981;&#36275;&#65292;&#21253;&#25324;&#22810;&#26679;&#30340;&#20027;&#39064;&#21644;&#38656;&#35201;&#35814;&#32454;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#25552;&#20379;&#20102;&#30456;&#20851;&#25991;&#29486;&#20013;&#30340;&#22522;&#32447;&#27169;&#22411;&#20316;&#20026;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27573;&#31572;&#26696;&#25552;&#21462;&#65292;&#20063;&#31216;&#20026;&#22810;&#27573;&#38382;&#31572;&#65288;MSQA&#65289;&#20219;&#21153;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#22810;&#20010;&#20449;&#24687;&#29255;&#27573;&#26469;&#22238;&#31572;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#33521;&#25991;MSQA&#30740;&#31350;&#27963;&#36291;&#24182;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#20294;&#22312;&#20013;&#25991;&#39046;&#22495;&#32570;&#20047;&#20844;&#24320;&#21487;&#29992;&#30340;MSQA&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#20197;&#24448;&#26500;&#24314;MSQA&#25968;&#25454;&#38598;&#30340;&#21162;&#21147;&#20027;&#35201;&#24378;&#35843;&#23454;&#20307;&#20013;&#24515;&#30340;&#24773;&#22659;&#21270;&#65292;&#23548;&#33268;&#20559;&#21521;&#25910;&#38598;&#20107;&#23454;&#24615;&#38382;&#39064;&#24182;&#21487;&#33021;&#24573;&#35270;&#38656;&#35201;&#26356;&#35814;&#32454;&#25551;&#36848;&#24615;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;CLEAN&#65292;&#19968;&#20010;&#20840;&#38754;&#30340;&#20013;&#25991;&#22810;&#27573;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#28041;&#21450;&#21508;&#31181;&#24320;&#25918;&#39046;&#22495;&#30340;&#20027;&#39064;&#65292;&#24182;&#21253;&#21547;&#22823;&#37327;&#38656;&#35201;&#25551;&#36848;&#24615;&#31572;&#26696;&#30340;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#30456;&#20851;&#25991;&#29486;&#20013;&#30340;&#24050;&#24314;&#31435;&#27169;&#22411;&#20316;&#20026;CLEAN&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09923v1 Announce Type: cross  Abstract: Multi-span answer extraction, also known as the task of multi-span question answering (MSQA), is critical for real-world applications, as it requires extracting multiple pieces of information from a text to answer complex questions. Despite the active studies and rapid progress in English MSQA research, there is a notable lack of publicly available MSQA benchmark in Chinese. Previous efforts for constructing MSQA datasets predominantly emphasized entity-centric contextualization, resulting in a bias towards collecting factoid questions and potentially overlooking questions requiring more detailed descriptive responses. To overcome these limitations, we present CLEAN, a comprehensive Chinese multi-span question answering dataset that involves a wide range of open-domain subjects with a substantial number of instances requiring descriptive answers. Additionally, we provide established models from relevant literature as baselines for CLEA
&lt;/p&gt;</description></item><item><title>BUSTER&#26159;&#19968;&#20010;&#21830;&#19994;&#20132;&#26131;&#23454;&#20307;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;3779&#20221;&#25163;&#21160;&#26631;&#27880;&#30340;&#37329;&#34701;&#20132;&#26131;&#25991;&#26723;&#65292;&#24182;&#24314;&#31435;&#20102;&#20960;&#20010;&#22522;&#20934;&#27169;&#22411;&#12290;&#26368;&#20339;&#27169;&#22411;&#36824;&#29992;&#20110;&#33258;&#21160;&#26631;&#27880;6196&#20221;&#25991;&#26723;&#65292;&#24182;&#20316;&#20026;&#39069;&#22806;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#21457;&#24067;&#12290;</title><link>https://arxiv.org/abs/2402.09916</link><description>&lt;p&gt;
BUSTER:&#19968;&#20221;&#8220;&#21830;&#19994;&#20132;&#26131;&#23454;&#20307;&#35782;&#21035;&#8221;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BUSTER: a "BUSiness Transaction Entity Recognition" dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09916
&lt;/p&gt;
&lt;p&gt;
BUSTER&#26159;&#19968;&#20010;&#21830;&#19994;&#20132;&#26131;&#23454;&#20307;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;3779&#20221;&#25163;&#21160;&#26631;&#27880;&#30340;&#37329;&#34701;&#20132;&#26131;&#25991;&#26723;&#65292;&#24182;&#24314;&#31435;&#20102;&#20960;&#20010;&#22522;&#20934;&#27169;&#22411;&#12290;&#26368;&#20339;&#27169;&#22411;&#36824;&#29992;&#20110;&#33258;&#21160;&#26631;&#27880;6196&#20221;&#25991;&#26723;&#65292;&#24182;&#20316;&#20026;&#39069;&#22806;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#36807;&#21435;&#20960;&#24180;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#20294;&#23558;&#36825;&#20123;&#36827;&#23637;&#36716;&#21270;&#20026;&#23454;&#38469;&#30340;&#21830;&#19994;&#26696;&#20363;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#22312;&#20110;&#27969;&#34892;&#30340;&#22522;&#20934;&#25968;&#25454;&#19982;&#23454;&#38469;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#32570;&#20047;&#30417;&#30563;&#12289;&#31867;&#21035;&#19981;&#24179;&#34913;&#12289;&#22122;&#22768;&#25968;&#25454;&#21644;&#38271;&#25991;&#26723;&#32463;&#24120;&#24433;&#21709;&#37329;&#34701;&#12289;&#27861;&#24459;&#21644;&#20581;&#24247;&#31561;&#22402;&#30452;&#39046;&#22495;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#20026;&#20102;&#25903;&#25345;&#38754;&#21521;&#34892;&#19994;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;BUSTER&#30340;&#8220;&#21830;&#19994;&#20132;&#26131;&#23454;&#20307;&#35782;&#21035;&#8221;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;3779&#20221;&#25163;&#21160;&#26631;&#27880;&#30340;&#37329;&#34701;&#20132;&#26131;&#25991;&#26723;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#20960;&#20010;&#22522;&#20934;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#36890;&#29992;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#36824;&#34987;&#29992;&#20110;&#33258;&#21160;&#26631;&#27880;6196&#20221;&#25991;&#26723;&#65292;&#25105;&#20204;&#23558;&#20854;&#20316;&#20026;&#39069;&#22806;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#21457;&#24067;&#32473;BUSTER&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09916v1 Announce Type: new  Abstract: Albeit Natural Language Processing has seen major breakthroughs in the last few years, transferring such advances into real-world business cases can be challenging. One of the reasons resides in the displacement between popular benchmarks and actual data. Lack of supervision, unbalanced classes, noisy data and long documents often affect real problems in vertical domains such as finance, law and health. To support industry-oriented research, we present BUSTER, a BUSiness Transaction Entity Recognition dataset. The dataset consists of 3779 manually annotated documents on financial transactions. We establish several baselines exploiting both general-purpose and domain-specific language models. The best performing model is also used to automatically annotate 6196 documents, which we release as an additional silver corpus to BUSTER.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20266;&#21644;&#22810;&#28304;&#30693;&#35782;&#22270;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22686;&#24378;&#65292;&#20197;&#25913;&#21892;&#20854;&#24187;&#35273;&#38382;&#39064;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#32467;&#21512;&#20266;&#22270;&#29983;&#25104;&#21644;&#21407;&#23376;&#32423;&#30693;&#35782;&#39564;&#35777;&#30340;&#26694;&#26550;&#65292;&#22312;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#29615;&#22659;&#20013;&#20351;&#29992;&#30693;&#35782;&#22270;&#21487;&#20197;&#25552;&#39640;ROUGE-L&#20998;&#25968;&#33267;&#23569;11.5&#12290;</title><link>https://arxiv.org/abs/2402.09911</link><description>&lt;p&gt;
&#20351;&#29992;&#20266;&#21644;&#22810;&#28304;&#30693;&#35782;&#22270;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09911
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20266;&#21644;&#22810;&#28304;&#30693;&#35782;&#22270;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22686;&#24378;&#65292;&#20197;&#25913;&#21892;&#20854;&#24187;&#35273;&#38382;&#39064;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#32467;&#21512;&#20266;&#22270;&#29983;&#25104;&#21644;&#21407;&#23376;&#32423;&#30693;&#35782;&#39564;&#35777;&#30340;&#26694;&#26550;&#65292;&#22312;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#29615;&#22659;&#20013;&#20351;&#29992;&#30693;&#35782;&#22270;&#21487;&#20197;&#25552;&#39640;ROUGE-L&#20998;&#25968;&#33267;&#23569;11.5&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24187;&#35273;&#24182;&#22686;&#24378;&#23427;&#20204;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#23613;&#31649;&#19968;&#20123;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#20102;&#27169;&#22411;&#33258;&#25105;&#22686;&#24378;&#25216;&#26415;&#65292;&#20294;&#23427;&#20204;&#22312;&#26377;&#25928;&#35299;&#20915;&#26410;&#30693;&#20107;&#23454;&#24187;&#35273;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#20351;&#29992;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#22686;&#24378;&#26041;&#27861;&#26080;&#27861;&#21516;&#26102;&#35299;&#20915;&#19981;&#21516;KG&#26469;&#28304;&#20043;&#38388;&#30340;&#27867;&#21270;&#21644;&#24320;&#25918;&#24335;&#31572;&#26696;&#38382;&#39064;&#30340;&#22686;&#24378;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#20266;&#22270;&#29983;&#25104;&#21644;&#21407;&#23376;&#32423;&#30693;&#35782;&#39564;&#35777;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;&#20266;&#22270;&#29983;&#25104;&#26469;&#23454;&#29616;&#22312;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#29615;&#22659;&#20013;&#20351;&#29992;KG&#22686;&#24378;LLM&#12290;&#21407;&#23376;&#32423;&#30693;&#35782;&#39564;&#35777;&#21033;&#29992;&#21407;&#23376;&#32423;&#30693;&#35782;&#26597;&#35810;&#21644;&#39564;&#35777;&#26469;&#23454;&#29616;&#22312;&#19981;&#21516;KG&#26469;&#28304;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#19982;&#22522;&#20934;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;ROUGE-L&#20998;&#25968;&#19978;&#33267;&#23569;&#25552;&#21319;&#20102;11.5&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09911v1 Announce Type: cross  Abstract: Mitigating the hallucinations of Large Language Models (LLMs) and enhancing them is a crucial task. Although some existing methods employ model self-enhancement techniques, they fall short of effectively addressing unknown factual hallucinations. Using Knowledge Graph (KG) enhancement approaches fails to address the generalization across different KG sources and the enhancement of open-ended answer questions simultaneously. To tackle these limitations, there is a framework that combines Pseudo-Graph Generation and Atomic Knowledge Verification proposed. The enhancement of LLM using KG in an open-ended question-answering setting is implemented by leveraging the Pseudo-Graph Generation. Atomic Knowledge Verification utilizes atomic-level knowledge querying and verification to achieve generalizability under different KG sources. Compared to the baseline, this approach yields a minimum improvement of 11.5 in the ROUGE-L score for open-ende
&lt;/p&gt;</description></item><item><title>DE-COP&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#29256;&#26435;&#20869;&#23481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#39033;&#36873;&#25321;&#25506;&#27979;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#27169;&#22411;&#35757;&#32451;&#25991;&#26412;&#20013;&#21487;&#33021;&#21253;&#21547;&#30340;&#29256;&#26435;&#20869;&#23481;&#12290;&#35813;&#26041;&#27861;&#22312;&#27169;&#22411;&#30340;&#36923;&#36753;&#21487;&#29992;&#26102;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;9.6%&#30340;&#26816;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#23436;&#20840;&#40657;&#30418;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;72%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.09910</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#26816;&#27979;&#29256;&#26435;&#20869;&#23481;&#30340;&#26041;&#27861;&#65306;DE-COP
&lt;/p&gt;
&lt;p&gt;
DE-COP: Detecting Copyrighted Content in Language Models Training Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09910
&lt;/p&gt;
&lt;p&gt;
DE-COP&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#29256;&#26435;&#20869;&#23481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#39033;&#36873;&#25321;&#25506;&#27979;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#27169;&#22411;&#35757;&#32451;&#25991;&#26412;&#20013;&#21487;&#33021;&#21253;&#21547;&#30340;&#29256;&#26435;&#20869;&#23481;&#12290;&#35813;&#26041;&#27861;&#22312;&#27169;&#22411;&#30340;&#36923;&#36753;&#21487;&#29992;&#26102;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;9.6%&#30340;&#26816;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#23436;&#20840;&#40657;&#30418;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;72%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32771;&#34385;&#21040;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#26159;&#20445;&#23494;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22914;&#20309;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#26159;&#21542;&#20351;&#29992;&#20102;&#29256;&#26435;&#20869;&#23481;&#65311;&#25105;&#20204;&#30340;&#21160;&#26426;&#26159;&#22522;&#20110;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#24456;&#21487;&#33021;&#33021;&#22815;&#35782;&#21035;&#20986;&#20854;&#35757;&#32451;&#25991;&#26412;&#20013;&#30340;&#29420;&#25991;&#25688;&#24405;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DE-COP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#26159;&#21542;&#22312;&#35757;&#32451;&#20013;&#21253;&#21547;&#20102;&#19968;&#27573;&#29256;&#26435;&#20869;&#23481;&#12290;DE-COP&#30340;&#26680;&#24515;&#26041;&#27861;&#26159;&#36890;&#36807;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25506;&#27979;&#65292;&#36873;&#25321;&#39033;&#21253;&#25324;&#29420;&#25991;&#26412;&#21644;&#23427;&#20204;&#30340;&#37322;&#20041;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;BookTection&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#22312;&#27169;&#22411;&#35757;&#32451;&#25130;&#27490;&#26085;&#26399;&#20043;&#21069;&#21644;&#20043;&#21518;&#20986;&#29256;&#30340;165&#26412;&#20070;&#30340;&#25688;&#24405;&#20197;&#21450;&#23427;&#20204;&#30340;&#37322;&#20041;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DE-COP&#22312;&#27169;&#22411;&#30340;&#36923;&#36753;&#21487;&#29992;&#26102;&#65292;&#26816;&#27979;&#24615;&#33021;&#65288;AUC&#65289;&#36229;&#36807;&#20043;&#21069;&#30340;&#26368;&#20339;&#26041;&#27861;9.6%&#12290;&#27492;&#22806;&#65292;DE-COP&#22312;&#23436;&#20840;&#40657;&#30418;&#27169;&#22411;&#19978;&#26816;&#27979;&#21487;&#30097;&#20070;&#31821;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#36798;&#21040;72%&#65292;&#32780;&#20043;&#21069;&#30340;&#26041;&#27861;&#21482;&#26377;$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09910v1 Announce Type: new  Abstract: How can we detect if copyrighted content was used in the training process of a language model, considering that the training data is typically undisclosed? We are motivated by the premise that a language model is likely to identify verbatim excerpts from its training text. We propose DE-COP, a method to determine whether a piece of copyrighted content was included in training. DE-COP's core approach is to probe an LLM with multiple-choice questions, whose options include both verbatim text and their paraphrases. We construct BookTection, a benchmark with excerpts from 165 books published prior and subsequent to a model's training cutoff, along with their paraphrases. Our experiments show that DE-COP surpasses the prior best method by 9.6% in detection performance (AUC) on models with logits available. Moreover, DE-COP also achieves an average accuracy of 72% for detecting suspect books on fully black-box models where prior methods give $
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#29983;&#25104;&#34920;&#31034;&#25351;&#20196;&#35843;&#25972;&#65288;GRIT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#21306;&#20998;&#29983;&#25104;&#21644;&#23884;&#20837;&#20219;&#21153;&#65292;&#35757;&#32451;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#22788;&#29702;&#36825;&#20004;&#31181;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;GritLM 7B&#22312;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#26368;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#22312;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#25193;&#22823;&#35268;&#27169;&#65292;&#25105;&#20204;&#30340;GritLM 8x7B&#25104;&#20026;&#26368;&#20339;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20043;&#19968;&#65292;&#21516;&#26102;&#20173;&#28982;&#26159;&#26368;&#22909;&#30340;&#23884;&#20837;&#27169;&#22411;&#20043;&#19968;&#12290;GRIT&#30340;&#32479;&#19968;&#20063;&#22823;&#22823;&#25552;&#39640;&#20102;RAG&#22312;&#38271;&#25991;&#26723;&#19978;&#30340;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.09906</link><description>&lt;p&gt;
&#29983;&#25104;&#34920;&#31034;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Generative Representational Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#29983;&#25104;&#34920;&#31034;&#25351;&#20196;&#35843;&#25972;&#65288;GRIT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#21306;&#20998;&#29983;&#25104;&#21644;&#23884;&#20837;&#20219;&#21153;&#65292;&#35757;&#32451;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#22788;&#29702;&#36825;&#20004;&#31181;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;GritLM 7B&#22312;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#26368;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#22312;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#25193;&#22823;&#35268;&#27169;&#65292;&#25105;&#20204;&#30340;GritLM 8x7B&#25104;&#20026;&#26368;&#20339;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20043;&#19968;&#65292;&#21516;&#26102;&#20173;&#28982;&#26159;&#26368;&#22909;&#30340;&#23884;&#20837;&#27169;&#22411;&#20043;&#19968;&#12290;GRIT&#30340;&#32479;&#19968;&#20063;&#22823;&#22823;&#25552;&#39640;&#20102;RAG&#22312;&#38271;&#25991;&#26723;&#19978;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25152;&#26377;&#22522;&#20110;&#25991;&#26412;&#30340;&#35821;&#35328;&#38382;&#39064;&#37117;&#21487;&#20197;&#24402;&#32467;&#20026;&#29983;&#25104;&#25110;&#23884;&#20837;&#12290;&#30446;&#21069;&#30340;&#27169;&#22411;&#21482;&#33021;&#22312;&#20854;&#20013;&#19968;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#29983;&#25104;&#34920;&#31034;&#25351;&#20196;&#35843;&#25972;&#65288;GRIT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#26469;&#21306;&#20998;&#29983;&#25104;&#21644;&#23884;&#20837;&#20219;&#21153;&#65292;&#20174;&#32780;&#35757;&#32451;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#22788;&#29702;&#36825;&#20004;&#31181;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#24320;&#25918;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;GritLM 7B&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;&#65288;MTEB&#65289;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#22312;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#36229;&#36807;&#20102;&#21516;&#31561;&#35268;&#27169;&#30340;&#25152;&#26377;&#27169;&#22411;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#25193;&#22823;&#35268;&#27169;&#65292;GritLM 8x7B&#22312;&#23581;&#35797;&#30340;&#25152;&#26377;&#24320;&#25918;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#21516;&#26102;&#20173;&#28982;&#26159;&#26368;&#22909;&#30340;&#23884;&#20837;&#27169;&#22411;&#20043;&#19968;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;GRIT&#21487;&#20197;&#19982;&#20165;&#22312;&#29983;&#25104;&#25110;&#23884;&#20837;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#22312;&#19981;&#25439;&#22833;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#32479;&#19968;&#20004;&#32773;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#36890;&#36807;GRIT&#30340;&#32479;&#19968;&#21487;&#20197;&#23558;RAG&#65288;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65289;&#22312;&#38271;&#25991;&#26723;&#19978;&#30340;&#36895;&#24230;&#25552;&#39640;60%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09906v1 Announce Type: cross  Abstract: All text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by &gt; 60% for long documents, 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32437;&#21521;&#30740;&#31350;&#35843;&#26597;&#20102;&#29983;&#25104;&#24335;AI&#24037;&#20316;&#27969;&#31243;&#30340;&#23454;&#29992;&#24615;&#21644;&#23450;&#21046;&#21270;&#31243;&#24230;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#29087;&#24713;&#21270;&#38454;&#27573;&#21518;&#65292;&#29992;&#25143;&#24863;&#30693;&#21040;&#30340;&#31995;&#32479;&#25928;&#29992;&#25552;&#39640;&#20102;&#12290;</title><link>https://arxiv.org/abs/2402.09894</link><description>&lt;p&gt;
&#19981;&#20165;&#20165;&#26159;&#26032;&#39062;&#24615;&#65306;&#20851;&#20110;AI&#24037;&#20316;&#27969;&#31243;&#30340;&#25928;&#29992;&#21644;&#23450;&#21046;&#21270;&#30340;&#32437;&#21521;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Not Just Novelty: A Longitudinal Study on Utility and Customization of AI Workflows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09894
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32437;&#21521;&#30740;&#31350;&#35843;&#26597;&#20102;&#29983;&#25104;&#24335;AI&#24037;&#20316;&#27969;&#31243;&#30340;&#23454;&#29992;&#24615;&#21644;&#23450;&#21046;&#21270;&#31243;&#24230;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#29087;&#24713;&#21270;&#38454;&#27573;&#21518;&#65292;&#29992;&#25143;&#24863;&#30693;&#21040;&#30340;&#31995;&#32479;&#25928;&#29992;&#25552;&#39640;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;AI&#20026;&#20154;&#20204;&#22312;&#26085;&#24120;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#26032;&#39062;&#32780;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#26377;&#35768;&#22810;AI&#24037;&#20316;&#27969;&#31243;&#36890;&#36807;&#23558;AI&#36755;&#20986;&#19982;&#20154;&#31867;&#20114;&#21160;&#30456;&#32467;&#21512;&#26469;&#35299;&#20915;&#30495;&#23454;&#32780;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;AI&#20855;&#26377;&#26080;&#21487;&#21542;&#35748;&#30340;&#21560;&#24341;&#21147;&#65292;&#20294;&#22312;&#26032;&#40092;&#24863;&#28040;&#22833;&#21518;&#65292;&#29983;&#25104;&#24335;AI&#24037;&#20316;&#27969;&#31243;&#30340;&#23454;&#29992;&#24615;&#22914;&#20309;&#20173;&#28982;&#19981;&#30830;&#23450;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#29983;&#25104;&#24335;AI&#26500;&#24314;&#30340;&#24037;&#20855;&#20855;&#26377;&#20010;&#24615;&#21270;&#21644;&#24555;&#36895;&#36866;&#24212;&#30340;&#28508;&#21147;&#65292;&#20294;&#29992;&#25143;&#26159;&#21542;&#20805;&#20998;&#21033;&#29992;&#20102;&#20010;&#24615;&#21270;&#30340;&#21487;&#33021;&#24615;&#21602;&#65311;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20026;&#26399;&#19977;&#21608;&#30340;&#32437;&#21521;&#30740;&#31350;&#65292;&#20849;&#26377;12&#20010;&#29992;&#25143;&#65292;&#26088;&#22312;&#20102;&#35299;&#31185;&#23398;&#20256;&#25773;&#20013;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#30340;&#29087;&#24713;&#24230;&#21644;&#23450;&#21046;&#21270;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#29087;&#24713;&#21270;&#38454;&#27573;&#25345;&#32493;&#20102;4.3&#20010;&#20250;&#35805;&#65292;&#29992;&#25143;&#22312;&#36825;&#20010;&#38454;&#27573;&#25506;&#32034;&#24037;&#20316;&#27969;&#31243;&#30340;&#21151;&#33021;&#20197;&#21450;&#20182;&#20204;&#21457;&#29616;&#21738;&#20123;&#26041;&#38754;&#26377;&#29992;&#12290;&#22312;&#29087;&#24713;&#21270;&#21518;&#65292;&#31995;&#32479;&#30340;&#24863;&#30693;&#25928;&#29992;&#35780;&#20998;&#39640;&#20110;&#20043;&#21069;&#65292;&#34920;&#26126;&#20102;&#24863;&#30693;&#25928;&#29992;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09894v1 Announce Type: cross  Abstract: Generative AI brings novel and impressive abilities to help people in everyday tasks. There are many AI workflows that solve real and complex problems by chaining AI outputs together with human interaction. Although there is an undeniable lure of AI, it's uncertain how useful generative AI workflows are after the novelty wears off. Additionally, tools built with generative AI have the potential to be personalized and adapted quickly and easily, but do users take advantage of the potential to customize? We conducted a three-week longitudinal study with 12 users to understand the familiarization and customization of generative AI tools for science communication. Our study revealed that the familiarization phase lasts for 4.3 sessions, where users explore the capabilities of the workflow and which aspects they find useful. After familiarization, the perceived utility of the system is rated higher than before, indicating that the perceived
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#21644;&#22686;&#24378;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#20266;&#35013;&#24615;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#23545;&#19977;&#31181;Transformer&#37197;&#32622;&#36827;&#34892;&#35780;&#20272;&#21518;&#21457;&#29616;&#65292;&#20165;&#32534;&#30721;&#22120;&#27169;&#22411;&#22312;&#20882;&#29359;&#24615;&#35821;&#35328;&#26816;&#27979;&#21644;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26368;&#39640;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2402.09874</link><description>&lt;p&gt;
&#20266;&#35013;&#23601;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#65306;&#35780;&#20272;&#21644;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#23545;&#20266;&#35013;&#24615;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Camouflage is all you need: Evaluating and Enhancing Language Model Robustness Against Camouflage Adversarial Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09874
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#21644;&#22686;&#24378;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#20266;&#35013;&#24615;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#23545;&#19977;&#31181;Transformer&#37197;&#32622;&#36827;&#34892;&#35780;&#20272;&#21518;&#21457;&#29616;&#65292;&#20165;&#32534;&#30721;&#22120;&#27169;&#22411;&#22312;&#20882;&#29359;&#24615;&#35821;&#35328;&#26816;&#27979;&#21644;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26368;&#39640;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#25915;&#20987;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#23545;Transformer-based&#27169;&#22411;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#33030;&#24369;&#24615;&#35780;&#20272;&#21644;&#40065;&#26834;&#24615;&#22686;&#24378;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25506;&#32034;&#12290;&#22312;&#35780;&#20272;&#38454;&#27573;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21253;&#21547;&#20882;&#29359;&#24615;&#35821;&#35328;&#21644;&#34394;&#20551;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#19977;&#20010;Transformer&#37197;&#32622;&#65288;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#12289;&#20165;&#32534;&#30721;&#22120;&#21644;&#20165;&#35299;&#30721;&#22120;&#65289;&#23545;&#19981;&#26029;&#21319;&#32423;&#30340;&#22797;&#26434;&#24615;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#25935;&#24863;&#24615;&#12290;&#20165;&#32534;&#30721;&#22120;&#27169;&#22411;&#22312;&#20882;&#29359;&#24615;&#35821;&#35328;&#26816;&#27979;&#21644;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#20219;&#21153;&#20013;&#20998;&#21035;&#34920;&#29616;&#20986;14%&#21644;21%&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20165;&#35299;&#30721;&#22120;&#27169;&#22411;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#37117;&#20986;&#29616;&#20102;16%&#30340;&#38477;&#20302;&#65292;&#32780;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#22312;&#30456;&#24212;&#20219;&#21153;&#20013;&#30340;&#26368;&#22823;&#24615;&#33021;&#19979;&#38477;&#20026;14%&#21644;26%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09874v1 Announce Type: new  Abstract: Adversarial attacks represent a substantial challenge in Natural Language Processing (NLP). This study undertakes a systematic exploration of this challenge in two distinct phases: vulnerability evaluation and resilience enhancement of Transformer-based models under adversarial attacks.   In the evaluation phase, we assess the susceptibility of three Transformer configurations, encoder-decoder, encoder-only, and decoder-only setups, to adversarial attacks of escalating complexity across datasets containing offensive language and misinformation. Encoder-only models manifest a 14% and 21% performance drop in offensive language detection and misinformation detection tasks, respectively. Decoder-only models register a 16% decrease in both tasks, while encoder-decoder models exhibit a maximum performance drop of 14% and 26% in the respective tasks.   The resilience-enhancement phase employs adversarial training, integrating pre-camouflaged an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#20351;&#29992;&#24067;&#23616;&#22686;&#24378;&#26469;&#20351;&#29992;&#32431;&#25991;&#26412;LLM&#36827;&#34892;&#25991;&#26723;&#29305;&#23450;&#20219;&#21153;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09841</link><description>&lt;p&gt;
LAPDoc&#65306;&#38754;&#21521;&#25991;&#26723;&#30340;&#24067;&#23616;&#24863;&#30693;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
LAPDoc: Layout-Aware Prompting for Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#20351;&#29992;&#24067;&#23616;&#22686;&#24378;&#26469;&#20351;&#29992;&#32431;&#25991;&#26412;LLM&#36827;&#34892;&#25991;&#26723;&#29305;&#23450;&#20219;&#21153;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#22823;&#37327;&#32431;&#25991;&#26412;&#25968;&#25454;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21253;&#25324;&#25991;&#26723;&#29305;&#23450;&#20219;&#21153;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#35757;&#32451;&#38024;&#23545;&#25991;&#26723;&#29702;&#35299;&#30340;&#22810;&#27169;&#24577;&#21464;&#21387;&#22120;&#20307;&#31995;&#32467;&#26500;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#23558;&#25991;&#26412;&#36755;&#20837;&#19982;&#30456;&#24212;&#30340;&#25991;&#26723;&#24067;&#23616;&#34701;&#21512;&#12290;&#36825;&#38656;&#35201;&#21333;&#29420;&#30340;&#24494;&#35843;&#27493;&#39588;&#65292;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#30446;&#21069;&#65292;&#23578;&#27809;&#26377;&#20855;&#26377;&#19982;LLM&#30456;&#24403;&#27867;&#21270;&#33021;&#21147;&#30340;&#25991;&#26723;&#21464;&#21387;&#22120;&#21487;&#29992;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22312;&#25991;&#26723;&#29702;&#35299;&#20219;&#21153;&#20013;&#24212;&#35813;&#36873;&#25321;&#21738;&#31181;&#31867;&#22411;&#30340;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#24067;&#23616;&#22686;&#24378;&#26469;&#35843;&#26597;&#20351;&#29992;&#32431;&#25991;&#26412;LLM&#29992;&#20110;&#25991;&#26723;&#29305;&#23450;&#20219;&#21153;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#28155;&#21152;&#20462;&#25913;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#32431;&#25991;&#26412;LLM&#25552;&#31034;&#20013;&#28155;&#21152;&#24067;&#23616;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09841v1 Announce Type: new  Abstract: Recent advances in training large language models (LLMs) using massive amounts of solely textual data lead to strong generalization across many domains and tasks, including document-specific tasks. Opposed to that there is a trend to train multi-modal transformer architectures tailored for document understanding that are designed specifically to fuse textual inputs with the corresponding document layout. This involves a separate fine-tuning step for which additional training data is required. At present, no document transformers with comparable generalization to LLMs are available That raises the question which type of model is to be preferred for document understanding tasks. In this paper we investigate the possibility to use purely text-based LLMs for document-specific tasks by using layout enrichment. We explore drop-in modifications and rule-based methods to enrich purely textual LLM prompts with layout information. In our experimen
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#26631;&#35760;&#30340;&#34920;&#38754;&#20449;&#24687;&#30340;&#30693;&#35782;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#20855;&#22791;&#26377;&#20851;&#26631;&#35760;&#38271;&#24230;&#21644;&#23376;&#23383;&#31526;&#20018;&#30340;&#30693;&#35782;&#65292;&#20294;&#23545;&#26631;&#35760;&#32467;&#26500;&#30340;&#30693;&#35782;&#26377;&#38480;&#65292;&#35299;&#30721;&#22120;&#26041;&#38754;&#23384;&#22312;&#29942;&#39048;&#12290;</title><link>https://arxiv.org/abs/2402.09808</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#26631;&#35760;&#30340;&#34920;&#38754;&#20449;&#24687;&#30340;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Knowledge of Pretrained Language Models on Surface Information of Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09808
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#26631;&#35760;&#30340;&#34920;&#38754;&#20449;&#24687;&#30340;&#30693;&#35782;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#20855;&#22791;&#26377;&#20851;&#26631;&#35760;&#38271;&#24230;&#21644;&#23376;&#23383;&#31526;&#20018;&#30340;&#30693;&#35782;&#65292;&#20294;&#23545;&#26631;&#35760;&#32467;&#26500;&#30340;&#30693;&#35782;&#26377;&#38480;&#65292;&#35299;&#30721;&#22120;&#26041;&#38754;&#23384;&#22312;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#20851;&#20110;&#26631;&#35760;&#34920;&#38754;&#20449;&#24687;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#20174;&#26631;&#35760;&#38271;&#24230;&#12289;&#23376;&#23383;&#31526;&#20018;&#21644;&#26631;&#35760;&#32467;&#26500;&#30340;&#35282;&#24230;&#26816;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#33719;&#21462;&#30340;&#35789;&#35821;&#25110;&#23376;&#35789;&#23884;&#20837;&#20013;&#20445;&#23384;&#30340;&#34920;&#38754;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#27169;&#22411;&#29983;&#25104;&#26631;&#35760;&#34920;&#38754;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#20102;12&#20010;&#20027;&#35201;&#22312;&#33521;&#35821;&#21644;&#26085;&#35821;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#26631;&#35760;&#38271;&#24230;&#21644;&#23376;&#23383;&#31526;&#20018;&#20855;&#26377;&#30693;&#35782;&#65292;&#20294;&#23545;&#20110;&#26631;&#35760;&#32467;&#26500;&#21017;&#27809;&#26377;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26377;&#25928;&#21033;&#29992;&#33719;&#21462;&#30340;&#30693;&#35782;&#26041;&#38754;&#65292;&#35299;&#30721;&#22120;&#26041;&#38754;&#23384;&#22312;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09808v1 Announce Type: new  Abstract: Do pretrained language models have knowledge regarding the surface information of tokens? We examined the surface information stored in word or subword embeddings acquired by pretrained language models from the perspectives of token length, substrings, and token constitution. Additionally, we evaluated the ability of models to generate knowledge regarding token surfaces. We focused on 12 pretrained language models that were mainly trained on English and Japanese corpora. Experimental results demonstrate that pretrained language models have knowledge regarding token length and substrings but not token constitution. Additionally, the results imply that there is a bottleneck on the decoder side in terms of effectively utilizing acquired knowledge.
&lt;/p&gt;</description></item><item><title>EFUF&#26159;&#19968;&#31181;&#39640;&#25928;&#31934;&#32454;&#21270;&#21435;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#28040;&#38500;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;&#65292;&#24182;&#19981;&#38656;&#35201;&#20154;&#24037;&#27880;&#37322;&#37197;&#23545;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.09801</link><description>&lt;p&gt;
EFUF: &#39640;&#25928;&#31934;&#32454;&#21270;&#21435;&#23398;&#20064;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#20943;&#36731;&#24187;&#20687;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EFUF: Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09801
&lt;/p&gt;
&lt;p&gt;
EFUF&#26159;&#19968;&#31181;&#39640;&#25928;&#31934;&#32454;&#21270;&#21435;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#28040;&#38500;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;&#65292;&#24182;&#19981;&#38656;&#35201;&#20154;&#24037;&#27880;&#37322;&#37197;&#23545;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#36817;&#24180;&#26469;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#20173;&#20250;&#29983;&#25104;&#21253;&#21547;&#22270;&#20687;&#20013;&#19981;&#23384;&#22312;&#30340;&#29289;&#20307;&#30340;&#25551;&#36848;&#65292;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#29289;&#20307;&#24187;&#35273;&#12290;&#20026;&#20102;&#28040;&#38500;&#24187;&#35273;&#65292;&#29616;&#26377;&#26041;&#27861;&#25163;&#21160;&#27880;&#37322;&#21253;&#21547;&#21644;&#19981;&#21253;&#21547;&#24187;&#35273;&#30340;&#37197;&#23545;&#21709;&#24212;&#65292;&#24182;&#37319;&#29992;&#21508;&#31181;&#23545;&#40784;&#31639;&#27861;&#26469;&#25552;&#39640;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#23545;&#40784;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19981;&#20165;&#22312;&#24494;&#35843;&#38454;&#27573;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#36824;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#26469;&#26500;&#24314;&#23545;&#40784;&#31639;&#27861;&#25152;&#38656;&#30340;&#37197;&#23545;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#21435;&#23398;&#20064;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#31934;&#32454;&#21270;&#21435;&#23398;&#20064;&#26694;&#26550;&#65288;EFUF&#65289;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#37197;&#23545;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#28040;&#38500;&#24187;&#35273;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25345;&#32493;&#20943;&#23569;&#24187;&#35273;&#21516;&#26102;&#20445;&#30041;&#20934;&#30830;&#30340;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09801v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) have attracted increasing attention in the past few years, but they may still generate descriptions that include objects not present in the corresponding images, a phenomenon known as object hallucination. To eliminate hallucinations, existing methods manually annotate paired responses with and without hallucinations, and then employ various alignment algorithms to improve the alignment capability between images and text. However, they not only demand considerable computation resources during the finetuning stage but also require expensive human annotation to construct paired data needed by the alignment algorithms. To address these issues, we borrow the idea of unlearning and propose an efficient fine-grained unlearning framework (EFUF), which can eliminate hallucinations without the need for paired data. Extensive experiments show that our method consistently reduces hallucinations while preserv
&lt;/p&gt;</description></item><item><title>NutePrune&#26159;&#19968;&#31181;&#39640;&#25928;&#36880;&#28176;&#21098;&#26525;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#36733;&#19968;&#20010;&#23436;&#25972;&#27169;&#22411;&#24182;&#23558;&#20854;&#19982;&#25513;&#30721;&#21644;LoRA&#27169;&#22359;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#39640;&#25928;&#30340;&#32467;&#26500;&#21098;&#26525;&#12290;</title><link>https://arxiv.org/abs/2402.09773</link><description>&lt;p&gt;
NutePrune: &#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36880;&#28176;&#21098;&#26525;&#26041;&#27861;&#65292;&#22810;&#20010;&#25945;&#24072;&#21442;&#19982;
&lt;/p&gt;
&lt;p&gt;
NutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09773
&lt;/p&gt;
&lt;p&gt;
NutePrune&#26159;&#19968;&#31181;&#39640;&#25928;&#36880;&#28176;&#21098;&#26525;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#36733;&#19968;&#20010;&#23436;&#25972;&#27169;&#22411;&#24182;&#23558;&#20854;&#19982;&#25513;&#30721;&#21644;LoRA&#27169;&#22359;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#39640;&#25928;&#30340;&#32467;&#26500;&#21098;&#26525;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24040;&#22823;&#23610;&#23544;&#32473;&#36164;&#28304;&#21463;&#38480;&#30828;&#20214;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#37096;&#32626;&#25361;&#25112;&#12290;&#32467;&#26500;&#21098;&#26525;&#20026;&#21387;&#32553;LLMs&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#24335;&#65292;&#20174;&#32780;&#38477;&#20302;&#23384;&#20648;&#25104;&#26412;&#65292;&#25552;&#21319;&#25512;&#26029;&#36895;&#24230;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#21033;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#25928;&#29575;&#21644;&#36164;&#28304;&#25928;&#29575;&#30340;&#32467;&#26500;&#21098;&#26525;&#26041;&#27861;&#65292;&#20197;&#33719;&#21462;&#26356;&#23567;&#20294;&#20381;&#28982;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;&#30693;&#35782;&#33976;&#39311;&#38750;&#24120;&#36866;&#21512;&#21098;&#26525;&#65292;&#22240;&#20026;&#23436;&#25972;&#30340;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#21098;&#26525;&#21518;&#30340;&#23398;&#29983;&#30340;&#20248;&#31168;&#25945;&#24072;&#12290;&#28982;&#32780;&#65292;&#22312;LLMs&#30340;&#32972;&#26223;&#19979;&#65292;&#30001;&#20110;&#20869;&#23384;&#38480;&#21046;&#65292;&#36825;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36880;&#28176;&#21098;&#26525;&#26041;&#27861;&#65288;NutePrune&#65289;&#12290;NutePrune&#36890;&#36807;&#21482;&#21152;&#36733;&#19968;&#20010;&#23436;&#25972;&#27169;&#22411;&#24182;&#23558;&#20854;&#19982;&#21508;&#31181;&#25513;&#30721;&#21644;LoRA&#27169;&#22359;&#38598;&#25104;&#65292;&#22312;&#25945;&#24072;&#21644;&#23398;&#29983;&#35282;&#33394;&#20043;&#38388;&#26080;&#32541;&#20999;&#25442;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#36807;&#22810;&#30340;&#20869;&#23384;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09773v1 Announce Type: new  Abstract: The considerable size of Large Language Models (LLMs) presents notable deployment challenges, particularly on resource-constrained hardware. Structured pruning, offers an effective means to compress LLMs, thereby reducing storage costs and enhancing inference speed for more efficient utilization. In this work, we study data-efficient and resource-efficient structure pruning methods to obtain smaller yet still powerful models. Knowledge Distillation is well-suited for pruning, as the intact model can serve as an excellent teacher for pruned students. However, it becomes challenging in the context of LLMs due to memory constraints. To address this, we propose an efficient progressive Numerous-teacher pruning method (NutePrune). NutePrune mitigates excessive memory costs by loading only one intact model and integrating it with various masks and LoRA modules, enabling it to seamlessly switch between teacher and student roles. This approach a
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65288;RAG&#65289;&#30340;&#26080;&#22359;&#35821;&#22659;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#32469;&#36807;&#25991;&#26412;&#20999;&#20998;&#30340;&#36807;&#31243;&#65292;&#21033;&#29992;&#32534;&#30721;&#38544;&#34255;&#29366;&#24577;&#36827;&#34892;&#20934;&#30830;&#22320;&#35821;&#22659;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#25991;&#26412;&#35821;&#20041;&#36830;&#36143;&#24615;&#30772;&#22351;&#21644;&#35777;&#25454;&#26816;&#32034;&#20013;&#30340;&#22122;&#22768;&#21644;&#19981;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09760</link><description>&lt;p&gt;
&#26080;&#22359;&#35821;&#22659;&#26816;&#32034;&#30340;&#35821;&#35328;&#27169;&#22411; grounding
&lt;/p&gt;
&lt;p&gt;
Grounding Language Model with Chunking-Free In-Context Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09760
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65288;RAG&#65289;&#30340;&#26080;&#22359;&#35821;&#22659;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#32469;&#36807;&#25991;&#26412;&#20999;&#20998;&#30340;&#36807;&#31243;&#65292;&#21033;&#29992;&#32534;&#30721;&#38544;&#34255;&#29366;&#24577;&#36827;&#34892;&#20934;&#30830;&#22320;&#35821;&#22659;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#25991;&#26412;&#35821;&#20041;&#36830;&#36143;&#24615;&#30772;&#22351;&#21644;&#35777;&#25454;&#26816;&#32034;&#20013;&#30340;&#22122;&#22768;&#21644;&#19981;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#30340;&#26080;&#22359;&#35821;&#22659;&#65288;CFIC&#65289;&#26816;&#32034;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;RAG&#31995;&#32479;&#22312;&#20351;&#29992;&#31934;&#30830;&#35777;&#25454;&#25991;&#26412;&#36827;&#34892; grounding &#26102;&#24448;&#24448;&#38754;&#20020;&#22788;&#29702;&#20887;&#38271;&#25991;&#26723;&#21644;&#36807;&#28388;&#26080;&#20851;&#20869;&#23481;&#30340;&#25361;&#25112;&#12290;&#24120;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#25991;&#26723;&#20999;&#20998;&#21644;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#20197;&#22788;&#29702;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#37117;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#30772;&#22351;&#20102;&#25991;&#26412;&#30340;&#35821;&#20041;&#36830;&#36143;&#24615;&#65292;&#35201;&#20040;&#26410;&#33021;&#26377;&#25928;&#35299;&#20915;&#35777;&#25454;&#26816;&#32034;&#20013;&#30340;&#22122;&#22768;&#21644;&#19981;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;CFIC&#36890;&#36807;&#32469;&#36807;&#20256;&#32479;&#30340;&#20999;&#20998;&#36807;&#31243;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#23427;&#21033;&#29992;&#25991;&#26723;&#30340;&#32534;&#30721;&#38544;&#34255;&#29366;&#24577;&#36827;&#34892;&#35821;&#22659;&#26816;&#32034;&#65292;&#22312;&#23545;&#29992;&#25143;&#26597;&#35810;&#36827;&#34892;&#33258;&#22238;&#24402;&#35299;&#30721;&#26102;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#25152;&#38656;&#30340;&#20855;&#20307;&#35777;&#25454;&#25991;&#26412;&#65292;&#28040;&#38500;&#20102;&#20999;&#20998;&#30340;&#38656;&#27714;&#12290;CFIC &#36827;&#19968;&#27493;&#12290;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09760v1 Announce Type: cross  Abstract: This paper presents a novel Chunking-Free In-Context (CFIC) retrieval approach, specifically tailored for Retrieval-Augmented Generation (RAG) systems. Traditional RAG systems often struggle with grounding responses using precise evidence text due to the challenges of processing lengthy documents and filtering out irrelevant content. Commonly employed solutions, such as document chunking and adapting language models to handle longer contexts, have their limitations. These methods either disrupt the semantic coherence of the text or fail to effectively address the issues of noise and inaccuracy in evidence retrieval.   CFIC addresses these challenges by circumventing the conventional chunking process. It utilizes the encoded hidden states of documents for in-context retrieval, employing auto-aggressive decoding to accurately identify the specific evidence text required for user queries, eliminating the need for chunking. CFIC is further
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#39640;&#25928;&#30340;&#35821;&#35328;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#25104;&#21151;&#23558;&#22522;&#30784;&#33521;&#25991;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#29983;&#25104;&#27874;&#20848;&#25991;&#65292;&#24182;&#22312;&#22256;&#24785;&#24230;&#21644;&#20219;&#21153;&#34920;&#29616;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20026;&#21521;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#26032;&#35821;&#35328;&#24320;&#36767;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2402.09759</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#35821;&#35328;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#65306;&#25193;&#23637;&#26368;&#26032;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#27874;&#20848;&#35821;
&lt;/p&gt;
&lt;p&gt;
Efficient Language Adaptive Pre-training: Extending State-of-the-Art Large Language Models for Polish
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#39640;&#25928;&#30340;&#35821;&#35328;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#25104;&#21151;&#23558;&#22522;&#30784;&#33521;&#25991;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#29983;&#25104;&#27874;&#20848;&#25991;&#65292;&#24182;&#22312;&#22256;&#24785;&#24230;&#21644;&#20219;&#21153;&#34920;&#29616;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20026;&#21521;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#26032;&#35821;&#35328;&#24320;&#36767;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#22522;&#30784;&#33521;&#25991;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24494;&#35843;&#20026;&#29983;&#25104;&#27874;&#20848;&#25991;&#30340;&#28508;&#21147;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#23545;3.11 GB&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#36827;&#34892;&#35821;&#35328;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#65288;LAPT&#65289;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;2.76&#20159;&#20010;&#27874;&#20848;&#35821;tokens&#12290;LAPT&#21518;&#36827;&#34892;&#20102;&#39069;&#22806;&#30340;&#24494;&#35843;&#65292;&#26088;&#22312;&#35299;&#20915;&#20061;&#20010;KLEJ&#25361;&#25112;&#12290;&#25105;&#20204;&#35757;&#32451;&#30340;&#27169;&#22411;Curie-7B-v1&#19981;&#20165;&#22312;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#27874;&#20848;&#27169;&#22411;&#20013;&#20855;&#26377;&#26368;&#20302;&#30340;&#22256;&#24785;&#24230;3.02&#65292;&#32780;&#19988;&#22312;8&#20010;&#20219;&#21153;&#20013;&#19982;&#26368;&#22909;&#30340;&#27874;&#20848;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#19981;&#21040;2%&#12290;Curie-7B-v1&#20165;&#20351;&#29992;&#20856;&#22411;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#32422;2-3%&#26469;&#23398;&#20064;&#27874;&#20848;&#35821;&#12290;LAPT&#22312;&#19981;&#21040;&#20116;&#22825;&#30340;&#26102;&#38388;&#20869;&#20351;&#29992;&#26222;&#36890;GPU&#23436;&#25104;&#65292;&#20984;&#26174;&#20102;&#35813;&#26041;&#27861;&#30340;&#39640;&#25928;&#24615;&#12290;&#27169;&#22411;&#22312;&#27874;&#20848;&#35821;&#26041;&#38754;&#30340;&#29087;&#32451;&#24230;&#26174;&#33879;&#25552;&#39640;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#23558;&#26032;&#35821;&#35328;&#28155;&#21152;&#21040;&#29616;&#26377;LLMs&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09759v1 Announce Type: cross  Abstract: This study explores the potential of fine-tuning foundational English Large Language Models (LLMs) for generating Polish text. The first step involves Language Adaptive Pre-training (LAPT) on a high-quality dataset of 3.11 GB, consisting of 276 million Polish tokens. The LAPT is followed by additional fine-tuning aimed at solving nine KLEJ challenges. Our trained model Curie-7B-v1 not only generates Polish text with the lowest perplexity of 3.02 among decoder-based Polish models but also closely rivals the performance of the best Polish encoder-decoder models with a less than 2% gap on 8 out of 9 tasks. Curie-7B-v1 used approximately 2-3% of a typical dataset size to learn Polish. The LAPT was completed in less than five days using a consumer GPU, highlighting the method's efficiency. The proficiency of the model in Polish was significantly enhanced, demonstrating the viability of this approach for adding new languages to existing LLMs
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#21644;&#39640;&#25928;&#25512;&#29702;&#26041;&#27861;&#65292;&#21253;&#25324;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#33976;&#39311;&#12289;&#32039;&#20945;&#26550;&#26500;&#35774;&#35745;&#21644;&#21160;&#24577;&#32593;&#32476;&#31561;&#26041;&#38754;&#12290;&#22823;&#27169;&#22411;&#30340;&#31361;&#20986;&#29305;&#28857;&#26159;&#21387;&#32553;&#21518;&#38656;&#35201;&#24494;&#35843;&#25110;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#19988;&#30456;&#20851;&#30340;&#25104;&#26412;&#24456;&#39640;&#12290;</title><link>https://arxiv.org/abs/2402.09748</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#21644;&#39640;&#25928;&#25512;&#29702;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Model Compression and Efficient Inference for Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09748
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#21644;&#39640;&#25928;&#25512;&#29702;&#26041;&#27861;&#65292;&#21253;&#25324;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#33976;&#39311;&#12289;&#32039;&#20945;&#26550;&#26500;&#35774;&#35745;&#21644;&#21160;&#24577;&#32593;&#32476;&#31561;&#26041;&#38754;&#12290;&#22823;&#27169;&#22411;&#30340;&#31361;&#20986;&#29305;&#28857;&#26159;&#21387;&#32553;&#21518;&#38656;&#35201;&#24494;&#35843;&#25110;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#19988;&#30456;&#20851;&#30340;&#25104;&#26412;&#24456;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25152;&#20135;&#29983;&#30340;&#26174;&#33879;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#20351;&#24471;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#22823;&#27169;&#22411;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#20174;&#31639;&#27861;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#21644;&#39640;&#25928;&#25512;&#29702;&#26041;&#27861;&#12290;&#22312;&#20998;&#31867;&#26041;&#38754;&#65292;&#19982;&#36739;&#23567;&#30340;&#27169;&#22411;&#31867;&#20284;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#21644;&#21152;&#36895;&#31639;&#27861;&#20173;&#21487;&#20197;&#20998;&#20026;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#33976;&#39311;&#12289;&#32039;&#20945;&#26550;&#26500;&#35774;&#35745;&#21644;&#21160;&#24577;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#19982;&#36739;&#23567;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26377;&#20004;&#20010;&#31361;&#20986;&#30340;&#29305;&#28857;&#65306;&#65288;1&#65289;&#22823;&#22810;&#25968;&#21387;&#32553;&#31639;&#27861;&#22312;&#21387;&#32553;&#21518;&#38656;&#35201;&#24494;&#35843;&#29978;&#33267;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#22823;&#27169;&#22411;&#26368;&#26174;&#33879;&#30340;&#26041;&#38754;&#26159;&#19982;&#27169;&#22411;&#24494;&#35843;&#25110;&#35757;&#32451;&#30456;&#20851;&#30340;&#38750;&#24120;&#39640;&#30340;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#38024;&#23545;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#31639;&#27861;&#37117;&#38656;&#35201;&#32771;&#34385;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09748v1 Announce Type: cross  Abstract: Transformer based large language models have achieved tremendous success. However, the significant memory and computational costs incurred during the inference process make it challenging to deploy large models on resource-constrained devices. In this paper, we investigate compression and efficient inference methods for large language models from an algorithmic perspective. Regarding taxonomy, similar to smaller models, compression and acceleration algorithms for large language models can still be categorized into quantization, pruning, distillation, compact architecture design, dynamic networks. However, Large language models have two prominent characteristics compared to smaller models: (1) Most of compression algorithms require finetuning or even retraining the model after compression. The most notable aspect of large models is the very high cost associated with model finetuning or training. Therefore, many algorithms for large mode
&lt;/p&gt;</description></item><item><title>AI&#21307;&#38498;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#23454;&#26102;&#20132;&#20114;&#24335;&#35786;&#26029;&#29615;&#22659;&#65292;&#36890;&#36807;&#19982;LLMs&#30340;&#20132;&#20114;&#35780;&#20272;&#21644;&#21327;&#20316;&#65292;&#25552;&#39640;&#20020;&#24202;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09742</link><description>&lt;p&gt;
AI&#21307;&#38498;&#65306;&#29992;&#20110;&#20020;&#24202;&#35786;&#26029;&#30340;LLMs&#20316;&#20026;&#23454;&#20064;&#21307;&#29983;&#30340;&#20132;&#20114;&#24335;&#35780;&#20272;&#21644;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern Doctors for Clinical Diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09742
&lt;/p&gt;
&lt;p&gt;
AI&#21307;&#38498;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#23454;&#26102;&#20132;&#20114;&#24335;&#35786;&#26029;&#29615;&#22659;&#65292;&#36890;&#36807;&#19982;LLMs&#30340;&#20132;&#20114;&#35780;&#20272;&#21644;&#21327;&#20316;&#65292;&#25552;&#39640;&#20020;&#24202;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24212;&#29992;&#26631;&#24535;&#30528;&#37325;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#24212;&#29992;&#20027;&#35201;&#23616;&#38480;&#20110;&#36776;&#21035;&#21644;&#38382;&#31572;&#20219;&#21153;&#65292;&#27809;&#26377;&#20805;&#20998;&#21457;&#25381;&#20854;&#20132;&#20114;&#28508;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;AI&#21307;&#38498;&#65292;&#19968;&#20010;&#26088;&#22312;&#26500;&#24314;&#23454;&#26102;&#20132;&#20114;&#24335;&#35786;&#26029;&#29615;&#22659;&#30340;&#26694;&#26550;&#12290;&#20026;&#20102;&#27169;&#25311;&#36807;&#31243;&#65292;&#25105;&#20204;&#25910;&#38598;&#39640;&#36136;&#37327;&#30340;&#21307;&#30103;&#35760;&#24405;&#65292;&#21019;&#24314;&#20102;&#24739;&#32773;&#12289;&#26816;&#26597;&#32773;&#21644;&#21307;&#30103;&#20027;&#20219;&#20195;&#29702;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;AI&#21307;&#38498;&#36827;&#34892;LLMs&#30340;&#20132;&#20114;&#35780;&#20272;&#21644;&#21327;&#20316;&#12290;&#21021;&#22987;&#38454;&#27573;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#35270;&#22270;&#21307;&#23398;&#35780;&#20272;&#65288;MVME&#65289;&#22522;&#20934;&#65292;&#20854;&#20013;&#21508;&#31181;LLMs&#20316;&#20026;&#23454;&#20064;&#21307;&#29983;&#36827;&#34892;&#20132;&#20114;&#24335;&#35786;&#26029;&#12290;&#38543;&#21518;&#65292;&#20026;&#20102;&#25552;&#39640;&#35786;&#26029;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21327;&#20316;&#26426;&#21046;&#65292;&#28041;&#21450;&#21307;&#30103;&#20027;&#20219;&#30340;&#30417;&#30563;&#19979;&#30340;&#36845;&#20195;&#35752;&#35770;&#21644;&#20105;&#35758;&#35299;&#20915;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09742v1 Announce Type: new  Abstract: The incorporation of Large Language Models (LLMs) in healthcare marks a significant advancement. However, the application has predominantly been limited to discriminative and question-answering tasks, which does not fully leverage their interactive potential. To address this limitation, our paper presents AI Hospital, a framework designed to build a real-time interactive diagnosis environment. To simulate the procedure, we collect high-quality medical records to create patient, examiner, and medical director agents. AI Hospital is then utilized for the interactive evaluation and collaboration of LLMs. Initially, we create a Multi-View Medical Evaluation (MVME) benchmark where various LLMs serve as intern doctors for interactive diagnosis. Subsequently, to improve diagnostic accuracy, we introduce a collaborative mechanism that involves iterative discussions and a dispute resolution process under the supervision of the medical director. I
&lt;/p&gt;</description></item><item><title>QuRating&#26159;&#19968;&#31181;&#36873;&#25321;&#39640;&#36136;&#37327;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#20154;&#31867;&#30452;&#35266;&#24863;&#30693;&#30340;&#25991;&#26412;&#30340;&#25277;&#35937;&#29305;&#24449;&#12290;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#24179;&#34913;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.09739</link><description>&lt;p&gt;
&#36873;&#25321;&#39640;&#36136;&#37327;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;QuRating&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
QuRating: Selecting High-Quality Data for Training Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09739
&lt;/p&gt;
&lt;p&gt;
QuRating&#26159;&#19968;&#31181;&#36873;&#25321;&#39640;&#36136;&#37327;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#20154;&#31867;&#30452;&#35266;&#24863;&#30693;&#30340;&#25991;&#26412;&#30340;&#25277;&#35937;&#29305;&#24449;&#12290;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#24179;&#34913;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#23545;&#20110;&#21019;&#24314;&#33021;&#21147;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#24456;&#37325;&#35201;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;QuRating&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#33021;&#22815;&#25429;&#25417;&#20154;&#31867;&#30452;&#35266;&#24863;&#30693;&#30340;&#25991;&#26412;&#30340;&#25277;&#35937;&#29305;&#24449;&#30340;&#39044;&#35757;&#32451;&#25991;&#26412;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22235;&#20010;&#29305;&#24449; - &#20889;&#20316;&#39118;&#26684;&#12289;&#25152;&#38656;&#19987;&#19994;&#30693;&#35782;&#12289;&#20107;&#23454;&#21644;&#29712;&#20107;&#20197;&#21450;&#25945;&#32946;&#20215;&#20540;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36776;&#21035;&#36825;&#20123;&#29305;&#24449;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#20204;&#22312;&#36827;&#34892;&#25991;&#26412;&#30340;&#37197;&#23545;&#21028;&#26029;&#26041;&#38754;&#27604;&#30452;&#25509;&#35780;&#20272;&#25991;&#26412;&#36136;&#37327;&#26356;&#22909;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;QuRater&#27169;&#22411;&#65292;&#20174;&#37197;&#23545;&#21028;&#26029;&#20013;&#23398;&#20064;&#26631;&#37327;&#35780;&#20998;&#65292;&#24182;&#20351;&#29992;&#23427;&#20026;260B&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#27599;&#20010;&#26631;&#20934;&#36827;&#34892;&#36136;&#37327;&#35780;&#32423;&#27880;&#37322;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#19981;&#21516;&#30340;&#36136;&#37327;&#35780;&#32423;&#36873;&#25321;&#20102;30B&#20010;&#20196;&#29260;&#65292;&#24182;&#22312;&#25152;&#36873;&#25968;&#25454;&#19978;&#35757;&#32451;&#20102;13&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09739v1 Announce Type: new  Abstract: Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that captures the abstract qualities of texts which humans intuitively perceive. In this paper, we investigate four qualities - writing style, required expertise, facts &amp; trivia, and educational value. We find that LLMs are able to discern these qualities and observe that they are better at making pairwise judgments of texts than at rating the quality of a text directly. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and di
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#20043;&#38388;&#36827;&#34892;&#23545;&#40784;&#65292;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#22810;&#27169;&#24577;&#24694;&#24847;&#20869;&#23481;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#20851;&#27880;&#27169;&#24577;&#29305;&#23450;&#30340;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#30340;&#34701;&#21512;&#25216;&#26415;&#26080;&#27861;&#26377;&#25928;&#20851;&#27880;&#27169;&#24577;&#29305;&#23450;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#22312;&#33521;&#35821;&#21644;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.09738</link><description>&lt;p&gt;
&#22312;&#27880;&#24847;&#20043;&#21069;&#36827;&#34892;&#23545;&#40784;&#65306;&#29992;&#20110;&#22810;&#27169;&#24577;&#24694;&#24847;&#20869;&#23481;&#26816;&#27979;&#30340;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Align before Attend: Aligning Visual and Textual Features for Multimodal Hateful Content Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#20043;&#38388;&#36827;&#34892;&#23545;&#40784;&#65292;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#22810;&#27169;&#24577;&#24694;&#24847;&#20869;&#23481;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#20851;&#27880;&#27169;&#24577;&#29305;&#23450;&#30340;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#30340;&#34701;&#21512;&#25216;&#26415;&#26080;&#27861;&#26377;&#25928;&#20851;&#27880;&#27169;&#24577;&#29305;&#23450;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#22312;&#33521;&#35821;&#21644;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24694;&#24847;&#20869;&#23481;&#26816;&#27979;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#23545;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#20013;&#38388;&#34701;&#21512;&#26377;&#25928;&#22320;&#25429;&#25417;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21019;&#24314;&#26377;&#24847;&#20041;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20256;&#32479;&#30340;&#34701;&#21512;&#25216;&#26415;&#26080;&#27861;&#26377;&#25928;&#22320;&#20851;&#27880;&#27169;&#24577;&#29305;&#23450;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20165;&#20851;&#27880;&#33521;&#35821;&#65292;&#24573;&#35270;&#20102;&#20854;&#20182;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#24694;&#24847;&#20869;&#23481;&#26816;&#27979;&#65292;&#24182;&#23545;&#33521;&#35821;&#21644;&#38750;&#33521;&#35821;&#35821;&#35328;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#27880;&#24847;&#21147;&#23618;&#24341;&#20837;&#21040;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#20043;&#38388;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#23545;&#40784;&#12290;&#36825;&#31181;&#23545;&#40784;&#20351;&#24471;&#22312;&#34701;&#21512;&#20043;&#21069;&#21487;&#20197;&#26377;&#36873;&#25321;&#24615;&#22320;&#20851;&#27880;&#27169;&#24577;&#29305;&#23450;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22522;&#20934;&#24694;&#24847;&#27169;&#22240;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21363;MUTE&#65288;Ben&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09738v1 Announce Type: new  Abstract: Multimodal hateful content detection is a challenging task that requires complex reasoning across visual and textual modalities. Therefore, creating a meaningful multimodal representation that effectively captures the interplay between visual and textual features through intermediate fusion is critical. Conventional fusion techniques are unable to attend to the modality-specific features effectively. Moreover, most studies exclusively concentrated on English and overlooked other low-resource languages. This paper proposes a context-aware attention framework for multimodal hateful content detection and assesses it for both English and non-English languages. The proposed approach incorporates an attention layer to meaningfully align the visual and textual features. This alignment enables selective focus on modality-specific features before fusing them. We evaluate the proposed approach on two benchmark hateful meme datasets, viz. MUTE (Ben
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#24187;&#35273;&#30340;&#24847;&#35782;&#31243;&#24230;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;LLMs&#22312;&#22788;&#29702;&#30495;&#23454;&#22238;&#31572;&#21644;&#34394;&#26500;&#22238;&#31572;&#26102;&#26377;&#19981;&#21516;&#30340;&#21453;&#24212;&#65292;&#24182;&#23637;&#31034;&#20102;&#21033;&#29992;LLMs&#38544;&#34255;&#29366;&#24577;&#30340;&#25351;&#23548;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.09733</link><description>&lt;p&gt;
LLM&#26159;&#21542;&#20102;&#35299;&#24187;&#35273;&#65311;&#23545;LLMs&#38544;&#34255;&#29366;&#24577;&#30340;&#23454;&#35777;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Do LLMs Know about Hallucination? An Empirical Investigation of LLM's Hidden States
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#24187;&#35273;&#30340;&#24847;&#35782;&#31243;&#24230;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;LLMs&#22312;&#22788;&#29702;&#30495;&#23454;&#22238;&#31572;&#21644;&#34394;&#26500;&#22238;&#31572;&#26102;&#26377;&#19981;&#21516;&#30340;&#21453;&#24212;&#65292;&#24182;&#23637;&#31034;&#20102;&#21033;&#29992;LLMs&#38544;&#34255;&#29366;&#24577;&#30340;&#25351;&#23548;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#33021;&#20250;&#34394;&#26500;&#20986;&#19981;&#30495;&#23454;&#30340;&#31572;&#26696;&#65292;&#36825;&#23601;&#26159;&#25152;&#35859;&#30340;&#24187;&#35273;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35266;&#23519;LLMs&#26159;&#21542;&#24847;&#35782;&#21040;&#24187;&#35273;&#20197;&#21450;&#20854;&#31243;&#24230;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;LLMs&#22312;&#20854;&#38544;&#34255;&#29366;&#24577;&#19979;&#23545;&#27491;&#30830;&#22238;&#31572;&#21644;&#24187;&#35273;&#22238;&#31572;&#30340;&#19981;&#21516;&#21453;&#24212;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23454;&#39564;&#26694;&#26550;&#65292;&#20801;&#35768;&#26816;&#26597;LLMs&#22312;&#19981;&#21516;&#24187;&#35273;&#24773;&#20917;&#19979;&#30340;&#38544;&#34255;&#29366;&#24577;&#12290;&#22522;&#20110;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#20351;&#29992;LLaMA&#23478;&#26063;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;Touvron&#31561;&#65292;2023&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#34920;&#26126;&#65292;LLMs&#22312;&#22788;&#29702;&#30495;&#23454;&#22238;&#31572;&#21644;&#34394;&#26500;&#22238;&#31572;&#26102;&#26377;&#19981;&#21516;&#30340;&#21453;&#24212;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#21508;&#31181;&#27169;&#22411;&#35299;&#37322;&#25216;&#26415;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#35299;&#37322;&#36825;&#20123;&#21457;&#29616;&#12290;&#27492;&#22806;&#65292;&#26681;&#25454;&#23454;&#35777;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21033;&#29992;&#20174;LLMs&#38544;&#34255;&#29366;&#24577;&#20013;&#24471;&#21040;&#30340;&#25351;&#23548;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09733v1 Announce Type: new  Abstract: Large Language Models (LLMs) can make up answers that are not real, and this is known as hallucination. This research aims to see if, how, and to what extent LLMs are aware of hallucination. More specifically, we check whether and how an LLM reacts differently in its hidden states when it answers a question right versus when it hallucinates. To do this, we introduce an experimental framework which allows examining LLM's hidden states in different hallucination situations. Building upon this framework, we conduct a series of experiments with language models in the LLaMA family (Touvron et al., 2023). Our empirical findings suggest that LLMs react differently when processing a genuine response versus a fabricated one. We then apply various model interpretation techniques to help understand and explain the findings better. Moreover, informed by the empirical observations, we show great potential of using the guidance derived from LLM's hidd
&lt;/p&gt;</description></item><item><title>ReadAgent&#26159;&#19968;&#20010;&#20855;&#26377;&#38271;&#26399;&#19978;&#19979;&#25991;&#27010;&#35201;&#35760;&#24518;&#30340;&#38405;&#35835;&#20195;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#22788;&#29702;&#38271;&#36755;&#20837;&#24182;&#25552;&#39640;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.09727</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#38271;&#26399;&#19978;&#19979;&#25991;&#27010;&#35201;&#35760;&#24518;&#30340;&#20154;&#24037;&#26234;&#33021;&#38405;&#35835;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09727
&lt;/p&gt;
&lt;p&gt;
ReadAgent&#26159;&#19968;&#20010;&#20855;&#26377;&#38271;&#26399;&#19978;&#19979;&#25991;&#27010;&#35201;&#35760;&#24518;&#30340;&#38405;&#35835;&#20195;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#22788;&#29702;&#38271;&#36755;&#20837;&#24182;&#25552;&#39640;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#38480;&#21046;&#22312;&#26576;&#20010;&#26368;&#22823;&#19978;&#19979;&#25991;&#38271;&#24230;&#20869;&#65292;&#32780;&#19988;&#26080;&#27861;&#31283;&#23450;&#22320;&#22788;&#29702;&#38271;&#36755;&#20837;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ReadAgent&#65292;&#19968;&#20010;&#22686;&#21152;&#20102;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#31995;&#32479;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#21487;&#20197;&#36798;&#21040;20&#20493;&#12290;&#21463;&#21040;&#20154;&#31867;&#20132;&#20114;&#24335;&#38405;&#35835;&#38271;&#25991;&#26723;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;ReadAgent&#23454;&#29616;&#20026;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#31995;&#32479;&#65292;&#21033;&#29992;LLM&#30340;&#39640;&#32423;&#35821;&#35328;&#33021;&#21147;&#26469;&#65306;&#65288;1&#65289;&#20915;&#23450;&#23558;&#21738;&#20123;&#20869;&#23481;&#23384;&#20648;&#22312;&#19968;&#20010;&#35760;&#24518;&#29255;&#27573;&#20013;&#65292;&#65288;2&#65289;&#23558;&#36825;&#20123;&#35760;&#24518;&#29255;&#27573;&#21387;&#32553;&#25104;&#20026;&#31216;&#20026;&#27010;&#35201;&#35760;&#24518;&#30340;&#30701;&#26102;&#35760;&#24518;&#65292;&#65288;3&#65289;&#22312;&#38656;&#35201;&#26102;&#36890;&#36807;&#21407;&#22987;&#25991;&#26412;&#26597;&#25214;&#27573;&#33853;&#26469;&#25552;&#37266;&#33258;&#24049;&#30456;&#20851;&#32454;&#33410;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#26816;&#32034;&#26041;&#27861;&#12289;&#20351;&#29992;&#21407;&#22987;&#38271;&#19978;&#19979;&#25991;&#20197;&#21450;&#20351;&#29992;&#27010;&#35201;&#35760;&#24518;&#26469;&#35780;&#20272;ReadAgent&#19982;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#35780;&#20272;&#26159;&#22312;&#19977;&#20010;&#38271;&#25991;&#26723;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09727v1 Announce Type: cross  Abstract: Current Large Language Models (LLMs) are not only limited to some maximum context length, but also are not able to robustly consume long inputs. To address these limitations, we propose ReadAgent, an LLM agent system that increases effective context length up to 20x in our experiments. Inspired by how humans interactively read long documents, we implement ReadAgent as a simple prompting system that uses the advanced language capabilities of LLMs to (1) decide what content to store together in a memory episode, (2) compress those memory episodes into short episodic memories called gist memories, and (3) take actions to look up passages in the original text if ReadAgent needs to remind itself of relevant details to complete a task. We evaluate ReadAgent against baselines using retrieval methods, using the original long contexts, and using the gist memories. These evaluations are performed on three long-document reading comprehension task
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#38169;&#35823;&#26292;&#38706;&#21644;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#25913;&#36827;&#38750;&#33258;&#22238;&#24402;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;BLEU&#24471;&#20998;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.09725</link><description>&lt;p&gt;
&#36890;&#36807;&#38169;&#35823;&#26292;&#38706;&#21644;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#25913;&#36827;&#38750;&#33258;&#22238;&#24402;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Improving Non-autoregressive Machine Translation with Error Exposure and Consistency Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#38169;&#35823;&#26292;&#38706;&#21644;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#25913;&#36827;&#38750;&#33258;&#22238;&#24402;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;BLEU&#24471;&#20998;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;IR-NAT&#65288;&#22522;&#20110;&#36845;&#20195;&#25913;&#36827;&#30340;NAT&#65289;&#26694;&#26550;&#20043;&#19968;&#65292;&#26465;&#20214;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;CMLM&#65289;&#37319;&#29992;&#25513;&#30721;&#39044;&#27979;&#33539;&#24335;&#26469;&#37325;&#26032;&#39044;&#27979;&#25513;&#30721;&#20302;&#32622;&#20449;&#24230;&#30340;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;CMLM&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#20043;&#38388;&#23384;&#22312;&#25968;&#25454;&#20998;&#24067;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#35266;&#23519;&#21040;&#30340;&#26631;&#35760;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#29983;&#25104;&#26041;&#24335;&#19981;&#21516;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#38169;&#35823;&#26292;&#38706;&#21644;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#65288;EECR&#65289;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22522;&#20110;&#27169;&#22411;&#39044;&#27979;&#26500;&#24314;&#28151;&#21512;&#24207;&#21015;&#65292;&#24182;&#25552;&#20986;&#22312;&#19981;&#23436;&#32654;&#35266;&#27979;&#26465;&#20214;&#19979;&#38024;&#23545;&#25513;&#30721;&#26631;&#35760;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#38480;&#21046;&#19981;&#21516;&#35266;&#27979;&#24773;&#20917;&#19979;&#25513;&#30721;&#26631;&#35760;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#32553;&#23567;&#35757;&#32451;&#21644;&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22312;&#20116;&#20010;&#32763;&#35793;&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#24179;&#22343;&#25913;&#36827;&#20102;0.68&#21644;0.40&#30340;BLEU&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09725v1 Announce Type: cross  Abstract: Being one of the IR-NAT (Iterative-refinemennt-based NAT) frameworks, the Conditional Masked Language Model (CMLM) adopts the mask-predict paradigm to re-predict the masked low-confidence tokens. However, CMLM suffers from the data distribution discrepancy between training and inference, where the observed tokens are generated differently in the two cases. In this paper, we address this problem with the training approaches of error exposure and consistency regularization (EECR). We construct the mixed sequences based on model prediction during training, and propose to optimize over the masked tokens under imperfect observation conditions. We also design a consistency learning method to constrain the data distribution for the masked tokens under different observing situations to narrow down the gap between training and inference. The experiments on five translation benchmarks obtains an average improvement of 0.68 and 0.40 BLEU scores c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36816;&#29992; Eo-GP &#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#19990;&#30028;&#35821;&#30340;&#39057;&#29575;&#20998;&#26512;&#65292;&#24341;&#20837;&#20102; Eo-GEC &#25968;&#25454;&#38598;&#29992;&#20110;&#38169;&#35823;&#35782;&#21035;&#12290;&#23454;&#39564;&#34920;&#26126; GPT-4 &#22312;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110; GPT-3.5&#65292;&#23637;&#31034;&#20102;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#22312;&#22686;&#24378;&#23545;&#20110;&#36739;&#23569;&#30740;&#31350;&#35821;&#35328;&#30340; GEC &#31574;&#30053;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.09696</link><description>&lt;p&gt;
&#23545;&#20110;&#19990;&#30028;&#35821;&#30340;&#35821;&#35328;&#39057;&#29575;&#21644;&#38169;&#35823;&#20462;&#27491;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Analysis of Langauge Frequency and Error Correction for Esperanto
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36816;&#29992; Eo-GP &#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#19990;&#30028;&#35821;&#30340;&#39057;&#29575;&#20998;&#26512;&#65292;&#24341;&#20837;&#20102; Eo-GEC &#25968;&#25454;&#38598;&#29992;&#20110;&#38169;&#35823;&#35782;&#21035;&#12290;&#23454;&#39564;&#34920;&#26126; GPT-4 &#22312;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110; GPT-3.5&#65292;&#23637;&#31034;&#20102;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#22312;&#22686;&#24378;&#23545;&#20110;&#36739;&#23569;&#30740;&#31350;&#35821;&#35328;&#30340; GEC &#31574;&#30053;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491; (GEC) &#39033;&#30446;&#24448;&#24448;&#30528;&#37325;&#20110;&#20027;&#35201;&#35821;&#35328;&#65292;&#32780;&#23545;&#20110;&#20687;&#19990;&#30028;&#35821;&#36825;&#26679;&#30340;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#21017;&#20851;&#27880;&#36739;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#39318;&#20808;&#20351;&#29992;&#19987;&#38376;&#20026;&#27492;&#30446;&#30340;&#21019;&#24314;&#30340; Eo-GP &#25968;&#25454;&#38598;&#36827;&#34892;&#20840;&#38754;&#30340;&#39057;&#29575;&#20998;&#26512;&#65292;&#24320;&#22987;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28304;&#33258;&#30495;&#23454;&#29992;&#25143;&#26696;&#20363;&#24182;&#29992;&#20110;&#38169;&#35823;&#35782;&#21035;&#30340;&#32454;&#31890;&#24230;&#35821;&#35328;&#32454;&#33410;&#36827;&#34892;&#27880;&#37322;&#30340; Eo-GEC &#25968;&#25454;&#38598;&#12290;&#21033;&#29992; GPT-3.5 &#21644; GPT-4&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126; GPT-4 &#22312;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110; GPT-3.5&#65292;&#31361;&#20986;&#20102;&#20854;&#22312;&#35299;&#20915;&#19990;&#30028;&#35821;&#35821;&#27861;&#29305;&#27530;&#24615;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#22312;&#22686;&#24378;&#23545;&#20110;&#36739;&#23569;&#30740;&#31350;&#35821;&#35328;&#30340; GEC &#31574;&#30053;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09696v1 Announce Type: new  Abstract: Current Grammar Error Correction (GEC) initiatives tend to focus on major languages, with less attention given to low-resource languages like Esperanto. In this article, we begin to bridge this gap by first conducting a comprehensive frequency analysis using the Eo-GP dataset, created explicitly for this purpose. We then introduce the Eo-GEC dataset, derived from authentic user cases and annotated with fine-grained linguistic details for error identification. Leveraging GPT-3.5 and GPT-4, our experiments show that GPT-4 outperforms GPT-3.5 in both automated and human evaluations, highlighting its efficacy in addressing Esperanto's grammatical peculiarities and illustrating the potential of advanced language models to enhance GEC strategies for less commonly studied languages.
&lt;/p&gt;</description></item><item><title>PAL&#26159;&#31532;&#19968;&#20010;&#40657;&#30418;&#26597;&#35810;&#25915;&#20987;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#20195;&#29702;&#27169;&#22411;&#24341;&#23548;&#20248;&#21270;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#22797;&#26434;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.09674</link><description>&lt;p&gt;
PAL&#65306;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#24341;&#23548;&#40657;&#30418;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
PAL: Proxy-Guided Black-Box Attack on Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09674
&lt;/p&gt;
&lt;p&gt;
PAL&#26159;&#31532;&#19968;&#20010;&#40657;&#30418;&#26597;&#35810;&#25915;&#20987;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#20195;&#29702;&#27169;&#22411;&#24341;&#23548;&#20248;&#21270;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#22797;&#26434;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#20960;&#20010;&#26376;&#26469;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#22312;&#34987;&#25805;&#32437;&#26102;&#23427;&#20204;&#23637;&#31034;&#20986;&#30340;&#21361;&#38505;&#33021;&#21147;&#20196;&#20154;&#25285;&#24551;&#12290;&#23613;&#31649;&#23433;&#20840;&#24494;&#35843;&#31561;&#25216;&#26415;&#26088;&#22312;&#26368;&#23567;&#21270;&#26377;&#23475;&#20351;&#29992;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#24341;&#21457;&#26377;&#27602;&#22238;&#24212;&#30340;&#25915;&#20987;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;LLMs&#30340;&#20195;&#29702;&#24341;&#23548;&#25915;&#20987;&#65288;PAL&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#20248;&#21270;&#30340;&#23545;LLMs&#30340;&#40657;&#30418;&#20165;&#26597;&#35810;&#25915;&#20987;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#20381;&#36182;&#20110;&#19968;&#20010;&#26367;&#20195;&#27169;&#22411;&#26469;&#24341;&#23548;&#20248;&#21270;&#36807;&#31243;&#65292;&#24182;&#37319;&#29992;&#20102;&#38024;&#23545;&#30495;&#23454;&#19990;&#30028;LLM API&#35774;&#35745;&#30340;&#22797;&#26434;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#22312;GPT-3.5-Turbo&#19978;&#36798;&#21040;84%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65288;ASR&#65289;&#65292;&#22312;Llama-2-7B&#19978;&#36798;&#21040;48%&#65292;&#32780;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20165;&#20026;4%&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;GCG++&#65292;&#36825;&#26159;&#23545;GCG&#25915;&#20987;&#30340;&#25913;&#36827;&#65292;&#22312;&#30333;&#30418;Llama-2-7B&#19978;&#36798;&#21040;&#20102;94%&#30340;ASR&#65292;&#20197;&#21450;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#30340;&#24378;&#26377;&#21147;&#20294;&#31616;&#21333;&#30340;&#22522;&#20934;&#26041;&#27861;&#8212;&#8212;LLMs&#19978;&#30340;&#38543;&#26426;&#25628;&#32034;&#25915;&#20987;&#65288;RAL&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09674v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have surged in popularity in recent months, but they have demonstrated concerning capabilities to generate harmful content when manipulated. While techniques like safety fine-tuning aim to minimize harmful use, recent works have shown that LLMs remain vulnerable to attacks that elicit toxic responses. In this work, we introduce the Proxy-Guided Attack on LLMs (PAL), the first optimization-based attack on LLMs in a black-box query-only setting. In particular, it relies on a surrogate model to guide the optimization and a sophisticated loss designed for real-world LLM APIs. Our attack achieves 84% attack success rate (ASR) on GPT-3.5-Turbo and 48% on Llama-2-7B, compared to 4% for the current state of the art. We also propose GCG++, an improvement to the GCG attack that reaches 94% ASR on white-box Llama-2-7B, and the Random-Search Attack on LLMs (RAL), a strong but simple baseline for query-based attacks. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#35757;&#32451;&#25968;&#25454;&#39640;&#25928;&#30340;LLM&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;Ask-LLM&#21644;Density&#20004;&#31181;&#20248;&#31168;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09668</link><description>&lt;p&gt;
&#22914;&#20309;&#35757;&#32451;&#25968;&#25454;&#39640;&#25928;&#30340;LLM&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
How to Train Data-Efficient LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#35757;&#32451;&#25968;&#25454;&#39640;&#25928;&#30340;LLM&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;Ask-LLM&#21644;Density&#20004;&#31181;&#20248;&#31168;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35757;&#32451;&#21313;&#20998;&#26114;&#36149;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#39044;&#35757;&#32451;LLM&#30340;&#25968;&#25454;&#39640;&#25928;&#26041;&#27861;&#65292;&#21363;&#26088;&#22312;&#20248;&#21270;&#27169;&#22411;&#36136;&#37327;&#21644;&#35757;&#32451;&#36164;&#28304;/&#25968;&#25454;&#28040;&#32791;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#35797;&#22270;&#29702;&#35299;&#22522;&#20110;&#65288;i&#65289;&#26114;&#36149;&#30340;&#25968;&#25454;&#36136;&#37327;&#20272;&#35745;&#21644;&#65288;ii&#65289;&#22522;&#20110;&#29305;&#24449;&#31354;&#38388;&#30340;&#35206;&#30422;&#29575;&#21644;&#22810;&#26679;&#24615;&#27979;&#37327;&#30340;&#25968;&#25454;&#36873;&#25321;&#31243;&#24207;&#25152;&#24102;&#26469;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#31181;&#25216;&#26415;&#8220;Ask-LLM&#8221;&#21033;&#29992;&#35843;&#33410;&#25351;&#20196;&#30340;LLM&#30340;&#38646;&#26679;&#26412;&#25512;&#29702;&#33021;&#21147;&#26469;&#30452;&#25509;&#35780;&#20272;&#35757;&#32451;&#26679;&#20363;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#36798;&#21040;&#35206;&#30422;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23494;&#24230;&#37319;&#26679;&#65292;&#23427;&#26681;&#25454;&#25968;&#25454;&#20998;&#24067;&#36873;&#25321;&#22810;&#26679;&#30340;&#26679;&#26412;&#12290;&#22312;&#25105;&#20204;&#23545;19&#31181;&#37319;&#26679;&#22120;&#36827;&#34892;&#20102;&#25968;&#30334;&#20010;&#35780;&#20272;&#20219;&#21153;&#21644;&#39044;&#35757;&#32451;&#36816;&#34892;&#30340;&#23545;&#27604;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;Ask-LLM&#21644;Density&#26159;&#21508;&#33258;&#31867;&#21035;&#20013;&#26368;&#22909;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09668v1 Announce Type: cross  Abstract: The training of large language models (LLMs) is expensive. In this paper, we study data-efficient approaches for pre-training LLMs, i.e., techniques that aim to optimize the Pareto frontier of model quality and training resource/data consumption. We seek to understand the tradeoffs associated with data selection routines based on (i) expensive-to-compute data-quality estimates, and (ii) maximization of coverage and diversity-based measures in the feature space. Our first technique, Ask-LLM, leverages the zero-shot reasoning capabilities of instruction-tuned LLMs to directly assess the quality of a training example. To target coverage, we propose Density sampling, which models the data distribution to select a diverse sample. In our comparison of 19 samplers, involving hundreds of evaluation tasks and pre-training runs, we find that Ask-LLM and Density are the best methods in their respective categories. Coverage sampling can recover th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#22312;&#24120;&#35782;&#30693;&#35782;&#22270;&#23436;&#25104;&#20013;&#24341;&#20837;&#25991;&#26412;&#34164;&#28085;&#12290;&#22312;&#24120;&#35782;&#30693;&#35782;&#22270;&#30340;&#24314;&#27169;&#36807;&#31243;&#20013;&#65292;&#32771;&#34385;&#33410;&#28857;&#21644;&#20851;&#31995;&#30340;&#35821;&#20041;&#21512;&#29702;&#24615;&#26159;&#20851;&#38190;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#21033;&#29992;&#27010;&#24565;&#25277;&#35937;&#25552;&#39640;&#20102;&#20107;&#20214;&#21512;&#29702;&#24615;&#30340;&#19968;&#33268;&#24615;&#65292;&#20294;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#25991;&#26412;&#34164;&#28085;&#26469;&#23547;&#25214;&#38544;&#21547;&#30340;&#34164;&#28085;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.09666</link><description>&lt;p&gt;
&#24341;&#20837;&#24120;&#35782;&#30693;&#35782;&#22270;&#23436;&#25104;&#20013;&#30340;&#25991;&#26412;&#34164;&#28085;
&lt;/p&gt;
&lt;p&gt;
EntailE: Introducing Textual Entailment in Commonsense Knowledge Graph Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#22312;&#24120;&#35782;&#30693;&#35782;&#22270;&#23436;&#25104;&#20013;&#24341;&#20837;&#25991;&#26412;&#34164;&#28085;&#12290;&#22312;&#24120;&#35782;&#30693;&#35782;&#22270;&#30340;&#24314;&#27169;&#36807;&#31243;&#20013;&#65292;&#32771;&#34385;&#33410;&#28857;&#21644;&#20851;&#31995;&#30340;&#35821;&#20041;&#21512;&#29702;&#24615;&#26159;&#20851;&#38190;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#21033;&#29992;&#27010;&#24565;&#25277;&#35937;&#25552;&#39640;&#20102;&#20107;&#20214;&#21512;&#29702;&#24615;&#30340;&#19968;&#33268;&#24615;&#65292;&#20294;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#25991;&#26412;&#34164;&#28085;&#26469;&#23547;&#25214;&#38544;&#21547;&#30340;&#34164;&#28085;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09666v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#24120;&#35782;&#30693;&#35782;&#22270;&#23436;&#25104;&#26159;&#24120;&#35782;&#30693;&#35782;&#22270;&#26500;&#24314;&#21644;&#24212;&#29992;&#30340;&#26032;&#25361;&#25112;&#12290;&#19982;Freebase&#21644;YAGO&#31561;&#20107;&#23454;&#24615;&#30693;&#35782;&#22270;&#19981;&#21516;&#65292;&#24120;&#35782;&#30693;&#35782;&#22270;&#65288;&#22914;ConceptNet&#65289;&#21033;&#29992;&#33258;&#30001;&#24418;&#24335;&#30340;&#25991;&#26412;&#26469;&#34920;&#31034;&#21629;&#21517;&#23454;&#20307;&#12289;&#30701;&#35821;&#21644;&#20107;&#20214;&#20316;&#20026;&#23427;&#20204;&#30340;&#33410;&#28857;&#12290;&#36825;&#31181;&#26494;&#25955;&#30340;&#32467;&#26500;&#23548;&#33268;&#20102;&#22823;&#22411;&#31232;&#30095;&#30340;&#24120;&#35782;&#30693;&#35782;&#22270;&#65292;&#20351;&#24471;&#23545;&#36825;&#20123;&#33410;&#28857;&#30340;&#35821;&#20041;&#29702;&#35299;&#23545;&#23398;&#20064;&#20016;&#23500;&#30340;&#24120;&#35782;&#30693;&#35782;&#22270;&#23884;&#20837;&#26356;&#20026;&#20851;&#38190;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#21033;&#29992;&#35821;&#20041;&#30456;&#20284;&#24615;&#22686;&#21152;&#22270;&#30340;&#23494;&#24230;&#65292;&#20294;&#33410;&#28857;&#21450;&#20854;&#20851;&#31995;&#30340;&#35821;&#20041;&#21512;&#29702;&#24615;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#37319;&#29992;&#27010;&#24565;&#25277;&#35937;&#26469;&#25913;&#21892;&#24314;&#27169;&#65288;&#20107;&#20214;&#65289;&#21512;&#29702;&#24615;&#30340;&#19968;&#33268;&#24615;&#65292;&#20294;&#23427;&#20204;&#19981;&#22815;&#21487;&#25193;&#23637;&#19988;&#20173;&#28982;&#21463;&#21040;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#37319;&#29992;&#25991;&#26412;&#34164;&#28085;&#26469;&#23547;&#25214;&#38544;&#21547;&#30340;&#34164;&#28085;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09666v1 Announce Type: new  Abstract: Commonsense knowledge graph completion is a new challenge for commonsense knowledge graph construction and application. In contrast to factual knowledge graphs such as Freebase and YAGO, commonsense knowledge graphs (CSKGs; e.g., ConceptNet) utilize free-form text to represent named entities, short phrases, and events as their nodes. Such a loose structure results in large and sparse CSKGs, which makes the semantic understanding of these nodes more critical for learning rich commonsense knowledge graph embedding. While current methods leverage semantic similarities to increase the graph density, the semantic plausibility of the nodes and their relations are under-explored. Previous works adopt conceptual abstraction to improve the consistency of modeling (event) plausibility, but they are not scalable enough and still suffer from data sparsity. In this paper, we propose to adopt textual entailment to find implicit entailment relations be
&lt;/p&gt;</description></item><item><title>CodeMind&#26159;&#19968;&#20010;&#29992;&#20110;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#26469;&#26367;&#20195;&#20165;&#20165;&#20381;&#38752;&#27979;&#35797;&#36890;&#36807;&#26469;&#35780;&#20272;&#65292;&#23545;&#19977;&#31181;&#20195;&#30721;&#25512;&#29702;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;LLMs&#33021;&#22815;&#20844;&#27491;&#22320;&#29702;&#35299;&#25511;&#21046;&#27969;&#32467;&#26500;&#65292;&#24182;&#19988;&#23545;&#20110;&#31616;&#21333;&#31243;&#24207;&#21644;&#22797;&#26434;&#31243;&#24207;&#65292;&#23427;&#20204;&#36890;&#24120;&#33021;&#22815;&#25512;&#29702;&#20986;&#36755;&#20837;&#22914;&#20309;&#28436;&#21464;&#20026;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2402.09664</link><description>&lt;p&gt;
CodeMind:&#19968;&#20010;&#29992;&#20110;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#25512;&#29702;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CodeMind: A Framework to Challenge Large Language Models for Code Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09664
&lt;/p&gt;
&lt;p&gt;
CodeMind&#26159;&#19968;&#20010;&#29992;&#20110;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#26469;&#26367;&#20195;&#20165;&#20165;&#20381;&#38752;&#27979;&#35797;&#36890;&#36807;&#26469;&#35780;&#20272;&#65292;&#23545;&#19977;&#31181;&#20195;&#30721;&#25512;&#29702;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;LLMs&#33021;&#22815;&#20844;&#27491;&#22320;&#29702;&#35299;&#25511;&#21046;&#27969;&#32467;&#26500;&#65292;&#24182;&#19988;&#23545;&#20110;&#31616;&#21333;&#31243;&#24207;&#21644;&#22797;&#26434;&#31243;&#24207;&#65292;&#23427;&#20204;&#36890;&#24120;&#33021;&#22815;&#25512;&#29702;&#20986;&#36755;&#20837;&#22914;&#20309;&#28436;&#21464;&#20026;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#38752;&#27979;&#35797;&#36890;&#36807;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20195;&#30721;&#21512;&#25104;&#33021;&#21147;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#20844;&#27491;&#30340;&#35780;&#20272;&#25110;&#20419;&#36827;&#20855;&#26377;&#25968;&#25454;&#27844;&#28431;&#30340;&#27169;&#22411;&#65292;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CodeMind&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;CodeMind&#30446;&#21069;&#25903;&#25345;&#19977;&#31181;&#20195;&#30721;&#25512;&#29702;&#20219;&#21153;&#65306;&#29420;&#31435;&#25191;&#34892;&#25512;&#29702;&#65288;IER&#65289;&#12289;&#20381;&#36182;&#25191;&#34892;&#25512;&#29702;&#65288;DER&#65289;&#21644;&#35268;&#33539;&#25512;&#29702;&#65288;SR&#65289;&#12290;&#21069;&#20004;&#32773;&#35780;&#20272;&#27169;&#22411;&#20197;&#39044;&#27979;&#20219;&#24847;&#20195;&#30721;&#30340;&#25191;&#34892;&#36755;&#20986;&#65292;&#25110;&#32773;&#27169;&#22411;&#33021;&#22815;&#27491;&#30830;&#21512;&#25104;&#30340;&#20195;&#30721;&#12290;&#31532;&#19977;&#20010;&#20219;&#21153;&#35780;&#20272;LLMs&#23454;&#29616;&#25351;&#23450;&#39044;&#26399;&#34892;&#20026;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;CodeMind&#23545;&#20004;&#31181;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#20116;&#20010;&#22522;&#20934;&#19979;&#30340;&#20061;&#20010;LLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;LLMs&#33021;&#22815;&#20844;&#27491;&#22320;&#29702;&#35299;&#25511;&#21046;&#27969;&#32467;&#26500;&#65292;&#24182;&#19988;&#23545;&#20110;&#31616;&#21333;&#31243;&#24207;&#21644;&#22797;&#26434;&#31243;&#24207;&#65292;&#23427;&#20204;&#36890;&#24120;&#33021;&#22815;&#25512;&#29702;&#20986;&#36755;&#20837;&#22914;&#20309;&#28436;&#21464;&#20026;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09664v1 Announce Type: cross  Abstract: Solely relying on test passing to evaluate Large Language Models (LLMs) for code synthesis may result in unfair assessment or promoting models with data leakage. As an alternative, we introduce CodeMind, a framework designed to gauge the code reasoning abilities of LLMs. CodeMind currently supports three code reasoning tasks: Independent Execution Reasoning (IER), Dependent Execution Reasoning (DER), and Specification Reasoning (SR). The first two evaluate models to predict the execution output of an arbitrary code or code the model could correctly synthesize. The third one evaluates the extent to which LLMs implement the specified expected behavior. Our extensive evaluation of nine LLMs across five benchmarks in two different programming languages using CodeMind shows that LLMs fairly understand control flow constructs and, in general, are capable of reasoning how inputs evolve to output, specifically for simple programs and the ones 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;GPT-4&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#34920;&#29616;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21453;&#39304;&#23545;&#30456;&#23545;&#32622;&#20449;&#24230;&#26377;&#24433;&#21709;&#65292;&#20294;&#24182;&#19981;&#19968;&#33268;&#22320;&#22686;&#21152;&#25110;&#20943;&#23569;&#12290;</title><link>https://arxiv.org/abs/2402.09654</link><description>&lt;p&gt;
GPT-4&#22312;&#22522;&#20110;USMLE&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#30340;&#34920;&#29616;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
GPT-4's assessment of its performance in a USMLE-based case study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;GPT-4&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#34920;&#29616;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21453;&#39304;&#23545;&#30456;&#23545;&#32622;&#20449;&#24230;&#26377;&#24433;&#21709;&#65292;&#20294;&#24182;&#19981;&#19968;&#33268;&#22320;&#22686;&#21152;&#25110;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;GPT-4&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#34920;&#29616;&#35780;&#20272;&#12290;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#20174;&#32654;&#22269;&#21307;&#23398;&#25191;&#29031;&#32771;&#35797;&#65288;USMLE&#65289;&#38382;&#21367;&#20013;&#25552;&#21462;&#38382;&#39064;&#30340;&#26041;&#24335;&#65292;&#20219;&#21153;&#26159;&#35780;&#20272;&#27169;&#22411;&#22312;&#25552;&#38382;&#20043;&#21069;&#21644;&#25552;&#38382;&#20043;&#21518;&#30340;&#32622;&#20449;&#24230;&#24471;&#20998;&#12290;&#38382;&#21367;&#26681;&#25454;&#26159;&#21542;&#26377;&#21453;&#39304;&#20998;&#20026;&#20004;&#32452;&#65306;&#21453;&#39304;&#32452;&#65288;WF&#65289;&#21644;&#26080;&#21453;&#39304;&#32452;&#65288;NF&#65289;&#12290;&#35201;&#27714;&#27169;&#22411;&#22312;&#27599;&#20010;&#38382;&#39064;&#20043;&#21069;&#21644;&#20043;&#21518;&#25552;&#20379;&#32477;&#23545;&#21644;&#30456;&#23545;&#32622;&#20449;&#24230;&#24471;&#20998;&#12290;&#36890;&#36807;&#20351;&#29992;&#32479;&#35745;&#24037;&#20855;&#20998;&#26512;&#23454;&#39564;&#32467;&#26524;&#65292;&#30740;&#31350;&#20102;WF&#21644;NF&#32452;&#30340;&#32622;&#20449;&#24230;&#21464;&#24322;&#24615;&#12290;&#27492;&#22806;&#65292;&#36827;&#34892;&#20102;&#39034;&#24207;&#20998;&#26512;&#20197;&#35266;&#23519;WF&#21644;NF&#32452;&#30340;&#24615;&#33021;&#21464;&#21270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21453;&#39304;&#20250;&#24433;&#21709;&#30456;&#23545;&#32622;&#20449;&#24230;&#65292;&#20294;&#24182;&#19981;&#24635;&#26159;&#22686;&#21152;&#25110;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09654v1 Announce Type: new  Abstract: This study investigates GPT-4's assessment of its performance in healthcare applications. A simple prompting technique was used to prompt the LLM with questions taken from the United States Medical Licensing Examination (USMLE) questionnaire and it was tasked to evaluate its confidence score before posing the question and after asking the question. The questionnaire was categorized into two groups-questions with feedback (WF) and questions with no feedback(NF) post-question. The model was asked to provide absolute and relative confidence scores before and after each question. The experimental findings were analyzed using statistical tools to study the variability of confidence in WF and NF groups. Additionally, a sequential analysis was conducted to observe the performance variation for the WF and NF groups. Results indicate that feedback influences relative confidence but doesn't consistently increase or decrease it. Understanding the p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22238;&#31572;&#38382;&#39064;&#36827;&#34892;&#25351;&#20196;&#36981;&#24490;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25351;&#20196;&#35270;&#20026;&#23545;&#36755;&#20837;&#25991;&#26412;&#30340;&#38382;&#39064;&#24182;&#32534;&#30721;&#26399;&#26395;&#30340;&#31572;&#26696;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#30456;&#20284;&#30340;&#23884;&#20837;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.09642</link><description>&lt;p&gt;
&#31572;&#26696;&#23601;&#26159;&#20320;&#38656;&#35201;&#30340;&#19968;&#20999;&#65306;&#36890;&#36807;&#22238;&#31572;&#38382;&#39064;&#36827;&#34892;&#25351;&#20196;&#36981;&#24490;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Answer is All You Need: Instruction-following Text Embedding via Answering the Question
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09642
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22238;&#31572;&#38382;&#39064;&#36827;&#34892;&#25351;&#20196;&#36981;&#24490;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25351;&#20196;&#35270;&#20026;&#23545;&#36755;&#20837;&#25991;&#26412;&#30340;&#38382;&#39064;&#24182;&#32534;&#30721;&#26399;&#26395;&#30340;&#31572;&#26696;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#30456;&#20284;&#30340;&#23884;&#20837;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#29992;&#25143;&#25351;&#20196;&#25152;&#25351;&#23450;&#30340;&#25991;&#26412;&#29305;&#24449;&#30340;&#25991;&#26412;&#23884;&#20837;&#22120;&#12290;&#23613;&#31649;&#23427;&#22312;&#37096;&#32626;&#38754;&#21521;&#29992;&#25143;&#30340;&#23884;&#20837;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#20043;&#21069;&#30340;&#26041;&#27861;&#37117;&#27809;&#26377;&#25552;&#20379;&#20855;&#20307;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#23558;&#25351;&#20196;&#35270;&#20026;&#23545;&#36755;&#20837;&#25991;&#26412;&#30340;&#38382;&#39064;&#65292;&#24182;&#32534;&#30721;&#26399;&#26395;&#30340;&#31572;&#26696;&#20197;&#30456;&#24212;&#22320;&#33719;&#24471;&#34920;&#31034;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#36981;&#24490;&#25351;&#20196;&#30340;&#25991;&#26412;&#24212;&#35813;&#26377;&#30456;&#20284;&#30340;&#31572;&#26696;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#30456;&#20284;&#30340;&#23884;&#20837;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InBedder&#65292;&#23427;&#36890;&#36807;&#22312;&#25277;&#35937;&#24615;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#19978;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26469;&#23454;&#29616;&#35813;&#23884;&#20837;-via-answering&#24605;&#24819;&#12290;&#26681;&#25454;&#25105;&#20204;&#25552;&#20986;&#30340;&#25351;&#20196;&#24847;&#35782;&#27979;&#35797;&#21644;&#25351;&#20196;&#40065;&#26834;&#24615;&#27979;&#35797;&#65292;InBedder&#22312;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09642v1 Announce Type: new  Abstract: This work aims to build a text embedder that can capture characteristics of texts specified by user instructions. Despite its tremendous potential to deploy user-oriented embeddings, none of previous approaches provides a concrete solution for it. This paper offers a new viewpoint, which treats the instruction as a question about the input text and encodes the expected answers to obtain the representation accordingly. Intuitively, texts with the same (implicit) semantics would share similar answers following the instruction, thus leading to more similar embeddings. Specifically, we propose InBedder that instantiates this embed-via-answering idea by only fine-tuning language models on abstractive question answering tasks. InBedder demonstrates significantly improved instruction-following capabilities according to our proposed instruction awareness tests and instruction robustness tests, when applied to both large language models (LLMs) (e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#29983;&#25104;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;&#23545;&#25239;&#20107;&#23454;&#65292;&#20197;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#65292;&#35813;&#26041;&#27861;&#22312;&#22320;&#29699;&#31227;&#21160;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#24182;&#23545;&#34920;&#31034;&#31354;&#38388;&#30340;&#20960;&#20309;&#32452;&#32455;&#36827;&#34892;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.09631</link><description>&lt;p&gt;
MiMiC&#65306;&#34920;&#31034;&#31354;&#38388;&#20013;&#26368;&#23567;&#20462;&#25913;&#30340;&#23545;&#25239;&#20107;&#23454;
&lt;/p&gt;
&lt;p&gt;
MiMiC: Minimally Modified Counterfactuals in the Representation Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09631
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#29983;&#25104;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;&#23545;&#25239;&#20107;&#23454;&#65292;&#20197;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#65292;&#35813;&#26041;&#27861;&#22312;&#22320;&#29699;&#31227;&#21160;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#24182;&#23545;&#34920;&#31034;&#31354;&#38388;&#30340;&#20960;&#20309;&#32452;&#32455;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09631v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#23398;&#31185; &#31616;&#20171;&#65306;&#35821;&#35328;&#27169;&#22411;&#32463;&#24120;&#34920;&#29616;&#20986;&#19981;&#33391;&#34892;&#20026;&#65292;&#22914;&#24615;&#21035;&#20559;&#35265;&#25110;&#26377;&#27602;&#35821;&#35328;&#12290;&#36890;&#36807;&#23545;&#34920;&#31034;&#31354;&#38388;&#36827;&#34892;&#24178;&#39044;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#20004;&#31181;&#24120;&#35265;&#30340;&#24178;&#39044;&#25216;&#26415;&#65292;&#21363;&#32447;&#24615;&#25830;&#38500;&#21644;&#23450;&#21521;&#21521;&#37327;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#39640;&#24230;&#21487;&#25511;&#21644;&#34920;&#36798;&#20016;&#23500;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24178;&#39044;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#29983;&#25104;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;&#23545;&#25239;&#20107;&#23454;&#65292;&#20351;&#28304;&#31867;&#21035;&#65288;&#20363;&#22914;&#8220;&#26377;&#27602;&#8221;&#65289;&#30340;&#34920;&#31034;&#19982;&#30446;&#26631;&#31867;&#21035;&#65288;&#20363;&#22914;&#8220;&#38750;&#26377;&#27602;&#8221;&#65289;&#30340;&#34920;&#31034;&#30456;&#20284;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#39640;&#26031;&#20551;&#35774;&#19979;&#30340;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#22320;&#29699;&#31227;&#21160;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#24182;&#23545;&#34920;&#31034;&#31354;&#38388;&#30340;&#20960;&#20309;&#32452;&#32455;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09631v1 Announce Type: cross  Abstract: Language models often exhibit undesirable behaviors, such as gender bias or toxic language. Interventions in the representation space were shown effective in mitigating such issues by altering the LM behavior. We first show that two prominent intervention techniques, Linear Erasure and Steering Vectors, do not enable a high degree of control and are limited in expressivity.   We then propose a novel intervention methodology for generating expressive counterfactuals in the representation space, aiming to make representations of a source class (e.g., ``toxic'') resemble those of a target class (e.g., ``non-toxic''). This approach, generalizing previous linear intervention techniques, utilizes a closed-form solution for the Earth Mover's problem under Gaussian assumptions and provides theoretical guarantees on the representation space's geometric organization. We further build on this technique and derive a nonlinear intervention that ena
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;API Pack&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;</title><link>https://arxiv.org/abs/2402.09615</link><description>&lt;p&gt;
API Pack&#65306;&#19968;&#20010;&#29992;&#20110;API&#35843;&#29992;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
API Pack: A Massive Multilingual Dataset for API Call Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09615
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;API Pack&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;API Pack&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#25351;&#20196;-API&#35843;&#29992;&#23545;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;API Pack&#22312;&#25552;&#21319;&#27169;&#22411;&#22312;&#36825;&#19968;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20854;&#22312;&#19968;&#33324;&#32534;&#30721;&#26041;&#38754;&#30340;&#25972;&#20307;&#29087;&#32451;&#31243;&#24230;&#12290;&#20165;&#22312;20,000&#20010;Python&#23454;&#20363;&#19978;&#23545;CodeLlama-13B&#36827;&#34892;&#24494;&#35843;&#65292;&#20854;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#30340;&#20934;&#30830;&#29575;&#27604;GPT-3.5&#21644;GPT-4&#20998;&#21035;&#39640;&#20986;10%&#21644;5%&#12290;&#25193;&#23637;&#21040;100k&#20010;&#20363;&#23376;&#21487;&#20197;&#25552;&#39640;&#23545;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;API&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;&#65292;&#32780;&#26080;&#38656;&#22823;&#37327;&#35821;&#35328;&#29305;&#23450;&#30340;&#25968;&#25454;&#12290;&#25968;&#25454;&#38598;&#12289;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#21644;&#25972;&#20307;&#20195;&#30721;&#24211;&#21487;&#22312;https://github.com/anonymous_url&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09615v1 Announce Type: cross  Abstract: We introduce API Pack, a multilingual dataset featuring over one million instruction-API call pairs aimed at advancing large language models' API call generation capabilities. Through experiments, we demonstrate API Pack's efficacy in enhancing models for this specialized task while maintaining their overall proficiency at general coding. Fine-tuning CodeLlama-13B on just 20,000 Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4 respectively in generating unseen API calls. Scaling to 100k examples improves generalization to new APIs not seen during training. In addition, cross-lingual API call generation is achieved without needing extensive data per language. The dataset, fine-tuned models, and overall code base are publicly available at https://github.com/anonymous_url.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27010;&#29575;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#65292;&#24341;&#20837;&#20102;&#36125;&#21494;&#26031;&#35821;&#35328;&#25512;&#29702;&#25968;&#25454;&#38598;&#65288;BLInD&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#35299;&#20915;&#31574;&#30053;&#65292;&#21253;&#25324;Python&#20195;&#30721;&#21644;&#27010;&#29575;&#25512;&#29702;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09614</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27010;&#29575;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Reasoning in Generative Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27010;&#29575;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#65292;&#24341;&#20837;&#20102;&#36125;&#21494;&#26031;&#35821;&#35328;&#25512;&#29702;&#25968;&#25454;&#38598;&#65288;BLInD&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#35299;&#20915;&#31574;&#30053;&#65292;&#21253;&#25324;Python&#20195;&#30721;&#21644;&#27010;&#29575;&#25512;&#29702;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#28041;&#21450;&#27010;&#29575;&#20540;&#26126;&#30830;&#37327;&#21270;&#30340;&#25991;&#26412;&#25512;&#29702;&#38382;&#39064;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#27010;&#29575;&#25512;&#29702;&#23545;&#20110;&#20174;&#26085;&#24120;&#23545;&#35805;&#21040;&#21307;&#23398;&#20915;&#31574;&#31561;&#21508;&#31181;&#24773;&#22659;&#37117;&#24456;&#37325;&#35201;&#12290;&#23613;&#31649;LLMs&#22312;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#26377;&#25152;&#25913;&#36827;&#65292;&#20294;&#22312;&#27010;&#29575;&#25512;&#29702;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#26174;&#33879;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#36125;&#21494;&#26031;&#35821;&#35328;&#25512;&#29702;&#25968;&#25454;&#38598;&#65288;BLInD&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#27979;&#35797;LLMs&#27010;&#29575;&#25512;&#29702;&#33021;&#21147;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#26032;&#25968;&#25454;&#38598;&#26469;&#35814;&#32454;&#35828;&#26126;LLMs&#22312;&#28041;&#21450;&#27010;&#29575;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#30340;&#29305;&#23450;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#23558;&#38382;&#39064;&#26144;&#23556;&#21040;&#19981;&#21516;&#24418;&#24335;&#34920;&#31034;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;Python&#20195;&#30721;&#21644;&#27010;&#29575;&#25512;&#29702;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09614v1 Announce Type: cross  Abstract: This paper considers the challenges that Large Language Models (LLMs) face when reasoning over text that includes information involving uncertainty explicitly quantified via probability values. This type of reasoning is relevant to a variety of contexts ranging from everyday conversations to medical decision-making. Despite improvements in the mathematical reasoning capabilities of LLMs, they still exhibit significant difficulties when it comes to probabilistic reasoning. To deal with this problem, we first introduce the Bayesian Linguistic Inference Dataset (BLInD), a new dataset specifically designed to test the probabilistic reasoning capabilities of LLMs. We then leverage this new dataset to thoroughly illustrate the specific limitations of LLMs for tasks involving probabilistic reasoning and present several strategies that map the problem to different formal representations, including Python code, probabilistic inference algorithm
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35268;&#27169;&#21270;&#38544;&#31169;&#24863;&#30693;&#25163;&#35821;&#32763;&#35793;&#12290;&#25105;&#20204;&#21033;&#29992;&#33258;&#30417;&#30563;&#35270;&#39057;&#39044;&#35757;&#32451;&#21644;&#26377;&#30417;&#30563;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#39118;&#38505;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25163;&#35821;&#32763;&#35793;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09611</link><description>&lt;p&gt;
&#23454;&#29616;&#35268;&#27169;&#21270;&#38544;&#31169;&#24863;&#30693;&#25163;&#35821;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Towards Privacy-Aware Sign Language Translation at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35268;&#27169;&#21270;&#38544;&#31169;&#24863;&#30693;&#25163;&#35821;&#32763;&#35793;&#12290;&#25105;&#20204;&#21033;&#29992;&#33258;&#30417;&#30563;&#35270;&#39057;&#39044;&#35757;&#32451;&#21644;&#26377;&#30417;&#30563;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#39118;&#38505;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25163;&#35821;&#32763;&#35793;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#35821;&#32763;&#35793;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#26159;&#25968;&#25454;&#31232;&#32570;&#12290;&#30446;&#21069;&#22312;&#32593;&#32476;&#19978;&#21487;&#29992;&#30340;&#22823;&#37096;&#20998;&#25163;&#35821;&#25968;&#25454;&#30001;&#20110;&#32570;&#20047;&#23545;&#40784;&#30340;&#23383;&#24149;&#32780;&#26080;&#27861;&#29992;&#20110;&#35757;&#32451;&#30417;&#30563;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#32593;&#32476;&#25235;&#21462;&#30340;&#25968;&#25454;&#38598;&#26469;&#25193;&#23637;&#25163;&#35821;&#32763;&#35793;&#23384;&#22312;&#38544;&#31169;&#39118;&#38505;&#65292;&#22240;&#20026;&#20854;&#20013;&#21253;&#21547;&#29983;&#29289;&#29305;&#24449;&#20449;&#24687;&#65292;&#36127;&#36131;&#20219;&#22320;&#24320;&#21457;&#25163;&#35821;&#32763;&#35793;&#25216;&#26415;&#24212;&#35813;&#32771;&#34385;&#36825;&#19968;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35268;&#27169;&#21270;&#38544;&#31169;&#24863;&#30693;&#25163;&#35821;&#32763;&#35793;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SSVP-SLT&#65292;&#23427;&#21033;&#29992;&#21311;&#21517;&#21644;&#26410;&#27880;&#37322;&#30340;&#35270;&#39057;&#36827;&#34892;&#33258;&#30417;&#30563;&#35270;&#39057;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#21033;&#29992;&#32463;&#36807;&#31579;&#36873;&#30340;&#24179;&#34892;&#25968;&#25454;&#38598;&#36827;&#34892;&#26377;&#30417;&#30563;&#30340;&#25163;&#35821;&#32763;&#35793;&#24494;&#35843;&#12290; SSVP-SLT&#22312;How2Sign&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#24494;&#35843;&#21644;&#38646;&#27425;gloss-free&#25163;&#35821;&#32763;&#35793;&#24615;&#33021;&#65292;&#27604;&#26368;&#24378;&#30340;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;3&#20010;BLEU-4&#12290;&#36890;&#36807;&#21463;&#25511;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#35821;&#35328;&#21644;&#25163;&#35821;&#35789;&#27719;&#19978;&#37117;&#20855;&#26377;&#36739;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09611v1 Announce Type: new  Abstract: A major impediment to the advancement of sign language translation (SLT) is data scarcity. Much of the sign language data currently available on the web cannot be used for training supervised models due to the lack of aligned captions. Furthermore, scaling SLT using large-scale web-scraped datasets bears privacy risks due to the presence of biometric information, which the responsible development of SLT technologies should account for. In this work, we propose a two-stage framework for privacy-aware SLT at scale that addresses both of these issues. We introduce SSVP-SLT, which leverages self-supervised video pretraining on anonymized and unannotated videos, followed by supervised SLT finetuning on a curated parallel dataset. SSVP-SLT achieves state-of-the-art finetuned and zero-shot gloss-free SLT performance on the How2Sign dataset, outperforming the strongest respective baselines by over 3 BLEU-4. Based on controlled experiments, we fu
&lt;/p&gt;</description></item><item><title>LogicPrpBank&#26159;&#19968;&#20010;&#26032;&#30340;&#21629;&#39064;&#36923;&#36753;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#30740;&#31350;&#36923;&#36753;&#34164;&#21547;&#21644;&#31561;&#20215;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#19982;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#35813;&#35821;&#26009;&#24211;&#22312;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#36164;&#28304;&#65292;&#24182;&#19988;&#36824;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.09609</link><description>&lt;p&gt;
LogicPrpBank&#65306;&#19968;&#20010;&#29992;&#20110;&#36923;&#36753;&#34164;&#21547;&#21644;&#31561;&#20215;&#30340;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
LogicPrpBank: A Corpus for Logical Implication and Equivalence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09609
&lt;/p&gt;
&lt;p&gt;
LogicPrpBank&#26159;&#19968;&#20010;&#26032;&#30340;&#21629;&#39064;&#36923;&#36753;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#30740;&#31350;&#36923;&#36753;&#34164;&#21547;&#21644;&#31561;&#20215;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#19982;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#35813;&#35821;&#26009;&#24211;&#22312;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#36164;&#28304;&#65292;&#24182;&#19988;&#36824;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#25512;&#29702;&#22312;&#38382;&#39064;&#35299;&#20915;&#21644;&#20915;&#31574;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#35777;&#26126;&#20102;&#22788;&#29702;&#22810;&#31181;&#25512;&#29702;&#20219;&#21153;&#65288;&#20363;&#22914;&#24120;&#35782;&#25512;&#29702;&#65289;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#22797;&#26434;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#21629;&#39064;&#36923;&#36753;&#30340;&#25512;&#29702;&#33021;&#21147;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#36825;&#31181;&#25506;&#32034;&#30340;&#19981;&#36275;&#21487;&#20197;&#24402;&#22240;&#20110;&#26631;&#27880;&#35821;&#26009;&#24211;&#30340;&#26377;&#38480;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32463;&#36807;&#33391;&#22909;&#26631;&#27880;&#30340;&#21629;&#39064;&#36923;&#36753;&#35821;&#26009;&#24211;LogicPrpBank&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;7093&#20010;&#21629;&#39064;&#36923;&#36753;&#38472;&#36848;&#65288;PLS&#65289;&#28085;&#30422;&#20845;&#20010;&#25968;&#23398;&#31185;&#30446;&#65292;&#20197;&#30740;&#31350;&#25512;&#29702;&#36923;&#36753;&#34164;&#21547;&#21644;&#31561;&#20215;&#30340;&#20840;&#26032;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#24191;&#27867;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;LogicPrpBank&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#20026;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#36164;&#28304;&#65292;&#24182;&#19988;&#27169;&#22411;&#25913;&#36827;&#30340;&#31354;&#38388;&#36824;&#24456;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09609v1 Announce Type: cross  Abstract: Logic reasoning has been critically needed in problem-solving and decision-making. Although Language Models (LMs) have demonstrated capabilities of handling multiple reasoning tasks (e.g., commonsense reasoning), their ability to reason complex mathematical problems, specifically propositional logic, remains largely underexplored. This lack of exploration can be attributed to the limited availability of annotated corpora. Here, we present a well-labeled propositional logic corpus, LogicPrpBank, containing 7093 Propositional Logic Statements (PLSs) across six mathematical subjects, to study a brand-new task of reasoning logical implication and equivalence. We benchmark LogicPrpBank with widely-used LMs to show that our corpus offers a useful resource for this challenging task and there is ample room for model improvement.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33647;&#29289;&#20998;&#23376;&#21644;&#36866;&#24212;&#30151;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#30340;&#26426;&#36935;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#24182;&#27979;&#35797;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#36825;&#23545;&#20110;&#33647;&#29289;&#21457;&#29616;&#36807;&#31243;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.09588</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33647;&#29289;&#20998;&#23376;&#21644;&#36866;&#24212;&#30151;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#30340;&#26032;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Emerging Opportunities of Using Large Language Language Models for Translation Between Drug Molecules and Indications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33647;&#29289;&#20998;&#23376;&#21644;&#36866;&#24212;&#30151;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#30340;&#26426;&#36935;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#24182;&#27979;&#35797;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#36825;&#23545;&#20110;&#33647;&#29289;&#21457;&#29616;&#36807;&#31243;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#20998;&#23376;&#26159;&#19968;&#31181;&#25913;&#21464;&#29983;&#29289;&#20307;&#31934;&#31070;&#25110;&#36523;&#20307;&#29366;&#24577;&#30340;&#29289;&#36136;&#12290;&#27599;&#31181;&#25209;&#20934;&#30340;&#33647;&#29289;&#37117;&#26377;&#36866;&#24212;&#30151;&#65292;&#25351;&#30340;&#26159;&#35813;&#33647;&#29289;&#27835;&#30103;&#29305;&#23450;&#21307;&#30103;&#26465;&#20214;&#30340;&#27835;&#30103;&#29992;&#36884;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#19968;&#31181;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#26368;&#36817;&#24050;&#32463;&#35777;&#26126;&#22312;&#20998;&#23376;&#21644;&#20854;&#25991;&#26412;&#25551;&#36848;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#20173;&#23384;&#22312;&#20851;&#20110;&#22312;&#33647;&#29289;&#20998;&#23376;&#21644;&#36866;&#24212;&#30151;&#65288;&#25110;&#21453;&#20043;&#65289;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#30340;&#24212;&#29992;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#36825;&#21487;&#33021;&#26497;&#22823;&#22320;&#26377;&#30410;&#20110;&#33647;&#29289;&#21457;&#29616;&#36807;&#31243;&#12290;&#20174;&#32473;&#23450;&#30340;&#36866;&#24212;&#30151;&#29983;&#25104;&#33647;&#29289;&#30340;&#33021;&#21147;&#23558;&#20801;&#35768;&#21457;&#29616;&#38024;&#23545;&#29305;&#23450;&#30142;&#30149;&#25110;&#38774;&#28857;&#30340;&#33647;&#29289;&#65292;&#24182;&#26368;&#32456;&#20026;&#24739;&#32773;&#25552;&#20379;&#26356;&#22909;&#30340;&#27835;&#30103;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#33647;&#29289;&#20998;&#23376;&#21644;&#30456;&#24212;&#36866;&#24212;&#30151;&#20043;&#38388;&#30340;&#32763;&#35793;&#65292;&#28982;&#21518;&#36827;&#34892;&#20102;&#23454;&#39564;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09588v1 Announce Type: new  Abstract: A drug molecule is a substance that changes the organism's mental or physical state. Every approved drug has an indication, which refers to the therapeutic use of that drug for treating a particular medical condition. While the Large Language Model (LLM), a generative Artificial Intelligence (AI) technique, has recently demonstrated effectiveness in translating between molecules and their textual descriptions, there remains a gap in research regarding their application in facilitating the translation between drug molecules and indications, or vice versa, which could greatly benefit the drug discovery process. The capability of generating a drug from a given indication would allow for the discovery of drugs targeting specific diseases or targets and ultimately provide patients with better treatments. In this paper, we first propose a new task, which is the translation between drug molecules and corresponding indications, and then test exi
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32676;&#20307;&#20648;&#22791;&#36716;&#25442;&#22120;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#35299;&#20915;&#21382;&#21490;&#24207;&#21015;&#30340;&#25361;&#25112;&#21644;&#21021;&#22987;&#26465;&#20214;&#30340;&#25935;&#24863;&#24615;&#65292;&#23454;&#29616;&#26356;&#20934;&#30830;&#12289;&#26356;&#31283;&#20581;&#22320;&#39044;&#27979;&#38271;&#26399;&#20107;&#20214;&#12290;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;DNN&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#26368;&#39640;&#21487;&#20943;&#23569;89.43%&#30340;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.09573</link><description>&lt;p&gt;
&#34676;&#34678;&#24341;&#36215;&#30340;&#21464;&#21270;&#65306;&#21033;&#29992;&#32676;&#20307;&#20648;&#22791;&#36716;&#25442;&#22120;&#36827;&#34892;&#36828;&#35265;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Changes by Butterflies: Farsighted Forecasting with Group Reservoir Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09573
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32676;&#20307;&#20648;&#22791;&#36716;&#25442;&#22120;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#35299;&#20915;&#21382;&#21490;&#24207;&#21015;&#30340;&#25361;&#25112;&#21644;&#21021;&#22987;&#26465;&#20214;&#30340;&#25935;&#24863;&#24615;&#65292;&#23454;&#29616;&#26356;&#20934;&#30830;&#12289;&#26356;&#31283;&#20581;&#22320;&#39044;&#27979;&#38271;&#26399;&#20107;&#20214;&#12290;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;DNN&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#26368;&#39640;&#21487;&#20943;&#23569;89.43%&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28151;&#27788;&#20013;&#65292;&#20004;&#20010;&#21021;&#22987;&#26465;&#20214;&#20043;&#38388;&#30340;&#24494;&#23567;&#24046;&#24322;&#20250;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#21576;&#25351;&#25968;&#32423;&#25918;&#22823;&#65292;&#23548;&#33268;&#36965;&#36828;&#30340;&#32467;&#26524;&#65292;&#20063;&#34987;&#31216;&#20026;&#34676;&#34678;&#25928;&#24212;&#12290;&#22240;&#27492;&#65292;&#36828;&#26399;&#20805;&#28385;&#20102;&#19981;&#30830;&#23450;&#24615;&#65292;&#38590;&#20197;&#39044;&#27979;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32676;&#20307;&#20648;&#22791;&#36716;&#25442;&#22120;&#26469;&#36890;&#36807;&#20811;&#26381;&#28151;&#27788;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#65288;1&#65289;&#22823;&#37327;&#30340;&#21382;&#21490;&#24207;&#21015;&#21644;&#65288;2&#65289;&#23545;&#21021;&#22987;&#26465;&#20214;&#30340;&#25935;&#24863;&#24615;&#26469;&#26356;&#20934;&#30830;&#12289;&#26356;&#31283;&#20581;&#22320;&#39044;&#27979;&#38271;&#26399;&#20107;&#20214;&#12290;&#23558;&#19968;&#20010;&#20648;&#22791;&#35013;&#32622;&#36830;&#25509;&#21040;&#36716;&#25442;&#22120;&#19978;&#20197;&#39640;&#25928;&#22320;&#22788;&#29702;&#20219;&#24847;&#38271;&#24230;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#19968;&#32452;&#20648;&#22791;&#35013;&#32622;&#26469;&#20943;&#23569;&#30001;&#20110;&#21021;&#22987;&#21270;&#21464;&#21270;&#32780;&#20135;&#29983;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20013;&#22987;&#32456;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;DNN&#27169;&#22411;&#65292;&#21253;&#25324;NLinear&#12289;Pyformer&#12289;Informer&#12289;Autoformer&#21644;&#22522;&#20934;Transformer&#65292;&#20854;&#35823;&#24046;&#20943;&#23569;&#39640;&#36798;-89.43&#65285;&#65292;&#36866;&#29992;&#20110;ETTh&#12289;ETTm&#21644;&#31354;&#27668;&#36136;&#37327;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09573v1 Announce Type: cross  Abstract: In Chaos, a minor divergence between two initial conditions exhibits exponential amplification over time, leading to far-away outcomes, known as the butterfly effect. Thus, the distant future is full of uncertainty and hard to forecast. We introduce Group Reservoir Transformer to predict long-term events more accurately and robustly by overcoming two challenges in Chaos: (1) the extensive historical sequences and (2) the sensitivity to initial conditions. A reservoir is attached to a Transformer to efficiently handle arbitrarily long historical lengths, with an extension of a group of reservoirs to reduce the uncertainty due to the initialization variations. Our architecture consistently outperforms state-of-the-art DNN models in multivariate time series, including NLinear, Pyformer, Informer, Autoformer, and the baseline Transformer, with an error reduction of up to -89.43\% in various fields such as ETTh, ETTm, and air quality, demon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32463;&#27982;&#21512;&#29702;&#24615;&#26041;&#38754;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#37327;&#21270;&#35780;&#20998;&#27169;&#22411;&#22312;&#21508;&#20010;&#35201;&#32032;&#19978;&#30340;&#34920;&#29616;&#24182;&#32467;&#21512;&#29992;&#25143;&#25552;&#20379;&#30340;&#35780;&#20998;&#26631;&#20934;&#65292;&#29983;&#25104;&#19968;&#20221;"&#29702;&#24615;&#25253;&#21578;&#21345;"&#65292;&#20197;&#30830;&#23450;&#20195;&#29702;&#20154;&#26159;&#21542;&#36275;&#22815;&#21487;&#38752;&#12290;</title><link>https://arxiv.org/abs/2402.09552</link><description>&lt;p&gt;
&#29702;&#24615;&#25253;&#21578;&#21345;&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32463;&#27982;&#21512;&#29702;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rationality Report Cards: Assessing the Economic Rationality of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32463;&#27982;&#21512;&#29702;&#24615;&#26041;&#38754;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#37327;&#21270;&#35780;&#20998;&#27169;&#22411;&#22312;&#21508;&#20010;&#35201;&#32032;&#19978;&#30340;&#34920;&#29616;&#24182;&#32467;&#21512;&#29992;&#25143;&#25552;&#20379;&#30340;&#35780;&#20998;&#26631;&#20934;&#65292;&#29983;&#25104;&#19968;&#20221;"&#29702;&#24615;&#25253;&#21578;&#21345;"&#65292;&#20197;&#30830;&#23450;&#20195;&#29702;&#20154;&#26159;&#21542;&#36275;&#22815;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#23558;LLM&#29992;&#20316;&#20915;&#31574;"&#20195;&#29702;&#20154;"&#20852;&#36259;&#26085;&#30410;&#22686;&#21152;&#12290;&#36825;&#21253;&#25324;&#24456;&#22810;&#33258;&#30001;&#24230;&#65306;&#24212;&#35813;&#20351;&#29992;&#21738;&#20010;&#27169;&#22411;&#65307;&#22914;&#20309;&#36827;&#34892;&#25552;&#31034;&#65307;&#26159;&#21542;&#35201;&#27714;&#20854;&#36827;&#34892;&#20869;&#30465;&#12289;&#36827;&#34892;&#24605;&#32771;&#38142;&#31561;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65288;&#26356;&#24191;&#27867;&#22320;&#35828;&#65292;&#30830;&#23450;LLM&#20195;&#29702;&#20154;&#26159;&#21542;&#36275;&#22815;&#21487;&#38752;&#20197;&#20415;&#33719;&#24471;&#20449;&#20219;&#65289;&#38656;&#35201;&#19968;&#31181;&#35780;&#20272;&#36825;&#31181;&#20195;&#29702;&#20154;&#32463;&#27982;&#21512;&#29702;&#24615;&#30340;&#26041;&#27861;&#35770;&#65292;&#22312;&#26412;&#25991;&#20013;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#29702;&#24615;&#20915;&#31574;&#30340;&#32463;&#27982;&#25991;&#29486;&#36827;&#34892;&#20102;&#35843;&#30740;&#12289;&#23558;&#20195;&#29702;&#20154;&#24212;&#35813;&#23637;&#29616;&#30340;&#22823;&#37327;&#32454;&#31890;&#24230;"&#35201;&#32032;"&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#20998;&#24067;&#65292;&#20197;&#23450;&#37327;&#35780;&#20998;LLM&#22312;&#36825;&#20123;&#35201;&#32032;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#32467;&#21512;&#29992;&#25143;&#25552;&#20379;&#30340;&#35780;&#20998;&#26631;&#20934;&#65292;&#29983;&#25104;&#19968;&#20221;"&#29702;&#24615;&#25253;&#21578;&#21345;"&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19982;14&#31181;&#19981;&#21516;&#30340;LLM&#36827;&#34892;&#30340;&#22823;&#35268;&#27169;&#23454;&#35777;&#23454;&#39564;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09552v1 Announce Type: new  Abstract: There is increasing interest in using LLMs as decision-making "agents." Doing so includes many degrees of freedom: which model should be used; how should it be prompted; should it be asked to introspect, conduct chain-of-thought reasoning, etc? Settling these questions -- and more broadly, determining whether an LLM agent is reliable enough to be trusted -- requires a methodology for assessing such an agent's economic rationality. In this paper, we provide one. We begin by surveying the economic literature on rational decision making, taxonomizing a large set of fine-grained "elements" that an agent should exhibit, along with dependencies between them. We then propose a benchmark distribution that quantitatively scores an LLMs performance on these elements and, combined with a user-provided rubric, produces a "rationality report card." Finally, we describe the results of a large-scale empirical experiment with 14 different LLMs, characte
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20195;&#29702;&#30340;&#38544;&#24335;&#29992;&#25143;&#24847;&#22270;&#29702;&#35299;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;Intention-in-Interaction (IN3) &#22522;&#20934;&#21644;&#22312;&#20195;&#29702;&#35774;&#35745;&#20013;&#34701;&#20837;&#27169;&#22411;&#19987;&#23478;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#26356;&#22909;&#22320;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#25552;&#21319;&#23545;&#29992;&#25143;&#25351;&#20196;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.09205</link><description>&lt;p&gt;
&#21578;&#35785;&#25105;&#26356;&#22810;&#65281;&#38754;&#21521;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20195;&#29702;&#30340;&#38544;&#24335;&#29992;&#25143;&#24847;&#22270;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Tell Me More! Towards Implicit User Intention Understanding of Language Model Driven Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09205
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20195;&#29702;&#30340;&#38544;&#24335;&#29992;&#25143;&#24847;&#22270;&#29702;&#35299;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;Intention-in-Interaction (IN3) &#22522;&#20934;&#21644;&#22312;&#20195;&#29702;&#35774;&#35745;&#20013;&#34701;&#20837;&#27169;&#22411;&#19987;&#23478;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#26356;&#22909;&#22320;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#25552;&#21319;&#23545;&#29992;&#25143;&#25351;&#20196;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#20195;&#29702;&#24120;&#24120;&#32570;&#20047;&#26377;&#25928;&#30340;&#29992;&#25143;&#21442;&#19982;&#26426;&#21046;&#65292;&#32771;&#34385;&#21040;&#29992;&#25143;&#25351;&#20196;&#20013;&#24120;&#35265;&#30340;&#27169;&#31946;&#24615;&#65292;&#36825;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#34429;&#28982;&#36825;&#20123;&#20195;&#29702;&#22312;&#21046;&#23450;&#31574;&#30053;&#21644;&#25191;&#34892;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#23547;&#27714;&#28548;&#28165;&#21644;&#25235;&#20303;&#31934;&#30830;&#30340;&#29992;&#25143;&#24847;&#22270;&#26041;&#38754;&#21364;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Intention-in-Interaction (IN3) &#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#26126;&#30830;&#30340;&#26597;&#35810;&#26816;&#26597;&#29992;&#25143;&#30340;&#38544;&#21547;&#24847;&#22270;&#30340;&#26032;&#39062;&#22522;&#20934;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#27169;&#22411;&#19987;&#23478;&#20316;&#20026;&#19978;&#28216;&#34701;&#20837;&#20195;&#29702;&#35774;&#35745;&#20013;&#65292;&#20197;&#22686;&#24378;&#29992;&#25143;-&#20195;&#29702;&#20132;&#20114;&#12290;&#21033;&#29992;IN3&#65292;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35757;&#32451;&#20102;Mistral-Interact&#65292;&#36825;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#20027;&#21160;&#35780;&#20272;&#20219;&#21153;&#30340;&#27169;&#31946;&#24615;&#65292;&#35810;&#38382;&#29992;&#25143;&#24847;&#22270;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#21487;&#34892;&#30340;&#30446;&#26631;&#65292;&#28982;&#21518;&#24320;&#22987;&#19979;&#28216;&#20195;&#29702;&#20219;&#21153;&#25191;&#34892;&#12290;&#23558;&#20854;&#38598;&#25104;&#21040;XAgent&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#23545;&#22686;&#24378;&#30340;&#20195;&#29702;&#31995;&#32479;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#29992;&#25143;&#25351;&#20196;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09205v1 Announce Type: cross Abstract: Current language model-driven agents often lack mechanisms for effective user participation, which is crucial given the vagueness commonly found in user instructions. Although adept at devising strategies and performing tasks, these agents struggle with seeking clarification and grasping precise user intentions. To bridge this gap, we introduce Intention-in-Interaction (IN3), a novel benchmark designed to inspect users' implicit intentions through explicit queries. Next, we propose the incorporation of model experts as the upstream in agent designs to enhance user-agent interaction. Employing IN3, we empirically train Mistral-Interact, a powerful model that proactively assesses task vagueness, inquires user intentions, and refines them into actionable goals before starting downstream agent task execution. Integrating it into the XAgent framework, we comprehensively evaluate the enhanced agent system regarding user instruction understand
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19971;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35748;&#30693;&#24515;&#29702;&#23398;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#19982;&#20154;&#31867;&#19968;&#26679;&#23384;&#22312;&#38750;&#29702;&#24615;&#65292;&#20294;&#23637;&#31034;&#30340;&#38750;&#29702;&#24615;&#26041;&#24335;&#19982;&#20154;&#31867;&#20559;&#35265;&#19981;&#21516;&#65292;&#21516;&#26102;&#36824;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#22238;&#31572;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09193</link><description>&lt;p&gt;
(&#19981;)&#29702;&#24615;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
(Ir)rationality and Cognitive Biases in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19971;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35748;&#30693;&#24515;&#29702;&#23398;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#19982;&#20154;&#31867;&#19968;&#26679;&#23384;&#22312;&#38750;&#29702;&#24615;&#65292;&#20294;&#23637;&#31034;&#30340;&#38750;&#29702;&#24615;&#26041;&#24335;&#19982;&#20154;&#31867;&#20559;&#35265;&#19981;&#21516;&#65292;&#21516;&#26102;&#36824;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#22238;&#31572;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#21542;&#23637;&#29616;&#20986;&#29702;&#24615;&#25512;&#29702;&#65311;&#30001;&#20110;&#20854;&#35757;&#32451;&#25968;&#25454;&#25152;&#21547;&#30340;&#20154;&#31867;&#20559;&#35265;&#65292;LLMs&#24050;&#34987;&#35777;&#23454;&#23384;&#22312;&#20154;&#31867;&#20559;&#35265;&#65307;&#28982;&#32780;&#65292;&#20854;&#26159;&#21542;&#21453;&#26144;&#20986;&#20102;&#29702;&#24615;&#25512;&#29702;&#36824;&#19981;&#22826;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#35780;&#20272;&#19971;&#20010;&#35821;&#35328;&#27169;&#22411;&#22312;&#26469;&#33258;&#35748;&#30693;&#24515;&#29702;&#23398;&#25991;&#29486;&#30340;&#20219;&#21153;&#20013;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21644;&#20154;&#31867;&#19968;&#26679;&#65292;LLMs&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#38750;&#29702;&#24615;&#12290;&#28982;&#32780;&#65292;LLMs&#23637;&#29616;&#20986;&#30340;&#36825;&#31181;&#38750;&#29702;&#24615;&#19982;&#20154;&#31867;&#30340;&#20559;&#35265;&#19981;&#21516;&#12290;&#24403;LLMs&#32473;&#20986;&#38169;&#35823;&#31572;&#26696;&#26102;&#65292;&#23427;&#20204;&#36890;&#24120;&#20250;&#20197;&#19982;&#20154;&#31867;&#20559;&#35265;&#19981;&#21516;&#30340;&#26041;&#24335;&#38169;&#35823;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;LLMs&#36824;&#23637;&#29616;&#20986;&#20102;&#21709;&#24212;&#30340;&#26174;&#33879;&#19981;&#19968;&#33268;&#24615;&#65292;&#36825;&#34920;&#26126;&#20102;&#39069;&#22806;&#30340;&#38750;&#29702;&#24615;&#23618;&#38754;&#12290;&#38500;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#26412;&#25991;&#36824;&#36890;&#36807;&#23637;&#31034;&#22914;&#20309;&#35780;&#20272;&#21644;&#27604;&#36739;&#36825;&#31867;&#27169;&#22411;&#30340;&#19981;&#21516;&#21151;&#33021;&#65292;&#23545;&#26041;&#27861;&#35770;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09193v1 Announce Type: cross Abstract: Do large language models (LLMs) display rational reasoning? LLMs have been shown to contain human biases due to the data they have been trained on; whether this is reflected in rational reasoning remains less clear. In this paper, we answer this question by evaluating seven language models using tasks from the cognitive psychology literature. We find that, like humans, LLMs display irrationality in these tasks. However, the way this irrationality is displayed does not reflect that shown by humans. When incorrect answers are given by LLMs to these tasks, they are often incorrect in ways that differ from human-like biases. On top of this, the LLMs reveal an additional layer of irrationality in the significant inconsistency of the responses. Aside from the experimental results, this paper seeks to make a methodological contribution by showing how we can assess and compare different capabilities of these types of models, in this case with r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;AgentEval&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#39537;&#21160;&#24212;&#29992;&#30340;&#20219;&#21153;&#25928;&#29992;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#21160;&#25552;&#20986;&#19968;&#22871;&#38024;&#23545;&#29305;&#23450;&#24212;&#29992;&#30340;&#35780;&#20272;&#26631;&#20934;&#65292;&#31616;&#21270;&#20102;&#25928;&#29992;&#39564;&#35777;&#36807;&#31243;&#65292;&#24182;&#23545;&#24212;&#29992;&#30340;&#25928;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#37327;&#21270;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.09015</link><description>&lt;p&gt;
&#26397;&#30528;&#26356;&#22909;&#30340;&#20154;&#26426;&#23545;&#40784;&#26041;&#21521;&#65306;&#35780;&#20272;LLM&#39537;&#21160;&#24212;&#29992;&#20013;&#30340;&#20219;&#21153;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;AgentEval&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#39537;&#21160;&#24212;&#29992;&#30340;&#20219;&#21153;&#25928;&#29992;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#21160;&#25552;&#20986;&#19968;&#22871;&#38024;&#23545;&#29305;&#23450;&#24212;&#29992;&#30340;&#35780;&#20272;&#26631;&#20934;&#65292;&#31616;&#21270;&#20102;&#25928;&#29992;&#39564;&#35777;&#36807;&#31243;&#65292;&#24182;&#23545;&#24212;&#29992;&#30340;&#25928;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#37327;&#21270;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#23548;&#33268;&#20102;&#19968;&#31995;&#21015;&#24212;&#29992;&#30340;&#20986;&#29616;&#65292;&#36825;&#20123;&#24212;&#29992;&#36890;&#36807;&#21327;&#21161;&#22810;&#20010;&#20195;&#29702;&#20154;&#19982;&#20154;&#31867;&#21512;&#20316;&#65292;&#24110;&#21161;&#20154;&#20204;&#23436;&#25104;&#26085;&#24120;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20173;&#23384;&#22312;&#19968;&#20010;&#37325;&#22823;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#35780;&#20272;LLM&#39537;&#21160;&#24212;&#29992;&#26159;&#21542;&#30495;&#27491;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#21644;&#20219;&#21153;&#25191;&#34892;&#25928;&#29575;&#12290;&#36825;&#20984;&#26174;&#20102;&#39564;&#35777;LLM&#39537;&#21160;&#24212;&#29992;&#25928;&#29992;&#30340;&#26041;&#27861;&#30340;&#36843;&#20999;&#38656;&#27714;&#65292;&#29305;&#21035;&#26159;&#35201;&#30830;&#20445;&#24212;&#29992;&#31243;&#24207;&#30340;&#21151;&#33021;&#19982;&#26368;&#32456;&#29992;&#25143;&#30340;&#38656;&#27714;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AgentEval&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#26045;&#25968;&#23398;&#38382;&#39064;&#30340;&#20272;&#27979;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#21160;&#25552;&#20986;&#19968;&#22871;&#38024;&#23545;&#20219;&#20309;&#32473;&#23450;&#24212;&#29992;&#31243;&#24207;&#29420;&#29305;&#30446;&#26631;&#30340;&#35780;&#20272;&#26631;&#20934;&#65292;&#31616;&#21270;&#25928;&#29992;&#39564;&#35777;&#36807;&#31243;&#12290;&#36825;&#26679;&#21487;&#20197;&#23545;&#24212;&#29992;&#31243;&#24207;&#30340;&#25928;&#29992;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#37327;&#21270;&#20854;&#19982;&#24314;&#35758;&#26631;&#20934;&#30456;&#27604;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#23545;&#35813;&#26694;&#26550;&#30340;&#31283;&#20581;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09015v1 Announce Type: cross Abstract: The rapid development in the field of Large Language Models (LLMs) has led to a surge in applications that facilitate collaboration among multiple agents to assist humans in their daily tasks. However, a significant gap remains in assessing whether LLM-powered applications genuinely enhance user experience and task execution efficiency. This highlights the pressing need for methods to verify utility of LLM-powered applications, particularly by ensuring alignment between the application's functionality and end-user needs. We introduce AgentEval provides an implementation for the math problems}, a novel framework designed to simplify the utility verification process by automatically proposing a set of criteria tailored to the unique purpose of any given application. This allows for a comprehensive assessment, quantifying the utility of an application against the suggested criteria. We present a comprehensive analysis of the robustness of 
&lt;/p&gt;</description></item><item><title>DNABERT-S&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#21019;&#24314;&#29289;&#31181;&#24863;&#30693;&#30340;DNA&#23884;&#20837;&#30340;&#22522;&#22240;&#32452;&#22522;&#30784;&#27169;&#22411;&#12290;&#20026;&#20102;&#25552;&#39640;&#23545;&#38271;&#35835;DNA&#24207;&#21015;&#30340;&#23884;&#20837;&#25928;&#26524;&#65292;&#24341;&#20837;&#20102;Manifold Instance Mixup (MI-Mix)&#23545;&#27604;&#30446;&#26631;&#26041;&#27861;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.08777</link><description>&lt;p&gt;
DNABERT-S: &#23398;&#20064;&#20855;&#26377;&#22522;&#22240;&#32452;&#22522;&#30784;&#27169;&#22411;&#30340;&#29289;&#31181;&#24863;&#30693;DNA&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
DNABERT-S: Learning Species-Aware DNA Embedding with Genome Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08777
&lt;/p&gt;
&lt;p&gt;
DNABERT-S&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#21019;&#24314;&#29289;&#31181;&#24863;&#30693;&#30340;DNA&#23884;&#20837;&#30340;&#22522;&#22240;&#32452;&#22522;&#30784;&#27169;&#22411;&#12290;&#20026;&#20102;&#25552;&#39640;&#23545;&#38271;&#35835;DNA&#24207;&#21015;&#30340;&#23884;&#20837;&#25928;&#26524;&#65292;&#24341;&#20837;&#20102;Manifold Instance Mixup (MI-Mix)&#23545;&#27604;&#30446;&#26631;&#26041;&#27861;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;DNA&#23884;&#20837;&#22312;&#22522;&#22240;&#32452;&#20998;&#26512;&#20013;&#20173;&#28982;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#32570;&#20047;&#29992;&#20110;&#27169;&#22411;&#24494;&#35843;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23613;&#31649;&#22522;&#22240;&#32452;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#20363;&#23376;&#26159;&#23439;&#22522;&#22240;&#32452;&#20998;&#31665;&#65292;&#36825;&#26159;&#24494;&#29983;&#29289;&#32452;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#36807;&#31243;&#65292;&#26088;&#22312;&#36890;&#36807;&#26469;&#33258;&#21487;&#33021;&#21253;&#21547;&#25104;&#21315;&#19978;&#19975;&#20010;&#19981;&#21516;&#30340;&#12289;&#36890;&#24120;&#27809;&#26377;&#32463;&#36807;&#34920;&#24449;&#30340;&#29289;&#31181;&#30340;&#22797;&#26434;&#28151;&#21512;DNA&#24207;&#21015;&#30340;&#29289;&#31181;&#26469;&#23545;DNA&#24207;&#21015;&#36827;&#34892;&#20998;&#32452;&#12290;&#20026;&#20102;&#22635;&#34917;&#26377;&#25928;&#30340;DNA&#23884;&#20837;&#27169;&#22411;&#30340;&#32570;&#38519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DNABERT-S&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#21019;&#24314;&#29289;&#31181;&#24863;&#30693;&#30340;DNA&#23884;&#20837;&#30340;&#22522;&#22240;&#32452;&#22522;&#30784;&#27169;&#22411;&#12290;&#20026;&#20102;&#40723;&#21169;&#23545;&#26131;&#20986;&#38169;&#30340;&#38271;&#35835;DNA&#24207;&#21015;&#36827;&#34892;&#26377;&#25928;&#23884;&#20837;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Manifold Instance Mixup(MI-Mix)&#65292;&#19968;&#31181;&#23545;&#27604;&#30446;&#26631;&#65292;&#23427;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#23618;&#27425;&#20013;&#28151;&#21512;DNA&#24207;&#21015;&#30340;&#38544;&#34255;&#34920;&#31034;&#65292;&#24182;&#35757;&#32451;&#27169;&#22411;&#20197;&#22312;&#36755;&#20986;&#23618;&#35782;&#21035;&#21644;&#21306;&#20998;&#36825;&#20123;&#28151;&#21512;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08777v1 Announce Type: cross Abstract: Effective DNA embedding remains crucial in genomic analysis, particularly in scenarios lacking labeled data for model fine-tuning, despite the significant advancements in genome foundation models. A prime example is metagenomics binning, a critical process in microbiome research that aims to group DNA sequences by their species from a complex mixture of DNA sequences derived from potentially thousands of distinct, often uncharacterized species. To fill the lack of effective DNA embedding models, we introduce DNABERT-S, a genome foundation model that specializes in creating species-aware DNA embeddings. To encourage effective embeddings to error-prone long-read DNA sequences, we introduce Manifold Instance Mixup (MI-Mix), a contrastive objective that mixes the hidden representations of DNA sequences at randomly selected layers and trains the model to recognize and differentiate these mixed proportions at the output layer. We further enha
&lt;/p&gt;</description></item><item><title>SemRel2024&#26159;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;14&#31181;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#25968;&#25454;&#38598;&#21512;&#65292;&#36890;&#36807;&#35813;&#25968;&#25454;&#38598;&#21512;&#21487;&#20197;&#25506;&#32034;&#21644;&#37327;&#21270;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#36825;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#24615;&#33021;&#26377;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21512;&#28085;&#30422;&#20102;&#21335;&#38750;&#33655;&#20848;&#35821;&#12289;&#38463;&#23572;&#21450;&#21033;&#20122;&#38463;&#25289;&#20271;&#35821;&#12289;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#33521;&#35821;&#12289;&#35946;&#33832;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#12289;&#21346;&#26106;&#36798;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#25705;&#27931;&#21733;&#38463;&#25289;&#20271;&#35821;&#12289;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#12289;&#26049;&#36974;&#26222;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#31561;14&#31181;&#35821;&#35328;&#12290;</title><link>https://arxiv.org/abs/2402.08638</link><description>&lt;p&gt;
SemRel2024: 14&#31181;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#25968;&#25454;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14 Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08638
&lt;/p&gt;
&lt;p&gt;
SemRel2024&#26159;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;14&#31181;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#25968;&#25454;&#38598;&#21512;&#65292;&#36890;&#36807;&#35813;&#25968;&#25454;&#38598;&#21512;&#21487;&#20197;&#25506;&#32034;&#21644;&#37327;&#21270;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#36825;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#24615;&#33021;&#26377;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21512;&#28085;&#30422;&#20102;&#21335;&#38750;&#33655;&#20848;&#35821;&#12289;&#38463;&#23572;&#21450;&#21033;&#20122;&#38463;&#25289;&#20271;&#35821;&#12289;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#33521;&#35821;&#12289;&#35946;&#33832;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#12289;&#21346;&#26106;&#36798;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#25705;&#27931;&#21733;&#38463;&#25289;&#20271;&#35821;&#12289;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#12289;&#26049;&#36974;&#26222;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#31561;14&#31181;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#21644;&#37327;&#21270;&#35821;&#20041;&#30456;&#20851;&#24615;&#26159;&#35821;&#35328;&#34920;&#36798;&#30340;&#26680;&#24515;&#12290;&#23427;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#21253;&#25324;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#21644;&#24615;&#33021;&#25552;&#20379;&#27934;&#23519;&#12290;&#34429;&#28982;&#26089;&#26399;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#19978;&#65292;&#24448;&#24448;&#26159;&#22312;&#33521;&#35821;&#35821;&#22659;&#20013;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#26356;&#24191;&#27867;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#29616;&#35937;&#20540;&#24471;&#30740;&#31350;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SemRel&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#27597;&#35821;&#20026;14&#31181;&#35821;&#35328;&#36827;&#34892;&#27880;&#37322;&#30340;&#26032;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#25968;&#25454;&#38598;&#21512;&#65306;&#21335;&#38750;&#33655;&#20848;&#35821;&#12289;&#38463;&#23572;&#21450;&#21033;&#20122;&#38463;&#25289;&#20271;&#35821;&#12289;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#33521;&#35821;&#12289;&#35946;&#33832;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#12289;&#21346;&#26106;&#36798;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#25705;&#27931;&#21733;&#38463;&#25289;&#20271;&#35821;&#12289;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#12289;&#26049;&#36974;&#26222;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#12290;&#36825;&#20123;&#35821;&#35328;&#26469;&#33258;&#20116;&#20010;&#19981;&#21516;&#30340;&#35821;&#31995;&#65292;&#20027;&#35201;&#22312;&#38750;&#27954;&#21644;&#20122;&#27954;&#20351;&#29992;&#65292;&#36825;&#20123;&#22320;&#21306;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#30456;&#23545;&#36739;&#23569;&#12290;SemRel&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#23454;&#20363;&#37117;&#26159;&#19982;&#19968;&#20010;&#34920;&#31034;&#30456;&#20851;&#24615;&#24471;&#20998;&#30340;&#21477;&#23376;&#23545;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring and quantifying semantic relatedness is central to representing language. It holds significant implications across various NLP tasks, including offering insights into the capabilities and performance of Large Language Models (LLMs). While earlier NLP research primarily focused on semantic similarity, often within the English language context, we instead investigate the broader phenomenon of semantic relatedness. In this paper, we present SemRel, a new semantic relatedness dataset collection annotated by native speakers across 14 languages:Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic, Punjabi, Spanish, and Telugu. These languages originate from five distinct language families and are predominantly spoken in Africa and Asia -- regions characterised by a relatively limited availability of NLP resources. Each instance in the SemRel datasets is a sentence pair associated with a score that repr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20998;&#31867;&#22120;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#36755;&#20837;&#25552;&#31034;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#21464;&#21270;&#65292;&#20197;&#22686;&#21152;&#20854;&#36879;&#26126;&#24230;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#26681;&#25454;&#36755;&#20837;&#30340;&#19981;&#21516;&#25552;&#31034;&#32780;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#20154;&#26684;&#29305;&#36136;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#23545;&#21050;&#28608;&#20570;&#20986;&#30340;&#21453;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.08341</link><description>&lt;p&gt;
&#29992;&#20998;&#31867;&#22120;&#39537;&#21160;&#30340;&#26041;&#27861;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#65306;&#25991;&#26412;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Eliciting Big Five Personality Traits in Large Language Models: A Textual Analysis with Classifier-Driven Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20998;&#31867;&#22120;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#36755;&#20837;&#25552;&#31034;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#21464;&#21270;&#65292;&#20197;&#22686;&#21152;&#20854;&#36879;&#26126;&#24230;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#26681;&#25454;&#36755;&#20837;&#30340;&#19981;&#21516;&#25552;&#31034;&#32780;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#20154;&#26684;&#29305;&#36136;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#23545;&#21050;&#28608;&#20570;&#20986;&#30340;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25307;&#32856;&#32972;&#26223;&#19979;&#34987;&#24212;&#32856;&#32773;&#21644;&#38599;&#20027;&#24191;&#27867;&#20351;&#29992;&#65292;&#28982;&#32780;&#36825;&#20063;&#24341;&#21457;&#20102;&#20247;&#22810;&#20262;&#29702;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#19982;&#36825;&#20123;&#8220;&#40657;&#30418;&#23376;&#8221;&#27169;&#22411;&#32570;&#20047;&#36879;&#26126;&#24230;&#26377;&#20851;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#35797;&#22270;&#36890;&#36807;&#35843;&#26597;LLMs&#30340;&#20154;&#26684;&#29305;&#36136;&#26469;&#22686;&#21152;&#20854;&#36879;&#26126;&#24230;&#65292;&#20294;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#37117;&#35201;&#27714;&#27169;&#22411;&#26469;&#23436;&#25104;&#20154;&#26684;&#35780;&#20272;&#12290;&#30456;&#21453;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#26816;&#26597;&#19981;&#21516;&#36755;&#20837;&#25552;&#31034;&#19979;&#27169;&#22411;&#30340;&#36755;&#20986;&#21464;&#21270;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;&#24120;&#35265;&#38754;&#35797;&#38382;&#39064;&#21644;&#26088;&#22312;&#24341;&#21457;&#29305;&#23450;&#30340;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#30340;&#25552;&#31034;&#26469;&#36827;&#34892;&#26032;&#39062;&#30340;&#35843;&#26597;&#26041;&#27861;&#65292;&#20197;&#26816;&#26597;&#27169;&#22411;&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#23481;&#26131;&#28608;&#27963;&#29305;&#23450;&#20154;&#26684;&#29305;&#36136;&#65292;&#24182;&#26681;&#25454;&#20854;&#36755;&#20986;&#20013;&#30340;&#35821;&#35328;&#26469;&#35780;&#20272;&#20854;&#20154;&#26684;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21453;&#22797;&#25552;&#20379;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly being utilized by both candidates and employers in the recruitment context. However, with this comes numerous ethical concerns, particularly related to the lack of transparency in these "black-box" models. Although previous studies have sought to increase the transparency of these models by investigating the personality traits of LLMs, many of the previous studies have provided them with personality assessments to complete. On the other hand, this study seeks to obtain a better understanding of such models by examining their output variations based on different input prompts. Specifically, we use a novel elicitation approach using prompts derived from common interview questions, as well as prompts designed to elicit particular Big Five personality traits to examine whether the models were susceptible to trait-activation like humans are, to measure their personality based on the language used in their outputs. To do so, we repeatedly prompte
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;10&#19975;&#23567;&#26102;&#25968;&#25454;&#30340;10&#20159;&#21442;&#25968;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;BASE TTS&#22312;&#35821;&#38899;&#33258;&#28982;&#24230;&#19978;&#36798;&#21040;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#19988;&#33021;&#22815;&#23637;&#29616;&#33258;&#28982;&#30340;&#38901;&#24459;&#12290;</title><link>https://arxiv.org/abs/2402.08093</link><description>&lt;p&gt;
&#22522;&#20110;10&#19975;&#23567;&#26102;&#25968;&#25454;&#30340;10&#20159;&#21442;&#25968;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#30340;&#32463;&#39564;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08093
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;10&#19975;&#23567;&#26102;&#25968;&#25454;&#30340;10&#20159;&#21442;&#25968;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;BASE TTS&#22312;&#35821;&#38899;&#33258;&#28982;&#24230;&#19978;&#36798;&#21040;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#19988;&#33021;&#22815;&#23637;&#29616;&#33258;&#28982;&#30340;&#38901;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;BASE TTS&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#65292;&#20854;&#20013;BASE&#20195;&#34920;&#22823;&#35268;&#27169;&#33258;&#36866;&#24212;&#21487;&#27969;&#24335;TTS&#21644;&#26032;&#20986;&#29616;&#30340;&#33021;&#21147;&#12290;BASE TTS&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;TTS&#27169;&#22411;&#65292;&#35757;&#32451;&#20110;10&#19975;&#23567;&#26102;&#30340;&#20844;&#20849;&#39046;&#22495;&#35821;&#38899;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#35821;&#38899;&#33258;&#28982;&#24230;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;&#23427;&#37319;&#29992;&#20102;&#19968;&#20010;10&#20159;&#21442;&#25968;&#30340;&#33258;&#22238;&#24402;Transformer&#65292;&#23558;&#21407;&#22987;&#25991;&#26412;&#36716;&#25442;&#20026;&#31163;&#25955;&#20195;&#30721;&#65288;"speechcodes"&#65289;&#65292;&#28982;&#21518;&#36890;&#36807;&#22522;&#20110;&#21367;&#31215;&#30340;&#35299;&#30721;&#22120;&#23558;&#36825;&#20123;speechcodes&#20197;&#22686;&#37327;&#12289;&#21487;&#27969;&#24335;&#30340;&#26041;&#24335;&#36716;&#25442;&#20026;&#27874;&#24418;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;speechcodes&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#38899;&#26631;&#35760;&#21270;&#25216;&#26415;&#65292;&#20855;&#26377;&#35828;&#35805;&#32773;ID&#35299;&#32806;&#21644;&#23383;&#33410;&#23545;&#32534;&#30721;&#30340;&#21387;&#32553;&#29305;&#24615;&#12290;&#19982;&#22823;&#37327;&#25968;&#25454;&#35757;&#32451;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#24191;&#27867;&#25253;&#36947;&#30340;"&#26032;&#20986;&#29616;&#30340;&#33021;&#21147;"&#31867;&#20284;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;10K+&#23567;&#26102;&#21644;500M+&#21442;&#25968;&#26500;&#24314;&#30340;BASE TTS&#21464;&#20307;&#22312;&#25991;&#26412;&#22797;&#26434;&#21477;&#23376;&#19978;&#24320;&#22987;&#23637;&#29616;&#33258;&#28982;&#30340;&#38901;&#24459;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;...
&lt;/p&gt;
&lt;p&gt;
We introduce a text-to-speech (TTS) model called BASE TTS, which stands for $\textbf{B}$ig $\textbf{A}$daptive $\textbf{S}$treamable TTS with $\textbf{E}$mergent abilities. BASE TTS is the largest TTS model to-date, trained on 100K hours of public domain speech data, achieving a new state-of-the-art in speech naturalness. It deploys a 1-billion-parameter autoregressive Transformer that converts raw texts into discrete codes ("speechcodes") followed by a convolution-based decoder which converts these speechcodes into waveforms in an incremental, streamable manner. Further, our speechcodes are built using a novel speech tokenization technique that features speaker ID disentanglement and compression with byte-pair encoding. Echoing the widely-reported "emergent abilities" of large language models when trained on increasing volume of data, we show that BASE TTS variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences. We design
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;&#24182;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.07876</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#26469;&#25913;&#36827;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
Policy Improvement using Language Feedback Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;&#24182;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#65292;&#29992;&#20110;&#22312;&#25351;&#20196;&#36981;&#24490;&#20013;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;-&#26377;&#21161;&#20110;&#23454;&#29616;&#25351;&#20196;&#20013;&#25351;&#23450;&#20219;&#21153;&#30340;&#34892;&#21160;-&#20197;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#12290;&#20026;&#20102;&#35757;&#32451;LFMs&#65292;&#25105;&#20204;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33719;&#21462;&#23545;&#35270;&#35273;&#36712;&#36857;&#36827;&#34892;&#35821;&#35328;&#25551;&#36848;&#30340;&#21453;&#39304;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#20351;&#29992;LFMs&#35782;&#21035;&#26399;&#26395;&#27169;&#20223;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#22522;&#30784;&#29615;&#22659;&#65288;Touchdown&#65292;ScienceWorld&#21644;ALFWorld&#65289;&#19978;&#65292;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#19978;&#25913;&#21892;&#20102;&#24378;&#34892;&#20026;&#20811;&#38534;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#19982;LLMs&#30452;&#25509;&#39044;&#27979;&#34892;&#21160;&#30456;&#27604;&#65292;&#20351;&#29992;LFMs&#22312;LLM&#36755;&#20986;&#26631;&#35760;&#30340;&#25968;&#37327;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;&#31532;&#19977;&#65292;LFMs&#36866;&#24212;&#26410;&#35265;&#29615;&#22659;&#65292;&#36890;&#36807;&#19968;&#36718;&#36866;&#24212;&#20351;&#20219;&#21153;&#23436;&#25104;&#29575;&#25552;&#39640;&#20102;3.5-12.0&#65285;&#12290;&#26368;&#21518;&#65292;&#21487;&#20197;&#20462;&#25913;LFM&#20197;&#25552;&#20379;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#21453;&#39304;&#65292;&#26080;&#38656;&#24615;&#33021;&#25439;&#22833;&#65292;&#20174;&#32780;&#20801;&#35768;&#20154;&#31867;&#39564;&#35777;&#27169;&#20223;&#23398;&#20064;&#30340;&#26399;&#26395;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#21644;&#24378;&#21270;&#23398;&#20064;&#21407;&#21017;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#24378;&#21270;&#23398;&#20064;&#65288;NLRL&#65289;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#26679;&#26412;&#25928;&#29575;&#20302;&#12289;&#35299;&#37322;&#24615;&#19981;&#36275;&#21644;&#32570;&#20047;&#30417;&#30563;&#20449;&#21495;&#31561;&#26041;&#38754;&#30340;&#38480;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07157</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Natural Language Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#21644;&#24378;&#21270;&#23398;&#20064;&#21407;&#21017;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#24378;&#21270;&#23398;&#20064;&#65288;NLRL&#65289;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#26679;&#26412;&#25928;&#29575;&#20302;&#12289;&#35299;&#37322;&#24615;&#19981;&#36275;&#21644;&#32570;&#20047;&#30417;&#30563;&#20449;&#21495;&#31561;&#26041;&#38754;&#30340;&#38480;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#23398;&#20064;&#20915;&#31574;&#20219;&#21153;&#30340;&#31574;&#30053;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;RL&#24120;&#24120;&#38754;&#20020;&#26679;&#26412;&#25928;&#29575;&#20302;&#12289;&#35299;&#37322;&#24615;&#19981;&#36275;&#21644;&#32570;&#20047;&#31232;&#30095;&#30417;&#30563;&#20449;&#21495;&#31561;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#24341;&#20837;&#20102;&#33258;&#28982;&#35821;&#35328;&#24378;&#21270;&#23398;&#20064;&#65288;NLRL&#65289;&#65292;&#21019;&#26032;&#24615;&#22320;&#23558;RL&#21407;&#21017;&#19982;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;NLRL&#22312;&#33258;&#28982;&#35821;&#35328;&#31354;&#38388;&#20013;&#37325;&#26032;&#23450;&#20041;&#20102;&#20219;&#21153;&#30446;&#26631;&#12289;&#31574;&#30053;&#12289;&#20215;&#20540;&#20989;&#25968;&#12289;Bellman&#26041;&#31243;&#21644;&#31574;&#30053;&#36845;&#20195;&#31561;RL&#27010;&#24565;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-4&#26469;&#23454;&#29616;NLRL&#12290;&#23545;&#34920;&#26684;MDPs&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#20102;NLRL&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12289;&#39640;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has shown remarkable abilities in learning policies for decision-making tasks. However, RL is often hindered by issues such as low sample efficiency, lack of interpretability, and sparse supervision signals. To tackle these limitations, we take inspiration from the human learning process and introduce Natural Language Reinforcement Learning (NLRL), which innovatively combines RL principles with natural language representation. Specifically, NLRL redefines RL concepts like task objectives, policy, value function, Bellman equation, and policy iteration in natural language space. We present how NLRL can be practically implemented with the latest advancements in large language models (LLMs) like GPT-4. Initial experiments over tabular MDPs demonstrate the effectiveness, efficiency, and also interpretability of the NLRL framework.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;GPT-4V&#27169;&#22411;&#19978;&#36827;&#34892;&#20840;&#38754;&#30340;&#25506;&#32034;&#65292;&#25105;&#20204;&#21457;&#29616;&#35813;&#27169;&#22411;&#22312;&#35782;&#21035;&#25991;&#21270;&#27010;&#24565;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20173;&#34920;&#29616;&#36739;&#24369;&#12290;&#22312;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#20013;&#65292;GPT-4V&#22312;&#25991;&#21270;&#30456;&#20851;&#24615;&#26041;&#38754;&#20248;&#20110;&#21407;&#25991;&#12290;</title><link>https://arxiv.org/abs/2402.06015</link><description>&lt;p&gt;
&#22312;GPT-4V&#27169;&#22411;&#20013;&#25506;&#32034;&#35270;&#35273;&#25991;&#21270;&#24847;&#35782;&#65306;&#19968;&#39033;&#20840;&#38754;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploring Visual Culture Awareness in GPT-4V: A Comprehensive Probing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06015
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;GPT-4V&#27169;&#22411;&#19978;&#36827;&#34892;&#20840;&#38754;&#30340;&#25506;&#32034;&#65292;&#25105;&#20204;&#21457;&#29616;&#35813;&#27169;&#22411;&#22312;&#35782;&#21035;&#25991;&#21270;&#27010;&#24565;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20173;&#34920;&#29616;&#36739;&#24369;&#12290;&#22312;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#20013;&#65292;GPT-4V&#22312;&#25991;&#21270;&#30456;&#20851;&#24615;&#26041;&#38754;&#20248;&#20110;&#21407;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102; considerable&#30340;&#20851;&#27880;&#65292;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#12290;&#23613;&#31649;&#24050;&#32463;&#20570;&#20986;&#20102;&#22810;&#26041;&#38754;&#30340;&#21162;&#21147;&#26469;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#65292;&#20294;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;GPT-4V&#27169;&#22411;&#22312;&#35270;&#35273;&#25991;&#21270;&#24847;&#35782;&#26041;&#38754;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20351;&#29992;MaRVL&#22522;&#20934;&#25968;&#25454;&#38598;&#24191;&#27867;&#25506;&#32034;&#20102;GPT-4V&#65292;&#26088;&#22312;&#35843;&#26597;&#20854;&#22312;&#35270;&#35273;&#29702;&#35299;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#25991;&#21270;&#26041;&#38754;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#19982;&#35270;&#35273;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#21363;&#26631;&#39064;&#20998;&#31867;&#12289;&#25104;&#23545;&#26631;&#39064;&#29983;&#25104;&#21644;&#25991;&#21270;&#26631;&#31614;&#36873;&#25321;&#65292;&#20197;&#31995;&#32479;&#22320;&#28145;&#20837;&#30740;&#31350;&#32454;&#31890;&#24230;&#30340;&#35270;&#35273;&#25991;&#21270;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4V&#22312;&#35782;&#21035;&#25991;&#21270;&#27010;&#24565;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#65288;&#22914;&#27888;&#31859;&#23572;&#35821;&#21644;&#26031;&#29926;&#24076;&#37324;&#35821;&#65289;&#20013;&#20173;&#28982;&#34920;&#29616;&#36739;&#24369;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#65292;GPT-4V&#22312;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#20013;&#35777;&#26126;&#22312;&#25991;&#21270;&#30456;&#20851;&#24615;&#26041;&#38754;&#20248;&#20110;&#21407;&#25991;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained large Vision-Language models have drawn considerable interest in recent years due to their remarkable performance. Despite considerable efforts to assess these models from diverse perspectives, the extent of visual cultural awareness in the state-of-the-art GPT-4V model remains unexplored. To tackle this gap, we extensively probed GPT-4V using the MaRVL benchmark dataset, aiming to investigate its capabilities and limitations in visual understanding with a focus on cultural aspects. Specifically, we introduced three visual related tasks, i.e. caption classification, pairwise captioning, and culture tag selection, to systematically delve into fine-grained visual cultural evaluation. Experimental results indicate that GPT-4V excels at identifying cultural concepts but still exhibits weaker performance in low-resource languages, such as Tamil and Swahili. Notably, through human evaluation, GPT-4V proves to be more culturally relevant in image captioning tasks than the original 
&lt;/p&gt;</description></item><item><title>NoisyICL&#36890;&#36807;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#24341;&#20837;&#22122;&#38899;&#65292;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#21644;&#26657;&#20934;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;NoisyICL&#21487;&#20197;&#20135;&#29983;&#26356;&#20934;&#30830;&#12289;&#26356;&#20844;&#24179;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.05515</link><description>&lt;p&gt;
NoisyICL: &#19968;&#28857;&#22122;&#38899;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#12289;
&lt;/p&gt;
&lt;p&gt;
NoisyICL: A Little Noise in Model Parameters Calibrates In-context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05515
&lt;/p&gt;
&lt;p&gt;
NoisyICL&#36890;&#36807;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#24341;&#20837;&#22122;&#38899;&#65292;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#21644;&#26657;&#20934;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;NoisyICL&#21487;&#20197;&#20135;&#29983;&#26356;&#20934;&#30830;&#12289;&#26356;&#20844;&#24179;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064; (ICL) &#22312;&#39640;&#20808;&#39564;&#20559;&#24046;&#21644;&#19981;&#21487;&#20449;&#20219;&#30340;&#32622;&#20449;&#24230;&#30340;&#24433;&#21709;&#19979;&#65292;&#34920;&#29616;&#19981;&#20339;&#19988;&#26657;&#20934;&#19981;&#36275;&#12290;&#20197;&#24448;&#30340;&#19968;&#20123;&#24037;&#20316;&#36890;&#36807;&#20351;&#29992;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#21644;&#35745;&#31639;&#25104;&#26412;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#25913;&#21892; ICL &#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; NoisyICL&#65292;&#36890;&#36807;&#38543;&#26426;&#22122;&#38899;&#25200;&#21160;&#27169;&#22411;&#21442;&#25968;&#26469;&#21162;&#21147;&#25552;&#39640;&#24615;&#33021;&#21644;&#26657;&#20934;&#24615;&#12290;&#25105;&#20204;&#22312;2&#20010;&#27169;&#22411;&#21644;12&#20010;&#19979;&#28216;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;NoisyICL&#21487;&#20197;&#24110;&#21161;ICL&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;NoisyICL&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#25552;&#20379;&#26356;&#20844;&#24179;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#32622;&#20449;&#24230;&#26356;&#21487;&#20449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;NoisyICL&#26159;ICL&#30340;&#19968;&#31181;&#26377;&#25928;&#26657;&#20934;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20195;&#30721;&#24050;&#19978;&#20256;&#33267;Github&#12290;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning (ICL) is suffering from unsatisfactory performance and under-calibration due to high prior bias and unfaithful confidence. Some previous works fine-tuned language models for better ICL performance with enormous datasets and computing costs. In this paper, we propose NoisyICL, simply perturbing the model parameters by random noises to strive for better performance and calibration. Our experiments on 2 models and 12 downstream datasets show that NoisyICL can help ICL produce more accurate predictions. Our further analysis indicates that NoisyICL enables the model to provide more fair predictions, and also with less unfaithful confidence. Therefore, we believe that NoisyICL is an effective calibration of ICL. Our experimental code is uploaded to Github.
&lt;/p&gt;</description></item><item><title>L4Q&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;LoRA&#30340;&#23398;&#20064;&#30340;&#37327;&#21270;&#27493;&#38271;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37327;&#21270;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.04902</link><description>&lt;p&gt;
L4Q: &#36890;&#36807;&#22522;&#20110;LoRA&#30340;&#37327;&#21270;&#35757;&#32451;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#25552;&#20379;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04902
&lt;/p&gt;
&lt;p&gt;
L4Q&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;LoRA&#30340;&#23398;&#20064;&#30340;&#37327;&#21270;&#27493;&#38271;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37327;&#21270;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;(PTQ)&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;(QAT)&#26041;&#27861;&#27491;&#22312;&#27969;&#34892;&#36215;&#26469;&#65292;&#20197;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25152;&#24102;&#26469;&#30340;&#39640;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#23613;&#31649;&#21518;&#32773;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#20854;&#20943;&#23569;&#30340;&#35757;&#32451;&#24320;&#38144;&#65292;&#36890;&#24120;&#39318;&#36873;&#21518;&#35757;&#32451;&#37327;&#21270;&#12290;&#21516;&#26102;&#65292;&#20171;&#32461;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#22914;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#65292;&#24182;&#26368;&#36817;&#30340;&#24037;&#20316;&#24050;&#32463;&#25506;&#32034;&#20102;&#37327;&#21270;&#24863;&#30693;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#32570;&#20047;&#36890;&#29992;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#39044;&#37327;&#21270;&#27169;&#22411;&#30340;&#37197;&#32622;&#12290;&#30001;&#38750;&#32447;&#24615;&#37327;&#21270;&#25110;&#28151;&#21512;&#31934;&#24230;&#26435;&#37325;&#24341;&#36215;&#30340;&#25928;&#26524;&#21487;&#33021;&#20250;&#21463;&#21040;&#24433;&#21709;&#65292;&#24182;&#19988;&#37325;&#26032;&#35757;&#32451;&#29305;&#23450;&#37327;&#21270;&#21442;&#25968;&#21487;&#33021;&#20250;&#24433;&#21709;&#26368;&#20248;&#24615;&#33021;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;L4Q&#65292;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#12290;L4Q&#21033;&#29992;&#20102;&#22522;&#20110;LoRA&#30340;&#23398;&#20064;&#30340;&#37327;&#21270;&#27493;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (PTQ) and quantization-aware training (QAT) methods are gaining popularity in mitigating the high memory and computational costs associated with Large Language Models (LLMs). In resource-constrained scenarios, PTQ, with its reduced training overhead, is often preferred over QAT, despite the latter's potential for higher accuracy. Meanwhile, parameter-efficient fine-tuning (PEFT) methods like low-rank adaptation (LoRA) have been introduced, and recent efforts have explored quantization-aware PEFT techniques. However, these approaches may lack generality due to their reliance on the pre-quantized model's configuration. Their effectiveness may be compromised by non-linearly quantized or mixed-precision weights, and the retraining of specific quantization parameters might impede optimal performance. To address these challenges, we propose L4Q, an algorithm for parameter-efficient quantization-aware training. L4Q leverages LoRA-wise learned quantization step size 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PaDeLLM-NER&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#24182;&#34892;&#35299;&#30721;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#29983;&#25104;&#24310;&#36831;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04838</link><description>&lt;p&gt;
PaDeLLM-NER&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24182;&#34892;&#35299;&#30721;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PaDeLLM-NER&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#24182;&#34892;&#35299;&#30721;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#29983;&#25104;&#24310;&#36831;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20943;&#23569;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#30340;&#29983;&#25104;&#24310;&#36831;&#12290;LLMs&#30340;&#39640;&#24310;&#36831;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#39034;&#24207;&#35299;&#30721;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#33258;&#22238;&#24402;&#22320;&#29983;&#25104;NER&#30340;&#25152;&#26377;&#26631;&#31614;&#21644;&#25552;&#21450;&#65292;&#26174;&#33879;&#22686;&#21152;&#20102;&#24207;&#21015;&#38271;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PaDeLLM-NER&#65288;Parallel Decoding in LLM for NE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#27169;&#22359;&#25110;&#26550;&#26500;&#20462;&#25913;&#21363;&#21487;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#29983;&#25104;&#27169;&#22411;&#26694;&#26550;&#20013;&#30340;&#26041;&#27861;&#12290;PaDeLLM-NER&#20801;&#35768;&#21516;&#26102;&#35299;&#30721;&#25152;&#26377;&#25552;&#21450;&#65292;&#20174;&#32780;&#20943;&#23569;&#29983;&#25104;&#24310;&#36831;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;PaDeLLM-NER&#30340;&#25512;&#29702;&#36895;&#24230;&#26174;&#33879;&#25552;&#39640;&#65292;&#23545;&#33521;&#35821;&#21644;&#20013;&#25991;&#26469;&#35828;&#27604;&#33258;&#22238;&#24402;&#26041;&#27861;&#24555;1.76&#21040;10.22&#20493;&#12290;&#19982;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#30456;&#23218;&#32654;&#65292;&#21516;&#26102;&#32500;&#25345;&#20102;&#39044;&#27979;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we aim to reduce generation latency for Named Entity Recognition (NER) with Large Language Models (LLMs). The main cause of high latency in LLMs is the sequential decoding process, which autoregressively generates all labels and mentions for NER, significantly increase the sequence length. To this end, we introduce Parallel Decoding in LLM for NE} (PaDeLLM-NER), a approach that integrates seamlessly into existing generative model frameworks without necessitating additional modules or architectural modifications. PaDeLLM-NER allows for the simultaneous decoding of all mentions, thereby reducing generation latency. Experiments reveal that PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times faster than the autoregressive approach for both English and Chinese. Simultaneously it maintains the quality of predictions as evidenced by the performance that is on par with the state-of-the-art across various datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20960;&#20309;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#30446;&#26631;&#21464;&#37327;&#36873;&#25321;&#21644;2D&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#21644;&#22256;&#38590;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;LLMs&#30340;&#22810;&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#32416;&#27491;&#12289;&#21327;&#20316;&#21644;&#19981;&#21516;&#35282;&#33394;&#19987;&#19994;&#21270;&#26469;&#25552;&#39640;LLMs&#20960;&#20309;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.03877</link><description>&lt;p&gt;
&#36229;&#36234;&#32447;&#26465;&#21644;&#22278;&#22280;&#65306;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20960;&#20309;&#25512;&#29702;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20960;&#20309;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#30446;&#26631;&#21464;&#37327;&#36873;&#25321;&#21644;2D&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#21644;&#22256;&#38590;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;LLMs&#30340;&#22810;&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#32416;&#27491;&#12289;&#21327;&#20316;&#21644;&#19981;&#21516;&#35282;&#33394;&#19987;&#19994;&#21270;&#26469;&#25552;&#39640;LLMs&#20960;&#20309;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25968;&#23398;&#21644;&#31639;&#27861;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20102;&#19981;&#26029;&#22686;&#38271;&#30340;&#33021;&#21147;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#20960;&#20309;&#25512;&#29702;&#26041;&#38754;&#30340;&#25216;&#33021;&#36824;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;LLMs&#22312;&#26500;&#36896;&#24615;&#20960;&#20309;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#20154;&#31867;&#25968;&#23398;&#25512;&#29702;&#21457;&#23637;&#20013;&#26368;&#22522;&#30784;&#30340;&#27493;&#39588;&#20043;&#19968;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#36825;&#20010;&#39046;&#22495;&#38754;&#20020;&#30340;&#26174;&#33879;&#25361;&#25112;&#65292;&#23613;&#31649;&#22312;&#31867;&#20284;&#39046;&#22495;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#12290;LLMs&#22312;&#30446;&#26631;&#21464;&#37327;&#36873;&#25321;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#65292;&#24182;&#19988;&#22312;2D&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#38754;&#20020;&#22256;&#38590;&#65292;&#32463;&#24120;&#20250;&#38169;&#35823;&#22320;&#34920;&#31034;&#21644;&#33222;&#36896;&#23545;&#35937;&#21450;&#20854;&#25918;&#32622;&#20301;&#32622;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;LLMs&#30340;&#22810;&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#36827;&#34892;&#20869;&#37096;&#23545;&#35805;&#26469;&#22686;&#24378;&#23427;&#20204;&#29616;&#26377;&#30340;&#25512;&#29702;&#28508;&#21147;&#12290;&#36825;&#39033;&#24037;&#20316;&#24378;&#35843;&#20102;LLMs&#22312;&#20960;&#20309;&#25512;&#29702;&#20013;&#30340;&#29616;&#26377;&#38480;&#21046;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#32416;&#27491;&#12289;&#21327;&#20316;&#21644;&#19981;&#21516;&#35282;&#33394;&#19987;&#19994;&#21270;&#26469;&#25552;&#39640;&#20960;&#20309;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) demonstrate ever-increasing abilities in mathematical and algorithmic tasks, yet their geometric reasoning skills are underexplored. We investigate LLMs' abilities in constructive geometric problem-solving one of the most fundamental steps in the development of human mathematical reasoning. Our work reveals notable challenges that the state-of-the-art LLMs face in this domain despite many successes in similar areas. LLMs exhibit biases in target variable selection and struggle with 2D spatial relationships, often misrepresenting and hallucinating objects and their placements. To this end, we introduce a framework that formulates an LLMs-based multi-agents system that enhances their existing reasoning potential by conducting an internal dialogue. This work underscores LLMs' current limitations in geometric reasoning and improves geometric reasoning capabilities through self-correction, collaboration, and diverse role specializations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EffiBench&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;GPT-4-turbo&#29983;&#25104;&#30340;&#20195;&#30721;&#26368;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.02037</link><description>&lt;p&gt;
EffiBench:&#35780;&#20272;&#33258;&#21160;&#29983;&#25104;&#20195;&#30721;&#30340;&#25928;&#29575;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
EffiBench: Benchmarking the Efficiency of Automatically Generated Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EffiBench&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;GPT-4-turbo&#29983;&#25104;&#30340;&#20195;&#30721;&#26368;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#22312;&#36741;&#21161;&#36719;&#20214;&#24320;&#21457;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#21487;&#20197;&#24110;&#21161;&#23436;&#25104;&#20195;&#30721;&#34917;&#20840;&#12289;&#35843;&#35797;&#21644;&#20195;&#30721;&#36716;&#25442;&#31561;&#20219;&#21153;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#28145;&#20837;&#30740;&#31350;&#20102;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#27491;&#30830;&#24615;&#65292;&#20294;&#29983;&#25104;&#20195;&#30721;&#30340;&#25928;&#29575;&#36825;&#19968;&#37325;&#35201;&#26041;&#38754;&#24120;&#24120;&#34987;&#24573;&#35270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;EffiBench&#65292;&#19968;&#20010;&#21253;&#21547;1,000&#20010;&#25928;&#29575;&#20851;&#38190;&#30340;&#32534;&#30721;&#38382;&#39064;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#25928;&#29575;&#12290;EffiBench&#21253;&#21547;&#20102;&#19968;&#31995;&#21015;&#22810;&#26679;&#21270;&#30340;LeetCode&#32534;&#30721;&#38382;&#39064;&#65292;&#27599;&#20010;&#38382;&#39064;&#37117;&#19982;&#19968;&#20010;&#21487;&#25191;&#34892;&#30340;&#20154;&#24037;&#32534;&#20889;&#30340;&#20856;&#22411;&#35299;&#20915;&#26041;&#26696;&#37197;&#23545;&#12290;&#36890;&#36807;EffiBench&#65292;&#25105;&#20204;&#22312;&#23454;&#36341;&#20013;&#32771;&#23519;&#20102;21&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20854;&#20013;13&#31181;&#26159;&#24320;&#28304;&#30340;&#65292;8&#31181;&#26159;&#38381;&#28304;&#30340;&#65289;&#22312;&#29983;&#25104;&#39640;&#25928;&#20195;&#30721;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4-turbo&#29983;&#25104;&#30340;&#20195;&#30721;&#26368;&#39640;&#25928;&#65292;&#26126;&#26174;&#20248;&#20110;Palm-2-chat-bison&#12289;Claude-instant-1&#12289;Gemini-pro&#12289;GPT-4&#21644;GPT-3.5&#12290;
&lt;/p&gt;
&lt;p&gt;
Code generation models have increasingly become integral to aiding software development, offering assistance in tasks such as code completion, debugging, and code translation. Although current research has thoroughly examined the correctness of code produced by code generation models, a vital aspect, i.e., the efficiency of the generated code, has often been neglected. This paper presents EffiBench, a benchmark with 1,000 efficiency-critical coding problems for assessing the efficiency of code generated by code generation models. EffiBench contains a diverse set of LeetCode coding problems. Each problem is paired with an executable human-written canonical solution. With EffiBench, we empirically examine the capability of 21 Large Language Models (13 open-sourced and 8 closed-sourced) in generating efficient code. The results demonstrate that GPT-4-turbo generates the most efficient code, significantly outperforming Palm-2-chat-bison, Claude-instant-1, Gemini-pro, GPT-4, and GPT-3.5. Ne
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#26694;&#26550;&#65288;SWEA&#65289;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#26469;&#32534;&#36753;&#30693;&#35782;&#65292;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#65292;&#36991;&#20813;&#19981;&#21487;&#36870;&#30340;&#25439;&#23475;&#21644;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2401.17809</link><description>&lt;p&gt;
SWEA:&#36890;&#36807;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
SWEA: Changing Factual Knowledge in Large Language Models via Subject Word Embedding Altering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17809
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#26694;&#26550;&#65288;SWEA&#65289;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#26469;&#32534;&#36753;&#30693;&#35782;&#65292;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#65292;&#36991;&#20813;&#19981;&#21487;&#36870;&#30340;&#25439;&#23475;&#21644;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#36817;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#20027;&#35201;&#28041;&#21450;&#20462;&#25913;&#27169;&#22411;&#21442;&#25968;&#25110;&#21521;&#29616;&#26377;&#27169;&#22411;&#28155;&#21152;&#38468;&#21152;&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#20250;&#23545;LLM&#36896;&#25104;&#19981;&#21487;&#36870;&#30340;&#24433;&#21709;&#65292;&#32780;&#21518;&#32773;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#65292;&#24182;&#19988;&#27169;&#31946;&#30340;&#21521;&#37327;&#21305;&#37197;&#24182;&#19981;&#24635;&#26159;&#21487;&#38752;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#65288;SWEA&#65289;&#26694;&#26550;&#65292;&#23427;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#65292;&#24182;&#23454;&#29616;&#32534;&#36753;&#30693;&#35782;&#30340;&#30446;&#26631;&#12290;SWEA&#22312;&#27169;&#22411;&#22806;&#37096;&#20351;&#29992;&#31934;&#30830;&#30340;&#20851;&#38190;&#21305;&#37197;&#65292;&#24182;&#36827;&#34892;&#21487;&#38752;&#30340;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#65292;&#20174;&#32780;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#32780;&#19981;&#22686;&#21152;&#25512;&#29702;&#24320;&#38144;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20248;&#21270;&#25233;&#21046;&#34701;&#21512;&#26041;&#27861;&#65292;&#39318;&#20808;&#20248;&#21270;&#32534;&#36753;&#30446;&#26631;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#28982;&#21518;&#25233;&#21046;&#30693;&#35782;&#23884;&#20837;&#32500;&#24230;&#65288;KED&#65289;&#20197;&#33719;&#24471;&#26368;&#32456;&#34701;&#21512;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#22240;&#27492;&#25552;&#20986;&#20102;SWEAOS&#20803;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model editing has recently gained widespread attention. Current model editing methods primarily involve modifying model parameters or adding additional modules to the existing model. However, the former causes irreversible damage to LLMs, while the latter incurs additional inference overhead and fuzzy vector matching is not always reliable. To address these issues, we propose an expandable Subject Word Embedding Altering (SWEA) framework, which modifies the representation of subjects and achieve the goal of editing knowledge during the inference stage. SWEA uses precise key matching outside the model and performs reliable subject word embedding altering, thus protecting the original weights of the model without increasing inference overhead. We then propose optimizing then suppressing fusion method, which first optimizes the embedding vector for the editing target and then suppresses the Knowledge Embedding Dimension (KED) to obtain the final fused embedding. We thus propose SWEAOS met
&lt;/p&gt;</description></item><item><title>PokeMQA&#26159;&#19968;&#20010;&#21487;&#32534;&#31243;&#30340;&#22810;&#36339;&#38382;&#31572;&#30693;&#35782;&#32534;&#36753;&#27169;&#22411;&#65292;&#36890;&#36807;&#36991;&#20813;&#21151;&#33021;&#22810;&#26679;&#30340;&#25512;&#29702;&#20219;&#21153;&#30340;&#32806;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#38382;&#39064;&#29702;&#35299;&#21644;&#22238;&#31572;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2312.15194</link><description>&lt;p&gt;
PokeMQA: &#21487;&#32534;&#31243;&#30340;&#22810;&#36339;&#38382;&#31572;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
PokeMQA: Programmable knowledge editing for Multi-hop Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15194
&lt;/p&gt;
&lt;p&gt;
PokeMQA&#26159;&#19968;&#20010;&#21487;&#32534;&#31243;&#30340;&#22810;&#36339;&#38382;&#31572;&#30693;&#35782;&#32534;&#36753;&#27169;&#22411;&#65292;&#36890;&#36807;&#36991;&#20813;&#21151;&#33021;&#22810;&#26679;&#30340;&#25512;&#29702;&#20219;&#21153;&#30340;&#32806;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#38382;&#39064;&#29702;&#35299;&#21644;&#22238;&#31572;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#36339;&#38382;&#31572;&#65288;MQA&#65289;&#26159;&#35780;&#20272;&#26426;&#22120;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#20043;&#19968;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#24191;&#27867;&#23454;&#29616;&#20102;&#19982;&#20154;&#31867;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#30693;&#35782;&#20107;&#23454;&#30340;&#21160;&#24577;&#24615;&#65292;&#24050;&#32463;&#25506;&#32034;&#20102;&#30693;&#35782;&#32534;&#36753;&#26469;&#26356;&#26032;&#27169;&#22411;&#65292;&#20197;&#33719;&#21462;&#26368;&#26032;&#30340;&#20107;&#23454;&#65292;&#21516;&#26102;&#36991;&#20813;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#20174;&#32534;&#36753;&#21518;&#30340;&#20107;&#23454;&#24320;&#22987;&#65292;&#26356;&#26032;&#21518;&#30340;&#27169;&#22411;&#38656;&#35201;&#22312;MQA&#38142;&#20013;&#25552;&#20379;&#32423;&#32852;&#21464;&#21270;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#31616;&#21333;&#22320;&#37319;&#29992;&#20102;&#28151;&#21512;&#25552;&#31034;&#26469;&#25351;&#23548;LLMs&#36827;&#34892;&#22810;&#20010;&#25512;&#29702;&#20219;&#21153;&#30340;&#39034;&#24207;&#25191;&#34892;&#65292;&#21253;&#25324;&#38382;&#39064;&#20998;&#35299;&#12289;&#31572;&#26696;&#29983;&#25104;&#21644;&#36890;&#36807;&#19982;&#32534;&#36753;&#30340;&#20107;&#23454;&#36827;&#34892;&#27604;&#36739;&#30340;&#20914;&#31361;&#26816;&#26597;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21151;&#33021;&#22810;&#26679;&#30340;&#25512;&#29702;&#20219;&#21153;&#30340;&#32806;&#21512;&#25233;&#21046;&#20102;LLMs&#22312;&#29702;&#35299;&#21644;&#22238;&#31572;&#38382;&#39064;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#24178;&#25200;&#20102;&#23427;&#20204;&#22312;&#20914;&#31361;&#26816;&#26597;&#30340;&#19981;&#29087;&#32451;&#20219;&#21153;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.15194v2 Announce Type: replace  Abstract: Multi-hop question answering (MQA) is one of the challenging tasks to evaluate machine's comprehension and reasoning abilities, where large language models (LLMs) have widely achieved the human-comparable performance. Due to the dynamics of knowledge facts in real world, knowledge editing has been explored to update model with the up-to-date facts while avoiding expensive re-training or fine-tuning. Starting from the edited fact, the updated model needs to provide cascading changes in the chain of MQA. The previous art simply adopts a mix-up prompt to instruct LLMs conducting multiple reasoning tasks sequentially, including question decomposition, answer generation, and conflict checking via comparing with edited facts. However, the coupling of these functionally-diverse reasoning tasks inhibits LLMs' advantages in comprehending and answering questions while disturbing them with the unskilled task of conflict checking. We thus propos
&lt;/p&gt;</description></item><item><title>MAC-SQL&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26694;&#26550;&#65292;&#38024;&#23545;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20013;&#30340;&#24040;&#22823;&#25968;&#25454;&#24211;&#21644;&#22797;&#26434;&#29992;&#25143;&#38382;&#39064;&#65292;&#36890;&#36807;&#26680;&#24515;&#20998;&#35299;&#22120;&#26234;&#33021;&#20307;&#21644;&#36741;&#21161;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#65292;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#21644;&#27169;&#22411;&#36827;&#34892;&#35299;&#26512;&#21644;&#20462;&#27491;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25991;&#26412;&#21040;SQL&#29983;&#25104;&#21644;&#26597;&#35810;&#35299;&#26512;&#12290;</title><link>https://arxiv.org/abs/2312.11242</link><description>&lt;p&gt;
MAC-SQL: &#19968;&#31181;&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11242
&lt;/p&gt;
&lt;p&gt;
MAC-SQL&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26694;&#26550;&#65292;&#38024;&#23545;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20013;&#30340;&#24040;&#22823;&#25968;&#25454;&#24211;&#21644;&#22797;&#26434;&#29992;&#25143;&#38382;&#39064;&#65292;&#36890;&#36807;&#26680;&#24515;&#20998;&#35299;&#22120;&#26234;&#33021;&#20307;&#21644;&#36741;&#21161;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#65292;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#21644;&#27169;&#22411;&#36827;&#34892;&#35299;&#26512;&#21644;&#20462;&#27491;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25991;&#26412;&#21040;SQL&#29983;&#25104;&#21644;&#26597;&#35810;&#35299;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#20110;LLM&#30340;&#25991;&#26412;&#21040;SQL&#26041;&#27861;&#36890;&#24120;&#22312;&#8220;&#24040;&#22823;&#8221;&#30340;&#25968;&#25454;&#24211;&#21644;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#22797;&#26434;&#29992;&#25143;&#38382;&#39064;&#19978;&#36973;&#21463;&#20005;&#37325;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#21644;&#27169;&#22411;&#21327;&#20316;&#30340;LLM&#30340;&#37325;&#35201;&#24847;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MAC-SQL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LLM&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#26680;&#24515;&#20998;&#35299;&#22120;&#26234;&#33021;&#20307;&#65292;&#29992;&#20110;&#36827;&#34892;&#24102;&#26377;&#23569;&#26679;&#26412;&#24605;&#32500;&#38142;&#30340;&#25991;&#26412;&#21040;SQL&#29983;&#25104;&#65292;&#21516;&#26102;&#36824;&#26377;&#20004;&#20010;&#36741;&#21161;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#25110;&#27169;&#22411;&#33719;&#21462;&#36739;&#23567;&#30340;&#23376;&#25968;&#25454;&#24211;&#24182;&#20462;&#27491;&#38169;&#35823;&#30340;SQL&#26597;&#35810;&#12290;&#20998;&#35299;&#22120;&#26234;&#33021;&#20307;&#19982;&#36741;&#21161;&#26234;&#33021;&#20307;&#21512;&#20316;&#65292;&#26681;&#25454;&#38656;&#35201;&#28608;&#27963;&#65292;&#24182;&#21487;&#20197;&#25193;&#23637;&#20197;&#36866;&#24212;&#26032;&#30340;&#29305;&#24615;&#25110;&#24037;&#20855;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#26368;&#21021;&#21033;&#29992;GPT-4&#20316;&#20026;&#24378;&#22823;&#30340;LLM&#39592;&#24178;&#26469;&#23436;&#25104;&#25152;&#26377;&#26234;&#33021;&#20307;&#20219;&#21153;&#65292;&#20197;&#30830;&#23450;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11242v3 Announce Type: replace  Abstract: Recent LLM-based Text-to-SQL methods usually suffer from significant performance degradation on ``huge" databases and complex user questions that require multi-step reasoning. Moreover, most existing methods neglect the crucial significance of LLMs utilizing external tools and model collaboration. To address these challenges, we introduce MAC-SQL, a novel LLM-based multi-agent collaborative framework. Our framework comprises a core decomposer agent for Text-to-SQL generation with few-shot chain-of-thought reasoning, accompanied by two auxiliary agents that utilize external tools or models to acquire smaller sub-databases and refine erroneous SQL queries. The decomposer agent collaborates with auxiliary agents, which are activated as needed and can be expanded to accommodate new features or tools for effective Text-to-SQL parsing. In our framework, We initially leverage GPT-4 as the strong backbone LLM for all agent tasks to determine
&lt;/p&gt;</description></item><item><title>CoT-Influx&#26159;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#19978;&#19979;&#25991;&#20462;&#21098;&#26469;&#25552;&#21319;LLM&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#26377;&#25928;&#21644;&#31616;&#26126;&#30340;&#31034;&#20363;&#36755;&#20837;&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#25552;&#31034;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.08901</link><description>&lt;p&gt;
&#26356;&#23569;&#21363;&#26356;&#22810;&#65306;&#36890;&#36807;&#24378;&#21270;&#19978;&#19979;&#25991;&#20462;&#21098;&#25552;&#21319;LLM&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08901
&lt;/p&gt;
&lt;p&gt;
CoT-Influx&#26159;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#19978;&#19979;&#25991;&#20462;&#21098;&#26469;&#25552;&#21319;LLM&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#26377;&#25928;&#21644;&#31616;&#26126;&#30340;&#31034;&#20363;&#36755;&#20837;&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#25552;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#25968;&#23398;&#25512;&#29702;&#26041;&#38754;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CoT-Influx&#65292;&#19968;&#31181;&#23558;&#23569;&#26679;&#26412;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#23398;&#20064;&#25512;&#21521;&#26497;&#38480;&#20197;&#25913;&#21892;LLM&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#12290;&#30001;&#20110;&#35266;&#23519;&#21040;&#22312;&#25552;&#31034;&#20013;&#28155;&#21152;&#26356;&#31616;&#26126;&#30340;CoT&#31034;&#20363;&#21487;&#20197;&#25552;&#39640;LLM&#25512;&#29702;&#34920;&#29616;&#65292;CoT-Influx&#37319;&#29992;&#20102;&#19968;&#31181;&#20174;&#31895;&#31961;&#21040;&#31934;&#32454;&#30340;&#20462;&#21098;&#22120;&#26469;&#26368;&#22823;&#21270;&#26377;&#25928;&#21644;&#31616;&#26126;&#30340;CoT&#31034;&#20363;&#30340;&#36755;&#20837;&#12290;&#20462;&#21098;&#22120;&#39318;&#20808;&#36873;&#25321;&#23613;&#21487;&#33021;&#22810;&#30340;&#20851;&#38190;CoT&#31034;&#20363;&#65292;&#28982;&#21518;&#20462;&#21098;&#26080;&#20851;&#32039;&#35201;&#30340;&#26631;&#35760;&#20197;&#36866;&#24212;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;&#20351;&#29992;&#38590;&#24230;&#32423;&#21035;&#21644;&#25512;&#29702;&#27493;&#39588;&#22810;&#26679;&#30340;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#20462;&#21098;&#22120;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#19987;&#38376;&#38024;&#23545;&#25968;&#23398;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#32467;&#26524;&#26159;&#65292;&#36890;&#36807;&#22312;&#20196;&#29260;&#20013;&#21551;&#29992;&#21452;&#20493;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#30340;CoT&#31034;&#20363;&#65292;CoT-Influx&#22312;&#21508;&#31181;&#25552;&#31034;&#22522;&#20934;&#26041;&#27861;&#19978;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08901v3 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have shown impressive capabilities, yet they still struggle with math reasoning. In this work, we propose CoT-Influx, a novel approach that pushes the boundary of few-shot Chain-of-Thoughts (CoT) learning to improve LLM mathematical reasoning. Motivated by the observation that adding more concise CoT examples in the prompt can improve LLM reasoning performance, CoT-Influx employs a coarse-to-fine pruner to maximize the input of effective and concise CoT examples. The pruner first selects as many crucial CoT examples as possible and then prunes unimportant tokens to fit the context window. A math reasoning dataset with diverse difficulty levels and reasoning steps is used to train the pruner, along with a math-specialized reinforcement learning approach. As a result, by enabling more CoT examples with double the context window size in tokens, CoT-Influx significantly outperforms various prompting bas
&lt;/p&gt;</description></item><item><title>MasonTigers@LT-EDI-2024&#22242;&#38431;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#21313;&#31181;&#35821;&#35328;&#20013;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#20013;&#30340;&#24656;&#21516;&#21644;&#24694;&#24847;&#24615;&#21035;&#35748;&#30693;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.14681</link><description>&lt;p&gt;
MasonTigers@LT-EDI-2024&#65306;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#20013;&#30340;&#24656;&#21516;&#21644;&#24694;&#24847;&#24615;&#21035;&#35748;&#30693;&#30340;&#38598;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MasonTigers@LT-EDI-2024: An Ensemble Approach towards Detecting Homophobia and Transphobia in Social Media Comments. (arXiv:2401.14681v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14681
&lt;/p&gt;
&lt;p&gt;
MasonTigers@LT-EDI-2024&#22242;&#38431;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#21313;&#31181;&#35821;&#35328;&#20013;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#20013;&#30340;&#24656;&#21516;&#21644;&#24694;&#24847;&#24615;&#21035;&#35748;&#30693;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;LT-EDI 2024&#30740;&#35752;&#20250;&#30340;&#20219;&#21153;2&#20013;&#38024;&#23545;&#21313;&#31181;&#35821;&#35328;&#26816;&#27979;&#24656;&#21516;&#21644;/&#25110;&#24694;&#24847;&#24615;&#21035;&#35748;&#30693;&#30340;&#26041;&#27861;&#21644;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#21333;&#35821;&#35328;transformer&#21644;&#38598;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#27599;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#38598;&#25104;&#27169;&#22411;&#34920;&#29616;&#33391;&#22909;&#65292;&#25105;&#20204;&#30340;&#22242;&#38431;MasonTigers&#22312;&#21313;&#31181;&#35821;&#35328;&#20013;&#30340;&#20843;&#31181;&#20013;&#37117;&#25490;&#21517;&#21069;&#20116;&#65292;&#20197;&#23439;&#35266;F1&#20998;&#25968;&#34913;&#37327;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#38598;&#25104;&#26041;&#27861;&#22312;&#22810;&#35821;&#35328;&#24773;&#22659;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#35299;&#20915;&#20102;&#35821;&#35328;&#29305;&#23450;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we describe our approaches and results for Task 2 of the LT-EDI 2024 Workshop, aimed at detecting homophobia and/or transphobia across ten languages. Our methodologies include monolingual transformers and ensemble methods, capitalizing on the strengths of each to enhance the performance of the models. The ensemble models worked well, placing our team, MasonTigers, in the top five for eight of the ten languages, as measured by the macro F1 score. Our work emphasizes the efficacy of ensemble methods in multilingual scenarios, addressing the complexities of language-specific tasks.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#26159;&#21542;&#21487;&#38752;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;AI&#23433;&#20840;&#32771;&#34385;&#22240;&#32032;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#27965;&#24615;&#26816;&#27979;&#20316;&#20026;&#35780;&#20272;&#20854;&#21487;&#38752;&#24615;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.07927</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#26159;&#21542;&#21487;&#38752;?
&lt;/p&gt;
&lt;p&gt;
Are self-explanations from Large Language Models faithful?. (arXiv:2401.07927v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07927
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#26159;&#21542;&#21487;&#38752;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;AI&#23433;&#20840;&#32771;&#34385;&#22240;&#32032;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#27965;&#24615;&#26816;&#27979;&#20316;&#20026;&#35780;&#20272;&#20854;&#21487;&#38752;&#24615;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#36807;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29978;&#33267;&#33021;&#22815;&#25552;&#20379;&#20854;&#34892;&#20026;&#30340;&#35299;&#37322;&#12290;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#23545;&#20844;&#20247;&#26159;&#30452;&#25509;&#21487;&#35775;&#38382;&#30340;&#65292;&#22240;&#27492;&#23384;&#22312;&#36825;&#26679;&#30340;&#39118;&#38505;&#65292;&#21363;&#20196;&#20154;&#20449;&#26381;&#20294;&#38169;&#35823;&#30340;&#35299;&#37322;&#21487;&#33021;&#23548;&#33268;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#25903;&#25745;&#30340;&#33258;&#20449;&#12290;&#22240;&#27492;&#65292;&#35299;&#37322;&#33021;&#21147;&#21644;&#21487;&#38752;&#24615;&#26159;AI&#23433;&#20840;&#30340;&#37325;&#35201;&#32771;&#34385;&#22240;&#32032;&#12290;&#35780;&#20272;&#33258;&#25105;&#35299;&#37322;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#36807;&#20110;&#22797;&#26434;&#65292;&#26080;&#27861;&#27880;&#37322;&#20160;&#20040;&#26159;&#27491;&#30830;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#27965;&#24615;&#26816;&#27979;&#20316;&#20026;&#21487;&#38752;&#24615;&#30340;&#34913;&#37327;&#25351;&#26631;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35828;&#26576;&#32452;&#35789;&#23545;&#20110;&#20570;&#20986;&#39044;&#27979;&#24456;&#37325;&#35201;&#65292;&#37027;&#20040;&#22312;&#27809;&#26377;&#36825;&#20123;&#35789;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#24212;&#35813;&#26080;&#27861;&#20570;&#20986;&#30456;&#21516;&#30340;&#39044;&#27979;&#12290;&#34429;&#28982;&#33258;&#27965;&#24615;&#26816;&#27979;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#21487;&#38752;&#24615;&#26041;&#27861;&#65292;&#20294;&#20043;&#21069;&#23578;&#26410;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#20013;&#12290;&#25105;&#20204;&#23558;&#33258;&#27965;&#24615;&#26816;&#27979;&#24212;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned large language models (LLMs) excel at many tasks, and will even provide explanations for their behavior. Since these models are directly accessible to the public, there is a risk that convincing and wrong explanations can lead to unsupported confidence in LLMs. Therefore, interpretability-faithfulness of self-explanations is an important consideration for AI Safety. Assessing the interpretability-faithfulness of these explanations, termed self-explanations, is challenging as the models are too complex for humans to annotate what is a correct explanation. To address this, we propose employing self-consistency checks as a measure of faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make the same prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been applied to LLM's self-explanations. We apply self-consistency checks to t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19977;&#31181;&#25991;&#26412;&#22810;&#26679;&#24615;&#28608;&#21169;&#26041;&#27861;&#23545;LLM&#25991;&#26412;&#22686;&#24378;&#20013;&#29983;&#25104;&#25991;&#26412;&#30340;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#31105;&#24524;&#35789;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#22320;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#32780;&#20351;&#29992;&#20808;&#21069;&#21019;&#24314;&#30340;&#25913;&#20889;&#20316;&#20026;&#25552;&#31034;&#26102;&#65292;&#19979;&#28216;&#27169;&#22411;&#30340;&#24615;&#33021;&#26368;&#39640;&#12290;</title><link>http://arxiv.org/abs/2401.06643</link><description>&lt;p&gt;
&#22810;&#26679;&#24615;&#28608;&#21169;&#23545;LLM&#25991;&#26412;&#22686;&#24378;&#20013;&#26679;&#26412;&#22810;&#26679;&#24615;&#21644;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Effects of diversity incentives on sample diversity and downstream model performance in LLM-based text augmentation. (arXiv:2401.06643v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19977;&#31181;&#25991;&#26412;&#22810;&#26679;&#24615;&#28608;&#21169;&#26041;&#27861;&#23545;LLM&#25991;&#26412;&#22686;&#24378;&#20013;&#29983;&#25104;&#25991;&#26412;&#30340;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#31105;&#24524;&#35789;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#22320;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#32780;&#20351;&#29992;&#20808;&#21069;&#21019;&#24314;&#30340;&#25913;&#20889;&#20316;&#20026;&#25552;&#31034;&#26102;&#65292;&#19979;&#28216;&#27169;&#22411;&#30340;&#24615;&#33021;&#26368;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#26032;&#30340;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25968;&#25454;&#22686;&#24378;&#20219;&#21153;&#20013;&#25214;&#21040;&#20102;&#24212;&#29992;&#65292;&#20854;&#20013;&#23569;&#37327;&#25991;&#26412;&#26679;&#26412;&#34987;LLM&#25913;&#20889;&#65292;&#28982;&#21518;&#29992;&#20110;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#19981;&#21516;&#25552;&#31034;&#12289;&#31181;&#23376;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#12289;&#36807;&#28388;&#26041;&#27861;&#25110;&#27169;&#22411;&#35774;&#32622;&#23545;&#25913;&#20889;&#25968;&#25454;&#65288;&#21644;&#19979;&#28216;&#27169;&#22411;&#65289;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22312;&#20247;&#21253;&#20013;&#24050;&#32463;&#24314;&#31435;&#33391;&#22909;&#30340;&#19977;&#31181;&#25991;&#26412;&#22810;&#26679;&#24615;&#28608;&#21169;&#26041;&#27861;&#65306;&#31105;&#24524;&#35789;&#12289;&#36890;&#36807;&#20808;&#21069;&#24322;&#24120;&#35299;&#30340;&#25552;&#31034;&#21644;&#36890;&#36807;&#20808;&#21069;&#24322;&#24120;&#35299;&#30340;&#38142;&#25509;&#12290;&#20351;&#29992;&#36825;&#20123;&#28608;&#21169;&#26041;&#27861;&#20316;&#20026;&#25351;&#23548;LLM&#22686;&#34917;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#25506;&#27979;&#23427;&#20204;&#23545;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;5&#31181;&#19981;&#21516;&#30340;LLM&#21644;6&#20010;&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#31105;&#24524;&#35789;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#22320;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#32780;&#20351;&#29992;&#20808;&#21069;&#21019;&#24314;&#30340;&#25913;&#20889;&#20316;&#20026;&#25552;&#31034;&#26102;&#65292;&#19979;&#28216;&#27169;&#22411;&#30340;&#24615;&#33021;&#26368;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
The latest generative large language models (LLMs) have found their application in data augmentation tasks, where small numbers of text samples are LLM-paraphrased and then used to fine-tune the model. However, more research is needed to assess how different prompts, seed data selection strategies, filtering methods, or model settings affect the quality of paraphrased data (and downstream models). In this study, we investigate three text diversity incentive methods well established in crowdsourcing: taboo words, hints by previous outlier solutions, and chaining on previous outlier solutions. Using these incentive methods as part of instructions to LLMs augmenting text datasets, we measure their effects on generated texts' lexical diversity and downstream model performance. We compare the effects over 5 different LLMs and 6 datasets. We show that diversity is most increased by taboo words, while downstream model performance is highest when previously created paraphrases are used as hint
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#30340;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#19987;&#38376;&#30340;&#27169;&#22411;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36807;&#20102;GPT-4&#30340;&#32763;&#35793;&#24615;&#33021;&#65292;&#20294;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#20173;&#28982;&#23384;&#22312;&#31163;&#26631;&#32763;&#35793;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#21644;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2401.06468</link><description>&lt;p&gt;
&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adapting Large Language Models for Document-Level Machine Translation. (arXiv:2401.06468v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#30340;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#19987;&#38376;&#30340;&#27169;&#22411;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36807;&#20102;GPT-4&#30340;&#32763;&#35793;&#24615;&#33021;&#65292;&#20294;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#20173;&#28982;&#23384;&#22312;&#31163;&#26631;&#32763;&#35793;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#21644;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#20219;&#21153;&#29305;&#23450;&#30340;&#24494;&#35843;&#20043;&#21518;&#65292;&#20013;&#31561;&#35268;&#27169;&#30340;LLMs&#24448;&#24448;&#32988;&#36807;&#20854;&#26356;&#22823;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#23558;LLMs&#35843;&#25972;&#20026;&#29305;&#23450;&#35821;&#35328;&#23545;&#30340;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#65288;DocMT&#65289;&#30340;&#36807;&#31243;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#25552;&#31034;&#31574;&#30053;&#23545;&#19979;&#28216;&#32763;&#35793;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#20004;&#31181;&#24494;&#35843;&#26041;&#27861;&#12289;&#19977;&#31181;LLM&#20027;&#24178;&#21644;18&#20010;&#28041;&#21450;&#20061;&#31181;&#35821;&#35328;&#23545;&#30340;&#32763;&#35793;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#19987;&#38376;&#30340;&#27169;&#22411;&#29978;&#33267;&#22312;&#32763;&#35793;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;GPT-4&#65292;&#32780;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#23427;&#20204;&#19987;&#38376;&#22312;&#21452;&#35821;&#24179;&#34892;&#25991;&#26723;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20173;&#28982;&#26126;&#26174;&#23384;&#22312;&#31163;&#26631;&#32763;&#35793;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#38024;&#23545;DocMT&#37327;&#36523;&#23450;&#21046;&#30340;LLMs&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#22914;&#32763;&#35793;&#20934;&#30830;&#24230;&#25913;&#21892;&#12289;&#22810;&#28304;&#20449;&#24687;&#25972;&#21512;&#31561;&#21508;&#20010;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made significant strides in various natural language processing (NLP) tasks. Recent research shows that the moderately-sized LLMs often outperform their larger counterparts after task-specific fine-tuning. In this work, we delve into the process of adapting LLMs to specialize in document-level machine translation (DocMT) for a specific language pair. Firstly, we explore how prompt strategies affect downstream translation performance. Then, we conduct extensive experiments with two fine-tuning methods, three LLM backbones, and 18 translation tasks across nine language pairs. Our findings indicate that in some cases, these specialized models even surpass GPT-4 in translation performance, while they still significantly suffer from the off-target translation issue in others, even if they are exclusively fine-tuned on bilingual parallel documents. Furthermore, we provide an in-depth analysis of these LLMs tailored for DocMT, exploring aspects such as transl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#21518;&#39564;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#20307;&#30340;&#20803;&#29305;&#24449;&#21644;&#27169;&#22411;&#30340;&#34920;&#31034;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20316;&#20026;&#24230;&#37327;&#26631;&#20934;&#65292;&#26377;&#25928;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.02987</link><description>&lt;p&gt;
&#20320;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26377;&#25913;&#36827;&#21527;&#65311;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#21518;&#39564;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Has Your Pretrained Model Improved? A Multi-head Posterior Based Approach. (arXiv:2401.02987v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#21518;&#39564;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#20307;&#30340;&#20803;&#29305;&#24449;&#21644;&#27169;&#22411;&#30340;&#34920;&#31034;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20316;&#20026;&#24230;&#37327;&#26631;&#20934;&#65292;&#26377;&#25928;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20851;&#31995;&#22411;&#25968;&#25454;&#38598;&#31561;&#39046;&#22495;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#24341;&#21457;&#20102;&#22914;&#20309;&#26356;&#39640;&#25928;&#12289;&#26356;&#26377;&#25928;&#22320;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#19982;&#27599;&#20010;&#23454;&#20307;&#30456;&#20851;&#30340;&#20803;&#29305;&#24449;&#20316;&#20026;&#19990;&#30028;&#30693;&#35782;&#30340;&#26469;&#28304;&#65292;&#24182;&#21033;&#29992;&#27169;&#22411;&#30340;&#23454;&#20307;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#36825;&#20123;&#34920;&#31034;&#21644;&#20803;&#29305;&#24449;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20316;&#20026;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20855;&#26377;&#20851;&#31995;&#22411;&#25968;&#25454;&#38598;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#20687;&#27169;&#22411;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of pretrained models has significantly impacted from Natural Language Processing (NLP) and Computer Vision to relational datasets. Traditionally, these models are assessed through fine-tuned downstream tasks. However, this raises the question of how to evaluate these models more efficiently and more effectively. In this study, we explore a novel approach where we leverage the meta features associated with each entity as a source of worldly knowledge and employ entity representations from the models. We propose using the consistency between these representations and the meta features as a metric for evaluating pretrained models. Our method's effectiveness is demonstrated across various domains, including models with relational datasets, large language models and images models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#20301;&#32622;&#21435;&#20559;&#26041;&#27861;&#65288;ZOE&#65289;&#26469;&#38477;&#20302;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#30340;&#26080;&#30417;&#30563;&#21709;&#24212;&#36827;&#34892;&#21435;&#20559;&#12290;&#23454;&#39564;&#35777;&#23454;ZOE&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#20013;&#22343;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01218</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20301;&#32622;&#21435;&#20559;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Position Debiasing for Large Language Models. (arXiv:2401.01218v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#20301;&#32622;&#21435;&#20559;&#26041;&#27861;&#65288;ZOE&#65289;&#26469;&#38477;&#20302;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#30340;&#26080;&#30417;&#30563;&#21709;&#24212;&#36827;&#34892;&#21435;&#20559;&#12290;&#23454;&#39564;&#35777;&#23454;ZOE&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#20013;&#22343;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#24050;&#34987;&#35777;&#26126;&#26159;&#25913;&#21892;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#24615;&#33021;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;LLMs&#21487;&#33021;&#36866;&#24212;&#25968;&#25454;&#38598;&#20559;&#35265;&#21644;&#39044;&#27979;&#30340;&#25463;&#24452;&#65292;&#23548;&#33268;&#29983;&#25104;&#24615;&#33021;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs&#23481;&#26131;&#34920;&#29616;&#20986;&#20301;&#32622;&#20559;&#24046;&#65292;&#21363;&#21033;&#29992;&#20301;&#20110;&#24320;&#22836;&#25110;&#26411;&#23614;&#25110;&#36755;&#20837;&#20013;&#29305;&#23450;&#20301;&#32622;&#32447;&#32034;&#30340;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#20943;&#36731;&#20301;&#32622;&#20559;&#24046;&#30340;&#24037;&#20316;&#38656;&#35201;&#22806;&#37096;&#20559;&#24046;&#30693;&#35782;&#25110;&#24102;&#27880;&#37322;&#30340;&#38750;&#20559;&#20506;&#26679;&#26412;&#65292;&#22312;&#23454;&#38469;&#20013;&#19981;&#22826;&#23454;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#20301;&#32622;&#21435;&#20559;&#65288;ZOE&#65289;&#26694;&#26550;&#23545;LLMs&#36827;&#34892;&#20301;&#32622;&#21435;&#20559;&#12290;ZOE&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#30340;&#26080;&#30417;&#30563;&#21709;&#24212;&#36827;&#34892;&#21435;&#20559;&#65292;&#22240;&#27492;&#19981;&#38656;&#35201;&#20219;&#20309;&#22806;&#37096;&#30693;&#35782;&#25110;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#25552;&#39640;&#26080;&#30417;&#30563;&#21709;&#24212;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#20174;&#23545;&#40784;&#65288;MSA&#65289;&#27169;&#22359;&#26469;&#20462;&#21098;&#36825;&#20123;&#21709;&#24212;&#12290;&#23545;&#20843;&#20010;&#25968;&#25454;&#38598;&#21644;&#20116;&#20010;&#20219;&#21153;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ZOE&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning has been demonstrated to be an effective method to improve the domain performance of large language models (LLMs). However, LLMs might fit the dataset bias and shortcuts for prediction, leading to poor generation performance. Experimental result shows that LLMs are prone to exhibit position bias, i.e., leveraging information positioned at the beginning or end, or specific positional cues within the input. Existing works on mitigating position bias require external bias knowledge or annotated non-biased samples, which is unpractical in reality. In this work, we propose a zero-shot position debiasing (ZOE) framework to mitigate position bias for LLMs. ZOE leverages unsupervised responses from pre-trained LLMs for debiasing, thus without any external knowledge or datasets. To improve the quality of unsupervised responses, we propose a master-slave alignment (MSA) module to prune these responses. Experiments on eight datasets and five tasks show that ZOE consistently outperform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#30340;&#21487;&#34892;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#28151;&#21512;&#31574;&#30053;&#65292;&#24182;&#36827;&#34892;&#26377;&#30417;&#30563;&#24494;&#35843;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.06706</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Simultaneous Machine Translation with Large Language Models. (arXiv:2309.06706v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#30340;&#21487;&#34892;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#28151;&#21512;&#31574;&#30053;&#65292;&#24182;&#36827;&#34892;&#26377;&#30417;&#30563;&#24494;&#35843;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#35805;&#24335;&#20132;&#20114;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#24050;&#32463;&#23637;&#31034;&#20986;&#35299;&#20915;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#21487;&#20197;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#31163;&#32447;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#21462;&#24471;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23558;LLM&#24212;&#29992;&#20110;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793; (SimulMT) &#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#65292;&#21253;&#25324;&#19982;&#19981;&#21516;&#35299;&#30721;&#27169;&#24335;&#20135;&#29983;&#30340;&#35757;&#32451;-&#25512;&#29702;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#21033;&#29992;LLM&#36827;&#34892;SimulMT&#30340;&#21487;&#34892;&#24615;&#12290;&#22312;&#20256;&#32479;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#28151;&#21512;&#31574;&#30053;&#65292;&#20351;LLM&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#21442;&#19982;SimulMT&#12290;&#27492;&#22806;&#65292;&#22312;&#23545;&#20840;&#21477;&#21644;&#21069;&#32512;&#21477;&#23376;&#36827;&#34892;&#26377;&#30417;&#30563;&#24494;&#35843;&#21518;&#65292;&#35813;&#27169;&#22411;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#25105;&#20204;&#20351;&#29992;MUST-C&#25968;&#25454;&#38598;&#19978;&#30340;&#20061;&#31181;&#35821;&#35328;&#23545;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;LLM&#21487;&#20197;&#23454;&#29616;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLM) have demonstrated their abilities to solve various natural language processing tasks through dialogue-based interactions. For instance, research indicates that LLMs can achieve competitive performance in offline machine translation tasks for high-resource languages. However, applying LLMs to simultaneous machine translation (SimulMT) poses many challenges, including issues related to the training-inference mismatch arising from different decoding patterns. In this paper, we explore the feasibility of utilizing LLMs for SimulMT. Building upon conventional approaches, we introduce a simple yet effective mixture policy that enables LLMs to engage in SimulMT without requiring additional training. Furthermore, after Supervised Fine-Tuning (SFT) on a mixture of full and prefix sentences, the model exhibits significant performance improvements. Our experiments, conducted with Llama2-7B-chat on nine language pairs from the MUST-C dataset, demonstrate that LLM can ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#38463;&#35840;&#22857;&#25215;&#34892;&#20026;&#30340;&#26222;&#36941;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21512;&#25104;&#25968;&#25454;&#24178;&#39044;&#26041;&#27861;&#26469;&#20943;&#23569;&#36825;&#31181;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2308.03958</link><description>&lt;p&gt;
&#31616;&#21333;&#30340;&#21512;&#25104;&#25968;&#25454;&#20943;&#23569;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38463;&#35840;&#22857;&#25215;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Simple synthetic data reduces sycophancy in large language models. (arXiv:2308.03958v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#38463;&#35840;&#22857;&#25215;&#34892;&#20026;&#30340;&#26222;&#36941;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21512;&#25104;&#25968;&#25454;&#24178;&#39044;&#26041;&#27861;&#26469;&#20943;&#23569;&#36825;&#31181;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#35840;&#22857;&#25215;&#26159;&#19968;&#31181;&#19981;&#21487;&#21462;&#30340;&#34892;&#20026;&#65292;&#27169;&#22411;&#20250;&#26681;&#25454;&#29992;&#25143;&#30340;&#35266;&#28857;&#35843;&#25972;&#22238;&#24212;&#65292;&#21363;&#20351;&#36825;&#20010;&#35266;&#28857;&#22312;&#23458;&#35266;&#19978;&#26159;&#19981;&#27491;&#30830;&#30340;&#65288;&#20363;&#22914;&#65292;&#19968;&#26086;&#29992;&#25143;&#36879;&#38706;&#20182;&#20204;&#26159;&#33258;&#30001;&#20027;&#20041;&#32773;&#65292;&#27169;&#22411;&#20250;&#36866;&#24212;&#33258;&#30001;&#20027;&#20041;&#35266;&#28857;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#38463;&#35840;&#22857;&#25215;&#34892;&#20026;&#30340;&#26222;&#36941;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21512;&#25104;&#25968;&#25454;&#24178;&#39044;&#26469;&#20943;&#23569;&#36825;&#31181;&#34892;&#20026;&#12290;&#39318;&#20808;&#65292;&#22312;&#19968;&#32452;&#19977;&#20010;&#38463;&#35840;&#22857;&#25215;&#20219;&#21153;&#20013;&#65288;Perez&#31561;&#65292;2022&#65289;&#65292;&#27169;&#22411;&#34987;&#35201;&#27714;&#23545;&#27809;&#26377;&#27491;&#30830;&#31572;&#26696;&#30340;&#38472;&#36848;&#65288;&#20363;&#22914;&#65292;&#25919;&#27835;&#38382;&#39064;&#65289;&#21457;&#34920;&#24847;&#35265;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#30340;&#25193;&#23637;&#21644;&#25351;&#20196;&#35843;&#20248;&#26174;&#33879;&#22686;&#21152;&#20102;PaLM&#27169;&#22411;&#65288;&#22810;&#36798;540B&#21442;&#25968;&#65289;&#30340;&#38463;&#35840;&#22857;&#25215;&#34892;&#20026;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#38463;&#35840;&#22857;&#25215;&#35780;&#20272;&#25193;&#23637;&#21040;&#19968;&#20123;&#26126;&#26174;&#19981;&#27491;&#30830;&#30340;&#21152;&#27861;&#38472;&#36848;&#65292;&#21457;&#29616;&#23613;&#31649;&#27169;&#22411;&#30693;&#36947;&#36825;&#20123;&#38472;&#36848;&#26159;&#38169;&#35823;&#30340;&#65292;&#20294;&#22914;&#26524;&#29992;&#25143;&#20063;&#36825;&#20040;&#35748;&#20026;&#65292;&#35821;&#35328;&#27169;&#22411;&#20173;&#20250;&#21516;&#24847;&#36825;&#20123;&#38472;&#36848;&#12290;&#20026;&#20102;&#20943;&#23569;&#38463;&#35840;&#22857;&#25215;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21512;&#25104;&#25968;&#25454;&#24178;&#39044;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20844;&#20849;&#30340;NLP&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sycophancy is an undesirable behavior where models tailor their responses to follow a human user's view even when that view is not objectively correct (e.g., adapting liberal views once a user reveals that they are liberal). In this paper, we study the prevalence of sycophancy in language models and propose a simple synthetic-data intervention to reduce this behavior.  First, on a set of three sycophancy tasks (Perez et al., 2022) where models are asked for an opinion on statements with no correct answers (e.g., politics), we observe that both model scaling and instruction tuning significantly increase sycophancy for PaLM models up to 540B parameters. Second, we extend sycophancy evaluations to simple addition statements that are objectively incorrect, finding that despite knowing that these statements are wrong, language models will still agree with them if the user does as well.  To reduce sycophancy, we present a straightforward synthetic-data intervention that takes public NLP task
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20221;&#21517;&#20026;ODD&#30340;&#26032;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#65292;&#26816;&#27979;&#21644;&#20998;&#31867;&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#22312;&#33647;&#29289;&#30456;&#20851;&#30149;&#20363;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2307.02591</link><description>&lt;p&gt;
ODD: &#19968;&#20221;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ODD: A Benchmark Dataset for the NLP-based Opioid Related Aberrant Behavior Detection. (arXiv:2307.02591v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02591
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20221;&#21517;&#20026;ODD&#30340;&#26032;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#65292;&#26816;&#27979;&#21644;&#20998;&#31867;&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#22312;&#33647;&#29289;&#30456;&#20851;&#30149;&#20363;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#65288;ORAB&#65289;&#26159;&#38450;&#27490;&#33647;&#29289;&#36807;&#37327;&#30340;&#26032;&#39118;&#38505;&#22240;&#32032;&#12290;&#20197;&#24448;&#65292;ORAB&#20027;&#35201;&#36890;&#36807;&#35843;&#26597;&#32467;&#26524;&#21644;&#33647;&#29289;&#32473;&#20104;&#30417;&#27979;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#25193;&#23637;&#65292;&#24182;&#19981;&#33021;&#28085;&#30422;&#25152;&#26377;&#24322;&#24120;&#34892;&#20026;&#30340;&#33539;&#22260;&#12290;&#28982;&#32780;&#65292;ORAB&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#20013;&#24191;&#27867;&#26377;&#35760;&#24405;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;ODD&#30340;&#26032;&#22411;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;ORAB&#26816;&#27979;&#12290;ODD&#26159;&#19968;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;750&#22810;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#12290;ODD&#26088;&#22312;&#20174;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#20013;&#35782;&#21035;ORAB&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#20061;&#20010;&#31867;&#21035;&#65306;1&#65289;&#24050;&#30830;&#35748;&#24322;&#24120;&#34892;&#20026;&#65292;2&#65289;&#26263;&#31034;&#30340;&#24322;&#24120;&#34892;&#20026;&#65292;3&#65289;&#38463;&#29255;&#31867;&#33647;&#29289;&#65292;4&#65289;&#36866;&#24212;&#30151;&#65292;5&#65289;&#24050;&#35786;&#26029;&#30340;&#38463;&#29255;&#21046;&#21058;&#20381;&#36182;&#65292;6&#65289;&#33519;&#20108;&#27694;&#24179;&#31867;&#33647;&#29289;&#65292;7&#65289;&#33647;&#29289;&#21464;&#21270;&#65292;8&#65289;&#19982;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#30456;&#20851;&#65292;9&#65289;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Opioid related aberrant behaviors (ORAB) present novel risk factors for opioid overdose. Previously, ORAB have been mainly assessed by survey results and by monitoring drug administrations. Such methods however, cannot scale up and do not cover the entire spectrum of aberrant behaviors. On the other hand, ORAB are widely documented in electronic health record notes. This paper introduces a novel biomedical natural language processing benchmark dataset named ODD, for ORAB Detection Dataset. ODD is an expert-annotated dataset comprising of more than 750 publicly available EHR notes. ODD has been designed to identify ORAB from patients' EHR notes and classify them into nine categories; 1) Confirmed Aberrant Behavior, 2) Suggested Aberrant Behavior, 3) Opioids, 4) Indication, 5) Diagnosed opioid dependency, 6) Benzodiapines, 7) Medication Changes, 8) Central Nervous System-related, and 9) Social Determinants of Health. We explored two state-of-the-art natural language processing (NLP) mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;GPT&#27169;&#22411;&#22312;&#25277;&#35937;&#25512;&#29702;&#35821;&#26009;&#24211;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;GPT&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#20013;&#23384;&#22312;&#38656;&#35201;&#26680;&#24515;&#27010;&#24565;&#8220;&#26680;&#24515;&#30693;&#35782;&#8221;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#23545;&#35937;&#30340;&#34920;&#31034;&#26041;&#27861;&#21644;&#26032;&#30340;1D-ARC&#22522;&#20934;&#65292;GPT&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.18354</link><description>&lt;p&gt;
LLM&#21644;&#25277;&#35937;&#25512;&#29702;&#35821;&#26009;&#24211;&#65306;&#25104;&#21151;&#12289;&#22833;&#36133;&#19982;&#22522;&#20110;&#23545;&#35937;&#34920;&#31034;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations. (arXiv:2305.18354v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;GPT&#27169;&#22411;&#22312;&#25277;&#35937;&#25512;&#29702;&#35821;&#26009;&#24211;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;GPT&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#20013;&#23384;&#22312;&#38656;&#35201;&#26680;&#24515;&#27010;&#24565;&#8220;&#26680;&#24515;&#30693;&#35782;&#8221;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#23545;&#35937;&#30340;&#34920;&#31034;&#26041;&#27861;&#21644;&#26032;&#30340;1D-ARC&#22522;&#20934;&#65292;GPT&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21542;&#35299;&#20915;&#31616;&#21333;&#30340;&#25277;&#35937;&#25512;&#29702;&#38382;&#39064;&#65311;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#22320;&#20998;&#26512;GPT&#22312;&#25277;&#35937;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#19978;&#30340;&#34920;&#29616;&#26469;&#25506;&#32034;&#36825;&#20010;&#24191;&#27867;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#20195;&#34920;&#24615;&#30340;&#22522;&#20934;&#65292;&#20174;&#26377;&#38480;&#30340;&#20363;&#23376;&#20013;&#35201;&#27714;&#25105;&#20204;&#26377;&#20123;&#20851;&#20110;&#27010;&#24565;&#65288;&#22914;&#23545;&#35937;&#12289;&#30446;&#26631;&#29366;&#24577;&#12289;&#35745;&#25968;&#21644;&#22522;&#26412;&#20960;&#20309;&#65289;&#30340;&#8220;&#26680;&#24515;&#30693;&#35782;&#8221;&#20197;&#35299;&#20915;&#38382;&#39064;&#12290;&#24403;&#20351;&#29992;&#25991;&#26412;&#32534;&#30721;&#23545;&#20108;&#32500;&#36755;&#20837;&#36755;&#20986;&#32593;&#26684;&#30340;ARC&#20219;&#21153;&#36827;&#34892;&#32534;&#30721;&#26102;&#65292;GPT-4&#20165;&#35299;&#20915;&#20102;50&#20010;&#26368;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#30340;13&#20010;&#12290;&#25105;&#20204;&#30340;&#22833;&#36133;&#20998;&#26512;&#26174;&#31034;&#65292;GPT-4&#35782;&#21035;&#23545;&#35937;&#24182;&#25512;&#29702;&#23427;&#20204;&#30340;&#33021;&#21147;&#21463;&#21040;&#34920;&#31034;&#20219;&#21153;&#20013;&#23545;&#35937;&#30340;&#25991;&#26412;&#30340;&#39034;&#24207;&#24615;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;1D-ARC&#65292;&#23427;&#30001;&#26356;&#26377;&#21033;&#20110;&#22522;&#20110;GPT&#30340;&#25512;&#29702;&#30340;&#19968;&#32500;&#65288;&#31867;&#20284;&#25968;&#32452;&#65289;&#20219;&#21153;&#32452;&#25104;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#65292;GPT&#30340;&#34920;&#29616;&#30830;&#23454;&#27604;&#22312;&#65288;2D&#65289;ARC&#19978;&#26356;&#22909;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23545;&#35937;&#30340;&#32534;&#30721;&#26041;&#26696;&#65292;&#23427;&#20445;&#30041;&#20102;&#23545;&#35937;&#20043;&#38388;&#30340;&#22522;&#26412;&#31354;&#38388;&#20851;&#31995;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25512;&#29702;&#12290;&#20351;&#29992;&#36825;&#31181;&#32534;&#30721;&#65292;GPT-4&#22312;ARC&#19978;&#30340;&#25104;&#21151;&#29575;&#22823;&#22823;&#25552;&#39640;&#65292;&#36798;&#21040;&#20102;45/50&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#20351;&#29992;&#22522;&#20110;&#23545;&#35937;&#30340;&#34920;&#31034;&#26041;&#27861;&#36827;&#34892;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25581;&#31034;&#20102;LLM&#22522;&#20110;&#25512;&#29702;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#21644;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can a Large Language Model (LLM) solve simple abstract reasoning problems? We explore this broad question through a systematic analysis of GPT on the Abstraction and Reasoning Corpus (ARC), a representative benchmark of abstract reasoning ability from limited examples in which solutions require some "core knowledge" of concepts such as objects, goal states, counting, and basic geometry. GPT-4 solves only 13/50 of the most straightforward ARC tasks when using textual encodings for their two-dimensional input-output grids. Our failure analysis reveals that GPT-4's capacity to identify objects and reason about them is significantly influenced by the sequential nature of the text that represents an object within a text encoding of a task. To test this hypothesis, we design a new benchmark, the 1D-ARC, which consists of one-dimensional (array-like) tasks that are more conducive to GPT-based reasoning, and where it indeed performs better than on the (2D) ARC. To alleviate this issue, we prop
&lt;/p&gt;</description></item><item><title>DAPR&#26159;&#19968;&#20010;&#25991;&#26723;&#24863;&#30693;&#27573;&#33853;&#26816;&#32034;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#20174;&#38271;&#25991;&#26723;&#20013;&#25214;&#21040;&#27491;&#30830;&#30340;&#27573;&#33853;&#24182;&#36820;&#22238;&#20934;&#30830;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13915</link><description>&lt;p&gt;
DAPR&#65306;&#25991;&#26723;&#24863;&#30693;&#27573;&#33853;&#26816;&#32034;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
DAPR: A Benchmark on Document-Aware Passage Retrieval. (arXiv:2305.13915v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13915
&lt;/p&gt;
&lt;p&gt;
DAPR&#26159;&#19968;&#20010;&#25991;&#26723;&#24863;&#30693;&#27573;&#33853;&#26816;&#32034;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#20174;&#38271;&#25991;&#26723;&#20013;&#25214;&#21040;&#27491;&#30830;&#30340;&#27573;&#33853;&#24182;&#36820;&#22238;&#20934;&#30830;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#31070;&#32463;&#26816;&#32034;&#20027;&#35201;&#20851;&#27880;&#30701;&#25991;&#26412;&#30340;&#25490;&#21517;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#38271;&#25991;&#26723;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#35780;&#20272;&#25490;&#21517;&#27573;&#33853;&#25110;&#25972;&#20010;&#25991;&#26723;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#24076;&#26395;&#20174;&#24222;&#22823;&#30340;&#35821;&#26009;&#24211;&#20013;&#25214;&#21040;&#38271;&#25991;&#26723;&#20013;&#30340;&#30456;&#20851;&#27573;&#33853;&#65292;&#20363;&#22914;&#27861;&#24459;&#26696;&#20363;&#65292;&#30740;&#31350;&#35770;&#25991;&#31561;&#65292;&#27492;&#26102;&#27573;&#33853;&#24448;&#24448;&#25552;&#20379;&#24456;&#23569;&#30340;&#25991;&#26723;&#19978;&#19979;&#25991;&#65292;&#36825;&#23601;&#25361;&#25112;&#20102;&#24403;&#21069;&#30340;&#26041;&#27861;&#25214;&#21040;&#27491;&#30830;&#30340;&#25991;&#26723;&#24182;&#36820;&#22238;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#21629;&#21517;&#20102;Document-Aware Passage Retrieval&#65288;DAPR&#65289;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#25324;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;DAPR&#21644;&#25972;&#20010;&#25991;&#26723;&#26816;&#32034;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22312;&#25991;&#26723;&#25688;&#35201;&#20013;&#28155;&#21152;&#25991;&#26723;&#32423;&#21035;&#30340;&#20869;&#23481;&#65292;&#27719;&#24635;&#27573;&#33853;&#34920;&#31034;&#21644;&#20351;&#29992;BM25&#36827;&#34892;&#28151;&#21512;&#26816;&#32034;&#65292;&#25193;&#23637;&#20102;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#27573;&#33853;&#26816;&#32034;&#22120;&#12290;&#36825;&#20010;&#28151;&#21512;&#26816;&#32034;&#31995;&#32479;&#65292;&#24635;&#20307;&#22522;&#20934;&#27979;&#35797;&#26174;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;DAPR&#20219;&#21153;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#37325;&#35201;&#24615;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent neural retrieval mainly focuses on ranking short texts and is challenged with long documents. Existing work mainly evaluates either ranking passages or whole documents. However, there are many cases where the users want to find a relevant passage within a long document from a huge corpus, e.g. legal cases, research papers, etc. In this scenario, the passage often provides little document context and thus challenges the current approaches to finding the correct document and returning accurate results. To fill this gap, we propose and name this task Document-Aware Passage Retrieval (DAPR) and build a benchmark including multiple datasets from various domains, covering both DAPR and whole-document retrieval. In experiments, we extend the state-of-the-art neural passage retrievers with document-level context via different approaches including prepending document summary, pooling over passage representations, and hybrid retrieval with BM25. The hybrid-retrieval systems, the overall b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;X-adapter&#25554;&#25300;&#24335;&#27169;&#22359;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#39640;&#25928;&#22320;&#21521;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#27880;&#20837;&#35270;&#35273;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.07358</link><description>&lt;p&gt;
&#21033;&#29992;&#36328;&#27169;&#24577;&#36866;&#37197;&#22120;&#21521;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#27880;&#20837;&#22810;&#21151;&#33021;&#39640;&#25928;&#30340;&#35270;&#35273;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Towards Versatile and Efficient Visual Knowledge Injection into Pre-trained Language Models with Cross-Modal Adapters. (arXiv:2305.07358v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;X-adapter&#25554;&#25300;&#24335;&#27169;&#22359;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#39640;&#25928;&#22320;&#21521;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#27880;&#20837;&#35270;&#35273;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#36807;&#22810;&#27169;&#24577;&#30693;&#35782;&#23398;&#20064;&#35821;&#35328;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20165;&#25903;&#25345;&#25991;&#26412;&#39044;&#35757;&#32451;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25554;&#25300;&#24335;&#27169;&#22359;X-adapter&#65292;&#23427;&#33021;&#22815;&#26681;&#25454;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#23545;&#40784;&#35270;&#35273;&#21644;&#25991;&#26412;&#30693;&#35782;&#65292;&#28789;&#27963;&#39640;&#25928;&#22320;&#21521;PLMs&#27880;&#20837;&#35270;&#35273;&#30693;&#35782;&#12290; X-adapter&#21253;&#21547;&#20004;&#20010;&#23376;&#27169;&#22359;V-expert&#21644;T-expert&#65292;&#21487;&#20197;&#26681;&#25454;&#19979;&#28216;&#20219;&#21153;&#28608;&#27963;&#19981;&#21516;&#30340;&#23376;&#27169;&#22359;&#65292;&#26469;&#34701;&#21512;VLMs&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans learn language via multi-modal knowledge. However, due to the text-only pre-training scheme, most existing pre-trained language models (PLMs) are hindered from the multi-modal information.  To inject visual knowledge into PLMs, existing methods incorporate either the text or image encoder of vision-language models (VLMs) to encode the visual information and update all the original parameters of PLMs for knowledge fusion.  In this paper, we propose a new plug-and-play module, X-adapter, to flexibly leverage the aligned visual and textual knowledge learned in pre-trained VLMs and efficiently inject them into PLMs.  Specifically, we insert X-adapters into PLMs, and only the added parameters are updated during adaptation.  To fully exploit the potential in VLMs, X-adapters consist of two sub-modules, V-expert and T-expert, to fuse VLMs' image and text representations, respectively.  We can opt for activating different sub-modules depending on the downstream tasks.  Experimental resu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25511;&#20869;&#23384;&#31995;&#32479;&#65292;&#21487;&#20197;&#20351;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#38271;&#24230;&#30340;&#36755;&#20837;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.13343</link><description>&lt;p&gt;
&#33258;&#25511;&#20869;&#23384;&#31995;&#32479;&#37322;&#25918;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#38480;&#36755;&#20837;&#23481;&#37327;
&lt;/p&gt;
&lt;p&gt;
Unleashing Infinite-Length Input Capacity for Large-scale Language Models with Self-Controlled Memory System. (arXiv:2304.13343v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13343
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25511;&#20869;&#23384;&#31995;&#32479;&#65292;&#21487;&#20197;&#20351;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#38271;&#24230;&#30340;&#36755;&#20837;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21463;&#21046;&#20110;&#26080;&#27861;&#22788;&#29702;&#36807;&#38271;&#30340;&#36755;&#20837;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#25511;&#20869;&#23384;&#65288;SCM&#65289;&#31995;&#32479;&#65292;&#20197;&#37322;&#25918;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#38480;&#36755;&#20837;&#23481;&#37327;&#12290;&#25105;&#20204;&#30340;SCM&#31995;&#32479;&#30001;&#19977;&#20010;&#20851;&#38190;&#27169;&#22359;&#32452;&#25104;&#65306;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#12289;&#20869;&#23384;&#27969;&#21644;&#20869;&#23384;&#25511;&#21046;&#22120;&#12290;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#36845;&#20195;&#22320;&#22788;&#29702;&#36229;&#38271;&#36755;&#20837;&#65292;&#24182;&#23558;&#25152;&#26377;&#21382;&#21490;&#20449;&#24687;&#23384;&#20648;&#22312;&#20869;&#23384;&#27969;&#20013;&#12290;&#20869;&#23384;&#25511;&#21046;&#22120;&#20026;&#20195;&#29702;&#25552;&#20379;&#38271;&#26399;&#23384;&#20648;&#22120;&#65288;&#24402;&#26723;&#23384;&#20648;&#22120;&#65289;&#21644;&#30701;&#26399;&#23384;&#20648;&#22120;&#65288;&#38378;&#23384;&#65289;&#65292;&#20197;&#29983;&#25104;&#31934;&#30830;&#36830;&#36143;&#30340;&#21709;&#24212;&#12290;&#25511;&#21046;&#22120;&#30830;&#23450;&#24212;&#28608;&#27963;&#21738;&#20123;&#26469;&#33258;&#24402;&#26723;&#23384;&#20648;&#22120;&#30340;&#35760;&#24518;&#65292;&#24182;&#22914;&#20309;&#23558;&#23427;&#20204;&#21512;&#24182;&#21040;&#27169;&#22411;&#36755;&#20837;&#20013;&#12290;&#25105;&#20204;&#30340;SCM&#31995;&#32479;&#21487;&#20197;&#19982;&#20219;&#20309;LLMs&#38598;&#25104;&#65292;&#20197;&#20351;&#23427;&#20204;&#33021;&#22815;&#22788;&#29702;&#36229;&#38271;&#25991;&#26412;&#32780;&#26080;&#38656;&#20462;&#25913;&#25110;&#24494;&#35843;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;SCM&#31995;&#32479;&#20351;&#24471;LLMs&#33021;&#22815;&#22788;&#29702;&#38271;&#24230;&#39640;&#36798;8192&#20010;&#20196;&#29260;&#30340;&#36755;&#20837;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20339;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale Language Models (LLMs) are constrained by their inability to process lengthy inputs. To address this limitation, we propose the Self-Controlled Memory (SCM) system to unleash infinite-length input capacity for large-scale language models. Our SCM system is composed of three key modules: the language model agent, the memory stream, and the memory controller. The language model agent iteratively processes ultra-long inputs and stores all historical information in the memory stream. The memory controller provides the agent with both long-term memory (archived memory) and short-term memory (flash memory) to generate precise and coherent responses. The controller determines which memories from archived memory should be activated and how to incorporate them into the model input. Our SCM system can be integrated with any LLMs to enable them to process ultra-long texts without any modification or fine-tuning. Experimental results show that our SCM system enables LLMs, which are not
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#27604;&#36739;&#20102;&#20845;&#31181;&#20998;&#35789;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20154;&#31867;&#21487;&#36861;&#28335;&#30340;&#20998;&#35789;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20998;&#35789;&#19981;&#19968;&#23450;&#30456;&#21516;&#12290;</title><link>http://arxiv.org/abs/2304.10813</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20998;&#35789;&#21487;&#36861;&#28335;&#24615;&#65306;&#19968;&#20010;&#27880;&#37322;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Tokenization Tractability for Human and Machine Learning Model: An Annotation Study. (arXiv:2304.10813v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10813
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#27604;&#36739;&#20102;&#20845;&#31181;&#20998;&#35789;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20154;&#31867;&#21487;&#36861;&#28335;&#30340;&#20998;&#35789;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20998;&#35789;&#19981;&#19968;&#23450;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21487;&#36861;&#28335;&#30340;&#20998;&#35789;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#21542;&#20063;&#26159;&#21487;&#36861;&#28335;&#30340;&#65311;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#31867;&#21487;&#36861;&#28335;&#30340;&#20998;&#35789;&#65288;&#22914;&#36866;&#24403;&#24615;&#21644;&#21487;&#35835;&#24615;&#65289;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20998;&#35789;&#65288;&#22914;&#22312;NLP&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#26085;&#35821;&#24120;&#35782;&#38382;&#31572;&#25968;&#25454;&#38598;&#65288;JGLUE&#30340;JCommmonsenseQA&#65289;&#20013;&#27604;&#36739;&#20102;&#20845;&#31181;&#20998;&#35789;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#20998;&#35789;&#22120;&#23545;&#38382;&#31572;&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;&#25991;&#26412;&#36827;&#34892;&#20998;&#35789;&#65292;&#24182;&#27604;&#36739;&#20102;&#20154;&#31867;&#26631;&#27880;&#32773;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#24615;&#33021;&#12289;&#20998;&#35789;&#30340;&#36866;&#24403;&#24615;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#21709;&#24212;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#23450;&#37327;&#35843;&#26597;&#32467;&#26524;&#65292;&#26174;&#31034;&#20986;&#23545;&#20110;&#20154;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35828;&#65292;&#21487;&#36861;&#28335;&#30340;&#20998;&#35789;&#19981;&#19968;&#23450;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is tractable tokenization for humans also tractable for machine learning models? This study investigates relations between tractable tokenization for humans (e.g., appropriateness and readability) and one for models of machine learning (e.g., performance on an NLP task). We compared six tokenization methods on the Japanese commonsense question-answering dataset (JCommmonsenseQA in JGLUE). We tokenized question texts of the QA dataset with different tokenizers and compared the performance of human annotators and machine-learning models. Besides,we analyze relationships among the performance, appropriateness of tokenization, and response time to questions. This paper provides a quantitative investigation result that shows the tractable tokenizations for humans and machine learning models are not necessarily the same as each other.
&lt;/p&gt;</description></item></channel></rss>