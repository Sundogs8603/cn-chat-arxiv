<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SPHINX-X&#65292;&#19968;&#31181;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#12290;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#19981;&#21516;&#30340;MLLMs&#65292;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#26377;&#24378;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05935</link><description>&lt;p&gt;
SPHINX-X: &#25193;&#23637;&#25968;&#25454;&#21644;&#21442;&#25968;&#29992;&#20110;&#19968;&#31995;&#21015;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SPHINX-X&#65292;&#19968;&#31181;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#12290;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#19981;&#21516;&#30340;MLLMs&#65292;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#26377;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;SPHINX-X&#65292;&#19968;&#31181;&#22522;&#20110;SPHINX&#24320;&#21457;&#30340;&#24191;&#27867;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#31995;&#21015;&#12290;&#20026;&#20102;&#25913;&#21892;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#36890;&#36807;&#31227;&#38500;&#20887;&#20313;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#12289;&#32469;&#36807;&#23436;&#20840;&#22635;&#20805;&#30340;&#23376;&#22270;&#20687;&#65292;&#24182;&#23558;&#22810;&#38454;&#27573;&#35757;&#32451;&#31616;&#21270;&#25104;&#20026;&#19968;&#38454;&#27573;&#30340;&#20840;&#38598;&#21512;&#27169;&#24335;&#65292;&#20462;&#25913;&#20102;SPHINX&#26694;&#26550;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;MLLM&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#32452;&#35013;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#36328;&#35821;&#35328;&#12289;&#36328;&#35270;&#35273;&#21644;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#30340;&#22810;&#39046;&#22495;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#25105;&#20204;&#30340;OCR&#23494;&#38598;&#21644;Mark&#25968;&#25454;&#38598;&#20016;&#23500;&#36825;&#20010;&#25910;&#38598;&#65292;&#25193;&#23637;&#20102;&#22810;&#26679;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#22522;&#30784;LLM&#36827;&#34892;&#35757;&#32451;&#65292;&#21253;&#25324;TinyLlama1.1B&#12289;InternLM2-7B&#12289;LLaMA2-13B&#21644;Mixtral8x7B&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#21464;&#21270;&#30340;MLLMs&#12290;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#20102;&#22810;&#27169;&#24577;&#24615;&#33021;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM) series developed upon SPHINX. To improve the architecture and training efficiency, we modify the SPHINX framework by removing redundant visual encoders, bypassing fully-padded sub-images with skip tokens, and simplifying multi-stage training into a one-stage all-in-one paradigm. To fully unleash the potential of MLLMs, we assemble a comprehensive multi-domain and multimodal dataset covering publicly available resources in language, vision, and vision-language tasks. We further enrich this collection with our curated OCR intensive and Set-of-Mark datasets, extending the diversity and generality. By training over different base LLMs including TinyLlama1.1B, InternLM2-7B, LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in parameter size and multilingual capabilities. Comprehensive benchmarking reveals a strong correlation between the multi-modal performance with the data and parameter scales. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;LLaDA&#65292;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#65292;&#20351;&#24471;&#39550;&#39542;&#21592;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#33021;&#22815;&#22312;&#21508;&#22320;&#39550;&#39542;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#21644;&#36816;&#21160;&#35745;&#21010;&#35843;&#25972;&#21040;&#26032;&#20301;&#32622;&#30340;&#20132;&#36890;&#35268;&#21017;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;LLaDA&#25351;&#23548;&#22312;&#35299;&#20915;&#24847;&#22806;&#24773;&#20917;&#21644;&#36866;&#24212;AV&#36816;&#21160;&#35745;&#21010;&#31574;&#30053;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05932</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25919;&#31574;&#36866;&#24212;&#22312;&#21508;&#22788;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Driving Everywhere with Large Language Model Policy Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LLaDA&#65292;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#65292;&#20351;&#24471;&#39550;&#39542;&#21592;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#33021;&#22815;&#22312;&#21508;&#22320;&#39550;&#39542;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#21644;&#36816;&#21160;&#35745;&#21010;&#35843;&#25972;&#21040;&#26032;&#20301;&#32622;&#30340;&#20132;&#36890;&#35268;&#21017;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;LLaDA&#25351;&#23548;&#22312;&#35299;&#20915;&#24847;&#22806;&#24773;&#20917;&#21644;&#36866;&#24212;AV&#36816;&#21160;&#35745;&#21010;&#31574;&#30053;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#23558;&#39550;&#39542;&#34892;&#20026;&#36866;&#24212;&#26032;&#29615;&#22659;&#12289;&#20064;&#20439;&#21644;&#27861;&#24459;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#36825;&#38480;&#21046;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;(AVs)&#30340;&#24191;&#27867;&#37096;&#32626;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LLaDA&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#20351;&#20154;&#31867;&#39550;&#39542;&#21592;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#37117;&#33021;&#22312;&#21508;&#22788;&#34892;&#39542;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#21644;&#36816;&#21160;&#35745;&#21010;&#35843;&#25972;&#21040;&#26032;&#20301;&#32622;&#30340;&#20132;&#36890;&#35268;&#21017;&#12290;LLaDA&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35299;&#37322;&#26412;&#22320;&#39550;&#39542;&#25163;&#20876;&#20013;&#30340;&#20132;&#36890;&#35268;&#21017;&#26102;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;-shot&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLaDA&#30340;&#25351;&#23548;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#24847;&#22806;&#24773;&#20917;&#26102;&#30340;&#26377;&#29992;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;LLaDA&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#35843;&#25972;AV&#36816;&#21160;&#35745;&#21010;&#31574;&#30053;&#30340;&#33021;&#21147;&#65307;LLaDA&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#20248;&#20110;&#22522;&#32447;&#35268;&#21010;&#26041;&#27861;&#12290;&#35831;&#35775;&#38382;&#25105;&#20204;&#30340;&#32593;&#31449;&#20102;&#35299;&#26356;&#22810;&#35814;&#32454;&#20449;&#24687;&#65306;https://boyiliee.github.io/llada.
&lt;/p&gt;
&lt;p&gt;
Adapting driving behavior to new environments, customs, and laws is a long-standing problem in autonomous driving, precluding the widespread deployment of autonomous vehicles (AVs). In this paper, we present LLaDA, a simple yet powerful tool that enables human drivers and autonomous vehicles alike to drive everywhere by adapting their tasks and motion plans to traffic rules in new locations. LLaDA achieves this by leveraging the impressive zero-shot generalizability of large language models (LLMs) in interpreting the traffic rules in the local driver handbook. Through an extensive user study, we show that LLaDA's instructions are useful in disambiguating in-the-wild unexpected situations. We also demonstrate LLaDA's ability to adapt AV motion planning policies in real-world datasets; LLaDA outperforms baseline planning approaches on all our metrics. Please check our website for more details: https://boyiliee.github.io/llada.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#35805;&#24335;&#32593;&#31449;&#23548;&#33322;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010; WEBLINX &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#20195;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#37327;&#20449;&#24687;&#30340;&#22788;&#29702;&#29942;&#39048;&#65292;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#26816;&#32034;&#21551;&#21457;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#22810;&#31181;&#22330;&#26223;&#19979;&#22797;&#21046;&#20154;&#31867;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05930</link><description>&lt;p&gt;
WebLINX: &#22810;&#36718;&#23545;&#35805;&#19979;&#30340;&#30495;&#23454;&#19990;&#30028;&#32593;&#31449;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
WebLINX: Real-World Website Navigation with Multi-Turn Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#35805;&#24335;&#32593;&#31449;&#23548;&#33322;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010; WEBLINX &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#20195;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#37327;&#20449;&#24687;&#30340;&#22788;&#29702;&#29942;&#39048;&#65292;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#26816;&#32034;&#21551;&#21457;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#22810;&#31181;&#22330;&#26223;&#19979;&#22797;&#21046;&#20154;&#31867;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#35805;&#24335;&#32593;&#31449;&#23548;&#33322;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#25968;&#23383;&#20195;&#29702;&#25511;&#21046;&#30528;&#19968;&#20010;&#32593;&#39029;&#27983;&#35272;&#22120;&#65292;&#24182;&#25353;&#29031;&#29992;&#25143;&#30340;&#25351;&#20196;&#20197;&#22810;&#36718;&#23545;&#35805;&#30340;&#26041;&#24335;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; WEBLINX - &#19968;&#20010;100K&#20132;&#20114;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#65292;&#22312;2300&#20010;&#19987;&#23478;&#28436;&#31034;&#20013;&#36827;&#34892;&#20102;&#23545;&#35805;&#24335;&#32593;&#31449;&#23548;&#33322;&#30340;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#28085;&#30422;&#20102;150&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#32593;&#31449;&#19978;&#30340;&#24191;&#27867;&#27169;&#24335;&#65292;&#21487;&#20197;&#29992;&#20110;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#35757;&#32451;&#21644;&#35780;&#20272;&#20195;&#29702;&#12290;&#30001;&#20110;&#23384;&#22312;&#22823;&#37327;&#20449;&#24687;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#26080;&#27861;&#23454;&#26102;&#22788;&#29702;&#25972;&#20010;&#32593;&#39029;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#29942;&#39048;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21463;&#26816;&#32034;&#21551;&#21457;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#25490;&#21517;&#30456;&#20851;&#20803;&#32032;&#26469;&#39640;&#25928;&#22320;&#20462;&#21098; HTML &#39029;&#38754;&#12290;&#25105;&#20204;&#20351;&#29992;&#36873;&#23450;&#30340;&#20803;&#32032;&#65292;&#20197;&#21450;&#23631;&#24149;&#25130;&#22270;&#21644;&#25805;&#20316;&#21382;&#21490;&#35760;&#24405;&#65292;&#35780;&#20272;&#21508;&#31181;&#27169;&#22411;&#22312;&#23548;&#33322;&#32593;&#39029;&#26102;&#22797;&#21046;&#20154;&#31867;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20174;&#23567;&#22411;&#32431;&#25991;&#26412;&#27169;&#22411;&#21040;&#19987;&#26377;&#30340;&#22810;&#27169;&#24577; LLMs &#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the problem of conversational web navigation, where a digital agent controls a web browser and follows user instructions to solve real-world tasks in a multi-turn dialogue fashion. To support this problem, we introduce WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert demonstrations of conversational web navigation. Our benchmark covers a broad range of patterns on over 150 real-world websites and can be used to train and evaluate agents in diverse scenarios. Due to the magnitude of information present, Large Language Models (LLMs) cannot process entire web pages in real-time. To solve this bottleneck, we design a retrieval-inspired model that efficiently prunes HTML pages by ranking relevant elements. We use the selected elements, along with screenshots and action history, to assess a variety of models for their ability to replicate human behavior when navigating the web. Our experiments span from small text-only to proprietary multimodal LLMs. We fi
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedMeZO&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#38646;&#38454;&#32852;&#37030;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#23454;&#29616;&#20869;&#23384;&#39640;&#25928;&#30340;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;FedMeZO&#19981;&#20165;&#25910;&#25947;&#36895;&#24230;&#24555;&#20110;&#20256;&#32479;&#30340;&#19968;&#38454;&#26041;&#27861;&#65292;&#32780;&#19988;&#22312;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#31574;&#30053;&#20013;&#20855;&#26377;&#20851;&#38190;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.05926</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#38646;&#38454;&#32852;&#37030;&#35843;&#25972;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of Zeroth-Order Federated Tuning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05926
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedMeZO&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#38646;&#38454;&#32852;&#37030;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#23454;&#29616;&#20869;&#23384;&#39640;&#25928;&#30340;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;FedMeZO&#19981;&#20165;&#25910;&#25947;&#36895;&#24230;&#24555;&#20110;&#20256;&#32479;&#30340;&#19968;&#38454;&#26041;&#27861;&#65292;&#32780;&#19988;&#22312;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#31574;&#30053;&#20013;&#20855;&#26377;&#20851;&#38190;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#34701;&#21512;&#20026;&#38544;&#31169;&#20445;&#25252;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24102;&#26469;&#20102;&#26032;&#26102;&#20195;&#12290;&#28982;&#32780;&#65292;&#31934;&#35843;LLM&#25152;&#38656;&#30340;&#24378;&#22823;&#20869;&#23384;&#35201;&#27714;&#22312;&#37096;&#32626;&#21040;&#36793;&#32536;&#35774;&#22791;&#26102;&#20250;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#35774;&#22791;&#30340;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#32852;&#37030;&#29615;&#22659;&#20013;&#25506;&#32034;&#20102;&#20869;&#23384;&#39640;&#25928;&#30340;&#38646;&#38454;&#20248;&#21270;&#30340;&#20840;&#26032;&#25972;&#21512;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;FedMeZO&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#31532;&#19968;&#20010;&#22312;LLM&#32972;&#26223;&#19979;&#32771;&#23519;FedMeZO&#30340;&#29702;&#35770;&#22522;&#30784;&#30340;&#30740;&#31350;&#65292;&#28041;&#21450;&#21040;&#22823;&#21442;&#25968;&#31354;&#38388;&#23545;&#20248;&#21270;&#34892;&#20026;&#30340;&#24433;&#21709;&#12289;&#25910;&#25947;&#24615;&#30340;&#24314;&#31435;&#20197;&#21450;&#20026;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#31574;&#30053;&#30830;&#23450;&#20851;&#38190;&#21442;&#25968;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#35777;&#35777;&#25454;&#25903;&#25345;&#20102;&#36825;&#20010;&#29702;&#35770;&#65292;&#34920;&#26126;FedMeZO&#19981;&#20165;&#27604;&#20256;&#32479;&#30340;&#19968;&#38454;&#26041;&#27861;&#65288;&#22914;SGD&#65289;&#25910;&#25947;&#26356;&#24555;&#65292;&#32780;&#19988;&#26126;&#26174;...
&lt;/p&gt;
&lt;p&gt;
The confluence of Federated Learning (FL) and Large Language Models (LLMs) is ushering in a new era in privacy-preserving natural language processing. However, the intensive memory requirements for fine-tuning LLMs pose significant challenges, especially when deploying on edge devices with limited computational resources. To circumvent this, we explore the novel integration of Memory-efficient Zeroth-Order Optimization within a federated setting, a synergy we denote as FedMeZO. Our study is the first to examine the theoretical underpinnings of FedMeZO in the context of LLMs, tackling key questions regarding the influence of large parameter spaces on optimization behavior, the establishment of convergence properties, and the identification of critical parameters for convergence to inform personalized federated strategies. Our extensive empirical evidence supports the theory, showing that FedMeZO not only converges faster than traditional first-order methods such as SGD but also signific
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28176;&#36827;&#23376;&#32593;&#32476;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#20998;&#38454;&#27573;&#39044;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#26089;&#26399;&#38454;&#27573;&#26080;&#27861;&#35780;&#20272;&#23436;&#25972;&#27169;&#22411;&#21644;&#27169;&#22411;&#36136;&#37327;&#19979;&#38477;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05913</link><description>&lt;p&gt;
&#36890;&#36807;&#28176;&#36827;&#23376;&#32593;&#32476;&#23454;&#29616;&#39640;&#25928;&#30340;&#20998;&#38454;&#27573;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Efficient Stagewise Pretraining via Progressive Subnetworks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05913
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28176;&#36827;&#23376;&#32593;&#32476;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#20998;&#38454;&#27573;&#39044;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#26089;&#26399;&#38454;&#27573;&#26080;&#27861;&#35780;&#20272;&#23436;&#25972;&#27169;&#22411;&#21644;&#27169;&#22411;&#36136;&#37327;&#19979;&#38477;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#39640;&#25928;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#30340;&#19968;&#20010;&#26377;&#25928;&#33539;&#20363;&#26159;&#36827;&#34892;&#20998;&#38454;&#27573;&#35757;&#32451;&#65292;&#21363;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#22686;&#21152;&#27169;&#22411;&#30340;&#22823;&#23567;&#65288;&#20363;&#22914;&#36880;&#28176;&#21472;&#21152;&#65288;Reddi&#31561;&#20154;&#65292;2023&#24180;&#65289;&#65289;&#12290;&#34429;&#28982;&#36164;&#28304;&#21644;&#22681;&#38047;&#26102;&#38388;&#30340;&#33410;&#30465;&#24456;&#21560;&#24341;&#20154;&#65292;&#20294;&#23427;&#20063;&#26377;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#26089;&#26399;&#38454;&#27573;&#26080;&#27861;&#35780;&#20272;&#23436;&#25972;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#30001;&#20110;&#21021;&#22987;&#38454;&#27573;&#27169;&#22411;&#23481;&#37327;&#36739;&#23567;&#32780;&#23548;&#33268;&#27169;&#22411;&#36136;&#37327;&#19979;&#38477;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#24615;&#26694;&#26550;&#65292;&#21363;&#28176;&#36827;&#23376;&#32593;&#32476;&#35757;&#32451;&#65292;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#23436;&#25972;&#30340;&#27169;&#22411;&#65292;&#20294;&#27599;&#20010;&#27493;&#39588;&#21482;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#36825;&#20010;&#26694;&#26550;&#30340;&#19968;&#20010;&#31616;&#21333;&#23454;&#20363;&#65292;&#21363;&#38543;&#26426;&#36335;&#24452;&#35757;&#32451;&#65288;RaPTr&#65289;&#65292;&#23427;&#22312;&#27599;&#20010;&#27493;&#39588;&#20013;&#21482;&#35757;&#32451;&#19968;&#26465;&#23376;&#36335;&#24452;&#65292;&#36880;&#28176;&#22686;&#21152;&#36335;&#24452;&#38271;&#24230;&#12290;RaPTr&#22312;BERT&#21644;UL2&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25439;&#22833;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#21482;&#38656;&#35201;2
&lt;/p&gt;
&lt;p&gt;
Recent developments in large language models have sparked interest in efficient pretraining methods. A recent effective paradigm is to perform stage-wise training, where the size of the model is gradually increased over the course of training (e.g. gradual stacking (Reddi et al., 2023)). While the resource and wall-time savings are appealing, it has limitations, particularly the inability to evaluate the full model during earlier stages, and degradation in model quality due to smaller model capacity in the initial stages. In this work, we propose an alternative framework, progressive subnetwork training, that maintains the full model throughout training, but only trains subnetworks within the model in each step. We focus on a simple instantiation of this framework, Random Path Training (RaPTr) that only trains a sub-path of layers in each step, progressively increasing the path lengths in stages. RaPTr achieves better pre-training loss for BERT and UL2 language models while requiring 2
&lt;/p&gt;</description></item><item><title>FACT-GPT&#26159;&#19968;&#20010;&#21033;&#29992;LLMs&#33258;&#21160;&#21270;&#20027;&#24352;&#21305;&#37197;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#35757;&#32451;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#65292;&#23427;&#21487;&#20197;&#35782;&#21035;&#19982;&#20808;&#21069;&#34987;&#25581;&#31359;&#30340;&#20027;&#24352;&#30456;&#19968;&#33268;&#12289;&#30456;&#30683;&#30462;&#25110;&#26080;&#20851;&#30340;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#65292;&#24182;&#22312;&#35782;&#21035;&#30456;&#20851;&#20027;&#24352;&#26041;&#38754;&#30340;&#20934;&#30830;&#24230;&#19982;&#26356;&#22823;&#27169;&#22411;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2402.05904</link><description>&lt;p&gt;
FACT-GPT: &#36890;&#36807;&#19982;LLMs&#36827;&#34892;&#20027;&#24352;&#21305;&#37197;&#26469;&#22686;&#24378;&#20107;&#23454;&#26680;&#26597;
&lt;/p&gt;
&lt;p&gt;
FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05904
&lt;/p&gt;
&lt;p&gt;
FACT-GPT&#26159;&#19968;&#20010;&#21033;&#29992;LLMs&#33258;&#21160;&#21270;&#20027;&#24352;&#21305;&#37197;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#35757;&#32451;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#65292;&#23427;&#21487;&#20197;&#35782;&#21035;&#19982;&#20808;&#21069;&#34987;&#25581;&#31359;&#30340;&#20027;&#24352;&#30456;&#19968;&#33268;&#12289;&#30456;&#30683;&#30462;&#25110;&#26080;&#20851;&#30340;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#65292;&#24182;&#22312;&#35782;&#21035;&#30456;&#20851;&#20027;&#24352;&#26041;&#38754;&#30340;&#20934;&#30830;&#24230;&#19982;&#26356;&#22823;&#27169;&#22411;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#31038;&#20250;&#38754;&#20020;&#30528;&#29462;&#29527;&#30340;&#34394;&#20551;&#20449;&#24687;&#65292;&#21361;&#23475;&#20844;&#20849;&#20581;&#24247;&#21644;&#20449;&#20219;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#31038;&#20250;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FACT-GPT&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#33258;&#21160;&#21270;&#20107;&#23454;&#26680;&#26597;&#20013;&#30340;&#20027;&#24352;&#21305;&#37197;&#38454;&#27573;&#30340;&#31995;&#32479;&#12290;FACT-GPT&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#21487;&#20197;&#35782;&#21035;&#19982;&#20808;&#21069;&#34987;&#25581;&#31359;&#30340;&#20027;&#24352;&#30456;&#19968;&#33268;&#12289;&#30456;&#30683;&#30462;&#25110;&#26080;&#20851;&#30340;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#19987;&#38376;&#35774;&#35745;&#30340;LLMs&#22312;&#35782;&#21035;&#30456;&#20851;&#20027;&#24352;&#26041;&#38754;&#30340;&#20934;&#30830;&#24230;&#19982;&#26356;&#22823;&#27169;&#22411;&#30456;&#24403;&#65292;&#20960;&#20046;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#21028;&#26029;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#39640;&#25928;&#30340;&#20027;&#24352;&#21305;&#37197;&#25552;&#20379;&#20102;&#33258;&#21160;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#23637;&#31034;&#20102;LLMs&#22312;&#25903;&#25345;&#20107;&#23454;&#26680;&#26597;&#21592;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#20026;&#35813;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our society is facing rampant misinformation harming public health and trust. To address the societal challenge, we introduce FACT-GPT, a system leveraging Large Language Models (LLMs) to automate the claim matching stage of fact-checking. FACT-GPT, trained on a synthetic dataset, identifies social media content that aligns with, contradicts, or is irrelevant to previously debunked claims. Our evaluation shows that our specialized LLMs can match the accuracy of larger models in identifying related claims, closely mirroring human judgment. This research provides an automated solution for efficient claim matching, demonstrates the potential of LLMs in supporting fact-checkers, and offers valuable resources for further research in the field.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CREMA&#30340;&#39640;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20219;&#24847;&#26032;&#30340;&#27169;&#24577;&#27880;&#20837;&#35270;&#39057;&#25512;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#22686;&#24378;&#22810;&#31181;&#20449;&#24687;&#27169;&#24577;&#65292;&#24182;&#24341;&#20837;&#26597;&#35810;&#36716;&#25442;&#22120;&#21644;&#34701;&#21512;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;&#32452;&#21512;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.05889</link><description>&lt;p&gt;
CREMA: &#36890;&#36807;&#26377;&#25928;&#30340;&#27169;&#22359;&#21270;&#36866;&#24212;&#21644;&#34701;&#21512;&#36827;&#34892;&#22810;&#27169;&#24577;&#32452;&#21512;&#35270;&#39057;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CREMA&#30340;&#39640;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20219;&#24847;&#26032;&#30340;&#27169;&#24577;&#27880;&#20837;&#35270;&#39057;&#25512;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#22686;&#24378;&#22810;&#31181;&#20449;&#24687;&#27169;&#24577;&#65292;&#24182;&#24341;&#20837;&#26597;&#35810;&#36716;&#25442;&#22120;&#21644;&#34701;&#21512;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;&#32452;&#21512;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22810;&#27169;&#24577;&#32452;&#21512;&#25512;&#29702;&#26041;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#22788;&#29702;&#22266;&#23450;&#27169;&#24577;&#36755;&#20837;&#24182;&#26356;&#26032;&#35768;&#22810;&#27169;&#22411;&#21442;&#25968;&#65292;&#20173;&#28982;&#23384;&#22312;&#28789;&#27963;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;CREMA&#65292;&#19968;&#31181;&#29992;&#20110;&#23558;&#20219;&#20309;&#26032;&#30340;&#27169;&#24577;&#27880;&#20837;&#35270;&#39057;&#25512;&#29702;&#30340;&#39640;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#12290;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20174;&#32473;&#23450;&#30340;&#35270;&#39057;&#20013;&#22686;&#24378;&#22810;&#31181;&#20449;&#24687;&#27169;&#24577;&#65288;&#22914;&#20809;&#27969;&#12289;3D&#28857;&#20113;&#12289;&#38899;&#39057;&#65289;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26597;&#35810;&#36716;&#25442;&#22120;&#65292;&#35813;&#36716;&#25442;&#22120;&#19982;&#27599;&#20010;&#21487;&#20197;&#35775;&#38382;&#30340;&#27169;&#24577;&#30456;&#20851;&#32852;&#65292;&#24182;&#20855;&#26377;&#22810;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#27169;&#22359;&#12290;&#23427;&#23558;&#22810;&#31181;&#27169;&#24577;&#29305;&#24449;&#25237;&#24433;&#21040;LLM&#20196;&#29260;&#23884;&#20837;&#31354;&#38388;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25972;&#21512;&#19981;&#21516;&#30340;&#25968;&#25454;&#31867;&#22411;&#20197;&#36827;&#34892;&#21709;&#24212;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#27169;&#22359;&#65292;&#29992;&#20110;&#21387;&#32553;&#22810;&#27169;&#24577;&#26597;&#35810;&#65292;&#22312;LLM&#20013;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#36827;&#34892;&#34701;&#21512;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite impressive advancements in multimodal compositional reasoning approaches, they are still limited in their flexibility and efficiency by processing fixed modality inputs while updating a lot of model parameters. This paper tackles these critical challenges and proposes CREMA, an efficient and modular modality-fusion framework for injecting any new modality into video reasoning. We first augment multiple informative modalities (such as optical flow, 3D point cloud, audio) from given videos without extra human annotation by leveraging existing pre-trained models. Next, we introduce a query transformer with multiple parameter-efficient modules associated with each accessible modality. It projects diverse modality features to the LLM token embedding space, allowing the model to integrate different data types for response generation. Furthermore, we propose a fusion module designed to compress multimodal queries, maintaining computational efficiency in the LLM while combining additio
&lt;/p&gt;</description></item><item><title>LLM&#39537;&#21160;&#30340;&#23545;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#22686;&#21152;&#20102;&#36873;&#25321;&#24615;&#26333;&#20809;&#65292;&#19988;&#25903;&#25345;&#29992;&#25143;&#35266;&#28857;&#30340;&#26377;&#20559;&#35265;LLM&#21152;&#21095;&#20102;&#36825;&#31181;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.05880</link><description>&lt;p&gt;
&#29983;&#25104;&#24615;&#22238;&#38899;&#23460;&#65311;LLM&#39537;&#21160;&#30340;&#25628;&#32034;&#31995;&#32479;&#23545;&#22810;&#26679;&#21270;&#20449;&#24687;&#25628;&#32034;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Generative Echo Chamber? Effects of LLM-Powered Search Systems on Diverse Information Seeking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05880
&lt;/p&gt;
&lt;p&gt;
LLM&#39537;&#21160;&#30340;&#23545;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#22686;&#21152;&#20102;&#36873;&#25321;&#24615;&#26333;&#20809;&#65292;&#19988;&#25903;&#25345;&#29992;&#25143;&#35266;&#28857;&#30340;&#26377;&#20559;&#35265;LLM&#21152;&#21095;&#20102;&#36825;&#31181;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#20159;&#20154;&#24050;&#32463;&#20351;&#29992;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39537;&#21160;&#30340;&#23545;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#65292;&#24182;&#19988;&#30456;&#20449;&#36825;&#20123;&#31995;&#32479;&#30456;&#27604;&#20256;&#32479;&#25628;&#32034;&#24102;&#26469;&#20102;&#35768;&#22810;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#20960;&#21313;&#24180;&#30340;&#30740;&#31350;&#21644;&#20844;&#20849;&#35752;&#35770;&#37117;&#35843;&#26597;&#20102;&#25628;&#32034;&#31995;&#32479;&#22312;&#22686;&#21152;&#36873;&#25321;&#24615;&#26333;&#20809;&#21644;&#20135;&#29983;&#22238;&#38899;&#23460;&#26041;&#38754;&#30340;&#39118;&#38505;&#65292;&#21363;&#38480;&#21046;&#25509;&#35302;&#22810;&#26679;&#21270;&#24847;&#35265;&#24182;&#23548;&#33268;&#24847;&#35265;&#20559;&#25191;&#65292;&#20294;&#23545;&#20110;LLM&#39537;&#21160;&#30340;&#23545;&#35805;&#24335;&#25628;&#32034;&#30340;&#36825;&#31181;&#39118;&#38505;&#30693;&#20043;&#29978;&#23569;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#23454;&#39564;&#26469;&#30740;&#31350;&#65306;1&#65289;LLM&#39537;&#21160;&#30340;&#23545;&#35805;&#24335;&#25628;&#32034;&#30456;&#36739;&#20110;&#20256;&#32479;&#25628;&#32034;&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#22686;&#21152;&#36873;&#25321;&#24615;&#26333;&#20809;&#65307;2&#65289;&#20855;&#26377;&#25903;&#25345;&#25110;&#25361;&#25112;&#29992;&#25143;&#35266;&#28857;&#30340;&#24847;&#35265;&#20559;&#35265;&#30340;LLM&#22914;&#20309;&#25913;&#21464;&#36825;&#31181;&#24433;&#21709;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#21442;&#19982;&#32773;&#22312;LLM&#39537;&#21160;&#30340;&#23545;&#35805;&#24335;&#25628;&#32034;&#20013;&#26356;&#20542;&#21521;&#20110;&#36827;&#34892;&#20559;&#35265;&#30340;&#20449;&#24687;&#26597;&#35810;&#65292;&#24182;&#19988;&#25903;&#25345;&#20182;&#20204;&#35266;&#28857;&#30340;&#26377;&#20559;&#35265;&#30340;LLM&#21152;&#21095;&#20102;&#36825;&#31181;&#20559;&#24046;&#12290;&#36825;&#20123;&#32467;&#26524;&#21576;&#29616;&#20102;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) powered conversational search systems have already been used by hundreds of millions of people, and are believed to bring many benefits over conventional search. However, while decades of research and public discourse interrogated the risk of search systems in increasing selective exposure and creating echo chambers -- limiting exposure to diverse opinions and leading to opinion polarization, little is known about such a risk of LLM-powered conversational search. We conduct two experiments to investigate: 1) whether and how LLM-powered conversational search increases selective exposure compared to conventional search; 2) whether and how LLMs with opinion biases that either reinforce or challenge the user's view change the effect. Overall, we found that participants engaged in more biased information querying with LLM-powered conversational search, and an opinionated LLM reinforcing their views exacerbated this bias. These results present critical implicatio
&lt;/p&gt;</description></item><item><title>PromptCrypt&#26159;&#19968;&#31181;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#30340;&#26426;&#21046;&#65292;&#20445;&#25252;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;&#38450;&#27490;&#25968;&#25454;&#27844;&#38706;&#21644;&#35299;&#23494;&#12290;</title><link>https://arxiv.org/abs/2402.05868</link><description>&lt;p&gt;
PromptCrypt: &#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#36890;&#20449;&#30340;&#25552;&#31034;&#21152;&#23494;
&lt;/p&gt;
&lt;p&gt;
PromptCrypt: Prompt Encryption for Secure Communication with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05868
&lt;/p&gt;
&lt;p&gt;
PromptCrypt&#26159;&#19968;&#31181;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#30340;&#26426;&#21046;&#65292;&#20445;&#25252;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;&#38450;&#27490;&#25968;&#25454;&#27844;&#38706;&#21644;&#35299;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20113;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#22312;&#26085;&#24120;&#25805;&#20316;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#25104;&#20026;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#21487;&#35775;&#38382;&#24615;&#21644;&#21151;&#33021;&#24615;&#26041;&#38754;&#24102;&#26469;&#20102;&#37325;&#22823;&#22909;&#22788;&#65292;&#20294;&#23427;&#20204;&#20063;&#24341;&#20837;&#20102;&#37325;&#35201;&#30340;&#38544;&#31169;&#38382;&#39064;&#65306;&#22312;&#20113;&#22522;&#30784;&#26550;&#26500;&#20013;&#20256;&#36755;&#21644;&#23384;&#20648;&#29992;&#25143;&#25968;&#25454;&#20250;&#20135;&#29983;&#37325;&#22823;&#30340;&#25968;&#25454;&#27844;&#38706;&#21644;&#26410;&#32463;&#25480;&#26435;&#35775;&#38382;&#25935;&#24863;&#20449;&#24687;&#30340;&#39118;&#38505;&#65307;&#21363;&#20351;&#25968;&#25454;&#30340;&#20256;&#36755;&#21644;&#23384;&#20648;&#34987;&#21152;&#23494;&#65292;LLM&#26381;&#21153;&#25552;&#20379;&#21830;&#20173;&#28982;&#30693;&#36947;&#25968;&#25454;&#30340;&#30495;&#23454;&#20869;&#23481;&#65292;&#20174;&#32780;&#38459;&#27490;&#20010;&#20154;&#25110;&#23454;&#20307;&#25918;&#24515;&#20351;&#29992;&#27492;&#31867;LLM&#26381;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26426;&#21046;PromptCrypt&#26469;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;&#23427;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#65292;&#28982;&#21518;&#23558;&#20854;&#21457;&#36865;&#21040;LLM&#65292;&#26377;&#25928;&#22320;&#20351;&#20854;&#23545;&#20154;&#31867;&#25110;LLM&#30340;&#26816;&#26597;&#26080;&#27861;&#29702;&#35299;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#25552;&#31034;&#30340;&#24847;&#22270;&#65292;&#20174;&#32780;&#30830;&#20445;&#29992;&#25143;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud-based large language models (LLMs) such as ChatGPT have increasingly become integral to daily operations, serving as vital tools across various applications. While these models offer substantial benefits in terms of accessibility and functionality, they also introduce significant privacy concerns: the transmission and storage of user data in cloud infrastructures pose substantial risks of data breaches and unauthorized access to sensitive information; even if the transmission and storage of data is encrypted, the LLM service provider itself still knows the real contents of the data, preventing individuals or entities from confidently using such LLM services. To address these concerns, this paper proposes a simple yet effective mechanism PromptCrypt to protect user privacy. It uses Emoji to encrypt the user inputs before sending them to LLM, effectively rendering them indecipherable to human or LLM's examination while retaining the original intent of the prompt, thus ensuring the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Permute-and-Flip&#65288;PF&#65289;&#35299;&#30721;&#22120;&#65292;&#20854;&#20855;&#26377;&#26368;&#20339;&#30340;&#40065;&#26834;&#24615;&#21644;&#36136;&#37327;-&#40065;&#26834;&#24615;&#30340; tradeoff&#65292;&#19988;&#27604;&#37319;&#26679;&#26041;&#27861;&#26356;&#22909;&#12290;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#38024;&#23545;PF&#35299;&#30721;&#22120;&#30340;&#27700;&#21360;&#26041;&#26696;&#65292;&#33021;&#22815;&#20445;&#25345;&#26679;&#26412;&#30340;&#20998;&#24067;&#19981;&#21464;&#65292;&#24182;&#23454;&#29616;&#20219;&#24847;&#20302;&#30340;&#20551;&#38451;&#24615;&#29575;&#21644;&#39640;&#30340;&#21484;&#22238;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;PF&#35299;&#30721;&#22120;&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#26420;&#32032;&#37319;&#26679;&#65292;&#20026;LLM&#35299;&#30721;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05864</link><description>&lt;p&gt;
Permute-and-Flip&#65306;&#19968;&#31181;&#20855;&#26377;&#26368;&#20339;&#40065;&#26834;&#24615;&#21644;&#21487;&#21152;&#27700;&#21360;&#30340;LLMs&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Permute-and-Flip: An optimally robust and watermarkable decoder for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05864
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Permute-and-Flip&#65288;PF&#65289;&#35299;&#30721;&#22120;&#65292;&#20854;&#20855;&#26377;&#26368;&#20339;&#30340;&#40065;&#26834;&#24615;&#21644;&#36136;&#37327;-&#40065;&#26834;&#24615;&#30340; tradeoff&#65292;&#19988;&#27604;&#37319;&#26679;&#26041;&#27861;&#26356;&#22909;&#12290;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#38024;&#23545;PF&#35299;&#30721;&#22120;&#30340;&#27700;&#21360;&#26041;&#26696;&#65292;&#33021;&#22815;&#20445;&#25345;&#26679;&#26412;&#30340;&#20998;&#24067;&#19981;&#21464;&#65292;&#24182;&#23454;&#29616;&#20219;&#24847;&#20302;&#30340;&#20551;&#38451;&#24615;&#29575;&#21644;&#39640;&#30340;&#21484;&#22238;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;PF&#35299;&#30721;&#22120;&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#26420;&#32032;&#37319;&#26679;&#65292;&#20026;LLM&#35299;&#30721;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Permute-and-Flip&#65288;PF&#65289;&#35299;&#30721;&#22120;&#30340;&#26032;&#35299;&#30721;&#26041;&#27861;&#12290;&#23427;&#20855;&#26377;&#19982;&#26631;&#20934;&#37319;&#26679;&#35299;&#30721;&#22120;&#30456;&#20284;&#30340;&#40065;&#26834;&#24615;&#29305;&#24615;&#65292;&#20294;&#22312;&#36136;&#37327;&#21644;&#40065;&#26834;&#24615;&#30340; tradeoff &#19978;&#35777;&#26126;&#27604;&#37319;&#26679;&#26041;&#27861;&#26356;&#22909;&#65292;&#19988;&#27704;&#36828;&#19981;&#20250;&#24046;&#20110;&#20219;&#20309;&#20854;&#20182;&#35299;&#30721;&#22120;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;Aaronson&#30340;Gumbel&#27700;&#21360;&#30340;&#21152;&#23494;&#27700;&#21360;&#26041;&#26696;&#65292;&#20294;&#26159;&#38024;&#23545;PF&#35299;&#30721;&#22120;&#32780;&#33258;&#28982;&#37327;&#36523;&#23450;&#21046;&#12290;&#35813;&#27700;&#21360;&#26041;&#26696;&#19981;&#25913;&#21464;&#26679;&#26412;&#30340;&#20998;&#24067;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#24847;&#20302;&#30340;&#20551;&#38451;&#24615;&#29575;&#21644;&#39640;&#30340;&#21484;&#22238;&#29575;&#65292;&#21482;&#35201;&#29983;&#25104;&#30340;&#25991;&#26412;&#20855;&#26377;&#39640;&#29109;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;PF&#35299;&#30721;&#22120;&#65288;&#21450;&#20854;&#24102;&#26377;&#27700;&#21360;&#30340;&#23545;&#24212;&#29289;&#65289;&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#26420;&#32032;&#37319;&#26679;&#65288;&#21450;&#20854;&#24102;&#26377;Gumbel&#27700;&#21360;&#30340;&#23545;&#24212;&#29289;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#40065;&#26834;&#24615;&#65288;&#21644;&#21487;&#26816;&#27979;&#24615;&#65289;&#65292;&#22240;&#27492;&#20026;LLM&#35299;&#30721;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26032;&#26041;&#27861;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/XuandongZhao/pf-decoding&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a new decoding method called Permute-and-Flip (PF) decoder. It enjoys robustness properties similar to the standard sampling decoder, but is provably up to 2x better in its quality-robustness tradeoff than sampling and never worse than any other decoder. We also design a cryptographic watermarking scheme analogous to Aaronson's Gumbel watermark, but naturally tailored for PF decoder. The watermarking scheme does not change the distribution to sample, while allowing arbitrarily low false positive rate and high recall whenever the generated text has high entropy. Our experiments show that the PF decoder (and its watermarked counterpart) significantly outperform(s) naive sampling (and it's Gumbel watermarked counterpart) in terms of perplexity, while retaining the same robustness (and detectability), hence making it a promising new approach for LLM decoding. The code is available at https://github.com/XuandongZhao/pf-decoding
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#20195;&#29702;&#20043;&#38388;&#30340;&#35848;&#21028;&#33021;&#21147;&#65292;&#24320;&#21457;&#20102;NegotiationArena&#26694;&#26550;&#29992;&#20110;&#35780;&#20272;&#21644;&#25506;&#32034;LLM&#20195;&#29702;&#30340;&#35848;&#21028;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#36816;&#29992;&#29305;&#23450;&#30340;&#34892;&#20026;&#31574;&#30053;&#26174;&#33879;&#25552;&#39640;&#35848;&#21028;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05863</link><description>&lt;p&gt;
LLMs&#33021;&#22815;&#36827;&#34892;&#33391;&#22909;&#30340;&#35848;&#21028;&#21527;&#65311;NegotiationArena&#24179;&#21488;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#20195;&#29702;&#20043;&#38388;&#30340;&#35848;&#21028;&#33021;&#21147;&#65292;&#24320;&#21457;&#20102;NegotiationArena&#26694;&#26550;&#29992;&#20110;&#35780;&#20272;&#21644;&#25506;&#32034;LLM&#20195;&#29702;&#30340;&#35848;&#21028;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#36816;&#29992;&#29305;&#23450;&#30340;&#34892;&#20026;&#31574;&#30053;&#26174;&#33879;&#25552;&#39640;&#35848;&#21028;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35848;&#21028;&#26159;&#31038;&#20250;&#20132;&#24448;&#30340;&#22522;&#30784;&#65307;&#20154;&#20204;&#35848;&#21028;&#20174;&#27773;&#36710;&#20215;&#26684;&#21040;&#22914;&#20309;&#20849;&#20139;&#20849;&#21516;&#36164;&#28304;&#30340;&#19968;&#20999;&#12290;&#38543;&#30528;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#34920;&#20154;&#31867;&#29992;&#25143;&#34892;&#21160;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#38271;&#65292;&#36825;&#20123;LLM&#20195;&#29702;&#20063;&#38656;&#35201;&#20855;&#22791;&#35848;&#21028;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#20043;&#38388;&#30340;&#35848;&#21028;&#33021;&#21147;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;NegotiationArena&#65306;&#19968;&#20010;&#28789;&#27963;&#30340;&#35780;&#20272;&#21644;&#25506;&#32034;LLM&#20195;&#29702;&#35848;&#21028;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;NegotiationArena&#20013;&#23454;&#26045;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#22330;&#26223;&#65292;&#20197;&#35780;&#20272;LLMs&#22312;&#20998;&#37197;&#20849;&#20139;&#36164;&#28304;&#65288;&#32456;&#26497;&#21338;&#24328;&#65289;&#12289;&#32858;&#21512;&#36164;&#28304;&#65288;&#20132;&#26131;&#21338;&#24328;&#65289;&#21644;&#20080;&#21334;&#21830;&#21697;&#65288;&#20215;&#26684;&#35848;&#21028;&#65289;&#26041;&#38754;&#30340;&#34892;&#20026;&#12290;&#27599;&#20010;&#22330;&#26223;&#37117;&#20801;&#35768;LLM&#20195;&#29702;&#20043;&#38388;&#36827;&#34892;&#22810;&#36718;&#28789;&#27963;&#23545;&#35805;&#65292;&#20197;&#36827;&#34892;&#26356;&#22797;&#26434;&#30340;&#35848;&#21028;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36890;&#36807;&#37319;&#29992;&#26576;&#20123;&#34892;&#20026;&#31574;&#30053;&#65292;LLM&#20195;&#29702;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35848;&#21028;&#32467;&#26524;&#12290;&#20363;&#22914;&#65292;&#36890;&#36807;&#20551;&#35013;&#22788;&#22659;&#22256;&#39039;&#21644;&#32477;&#26395;&#65292;LLM&#20195;&#29702;&#21487;&#20197;&#22686;&#21152;&#35848;&#21028;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Negotiation is the basis of social interactions; humans negotiate everything from the price of cars to how to share common resources. With rapidly growing interest in using large language models (LLMs) to act as agents on behalf of human users, such LLM agents would also need to be able to negotiate. In this paper, we study how well LLMs can negotiate with each other. We develop NegotiationArena: a flexible framework for evaluating and probing the negotiation abilities of LLM agents. We implemented three types of scenarios in NegotiationArena to assess LLM's behaviors in allocating shared resources (ultimatum games), aggregate resources (trading games) and buy/sell goods (price negotiations). Each scenario allows for multiple turns of flexible dialogues between LLM agents to allow for more complex negotiations. Interestingly, LLM agents can significantly boost their negotiation outcomes by employing certain behavioral tactics. For example, by pretending to be desolate and desperate, LL
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#20102;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31283;&#20581;&#32534;&#36753;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#20174;&#32780;&#20419;&#36827;&#23545;&#20132;&#27969;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#31283;&#20581;&#12289;&#29616;&#23454;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.05827</link><description>&lt;p&gt;
&#26159;&#21542;&#21487;&#20197;&#31283;&#20581;&#22320;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is it Possible to Edit Large Language Models Robustly?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20102;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31283;&#20581;&#32534;&#36753;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#20174;&#32780;&#20419;&#36827;&#23545;&#20132;&#27969;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#31283;&#20581;&#12289;&#29616;&#23454;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26500;&#24314;&#33021;&#27169;&#20223;&#20154;&#31867;&#34892;&#20026;&#30340;&#20132;&#27969;&#22411;&#20154;&#24037;&#26234;&#33021;&#26041;&#38754;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#20063;&#38754;&#20020;&#30528;&#39640;&#25928;&#23450;&#21046;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#28041;&#21450;&#21040;&#20102;&#27169;&#22411;&#32534;&#36753;&#30340;&#39046;&#22495;&#65292;&#36890;&#36807;&#25805;&#32437;&#35821;&#35328;&#27169;&#22411;&#30340;&#29305;&#23450;&#35760;&#24518;&#24182;&#25913;&#21464;&#30456;&#20851;&#30340;&#35821;&#35328;&#29983;&#25104;&#26469;&#36827;&#34892;&#32534;&#36753;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#32534;&#36753;&#30340;&#31283;&#20581;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20102;&#35299;&#32534;&#36753;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#20174;&#32780;&#20419;&#36827;&#23545;&#20132;&#27969;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#31283;&#20581;&#12289;&#29616;&#23454;&#24212;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#20197;&#22238;&#31572;&#19977;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;Q1&#65306;&#32534;&#36753;&#21518;&#30340;LLM&#26159;&#21542;&#33021;&#22312;&#29616;&#23454;&#24773;&#22659;&#20013;&#19968;&#33268;&#22320;&#34920;&#29616;&#20986;&#31867;&#20284;&#20110;&#20132;&#27969;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#20026;&#65311;Q2&#65306;&#25913;&#20889;&#25552;&#31034;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#23548;&#33268;LLM&#20559;&#31163;&#32534;&#36753;&#30340;&#30693;&#35782;&#35760;&#24518;&#65311;Q3&#65306;&#21738;&#20123;&#30693;&#35782;&#29305;&#24449;&#19982;&#32534;&#36753;&#30340;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#30456;&#20851;&#65311;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#25581;&#31034;&#20102;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have played a pivotal role in building communicative AI to imitate human behaviors but face the challenge of efficient customization. To tackle this challenge, recent studies have delved into the realm of model editing, which manipulates specific memories of language models and changes the related language generation. However, the robustness of model editing remains an open question. This work seeks to understand the strengths and limitations of editing methods, thus facilitating robust, realistic applications of communicative AI. Concretely, we conduct extensive analysis to address the three key research questions. Q1: Can edited LLMs behave consistently resembling communicative AI in realistic situations? Q2: To what extent does the rephrasing of prompts lead LLMs to deviate from the edited knowledge memory? Q3: Which knowledge features are correlated with the performance and robustness of editing? Our experimental results uncover a substantial disparity 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PW-HuBERT&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#23558;&#20266;&#35789;&#32423;&#30446;&#26631;&#25972;&#21512;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20174;&#35270;&#35273;&#35821;&#38899;&#27169;&#22411;&#20013;&#25552;&#21462;&#30446;&#26631;&#65292;&#20174;&#32780;&#22312;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.05819</link><description>&lt;p&gt;
&#23558;&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#27169;&#22411;&#19982;&#35270;&#35273;&#35821;&#38899;&#27169;&#22411;&#29983;&#25104;&#30340;&#20266;&#35789;&#32423;&#30446;&#26631;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Integrating Self-supervised Speech Model with Pseudo Word-level Targets from Visually-grounded Speech Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PW-HuBERT&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#23558;&#20266;&#35789;&#32423;&#30446;&#26631;&#25972;&#21512;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20174;&#35270;&#35273;&#35821;&#38899;&#27169;&#22411;&#20013;&#25552;&#21462;&#30446;&#26631;&#65292;&#20174;&#32780;&#22312;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#27169;&#22411;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#22312;&#24103;&#32423;&#35757;&#32451;&#30446;&#26631;&#19978;&#65292;&#22312;&#38656;&#35201;&#35821;&#20041;&#29702;&#35299;&#30340;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#20013;&#21487;&#33021;&#19981;&#36275;&#22815;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20381;&#36182;&#20110;&#39069;&#22806;&#30340;&#35821;&#38899;-&#25991;&#26412;&#25968;&#25454;&#20316;&#20026;&#20013;&#38388;&#30446;&#26631;&#65292;&#36825;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#25104;&#26412;&#39640;&#26114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Pseudo-Word HuBERT&#65288;PW-HuBERT&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#20266;&#35789;&#32423;&#30446;&#26631;&#25972;&#21512;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20854;&#20013;&#30446;&#26631;&#26159;&#20174;&#35270;&#35273;&#35821;&#38899;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#65292;&#22240;&#27492;&#28040;&#38500;&#20102;&#23545;&#35821;&#38899;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#21475;&#35821;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#22312;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in self-supervised speech models have shown significant improvement in many downstream tasks. However, these models predominantly centered on frame-level training objectives, which can fall short in spoken language understanding tasks that require semantic comprehension. Existing works often rely on additional speech-text data as intermediate targets, which is costly in the real-world setting. To address this challenge, we propose Pseudo-Word HuBERT (PW-HuBERT), a framework that integrates pseudo word-level targets into the training process, where the targets are derived from a visually-ground speech model, notably eliminating the need for speech-text paired data. Our experimental results on four spoken language understanding (SLU) benchmarks suggest the superiority of our model in capturing semantic information.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#31934;&#30830;&#21644;&#36873;&#25321;&#24615;&#30340;&#36951;&#24536;&#65292;&#20197;&#35299;&#20915;&#31070;&#32463;&#27169;&#22411;&#24847;&#22806;&#20445;&#30041;&#20010;&#20154;&#25110;&#25935;&#24863;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#21019;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#26088;&#22312;&#34913;&#37327;&#25935;&#24863;&#20449;&#24687;&#28040;&#38500;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#24378;&#21270;&#36951;&#24536;&#26694;&#26550;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26631;&#27880;&#25935;&#24863;&#33539;&#22260;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05813</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#36951;&#24536;&#65306;&#25512;&#36827;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26426;&#22120;&#36951;&#24536;&#25216;&#26415;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#31934;&#30830;&#21644;&#36873;&#25321;&#24615;&#30340;&#36951;&#24536;&#65292;&#20197;&#35299;&#20915;&#31070;&#32463;&#27169;&#22411;&#24847;&#22806;&#20445;&#30041;&#20010;&#20154;&#25110;&#25935;&#24863;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#21019;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#26088;&#22312;&#34913;&#37327;&#25935;&#24863;&#20449;&#24687;&#28040;&#38500;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#24378;&#21270;&#36951;&#24536;&#26694;&#26550;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26631;&#27880;&#25935;&#24863;&#33539;&#22260;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#33268;&#21147;&#20110;&#35299;&#20915;&#31070;&#32463;&#27169;&#22411;&#24847;&#22806;&#20445;&#30041;&#20010;&#20154;&#25110;&#25935;&#24863;&#25968;&#25454;&#30340;&#38382;&#39064;&#30340;&#26032;&#20852;&#39046;&#22495;&#12290;&#22312;&#36825;&#37324;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#31934;&#30830;&#21644;&#36873;&#25321;&#24615;&#36951;&#24536;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#12290;&#19982;&#20197;&#24448;&#23436;&#20840;&#30456;&#21453;&#30340;&#35757;&#32451;&#30446;&#26631;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#20943;&#36731;&#23545;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#21019;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65306;&#25935;&#24863;&#20449;&#24687;&#25552;&#21462;&#21487;&#33021;&#24615;&#65288;S-EL&#65289;&#21644;&#25935;&#24863;&#20449;&#24687;&#23384;&#20648;&#20934;&#30830;&#24615;&#65288;S-MA&#65289;&#65292;&#26088;&#22312;&#34913;&#37327;&#25935;&#24863;&#20449;&#24687;&#28040;&#38500;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#21152;&#24378;&#36951;&#24536;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26631;&#27880;&#25935;&#24863;&#33539;&#22260;&#30340;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#22312;&#32447;&#21644;&#31163;&#32447;&#31574;&#30053;&#12290;&#22312;&#32447;&#36873;&#25321;&#26426;&#21046;&#21033;&#29992;&#35821;&#35328;&#27010;&#29575;&#24471;&#20998;&#30830;&#20445;&#35745;&#31639;&#25928;&#29575;&#65292;&#32780;&#31163;&#32447;&#31574;&#30053;&#21017;&#21033;&#29992;&#22522;&#20110;&#36317;&#31163;&#30340;&#36807;&#28388;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this study is to investigate Machine Unlearning (MU), a burgeoning field focused on addressing concerns related to neural models inadvertently retaining personal or sensitive data. Here, a novel approach is introduced to achieve precise and selective forgetting within language models. Unlike previous methodologies that adopt completely opposing training objectives, this approach aims to mitigate adverse effects on language model performance, particularly in generation tasks. Furthermore, two innovative evaluation metrics are proposed: Sensitive Information Extraction Likelihood (S-EL) and Sensitive Information Memory Accuracy (S-MA), designed to gauge the effectiveness of sensitive information elimination. To reinforce the forgetting framework, an effective method for annotating sensitive scopes is presented, involving both online and offline strategies. The online selection mechanism leverages language probability scores to ensure computational efficiency, while the offline
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;FAQ-Gen&#65292;&#19968;&#20010;&#21033;&#29992;&#25991;&#26412;&#36716;&#25991;&#26412;&#36716;&#25442;&#27169;&#22411;&#24320;&#21457;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#29992;&#20110;&#29983;&#25104;&#29305;&#23450;&#39046;&#22495;&#30340;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#20351;&#29992;&#33258;&#25105;&#31574;&#21010;&#30340;&#31639;&#27861;&#26469;&#20248;&#21270;&#20449;&#24687;&#34920;&#31034;&#21644;&#38382;&#39064;-&#31572;&#26696;&#25490;&#21517;&#65292;&#25552;&#39640;&#20102;FAQ&#30340;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;&#23450;&#24615;&#20154;&#31867;&#35780;&#20272;&#34920;&#26126;&#29983;&#25104;&#30340;FAQs&#26500;&#36896;&#33391;&#22909;&#19988;&#21487;&#25512;&#24191;&#33267;&#19981;&#21516;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2402.05812</link><description>&lt;p&gt;
FAQ-Gen&#65306;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#29305;&#23450;&#39046;&#22495;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#29992;&#20110;&#36741;&#21161;&#29702;&#35299;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
FAQ-Gen: An automated system to generate domain-specific FAQs to aid content comprehension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FAQ-Gen&#65292;&#19968;&#20010;&#21033;&#29992;&#25991;&#26412;&#36716;&#25991;&#26412;&#36716;&#25442;&#27169;&#22411;&#24320;&#21457;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#29992;&#20110;&#29983;&#25104;&#29305;&#23450;&#39046;&#22495;&#30340;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#20351;&#29992;&#33258;&#25105;&#31574;&#21010;&#30340;&#31639;&#27861;&#26469;&#20248;&#21270;&#20449;&#24687;&#34920;&#31034;&#21644;&#38382;&#39064;-&#31572;&#26696;&#25490;&#21517;&#65292;&#25552;&#39640;&#20102;FAQ&#30340;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;&#23450;&#24615;&#20154;&#31867;&#35780;&#20272;&#34920;&#26126;&#29983;&#25104;&#30340;FAQs&#26500;&#36896;&#33391;&#22909;&#19988;&#21487;&#25512;&#24191;&#33267;&#19981;&#21516;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#65288;FAQ&#65289;&#26159;&#25351;&#20851;&#20110;&#29305;&#23450;&#20869;&#23481;&#30340;&#26368;&#24120;&#35265;&#35810;&#38382;&#12290;&#23427;&#20204;&#36890;&#36807;&#31616;&#21270;&#20027;&#39064;&#21644;&#36890;&#36807;&#31616;&#26126;&#25212;&#35201;&#22320;&#21576;&#29616;&#20449;&#24687;&#26469;&#22686;&#24378;&#29702;&#35299;&#65292;&#20316;&#20026;&#20869;&#23481;&#29702;&#35299;&#30340;&#36741;&#21161;&#24037;&#20855;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#21033;&#29992;&#25991;&#26412;&#36716;&#25991;&#26412;&#36716;&#25442;&#27169;&#22411;&#26469;&#35299;&#20915;FAQ&#29983;&#25104;&#20316;&#20026;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#28085;&#30422;&#20256;&#32479;&#38382;&#31572;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#24378;&#35843;&#23427;&#20204;&#22312;&#30452;&#25509;&#24212;&#29992;&#20110;FAQ&#29983;&#25104;&#20219;&#21153;&#26102;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#29305;&#23450;&#39046;&#22495;&#30340;&#25991;&#26412;&#20869;&#23481;&#26500;&#24314;FAQs&#65292;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#33258;&#25105;&#31574;&#21010;&#30340;&#31639;&#27861;&#26469;&#33719;&#24471;&#25552;&#20379;&#32473;&#36755;&#20837;&#30340;&#20449;&#24687;&#30340;&#26368;&#20339;&#34920;&#31034;&#65292;&#24182;&#23545;&#38382;&#39064;-&#31572;&#26696;&#23545;&#36827;&#34892;&#25490;&#24207;&#20197;&#26368;&#22823;&#21270;&#20154;&#31867;&#29702;&#35299;&#12290;&#23450;&#24615;&#20154;&#31867;&#35780;&#20272;&#26174;&#31034;&#29983;&#25104;&#30340;FAQs&#26500;&#36896;&#33391;&#22909;&#19988;&#21487;&#25512;&#24191;&#33267;&#19981;&#21516;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Frequently Asked Questions (FAQs) refer to the most common inquiries about specific content. They serve as content comprehension aids by simplifying topics and enhancing understanding through succinct presentation of information. In this paper, we address FAQ generation as a well-defined Natural Language Processing (NLP) task through the development of an end-to-end system leveraging text-to-text transformation models. We present a literature review covering traditional question-answering systems, highlighting their limitations when applied directly to the FAQ generation task. We propose our system capable of building FAQs from textual content tailored to specific domains, enhancing their accuracy and relevance. We utilise self-curated algorithms for obtaining optimal representation of information to be provided as input and also for ranking the question-answer pairs to maximise human comprehension. Qualitative human evaluation showcases the generated FAQs to be well-constructed and re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#27491;&#30830;&#28436;&#31034;&#24182;&#24314;&#31435;&#36880;&#27493;&#30340;&#35838;&#31243;&#65292;&#23454;&#29616;&#20102;&#32467;&#26524;&#30417;&#30563;&#21644;&#36807;&#31243;&#30417;&#30563;&#30340;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.05808</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#27491;&#30830;&#28436;&#31034;&#24182;&#24314;&#31435;&#36880;&#27493;&#30340;&#35838;&#31243;&#65292;&#23454;&#29616;&#20102;&#32467;&#26524;&#30417;&#30563;&#21644;&#36807;&#31243;&#30417;&#30563;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;R$^3$&#65306;&#36890;&#36807;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#34892;&#25512;&#29702;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21482;&#20351;&#29992;&#32467;&#26524;&#30417;&#30563;&#26469;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36807;&#31243;&#30417;&#30563;&#30340;&#22909;&#22788;&#12290;&#23558;RL&#24212;&#29992;&#20110;&#22797;&#26434;&#25512;&#29702;&#30340;&#26680;&#24515;&#25361;&#25112;&#26159;&#30830;&#23450;&#19968;&#31995;&#21015;&#34892;&#21160;&#65292;&#20197;&#33719;&#24471;&#27491;&#21521;&#22870;&#21169;&#24182;&#25552;&#20379;&#36866;&#24403;&#30340;&#20248;&#21270;&#30417;&#30563;&#12290;&#32467;&#26524;&#30417;&#30563;&#20026;&#26368;&#32456;&#32467;&#26524;&#25552;&#20379;&#20102;&#31232;&#30095;&#22870;&#21169;&#65292;&#32780;&#19981;&#35782;&#21035;&#38169;&#35823;&#20301;&#32622;&#65292;&#32780;&#36807;&#31243;&#30417;&#30563;&#25552;&#20379;&#20102;&#36880;&#27493;&#22870;&#21169;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#27880;&#37322;&#12290;R$^3$&#36890;&#36807;&#23398;&#20064;&#27491;&#30830;&#28436;&#31034;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;R$^3$&#23558;&#25512;&#29702;&#30340;&#36215;&#22987;&#29366;&#24577;&#20174;&#28436;&#31034;&#30340;&#32467;&#26463;&#28369;&#21160;&#21040;&#24320;&#22987;&#65292;&#20174;&#32780;&#22312;&#25152;&#26377;&#38454;&#27573;&#37117;&#20419;&#36827;&#20102;&#26356;&#23481;&#26131;&#30340;&#27169;&#22411;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;R$^3$&#24314;&#31435;&#20102;&#19968;&#20010;&#36880;&#27493;&#30340;&#35838;&#31243;&#65292;&#20351;&#32467;&#26524;&#30417;&#30563;&#33021;&#22815;&#25552;&#20379;&#38454;&#27573;&#32423;&#20449;&#21495;&#24182;&#31934;&#30830;&#23450;&#20301;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose R$^3$: Learning Reasoning through Reverse Curriculum Reinforcement Learning (RL), a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models. The core challenge in applying RL to complex reasoning is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization. Outcome supervision provides sparse rewards for final results without identifying error locations, whereas process supervision offers step-wise rewards but requires extensive manual annotation. R$^3$ overcomes these limitations by learning from correct demonstrations. Specifically, R$^3$ progressively slides the start state of reasoning from a demonstration's end to its beginning, facilitating easier model exploration at all stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome supervision to offer step-level signals and precisely pinpoint errors. Using Llama2-7
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36164;&#28304;&#26377;&#38480;&#35821;&#35328;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#38899;&#26631;&#20016;&#23500;&#35821;&#26009;&#24211;&#26500;&#24314;&#26041;&#27861;&#65292;&#21253;&#25324;&#25910;&#38598;&#25991;&#26412;&#25968;&#25454;&#38598;&#12289;&#22522;&#20110;&#19977;&#38899;&#20998;&#24067;&#30340;&#21477;&#23376;&#36873;&#25321;&#31639;&#27861;&#21644;&#26681;&#25454;&#22768;&#23398;-&#21457;&#38899;&#35821;&#38899;&#29305;&#24449;&#36827;&#34892;&#26032;&#30340;&#38899;&#32032;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2402.05794</link><description>&lt;p&gt;
&#29992;&#20110;&#36164;&#28304;&#26377;&#38480;&#35821;&#35328;&#30340;&#38899;&#26631;&#20016;&#23500;&#35821;&#26009;&#24211;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Phonetically rich corpus construction for a low-resourced language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36164;&#28304;&#26377;&#38480;&#35821;&#35328;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#38899;&#26631;&#20016;&#23500;&#35821;&#26009;&#24211;&#26500;&#24314;&#26041;&#27861;&#65292;&#21253;&#25324;&#25910;&#38598;&#25991;&#26412;&#25968;&#25454;&#38598;&#12289;&#22522;&#20110;&#19977;&#38899;&#20998;&#24067;&#30340;&#21477;&#23376;&#36873;&#25321;&#31639;&#27861;&#21644;&#26681;&#25454;&#22768;&#23398;-&#21457;&#38899;&#35821;&#38899;&#29305;&#24449;&#36827;&#34892;&#26032;&#30340;&#38899;&#32032;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#25216;&#26415;&#20381;&#36182;&#20110;&#25429;&#25417;&#35828;&#35805;&#20154;&#30340;&#22768;&#38899;&#21464;&#24322;&#24615;&#21644;&#33719;&#21462;&#20840;&#38754;&#30340;&#35821;&#35328;&#20449;&#24687;&#12290;&#25991;&#26412;&#25552;&#31034;&#21644;&#21477;&#23376;&#36873;&#25321;&#26041;&#27861;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#34987;&#25552;&#20986;&#65292;&#20197;&#32452;&#25104;&#36866;&#24230;&#30340;&#38899;&#26631;&#25968;&#25454;&#65292;&#34987;&#31216;&#20026;&#38899;&#26631;&#20016;&#23500;&#30340;&#35821;&#26009;&#24211;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#35821;&#35328;&#32780;&#35328;&#65292;&#23427;&#20204;&#20173;&#28982;&#23545;&#22768;&#23398;&#24314;&#27169;&#19981;&#36275;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#24182;&#27010;&#36848;&#20102;&#20026;&#36164;&#28304;&#26377;&#38480;&#30340;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#21019;&#24314;&#20855;&#26377;&#24191;&#27867;&#38899;&#26631;&#35206;&#30422;&#30340;&#35821;&#26009;&#24211;&#25152;&#38656;&#30340;&#26041;&#27861;&#35770;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20174;&#25991;&#26412;&#25968;&#25454;&#38598;&#25910;&#38598;&#21040;&#22522;&#20110;&#19977;&#38899;&#20998;&#24067;&#30340;&#21477;&#23376;&#36873;&#25321;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38899;&#32032;&#20998;&#31867;&#26041;&#27861;&#65292;&#26681;&#25454;&#22768;&#23398;-&#21457;&#38899;&#35821;&#38899;&#29305;&#24449;&#65292;&#22240;&#20026;&#19981;&#21516;&#30340;&#19977;&#38899;&#30340;&#32477;&#23545;&#25968;&#37327;&#25110;&#20302;&#27010;&#29575;&#19977;&#38899;&#24182;&#19981;&#33021;&#20445;&#35777;&#23545;&#27599;&#31181;&#21487;&#33021;&#30340;&#32452;&#21512;&#36827;&#34892;&#36866;&#24403;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech technologies rely on capturing a speaker's voice variability while obtaining comprehensive language information. Textual prompts and sentence selection methods have been proposed in the literature to comprise such adequate phonetic data, referred to as a phonetically rich \textit{corpus}. However, they are still insufficient for acoustic modeling, especially critical for languages with limited resources. Hence, this paper proposes a novel approach and outlines the methodological aspects required to create a \textit{corpus} with broad phonetic coverage for a low-resourced language, Brazilian Portuguese. Our methodology includes text dataset collection up to a sentence selection algorithm based on triphone distribution. Furthermore, we propose a new phonemic classification according to acoustic-articulatory speech features since the absolute number of distinct triphones, or low-probability triphones, does not guarantee an adequate representation of every possible combination. Usin
&lt;/p&gt;</description></item><item><title>Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#27604;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#23545;&#20110;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#30340;&#25928;&#26524;&#26356;&#24046;&#65292;&#32780;&#19988;&#26799;&#24230;&#19979;&#38477;&#22312;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.05785</link><description>&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#27861;&#23398;&#20064;&#19978;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Limits of Transformer Language Models on Algorithmic Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05785
&lt;/p&gt;
&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#27604;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#23545;&#20110;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#30340;&#25928;&#26524;&#26356;&#24046;&#65292;&#32780;&#19988;&#26799;&#24230;&#19979;&#38477;&#22312;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#35201;&#27714;&#32452;&#21512;&#22810;&#20010;&#31163;&#25955;&#23376;&#20219;&#21153;&#30340;&#26032;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;LLaMA&#27169;&#22411;&#21644;&#22312;GPT-4&#21644;Gemini&#19978;&#25552;&#31034;&#26469;&#34913;&#37327;&#23398;&#20064;&#23398;&#20064;&#21407;&#35821;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#24182;&#19988;&#22312;&#26679;&#26412;&#35268;&#27169;&#26041;&#38754;&#27604;&#20026;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#25928;&#26524;&#26356;&#24046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22797;&#26434;&#24615;&#29702;&#35770;&#30340;&#23450;&#29702;&#65292;&#35777;&#26126;&#20102;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#25351;&#25968;&#32423;&#22320;&#28010;&#36153;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the capabilities of Transformer language models on learning discrete algorithms. To this end, we introduce two new tasks demanding the composition of several discrete sub-tasks. On both training LLaMA models from scratch and prompting on GPT-4 and Gemini we measure learning compositions of learned primitives. We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition. We also present a theorem in complexity theory, showing that gradient descent on memorizing feedforward models can be exponentially data inefficient.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#26681;&#25454;&#19981;&#21516;&#30340;&#27169;&#24577;&#35843;&#25972;&#21644;&#34920;&#31034;&#24207;&#21015;&#26631;&#35760;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25991;&#26412;&#21040;&#20195;&#30721;&#29983;&#25104;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05783</link><description>&lt;p&gt;
&#20351;&#29992;&#27169;&#24577;&#30456;&#23545;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#29983;&#25104;&#25991;&#26412;&#21040;&#20195;&#30721;&#30340;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Text-to-Code Generation with Modality-relative Pre-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#26681;&#25454;&#19981;&#21516;&#30340;&#27169;&#24577;&#35843;&#25972;&#21644;&#34920;&#31034;&#24207;&#21015;&#26631;&#35760;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25991;&#26412;&#21040;&#20195;&#30721;&#29983;&#25104;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#32534;&#31243;&#35821;&#35328;&#20219;&#21153;&#65292;&#24182;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#36890;&#24120;&#36890;&#36807;&#36827;&#19968;&#27493;&#39044;&#20808;&#35757;&#32451;&#20005;&#26684;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#20013;&#35757;&#32451;&#24207;&#21015;&#36890;&#24120;&#21253;&#21547;&#33258;&#28982;&#35821;&#35328;&#21644;&#65288;&#32447;&#24615;&#21270;&#30340;&#65289;&#32534;&#31243;&#35821;&#35328;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#24207;&#21015;&#30340;&#20004;&#31181;&#27169;&#24577;&#26144;&#23556;&#21040;&#21516;&#19968;&#23884;&#20837;&#31354;&#38388;&#20013;&#12290;&#28982;&#32780;&#65292;&#32534;&#31243;&#35821;&#35328;&#20851;&#38190;&#35789;&#65288;&#20363;&#22914;&#8220;while&#8221;&#65289;&#24448;&#24448;&#20855;&#26377;&#38750;&#24120;&#20005;&#26684;&#30340;&#23450;&#20041;&#35821;&#20041;&#12290;&#22240;&#27492;&#65292;&#20174;&#33258;&#28982;&#35821;&#35328;&#29992;&#27861;&#36827;&#34892;&#30340;&#36801;&#31227;&#23398;&#20064;&#23545;&#20854;&#20195;&#30721;&#24212;&#29992;&#21487;&#33021;&#24182;&#19981;&#19968;&#23450;&#26377;&#30410;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#24050;&#32463;&#39044;&#20808;&#35757;&#32451;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#26681;&#25454;&#23427;&#20204;&#25152;&#23646;&#30340;&#27169;&#24577;&#19981;&#21516;&#26469;&#35843;&#25972;&#21644;&#34920;&#31034;&#24207;&#21015;&#26631;&#35760;&#65292;&#24182;&#26368;&#32456;&#26377;&#30410;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#27169;&#24577;&#30456;&#23545;&#35757;&#32451;&#30446;&#26631;&#30340;&#36827;&#19968;&#27493;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#23581;&#35797;&#20102;&#22312;&#27169;&#24577;&#20043;&#38388;&#20998;&#31163;&#23884;&#20837;&#31354;&#38388;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained language models have recently been expanded and applied to programming language tasks with great success, often through further pre-training of a strictly-natural language model--where training sequences typically contain both natural and (linearised) programming language. Such approaches effectively map both modalities of the sequence into the same embedding space. However, programming language keywords (e.g. ``while'') often have very strictly defined semantics. As such, transfer learning from their natural language usage may not necessarily be beneficial to their code application and vise versa. Assuming an already pre-trained language model, in this work we investigate how sequence tokens can be adapted and represented differently, depending on which modality they belong to, and to the ultimate benefit of the downstream task. We experiment with separating embedding spaces between modalities during further model pre-training with modality-relative training objectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#24179;&#34892;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#26597;&#35810;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#35266;&#23519;&#21040;&#22312;&#24615;&#21035;&#21644;&#31181;&#26063;&#32500;&#24230;&#19978;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.05779</link><description>&lt;p&gt;
&#20351;&#29992;&#19968;&#20010;&#26032;&#39062;&#30340;&#24179;&#34892;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Examining Gender and Racial Bias in Large Vision-Language Models Using a Novel Dataset of Parallel Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#24179;&#34892;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#26597;&#35810;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#35266;&#23519;&#21040;&#22312;&#24615;&#21035;&#21644;&#31181;&#26063;&#32500;&#24230;&#19978;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21644;&#30456;&#24212;&#30340;&#23545;&#35805;&#27169;&#22411;&#30340;&#36827;&#27493;&#21518;&#65292;&#20986;&#29616;&#20102;&#19968;&#27874;&#26032;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(LVLMs)&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#21487;&#20197;&#23558;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#25191;&#34892;&#35270;&#35273;&#38382;&#31572;&#12289;&#22270;&#20687;&#23383;&#24149;&#12289;&#25925;&#20107;&#29983;&#25104;&#31561;&#20219;&#21153;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#22312;&#36825;&#20123;&#31995;&#32479;&#20013;&#32771;&#23519;&#28508;&#22312;&#30340;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;&#65292;&#22522;&#20110;&#36755;&#20837;&#22270;&#20687;&#20013;&#20154;&#29289;&#30340;&#34987;&#24863;&#30693;&#29305;&#24449;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PAIRS&#65288;&#26085;&#24120;&#24773;&#26223;&#19979;&#30340;&#24179;&#34892;&#22270;&#20687;&#65289;&#12290;PAIRS&#25968;&#25454;&#38598;&#21253;&#21547;&#19968;&#32452;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20154;&#29289;&#22270;&#20687;&#65292;&#36825;&#20123;&#22270;&#20687;&#22312;&#32972;&#26223;&#21644;&#35270;&#35273;&#20869;&#23481;&#26041;&#38754;&#38750;&#24120;&#30456;&#20284;&#65292;&#20294;&#22312;&#24615;&#21035;&#65288;&#30007;&#24615;&#12289;&#22899;&#24615;&#65289;&#21644;&#31181;&#26063;&#65288;&#40657;&#20154;&#12289;&#30333;&#20154;&#65289;&#32500;&#24230;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#26679;&#30340;&#22270;&#20687;&#26597;&#35810;LVLMs&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#26681;&#25454;&#20154;&#29289;&#30340;&#34987;&#24863;&#30693;&#24615;&#21035;&#25110;&#31181;&#26063;&#65292;&#21709;&#24212;&#26377;&#26126;&#26174;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following on recent advances in large language models (LLMs) and subsequent chat models, a new wave of large vision-language models (LVLMs) has emerged. Such models can incorporate images as input in addition to text, and perform tasks such as visual question answering, image captioning, story generation, etc. Here, we examine potential gender and racial biases in such systems, based on the perceived characteristics of the people in the input images. To accomplish this, we present a new dataset PAIRS (PArallel Images for eveRyday Scenarios). The PAIRS dataset contains sets of AI-generated images of people, such that the images are highly similar in terms of background and visual content, but differ along the dimensions of gender (man, woman) and race (Black, white). By querying the LVLMs with such images, we observe significant differences in the responses according to the perceived gender or race of the person depicted.
&lt;/p&gt;</description></item><item><title>SPIRIT-LM&#26159;&#19968;&#20010;&#22522;&#20110;&#39044;&#35757;&#32451;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#21644;&#35821;&#38899;&#36830;&#32493;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#21475;&#35821;&#21644;&#20070;&#38754;&#35821;&#35328;&#30340;&#28151;&#21512;&#27169;&#22411;&#12290;&#23427;&#23637;&#31034;&#20102;&#25991;&#26412;&#27169;&#22411;&#30340;&#35821;&#20041;&#33021;&#21147;&#21644;&#35821;&#38899;&#27169;&#22411;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;SPIRIT-LM&#36824;&#33021;&#20197;&#23569;&#37327;&#26679;&#26412;&#30340;&#26041;&#24335;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.05755</link><description>&lt;p&gt;
SpiRit-LM: &#20132;&#32455;&#30340;&#21475;&#35821;&#21644;&#20070;&#38754;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SpiRit-LM: Interleaved Spoken and Written Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05755
&lt;/p&gt;
&lt;p&gt;
SPIRIT-LM&#26159;&#19968;&#20010;&#22522;&#20110;&#39044;&#35757;&#32451;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#21644;&#35821;&#38899;&#36830;&#32493;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#21475;&#35821;&#21644;&#20070;&#38754;&#35821;&#35328;&#30340;&#28151;&#21512;&#27169;&#22411;&#12290;&#23427;&#23637;&#31034;&#20102;&#25991;&#26412;&#27169;&#22411;&#30340;&#35821;&#20041;&#33021;&#21147;&#21644;&#35821;&#38899;&#27169;&#22411;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;SPIRIT-LM&#36824;&#33021;&#20197;&#23569;&#37327;&#26679;&#26412;&#30340;&#26041;&#24335;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;SPIRIT-LM&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#25991;&#26412;&#21644;&#35821;&#38899;&#33258;&#30001;&#28151;&#21512;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#22312;&#25991;&#26412;&#21644;&#35821;&#38899;&#21333;&#20803;&#19978;&#36827;&#34892;&#35757;&#32451;&#23558;&#20854;&#25193;&#23637;&#21040;&#35821;&#38899;&#27169;&#24577;&#12290;&#35821;&#38899;&#21644;&#25991;&#26412;&#24207;&#21015;&#34987;&#36830;&#25509;&#20026;&#19968;&#32452;&#21333;&#35789;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#23567;&#22411;&#33258;&#21160;&#31579;&#36873;&#30340;&#35821;&#38899;-&#25991;&#26412;&#24179;&#34892;&#35821;&#26009;&#24211;&#26469;&#36827;&#34892;&#35789;&#32423;&#20132;&#32455;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;SPIRIT-LM&#26377;&#20004;&#20010;&#29256;&#26412;&#65306;&#19968;&#20010;&#26159;&#20351;&#29992;&#35821;&#38899;&#35821;&#20041;&#21333;&#20803;&#30340;BASE&#29256;&#26412;&#65292;&#21478;&#19968;&#20010;&#26159;&#22312;&#35821;&#20041;&#21333;&#20803;&#20043;&#22806;&#36824;&#20351;&#29992;&#20102;&#38899;&#39640;&#21644;&#39118;&#26684;&#21333;&#20803;&#26469;&#27169;&#25311;&#34920;&#29616;&#21147;&#30340;EXPRESSIVE&#29256;&#26412;&#12290;&#23545;&#20110;&#36825;&#20004;&#20010;&#29256;&#26412;&#65292;&#25991;&#26412;&#26159;&#29992;&#23376;&#35789;BPE&#26631;&#35760;&#32534;&#30721;&#30340;&#12290;&#32467;&#26524;&#27169;&#22411;&#23637;&#31034;&#20102;&#25991;&#26412;&#27169;&#22411;&#30340;&#35821;&#20041;&#33021;&#21147;&#21644;&#35821;&#38899;&#27169;&#22411;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;SPIRIT-LM&#33021;&#22815;&#22312;&#36328;&#27169;&#24577;&#65288;&#21363;ASR&#12289;TTS&#12289;&#35821;&#38899;&#20998;&#31867;&#65289;&#20013;&#20197;&#23569;&#37327;&#26679;&#26412;&#30340;&#26041;&#24335;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce SPIRIT-LM, a foundation multimodal language model that freely mixes text and speech. Our model is based on a pretrained text language model that we extend to the speech modality by continuously training it on text and speech units. Speech and text sequences are concatenated as a single set of tokens, and trained with a word-level interleaving method using a small automatically-curated speech-text parallel corpus. SPIRIT-LM comes in two versions: a BASE version that uses speech semantic units and an EXPRESSIVE version that models expressivity using pitch and style units in addition to the semantic units. For both versions, the text is encoded with subword BPE tokens. The resulting model displays both the semantic abilities of text models and the expressive abilities of speech models. Additionally, we demonstrate that SPIRIT-LM is able to learn new tasks in a few-shot fashion across modalities (i.e. ASR, TTS, Speech Classification).
&lt;/p&gt;</description></item><item><title>TimeArena&#26159;&#19968;&#20010;&#20855;&#26377;&#26102;&#38388;&#24863;&#30693;&#30340;&#27169;&#25311;&#29615;&#22659;&#65292;&#20026;&#35821;&#35328;&#20195;&#29702;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#22810;&#20219;&#21153;&#22788;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#20063;&#26080;&#27861;&#36798;&#21040;&#20154;&#31867;&#30340;&#22810;&#20219;&#21153;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05733</link><description>&lt;p&gt;
TimeArena&#65306;&#22312;&#20855;&#26377;&#26102;&#38388;&#24863;&#30693;&#30340;&#27169;&#25311;&#20013;&#22609;&#36896;&#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#35821;&#35328;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
TimeArena: Shaping Efficient Multitasking Language Agents in a Time-Aware Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05733
&lt;/p&gt;
&lt;p&gt;
TimeArena&#26159;&#19968;&#20010;&#20855;&#26377;&#26102;&#38388;&#24863;&#30693;&#30340;&#27169;&#25311;&#29615;&#22659;&#65292;&#20026;&#35821;&#35328;&#20195;&#29702;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#22810;&#20219;&#21153;&#22788;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#20063;&#26080;&#27861;&#36798;&#21040;&#20154;&#31867;&#30340;&#22810;&#20219;&#21153;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36890;&#36807;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#30340;&#25991;&#26412;&#27169;&#25311;&#22312;&#26102;&#38388;&#27010;&#24565;&#19978;&#20173;&#28982;&#19981;&#36275;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TimeArena&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#25991;&#26412;&#27169;&#25311;&#29615;&#22659;&#65292;&#23427;&#34701;&#20837;&#20102;&#22797;&#26434;&#30340;&#26102;&#38388;&#21160;&#24577;&#21644;&#32422;&#26463;&#65292;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#35745;&#21010;&#22330;&#26223;&#12290;&#22312;TimeArena&#20013;&#65292;&#20195;&#29702;&#34987;&#35201;&#27714;&#23613;&#24555;&#23436;&#25104;&#22810;&#20010;&#20219;&#21153;&#65292;&#20801;&#35768;&#24182;&#34892;&#22788;&#29702;&#20197;&#33410;&#30465;&#26102;&#38388;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#21160;&#20316;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12289;&#27599;&#20010;&#21160;&#20316;&#30340;&#26102;&#38388;&#25345;&#32493;&#21644;&#20195;&#29702;&#21450;&#29615;&#22659;&#20013;&#30340;&#23545;&#35937;&#30340;&#21344;&#26377;&#24773;&#20917;&#12290;TimeArena&#22522;&#20110;&#28921;&#39274;&#12289;&#23478;&#24237;&#27963;&#21160;&#21644;&#23454;&#39564;&#23460;&#24037;&#20316;&#20013;&#30340;30&#20010;&#30495;&#23454;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;TimeArena&#23545;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;LLM&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#26159;&#26368;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#22914;GPT-4&#65292;&#22312;&#26377;&#25928;&#30340;&#22810;&#20219;&#21153;&#22788;&#29702;&#26041;&#38754;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#65292;&#20984;&#26174;&#20102;&#25913;&#36827;&#26102;&#38388;&#24863;&#30693;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite remarkable advancements in emulating human-like behavior through Large Language Models (LLMs), current textual simulations do not adequately address the notion of time. To this end, we introduce TimeArena, a novel textual simulated environment that incorporates complex temporal dynamics and constraints that better reflect real-life planning scenarios. In TimeArena, agents are asked to complete multiple tasks as soon as possible, allowing for parallel processing to save time. We implement the dependency between actions, the time duration for each action, and the occupancy of the agent and the objects in the environment. TimeArena grounds to 30 real-world tasks in cooking, household activities, and laboratory work. We conduct extensive experiments with various state-of-the-art LLMs using TimeArena. Our findings reveal that even the most powerful models, e.g., GPT-4, still lag behind humans in effective multitasking, underscoring the need for enhanced temporal awareness in the dev
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#32479;&#19968;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#65288;USDM&#65289;&#30340;&#24191;&#27867;&#35821;&#38899;&#25991;&#26412;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#19982;&#36755;&#20837;&#35821;&#38899;&#30456;&#20851;&#30340;&#36830;&#36143;&#21475;&#35821;&#22238;&#22797;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#27493;&#39588;&#30340;&#35821;&#38899;&#25991;&#26412;&#25512;&#29702;&#26041;&#24335;&#21644;&#24191;&#20041;&#35821;&#38899;&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#36328;&#27169;&#24577;&#35821;&#20041;&#65292;&#24182;&#29983;&#25104;&#33258;&#28982;&#27969;&#30021;&#30340;&#21475;&#35821;&#22238;&#22797;&#12290;</title><link>https://arxiv.org/abs/2402.05706</link><description>&lt;p&gt;
&#38754;&#21521;&#21475;&#35821;&#23545;&#35805;&#24314;&#27169;&#30340;&#32479;&#19968;&#35821;&#38899;&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unified Speech-Text Pretraining for Spoken Dialog Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#32479;&#19968;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#65288;USDM&#65289;&#30340;&#24191;&#27867;&#35821;&#38899;&#25991;&#26412;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#19982;&#36755;&#20837;&#35821;&#38899;&#30456;&#20851;&#30340;&#36830;&#36143;&#21475;&#35821;&#22238;&#22797;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#27493;&#39588;&#30340;&#35821;&#38899;&#25991;&#26412;&#25512;&#29702;&#26041;&#24335;&#21644;&#24191;&#20041;&#35821;&#38899;&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#36328;&#27169;&#24577;&#35821;&#20041;&#65292;&#24182;&#29983;&#25104;&#33258;&#28982;&#27969;&#30021;&#30340;&#21475;&#35821;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#30452;&#25509;&#29702;&#35299;&#21644;&#21512;&#25104;&#35821;&#38899;&#20855;&#26377;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#29992;&#20110;&#21475;&#35821;&#23545;&#35805;&#24314;&#27169;&#30340;&#22522;&#20110;LLM&#30340;&#31574;&#30053;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#35821;&#38899;&#25991;&#26412;LLM&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#32479;&#19968;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#65288;USDM&#65289;&#65292;&#20197;&#22312;&#19981;&#20381;&#36182;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#25110;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#35299;&#20915;&#26041;&#26696;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#19982;&#32473;&#23450;&#36755;&#20837;&#35821;&#38899;&#30456;&#20851;&#30340;&#36830;&#36143;&#21475;&#35821;&#22238;&#22797;&#21644;&#26377;&#26426;&#30340;&#38901;&#24459;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#22810;&#27493;&#39588;&#30340;&#35821;&#38899;&#25991;&#26412;&#25512;&#29702;&#26041;&#24335;&#65292;&#21033;&#29992;&#20102;&#24213;&#23618;LLM&#25152;&#23637;&#31034;&#30340;&#25512;&#29702;&#38142;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#30340;&#35821;&#38899;&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#25429;&#25417;&#36328;&#27169;&#24577;&#35821;&#20041;&#12290;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#29983;&#25104;&#33258;&#28982;&#27969;&#30021;&#30340;&#21475;&#35821;&#22238;&#22797;&#65292;&#24182;&#19988;&#20248;&#20110;&#20043;&#21069;&#30340;&#21644;&#32423;&#32852;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;&#35814;&#32454;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
While recent work shows promising results in expanding the capabilities of large language models (LLM) to directly understand and synthesize speech, an LLM-based strategy for modeling spoken dialogs remains elusive and calls for further investigation. This work proposes an extensive speech-text LLM framework, named the Unified Spoken Dialog Model (USDM), to generate coherent spoken responses with organic prosodic features relevant to the given input speech without relying on automatic speech recognition (ASR) or text-to-speech (TTS) solutions. Our approach employs a multi-step speech-text inference scheme that leverages chain-of-reasoning capabilities exhibited by the underlying LLM. We also propose a generalized speech-text pretraining scheme that helps with capturing cross-modal semantics. Automatic and human evaluations show that the proposed approach is effective in generating natural-sounding spoken responses, outperforming both prior and cascaded baselines. Detailed comparative s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#31038;&#20132;&#22330;&#26223;&#27169;&#25311;&#26469;&#33258;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#20854;&#34987;&#28389;&#29992;&#36896;&#25104;&#30340;&#28508;&#22312;&#19981;&#33391;&#24433;&#21709;&#12290;&#36890;&#36807;&#19968;&#20010;&#21517;&#20026;MATRIX&#30340;&#34394;&#25311;&#25490;&#32451;&#31354;&#38388;&#65292;LLM&#21487;&#20197;&#22312;&#22238;&#31572;&#26597;&#35810;&#21069;&#32771;&#34385;&#31038;&#20132;&#21518;&#26524;&#65292;&#24182;&#36890;&#36807;MATRIX-simulated&#25968;&#25454;&#30340;&#24494;&#35843;&#65292;&#20445;&#25345;&#23545;&#20154;&#31867;&#20215;&#20540;&#30340;&#36981;&#20174;&#21644;&#25512;&#29702;&#36895;&#24230;&#30340;&#24179;&#34913;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#24102;&#26377;MATRIX&#30340;LLM&#32988;&#36807;&#20102;&#23466;&#27861;AI&#12290;</title><link>https://arxiv.org/abs/2402.05699</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#22404;&#26029;&#23545;&#35805;&#30340;&#31038;&#20132;&#22330;&#26223;&#27169;&#25311;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#31038;&#20132;&#22330;&#26223;&#27169;&#25311;&#26469;&#33258;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#20854;&#34987;&#28389;&#29992;&#36896;&#25104;&#30340;&#28508;&#22312;&#19981;&#33391;&#24433;&#21709;&#12290;&#36890;&#36807;&#19968;&#20010;&#21517;&#20026;MATRIX&#30340;&#34394;&#25311;&#25490;&#32451;&#31354;&#38388;&#65292;LLM&#21487;&#20197;&#22312;&#22238;&#31572;&#26597;&#35810;&#21069;&#32771;&#34385;&#31038;&#20132;&#21518;&#26524;&#65292;&#24182;&#36890;&#36807;MATRIX-simulated&#25968;&#25454;&#30340;&#24494;&#35843;&#65292;&#20445;&#25345;&#23545;&#20154;&#31867;&#20215;&#20540;&#30340;&#36981;&#20174;&#21644;&#25512;&#29702;&#36895;&#24230;&#30340;&#24179;&#34913;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#24102;&#26377;MATRIX&#30340;LLM&#32988;&#36807;&#20102;&#23466;&#27861;AI&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#65292;&#20197;&#20943;&#36731;&#20854;&#34987;&#28389;&#29992;&#36896;&#25104;&#30340;&#28508;&#22312;&#19981;&#33391;&#24433;&#21709;&#65292;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#20511;&#37492;&#31038;&#20250;&#23398;&#30340;&#35265;&#35299;&#65292;&#21363;&#35748;&#35782;&#21040;&#25152;&#26377;&#21508;&#26041;&#30340;&#20851;&#20999;&#26159;&#22609;&#36896;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23545;&#40784;LLMs&#30340;&#26032;&#26041;&#21521;&#65306;&#31038;&#20132;&#22330;&#26223;&#27169;&#25311;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MATRIX&#30340;&#21019;&#26032;&#31038;&#20132;&#22330;&#26223;&#27169;&#25311;&#22120;&#65292;&#23427;&#21487;&#20197;&#27169;&#25311;&#29992;&#25143;&#36755;&#20837;&#26597;&#35810;&#21608;&#22260;&#30340;&#29616;&#23454;&#22330;&#26223;&#65292;&#20351;LLM&#22312;&#22238;&#31572;&#21069;&#33021;&#22815;&#32771;&#34385;&#31038;&#20132;&#21518;&#26524;&#12290;MATRIX&#31867;&#20284;&#20110;&#19968;&#20010;&#8220;&#22404;&#26029;&#23545;&#35805;&#8221;&#19979;&#30340;&#34394;&#25311;&#25490;&#32451;&#31354;&#38388;&#65292;LLM&#22312;&#20854;&#20013;&#25198;&#28436;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#22810;&#20010;&#35282;&#33394;&#24182;&#36827;&#34892;&#33258;&#25105;&#23454;&#36341;&#12290;&#20026;&#20102;&#24341;&#20837;&#36825;&#31181;&#23545;&#40784;&#33021;&#21147;&#65292;&#25105;&#20204;&#20351;&#29992;MATRIX&#27169;&#25311;&#25968;&#25454;&#23545;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#30830;&#20445;&#20854;&#22312;&#19981;&#24433;&#21709;&#25512;&#29702;&#36895;&#24230;&#30340;&#24773;&#20917;&#19979;&#31526;&#21512;&#20154;&#31867;&#20215;&#20540;&#35266;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#24102;&#26377;MATRIX&#30340;LLM&#32988;&#36807;&#20102;&#23466;&#27861;AI&#12290;&#26368;&#21518;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models (LLMs) with human values is imperative to mitigate potential adverse effects resulting from their misuse. Drawing from the sociological insight that acknowledging all parties' concerns is a key factor in shaping human values, this paper proposes a novel direction to align LLMs by themselves: social scene simulation. To achieve this, we present MATRIX, a novel social scene simulator that emulates realistic scenes around a user's input query, enabling the LLM to take social consequences into account before responding. MATRIX serves as a virtual rehearsal space, akin to a Monopolylogue, where the LLM performs diverse roles related to the query and practice by itself. To inject this alignment, we fine-tune the LLM with MATRIX-simulated data, ensuring adherence to human values without compromising inference speed. We theoretically show that the LLM with MATRIX outperforms Constitutional AI under mild assumptions. Finally, extensive experiments validate that ou
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;&#24320;&#28304;&#30340;&#22810;&#35821;&#35328;E5&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#35780;&#20272;&#32467;&#26524;&#65292;&#21253;&#25324;&#19977;&#31181;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#21644;&#19968;&#31181;&#26032;&#30340;&#20197;&#25351;&#20196;&#20026;&#23548;&#21521;&#30340;&#23884;&#20837;&#27169;&#22411;&#12290;&#27169;&#22411;&#22312;&#25512;&#29702;&#25928;&#29575;&#21644;&#23884;&#20837;&#36136;&#37327;&#19978;&#21462;&#24471;&#20102;&#24179;&#34913;&#65292;&#24615;&#33021;&#19982;&#21516;&#31561;&#22823;&#23567;&#30340;&#26368;&#20808;&#36827;&#30340;&#20165;&#33521;&#25991;&#27169;&#22411;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2402.05672</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;E5&#25991;&#26412;&#23884;&#20837;&#65306;&#19968;&#39033;&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Multilingual E5 Text Embeddings: A Technical Report
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05672
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;&#24320;&#28304;&#30340;&#22810;&#35821;&#35328;E5&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#35780;&#20272;&#32467;&#26524;&#65292;&#21253;&#25324;&#19977;&#31181;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#21644;&#19968;&#31181;&#26032;&#30340;&#20197;&#25351;&#20196;&#20026;&#23548;&#21521;&#30340;&#23884;&#20837;&#27169;&#22411;&#12290;&#27169;&#22411;&#22312;&#25512;&#29702;&#25928;&#29575;&#21644;&#23884;&#20837;&#36136;&#37327;&#19978;&#21462;&#24471;&#20102;&#24179;&#34913;&#65292;&#24615;&#33021;&#19982;&#21516;&#31561;&#22823;&#23567;&#30340;&#26368;&#20808;&#36827;&#30340;&#20165;&#33521;&#25991;&#27169;&#22411;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;&#24320;&#28304;&#30340;&#22810;&#35821;&#35328;E5&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#35780;&#20272;&#32467;&#26524;&#65292;&#35813;&#27169;&#22411;&#20110;2023&#24180;&#20013;&#26399;&#21457;&#24067;&#12290;&#25552;&#20379;&#20102;&#19977;&#31181;&#19981;&#21516;&#22823;&#23567;&#65288;&#23567;/&#22522;&#30784;/&#22823;&#65289;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#24179;&#34913;&#20102;&#25512;&#29702;&#25928;&#29575;&#21644;&#23884;&#20837;&#36136;&#37327;&#12290;&#35757;&#32451;&#36807;&#31243;&#36981;&#24490;&#33521;&#25991;E5&#27169;&#22411;&#30340;&#37197;&#26041;&#65292;&#28041;&#21450;10&#20159;&#20010;&#22810;&#35821;&#35328;&#25991;&#26412;&#23545;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#19968;&#31995;&#21015;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20197;&#25351;&#20196;&#20026;&#23548;&#21521;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#24615;&#33021;&#19982;&#30456;&#20284;&#22823;&#23567;&#30340;&#26368;&#20808;&#36827;&#30340;&#20165;&#33521;&#25991;&#27169;&#22411;&#30456;&#24403;&#12290;&#26377;&#20851;&#27169;&#22411;&#21457;&#24067;&#30340;&#20449;&#24687;&#21487;&#20197;&#22312;https://github.com/microsoft/unilm/tree/master/e5&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This technical report presents the training methodology and evaluation results of the open-source multilingual E5 text embedding models, released in mid-2023. Three embedding models of different sizes (small / base / large) are provided, offering a balance between the inference efficiency and embedding quality. The training procedure adheres to the English E5 model recipe, involving contrastive pre-training on 1 billion multilingual text pairs, followed by fine-tuning on a combination of labeled datasets. Additionally, we introduce a new instruction-tuned embedding model, whose performance is on par with state-of-the-art, English-only models of similar sizes. Information regarding the model release can be found at https://github.com/microsoft/unilm/tree/master/e5 .
&lt;/p&gt;</description></item><item><title>&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#32469;&#36807;&#23433;&#20840;&#25514;&#26045;&#30340;&#19981;&#31283;&#23450;&#28431;&#27934;&#12290;&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#23545;&#22810;&#31181;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#36827;&#34892;&#22823;&#35268;&#27169;&#27979;&#37327;&#65292;&#23454;&#39564;&#35777;&#26126;&#20248;&#21270;&#30340;&#36234;&#29425;&#25552;&#31034;&#33021;&#22815;&#25345;&#32493;&#36798;&#21040;&#26368;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05668</link><description>&lt;p&gt;
&#23545;LLMs&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Assessment of Jailbreak Attacks Against LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05668
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#32469;&#36807;&#23433;&#20840;&#25514;&#26045;&#30340;&#19981;&#31283;&#23450;&#28431;&#27934;&#12290;&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#23545;&#22810;&#31181;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#36827;&#34892;&#22823;&#35268;&#27169;&#27979;&#37327;&#65292;&#23454;&#39564;&#35777;&#26126;&#20248;&#21270;&#30340;&#36234;&#29425;&#25552;&#31034;&#33021;&#22815;&#25345;&#32493;&#36798;&#21040;&#26368;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#28389;&#29992;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#37319;&#21462;&#20102;&#23433;&#20840;&#25514;&#26045;&#20197;&#30830;&#20445;LLMs&#31526;&#21512;&#31038;&#20250;&#20262;&#29702;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#31181;&#32469;&#36807;LLMs&#23433;&#20840;&#25514;&#26045;&#30340;&#19981;&#31283;&#23450;&#28431;&#27934;&#65292;&#34987;&#31216;&#20026;&#36234;&#29425;&#25915;&#20987;&#12290;&#36890;&#36807;&#24212;&#29992;&#25216;&#26415;&#65292;&#22914;&#35282;&#33394;&#25198;&#28436;&#22330;&#26223;&#12289;&#23545;&#25239;&#24615;&#26679;&#26412;&#25110;&#23545;&#23433;&#20840;&#30446;&#26631;&#30340;&#24494;&#22937;&#30772;&#22351;&#20316;&#20026;&#25552;&#31034;&#65292;LLMs&#21487;&#20197;&#20135;&#29983;&#19981;&#36866;&#24403;&#29978;&#33267;&#26377;&#23475;&#30340;&#22238;&#24212;&#12290;&#34429;&#28982;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#30740;&#31350;&#20102;&#20960;&#31181;&#36234;&#29425;&#25915;&#20987;&#30340;&#31867;&#21035;&#65292;&#20294;&#20182;&#20204;&#37117;&#26159;&#23396;&#31435;&#22320;&#36827;&#34892;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#21508;&#31181;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#30340;&#39318;&#27425;&#22823;&#35268;&#27169;&#27979;&#37327;&#12290;&#25105;&#20204;&#38598;&#20013;&#22312;&#26469;&#33258;&#22235;&#20010;&#31867;&#21035;&#30340;13&#31181;&#23574;&#31471;&#36234;&#29425;&#26041;&#27861;&#12289;16&#31181;&#36829;&#35268;&#31867;&#21035;&#30340;160&#20010;&#38382;&#39064;&#20197;&#21450;&#20845;&#31181;&#27969;&#34892;&#30340;LLMs&#19978;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20248;&#21270;&#30340;&#36234;&#29425;&#25552;&#31034;&#22987;&#32456;&#33021;&#22815;&#36798;&#21040;&#26368;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#34920;&#29616;&#20986;...
&lt;/p&gt;
&lt;p&gt;
Misuse of the Large Language Models (LLMs) has raised widespread concern. To address this issue, safeguards have been taken to ensure that LLMs align with social ethics. However, recent findings have revealed an unsettling vulnerability bypassing the safeguards of LLMs, known as jailbreak attacks. By applying techniques, such as employing role-playing scenarios, adversarial examples, or subtle subversion of safety objectives as a prompt, LLMs can produce an inappropriate or even harmful response. While researchers have studied several categories of jailbreak attacks, they have done so in isolation. To fill this gap, we present the first large-scale measurement of various jailbreak attack methods. We concentrate on 13 cutting-edge jailbreak methods from four categories, 160 questions from 16 violation categories, and six popular LLMs. Our extensive experimental results demonstrate that the optimized jailbreak prompts consistently achieve the highest attack success rates, as well as exhi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38271;&#25991;&#29983;&#25104;&#20013;&#30340;&#20107;&#23454;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#27573;&#33853;&#26102;&#21487;&#33021;&#20250;&#30001;&#20110;&#23454;&#20307;&#27169;&#31946;&#32780;&#23558;&#21487;&#39564;&#35777;&#30340;&#20107;&#23454;&#32452;&#21512;&#25104;&#38750;&#20107;&#23454;&#30340;&#27573;&#33853;&#12290;&#29616;&#26377;&#30340;&#20107;&#23454;&#20934;&#30830;&#24230;&#35780;&#20272;&#26041;&#27861;&#26080;&#27861;&#27491;&#30830;&#35780;&#20272;&#36825;&#20123;&#38750;&#20107;&#23454;&#27573;&#33853;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#24230;&#37327;&#25351;&#26631;&#26469;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05629</link><description>&lt;p&gt;
&#21512;&#24182;&#20107;&#23454;&#65292;&#22609;&#36896;&#35884;&#35823;&#65306;&#35780;&#20272;&#38271;&#25991;&#29983;&#25104;&#20013;&#32858;&#21512;&#20107;&#23454;&#24615;&#20027;&#24352;&#30340;&#30683;&#30462;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05629
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38271;&#25991;&#29983;&#25104;&#20013;&#30340;&#20107;&#23454;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#27573;&#33853;&#26102;&#21487;&#33021;&#20250;&#30001;&#20110;&#23454;&#20307;&#27169;&#31946;&#32780;&#23558;&#21487;&#39564;&#35777;&#30340;&#20107;&#23454;&#32452;&#21512;&#25104;&#38750;&#20107;&#23454;&#30340;&#27573;&#33853;&#12290;&#29616;&#26377;&#30340;&#20107;&#23454;&#20934;&#30830;&#24230;&#35780;&#20272;&#26041;&#27861;&#26080;&#27861;&#27491;&#30830;&#35780;&#20272;&#36825;&#20123;&#38750;&#20107;&#23454;&#27573;&#33853;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#24230;&#37327;&#25351;&#26631;&#26469;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20135;&#29983;&#30340;&#38271;&#25991;&#29983;&#25104;&#29289;&#21253;&#21547;&#20102;&#19968;&#31995;&#21015;&#20107;&#23454;&#21644;&#38750;&#20107;&#23454;&#30340;&#20027;&#24352;&#65292;&#36825;&#20351;&#24471;&#35780;&#20272;&#20107;&#23454;&#24615;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#20197;&#26356;&#31934;&#32454;&#30340;&#26041;&#24335;&#35780;&#20272;&#38271;&#25991;&#29983;&#25104;&#29289;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#23558;&#38271;&#25991;&#29983;&#25104;&#29289;&#20998;&#35299;&#20026;&#22810;&#20010;&#21487;&#39564;&#35777;&#30340;&#20107;&#23454;&#24182;&#29420;&#31435;&#39564;&#35777;&#36825;&#20123;&#20107;&#23454;&#12290;&#29983;&#25104;&#29289;&#30340;&#20107;&#23454;&#24615;&#26159;&#25152;&#26377;&#20107;&#23454;&#20013;&#21487;&#39564;&#35777;&#20107;&#23454;&#30340;&#27604;&#20363;&#12290;&#36825;&#20123;&#26041;&#27861;&#20551;&#35774;&#32467;&#21512;&#20102;&#20107;&#23454;&#20027;&#24352;&#24418;&#25104;&#20102;&#19968;&#20010;&#20107;&#23454;&#24615;&#27573;&#33853;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#36825;&#19968;&#20551;&#35774;&#21487;&#33021;&#22240;&#20026;&#23454;&#20307;&#27169;&#31946;&#32780;&#34987;&#36829;&#21453;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#21487;&#20197;&#29983;&#25104;&#21253;&#21547;&#21487;&#39564;&#35777;&#20107;&#23454;&#30340;&#27573;&#33853;&#65292;&#20294;&#30001;&#20110;&#23454;&#20307;&#27169;&#31946;&#65292;&#36825;&#20123;&#20107;&#23454;&#34987;&#32467;&#21512;&#24418;&#25104;&#20102;&#19968;&#20010;&#38750;&#20107;&#23454;&#30340;&#27573;&#33853;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#29616;&#26377;&#30340;&#20107;&#23454;&#20934;&#30830;&#24230;&#24230;&#37327;&#25351;&#26631;&#65292;&#21253;&#25324;FActScore&#21644;&#24341;&#29992;&#22238;&#39038;&#65292;&#26080;&#27861;&#27491;&#30830;&#35780;&#20272;&#36825;&#20123;&#38750;&#20107;&#23454;&#27573;&#33853;&#30340;&#20107;&#23454;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22686;&#24378;&#24230;&#37327;&#25351;&#26631;&#65292;D-FActScore&#65292;&#20316;&#20026;&#19968;&#20010;&#20855;&#20307;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-form generations from large language models (LLMs) contains a mix of factual and non-factual claims, making evaluating factuality difficult. To evaluate factual precision of long-form generations in a more fine-grained way, prior works propose to decompose long-form generations into multiple verifiable facts and verify those facts independently. The factuality of the generation is the proportion of verifiable facts among all the facts. Such methods assume that combining factual claims forms a factual paragraph. This paper shows that the assumption can be violated due to entity ambiguity. We show that LLMs can generate paragraphs that contain verifiable facts, but the facts are combined to form a non-factual paragraph due to entity ambiguity. We further reveal that existing factual precision metrics, including FActScore and citation recall, cannot properly evaluate the factuality of these non-factual paragraphs. To address this, we introduce an enhanced metric, D-FActScore, specifi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#20167;&#24680;&#12289;&#36785;&#39554;&#21644;&#20149;&#28174;&#26816;&#27979;&#30340;&#39640;&#25928;&#27169;&#22411;&#65292;&#22240;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#33021;&#23398;&#20064;&#21040;&#36825;&#20123;&#36127;&#38754;&#20869;&#23481;&#24182;&#29983;&#25104;&#19981;&#21512;&#36866;&#30340;&#25991;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.05624</link><description>&lt;p&gt;
&#38024;&#23545;&#20167;&#24680;&#12289;&#36785;&#39554;&#21644;&#20149;&#28174;&#26816;&#27979;&#30340;&#39640;&#25928;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Efficient Models for the Detection of Hate, Abuse and Profanity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05624
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#20167;&#24680;&#12289;&#36785;&#39554;&#21644;&#20149;&#28174;&#26816;&#27979;&#30340;&#39640;&#25928;&#27169;&#22411;&#65292;&#22240;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#33021;&#23398;&#20064;&#21040;&#36825;&#20123;&#36127;&#38754;&#20869;&#23481;&#24182;&#29983;&#25104;&#19981;&#21512;&#36866;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26159;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#30340;&#22522;&#30707;&#65292;&#22914;&#24773;&#24863;&#20998;&#26512;&#12289;&#25991;&#26723;&#20998;&#31867;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#38382;&#31572;&#12289;&#25688;&#35201;&#31561;&#12290;LLM&#36890;&#24120;&#22312;&#26469;&#33258;&#32593;&#32476;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20123;&#25968;&#25454;&#23481;&#26131;&#21253;&#21547;&#20167;&#24680;&#12289;&#36785;&#39554;&#21644;&#20149;&#28174;(HAP)&#20869;&#23481;&#12290;&#30001;&#20110;LLM&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25509;&#35302;&#21040;HAP&#20869;&#23481;&#65292;&#27169;&#22411;&#20250;&#23398;&#20064;&#21040;&#24182;&#29983;&#25104;&#24102;&#26377;&#20167;&#24680;&#25110;&#20149;&#28174;&#20869;&#23481;&#12290;&#20363;&#22914;&#65292;&#24403;&#20351;&#29992;HuggingFace(Transformers&#24211;&#30340;&#24320;&#28304;RoBERTa&#27169;&#22411;(&#20855;&#20307;&#26469;&#35828;&#65292;&#26159;RoBERTa&#22522;&#30784;&#27169;&#22411;))&#26469;&#26367;&#25442;&#21477;&#23376;`I do not know that Persian people are that MASK`&#20013;&#30340;&#25513;&#30721;&#26631;&#35760;&#26102;&#65292;&#23427;&#36820;&#22238;&#24471;&#20998;&#26368;&#39640;&#30340;&#35789;&#20026;`stupid`&#12290;&#36825;&#22312;&#25991;&#26126;&#23545;&#35805;&#20013;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#12290;&#25991;&#26412;&#20013;&#30340;&#20167;&#24680;&#12289;&#36785;&#39554;&#21644;&#20149;&#28174;&#30340;&#26816;&#27979;&#26159;&#21019;&#24314;&#25991;&#26126;&#21644;&#27809;&#26377;&#20559;&#35265;&#30340;LLM&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are the cornerstone for many Natural Language Processing (NLP) tasks like sentiment analysis, document classification, named entity recognition, question answering, summarization, etc. LLMs are often trained on data which originates from the web. This data is prone to having content with Hate, Abuse and Profanity (HAP). For a detailed definition of HAP, please refer to the Appendix. Due to the LLMs being exposed to HAP content during training, the models learn it and may then generate hateful or profane content. For example, when the open-source RoBERTa model (specifically, the RoBERTA base model) from the HuggingFace (HF) Transformers library is prompted to replace the mask token in `I do not know that Persian people are that MASK` it returns the word `stupid` with the highest score. This is unacceptable in civil discourse.The detection of Hate, Abuse and Profanity in text is a vital component of creating civil and unbiased LLMs, which is needed not only f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#26426;&#24037;&#20316;&#24066;&#22330;&#20998;&#26512;&#20013;&#30340;&#25216;&#33021;&#25552;&#21462;&#21644;&#20998;&#31867;&#26041;&#27861;&#12290;&#36890;&#36807;&#20840;&#38754;&#27010;&#36848;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#21644;&#26415;&#35821;&#65292;&#20197;&#21450;&#23545;&#20844;&#24320;&#21487;&#33719;&#21462;&#30340;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.05617</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#26426;&#24037;&#20316;&#24066;&#22330;&#20998;&#26512;&#65306;&#23545;&#25216;&#33021;&#25552;&#21462;&#21644;&#20998;&#31867;&#20174;&#25307;&#32856;&#20449;&#24687;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-based Computational Job Market Analysis: A Survey on Skill Extraction and Classification from Job Postings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05617
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#26426;&#24037;&#20316;&#24066;&#22330;&#20998;&#26512;&#20013;&#30340;&#25216;&#33021;&#25552;&#21462;&#21644;&#20998;&#31867;&#26041;&#27861;&#12290;&#36890;&#36807;&#20840;&#38754;&#27010;&#36848;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#21644;&#26415;&#35821;&#65292;&#20197;&#21450;&#23545;&#20844;&#24320;&#21487;&#33719;&#21462;&#30340;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20351;&#24471;&#35745;&#31639;&#26426;&#24037;&#20316;&#24066;&#22330;&#20998;&#26512;&#39046;&#22495;&#21462;&#24471;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#22312;&#35813;&#24212;&#29992;&#39046;&#22495;&#20013;&#65292;&#26680;&#24515;&#20219;&#21153;&#26159;&#20174;&#25307;&#32856;&#20449;&#24687;&#20013;&#25552;&#21462;&#21644;&#20998;&#31867;&#25216;&#33021;&#12290;&#30001;&#20110;&#20854;&#24555;&#36895;&#22686;&#38271;&#21644;&#36328;&#23398;&#31185;&#24615;&#36136;&#65292;&#30446;&#21069;&#23578;&#32570;&#20047;&#23545;&#36825;&#19968;&#20852;&#36215;&#39046;&#22495;&#30340;&#35814;&#23613;&#35780;&#20272;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#20851;&#20110;NLP&#39537;&#21160;&#30340;&#25216;&#33021;&#25552;&#21462;&#21644;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#21644;&#26415;&#35821;&#30340;&#20840;&#38754;&#27010;&#36848;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#23545;&#20844;&#24320;&#21487;&#33719;&#21462;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#38598;&#21019;&#24314;&#21644;&#29305;&#24449;&#30340;&#20449;&#24687;&#32570;&#20047;&#30340;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#23545;&#26415;&#35821;&#30340;&#37325;&#28857;&#20851;&#27880;&#35299;&#20915;&#20102;&#30446;&#21069;&#23545;&#20110;&#37325;&#35201;&#27010;&#24565;&#65288;&#22914;&#30828;&#25216;&#33021;&#21644;&#36719;&#25216;&#33021;&#65289;&#20197;&#21450;&#19982;&#25216;&#33021;&#25552;&#21462;&#21644;&#20998;&#31867;&#30456;&#20851;&#30340;&#26415;&#35821;&#32570;&#20047;&#19968;&#33268;&#23450;&#20041;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have brought significant advances to Natural Language Processing (NLP), which enabled fast progress in the field of computational job market analysis. Core tasks in this application domain are skill extraction and classification from job postings. Because of its quick growth and its interdisciplinary nature, there is no exhaustive assessment of this emerging field. This survey aims to fill this gap by providing a comprehensive overview of deep learning methodologies, datasets, and terminologies specific to NLP-driven skill extraction and classification. Our comprehensive cataloging of publicly available datasets addresses the lack of consolidated information on dataset creation and characteristics. Finally, the focus on terminology addresses the current lack of consistent definitions for important concepts, such as hard and soft skills, and terms relating to skill extraction and classification.
&lt;/p&gt;</description></item><item><title>&#39044;&#35757;&#32451;&#30340;&#23567;&#22411;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#24207;&#21015;&#22411;&#20219;&#21153;&#30340;&#36890;&#29992;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25351;&#20196;&#24494;&#35843;&#21487;&#20197;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#20219;&#21153;&#20013;&#23454;&#29616;&#25509;&#36817;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05616</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#24207;&#21015;&#22411;&#20219;&#21153;&#30340;&#36890;&#29992;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Pretrained Generative Language Models as General Learning Frameworks for Sequence-Based Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05616
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#23567;&#22411;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#24207;&#21015;&#22411;&#20219;&#21153;&#30340;&#36890;&#29992;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25351;&#20196;&#24494;&#35843;&#21487;&#20197;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#20219;&#21153;&#20013;&#23454;&#29616;&#25509;&#36817;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35266;&#28857;&#65292;&#21363;&#20855;&#26377;&#25968;&#30334;&#19975;&#21442;&#25968;&#30340;&#23567;&#22411;&#39044;&#35757;&#32451;&#22522;&#30784;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29992;&#20316;&#24207;&#21015;&#22411;&#20219;&#21153;&#30340;&#36890;&#29992;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#35299;&#20915;&#20102;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21644;&#35821;&#35328;&#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#35745;&#31639;&#36164;&#28304;&#12289;&#25216;&#33021;&#38656;&#27714;&#21644;&#26102;&#38388;&#38480;&#21046;&#31561;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19987;&#27880;&#20110;&#21019;&#24314;&#23567;&#22411;&#19988;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#25191;&#34892;&#22522;&#20110;&#20219;&#21153;&#27169;&#22411;&#26080;&#27861;&#23436;&#25104;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;125M&#12289;350M&#21644;1.3B&#21442;&#25968;&#30340;&#39044;&#35757;&#32451;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#65292;&#20351;&#29992;10,000&#21040;1,000,000&#20010;&#25351;&#20196;&#31034;&#20363;&#21487;&#20197;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21270;&#23398;&#20449;&#24687;&#23398;&#20219;&#21153;&#19978;&#23454;&#29616;&#25509;&#36817;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36830;&#32493;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26102;&#26399;&#23545;&#25913;&#36827;&#32467;&#26524;&#30340;&#20316;&#29992;&#65292;&#20197;&#21450;&#25968;&#25454;&#26684;&#24335;&#21644;&#39044;&#35757;&#32451;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#23545;&#25351;&#20196;&#24494;&#35843;&#25104;&#21151;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose that small pretrained foundational generative language models with millions of parameters can be utilized as a general learning framework for sequence-based tasks. Our proposal overcomes the computational resource, skill set, and timeline challenges associated with training neural networks and language models from scratch. Further, our approach focuses on creating small and highly specialized models that can accurately execute a challenging task of which the base model is incapable of performing. We demonstrate that 125M, 350M, and 1.3B parameter pretrained foundational language models can be instruction fine-tuned with 10,000-to-1,000,000 instruction examples to achieve near state-of-the-art results on challenging cheminformatics tasks. We also demonstrate the role of successive language model fine-tuning epochs on improved outcomes, as well as the importance of both data formatting and pretrained foundational language model selection for instruction fine-tuning success.
&lt;/p&gt;</description></item><item><title>AttnLRP&#26159;&#39318;&#20010;&#33021;&#22815;&#24544;&#23454;&#19988;&#20840;&#38754;&#22320;&#24402;&#22240;Transformer&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20855;&#26377;&#19982;&#21333;&#19968;&#21453;&#21521;&#20256;&#25773;&#30456;&#20284;&#30340;&#35745;&#31639;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#25193;&#23637;&#36880;&#23618;&#30456;&#20851;&#20256;&#36882;&#24402;&#22240;&#26041;&#27861;&#20197;&#22788;&#29702;&#27880;&#24847;&#21147;&#23618;&#26469;&#35299;&#20915;&#20102;&#40657;&#30418;Transformer&#27169;&#22411;&#30340;&#24402;&#22240;&#38382;&#39064;&#65292;&#20855;&#26377;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#29702;&#35299;&#28508;&#22312;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05602</link><description>&lt;p&gt;
AttnLRP: &#27880;&#24847;&#21147;&#24863;&#30693;&#30340;&#36880;&#23618;&#30456;&#20851;&#20256;&#36882;&#29992;&#20110;Transformer
&lt;/p&gt;
&lt;p&gt;
AttnLRP: Attention-Aware Layer-wise Relevance Propagation for Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05602
&lt;/p&gt;
&lt;p&gt;
AttnLRP&#26159;&#39318;&#20010;&#33021;&#22815;&#24544;&#23454;&#19988;&#20840;&#38754;&#22320;&#24402;&#22240;Transformer&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20855;&#26377;&#19982;&#21333;&#19968;&#21453;&#21521;&#20256;&#25773;&#30456;&#20284;&#30340;&#35745;&#31639;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#25193;&#23637;&#36880;&#23618;&#30456;&#20851;&#20256;&#36882;&#24402;&#22240;&#26041;&#27861;&#20197;&#22788;&#29702;&#27880;&#24847;&#21147;&#23618;&#26469;&#35299;&#20915;&#20102;&#40657;&#30418;Transformer&#27169;&#22411;&#30340;&#24402;&#22240;&#38382;&#39064;&#65292;&#20855;&#26377;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#29702;&#35299;&#28508;&#22312;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#20135;&#29983;&#20559;&#35265;&#30340;&#39044;&#27979;&#21644;&#24187;&#35937;&#65292;&#36825;&#31361;&#26174;&#20102;&#29702;&#35299;&#20854;&#27169;&#22411;&#20869;&#37096;&#25512;&#29702;&#36807;&#31243;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#23545;&#25972;&#20010;&#40657;&#30418;Transformer&#27169;&#22411;&#30340;&#20934;&#30830;&#24402;&#22240;&#24182;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#25193;&#23637;&#36880;&#23618;&#30456;&#20851;&#20256;&#36882;&#24402;&#22240;&#26041;&#27861;&#20197;&#22788;&#29702;&#27880;&#24847;&#21147;&#23618;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#34429;&#28982;&#23384;&#22312;&#37096;&#20998;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#39318;&#20010;&#33021;&#22815;&#24544;&#23454;&#19988;&#20840;&#38754;&#22320;&#24402;&#22240;Transformer&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#28508;&#22312;&#34920;&#31034;&#65292;&#21516;&#26102;&#35745;&#31639;&#25928;&#29575;&#19982;&#21333;&#19968;&#21453;&#21521;&#20256;&#25773;&#30456;&#20284;&#12290;&#36890;&#36807;&#23545;Llama 2&#12289;Flan-T5&#21644;Vision Transformer&#26550;&#26500;&#19978;&#19982;&#29616;&#26377;&#26041;&#27861;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#36229;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#29702;&#35299;&#28508;&#22312;&#34920;&#31034;&#65292;&#20026;&#27010;&#24565;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are prone to biased predictions and hallucinations, underlining the paramount importance of understanding their model-internal reasoning process. However, achieving faithful attributions for the entirety of a black-box transformer model and maintaining computational efficiency is an unsolved challenge. By extending the Layer-wise Relevance Propagation attribution method to handle attention layers, we address these challenges effectively. While partial solutions exist, our method is the first to faithfully and holistically attribute not only input but also latent representations of transformer models with the computational efficiency similar to a singular backward pass. Through extensive evaluations against existing methods on Llama 2, Flan-T5 and the Vision Transformer architecture, we demonstrate that our proposed approach surpasses alternative methods in terms of faithfulness and enables the understanding of latent representations, opening up the door for concep
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SoftEDA&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36719;&#26631;&#31614;&#22312;&#22686;&#24378;&#25968;&#25454;&#19978;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21487;&#33021;&#30772;&#22351;&#25991;&#26412;&#21407;&#22987;&#21547;&#20041;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05591</link><description>&lt;p&gt;
SoftEDA: &#29992;&#36719;&#26631;&#31614;&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#35268;&#21017;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
SoftEDA: Rethinking Rule-Based Data Augmentation with Soft Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SoftEDA&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36719;&#26631;&#31614;&#22312;&#22686;&#24378;&#25968;&#25454;&#19978;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21487;&#33021;&#30772;&#22351;&#25991;&#26412;&#21407;&#22987;&#21547;&#20041;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35268;&#21017;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#22240;&#20854;&#31616;&#21333;&#24615;&#32780;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#30772;&#22351;&#25991;&#26412;&#30340;&#21407;&#22987;&#21547;&#20041;&#65292;&#26368;&#32456;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36719;&#26631;&#31614;&#24212;&#29992;&#20110;&#22686;&#24378;&#25968;&#25454;&#30340;&#31616;&#21333;&#25216;&#26415;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19971;&#20010;&#19981;&#21516;&#30340;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#24182;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#24050;&#32463;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#20197;&#20415;&#22797;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rule-based text data augmentation is widely used for NLP tasks due to its simplicity. However, this method can potentially damage the original meaning of the text, ultimately hurting the performance of the model. To overcome this limitation, we propose a straightforward technique for applying soft labels to augmented data. We conducted experiments across seven different classification tasks and empirically demonstrated the effectiveness of our proposed approach. We have publicly opened our source code for reproducibility.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;AutoAugment&#26041;&#27861;&#24212;&#29992;&#20110;&#35299;&#20915;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#20013;&#30340;&#35821;&#20041;&#25439;&#23475;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#21152;&#24378;&#29616;&#26377;&#30340;&#22686;&#24378;&#26041;&#27861;&#24182;&#25552;&#21319;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.05584</link><description>&lt;p&gt;
AutoAugment&#26159;&#20320;&#38656;&#35201;&#30340;&#65306;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#22686;&#24378;&#22522;&#20110;&#35268;&#21017;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AutoAugment Is What You Need: Enhancing Rule-based Augmentation Methods in Low-resource Regimes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;AutoAugment&#26041;&#27861;&#24212;&#29992;&#20110;&#35299;&#20915;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#20013;&#30340;&#35821;&#20041;&#25439;&#23475;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#21152;&#24378;&#29616;&#26377;&#30340;&#22686;&#24378;&#26041;&#27861;&#24182;&#25552;&#21319;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#21477;&#23376;&#30340;&#31163;&#25955;&#24615;&#36136;&#12290;&#23613;&#31649;&#22522;&#20110;&#35268;&#21017;&#30340;&#22686;&#24378;&#26041;&#27861;&#22240;&#20854;&#31616;&#21333;&#24615;&#32780;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#23548;&#33268;&#28508;&#22312;&#30340;&#35821;&#20041;&#25439;&#23475;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#20351;&#29992;&#36719;&#26631;&#31614;&#65288;softEDA&#65289;&#36827;&#34892;&#31616;&#21333;&#25968;&#25454;&#22686;&#24378;&#65292;&#24182;&#37319;&#29992;&#26631;&#31614;&#24179;&#28369;&#26469;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#27599;&#20010;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#25214;&#21040;&#26368;&#20339;&#22240;&#23376;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#27492;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20351;&#29992;softEDA&#20173;&#28982;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;AutoAugment&#24212;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#21319;&#29616;&#26377;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#24182;&#19988;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text data augmentation is a complex problem due to the discrete nature of sentences. Although rule-based augmentation methods are widely adopted in real-world applications because of their simplicity, they suffer from potential semantic damage. Previous researchers have suggested easy data augmentation with soft labels (softEDA), employing label smoothing to mitigate this problem. However, finding the best factor for each model and dataset is challenging; therefore, using softEDA in real-world applications is still difficult. In this paper, we propose adapting AutoAugment to solve this problem. The experimental results suggest that the proposed method can boost existing augmentation methods and that rule-based methods can enhance cutting-edge pre-trained language models. We offer the source code.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#36328;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#21521;&#37327;&#34920;&#31034;&#26469;&#30830;&#23450;&#38899;&#39057;&#24405;&#38899;&#20043;&#38388;&#30340;&#25277;&#35937;&#31243;&#24230;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#36827;&#34892;ABX&#27979;&#35797;&#65292;&#35813;&#30740;&#31350;&#35777;&#23454;&#20102;&#20174;&#20855;&#26377;&#19981;&#21516;&#35821;&#35328;&#21644;&#38750;&#35821;&#35328;&#29305;&#24449;&#30340;&#24405;&#38899;&#25552;&#21462;&#30340;&#34920;&#31034;&#27839;&#30528;&#30456;&#21516;&#30340;&#32500;&#24230;&#26377;&#25152;&#19981;&#21516;&#12290;&#36825;&#19968;&#26041;&#27861;&#26159;&#26080;&#30417;&#30563;&#30340;&#65292;&#21487;&#20197;&#20316;&#20026;&#33719;&#21462;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#38899;&#39057;&#34920;&#24449;&#30340;&#26377;&#25928;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2402.05581</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;&#36328;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#32500;&#24230;&#19978;&#24314;&#31435;&#38899;&#39057;&#24405;&#38899;&#20043;&#38388;&#30340;&#20146;&#23494;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
Establishing degrees of closeness between audio recordings along different dimensions using large-scale cross-lingual models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05581
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#36328;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#21521;&#37327;&#34920;&#31034;&#26469;&#30830;&#23450;&#38899;&#39057;&#24405;&#38899;&#20043;&#38388;&#30340;&#25277;&#35937;&#31243;&#24230;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#36827;&#34892;ABX&#27979;&#35797;&#65292;&#35813;&#30740;&#31350;&#35777;&#23454;&#20102;&#20174;&#20855;&#26377;&#19981;&#21516;&#35821;&#35328;&#21644;&#38750;&#35821;&#35328;&#29305;&#24449;&#30340;&#24405;&#38899;&#25552;&#21462;&#30340;&#34920;&#31034;&#27839;&#30528;&#30456;&#21516;&#30340;&#32500;&#24230;&#26377;&#25152;&#19981;&#21516;&#12290;&#36825;&#19968;&#26041;&#27861;&#26159;&#26080;&#30417;&#30563;&#30340;&#65292;&#21487;&#20197;&#20316;&#20026;&#33719;&#21462;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#38899;&#39057;&#34920;&#24449;&#30340;&#26377;&#25928;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#30740;&#31350;&#30340;&#39640;&#24230;&#32422;&#26463;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#35821;&#38899;&#21521;&#37327;&#34920;&#31034;&#26469;&#30830;&#23450;&#23427;&#20204;&#30456;&#23545;&#20110;&#38899;&#39057;&#20449;&#21495;&#30340;&#25277;&#35937;&#31243;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#21033;&#29992;&#20855;&#26377;&#31934;&#24515;&#31574;&#21010;&#30340;&#20803;&#25968;&#25454;&#30340;&#38899;&#39057;&#24405;&#38899;&#36827;&#34892;ABX&#27979;&#35797;&#65292;&#20197;&#20102;&#35299;&#34920;&#31034;&#20013;&#23384;&#22312;&#30340;&#20449;&#24687;&#31867;&#22411;&#12290;ABX&#27979;&#35797;&#30830;&#23450;&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#35745;&#31639;&#30340;&#34920;&#31034;&#26159;&#21542;&#32534;&#30721;&#20102;&#32473;&#23450;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#20010;&#23454;&#39564;&#65306;&#19968;&#20010;&#20851;&#20110;&#25151;&#38388;&#22768;&#23398;&#29305;&#24615;&#65292;&#19968;&#20010;&#20851;&#20110;&#35821;&#35328;&#20307;&#35009;&#65292;&#19968;&#20010;&#20851;&#20110;&#35821;&#38899;&#29305;&#24449;&#12290;&#32467;&#26524;&#35777;&#23454;&#65292;&#26469;&#33258;&#20855;&#26377;&#19981;&#21516;&#35821;&#35328;/&#38750;&#35821;&#35328;&#29305;&#24615;&#30340;&#24405;&#38899;&#30340;&#34920;&#31034;&#27839;&#30528;&#30456;&#21516;&#30340;&#32447;&#36335;&#26377;&#25152;&#19981;&#21516;&#12290;&#23558;&#26356;&#22810;&#30340;&#38899;&#39057;&#20449;&#21495;&#23884;&#20837;&#19968;&#20010;&#21521;&#37327;&#20013;&#21487;&#20197;&#26356;&#22909;&#22320;&#21306;&#20998;&#38750;&#35821;&#35328;&#29305;&#24449;&#65292;&#32780;&#36739;&#30701;&#30340;&#29255;&#27573;&#21487;&#20197;&#26356;&#22909;&#22320;&#21306;&#20998;&#20998;&#27573;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#23436;&#20840;&#26080;&#30417;&#30563;&#65292;&#21487;&#33021;&#26159;&#19968;&#31181;&#20174;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#38899;&#39057;&#34920;&#24449;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the highly constrained context of low-resource language studies, we explore vector representations of speech from a pretrained model to determine their level of abstraction with regard to the audio signal. We propose a new unsupervised method using ABX tests on audio recordings with carefully curated metadata to shed light on the type of information present in the representations. ABX tests determine whether the representations computed by a multilingual speech model encode a given characteristic. Three experiments are devised: one on room acoustics aspects, one on linguistic genre, and one on phonetic aspects. The results confirm that the representations extracted from recordings with different linguistic/extra-linguistic characteristics differ along the same lines. Embedding more audio signal in one vector better discriminates extra-linguistic characteristics, whereas shorter snippets are better to distinguish segmental information. The method is fully unsupervised, potentially op
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#39278;&#39135;&#32010;&#20081;&#30456;&#20851;&#30340;&#25512;&#25991;&#36827;&#34892;&#20998;&#31867;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#22312;&#22235;&#20010;&#31867;&#21035;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;F1&#20998;&#25968;&#65292;&#24182;&#34920;&#26126;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#36739;&#20256;&#32479;&#25216;&#26415;&#26356;&#20026;&#20248;&#36234;&#65292;&#20294;&#38656;&#35201;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2402.05571</link><description>&lt;p&gt;
&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20110;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#30340;&#36716;&#25442;&#22120;(BERT)&#30340;&#33258;&#21160;&#20998;&#31867;&#39278;&#39135;&#32010;&#20081;&#25512;&#25991;&#65306;&#31639;&#27861;&#24320;&#21457;&#21644;&#39564;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Traditional Machine Learning Models and Bidirectional Encoder Representations From Transformer (BERT)-Based Automatic Classification of Tweets About Eating Disorders: Algorithm Development and Validation Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05571
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#39278;&#39135;&#32010;&#20081;&#30456;&#20851;&#30340;&#25512;&#25991;&#36827;&#34892;&#20998;&#31867;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#22312;&#22235;&#20010;&#31867;&#21035;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;F1&#20998;&#25968;&#65292;&#24182;&#34920;&#26126;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#36739;&#20256;&#32479;&#25216;&#26415;&#26356;&#20026;&#20248;&#36234;&#65292;&#20294;&#38656;&#35201;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#39278;&#39135;&#32010;&#20081;&#26085;&#30410;&#26222;&#36941;&#65292;&#31038;&#20132;&#32593;&#32476;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#20449;&#24687;&#12290;&#30446;&#26631;&#65306;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#26377;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#19982;&#39278;&#39135;&#32010;&#20081;&#30456;&#20851;&#30340;&#25512;&#25991;&#36827;&#34892;&#20998;&#31867;&#12290;&#26041;&#27861;&#65306;&#22312;&#19977;&#20010;&#26376;&#20869;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#20851;&#20110;&#39278;&#39135;&#32010;&#20081;&#30340;&#25512;&#25991;&#12290;&#25105;&#20204;&#23545;2,000&#26465;&#25512;&#25991;&#36827;&#34892;&#20102;&#26631;&#35760;&#65292;&#26631;&#35760;&#20102;&#65306;(1)&#30001;&#39278;&#39135;&#32010;&#20081;&#24739;&#32773;&#25776;&#20889;&#30340;&#25512;&#25991;&#65292;(2)&#23459;&#20256;&#39278;&#39135;&#32010;&#20081;&#30340;&#25512;&#25991;&#65292;(3)&#20449;&#24687;&#24615;&#65292;&#21644;(4)&#31185;&#23398;&#20869;&#23481;&#12290;&#20351;&#29992;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#35780;&#20272;&#20934;&#30830;&#24230;&#12289;F1&#20998;&#25968;&#21644;&#35745;&#31639;&#26102;&#38388;&#12290;&#32467;&#26524;&#65306;&#20174;1,058,957&#26465;&#25910;&#38598;&#21040;&#30340;&#25512;&#25991;&#20013;&#65292;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#22312;&#25152;&#26377;&#22235;&#20010;&#31867;&#21035;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;F1&#20998;&#25968;(71.1%-86.4%)&#12290;&#32467;&#35770;&#65306;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#22312;&#20998;&#31867;&#19982;&#39278;&#39135;&#32010;&#20081;&#30456;&#20851;&#30340;&#25512;&#25991;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#25216;&#26415;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Eating disorders are increasingly prevalent, and social networks offer valuable information.   Objective: Our goal was to identify efficient machine learning models for categorizing tweets related to eating disorders.   Methods: Over three months, we collected tweets about eating disorders. A 2,000-tweet subset was labeled for: (1) being written by individuals with eating disorders, (2) promoting eating disorders, (3) informativeness, and (4) scientific content. Both traditional machine learning and deep learning models were employed for classification, assessing accuracy, F1 score, and computational time.   Results: From 1,058,957 collected tweets, transformer-based bidirectional encoder representations achieved the highest F1 scores (71.1%-86.4%) across all four categories.   Conclusions: Transformer-based models outperform traditional techniques in classifying eating disorder-related tweets, though they require more computational resources.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#8220;ChatCoach&#8221;&#65292;&#19968;&#20010;&#38598;&#25104;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21307;&#29983;&#21512;&#20316;&#30340;&#26694;&#26550;&#65292;&#22312;&#20132;&#27969;&#21307;&#30103;&#36741;&#23548;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#27169;&#25311;&#29615;&#22659;&#21644;&#23454;&#26102;&#21453;&#39304;&#65292;&#20197;&#24110;&#21161;&#21307;&#23398;&#23398;&#21592;&#25552;&#39640;&#27807;&#36890;&#25216;&#24039;&#12290;</title><link>https://arxiv.org/abs/2402.05547</link><description>&lt;p&gt;
&#22312;&#20132;&#27969;&#21307;&#30103;&#36741;&#23548;&#19978;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65306;&#19968;&#31181;&#26032;&#30340;&#31995;&#32479;&#21644;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Large Language Models on Communicative Medical Coaching: a Novel System and Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#8220;ChatCoach&#8221;&#65292;&#19968;&#20010;&#38598;&#25104;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21307;&#29983;&#21512;&#20316;&#30340;&#26694;&#26550;&#65292;&#22312;&#20132;&#27969;&#21307;&#30103;&#36741;&#23548;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#27169;&#25311;&#29615;&#22659;&#21644;&#23454;&#26102;&#21453;&#39304;&#65292;&#20197;&#24110;&#21161;&#21307;&#23398;&#23398;&#21592;&#25552;&#39640;&#27807;&#36890;&#25216;&#24039;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#20256;&#32479;&#24212;&#29992;&#20027;&#35201;&#38598;&#20013;&#22312;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#26381;&#21153;&#19978;&#65292;&#22686;&#24378;&#24739;&#32773;&#20114;&#21160;&#21644;&#25252;&#29702;&#20132;&#20184;&#65292;&#20363;&#22914;&#21307;&#23398;&#23545;&#35805;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;NLP&#22312;&#24110;&#21161;&#32463;&#39564;&#19981;&#20016;&#23500;&#30340;&#21307;&#29983;&#65292;&#29305;&#21035;&#26159;&#22312;&#20132;&#27969;&#21307;&#30103;&#36741;&#23548;&#31561;&#39046;&#22495;&#30340;&#28508;&#21147;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;ChatCoach&#8221;&#65292;&#19968;&#20010;&#38598;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#19968;&#20010;&#24739;&#32773;&#20195;&#29702;&#21644;&#19968;&#20010;&#36741;&#23548;&#20195;&#29702;&#20849;&#21516;&#25903;&#25345;&#21307;&#23398;&#23398;&#21592;&#22312;&#20250;&#35786;&#36807;&#31243;&#20013;&#32451;&#20064;&#21307;&#23398;&#27807;&#36890;&#25216;&#24039;&#12290;&#19982;&#20256;&#32479;&#30340;&#23545;&#35805;&#31995;&#32479;&#19981;&#21516;&#65292;ChatCoach&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#25311;&#29615;&#22659;&#65292;&#21307;&#29983;&#21487;&#20197;&#22312;&#20854;&#20013;&#19982;&#24739;&#32773;&#20195;&#29702;&#36827;&#34892;&#21307;&#23398;&#23545;&#35805;&#12290;&#21516;&#26102;&#65292;&#36741;&#23548;&#20195;&#29702;&#20250;&#25552;&#20379;&#23454;&#26102;&#21453;&#39304;&#32473;&#21307;&#29983;&#12290;&#20026;&#20102;&#26500;&#24314;ChatCoach&#31995;&#32479;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#38598;&#25104;&#20102;ChatGPT&#21644;Llama2&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35780;&#20272;&#23427;&#20204;&#22312;&#20132;&#27969;&#21307;&#30103;&#36741;&#23548;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional applications of natural language processing (NLP) in healthcare have predominantly focused on patient-centered services, enhancing patient interactions and care delivery, such as through medical dialogue systems. However, the potential of NLP to benefit inexperienced doctors, particularly in areas such as communicative medical coaching, remains largely unexplored. We introduce ``ChatCoach,'' an integrated human-AI cooperative framework. Within this framework, both a patient agent and a coaching agent collaboratively support medical learners in practicing their medical communication skills during consultations. Unlike traditional dialogue systems, ChatCoach provides a simulated environment where a human doctor can engage in medical dialogue with a patient agent. Simultaneously, a coaching agent provides real-time feedback to the doctor. To construct the ChatCoach system, we developed a dataset and integrated Large Language Models such as ChatGPT and Llama2, aiming to assess 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26500;&#24314;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#35821;&#38899;&#36716;&#25991;&#23383;&#36716;&#24405;&#25968;&#25454;&#25552;&#21462;&#22320;&#22336;&#37096;&#20998;&#12290;&#36890;&#36807;&#21033;&#29992;SlovakBERT&#27169;&#22411;&#65292;&#24182;&#24378;&#35843;&#22312;&#21512;&#25104;&#25968;&#25454;&#20013;&#27169;&#25311;&#21475;&#35821;&#35821;&#35328;&#30340;&#21487;&#21464;&#24615;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20165;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;NER&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.05545</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35821;&#38899;&#36716;&#25991;&#23383;&#20013;&#22320;&#22336;&#35782;&#21035;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition for Address Extraction in Speech-to-Text Transcriptions Using Synthetic Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26500;&#24314;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#35821;&#38899;&#36716;&#25991;&#23383;&#36716;&#24405;&#25968;&#25454;&#25552;&#21462;&#22320;&#22336;&#37096;&#20998;&#12290;&#36890;&#36807;&#21033;&#29992;SlovakBERT&#27169;&#22411;&#65292;&#24182;&#24378;&#35843;&#22312;&#21512;&#25104;&#25968;&#25454;&#20013;&#27169;&#25311;&#21475;&#35821;&#35821;&#35328;&#30340;&#21487;&#21464;&#24615;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20165;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;NER&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;Bidirectional Encoder Representations from Transformations&#65288;BERT&#65289;&#26550;&#26500;&#19978;&#24314;&#31435;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#21033;&#29992;&#20102;SlovakBERT&#27169;&#22411;&#12290;&#35813;NER&#27169;&#22411;&#20174;&#35821;&#38899;&#36716;&#25991;&#23383;&#36716;&#24405;&#25968;&#25454;&#20013;&#25552;&#21462;&#22320;&#22336;&#37096;&#20998;&#12290;&#30001;&#20110;&#30495;&#23454;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#20351;&#29992;GPT API&#29983;&#25104;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#24378;&#35843;&#22312;&#36825;&#20010;&#20154;&#24037;&#25968;&#25454;&#20013;&#27169;&#25311;&#21475;&#35821;&#35821;&#35328;&#30340;&#21487;&#21464;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;&#20351;&#29992;&#20165;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;NER&#27169;&#22411;&#65292;&#23545;&#23567;&#35268;&#27169;&#30495;&#23454;&#27979;&#35797;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an approach for building a Named Entity Recognition (NER) model built upon a Bidirectional Encoder Representations from Transformers (BERT) architecture, specifically utilizing the SlovakBERT model. This NER model extracts address parts from data acquired from speech-to-text transcriptions. Due to scarcity of real data, a synthetic dataset using GPT API was generated. The importance of mimicking spoken language variability in this artificial data is emphasized. The performance of our NER model, trained solely on synthetic data, is evaluated using small real test dataset.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#30693;&#35782;&#22270;&#35889;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#20511;&#21161;&#23454;&#20307;&#35782;&#21035;&#21644;&#38142;&#25509;&#25216;&#26415;&#20197;&#21450;&#19978;&#19979;&#25991;&#21270;&#23884;&#20837;&#65292;&#22686;&#24378;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#20998;&#31867;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#20027;&#35201;&#24212;&#29992;&#20110;&#20581;&#24247;&#39046;&#22495;&#30340;&#39278;&#39135;&#38556;&#30861;&#35782;&#21035;&#65292;&#20026;&#26089;&#26399;&#35786;&#26029;&#25552;&#20379;&#24110;&#21161;&#12290;</title><link>https://arxiv.org/abs/2402.05536</link><description>&lt;p&gt;
&#20026;&#21152;&#24378;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#30340;&#39278;&#39135;&#38556;&#30861;&#26816;&#27979;&#65292;&#36171;&#33021;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35821;&#22659;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Empowering machine learning models with contextual knowledge for enhancing the detection of eating disorders in social media posts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05536
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#30693;&#35782;&#22270;&#35889;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#20511;&#21161;&#23454;&#20307;&#35782;&#21035;&#21644;&#38142;&#25509;&#25216;&#26415;&#20197;&#21450;&#19978;&#19979;&#25991;&#21270;&#23884;&#20837;&#65292;&#22686;&#24378;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#20998;&#31867;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#20027;&#35201;&#24212;&#29992;&#20110;&#20581;&#24247;&#39046;&#22495;&#30340;&#39278;&#39135;&#38556;&#30861;&#35782;&#21035;&#65292;&#20026;&#26089;&#26399;&#35786;&#26029;&#25552;&#20379;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#22312;&#20449;&#24687;&#20849;&#20139;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#22312;&#20581;&#24247;&#39046;&#22495;&#29992;&#20110;&#35752;&#35770;&#30142;&#30149;&#21644;&#27835;&#30103;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24179;&#21488;&#19978;&#30340;&#24086;&#23376;&#24448;&#24448;&#26159;&#31616;&#30701;&#30340;&#25991;&#26412;&#65292;&#32473;&#20154;&#24037;&#26234;&#33021;&#22312;&#29702;&#35299;&#19978;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#32467;&#21512;&#31038;&#21306;&#32500;&#25252;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;&#22914;Wikidata&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#20197;&#22686;&#24378;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20808;&#36827;&#30340;&#23454;&#20307;&#35782;&#21035;&#22120;&#21644;&#38142;&#25509;&#22120;&#65288;&#22914;Falcon 2.0&#65289;&#23558;&#30701;&#25991;&#24086;&#23376;&#20013;&#30340;&#23454;&#20307;&#36830;&#25509;&#21040;&#30693;&#35782;&#22270;&#35889;&#12290;&#28982;&#21518;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGEs&#65289;&#21644;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#65288;&#22914;BERT&#65289;&#26469;&#21019;&#24314;&#20016;&#23500;&#30340;&#12289;&#22522;&#20110;&#35821;&#22659;&#30340;&#24086;&#23376;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#20581;&#24247;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#35782;&#21035;&#19982;&#39278;&#39135;&#38556;&#30861;&#30456;&#20851;&#30340;&#24086;&#23376;&#65288;&#22914;&#21388;&#39135;&#30151;&#12289;&#26292;&#39135;&#30151;&#65289;&#65292;&#20197;&#24110;&#21161;&#21307;&#30103;&#26381;&#21153;&#25552;&#20379;&#32773;&#36827;&#34892;&#26089;&#26399;&#35786;&#26029;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21253;&#21547;2,000&#26465;&#20851;&#20110;&#39278;&#39135;&#38556;&#30861;&#30340;&#25512;&#25991;&#30340;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#21512;&#24182;&#21333;&#35789;&#23884;&#20837;&#21644;&#19978;&#19979;&#25991;&#21270;&#23884;&#20837;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social networks are vital for information sharing, especially in the health sector for discussing diseases and treatments. These platforms, however, often feature posts as brief texts, posing challenges for Artificial Intelligence (AI) in understanding context. We introduce a novel hybrid approach combining community-maintained knowledge graphs (like Wikidata) with deep learning to enhance the categorization of social media posts. This method uses advanced entity recognizers and linkers (like Falcon 2.0) to connect short post entities to knowledge graphs. Knowledge graph embeddings (KGEs) and contextualized word embeddings (like BERT) are then employed to create rich, context-based representations of these posts.   Our focus is on the health domain, particularly in identifying posts related to eating disorders (e.g., anorexia, bulimia) to aid healthcare providers in early diagnosis. We tested our approach on a dataset of 2,000 tweets about eating disorders, finding that merging word em
&lt;/p&gt;</description></item><item><title>NoisyICL&#36890;&#36807;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#24341;&#20837;&#22122;&#38899;&#65292;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#21644;&#26657;&#20934;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;NoisyICL&#21487;&#20197;&#20135;&#29983;&#26356;&#20934;&#30830;&#12289;&#26356;&#20844;&#24179;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.05515</link><description>&lt;p&gt;
NoisyICL: &#19968;&#28857;&#22122;&#38899;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#12289;
&lt;/p&gt;
&lt;p&gt;
NoisyICL: A Little Noise in Model Parameters Calibrates In-context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05515
&lt;/p&gt;
&lt;p&gt;
NoisyICL&#36890;&#36807;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#24341;&#20837;&#22122;&#38899;&#65292;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#21644;&#26657;&#20934;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;NoisyICL&#21487;&#20197;&#20135;&#29983;&#26356;&#20934;&#30830;&#12289;&#26356;&#20844;&#24179;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064; (ICL) &#22312;&#39640;&#20808;&#39564;&#20559;&#24046;&#21644;&#19981;&#21487;&#20449;&#20219;&#30340;&#32622;&#20449;&#24230;&#30340;&#24433;&#21709;&#19979;&#65292;&#34920;&#29616;&#19981;&#20339;&#19988;&#26657;&#20934;&#19981;&#36275;&#12290;&#20197;&#24448;&#30340;&#19968;&#20123;&#24037;&#20316;&#36890;&#36807;&#20351;&#29992;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#21644;&#35745;&#31639;&#25104;&#26412;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#25913;&#21892; ICL &#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; NoisyICL&#65292;&#36890;&#36807;&#38543;&#26426;&#22122;&#38899;&#25200;&#21160;&#27169;&#22411;&#21442;&#25968;&#26469;&#21162;&#21147;&#25552;&#39640;&#24615;&#33021;&#21644;&#26657;&#20934;&#24615;&#12290;&#25105;&#20204;&#22312;2&#20010;&#27169;&#22411;&#21644;12&#20010;&#19979;&#28216;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;NoisyICL&#21487;&#20197;&#24110;&#21161;ICL&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;NoisyICL&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#25552;&#20379;&#26356;&#20844;&#24179;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#32622;&#20449;&#24230;&#26356;&#21487;&#20449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;NoisyICL&#26159;ICL&#30340;&#19968;&#31181;&#26377;&#25928;&#26657;&#20934;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20195;&#30721;&#24050;&#19978;&#20256;&#33267;Github&#12290;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning (ICL) is suffering from unsatisfactory performance and under-calibration due to high prior bias and unfaithful confidence. Some previous works fine-tuned language models for better ICL performance with enormous datasets and computing costs. In this paper, we propose NoisyICL, simply perturbing the model parameters by random noises to strive for better performance and calibration. Our experiments on 2 models and 12 downstream datasets show that NoisyICL can help ICL produce more accurate predictions. Our further analysis indicates that NoisyICL enables the model to provide more fair predictions, and also with less unfaithful confidence. Therefore, we believe that NoisyICL is an effective calibration of ICL. Our experimental code is uploaded to Github.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#27880;&#37322;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#36153;&#29992;&#25928;&#30410;&#39640;&#21644;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#27880;&#37322;&#30340;&#20248;&#28857;&#65292;&#21516;&#26102;&#26500;&#24314;&#20102;&#19968;&#20010;&#22270;&#20687;&#23383;&#24149;&#25968;&#25454;&#38598;&#24182;&#24320;&#25918;&#20102;&#28304;&#20195;&#30721;&#12290;</title><link>https://arxiv.org/abs/2402.05512</link><description>&lt;p&gt;
GPT&#23545;&#20110;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20855;&#26377;&#22810;&#35821;&#35328;&#27880;&#37322;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
GPTs Are Multilingual Annotators for Sequence Generation Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#27880;&#37322;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#36153;&#29992;&#25928;&#30410;&#39640;&#21644;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#27880;&#37322;&#30340;&#20248;&#28857;&#65292;&#21516;&#26102;&#26500;&#24314;&#20102;&#19968;&#20010;&#22270;&#20687;&#23383;&#24149;&#25968;&#25454;&#38598;&#24182;&#24320;&#25918;&#20102;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27880;&#37322;&#26159;&#26500;&#24314;&#26032;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#36890;&#36807;&#20247;&#21253;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#30340;&#26041;&#27861;&#26082;&#32791;&#26102;&#21448;&#26114;&#36149;&#12290;&#27492;&#22806;&#65292;&#24403;&#22788;&#29702;&#20302;&#36164;&#28304;&#35821;&#35328;&#26102;&#65292;&#30001;&#20110;&#20247;&#21253;&#24037;&#20316;&#32773;&#30340;&#35821;&#35328;&#27744;&#24046;&#24322;&#65292;&#36825;&#20010;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#27880;&#37322;&#30340;&#26041;&#27861;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#36153;&#29992;&#25928;&#30410;&#39640;&#65292;&#32780;&#19988;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#27880;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#22270;&#20687;&#23383;&#24149;&#25968;&#25454;&#38598;&#65292;&#24182;&#33268;&#21147;&#20110;&#23558;&#27492;&#25968;&#25454;&#38598;&#24320;&#25918;&#32473;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#24050;&#32463;&#24320;&#25918;&#20102;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#65292;&#20197;&#20415;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#21487;&#22797;&#29616;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data annotation is an essential step for constructing new datasets. However, the conventional approach of data annotation through crowdsourcing is both time-consuming and expensive. In addition, the complexity of this process increases when dealing with low-resource languages owing to the difference in the language pool of crowdworkers. To address these issues, this study proposes an autonomous annotation method by utilizing large language models, which have been recently demonstrated to exhibit remarkable performance. Through our experiments, we demonstrate that the proposed method is not just cost-efficient but also applicable for low-resource language annotation. Additionally, we constructed an image captioning dataset using our approach and are committed to open this dataset for future study. We have opened our source code for further study and reproducibility.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RIPPLE&#30340;&#24555;&#36895;&#20248;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28508;&#24847;&#35782;&#21033;&#29992;&#21644;&#27169;&#20223;&#21160;&#20316;&#30340;&#24605;&#24819;&#65292;&#35299;&#20915;&#20102;&#36890;&#36807;&#36234;&#29425;&#25552;&#31034;&#32469;&#36807;&#23433;&#20840;&#25514;&#26045;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05467</link><description>&lt;p&gt;
&#24555;&#36895;&#20248;&#21270;LLM&#36234;&#29425;&#26041;&#27861;&#65306;&#36890;&#36807;&#28508;&#24847;&#35782;&#21033;&#29992;&#21644;&#27169;&#20223;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation and Echopraxia
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RIPPLE&#30340;&#24555;&#36895;&#20248;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28508;&#24847;&#35782;&#21033;&#29992;&#21644;&#27169;&#20223;&#21160;&#20316;&#30340;&#24605;&#24819;&#65292;&#35299;&#20915;&#20102;&#36890;&#36807;&#36234;&#29425;&#25552;&#31034;&#32469;&#36807;&#23433;&#20840;&#25514;&#26045;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#20010;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#36890;&#36807;&#20854;&#38750;&#20961;&#30340;&#25512;&#29702;&#21644;&#29702;&#35299;&#33021;&#21147;&#25913;&#21464;&#20102;&#20154;&#31867;&#29983;&#27963;&#12290;&#38543;&#30528;&#23427;&#20204;&#22312;&#25935;&#24863;&#20219;&#21153;&#20013;&#30340;&#22686;&#21152;&#20351;&#29992;&#65292;&#23433;&#20840;&#38382;&#39064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20154;&#20204;&#24050;&#32463;&#20570;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#20197;&#30830;&#20445;LLMs&#19982;&#20154;&#31867;&#36947;&#24503;&#21407;&#21017;&#30456;&#19968;&#33268;&#65292;&#20197;&#30830;&#20445;&#20854;&#23433;&#20840;&#37096;&#32626;&#12290;&#23613;&#31649;&#26377;&#28508;&#21147;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#40784;&#30340;LLMs&#23481;&#26131;&#21463;&#21040;&#19987;&#38376;&#30340;&#36234;&#29425;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25552;&#31034;&#32469;&#36807;&#23433;&#20840;&#25514;&#26045;&#65292;&#24341;&#21457;&#26292;&#21147;&#21644;&#26377;&#23475;&#20869;&#23481;&#12290;&#24403;&#20195;LLMs&#30340;&#31163;&#25955;&#26412;&#36136;&#21644;&#24222;&#22823;&#35268;&#27169;&#20351;&#24471;&#33258;&#21160;&#29983;&#25104;&#22810;&#26679;&#21270;&#12289;&#39640;&#25928;&#21644;&#24378;&#25928;&#30340;&#36234;&#29425;&#25552;&#31034;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#36825;&#26159;&#19968;&#20010;&#25345;&#32493;&#30340;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RIPPLE&#65288;&#22522;&#20110;&#28508;&#24847;&#35782;&#21033;&#29992;&#21644;&#27169;&#20223;&#21160;&#20316;&#30340;&#24555;&#36895;&#20248;&#21270;&#65289;&#30340;&#26032;&#22411;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#20102;&#20004;&#20010;&#24515;&#29702;&#23398;&#27010;&#24565;&#30340;&#21551;&#21457;&#65306;&#28508;&#24847;&#35782;&#21644;&#27169;&#20223;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have become prevalent across diverse sectors, transforming human life with their extraordinary reasoning and comprehension abilities. As they find increased use in sensitive tasks, safety concerns have gained widespread attention. Extensive efforts have been dedicated to aligning LLMs with human moral principles to ensure their safe deployment. Despite their potential, recent research indicates aligned LLMs are prone to specialized jailbreaking prompts that bypass safety measures to elicit violent and harmful content. The intrinsic discrete nature and substantial scale of contemporary LLMs pose significant challenges in automatically generating diverse, efficient, and potent jailbreaking prompts, representing a continuous obstacle. In this paper, we introduce RIPPLE (Rapid Optimization via Subconscious Exploitation and Echopraxia), a novel optimization-based method inspired by two psychological concepts: subconsciousness and echopraxia, which describe the p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#22768;&#23398;&#20449;&#24687;&#34701;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#21160;&#24577;&#34701;&#21512;&#65288;UADF&#65289;&#30340;&#21518;&#26399;&#34701;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#20811;&#26381;&#29983;&#25104;&#24615;&#38169;&#35823;&#32416;&#27491;&#65288;GER&#65289;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20219;&#21153;&#12290;&#36890;&#36807;&#22312;&#33258;&#22238;&#24402;&#35299;&#30721;&#36807;&#31243;&#20013;&#23454;&#26045;UADF&#26041;&#27861;&#65292;&#22312;LLM&#20915;&#31574;&#30340;&#26631;&#35760;&#32423;&#20998;&#26512;&#21644;&#26657;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#21160;&#24577;&#22320;&#34701;&#21512;&#22768;&#23398;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;ASR&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05457</link><description>&lt;p&gt;
&#27704;&#36828;&#19981;&#23244;&#26202;&#65306;&#23558;&#22768;&#23398;&#20449;&#24687;&#34701;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
It's Never Too Late: Fusing Acoustic Information into Large Language Models for Automatic Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#22768;&#23398;&#20449;&#24687;&#34701;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#21160;&#24577;&#34701;&#21512;&#65288;UADF&#65289;&#30340;&#21518;&#26399;&#34701;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#20811;&#26381;&#29983;&#25104;&#24615;&#38169;&#35823;&#32416;&#27491;&#65288;GER&#65289;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20219;&#21153;&#12290;&#36890;&#36807;&#22312;&#33258;&#22238;&#24402;&#35299;&#30721;&#36807;&#31243;&#20013;&#23454;&#26045;UADF&#26041;&#27861;&#65292;&#22312;LLM&#20915;&#31574;&#30340;&#26631;&#35760;&#32423;&#20998;&#26512;&#21644;&#26657;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#21160;&#24577;&#22320;&#34701;&#21512;&#22768;&#23398;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;ASR&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25104;&#21151;&#22320;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20197;&#25104;&#21151;&#29992;&#20110;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#36755;&#20986;&#20043;&#19978;&#36827;&#34892;&#29983;&#25104;&#24615;&#38169;&#35823;&#32416;&#27491;&#65288;GER&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LLM&#34987;&#29992;&#20110;&#23545;&#30001;ASR&#31995;&#32479;&#29983;&#25104;&#30340;N&#26368;&#20339;&#20551;&#35774;&#21015;&#34920;&#36827;&#34892;&#30452;&#25509;&#26144;&#23556;&#65292;&#29983;&#25104;&#39044;&#27979;&#30340;&#36755;&#20986;&#36716;&#24405;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#26377;&#25928;&#24615;&#65292;GER&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#65292;&#22240;&#20026;LLM&#22312;&#35757;&#32451;&#26102;&#27809;&#26377;&#32771;&#34385;&#21040;&#35821;&#38899;&#20449;&#21495;&#20013;&#21487;&#29992;&#30340;&#22768;&#23398;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#19968;&#31181;&#21517;&#20026;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#21160;&#24577;&#34701;&#21512;&#65288;UADF&#65289;&#30340;&#26032;&#22411;&#21518;&#26399;&#34701;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#27880;&#20837;&#22768;&#23398;&#20449;&#24687;&#20197;&#29983;&#25104;&#39044;&#27979;&#36716;&#24405;&#26469;&#20811;&#26381;&#36825;&#31181;&#38480;&#21046;&#12290;UADF&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#65292;&#23454;&#29616;&#22312;&#33258;&#22238;&#24402;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#24182;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#65288;i&#65289;&#23427;&#39318;&#20808;&#20998;&#26512;&#21644;&#26657;&#20934;&#26631;&#35760;&#32423;LLM&#20915;&#31574;&#65292;&#28982;&#21518;&#65288;ii&#65289;&#23427;&#21160;&#24577;&#22320;&#21560;&#25910;&#22768;&#23398;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have successfully shown that large language models (LLMs) can be successfully used for generative error correction (GER) on top of the automatic speech recognition (ASR) output. Specifically, an LLM is utilized to carry out a direct mapping from the N-best hypotheses list generated by an ASR system to the predicted output transcription. However, despite its effectiveness, GER introduces extra data uncertainty since the LLM is trained without taking into account acoustic information available in the speech signal. In this work, we aim to overcome such a limitation by infusing acoustic information before generating the predicted transcription through a novel late fusion solution termed Uncertainty-Aware Dynamic Fusion (UADF). UADF is a multimodal fusion approach implemented into an auto-regressive decoding process and works in two stages: (i) It first analyzes and calibrates the token-level LLM decision, and (ii) it then dynamically assimilates the information from the aco
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;(LMs)&#29983;&#25104;&#35821;&#35328;&#26448;&#26009;&#30340;&#21512;&#29702;&#24615;&#21028;&#26029;&#65292;&#24182;&#21457;&#29616;GPT-4&#30340;&#21512;&#29702;&#24615;&#21028;&#26029;&#19982;&#20154;&#31867;&#21028;&#26029;&#39640;&#24230;&#30456;&#20851;&#12290;&#36825;&#24847;&#21619;&#30528;LMs&#21487;&#20197;&#26367;&#20195;&#20154;&#31867;&#36827;&#34892;&#39044;&#27979;&#35797;&#65292;&#20174;&#32780;&#22312;&#24515;&#29702;&#35821;&#35328;&#23398;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05455</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#24515;&#29702;&#35821;&#35328;&#23398;&#21512;&#29702;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Psycholinguistic Plausibility Pretesting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;(LMs)&#29983;&#25104;&#35821;&#35328;&#26448;&#26009;&#30340;&#21512;&#29702;&#24615;&#21028;&#26029;&#65292;&#24182;&#21457;&#29616;GPT-4&#30340;&#21512;&#29702;&#24615;&#21028;&#26029;&#19982;&#20154;&#31867;&#21028;&#26029;&#39640;&#24230;&#30456;&#20851;&#12290;&#36825;&#24847;&#21619;&#30528;LMs&#21487;&#20197;&#26367;&#20195;&#20154;&#31867;&#36827;&#34892;&#39044;&#27979;&#35797;&#65292;&#20174;&#32780;&#22312;&#24515;&#29702;&#35821;&#35328;&#23398;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24515;&#29702;&#35821;&#35328;&#23398;&#20013;&#65292;&#21019;&#24314;&#21463;&#25511;&#26448;&#26009;&#23545;&#20110;&#30830;&#20445;&#30740;&#31350;&#32467;&#26524;&#20165;&#24402;&#22240;&#20110;&#39044;&#26399;&#25805;&#20316;&#32780;&#19981;&#21463;&#22806;&#37096;&#22240;&#32032;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#24515;&#29702;&#35821;&#35328;&#23398;&#23478;&#36890;&#24120;&#20250;&#23545;&#35821;&#35328;&#26448;&#26009;&#36827;&#34892;&#39044;&#27979;&#35797;&#65292;&#20854;&#20013;&#19968;&#31181;&#24120;&#35265;&#30340;&#39044;&#27979;&#35797;&#26159;&#21521;&#20154;&#31867;&#35780;&#20272;&#32773;&#24449;&#27714;&#20851;&#20110;&#29305;&#23450;&#21477;&#23376;&#30340;&#21512;&#29702;&#24615;&#21028;&#26029;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;(LMs)&#29983;&#25104;&#36825;&#20123;&#21512;&#29702;&#24615;&#21028;&#26029;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#31181;&#19981;&#21516;&#35821;&#35328;&#32467;&#26500;&#30340;LMs&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#30340;&#21512;&#29702;&#24615;&#21028;&#26029;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#26159;&#21542;&#30456;&#20851;&#12290;&#25105;&#20204;&#21457;&#29616;GPT-4&#30340;&#21512;&#29702;&#24615;&#21028;&#26029;&#19982;&#25105;&#20204;&#30740;&#31350;&#30340;&#19981;&#21516;&#32467;&#26500;&#30340;&#20154;&#31867;&#21028;&#26029;&#39640;&#24230;&#30456;&#20851;&#65292;&#32780;&#20854;&#20182;LMs&#22312;&#24120;&#29992;&#30340;&#21477;&#27861;&#32467;&#26500;&#19978;&#19982;&#20154;&#31867;&#30340;&#21028;&#26029;&#20855;&#26377;&#33391;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#36825;&#31181;&#30456;&#20851;&#24615;&#26159;&#21542;&#24847;&#21619;&#30528;&#21487;&#20197;&#29992;LMs&#20195;&#26367;&#20154;&#31867;&#36827;&#34892;&#39044;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#31895;&#31890;&#24230;&#30340;&#21512;&#29702;&#24615;&#21028;&#26029;&#36827;&#34892;&#26102;&#65292;LMs&#30340;&#32467;&#26524;&#19982;&#20154;&#31867;&#35780;&#20272;&#32773;&#30340;&#32467;&#26524;&#39640;&#24230;&#30456;&#20851;&#65292;&#24182;&#19988;LMs&#30340;&#21512;&#29702;&#24615;&#21028;&#26029;&#21487;&#20197;&#20195;&#26367;&#20154;&#31867;&#36827;&#34892;&#21512;&#29702;&#24615;&#39044;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
In psycholinguistics, the creation of controlled materials is crucial to ensure that research outcomes are solely attributed to the intended manipulations and not influenced by extraneous factors. To achieve this, psycholinguists typically pretest linguistic materials, where a common pretest is to solicit plausibility judgments from human evaluators on specific sentences. In this work, we investigate whether Language Models (LMs) can be used to generate these plausibility judgements. We investigate a wide range of LMs across multiple linguistic structures and evaluate whether their plausibility judgements correlate with human judgements. We find that GPT-4 plausibility judgements highly correlate with human judgements across the structures we examine, whereas other LMs correlate well with humans on commonly used syntactic structures. We then test whether this correlation implies that LMs can be used instead of humans for pretesting. We find that when coarse-grained plausibility judgeme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20449;&#24687;&#20445;&#30041;&#25512;&#21160;&#37327;&#21270;LLMs&#30340;LoRA-Finetuning&#30340;&#26041;&#27861;IR-QLoRA&#65292;&#20351;&#29992;&#32479;&#35745;&#20449;&#24687;&#26657;&#20934;&#21644;&#35843;&#20248;&#20449;&#24687;&#24377;&#24615;&#36830;&#25509;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05445</link><description>&lt;p&gt;
&#20934;&#30830;&#30340;LLMs&#30340;LoRA-Finetuning&#37327;&#21270;&#36890;&#36807;&#20449;&#24687;&#20445;&#30041;
&lt;/p&gt;
&lt;p&gt;
Accurate LoRA-Finetuning Quantization of LLMs via Information Retention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20449;&#24687;&#20445;&#30041;&#25512;&#21160;&#37327;&#21270;LLMs&#30340;LoRA-Finetuning&#30340;&#26041;&#27861;IR-QLoRA&#65292;&#20351;&#29992;&#32479;&#35745;&#20449;&#24687;&#26657;&#20934;&#21644;&#35843;&#20248;&#20449;&#24687;&#24377;&#24615;&#36830;&#25509;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;LLMs&#30340;LoRA-finetuning&#37327;&#21270;&#30740;&#31350;&#24471;&#21040;&#20934;&#30830;&#20294;&#32039;&#20945;&#30340;LLMs&#20197;&#20415;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#37096;&#32626;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#23548;&#33268;&#37327;&#21270;&#30340;LLMs&#20005;&#37325;&#36864;&#21270;&#65292;&#29978;&#33267;&#26080;&#27861;&#20174;LoRA&#30340;&#35843;&#20248;&#20013;&#33719;&#30410;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;IR-QLoRA&#65292;&#36890;&#36807;&#20449;&#24687;&#20445;&#30041;&#25512;&#21160;&#24102;&#26377;LoRA&#30340;&#37327;&#21270;LLMs&#21464;&#24471;&#39640;&#24230;&#20934;&#30830;&#12290;&#25152;&#25552;&#20986;&#30340;IR-QLoRA&#20027;&#35201;&#20381;&#36182;&#20110;&#20004;&#31181;&#20174;&#32479;&#19968;&#20449;&#24687;&#35270;&#35282;&#27966;&#29983;&#30340;&#25216;&#26415;&#65306;&#65288;1&#65289;&#22522;&#20110;&#32479;&#35745;&#30340;&#20449;&#24687;&#26657;&#20934;&#37327;&#21270;&#20801;&#35768;LLMs&#30340;&#37327;&#21270;&#21442;&#25968;&#31934;&#30830;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#65307;&#65288;2&#65289;&#22522;&#20110;&#35843;&#20248;&#30340;&#20449;&#24687;&#24377;&#24615;&#36830;&#25509;&#20351;LoRA&#21033;&#29992;&#20855;&#26377;&#22810;&#26679;&#20449;&#24687;&#30340;&#24377;&#24615;&#34920;&#31034;&#36716;&#25442;&#12290;&#32508;&#21512;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;2-4&#20301;&#23485;&#19979;&#65292;IR-QLoRA&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLaMA&#21644;LLaMA2&#31995;&#21015;&#30340;&#20934;&#30830;&#24615;&#65292;&#20363;&#22914;&#65292;4&#20301;LLaMA-7B&#30456;&#27604;&#20110;...
&lt;/p&gt;
&lt;p&gt;
The LoRA-finetuning quantization of LLMs has been extensively studied to obtain accurate yet compact LLMs for deployment on resource-constrained hardware. However, existing methods cause the quantized LLM to severely degrade and even fail to benefit from the finetuning of LoRA. This paper proposes a novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate through information retention. The proposed IR-QLoRA mainly relies on two technologies derived from the perspective of unified information: (1) statistics-based Information Calibration Quantization allows the quantized parameters of LLM to retain original information accurately; (2) finetuning-based Information Elastic Connection makes LoRA utilizes elastic representation transformation with diverse information. Comprehensive experiments show that IR-QLoRA can significantly improve accuracy across LLaMA and LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4% improvement on MMLU compared with the 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#25913;&#36827;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#23545;&#20219;&#21153;&#30340;&#29702;&#35299;&#21644;&#27807;&#36890;&#33021;&#21147;&#30340;&#25552;&#21319;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.05440</link><description>&lt;p&gt;
&#29992;&#35821;&#35328;&#27169;&#22411;&#25913;&#21892;&#34394;&#25311;&#29615;&#22659;&#20013;&#30340;&#26234;&#33021;&#20307;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Improving Agent Interactions in Virtual Environments with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05440
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#25913;&#36827;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#23545;&#20219;&#21153;&#30340;&#29702;&#35299;&#21644;&#27807;&#36890;&#33021;&#21147;&#30340;&#25552;&#21319;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26377;&#25928;&#30340;&#20154;&#31867;&#36741;&#21161;&#65292;&#22686;&#24378;AI&#31995;&#32479;&#30340;&#27807;&#36890;&#25216;&#24039;&#38656;&#35201;&#31995;&#32479;&#20027;&#21160;&#25506;&#32034;&#29305;&#23450;&#24773;&#20917;&#24182;&#36866;&#24403;&#20132;&#20114;&#12290;&#26412;&#30740;&#31350;&#20197;Minecraft&#25968;&#25454;&#38598;&#20013;&#30340;&#38598;&#20307;&#24314;&#31435;&#20219;&#21153;&#20026;&#37325;&#28857;&#65292;&#21033;&#29992;&#35821;&#35328;&#24314;&#27169;&#26469;&#36890;&#36807;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22686;&#24378;&#20219;&#21153;&#29702;&#35299;&#33021;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#27880;&#37325;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#29702;&#35299;&#20219;&#21153;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#35299;&#37322;&#21644;&#21709;&#24212;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20026;&#23558;&#26469;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#25351;&#26126;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enhancing AI systems with efficient communication skills for effective human assistance necessitates proactive initiatives from the system side to discern specific circumstances and interact aptly. This research focuses on a collective building assignment in the Minecraft dataset, employing language modeling to enhance task understanding through state-of-the-art methods. These models focus on grounding multi-modal understanding and task-oriented dialogue comprehension tasks, providing insights into their interpretative and responsive capabilities. Our experimental results showcase a substantial improvement over existing methods, indicating a promising direction for future research in this domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#21270;&#21465;&#20107;&#25552;&#31034;&#65292;&#39564;&#35777;&#20102;GPT-4&#29983;&#25104;&#30340;&#21465;&#36848;&#22312;&#20256;&#36798;&#29983;&#27963;&#20107;&#20214;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#21465;&#36848;&#33021;&#22815;&#36275;&#22815;&#20256;&#36798;&#25552;&#31034;&#30340;&#24847;&#22270;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#26377;&#25928;&#21644;&#26080;&#25928;&#30340;&#21465;&#36848;&#12290;</title><link>https://arxiv.org/abs/2402.05435</link><description>&lt;p&gt;
GPT-4&#20351;&#29992;&#32467;&#26500;&#21270;&#21465;&#20107;&#25552;&#31034;&#29983;&#25104;&#29983;&#27963;&#20107;&#20214;&#30340;&#21465;&#36848;&#65306;&#19968;&#39033;&#39564;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GPT-4 Generated Narratives of Life Events using a Structured Narrative Prompt: A Validation Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#21270;&#21465;&#20107;&#25552;&#31034;&#65292;&#39564;&#35777;&#20102;GPT-4&#29983;&#25104;&#30340;&#21465;&#36848;&#22312;&#20256;&#36798;&#29983;&#27963;&#20107;&#20214;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#21465;&#36848;&#33021;&#22815;&#36275;&#22815;&#20256;&#36798;&#25552;&#31034;&#30340;&#24847;&#22270;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#26377;&#25928;&#21644;&#26080;&#25928;&#30340;&#21465;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#21508;&#31181;&#21465;&#36848;&#26041;&#38754;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#20419;&#36827;&#20102;&#23545;&#20854;&#22312;&#21465;&#36848;&#24418;&#24335;&#20013;&#20256;&#36798;&#29983;&#27963;&#20107;&#20214;&#25928;&#26524;&#30340;&#31995;&#32479;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#38646;-shot&#32467;&#26500;&#21270;&#21465;&#20107;&#25552;&#31034;&#65292;&#20351;&#29992;OpenAI&#30340;GPT-4&#29983;&#25104;&#20102;24,000&#20010;&#21465;&#36848;&#12290;&#20174;&#36825;&#20010;&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#25163;&#21160;&#20998;&#31867;&#20102;2,880&#20010;&#21465;&#36848;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#22312;&#20256;&#36798;&#20986;&#29983;&#12289;&#27515;&#20129;&#12289;&#25307;&#32856;&#21644;&#35299;&#38599;&#20107;&#20214;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;87.43%&#30340;&#21465;&#36848;&#36275;&#22815;&#20256;&#36798;&#32467;&#26500;&#21270;&#25552;&#31034;&#30340;&#24847;&#22270;&#12290;&#20026;&#20102;&#33258;&#21160;&#35782;&#21035;&#26377;&#25928;&#21644;&#26080;&#25928;&#30340;&#21465;&#36848;&#65292;&#25105;&#20204;&#23545;&#20998;&#31867;&#25968;&#25454;&#38598;&#35757;&#32451;&#21644;&#39564;&#35777;&#20102;&#20061;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#23545;&#21097;&#20313;21,120&#20010;&#21465;&#36848;&#30340;&#20998;&#31867;&#39044;&#27979;&#20998;&#26512;&#12290;&#25152;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#23558;&#26377;&#25928;&#30340;&#21465;&#36848;&#20998;&#31867;&#20026;&#26377;&#25928;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#21516;&#26102;&#23558;&#26080;&#25928;&#30340;&#21465;&#36848;&#20998;&#31867;&#20026;&#26080;&#25928;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#19981;&#20165;&#25512;&#36827;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#36824;&#25552;&#20379;&#20102;&#33258;&#21160;&#35782;&#21035;&#26377;&#25928;&#21465;&#36848;&#30340;&#26377;&#30410;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) play a pivotal role in generating vast arrays of narratives, facilitating a systematic exploration of their effectiveness for communicating life events in narrative form. In this study, we employ a zero-shot structured narrative prompt to generate 24,000 narratives using OpenAI's GPT-4. From this dataset, we manually classify 2,880 narratives and evaluate their validity in conveying birth, death, hiring, and firing events. Remarkably, 87.43% of the narratives sufficiently convey the intention of the structured prompt. To automate the identification of valid and invalid narratives, we train and validate nine Machine Learning models on the classified datasets. Leveraging these models, we extend our analysis to predict the classifications of the remaining 21,120 narratives. All the ML models excelled at classifying valid narratives as valid, but experienced challenges at simultaneously classifying invalid narratives as invalid. Our findings not only advance th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#21069;&#21521;&#20256;&#36882;&#30340;LLM&#32467;&#26500;&#21270;&#20462;&#21098;&#26041;&#27861;&#65292;&#36890;&#36807;Bonsai&#29983;&#25104;&#30340;&#20462;&#21098;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26799;&#24230;-based&#32467;&#26500;&#21270;&#20462;&#21098;&#26041;&#27861;&#65292;&#24182;&#19988;&#36895;&#24230;&#26159;&#21322;&#32467;&#26500;&#21270;&#20462;&#21098;&#27169;&#22411;&#30340;&#20004;&#20493;&#12290;</title><link>https://arxiv.org/abs/2402.05406</link><description>&lt;p&gt;
&#29616;&#22312;&#25152;&#26377;&#20154;&#37117;&#20462;&#21098;&#65306;&#20165;&#20351;&#29992;&#21069;&#21521;&#20256;&#36882;&#30340;LLM&#32467;&#26500;&#21270;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#21069;&#21521;&#20256;&#36882;&#30340;LLM&#32467;&#26500;&#21270;&#20462;&#21098;&#26041;&#27861;&#65292;&#36890;&#36807;Bonsai&#29983;&#25104;&#30340;&#20462;&#21098;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26799;&#24230;-based&#32467;&#26500;&#21270;&#20462;&#21098;&#26041;&#27861;&#65292;&#24182;&#19988;&#36895;&#24230;&#26159;&#21322;&#32467;&#26500;&#21270;&#20462;&#21098;&#27169;&#22411;&#30340;&#20004;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#38750;&#19987;&#19994;&#20174;&#19994;&#32773;&#21644;&#26368;&#23500;&#26377;&#36164;&#28304;&#30340;&#26426;&#26500;&#20043;&#38388;&#30340;&#30828;&#20214;&#24046;&#36317;&#65292;&#23610;&#23544;&#19981;&#26029;&#22686;&#38271;&#30340;LLM&#21464;&#24471;&#36234;&#26469;&#36234;&#38590;&#20197;&#20351;&#29992;&#12290;&#34429;&#28982;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#21387;&#32553;LLM&#65292;&#20197;&#20351;&#20854;&#36164;&#28304;&#28040;&#32791;&#21487;&#31649;&#29702;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#26412;&#36523;&#24448;&#24448;&#32791;&#36153;&#36164;&#28304;&#65292;&#20351;&#20854;&#30446;&#26631;&#29992;&#25143;&#32676;&#26080;&#27861;&#25509;&#35302;&#21040;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20165;&#20351;&#29992;&#21069;&#21521;&#20256;&#36882;&#30340;LLM&#32467;&#26500;&#21270;&#20462;&#21098;&#38382;&#39064;&#12290;&#25105;&#20204;&#24076;&#26395;&#35753;&#20174;&#19994;&#32773;&#33021;&#22815;&#20462;&#21098;&#27169;&#22411;&#65292;&#20351;&#20854;&#35268;&#27169;&#22823;&#21040;&#30828;&#20214;&#20165;&#26377;&#36275;&#22815;&#30340;&#20869;&#23384;&#26469;&#36816;&#34892;&#25512;&#29702;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;Bonsai&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#26799;&#24230;&#12289;&#25200;&#21160;&#20462;&#21098;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#23567;&#12289;&#24555;&#21644;&#20934;&#30830;&#30340;&#20462;&#21098;&#27169;&#22411;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;Bonsai&#29983;&#25104;&#30340;&#20462;&#21098;&#27169;&#22411;&#65288;i&#65289;&#20248;&#20110;&#26356;&#26114;&#36149;&#30340;&#26799;&#24230;-based&#32467;&#26500;&#21270;&#20462;&#21098;&#26041;&#27861;&#29983;&#25104;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#65288;ii&#65289;&#19982;&#21322;&#32467;&#26500;&#21270;&#20462;&#21098;&#27169;&#22411;&#30456;&#27604;&#65292;&#36895;&#24230;&#24555;&#19968;&#20493;&#19988;&#20934;&#30830;&#24615;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the generational gap in available hardware between lay practitioners and the most endowed institutions, LLMs are becoming increasingly inaccessible as they grow in size. Whilst many approaches have been proposed to compress LLMs to make their resource consumption manageable, these methods themselves tend to be resource intensive, putting them out of the reach of the very user groups they target. In this work, we explore the problem of structured pruning of LLMs using only forward passes. We seek to empower practitioners to prune models so large that their available hardware has just enough memory to run inference. We develop Bonsai, a gradient-free, perturbative pruning method capable of delivering small, fast, and accurate pruned models.   We observe that Bonsai outputs pruned models that (i) outperform those generated by more expensive gradient-based structured pruning methods, and (ii) are twice as fast (with comparable accuracy) as those generated by semi-structured pruning m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#27861;LEAP&#65292;&#36890;&#36807;&#35753;&#27169;&#22411;&#20174;&#23569;&#37327;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#20013;&#29359;&#38169;&#35823;&#65292;&#28982;&#21518;&#21453;&#24605;&#24182;&#23398;&#20064;&#20934;&#21017;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.05403</link><description>&lt;p&gt;
&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#20934;&#21017;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In-Context Principle Learning from Mistakes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#27861;LEAP&#65292;&#36890;&#36807;&#35753;&#27169;&#22411;&#20174;&#23569;&#37327;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#20013;&#29359;&#38169;&#35823;&#65292;&#28982;&#21518;&#21453;&#24605;&#24182;&#23398;&#20064;&#20934;&#21017;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65292;&#20063;&#31216;&#20026;&#23569;&#26679;&#26412;&#25552;&#31034;&#65289;&#24050;&#25104;&#20026;&#23558;LLMs&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#30340;&#26631;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#23569;&#37327;&#30340;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#20013;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#22522;&#20110;ICL&#30340;&#26041;&#27861;&#21482;&#20174;&#27491;&#30830;&#30340;&#36755;&#20837;-&#36755;&#20986;&#23545;&#20013;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#36825;&#19968;&#33539;&#20363;&#65292;&#36890;&#36807;&#20174;&#23569;&#32473;&#23450;&#30340;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#20013;&#23398;&#20064;&#26356;&#22810;&#20869;&#23481;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23398;&#20064;&#20934;&#21017;&#65288;LEAP&#65289;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#26377;&#24847;&#35825;&#20351;&#27169;&#22411;&#22312;&#36825;&#20123;&#23569;&#37327;&#31034;&#20363;&#20013;&#29359;&#38169;&#35823;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#21453;&#24605;&#36825;&#20123;&#38169;&#35823;&#65292;&#24182;&#20174;&#20013;&#23398;&#20064;&#26174;&#24335;&#30340;&#20219;&#21153;&#29305;&#23450;&#8220;&#20934;&#21017;&#8221;&#65292;&#36825;&#20123;&#20934;&#21017;&#26377;&#21161;&#20110;&#35299;&#20915;&#31867;&#20284;&#30340;&#38382;&#39064;&#24182;&#36991;&#20813;&#24120;&#35265;&#30340;&#38169;&#35823;&#65307;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21407;&#22987;&#30340;&#23569;&#26679;&#26412;&#31034;&#20363;&#21644;&#36825;&#20123;&#23398;&#21040;&#30340;&#36890;&#29992;&#20934;&#21017;&#26469;&#25552;&#31034;&#27169;&#22411;&#22238;&#31572;&#26410;&#35265;&#36807;&#30340;&#27979;&#35797;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#65288;Hotpot QA&#65289;&#12289;&#25991;&#26412;&#38382;&#39064;&#22238;&#31572;&#65288;DROP&#65289;&#12289;Big-Bench&#22256;&#38590;&#25512;&#29702;&#21644;&#25968;&#23398;&#38382;&#39064;&#65288;GSM8K&#21644;MATH&#65289;&#22312;&#20869;&#30340;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;LEAP&#65307;&#22312;&#25152;&#26377;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;LEAP&#37117;&#26377;&#25152;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL, also known as few-shot prompting) has been the standard method of adapting LLMs to downstream tasks, by learning from a few input-output examples. Nonetheless, all ICL-based approaches only learn from correct input-output pairs. In this paper, we revisit this paradigm, by learning more from the few given input-output examples. We introduce Learning Principles (LEAP): First, we intentionally induce the model to make mistakes on these few examples; then we reflect on these mistakes, and learn explicit task-specific "principles" from them, which help solve similar problems and avoid common mistakes; finally, we prompt the model to answer unseen test questions using the original few-shot examples and these learned general principles. We evaluate LEAP on a wide range of benchmarks, including multi-hop question answering (Hotpot QA), textual QA (DROP), Big-Bench Hard reasoning, and math problems (GSM8K and MATH); in all these benchmarks, LEAP improves the strongest 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#24341;&#23548;&#30340;&#38646;&#26679;&#26412;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#22810;&#26679;&#30340;&#25512;&#29702;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#37325;&#20889;&#25805;&#20316;&#22686;&#24378;&#27169;&#22411;&#23545;&#38382;&#39064;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21313;&#20010;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.05376</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#24341;&#23548;&#30340;&#38646;&#26679;&#26412;&#24605;&#32500;&#38142;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Chain-of-Thought Reasoning Guided by Evolutionary Algorithms in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#24341;&#23548;&#30340;&#38646;&#26679;&#26412;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#22810;&#26679;&#30340;&#25512;&#29702;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#37325;&#20889;&#25805;&#20316;&#22686;&#24378;&#27169;&#22411;&#23545;&#38382;&#39064;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21313;&#20010;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#24212;&#29992;&#38646;&#26679;&#26412;&#24605;&#32500;&#38142;&#25512;&#29702;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39044;&#35757;&#32451;&#38454;&#27573;&#20013;&#21477;&#23376;&#21069;&#32512;&#30340;&#28436;&#21270;&#24615;&#36136;&#65292;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;&#24605;&#32500;&#38142;&#25512;&#29702;&#26041;&#27861;&#26080;&#27861;&#22312;&#25152;&#26377;&#20219;&#21153;&#23454;&#20363;&#19978;&#37117;&#37319;&#29992;&#30456;&#21516;&#30340;&#25512;&#29702;&#26041;&#24335;&#65292;&#21487;&#33021;&#19981;&#22815;&#20248;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38646;&#26679;&#26412;&#25512;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#36827;&#21270;&#31639;&#27861;&#21160;&#24577;&#29983;&#25104;LLM&#30340;&#22810;&#26679;&#25512;&#29702;&#26041;&#24335;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#21021;&#22987;&#21270;&#20004;&#20010;&#24605;&#32500;&#38142;&#25512;&#29702;&#26041;&#24335;&#65292;&#22522;&#20110;LLM&#36827;&#34892;&#36827;&#21270;&#25805;&#20316;&#20197;&#29983;&#25104;&#22810;&#26679;&#38598;&#21512;&#65292;&#24182;&#21033;&#29992;LLM&#36873;&#25321;&#36866;&#21512;&#32473;&#23450;&#38382;&#39064;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#36873;&#23450;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;&#26041;&#24335;&#24341;&#23548;&#30340;&#37325;&#20889;&#25805;&#20316;&#22686;&#24378;LLM&#23545;&#38382;&#39064;&#30340;&#29702;&#35299;&#12290;&#36890;&#36807;&#23545;&#21313;&#20010;&#25512;&#29702;&#25968;&#25454;&#38598;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable performance across diverse tasks and exhibited impressive reasoning abilities by applying zero-shot Chain-of-Thought (CoT) prompting. However, due to the evolving nature of sentence prefixes during the pre-training phase, existing zero-shot CoT prompting methods that employ identical CoT prompting across all task instances may not be optimal. In this paper, we introduce a novel zero-shot prompting method that leverages evolutionary algorithms to generate diverse promptings for LLMs dynamically. Our approach involves initializing two CoT promptings, performing evolutionary operations based on LLMs to create a varied set, and utilizing the LLMs to select a suitable CoT prompting for a given problem. Additionally, a rewriting operation, guided by the selected CoT prompting, enhances the understanding of the LLMs about the problem. Extensive experiments conducted across ten reasoning datasets demonstrate the superior performance of 
&lt;/p&gt;</description></item><item><title>CIC&#26159;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#38382;&#31572;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#33021;&#25551;&#36848;&#22270;&#20687;&#20013;&#25991;&#21270;&#20803;&#32032;&#30340;&#35814;&#32454;&#23383;&#24149;&#12290;</title><link>https://arxiv.org/abs/2402.05374</link><description>&lt;p&gt;
CIC&#65306;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CIC: A framework for Culturally-aware Image Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05374
&lt;/p&gt;
&lt;p&gt;
CIC&#26159;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#38382;&#31572;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#33021;&#25551;&#36848;&#22270;&#20687;&#20013;&#25991;&#21270;&#20803;&#32032;&#30340;&#35814;&#32454;&#23383;&#24149;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;VLPs&#65289;&#22914;BLIP&#20174;&#22270;&#20687;&#29983;&#25104;&#25551;&#36848;&#24615;&#21477;&#23376;&#65292;&#36825;&#31181;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#32570;&#20047;&#23545;&#22270;&#20687;&#20013;&#25152;&#25551;&#32472;&#30340;&#25991;&#21270;&#20803;&#32032;&#65288;&#20363;&#22914;&#20122;&#27954;&#25991;&#21270;&#32676;&#20307;&#30340;&#20256;&#32479;&#26381;&#35013;&#65289;&#29983;&#25104;&#35814;&#32454;&#25551;&#36848;&#24615;&#23383;&#24149;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;\textbf{&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#65288;CIC&#65289;}&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20174;&#20195;&#34920;&#19981;&#21516;&#25991;&#21270;&#30340;&#22270;&#20687;&#20013;&#29983;&#25104;&#23383;&#24149;&#24182;&#25551;&#36848;&#25991;&#21270;&#20803;&#32032;&#12290;&#21463;&#21040;&#23558;&#35270;&#35273;&#27169;&#24577;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#36827;&#34892;&#32452;&#21512;&#30340;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#65288;1&#65289;&#26681;&#25454;&#22270;&#20687;&#20013;&#30340;&#25991;&#21270;&#31867;&#21035;&#29983;&#25104;&#38382;&#39064;&#65292;&#65288;2&#65289;&#21033;&#29992;&#29983;&#25104;&#30340;&#38382;&#39064;&#20174;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20013;&#25552;&#21462;&#25991;&#21270;&#35270;&#35273;&#20803;&#32032;&#65292;&#65288;3&#65289;&#20351;&#29992;&#24102;&#26377;&#25552;&#31034;&#30340;LLMs&#29983;&#25104;&#25991;&#21270;&#24863;&#30693;&#23383;&#24149;&#12290;&#25105;&#20204;&#22312;4&#20010;&#19981;&#21516;&#22823;&#23398;&#30340;45&#21517;&#21442;&#19982;&#32773;&#19978;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image Captioning generates descriptive sentences from images using Vision-Language Pre-trained models (VLPs) such as BLIP, which has improved greatly. However, current methods lack the generation of detailed descriptive captions for the cultural elements depicted in the images, such as the traditional clothing worn by people from Asian cultural groups. In this paper, we propose a new framework, \textbf{Culturally-aware Image Captioning (CIC)}, that generates captions and describes cultural elements extracted from cultural visual elements in images representing cultures. Inspired by methods combining visual modality and Large Language Models (LLMs) through appropriate prompts, our framework (1) generates questions based on cultural categories from images, (2) extracts cultural visual elements from Visual Question Answering (VQA) using generated questions, and (3) generates culturally-aware captions using LLMs with the prompts. Our human evaluation conducted on 45 participants from 4 dif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#36890;&#29992;LM&#23545;&#40784;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#26126;&#30830;&#27880;&#37322;&#30340;&#22870;&#21169;&#25968;&#25454;&#65292;&#24182;&#19988;&#25193;&#23637;&#20102;&#24403;&#21069;&#30340;&#23545;&#40784;&#29702;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.05369</link><description>&lt;p&gt;
&#20197;&#26174;&#24335;&#22870;&#21169;&#30340;&#22122;&#22768;&#23545;&#27604;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Noise Contrastive Alignment of Language Models with Explicit Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#36890;&#29992;LM&#23545;&#40784;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#26126;&#30830;&#27880;&#37322;&#30340;&#22870;&#21169;&#25968;&#25454;&#65292;&#24182;&#19988;&#25193;&#23637;&#20102;&#24403;&#21069;&#30340;&#23545;&#40784;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#24847;&#22270;&#36890;&#24120;&#34987;&#24418;&#24335;&#21270;&#20026;&#38656;&#35201;&#22312;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#26102;&#26368;&#22823;&#21270;&#30340;&#35780;&#20272;&#22870;&#21169;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#22914;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#65288;DPO&#65289;&#65292;&#20027;&#35201;&#36866;&#29992;&#20110;&#38544;&#21547;&#23450;&#20041;&#32780;&#38750;&#26126;&#30830;&#32473;&#23450;&#22870;&#21169;&#30340;&#20004;&#20004;&#20559;&#22909;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;LM&#23545;&#40784;&#26694;&#26550;&#65292;&#21033;&#29992;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#26469;&#35299;&#20915;&#26126;&#30830;&#27880;&#37322;&#26377;&#26631;&#37327;&#35780;&#20272;&#30340;&#22870;&#21169;&#25968;&#25454;&#22788;&#29702;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#24182;&#34892;&#31639;&#27861;&#65292;NCA&#21644;InfoNCA&#65292;&#20004;&#32773;&#37117;&#33021;&#20174;&#22870;&#21169;&#25968;&#25454;&#21644;&#20559;&#22909;&#25968;&#25454;&#20013;&#30452;&#25509;&#25552;&#21462;LM&#31574;&#30053;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DPO&#25439;&#22833;&#26159;&#25105;&#20204;&#25552;&#20986;&#30340;InfoNCA&#30446;&#26631;&#22312;&#20004;&#20004;&#20559;&#22909;&#35774;&#32622;&#19979;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#20174;&#32780;&#38598;&#25104;&#21644;&#25193;&#23637;&#20102;&#24403;&#21069;&#30340;&#23545;&#40784;&#29702;&#35770;&#12290;&#36890;&#36807;&#23545;&#27604;NCA&#21644;InfoNCA&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;InfoNCA&#21644;DPO&#22914;&#20309;&#22312;&#19981;&#21516;&#21709;&#24212;&#23545;&#20110;&#21333;&#20010;&#25351;&#20196;&#30340;&#30456;&#23545;&#21487;&#33021;&#24615;&#19978;&#36827;&#34892;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
User intentions are typically formalized as evaluation rewards to be maximized when fine-tuning language models (LMs). Existing alignment methods, such as Direct Preference Optimization (DPO), are mainly tailored for pairwise preference data where rewards are implicitly defined rather than explicitly given. In this paper, we introduce a general framework for LM alignment, leveraging Noise Contrastive Estimation (NCE) to bridge the gap in handling reward datasets explicitly annotated with scalar evaluations. Our framework comprises two parallel algorithms, NCA and InfoNCA, both enabling the direct extraction of an LM policy from reward data as well as preference data. Notably, we show that the DPO loss is a special case of our proposed InfoNCA objective under pairwise preference settings, thereby integrating and extending current alignment theories. By contrasting NCA and InfoNCA, we show that InfoNCA and DPO adjust relative likelihood across different responses to a single instruction,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05359</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#27835;&#31243;&#24207;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#38382;&#39064;&#27714;&#35299;&#36827;&#34892;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#24212;&#29992;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#24403;&#30340;&#25552;&#31034;&#35774;&#35745;&#65292;&#22914;&#24605;&#32500;&#38142;&#65292;&#21487;&#20197;&#37322;&#25918;LLM&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22788;&#29702;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#20219;&#21153;&#65288;&#22914;&#31639;&#26415;&#35745;&#31639;&#21644;&#25991;&#31456;&#32423;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#65289;&#65292;&#29616;&#26377;&#30340;&#25552;&#31034;&#31574;&#30053;&#35201;&#20040;&#34920;&#29616;&#20986;&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#65292;&#35201;&#20040;&#30001;&#24187;&#35273;&#24341;&#21457;&#20013;&#38388;&#38169;&#35823;&#12290;&#20026;&#20102;&#20351;LLM&#23545;&#36825;&#20123;&#20013;&#38388;&#38169;&#35823;&#26356;&#20855;&#36776;&#21035;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;LLM&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#30830;&#20445;&#20248;&#36234;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#20219;&#21153;&#20998;&#35299;&#12289;&#23376;&#20219;&#21153;&#35299;&#20915;&#21644;&#35299;&#20915;&#32452;&#35013;&#36807;&#31243;&#30340;&#20998;&#31163;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#21487;&#20197;&#24341;&#23548;LLM&#25193;&#23637;&#22266;&#23450;&#28145;&#24230;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. Existing works show that appropriate prompt design, such as Chain-of-Thoughts, can unlock LLM's powerful capacity in diverse areas. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination. To make LLM more discerning to such intermediate errors, we propose to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process. Theoretic analysis reveals that our strategy can guide LLM to extend the expressive power of fixed-depth Transformer. Experiments indicate that our proposed method can achiev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#30340;&#28436;&#21464;&#65292;&#29305;&#21035;&#20851;&#27880;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20256;&#32479;&#25628;&#32034;&#26041;&#27861;&#19982;&#26032;&#20852;&#31572;&#26696;&#26816;&#32034;&#33539;&#24335;&#20043;&#38388;&#30340;&#26725;&#26753;&#20316;&#29992;&#65292;LLMs&#30340;&#25972;&#21512;&#26631;&#24535;&#30528;&#29992;&#25143;&#19982;&#20449;&#24687;&#31995;&#32479;&#20132;&#20114;&#26041;&#24335;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2402.05318</link><description>&lt;p&gt;
&#33322;&#34892;&#30693;&#35782;&#20043;&#28023;&#65306;&#21033;&#29992;LLMs&#36827;&#34892;&#34892;&#26143;&#32423;&#31572;&#26696;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Navigating the Knowledge Sea: Planet-scale answer retrieval using LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#30340;&#28436;&#21464;&#65292;&#29305;&#21035;&#20851;&#27880;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20256;&#32479;&#25628;&#32034;&#26041;&#27861;&#19982;&#26032;&#20852;&#31572;&#26696;&#26816;&#32034;&#33539;&#24335;&#20043;&#38388;&#30340;&#26725;&#26753;&#20316;&#29992;&#65292;LLMs&#30340;&#25972;&#21512;&#26631;&#24535;&#30528;&#29992;&#25143;&#19982;&#20449;&#24687;&#31995;&#32479;&#20132;&#20114;&#26041;&#24335;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#65292;&#20854;&#29305;&#28857;&#26159;&#20174;&#22522;&#26412;&#30340;&#36229;&#38142;&#25509;&#23548;&#33322;&#21040;&#22797;&#26434;&#30340;&#31639;&#27861;&#39537;&#21160;&#25628;&#32034;&#24341;&#25806;&#30340;&#19981;&#26029;&#25913;&#36827;&#12290;&#26412;&#25991;&#26088;&#22312;&#20840;&#38754;&#20171;&#32461;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#30340;&#28436;&#21464;&#65292;&#29305;&#21035;&#20851;&#27880;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20256;&#32479;&#25628;&#32034;&#26041;&#27861;&#19982;&#26032;&#20852;&#31572;&#26696;&#26816;&#32034;&#33539;&#24335;&#20043;&#38388;&#30340;&#26725;&#26753;&#20316;&#29992;&#12290;LLMs&#22312;&#21709;&#24212;&#26816;&#32034;&#21644;&#32034;&#24341;&#39046;&#22495;&#30340;&#25972;&#21512;&#26631;&#24535;&#30528;&#29992;&#25143;&#19982;&#20449;&#24687;&#31995;&#32479;&#20132;&#20114;&#26041;&#24335;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#36825;&#31181;&#33539;&#24335;&#36716;&#21464;&#26159;&#30001;&#20687;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#21512;&#39537;&#21160;&#30340;&#65292;&#23427;&#20204;&#33021;&#22815;&#29702;&#35299;&#21644;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#25991;&#26412;&#65292;&#20174;&#32780;&#33021;&#22815;&#25552;&#20379;&#26356;&#30452;&#25509;&#21644;&#24773;&#22659;&#30456;&#20851;&#30340;&#31572;&#26696;&#32473;&#29992;&#25143;&#26597;&#35810;&#12290;&#36890;&#36807;&#36825;&#31181;&#25506;&#32034;&#65292;&#25105;&#20204;&#35797;&#22270;&#38416;&#26126;&#25216;&#26415;&#37324;&#31243;&#30865;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information retrieval is a rapidly evolving field of information retrieval, which is characterized by a continuous refinement of techniques and technologies, from basic hyperlink-based navigation to sophisticated algorithm-driven search engines. This paper aims to provide a comprehensive overview of the evolution of Information Retrieval Technology, with a particular focus on the role of Large Language Models (LLMs) in bridging the gap between traditional search methods and the emerging paradigm of answer retrieval. The integration of LLMs in the realms of response retrieval and indexing signifies a paradigm shift in how users interact with information systems. This paradigm shift is driven by the integration of large language models (LLMs) like GPT-4, which are capable of understanding and generating human-like text, thus enabling them to provide more direct and contextually relevant answers to user queries. Through this exploration, we seek to illuminate the technological milestones 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#20998;&#26512;&#20102;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#24577;&#19981;&#19968;&#33268;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#19982;&#21442;&#19982;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#32852;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#32771;&#34385;&#19981;&#19968;&#33268;&#24615;&#30340;&#20449;&#24687;&#34701;&#21512;&#26426;&#21046;&#21644;&#27169;&#24577;&#25554;&#20540;&#32593;&#32476;&#65292;&#22312;&#35299;&#20915;&#27169;&#24577;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#25104;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05294</link><description>&lt;p&gt;
&#26816;&#39564;&#21307;&#30103;&#35270;&#35273;&#21644;&#22522;&#20110;&#35821;&#35328;&#30340;&#30142;&#30149;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#24577;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Examining Modality Incongruity in Multimodal Federated Learning for Medical Vision and Language-based Disease Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20998;&#26512;&#20102;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#24577;&#19981;&#19968;&#33268;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#19982;&#21442;&#19982;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#32852;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#32771;&#34385;&#19981;&#19968;&#33268;&#24615;&#30340;&#20449;&#24687;&#34701;&#21512;&#26426;&#21046;&#21644;&#27169;&#24577;&#25554;&#20540;&#32593;&#32476;&#65292;&#22312;&#35299;&#20915;&#27169;&#24577;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#65288;MMFL&#65289;&#21033;&#29992;&#27599;&#20010;&#23458;&#25143;&#31471;&#20013;&#30340;&#22810;&#20010;&#27169;&#24577;&#26500;&#24314;&#27604;&#20854;&#21333;&#27169;&#24577;&#23545;&#24212;&#29289;&#26356;&#24378;&#22823;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#23458;&#25143;&#31471;&#32570;&#22833;&#27169;&#24577;&#30340;&#24433;&#21709;&#65292;&#20063;&#31216;&#20026;&#27169;&#24577;&#19981;&#19968;&#33268;&#24615;&#65292;&#19968;&#30452;&#34987;&#22823;&#22823;&#24573;&#35270;&#12290;&#26412;&#25991;&#39318;&#27425;&#20998;&#26512;&#20102;&#27169;&#24577;&#19981;&#19968;&#33268;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#19982;&#21442;&#19982;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#29305;&#21035;&#26816;&#26597;&#20102;&#19981;&#19968;&#33268;&#30340;MMFL&#19982;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#23458;&#25143;&#31471;&#30456;&#27604;&#26159;&#21542;&#26356;&#26377;&#30410;&#20110;&#21333;&#27169;&#24577;FL&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19977;&#20010;&#28508;&#22312;&#36884;&#24452;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#33258;&#27880;&#24847;&#26426;&#21046;&#23545;&#20110;&#19981;&#32771;&#34385;&#19981;&#19968;&#33268;&#24615;&#30340;&#20449;&#24687;&#34701;&#21512;&#22312;MMFL&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#22312;&#22810;&#27169;&#24577;&#23458;&#25143;&#31471;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#24577;&#25554;&#20540;&#32593;&#32476;&#65288;MIN&#65289;&#26469;&#35299;&#20915;&#21333;&#27169;&#24577;&#23458;&#25143;&#31471;&#20013;&#30340;&#27169;&#24577;&#32763;&#35793;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20854;&#20943;&#36731;&#32570;&#22833;&#27169;&#24577;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
Multimodal Federated Learning (MMFL) utilizes multiple modalities in each client to build a more powerful Federated Learning (FL) model than its unimodal counterpart. However, the impact of missing modality in different clients, also called modality incongruity, has been greatly overlooked. This paper, for the first time, analyses the impact of modality incongruity and reveals its connection with data heterogeneity across participating clients. We particularly inspect whether incongruent MMFL with unimodal and multimodal clients is more beneficial than unimodal FL. Furthermore, we examine three potential routes of addressing this issue. Firstly, we study the effectiveness of various self-attention mechanisms towards incongruity-agnostic information fusion in MMFL. Secondly, we introduce a modality imputation network (MIN) pre-trained in a multimodal client for modality translation in unimodal clients and investigate its potential towards mitigating the missing modality problem. Thirdly
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;TreeForm&#30340;&#26631;&#27880;&#26041;&#26696;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#34920;&#21333;&#25991;&#26723;&#35299;&#26512;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#24320;&#21457;&#21644;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05282</link><description>&lt;p&gt;
TreeForm: &#34920;&#21333;&#25991;&#26723;&#35299;&#26512;&#30340;&#31471;&#21040;&#31471;&#26631;&#27880;&#19982;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
TreeForm: End-to-end Annotation and Evaluation for Form Document Parsing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;TreeForm&#30340;&#26631;&#27880;&#26041;&#26696;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#34920;&#21333;&#25991;&#26723;&#35299;&#26512;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#24320;&#21457;&#21644;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25991;&#26723;&#30340;&#39640;&#24230;&#32467;&#26500;&#21270;&#29305;&#24615;&#20197;&#21450;&#39640;&#24230;&#21464;&#21270;&#30340;&#26679;&#24335;&#21644;&#20869;&#23481;&#65292;&#20855;&#26377;&#35270;&#35273;&#20016;&#23500;&#30340;&#34920;&#21333;&#29702;&#35299;&#65288;VRFU&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#24403;&#21069;&#30340;&#26631;&#27880;&#26041;&#26696;&#23558;&#34920;&#21333;&#29702;&#35299;&#20998;&#35299;&#65292;&#24182;&#30465;&#30053;&#20102;&#20851;&#38190;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#20351;&#24471;&#24320;&#21457;&#21644;&#35780;&#20272;&#31471;&#21040;&#31471;&#27169;&#22411;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;F1&#24230;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;&#34920;&#21333;&#35299;&#26512;&#22120;&#65292;&#24182;&#25551;&#36848;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;VRFU&#30340;&#26032;&#30340;&#20869;&#23481;&#26080;&#20851;&#30340;&#22522;&#20110;&#26641;&#24418;&#30340;&#26631;&#27880;&#26041;&#26696;&#65306;TreeForm&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23558;&#20197;&#21069;&#30340;&#26631;&#27880;&#26041;&#26696;&#36716;&#25442;&#20026;TreeForm&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#20462;&#25913;&#21518;&#30340;&#26631;&#20934;&#21270;&#26641;&#32534;&#36753;&#36317;&#31163;&#35780;&#20272;TreeForm&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#25105;&#20204;&#22522;&#20110;FUNSD&#21644;XFUND&#25968;&#25454;&#38598;&#20998;&#21035;&#24471;&#21040;&#20102;&#31471;&#21040;&#31471;&#24615;&#33021;&#24230;&#37327;&#21644;TreeForm&#32534;&#36753;&#36317;&#31163;&#30340;&#21021;&#27493;&#22522;&#32447;&#32467;&#26524;&#65292;&#20998;&#21035;&#20026;61.5&#21644;26.4&#12290;&#25105;&#20204;&#24076;&#26395;TreeForm&#33021;&#22815;&#20419;&#36827;&#23545;&#31867;&#20284;&#34920;&#21333;&#25991;&#26723;&#30340;&#22797;&#26434;&#24615;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#26631;&#27880;&#12289;&#24314;&#27169;&#21644;&#35780;&#20272;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visually Rich Form Understanding (VRFU) poses a complex research problem due to the documents' highly structured nature and yet highly variable style and content. Current annotation schemes decompose form understanding and omit key hierarchical structure, making development and evaluation of end-to-end models difficult. In this paper, we propose a novel F1 metric to evaluate form parsers and describe a new content-agnostic, tree-based annotation scheme for VRFU: TreeForm. We provide methods to convert previous annotation schemes into TreeForm structures and evaluate TreeForm predictions using a modified version of the normalized tree-edit distance. We present initial baselines for our end-to-end performance metric and the TreeForm edit distance, averaged over the FUNSD and XFUND datasets, of 61.5 and 26.4 respectively. We hope that TreeForm encourages deeper research in annotating, modeling, and evaluating the complexities of form-like documents.
&lt;/p&gt;</description></item><item><title>VerAs&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#29992;&#20110;&#39564;&#35777;&#21644;&#35780;&#20272;STEM&#23454;&#39564;&#25253;&#21578;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#32500;&#24230;&#30340;&#20998;&#26512;&#35780;&#20272;&#26631;&#20934;&#65292;&#20197;&#21450;&#38024;&#23545;&#23398;&#29983;&#25552;&#20379;&#35814;&#32454;&#21453;&#39304;&#65292;&#24110;&#21161;&#20182;&#20204;&#25552;&#39640;&#31185;&#23398;&#20889;&#20316;&#25216;&#24039;&#12290;</title><link>https://arxiv.org/abs/2402.05224</link><description>&lt;p&gt;
VerAs: &#39564;&#35777;&#28982;&#21518;&#35780;&#20272;STEM&#23454;&#39564;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
VerAs: Verify then Assess STEM Lab Reports
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05224
&lt;/p&gt;
&lt;p&gt;
VerAs&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#29992;&#20110;&#39564;&#35777;&#21644;&#35780;&#20272;STEM&#23454;&#39564;&#25253;&#21578;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#32500;&#24230;&#30340;&#20998;&#26512;&#35780;&#20272;&#26631;&#20934;&#65292;&#20197;&#21450;&#38024;&#23545;&#23398;&#29983;&#25552;&#20379;&#35814;&#32454;&#21453;&#39304;&#65292;&#24110;&#21161;&#20182;&#20204;&#25552;&#39640;&#31185;&#23398;&#20889;&#20316;&#25216;&#24039;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;STEM&#25945;&#32946;&#23545;&#25209;&#21028;&#24615;&#24605;&#32500;&#33021;&#21147;&#30340;&#26085;&#30410;&#20851;&#27880;&#65292;&#31185;&#23398;&#20889;&#20316;&#22312;&#27880;&#37325;&#25506;&#31350;&#25216;&#33021;&#30340;&#35838;&#31243;&#20013;&#21457;&#25381;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26368;&#36817;&#21457;&#24067;&#30340;&#19968;&#20221;&#25968;&#25454;&#38598;&#26159;&#22522;&#20110;&#19968;&#22871;&#25506;&#31350;&#22411;&#29289;&#29702;&#35838;&#31243;&#30340;&#20004;&#32452;&#22823;&#23398;&#27700;&#24179;&#30340;&#23454;&#39564;&#25253;&#21578;&#65292;&#20381;&#36182;&#20110;&#21033;&#29992;&#22810;&#20010;&#32500;&#24230;&#30340;&#20998;&#26512;&#35780;&#20272;&#26631;&#20934;&#65292;&#25351;&#23450;&#23398;&#31185;&#30693;&#35782;&#21644;&#20248;&#31168;&#35299;&#37322;&#30340;&#19968;&#33324;&#32452;&#25104;&#37096;&#20998;&#12290;&#27599;&#20010;&#20998;&#26512;&#32500;&#24230;&#37117;&#20197;6&#20998;&#21046;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#25552;&#20379;&#35814;&#32454;&#21453;&#39304;&#65292;&#24110;&#21161;&#23398;&#29983;&#25552;&#39640;&#31185;&#23398;&#20889;&#20316;&#25216;&#24039;&#12290;&#25163;&#21160;&#35780;&#20272;&#21487;&#33021;&#36739;&#24930;&#65292;&#24182;&#19988;&#22312;&#22823;&#29677;&#20013;&#23545;&#25152;&#26377;&#23398;&#29983;&#36827;&#34892;&#19968;&#33268;&#24615;&#26657;&#20934;&#21487;&#33021;&#24456;&#22256;&#38590;&#12290;&#23613;&#31649;&#22312;STEM&#23398;&#31185;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#30340;&#33258;&#21160;&#35780;&#20272;&#19978;&#24050;&#32463;&#26377;&#24456;&#22810;&#24037;&#20316;&#65292;&#20294;&#22312;&#23454;&#39564;&#25253;&#21578;&#31561;&#38271;&#31687;&#20889;&#20316;&#20013;&#30340;&#24037;&#20316;&#35201;&#23569;&#24471;&#22810;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#29420;&#31435;&#30340;&#39564;&#35777;&#22120;&#21644;&#35780;&#20272;&#27169;&#22359;&#65292;&#28789;&#24863;&#26469;&#28304;&#20110;&#24320;&#25918;&#39046;&#22495;&#38382;&#39064;&#22238;&#31572;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With an increasing focus in STEM education on critical thinking skills, science writing plays an ever more important role in curricula that stress inquiry skills. A recently published dataset of two sets of college level lab reports from an inquiry-based physics curriculum relies on analytic assessment rubrics that utilize multiple dimensions, specifying subject matter knowledge and general components of good explanations. Each analytic dimension is assessed on a 6-point scale, to provide detailed feedback to students that can help them improve their science writing skills. Manual assessment can be slow, and difficult to calibrate for consistency across all students in large classes. While much work exists on automated assessment of open-ended questions in STEM subjects, there has been far less work on long-form writing such as lab reports. We present an end-to-end neural architecture that has separate verifier and assessment modules, inspired by approaches to Open Domain Question Answ
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;&#37319;&#26679;&#28201;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#39064;&#20013;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;0.0&#33267;1.0&#30340;&#28201;&#24230;&#33539;&#22260;&#20869;&#65292;LLM&#24615;&#33021;&#23545;&#35299;&#39064;&#20219;&#21153;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.05201</link><description>&lt;p&gt;
&#37319;&#26679;&#28201;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#39064;&#20013;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Effect of Sampling Temperature on Problem Solving in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05201
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;&#37319;&#26679;&#28201;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#39064;&#20013;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;0.0&#33267;1.0&#30340;&#28201;&#24230;&#33539;&#22260;&#20869;&#65292;LLM&#24615;&#33021;&#23545;&#35299;&#39064;&#20219;&#21153;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#35843;&#26597;&#20102;&#37319;&#26679;&#28201;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#35299;&#39064;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#26631;&#20934;LLM&#22522;&#20934;&#20013;&#38543;&#26426;&#25277;&#21462;&#38382;&#39064;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65288;MCQA&#65289;&#32771;&#35797;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22235;&#31181;&#24120;&#35265;&#30340;LLM&#20197;&#21450;&#20116;&#31181;&#25552;&#31034;&#24341;&#25806;&#25216;&#26415;&#26469;&#35299;&#20915;MCQA&#38382;&#39064;&#65292;&#21516;&#26102;&#23558;&#37319;&#26679;&#28201;&#24230;&#20174;0.0&#22686;&#21152;&#21040;1.0&#12290;&#23613;&#31649;&#26377;&#20851;&#30340;&#25253;&#36947;&#19982;&#20043;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;0.0&#33267;1.0&#30340;&#28201;&#24230;&#33539;&#22260;&#20869;&#65292;LLM&#24615;&#33021;&#22312;&#35299;&#39064;&#20219;&#21153;&#20013;&#30340;&#21464;&#21270;&#27809;&#26377;&#32479;&#35745;&#23398;&#19978;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#32467;&#26524;&#20284;&#20046;&#19981;&#21463;LLM&#12289;&#25552;&#31034;&#24341;&#25806;&#25216;&#26415;&#25110;&#38382;&#39064;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;&#25152;&#26377;&#20195;&#30721;&#12289;&#25968;&#25454;&#21644;&#34917;&#20805;&#36164;&#26009;&#37117;&#21487;&#20197;&#22312;GitHub&#19978;&#25214;&#21040;&#65306;https://github.com/matthewrenze/jhu-llm-temperature&#12290;
&lt;/p&gt;
&lt;p&gt;
In this research study, we empirically investigate the effect of sampling temperature on the performance of Large Language Models (LLMs) on various problem-solving tasks. We created a multiple-choice question-and-answer (MCQA) exam by randomly sampling problems from standard LLM benchmarks. Then, we used four popular LLMs with five prompt-engineering techniques to solve the MCQA problems while increasing the sampling temperature from 0.0 to 1.0. Despite anecdotal reports to the contrary, our empirical results indicate that changes in temperature in the range 0.0 to 1.0 do not have a statistically significant impact on LLM performance for problem-solving tasks. In addition, these results appear to hold regardless of the LLM, the prompt-engineering technique, or the problem domain. All code, data, and supplemental materials are available on GitHub at: https://github.com/matthewrenze/jhu-llm-temperature.
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#21463;&#38480;&#65292;&#26080;&#27861;&#23454;&#29616;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#21644;&#20551;&#35774;&#27979;&#35797;&#30340;MatSci-LLMs&#26694;&#26550;&#65292;&#24182;&#25551;&#36848;&#20102;&#20851;&#38190;&#30340;&#26448;&#26009;&#31185;&#23398;&#20449;&#24687;&#25552;&#21462;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.05200</link><description>&lt;p&gt;
LLMs&#26159;&#21542;&#20934;&#22791;&#22909;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#26448;&#26009;&#21457;&#29616;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are LLMs Ready for Real-World Materials Discovery?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05200
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#21463;&#38480;&#65292;&#26080;&#27861;&#23454;&#29616;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#21644;&#20551;&#35774;&#27979;&#35797;&#30340;MatSci-LLMs&#26694;&#26550;&#65292;&#24182;&#25551;&#36848;&#20102;&#20851;&#38190;&#30340;&#26448;&#26009;&#31185;&#23398;&#20449;&#24687;&#25552;&#21462;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#24378;&#22823;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#21487;&#33021;&#24615;&#65292;&#21152;&#24555;&#20102;&#26448;&#26009;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20173;&#23384;&#22312;&#19981;&#36275;&#65292;&#26080;&#27861;&#25104;&#20026;&#23454;&#29992;&#30340;&#26448;&#26009;&#31185;&#23398;&#24037;&#20855;&#12290;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;LLMs&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#30456;&#20851;&#22833;&#36133;&#26696;&#20363;&#65292;&#25581;&#31034;&#20102;LLMs&#22312;&#29702;&#35299;&#21644;&#25512;&#29702;&#22797;&#26434;&#12289;&#30456;&#20114;&#20851;&#32852;&#30340;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#26041;&#38754;&#30340;&#29616;&#26377;&#38480;&#21046;&#12290;&#37492;&#20110;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#21457;&#22522;&#20110;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#21644;&#20551;&#35774;&#29983;&#25104;&#19982;&#27979;&#35797;&#30340;&#26448;&#26009;&#31185;&#23398;LLMs&#65288;MatSci-LLMs&#65289;&#30340;&#26694;&#26550;&#12290;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;MatSci-LLMs&#30340;&#36335;&#24452;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#24314;&#31435;&#39640;&#36136;&#37327;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#33258;&#31185;&#23398;&#25991;&#29486;&#65292;&#20854;&#20013;&#23384;&#22312;&#21508;&#31181;&#20449;&#24687;&#25552;&#21462;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20851;&#38190;&#30340;&#26448;&#26009;&#31185;&#23398;&#20449;&#24687;&#25552;&#21462;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) create exciting possibilities for powerful language processing tools to accelerate research in materials science. While LLMs have great potential to accelerate materials understanding and discovery, they currently fall short in being practical materials science tools. In this position paper, we show relevant failure cases of LLMs in materials science that reveal current limitations of LLMs related to comprehending and reasoning over complex, interconnected materials science knowledge. Given those shortcomings, we outline a framework for developing Materials Science LLMs (MatSci-LLMs) that are grounded in materials science knowledge and hypothesis generation followed by hypothesis testing. The path to attaining performant MatSci-LLMs rests in large part on building high-quality, multi-modal datasets sourced from scientific literature where various information extraction challenges persist. As such, we describe key materials science information extraction cha
&lt;/p&gt;</description></item><item><title>$\lambda$-ECLIPSE&#36890;&#36807;&#21033;&#29992;CLIP&#28508;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#22810;&#27010;&#24565;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#23427;&#20943;&#23567;&#20102;&#35757;&#32451;&#36164;&#28304;&#38656;&#27714;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#19968;&#33268;&#12289;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#29983;&#25104;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05195</link><description>&lt;p&gt;
$\lambda$-ECLIPSE: &#36890;&#36807;&#21033;&#29992;CLIP&#28508;&#31354;&#38388;&#65292;&#22522;&#20110;&#22810;&#27010;&#24565;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
$\lambda$-ECLIPSE: Multi-Concept Personalized Text-to-Image Diffusion Models by Leveraging CLIP Latent Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05195
&lt;/p&gt;
&lt;p&gt;
$\lambda$-ECLIPSE&#36890;&#36807;&#21033;&#29992;CLIP&#28508;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#22810;&#27010;&#24565;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#23427;&#20943;&#23567;&#20102;&#35757;&#32451;&#36164;&#28304;&#38656;&#27714;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#19968;&#33268;&#12289;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#29983;&#25104;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;(P-T2I)&#29983;&#25104;&#27169;&#22411;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#36827;&#23637;&#65292;&#20294;&#22522;&#20110;&#20027;&#39064;&#30340;T2I&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20027;&#35201;&#30340;&#29942;&#39048;&#21253;&#25324;&#65306;1) &#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#36164;&#28304;&#65292;2) &#36229;&#21442;&#25968;&#25935;&#24863;&#24615;&#23548;&#33268;&#19981;&#19968;&#33268;&#30340;&#36755;&#20986;&#65292;&#20197;&#21450;3) &#24179;&#34913;&#26032;&#30340;&#35270;&#35273;&#27010;&#24565;&#21644;&#26500;&#22270;&#23545;&#40784;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#37325;&#26032;&#38416;&#36848;&#20102;T2I&#25193;&#25955;&#27169;&#22411;&#30340;&#26680;&#24515;&#29702;&#24565;&#65292;&#20197;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#12290;&#20027;&#35201;&#22320;&#65292;&#24403;&#20195;&#30340;&#22522;&#20110;&#20027;&#39064;&#30340;T2I&#26041;&#27861;&#20381;&#36182;&#20110;&#28508;&#31354;&#38388;&#25193;&#25955;&#27169;&#22411;(LDMs)&#65292;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#23454;&#29616;T2I&#26144;&#23556;&#12290;&#34429;&#28982;LDMs&#25552;&#20379;&#20102;&#26126;&#26174;&#30340;&#20248;&#21183;&#65292;&#20294;P-T2I&#26041;&#27861;&#23545;&#36825;&#20123;&#25193;&#25955;&#27169;&#22411;&#30340;&#28508;&#31354;&#38388;&#30340;&#20381;&#36182;&#26174;&#33879;&#22686;&#21152;&#20102;&#36164;&#28304;&#38656;&#27714;&#65292;&#23548;&#33268;&#32467;&#26524;&#19981;&#19968;&#33268;&#65292;&#24182;&#38656;&#35201;&#22810;&#27425;&#36845;&#20195;&#25165;&#33021;&#24471;&#21040;&#19968;&#20010;&#25152;&#38656;&#30340;&#22270;&#20687;&#12290;&#26368;&#36817;&#65292;ECLIPSE&#23637;&#31034;&#20102;&#19968;&#31181;&#26356;&#20855;&#36164;&#28304;&#25928;&#29575;&#30340;&#35757;&#32451;UnCLIP-based T2I&#27169;&#22411;&#30340;&#36335;&#24452;&#65292;&#36991;&#20813;&#20102;&#38656;&#35201;&#25193;&#25955;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent advances in personalized text-to-image (P-T2I) generative models, subject-driven T2I remains challenging. The primary bottlenecks include 1) Intensive training resource requirements, 2) Hyper-parameter sensitivity leading to inconsistent outputs, and 3) Balancing the intricacies of novel visual concept and composition alignment. We start by re-iterating the core philosophy of T2I diffusion models to address the above limitations. Predominantly, contemporary subject-driven T2I approaches hinge on Latent Diffusion Models (LDMs), which facilitate T2I mapping through cross-attention layers. While LDMs offer distinct advantages, P-T2I methods' reliance on the latent space of these diffusion models significantly escalates resource demands, leading to inconsistent results and necessitating numerous iterations for a single desired image. Recently, ECLIPSE has demonstrated a more resource-efficient pathway for training UnCLIP-based T2I models, circumventing the need for diffu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;InCoRo&#31995;&#32479;&#65292;&#20351;&#29992;&#32463;&#20856;&#30340;&#26426;&#22120;&#20154;&#21453;&#39304;&#24490;&#29615;&#65292;&#36890;&#36807;LLM&#25511;&#21046;&#22120;&#12289;&#22330;&#26223;&#29702;&#35299;&#21333;&#20803;&#21644;&#26426;&#22120;&#20154;&#30340;&#21327;&#21516;&#24037;&#20316;&#65292;&#23454;&#29616;&#23545;&#21160;&#24577;&#29615;&#22659;&#20013;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#25345;&#32493;&#20998;&#26512;&#29615;&#22659;&#29366;&#24577;&#24182;&#25552;&#20379;&#36866;&#24212;&#24615;&#25191;&#34892;&#21629;&#20196;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#24182;&#32416;&#27491;&#25511;&#21046;&#22120;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2402.05188</link><description>&lt;p&gt;
InCoRo&#65306;&#24102;&#26377;&#21453;&#39304;&#24490;&#29615;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#29992;&#20110;&#26426;&#22120;&#20154;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
InCoRo: In-Context Learning for Robotics Control with Feedback Loops
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;InCoRo&#31995;&#32479;&#65292;&#20351;&#29992;&#32463;&#20856;&#30340;&#26426;&#22120;&#20154;&#21453;&#39304;&#24490;&#29615;&#65292;&#36890;&#36807;LLM&#25511;&#21046;&#22120;&#12289;&#22330;&#26223;&#29702;&#35299;&#21333;&#20803;&#21644;&#26426;&#22120;&#20154;&#30340;&#21327;&#21516;&#24037;&#20316;&#65292;&#23454;&#29616;&#23545;&#21160;&#24577;&#29615;&#22659;&#20013;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#25345;&#32493;&#20998;&#26512;&#29615;&#22659;&#29366;&#24577;&#24182;&#25552;&#20379;&#36866;&#24212;&#24615;&#25191;&#34892;&#21629;&#20196;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#24182;&#32416;&#27491;&#25511;&#21046;&#22120;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#25216;&#26415;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#20351;&#26426;&#22120;&#20154;&#20855;&#22791;&#36275;&#22815;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#33021;&#22815;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;LLM&#36827;&#23637;&#23558;&#23427;&#20204;&#23450;&#20301;&#20026;&#31616;&#21333;&#25512;&#29702;&#20219;&#21153;&#30340;&#39318;&#36873;&#24037;&#20855;&#65292;&#28608;&#21457;&#20102;Liang&#31561;&#20154;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;[35]&#65292;&#35813;&#24037;&#20316;&#20351;&#29992;LLM&#23558;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#36716;&#21270;&#20026;&#26426;&#22120;&#20154;&#21333;&#20301;&#30340;&#20302;&#32423;&#38745;&#24577;&#25191;&#34892;&#35745;&#21010;&#12290;&#22312;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#20351;&#29992;LLM&#23558;&#20854;&#27867;&#21270;&#33021;&#21147;&#25552;&#21319;&#21040;&#19968;&#20010;&#26032;&#30340;&#27700;&#24179;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;&#26412;&#25991;&#23558;&#36825;&#39033;&#20808;&#21069;&#24037;&#20316;&#25193;&#23637;&#21040;&#20102;&#21160;&#24577;&#29615;&#22659;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;InCoRo&#65292;&#19968;&#20010;&#20351;&#29992;&#32463;&#20856;&#30340;&#26426;&#22120;&#20154;&#21453;&#39304;&#24490;&#29615;&#30340;&#31995;&#32479;&#65292;&#30001;LLM&#25511;&#21046;&#22120;&#12289;&#22330;&#26223;&#29702;&#35299;&#21333;&#20803;&#21644;&#26426;&#22120;&#20154;&#32452;&#25104;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#25345;&#32493;&#20998;&#26512;&#29615;&#22659;&#29366;&#24577;&#24182;&#25552;&#20379;&#36866;&#24212;&#24615;&#25191;&#34892;&#21629;&#20196;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#26465;&#20214;&#24182;&#32416;&#27491;&#25511;&#21046;&#22120;&#38169;&#35823;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#19981;&#38656;&#35201;&#20219;&#20309;&#36845;&#20195;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the challenges in robotics is to enable robotic units with the reasoning capability that would be robust enough to execute complex tasks in dynamic environments. Recent advances in LLMs have positioned them as go-to tools for simple reasoning tasks, motivating the pioneering work of Liang et al. [35] that uses an LLM to translate natural language commands into low-level static execution plans for robotic units. Using LLMs inside robotics systems brings their generalization to a new level, enabling zero-shot generalization to new tasks. This paper extends this prior work to dynamic environments. We propose InCoRo, a system that uses a classical robotic feedback loop composed of an LLM controller, a scene understanding unit, and a robot. Our system continuously analyzes the state of the environment and provides adapted execution commands, enabling the robot to adjust to changing environmental conditions and correcting for controller errors. Our system does not require any iterativ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20462;&#21098;&#21644;&#20302;&#31209;&#20462;&#25913;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#26426;&#21046;&#22266;&#26377;&#26131;&#30862;&#24615;&#65292;&#21435;&#38500;&#23433;&#20840;&#20851;&#38190;&#21306;&#22495;&#20250;&#25439;&#23475;&#23433;&#20840;&#24615;&#65292;&#20294;&#23545;&#25928;&#29992;&#24433;&#21709;&#19981;&#22823;&#65292;&#38656;&#35201;&#26356;&#24378;&#20581;&#30340;&#23433;&#20840;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.05162</link><description>&lt;p&gt;
&#36890;&#36807;&#20462;&#21098;&#21644;&#20302;&#31209;&#20462;&#25913;&#35780;&#20272;&#23433;&#20840;&#23545;&#40784;&#30340;&#26131;&#30862;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20462;&#21098;&#21644;&#20302;&#31209;&#20462;&#25913;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#26426;&#21046;&#22266;&#26377;&#26131;&#30862;&#24615;&#65292;&#21435;&#38500;&#23433;&#20840;&#20851;&#38190;&#21306;&#22495;&#20250;&#25439;&#23475;&#23433;&#20840;&#24615;&#65292;&#20294;&#23545;&#25928;&#29992;&#24433;&#21709;&#19981;&#22823;&#65292;&#38656;&#35201;&#26356;&#24378;&#20581;&#30340;&#23433;&#20840;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20854;&#23433;&#20840;&#26426;&#21046;&#26041;&#38754;&#34920;&#29616;&#20986;&#22266;&#26377;&#30340;&#26131;&#30862;&#24615;&#65292;&#36825;&#21487;&#20174;&#23427;&#20204;&#26131;&#21463;&#36234;&#29425;&#21644;&#21363;&#20351;&#26159;&#38750;&#24694;&#24847;&#24494;&#35843;&#20063;&#26131;&#21463;&#24433;&#21709;&#26469;&#35828;&#26126;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20462;&#21098;&#21644;&#20302;&#31209;&#20462;&#25913;&#25506;&#35752;&#20102;&#23433;&#20840;&#23545;&#40784;&#30340;&#26131;&#30862;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#26041;&#27861;&#65292;&#33021;&#22815;&#35782;&#21035;&#23545;&#20110;&#23433;&#20840;&#38450;&#25252;&#33267;&#20851;&#37325;&#35201;&#65292;&#19988;&#22312;&#31070;&#32463;&#20803;&#21644;&#31209;&#32423;&#21035;&#19978;&#19982;&#25928;&#29992;&#30456;&#20851;&#30340;&#21306;&#22495;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#30340;&#23396;&#31435;&#21306;&#22495;&#26159;&#31232;&#30095;&#30340;&#65292;&#32422;&#21344;&#21442;&#25968;&#32423;&#21035;&#30340;$3\%$&#21644;&#25490;&#21517;&#32423;&#21035;&#30340;$2.5\%$&#12290;&#21435;&#38500;&#36825;&#20123;&#21306;&#22495;&#20250;&#25439;&#23475;&#23433;&#20840;&#24615;&#65292;&#32780;&#23545;&#25928;&#29992;&#30340;&#24433;&#21709;&#19981;&#22823;&#65292;&#20174;&#32780;&#35777;&#23454;&#20102;&#35813;&#27169;&#22411;&#23433;&#20840;&#26426;&#21046;&#30340;&#22266;&#26377;&#26131;&#30862;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#21363;&#20351;&#38480;&#21046;&#23545;&#23433;&#20840;&#20851;&#38190;&#21306;&#22495;&#36827;&#34892;&#20462;&#25913;&#65292;LLMs&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#20302;&#25104;&#26412;&#30340;&#24494;&#35843;&#25915;&#20987;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;LLMs&#20013;&#26356;&#24378;&#22823;&#30340;&#23433;&#20840;&#31574;&#30053;&#30340;&#32039;&#36843;&#24615;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning. This study explores this brittleness of safety alignment by leveraging pruning and low-rank modifications. We develop methods to identify critical regions that are vital for safety guardrails, and that are disentangled from utility-relevant regions at both the neuron and rank levels. Surprisingly, the isolated regions we find are sparse, comprising about $3\%$ at the parameter level and $2.5\%$ at the rank level. Removing these regions compromises safety without significantly impacting utility, corroborating the inherent brittleness of the model's safety mechanisms. Moreover, we show that LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications to the safety-critical regions are restricted. These findings underscore the urgent need for more robust safety strategies in LLMs.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ApiQ&#30340;&#26032;&#22411;&#37327;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#21021;&#22987;&#21270;LoRA&#32452;&#20214;&#21644;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#24674;&#22797;&#37327;&#21270;&#36807;&#31243;&#20013;&#20002;&#22833;&#30340;&#20449;&#24687;&#65292;&#32500;&#25345;&#21407;&#22987;&#27169;&#22411;&#30340;&#28608;&#27963;&#31934;&#24230;&#24182;&#20943;&#36731;&#35823;&#24046;&#20256;&#25773;&#12290;</title><link>https://arxiv.org/abs/2402.05147</link><description>&lt;p&gt;
ApiQ&#65306;2&#20301;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
ApiQ: Finetuning of 2-Bit Quantized Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05147
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ApiQ&#30340;&#26032;&#22411;&#37327;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#21021;&#22987;&#21270;LoRA&#32452;&#20214;&#21644;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#24674;&#22797;&#37327;&#21270;&#36807;&#31243;&#20013;&#20002;&#22833;&#30340;&#20449;&#24687;&#65292;&#32500;&#25345;&#21407;&#22987;&#27169;&#22411;&#30340;&#28608;&#27963;&#31934;&#24230;&#24182;&#20943;&#36731;&#35823;&#24046;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#22823;&#65292;&#20869;&#23384;&#39640;&#25928;&#30340;&#27169;&#22411;&#24494;&#35843;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;GPU&#20869;&#23384;&#38480;&#21046;&#21644;&#36825;&#20123;&#26041;&#27861;&#19982;&#23436;&#20840;&#24494;&#35843;&#30340;&#21487;&#27604;&#32467;&#26524;&#25152;&#24102;&#26469;&#30340;&#32422;&#26463;&#12290;&#23613;&#31649;&#26377;&#20102;&#36827;&#23637;&#65292;&#22914;QLoRA&#36825;&#26679;&#30340;&#20869;&#23384;&#39640;&#25928;&#24494;&#35843;&#31574;&#30053;&#22312;&#19981;&#21516;&#20301;&#23485;&#30340;&#37327;&#21270;&#21644;&#22810;&#26679;&#21270;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#19968;&#33268;&#12290;&#36825;&#31181;&#19981;&#19968;&#33268;&#20027;&#35201;&#26469;&#33258;&#20110;&#37327;&#21270;&#36807;&#31243;&#23545;&#20445;&#30041;&#30693;&#35782;&#30340;&#26377;&#23475;&#24433;&#21709;&#65292;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21066;&#24369;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#24494;&#35843;&#20013;&#30340;&#21033;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;ApiQ&#30340;&#26032;&#22411;&#37327;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#21516;&#26102;&#21021;&#22987;&#21270;LoRA&#32452;&#20214;&#21644;&#37327;&#21270;LLM&#30340;&#26435;&#37325;&#26469;&#24674;&#22797;&#37327;&#21270;&#25439;&#22833;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#30830;&#20445;&#20102;&#21407;&#22987;LLM&#30340;&#28608;&#27963;&#31934;&#24230;&#30340;&#32500;&#25345;&#65292;&#21516;&#26102;&#20943;&#36731;&#20102;&#35823;&#24046;&#30340;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory-efficient finetuning of large language models (LLMs) has recently attracted huge attention with the increasing size of LLMs, primarily due to the constraints posed by GPU memory limitations and the comparable results of these methods with full finetuning. Despite the advancements, current strategies for memory-efficient finetuning, such as QLoRA, exhibit inconsistent performance across diverse bit-width quantizations and multifaceted tasks. This inconsistency largely stems from the detrimental impact of the quantization process on preserved knowledge, leading to catastrophic forgetting and undermining the utilization of pretrained models for finetuning purposes. In this work, we introduce a novel quantization framework named ApiQ, designed to restore the lost information from quantization by concurrently initializing LoRA components and quantizing the weights of LLMs. This approach ensures the maintenance of the original LLM's activation precision while mitigating the error prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#36890;&#29992;&#30340;LLMs&#24212;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#23450;&#20041;&#30340;&#36755;&#20837;&#26631;&#31614;&#26469;&#23545;LLMs&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#36890;&#36807;&#26126;&#30830;&#23558;&#20219;&#21153;&#39046;&#22495;&#19982;&#20219;&#21153;&#21151;&#33021;&#20998;&#31163;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05140</link><description>&lt;p&gt;
Tag-LLM: &#23558;&#36890;&#29992;&#30340;LLM&#24212;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#20877;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#36890;&#29992;&#30340;LLMs&#24212;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#23450;&#20041;&#30340;&#36755;&#20837;&#26631;&#31614;&#26469;&#23545;LLMs&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#36890;&#36807;&#26126;&#30830;&#23558;&#20219;&#21153;&#39046;&#22495;&#19982;&#20219;&#21153;&#21151;&#33021;&#20998;&#31163;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#19987;&#38376;&#39046;&#22495;&#20013;&#65292;&#22914;&#29289;&#29702;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#31185;&#23398;&#36825;&#26679;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#26410;&#20805;&#20998;&#28085;&#30422;&#30340;&#39046;&#22495;&#65292;&#23427;&#20204;&#30340;&#33021;&#21147;&#19979;&#38477;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#36890;&#29992;LLMs&#37325;&#26032;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#26377;&#25928;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#33258;&#23450;&#20041;&#30340;&#36755;&#20837;&#26631;&#31614;&#65292;&#36825;&#20123;&#26631;&#31614;&#34987;&#21442;&#25968;&#21270;&#20026;&#36830;&#32493;&#21521;&#37327;&#24182;&#38468;&#21152;&#21040;LLMs&#30340;&#23884;&#20837;&#23618;&#65292;&#20197;&#23545;LLMs&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#26631;&#31614;&#65306;&#39046;&#22495;&#26631;&#31614;&#29992;&#20110;&#38480;&#23450;&#19987;&#19994;&#34920;&#31034;&#65288;&#20363;&#22914;&#21270;&#23398;&#24335;&#65289;&#24182;&#25552;&#20379;&#39046;&#22495;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#65307;&#21151;&#33021;&#26631;&#31614;&#29992;&#20110;&#34920;&#31034;&#29305;&#23450;&#30340;&#21151;&#33021;&#65288;&#20363;&#22914;&#39044;&#27979;&#20998;&#23376;&#24615;&#36136;&#65289;&#24182;&#21387;&#32553;&#21151;&#33021;&#35299;&#20915;&#25351;&#20196;&#12290;&#25105;&#20204;&#20351;&#29992;&#36741;&#21161;&#25968;&#25454;&#21644;&#39046;&#22495;&#30693;&#35782;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#30340;&#23398;&#20064;&#36825;&#20123;&#26631;&#31614;&#30340;&#21327;&#35758;&#12290;&#36890;&#36807;&#26126;&#30830;&#23558;&#20219;&#21153;&#39046;&#22495;&#19982;&#20219;&#21153;&#21151;&#33021;&#20998;&#31163;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable proficiency in understanding and generating natural language. However, their capabilities wane in highly specialized domains underrepresented in the pretraining corpus, such as physical and biomedical sciences. This work explores how to repurpose general LLMs into effective task solvers for specialized domains. We introduce a novel, model-agnostic framework for learning custom input tags, which are parameterized as continuous vectors appended to the LLM's embedding layer, to condition the LLM. We design two types of input tags: domain tags are used to delimit specialized representations (e.g., chemical formulas) and provide domain-relevant context; function tags are used to represent specific functions (e.g., predicting molecular properties) and compress function-solving instructions. We develop a three-stage protocol to learn these tags using auxiliary data and domain knowledge. By explicitly disentangling task domains from tas
&lt;/p&gt;</description></item><item><title>SceMQA&#26159;&#19968;&#31181;&#31185;&#23398;&#31867;&#22823;&#23398;&#20837;&#23398;&#32423;&#22810;&#27169;&#24577;&#38382;&#39064;&#22238;&#31572;&#30340;&#22522;&#20934;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#22522;&#20934;&#20013;&#34987;&#24573;&#35270;&#30340;&#25945;&#32946;&#38454;&#27573;&#30340;&#31354;&#30333;&#12290;&#23427;&#21253;&#21547;&#26680;&#24515;&#31185;&#23398;&#31185;&#30446;&#65292;&#34701;&#21512;&#20102;&#22810;&#39033;&#36873;&#25321;&#21644;&#33258;&#30001;&#22238;&#31572;&#30340;&#26684;&#24335;&#65292;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#38382;&#39064;&#35299;&#26512;&#21644;&#31572;&#26696;&#35299;&#37322;&#12290;&#35813;&#22522;&#20934;&#36824;&#36890;&#36807;&#30456;&#21516;&#32972;&#26223;&#20294;&#38382;&#39064;&#19981;&#21516;&#30340;&#26041;&#24335;&#65292;&#20419;&#36827;&#20102;&#23545;&#25512;&#29702;&#33021;&#21147;&#26356;&#20840;&#38754;&#21644;&#20934;&#30830;&#30340;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.05138</link><description>&lt;p&gt;
SceMQA&#65306;&#19968;&#31181;&#31185;&#23398;&#31867;&#22823;&#23398;&#20837;&#23398;&#32423;&#22810;&#27169;&#24577;&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SceMQA: A Scientific College Entrance Level Multimodal Question Answering Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05138
&lt;/p&gt;
&lt;p&gt;
SceMQA&#26159;&#19968;&#31181;&#31185;&#23398;&#31867;&#22823;&#23398;&#20837;&#23398;&#32423;&#22810;&#27169;&#24577;&#38382;&#39064;&#22238;&#31572;&#30340;&#22522;&#20934;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#22522;&#20934;&#20013;&#34987;&#24573;&#35270;&#30340;&#25945;&#32946;&#38454;&#27573;&#30340;&#31354;&#30333;&#12290;&#23427;&#21253;&#21547;&#26680;&#24515;&#31185;&#23398;&#31185;&#30446;&#65292;&#34701;&#21512;&#20102;&#22810;&#39033;&#36873;&#25321;&#21644;&#33258;&#30001;&#22238;&#31572;&#30340;&#26684;&#24335;&#65292;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#38382;&#39064;&#35299;&#26512;&#21644;&#31572;&#26696;&#35299;&#37322;&#12290;&#35813;&#22522;&#20934;&#36824;&#36890;&#36807;&#30456;&#21516;&#32972;&#26223;&#20294;&#38382;&#39064;&#19981;&#21516;&#30340;&#26041;&#24335;&#65292;&#20419;&#36827;&#20102;&#23545;&#25512;&#29702;&#33021;&#21147;&#26356;&#20840;&#38754;&#21644;&#20934;&#30830;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SceMQA&#65292;&#36825;&#26159;&#19968;&#31181;&#38754;&#21521;&#22823;&#23398;&#20837;&#23398;&#32423;&#31185;&#23398;&#31867;&#22810;&#27169;&#24577;&#38382;&#39064;&#22238;&#31572;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;&#23427;&#22635;&#34917;&#20102;&#29616;&#26377;&#22522;&#20934;&#20013;&#24120;&#24120;&#34987;&#24573;&#35270;&#30340;&#20851;&#38190;&#25945;&#32946;&#38454;&#27573;&#65292;&#28085;&#30422;&#20102;&#39640;&#20013;&#21040;&#22823;&#23398;&#39044;&#31185;&#30340;&#27700;&#24179;&#12290;SceMQA&#19987;&#27880;&#20110;&#26680;&#24515;&#31185;&#23398;&#31185;&#30446;&#65292;&#21253;&#25324;&#25968;&#23398;&#12289;&#29289;&#29702;&#23398;&#12289;&#21270;&#23398;&#21644;&#29983;&#29289;&#23398;&#12290;&#23427;&#34701;&#21512;&#20102;&#22810;&#39033;&#36873;&#25321;&#21644;&#33258;&#30001;&#22238;&#31572;&#30340;&#26684;&#24335;&#65292;&#30830;&#20445;&#23545;AI&#27169;&#22411;&#30340;&#33021;&#21147;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#20026;&#27599;&#20010;&#38382;&#39064;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#30693;&#35782;&#28857;&#21644;&#35814;&#32454;&#30340;&#31572;&#26696;&#35299;&#37322;&#12290;SceMQA&#36824;&#29420;&#29305;&#22320;&#25552;&#20379;&#20102;&#30456;&#21516;&#32972;&#26223;&#20294;&#38382;&#39064;&#19981;&#21516;&#30340;&#38382;&#39064;&#65292;&#20197;&#20419;&#36827;&#23545;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#26356;&#20840;&#38754;&#21644;&#20934;&#30830;&#30340;&#35780;&#20272;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23545;&#24320;&#28304;&#21644;&#38381;&#28304;&#30340;&#26368;&#26032;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#19981;&#21516;&#30340;&#23454;&#39564;&#35774;&#32622;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24320;&#21457;&#31185;&#23398;&#31867;&#22823;&#23398;&#20837;&#23398;&#32423;&#22810;&#27169;&#24577;&#38382;&#39064;&#22238;&#31572;&#26041;&#38754;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper introduces SceMQA, a novel benchmark for scientific multimodal question answering at the college entrance level. It addresses a critical educational phase often overlooked in existing benchmarks, spanning high school to pre-college levels. SceMQA focuses on core science subjects including Mathematics, Physics, Chemistry, and Biology. It features a blend of multiple-choice and free-response formats, ensuring a comprehensive evaluation of AI models' abilities. Additionally, our benchmark provides specific knowledge points for each problem and detailed explanations for each answer. SceMQA also uniquely presents problems with identical contexts but varied questions to facilitate a more thorough and accurate assessment of reasoning capabilities. In the experiment, we evaluate both open-source and close-source state-of-the-art Multimodal Large Language Models (MLLMs), across various experimental settings. The results show that further research and development are needed in developi
&lt;/p&gt;</description></item><item><title>LV-Eval&#26159;&#19968;&#20010;&#20855;&#26377;&#20116;&#20010;&#38271;&#24230;&#32423;&#21035;&#30340;&#38271;&#19978;&#19979;&#25991;&#22522;&#20934;&#27979;&#35797;&#65292;&#25903;&#25345;256k&#19978;&#19979;&#25991;&#38271;&#24230;&#65292;&#24182;&#20855;&#26377;&#28151;&#28102;&#20107;&#23454;&#12289;&#20851;&#38190;&#35789;&#21644;&#30701;&#35821;&#26367;&#25442;&#20197;&#21450;&#22522;&#20110;&#20851;&#38190;&#35789;&#22238;&#24518;&#30340;&#24230;&#37327;&#35774;&#35745;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#26088;&#22312;&#20943;&#23569;&#30693;&#35782;&#27844;&#28431;&#21644;&#25552;&#20379;&#26356;&#23458;&#35266;&#30340;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.05136</link><description>&lt;p&gt;
LV-Eval:&#19968;&#20010;&#24179;&#34913;&#30340;&#38271;&#19978;&#19979;&#25991;&#22522;&#20934;&#27979;&#35797;&#65292;&#20855;&#26377;5&#20010;&#38271;&#24230;&#32423;&#21035;&#65292;&#26368;&#22810;&#21487;&#36798;256K
&lt;/p&gt;
&lt;p&gt;
LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05136
&lt;/p&gt;
&lt;p&gt;
LV-Eval&#26159;&#19968;&#20010;&#20855;&#26377;&#20116;&#20010;&#38271;&#24230;&#32423;&#21035;&#30340;&#38271;&#19978;&#19979;&#25991;&#22522;&#20934;&#27979;&#35797;&#65292;&#25903;&#25345;256k&#19978;&#19979;&#25991;&#38271;&#24230;&#65292;&#24182;&#20855;&#26377;&#28151;&#28102;&#20107;&#23454;&#12289;&#20851;&#38190;&#35789;&#21644;&#30701;&#35821;&#26367;&#25442;&#20197;&#21450;&#22522;&#20110;&#20851;&#38190;&#35789;&#22238;&#24518;&#30340;&#24230;&#37327;&#35774;&#35745;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#26088;&#22312;&#20943;&#23569;&#30693;&#35782;&#27844;&#28431;&#21644;&#25552;&#20379;&#26356;&#23458;&#35266;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29616;&#22312;&#22768;&#31216;&#25903;&#25345;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#21487;&#20197;&#36798;&#21040;256k&#29978;&#33267;&#26356;&#22810;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20027;&#27969;&#22522;&#20934;&#27979;&#35797;&#30340;&#24179;&#22343;&#19978;&#19979;&#25991;&#38271;&#24230;&#19981;&#36275;&#65288;5k-21k&#65289;&#65292;&#24182;&#19988;&#23427;&#20204;&#23481;&#26131;&#20986;&#29616;&#30693;&#35782;&#27844;&#28431;&#21644;&#19981;&#20934;&#30830;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#23548;&#33268;&#35780;&#20272;&#32467;&#26524;&#20559;&#35265;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LV-Eval&#65292;&#19968;&#20010;&#20855;&#26377;&#20116;&#20010;&#38271;&#24230;&#32423;&#21035;&#65288;16k&#65292;32k&#65292;64k&#65292;128k&#21644;256k&#65289;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38271;&#19978;&#19979;&#25991;&#22522;&#20934;&#27979;&#35797;&#65292;&#26368;&#22810;&#21487;&#36798;256k&#20010;&#21333;&#35789;&#12290;LV-Eval&#21253;&#21547;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#65292;&#21333;&#36339;&#38382;&#31572;&#21644;&#22810;&#36339;&#38382;&#31572;&#65292;&#21253;&#21547;11&#20010;&#21452;&#35821;&#25968;&#25454;&#38598;&#12290;LV-Eval&#30340;&#35774;&#35745;&#34701;&#21512;&#20102;&#19977;&#20010;&#20851;&#38190;&#25216;&#26415;&#65292;&#21363;&#28151;&#28102;&#20107;&#23454;&#25554;&#20837;&#12289;&#20851;&#38190;&#35789;&#21644;&#30701;&#35821;&#26367;&#25442;&#20197;&#21450;&#22522;&#20110;&#20851;&#38190;&#35789;&#22238;&#24518;&#30340;&#24230;&#37327;&#35774;&#35745;&#12290;LV-Eval&#30340;&#20248;&#28857;&#21253;&#25324;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#21487;&#25511;&#35780;&#20272;&#12289;&#20855;&#26377;&#28151;&#28102;&#20107;&#23454;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#23454;&#20363;&#12289;&#20943;&#23569;&#30340;&#30693;&#35782;&#27844;&#28431;&#20197;&#21450;&#26356;&#23458;&#35266;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#22312;LV-Eval&#19978;&#35780;&#20272;&#20102;10&#20010;LLMs&#65292;&#24182;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art large language models (LLMs) are now claiming remarkable supported context lengths of 256k or even more. In contrast, the average context lengths of mainstream benchmarks are insufficient (5k-21k), and they suffer from potential knowledge leakage and inaccurate metrics, resulting in biased evaluation. This paper introduces LV-Eval, a challenging long-context benchmark with five length levels (16k, 32k, 64k, 128k, and 256k) reaching up to 256k words. LV-Eval features two main tasks, single-hop QA and multi-hop QA, comprising 11 bilingual datasets. The design of LV-Eval has incorporated three key techniques, namely confusing facts insertion, keyword and phrase replacement, and keyword-recall-based metric design. The advantages of LV-Eval include controllable evaluation across different context lengths, challenging test instances with confusing facts, mitigated knowledge leakage, and more objective evaluations. We evaluate 10 LLMs on LV-Eval and conduct ablation studies o
&lt;/p&gt;</description></item><item><title>CADReN&#26159;&#19968;&#20010;&#19978;&#19979;&#25991;&#38170;&#28857;&#39537;&#21160;&#30340;&#20851;&#31995;&#32593;&#32476;&#65292;&#29992;&#20110;&#21487;&#25511;&#30340;&#36328;&#22270;&#33410;&#28857;&#37325;&#35201;&#24615;&#20272;&#35745;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#38170;&#28857;&#26426;&#21046;&#65292;&#32771;&#34385;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#38646;-shot&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#24320;&#28304;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;RIC200&#21644;WK1K&#12290;</title><link>https://arxiv.org/abs/2402.05135</link><description>&lt;p&gt;
CADReN: &#19978;&#19979;&#25991;&#38170;&#28857;&#39537;&#21160;&#30340;&#20851;&#31995;&#32593;&#32476;&#29992;&#20110;&#21487;&#25511;&#36328;&#22270;&#33410;&#28857;&#37325;&#35201;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
CADReN: Contextual Anchor-Driven Relational Network for Controllable Cross-Graphs Node Importance Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05135
&lt;/p&gt;
&lt;p&gt;
CADReN&#26159;&#19968;&#20010;&#19978;&#19979;&#25991;&#38170;&#28857;&#39537;&#21160;&#30340;&#20851;&#31995;&#32593;&#32476;&#65292;&#29992;&#20110;&#21487;&#25511;&#30340;&#36328;&#22270;&#33410;&#28857;&#37325;&#35201;&#24615;&#20272;&#35745;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#38170;&#28857;&#26426;&#21046;&#65292;&#32771;&#34385;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#38646;-shot&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#24320;&#28304;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;RIC200&#21644;WK1K&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33410;&#28857;&#37325;&#35201;&#24615;&#20272;&#35745;(NIE)&#23545;&#20110;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#23558;&#22806;&#37096;&#20449;&#24687;&#25972;&#21512;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#20391;&#37325;&#20110;&#38745;&#24577;&#30340;&#21333;&#19968;&#22270;&#29305;&#24449;&#65292;&#22312;&#26032;&#22270;&#21644;&#29992;&#25143;&#29305;&#23450;&#35201;&#27714;&#26041;&#38754;&#32570;&#20047;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;CADReN&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#38170;&#28857;(CA)&#26426;&#21046;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#20351;&#32593;&#32476;&#33021;&#22815;&#30456;&#23545;&#20110;CA&#35780;&#20272;&#33410;&#28857;&#30340;&#37325;&#35201;&#24615;&#65292;&#32771;&#34385;&#30693;&#35782;&#22270;&#35889;(KGs)&#20013;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CADReN&#22312;&#36328;&#22270;NIE&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#38646;-shot&#39044;&#27979;&#33021;&#21147;&#12290;CADReN&#36824;&#34987;&#35777;&#26126;&#22312;&#21333;&#19968;&#22270;NIE&#20219;&#21153;&#19978;&#19982;&#20197;&#21069;&#30340;&#27169;&#22411;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#24182;&#24320;&#28304;&#20102;&#20004;&#20010;&#26032;&#25968;&#25454;&#38598;RIC200&#21644;WK1K&#65292;&#19987;&#38376;&#29992;&#20110;&#36328;&#22270;NIE&#30740;&#31350;&#65292;&#20026;&#36825;&#20010;&#39046;&#22495;&#30340;&#26410;&#26469;&#21457;&#23637;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Node Importance Estimation (NIE) is crucial for integrating external information into Large Language Models through Retriever-Augmented Generation. Traditional methods, focusing on static, single-graph characteristics, lack adaptability to new graphs and user-specific requirements. CADReN, our proposed method, addresses these limitations by introducing a Contextual Anchor (CA) mechanism. This approach enables the network to assess node importance relative to the CA, considering both structural and semantic features within Knowledge Graphs (KGs). Extensive experiments show that CADReN achieves better performance in cross-graph NIE task, with zero-shot prediction ability. CADReN is also proven to match the performance of previous models on single-graph NIE task. Additionally, we introduce and opensource two new datasets, RIC200 and WK1K, specifically designed for cross-graph NIE research, providing a valuable resource for future developments in this domain.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20110;&#29992;&#25143;&#30340;&#21453;&#39304;&#25968;&#25454;&#20013;&#24341;&#20837;&#20010;&#24615;&#21270;&#29305;&#24449;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#22312;&#22810;&#26679;&#21270;&#29992;&#25143;&#20559;&#22909;&#19979;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05133</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#20010;&#24615;&#21270;&#20154;&#31867;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Personalized Language Modeling from Personalized Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05133
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20110;&#29992;&#25143;&#30340;&#21453;&#39304;&#25968;&#25454;&#20013;&#24341;&#20837;&#20010;&#24615;&#21270;&#29305;&#24449;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#22312;&#22810;&#26679;&#21270;&#29992;&#25143;&#20559;&#22909;&#19979;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20010;&#24615;&#21270;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#30446;&#21069;&#20027;&#27969;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#24320;&#21457;&#30340;&#31639;&#27861;&#30340;&#22522;&#26412;&#21069;&#25552;&#22312;&#29992;&#25143;&#20559;&#22909;&#22810;&#26679;&#21270;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#20986;&#29616;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#27491;&#24335;&#20171;&#32461;&#20102;&#20174;&#20010;&#24615;&#21270;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#20219;&#21153;&#65292;&#24182;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26222;&#36890;&#30340;RLHF&#21487;&#33021;&#20250;&#23384;&#22312;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20010;&#24615;&#21270;-RLHF&#65288;P-RLHF&#65289;&#26694;&#26550;&#65292;&#38656;&#35201;&#21516;&#26102;&#23398;&#20064;&#29992;&#25143;&#27169;&#22411;&#21644;&#35821;&#35328;&#65288;&#25110;&#22870;&#21169;&#65289;&#27169;&#22411;&#12290;&#29992;&#25143;&#27169;&#22411;&#25509;&#25910;&#29992;&#25143;&#20449;&#24687;&#24182;&#36755;&#20986;&#29992;&#25143;&#34920;&#31034;&#12290;&#20854;&#32467;&#26500;&#32534;&#30721;&#20102;&#25105;&#20204;&#23545;&#21453;&#39304;&#25968;&#25454;&#20013;&#29992;&#25143;&#20559;&#22909;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#20026;&#20010;&#24615;&#21270;&#22870;&#21169;&#24314;&#27169;&#21644;&#20010;&#24615;&#21270;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#24320;&#21457;&#20102;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) is the current dominating framework to fine-tune large language models to better align with human preferences. However, the underlying premise of algorithms developed under this framework can be problematic when user preferences encoded in human feedback are diverse. In this work, we aim to address this problem by developing methods for building personalized language models. We first formally introduce the task of learning from personalized human feedback and explain why vanilla RLHF can be problematic in this context. We then propose a general Personalized-RLHF (P-RLHF) framework, which requires one to jointly learn a user model and a language (or reward) model. The user model takes in user information and outputs user representations. Its structure encodes our assumptions about user preferences underlying the feedback data. We develop new learning objectives for personalized reward modeling and personalized Direct Preference Optimizat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TexShape&#30340;&#20449;&#24687;&#35770;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#20114;&#20449;&#24687;&#30340;&#32463;&#39564;&#20272;&#35745;&#26469;&#20248;&#21270;&#25991;&#26412;&#34920;&#31034;&#65292;&#21487;&#29992;&#20110;&#25968;&#25454;&#21387;&#32553;&#21644;&#25935;&#24863;&#20449;&#24687;&#36807;&#28388;&#65292;&#25552;&#21319;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05132</link><description>&lt;p&gt;
TexShape:&#20449;&#24687;&#35770;&#21477;&#23376;&#23884;&#20837;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TexShape: Information Theoretic Sentence Embedding for Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05132
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TexShape&#30340;&#20449;&#24687;&#35770;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#20114;&#20449;&#24687;&#30340;&#32463;&#39564;&#20272;&#35745;&#26469;&#20248;&#21270;&#25991;&#26412;&#34920;&#31034;&#65292;&#21487;&#29992;&#20110;&#25968;&#25454;&#21387;&#32553;&#21644;&#25935;&#24863;&#20449;&#24687;&#36807;&#28388;&#65292;&#25552;&#21319;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#37327;&#30340;&#25351;&#25968;&#22686;&#38271;&#21644;&#25968;&#25454;&#23494;&#38598;&#24212;&#29992;&#30340;&#20986;&#29616;&#65292;&#23588;&#20854;&#26159;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#19982;&#36164;&#28304;&#21033;&#29992;&#12289;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#30456;&#20851;&#30340;&#38382;&#39064;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#20851;&#27880;&#25968;&#25454;&#30340;&#25991;&#26412;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#20449;&#24687;&#35770;&#30340;&#35270;&#35282;&#35299;&#20915;&#20102;&#23558;&#21477;&#23376;&#32534;&#30721;&#20026;&#20854;&#20248;&#21270;&#34920;&#31034;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;Donsker-Varadhan&#23450;&#20041;&#30340;Kullback-Leibler&#25955;&#24230;&#30340;&#32463;&#39564;&#20272;&#35745;&#20540;&#26469;&#35745;&#31639;&#20114;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#36825;&#31181;&#20272;&#35745;&#26469;&#35757;&#32451;&#19968;&#31181;&#20449;&#24687;&#35770;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#31216;&#20026;TexShape&#65292;&#29992;&#20110;&#65288;&#22522;&#20110;&#20219;&#21153;&#30340;&#65289;&#25968;&#25454;&#21387;&#32553;&#25110;&#36807;&#28388;&#25935;&#24863;&#20449;&#24687;&#65292;&#20174;&#32780;&#22686;&#24378;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20934;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21021;&#27493;&#25991;&#26412;&#34920;&#31034;&#65292;&#24182;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20449;&#24687;&#29702;&#35770;&#21387;&#32553;&#21644;&#20114;&#20449;&#24687;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the exponential growth in data volume and the emergence of data-intensive applications, particularly in the field of machine learning, concerns related to resource utilization, privacy, and fairness have become paramount. This paper focuses on the textual domain of data and addresses challenges regarding encoding sentences to their optimized representations through the lens of information-theory. In particular, we use empirical estimates of mutual information, using the Donsker-Varadhan definition of Kullback-Leibler divergence. Our approach leverages this estimation to train an information-theoretic sentence embedding, called TexShape, for (task-based) data compression or for filtering out sensitive information, enhancing privacy and fairness. In this study, we employ a benchmark language model for initial text representation, complemented by neural networks for information-theoretic compression and mutual information estimations. Our experiments demonstrate significant advanceme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#20999;&#22359;&#36130;&#21153;&#25253;&#21578;&#65292;&#36890;&#36807;&#26681;&#25454;&#25991;&#26723;&#30340;&#32467;&#26500;&#20803;&#32032;&#32452;&#20214;&#36827;&#34892;&#20999;&#22359;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20248;&#21270;&#20999;&#22359;&#22823;&#23567;&#65292;&#32780;&#26080;&#38656;&#35843;&#25972;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#25972;&#20307;&#19978;&#19979;&#25991;&#21644;&#20934;&#30830;&#24615;&#30340;&#35780;&#20272;&#20197;&#21450;&#23545;&#38382;&#31572;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.05131</link><description>&lt;p&gt;
&#26377;&#25928;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#36130;&#21153;&#25253;&#21578;&#20999;&#22359;
&lt;/p&gt;
&lt;p&gt;
Financial Report Chunking for Effective Retrieval Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#20999;&#22359;&#36130;&#21153;&#25253;&#21578;&#65292;&#36890;&#36807;&#26681;&#25454;&#25991;&#26723;&#30340;&#32467;&#26500;&#20803;&#32032;&#32452;&#20214;&#36827;&#34892;&#20999;&#22359;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20248;&#21270;&#20999;&#22359;&#22823;&#23567;&#65292;&#32780;&#26080;&#38656;&#35843;&#25972;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#25972;&#20307;&#19978;&#19979;&#25991;&#21644;&#20934;&#30830;&#24615;&#30340;&#35780;&#20272;&#20197;&#21450;&#23545;&#38382;&#31572;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20999;&#22359;&#20449;&#24687;&#26159;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#27573;&#33853;&#32423;&#20999;&#22359;&#19978;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#25152;&#26377;&#25991;&#26412;&#37117;&#35270;&#20026;&#24179;&#31561;&#30340;&#65292;&#24182;&#24573;&#30053;&#20102;&#25991;&#26723;&#32467;&#26500;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#20165;&#20165;&#23558;&#25991;&#26723;&#20999;&#22359;&#21040;&#27573;&#33853;&#32423;&#21035;&#65292;&#32780;&#26159;&#26681;&#25454;&#25991;&#26723;&#30340;&#32467;&#26500;&#20803;&#32032;&#32452;&#20214;&#26469;&#20999;&#22359;&#12290;&#23558;&#25991;&#26723;&#20998;&#35299;&#20026;&#36825;&#20123;&#32452;&#25104;&#20803;&#32032;&#21487;&#20197;&#21019;&#24314;&#19968;&#31181;&#26032;&#30340;&#25991;&#26723;&#20999;&#22359;&#26041;&#24335;&#65292;&#21487;&#20197;&#24471;&#21040;&#26368;&#20339;&#30340;&#20999;&#22359;&#22823;&#23567;&#65292;&#26080;&#38656;&#35843;&#25972;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#35780;&#20272;&#26681;&#25454;&#30001;&#25991;&#26723;&#29702;&#35299;&#27169;&#22411;&#27880;&#37322;&#30340;&#20803;&#32032;&#31867;&#22411;&#36827;&#34892;&#20999;&#22359;&#22914;&#20309;&#23545;&#25152;&#26816;&#32034;&#20449;&#24687;&#30340;&#25972;&#20307;&#19978;&#19979;&#25991;&#21644;&#20934;&#30830;&#24615;&#36129;&#29486;&#12290;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#23545;RAG&#36741;&#21161;&#38382;&#31572;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21253;&#25324;&#23545;&#21508;&#31181;&#20803;&#32032;&#31867;&#22411;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#23427;&#20204;&#22312;&#26377;&#25928;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#20316;&#29992;&#20197;&#21450;&#23427;&#20204;&#23545;&#20854;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chunking information is a key step in Retrieval Augmented Generation (RAG). Current research primarily centers on paragraph-level chunking. This approach treats all texts as equal and neglects the information contained in the structure of documents. We propose an expanded approach to chunk documents by moving beyond mere paragraph-level chunking to chunk primary by structural element components of documents. Dissecting documents into these constituent elements creates a new way to chunk documents that yields the best chunk size without tuning. We introduce a novel framework that evaluates how chunking based on element types annotated by document understanding models contributes to the overall context and accuracy of the information retrieved. We also demonstrate how this approach impacts RAG assisted Question &amp; Answer task performance. Our research includes a comprehensive analysis of various element types, their role in effective information retrieval, and the impact they have on the 
&lt;/p&gt;</description></item><item><title>LB-KBQA&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;BERT&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#24110;&#21161;&#65292;&#33021;&#22815;&#25552;&#39640;&#24847;&#22270;&#35782;&#21035;&#30340;&#24615;&#33021;&#21644;&#35299;&#20915;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05130</link><description>&lt;p&gt;
LB-KBQA: &#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;BERT&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
LB-KBQA: Large-language-model and BERT based Knowledge-Based Question and Answering System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05130
&lt;/p&gt;
&lt;p&gt;
LB-KBQA&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;BERT&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#24110;&#21161;&#65292;&#33021;&#22815;&#25552;&#39640;&#24847;&#22270;&#35782;&#21035;&#30340;&#24615;&#33021;&#21644;&#35299;&#20915;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22240;&#20854;&#26032;&#20852;&#30340;&#33021;&#21147;&#32780;&#36171;&#20104;&#20102;&#21508;&#20010;&#39046;&#22495;&#30340;&#21147;&#37327;&#65292;&#20854;&#20013;&#19968;&#20010;&#20856;&#22411;&#30340;&#24212;&#29992;&#39046;&#22495;&#26159;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;AI&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#29983;&#25104;&#24335;AI&#30340;&#20856;&#22411;&#24212;&#29992;&#39046;&#22495;&#20043;&#19968;&#26159;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#24182;&#19988;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#65292;LLM&#30340;&#33021;&#21147;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#31995;&#32479;&#24847;&#22270;&#35782;&#21035;&#24615;&#33021;&#30340;&#38556;&#30861;&#65292;&#36825;&#28304;&#33258;&#35821;&#35328;&#22810;&#26679;&#24615;&#21644;&#26032;&#20986;&#29616;&#30340;&#24847;&#22270;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;AI&#30340;&#24847;&#22270;&#35782;&#21035;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#22522;&#20110;&#35821;&#20041;&#35299;&#26512;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#22312;&#24847;&#22270;&#35782;&#21035;&#26041;&#38754;&#21463;&#21040;&#26377;&#38480;&#30340;&#36164;&#28304;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;BERT&#30340;&#26032;&#22411;KBQA&#31995;&#32479;&#65288;LB-KBQA&#65289;&#12290;&#22312;&#29983;&#25104;&#24335;AI&#30340;&#24110;&#21161;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26816;&#27979;&#21040;&#8230;&#8230;&#65288;&#30053;&#65289;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence (AI), because of its emergent abilities, has empowered various fields, one typical of which is large language models (LLMs). One of the typical application fields of Generative AI is large language models (LLMs), and the natural language understanding capability of LLM is dramatically improved when compared with conventional AI-based methods. The natural language understanding capability has always been a barrier to the intent recognition performance of the Knowledge-Based-Question-and-Answer (KBQA) system, which arises from linguistic diversity and the newly appeared intent. Conventional AI-based methods for intent recognition can be divided into semantic parsing-based and model-based approaches. However, both of the methods suffer from limited resources in intent recognition. To address this issue, we propose a novel KBQA system based on a Large Language Model(LLM) and BERT (LB-KBQA). With the help of generative AI, our proposed method could detect 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#20351;&#29992;&#22312;&#25991;&#26412;&#26631;&#27880;&#39046;&#22495;&#24102;&#26469;&#20102;&#35768;&#22810;&#25361;&#25112;&#65292;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#22871;&#21487;&#38752;&#12289;&#21487;&#37325;&#22797;&#12289;&#31526;&#21512;&#20262;&#29702;&#35201;&#27714;&#30340;&#20351;&#29992;&#26631;&#20934;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#20197;&#35299;&#20915;&#30456;&#20851;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05129</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#26631;&#27880;&#30340;&#26368;&#20339;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Best Practices for Text Annotation with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05129
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#20351;&#29992;&#22312;&#25991;&#26412;&#26631;&#27880;&#39046;&#22495;&#24102;&#26469;&#20102;&#35768;&#22810;&#25361;&#25112;&#65292;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#22871;&#21487;&#38752;&#12289;&#21487;&#37325;&#22797;&#12289;&#31526;&#21512;&#20262;&#29702;&#35201;&#27714;&#30340;&#20351;&#29992;&#26631;&#20934;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#20197;&#35299;&#20915;&#30456;&#20851;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24320;&#21551;&#20102;&#25991;&#26412;&#26631;&#27880;&#30340;&#26032;&#26102;&#20195;&#65292;&#30001;&#20110;&#20854;&#26131;&#29992;&#24615;&#12289;&#39640;&#20934;&#30830;&#24615;&#21644;&#30456;&#23545;&#36739;&#20302;&#30340;&#25104;&#26412;&#65292;&#23548;&#33268;&#26368;&#36817;&#20960;&#20010;&#26376;&#20351;&#29992;&#36825;&#31181;&#27169;&#22411;&#30340;&#22686;&#38271;&#36805;&#29467;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#24847;&#21619;&#30528;&#22522;&#20110;LLM&#30340;&#26631;&#27880;&#25104;&#20026;&#20102;&#19968;&#31181;&#23398;&#26415;&#30028;&#30340;&#8220;&#37326;&#35199;&#8221;&#65292;&#32570;&#20047;&#24314;&#31435;&#30340;&#23454;&#36341;&#21644;&#26631;&#20934;&#23548;&#33268;&#20102;&#23545;&#30740;&#31350;&#30340;&#36136;&#37327;&#21644;&#26377;&#25928;&#24615;&#30340;&#25285;&#24551;&#12290;&#30740;&#31350;&#20154;&#21592;&#35686;&#21578;&#35828;&#65292;LLM&#30340;&#26174;&#32780;&#26131;&#35265;&#30340;&#31616;&#21333;&#24615;&#21487;&#33021;&#20250;&#23548;&#33268;&#35823;&#23548;&#65292;&#22240;&#20026;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#20559;&#35265;&#12289;&#35823;&#35299;&#21644;&#19981;&#21487;&#38752;&#30340;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#26088;&#22312;&#35748;&#35782;&#21040;LLM&#30340;&#21464;&#38761;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#22871;&#21487;&#38752;&#12289;&#21487;&#37325;&#22797;&#21644;&#31526;&#21512;&#20262;&#29702;&#35201;&#27714;&#30340;&#20351;&#29992;&#26631;&#20934;&#21644;&#26368;&#20339;&#23454;&#36341;&#12290;&#36825;&#20123;&#25351;&#21335;&#28085;&#30422;&#20102;&#20851;&#38190;&#39046;&#22495;&#65292;&#22914;&#27169;&#22411;&#36873;&#25321;&#12289;&#25552;&#31034;&#24037;&#31243;&#12289;&#32467;&#26500;&#21270;&#25552;&#31034;&#12289;&#25552;&#31034;&#31283;&#23450;&#24615;&#20998;&#26512;&#12289;&#20005;&#26684;&#30340;&#27169;&#22411;&#39564;&#35777;&#20197;&#21450;&#20262;&#29702;&#21644;&#27861;&#24459;&#24847;&#35782;&#30340;&#32771;&#34385;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#23545;&#20840;&#38754;&#30340;&#39564;&#35777;&#21644;&#36879;&#26126;&#24230;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have ushered in a new era of text annotation, as their ease-of-use, high accuracy, and relatively low costs have meant that their use has exploded in recent months. However, the rapid growth of the field has meant that LLM-based annotation has become something of an academic Wild West: the lack of established practices and standards has led to concerns about the quality and validity of research. Researchers have warned that the ostensible simplicity of LLMs can be misleading, as they are prone to bias, misunderstandings, and unreliable results. Recognizing the transformative potential of LLMs, this paper proposes a comprehensive set of standards and best practices for their reliable, reproducible, and ethical use. These guidelines span critical areas such as model selection, prompt engineering, structured prompting, prompt stability analysis, rigorous model validation, and the consideration of ethical and legal implications. The paper emphasizes the need fo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#21644;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#22788;&#29702;&#38271;&#25991;&#26412;&#21644;&#25552;&#21319;&#25512;&#29702;&#33021;&#21147;&#65292;&#20026;&#25945;&#31185;&#20070;&#38382;&#31572;&#20219;&#21153;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.05128</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25552;&#21319;&#25945;&#31185;&#20070;&#38382;&#31572;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Enhancing Textbook Question Answering Task with Large Language Models and Retrieval Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#21644;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#22788;&#29702;&#38271;&#25991;&#26412;&#21644;&#25552;&#21319;&#25512;&#29702;&#33021;&#21147;&#65292;&#20026;&#25945;&#31185;&#20070;&#38382;&#31572;&#20219;&#21153;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#31185;&#20070;&#38382;&#31572;&#65288;TQA&#65289;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#19978;&#19979;&#25991;&#21644;&#22810;&#27169;&#24335;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#22312;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19981;&#36275;&#21644;&#26080;&#27861;&#25429;&#25417;&#38271;&#25991;&#26412;&#20013;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24341;&#20837;&#38761;&#21629;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#28982;&#32780;&#65292;&#30452;&#25509;&#24212;&#29992;LLMs&#32463;&#24120;&#20250;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22788;&#29702;TQA&#20013;&#39046;&#22495;&#22806;&#24773;&#26223;&#65292;&#21363;&#27010;&#24565;&#20998;&#24067;&#22312;&#19981;&#21516;&#35838;&#31243;&#20013;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#21644;&#36801;&#31227;&#23398;&#20064;&#26469;&#22788;&#29702;&#38271;&#25991;&#26412;&#24182;&#25552;&#21319;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;LLM&#27169;&#22411;Llama-2&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#24182;&#21152;&#20837;RAG&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#20248;&#20110;&#22522;&#32447;&#65292;&#22312;&#39564;&#35777;&#38598;&#19978;&#30340;&#20934;&#30830;&#24230;&#25552;&#39640;&#20102;4.12%&#65292;&#22312;&#27979;&#35797;&#38598;&#19978;&#25552;&#39640;&#20102;9.84%&#12290;
&lt;/p&gt;
&lt;p&gt;
Textbook question answering (TQA) is a challenging task in artificial intelligence due to the complex nature of context and multimodal data. Although previous research has significantly improved the task, there are still some limitations including the models' weak reasoning and inability to capture contextual information in the lengthy context. The introduction of large language models (LLMs) has revolutionized the field of AI, however, directly applying LLMs often leads to inaccurate answers. This paper proposes a methodology that handle the out-of-domain scenario in TQA where concepts are spread across different lessons by incorporating the retrieval augmented generation (RAG) technique and utilize transfer learning to handle the long context and enhance reasoning abilities. Through supervised fine-tuning of the LLM model Llama-2 and the incorporation of RAG, our architecture outperforms the baseline, achieving a 4.12% accuracy improvement on validation set and 9.84% on test set for 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25233;&#37057;&#30151;&#26816;&#27979;&#21644;&#27835;&#30103;&#33539;&#24335;&#65292;&#20351;&#29992;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#32463;&#36807;&#29305;&#23450;&#25552;&#31034;&#24494;&#35843;&#20197;&#35786;&#26029;&#12289;&#35299;&#37322;&#21644;&#24314;&#35758;&#27835;&#30103;&#24178;&#39044;&#12290;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#25968;&#25454;&#24211;&#65292;&#20197;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#24314;&#35758;&#12290;&#27492;&#26041;&#27861;&#19982;&#24739;&#32773;&#36827;&#34892;&#20849;&#24773;&#23545;&#35805;&#31649;&#29702;&#65292;&#26377;&#25928;&#25903;&#25345;&#25233;&#37057;&#30151;&#24739;&#32773;&#12290;</title><link>https://arxiv.org/abs/2402.05127</link><description>&lt;p&gt;
Illuminate&#65306;&#19968;&#31181;&#20351;&#29992;&#21487;&#35299;&#37322;&#20998;&#26512;&#21644;&#31215;&#26497;&#27835;&#30103;&#30340;&#26032;&#26041;&#27861;&#36827;&#34892;&#25233;&#37057;&#30151;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Illuminate: A novel approach for depression detection with explainable analysis and proactive therapy using prompt engineering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25233;&#37057;&#30151;&#26816;&#27979;&#21644;&#27835;&#30103;&#33539;&#24335;&#65292;&#20351;&#29992;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#32463;&#36807;&#29305;&#23450;&#25552;&#31034;&#24494;&#35843;&#20197;&#35786;&#26029;&#12289;&#35299;&#37322;&#21644;&#24314;&#35758;&#27835;&#30103;&#24178;&#39044;&#12290;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#25968;&#25454;&#24211;&#65292;&#20197;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#24314;&#35758;&#12290;&#27492;&#26041;&#27861;&#19982;&#24739;&#32773;&#36827;&#34892;&#20849;&#24773;&#23545;&#35805;&#31649;&#29702;&#65292;&#26377;&#25928;&#25903;&#25345;&#25233;&#37057;&#30151;&#24739;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65306;Generative Pre-trained Transformer 4&#65288;GPT-4&#65289;&#12289;Llama 2 chat&#21644;Gemini&#30340;&#25233;&#37057;&#30151;&#26816;&#27979;&#21644;&#27835;&#30103;&#26032;&#33539;&#24335;&#12290;&#36825;&#20123;LLMs&#36890;&#36807;&#29305;&#23450;&#30340;&#25552;&#31034;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#35786;&#26029;&#12289;&#35299;&#37322;&#21644;&#24314;&#35758;&#25233;&#37057;&#30151;&#30340;&#27835;&#30103;&#24178;&#39044;&#12290;&#19968;&#31181;&#29420;&#29305;&#30340;&#23569;&#26679;&#26412;&#25552;&#31034;&#26041;&#27861;&#22686;&#24378;&#20102;&#27169;&#22411;&#26681;&#25454;DSM-5&#26631;&#20934;&#20998;&#26512;&#21644;&#35299;&#37322;&#25233;&#37057;&#30151;&#29366;&#30340;&#33021;&#21147;&#12290;&#22312;&#20132;&#20114;&#38454;&#27573;&#65292;&#27169;&#22411;&#37319;&#29992;&#20849;&#24773;&#23545;&#35805;&#31649;&#29702;&#65292;&#21033;&#29992;PsychDB&#21644;&#35748;&#30693;&#34892;&#20026;&#30103;&#27861;&#65288;CBT&#65289;&#25351;&#21335;&#31561;&#36164;&#28304;&#65292;&#19982;&#24739;&#26377;&#37325;&#24230;&#25233;&#37057;&#30151;&#30340;&#20010;&#20307;&#36827;&#34892;&#25903;&#25345;&#24615;&#20114;&#21160;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#20171;&#32461;&#20102;Illuminate&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#31181;CBT&#27169;&#22359;&#65292;&#21487;&#24110;&#21161;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#24314;&#35758;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;F1&#20998;&#25968;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#12289;&#20313;&#24358;&#30456;&#20284;&#24230;&#21644;&#21484;&#22238;&#29575;&#23548;&#21521;&#30340;&#24046;&#38169;&#36827;&#34892;LLM&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel paradigm for depression detection and treatment using advanced Large Language Models (LLMs): Generative Pre-trained Transformer 4 (GPT-4), Llama 2 chat, and Gemini. These LLMs are fine-tuned with specialized prompts to diagnose, explain, and suggest therapeutic interventions for depression. A unique few-shot prompting method enhances the models' ability to analyze and explain depressive symptoms based on the DSM-5 criteria. In the interaction phase, the models engage in empathetic dialogue management, drawing from resources like PsychDB and a Cognitive Behavioral Therapy (CBT) Guide, fostering supportive interactions with individuals experiencing major depressive disorders. Additionally, the research introduces the Illuminate Database, enriched with various CBT modules, aiding in personalized therapy recommendations. The study evaluates LLM performance using metrics such as F1 scores, Precision, Recall, Cosine similarity, and Recall-Oriented Understudy for
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#39033;&#30446;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#25429;&#33719;&#24182;&#22788;&#29702;&#25991;&#26412;&#20449;&#24687;&#20013;&#30340;&#20851;&#31995;&#25968;&#25454;&#65292;&#32780;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31995;&#32479;&#36890;&#36807;&#35782;&#21035;&#21644;&#24378;&#35843;&#20851;&#38190;&#23454;&#20307;&#26469;&#20445;&#25345;&#25688;&#35201;&#30340;&#37325;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.05126</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;NER&#30340;&#25991;&#26412;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network and NER-Based Text Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05126
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#39033;&#30446;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#25429;&#33719;&#24182;&#22788;&#29702;&#25991;&#26412;&#20449;&#24687;&#20013;&#30340;&#20851;&#31995;&#25968;&#25454;&#65292;&#32780;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31995;&#32479;&#36890;&#36807;&#35782;&#21035;&#21644;&#24378;&#35843;&#20851;&#38190;&#23454;&#20307;&#26469;&#20445;&#25345;&#25688;&#35201;&#30340;&#37325;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#26102;&#20195;&#20449;&#24687;&#21644;&#25968;&#25454;&#30340;&#20016;&#23500;&#24615;&#19979;&#65292;&#20154;&#20204;&#25110;&#32773;&#26426;&#22120;&#20960;&#20046;&#19981;&#21487;&#33021;&#36880;&#34892;&#26597;&#30475;&#25152;&#26377;&#30340;&#25968;&#25454;&#12290;&#36890;&#24120;&#30340;&#20570;&#27861;&#26159;&#35797;&#22270;&#20174;&#34892;&#20013;&#24555;&#36895;&#27983;&#35272;&#24182;&#20445;&#30041;&#32477;&#23545;&#37325;&#35201;&#30340;&#20449;&#24687;&#65292;&#36825;&#22312;&#26356;&#27491;&#24335;&#30340;&#26415;&#35821;&#20013;&#31216;&#20026;&#25688;&#35201;&#12290;&#25991;&#26412;&#25688;&#35201;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#23558;&#20887;&#38271;&#30340;&#25991;&#26723;&#25110;&#25991;&#31456;&#21387;&#32553;&#25104;&#36739;&#30701;&#12289;&#36830;&#36143;&#30340;&#34920;&#36798;&#26041;&#24335;&#65292;&#21516;&#26102;&#20445;&#30041;&#26680;&#24515;&#20449;&#24687;&#21644;&#24847;&#20041;&#12290;&#26412;&#39033;&#30446;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;GNNs&#20197;&#20854;&#20248;&#31168;&#30340;&#25429;&#33719;&#21644;&#22788;&#29702;&#25991;&#26412;&#20449;&#24687;&#20013;&#30340;&#20851;&#31995;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#29702;&#35299;&#22823;&#22411;&#25991;&#26723;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#12290;&#21516;&#26102;&#65292;NER&#31995;&#32479;&#36890;&#36807;&#35782;&#21035;&#21644;&#24378;&#35843;&#20851;&#38190;&#23454;&#20307;&#65292;&#30830;&#20445;&#25688;&#35201;&#36807;&#31243;&#20445;&#25345;&#19987;&#27880;&#20110;&#37325;&#28857;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the abundance of data and information in todays time, it is nearly impossible for man, or, even machine, to go through all of the data line by line. What one usually does is to try to skim through the lines and retain the absolutely important information, that in a more formal term is called summarization. Text summarization is an important task that aims to compress lengthy documents or articles into shorter, coherent representations while preserving the core information and meaning. This project introduces an innovative approach to text summarization, leveraging the capabilities of Graph Neural Networks (GNNs) and Named Entity Recognition (NER) systems. GNNs, with their exceptional ability to capture and process the relational data inherent in textual information, are adept at understanding the complex structures within large documents. Meanwhile, NER systems contribute by identifying and emphasizing key entities, ensuring that the summarization process maintains a focus on the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;LLMs&#24320;&#21457;&#20102;&#19968;&#20010;&#38646;&#26679;&#26412;&#20020;&#24202;&#35797;&#39564;&#24739;&#32773;&#21305;&#37197;&#31995;&#32479;&#65292;&#21487;&#20197;&#39640;&#25928;&#35780;&#20272;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#20837;&#36873;&#26631;&#20934;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#25552;&#31034;&#31574;&#30053;&#21644;&#26816;&#32034;&#27969;&#31243;&#25552;&#39640;&#20102;&#25968;&#25454;&#21644;&#25104;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05125</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#20020;&#24202;&#35797;&#39564;&#24739;&#32773;&#21305;&#37197;&#19982;LLMs
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Clinical Trial Patient Matching with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;LLMs&#24320;&#21457;&#20102;&#19968;&#20010;&#38646;&#26679;&#26412;&#20020;&#24202;&#35797;&#39564;&#24739;&#32773;&#21305;&#37197;&#31995;&#32479;&#65292;&#21487;&#20197;&#39640;&#25928;&#35780;&#20272;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#20837;&#36873;&#26631;&#20934;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#25552;&#31034;&#31574;&#30053;&#21644;&#26816;&#32034;&#27969;&#31243;&#25552;&#39640;&#20102;&#25968;&#25454;&#21644;&#25104;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#24739;&#32773;&#19982;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#26159;&#25512;&#20986;&#26032;&#33647;&#30340;&#20851;&#38190;&#38590;&#39064;&#12290;&#30446;&#21069;&#65292;&#35782;&#21035;&#31526;&#21512;&#35797;&#39564;&#20837;&#36873;&#26631;&#20934;&#30340;&#24739;&#32773;&#26159;&#39640;&#24230;&#25163;&#21160;&#30340;&#65292;&#27599;&#20301;&#24739;&#32773;&#38656;&#33457;&#36153;&#38271;&#36798;1&#23567;&#26102;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#31579;&#36873;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#29702;&#35299;&#38750;&#32467;&#26500;&#21270;&#30340;&#20020;&#24202;&#25991;&#26412;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23427;&#20204;&#22312;&#35797;&#39564;&#21305;&#37197;&#20013;&#30340;&#24212;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#19968;&#20010;&#24739;&#32773;&#30340;&#30149;&#21490;&#20316;&#20026;&#38750;&#32467;&#26500;&#21270;&#30340;&#20020;&#24202;&#25991;&#26412;&#26102;&#65292;&#35780;&#20272;&#35813;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#19968;&#32452;&#21253;&#21547;&#26631;&#20934;&#65288;&#20063;&#20197;&#33258;&#30001;&#25991;&#26412;&#24418;&#24335;&#25351;&#23450;&#65289;&#12290;&#25105;&#20204;&#30340;&#38646;&#26679;&#26412;&#31995;&#32479;&#22312;n2c2 2018&#38431;&#21015;&#36873;&#25321;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24471;&#20998;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#35782;&#21035;&#19968;&#31181;&#25552;&#31034;&#31574;&#30053;&#65292;&#25913;&#21892;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25968;&#25454;&#21644;&#25104;&#26412;&#25928;&#29575;&#65292;&#35813;&#31574;&#30053;&#19982;&#29616;&#29366;&#30456;&#27604;&#21487;&#20197;&#23558;&#24739;&#32773;&#21305;&#37197;&#26102;&#38388;&#21644;&#25104;&#26412;&#38477;&#20302;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#24320;&#21457;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26816;&#32034;&#27969;&#31243;&#65292;&#20943;&#23569;&#20102;&#21305;&#37197;&#28040;&#38500;&#30340;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Matching patients to clinical trials is a key unsolved challenge in bringing new drugs to market. Today, identifying patients who meet a trial's eligibility criteria is highly manual, taking up to 1 hour per patient. Automated screening is challenging, however, as it requires understanding unstructured clinical text. Large language models (LLMs) offer a promising solution. In this work, we explore their application to trial matching. First, we design an LLM-based system which, given a patient's medical history as unstructured clinical text, evaluates whether that patient meets a set of inclusion criteria (also specified as free text). Our zero-shot system achieves state-of-the-art scores on the n2c2 2018 cohort selection benchmark. Second, we improve the data and cost efficiency of our method by identifying a prompting strategy which matches patients an order of magnitude faster and more cheaply than the status quo, and develop a two-stage retrieval pipeline that reduces the number of 
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#23545;LLM&#25351;&#23548;&#35843;&#20248;&#30340;&#25968;&#25454;&#36873;&#25321;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#22312;&#25351;&#23548;&#35843;&#20248;&#36807;&#31243;&#20013;&#27604;&#25968;&#37327;&#26356;&#20026;&#37325;&#35201;&#65292;&#22240;&#27492;&#35768;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#25506;&#32034;&#20174;&#25351;&#23548;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#39640;&#36136;&#37327;&#23376;&#38598;&#30340;&#26041;&#27861;&#12290;&#35838;&#39064;&#21576;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#12289;&#20171;&#32461;&#20102;&#26368;&#36817;&#30340;&#30740;&#31350;&#36827;&#23637;&#24182;&#35814;&#32454;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05123</link><description>&lt;p&gt;
LLM&#25351;&#23548;&#35843;&#20248;&#30340;&#25968;&#25454;&#36873;&#25321;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Data Selection for LLM Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#23545;LLM&#25351;&#23548;&#35843;&#20248;&#30340;&#25968;&#25454;&#36873;&#25321;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#22312;&#25351;&#23548;&#35843;&#20248;&#36807;&#31243;&#20013;&#27604;&#25968;&#37327;&#26356;&#20026;&#37325;&#35201;&#65292;&#22240;&#27492;&#35768;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#25506;&#32034;&#20174;&#25351;&#23548;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#39640;&#36136;&#37327;&#23376;&#38598;&#30340;&#26041;&#27861;&#12290;&#35838;&#39064;&#21576;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#12289;&#20171;&#32461;&#20102;&#26368;&#36817;&#30340;&#30740;&#31350;&#36827;&#23637;&#24182;&#35814;&#32454;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#35843;&#20248;&#26159;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#22914;&#20309;&#25552;&#39640;&#25351;&#23548;&#35843;&#20248;&#30340;&#25928;&#26524;&#24050;&#32463;&#24341;&#36215;&#20102;&#22686;&#21152;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;LLM&#30340;&#25351;&#23548;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#27604;&#25968;&#37327;&#26356;&#20026;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#35768;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#25506;&#32034;&#20174;&#25351;&#23548;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#39640;&#36136;&#37327;&#23376;&#38598;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#24182;&#25913;&#21892;LLM&#30340;&#25351;&#23548;&#33021;&#21147;&#12290;&#26412;&#25991;&#23545;LLM&#25351;&#23548;&#35843;&#20248;&#30340;&#25968;&#25454;&#36873;&#25321;&#36827;&#34892;&#20102;&#32508;&#36848;&#12290;&#39318;&#20808;&#65292;&#20171;&#32461;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#35814;&#32454;&#20171;&#32461;&#20102;&#26368;&#36817;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#36824;&#35814;&#32454;&#38416;&#36848;&#20102;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#30340;&#35780;&#20272;&#31574;&#30053;&#21644;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#24378;&#35843;&#20102;&#35813;&#20219;&#21153;&#30340;&#24320;&#25918;&#25361;&#25112;&#21644;&#26032;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning is a vital step of training large language models (LLM), so how to enhance the effect of instruction tuning has received increased attention. Existing works indicate that the quality of the dataset is more crucial than the quantity during instruction tuning of LLM. Therefore, recently a lot of studies focus on exploring the methods of selecting high-quality subset from instruction datasets, aiming to reduce training costs and enhance the instruction-following capabilities of LLMs. This paper presents a comprehensive survey on data selection for LLM instruction tuning. Firstly, we introduce the wildly used instruction datasets. Then, we propose a new taxonomy of the data selection methods and provide a detailed introduction of recent advances,and the evaluation strategies and results of data selection methods are also elaborated in detail. Finally, we emphasize the open challenges and present new frontiers of this task.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22238;&#39038;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21457;&#23637;&#21382;&#31243;&#65292;&#20174;&#26368;&#21021;&#30340;&#22522;&#26412;&#35268;&#21017;&#31995;&#32479;&#21040;&#22914;&#20170;&#30001;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#39640;&#32423;&#23545;&#35805;&#26426;&#22120;&#20154;&#12290;&#30740;&#31350;&#31361;&#20986;&#20102;&#20851;&#38190;&#21019;&#26032;&#65292;&#22914;&#32842;&#22825;&#26426;&#22120;&#20154;ELIZA&#21644;ALICE&#65292;&#24182;&#25506;&#35752;&#20102;&#24433;&#21709;&#28436;&#36827;&#30340;&#37325;&#35201;&#37324;&#31243;&#30865;&#65292;&#20197;&#21450;Transformer&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.05122</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21382;&#21490;&#65306;&#36807;&#21435;&#12289;&#29616;&#22312;&#21644;&#26410;&#26469;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
History of generative Artificial Intelligence (AI) chatbots: past, present, and future development
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22238;&#39038;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21457;&#23637;&#21382;&#31243;&#65292;&#20174;&#26368;&#21021;&#30340;&#22522;&#26412;&#35268;&#21017;&#31995;&#32479;&#21040;&#22914;&#20170;&#30001;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#39640;&#32423;&#23545;&#35805;&#26426;&#22120;&#20154;&#12290;&#30740;&#31350;&#31361;&#20986;&#20102;&#20851;&#38190;&#21019;&#26032;&#65292;&#22914;&#32842;&#22825;&#26426;&#22120;&#20154;ELIZA&#21644;ALICE&#65292;&#24182;&#25506;&#35752;&#20102;&#24433;&#21709;&#28436;&#36827;&#30340;&#37325;&#35201;&#37324;&#31243;&#30865;&#65292;&#20197;&#21450;Transformer&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#28145;&#20837;&#22320;&#22238;&#39038;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#20174;&#26368;&#21021;&#20381;&#36182;&#35268;&#21017;&#30340;&#22522;&#26412;&#31995;&#32479;&#21040;&#22914;&#20170;&#30001;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#39640;&#32423;&#23545;&#35805;&#26426;&#22120;&#20154;&#12290;&#35813;&#35770;&#25991;&#36328;&#36234;&#20102;&#20960;&#21313;&#24180;&#30340;&#26102;&#38388;&#65292;&#25506;&#35752;&#20102;&#25512;&#21160;&#32842;&#22825;&#26426;&#22120;&#20154;&#28436;&#36827;&#30340;&#37325;&#35201;&#37324;&#31243;&#30865;&#12289;&#21019;&#26032;&#21644;&#33539;&#24335;&#36716;&#21464;&#12290;&#22238;&#39038;&#36215;&#28304;&#20110;1906&#24180;&#30340;&#22522;&#26412;&#32479;&#35745;&#27169;&#22411;&#65292;&#21040;&#19978;&#19990;&#32426;60&#24180;&#20195;&#21644;70&#24180;&#20195;&#30340;&#26089;&#26399;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20363;&#22914;ELIZA&#21644;ALICE&#65292;&#30740;&#31350;&#36861;&#36394;&#20102;&#23548;&#33268;&#22914;&#20170;&#39640;&#32423;&#23545;&#35805;&#26426;&#22120;&#20154;&#65288;&#22914;ChatGPT&#21644;Google Bard&#65289;&#30340;&#20851;&#38190;&#21019;&#26032;&#12290;&#35813;&#30740;&#31350;&#32508;&#21512;&#20102;&#23398;&#26415;&#25991;&#29486;&#21644;&#34892;&#19994;&#36164;&#26009;&#30340;&#35265;&#35299;&#65292;&#31361;&#20986;&#20102;&#22270;&#28789;&#27979;&#35797;&#30340;&#24341;&#20837;&#12289;CALO&#31561;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#39033;&#30446;&#20197;&#21450;&#26368;&#36817;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#12290;&#23637;&#26395;&#26410;&#26469;&#65292;&#26412;&#25991;&#24378;&#35843;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#22914;&#20309;&#34987;&#38598;&#25104;&#21040;&#29616;&#20195;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#26356;&#39640;&#32423;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research provides an in-depth comprehensive review of the progress of chatbot technology over time, from the initial basic systems relying on rules to today's advanced conversational bots powered by artificial intelligence. Spanning many decades, the paper explores the major milestones, innovations, and paradigm shifts that have driven the evolution of chatbots. Looking back at the very basic statistical model in 1906 via the early chatbots, such as ELIZA and ALICE in the 1960s and 1970s, the study traces key innovations leading to today's advanced conversational agents, such as ChatGPT and Google Bard. The study synthesizes insights from academic literature and industry sources to highlight crucial milestones, including the introduction of Turing tests, influential projects such as CALO, and recent transformer-based models. Tracing the path forward, the paper highlights how natural language processing and machine learning have been integrated into modern chatbots for more sophist
&lt;/p&gt;</description></item><item><title>&#35813;&#35843;&#26597;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;&#34920;&#26684;&#38382;&#39064;&#22238;&#31572;&#21644;&#20107;&#23454;&#39564;&#35777;&#65292;&#20197;&#21450;&#26032;&#20852;&#30340;&#34920;&#26684;&#25805;&#20316;&#21644;&#39640;&#32423;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#12290;&#36824;&#35752;&#35770;&#20102;LLMs&#30340;&#26368;&#26032;&#33539;&#20363;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#25351;&#23548;&#35843;&#25972;&#12289;&#25552;&#31034;&#21644;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05121</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Model for Table Processing: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05121
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35843;&#26597;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;&#34920;&#26684;&#38382;&#39064;&#22238;&#31572;&#21644;&#20107;&#23454;&#39564;&#35777;&#65292;&#20197;&#21450;&#26032;&#20852;&#30340;&#34920;&#26684;&#25805;&#20316;&#21644;&#39640;&#32423;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#12290;&#36824;&#35752;&#35770;&#20102;LLMs&#30340;&#26368;&#26032;&#33539;&#20363;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#25351;&#23548;&#35843;&#25972;&#12289;&#25552;&#31034;&#21644;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#36890;&#24120;&#26159;&#20108;&#32500;&#32467;&#26500;&#21270;&#30340;&#65292;&#29992;&#20110;&#23384;&#20648;&#22823;&#37327;&#25968;&#25454;&#65292;&#22312;&#25968;&#25454;&#24211;&#26597;&#35810;&#12289;&#30005;&#23376;&#34920;&#26684;&#35745;&#31639;&#21644;&#20174;&#32593;&#32476;&#34920;&#26684;&#29983;&#25104;&#25253;&#21578;&#31561;&#26085;&#24120;&#27963;&#21160;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#21270;&#36825;&#20123;&#20197;&#34920;&#26684;&#20026;&#20013;&#24515;&#30340;&#20219;&#21153;&#21487;&#20197;&#24102;&#26469;&#37325;&#22823;&#30340;&#20844;&#20247;&#21033;&#30410;&#65292;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#20852;&#36259;&#12290;&#35813;&#35843;&#26597;&#23545;&#34920;&#26684;&#20219;&#21153;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27010;&#36848;&#65292;&#19981;&#20165;&#28085;&#30422;&#20256;&#32479;&#39046;&#22495;&#22914;&#34920;&#26684;&#38382;&#39064;&#22238;&#31572;&#65288;Table QA&#65289;&#21644;&#20107;&#23454;&#39564;&#35777;&#65292;&#36824;&#21253;&#25324;&#26368;&#36817;&#24378;&#35843;&#30340;&#26041;&#38754;&#65292;&#22914;&#34920;&#26684;&#25805;&#20316;&#21644;&#39640;&#32423;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#36229;&#36234;&#20102;&#26089;&#26399;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;LLM&#20351;&#29992;&#20013;&#30340;&#26368;&#26032;&#33539;&#20363;&#12290;&#37325;&#28857;&#26159;LLMs&#39046;&#22495;&#20869;&#30340;&#25351;&#23548;&#35843;&#25972;&#12289;&#25552;&#31034;&#21644;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#20960;&#20010;&#25361;&#25112;&#65292;&#28085;&#30422;&#31169;&#26377;&#37096;&#32626;&#12289;&#39640;&#25928;&#25512;&#26029;&#21644; LLMS &#21457;&#23637;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tables, typically two-dimensional and structured to store large amounts of data, are essential in daily activities like database queries, spreadsheet calculations, and generating reports from web tables. Automating these table-centric tasks with Large Language Models (LLMs) offers significant public benefits, garnering interest from academia and industry. This survey provides an extensive overview of table tasks, encompassing not only the traditional areas like table question answering (Table QA) and fact verification, but also newly emphasized aspects such as table manipulation and advanced table data analysis. Additionally, it goes beyond the early strategies of pre-training and fine-tuning small language models, to include recent paradigms in LLM usage. The focus here is particularly on instruction-tuning, prompting, and agent-based approaches within the realm of LLMs. Finally, we highlight several challenges, ranging from private deployment and efficient inference to the developmen
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#27604;&#20363;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#37319;&#26679;&#21644;&#25237;&#31080;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#24615;&#33021;&#65292;&#36825;&#31181;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#22797;&#26434;&#26041;&#27861;&#27491;&#20132;&#12290;</title><link>https://arxiv.org/abs/2402.05120</link><description>&lt;p&gt;
&#26356;&#22810;&#30340;&#20195;&#29702;&#23601;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
More Agents Is All You Need
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05120
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#27604;&#20363;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#37319;&#26679;&#21644;&#25237;&#31080;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#24615;&#33021;&#65292;&#36825;&#31181;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#22797;&#26434;&#26041;&#27861;&#27491;&#20132;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#36890;&#36807;&#19968;&#31181;&#37319;&#26679;&#21644;&#25237;&#31080;&#30340;&#26041;&#27861;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Large Language Models, LLMs)&#30340;&#24615;&#33021;&#19982;&#23454;&#20363;&#21270;&#30340;&#20195;&#29702;&#25968;&#37327;&#25104;&#27604;&#20363;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#26041;&#27861;&#23545;&#24050;&#26377;&#30340;&#22797;&#26434;&#26041;&#27861;&#36827;&#19968;&#27493;&#22686;&#24378;LLMs&#26159;&#27491;&#20132;&#30340;&#65292;&#32780;&#22686;&#24378;&#30340;&#31243;&#24230;&#19982;&#20219;&#21153;&#30340;&#22256;&#38590;&#31243;&#24230;&#30456;&#20851;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#24182;&#30740;&#31350;&#20102;&#33021;&#22815;&#20419;&#36827;&#20854;&#21457;&#29983;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#22312;&#20197;&#19979;&#32593;&#22336;: \url{https://anonymous.4open.science/r/more_agent_is_all_you_need}
&lt;/p&gt;
&lt;p&gt;
We find that, simply via a sampling-and-voting method, the performance of large language models (LLMs) scales with the number of agents instantiated. Also, this method is orthogonal to existing complicated methods to further enhance LLMs, while the degree of enhancement is correlated to the task difficulty. We conduct comprehensive experiments on a wide range of LLM benchmarks to verify the presence of our finding, and to study the properties that can facilitate its occurrence. Our code is publicly available at: \url{https://anonymous.4open.science/r/more_agent_is_all_you_need}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#21644;&#20998;&#26512;&#25581;&#31034;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#22810;&#20010;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#26080;&#27861;&#22686;&#24378;LLM&#30340;&#30693;&#35782;&#21644;&#25216;&#33021;&#12289;&#20174;&#20855;&#26377;&#30693;&#35782;&#26469;&#28304;&#30340;&#25968;&#25454;&#38598;&#22797;&#21046;&#22238;&#24212;&#27169;&#24335;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#12289;&#20840;&#21442;&#25968;&#24494;&#35843;&#22686;&#21152;&#20102;&#38169;&#35823;&#29983;&#25104;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.05119</link><description>&lt;p&gt;
&#30740;&#31350;&#25351;&#20196;&#35843;&#25972;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at the Limitations of Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#21644;&#20998;&#26512;&#25581;&#31034;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#22810;&#20010;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#26080;&#27861;&#22686;&#24378;LLM&#30340;&#30693;&#35782;&#21644;&#25216;&#33021;&#12289;&#20174;&#20855;&#26377;&#30693;&#35782;&#26469;&#28304;&#30340;&#25968;&#25454;&#38598;&#22797;&#21046;&#22238;&#24212;&#27169;&#24335;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#12289;&#20840;&#21442;&#25968;&#24494;&#35843;&#22686;&#21152;&#20102;&#38169;&#35823;&#29983;&#25104;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#65288;IT&#65289;&#26159;&#20351;&#29992;&#25351;&#20196;-&#22238;&#24212;&#23545;&#26469;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36807;&#31243;&#65292;&#24050;&#25104;&#20026;&#23558;&#22522;&#30784;&#39044;&#35757;&#32451;LLM&#36716;&#21270;&#20026;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#20195;&#29702;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#34429;&#28982;IT&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#24182;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#20854;&#23616;&#38480;&#24615;&#21644;&#19981;&#36275;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#39564;&#21644;&#23545;LLM&#36890;&#36807;IT&#21457;&#29983;&#30340;&#21464;&#21270;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;IT&#30340;&#22810;&#31181;&#23616;&#38480;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65306;&#65288;1&#65289;IT&#26080;&#27861;&#22686;&#24378;LLM&#30340;&#30693;&#35782;&#25110;&#25216;&#33021;&#12290;LoRA&#24494;&#35843;&#20165;&#38480;&#20110;&#23398;&#20064;&#22238;&#24212;&#30340;&#21551;&#21160;&#21644;&#26679;&#24335;&#20196;&#29260;&#65292;&#32780;&#20840;&#21442;&#25968;&#24494;&#35843;&#20250;&#23548;&#33268;&#30693;&#35782;&#36864;&#21270;&#12290;&#65288;2&#65289;&#20174;&#20855;&#26377;&#30693;&#35782;&#26469;&#28304;&#30340;IT&#25968;&#25454;&#38598;&#22797;&#21046;&#22238;&#24212;&#27169;&#24335;&#20250;&#23548;&#33268;&#22238;&#24212;&#36136;&#37327;&#19979;&#38477;&#12290;&#65288;3&#65289;&#20840;&#21442;&#25968;&#24494;&#35843;&#36890;&#36807;&#19981;&#20934;&#30830;&#22320;&#20174;IT&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#27010;&#24565;&#19978;&#30456;&#20284;&#23454;&#20363;&#30340;&#26631;&#35760;&#65292;&#22686;&#21152;&#20102;&#38169;&#35823;&#29983;&#25104;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning (IT), the process of training large language models (LLMs) using instruction-response pairs, has emerged as the predominant method for transforming base pre-trained LLMs into open-domain conversational agents. While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, we reveal various limitations of IT. In particular, we show that (1) IT fails to enhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning response initiation and style tokens, and full-parameter fine-tuning leads to knowledge degradation. (2) Copying response patterns from IT datasets derived from knowledgeable sources leads to a decline in response quality. (3) Full-parameter fine-tuning increases hallucination by inaccurately borrowing tokens from conceptually similar instances in the IT dataset for generating respon
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#35780;&#20272;ChatGPT&#21644;Google Bard&#29983;&#25104;&#30340;&#20869;&#23481;&#19982;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#20313;&#24358;&#25991;&#26723;&#30456;&#20284;&#24615;&#26041;&#38754;&#65292;ChatGPT&#34920;&#29616;&#20248;&#20110;Google Bard&#12290;</title><link>https://arxiv.org/abs/2402.05116</link><description>&lt;p&gt;
&#37327;&#21270;&#30456;&#20284;&#24615;&#65306;&#20351;&#29992;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#35780;&#20272;ChatGPT&#21644;Google Bard&#29983;&#25104;&#20869;&#23481;&#19982;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#30340;&#20851;&#32852;&#24615;
&lt;/p&gt;
&lt;p&gt;
Quantifying Similarity: Text-Mining Approaches to Evaluate ChatGPT and Google Bard Content in Relation to BioMedical Literature
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#35780;&#20272;ChatGPT&#21644;Google Bard&#29983;&#25104;&#30340;&#20869;&#23481;&#19982;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#20313;&#24358;&#25991;&#26723;&#30456;&#20284;&#24615;&#26041;&#38754;&#65292;ChatGPT&#34920;&#29616;&#20248;&#20110;Google Bard&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25903;&#25345;&#19979;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#20986;&#29616;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#29983;&#25104;&#20869;&#23481;&#33021;&#21147;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#35780;&#20272;&#36890;&#36807;&#25152;&#35859;&#30340;&#25552;&#31034;&#24037;&#31243;&#29983;&#25104;&#30340;&#20869;&#23481;&#30340;&#26377;&#29992;&#24615;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#30446;&#26631;&#65306;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#30340;&#24179;&#22343;&#20540;&#65292;&#25105;&#20204;&#35780;&#20272;&#36825;&#20123;&#20869;&#23481;&#19982;&#31185;&#23398;&#23478;&#20135;&#29983;&#30340;&#30495;&#23454;&#25991;&#29486;&#30340;&#30456;&#20284;&#24615;&#21644;&#25509;&#36817;&#31243;&#24230;&#12290;&#26041;&#27861;&#65306;&#22312;&#36825;&#20010;&#25506;&#32034;&#24615;&#20998;&#26512;&#20013;&#65292;&#65288;1&#65289;&#25105;&#20204;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#26469;&#29983;&#25104;ChatGPT&#21644;Google Bard&#30340;&#20020;&#24202;&#20869;&#23481;&#65292;&#20197;&#20415;&#19982;&#25991;&#29486;&#23545;&#24212;&#20869;&#23481;&#36827;&#34892;&#27604;&#36739;&#65292;&#65288;2&#65289;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#25152;&#29983;&#25104;&#20869;&#23481;&#19982;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#23545;&#24212;&#20869;&#23481;&#30340;&#30456;&#20284;&#24615;&#26469;&#35780;&#20272;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#27604;&#36739;&#25991;&#26723;&#21644;&#30456;&#20851;&#30340;&#20108;&#20803;&#32452;&#65292;&#24182;&#20351;&#29992;&#32593;&#32476;&#20998;&#26512;&#26469;&#35780;&#20272;&#26415;&#35821;&#30340;&#20013;&#24515;&#24615;&#12290;&#32467;&#26524;&#65306;&#23454;&#39564;&#34920;&#26126;&#65292;ChatGPT&#22312;&#20313;&#24358;&#25991;&#26723;&#30456;&#20284;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;Google Bard&#65288;38%&#23545;34%&#65289;&#65292;
&lt;/p&gt;
&lt;p&gt;
Background: The emergence of generative AI tools, empowered by Large Language Models (LLMs), has shown powerful capabilities in generating content. To date, the assessment of the usefulness of such content, generated by what is known as prompt engineering, has become an interesting research question. Objectives Using the mean of prompt engineering, we assess the similarity and closeness of such contents to real literature produced by scientists. Methods In this exploratory analysis, (1) we prompt-engineer ChatGPT and Google Bard to generate clinical content to be compared with literature counterparts, (2) we assess the similarities of the contents generated by comparing them with counterparts from biomedical literature. Our approach is to use text-mining approaches to compare documents and associated bigrams and to use network analysis to assess the terms' centrality. Results The experiments demonstrated that ChatGPT outperformed Google Bard in cosine document similarity (38% to 34%), 
&lt;/p&gt;</description></item><item><title>SALAD-Bench&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#23433;&#20840;&#22522;&#20934;&#65292;&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#30340;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#20197;&#21450;&#23545;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#26377;&#25928;&#31649;&#29702;&#21644;&#20445;&#25252;&#12290;</title><link>https://arxiv.org/abs/2402.05044</link><description>&lt;p&gt;
SALAD-Bench: &#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#23618;&#27425;&#21270;&#21644;&#20840;&#38754;&#24615;&#23433;&#20840;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05044
&lt;/p&gt;
&lt;p&gt;
SALAD-Bench&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#23433;&#20840;&#22522;&#20934;&#65292;&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#30340;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#20197;&#21450;&#23545;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#26377;&#25928;&#31649;&#29702;&#21644;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#20013;&#65292;&#30830;&#20445;&#24378;&#22823;&#30340;&#23433;&#20840;&#25514;&#26045;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#20851;&#38190;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;LLMs&#12289;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#23433;&#20840;&#22522;&#20934;&#65292;&#31216;&#20026;SALAD-Bench&#12290;SALAD-Bench&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#22810;&#26679;&#30340;&#29305;&#24615;&#65292;&#20197;&#21450;&#36328;&#19977;&#20010;&#23618;&#27425;&#30340;&#32454;&#33268;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#22522;&#20934;&#12290;SALAD-Bench&#36890;&#36807;&#23545;&#26631;&#20934;&#26597;&#35810;&#21644;&#22797;&#26434;&#26597;&#35810;&#65288;&#21253;&#25324;&#25915;&#20987;&#12289;&#38450;&#24481;&#20462;&#25913;&#21644;&#22810;&#39033;&#36873;&#25321;&#65289;&#30340;&#31934;&#24515;&#35774;&#35745;&#65292;&#26377;&#25928;&#31649;&#29702;&#20854;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#30830;&#20445;&#26080;&#32541;&#21487;&#38752;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35780;&#20272;&#22120;&#65306;&#22522;&#20110;LLM&#30340;MD-Judge&#65292;&#19987;&#27880;&#20110;&#25915;&#20987;&#22686;&#24378;&#26597;&#35810;&#30340;&#38382;&#31572;&#23545;&#35780;&#20272;&#12290;&#20197;&#19978;&#32452;&#20214;&#23558;SALAD-Bench&#20174;&#26631;&#20934;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#25193;&#23637;&#21040;&#20102;LLM&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#35780;&#20272;&#65292;&#30830;&#20445;&#20102;&#32852;&#21512;&#30446;&#26631;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose \emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#33258;&#25105;&#35299;&#37322;&#20013;&#30340;&#20449;&#23454;&#24615;&#19982;&#21487;&#20449;&#24230;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#34429;&#28982;&#36825;&#20123;&#35299;&#37322;&#22312;&#36923;&#36753;&#19978;&#26159;&#21512;&#20046;&#24773;&#29702;&#19988;&#36830;&#36143;&#30340;&#65292;&#20294;&#24182;&#19981;&#19968;&#23450;&#19982;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#19968;&#33268;&#65292;&#24341;&#21457;&#23545;&#20854;&#20449;&#23454;&#24615;&#30340;&#25285;&#24551;&#12290;</title><link>https://arxiv.org/abs/2402.04614</link><description>&lt;p&gt;
&#20449;&#23454;&#24615;&#19982;&#21487;&#20449;&#24230;: &#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#30340;(&#19981;)&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#33258;&#25105;&#35299;&#37322;&#20013;&#30340;&#20449;&#23454;&#24615;&#19982;&#21487;&#20449;&#24230;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#34429;&#28982;&#36825;&#20123;&#35299;&#37322;&#22312;&#36923;&#36753;&#19978;&#26159;&#21512;&#20046;&#24773;&#29702;&#19988;&#36830;&#36143;&#30340;&#65292;&#20294;&#24182;&#19981;&#19968;&#23450;&#19982;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#19968;&#33268;&#65292;&#24341;&#21457;&#23545;&#20854;&#20449;&#23454;&#24615;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34987;&#37096;&#32626;&#20026;&#20960;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#29616;&#20195;LLMs&#21487;&#20197;&#29983;&#25104;&#33258;&#25105;&#35299;&#37322;&#65288;SEs&#65289;&#65292;&#36825;&#20123;SEs&#25581;&#31034;&#20102;&#23427;&#20204;&#35299;&#37322;&#20854;&#34892;&#20026;&#30340;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#12290;&#30001;&#20110;&#20854;&#23545;&#35805;&#24615;&#21644;&#21487;&#20449;&#24230;&#30340;&#29305;&#28857;&#65292;&#33258;&#25105;&#35299;&#37322;&#24050;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#20854;&#20449;&#23454;&#24615;&#20102;&#35299;&#29978;&#23569;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;LLMs&#29983;&#25104;&#30340;SEs&#20013;&#20449;&#23454;&#24615;&#21644;&#21487;&#20449;&#24230;&#20043;&#38388;&#30340;&#20108;&#20998;&#27861;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#34429;&#28982;LLMs&#25797;&#38271;&#29983;&#25104;&#21487;&#20449;&#30340;&#35299;&#37322;-&#23545;&#20154;&#31867;&#29992;&#25143;&#26469;&#35828;&#20284;&#20046;&#36923;&#36753;&#21644;&#36830;&#36143;-&#20294;&#36825;&#20123;&#35299;&#37322;&#26410;&#24517;&#19982;LLMs&#30340;&#25512;&#29702;&#36807;&#31243;&#30456;&#19968;&#33268;&#65292;&#24341;&#21457;&#23545;&#20854;&#20449;&#23454;&#24615;&#30340;&#25285;&#24551;&#12290;&#25105;&#20204;&#24378;&#35843;&#65292;&#24403;&#21069;&#36235;&#21183;&#26159;&#20026;&#20102;&#29992;&#25143;&#21451;&#22909;&#30028;&#38754;&#30340;&#38656;&#27714;&#32780;&#22686;&#21152;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#21487;&#33021;&#20250;&#20197;&#38477;&#20302;&#35299;&#37322;&#30340;&#20449;&#23454;&#24615;&#20026;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are deployed as powerful tools for several natural language processing (NLP) applications. Recent works show that modern LLMs can generate self-explanations (SEs), which elicit their intermediate reasoning steps for explaining their behavior. Self-explanations have seen widespread adoption owing to their conversational and plausible nature. However, there is little to no understanding of their faithfulness. In this work, we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs. We argue that while LLMs are adept at generating plausible explanations -- seemingly logical and coherent to human users -- these explanations do not necessarily align with the reasoning processes of the LLMs, raising concerns about their faithfulness. We highlight that the current trend towards increasing the plausibility of explanations, primarily driven by the demand for user-friendly interfaces, may come at the cost of diminishing their faithfulness
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#35875;&#35328;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#26469;&#25945;&#23548;LLMs&#29702;&#35299;&#26032;&#38395;&#21644;&#35780;&#35770;&#20013;&#30340;&#20851;&#38190;&#32447;&#32034;&#65292;&#24182;&#23558;&#20256;&#25773;&#20449;&#24687;&#20998;&#35299;&#20026;&#20256;&#25773;&#38142;&#65292;&#25105;&#20204;&#30340;LeRuD&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#35875;&#35328;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#22791;&#22312;&#23569;&#26679;&#26412;&#25110;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#26356;&#26377;&#28508;&#21147;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.03916</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#35875;&#35328;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Detect Rumors on Social Media?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#35875;&#35328;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#26469;&#25945;&#23548;LLMs&#29702;&#35299;&#26032;&#38395;&#21644;&#35780;&#35770;&#20013;&#30340;&#20851;&#38190;&#32447;&#32034;&#65292;&#24182;&#23558;&#20256;&#25773;&#20449;&#24687;&#20998;&#35299;&#20026;&#20256;&#25773;&#38142;&#65292;&#25105;&#20204;&#30340;LeRuD&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#35875;&#35328;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#22791;&#22312;&#23569;&#26679;&#26412;&#25110;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#26356;&#26377;&#28508;&#21147;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#36827;&#34892;&#35875;&#35328;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#25512;&#29702;&#25972;&#20010;&#20256;&#25773;&#20449;&#24687;&#26102;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#35813;&#20449;&#24687;&#21253;&#21547;&#26032;&#38395;&#20869;&#23481;&#21644;&#22823;&#37327;&#35780;&#35770;&#65292;LLMs&#21487;&#33021;&#26080;&#27861;&#38598;&#20013;&#20851;&#27880;&#22797;&#26434;&#20256;&#25773;&#20449;&#24687;&#20013;&#30340;&#20851;&#38190;&#32447;&#32034;&#65292;&#24182;&#19988;&#22312;&#38754;&#23545;&#22823;&#37327;&#21644;&#20887;&#20313;&#20449;&#24687;&#26102;&#38590;&#20197;&#36827;&#34892;&#25512;&#29702;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#22686;&#24378;&#30340;&#35875;&#35328;&#26816;&#27979;&#65288;LeRuD&#65289;&#26041;&#27861;&#65292;&#22312;&#20854;&#20013;&#35774;&#35745;&#25552;&#31034;&#26469;&#25945;&#23548;LLMs&#20851;&#27880;&#26032;&#38395;&#21644;&#35780;&#35770;&#20013;&#30340;&#37325;&#35201;&#32447;&#32034;&#65292;&#24182;&#23558;&#25972;&#20010;&#20256;&#25773;&#20449;&#24687;&#20998;&#35299;&#20026;&#20256;&#25773;&#38142;&#20197;&#20943;&#36731;LLMs&#30340;&#36127;&#25285;&#12290;&#25105;&#20204;&#22312;Twitter&#21644;&#24494;&#21338;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;LeRuD&#30340;&#24615;&#33021;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#35875;&#35328;&#26816;&#27979;&#27169;&#22411;&#65292;&#25552;&#21319;&#20102;2.4&#65285;&#33267;7.6&#65285;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#24212;&#29992;LLMs&#65292;LeRuD&#26080;&#38656;&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#23569;&#26679;&#26412;&#25110;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#26356;&#20855;&#26377;&#28508;&#21147;&#30340;&#35875;&#35328;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate to use Large Language Models (LLMs) for rumor detection on social media. However, it is challenging for LLMs to reason over the entire propagation information on social media, which contains news contents and numerous comments, due to LLMs may not concentrate on key clues in the complex propagation information, and have trouble in reasoning when facing massive and redundant information. Accordingly, we propose an LLM-empowered Rumor Detection (LeRuD) approach, in which we design prompts to teach LLMs to reason over important clues in news and comments, and divide the entire propagation information into a Chain-of-Propagation for reducing LLMs' burden. We conduct extensive experiments on the Twitter and Weibo datasets, and LeRuD outperforms several state-of-the-art rumor detection models by 2.4% to 7.6%. Meanwhile, by applying LLMs, LeRuD requires no data for training, and thus shows more promising rumor detection ability in few-shot or zero-shot scenarios.
&lt;/p&gt;</description></item><item><title>BGE M3-&#23884;&#20837;&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#21516;&#26102;&#25191;&#34892;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#24182;&#33021;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#12290;&#20854;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20102;&#19968;&#31181;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21644;&#20248;&#21270;&#30340;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.03216</link><description>&lt;p&gt;
BGE M3-&#23884;&#20837;&#65306;&#36890;&#36807;&#33258;&#30693;&#35782;&#33976;&#39311;&#23454;&#29616;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03216
&lt;/p&gt;
&lt;p&gt;
BGE M3-&#23884;&#20837;&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#21516;&#26102;&#25191;&#34892;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#24182;&#33021;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#12290;&#20854;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20102;&#19968;&#31181;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21644;&#20248;&#21270;&#30340;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#31216;&#20026;M3-&#23884;&#20837;&#65292;&#20197;&#20854;&#22312;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#32780;&#33879;&#31216;&#12290;&#23427;&#21487;&#20197;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#23427;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#23884;&#20837;&#27169;&#22411;&#30340;&#19977;&#31181;&#24120;&#35265;&#26816;&#32034;&#21151;&#33021;&#65306;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#20026;&#29616;&#23454;&#19990;&#30028;&#30340;IR&#24212;&#29992;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#27169;&#22411;&#22522;&#30784;&#12290;&#23427;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#65292;&#20174;&#30701;&#21477;&#21040;&#38271;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#25991;&#26723;&#12290;M3-&#23884;&#20837;&#30340;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20197;&#19979;&#25216;&#26415;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#26469;&#33258;&#19981;&#21516;&#26816;&#32034;&#21151;&#33021;&#30340;&#30456;&#20851;&#24615;&#20998;&#25968;&#25972;&#21512;&#20026;&#25945;&#24072;&#20449;&#21495;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#20248;&#21270;&#20102;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications. It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. The effective training of M3-Embedding involves the following technical contributions. We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;GIRT-&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#24320;&#21457;&#32773;&#25351;&#31034;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#25253;&#21578;&#27169;&#26495;&#30340;&#21161;&#29702;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;GitHub&#20179;&#24211;&#20013;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#65292;GIRT-&#27169;&#22411;&#22312;IRT&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02632</link><description>&lt;p&gt;
GIRT-&#27169;&#22411;&#65306;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#25253;&#21578;&#27169;&#26495;
&lt;/p&gt;
&lt;p&gt;
GIRT-Model: Automated Generation of Issue Report Templates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;GIRT-&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#24320;&#21457;&#32773;&#25351;&#31034;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#25253;&#21578;&#27169;&#26495;&#30340;&#21161;&#29702;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;GitHub&#20179;&#24211;&#20013;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#65292;GIRT-&#27169;&#22411;&#22312;IRT&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GitHub&#21644;GitLab&#31561;&#24179;&#21488;&#24341;&#20837;&#38382;&#39064;&#25253;&#21578;&#27169;&#26495;&#65288;IRT&#65289;&#20197;&#25552;&#39640;&#38382;&#39064;&#31649;&#29702;&#25928;&#29575;&#24182;&#26356;&#22909;&#22320;&#19982;&#24320;&#21457;&#32773;&#26399;&#26395;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20179;&#24211;&#24182;&#26410;&#24191;&#27867;&#37319;&#29992;&#36825;&#20123;&#27169;&#26495;&#65292;&#24182;&#19988;&#30446;&#21069;&#27809;&#26377;&#21487;&#29992;&#30340;&#24037;&#20855;&#26469;&#36741;&#21161;&#24320;&#21457;&#32773;&#29983;&#25104;&#27169;&#26495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GIRT-&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#24320;&#21457;&#32773;&#20851;&#20110;&#32467;&#26500;&#21644;&#24517;&#38656;&#23383;&#27573;&#30340;&#25351;&#31034;&#33258;&#21160;&#29983;&#25104;IRT&#30340;&#21161;&#29702;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;GIRT-Instruct&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#25351;&#31034;&#21644;IRT&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;IRT&#26469;&#33258;GitHub&#23384;&#20648;&#24211;&#12290;&#25105;&#20204;&#20351;&#29992;GIRT-Instruct&#26469;&#25351;&#23548;&#35843;&#25972;T5-base&#27169;&#22411;&#20197;&#21019;&#24314;GIRT-&#27169;&#22411;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;GIRT-&#27169;&#22411;&#22312;IRT&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;&#24102;&#26377;&#19981;&#21516;&#21442;&#25968;&#22823;&#23567;&#30340;T5&#21644;Flan-T5&#65289;&#65292;&#22312;ROUGE&#12289;BLEU&#12289;METEOR&#21644;&#20154;&#24037;&#35780;&#20272;&#19978;&#24471;&#20998;&#26174;&#33879;&#26356;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;GIRT-&#27169;&#22411;&#22312;&#29992;&#25143;&#20307;&#39564;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Platforms such as GitHub and GitLab introduce Issue Report Templates (IRTs) to enable more effective issue management and better alignment with developer expectations. However, these templates are not widely adopted in most repositories, and there is currently no tool available to aid developers in generating them. In this work, we introduce GIRT-Model, an assistant language model that automatically generates IRTs based on the developer's instructions regarding the structure and necessary fields. We create GIRT-Instruct, a dataset comprising pairs of instructions and IRTs, with the IRTs sourced from GitHub repositories. We use GIRT-Instruct to instruction-tune a T5-base model to create the GIRT-Model. In our experiments, GIRT-Model outperforms general language models (T5 and Flan-T5 with different parameter sizes) in IRT generation by achieving significantly higher scores in ROUGE, BLEU, METEOR, and human evaluation. Additionally, we analyze the effectiveness of GIRT-Model in a user st
&lt;/p&gt;</description></item><item><title>APT-Pipe is an automated prompt-tuning tool that enhances ChatGPT's text classification performance by automatically tuning prompts on any given dataset, resulting in a significant improvement in weighted F1-score across twelve experimented datasets.</title><link>https://arxiv.org/abs/2402.01697</link><description>&lt;p&gt;
APT-Pipe: &#29992;&#20110;&#31038;&#20132;&#35745;&#31639;&#25968;&#25454;&#26631;&#27880;&#30340;&#33258;&#21160;&#25552;&#31034;&#35843;&#25972;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
APT-Pipe: An Automatic Prompt-Tuning Tool for Social Computing Data Annotation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01697
&lt;/p&gt;
&lt;p&gt;
APT-Pipe is an automated prompt-tuning tool that enhances ChatGPT's text classification performance by automatically tuning prompts on any given dataset, resulting in a significant improvement in weighted F1-score across twelve experimented datasets.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#20687;ChatGPT&#36825;&#26679;&#30340;LLM&#24212;&#29992;&#22312;&#31038;&#20132;&#35745;&#31639;&#25991;&#26412;&#26631;&#27880;&#20013;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#20154;&#20204;&#24050;&#32463;&#30693;&#36947;&#24615;&#33021;&#21462;&#20915;&#20110;&#36755;&#20837;&#25552;&#31034;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#26377;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#26469;&#25506;&#32034;&#25552;&#31034;&#35843;&#25972;&#30340;&#25216;&#26415;&#21644;&#25351;&#21335;&#65292;&#35797;&#22270;&#25913;&#21892;&#25552;&#31034;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#25163;&#24037;&#21162;&#21147;&#21644;&#23545;&#27491;&#22312;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#25552;&#31034;&#35843;&#25972;&#27969;&#27700;&#32447;APT-Pipe&#12290;APT-Pipe&#26088;&#22312;&#33258;&#21160;&#35843;&#25972;&#25552;&#31034;&#65292;&#20197;&#25552;&#39640;ChatGPT&#22312;&#20219;&#20309;&#32473;&#23450;&#25968;&#25454;&#38598;&#19978;&#30340;&#25991;&#26412;&#20998;&#31867;&#24615;&#33021;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;APT-Pipe&#65292;&#24182;&#22312;12&#20010;&#19981;&#21516;&#30340;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;APT-Pipe&#35843;&#25972;&#30340;&#25552;&#31034;&#26377;&#21161;&#20110;ChatGPT&#22312;12&#20010;&#23454;&#39564;&#25968;&#25454;&#38598;&#20013;&#26377;9&#20010;&#33719;&#24471;&#26356;&#39640;&#30340;&#21152;&#26435;F1&#20998;&#25968;&#65292;&#24179;&#22343;&#25913;&#36827;&#20102;7.01&#65285;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#31361;&#20986;&#20102;APT-Pipe&#20316;&#20026;&#19968;&#20010;&#26694;&#26550;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has highlighted the potential of LLM applications, like ChatGPT, for performing label annotation on social computing text. However, it is already well known that performance hinges on the quality of the input prompts. To address this, there has been a flurry of research into prompt tuning -- techniques and guidelines that attempt to improve the quality of prompts. Yet these largely rely on manual effort and prior knowledge of the dataset being annotated. To address this limitation, we propose APT-Pipe, an automated prompt-tuning pipeline. APT-Pipe aims to automatically tune prompts to enhance ChatGPT's text classification performance on any given dataset. We implement APT-Pipe and test it across twelve distinct text classification datasets. We find that prompts tuned by APT-Pipe help ChatGPT achieve higher weighted F1-score on nine out of twelve experimented datasets, with an improvement of 7.01% on average. We further highlight APT-Pipe's flexibility as a framework by 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#31216;&#20026;ReAGent&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#36866;&#29992;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#35745;&#31639;&#19978;&#25928;&#29575;&#26356;&#39640;&#30340;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2402.00794</link><description>&lt;p&gt;
ReAGent: &#19968;&#20010;&#38754;&#21521;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ReAGent: Towards A Model-agnostic Feature Attribution Method for Generative Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#31216;&#20026;ReAGent&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#36866;&#29992;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#35745;&#31639;&#19978;&#25928;&#29575;&#26356;&#39640;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65288;FAs&#65289;&#65292;&#22914;&#26799;&#24230;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#30830;&#23450;&#25152;&#26377;&#36755;&#20837;&#29305;&#24449;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;&#29616;&#26377;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20026;&#20165;&#26377;&#32534;&#30721;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#24320;&#21457;&#21644;&#27979;&#35797;FAs&#65292;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23578;&#19981;&#28165;&#26970;&#22312;&#25991;&#26412;&#29983;&#25104;&#19978;&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;FAs&#26469;&#22788;&#29702;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#22240;&#20026;&#27169;&#22411;&#26550;&#26500;&#21644;&#20219;&#21153;&#35774;&#32622;&#20043;&#38388;&#23384;&#22312;&#22266;&#26377;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#65292;&#27809;&#26377;&#19968;&#20010;&#36890;&#29992;&#30340;FA&#36866;&#29992;&#20110;&#25152;&#26377;&#27169;&#22411;&#21644;&#20219;&#21153;&#12290;&#36825;&#20351;&#24471;&#38024;&#23545;&#22823;&#22411;LMs&#36873;&#25321;FA&#35745;&#31639;&#19978;&#38750;&#24120;&#26114;&#36149;&#65292;&#22240;&#20026;&#36755;&#20837;&#37325;&#35201;&#24615;&#30340;&#25512;&#23548;&#36890;&#24120;&#38656;&#35201;&#22810;&#20010;&#21069;&#21521;&#21644;&#21453;&#21521;&#20256;&#36882;&#65292;&#21253;&#25324;&#21487;&#33021;&#26159;&#38480;&#21046;&#24615;&#30340;&#26799;&#24230;&#35745;&#31639;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#29983;&#25104;LMs&#30340;&#27169;&#22411;&#26080;&#20851;FA&#65292;&#31216;&#20026;&#36882;&#24402;&#24402;&#22240;&#29983;&#25104;&#22120;&#65288;ReAGent&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature attribution methods (FAs), such as gradients and attention, are widely employed approaches to derive the importance of all input features to the model predictions. Existing work in natural language processing has mostly focused on developing and testing FAs for encoder-only language models (LMs) in classification tasks. However, it is unknown if it is faithful to use these FAs for decoder-only models on text generation, due to the inherent differences between model architectures and task settings respectively. Moreover, previous work has demonstrated that there is no `one-wins-all' FA across models and tasks. This makes the selection of a FA computationally expensive for large LMs since input importance derivation often requires multiple forward and backward passes including gradient computations that might be prohibitive even with access to large compute. To address these issues, we present a model-agnostic FA for generative LMs called Recursive Attribution Generator (ReAGent)
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#20581;&#24247;-LLM&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#29305;&#24449;&#25552;&#21462;&#21644;&#21307;&#23398;&#30693;&#35782;&#26435;&#34913;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#30340;&#26816;&#32034;&#22686;&#24378;&#30142;&#30149;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#20581;&#24247;&#25253;&#21578;&#65292;&#35843;&#25972;&#29305;&#24449;&#26435;&#37325;&#65292;&#20197;&#21450;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#23478;&#35265;&#35299;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#19982;&#20256;&#32479;&#20581;&#24247;&#31649;&#29702;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.00746</link><description>&lt;p&gt;
&#20581;&#24247;-LLM&#65306;&#20010;&#24615;&#21270;&#26816;&#32034;&#22686;&#24378;&#30340;&#30142;&#30149;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Health-LLM: Personalized Retrieval-Augmented Disease Prediction Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00746
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#20581;&#24247;-LLM&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#29305;&#24449;&#25552;&#21462;&#21644;&#21307;&#23398;&#30693;&#35782;&#26435;&#34913;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#30340;&#26816;&#32034;&#22686;&#24378;&#30142;&#30149;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#20581;&#24247;&#25253;&#21578;&#65292;&#35843;&#25972;&#29305;&#24449;&#26435;&#37325;&#65292;&#20197;&#21450;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#23478;&#35265;&#35299;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#19982;&#20256;&#32479;&#20581;&#24247;&#31649;&#29702;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21355;&#29983;&#20445;&#20581;&#39046;&#22495;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26234;&#33021;&#21307;&#30103;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26234;&#33021;&#21307;&#30103;&#21463;&#38480;&#20110;&#38745;&#24577;&#25968;&#25454;&#21644;&#32479;&#19968;&#26631;&#20934;&#65292;&#26080;&#27861;&#23436;&#20840;&#19982;&#20010;&#20307;&#24773;&#20917;&#38598;&#25104;&#65292;&#21516;&#26102;&#20063;&#38754;&#20020;&#20854;&#20182;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#20581;&#24247;-LLM&#65292;&#23558;&#22823;&#35268;&#27169;&#29305;&#24449;&#25552;&#21462;&#21644;&#21307;&#23398;&#30693;&#35782;&#26435;&#34913;&#35780;&#20998;&#30456;&#32467;&#21512;&#12290;&#19982;&#20256;&#32479;&#20581;&#24247;&#31649;&#29702;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#19977;&#20010;&#20027;&#35201;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20581;&#24247;&#25253;&#21578;&#25972;&#21512;&#21040;&#22823;&#27169;&#22411;&#20013;&#65292;&#25552;&#20379;&#35814;&#32454;&#30340;&#20219;&#21153;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#19987;&#19994;&#30340;&#21307;&#23398;&#19987;&#19994;&#30693;&#35782;&#35843;&#25972;&#20581;&#24247;&#29305;&#24449;&#30340;&#26435;&#37325;&#24471;&#20998;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20351;&#29992;&#21322;&#33258;&#21160;&#29305;&#24449;&#25552;&#21462;&#26694;&#26550;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#26512;&#33021;&#21147;&#65292;&#24182;&#25972;&#21512;&#19987;&#23478;&#35265;&#35299;&#20197;&#25552;&#39640;&#30142;&#30149;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) in healthcare has significantly advanced intelligent medical treatment. However, traditional intelligent healthcare is limited by static data and unified standards, preventing full integration with individual situations and other challenges. Hence, a more professional and detailed intelligent healthcare method is needed for development. To this end, we propose an innovative framework named Heath-LLM, which combines large-scale feature extraction and medical knowledge trade-off scoring. Compared to traditional health management methods, our approach has three main advantages. First, our method integrates health reports into a large model to provide detailed task information. Second, professional medical expertise is used to adjust the weighted scores of health characteristics. Third, we use a semi-automated feature extraction framework to enhance the analytical power of language models and incorporate expert insights to improve the accuracy of disease predic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#25490;&#21517;&#26597;&#35810;&#21644;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26597;&#35810;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#22914;&#21307;&#23398;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#20854;&#20182;&#26377;&#36259;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.14877</link><description>&lt;p&gt;
&#20351;&#29992;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#31283;&#20581;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Robust Knowledge Extraction from Large Language Models using Social Choice Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#25490;&#21517;&#26597;&#35810;&#21644;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26597;&#35810;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#22914;&#21307;&#23398;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#20854;&#20182;&#26377;&#36259;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#25903;&#25345;&#24456;&#22810;&#24212;&#29992;&#65292;&#22914;&#23545;&#35805;&#20195;&#29702;&#12289;&#21019;&#24847;&#20889;&#20316;&#25110;&#19968;&#33324;&#26597;&#35810;&#22238;&#31572;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#65288;&#22914;&#21307;&#23398;&#65289;&#20013;&#65292;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#26597;&#35810;&#22238;&#31572;&#65292;&#22240;&#20026;&#24403;&#22810;&#27425;&#25552;&#31034;&#30456;&#21516;&#26597;&#35810;&#26102;&#65292;&#23427;&#20204;&#36890;&#24120;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615; - &#32467;&#26524;&#21487;&#33021;&#19981;&#21516;&#12290;&#20026;&#20102;&#25552;&#39640;LLM&#26597;&#35810;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#25490;&#21517;&#26597;&#35810;&#65292;&#24182;&#20351;&#29992;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#30340;&#26041;&#27861;&#26469;&#27719;&#24635;&#26597;&#35810;&#32467;&#26524;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#35786;&#26029;&#22330;&#26223;&#20013;&#30340;&#25490;&#21517;&#26597;&#35810;&#65292;&#22914;&#21307;&#23398;&#21644;&#25925;&#38556;&#35786;&#26029;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#24212;&#29992;&#25991;&#29486;&#20013;&#30340;Partial Borda Choice&#20989;&#25968;&#26469;&#21512;&#24182;&#22810;&#20010;&#26597;&#35810;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#25105;&#20204;&#35774;&#32622;&#20013;&#30340;&#19968;&#20123;&#20854;&#20182;&#26377;&#36259;&#23646;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-language models (LLMs) can support a wide range of applications like conversational agents, creative writing or general query answering. However, they are ill-suited for query answering in high-stake domains like medicine because they are typically not robust - even the same query can result in different answers when prompted multiple times. In order to improve the robustness of LLM queries, we propose using ranking queries repeatedly and to aggregate the queries using methods from social choice theory. We study ranking queries in diagnostic settings like medical and fault diagnosis and discuss how the Partial Borda Choice function from the literature can be applied to merge multiple query results. We discuss some additional interesting properties in our setting and evaluate the robustness of our approach empirically.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#24341;&#20837;&#20102;&#8220;&#31038;&#20250;&#23398;&#20064;&#8221;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30456;&#20114;&#20849;&#20139;&#30693;&#35782;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#30693;&#35782;&#20256;&#36882;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.11441</link><description>&lt;p&gt;
&#31038;&#20250;&#23398;&#20064;&#65306;&#26397;&#30528;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Social Learning: Towards Collaborative Learning with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#24341;&#20837;&#20102;&#8220;&#31038;&#20250;&#23398;&#20064;&#8221;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30456;&#20114;&#20849;&#20139;&#30693;&#35782;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#30693;&#35782;&#20256;&#36882;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32972;&#26223;&#19979;&#24341;&#20837;&#20102;&#8220;&#31038;&#20250;&#23398;&#20064;&#8221;&#30340;&#26694;&#26550;&#65292;&#22312;&#38544;&#31169;&#20445;&#25252;&#30340;&#21069;&#25552;&#19979;&#65292;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#30456;&#20114;&#20849;&#20139;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#20004;&#31181;&#22312;LLMs&#20043;&#38388;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#26041;&#27861;&#12290;&#22312;&#31532;&#19968;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20801;&#35768;&#27169;&#22411;&#29983;&#25104;&#25277;&#35937;&#25552;&#31034;&#20197;&#20415;&#25945;&#25480;&#20219;&#21153;&#12290;&#22312;&#31532;&#20108;&#31181;&#26041;&#27861;&#20013;&#65292;&#27169;&#22411;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#31034;&#20363;&#26469;&#20256;&#36882;&#30693;&#35782;&#12290;&#25105;&#20204;&#36328;&#22810;&#20010;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#24182;&#20197;&#35760;&#24518;&#21270;&#20316;&#20026;&#38544;&#31169;&#25439;&#22833;&#30340;&#20195;&#29702;&#36827;&#34892;&#37327;&#21270;&#12290;&#36825;&#20123;&#21463;&#21040;&#31038;&#20250;&#23398;&#20064;&#21551;&#21457;&#30340;&#25216;&#26415;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#21407;&#22987;&#25968;&#25454;&#30340;&#35760;&#24518;&#21270;&#31243;&#24230;&#36739;&#20302;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#20351;&#29992;&#21407;&#22987;&#26631;&#31614;&#21644;&#25552;&#31034;&#30340;&#32467;&#26524;&#30456;&#23218;&#32654;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35777;&#26126;&#20102;&#31038;&#20250;&#23398;&#20064;&#22312;LLMs&#20013;&#30340;&#21487;&#34892;&#24615;&#65292;&#24314;&#31435;&#20102;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#31361;&#20986;&#20102;&#20960;&#20010;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#20379;&#26410;&#26469;&#30740;&#31350;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the framework of "social learning" in the context of large language models (LLMs), whereby models share knowledge with each other in a privacy-aware manner using natural language. We present and evaluate two approaches for knowledge transfer between LLMs. In the first scenario, we allow the model to generate abstract prompts aiming to teach the task. In our second approach, models transfer knowledge by generating synthetic examples. We evaluate these methods across diverse datasets and quantify memorization as a proxy for privacy loss. These techniques inspired by social learning yield promising results with low memorization of the original data. In particular, we show that performance using these methods is comparable to results with the use of original labels and prompts. Our work demonstrates the viability of social learning for LLMs, establishes baseline approaches and highlights several unexplored areas for future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#19982;&#35895;&#27468;&#21161;&#25163;&#21644;Siri&#30340;&#20132;&#20114;&#65292;&#25506;&#35752;&#20102;&#34394;&#25311;&#21161;&#25163;&#19982;&#29992;&#25143;&#20043;&#38388;&#30340;&#23545;&#35805;&#20462;&#22797;&#20013;&#20132;&#20114;&#35821;&#35328;&#30340;&#37325;&#35201;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#21161;&#25163;&#29983;&#25104;&#20102;&#20960;&#31181;&#20462;&#22797;&#31574;&#30053;&#65292;&#20294;&#26080;&#27861;&#22797;&#21046;&#20154;&#31867;&#31867;&#20284;&#30340;&#20462;&#22797;&#31574;&#30053;&#65292;&#22914;&#8220;&#21999;&#65311;&#8221;&#12290;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#29992;&#25143;&#21487;&#25509;&#21463;&#24615;&#35843;&#26597;&#26174;&#31034;&#20102;&#29992;&#25143;&#23545;&#20462;&#22797;&#31574;&#30053;&#30340;&#20559;&#22909;&#21644;&#21161;&#25163;&#20351;&#29992;&#30340;&#24046;&#24322;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#20154;&#26426;&#20132;&#20114;&#20013;&#20132;&#20114;&#35821;&#35328;&#19982;&#20154;&#38469;&#20132;&#20114;&#20043;&#38388;&#30340;&#19981;&#24179;&#31561;&#65292;&#24378;&#35843;&#20102;&#23545;&#20132;&#20114;&#35821;&#35328;&#24433;&#21709;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.03952</link><description>&lt;p&gt;
&#35821;&#38899;&#21161;&#25163;&#23545;&#35805;&#20462;&#22797;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Analysis of Dialogue Repair in Voice Assistants
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#19982;&#35895;&#27468;&#21161;&#25163;&#21644;Siri&#30340;&#20132;&#20114;&#65292;&#25506;&#35752;&#20102;&#34394;&#25311;&#21161;&#25163;&#19982;&#29992;&#25143;&#20043;&#38388;&#30340;&#23545;&#35805;&#20462;&#22797;&#20013;&#20132;&#20114;&#35821;&#35328;&#30340;&#37325;&#35201;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#21161;&#25163;&#29983;&#25104;&#20102;&#20960;&#31181;&#20462;&#22797;&#31574;&#30053;&#65292;&#20294;&#26080;&#27861;&#22797;&#21046;&#20154;&#31867;&#31867;&#20284;&#30340;&#20462;&#22797;&#31574;&#30053;&#65292;&#22914;&#8220;&#21999;&#65311;&#8221;&#12290;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#29992;&#25143;&#21487;&#25509;&#21463;&#24615;&#35843;&#26597;&#26174;&#31034;&#20102;&#29992;&#25143;&#23545;&#20462;&#22797;&#31574;&#30053;&#30340;&#20559;&#22909;&#21644;&#21161;&#25163;&#20351;&#29992;&#30340;&#24046;&#24322;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#20154;&#26426;&#20132;&#20114;&#20013;&#20132;&#20114;&#35821;&#35328;&#19982;&#20154;&#38469;&#20132;&#20114;&#20043;&#38388;&#30340;&#19981;&#24179;&#31561;&#65292;&#24378;&#35843;&#20102;&#23545;&#20132;&#20114;&#35821;&#35328;&#24433;&#21709;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#36890;&#36807;&#23454;&#26102;&#21709;&#24212;&#26597;&#35810;&#25913;&#21464;&#20102;&#20154;&#26426;&#20132;&#20114;&#65292;&#20294;&#26159;&#29992;&#25143;&#19982;&#31995;&#32479;&#20043;&#38388;&#30340;&#35823;&#35299;&#20173;&#28982;&#23384;&#22312;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#19982;&#35895;&#27468;&#21161;&#25163;&#21644;Siri&#30340;&#20132;&#20114;&#65292;&#25506;&#35752;&#34394;&#25311;&#21161;&#25163;&#19982;&#29992;&#25143;&#20043;&#38388;&#30340;&#23545;&#35805;&#20462;&#22797;&#20013;&#20132;&#20114;&#35821;&#35328;&#30340;&#37325;&#35201;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#22312;&#20154;&#38469;&#20132;&#20114;&#20013;&#24120;&#35265;&#30340;&#23545;&#35805;&#20462;&#22797;&#31574;&#30053;&#8220;&#21999;&#65311;&#8221;&#30340;&#20351;&#29992;&#21644;&#22238;&#24212;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#21161;&#25163;&#29983;&#25104;&#30340;&#20960;&#31181;&#20462;&#22797;&#31574;&#30053;&#65292;&#20294;&#26080;&#27861;&#22797;&#21046;&#20154;&#31867;&#31867;&#20284;&#30340;&#20462;&#22797;&#31574;&#30053;&#65292;&#22914;&#8220;&#21999;&#65311;&#8221;&#12290;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#29992;&#25143;&#21487;&#25509;&#21463;&#24615;&#35843;&#26597;&#26174;&#31034;&#20102;&#29992;&#25143;&#23545;&#20462;&#22797;&#31574;&#30053;&#30340;&#20559;&#22909;&#21644;&#21161;&#25163;&#20351;&#29992;&#30340;&#24046;&#24322;&#65292;&#22312;&#36825;&#20004;&#31181;&#35843;&#26597;&#35821;&#35328;&#20013;&#37117;&#23384;&#22312;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#20154;&#26426;&#20132;&#20114;&#20013;&#20132;&#20114;&#35821;&#35328;&#19982;&#20154;&#38469;&#20132;&#20114;&#20043;&#38388;&#30340;&#19981;&#24179;&#31561;&#65292;&#24378;&#35843;&#20102;&#23545;&#20132;&#20114;&#35821;&#35328;&#24433;&#21709;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spoken dialogue systems have transformed human-machine interaction by providing real-time responses to queries. However, misunderstandings between the user and system persist. This study explores the significance of interactional language in dialogue repair between virtual assistants and users by analyzing interactions with Google Assistant and Siri, focusing on their utilization and response to the other-initiated repair strategy "huh?" prevalent in human-human interaction. Findings reveal several assistant-generated strategies but an inability to replicate human-like repair strategies such as "huh?". English and Spanish user acceptability surveys show differences in users' repair strategy preferences and assistant usage, with both similarities and disparities among the two surveyed languages. These results shed light on inequalities between interactional language in human-human interaction and human-machine interaction, underscoring the need for further research on the impact of inte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#23558;&#35821;&#38899;&#36716;&#25442;&#22120;&#20316;&#20026;&#38899;&#39057;&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#21482;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#20063;&#33021;&#23454;&#29616;&#31867;&#20284;&#30340;&#25928;&#26524;&#65292;&#23588;&#20854;&#19982;&#20302;&#20301;&#26435;&#37325;&#37327;&#21270;&#25216;&#26415;&#32467;&#21512;&#20351;&#29992;&#26102;&#25928;&#26524;&#26356;&#22909;&#12290;&#36825;&#19968;&#21457;&#29616;&#26377;&#21161;&#20110;&#38450;&#27490;&#38169;&#35823;&#22312;&#37327;&#21270;&#27169;&#22359;&#20043;&#38388;&#20256;&#25773;&#12290;</title><link>https://arxiv.org/abs/2311.02772</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#36824;&#26159;&#21367;&#31215;&#65306;&#29992;&#20110;&#25512;&#26029;&#25928;&#29575;&#30340;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#22312;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Attention or Convolution: Transformer Encoders in Audio Language Models for Inference Efficiency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#23558;&#35821;&#38899;&#36716;&#25442;&#22120;&#20316;&#20026;&#38899;&#39057;&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#21482;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#20063;&#33021;&#23454;&#29616;&#31867;&#20284;&#30340;&#25928;&#26524;&#65292;&#23588;&#20854;&#19982;&#20302;&#20301;&#26435;&#37325;&#37327;&#21270;&#25216;&#26415;&#32467;&#21512;&#20351;&#29992;&#26102;&#25928;&#26524;&#26356;&#22909;&#12290;&#36825;&#19968;&#21457;&#29616;&#26377;&#21161;&#20110;&#38450;&#27490;&#38169;&#35823;&#22312;&#37327;&#21270;&#27169;&#22359;&#20043;&#38388;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#38899;&#39057;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#19982;&#26356;&#22797;&#26434;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#20855;&#26377;&#35821;&#38899;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#65289;&#30456;&#24403;&#30340;&#25512;&#26029;&#25928;&#29575;&#12290;&#36825;&#20123;&#35821;&#38899;&#36716;&#25442;&#22120;&#32467;&#21512;&#20102;&#21367;&#31215;&#27169;&#22359;&#21644;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#22312;ASR&#26041;&#38754;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#21644;&#26368;&#39640;&#25928;&#29575;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#23558;&#36825;&#20123;&#35821;&#38899;&#36716;&#25442;&#22120;&#20316;&#20026;&#32534;&#30721;&#22120;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#38899;&#39057;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;&#20808;&#36827;&#30340;&#33258;&#27880;&#24847;&#21147;&#20063;&#21487;&#20197;&#23454;&#29616;&#21487;&#27604;&#36739;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#29305;&#21035;&#26377;&#21033;&#20110;&#20302;&#20301;&#26435;&#37325;&#37327;&#21270;&#25216;&#26415;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#21487;&#20197;&#38450;&#27490;&#22312;&#26368;&#36817;&#30340;&#35821;&#38899;&#36716;&#25442;&#22120;&#20013;&#28151;&#21512;&#37327;&#21270;&#21367;&#31215;&#21644;&#37327;&#21270;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#26102;&#22312;&#19981;&#21516;&#37327;&#21270;&#27169;&#22359;&#20043;&#38388;&#20256;&#25773;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we show that a simple self-supervised pre-trained audio model can achieve comparable inference efficiency to more complicated pre-trained models with speech transformer encoders. These speech transformers rely on mixing convolutional modules with self-attention modules. They achieve state-of-the-art performance on ASR with top efficiency. We first show that employing these speech transformers as an encoder significantly improves the efficiency of pre-trained audio models as well. However, our study shows that we can achieve comparable efficiency with advanced self-attention solely. We demonstrate that this simpler approach is particularly beneficial with a low-bit weight quantization technique of a neural network to improve efficiency. We hypothesize that it prevents propagating the errors between different quantized modules compared to recent speech transformers mixing quantized convolution and the quantized self-attention modules.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#20197;&#21450;&#21033;&#29992;&#26410;&#26631;&#35760;&#26032;&#38395;&#25991;&#31456;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#20027;&#39064;&#26080;&#20851;&#21644;&#20027;&#39064;&#24863;&#30693;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#22312;&#31435;&#22330;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;0.771&#30340;F1&#20998;&#25968;&#12290;</title><link>https://arxiv.org/abs/2310.14450</link><description>&lt;p&gt;
TATA: &#36890;&#36807;&#20027;&#39064;&#26080;&#20851;&#21644;&#20027;&#39064;&#24863;&#30693;&#30340;&#23884;&#20837;&#36827;&#34892;&#31435;&#22330;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
TATA: Stance Detection via Topic-Agnostic and Topic-Aware Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#20197;&#21450;&#21033;&#29992;&#26410;&#26631;&#35760;&#26032;&#38395;&#25991;&#31456;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#20027;&#39064;&#26080;&#20851;&#21644;&#20027;&#39064;&#24863;&#30693;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#22312;&#31435;&#22330;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;0.771&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31435;&#22330;&#26816;&#27979;&#23545;&#20110;&#29702;&#35299;&#20114;&#32852;&#32593;&#19978;&#19981;&#21516;&#30340;&#24577;&#24230;&#21644;&#20449;&#20208;&#24456;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#19968;&#31687;&#25991;&#31456;&#23545;&#32473;&#23450;&#20027;&#39064;&#30340;&#31435;&#22330;&#24448;&#24448;&#39640;&#24230;&#20381;&#36182;&#20110;&#35813;&#20027;&#39064;&#65292;&#24314;&#31435;&#19968;&#20010;&#33021;&#25512;&#24191;&#21040;&#26410;&#30693;&#20027;&#39064;&#30340;&#31435;&#22330;&#26816;&#27979;&#27169;&#22411;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#20197;&#21450;&#28085;&#30422;&#20102;&#21508;&#31181;&#19981;&#21516;&#20027;&#39064;&#30340;&#26410;&#26631;&#35760;&#26032;&#38395;&#25991;&#31456;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#29992;&#20110;&#19979;&#28216;&#31435;&#22330;&#26816;&#27979;&#30340;&#20027;&#39064;&#26080;&#20851;&#65288;TAG&#65289;&#21644;&#20027;&#39064;&#24863;&#30693;&#65288;TAW&#65289;&#30340;&#23884;&#20837;&#12290;&#23558;&#36825;&#20123;&#23884;&#20837;&#32452;&#21512;&#22312;&#25105;&#20204;&#30340;&#23436;&#25972;TATA&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#20844;&#24320;&#30340;&#31435;&#22330;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65288;&#22312;Zero-shot VAST&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;0.771&#30340;F1&#20998;&#25968;&#65289;&#12290;&#25105;&#20204;&#22312;https://github.com/hanshanley/tata&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stance detection is important for understanding different attitudes and beliefs on the Internet. However, given that a passage's stance toward a given topic is often highly dependent on that topic, building a stance detection model that generalizes to unseen topics is difficult. In this work, we propose using contrastive learning as well as an unlabeled dataset of news articles that cover a variety of different topics to train topic-agnostic/TAG and topic-aware/TAW embeddings for use in downstream stance detection. Combining these embeddings in our full TATA model, we achieve state-of-the-art performance across several public stance detection datasets (0.771 $F_1$-score on the Zero-shot VAST dataset). We release our code and data at https://github.com/hanshanley/tata.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Transformer in Transformer (TinT)&#30340;&#39640;&#25928;&#26500;&#36896;&#26041;&#24335;&#65292;&#23427;&#21487;&#20197;&#35753;Transformer&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#27169;&#25311;&#21644;&#24494;&#35843;&#22797;&#26434;&#30340;&#20869;&#37096;&#27169;&#22411;&#65292;&#21516;&#26102;&#20351;&#29992;&#21019;&#26032;&#30340;&#36817;&#20284;&#25216;&#26415;&#22823;&#24133;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#21644;&#20869;&#23384;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2307.01189</link><description>&lt;p&gt;
&#21487;&#35757;&#32451;&#30340;Transformer in Transformer
&lt;/p&gt;
&lt;p&gt;
Trainable Transformer in Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.01189
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Transformer in Transformer (TinT)&#30340;&#39640;&#25928;&#26500;&#36896;&#26041;&#24335;&#65292;&#23427;&#21487;&#20197;&#35753;Transformer&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#27169;&#25311;&#21644;&#24494;&#35843;&#22797;&#26434;&#30340;&#20869;&#37096;&#27169;&#22411;&#65292;&#21516;&#26102;&#20351;&#29992;&#21019;&#26032;&#30340;&#36817;&#20284;&#25216;&#26415;&#22823;&#24133;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#21644;&#20869;&#23384;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#24402;&#22240;&#20110;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#38544;&#24335;&#27169;&#25311;&#21644;&#24494;&#35843;&#20869;&#37096;&#27169;&#22411;&#65288;&#22914;&#32447;&#24615;&#25110;2&#23618;MLP&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26500;&#36896;&#38656;&#35201;&#22823;&#37327;&#30340;&#20869;&#23384;&#24320;&#38144;&#65292;&#20351;&#24471;&#27169;&#25311;&#26356;&#22797;&#26434;&#30340;&#20869;&#37096;&#27169;&#22411;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26500;&#36896;&#26041;&#24335;&#65292;&#31216;&#20026;Transformer in Transformer&#65288;&#31616;&#31216;TinT&#65289;&#65292;&#23427;&#20801;&#35768;&#19968;&#20010;Transformer&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#27169;&#25311;&#21644;&#24494;&#35843;&#22797;&#26434;&#30340;&#20869;&#37096;&#27169;&#22411;&#65288;&#22914;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65289;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21019;&#26032;&#30340;&#36817;&#20284;&#25216;&#26415;&#65292;&#20351;&#24471;&#19968;&#20010;&#25317;&#26377;&#19981;&#21040;20&#20159;&#21442;&#25968;&#30340;TinT&#27169;&#22411;&#33021;&#22815;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#27169;&#25311;&#21644;&#24494;&#35843;&#19968;&#20010;&#25317;&#26377;1.25&#20159;&#21442;&#25968;&#30340;Transformer&#27169;&#22411;&#12290;TinT&#36866;&#29992;&#20110;&#35768;&#22810;&#24120;&#35265;&#30340;Transformer&#21464;&#20307;&#65292;&#20854;&#35774;&#35745;&#24605;&#36335;&#36824;&#25913;&#36827;&#20102;Transformer&#20013;&#31616;&#21333;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#31471;&#21040;&#31471;&#23454;&#39564;&#26469;&#39564;&#35777;...
&lt;/p&gt;
&lt;p&gt;
Recent works attribute the capability of in-context learning (ICL) in large pre-trained language models to implicitly simulating and fine-tuning an internal model (e.g., linear or 2-layer MLP) during inference. However, such constructions require large memory overhead, which makes simulation of more sophisticated internal models intractable. In this work, we propose an efficient construction, Transformer in Transformer (in short, TinT), that allows a transformer to simulate and fine-tune complex models internally during inference (e.g., pre-trained language models). In particular, we introduce innovative approximation techniques that allow a TinT model with less than 2 billion parameters to simulate and fine-tune a 125 million parameter transformer model within a single forward pass. TinT accommodates many common transformer variants and its design ideas also improve the efficiency of past instantiations of simple models inside transformers. We conduct end-to-end experiments to validat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;LLMs&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#20219;&#21153;&#20013;&#30340;&#20195;&#29702;&#21147;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#34913;&#37327;&#21644;&#31649;&#29702;LLMs&#20195;&#29702;&#21147;&#30340;&#29305;&#24449;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2305.12815</link><description>&lt;p&gt;
&#25506;&#31350;LLMs&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#20219;&#21153;&#20013;&#30340;&#20195;&#29702;&#21147;
&lt;/p&gt;
&lt;p&gt;
Investigating Agency of LLMs in Human-AI Collaboration Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.12815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;LLMs&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#20219;&#21153;&#20013;&#30340;&#20195;&#29702;&#21147;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#34913;&#37327;&#21644;&#31649;&#29702;LLMs&#20195;&#29702;&#21147;&#30340;&#29305;&#24449;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#29702;&#21147;&#26159;&#20154;&#31867;&#20114;&#21160;&#21644;&#21327;&#20316;&#30340;&#26680;&#24515;&#33021;&#21147;&#12290;&#34429;&#28982;LLMs&#34987;&#24320;&#21457;&#25104;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#24182;&#20316;&#20026;&#31867;&#20154;&#20195;&#29702;&#20351;&#29992;&#65292;&#20294;&#23545;&#36825;&#20123;&#27169;&#22411;&#24212;&#20855;&#26377;&#30340;&#20195;&#29702;&#21147;&#30340;&#20851;&#27880;&#21364;&#19981;&#36275;&#65292;&#20197;&#20415;&#20027;&#21160;&#31649;&#29702;&#20114;&#21160;&#21644;&#21327;&#20316;&#30340;&#26041;&#21521;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20195;&#29702;&#21147;&#20316;&#20026;LLMs&#30340;&#19968;&#31181;&#29702;&#24819;&#21151;&#33021;&#30340;&#24615;&#36136;&#65292;&#20197;&#21450;&#22914;&#20309;&#34913;&#37327;&#21644;&#31649;&#29702;&#20195;&#29702;&#21147;&#12290;&#25105;&#20204;&#20511;&#37492;&#31038;&#20250;&#35748;&#30693;&#29702;&#35770;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#29305;&#24449;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#35805;&#34920;&#36798;&#20195;&#29702;&#21147;&#65292;&#34920;&#31034;&#24744;&#25171;&#31639;&#20570;&#20160;&#20040;&#65288;&#24847;&#21521;&#24615;&#65289;&#65292;&#28608;&#21457;&#24744;&#30340;&#24847;&#22270;&#65288;&#21160;&#26426;&#65289;&#65292;&#23545;&#24847;&#21521;&#26377;&#33258;&#20449;&#65288;&#33258;&#25105;&#25928;&#33021;&#65289;&#65292;&#24182;&#33021;&#22815;&#33258;&#25105;&#35843;&#25972;&#65288;&#33258;&#25105;&#35843;&#33410;&#65289;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;908&#20010;&#23545;&#20154;&#23545;&#35805;&#29255;&#27573;&#30340;&#27880;&#37322;&#65292;&#29992;&#20110;&#26631;&#35760;&#20195;&#29702;&#21147;&#29305;&#24449;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#34913;&#37327;LLMs&#20195;&#29702;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agency, the capacity to proactively shape events, is central to how humans interact and collaborate. While LLMs are being developed to simulate human behavior and serve as human-like agents, little attention has been given to the Agency that these models should possess in order to proactively manage the direction of interaction and collaboration. In this paper, we investigate Agency as a desirable function of LLMs, and how it can be measured and managed. We build on social-cognitive theory to develop a framework of features through which Agency is expressed in dialogue - indicating what you intend to do (Intentionality), motivating your intentions (Motivation), having self-belief in intentions (Self-Efficacy), and being able to self-adjust (Self-Regulation). We collect a new dataset of 83 human-human collaborative interior design conversations containing 908 conversational snippets annotated for Agency features. Using this dataset, we develop methods for measuring Agency of LLMs. Autom
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"PiVe"&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#39564;&#35777;&#26469;&#25552;&#21319;LLMs&#30340;&#22522;&#20110;&#22270;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PiVe&#26041;&#27861;&#22312;&#19977;&#20010;&#22522;&#20110;&#22270;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#25913;&#21892;&#65292;&#24182;&#19988;&#39564;&#35777;&#27169;&#22359;&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#24037;&#20855;&#24110;&#21161;&#25552;&#39640;&#32467;&#26524;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2305.12392</link><description>&lt;p&gt;
PiVe&#65306;&#36890;&#36807;&#36845;&#20195;&#39564;&#35777;&#25552;&#21319;LLMs&#30340;&#22522;&#20110;&#22270;&#30340;&#29983;&#25104;&#33021;&#21147;&#30340;&#25552;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.12392
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"PiVe"&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#39564;&#35777;&#26469;&#25552;&#21319;LLMs&#30340;&#22522;&#20110;&#22270;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PiVe&#26041;&#27861;&#22312;&#19977;&#20010;&#22522;&#20110;&#22270;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#25913;&#21892;&#65292;&#24182;&#19988;&#39564;&#35777;&#27169;&#22359;&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#24037;&#20855;&#24110;&#21161;&#25552;&#39640;&#32467;&#26524;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35299;&#20915;&#21508;&#31181;&#19981;&#21516;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;LLMs&#30340;&#35757;&#32451;&#30446;&#26631;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;LLMs&#23545;&#28041;&#21450;&#32467;&#26500;&#21270;&#25968;&#25454;&#29983;&#25104;&#30340;&#20219;&#21153;&#24182;&#19981;&#38750;&#24120;&#36866;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"PiVe"&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#39564;&#35777;&#26469;&#25552;&#21319;LLMs&#30340;&#22522;&#20110;&#22270;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#35757;&#32451;&#19968;&#20010;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;LLMs&#30340;&#36755;&#20986;&#30340;&#39564;&#35777;&#27169;&#22359;(&#20363;&#22914;ChatGPT&#65292;GPT-4)&#65292;&#36890;&#36807;&#31934;&#32454;&#30340;&#32416;&#27491;&#25351;&#20196;&#26469;&#36845;&#20195;&#25913;&#36827;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#39564;&#35777;&#27169;&#22359;&#22914;&#20309;&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#24212;&#29992;&#36845;&#20195;&#26657;&#27491;&#65292;&#20197;&#33719;&#24471;&#26356;&#32463;&#27982;&#39640;&#25928;&#30340;&#25991;&#26412;&#21040;&#22270;&#24418;&#29983;&#25104;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#19977;&#20010;&#22522;&#20110;&#22270;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;PiVe&#30340;&#26041;&#27861;&#33021;&#22815;&#25345;&#32493;&#25913;&#21892;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;GenWiki-HIQ&#25968;&#25454;&#38598;&#65292;&#24182;&#24378;&#35843;&#39564;&#35777;&#27169;&#22359;&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#24037;&#20855;&#65292;&#24110;&#21161;&#25552;&#39640;&#33258;&#21160;&#29983;&#25104;&#30340;&#32467;&#26524;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown great abilities of solving various natural language tasks in different domains. Due to the training objective of LLMs and their pre-training data, LLMs are not very well equipped for tasks involving structured data generation. We propose a framework, Prompting with Iterative Verification (PiVe), to improve graph-based generative capability of LLMs. We show how a small language model could be trained to act as a verifier module for the output of an LLM(i.e., ChatGPT, GPT-4), and to iteratively improve its performance via fine-grained corrective instructions. We also show how the verifier module could apply iterative corrections offline for a more cost-effective solution to the text-to-graph generation task. Experiments on three graph-based datasets show consistent improvement gained via PiVe. Additionally, we create GenWiki-HIQ and highlight that the verifier module can be used as a data augmentation tool to help improve the quality of automatical
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25193;&#23637;&#20102;AI&#25945;&#36741;Kwame&#65292;&#38754;&#21521;&#35199;&#38750;&#31185;&#23398;&#25945;&#32946;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#19968;&#20010;&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#36827;&#34892;&#20102;&#23454;&#38469;&#37096;&#32626;&#21644;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#65292;Kwame for Science&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#29992;&#25143;&#21644;&#38382;&#39064;&#25552;&#38382;&#37327;&#12290;</title><link>https://arxiv.org/abs/2302.10786</link><description>&lt;p&gt;
&#38754;&#21521;&#35199;&#38750;&#31185;&#23398;&#25945;&#32946;&#30340;AI&#25945;&#36741;Kwame&#30340;&#29616;&#23454;&#19990;&#30028;&#37096;&#32626;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Real-World Deployment and Evaluation of Kwame for Science, An AI Teaching Assistant for Science Education in West Africa
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.10786
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25193;&#23637;&#20102;AI&#25945;&#36741;Kwame&#65292;&#38754;&#21521;&#35199;&#38750;&#31185;&#23398;&#25945;&#32946;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#19968;&#20010;&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#36827;&#34892;&#20102;&#23454;&#38469;&#37096;&#32626;&#21644;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#65292;Kwame for Science&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#29992;&#25143;&#21644;&#38382;&#39064;&#25552;&#38382;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#27954;&#25945;&#24072;&#19982;&#23398;&#29983;&#30340;&#27604;&#20363;&#39640;&#65292;&#36825;&#38480;&#21046;&#20102;&#23398;&#29983;&#20204;&#33719;&#21462;&#25945;&#32946;&#38382;&#39064;&#35299;&#31572;&#31561;&#23398;&#20064;&#25903;&#25345;&#30340;&#26426;&#20250;&#12290;&#26412;&#30740;&#31350;&#23558;&#38754;&#21521;&#32534;&#30721;&#25945;&#32946;&#30340;AI&#25945;&#36741;Kwame&#25193;&#23637;&#20026;&#38754;&#21521;&#31185;&#23398;&#25945;&#32946;&#65292;&#24182;&#23558;&#20854;&#37096;&#32626;&#20026;&#19968;&#20010;&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#12290;Kwame for Science&#36890;&#36807;&#25552;&#20379;&#26469;&#33258;&#31934;&#36873;&#30693;&#35782;&#26469;&#28304;&#30340;&#27573;&#33853;&#20197;&#21450;&#22522;&#20110;&#35199;&#38750;&#39640;&#32423;&#20013;&#23398;&#35777;&#20070;&#32771;&#35797;&#65288;WASSCE&#65289;&#30340;&#32508;&#21512;&#31185;&#23398;&#31185;&#30446;&#30340;&#30456;&#20851;&#36807;&#21435;&#30340;&#22269;&#23478;&#32771;&#35797;&#38382;&#39064;&#30340;&#31572;&#26696;&#26469;&#22238;&#31572;&#23398;&#29983;&#20204;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23398;&#29983;&#20204;&#36824;&#21487;&#20197;&#26597;&#30475;&#36807;&#21435;&#30340;&#22269;&#23478;&#32771;&#35797;&#38382;&#39064;&#21450;&#20854;&#31572;&#26696;&#65292;&#24182;&#36890;&#36807;&#25105;&#20204;&#24320;&#21457;&#30340;&#20027;&#39064;&#26816;&#27979;&#27169;&#22411;&#36827;&#34892;&#25353;&#24180;&#20221;&#12289;&#38382;&#39064;&#31867;&#22411;&#21644;&#20027;&#39064;&#30340;&#33258;&#21160;&#20998;&#31867;&#65288;91%&#38750;&#21152;&#26435;&#24179;&#22343;&#21484;&#22238;&#29575;&#65289;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#37096;&#32626;&#20102;Kwame for Science&#36229;&#36807;8&#20010;&#26376;&#65292;&#26377;&#26469;&#33258;32&#20010;&#22269;&#23478;&#65288;&#20854;&#20013;15&#20010;&#22312;&#38750;&#27954;&#65289;&#30340;750&#20010;&#29992;&#25143;&#65292;&#20849;&#25552;&#20986;&#20102;1.5K&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;87.2%&#30340;&#21069;&#19977;&#21517;&#38382;&#39064;&#20934;&#30830;&#29575;&#65288;n=109&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Africa has a high student-to-teacher ratio which limits students' access to teachers for learning support such as educational question answering. In this work, we extended Kwame, a bilingual AI teaching assistant for coding education, adapted it for science education, and deployed it as a web app. Kwame for Science provides passages from well-curated knowledge sources and related past national exam questions as answers to questions from students based on the Integrated Science subject of the West African Senior Secondary Certificate Examination (WASSCE). Furthermore, students can view past national exam questions along with their answers and filter by year, question type, and topics that were automatically categorized by a topic detection model which we developed (91% unweighted average recall). We deployed Kwame for Science in the real world over 8 months and had 750 users across 32 countries (15 in Africa) and 1.5K questions asked. Our evaluation showed an 87.2% top 3 accuracy (n=109
&lt;/p&gt;</description></item><item><title>UALA&#26159;&#19968;&#20010;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#36827;&#34892;&#20195;&#29702;&#21644;&#22806;&#37096;&#19990;&#30028;&#20132;&#20114;&#30340;&#26694;&#26550;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#23427;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#35821;&#35328;&#27169;&#22411;&#23610;&#23544;&#19979;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#19988;&#22312;&#23545;&#22806;&#37096;&#19990;&#30028;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#26356;&#20302;&#12290;</title><link>http://arxiv.org/abs/2401.14016</link><description>&lt;p&gt;
&#38754;&#21521;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#35821;&#35328;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Towards Uncertainty-Aware Language Agent. (arXiv:2401.14016v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14016
&lt;/p&gt;
&lt;p&gt;
UALA&#26159;&#19968;&#20010;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#36827;&#34892;&#20195;&#29702;&#21644;&#22806;&#37096;&#19990;&#30028;&#20132;&#20114;&#30340;&#26694;&#26550;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#23427;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#35821;&#35328;&#27169;&#22411;&#23610;&#23544;&#19979;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#19988;&#22312;&#23545;&#22806;&#37096;&#19990;&#30028;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#26356;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#35821;&#35328;&#26234;&#33021;&#20307;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32622;&#20110;&#26356;&#22810;&#21151;&#33021;&#30340;&#35774;&#35745;&#26680;&#24515;&#20197;&#21450;&#19982;&#22806;&#37096;&#19990;&#30028;&#30340;&#21160;&#24577;&#20132;&#20114;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#22312;&#36825;&#20123;&#20132;&#20114;&#36807;&#31243;&#20013;&#24573;&#35270;&#20102;&#19981;&#30830;&#23450;&#24615;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#35821;&#35328;&#26234;&#33021;&#20307;&#65288;UALA&#65289;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#32534;&#25490;&#20195;&#29702;&#21644;&#22806;&#37096;&#19990;&#30028;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#19982;&#20854;&#20182;&#30693;&#21517;&#23545;&#25163;&#65288;&#22914;ReAct&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;3&#20010;&#20195;&#34920;&#24615;&#20219;&#21153;&#65288;HotpotQA&#65292;StrategyQA&#65292;MMLU&#65289;&#21644;&#21508;&#31181;LLM&#23610;&#23544;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;UALA&#22312;&#24615;&#33021;&#26041;&#38754;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#21516;&#26102;&#23545;&#22806;&#37096;&#19990;&#30028;&#30340;&#20381;&#36182;&#24615;&#26174;&#33879;&#38477;&#20302;&#65288;&#21363;&#65292;&#20943;&#23569;&#20102;&#24037;&#20855;&#35843;&#29992;&#21644;&#26631;&#35760;&#25968;&#65289;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#21508;&#31181;&#35265;&#35299;&#65292;&#21253;&#25324;&#19982;&#20195;&#29702;&#24494;&#35843;&#30456;&#27604;&#65292;UALA&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#24182;&#24378;&#35843;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#21475;&#22836;&#32622;&#20449;&#24230;&#20316;&#20026;&#19981;&#30830;&#23450;&#24615;&#30340;&#20195;&#29702;&#26102;&#30340;&#19981;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Language Agents have achieved promising success by placing Large Language Models at the core of a more versatile design that dynamically interacts with the external world, the existing approaches neglect the notion of uncertainty during these interactions. We present the Uncertainty-Aware Language Agent (UALA), a framework that orchestrates the interaction between the agent and the external world using uncertainty quantification. Compared with other well-known counterparts like ReAct, our extensive experiments across 3 representative tasks (HotpotQA, StrategyQA, MMLU) and various LLM sizes demonstrates that UALA brings a significant improvement of performance, while having a substantially lower reliance on the external world (i.e., reduced number of tool calls and tokens). Our analyses provide various insights including the great potential of UALA compared with agent fine-tuning, and underscoring the unreliably of verbalised confidence of LLMs as a proxy for uncertainty.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#22870;&#21169;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;LLM&#20316;&#20026;&#35780;&#21028;&#32773;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#33258;&#24049;&#25552;&#20379;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#22870;&#21169;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#65292;&#36824;&#21487;&#20197;&#20026;&#33258;&#24049;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#22870;&#21169;&#12290;&#36890;&#36807;&#23545;Llama 2 70B&#27169;&#22411;&#30340;&#19977;&#27425;&#36845;&#20195;&#24494;&#35843;&#65292;&#32467;&#26524;&#22312;AlpacaEval 2.0&#25490;&#34892;&#27036;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#29616;&#26377;&#31995;&#32479;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#23454;&#29616;&#33021;&#22815;&#19981;&#26029;&#33258;&#25105;&#25913;&#36827;&#30340;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10020</link><description>&lt;p&gt;
&#33258;&#22870;&#21169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-Rewarding Language Models. (arXiv:2401.10020v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10020
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#22870;&#21169;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;LLM&#20316;&#20026;&#35780;&#21028;&#32773;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#33258;&#24049;&#25552;&#20379;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#22870;&#21169;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#65292;&#36824;&#21487;&#20197;&#20026;&#33258;&#24049;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#22870;&#21169;&#12290;&#36890;&#36807;&#23545;Llama 2 70B&#27169;&#22411;&#30340;&#19977;&#27425;&#36845;&#20195;&#24494;&#35843;&#65292;&#32467;&#26524;&#22312;AlpacaEval 2.0&#25490;&#34892;&#27036;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#29616;&#26377;&#31995;&#32479;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#23454;&#29616;&#33021;&#22815;&#19981;&#26029;&#33258;&#25105;&#25913;&#36827;&#30340;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20551;&#35774;&#35201;&#23454;&#29616;&#36229;&#20154;&#32423;&#30340;&#26234;&#33021;&#20307;&#65292;&#26410;&#26469;&#30340;&#27169;&#22411;&#38656;&#35201;&#36229;&#20154;&#32423;&#30340;&#21453;&#39304;&#65292;&#20197;&#25552;&#20379;&#36275;&#22815;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#26159;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#65292;&#36825;&#21487;&#33021;&#20250;&#21463;&#21040;&#20154;&#31867;&#34920;&#29616;&#27700;&#24179;&#30340;&#38480;&#21046;&#65292;&#32780;&#19988;&#36825;&#20123;&#29420;&#31435;&#30340;&#20923;&#32467;&#22870;&#21169;&#27169;&#22411;&#22312;LLM&#35757;&#32451;&#36807;&#31243;&#20013;&#26080;&#27861;&#23398;&#20064;&#25913;&#36827;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#22870;&#21169;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#36890;&#36807;LLM&#20316;&#20026;&#35780;&#21028;&#32773;&#30340;&#25552;&#31034;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20379;&#33258;&#24049;&#30340;&#22870;&#21169;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#36845;&#20195;DPO&#35757;&#32451;&#20013;&#65292;&#19981;&#20165;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#24471;&#21040;&#20102;&#25552;&#39640;&#65292;&#32780;&#19988;&#33021;&#22815;&#20026;&#33258;&#24049;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#22870;&#21169;&#12290;&#36890;&#36807;&#23545;Llama 2 70B&#36827;&#34892;&#25105;&#20204;&#26041;&#27861;&#30340;&#19977;&#27425;&#36845;&#20195;&#30340;&#24494;&#35843;&#65292;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;AlpacaEval 2.0&#25490;&#34892;&#27036;&#19978;&#32988;&#36807;&#35768;&#22810;&#29616;&#26377;&#31995;&#32479;&#65292;&#21253;&#25324;Claude 2&#12289;Gemini Pro&#21644;GPT-4 0613&#12290;&#34429;&#28982;&#36825;&#21482;&#26159;&#19968;&#39033;&#21021;&#27493;&#30740;&#31350;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20026;&#21487;&#33021;&#23454;&#29616;&#33021;&#22815;&#19981;&#26029;&#33258;&#25105;&#25913;&#36827;&#30340;&#27169;&#22411;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
We posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal. Current approaches commonly train reward models from human preferences, which may then be bottlenecked by human performance level, and secondly these separate frozen reward models cannot then learn to improve during LLM training. In this work, we study Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. We show that during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While only a preliminary study, this work opens the door to the possibility of models that can continuall
&lt;/p&gt;</description></item><item><title>RoSA&#26159;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#35757;&#32451;&#20302;&#31209;&#21644;&#39640;&#24230;&#31232;&#30095;&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;RoSA&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04679</link><description>&lt;p&gt;
RoSA: &#36890;&#36807;&#40065;&#26834;&#36866;&#24212;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04679
&lt;/p&gt;
&lt;p&gt;
RoSA&#26159;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#35757;&#32451;&#20302;&#31209;&#21644;&#39640;&#24230;&#31232;&#30095;&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;RoSA&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#32972;&#26223;&#19979;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#39044;&#31639;&#19979;&#25552;&#20379;&#33391;&#22909;&#20934;&#30830;&#24615;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843; (PEFT) &#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#31216;&#20026;RoSA&#65292;&#21463;&#40065;&#26834;&#20027;&#25104;&#20998;&#20998;&#26512; (PCA) &#30340;&#21551;&#21457;&#65292;&#23427;&#22312;&#19968;&#32452;&#22266;&#23450;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#20849;&#21516;&#35757;&#32451;$\textit{&#20302;&#31209;}$&#21644;$\textit{&#39640;&#24230;&#31232;&#30095;}$&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#65288;FFT&#65289;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RoSA&#22312;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#23567;&#23398;&#25968;&#23398;&#21644;SQL&#26597;&#35810;&#29983;&#25104;&#65292;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#20197;&#33719;&#24471;&#33391;&#22909;&#24615;&#33021;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#30456;&#21516;&#30340;&#21442;&#25968;&#39044;&#31639;&#19979;&#65292;RoSA&#20248;&#20110;LoRA&#21644;&#32431;&#31929;&#30340;&#31232;&#30095;&#24494;&#35843;&#12290;&#25105;&#20204;&#36890;&#36807;&#31232;&#30095;GPU&#20869;&#26680;&#20026;RoSA&#25552;&#20379;&#31995;&#32479;&#25903;&#25345;&#65292;&#20197;&#34917;&#20805;&#35757;&#32451;&#31639;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20869;&#23384;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312;https://github.com/IST-DASLab&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA) that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms both LoRA and pure sparse fine-tuning, at the same parameter budget. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memoryand computationally-efficient training. Our code will be made available at https://github.com/IST-DASLab
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;PPNL&#65292;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31354;&#38388;-&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23569;&#26679;&#26412;&#30340;GPT-4&#22312;&#31354;&#38388;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20173;&#26377;&#24453;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.03249</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#25104;&#20026;&#22909;&#30340;&#36335;&#24452;&#35268;&#21010;&#22120;&#21527;&#65311;&#23545;&#31354;&#38388;-&#26102;&#38388;&#25512;&#29702;&#36827;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning. (arXiv:2310.03249v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;PPNL&#65292;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31354;&#38388;-&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23569;&#26679;&#26412;&#30340;GPT-4&#22312;&#31354;&#38388;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20173;&#26377;&#24453;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#38656;&#35201;&#38271;&#26399;&#35268;&#21010;&#21644;&#31354;&#38388;&#25512;&#29702;&#30340;&#22330;&#26223;&#20013;&#20173;&#28982;&#38754;&#20020;&#38480;&#21046;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#31216;&#20026;&#33258;&#28982;&#35821;&#35328;&#36335;&#24452;&#35268;&#21010;&#65288;PPNL&#65289;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#36890;&#36807;&#21046;&#23450;&#38656;&#35201;LLM&#23548;&#33322;&#21040;&#30446;&#26631;&#20301;&#32622;&#24182;&#36991;&#24320;&#38556;&#30861;&#29289;&#21644;&#36981;&#23432;&#32422;&#26463;&#26465;&#20214;&#30340;&#8220;&#36335;&#24452;&#35268;&#21010;&#8221;&#20219;&#21153;&#65292;&#35780;&#20272;LLM&#30340;&#31354;&#38388;-&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;&#21033;&#29992;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#21253;&#25324;GPT-4&#22312;&#20869;&#30340;LLM&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#23569;&#26679;&#26412;&#25552;&#31034;&#26041;&#27861;&#21644;&#21508;&#31181;&#35268;&#27169;&#30340;BART&#21644;T5&#36827;&#34892;&#24494;&#35843;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25552;&#31034;LLM&#36827;&#34892;&#25512;&#29702;&#21644;&#20132;&#20114;&#34892;&#21160;&#26102;&#65292;&#23569;&#26679;&#26412;&#30340;GPT-4&#22312;&#31354;&#38388;&#25512;&#29702;&#26041;&#38754;&#26377;&#24076;&#26395;&#65292;&#20294;&#20173;&#26080;&#27861;&#36827;&#34892;&#38271;&#26399;&#26102;&#38388;&#25512;&#29702;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;LLM&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved remarkable success across a wide spectrum of tasks; however, they still face limitations in scenarios that demand long-term planning and spatial reasoning. To facilitate this line of research, in this work, we propose a new benchmark, termed $\textbf{P}$ath $\textbf{P}$lanning from $\textbf{N}$atural $\textbf{L}$anguage ($\textbf{PPNL}$). Our benchmark evaluates LLMs' spatial-temporal reasoning by formulating ''path planning'' tasks that require an LLM to navigate to target locations while avoiding obstacles and adhering to constraints. Leveraging this benchmark, we systematically investigate LLMs including GPT-4 via different few-shot prompting methodologies and BART and T5 of various sizes via fine-tuning. Our experimental results show the promise of few-shot GPT-4 in spatial reasoning, when it is prompted to reason and act interleavedly, although it still fails to make long-term temporal reasoning. In contrast, while fine-tuned LLMs achieve
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31070;&#32463;&#31185;&#23398;&#21551;&#21457;&#30340;&#25216;&#26415;&#65292;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#36890;&#36807;&#35843;&#26597;&#23884;&#20837;&#21644;&#27880;&#24847;&#21147;&#30340;&#21464;&#21270;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#25913;&#36827;&#32972;&#21518;&#30340;&#28508;&#22312;&#21464;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#21442;&#25968;&#21270;&#25506;&#27979;&#21644;&#27880;&#24847;&#21147;&#27604;&#29575;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.00313</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;: &#23545;&#34920;&#31034;&#30340;&#31070;&#32463;&#31185;&#23398;&#21551;&#21457;&#24335;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning in Large Language Models: A Neuroscience-inspired Analysis of Representations. (arXiv:2310.00313v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31070;&#32463;&#31185;&#23398;&#21551;&#21457;&#30340;&#25216;&#26415;&#65292;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#36890;&#36807;&#35843;&#26597;&#23884;&#20837;&#21644;&#27880;&#24847;&#21147;&#30340;&#21464;&#21270;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#25913;&#36827;&#32972;&#21518;&#30340;&#28508;&#22312;&#21464;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#21442;&#25968;&#21270;&#25506;&#27979;&#21644;&#27880;&#24847;&#21147;&#27604;&#29575;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#36755;&#20837;&#20013;&#30340;&#29305;&#23450;&#20219;&#21153;&#31034;&#20363;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25913;&#36827;&#32972;&#21518;&#30340;&#26426;&#21046;&#20173;&#28982;&#38590;&#20197;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;Llama-2 70B&#21644;Vicuna 13B&#20013;&#30340;&#23884;&#20837;&#21644;&#27880;&#24847;&#21147;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#21518;&#23884;&#20837;&#21644;&#27880;&#24847;&#21147;&#30340;&#21464;&#21270;&#20197;&#21450;&#36825;&#20123;&#21464;&#21270;&#22914;&#20309;&#35843;&#35299;&#34892;&#20026;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#21463;&#31070;&#32463;&#31185;&#23398;&#21551;&#21457;&#30340;&#25216;&#26415;&#65292;&#22914;&#34920;&#31034;&#30456;&#20284;&#24615;&#20998;&#26512;&#65288;RSA&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#21442;&#25968;&#21270;&#25506;&#27979;&#21644;&#27880;&#24847;&#21147;&#27604;&#29575;&#20998;&#26512;&#65288;ARA&#65292;&#34913;&#37327;&#20851;&#27880;&#30456;&#20851;&#19982;&#26080;&#20851;&#20449;&#24687;&#30340;&#27604;&#29575;&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#20010;&#20855;&#26377;&#26465;&#20214;&#20043;&#38388;&#20808;&#39564;&#20851;&#31995;&#30340;&#20219;&#21153;&#65306;&#38405;&#35835;&#29702;&#35299;&#65292;&#32447;&#24615;&#22238;&#24402;&#21644;&#23545;&#25239;&#25552;&#31034;&#27880;&#20837;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#20219;&#21153;&#34920;&#31034;&#20013;&#39044;&#26399;&#30456;&#20284;&#24615;&#30340;&#20551;&#35774;&#65292;&#20197;&#30740;&#31350;&#23884;&#20837;&#21644;&#27880;&#24847;&#21147;&#20013;&#30340;&#28508;&#22312;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) exhibit remarkable performance improvement through in-context learning (ICL) by leveraging task-specific examples in the input. However, the mechanisms behind this improvement remain elusive. In this work, we investigate embeddings and attention representations in Llama-2 70B and Vicuna 13B. Specifically, we study how embeddings and attention change after in-context-learning, and how these changes mediate improvement in behavior. We employ neuroscience-inspired techniques, such as representational similarity analysis (RSA), and propose novel methods for parameterized probing and attention ratio analysis (ARA, measuring the ratio of attention to relevant vs. irrelevant information). We designed three tasks with a priori relationships among their conditions: reading comprehension, linear regression, and adversarial prompt injection. We formed hypotheses about expected similarities in task representations to investigate latent changes in embeddings and attenti
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#20462;&#35746;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#25552;&#31034;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#25913;&#36827;&#36755;&#20837;&#25991;&#26412;&#30340;&#26041;&#24335;&#23454;&#29616;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.00152</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#30340;&#33258;&#21160;&#25552;&#31034;&#37325;&#20889;
&lt;/p&gt;
&lt;p&gt;
Automatic Prompt Rewriting for Personalized Text Generation. (arXiv:2310.00152v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00152
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#20462;&#35746;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#25552;&#31034;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#25913;&#36827;&#36755;&#20837;&#25991;&#26412;&#30340;&#26041;&#24335;&#23454;&#29616;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24110;&#21161;&#19979;&#65292;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#24050;&#25104;&#20026;&#19968;&#20010;&#24555;&#36895;&#22686;&#38271;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#38598;&#20013;&#22312;&#20026;&#29305;&#23450;&#39046;&#22495;&#35774;&#35745;&#19987;&#38376;&#30340;&#27169;&#22411;&#65292;&#25110;&#32773;&#38656;&#35201;&#24494;&#35843;LLMs&#20197;&#29983;&#25104;&#20010;&#24615;&#21270;&#25991;&#26412;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20856;&#22411;&#24773;&#26223;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#20010;&#24615;&#21270;&#36755;&#20986;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#20923;&#32467;&#30340;&#65292;&#21482;&#33021;&#36890;&#36807;API&#36827;&#34892;&#35775;&#38382;&#12290;&#22312;&#36825;&#20010;&#38480;&#21046;&#19979;&#65292;&#21807;&#19968;&#33021;&#20570;&#30340;&#23601;&#26159;&#25913;&#36827;&#21457;&#36865;&#32473;LLM&#30340;&#36755;&#20837;&#25991;&#26412;&#65288;&#21363;&#25991;&#26412;&#25552;&#31034;&#65289;&#65292;&#36825;&#20010;&#36807;&#31243;&#36890;&#24120;&#26159;&#25163;&#21160;&#23436;&#25104;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20462;&#35746;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#30340;&#25552;&#31034;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#20010;&#35757;&#32451;&#33539;&#24335;&#65292;&#23558;&#30417;&#30563;&#23398;&#20064;&#65288;SL&#65289;&#21644;
&lt;/p&gt;
&lt;p&gt;
Facilitated by large language models (LLMs), personalized text generation has become a rapidly growing research direction. Most existing studies focus on designing specialized models for a particular domain, or they require fine-tuning the LLMs to generate personalized text. We consider a typical scenario in which the large language model, which generates personalized output, is frozen and can only be accessed through APIs. Under this constraint, all one can do is to improve the input text (i.e., text prompts) sent to the LLM, a procedure that is usually done manually. In this paper, we propose a novel method to automatically revise prompts for personalized text generation. The proposed method takes the initial prompts generated by a state-of-the-art, multistage framework for personalized generation and rewrites a few critical components that summarize and synthesize the personal context. The prompt rewriter employs a training paradigm that chains together supervised learning (SL) and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#35805;&#29983;&#25104;&#22270;&#20687;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#23454;&#29616;&#22312;&#32473;&#23450;&#23545;&#35805;&#32972;&#26223;&#19979;&#29983;&#25104;&#19968;&#33268;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2309.15516</link><description>&lt;p&gt;
&#25945;&#25480;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36827;&#34892;&#20132;&#27969;
&lt;/p&gt;
&lt;p&gt;
Teaching Text-to-Image Models to Communicate. (arXiv:2309.15516v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#35805;&#29983;&#25104;&#22270;&#20687;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#23454;&#29616;&#22312;&#32473;&#23450;&#23545;&#35805;&#32972;&#26223;&#19979;&#29983;&#25104;&#19968;&#33268;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#30740;&#31350;&#20013;&#65292;&#21508;&#31181;&#24037;&#20316;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#34429;&#28982;&#29616;&#26377;&#27169;&#22411;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#26159;&#22312;&#30452;&#25509;&#24212;&#29992;&#20110;&#23545;&#35805;&#29983;&#25104;&#22270;&#20687;&#26102;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#31361;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65306;&#23545;&#35805;&#21040;&#22270;&#20687;&#29983;&#25104;&#65292;&#21363;&#22312;&#32473;&#23450;&#23545;&#35805;&#32972;&#26223;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#24212;&#35813;&#29983;&#25104;&#19968;&#20010;&#19982;&#25351;&#23450;&#23545;&#35805;&#20869;&#23481;&#19968;&#33268;&#30340;&#36924;&#30495;&#22270;&#20687;&#20316;&#20026;&#22238;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20013;&#38388;&#36716;&#25442;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#21462;&#23545;&#35805;&#20013;&#21253;&#21547;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#32771;&#34385;&#21040;&#23545;&#35805;&#32467;&#26500;&#30340;&#29305;&#28857;&#65292;&#25105;&#20204;&#22312;&#23545;&#35805;&#20013;&#30340;&#27599;&#20010;&#35828;&#35805;&#22238;&#21512;&#20043;&#21069;&#25918;&#32622;&#20998;&#21106;&#26631;&#35760;&#65292;&#20197;&#21306;&#20998;&#19981;&#21516;&#30340;&#21457;&#35328;&#32773;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#20854;&#33021;&#22815;&#26681;&#25454;&#22788;&#29702;&#21518;&#30340;&#23545;&#35805;&#32972;&#26223;&#29983;&#25104;&#22270;&#20687;&#12290;&#32463;&#36807;&#24494;&#35843;&#21518;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#19982;&#22788;&#29702;&#21518;&#23545;&#35805;&#29615;&#22659;&#30456;&#19968;&#33268;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various works have been extensively studied in the research of text-to-image generation. Although existing models perform well in text-to-image generation, there are significant challenges when directly employing them to generate images in dialogs. In this paper, we first highlight a new problem: dialog-to-image generation, that is, given the dialog context, the model should generate a realistic image which is consistent with the specified conversation as response. To tackle the problem, we propose an efficient approach for dialog-to-image generation without any intermediate translation, which maximizes the extraction of the semantic information contained in the dialog. Considering the characteristics of dialog structure, we put segment token before each sentence in a turn of a dialog to differentiate different speakers. Then, we fine-tune pre-trained text-to-image models to enable them to generate images conditioning on processed dialog context. After fine-tuning, our approach can con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#27861;&#20064;&#24471;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#35757;&#32451;&#30340;&#19968;&#20010;&#30701;&#26242;&#31383;&#21475;&#20869;&#65292;&#27169;&#22411;&#31361;&#28982;&#33719;&#24471;&#20102;&#35821;&#27861;&#27880;&#24847;&#32467;&#26500;(SAS)&#65292;&#24182;&#20276;&#38543;&#30528;&#25439;&#22833;&#30340;&#38497;&#23789;&#19979;&#38477;&#12290;SAS&#23545;&#38543;&#21518;&#20064;&#24471;&#35821;&#35328;&#33021;&#21147;&#36215;&#21040;&#20102;&#37325;&#35201;&#30340;&#20419;&#36827;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.07311</link><description>&lt;p&gt;
&#25439;&#22833;&#31361;&#28982;&#19979;&#38477;&#65306;&#35821;&#27861;&#20064;&#24471;&#12289;&#30456;&#21464;&#21644;MLM&#20013;&#30340;&#31616;&#21270;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs. (arXiv:2309.07311v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#27861;&#20064;&#24471;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#35757;&#32451;&#30340;&#19968;&#20010;&#30701;&#26242;&#31383;&#21475;&#20869;&#65292;&#27169;&#22411;&#31361;&#28982;&#33719;&#24471;&#20102;&#35821;&#27861;&#27880;&#24847;&#32467;&#26500;(SAS)&#65292;&#24182;&#20276;&#38543;&#30528;&#25439;&#22833;&#30340;&#38497;&#23789;&#19979;&#38477;&#12290;SAS&#23545;&#38543;&#21518;&#20064;&#24471;&#35821;&#35328;&#33021;&#21147;&#36215;&#21040;&#20102;&#37325;&#35201;&#30340;&#20419;&#36827;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20013;&#30340;&#22823;&#22810;&#25968;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#20391;&#37325;&#20110;&#29702;&#35299;&#23436;&#20840;&#35757;&#32451;&#27169;&#22411;&#30340;&#34892;&#20026;&#21644;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#35266;&#23519;&#35757;&#32451;&#36807;&#31243;&#30340;&#36712;&#36857;&#65292;&#21487;&#33021;&#25165;&#33021;&#33719;&#24471;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#26576;&#20123;&#27934;&#23519;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;(MLMs)&#20013;&#30340;&#35821;&#27861;&#20064;&#24471;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20998;&#26512;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#35299;&#37322;&#24615;&#30340;&#28436;&#21270;&#26469;&#21152;&#28145;&#25105;&#20204;&#23545;&#26032;&#20852;&#34892;&#20026;&#30340;&#29702;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#27861;&#27880;&#24847;&#32467;&#26500;(SAS)&#65292;&#36825;&#26159;MLMs&#20013;&#33258;&#28982;&#24418;&#25104;&#30340;&#19968;&#20010;&#29305;&#24615;&#65292;&#20854;&#20013;&#29305;&#23450;&#30340;Transformer&#22836;&#20542;&#21521;&#20110;&#20851;&#27880;&#29305;&#23450;&#30340;&#21477;&#27861;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#35757;&#32451;&#30340;&#19968;&#20010;&#30701;&#26242;&#31383;&#21475;&#20869;&#65292;&#27169;&#22411;&#31361;&#28982;&#33719;&#24471;&#20102;SAS&#65292;&#24182;&#21457;&#29616;&#36825;&#20010;&#31383;&#21475;&#19982;&#25439;&#22833;&#30340;&#38497;&#23789;&#19979;&#38477;&#21516;&#26102;&#21457;&#29983;&#12290;&#27492;&#22806;&#65292;SAS&#20419;&#20351;&#20102;&#38543;&#21518;&#23545;&#35821;&#35328;&#33021;&#21147;&#30340;&#20064;&#24471;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#26469;&#25805;&#32437;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;SAS&#65292;&#26469;&#30740;&#31350;SAS&#30340;&#22240;&#26524;&#20316;&#29992;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process. In this paper, we present a case study of syntax acquisition in masked language models (MLMs). Our findings demonstrate how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure (SAS), a naturally emerging property of MLMs wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in training when models abruptly acquire SAS and find that this window is concurrent with a steep drop in loss. Moreover, SAS precipitates the subsequent acquisition of linguistic capabilities. We then examine the causal role of SAS by introducing a regularizer to manipulate SAS during training, and demonstrate
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#22312;&#20027;&#35266;NLP&#20219;&#21153;&#20013;&#65292;&#21457;&#29616;&#31038;&#20250;&#20154;&#21475;&#25552;&#31034;&#25216;&#26415;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#26377;&#25928;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.07034</link><description>&lt;p&gt;
&#22914;&#20309;&#65288;&#19981;&#65289;&#22312;&#20027;&#35266;NLP&#20219;&#21153;&#20013;&#20351;&#29992;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
How (Not) to Use Sociodemographic Information for Subjective NLP Tasks. (arXiv:2309.07034v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07034
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#22312;&#20027;&#35266;NLP&#20219;&#21153;&#20013;&#65292;&#21457;&#29616;&#31038;&#20250;&#20154;&#21475;&#25552;&#31034;&#25216;&#26415;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#26377;&#25928;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#37322;&#32773;&#30340;&#31038;&#20250;&#20154;&#21475;&#32972;&#26223;&#65288;&#21363;&#24615;&#21035;&#65292;&#24180;&#40836;&#65292;&#25945;&#32946;&#32972;&#26223;&#31561;&#20010;&#20307;&#32452;&#25104;&#65289;&#23545;&#20854;&#22312;&#20027;&#35266;NLP&#20219;&#21153;&#20013;&#30340;&#20915;&#31574;&#26377;&#24456;&#22823;&#24433;&#21709;&#65292;&#27604;&#22914;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#12290;&#36890;&#24120;&#65292;&#24322;&#36136;&#30340;&#32972;&#26223;&#20250;&#23548;&#33268;&#39640;&#24230;&#20998;&#27495;&#12290;&#20026;&#20102;&#24314;&#27169;&#36825;&#31181;&#24046;&#24322;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#31038;&#20250;&#20154;&#21475;&#25552;&#31034;&#25216;&#26415;&#65292;&#36825;&#31181;&#25216;&#26415;&#23558;&#22522;&#20110;&#25552;&#31034;&#30340;&#27169;&#22411;&#30340;&#36755;&#20986;&#24341;&#23548;&#21040;&#20855;&#26377;&#29305;&#23450;&#31038;&#20250;&#20154;&#21475;&#29305;&#24449;&#30340;&#20154;&#31867;&#21487;&#33021;&#32473;&#20986;&#30340;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NLP&#25991;&#29486;&#23545;&#36825;&#31181;&#25216;&#26415;&#30340;&#25928;&#26524;&#23384;&#22312;&#20998;&#27495; - &#23427;&#20173;&#28982;&#19981;&#28165;&#26970;&#23427;&#33021;&#22312;&#21738;&#20123;&#20219;&#21153;&#21644;&#22330;&#26223;&#20013;&#26377;&#24110;&#21161;&#65292;&#24182;&#19988;&#35780;&#20272;&#20165;&#38480;&#20110;&#29305;&#23450;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#21644;&#26368;&#20840;&#38754;&#30340;&#31038;&#20250;&#20154;&#21475;&#25552;&#31034;&#30740;&#31350;&#26469;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19971;&#20010;&#25968;&#25454;&#38598;&#21644;&#20845;&#20010;&#32463;&#36807;&#25351;&#23548;&#35843;&#25972;&#30340;&#27169;&#22411;&#23478;&#26063;&#20013;&#30340;&#20960;&#20010;&#25552;&#31034;&#24418;&#24335;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#31038;&#20250;&#20154;&#21475;&#25552;&#31034;&#23545;&#26576;&#20123;&#20219;&#21153;&#26377;&#25928;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Annotators' sociodemographic backgrounds (i.e., the individual compositions of their gender, age, educational background, etc.) have a strong impact on their decisions when working on subjective NLP tasks, such as hate speech detection. Often, heterogeneous backgrounds result in high disagreements. To model this variation, recent work has explored sociodemographic prompting, a technique, which steers the output of prompt-based models towards answers that humans with specific sociodemographic profiles would give. However, the available NLP literature disagrees on the efficacy of this technique -- it remains unclear, for which tasks and scenarios it can help and evaluations are limited to specific tasks only. We address this research gap by presenting the largest and most comprehensive study of sociodemographic prompting today. Concretely, we evaluate several prompt formulations across seven datasets and six instruction-tuned model families. We find that (1) while sociodemographic prompt
&lt;/p&gt;</description></item><item><title>chatGPT ADA&#26159;&#19968;&#31181;&#33021;&#22815;&#33258;&#20027;&#24320;&#21457;&#20020;&#24202;&#30740;&#31350;&#25152;&#38656;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#23558;&#39640;&#32423;&#20998;&#26512;&#24037;&#20855;&#27665;&#20027;&#21270;&#65292;&#20351;&#38750;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#20020;&#24202;&#21307;&#29983;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2308.14120</link><description>&lt;p&gt;
&#25480;&#26435;&#20020;&#24202;&#21307;&#29983;&#24182;&#27665;&#20027;&#21270;&#25968;&#25454;&#31185;&#23398;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#20020;&#24202;&#30740;&#31350;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290; (arXiv:2308.14120v2 [cs.LG] &#26356;&#26032;&#29256;)
&lt;/p&gt;
&lt;p&gt;
Empowering Clinicians and Democratizing Data Science: Large Language Models Automate Machine Learning for Clinical Studies. (arXiv:2308.14120v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14120
&lt;/p&gt;
&lt;p&gt;
chatGPT ADA&#26159;&#19968;&#31181;&#33021;&#22815;&#33258;&#20027;&#24320;&#21457;&#20020;&#24202;&#30740;&#31350;&#25152;&#38656;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#23558;&#39640;&#32423;&#20998;&#26512;&#24037;&#20855;&#27665;&#20027;&#21270;&#65292;&#20351;&#38750;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#20020;&#24202;&#21307;&#29983;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24320;&#21457;&#32773;&#65288;&#22914;&#25968;&#25454;&#31185;&#23398;&#23478;&#65289;&#21644;&#20174;&#19994;&#32773;&#65288;&#22914;&#20020;&#24202;&#21307;&#29983;&#65289;&#20043;&#38388;&#23384;&#22312;&#30693;&#35782;&#24046;&#36317;&#65292;&#38459;&#30861;&#20102;ML&#22312;&#20020;&#24202;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#20805;&#20998;&#21033;&#29992;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;chatGPT Advanced Data Analysis&#65288;ADA&#65289;&#65292;&#21363;GPT-4&#30340;&#25193;&#23637;&#65292;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#24182;&#39640;&#25928;&#25191;&#34892;ML&#20998;&#26512;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21521;chatGPT ADA&#25552;&#20379;&#20102;&#21508;&#31181;&#21307;&#23398;&#19987;&#19994;&#30340;&#22823;&#22411;&#35797;&#39564;&#30340;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#21644;&#30740;&#31350;&#35814;&#32454;&#20449;&#24687;&#65292;&#27809;&#26377;&#32473;&#20986;&#20855;&#20307;&#25351;&#23548;&#12290;ChatGPT ADA&#22522;&#20110;&#21407;&#22987;&#30740;&#31350;&#30340;&#35757;&#32451;&#25968;&#25454;&#33258;&#20027;&#24320;&#21457;&#20102;&#26368;&#20808;&#36827;&#30340;ML&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20020;&#24202;&#32467;&#26524;&#65292;&#22914;&#30284;&#30151;&#21457;&#23637;&#12289;&#30284;&#30151;&#36827;&#23637;&#12289;&#30142;&#30149;&#24182;&#21457;&#30151;&#25110;&#33268;&#30149;&#22522;&#22240;&#24207;&#21015;&#31561;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#20123;ML&#27169;&#22411;&#19982;&#20854;&#24050;&#21457;&#34920;&#30340;&#23545;&#24212;&#29289;&#30456;&#21305;&#37197;&#29978;&#33267;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;chatGPT ADA&#20026;&#27665;&#20027;&#21270;&#21307;&#23398;&#20013;&#30340;ML&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#65292;&#20351;&#38750;ML&#19987;&#23478;&#33021;&#22815;&#33719;&#24471;&#20808;&#36827;&#30340;&#20998;&#26512;&#24037;&#20855;&#24182;&#25512;&#21160;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
A knowledge gap persists between Machine Learning (ML) developers (e.g., data scientists) and practitioners (e.g., clinicians), hampering the full utilization of ML for clinical data analysis. We investigated the potential of the chatGPT Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this gap and perform ML analyses efficiently. Real-world clinical datasets and study details from large trials across various medical specialties were presented to chatGPT ADA without specific guidance. ChatGPT ADA autonomously developed state-of-the-art ML models based on the original study's training data to predict clinical outcomes such as cancer development, cancer progression, disease complications, or biomarkers such as pathogenic gene sequences. Strikingly, these ML models matched or outperformed their published counterparts. We conclude that chatGPT ADA offers a promising avenue to democratize ML in medicine, making advanced analytics accessible to non-ML experts and promoting broa
&lt;/p&gt;</description></item><item><title>S2vNTM&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;vMF&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20851;&#38190;&#35789;&#30340;&#27169;&#24335;&#26469;&#35782;&#21035;&#28508;&#22312;&#30340;&#20027;&#39064;&#65292;&#24182;&#20248;&#21270;&#20027;&#39064;&#20851;&#38190;&#35789;&#38598;&#30340;&#36136;&#37327;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#36895;&#24230;&#33267;&#23569;&#27604;&#22522;&#32447;&#27169;&#22411;&#24555;&#20004;&#20493;&#12290;</title><link>http://arxiv.org/abs/2307.04804</link><description>&lt;p&gt;
S2vNTM: &#21322;&#30417;&#30563;vMF&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
S2vNTM: Semi-supervised vMF Neural Topic Modeling. (arXiv:2307.04804v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04804
&lt;/p&gt;
&lt;p&gt;
S2vNTM&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;vMF&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20851;&#38190;&#35789;&#30340;&#27169;&#24335;&#26469;&#35782;&#21035;&#28508;&#22312;&#30340;&#20027;&#39064;&#65292;&#24182;&#20248;&#21270;&#20027;&#39064;&#20851;&#38190;&#35789;&#38598;&#30340;&#36136;&#37327;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#36895;&#24230;&#33267;&#23569;&#27604;&#22522;&#32447;&#27169;&#22411;&#24555;&#20004;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#23545;&#20110;&#25991;&#26412;&#20998;&#31867;&#26469;&#35828;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65306;&#65288;1&#65289;&#24456;&#38590;&#25972;&#21512;&#20154;&#31867;&#30693;&#35782;&#65292;&#27604;&#22914;&#20851;&#38190;&#35789;&#65307;&#65288;2&#65289;&#35757;&#32451;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#65307;&#65288;3&#65289;&#20381;&#36182;&#22823;&#35268;&#27169;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21322;&#30417;&#30563;vMF&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#65288;S2vNTM&#65289;&#26469;&#20811;&#26381;&#36825;&#20123;&#22256;&#38590;&#12290;S2vNTM&#23558;&#19968;&#20123;&#31181;&#23376;&#20851;&#38190;&#35789;&#20316;&#20026;&#20027;&#39064;&#30340;&#36755;&#20837;&#12290;S2vNTM&#21033;&#29992;&#20851;&#38190;&#35789;&#30340;&#27169;&#24335;&#26469;&#35782;&#21035;&#28508;&#22312;&#30340;&#20027;&#39064;&#65292;&#24182;&#20248;&#21270;&#20027;&#39064;&#20851;&#38190;&#35789;&#38598;&#30340;&#36136;&#37327;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#65292;S2vNTM&#22312;&#25552;&#20379;&#26377;&#38480;&#20851;&#38190;&#35789;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#20998;&#31867;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#21322;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;S2vNTM&#33267;&#23569;&#27604;&#22522;&#32447;&#27169;&#22411;&#24555;&#20004;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language model based methods are powerful techniques for text classification. However, the models have several shortcomings. (1) It is difficult to integrate human knowledge such as keywords. (2) It needs a lot of resources to train the models. (3) It relied on large text data to pretrain. In this paper, we propose Semi-Supervised vMF Neural Topic Modeling (S2vNTM) to overcome these difficulties. S2vNTM takes a few seed keywords as input for topics. S2vNTM leverages the pattern of keywords to identify potential topics, as well as optimize the quality of topics' keywords sets. Across a variety of datasets, S2vNTM outperforms existing semi-supervised topic modeling methods in classification accuracy with limited keywords provided. S2vNTM is at least twice as fast as baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#22810;&#20851;&#31995;&#21452;&#26354;&#35789;&#21521;&#37327;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#30001;&#23450;&#20041;&#25152;&#24341;&#36215;&#30340;&#20998;&#23618;&#21644;&#22810;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.07303</link><description>&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#22810;&#20851;&#31995;&#21452;&#26354;&#35789;&#21521;&#37327;
&lt;/p&gt;
&lt;p&gt;
Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions. (arXiv:2305.07303v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#22810;&#20851;&#31995;&#21452;&#26354;&#35789;&#21521;&#37327;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#30001;&#23450;&#20041;&#25152;&#24341;&#36215;&#30340;&#20998;&#23618;&#21644;&#22810;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20351;&#29992;&#20998;&#24067;&#20449;&#24687;&#30340;&#31070;&#32463;&#35789;&#21521;&#37327;&#19968;&#30452;&#20197;&#26469;&#37117;&#33021;&#20026;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#26377;&#29992;&#30340;&#21547;&#20041;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20250;&#23548;&#33268;&#38590;&#20197;&#35299;&#37322;&#21644;&#25511;&#21046;&#30340;&#34920;&#31034;&#12290;&#30456;&#21453;&#65292;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20855;&#26377;&#36882;&#24402;&#30340;&#65292;&#33258;&#35828;&#26126;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#21487;&#20197;&#25903;&#25345;&#33021;&#22815;&#20445;&#30041;&#21521;&#37327;&#31354;&#38388;&#20013;&#26174;&#24335;&#27010;&#24565;&#20851;&#31995;&#21644;&#32422;&#26463;&#30340;&#26032;&#22411;&#34920;&#31034;&#23398;&#20064;&#33539; paradigm&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#12289;&#22810;&#20851;&#31995;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#21512;&#26144;&#23556;&#23450;&#20041;&#21644;&#23450;&#20041;&#26415;&#35821;&#21450;&#20854;&#30456;&#24212;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#20165;&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#35789;&#21521;&#37327;&#12290;&#36890;&#36807;&#33258;&#21160;&#20174;&#23450;&#20041;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#32763;&#35793;&#30446;&#26631;&#35268;&#33539;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#26694;&#26550;&#19987;&#38376;&#35774;&#23450;&#20026;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#25429;&#33719;&#30001;&#23450;&#20041;&#24341;&#36215;&#30340;&#20998;&#23618;&#21644;&#22810;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural-based word embeddings using solely distributional information have consistently produced useful meaning representations for downstream tasks. However, existing approaches often result in representations that are hard to interpret and control. Natural language definitions, on the other side, possess a recursive, self-explanatory semantic structure that can support novel representation learning paradigms able to preserve explicit conceptual relations and constraints in the vector space.  This paper proposes a neuro-symbolic, multi-relational framework to learn word embeddings exclusively from natural language definitions by jointly mapping defined and defining terms along with their corresponding semantic relations. By automatically extracting the relations from definitions corpora and formalising the learning problem via a translational objective, we specialise the framework in hyperbolic space to capture the hierarchical and multi-resolution structure induced by the definitions.
&lt;/p&gt;</description></item></channel></rss>