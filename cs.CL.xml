<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#23545;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22810;&#36873;&#39064;&#22238;&#31572;&#30340;&#21512;&#29702;&#24615;&#36827;&#34892;&#20102;&#22238;&#39038;&#65292;&#21457;&#29616;&#24403;&#21069;&#22522;&#20110;&#22810;&#36873;&#39064;&#22238;&#31572;&#30340;&#22522;&#20934;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01349</link><description>&lt;p&gt;
&#36229;&#36234;&#31572;&#26696;&#65306;&#23545;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22810;&#36873;&#39064;&#22238;&#31572;&#30340;&#21512;&#29702;&#24615;&#30340;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
Beyond the Answers: Reviewing the Rationality of Multiple Choice Question Answering for the Evaluation of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01349
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22810;&#36873;&#39064;&#22238;&#31572;&#30340;&#21512;&#29702;&#24615;&#36827;&#34892;&#20102;&#22238;&#39038;&#65292;&#21457;&#29616;&#24403;&#21069;&#22522;&#20110;&#22810;&#36873;&#39064;&#22238;&#31572;&#30340;&#22522;&#20934;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#21457;&#20102;&#19968;&#22330;&#33539;&#24335;&#36716;&#21464;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#23545;LLMs&#30340;&#20840;&#38754;&#35780;&#20272;&#20173;&#28982;&#26159;&#31038;&#21306;&#38754;&#20020;&#30340;&#24517;&#28982;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#23558;&#22810;&#36873;&#39064;&#22238;&#31572;&#65288;MCQA&#65289;&#20316;&#20026;LLMs&#30340;&#22522;&#20934;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;MCQA&#20316;&#20026;LLMs&#35780;&#20272;&#26041;&#27861;&#30340;&#21512;&#29702;&#24615;&#12290;&#22914;&#26524;LLMs&#30495;&#27491;&#29702;&#35299;&#38382;&#39064;&#30340;&#35821;&#20041;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#24212;&#35813;&#22312;&#20174;&#30456;&#21516;&#38382;&#39064;&#27966;&#29983;&#30340;&#21508;&#31181;&#37197;&#32622;&#19978;&#34920;&#29616;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;LLMs&#30340;&#21709;&#24212;&#19968;&#33268;&#24615;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#25105;&#20204;&#23558;&#20043;&#23450;&#20041;&#20026;LLMs&#30340;&#21709;&#24212;&#21487;&#21464;&#24615;&#32508;&#21512;&#24449;&#65288;REVAS&#65289;&#65292;&#36825;&#34920;&#26126;&#30446;&#21069;&#22522;&#20110;MCQA&#30340;&#22522;&#20934;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;LLMs&#30340;&#30495;&#23454;&#33021;&#21147;&#65292;&#24378;&#35843;&#20102;&#23545;&#26356;&#21512;&#36866;&#30340;&#35780;&#20272;&#26041;&#27861;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of natural language processing (NLP), Large Language Models (LLMs) have precipitated a paradigm shift, markedly enhancing performance in natural language generation tasks. Despite these advancements, the comprehensive evaluation of LLMs remains an inevitable challenge for the community. Recently, the utilization of Multiple Choice Question Answering (MCQA) as a benchmark for LLMs has gained considerable traction. This study investigates the rationality of MCQA as an evaluation method for LLMs. If LLMs genuinely understand the semantics of questions, their performance should exhibit consistency across the varied configurations derived from the same questions. Contrary to this expectation, our empirical findings suggest a notable disparity in the consistency of LLM responses, which we define as REsponse VAriability Syndrome (REVAS) of the LLMs, indicating that current MCQA-based benchmarks may not adequately capture the true capabilities of LLMs, which underscores the need f
&lt;/p&gt;</description></item><item><title>DeFT&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;IO&#24847;&#35782;&#30340;&#26641;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;QKV&#20934;&#22791;&#21644;&#27880;&#24847;&#21147;&#35745;&#31639;&#38454;&#27573;&#23454;&#29616;&#20869;&#23384;&#39640;&#25928;&#30340;&#35745;&#31639;&#65292;&#38477;&#20302;&#20869;&#23384;&#21360;&#35760;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26641;&#35299;&#30721;&#31574;&#30053;&#21644;&#25512;&#26029;&#31995;&#32479;&#19981;&#36866;&#37197;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00242</link><description>&lt;p&gt;
DeFT&#65306;&#24102;IO&#24847;&#35782;&#30340;Flash Tree-attention&#29992;&#20110;&#39640;&#25928;&#30340;&#22522;&#20110;&#26641;&#25628;&#32034;&#30340;LLM&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00242
&lt;/p&gt;
&lt;p&gt;
DeFT&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;IO&#24847;&#35782;&#30340;&#26641;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;QKV&#20934;&#22791;&#21644;&#27880;&#24847;&#21147;&#35745;&#31639;&#38454;&#27573;&#23454;&#29616;&#20869;&#23384;&#39640;&#25928;&#30340;&#35745;&#31639;&#65292;&#38477;&#20302;&#20869;&#23384;&#21360;&#35760;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26641;&#35299;&#30721;&#31574;&#30053;&#21644;&#25512;&#26029;&#31995;&#32479;&#19981;&#36866;&#37197;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26641;&#25628;&#32034;&#36827;&#34892;&#35299;&#30721;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#26029;&#36136;&#37327;&#12290;&#26681;&#25454;&#24341;&#23548;&#20449;&#21495;&#65292;&#23427;&#36890;&#36807;&#24418;&#25104;LLM&#36755;&#20986;&#20174;&#26681;&#21040;&#21494;&#23376;&#30340;&#26368;&#20339;&#36335;&#24452;&#26469;&#25552;&#39640;&#21487;&#25511;&#24615;&#12289;&#25512;&#29702;&#33021;&#21147;&#12289;&#23545;&#40784;&#31561;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35745;&#31639;&#20887;&#20313;&#12289;&#20869;&#23384;&#21344;&#29992;&#21644;&#20869;&#23384;&#35775;&#38382;&#65292;&#24403;&#21069;&#30340;&#26641;&#35299;&#30721;&#31574;&#30053;&#21450;&#20854;&#25512;&#26029;&#31995;&#32479;&#20114;&#30456;&#19981;&#36866;&#37197;&#65292;&#23548;&#33268;&#25512;&#26029;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeFT&#65292;&#19968;&#31181;IO&#24863;&#30693;&#26641;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#23427;&#22312;&#20004;&#20010;&#38454;&#27573;&#20013;&#20445;&#25345;&#20869;&#23384;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#65292;&#38477;&#20302;&#20869;&#23384;&#21360;&#35760;&#65306;&#65288;1&#65289;QKV&#20934;&#22791;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;KV&#24341;&#23548;&#26641;&#20998;&#35010;&#31574;&#30053;&#65292;&#20026;GPU&#30340;&#39640;&#21033;&#29992;&#29575;&#21644;&#23613;&#21487;&#33021;&#20943;&#23569;GPU&#20840;&#23616;&#20869;&#23384;&#21644;&#33455;&#29255;&#19978;&#20849;&#20139;&#20869;&#23384;&#20043;&#38388;&#30340;KV&#32531;&#23384;&#30340;&#20869;&#23384;&#35835;/&#20889;; &#65288;2&#65289;&#27880;&#24847;&#21147;&#35745;&#31639;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00242v1 Announce Type: cross  Abstract: Decoding using tree search can greatly enhance the inference quality for transformer-based Large Language Models (LLMs). Depending on the guidance signal, it searches for the best path from root to leaf in the tree by forming LLM outputs to improve controllability, reasoning ability, alignment, et cetera. However, current tree decoding strategies and their inference systems do not suit each other well due to redundancy in computation, memory footprints, and memory access, resulting in inefficient inference. To address this issue, we propose DeFT, an IO-aware tree attention algorithm that maintains memory-efficient attention calculation with low memory footprints in two stages: (1) QKV Preparation: we propose a KV-Guided Tree Split strategy to group QKV wisely for high utilization of GPUs and reduction of memory reads/writes for the KV cache between GPU global memory and on-chip shared memory as much as possible; (2) Attention Calculati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#38463;&#25289;&#20271;&#35821;&#20041;&#25628;&#32034;&#30340;&#22522;&#20934;&#65292;&#24182;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26694;&#26550;&#20869;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18350</link><description>&lt;p&gt;
&#35780;&#20272;&#35821;&#20041;&#25628;&#32034;&#21450;&#20854;&#22312;&#38463;&#25289;&#20271;&#35821;&#35328;&#20013;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Semantic Search and its Role in Retrieved-Augmented-Generation (RAG) for Arabic Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#38463;&#25289;&#20271;&#35821;&#20041;&#25628;&#32034;&#30340;&#22522;&#20934;&#65292;&#24182;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26694;&#26550;&#20869;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#24102;&#26469;&#20102;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#27010;&#24565;&#65292;&#24050;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#35777;&#26126;&#20855;&#26377;&#24040;&#22823;&#30410;&#22788;&#65292;&#22823;&#22823;&#21462;&#20195;&#20102;&#20851;&#38190;&#35789;&#25628;&#32034;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#35821;&#20041;&#30456;&#20284;&#24615;&#24182;&#22312;&#21508;&#31181;&#25991;&#26723;&#20013;&#20026;&#29305;&#23450;&#26597;&#35810;&#36827;&#34892;&#25628;&#32034;&#20173;&#28982;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#36825;&#31181;&#22797;&#26434;&#24615;&#28304;&#20110;&#20219;&#21153;&#30340;&#22810;&#26041;&#38754;&#24615;&#65292;&#32570;&#20047;&#26631;&#20934;&#22522;&#20934;&#65292;&#32780;&#36825;&#20123;&#25361;&#25112;&#23545;&#20110;&#38463;&#25289;&#20271;&#35821;&#35328;&#32780;&#35328;&#26356;&#21152;&#22797;&#26434;&#12290;&#26412;&#25991;&#33268;&#21147;&#20110;&#20026;&#38463;&#25289;&#20271;&#35821;&#20041;&#25628;&#32034;&#24314;&#31435;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#31934;&#30830;&#35780;&#20272;&#36825;&#20123;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26694;&#26550;&#20869;&#36827;&#34892;&#35821;&#20041;&#25628;&#32034;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18350v1 Announce Type: new  Abstract: The latest advancements in machine learning and deep learning have brought forth the concept of semantic similarity, which has proven immensely beneficial in multiple applications and has largely replaced keyword search. However, evaluating semantic similarity and conducting searches for a specific query across various documents continue to be a complicated task. This complexity is due to the multifaceted nature of the task, the lack of standard benchmarks, whereas these challenges are further amplified for Arabic language. This paper endeavors to establish a straightforward yet potent benchmark for semantic search in Arabic. Moreover, to precisely evaluate the effectiveness of these metrics and the dataset, we conduct our assessment of semantic search within the framework of retrieval augmented generation (RAG).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#20855;&#26377;&#22823;&#37327;&#35789;&#27719;&#37325;&#21472;&#30340;NLI&#25361;&#25112;&#25968;&#25454;&#38598;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#29305;&#23450;&#32467;&#26500;&#26102;&#20986;&#29616;&#30340;&#22833;&#36133;&#29616;&#35937;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#21306;&#20998;&#29305;&#23450;&#32467;&#26500;&#26102;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>https://arxiv.org/abs/2403.17760</link><description>&lt;p&gt;
&#26045;&#24037;&#22914;&#27492;&#22256;&#38590;&#65292;&#20197;&#33267;&#20110;&#21363;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#22240;&#38169;&#35823;&#21407;&#22240;&#32780;&#27491;&#30830;
&lt;/p&gt;
&lt;p&gt;
Constructions Are So Difficult That Even Large Language Models Get Them Right for the Wrong Reasons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#20855;&#26377;&#22823;&#37327;&#35789;&#27719;&#37325;&#21472;&#30340;NLI&#25361;&#25112;&#25968;&#25454;&#38598;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#29305;&#23450;&#32467;&#26500;&#26102;&#20986;&#29616;&#30340;&#22833;&#36133;&#29616;&#35937;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#21306;&#20998;&#29305;&#23450;&#32467;&#26500;&#26102;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#20004;&#26041;&#38754;&#29702;&#35299;&#30340;&#36129;&#29486;&#65306;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35282;&#24230;&#30475;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20855;&#26377;&#22823;&#37327;&#35789;&#27719;&#37325;&#21472;&#30340;NLI&#25361;&#25112;&#25968;&#25454;&#38598;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20102;&#27169;&#22411;&#20165;&#22522;&#20110;&#26631;&#35760;&#21306;&#21035;&#26469;&#21306;&#20998;&#34164;&#28085;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;GPT-4&#21644;Llama 2&#20197;&#24378;&#28872;&#30340;&#20559;&#35265;&#22833;&#36133;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21019;&#24314;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23376;&#20219;&#21153;&#65292;&#20197;&#35299;&#37322;&#36825;&#31181;&#22833;&#36133;&#12290;&#20174;&#35745;&#31639;&#35821;&#35328;&#23398;&#30340;&#35282;&#24230;&#30475;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#21253;&#21547;&#19977;&#31867;&#24418;&#23481;&#35789;&#30340;&#32467;&#26500;&#32452;&#65292;&#36825;&#20123;&#24418;&#23481;&#35789;&#26080;&#27861;&#36890;&#36807;&#34920;&#38754;&#29305;&#24449;&#26469;&#21306;&#20998;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20197;&#21508;&#31181;&#26041;&#24335;&#25506;&#31350;LLM&#23545;&#36825;&#20123;&#32467;&#26500;&#30340;&#29702;&#35299;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#21508;&#31181;&#26041;&#24335;&#19978;&#37117;&#26080;&#27861;&#21306;&#20998;&#23427;&#20204;&#65292;&#34920;&#26126;&#23427;&#20204;&#19981;&#36275;&#20197;&#20195;&#34920;&#23427;&#20204;&#30340;&#21547;&#20041;&#25110;&#25429;&#25417;&#30701;&#35821;&#22836;&#30340;&#35789;&#27719;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17760v1 Announce Type: new  Abstract: In this paper, we make a contribution that can be understood from two perspectives: from an NLP perspective, we introduce a small challenge dataset for NLI with large lexical overlap, which minimises the possibility of models discerning entailment solely based on token distinctions, and show that GPT-4 and Llama 2 fail it with strong bias. We then create further challenging sub-tasks in an effort to explain this failure. From a Computational Linguistics perspective, we identify a group of constructions with three classes of adjectives which cannot be distinguished by surface features. This enables us to probe for LLM's understanding of these constructions in various ways, and we find that they fail in a variety of ways to distinguish between them, suggesting that they don't adequately represent their meaning or capture the lexical properties of phrasal heads.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31995;&#32479;&#35780;&#20215;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#20854;&#22312;&#26089;&#26399;&#31579;&#26597;&#12289;&#25968;&#23383;&#24178;&#39044;&#21644;&#20854;&#20182;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;</title><link>https://arxiv.org/abs/2403.15401</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#31995;&#32479;&#35780;&#20215;
&lt;/p&gt;
&lt;p&gt;
Large Language Model for Mental Health: A Systematic Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15401
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31995;&#32479;&#35780;&#20215;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#20854;&#22312;&#26089;&#26399;&#31579;&#26597;&#12289;&#25968;&#23383;&#24178;&#39044;&#21644;&#20854;&#20182;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25968;&#23383;&#20581;&#24247;&#39046;&#22495;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23637;&#29616;&#20986;&#20102;&#28508;&#22312;&#30340;&#24212;&#29992;&#24615;&#65292;&#20294;&#23427;&#20204;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#24212;&#29992;&#20173;&#22312;&#25345;&#32493;&#35752;&#35770;&#20013;&#12290;&#36825;&#39033;&#31995;&#32479;&#24615;&#35780;&#20215;&#26088;&#22312;&#24635;&#32467;&#21644;&#34920;&#24449;LLMs&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#35843;&#26597;LLMs&#26368;&#26032;&#30740;&#31350;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#35752;&#35770;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#26089;&#26399;&#31579;&#26597;&#12289;&#25968;&#23383;&#24178;&#39044;&#20197;&#21450;&#20854;&#20182;&#20020;&#24202;&#24212;&#29992;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#26681;&#25454;PRISMA&#25351;&#21335;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;PubMed&#12289;DBLP&#35745;&#31639;&#26426;&#31185;&#23398;&#25991;&#29486;&#25968;&#25454;&#24211;&#21644;IEEE Xplore&#19978;&#21457;&#34920;&#30340;&#33521;&#25991;&#25991;&#31456;&#65292;&#26102;&#38388;&#36328;&#24230;&#20026;2017&#24180;1&#26376;1&#26085;&#33267;2023&#24180;9&#26376;1&#26085;&#65292;&#37325;&#28857;&#20851;&#27880;&#24515;&#29702;&#20581;&#24247;&#21644;LLMs&#12290;&#35813;&#32508;&#36848;&#20998;&#26512;&#20102;32&#31687;&#25991;&#31456;&#65292;&#21253;&#25324;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#38598;&#36827;&#34892;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#30340;&#65288;n=13&#65289;&#12289;&#24515;&#29702;&#20581;&#24247;&#32842;&#22825;&#26426;&#22120;&#20154;&#65288;n=10&#65289;&#20197;&#21450;&#20854;&#20182;&#24515;&#29702;&#20581;&#24247;&#24212;&#29992;&#65288;n=9&#65289;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;LLMs&#22312;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15401v1 Announce Type: cross  Abstract: Large language models (LLMs) have received much attention and shown their potential in digital health, while their application in mental health is subject to ongoing debate. This systematic review aims to summarize and characterize the use of LLMs in mental health by investigating the strengths and limitations of the latest work in LLMs and discusses the challenges and opportunities for early screening, digital interventions, and other clinical applications in mental health. Following PRISMA guidelines, we examined English articles from PubMed, DBLP Computer Science Bibliography, and IEEE Xplore, published between 1 January 2017, and 1 September 2023, focusing on mental health and LLMs. The review analyzed 32 articles, including mental health analysis using social media datasets (n=13), mental health chatbots (n=10), and other mental health applications (n=9). Findings reveal LLMs' effectiveness in mental health issue detection and the
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#23884;&#20837;&#33021;&#22815;&#25429;&#25417;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#65292;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;</title><link>https://arxiv.org/abs/2403.15112</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#23884;&#20837;&#36827;&#34892;&#25991;&#26412;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text clustering with LLM embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15112
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#23884;&#20837;&#33021;&#22815;&#25429;&#25417;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#65292;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#32858;&#31867;&#26159;&#32452;&#32455;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#23383;&#20869;&#23481;&#30340;&#37325;&#35201;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#32467;&#26500;&#21270;&#21644;&#21457;&#29616;&#26410;&#20998;&#31867;&#25968;&#25454;&#20013;&#30340;&#38544;&#34255;&#27169;&#24335;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19981;&#21516;&#25991;&#26412;&#23884;&#20837;&#65288;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#20013;&#20351;&#29992;&#30340;&#65289;&#21644;&#32858;&#31867;&#31639;&#27861;&#22914;&#20309;&#24433;&#21709;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#26041;&#24335;&#12290;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#20197;&#35780;&#20272;&#23884;&#20837;&#26159;&#22914;&#20309;&#24433;&#21709;&#32858;&#31867;&#32467;&#26524;&#30340;&#65292;&#20197;&#21450;&#36890;&#36807;&#25688;&#35201;&#36827;&#34892;&#38477;&#32500;&#21644;&#23884;&#20837;&#22823;&#23567;&#35843;&#25972;&#30340;&#20316;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;LLM&#23884;&#20837;&#22312;&#25429;&#33719;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;&#65292;&#36825;&#34920;&#26126;&#36825;&#20123;&#31574;&#30053;&#38656;&#35201;&#20180;&#32454;&#20998;&#26512;&#25165;&#33021;&#22312;&#23454;&#38469;&#27169;&#22411;&#20013;&#20351;&#29992;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#20986;&#20102;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15112v1 Announce Type: cross  Abstract: Text clustering is an important approach for organising the growing amount of digital content, helping to structure and find hidden patterns in uncategorised data. In this research, we investigated how different textual embeddings - particularly those used in large language models (LLMs) - and clustering algorithms affect how text datasets are clustered. A series of experiments were conducted to assess how embeddings influence clustering results, the role played by dimensionality reduction through summarisation, and embedding size adjustment. Results reveal that LLM embeddings excel at capturing the nuances of structured language, while BERT leads the lightweight options in performance. In addition, we find that increasing embedding dimensionality and summarisation techniques do not uniformly improve clustering efficiency, suggesting that these strategies require careful analysis to use in real-life models. These results highlight a co
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;CICLe&#26694;&#26550;&#65292;&#22522;&#20110;&#36866;&#24212;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#22312;&#22823;&#35268;&#27169;&#22810;&#31867;&#39135;&#21697;&#39118;&#38505;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#31526;&#21512;&#39044;&#27979;&#30340;LLM-in-the-loop&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#39640;&#22522;&#26412;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#20943;&#23569;&#33021;&#28304;&#28040;&#32791;&#12290;</title><link>https://arxiv.org/abs/2403.11904</link><description>&lt;p&gt;
CICLe: &#36866;&#24212;&#19978;&#19979;&#25991;&#30340;&#22823;&#35268;&#27169;&#22810;&#31867;&#39135;&#21697;&#39118;&#38505;&#20998;&#31867;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CICLe: Conformal In-Context Learning for Largescale Multi-Class Food Risk Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11904
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;CICLe&#26694;&#26550;&#65292;&#22522;&#20110;&#36866;&#24212;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#22312;&#22823;&#35268;&#27169;&#22810;&#31867;&#39135;&#21697;&#39118;&#38505;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#31526;&#21512;&#39044;&#27979;&#30340;LLM-in-the-loop&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#39640;&#22522;&#26412;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#20943;&#23569;&#33021;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#27745;&#26579;&#25110;&#25530;&#20551;&#39135;&#21697;&#23545;&#20154;&#31867;&#20581;&#24247;&#26500;&#25104;&#37325;&#22823;&#39118;&#38505;&#12290;&#22312;&#32473;&#23450;&#20102;&#29992;&#20110;&#35757;&#32451;&#30340;&#26631;&#35760;&#32593;&#32476;&#25991;&#26412;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26469;&#33258;&#21160;&#26816;&#27979;&#36825;&#31181;&#39118;&#38505;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;7,546&#20010;&#25551;&#36848;&#20844;&#20849;&#39135;&#21697;&#21484;&#22238;&#20844;&#21578;&#30340;&#30701;&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#25991;&#26412;&#37117;&#32463;&#36807;&#25163;&#21160;&#26631;&#35760;&#65292;&#20998;&#20026;&#20004;&#20010;&#31890;&#24230;&#32423;&#21035;&#65288;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#65289;&#65292;&#29992;&#20110;&#34920;&#31034;&#21484;&#22238;&#23545;&#24212;&#30340;&#39135;&#21697;&#20135;&#21697;&#21644;&#21361;&#23475;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25968;&#25454;&#38598;&#24182;&#23545;&#26420;&#32032;&#12289;&#20256;&#32479;&#21644;Transformer&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#22522;&#20110;tf-idf&#34920;&#31034;&#30340;&#36923;&#36753;&#22238;&#24402;&#22312;&#25903;&#25345;&#36739;&#20302;&#30340;&#31867;&#21035;&#19978;&#20248;&#20110;RoBERTa&#21644;XLM-R&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31526;&#21512;&#39044;&#27979;&#30340;LLM-in-the-loop&#26694;&#26550;&#65292;&#36825;&#21487;&#20197;&#25552;&#39640;&#22522;&#26412;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#19982;&#26222;&#36890;&#25552;&#31034;&#30456;&#27604;&#30340;&#33021;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11904v1 Announce Type: new  Abstract: Contaminated or adulterated food poses a substantial risk to human health. Given sets of labeled web texts for training, Machine Learning and Natural Language Processing can be applied to automatically detect such risks. We publish a dataset of 7,546 short texts describing public food recall announcements. Each text is manually labeled, on two granularity levels (coarse and fine), for food products and hazards that the recall corresponds to. We describe the dataset and benchmark naive, traditional, and Transformer models. Based on our analysis, Logistic Regression based on a tf-idf representation outperforms RoBERTa and XLM-R on classes with low support. Finally, we discuss different prompting strategies and present an LLM-in-the-loop framework, based on Conformal Prediction, which boosts the performance of the base classifier while reducing energy consumption compared to normal prompting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24490;&#29615;&#33609;&#31295;&#26426;&#21046;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#21452;&#27169;&#22411;&#21644;&#26368;&#26032;&#21333;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#36816;&#29992;&#24490;&#29615;&#20381;&#36182;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#27979;&#35299;&#30721;&#12290;</title><link>https://arxiv.org/abs/2403.09919</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29992;&#20110;&#24555;&#36895;&#25512;&#27979;&#35299;&#30721;&#30340;&#24490;&#29615;&#33609;&#31295;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Recurrent Drafter for Fast Speculative Decoding in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24490;&#29615;&#33609;&#31295;&#26426;&#21046;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#21452;&#27169;&#22411;&#21644;&#26368;&#26032;&#21333;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#36816;&#29992;&#24490;&#29615;&#20381;&#36182;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#27979;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#25913;&#36827;&#30340;&#25512;&#27979;&#35299;&#30721;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#20004;&#31181;&#25104;&#29087;&#25216;&#26415;&#30340;&#20248;&#21183;&#65306;&#32463;&#20856;&#30340;&#21452;&#27169;&#22411;&#25512;&#27979;&#35299;&#30721;&#26041;&#27861;&#21644;&#36739;&#26032;&#30340;&#21333;&#27169;&#22411;&#26041;&#27861;Medusa&#12290;&#20174;Medusa&#24471;&#21040;&#28789;&#24863;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#21333;&#27169;&#22411;&#31574;&#30053;&#36827;&#34892;&#25512;&#27979;&#35299;&#30721;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#24490;&#29615;&#20381;&#36182;&#35774;&#35745;&#30340;&#21333;&#20010;&#36731;&#37327;&#32423;&#33609;&#31295;&#22836;&#26469;&#21306;&#20998;&#33258;&#24049;&#65292;&#26412;&#36136;&#19978;&#31867;&#20284;&#20110;&#32463;&#20856;&#25512;&#27979;&#35299;&#30721;&#20013;&#20351;&#29992;&#30340;&#23567;&#22411;&#33609;&#31295;&#27169;&#22411;&#65292;&#20294;&#36991;&#20813;&#20102;&#23436;&#25972;transformer&#26550;&#26500;&#30340;&#22797;&#26434;&#24615;&#12290;&#30001;&#20110;&#24490;&#29615;&#20381;&#36182;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#27874;&#26463;&#25628;&#32034;&#24555;&#36895;&#36807;&#28388;&#20986;&#33609;&#31295;&#22836;&#20013;&#19981;&#38656;&#35201;&#30340;&#20505;&#36873;&#39033;&#12290;&#20854;&#32467;&#26524;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#21333;&#27169;&#22411;&#35774;&#35745;&#31616;&#26131;&#24615;&#24182;&#36991;&#20813;&#20102;&#21019;&#24314;&#25968;&#25454;&#30456;&#20851;&#26641;&#20381;&#36182;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09919v1 Announce Type: new  Abstract: In this paper, we introduce an improved approach of speculative decoding aimed at enhancing the efficiency of serving large language models. Our method capitalizes on the strengths of two established techniques: the classic two-model speculative decoding approach, and the more recent single-model approach, Medusa. Drawing inspiration from Medusa, our approach adopts a single-model strategy for speculative decoding. However, our method distinguishes itself by employing a single, lightweight draft head with a recurrent dependency design, akin in essence to the small, draft model uses in classic speculative decoding, but without the complexities of the full transformer architecture. And because of the recurrent dependency, we can use beam search to swiftly filter out undesired candidates with the draft head. The outcome is a method that combines the simplicity of single-model design and avoids the need to create a data-dependent tree attent
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SPA&#65288;Side Plugin Adaption&#65289;&#30340;&#36731;&#37327;&#32423;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#20005;&#26684;&#30340;&#35774;&#22791;&#35745;&#31639;&#21644;&#20869;&#23384;&#32422;&#26463;&#26465;&#20214;&#19979;&#24555;&#36895;&#36827;&#34892;&#25512;&#26029;&#65292;&#21516;&#26102;&#20445;&#30041;&#38544;&#31169;&#12290;</title><link>https://arxiv.org/abs/2403.07088</link><description>&lt;p&gt;
SPA&#65306;&#38754;&#21521;&#20113;&#31471;&#21644;&#35774;&#22791;&#21327;&#20316;&#30340;&#35745;&#31639;&#21451;&#22909;&#22411;Seq2seq&#20010;&#24615;&#21270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07088
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SPA&#65288;Side Plugin Adaption&#65289;&#30340;&#36731;&#37327;&#32423;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#20005;&#26684;&#30340;&#35774;&#22791;&#35745;&#31639;&#21644;&#20869;&#23384;&#32422;&#26463;&#26465;&#20214;&#19979;&#24555;&#36895;&#36827;&#34892;&#25512;&#26029;&#65292;&#21516;&#26102;&#20445;&#30041;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34920;&#29616;&#20986;&#33394;&#30340;&#33021;&#21147;&#24050;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#38382;&#31572;&#20013;&#24471;&#21040;&#23637;&#31034;&#12290;&#28982;&#32780;&#65292;LLMs&#38656;&#35201;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#22823;&#20869;&#23384;&#25104;&#26412;&#12290;&#21516;&#26102;&#65292;&#24403;&#35757;&#32451;&#25110;&#39044;&#27979;&#36807;&#31243;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#26102;&#65292;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#38544;&#31169;&#27844;&#38706;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SPA&#65288;Side Plugin Adaption&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#26550;&#26500;&#65292;&#29992;&#20110;&#24555;&#36895;&#35774;&#22791;&#19978;&#30340;&#25512;&#26029;&#21644;&#22312;&#20005;&#26684;&#30340;&#35774;&#22791;&#35745;&#31639;&#21644;&#20869;&#23384;&#32422;&#26463;&#26465;&#20214;&#19979;&#20445;&#25345;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07088v1 Announce Type: new  Abstract: Large language models(LLMs) have shown its outperforming ability on various tasks and question answering. However, LLMs require high computation cost and large memory cost. At the same time, LLMs may cause privacy leakage when training or prediction procedure contains sensitive information. In this paper, we propose SPA(Side Plugin Adaption), a lightweight architecture for fast on-devices inference and privacy retaining on the constraints of strict on-devices computation and memory constraints. Compared with other on-devices seq2seq generation, SPA could make a fast and stable inference on low-resource constraints, allowing it to obtain cost effiency. Our method establish an interaction between a pretrained LLMs on-cloud and additive parameters on-devices, which could provide the knowledge on both pretrained LLMs and private personal feature.Further more, SPA provides a framework to keep feature-base parameters on private guaranteed but 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#38024;&#23545;&#38463;&#25289;&#20271;&#30005;&#35805;&#23545;&#35805;&#39046;&#22495;&#25361;&#25112;&#30340;&#20840;&#38754;&#35780;&#20272;&#22522;&#20934;&#65292;&#26088;&#22312;&#20026;&#24320;&#21457;&#21644;&#35780;&#20272;&#33021;&#22815;&#36866;&#24212;&#30495;&#23454;&#30005;&#35805;&#36890;&#35759;&#26465;&#20214;&#30340;ASR&#31995;&#32479;&#25552;&#20379;&#19968;&#20010;&#20005;&#26684;&#30340;&#27979;&#35797;&#24179;&#21488;&#12290;</title><link>https://arxiv.org/abs/2403.04280</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;&#38463;&#25289;&#20271;&#21628;&#21483;&#39046;&#22495;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#26032;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A New Benchmark for Evaluating Automatic Speech Recognition in the Arabic Call Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04280
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#38024;&#23545;&#38463;&#25289;&#20271;&#30005;&#35805;&#23545;&#35805;&#39046;&#22495;&#25361;&#25112;&#30340;&#20840;&#38754;&#35780;&#20272;&#22522;&#20934;&#65292;&#26088;&#22312;&#20026;&#24320;&#21457;&#21644;&#35780;&#20272;&#33021;&#22815;&#36866;&#24212;&#30495;&#23454;&#30005;&#35805;&#36890;&#35759;&#26465;&#20214;&#30340;ASR&#31995;&#32479;&#25552;&#20379;&#19968;&#20010;&#20005;&#26684;&#30340;&#27979;&#35797;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#24341;&#20837;&#19968;&#20010;&#20840;&#38754;&#30340;&#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#38899;&#35782;&#21035;&#35780;&#20272;&#30340;&#22522;&#20934;&#65292;&#19987;&#38376;&#38024;&#23545;&#38463;&#25289;&#20271;&#35821;&#30005;&#35805;&#23545;&#35805;&#20013;&#30340;&#25361;&#25112;&#36827;&#34892;&#20102;&#23450;&#21046;&#12290;&#38463;&#25289;&#20271;&#35821;&#20197;&#20854;&#20016;&#23500;&#30340;&#26041;&#35328;&#22810;&#26679;&#24615;&#21644;&#35821;&#38899;&#22797;&#26434;&#24615;&#32780;&#38395;&#21517;&#65292;&#23545;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#25552;&#20986;&#20102;&#35768;&#22810;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#22312;&#30005;&#35805;&#36890;&#35805;&#39046;&#22495;&#36827;&#19968;&#27493;&#25918;&#22823;&#65292;&#37027;&#37324;&#30340;&#38899;&#39057;&#36136;&#37327;&#12289;&#32972;&#26223;&#22122;&#38899;&#21644;&#20250;&#35805;&#24335;&#35821;&#38899;&#39118;&#26684;&#20250;&#23545;&#35782;&#21035;&#20934;&#30830;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#31283;&#20581;&#30340;&#22522;&#20934;&#65292;&#19981;&#20165;&#21253;&#21547;&#24191;&#27867;&#30340;&#38463;&#25289;&#20271;&#26041;&#35328;&#33539;&#22260;&#65292;&#36824;&#27169;&#25311;&#21628;&#21483;&#36890;&#35759;&#30340;&#30495;&#23454;&#26465;&#20214;&#12290;&#36890;&#36807;&#32467;&#21512;&#22810;&#26679;&#30340;&#26041;&#35328;&#34920;&#36798;&#65292;&#24182;&#32771;&#34385;&#21628;&#21483;&#24405;&#38899;&#30340;&#21487;&#21464;&#36136;&#37327;&#65292;&#36825;&#20010;&#22522;&#20934;&#26088;&#22312;&#20026;&#24320;&#21457;&#21644;&#35780;&#20272;&#33021;&#22815;&#24212;&#23545;&#23454;&#38469;&#25361;&#25112;&#30340;ASR&#31995;&#32479;&#25552;&#20379;&#20005;&#26684;&#30340;&#27979;&#35797;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04280v1 Announce Type: new  Abstract: This work is an attempt to introduce a comprehensive benchmark for Arabic speech recognition, specifically tailored to address the challenges of telephone conversations in Arabic language. Arabic, characterized by its rich dialectal diversity and phonetic complexity, presents a number of unique challenges for automatic speech recognition (ASR) systems. These challenges are further amplified in the domain of telephone calls, where audio quality, background noise, and conversational speech styles negatively affect recognition accuracy. Our work aims to establish a robust benchmark that not only encompasses the broad spectrum of Arabic dialects but also emulates the real-world conditions of call-based communications. By incorporating diverse dialectical expressions and accounting for the variable quality of call recordings, this benchmark seeks to provide a rigorous testing ground for the development and evaluation of ASR systems capable of
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25299;&#23637;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#27602;&#24615;&#32531;&#35299;&#30340;&#33539;&#22260;&#65292;&#28085;&#30422;&#20102;&#22810;&#35821;&#35328;&#29615;&#22659;&#65292;&#36890;&#36807;&#32763;&#35793;&#25968;&#25454;&#35780;&#20272;&#21644;&#22686;&#24378;&#32531;&#35299;&#25216;&#26415;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#32531;&#35299;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#27169;&#22411;&#22823;&#23567;&#21644;&#25968;&#25454;&#37327;&#23545;&#32531;&#35299;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.03893</link><description>&lt;p&gt;
&#20174;&#21333;&#19968;&#21040;&#22810;&#26679;&#65306;&#25299;&#23637;&#35821;&#35328;&#27169;&#22411;&#20013;&#27602;&#24615;&#32531;&#35299;&#30340;&#33539;&#22260;
&lt;/p&gt;
&lt;p&gt;
From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03893
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25299;&#23637;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#27602;&#24615;&#32531;&#35299;&#30340;&#33539;&#22260;&#65292;&#28085;&#30422;&#20102;&#22810;&#35821;&#35328;&#29615;&#22659;&#65292;&#36890;&#36807;&#32763;&#35793;&#25968;&#25454;&#35780;&#20272;&#21644;&#22686;&#24378;&#32531;&#35299;&#25216;&#26415;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#32531;&#35299;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#27169;&#22411;&#22823;&#23567;&#21644;&#25968;&#25454;&#37327;&#23545;&#32531;&#35299;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36804;&#20170;&#20026;&#27490;&#65292;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27602;&#24615;&#32531;&#35299;&#20960;&#20046;&#23436;&#20840;&#38598;&#20013;&#22312;&#21333;&#35821;&#35328;&#29615;&#22659;&#20013;&#12290;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#25317;&#25265;&#22810;&#35821;&#35328;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#23433;&#20840;&#25514;&#26045;&#36319;&#19978;&#27493;&#20240;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#24847;&#35782;&#21040;&#20102;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20256;&#32479;&#30340;&#27602;&#24615;&#32531;&#35299;&#33539;&#22260;&#25193;&#23637;&#21040;&#24212;&#23545;&#22810;&#35821;&#35328;&#24102;&#26469;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#32570;&#20047;&#36328;&#35821;&#35328;&#30340;&#36275;&#22815;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#32763;&#35793;&#25968;&#25454;&#26469;&#35780;&#20272;&#21644;&#22686;&#24378;&#25105;&#20204;&#30340;&#32531;&#35299;&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#22312;&#38745;&#24577;&#21644;&#25345;&#32493;&#27602;&#24615;&#32531;&#35299;&#22330;&#26223;&#19979;&#27604;&#36739;&#20102;&#24494;&#35843;&#32531;&#35299;&#26041;&#27861;&#21644;&#26816;&#32034;&#22686;&#24378;&#25216;&#26415;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#26816;&#39564;&#32763;&#35793;&#36136;&#37327;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#23545;&#27602;&#24615;&#32531;&#35299;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#27169;&#22411;&#22823;&#23567;&#21644;&#25968;&#25454;&#25968;&#37327;&#22914;&#20309;&#24433;&#21709;&#36825;&#20123;&#32531;&#35299;&#24037;&#20316;&#30340;&#25104;&#21151;&#12290;&#28085;&#30422;&#20102;&#20061;&#31181;&#35821;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20195;&#34920;&#20102;&#24191;&#27867;&#30340;&#35821;&#35328;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03893v1 Announce Type: cross  Abstract: To date, toxicity mitigation in language models has almost entirely been focused on single-language settings. As language models embrace multilingual capabilities, it's crucial our safety measures keep pace. Recognizing this research gap, our approach expands the scope of conventional toxicity mitigation to address the complexities presented by multiple languages. In the absence of sufficient annotated datasets across languages, we employ translated data to evaluate and enhance our mitigation techniques. We also compare finetuning mitigation approaches against retrieval-augmented techniques under both static and continual toxicity mitigation scenarios. This allows us to examine the effects of translation quality and the cross-lingual transfer on toxicity mitigation. We also explore how model size and data quantity affect the success of these mitigation efforts. Covering nine languages, our study represents a broad array of linguistic f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#25928;&#29575;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#26631;&#20934;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#37096;&#32626;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.01643</link><description>&lt;p&gt;
&#24744;&#38656;&#35201;&#26356;&#22909;&#22320;&#20851;&#27880;&#20184;&#36153;
&lt;/p&gt;
&lt;p&gt;
You Need to Pay Better Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01643
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#25928;&#29575;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#26631;&#20934;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#37096;&#32626;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36825;&#20123;&#26426;&#21046;&#22312;&#25928;&#29575;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#32988;&#36807;&#26631;&#20934;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#37096;&#32626;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#20248;&#21270;&#27880;&#24847;&#21147;&#65292;&#20854;&#24615;&#33021;&#19982;&#26631;&#20934;&#27880;&#24847;&#21147;&#30456;&#20284;&#65292;&#20294;&#21442;&#25968;&#25968;&#37327;&#23569;&#20102;&#22235;&#20998;&#20043;&#19977;&#65292;&#27599;&#20010;&#22836;&#37096;&#23569;&#20102;&#19968;&#20010;&#30697;&#38453;&#20056;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39640;&#25928;&#27880;&#24847;&#21147;&#65292;&#20854;&#24615;&#33021;&#19982;&#26631;&#20934;&#27880;&#24847;&#21147;&#30456;&#24403;&#65292;&#20294;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569;&#20102;&#19968;&#21322;&#65292;&#27599;&#20010;&#22836;&#37096;&#20943;&#23569;&#20102;&#20004;&#20010;&#30697;&#38453;&#20056;&#27861;&#65292;&#24182;&#19988;&#27604;&#26631;&#20934;&#27880;&#24847;&#21147;&#24555;&#20004;&#20493;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36229;&#32423;&#27880;&#24847;&#21147;&#65292;&#22312;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#26126;&#26174;&#36229;&#36234;&#20102;&#26631;&#20934;&#27880;&#24847;&#21147;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#30697;&#38453;&#20056;&#27861;&#12290;&#38500;&#20102;&#25552;&#20379;&#20005;&#26684;&#30340;&#25968;&#23398;&#27604;&#36739;&#65292;&#25105;&#20204;&#22312;MN&#20013;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01643v1 Announce Type: cross  Abstract: We introduce three new attention mechanisms that outperform standard multi-head attention in terms of efficiency and learning capabilities, thereby improving the performance and broader deployability of Transformer models. Our first contribution is Optimised Attention, which performs similarly to standard attention, but has 3/4 as many parameters and one matrix multiplication fewer per head. Next, we introduce Efficient Attention, which performs on par with standard attention with only 1/2 as many parameters as many parameters and two matrix multiplications fewer per head and is up to twice as fast as standard attention. Lastly, we introduce Super Attention, which surpasses standard attention by a significant margin in both vision and natural language processing tasks while having fewer parameters and matrix multiplications. In addition to providing rigorous mathematical comparisons, we evaluate the presented attention mechanisms on MN
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31070;&#32463;&#28608;&#27963;&#32447;&#24615;&#35299;&#26512;&#35821;&#35328;&#27169;&#22411;&#20013;&#20195;&#29702;&#20154;&#35266;&#28857;&#19979;&#30340;&#20449;&#24565;&#29366;&#24577;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#34920;&#36848;&#33258;&#25105;&#21644;&#20182;&#20154;&#20449;&#24565;&#65292;&#36825;&#23545;&#31038;&#20250;&#25512;&#29702;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#22312;&#22810;&#26679;&#31038;&#20250;&#25512;&#29702;&#20219;&#21153;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18496</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#34920;&#36798;&#33258;&#25105;&#21644;&#20182;&#20154;&#20449;&#24565;
&lt;/p&gt;
&lt;p&gt;
Language Models Represent Beliefs of Self and Others
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18496
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#28608;&#27963;&#32447;&#24615;&#35299;&#26512;&#35821;&#35328;&#27169;&#22411;&#20013;&#20195;&#29702;&#20154;&#35266;&#28857;&#19979;&#30340;&#20449;&#24565;&#29366;&#24577;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#34920;&#36848;&#33258;&#25105;&#21644;&#20182;&#20154;&#20449;&#24565;&#65292;&#36825;&#23545;&#31038;&#20250;&#25512;&#29702;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#22312;&#22810;&#26679;&#31038;&#20250;&#25512;&#29702;&#20219;&#21153;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21644;&#24402;&#22240;&#24515;&#29702;&#29366;&#24577;&#65292;&#21363;&#24515;&#28789;&#29702;&#35770;&#65288;ToM&#65289;&#65292;&#34987;&#35270;&#20026;&#20154;&#31867;&#31038;&#20250;&#25512;&#29702;&#30340;&#22522;&#26412;&#33021;&#21147;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20284;&#20046;&#20855;&#26377;&#26576;&#20123;ToM&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#33021;&#21147;&#32972;&#21518;&#30340;&#26426;&#21046;&#20173;&#28982;&#20196;&#20154;&#36153;&#35299;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30340;&#31070;&#32463;&#28608;&#27963;&#32447;&#24615;&#35299;&#30721;&#21508;&#20010;&#20195;&#29702;&#20154;&#35266;&#28857;&#19979;&#30340;&#20449;&#24565;&#29366;&#24577;&#26159;&#21487;&#33021;&#30340;&#65292;&#36825;&#34920;&#26126;&#23384;&#22312;&#33258;&#25105;&#30340;&#20869;&#37096;&#34920;&#36848;&#21644;&#20182;&#20154;&#20449;&#24565;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#25805;&#32437;&#36825;&#20123;&#34920;&#24449;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#30340;ToM&#24615;&#33021;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#65292;&#31361;&#26174;&#20102;&#20854;&#22312;&#31038;&#20250;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#36824;&#24310;&#20280;&#21040;&#28041;&#21450;&#19981;&#21516;&#22240;&#26524;&#25512;&#29702;&#27169;&#24335;&#30340;&#22810;&#26679;&#31038;&#20250;&#25512;&#29702;&#20219;&#21153;&#65292;&#26263;&#31034;&#20102;&#36825;&#20123;&#34920;&#24449;&#30340;&#28508;&#22312;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18496v1 Announce Type: new  Abstract: Understanding and attributing mental states, known as Theory of Mind (ToM), emerges as a fundamental capability for human social reasoning. While Large Language Models (LLMs) appear to possess certain ToM abilities, the mechanisms underlying these capabilities remain elusive. In this study, we discover that it is possible to linearly decode the belief status from the perspectives of various agents through neural activations of language models, indicating the existence of internal representations of self and others' beliefs. By manipulating these representations, we observe dramatic changes in the models' ToM performance, underscoring their pivotal role in the social reasoning process. Additionally, our findings extend to diverse social reasoning tasks that involve different causal inference patterns, suggesting the potential generalizability of these representations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25506;&#35752;&#20102;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#26426;&#22120;&#36951;&#24536;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#36951;&#24536;&#26694;&#26550;&#21450;&#39640;&#25928;&#30340;&#36951;&#24536;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25913;&#36827;&#36229;&#21442;&#25968;&#40065;&#26834;&#24615;&#20197;&#21450;&#39640;&#25928;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#25351;&#21335;&#12290;</title><link>https://arxiv.org/abs/2402.15159</link><description>&lt;p&gt;
&#38754;&#21521;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning of Pre-trained Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25506;&#35752;&#20102;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#26426;&#22120;&#36951;&#24536;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#36951;&#24536;&#26694;&#26550;&#21450;&#39640;&#25928;&#30340;&#36951;&#24536;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25913;&#36827;&#36229;&#21442;&#25968;&#40065;&#26834;&#24615;&#20197;&#21450;&#39640;&#25928;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32972;&#26223;&#19979;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#20197;&#26426;&#22120;&#36951;&#24536;&#20316;&#20026;&#19968;&#20010;&#20851;&#38190;&#35299;&#20915;&#26041;&#26696;&#65292;&#37325;&#28857;&#20851;&#27880;&#39044;&#35757;&#32451;&#27169;&#22411;&#8212;&#8212;&#19968;&#20010;&#26126;&#26174;&#32570;&#20047;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;LLMs&#20013;&#21246;&#21202;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26426;&#22120;&#36951;&#24536;&#26694;&#26550;&#65292;&#21253;&#25324;&#23545;&#19971;&#31181;&#19981;&#21516;&#36951;&#24536;&#26041;&#27861;&#30340;&#25209;&#21028;&#24615;&#20998;&#26512;&#12290;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;arXiv&#12289;&#20070;&#31821;&#21644;GitHub&#30340;&#31574;&#21010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26377;&#21147;&#30340;&#26426;&#22120;&#36951;&#24536;&#24615;&#33021;&#22522;&#20934;&#65292;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#27604;&#37325;&#26032;&#35757;&#32451;&#39640;&#20986; $10^5$ &#20493;&#20197;&#19978;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20998;&#24067;&#25968;&#25454;&#19978;&#23558;&#26799;&#24230;&#19978;&#21319;&#19982;&#26799;&#24230;&#19979;&#38477;&#32467;&#21512;&#21487;&#20197;&#25913;&#21892;&#36229;&#21442;&#25968;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#36951;&#24536;&#36807;&#31243;&#20013;&#36827;&#34892;&#39640;&#25928;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#35814;&#32454;&#25351;&#21335;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25512;&#21160;&#20102;&#26377;&#20851;&#20262;&#29702;&#20154;&#24037;&#26234;&#33021;&#23454;&#36341;&#30340;&#35752;&#35770;&#65292;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15159v1 Announce Type: cross  Abstract: This study investigates the concept of the `right to be forgotten' within the context of large language models (LLMs). We explore machine unlearning as a pivotal solution, with a focus on pre-trained models--a notably under-researched area. Our research delineates a comprehensive framework for machine unlearning in pre-trained LLMs, encompassing a critical analysis of seven diverse unlearning methods. Through rigorous evaluation using curated datasets from arXiv, books, and GitHub, we establish a robust benchmark for unlearning performance, demonstrating that these methods are over $10^5$ times more computationally efficient than retraining. Our results show that integrating gradient ascent with gradient descent on in-distribution data improves hyperparameter robustness. We also provide detailed guidelines for efficient hyperparameter tuning in the unlearning process. Our findings advance the discourse on ethical AI practices, offering
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#30340;RelayAttention&#31639;&#27861;&#26088;&#22312;&#25913;&#21892;&#28041;&#21450;&#38271;&#31995;&#32479;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#25928;&#29575;&#65292;&#36890;&#36807;&#19968;&#27425;&#24615;&#20174;DRAM&#35835;&#21462;&#38544;&#34255;&#29366;&#24577;&#26469;&#28040;&#38500;&#29616;&#26377;&#22240;&#26524;&#27880;&#24847;&#21147;&#31639;&#27861;&#20013;&#30340;&#20869;&#23384;&#35775;&#38382;&#20887;&#20313;&#12290;</title><link>https://arxiv.org/abs/2402.14808</link><description>&lt;p&gt;
RelayAttention&#65306;&#29992;&#20110;&#39640;&#25928;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#19982;&#38271;&#31995;&#32479;&#25552;&#31034;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
RelayAttention for Efficient Large Language Model Serving with Long System Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#30340;RelayAttention&#31639;&#27861;&#26088;&#22312;&#25913;&#21892;&#28041;&#21450;&#38271;&#31995;&#32479;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#25928;&#29575;&#65292;&#36890;&#36807;&#19968;&#27425;&#24615;&#20174;DRAM&#35835;&#21462;&#38544;&#34255;&#29366;&#24577;&#26469;&#28040;&#38500;&#29616;&#26377;&#22240;&#26524;&#27880;&#24847;&#21147;&#31639;&#27861;&#20013;&#30340;&#20869;&#23384;&#35775;&#38382;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26381;&#21153;&#21487;&#33021;&#28041;&#21450;&#19968;&#20010;&#38271;&#30340;&#31995;&#32479;&#25552;&#31034;&#65292;&#20854;&#20013;&#21253;&#21547;&#20219;&#21153;&#30340;&#25351;&#31034;&#12289;&#31034;&#20363;&#21644;&#30693;&#35782;&#25991;&#26723;&#65292;&#24182;&#22312;&#35768;&#22810;&#35831;&#27714;&#20013;&#22797;&#29992;&#12290;&#28982;&#32780;&#65292;&#38271;&#31995;&#32479;&#25552;&#31034;&#20250;&#23548;&#33268;&#21534;&#21520;&#37327;/&#24310;&#36831;&#29942;&#39048;&#65292;&#22240;&#20026;&#29983;&#25104;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#25104;&#26412;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#38271;&#32780;&#22686;&#21152;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#39640;&#28041;&#21450;&#38271;&#31995;&#32479;&#25552;&#31034;&#30340;LLM&#26381;&#21153;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#22788;&#29702;&#36825;&#20123;&#31995;&#32479;&#25552;&#31034;&#22312;&#29616;&#26377;&#22240;&#26524;&#27880;&#24847;&#21147;&#35745;&#31639;&#31639;&#27861;&#20013;&#38656;&#35201;&#22823;&#37327;&#20887;&#20313;&#30340;&#20869;&#23384;&#35775;&#38382;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23545;&#20110;&#25209;&#37327;&#35831;&#27714;&#65292;&#31995;&#32479;&#25552;&#31034;&#30340;&#32531;&#23384;&#38544;&#34255;&#29366;&#24577;&#65288;&#21363;&#38190;-&#20540;&#23545;&#65289;&#34987;&#22810;&#27425;&#20174;&#33455;&#29255;&#22806;&#30340;DRAM&#20256;&#36755;&#21040;&#33455;&#29255;&#19978;&#30340;SRAM&#65292;&#27599;&#27425;&#23545;&#24212;&#19968;&#20010;&#21333;&#29420;&#30340;&#35831;&#27714;&#12290;&#20026;&#20102;&#28040;&#38500;&#36825;&#31181;&#20887;&#20313;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RelayAttention&#65292;&#19968;&#31181;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#20801;&#35768;&#20165;&#20174;DRAM&#35835;&#21462;&#36825;&#20123;&#38544;&#34255;&#29366;&#24577;&#19968;&#27425;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14808v1 Announce Type: new  Abstract: Practical large language model (LLM) services may involve a long system prompt, which specifies the instructions, examples, and knowledge documents of the task and is reused across numerous requests. However, the long system prompt causes throughput/latency bottlenecks as the cost of generating the next token grows w.r.t. the sequence length. This paper aims to improve the efficiency of LLM services that involve long system prompts. Our key observation is that handling these system prompts requires heavily redundant memory accesses in existing causal attention computation algorithms. Specifically, for batched requests, the cached hidden states (i.e., key-value pairs) of system prompts are transferred from off-chip DRAM to on-chip SRAM multiple times, each corresponding to an individual request. To eliminate such a redundancy, we propose RelayAttention, an attention algorithm that allows reading these hidden states from DRAM exactly once 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19987;&#23478;&#32423;&#31232;&#30095;&#21270;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19987;&#23478;&#20462;&#21098;&#21644;&#36339;&#36807;&#30340;&#21518;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;MoE LLMs&#30340;&#37096;&#32626;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14800</link><description>&lt;p&gt;
&#24182;&#38750;&#25152;&#26377;&#19987;&#23478;&#37117;&#30456;&#31561;: &#28151;&#21512;&#19987;&#23478;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#19987;&#23478;&#20462;&#21098;&#21644;&#36339;&#36807;
&lt;/p&gt;
&lt;p&gt;
Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14800
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19987;&#23478;&#32423;&#31232;&#30095;&#21270;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19987;&#23478;&#20462;&#21098;&#21644;&#36339;&#36807;&#30340;&#21518;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;MoE LLMs&#30340;&#37096;&#32626;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#23637;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#36827;&#23637;&#26159;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;LLMs&#30340;&#20986;&#29616;&#12290;&#19982;&#20256;&#32479;&#30340;LLMs&#30456;&#27604;&#65292;MoE LLMs&#21487;&#20197;&#22312;&#26356;&#23569;&#30340;&#21442;&#25968;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#21442;&#25968;&#22823;&#23567;&#65292;&#20173;&#28982;&#24456;&#38590;&#37096;&#32626;&#23427;&#20204;&#12290;&#19982;&#20808;&#21069;&#20381;&#36182;&#20110;&#19987;&#38376;&#35774;&#35745;&#30340;&#30828;&#20214;&#30340;&#26435;&#37325;&#21098;&#26525;&#26041;&#27861;&#19981;&#21516;&#65292;&#26412;&#25991;&#20027;&#35201;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#21363;&#25554;&#21363;&#29992;&#30340;&#19987;&#23478;&#32423;&#31232;&#30095;&#21270;&#25216;&#26415;&#26469;&#25552;&#39640;MoE LLMs&#30340;&#37096;&#32626;&#25928;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#38024;&#23545;&#20219;&#21153;&#19981;&#21487;&#30693;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;MoE LLMs&#19987;&#23478;&#20462;&#21098;&#21644;&#36339;&#36807;&#30340;&#21518;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#22312;&#24191;&#27867;&#20219;&#21153;&#33539;&#22260;&#20869;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#39640;&#37096;&#32626;&#25928;&#29575;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#24182;&#22686;&#21152;&#25512;&#26029;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#39281;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14800v1 Announce Type: cross  Abstract: A pivotal advancement in the progress of large language models (LLMs) is the emergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs, MoE LLMs can achieve higher performance with fewer parameters, but it is still hard to deploy them due to their immense parameter sizes. Different from previous weight pruning methods that rely on specifically designed hardware, this paper mainly aims to enhance the deployment efficiency of MoE LLMs by introducing plug-and-play expert-level sparsification techniques. Specifically, we propose, for the first time to our best knowledge, post-training approaches for task-agnostic and task-specific expert pruning and skipping of MoE LLMs, tailored to improve deployment efficiency while maintaining model performance across a wide range of tasks. Extensive experiments show that our proposed methods can simultaneously reduce model sizes and increase the inference speed, while maintaining sat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#21306;&#22495;&#21010;&#20998;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#21457;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#24212;&#35821;&#35328;&#33021;&#21147;&#30340;&#26680;&#24515;&#21306;&#22495;&#65292;&#24182;&#23637;&#31034;&#20102;&#21435;&#38500;&#35813;&#21306;&#22495;&#20250;&#23548;&#33268;&#36328;30&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#26174;&#33879;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2402.14700</link><description>&lt;p&gt;
&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Unveiling Linguistic Regions in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#21306;&#22495;&#21010;&#20998;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#21457;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#24212;&#35821;&#35328;&#33021;&#21147;&#30340;&#26680;&#24515;&#21306;&#22495;&#65292;&#24182;&#23637;&#31034;&#20102;&#21435;&#38500;&#35813;&#21306;&#22495;&#20250;&#23548;&#33268;&#36328;30&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#26174;&#33879;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#23637;&#31034;&#20102;&#30456;&#24403;&#22823;&#30340;&#36328;&#35821;&#35328;&#23545;&#40784;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#24403;&#21069;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25913;&#21892;LLMs&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#19978;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;LLMs&#22914;&#20309;&#23454;&#29616;&#36328;&#35821;&#35328;&#23545;&#40784;&#30340;&#20869;&#22312;&#26426;&#21046;&#20173;&#28982;&#32570;&#20047;&#30740;&#31350;&#12290;&#20174;&#21306;&#22495;&#21010;&#20998;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#26412;&#25991;&#22312;LLMs&#30340;&#35821;&#35328;&#33021;&#21147;&#19978;&#36827;&#34892;&#20102;&#20960;&#39033;&#35843;&#26597;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#21306;&#22495;&#23545;&#24212;&#20110;&#35821;&#35328;&#33021;&#21147;&#65292;&#22823;&#32422;&#21344;&#24635;&#27169;&#22411;&#21442;&#25968;&#30340;1%&#12290;&#36890;&#36807;&#23558;&#21442;&#25968;&#35774;&#32622;&#20026;&#38646;&#26469;&#21435;&#38500;&#36825;&#20010;&#26680;&#24515;&#21306;&#22495;&#65292;&#20250;&#23548;&#33268;30&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#26174;&#33879;&#24615;&#33021;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#26680;&#24515;&#21306;&#22495;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#32500;&#24230;&#20381;&#36182;&#24615;&#65292;&#23545;&#29305;&#23450;&#32500;&#24230;&#19978;&#30340;&#21333;&#20010;&#21442;&#25968;&#30340;&#25200;&#21160;&#20250;&#23548;&#33268;&#35821;&#35328;&#33021;&#21147;&#30340;&#20007;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#29420;&#29305;&#30340;&#21306;&#22495;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14700v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated considerable cross-lingual alignment and generalization ability. Current research primarily focuses on improving LLMs' cross-lingual generalization capabilities. However, there is still a lack of research on the intrinsic mechanisms of how LLMs achieve cross-lingual alignment. From the perspective of region partitioning, this paper conducts several investigations on the linguistic competence of LLMs. We discover a core region in LLMs that corresponds to linguistic competence, accounting for approximately 1% of the total model parameters. Removing this core region by setting parameters to zero results in a significant performance decrease across 30 different languages. Furthermore, this core region exhibits significant dimensional dependency, perturbations to even a single parameter on specific dimensions leading to a loss of linguistic competence. Moreover, we discover that distinct regions 
&lt;/p&gt;</description></item><item><title>GradSafe&#36890;&#36807;&#20998;&#26512;LLMs&#20013;&#20851;&#38190;&#23433;&#20840;&#21442;&#25968;&#30340;&#26799;&#24230;&#65292;&#26377;&#25928;&#26816;&#27979;&#19981;&#23433;&#20840;&#25552;&#31034;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#21363;&#21487;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.13494</link><description>&lt;p&gt;
GradSafe: &#36890;&#36807;&#23433;&#20840;&#20851;&#38190;&#26799;&#24230;&#20998;&#26512;&#26816;&#27979;LLMs&#20013;&#30340;&#19981;&#23433;&#20840;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13494
&lt;/p&gt;
&lt;p&gt;
GradSafe&#36890;&#36807;&#20998;&#26512;LLMs&#20013;&#20851;&#38190;&#23433;&#20840;&#21442;&#25968;&#30340;&#26799;&#24230;&#65292;&#26377;&#25928;&#26816;&#27979;&#19981;&#23433;&#20840;&#25552;&#31034;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#21363;&#21487;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38754;&#20020;&#26469;&#33258;&#19981;&#23433;&#20840;&#25552;&#31034;&#30340;&#23041;&#32961;&#12290;&#29616;&#26377;&#30340;&#26816;&#27979;&#19981;&#23433;&#20840;&#25552;&#31034;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#22312;&#32447;&#20869;&#23481;&#23457;&#26680;API&#25110;&#24494;&#35843;LLMs&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31574;&#30053;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#21644;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GradSafe&#65292;&#36890;&#36807;&#20180;&#32454;&#23457;&#26597;LLMs&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#21442;&#25968;&#30340;&#26799;&#24230;&#26377;&#25928;&#22320;&#26816;&#27979;&#19981;&#23433;&#20840;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;LLMs&#23545;&#20110;&#19982;&#21512;&#35268;&#21709;&#24212;&#37197;&#23545;&#30340;&#19981;&#23433;&#20840;&#25552;&#31034;&#30340;&#25439;&#22833;&#30340;&#26799;&#24230;&#22312;&#26576;&#20123;&#23433;&#20840;&#20851;&#38190;&#21442;&#25968;&#19978;&#34920;&#29616;&#20986;&#30456;&#20284;&#30340;&#27169;&#24335;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23433;&#20840;&#25552;&#31034;&#23548;&#33268;&#26126;&#26174;&#19981;&#21516;&#30340;&#26799;&#24230;&#27169;&#24335;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;GradSafe&#20998;&#26512;&#26469;&#33258;&#25552;&#31034;&#65288;&#19982;&#21512;&#35268;&#21709;&#24212;&#37197;&#23545;&#65289;&#30340;&#26799;&#24230;&#20197;&#20934;&#30830;&#22320;&#26816;&#27979;&#19981;&#23433;&#20840;&#25552;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#24212;&#29992;&#20110;Llama-2&#30340;GradSafe&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#35757;&#32451;&#21363;&#21487;&#32988;&#36807;Llama Guard&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13494v1 Announce Type: new  Abstract: Large Language Models (LLMs) face threats from unsafe prompts. Existing methods for detecting unsafe prompts are primarily online moderation APIs or finetuned LLMs. These strategies, however, often require extensive and resource-intensive data collection and training processes. In this study, we propose GradSafe, which effectively detects unsafe prompts by scrutinizing the gradients of safety-critical parameters in LLMs. Our methodology is grounded in a pivotal observation: the gradients of an LLM's loss for unsafe prompts paired with compliance response exhibit similar patterns on certain safety-critical parameters. In contrast, safe prompts lead to markedly different gradient patterns. Building on this observation, GradSafe analyzes the gradients from prompts (paired with compliance responses) to accurately detect unsafe prompts. We show that GradSafe, applied to Llama-2 without further training, outperforms Llama Guard, despite its ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#21475;&#35821;&#30340;&#19981;&#21516;&#35821;&#35328;&#39118;&#26684;&#20570;&#20986;&#24688;&#24403;&#22238;&#24212;&#65292;&#24182;&#20026;&#27492;&#25910;&#38598;&#20102;&#19968;&#32452;&#36866;&#21512;&#35757;&#32451;&#30340;&#35821;&#38899;&#23545;&#35821;&#38899;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.12786</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#36215;&#26469;&#65292;&#20197;&#25429;&#25417;&#19981;&#21516;&#35821;&#35328;&#39118;&#26684;&#24182;&#22312;&#21475;&#35821;&#20132;&#27969;&#20013;&#20570;&#20986;&#24688;&#24403;&#22238;&#24212;
&lt;/p&gt;
&lt;p&gt;
Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#21475;&#35821;&#30340;&#19981;&#21516;&#35821;&#35328;&#39118;&#26684;&#20570;&#20986;&#24688;&#24403;&#22238;&#24212;&#65292;&#24182;&#20026;&#27492;&#25910;&#38598;&#20102;&#19968;&#32452;&#36866;&#21512;&#35757;&#32451;&#30340;&#35821;&#38899;&#23545;&#35821;&#38899;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21475;&#35821;&#23545;&#35805;&#20013;&#65292;&#21363;&#20351;&#20004;&#20010;&#24403;&#21069;&#23545;&#35805;&#26159;&#30456;&#21516;&#30340;&#21477;&#23376;&#65292;&#20294;&#24403;&#20197;&#19981;&#21516;&#39118;&#26684;&#21457;&#38899;&#26102;&#65292;&#23427;&#20204;&#30340;&#22238;&#24212;&#20173;&#21487;&#33021;&#19981;&#21516;&#12290;&#35821;&#38899;&#39118;&#26684;&#21253;&#21547;&#35821;&#35328;&#38468;&#21152;&#20449;&#24687;&#21644;&#38901;&#24459;&#20449;&#24687;&#65292;&#26631;&#24535;&#30528;&#25991;&#26412;&#21644;&#35821;&#38899;&#24418;&#24335;&#20043;&#38388;&#26368;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;&#20351;&#29992;&#20165;&#25991;&#26412;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24314;&#27169;&#21475;&#35821;&#23545;&#35805;&#26102;&#65292;&#32431;&#25991;&#26412;&#30340;LLMs&#26080;&#27861;&#26681;&#25454;&#24403;&#21069;&#23545;&#35805;&#30340;&#35821;&#38899;&#39118;&#26684;&#32473;&#20986;&#19981;&#21516;&#30340;&#22238;&#24212;&#12290;&#26412;&#25991;&#26088;&#22312;&#20351;LLMs&#33021;&#22815;&#20542;&#21548;&#35828;&#35805;&#39118;&#26684;&#24182;&#20316;&#20986;&#24688;&#24403;&#22238;&#24212;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25945;&#20250;LLM&#8220;&#21363;&#20351;&#21477;&#23376;&#30456;&#21516;&#65292;&#22914;&#26524;&#20197;&#19981;&#21516;&#39118;&#26684;&#35828;&#20986;&#65292;&#30456;&#24212;&#30340;&#22238;&#24212;&#21487;&#33021;&#20250;&#19981;&#21516;&#8221;&#12290;&#30001;&#20110;&#30446;&#21069;&#27809;&#26377;&#36866;&#21512;&#23454;&#29616;&#27492;&#30446;&#26631;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#32452;&#35821;&#38899;&#23545;&#35821;&#38899;&#30340;&#25968;&#25454;&#38598;StyleTalk&#65292;&#20855;&#26377;&#20197;&#19979;&#25152;&#38656;&#29305;&#24449;&#65306;&#24403;&#20004;&#20010;&#24403;&#21069;&#35821;&#38899;&#20855;&#26377;&#30456;&#21516;&#20869;&#23481;&#20294;&#20197;&#19981;&#21516;&#39118;&#26684;&#35828;&#20986;&#26102;&#65292;&#23427;&#20204;&#30340;&#22238;&#24212;&#23558;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12786v1 Announce Type: new  Abstract: In spoken dialogue, even if two current turns are the same sentence, their responses might still differ when they are spoken in different styles. The spoken styles, containing paralinguistic and prosodic information, mark the most significant difference between text and speech modality. When using text-only LLMs to model spoken dialogue, text-only LLMs cannot give different responses based on the speaking style of the current turn. In this paper, we focus on enabling LLMs to listen to the speaking styles and respond properly. Our goal is to teach the LLM that "even if the sentences are identical if they are spoken in different styles, their corresponding responses might be different". Since there is no suitable dataset for achieving this goal, we collect a speech-to-speech dataset, StyleTalk, with the following desired characteristics: when two current speeches have the same content but are spoken in different styles, their responses wil
&lt;/p&gt;</description></item><item><title>BIDER&#36890;&#36807;&#30693;&#35782;&#32508;&#21512;&#21644;&#20559;&#22909;&#23545;&#40784;&#65292;&#23558;&#26816;&#32034;&#25991;&#26723;&#36716;&#21270;&#20026;&#20851;&#38190;&#25903;&#25345;&#35777;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;LLMs&#30340;&#31572;&#26696;&#36136;&#37327;&#24182;&#20943;&#23569;&#20102;&#36755;&#20837;&#20869;&#23481;&#38271;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.12174</link><description>&lt;p&gt;
BIDER: &#36890;&#36807;&#20851;&#38190;&#25903;&#25345;&#35777;&#25454;&#24357;&#21512;&#26377;&#25928;&#26816;&#32034;&#22686;&#24378;&#30340;LLMs&#20013;&#30340;&#30693;&#35782;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
BIDER: Bridging Knowledge Inconsistency for Efficient Retrieval-Augmented LLMs via Key Supporting Evidence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12174
&lt;/p&gt;
&lt;p&gt;
BIDER&#36890;&#36807;&#30693;&#35782;&#32508;&#21512;&#21644;&#20559;&#22909;&#23545;&#40784;&#65292;&#23558;&#26816;&#32034;&#25991;&#26723;&#36716;&#21270;&#20026;&#20851;&#38190;&#25903;&#25345;&#35777;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;LLMs&#30340;&#31572;&#26696;&#36136;&#37327;&#24182;&#20943;&#23569;&#20102;&#36755;&#20837;&#20869;&#23481;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24320;&#25918;&#39046;&#22495;QA&#31561;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#26356;&#26032;&#21644;&#20107;&#23454;&#19981;&#36275;&#31561;&#22266;&#26377;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#26816;&#32034;&#30693;&#35782;&#19982;LLMs&#25152;&#38656;&#30693;&#35782;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#23548;&#33268;&#20102;LLMs&#31572;&#26696;&#36136;&#37327;&#19979;&#38477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BIDER&#65292;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#32508;&#21512;&#12289;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#20559;&#22909;&#23545;&#40784;&#65292;&#23558;&#26816;&#32034;&#25991;&#26723;&#32454;&#21270;&#20026;&#20851;&#38190;&#25903;&#25345;&#35777;&#25454;&#65288;KSE&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#21046;&#23450;KSE&#26469;&#35757;&#32451;BIDER&#65292;&#21516;&#26102;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23558;&#20854;&#36755;&#20986;&#26368;&#22823;&#21270;&#65292;&#20197;&#20351;&#20854;&#19982;LLMs&#30340;&#20449;&#24687;&#33719;&#21462;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#12290;&#22312;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;BIDER&#25552;&#39640;&#20102;LLMs&#31572;&#26696;&#36136;&#37327;7&#65285;&#65292;&#21516;&#26102;&#23558;&#26816;&#32034;&#25991;&#26723;&#30340;&#36755;&#20837;&#20869;&#23481;&#38271;&#24230;&#20943;&#23569;&#20102;80&#65285;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;KSE&#27169;&#25311;&#26377;&#25928;&#22320;&#20026;LLMs&#25552;&#20379;&#20102;&#24517;&#35201;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12174v1 Announce Type: new  Abstract: Retrieval-augmented large language models (LLMs) have demonstrated efficacy in knowledge-intensive tasks such as open-domain QA, addressing inherent challenges in knowledge update and factual inadequacy. However, inconsistencies between retrieval knowledge and the necessary knowledge for LLMs, leading to a decline in LLM's answer quality. This paper introduces BIDER, an approach that refines retrieval documents into Key Supporting Evidence (KSE) through knowledge synthesis, supervised fine-tuning (SFT), and preference alignment. We train BIDER by learning from crafting KSE, while maximizing its output to align with LLM's information acquisition preferences through reinforcement learning. Evaluations across five datasets show BIDER boosts LLMs' answer quality by 7% while reducing input content length in retrieval documents by 80%, outperforming existing methods. The proposed KSE simulation effectively equips LLMs with essential informatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21327;&#20316;&#26041;&#27861;SlimPLM&#65292;&#36890;&#36807;&#31934;&#31616;&#20195;&#29702;&#27169;&#22411;&#26816;&#27979;LLMs&#20013;&#32570;&#22833;&#30340;&#30693;&#35782;&#65292;&#20197;&#22686;&#24378;LLMs&#30340;&#30693;&#35782;&#33719;&#21462;&#36807;&#31243;</title><link>https://arxiv.org/abs/2402.12052</link><description>&lt;p&gt;
&#23567;&#27169;&#22411;&#65292;&#22823;&#35265;&#35299;&#65306;&#21033;&#29992;&#31934;&#31616;&#20195;&#29702;&#27169;&#22411;&#30830;&#23450;LLMs&#20309;&#26102;&#20197;&#21450;&#20026;&#20309;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21327;&#20316;&#26041;&#27861;SlimPLM&#65292;&#36890;&#36807;&#31934;&#31616;&#20195;&#29702;&#27169;&#22411;&#26816;&#27979;LLMs&#20013;&#32570;&#22833;&#30340;&#30693;&#35782;&#65292;&#20197;&#22686;&#24378;LLMs&#30340;&#30693;&#35782;&#33719;&#21462;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12052v1 &#20844;&#21578;&#31867;&#22411;:&#26032;&#25688;&#35201;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#25628;&#32034;&#24341;&#25806;&#30340;&#25972;&#21512;&#20195;&#34920;&#20102;&#30693;&#35782;&#33719;&#21462;&#26041;&#27861;&#30340;&#37325;&#35201;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;LLM&#24050;&#32463;&#20855;&#22791;&#30340;&#30693;&#35782;&#21644;&#38656;&#35201;&#25628;&#32034;&#24341;&#25806;&#24110;&#21161;&#30340;&#30693;&#35782;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;LLM&#26412;&#36523;&#39044;&#22788;&#29702;&#31572;&#26696;&#25110;&#25512;&#29702;&#30340;&#32467;&#26524;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#36825;&#24102;&#26469;&#20102;&#36807;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#20316;&#26041;&#27861;&#65292;&#21363;SlimPLM&#65292;&#36890;&#36807;&#31934;&#31616;&#20195;&#29702;&#27169;&#22411;&#26816;&#27979;LLMs&#20013;&#32570;&#22833;&#30340;&#30693;&#35782;&#65292;&#20197;&#22686;&#24378;LLMs&#30340;&#30693;&#35782;&#33719;&#21462;&#36807;&#31243;&#12290;&#25105;&#20204;&#37319;&#29992;&#21442;&#25968;&#36828;&#36828;&#26356;&#23569;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#31572;&#26696;&#35270;&#20026;&#21551;&#21457;&#24335;&#31572;&#26696;&#12290;&#28982;&#21518;&#21033;&#29992;&#21551;&#21457;&#24335;&#31572;&#26696;&#26469;&#39044;&#27979;&#22238;&#31572;&#29992;&#25143;&#38382;&#39064;&#25152;&#38656;&#30340;&#30693;&#35782;&#65292;&#20197;&#21450;LLM&#20013;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#21482;&#20026;&#26410;&#30693;&#30693;&#35782;&#36827;&#34892;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12052v1 Announce Type: new  Abstract: The integration of large language models (LLMs) and search engines represents a significant evolution in knowledge acquisition methodologies. However, determining the knowledge that an LLM already possesses and the knowledge that requires the help of a search engine remains an unresolved issue. Most existing methods solve this problem through the results of preliminary answers or reasoning done by the LLM itself, but this incurs excessively high computational costs. This paper introduces a novel collaborative approach, namely SlimPLM, that detects missing knowledge in LLMs with a slim proxy model, to enhance the LLM's knowledge acquisition process. We employ a proxy model which has far fewer parameters, and take its answers as heuristic answers. Heuristic answers are then utilized to predict the knowledge required to answer the user question, as well as the known and unknown knowledge within the LLM. We only conduct retrieval for the mis
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;FlexLoRA&#65292;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#32858;&#21512;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20805;&#20998;&#21033;&#29992;&#24322;&#36136;&#23458;&#25143;&#36164;&#28304;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#26412;&#22320;LoRA&#25490;&#21517;&#21644;&#37319;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#36827;&#34892;&#26435;&#37325;&#37325;&#26032;&#20998;&#37197;&#65292;&#25552;&#21319;&#20840;&#23616;&#27169;&#22411;&#30340;&#24191;&#27867;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.11505</link><description>&lt;p&gt;
&#22312;&#24322;&#26500;&#35821;&#35328;&#20219;&#21153;&#21644;&#23458;&#25143;&#36164;&#28304;&#19979;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32852;&#37030;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Federated Fine-tuning of Large Language Models under Heterogeneous Language Tasks and Client Resources
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11505
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;FlexLoRA&#65292;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#32858;&#21512;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20805;&#20998;&#21033;&#29992;&#24322;&#36136;&#23458;&#25143;&#36164;&#28304;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#26412;&#22320;LoRA&#25490;&#21517;&#21644;&#37319;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#36827;&#34892;&#26435;&#37325;&#37325;&#26032;&#20998;&#37197;&#65292;&#25552;&#21319;&#20840;&#23616;&#27169;&#22411;&#30340;&#24191;&#27867;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#34987;&#24212;&#29992;&#20110;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23458;&#25143;&#30340;&#36164;&#28304;&#21644;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#65292;&#36825;&#24341;&#21457;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;FlexLoRA&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;LLM&#24494;&#35843;&#32858;&#21512;&#26041;&#26696;&#65292;&#23427;&#21487;&#20197;&#32531;&#35299;&#20256;&#32479;FL&#20013;&#30340;&#8220;&#26742;&#25928;&#24212;&#8221;&#65292;&#35813;&#25928;&#24212;&#38480;&#21046;&#20102;&#25317;&#26377;&#20016;&#23500;&#36164;&#28304;&#30340;&#23458;&#25143;&#23454;&#29616;&#28508;&#21147;&#65292;&#23558;&#20182;&#20204;&#19982;&#26368;&#32570;&#20047;&#36164;&#28304;&#30340;&#21442;&#19982;&#32773;&#30340;&#33021;&#21147;&#25414;&#32465;&#22312;&#19968;&#36215;&#12290;FlexLoRA&#20801;&#35768;&#21160;&#24577;&#35843;&#25972;&#26412;&#22320;LoRA&#25490;&#21517;&#65292;&#20419;&#36827;&#20840;&#23616;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#24182;&#36171;&#20104;&#26356;&#24191;&#27867;&#12289;&#19981;&#22826;&#20219;&#21153;&#29305;&#23450;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#20174;&#20010;&#20307;&#23458;&#25143;&#36129;&#29486;&#20013;&#21512;&#25104;&#23436;&#25972;&#22823;&#23567;&#30340;LoRA&#26435;&#37325;&#65292;&#24182;&#21033;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#36827;&#34892;&#26435;&#37325;&#37325;&#26032;&#20998;&#37197;&#65292;FlexLoRA&#20805;&#20998;&#21033;&#29992;&#20102;&#23458;&#25143;&#38388;&#30340;&#36164;&#28304;&#24046;&#24322;&#12290;&#26412;&#30740;&#31350;&#28041;&#21450;&#36229;&#36807;1600&#20010;&#25191;&#34892;&#22810;&#26679;NLP&#20219;&#21153;&#30340;&#23458;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11505v1 Announce Type: cross  Abstract: Federated Learning (FL) has recently been applied to the parameter-efficient fine-tuning of Large Language Models (LLMs). While promising, it raises significant challenges due to the heterogeneous resources and data distributions of clients.This study introduces FlexLoRA, a simple yet effective aggregation scheme for LLM fine-tuning, which mitigates the "buckets effect" in traditional FL that restricts the potential of clients with ample resources by tying them to the capabilities of the least-resourced participants. FlexLoRA allows for dynamic adjustment of local LoRA ranks, fostering the development of a global model imbued with broader, less task-specific knowledge. By synthesizing a full-size LoRA weight from individual client contributions and employing Singular Value Decomposition (SVD) for weight redistribution, FlexLoRA fully leverages heterogeneous client resources. Involving over 1,600 clients performing diverse NLP tasks, ou
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20989;&#25968;&#35843;&#29992;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#38646;-shot&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#21462;&#24471;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#36866;&#24212;&#19981;&#21516;&#39046;&#22495;&#32780;&#26080;&#38656;&#22823;&#37327;&#25968;&#25454;&#25910;&#38598;&#25110;&#27169;&#22411;&#35843;&#25972;&#12290;</title><link>https://arxiv.org/abs/2402.10466</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#22120;&#36890;&#36807;&#20989;&#25968;&#35843;&#29992;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Zero-shot Dialogue State Tracker through Function Calling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20989;&#25968;&#35843;&#29992;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#38646;-shot&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#21462;&#24471;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#36866;&#24212;&#19981;&#21516;&#39046;&#22495;&#32780;&#26080;&#38656;&#22823;&#37327;&#25968;&#25454;&#25910;&#38598;&#25110;&#27169;&#22411;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20250;&#35805;&#31995;&#32479;&#20013;&#26085;&#30410;&#26222;&#36941;&#65292;&#36825;&#26159;&#22240;&#20026;&#23427;&#20204;&#22312;&#19968;&#33324;&#24773;&#22659;&#20013;&#20855;&#26377;&#20808;&#36827;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#19981;&#20165;&#36827;&#34892;&#21709;&#24212;&#29983;&#25104;&#36824;&#38656;&#35201;&#22312;&#29305;&#23450;&#20219;&#21153;&#21644;&#39046;&#22495;&#20869;&#36827;&#34892;&#26377;&#25928;&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#65288;DST&#65289;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#65288;TOD&#65289;&#20013;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#20173;&#19981;&#23613;&#20154;&#24847;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20989;&#25968;&#35843;&#29992;&#35299;&#20915;LLMs&#20013;&#30340;DST&#30340;&#26032;&#26041;&#27861;FnCTOD&#12290;&#36825;&#31181;&#26041;&#27861;&#25913;&#36827;&#20102;&#38646;-shot DST&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#21508;&#31181;&#39046;&#22495;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#22823;&#37327;&#25968;&#25454;&#25910;&#38598;&#25110;&#27169;&#22411;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#24320;&#28304;&#25110;&#19987;&#26377;LLMs&#26102;&#37117;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65306;&#36890;&#36807;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#20351;&#24471;&#21508;&#31181;7B&#25110;13B&#21442;&#25968;&#27169;&#22411;&#36229;&#36234;&#20102;&#20043;&#21069;&#30001;ChatGPT&#23454;&#29616;&#30340;&#26368;&#26032;&#25216;&#26415;&#25104;&#26524;&#65288;SOTA&#65289;&#30340;&#27700;&#24179;&#65292;&#24182;&#25552;&#39640;&#20102;ChatGPT&#30340;&#24615;&#33021;&#65292;&#20987;&#36133;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10466v1 Announce Type: cross  Abstract: Large language models (LLMs) are increasingly prevalent in conversational systems due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state tracking (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel approach FnCTOD for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing adaptation to diverse domains without extensive data collection or model tuning. Our experimental results demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also proprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous state-of-the-art (SOTA) achieved by ChatGPT, and improves ChatGPT's performance beating the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#35270;&#35273;&#26465;&#20214;&#21270;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#35774;&#35745;&#30340;&#20851;&#38190;&#31354;&#38388;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#26631;&#20934;&#21270;&#35780;&#20272;&#65292;&#21516;&#26102;&#36824;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#34920;&#31034;&#21644;&#26435;&#34913;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07865</link><description>&lt;p&gt;
&#36879;&#35270;VLMs&#65306;&#25506;&#32034;&#35270;&#35273;&#26465;&#20214;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#35270;&#35273;&#26465;&#20214;&#21270;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#35774;&#35745;&#30340;&#20851;&#38190;&#31354;&#38388;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#26631;&#20934;&#21270;&#35780;&#20272;&#65292;&#21516;&#26102;&#36824;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#34920;&#31034;&#21644;&#26435;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#26465;&#20214;&#21270;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#35270;&#35273;&#23545;&#35805;&#12289;&#22330;&#26223;&#29702;&#35299;&#21644;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#31561;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#65292;&#36825;&#31181;&#24212;&#29992;&#20419;&#20351;&#20102;&#20687;LLaVa&#12289;InstructBLIP&#21644;PaLI-3&#31561;&#35768;&#22810;&#26032;&#27169;&#22411;&#30340;&#20986;&#29616;&#12290;&#23613;&#31649;&#26377;&#36825;&#20040;&#22810;&#26032;&#30340;&#21457;&#24067;&#65292;&#20294;&#20851;&#20110;&#22270;&#20687;&#39044;&#22788;&#29702;&#12289;&#26550;&#26500;&#21644;&#20248;&#21270;&#30340;&#20851;&#38190;&#35774;&#35745;&#20915;&#31574;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#24456;&#38590;&#29702;&#35299;&#27169;&#22411;&#24615;&#33021;&#30340;&#22240;&#32032;&#65292;&#36825;&#19968;&#25361;&#25112;&#21448;&#22240;&#32570;&#20047;&#23458;&#35266;&#12289;&#19968;&#33268;&#30340;&#35780;&#20272;&#32780;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#20808;&#32534;&#21046;&#20102;&#19968;&#22871;&#26631;&#20934;&#21270;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#35270;&#35273;&#38382;&#31572;&#12289;&#20174;&#35821;&#35328;&#20013;&#23450;&#20301;&#29289;&#20307;&#20197;&#21450;&#25506;&#32034;&#24187;&#35273;&#31561;&#23646;&#24615;&#30340;&#30446;&#26631;&#25361;&#25112;&#38598;&#65292;&#36825;&#20123;&#35780;&#20272;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;VLM&#33021;&#21147;&#30340;&#31934;&#32454;&#12289;&#20934;&#30830;&#30340;&#35265;&#35299;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#20851;&#38190;&#30340;&#35774;&#35745;&#36724;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#30740;&#31350;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#34920;&#31034;&#21644;&#20351;&#29992;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visually-conditioned language models (VLMs) have seen growing adoption in applications such as visual dialogue, scene understanding, and robotic task planning; adoption that has fueled a wealth of new models such as LLaVa, InstructBLIP, and PaLI-3. Despite the volume of new releases, key design decisions around image preprocessing, architecture, and optimization are under-explored, making it challenging to understand what factors account for model performance $-$ a challenge further complicated by the lack of objective, consistent evaluations. To address these gaps, we first compile a suite of standardized evaluations spanning visual question answering, object localization from language, and targeted challenge sets that probe properties such as hallucination; evaluations that provide calibrated, fine-grained insight into a VLM's capabilities. Second, we rigorously investigate VLMs along key design axes, including pretrained visual representations and quantifying the tradeoffs of using 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Midi-Tuning&#30340;&#22810;&#36718;&#20132;&#20114;&#23545;&#35805;&#35843;&#25972;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#21035;&#23545;&#20195;&#29702;&#20154;&#21644;&#29992;&#25143;&#24314;&#27169;&#65292;&#24182;&#21033;&#29992;&#36718;&#27425;&#32423;&#20869;&#23384;&#32531;&#23384;&#26426;&#21046;&#36827;&#34892;&#35843;&#25972;&#65292;&#23454;&#29616;&#20102;&#23545;&#35805;&#20195;&#29702;&#30340;&#19968;&#33268;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06967</link><description>&lt;p&gt;
&#19968;&#27425;&#25351;&#23548;&#65292;&#22810;&#36718;&#31283;&#23450;&#23545;&#35805;&#65306;&#23545;&#35805;&#30340;&#39640;&#25928;&#35843;&#25972;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Instruct Once, Chat Consistently in Multiple Rounds: An Efficient Tuning Framework for Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Midi-Tuning&#30340;&#22810;&#36718;&#20132;&#20114;&#23545;&#35805;&#35843;&#25972;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#21035;&#23545;&#20195;&#29702;&#20154;&#21644;&#29992;&#25143;&#24314;&#27169;&#65292;&#24182;&#21033;&#29992;&#36718;&#27425;&#32423;&#20869;&#23384;&#32531;&#23384;&#26426;&#21046;&#36827;&#34892;&#35843;&#25972;&#65292;&#23454;&#29616;&#20102;&#23545;&#35805;&#20195;&#29702;&#30340;&#19968;&#33268;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35843;&#25972;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#23454;&#29616;&#23545;&#35805;&#29983;&#25104;&#24050;&#25104;&#20026;&#26500;&#24314;&#33021;&#21147;&#24378;&#22823;&#30340;&#23545;&#35805;&#20195;&#29702;&#30340;&#20027;&#27969;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#35843;&#25972;&#26041;&#24335;&#29421;&#38552;&#22320;&#23558;&#23545;&#35805;&#29983;&#25104;&#35270;&#20026;&#31867;&#20284;&#20854;&#20182;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#30340;&#36807;&#31243;&#65292;&#24573;&#35270;&#20102;&#23545;&#35805;&#32773;&#20043;&#38388;&#30340;&#35282;&#33394;&#24046;&#24322;&#21644;&#23545;&#35805;&#24212;&#20855;&#22791;&#30340;&#22810;&#36718;&#20132;&#20114;&#36807;&#31243;&#12290;&#36825;&#31181;&#26041;&#24335;&#23548;&#33268;&#20102;&#25152;&#26500;&#24314;&#20195;&#29702;&#20154;&#30340;&#23545;&#35805;&#19968;&#33268;&#24615;&#19981;&#23613;&#22914;&#20154;&#24847;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#23545;&#35805;&#30340;&#20132;&#20114;&#24615;&#21644;&#27807;&#36890;&#24615;&#65292;&#24182;&#35748;&#20026;&#20998;&#21035;&#23545;&#20195;&#29702;&#20154;&#21644;&#29992;&#25143;&#30340;&#35762;&#35805;&#32773;&#35282;&#33394;&#36827;&#34892;&#24314;&#27169;&#26356;&#20026;&#21487;&#34892;&#65292;&#20351;&#24471;&#20195;&#29702;&#20154;&#33021;&#22815;&#20445;&#25345;&#19968;&#33268;&#30340;&#35282;&#33394;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22810;&#36718;&#20132;&#20114;&#23545;&#35805;&#35843;&#25972;&#65288;Midi-Tuning&#65289;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20004;&#20010;&#36866;&#37197;&#22120;&#20998;&#21035;&#23545;&#20195;&#29702;&#20154;&#21644;&#29992;&#25143;&#36827;&#34892;&#24314;&#27169;&#65292;&#23427;&#20204;&#25353;&#36718;&#27425;&#20132;&#26367;&#20351;&#29992;&#35805;&#35821;&#65292;&#24182;&#36890;&#36807;&#36718;&#27425;&#32423;&#20869;&#23384;&#32531;&#23384;&#26426;&#21046;&#36827;&#34892;&#35843;&#25972;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#23545;&#35805;&#20195;&#29702;&#30340;&#19968;&#33268;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tuning pretrained language models for dialogue generation has been a prevalent paradigm for building capable dialogue agents. Yet, traditional tuning narrowly views dialogue generation as resembling other language generation tasks, ignoring the role disparities between two speakers and the multi-round interactive process that dialogues ought to be. Such a manner leads to unsatisfactory chat consistency of the built agent. In this work, we emphasize the interactive, communicative nature of dialogue and argue that it is more feasible to model the speaker roles of agent and user separately, enabling the agent to adhere to its role consistently. We propose an efficient Multi-round Interactive Dialogue Tuning (Midi-Tuning) framework. It models the agent and user individually with two adapters built upon large language models, where they utilize utterances round by round in alternating order and are tuned via a round-level memory caching mechanism. Extensive experiments demonstrate that, our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26356;&#24369;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#35780;&#20272;&#26356;&#24378;&#30340;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#36827;&#34892;&#36777;&#35770;&#65292;&#38750;&#19987;&#23478;&#27169;&#22411;&#21644;&#20154;&#31867;&#22238;&#31572;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#37117;&#26377;&#25152;&#25552;&#39640;&#12290;</title><link>https://arxiv.org/abs/2402.06782</link><description>&lt;p&gt;
&#19982;&#26356;&#26377;&#35828;&#26381;&#21147;&#30340;LLMs&#36777;&#35770;&#20250;&#23548;&#33268;&#26356;&#30495;&#23454;&#30340;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Debating with More Persuasive LLMs Leads to More Truthful Answers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26356;&#24369;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#35780;&#20272;&#26356;&#24378;&#30340;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#36827;&#34892;&#36777;&#35770;&#65292;&#38750;&#19987;&#23478;&#27169;&#22411;&#21644;&#20154;&#31867;&#22238;&#31572;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#37117;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#25152;&#38656;&#34892;&#20026;&#19968;&#33268;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24120;&#35265;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20154;&#24037;&#26631;&#27880;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#23427;&#20204;&#23558;&#36229;&#36807;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#65292;&#20154;&#31867;&#35780;&#20272;&#30340;&#35282;&#33394;&#23558;&#28436;&#21464;&#20026;&#38750;&#19987;&#23478;&#30417;&#30563;&#19987;&#23478;&#12290;&#22312;&#27492;&#20043;&#21069;&#65292;&#25105;&#20204;&#38382;&#65306;&#26356;&#24369;&#30340;&#27169;&#22411;&#33021;&#35780;&#20272;&#26356;&#24378;&#30340;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#21527;&#65311;&#25105;&#20204;&#22312;&#31867;&#20284;&#30340;&#29615;&#22659;&#20013;&#35843;&#26597;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#26356;&#24378;&#30340;&#27169;&#22411;&#65288;&#19987;&#23478;&#65289;&#25317;&#26377;&#22238;&#31572;&#38382;&#39064;&#25152;&#38656;&#30340;&#20449;&#24687;&#65292;&#32780;&#26356;&#24369;&#30340;&#27169;&#22411;&#65288;&#38750;&#19987;&#23478;&#65289;&#32570;&#20047;&#36825;&#20123;&#20449;&#24687;&#12290;&#25105;&#20204;&#35780;&#20272;&#30340;&#26041;&#27861;&#26159;\textit{&#36777;&#35770;}&#65292;&#20854;&#20013;&#20004;&#20010;LLM&#19987;&#23478;&#20998;&#21035;&#25903;&#25345;&#19981;&#21516;&#30340;&#31572;&#26696;&#65292;&#19968;&#20010;&#38750;&#19987;&#23478;&#36873;&#25321;&#31572;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#36777;&#35770; consistently&#24110;&#21161;&#38750;&#19987;&#23478;&#27169;&#22411;&#21644;&#20154;&#31867;&#22238;&#31572;&#38382;&#39064;&#65292;&#20998;&#21035;&#36798;&#21040;76%&#21644;88%&#30340;&#20934;&#30830;&#24615;&#65288;&#26420;&#32032;&#22522;&#20934;&#20998;&#21035;&#20026;48%&#21644;60%&#65289;&#12290;&#27492;&#22806;&#65292;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#20248;&#21270;&#19987;&#23478;&#36777;&#35770;&#32773;&#30340;&#35828;&#26381;&#21147;&#20250;&#25552;&#39640;&#38750;&#19987;&#23478;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method we evaluate is \textit{debate}, where two LLM experts each argue for a different answer, and a non-expert selects the answer. We find that debate consistently helps both non-expert models and humans answer questions, achieving 76\% and 88\% accuracy respectively (naive baselines obtain 48\% and 60\%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert abilit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#36890;&#29992;&#30340;LLMs&#24212;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#23450;&#20041;&#30340;&#36755;&#20837;&#26631;&#31614;&#26469;&#23545;LLMs&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#36890;&#36807;&#26126;&#30830;&#23558;&#20219;&#21153;&#39046;&#22495;&#19982;&#20219;&#21153;&#21151;&#33021;&#20998;&#31163;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05140</link><description>&lt;p&gt;
Tag-LLM: &#23558;&#36890;&#29992;&#30340;LLM&#24212;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#20877;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#36890;&#29992;&#30340;LLMs&#24212;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#23450;&#20041;&#30340;&#36755;&#20837;&#26631;&#31614;&#26469;&#23545;LLMs&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#36890;&#36807;&#26126;&#30830;&#23558;&#20219;&#21153;&#39046;&#22495;&#19982;&#20219;&#21153;&#21151;&#33021;&#20998;&#31163;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#19987;&#38376;&#39046;&#22495;&#20013;&#65292;&#22914;&#29289;&#29702;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#31185;&#23398;&#36825;&#26679;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#26410;&#20805;&#20998;&#28085;&#30422;&#30340;&#39046;&#22495;&#65292;&#23427;&#20204;&#30340;&#33021;&#21147;&#19979;&#38477;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#36890;&#29992;LLMs&#37325;&#26032;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#26377;&#25928;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#33258;&#23450;&#20041;&#30340;&#36755;&#20837;&#26631;&#31614;&#65292;&#36825;&#20123;&#26631;&#31614;&#34987;&#21442;&#25968;&#21270;&#20026;&#36830;&#32493;&#21521;&#37327;&#24182;&#38468;&#21152;&#21040;LLMs&#30340;&#23884;&#20837;&#23618;&#65292;&#20197;&#23545;LLMs&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#26631;&#31614;&#65306;&#39046;&#22495;&#26631;&#31614;&#29992;&#20110;&#38480;&#23450;&#19987;&#19994;&#34920;&#31034;&#65288;&#20363;&#22914;&#21270;&#23398;&#24335;&#65289;&#24182;&#25552;&#20379;&#39046;&#22495;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#65307;&#21151;&#33021;&#26631;&#31614;&#29992;&#20110;&#34920;&#31034;&#29305;&#23450;&#30340;&#21151;&#33021;&#65288;&#20363;&#22914;&#39044;&#27979;&#20998;&#23376;&#24615;&#36136;&#65289;&#24182;&#21387;&#32553;&#21151;&#33021;&#35299;&#20915;&#25351;&#20196;&#12290;&#25105;&#20204;&#20351;&#29992;&#36741;&#21161;&#25968;&#25454;&#21644;&#39046;&#22495;&#30693;&#35782;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#30340;&#23398;&#20064;&#36825;&#20123;&#26631;&#31614;&#30340;&#21327;&#35758;&#12290;&#36890;&#36807;&#26126;&#30830;&#23558;&#20219;&#21153;&#39046;&#22495;&#19982;&#20219;&#21153;&#21151;&#33021;&#20998;&#31163;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable proficiency in understanding and generating natural language. However, their capabilities wane in highly specialized domains underrepresented in the pretraining corpus, such as physical and biomedical sciences. This work explores how to repurpose general LLMs into effective task solvers for specialized domains. We introduce a novel, model-agnostic framework for learning custom input tags, which are parameterized as continuous vectors appended to the LLM's embedding layer, to condition the LLM. We design two types of input tags: domain tags are used to delimit specialized representations (e.g., chemical formulas) and provide domain-relevant context; function tags are used to represent specific functions (e.g., predicting molecular properties) and compress function-solving instructions. We develop a three-stage protocol to learn these tags using auxiliary data and domain knowledge. By explicitly disentangling task domains from tas
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35268;&#21010;&#65288;UoT&#65289;&#31639;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#23547;&#27714;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#27169;&#25311;&#26410;&#26469;&#22330;&#26223;&#12289;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#22870;&#21169;&#26426;&#21046;&#21644;&#22870;&#21169;&#20256;&#25773;&#26041;&#26696;&#65292;&#20248;&#21270;&#38382;&#39064;&#25552;&#38382;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.03271</link><description>&lt;p&gt;
&#24819;&#27861;&#30340;&#19981;&#30830;&#23450;&#24615;&#65306;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35268;&#21010;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24687;&#25628;&#32034;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03271
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35268;&#21010;&#65288;UoT&#65289;&#31639;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#23547;&#27714;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#27169;&#25311;&#26410;&#26469;&#22330;&#26223;&#12289;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#22870;&#21169;&#26426;&#21046;&#21644;&#22870;&#21169;&#20256;&#25773;&#26041;&#26696;&#65292;&#20248;&#21270;&#38382;&#39064;&#25552;&#38382;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#23547;&#27714;&#20449;&#24687;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#27604;&#22914;&#21307;&#23398;&#35786;&#26029;&#21644;&#25925;&#38556;&#25490;&#38500;&#65292;&#35299;&#20915;&#20219;&#21153;&#25152;&#38656;&#30340;&#20449;&#24687;&#19981;&#26159;&#21021;&#22987;&#32473;&#23450;&#30340;&#65292;&#32780;&#38656;&#35201;&#36890;&#36807;&#35810;&#38382;&#21518;&#32493;&#38382;&#39064;&#26469;&#20027;&#21160;&#23547;&#27714;&#65288;&#20363;&#22914;&#65292;&#21307;&#29983;&#21521;&#24739;&#32773;&#35810;&#38382;&#30151;&#29366;&#30340;&#26356;&#22810;&#32454;&#33410;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24605;&#24819;&#30340;&#19981;&#30830;&#23450;&#24615;&#65288;UoT&#65289;&#65292;&#19968;&#31181;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19982;&#20027;&#21160;&#25552;&#38382;&#20449;&#24687;&#30340;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;UoT&#32467;&#21512;&#20102;1&#65289;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20223;&#30495;&#26041;&#27861;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#21487;&#33021;&#30340;&#26410;&#26469;&#22330;&#26223;&#65292;&#24182;&#20272;&#35745;&#20854;&#21457;&#29983;&#30340;&#21487;&#33021;&#24615;&#65307;2&#65289;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#22870;&#21169;&#26426;&#21046;&#65292;&#28608;&#21169;&#27169;&#22411;&#23547;&#27714;&#20449;&#24687;&#65307;3&#65289;&#22870;&#21169;&#20256;&#25773;&#26041;&#26696;&#65292;&#20197;&#26368;&#22823;&#21270;&#39044;&#26399;&#22870;&#21169;&#30340;&#26041;&#24335;&#36873;&#25321;&#26368;&#20339;&#30340;&#38382;&#39064;&#25552;&#38382;&#26041;&#24335;&#12290;&#22312;&#21307;&#23398;&#35786;&#26029;&#12289;&#25925;&#38556;&#25490;&#38500;&#21644;'20&#30340;&#23454;&#39564;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the face of uncertainty, the ability to seek information is of fundamental importance. In many practical applications, such as medical diagnosis and troubleshooting, the information needed to solve the task is not initially given, and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms). In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment large language models with the ability to actively seek information by asking effective questions. UoT combines 1) an uncertainty-aware simulation approach which enables the model to simulate possible future scenarios and how likely they are to occur, 2) uncertainty-based rewards motivated by information gain which incentivizes the model to seek information, and 3) a reward propagation scheme to select the optimal question to ask in a way that maximizes the expected reward. In experiments on medical diagnosis, troubleshooting and the '20 
&lt;/p&gt;</description></item><item><title>LQER&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#21644;&#28608;&#27963;&#24341;&#36215;&#30340;&#23610;&#24230;&#30697;&#38453;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#36817;&#20046;&#26080;&#25439;&#37327;&#21270;&#65292;&#26080;&#38656;&#30693;&#35782;&#33976;&#39311;&#25110;&#26799;&#24230;&#20248;&#21270;&#65292;&#24182;&#22823;&#24133;&#20943;&#23569;&#30828;&#20214;&#36164;&#28304;&#30340;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.02446</link><description>&lt;p&gt;
LQER: &#20302;&#31209;&#37327;&#21270;&#35823;&#24046;&#37325;&#24314;&#29992;&#20110;LLMs
&lt;/p&gt;
&lt;p&gt;
LQER: Low-Rank Quantization Error Reconstruction for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02446
&lt;/p&gt;
&lt;p&gt;
LQER&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#21644;&#28608;&#27963;&#24341;&#36215;&#30340;&#23610;&#24230;&#30697;&#38453;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#36817;&#20046;&#26080;&#25439;&#37327;&#21270;&#65292;&#26080;&#38656;&#30693;&#35782;&#33976;&#39311;&#25110;&#26799;&#24230;&#20248;&#21270;&#65292;&#24182;&#22823;&#24133;&#20943;&#23569;&#30828;&#20214;&#36164;&#28304;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35757;&#32451;&#21518;&#37327;&#21270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20302;&#31209;&#37327;&#21270;&#35823;&#24046;&#20943;&#23569;&#65288;LQER&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#37327;&#21270;&#21644;&#20302;&#31209;&#36924;&#36817;&#26469;&#24674;&#22797;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;LQER&#21033;&#29992;&#28608;&#27963;&#24341;&#36215;&#30340;&#23610;&#24230;&#30697;&#38453;&#23558;&#37327;&#21270;&#35823;&#24046;&#30340;&#22855;&#24322;&#20540;&#20998;&#24067;&#25512;&#21521;&#26399;&#26395;&#30340;&#20998;&#24067;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;LLMs&#21644;&#19979;&#28216;&#20219;&#21153;&#19978;&#36817;&#20046;&#26080;&#25439;&#30340;W4A8&#37327;&#21270;&#65292;&#26080;&#38656;&#30693;&#35782;&#33976;&#39311;&#12289;&#32593;&#26684;&#25628;&#32034;&#25110;&#22522;&#20110;&#26799;&#24230;&#30340;&#36845;&#20195;&#20248;&#21270;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;LQER&#30340;&#35745;&#31639;&#27169;&#24335;&#28040;&#38500;&#20102;&#20174;&#19981;&#35268;&#21017;&#20869;&#23384;&#20301;&#32622;&#25910;&#38598;&#39640;&#31934;&#24230;&#26435;&#37325;&#25152;&#38656;&#30340;&#19987;&#29992;Scatter&#21644;Gather&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;W4A8 LLMs&#22312;&#20845;&#20010;&#28909;&#38376;&#19979;&#28216;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#36817;&#20046;&#26080;&#25439;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#30340;&#30828;&#20214;&#36164;&#28304;&#27604;&#39046;&#20808;&#30340;&#26368;&#26032;&#26041;&#27861;&#23569;1.36&#20493;&#12290;&#19968;&#26086;&#35770;&#25991;&#34987;&#25509;&#21463;&#65292;&#25105;&#20204;&#23558;&#24320;&#28304;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization of Large Language Models (LLMs) is challenging. In this work, we introduce Low-rank Quantization Error Reduction (LQER), which combines quantization and low-rank approximation to recover the model capability. LQER leverages an activation-induced scale matrix to drive the singular value distribution of quantization error towards a desirable distribution, which enables nearly-lossless W4A8 quantization on various LLMs and downstream tasks without the need for knowledge distillation, grid search, or gradient-base iterative optimization. Unlike existing methods, the computation pattern of LQER eliminates the need for specialized Scatter and Gather processes to collect high-precision weights from irregular memory locations. Our W4A8 LLMs achieve near-lossless performance on six popular downstream tasks, while using 1.36$\times$ fewer hardware resources than the leading state-of-the-art method. We will open-source our framework once the paper is accepted.
&lt;/p&gt;</description></item><item><title>APIServe&#26159;&#38024;&#23545;API&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#19968;&#20010;&#39640;&#25928;&#24037;&#20855;&#65292;&#23427;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#20102;&#30001; API &#35843;&#29992;&#24341;&#36215;&#30340; GPU &#36164;&#28304;&#28010;&#36153;&#65292;&#24182;&#25552;&#39640;&#20102;&#25972;&#20307;&#26381;&#21153;&#21534;&#21520;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.01869</link><description>&lt;p&gt;
APIServe: &#39640;&#25928;&#25903;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;API&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
APIServe: Efficient API Support for Large-Language Model Inferencing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01869
&lt;/p&gt;
&lt;p&gt;
APIServe&#26159;&#38024;&#23545;API&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#19968;&#20010;&#39640;&#25928;&#24037;&#20855;&#65292;&#23427;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#20102;&#30001; API &#35843;&#29992;&#24341;&#36215;&#30340; GPU &#36164;&#28304;&#28010;&#36153;&#65292;&#24182;&#25552;&#39640;&#20102;&#25972;&#20307;&#26381;&#21153;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#19982;&#22806;&#37096;&#24037;&#20855;&#21644;API&#38598;&#25104;&#65292;&#22914;ChatGPT&#25554;&#20214;&#65292;&#20197;&#25193;&#23637;&#20854;&#33021;&#21147;&#20197;&#22806;&#30340;&#35821;&#35328;&#20013;&#24515;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;LLM&#25512;&#29702;&#31995;&#32479;&#26159;&#20026;&#29420;&#31435;&#30340;LLM&#35774;&#35745;&#30340;&#12290;&#23427;&#20204;&#23558;API&#35843;&#29992;&#35270;&#20026;&#26032;&#35831;&#27714;&#65292;&#23548;&#33268;&#19981;&#24517;&#35201;&#30340;&#37325;&#26032;&#35745;&#31639;&#24050;&#32463;&#35745;&#31639;&#36807;&#30340;&#19978;&#19979;&#25991;&#65292;&#36825;&#21344;&#20102;&#24635;&#27169;&#22411;&#21069;&#21521;&#26102;&#38388;&#30340;37-40%&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;APIServe&#65292;&#36825;&#26159;&#38024;&#23545;API&#22686;&#24378;&#30340;LLM&#25512;&#29702;&#26694;&#26550;&#12290;APIServe&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#20102;&#30001;API&#35843;&#29992;&#24341;&#36215;&#30340;GPU&#36164;&#28304;&#28010;&#36153;&#65292;&#24182;&#23558;&#33410;&#30465;&#30340;&#20869;&#23384;&#29992;&#20110;&#26381;&#21153;&#26356;&#22810;&#30340;&#35831;&#27714;&#12290;&#19982;&#29616;&#26377;&#30340;LLM&#25512;&#29702;&#31995;&#32479;&#30456;&#27604;&#65292;APIServe&#23558;&#25972;&#20307;&#26381;&#21153;&#21534;&#21520;&#37327;&#25552;&#21319;&#20102;1.6&#20493;&#65292;&#27599;&#31186;&#23436;&#25104;&#30340;&#35831;&#27714;&#22686;&#21152;&#20102;2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are increasingly integrated with external tools and APIs like ChatGPT plugins to extend their capability beyond language-centric tasks. However, today's LLM inference systems are designed for standalone LLMs. They treat API calls as new requests, causing unnecessary recomputation of already computed contexts, which accounts for 37-40% of total model forwarding time. This paper presents APIServe, the first LLM inference framework targeting API-augmented LLMs. APISERVE minimizes the GPU resource waste caused by API calls and dedicates saved memory for serving more requests. APISERVE improves the overall serving throughput by 1.6x and completes 2x more requests per second compared to the state-of-the-art LLM inference systems.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"PiVe"&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#39564;&#35777;&#26469;&#25552;&#21319;LLMs&#30340;&#22522;&#20110;&#22270;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PiVe&#26041;&#27861;&#22312;&#19977;&#20010;&#22522;&#20110;&#22270;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#25913;&#21892;&#65292;&#24182;&#19988;&#39564;&#35777;&#27169;&#22359;&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#24037;&#20855;&#24110;&#21161;&#25552;&#39640;&#32467;&#26524;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2305.12392</link><description>&lt;p&gt;
PiVe&#65306;&#36890;&#36807;&#36845;&#20195;&#39564;&#35777;&#25552;&#21319;LLMs&#30340;&#22522;&#20110;&#22270;&#30340;&#29983;&#25104;&#33021;&#21147;&#30340;&#25552;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.12392
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"PiVe"&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#39564;&#35777;&#26469;&#25552;&#21319;LLMs&#30340;&#22522;&#20110;&#22270;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PiVe&#26041;&#27861;&#22312;&#19977;&#20010;&#22522;&#20110;&#22270;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#25913;&#21892;&#65292;&#24182;&#19988;&#39564;&#35777;&#27169;&#22359;&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#24037;&#20855;&#24110;&#21161;&#25552;&#39640;&#32467;&#26524;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35299;&#20915;&#21508;&#31181;&#19981;&#21516;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;LLMs&#30340;&#35757;&#32451;&#30446;&#26631;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;LLMs&#23545;&#28041;&#21450;&#32467;&#26500;&#21270;&#25968;&#25454;&#29983;&#25104;&#30340;&#20219;&#21153;&#24182;&#19981;&#38750;&#24120;&#36866;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"PiVe"&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#39564;&#35777;&#26469;&#25552;&#21319;LLMs&#30340;&#22522;&#20110;&#22270;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#35757;&#32451;&#19968;&#20010;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;LLMs&#30340;&#36755;&#20986;&#30340;&#39564;&#35777;&#27169;&#22359;(&#20363;&#22914;ChatGPT&#65292;GPT-4)&#65292;&#36890;&#36807;&#31934;&#32454;&#30340;&#32416;&#27491;&#25351;&#20196;&#26469;&#36845;&#20195;&#25913;&#36827;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#39564;&#35777;&#27169;&#22359;&#22914;&#20309;&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#24212;&#29992;&#36845;&#20195;&#26657;&#27491;&#65292;&#20197;&#33719;&#24471;&#26356;&#32463;&#27982;&#39640;&#25928;&#30340;&#25991;&#26412;&#21040;&#22270;&#24418;&#29983;&#25104;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#19977;&#20010;&#22522;&#20110;&#22270;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;PiVe&#30340;&#26041;&#27861;&#33021;&#22815;&#25345;&#32493;&#25913;&#21892;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;GenWiki-HIQ&#25968;&#25454;&#38598;&#65292;&#24182;&#24378;&#35843;&#39564;&#35777;&#27169;&#22359;&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#24037;&#20855;&#65292;&#24110;&#21161;&#25552;&#39640;&#33258;&#21160;&#29983;&#25104;&#30340;&#32467;&#26524;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown great abilities of solving various natural language tasks in different domains. Due to the training objective of LLMs and their pre-training data, LLMs are not very well equipped for tasks involving structured data generation. We propose a framework, Prompting with Iterative Verification (PiVe), to improve graph-based generative capability of LLMs. We show how a small language model could be trained to act as a verifier module for the output of an LLM(i.e., ChatGPT, GPT-4), and to iteratively improve its performance via fine-grained corrective instructions. We also show how the verifier module could apply iterative corrections offline for a more cost-effective solution to the text-to-graph generation task. Experiments on three graph-based datasets show consistent improvement gained via PiVe. Additionally, we create GenWiki-HIQ and highlight that the verifier module can be used as a data augmentation tool to help improve the quality of automatical
&lt;/p&gt;</description></item><item><title>UALA&#26159;&#19968;&#20010;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#36827;&#34892;&#20195;&#29702;&#21644;&#22806;&#37096;&#19990;&#30028;&#20132;&#20114;&#30340;&#26694;&#26550;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#23427;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#35821;&#35328;&#27169;&#22411;&#23610;&#23544;&#19979;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#19988;&#22312;&#23545;&#22806;&#37096;&#19990;&#30028;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#26356;&#20302;&#12290;</title><link>http://arxiv.org/abs/2401.14016</link><description>&lt;p&gt;
&#38754;&#21521;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#35821;&#35328;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Towards Uncertainty-Aware Language Agent. (arXiv:2401.14016v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14016
&lt;/p&gt;
&lt;p&gt;
UALA&#26159;&#19968;&#20010;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#36827;&#34892;&#20195;&#29702;&#21644;&#22806;&#37096;&#19990;&#30028;&#20132;&#20114;&#30340;&#26694;&#26550;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#23427;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#35821;&#35328;&#27169;&#22411;&#23610;&#23544;&#19979;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#19988;&#22312;&#23545;&#22806;&#37096;&#19990;&#30028;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#26356;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#35821;&#35328;&#26234;&#33021;&#20307;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32622;&#20110;&#26356;&#22810;&#21151;&#33021;&#30340;&#35774;&#35745;&#26680;&#24515;&#20197;&#21450;&#19982;&#22806;&#37096;&#19990;&#30028;&#30340;&#21160;&#24577;&#20132;&#20114;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#22312;&#36825;&#20123;&#20132;&#20114;&#36807;&#31243;&#20013;&#24573;&#35270;&#20102;&#19981;&#30830;&#23450;&#24615;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#35821;&#35328;&#26234;&#33021;&#20307;&#65288;UALA&#65289;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#32534;&#25490;&#20195;&#29702;&#21644;&#22806;&#37096;&#19990;&#30028;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#19982;&#20854;&#20182;&#30693;&#21517;&#23545;&#25163;&#65288;&#22914;ReAct&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;3&#20010;&#20195;&#34920;&#24615;&#20219;&#21153;&#65288;HotpotQA&#65292;StrategyQA&#65292;MMLU&#65289;&#21644;&#21508;&#31181;LLM&#23610;&#23544;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;UALA&#22312;&#24615;&#33021;&#26041;&#38754;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#21516;&#26102;&#23545;&#22806;&#37096;&#19990;&#30028;&#30340;&#20381;&#36182;&#24615;&#26174;&#33879;&#38477;&#20302;&#65288;&#21363;&#65292;&#20943;&#23569;&#20102;&#24037;&#20855;&#35843;&#29992;&#21644;&#26631;&#35760;&#25968;&#65289;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#21508;&#31181;&#35265;&#35299;&#65292;&#21253;&#25324;&#19982;&#20195;&#29702;&#24494;&#35843;&#30456;&#27604;&#65292;UALA&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#24182;&#24378;&#35843;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#21475;&#22836;&#32622;&#20449;&#24230;&#20316;&#20026;&#19981;&#30830;&#23450;&#24615;&#30340;&#20195;&#29702;&#26102;&#30340;&#19981;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Language Agents have achieved promising success by placing Large Language Models at the core of a more versatile design that dynamically interacts with the external world, the existing approaches neglect the notion of uncertainty during these interactions. We present the Uncertainty-Aware Language Agent (UALA), a framework that orchestrates the interaction between the agent and the external world using uncertainty quantification. Compared with other well-known counterparts like ReAct, our extensive experiments across 3 representative tasks (HotpotQA, StrategyQA, MMLU) and various LLM sizes demonstrates that UALA brings a significant improvement of performance, while having a substantially lower reliance on the external world (i.e., reduced number of tool calls and tokens). Our analyses provide various insights including the great potential of UALA compared with agent fine-tuning, and underscoring the unreliably of verbalised confidence of LLMs as a proxy for uncertainty.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Chem-FINESE&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#21270;&#23398;&#39046;&#22495;&#20013;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#21644;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#26469;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#24182;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10189</link><description>&lt;p&gt;
Chem-FINESE: &#36890;&#36807;&#25991;&#26412;&#37325;&#26500;&#39564;&#35777;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction. (arXiv:2401.10189v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10189
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Chem-FINESE&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#21270;&#23398;&#39046;&#22495;&#20013;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#21644;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#26469;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#24182;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21270;&#23398;&#39046;&#22495;&#20013;&#65292;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#38754;&#20020;&#20004;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#19982;&#19968;&#33324;&#39046;&#22495;&#30340;&#23454;&#20307;&#25552;&#21462;&#20219;&#21153;&#30456;&#27604;&#65292;&#21270;&#23398;&#35770;&#25991;&#20013;&#30340;&#21477;&#23376;&#36890;&#24120;&#21253;&#21547;&#26356;&#22810;&#30340;&#23454;&#20307;&#12290;&#27492;&#22806;&#65292;&#23454;&#20307;&#25552;&#21462;&#27169;&#22411;&#36890;&#24120;&#38590;&#20197;&#25552;&#21462;&#38271;&#23614;&#31867;&#22411;&#30340;&#23454;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;Chem-FINESE&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;Chem-FINESE&#21253;&#21547;&#20004;&#20010;&#32452;&#20214;&#65306;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#29992;&#20110;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#65292;&#20197;&#21450;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#29992;&#20110;&#20174;&#25552;&#21462;&#30340;&#23454;&#20307;&#20013;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#21463;&#21040;&#19968;&#20010;&#22909;&#30340;&#23454;&#20307;&#25552;&#21462;&#31995;&#32479;&#38656;&#35201;&#24544;&#23454;&#25552;&#21462;&#23454;&#20307;&#30340;&#20107;&#23454;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26032;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#21033;&#29992;&#23454;&#20307;&#25552;&#21462;&#32467;&#26524;&#26469;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#25439;&#22833;&#26469;&#20943;&#23569;&#22312;&#25552;&#21462;&#36807;&#31243;&#20013;&#30340;&#36807;&#24230;&#22797;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-grained few-shot entity extraction in the chemical domain faces two unique challenges. First, compared with entity extraction tasks in the general domain, sentences from chemical papers usually contain more entities. Moreover, entity extraction models usually have difficulty extracting entities of long-tailed types. In this paper, we propose Chem-FINESE, a novel sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to address these two challenges. Our Chem-FINESE has two components: a seq2seq entity extractor to extract named entities from the input sentence and a seq2seq self-validation module to reconstruct the original input sentence from extracted entities. Inspired by the fact that a good entity extraction system needs to extract entities faithfully, our new self-validation module leverages entity extraction results to reconstruct the original input sentence. Besides, we design a new contrastive loss to reduce excessive copying during the extraction proces
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#36923;&#36753;&#22238;&#24402;&#30340;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33609;&#22270;&#24341;&#23548;&#32422;&#26463;&#35299;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26412;&#22320;&#36741;&#21161;&#27169;&#22411;&#20248;&#21270;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20197;&#21021;&#27493;&#36755;&#20986;&#20316;&#20026;&#36827;&#19968;&#27493;&#25193;&#23637;&#30340; "&#33609;&#22270;"&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26377;&#38480;&#32422;&#26463;&#35299;&#30721;&#30340;&#24212;&#29992;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.09967</link><description>&lt;p&gt;
&#26080;&#38656;&#35775;&#38382;&#36923;&#36753;&#22238;&#24402;&#30340;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33609;&#22270;&#24341;&#23548;&#32422;&#26463;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Sketch-Guided Constrained Decoding for Boosting Blackbox Large Language Models without Logit Access. (arXiv:2401.09967v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#36923;&#36753;&#22238;&#24402;&#30340;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33609;&#22270;&#24341;&#23548;&#32422;&#26463;&#35299;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26412;&#22320;&#36741;&#21161;&#27169;&#22411;&#20248;&#21270;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20197;&#21021;&#27493;&#36755;&#20986;&#20316;&#20026;&#36827;&#19968;&#27493;&#25193;&#23637;&#30340; "&#33609;&#22270;"&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26377;&#38480;&#32422;&#26463;&#35299;&#30721;&#30340;&#24212;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#32422;&#26463;&#22312;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#25511;&#21046;&#19978;&#25552;&#20379;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#25110;&#26550;&#26500;&#20462;&#25913;&#30340;&#26041;&#24335;&#65292;&#20294;&#36890;&#24120;&#21482;&#36866;&#29992;&#20110;&#25317;&#26377;&#36923;&#36753;&#22238;&#24402;&#35775;&#38382;&#26435;&#38480;&#30340;&#27169;&#22411;&#65292;&#36825;&#23545;&#20110;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#38480;&#21046;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33609;&#22270;&#24341;&#23548;&#30340;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32422;&#26463;&#35299;&#30721;&#65288;SGCD&#65289;&#26041;&#27861;&#65292;&#26080;&#38656;&#35775;&#38382;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#22238;&#24402;&#12290;SGCD&#21033;&#29992;&#26412;&#22320;&#36741;&#21161;&#27169;&#22411;&#26469;&#20248;&#21270;&#26080;&#32422;&#26463;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#23558;&#20854;&#20316;&#20026;&#36827;&#19968;&#27493;&#25193;&#23637;&#30340;&#8220;&#33609;&#22270;&#8221;&#12290;&#27492;&#26041;&#27861;&#21487;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#36923;&#36753;&#22238;&#24402;&#30340;&#25216;&#26415;&#30456;&#20114;&#34917;&#20805;&#65292;&#20351;&#26377;&#38480;&#32422;&#26463;&#35299;&#30721;&#22312;&#26080;&#27861;&#23436;&#20840;&#36879;&#26126;&#30340;&#27169;&#22411;&#29615;&#22659;&#20013;&#24212;&#29992;&#12290;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20102;SGCD&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constrained decoding, a technique for enforcing constraints on language model outputs, offers a way to control text generation without retraining or architectural modifications. Its application is, however, typically restricted to models that give users access to next-token distributions (usually via softmax logits), which poses a limitation with blackbox large language models (LLMs). This paper introduces sketch-guided constrained decoding (SGCD), a novel approach to constrained decoding for blackbox LLMs, which operates without access to the logits of the blackbox LLM. SGCD utilizes a locally hosted auxiliary model to refine the output of an unconstrained blackbox LLM, effectively treating this initial output as a "sketch" for further elaboration. This approach is complementary to traditional logit-based techniques and enables the application of constrained decoding in settings where full model transparency is unavailable. We demonstrate the efficacy of SGCD through experiments in cl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;Patchscope&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#32479;&#19968;&#20102;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;&#36824;&#35299;&#20915;&#20102;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#24182;&#19988;&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06102</link><description>&lt;p&gt;
Patchscope: &#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models. (arXiv:2401.06102v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;Patchscope&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#32479;&#19968;&#20102;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;&#36824;&#35299;&#20915;&#20102;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#24182;&#19988;&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38544;&#34255;&#34920;&#31034;&#20013;&#32534;&#30721;&#30340;&#20449;&#24687;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#30340;&#34892;&#20026;&#24182;&#39564;&#35777;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#12290;&#37492;&#20110;LLM&#29983;&#25104;&#20154;&#31867;&#21487;&#29702;&#35299;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#27169;&#22411;&#26412;&#36523;&#20197;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20854;&#20869;&#37096;&#34920;&#31034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;Patchscopes&#30340;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23427;&#26469;&#22238;&#31572;&#20851;&#20110;LLM&#35745;&#31639;&#30340;&#21508;&#31181;&#30740;&#31350;&#38382;&#39064;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20808;&#21069;&#22522;&#20110;&#23558;&#34920;&#31034;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#21644;&#24178;&#39044;LLM&#35745;&#31639;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#35813;&#26694;&#26550;&#30340;&#29305;&#27530;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;Patchscope&#21487;&#20197;&#24357;&#34917;&#20248;&#21183;&#65292;&#22914;&#26816;&#26597;&#26089;&#26399;&#23618;&#22833;&#36133;&#25110;&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#12290;&#38500;&#20102;&#32479;&#19968;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;Patchscopes&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#20363;&#22914;&#20351;&#29992;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26469;&#35299;&#37322;&#36739;&#23567;&#27169;&#22411;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspecting the information encoded in hidden representations of large language models (LLMs) can explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of research questions about an LLM's computation. We show that prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation, can be viewed as special instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by a Patchscope. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;LLMs&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;Transformer&#27169;&#22359;&#24182;&#20351;&#29992;&#26032;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#35843;&#25972;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#30693;&#35782;&#65292;&#32780;&#19981;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#22312;&#20195;&#30721;&#21644;&#25968;&#23398;&#39046;&#22495;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLaMA Pro&#26159;&#19968;&#31181;&#21151;&#33021;&#24378;&#22823;&#19988;&#36866;&#29992;&#20110;&#36890;&#29992;&#20219;&#21153;&#12289;&#32534;&#31243;&#21644;&#25968;&#23398;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#26234;&#33021;&#20307;&#25512;&#29702;&#21644;&#35299;&#20915;&#22810;&#26679;&#21270;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.02415</link><description>&lt;p&gt;
LLaMA Pro: &#24102;&#26377;&#27169;&#22359;&#25193;&#23637;&#30340;&#28176;&#36827;LLaMA&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LLaMA Pro: Progressive LLaMA with Block Expansion. (arXiv:2401.02415v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;LLMs&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;Transformer&#27169;&#22359;&#24182;&#20351;&#29992;&#26032;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#35843;&#25972;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#30693;&#35782;&#65292;&#32780;&#19981;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#22312;&#20195;&#30721;&#21644;&#25968;&#23398;&#39046;&#22495;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLaMA Pro&#26159;&#19968;&#31181;&#21151;&#33021;&#24378;&#22823;&#19988;&#36866;&#29992;&#20110;&#36890;&#29992;&#20219;&#21153;&#12289;&#32534;&#31243;&#21644;&#25968;&#23398;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#26234;&#33021;&#20307;&#25512;&#29702;&#21644;&#35299;&#20915;&#22810;&#26679;&#21270;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#24120;&#22312;&#19981;&#29306;&#29298;&#26087;&#25216;&#33021;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26032;&#25216;&#33021;&#65307;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20363;&#22914;&#20174;LLaMA&#21040;CodeLLaMA&#21017;&#30456;&#21453;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;Transformer&#27169;&#22359;&#25193;&#23637;&#30340;LLMs&#30340;&#26032;&#30340;&#39044;&#35757;&#32451;&#21518;&#22788;&#29702;&#26041;&#27861;&#12290;&#25105;&#20204;&#20165;&#20351;&#29992;&#26032;&#30340;&#35821;&#26009;&#24211;&#35843;&#25972;&#25193;&#23637;&#30340;&#27169;&#22359;&#65292;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#27169;&#22411;&#30340;&#30693;&#35782;&#32780;&#19981;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#26412;&#25991;&#22312;&#20195;&#30721;&#21644;&#25968;&#23398;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24471;&#21040;&#20102;&#20174;LLaMA2-7B&#21021;&#22987;&#21270;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;LLaMA Pro-8.3B&#65292;&#22312;&#24120;&#35268;&#20219;&#21153;&#12289;&#32534;&#31243;&#21644;&#25968;&#23398;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;LLaMA Pro&#21450;&#20854;&#25353;&#29031;&#25351;&#20196;&#25191;&#34892;&#30340;&#23545;&#24212;&#27169;&#22411;&#65288;LLaMA Pro-Instruct&#65289;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#21462;&#24471;&#20102;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#22312;LLaMA&#31995;&#21015;&#21644;&#20854;&#20182;&#24320;&#25918;&#27169;&#22411;&#20013;&#30340;&#20248;&#36234;&#24615;&#20197;&#21450;&#20316;&#20026;&#26234;&#33021;&#20307;&#25512;&#29702;&#21644;&#35299;&#20915;&#22810;&#26679;&#21270;&#20219;&#21153;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#23545;&#20110;&#25972;&#21512;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#20026;&#21457;&#23637;&#25512;&#29702;&#21644;&#35299;&#20915;&#22810;&#26679;&#21270;&#20219;&#21153;&#30340;&#26234;&#33021;&#20307;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans generally acquire new skills without compromising the old; however, the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with an expansion of Transformer blocks. We tune the expanded blocks using only new corpus, efficiently and effectively improving the model's knowledge without catastrophic forgetting. In this paper, we experiment on the corpus of code and math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro and its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced performance among various benchmarks, demonstrating superiority over existing open models in the LLaMA family and the immense potential of reasoning and addressing diverse tasks as an intelligent agent. Our findings provide valuable insights into integrating natural and programming languages, laying a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MVRE&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20851;&#31995;&#35299;&#32806;&#20026;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#29983;&#25104;&#22810;&#35270;&#35282;&#20851;&#31995;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#33021;&#21147;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.17267</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#35270;&#35282;&#35299;&#32806;&#23398;&#20064;&#25913;&#36827;&#20302;&#36164;&#28304;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#20851;&#31995;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Improving Low-resource Prompt-based Relation Representation with Multi-view Decoupling Learning. (arXiv:2312.17267v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17267
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MVRE&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20851;&#31995;&#35299;&#32806;&#20026;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#29983;&#25104;&#22810;&#35270;&#35282;&#20851;&#31995;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#33021;&#21147;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#36827;&#34892;&#25552;&#31034;&#35843;&#25972;&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#20219;&#21153;&#30340;&#22686;&#24378;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#65292;&#21363;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#23545;&#20851;&#31995;&#30340;&#34920;&#23618;&#29702;&#35299;&#65292;&#20808;&#21069;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#21487;&#33021;&#20173;&#28982;&#34920;&#29616;&#19981;&#20339;&#65292;&#29992;&#20110;&#34920;&#31034;&#23398;&#20064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24378;&#35843;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#23398;&#20064;&#39640;&#36136;&#37327;&#20851;&#31995;&#34920;&#31034;&#23545;&#20110;RE&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#20851;&#31995;&#34920;&#31034;&#26041;&#27861;&#65292;&#21517;&#20026;MVRE&#65288;&#22810;&#35270;&#35282;&#20851;&#31995;&#25277;&#21462;&#65289;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;PLMs&#30340;&#33021;&#21147;&#26469;&#25913;&#21892;&#20302;&#36164;&#28304;&#25552;&#31034;&#35843;&#25972;&#33539;&#24335;&#19979;&#30340;RE&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MVRE&#23558;&#27599;&#20010;&#20851;&#31995;&#35299;&#32806;&#20026;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#20197;&#21253;&#21547;&#22810;&#35270;&#35282;&#30340;&#20851;&#31995;&#34920;&#31034;&#65292;&#20197;&#26368;&#22823;&#21270;&#20851;&#31995;&#25512;&#26029;&#36807;&#31243;&#20013;&#30340;&#20284;&#28982;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#20840;&#23616;&#24615;&#30340;&#20302;&#39046;&#22495;&#20219;&#21153;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20851;&#31995;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, prompt-tuning with pre-trained language models (PLMs) has demonstrated the significantly enhancing ability of relation extraction (RE) tasks. However, in low-resource scenarios, where the available training data is scarce, previous prompt-based methods may still perform poorly for prompt-based representation learning due to a superficial understanding of the relation. To this end, we highlight the importance of learning high-quality relation representation in low-resource scenarios for RE, and propose a novel prompt-based relation representation method, named MVRE (\underline{M}ulti-\underline{V}iew \underline{R}elation \underline{E}xtraction), to better leverage the capacity of PLMs to improve the performance of RE within the low-resource prompt-tuning paradigm. Specifically, MVRE decouples each relation into different perspectives to encompass multi-view relation representations for maximizing the likelihood during relation inference. Furthermore, we also design a Global-Lo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#34920;&#36798;&#24615;&#65292;&#25193;&#23637;&#20102;&#22270;&#28789;&#23436;&#22791;&#24615;&#32467;&#26524;&#21040;&#27010;&#29575;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#20102;&#19978;&#19979;&#30028;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.12942</link><description>&lt;p&gt;
&#20851;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Representational Capacity of Recurrent Neural Language Models. (arXiv:2310.12942v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#34920;&#36798;&#24615;&#65292;&#25193;&#23637;&#20102;&#22270;&#28789;&#23436;&#22791;&#24615;&#32467;&#26524;&#21040;&#27010;&#29575;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#20102;&#19978;&#19979;&#30028;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNNs)&#30340;&#35821;&#35328;&#27169;&#22411;(LMs)&#30340;&#35745;&#31639;&#34920;&#36798;&#24615;&#12290;Siegelmann&#21644;Sontag(1992)&#26366;&#32463;&#23637;&#31034;&#20102;&#20855;&#26377;&#26377;&#29702;&#26435;&#37325;&#21644;&#38544;&#34255;&#29366;&#24577;&#20197;&#21450;&#26080;&#38480;&#35745;&#31639;&#26102;&#38388;&#30340;RNNs&#26159;&#22270;&#28789;&#23436;&#22791;&#30340;&#12290;&#28982;&#32780;&#65292;LMs&#19981;&#20165;&#23450;&#20041;&#20102;&#23383;&#31526;&#20018;&#19978;&#30340;&#21152;&#26435;&#65292;&#36824;&#23450;&#20041;&#20102;(&#38750;&#21152;&#26435;)&#35821;&#35328;&#25104;&#21592;&#20851;&#31995;&#65292;&#23545;RNN LMs&#65288;RLMs&#65289;&#30340;&#35745;&#31639;&#33021;&#21147;&#20998;&#26512;&#24212;&#35813;&#21453;&#26144;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#23558;&#22270;&#28789;&#23436;&#22791;&#24615;&#32467;&#26524;&#25193;&#23637;&#21040;&#27010;&#29575;&#24773;&#20917;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#26377;&#29702;&#26435;&#37325;&#30340;RLM&#21644;&#26080;&#38480;&#35745;&#31639;&#26102;&#38388;&#26469;&#27169;&#25311;&#20219;&#20309;&#27010;&#29575;&#22270;&#28789;&#26426;(PTM)&#12290;&#30001;&#20110;&#22312;&#23454;&#36341;&#20013;&#65292;RLMs&#23454;&#26102;&#24037;&#20316;&#65292;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#22788;&#29702;&#19968;&#20010;&#31526;&#21495;&#65292;&#22240;&#27492;&#25105;&#20204;&#23558;&#19978;&#36848;&#32467;&#26524;&#20316;&#20026;RLMs&#34920;&#36798;&#24615;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23637;&#31034;&#22312;&#23454;&#26102;&#35745;&#31639;&#38480;&#21046;&#19979;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#27169;&#25311;&#30830;&#23450;&#24615;&#23454;&#26102;&#26377;&#29702;PTMs&#26469;&#25552;&#20379;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates the computational expressivity of language models (LMs) based on recurrent neural networks (RNNs). Siegelmann and Sontag (1992) famously showed that RNNs with rational weights and hidden states and unbounded computation time are Turing complete. However, LMs define weightings over strings in addition to just (unweighted) language membership and the analysis of the computational power of RNN LMs (RLMs) should reflect this. We extend the Turing completeness result to the probabilistic case, showing how a rationally weighted RLM with unbounded computation time can simulate any probabilistic Turing machine (PTM). Since, in practice, RLMs work in real-time, processing a symbol at every time step, we treat the above result as an upper bound on the expressivity of RLMs. We also provide a lower bound by showing that under the restriction to real-time computation, such models can simulate deterministic real-time rational PTMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#31995;&#32479;&#21270;&#38450;&#24481;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2310.12815</link><description>&lt;p&gt;
LLM-&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#21644;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Prompt Injection Attacks and Defenses in LLM-Integrated Applications. (arXiv:2310.12815v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#31995;&#32479;&#21270;&#38450;&#24481;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20316;&#21508;&#31181;&#31216;&#20026;LLM-&#38598;&#25104;&#24212;&#29992;&#30340;&#23454;&#38469;&#24212;&#29992;&#31243;&#24207;&#30340;&#21518;&#31471;&#12290;&#26368;&#36817;&#30340;&#22810;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;LLM-&#38598;&#25104;&#24212;&#29992;&#23481;&#26131;&#21463;&#21040;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#23558;&#24694;&#24847;&#25351;&#20196;/&#25968;&#25454;&#27880;&#20837;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#30340;&#36755;&#20837;&#20013;&#65292;&#20197;&#36798;&#21040;&#25915;&#20987;&#32773;&#30340;&#39044;&#26399;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#38480;&#20110;&#26696;&#20363;&#30740;&#31350;&#65292;&#32570;&#20047;&#23545;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#21450;&#20854;&#38450;&#24481;&#30340;&#31995;&#32479;&#29702;&#35299;&#12290;&#26412;&#35770;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#23558;&#30740;&#31350;&#35770;&#25991;&#21644;&#21338;&#23458;&#25991;&#31456;&#20013;&#35752;&#35770;&#30340;&#29616;&#26377;&#25915;&#20987;&#35270;&#20026;&#25105;&#20204;&#26694;&#26550;&#30340;&#29305;&#20363;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#32452;&#21512;&#29616;&#26377;&#25915;&#20987;&#35774;&#35745;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#38450;&#24481;&#30340;&#26694;&#26550;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#38450;&#21644;&#32531;&#35299;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly deployed as the backend for a variety of real-world applications called LLM-Integrated Applications. Multiple recent works showed that LLM-Integrated Applications are vulnerable to prompt injection attacks, in which an attacker injects malicious instruction/data into the input of those applications such that they produce results as the attacker desires. However, existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a general framework to formalize prompt injection attacks. Existing attacks, which are discussed in research papers and blog posts, are special cases in our framework. Our framework enables us to design a new attack by combining existing attacks. Moreover, we also propose a framework to systematize defenses against prompt injection attacks. Using our frameworks, we con
&lt;/p&gt;</description></item><item><title>ChatKBQA&#26159;&#19968;&#20010;&#22522;&#20110;&#31934;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;-&#26816;&#32034;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#30693;&#35782;&#24211;&#38382;&#31572;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#22909;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.08975</link><description>&lt;p&gt;
ChatKBQA: &#19968;&#20010;&#22522;&#20110;&#31934;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;-&#26816;&#32034;&#26694;&#26550;&#29992;&#20110;&#30693;&#35782;&#24211;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models. (arXiv:2310.08975v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08975
&lt;/p&gt;
&lt;p&gt;
ChatKBQA&#26159;&#19968;&#20010;&#22522;&#20110;&#31934;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;-&#26816;&#32034;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#30693;&#35782;&#24211;&#38382;&#31572;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#26088;&#22312;&#36890;&#36807;&#22823;&#35268;&#27169;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#33719;&#21462;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#30340;&#31572;&#26696;&#65292;&#36890;&#24120;&#20998;&#20026;&#20004;&#20010;&#30740;&#31350;&#32452;&#25104;&#37096;&#20998;&#65306;&#30693;&#35782;&#26816;&#32034;&#21644;&#35821;&#20041;&#35299;&#26512;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19977;&#20010;&#26680;&#24515;&#25361;&#25112;&#65292;&#21253;&#25324;&#20302;&#25928;&#30340;&#30693;&#35782;&#26816;&#32034;&#12289;&#26816;&#32034;&#38169;&#35823;&#23545;&#35821;&#20041;&#35299;&#26512;&#30340;&#19981;&#21033;&#24433;&#21709;&#20197;&#21450;&#20043;&#21069;&#30340;KBQA&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26102;&#20195;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChatKBQA&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#31934;&#35843;&#24320;&#28304;LLMs&#65288;&#22914;Llama-2&#12289;ChatGLM2&#21644;Baichuan2&#65289;&#26500;&#24314;&#30340;&#29983;&#25104;-&#26816;&#32034;KBQA&#26694;&#26550;&#12290;ChatKBQA&#25552;&#35758;&#39318;&#20808;&#20351;&#29992;&#31934;&#35843;&#30340;LLMs&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#65292;&#28982;&#21518;&#36890;&#36807;&#26080;&#30417;&#30563;&#26816;&#32034;&#26041;&#27861;&#26816;&#32034;&#21644;&#26367;&#25442;&#23454;&#20307;&#21644;&#20851;&#31995;&#65292;&#20174;&#32780;&#26356;&#30452;&#35266;&#22320;&#25913;&#36827;&#20102;&#29983;&#25104;&#21644;&#26816;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ChatKBQA&#22312;&#26631;&#20934;KBQA&#25968;&#25454;&#38598;WebQSP&#21644;ComplexWebQuestions (CWQ)&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Base Question Answering (KBQA) aims to derive answers to natural language questions over large-scale knowledge bases (KBs), which are generally divided into two research components: knowledge retrieval and semantic parsing. However, three core challenges remain, including inefficient knowledge retrieval, retrieval errors adversely affecting semantic parsing, and the complexity of previous KBQA methods. In the era of large language models (LLMs), we introduce ChatKBQA, a novel generate-then-retrieve KBQA framework built on fine-tuning open-source LLMs such as Llama-2, ChatGLM2 and Baichuan2. ChatKBQA proposes generating the logical form with fine-tuned LLMs first, then retrieving and replacing entities and relations through an unsupervised retrieval method, which improves both generation and retrieval more straightforwardly. Experimental results reveal that ChatKBQA achieves new state-of-the-art performance on standard KBQA datasets, WebQSP, and ComplexWebQuestions (CWQ). This
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#38646;&#23556;&#20132;&#20114;&#20010;&#24615;&#21270;&#23545;&#35937;&#23548;&#33322;&#65288;ZIPON&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#29992;&#25143;&#21453;&#39304;&#65292;&#35299;&#20915;&#20102;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#23548;&#33322;&#21040;&#20010;&#24615;&#21270;&#30446;&#26631;&#23545;&#35937;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07968</link><description>&lt;p&gt;
&#24605;&#32771;&#12289;&#34892;&#21160;&#21644;&#38382;&#65306;&#24320;&#25918;&#19990;&#30028;&#20114;&#21160;&#20010;&#24615;&#21270;&#26426;&#22120;&#20154;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Think, Act, and Ask: Open-World Interactive Personalized Robot Navigation. (arXiv:2310.07968v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07968
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#38646;&#23556;&#20132;&#20114;&#20010;&#24615;&#21270;&#23545;&#35937;&#23548;&#33322;&#65288;ZIPON&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#29992;&#25143;&#21453;&#39304;&#65292;&#35299;&#20915;&#20102;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#23548;&#33322;&#21040;&#20010;&#24615;&#21270;&#30446;&#26631;&#23545;&#35937;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#23556;&#21629;&#20196;&#23545;&#35937;&#23548;&#33322;&#65288;ZSON&#65289;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#23548;&#33322;&#21040;&#24320;&#25918;&#35789;&#27719;&#23545;&#35937;&#12290;&#29616;&#26377;&#30340;ZSON&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#36981;&#24490;&#20010;&#21035;&#25351;&#20196;&#20197;&#23547;&#25214;&#36890;&#29992;&#23545;&#35937;&#31867;&#65292;&#24573;&#30053;&#20102;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30340;&#21033;&#29992;&#21644;&#35782;&#21035;&#29992;&#25143;&#29305;&#23450;&#23545;&#35937;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38646;&#23556;&#20132;&#20114;&#20010;&#24615;&#21270;&#23545;&#35937;&#23548;&#33322;&#65288;ZIPON&#65289;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#38656;&#35201;&#22312;&#19982;&#29992;&#25143;&#23545;&#35805;&#30340;&#21516;&#26102;&#23548;&#33322;&#21040;&#20010;&#24615;&#21270;&#30446;&#26631;&#23545;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;ZIPON&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#31216;&#20026;&#24320;&#25918;&#19990;&#30028;&#20114;&#21160;&#20010;&#24615;&#21270;&#23548;&#33322;&#65288;ORION&#65289;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#24207;&#21015;&#20915;&#31574;&#65292;&#20197;&#25805;&#20316;&#19981;&#21516;&#30340;&#24863;&#30693;&#12289;&#23548;&#33322;&#21644;&#36890;&#20449;&#27169;&#22359;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#33021;&#22815;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#30340;&#20114;&#21160;&#20195;&#29702;&#30340;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#22312;&#20219;&#21153;&#23436;&#25104;&#21644;&#29992;&#25143;&#28385;&#24847;&#24230;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#30340;&#24179;&#34913;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Object Navigation (ZSON) enables agents to navigate towards open-vocabulary objects in unknown environments. The existing works of ZSON mainly focus on following individual instructions to find generic object classes, neglecting the utilization of natural language interaction and the complexities of identifying user-specific objects. To address these limitations, we introduce Zero-shot Interactive Personalized Object Navigation (ZIPON), where robots need to navigate to personalized goal objects while engaging in conversations with users. To solve ZIPON, we propose a new framework termed Open-woRld Interactive persOnalized Navigation (ORION), which uses Large Language Models (LLMs) to make sequential decisions to manipulate different modules for perception, navigation and communication. Experimental results show that the performance of interactive agents that can leverage user feedback exhibits significant improvement. However, obtaining a good balance between task completion 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#29983;&#25104;&#25439;&#22833;&#20989;&#25968;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#26041;&#27861;&#20013;&#35789;&#27719;&#21305;&#37197;&#21644;&#32570;&#23569;&#19978;&#19979;&#25991;&#32771;&#34385;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.05804</link><description>&lt;p&gt;
&#29983;&#25104;&#8220;nice&#8221;&#32780;&#19981;&#26159;&#29983;&#25104;&#8220;good&#8221;&#19981;&#20687;&#29983;&#25104;&#8220;rice&#8221;&#37027;&#20040;&#31967;&#31957;&#65281;&#26397;&#30528;&#19978;&#19979;&#25991;&#21644;&#35821;&#20041;&#34701;&#21512;&#30340;&#23545;&#35805;&#29983;&#25104;&#25439;&#22833;&#20989;&#25968;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hi Model, generating 'nice' instead of 'good' is not as bad as generating 'rice'! Towards Context and Semantic Infused Dialogue Generation Loss Function and Evaluation Metric. (arXiv:2309.05804v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05804
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#29983;&#25104;&#25439;&#22833;&#20989;&#25968;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#26041;&#27861;&#20013;&#35789;&#27719;&#21305;&#37197;&#21644;&#32570;&#23569;&#19978;&#19979;&#25991;&#32771;&#34385;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20108;&#21313;&#24180;&#20013;&#65292;&#23545;&#35805;&#24314;&#27169;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20174;&#31616;&#21333;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#22238;&#31572;&#21457;&#23637;&#21040;&#20010;&#24615;&#21270;&#21644;&#26377;&#35828;&#26381;&#21147;&#30340;&#22238;&#31572;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#20123;&#36827;&#23637;&#65292;&#23545;&#35805;&#29983;&#25104;&#30340;&#30446;&#26631;&#20989;&#25968;&#21644;&#35780;&#20272;&#25351;&#26631;&#20173;&#28982;&#20572;&#28382;&#19981;&#21069;&#65292;&#20998;&#21035;&#26159;&#20132;&#21449;&#29109;&#21644;BLEU&#12290;&#36825;&#20123;&#22522;&#20110;&#35789;&#27719;&#30340;&#25351;&#26631;&#23384;&#22312;&#20197;&#19979;&#20027;&#35201;&#38480;&#21046;&#65306;(a)&#27809;&#26377;&#35821;&#20041;&#32771;&#34385;&#30340;&#35789;&#23545;&#35789;&#21305;&#37197;&#65306;&#23427;&#23558;&#29983;&#25104;&#8220;nice&#8221;&#21644;&#29983;&#25104;&#8220;rice&#8221;&#20316;&#20026;&#8220;good&#8221;&#30340;&#22833;&#36133;&#32479;&#19968;&#32771;&#34385;&#12290;(b)&#32570;&#23569;&#20026;&#35780;&#20272;&#29983;&#25104;&#30340;&#22238;&#31572;&#25552;&#20379;&#19978;&#19979;&#25991;&#23646;&#24615;&#65306;&#21363;&#20351;&#29983;&#25104;&#30340;&#22238;&#31572;&#19982;&#27491;&#22312;&#36827;&#34892;&#30340;&#23545;&#35805;&#19978;&#19979;&#25991;&#30456;&#20851;&#65292;&#22914;&#26524;&#19981;&#19982;&#35821;&#26009;&#24211;&#20013;&#25552;&#20379;&#30340;&#40644;&#37329;&#35805;&#35821;&#21305;&#37197;&#65292;&#20173;&#28982;&#21487;&#33021;&#21463;&#21040;&#24809;&#32602;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20840;&#38754;&#35843;&#26597;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19978;&#19979;&#25991;&#35821;&#20041;&#34701;&#21512;&#23545;&#35805;(SemTextualLogue)&#25439;&#22833;&#20989;&#25968;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21046;&#23450;&#20102;&#19968;&#22871;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;
&lt;/p&gt;
&lt;p&gt;
Over the past two decades, dialogue modeling has made significant strides, moving from simple rule-based responses to personalized and persuasive response generation. However, despite these advancements, the objective functions and evaluation metrics for dialogue generation have remained stagnant, i.e., cross-entropy and BLEU, respectively. These lexical-based metrics have the following key limitations: (a) word-to-word matching without semantic consideration: It assigns the same credit for failure to generate 'nice' and 'rice' for 'good'. (b) missing context attribute for evaluating the generated response: Even if a generated response is relevant to the ongoing dialogue context, it may still be penalized for not matching the gold utterance provided in the corpus. In this paper, we first investigate these limitations comprehensively and propose a new loss function called Semantic Infused Contextualized diaLogue (SemTextualLogue) loss function. Furthermore, we formulate a new evaluation
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#24320;&#28304;&#30340;&#27861;&#24459;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatLaw&#65292;&#23427;&#36890;&#36807;&#32508;&#21512;&#22806;&#37096;&#30693;&#35782;&#24211;&#20026;&#20013;&#22269;&#27861;&#24459;&#39046;&#22495;&#30340;&#25968;&#23383;&#21270;&#36716;&#22411;&#25552;&#20379;&#25903;&#25345;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#31934;&#24515;&#35774;&#35745;&#30340;&#27861;&#24459;&#39046;&#22495;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#23558;&#21521;&#37327;&#25968;&#25454;&#24211;&#26816;&#32034;&#19982;&#20851;&#38190;&#35789;&#26816;&#32034;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#27861;&#24459;&#25968;&#25454;&#31579;&#36873;&#20013;&#30340;&#27169;&#22411;&#24187;&#35273;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#27880;&#24847;&#21147;&#26041;&#27861;&#20197;&#22686;&#24378;&#23545;&#25991;&#26412;&#20013;&#20851;&#38190;&#20449;&#24687;&#30340;&#27880;&#24847;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.16092</link><description>&lt;p&gt;
ChatLaw: &#22522;&#20110;&#24320;&#28304;&#27861;&#24459;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#22806;&#37096;&#30693;&#35782;&#24211;
&lt;/p&gt;
&lt;p&gt;
ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases. (arXiv:2306.16092v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#24320;&#28304;&#30340;&#27861;&#24459;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatLaw&#65292;&#23427;&#36890;&#36807;&#32508;&#21512;&#22806;&#37096;&#30693;&#35782;&#24211;&#20026;&#20013;&#22269;&#27861;&#24459;&#39046;&#22495;&#30340;&#25968;&#23383;&#21270;&#36716;&#22411;&#25552;&#20379;&#25903;&#25345;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#31934;&#24515;&#35774;&#35745;&#30340;&#27861;&#24459;&#39046;&#22495;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#23558;&#21521;&#37327;&#25968;&#25454;&#24211;&#26816;&#32034;&#19982;&#20851;&#38190;&#35789;&#26816;&#32034;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#27861;&#24459;&#25968;&#25454;&#31579;&#36873;&#20013;&#30340;&#27169;&#22411;&#24187;&#35273;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#27880;&#24847;&#21147;&#26041;&#27861;&#20197;&#22686;&#24378;&#23545;&#25991;&#26412;&#20013;&#20851;&#38190;&#20449;&#24687;&#30340;&#27880;&#24847;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#25913;&#21464;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#28508;&#21147;&#65292;&#24341;&#21457;&#20102;&#23545;&#22402;&#30452;&#29305;&#23450;&#22823;&#27169;&#22411;&#30340;&#26497;&#22823;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#19982;&#20687;BloombergGPT&#21644;FinGPT&#36825;&#26679;&#21033;&#29992;&#20854;&#29420;&#29305;&#25968;&#25454;&#31215;&#32047;&#22312;&#37329;&#34701;&#39046;&#22495;&#21462;&#24471;&#36827;&#23637;&#30340;&#19987;&#26377;&#27169;&#22411;&#19981;&#21516;&#65292;&#20013;&#22269;&#27861;&#24459;&#39046;&#22495;&#20013;&#27809;&#26377;&#31867;&#20284;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#20419;&#36827;&#25968;&#23383;&#21270;&#36716;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ChatLaw&#30340;&#24320;&#28304;&#27861;&#24459;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#30001;&#20110;&#25968;&#25454;&#36136;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#19968;&#20010;&#27861;&#24459;&#39046;&#22495;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#22312;&#21442;&#32771;&#25968;&#25454;&#26816;&#32034;&#36807;&#31243;&#20013;&#27861;&#24459;&#25968;&#25454;&#31579;&#36873;&#20013;&#30340;&#27169;&#22411;&#24187;&#35273;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23558;&#21521;&#37327;&#25968;&#25454;&#24211;&#26816;&#32034;&#19982;&#20851;&#38190;&#35789;&#26816;&#32034;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#20943;&#23569;&#21482;&#20381;&#38752;&#21521;&#37327;&#25968;&#25454;&#24211;&#26816;&#32034;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#27880;&#24847;&#21147;&#26041;&#27861;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#23545;&#25991;&#26412;&#20013;&#20851;&#38190;&#20449;&#24687;&#30340;&#27880;&#24847;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown the potential to revolutionize natural language processing tasks in various domains, sparking great interest in vertical-specific large models. However, unlike proprietary models such as BloombergGPT and FinGPT, which have leveraged their unique data accumulations to make strides in the finance domain, there hasn't not many similar large language models in the Chinese legal domain to facilitate its digital transformation.  In this paper, we propose an open-source legal large language model named ChatLaw. Due to the importance of data quality, we carefully designed a legal domain fine-tuning dataset. Additionally, to overcome the problem of model hallucinations in legal data screening during reference data retrieval, we introduce a method that combines vector database retrieval with keyword retrieval to effectively reduce the inaccuracy of relying solely on vector database retrieval. Furthermore, we propose a self-attention method to enhance the a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#28151;&#21512;&#26469;&#28040;&#38500;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#65292;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#26032;&#26679;&#26412;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#32463;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#22810;&#31181;&#20219;&#21153;&#23454;&#39564;&#39564;&#35777;&#65292;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14521</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#28151;&#21512;&#28040;&#38500;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Eliminating Spurious Correlations from Pre-trained Models via Data Mixing. (arXiv:2305.14521v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#28151;&#21512;&#26469;&#28040;&#38500;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#65292;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#26032;&#26679;&#26412;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#32463;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#22810;&#31181;&#20219;&#21153;&#23454;&#39564;&#39564;&#35777;&#65292;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25910;&#25947;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#24448;&#24448;&#21033;&#29992;&#20102;&#26576;&#20123;&#23646;&#24615;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#22312;&#29305;&#23450;&#31867;&#21035;&#30340;&#22823;&#22810;&#25968;&#31034;&#20363;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20294;&#24182;&#19981;&#36275;&#20197;&#39044;&#27979;&#36825;&#20123;&#31867;&#21035;&#12290;&#23398;&#21040;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#21487;&#33021;&#20250;&#22312;&#23545;&#26032;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#21518;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#20250;&#38477;&#20302;&#27169;&#22411;&#23545;&#19981;&#23637;&#29616;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#31034;&#20363;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#20197;&#28040;&#38500;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#19968;&#23567;&#32452;&#24102;&#26377;&#34394;&#20551;&#23646;&#24615;&#30340;&#31034;&#20363;&#65292;&#24182;&#36890;&#36807;&#25968;&#25454;&#28151;&#21512;&#26469;&#24179;&#34913;&#25152;&#26377;&#31867;&#21035;&#20013;&#30340;&#34394;&#20551;&#23646;&#24615;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#35270;&#35273;&#21644;NLP&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#65292;&#21253;&#25324;&#28040;&#38500;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models pre-trained on large datasets have achieved remarkable convergence and robustness properties. However, these models often exploit spurious correlations between certain attributes and labels, which are prevalent in the majority of examples within specific categories but are not predictive of these categories in general. The learned spurious correlations may persist even after fine-tuning on new data, which degrades models' performance on examples that do not exhibit the spurious correlation. In this work, we propose a simple and highly effective method to eliminate spurious correlations from pre-trained models. The key idea of our method is to leverage a small set of examples with spurious attributes, and balance the spurious attributes across all classes via data mixing. We theoretically confirm the effectiveness of our method, and empirically demonstrate its state-of-the-art performance on various vision and NLP tasks, including eliminating spurious correlation
&lt;/p&gt;</description></item></channel></rss>