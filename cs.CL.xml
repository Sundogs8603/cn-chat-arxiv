<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>MMICL&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#35774;&#35745;&#65292;&#20197;&#35299;&#20915;VLM&#22312;&#29702;&#35299;&#22797;&#26434;&#22810;&#27169;&#24577;&#25552;&#31034;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2309.07915</link><description>&lt;p&gt;
MMICL&#65306;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#22686;&#24378;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning. (arXiv:2309.07915v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07915
&lt;/p&gt;
&lt;p&gt;
MMICL&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#35774;&#35745;&#65292;&#20197;&#35299;&#20915;VLM&#22312;&#29702;&#35299;&#22797;&#26434;&#22810;&#27169;&#24577;&#25552;&#31034;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#28145;&#24230;&#23398;&#20064;&#30340;&#22797;&#33487;&#24320;&#22987;&#65292;&#20511;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#21464;&#24471;&#38750;&#24120;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;LLM&#21487;&#20197;&#21033;&#29992;&#20016;&#23500;&#30340;&#32972;&#26223;&#30693;&#35782;&#21644;&#20219;&#21153;&#20449;&#24687;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#22823;&#22810;&#25968;VLM&#22312;&#29702;&#35299;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#65288;&#21253;&#21547;&#22810;&#20010;&#22270;&#20687;&#65289;&#26041;&#38754;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#36861;&#28335;&#21040;VLM&#30340;&#26550;&#26500;&#35774;&#35745;&#25110;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24403;&#21069;&#30340;VLM&#20027;&#35201;&#24378;&#35843;&#21033;&#29992;&#24102;&#26377;&#21333;&#20010;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#24102;&#26377;&#20132;&#38169;&#22810;&#20010;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#12290;&#23613;&#31649;&#19968;&#20123;&#26032;&#25552;&#20986;&#30340;VLM&#21487;&#20197;&#22788;&#29702;&#24102;&#26377;&#22810;&#20010;&#22270;&#20687;&#30340;&#29992;&#25143;&#25552;&#31034;&#65292;&#20294;&#39044;&#35757;&#32451;&#25968;&#25454;&#27809;&#26377;&#25552;&#20379;&#27604;&#20174;Web&#25235;&#21462;&#26102;&#20132;&#38169;&#22270;&#20687;&#21644;&#25991;&#26412;&#26356;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MMICL&#65292;&#20174;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#35282;&#24230;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#26080;&#32541;&#22320;&#38598;&#25104;&#35270;&#35273;&#21644;&#35821;&#35328;&#20449;&#24687;&#65292;&#24182;&#25552;&#20379;&#26356;&#20016;&#23500;&#30340;&#22810;&#27169;&#24577;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Starting from the resurgence of deep learning, vision-language models (VLMs) benefiting from large language models (LLMs) have never been so popular. However, while LLMs can utilize extensive background knowledge and task information with in-context learning, most VLMs still struggle with understanding complex multi-modal prompts with multiple images. The issue can traced back to the architectural design of VLMs or pre-training data. Specifically, the current VLMs primarily emphasize utilizing multi-modal data with a single image some, rather than multi-modal prompts with interleaved multiple images and text. Even though some newly proposed VLMs could handle user prompts with multiple images, pre-training data does not provide more sophisticated multi-modal prompts than interleaved image and text crawled from the web. We propose MMICL to address the issue by considering both the model and data perspectives. We introduce a well-designed architecture capable of seamlessly integrating vis
&lt;/p&gt;</description></item><item><title>&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#36873;&#25321;&#19982;&#27979;&#35797;&#36755;&#20837;&#35821;&#20041;&#30456;&#20284;&#30340;&#28436;&#31034;&#26377;&#21161;&#20110;&#25552;&#39640;&#19979;&#28216;&#24615;&#33021;&#65292;&#20294;&#26159;&#32771;&#34385;&#21040;&#35821;&#35328;&#27169;&#22411;&#20851;&#20110;&#20219;&#21153;&#30340;&#29616;&#26377;&#30693;&#35782;&#33021;&#22815;&#26356;&#22909;&#22320;&#25351;&#23548;&#28436;&#31034;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2309.07900</link><description>&lt;p&gt;
&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#27495;&#20041;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Ambiguity-Aware In-Context Learning with Large Language Models. (arXiv:2309.07900v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07900
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#36873;&#25321;&#19982;&#27979;&#35797;&#36755;&#20837;&#35821;&#20041;&#30456;&#20284;&#30340;&#28436;&#31034;&#26377;&#21161;&#20110;&#25552;&#39640;&#19979;&#28216;&#24615;&#33021;&#65292;&#20294;&#26159;&#32771;&#34385;&#21040;&#35821;&#35328;&#27169;&#22411;&#20851;&#20110;&#20219;&#21153;&#30340;&#29616;&#26377;&#30693;&#35782;&#33021;&#22815;&#26356;&#22909;&#22320;&#25351;&#23548;&#28436;&#31034;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;In-context learning, ICL&#65289;&#20013;&#65292;&#20165;&#21521;LLMs&#23637;&#31034;&#23569;&#37327;&#20219;&#21153;&#29305;&#23450;&#28436;&#31034;&#24050;&#32463;&#23548;&#33268;&#20102;&#19979;&#28216;&#22686;&#30410;&#65292;&#26080;&#38656;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;LLMs&#23545;&#20110;&#25552;&#31034;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#65292;&#22240;&#27492;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#38382;&#39064;&#26159;&#22914;&#20309;&#20026;ICL&#36873;&#25321;&#22909;&#30340;&#28436;&#31034;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#26159;&#21033;&#29992;ICL&#28436;&#31034;&#21644;&#27979;&#35797;&#36755;&#20837;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#26816;&#32034;&#22120;&#65292;&#28982;&#32780;&#36825;&#31181;&#26041;&#27861;&#24182;&#19981;&#32771;&#34385;LLM&#20851;&#20110;&#35813;&#20219;&#21153;&#30340;&#29616;&#26377;&#30693;&#35782;&#65292;&#22240;&#27492;&#24182;&#19981;&#26368;&#20248;&#12290;&#26681;&#25454;&#20043;&#21069;&#30340;&#24037;&#20316;&#65288;Min&#31561;&#65292;2022&#65289;&#65292;&#25105;&#20204;&#24050;&#32463;&#30693;&#36947;&#19982;&#28436;&#31034;&#37197;&#23545;&#30340;&#26631;&#31614;&#20250;&#23545;&#27169;&#22411;&#39044;&#27979;&#36896;&#25104;&#20559;&#35265;&#12290;&#36825;&#24341;&#23548;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#35774;&#65306;&#32771;&#34385;&#21040;LLM&#20851;&#20110;&#20219;&#21153;&#30340;&#29616;&#26377;&#30693;&#35782;&#65292;&#29305;&#21035;&#26159;&#19982;&#36755;&#20986;&#26631;&#31614;&#31354;&#38388;&#30456;&#20851;&#30340;&#30693;&#35782;&#65292;&#26159;&#21542;&#26377;&#21161;&#20110;&#26356;&#22909;&#30340;&#28436;&#31034;&#36873;&#25321;&#31574;&#30053;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#20165;&#36873;&#25321;&#35821;&#20041;&#30456;&#20284;&#30340;ICL&#28436;&#31034;&#26159;&#26377;&#30410;&#30340;&#65292;&#21516;&#26102;&#20063;&#35201;&#32771;&#34385;LLM&#20851;&#20110;&#20219;&#21153;&#30340;&#29616;&#26377;&#30693;&#35782;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#28436;&#31034;&#36873;&#25321;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) i.e. showing LLMs only a few task-specific demonstrations has led to downstream gains with no task-specific fine-tuning required. However, LLMs are sensitive to the choice of prompts, and therefore a crucial research question is how to select good demonstrations for ICL. One effective strategy is leveraging semantic similarity between the ICL demonstrations and test inputs by using a text retriever, which however is sub-optimal as that does not consider the LLM's existing knowledge about that task. From prior work (Min et al., 2022), we already know that labels paired with the demonstrations bias the model predictions. This leads us to our hypothesis whether considering LLM's existing knowledge about the task, especially with respect to the output label space can help in a better demonstration selection strategy. Through extensive experimentation on three text classification tasks, we find that it is beneficial to not only choose semantically similar ICL demon
&lt;/p&gt;</description></item><item><title>&#22312;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#20196;&#26102;&#65292;&#20165;&#24378;&#35843;&#24110;&#21161;&#24615;&#32780;&#19981;&#32771;&#34385;&#23433;&#20840;&#24615;&#20250;&#23548;&#33268;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#20869;&#23481;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35757;&#32451;LLaMA&#27169;&#22411;&#26102;&#28155;&#21152;&#23569;&#37327;&#23433;&#20840;&#31034;&#20363;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#23433;&#20840;&#24615;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#33021;&#21147;&#21644;&#24110;&#21161;&#24615;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#23433;&#20840;&#35843;&#20248;&#20250;&#20351;&#27169;&#22411;&#25298;&#32477;&#22238;&#24212;&#34920;&#38754;&#19978;&#31867;&#20284;&#20110;&#19981;&#23433;&#20840;&#25552;&#31034;&#30340;&#21512;&#29702;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.07875</link><description>&lt;p&gt;
&#23433;&#20840;&#35843;&#20248;&#30340;LLaMAs&#65306;&#20174;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#20196;&#30340;&#23433;&#20840;&#24615;&#20013;&#23398;&#21040;&#30340;&#32463;&#39564;
&lt;/p&gt;
&lt;p&gt;
Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions. (arXiv:2309.07875v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07875
&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#20196;&#26102;&#65292;&#20165;&#24378;&#35843;&#24110;&#21161;&#24615;&#32780;&#19981;&#32771;&#34385;&#23433;&#20840;&#24615;&#20250;&#23548;&#33268;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#20869;&#23481;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35757;&#32451;LLaMA&#27169;&#22411;&#26102;&#28155;&#21152;&#23569;&#37327;&#23433;&#20840;&#31034;&#20363;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#23433;&#20840;&#24615;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#33021;&#21147;&#21644;&#24110;&#21161;&#24615;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#23433;&#20840;&#35843;&#20248;&#20250;&#20351;&#27169;&#22411;&#25298;&#32477;&#22238;&#24212;&#34920;&#38754;&#19978;&#31867;&#20284;&#20110;&#19981;&#23433;&#20840;&#25552;&#31034;&#30340;&#21512;&#29702;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#20196;&#21487;&#20197;&#20351;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#24471;&#26356;&#22909;&#65292;&#36890;&#24120;&#26356;&#20855;&#26377;&#24110;&#21161;&#24615;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#23436;&#20840;&#26377;&#29992;&#30340;&#27169;&#22411;&#20250;&#36981;&#24490;&#29978;&#33267;&#26368;&#24694;&#24847;&#30340;&#25351;&#20196;&#65292;&#24182;&#36731;&#26131;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#26412;&#25991;&#20851;&#27880;&#30340;&#26159;&#37027;&#20123;&#21482;&#24378;&#35843;&#24110;&#21161;&#24615;&#32780;&#19981;&#32771;&#34385;&#23433;&#20840;&#24615;&#30340;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20123;&#27969;&#34892;&#30340;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#38750;&#24120;&#19981;&#23433;&#20840;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;fine-tuning&#31867;&#20284;LLaMA&#30340;&#27169;&#22411;&#26102;&#65292;&#21482;&#38656;&#23558;3%&#30340;&#23433;&#20840;&#31034;&#20363;&#65288;&#20960;&#30334;&#20010;&#28436;&#31034;&#65289;&#28155;&#21152;&#21040;&#35757;&#32451;&#38598;&#20013;&#65292;&#23601;&#33021;&#26174;&#33879;&#25552;&#39640;&#20854;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#30340;&#23433;&#20840;&#35843;&#20248;&#24182;&#19981;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#30340;&#33021;&#21147;&#25110;&#24110;&#21161;&#24615;&#65292;&#36825;&#26159;&#36890;&#36807;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#26469;&#34913;&#37327;&#30340;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#31181;&#36807;&#24230;&#23433;&#20840;&#30340;&#34892;&#20026;&#65292;&#21363;&#36807;&#24230;&#30340;&#23433;&#20840;&#35843;&#20248;&#20250;&#20351;&#24471;&#27169;&#22411;&#25298;&#32477;&#23545;&#34920;&#38754;&#19978;&#31867;&#20284;&#20110;&#19981;&#23433;&#20840;&#25552;&#31034;&#30340;&#21512;&#29702;&#25552;&#31034;&#20570;&#20986;&#22238;&#24212;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#35757;&#32451;LLM&#27169;&#22411;&#36981;&#24490;&#25351;&#20196;&#26102;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training large language models to follow instructions makes them perform better on a wide range of tasks, generally becoming more helpful. However, a perfectly helpful model will follow even the most malicious instructions and readily generate harmful content. In this paper, we raise concerns over the safety of models that only emphasize helpfulness, not safety, in their instruction-tuning. We show that several popular instruction-tuned models are highly unsafe. Moreover, we show that adding just 3% safety examples (a few hundred demonstrations) in the training set when fine-tuning a model like LLaMA can substantially improve their safety. Our safety-tuning does not make models significantly less capable or helpful as measured by standard benchmarks. However, we do find a behavior of exaggerated safety, where too much safety-tuning makes models refuse to respond to reasonable prompts that superficially resemble unsafe ones. Our study sheds light on trade-offs in training LLMs to follow
&lt;/p&gt;</description></item><item><title>Agents&#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#25903;&#25345;&#26500;&#24314;&#33258;&#20027;&#35821;&#35328;&#20195;&#29702;&#30340;&#21508;&#31181;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#25509;&#21475;&#21644;&#23545;&#30740;&#31350;&#20154;&#21592;&#30340;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07870</link><description>&lt;p&gt;
&#33258;&#20027;&#35821;&#35328;&#20195;&#29702;&#30340;&#24320;&#28304;&#26694;&#26550;&#65306;Agents
&lt;/p&gt;
&lt;p&gt;
Agents: An Open-source Framework for Autonomous Language Agents. (arXiv:2309.07870v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07870
&lt;/p&gt;
&lt;p&gt;
Agents&#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#25903;&#25345;&#26500;&#24314;&#33258;&#20027;&#35821;&#35328;&#20195;&#29702;&#30340;&#21508;&#31181;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#25509;&#21475;&#21644;&#23545;&#30740;&#31350;&#20154;&#21592;&#30340;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#39640;&#32423;&#36827;&#23637;&#20351;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#33021;&#22815;&#26500;&#24314;&#33258;&#20027;&#35821;&#35328;&#20195;&#29702;&#65292;&#36825;&#20123;&#20195;&#29702;&#33021;&#22815;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#33258;&#21160;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#24182;&#19982;&#29615;&#22659;&#12289;&#20154;&#31867;&#21644;&#20854;&#20182;&#20195;&#29702;&#20132;&#20114;&#12290;&#25105;&#20204;&#23558;&#35821;&#35328;&#20195;&#29702;&#35270;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#24182;&#21457;&#24067;Agents&#65292;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#26088;&#22312;&#21521;&#26356;&#24191;&#27867;&#30340;&#38750;&#19987;&#19994;&#20154;&#22763;&#24320;&#25918;&#36825;&#20123;&#36827;&#23637;&#12290;Agents&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#65292;&#25903;&#25345;&#37325;&#35201;&#21151;&#33021;&#65292;&#21253;&#25324;&#35268;&#21010;&#12289;&#35760;&#24518;&#12289;&#24037;&#20855;&#20351;&#29992;&#12289;&#22810;&#20195;&#29702;&#36890;&#20449;&#21644;&#32454;&#31890;&#24230;&#30340;&#31526;&#21495;&#25511;&#21046;&#12290;Agents&#29992;&#25143;&#21451;&#22909;&#65292;&#20351;&#38750;&#19987;&#19994;&#20154;&#22763;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#32534;&#20889;&#22826;&#22810;&#20195;&#30721;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#12289;&#23450;&#21046;&#12289;&#27979;&#35797;&#12289;&#35843;&#20248;&#21644;&#37096;&#32626;&#26368;&#20808;&#36827;&#30340;&#33258;&#20027;&#35821;&#35328;&#20195;&#29702;&#12290;&#35813;&#24211;&#20063;&#23545;&#30740;&#31350;&#20154;&#21592;&#21451;&#22909;&#65292;&#20854;&#27169;&#22359;&#21270;&#35774;&#35745;&#20351;&#20854;&#26131;&#20110;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances on large language models (LLMs) enable researchers and developers to build autonomous language agents that can automatically solve various tasks and interact with environments, humans, and other agents using natural language interfaces. We consider language agents as a promising direction towards artificial general intelligence and release Agents, an open-source library with the goal of opening up these advances to a wider non-specialist audience. Agents is carefully engineered to support important features including planning, memory, tool usage, multi-agent communication, and fine-grained symbolic control. Agents is user-friendly as it enables non-specialists to build, customize, test, tune, and deploy state-of-the-art autonomous language agents without much coding. The library is also research-friendly as its modularized design makes it easily extensible for researchers. Agents is available at https://github.com/aiwaves-cn/agents.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#30340;&#23835;&#36215;&#21644;&#28508;&#21147;&#65306;&#19968;&#39033;&#35843;&#26597;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#26500;&#24314;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#28508;&#22312;&#20652;&#21270;&#21058;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#21462;&#24471;&#37325;&#35201;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.07864</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#30340;&#23835;&#36215;&#21644;&#28508;&#21147;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
The Rise and Potential of Large Language Model Based Agents: A Survey. (arXiv:2309.07864v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07864
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#30340;&#23835;&#36215;&#21644;&#28508;&#21147;&#65306;&#19968;&#39033;&#35843;&#26597;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#26500;&#24314;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#28508;&#22312;&#20652;&#21270;&#21058;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#21462;&#24471;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#31867;&#19968;&#30452;&#36861;&#27714;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36798;&#21040;&#25110;&#36229;&#36234;&#20154;&#31867;&#27700;&#24179;&#30340;&#30446;&#26631;&#65292;&#32780;&#34987;&#35748;&#20026;&#26159;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#26377;&#26395;&#26041;&#24335;&#30340;AI&#20195;&#29702;&#12290;AI&#20195;&#29702;&#26159;&#33021;&#24863;&#30693;&#29615;&#22659;&#12289;&#20570;&#20986;&#20915;&#31574;&#21644;&#37319;&#21462;&#34892;&#21160;&#30340;&#20154;&#24037;&#23454;&#20307;&#12290;&#33258;20&#19990;&#32426;&#20013;&#21494;&#20197;&#26469;&#65292;&#20154;&#20204;&#20026;&#24320;&#21457;&#26234;&#33021;AI&#20195;&#29702;&#36827;&#34892;&#20102;&#35768;&#22810;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#31639;&#27861;&#25110;&#35757;&#32451;&#31574;&#30053;&#30340;&#36827;&#27493;&#19978;&#65292;&#20197;&#22686;&#24378;&#29305;&#23450;&#33021;&#21147;&#25110;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#23454;&#38469;&#19978;&#65292;&#31038;&#21306;&#25152;&#32570;&#20047;&#30340;&#26159;&#19968;&#20010;&#36275;&#22815;&#36890;&#29992;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#20316;&#20026;&#35774;&#35745;&#33021;&#36866;&#24212;&#21508;&#31181;&#22330;&#26223;&#30340;AI&#20195;&#29702;&#30340;&#36215;&#28857;&#12290;&#30001;&#20110;&#23637;&#31034;&#20986;&#30340;&#22810;&#21151;&#33021;&#21644;&#26174;&#33879;&#33021;&#21147;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#35270;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#28508;&#22312;&#20652;&#21270;&#21058;&#65292;&#20026;&#26500;&#24314;&#36890;&#29992;AI&#20195;&#29702;&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;&#35768;&#22810;&#30740;&#31350;&#24037;&#20316;&#21033;&#29992;LLMs&#20316;&#20026;&#26500;&#24314;AI&#20195;&#29702;&#30340;&#22522;&#30784;&#65292;&#24182;&#19988;&#24050;&#32463;&#21462;&#24471;&#37325;&#35201;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent AI agents since the mid-20th century. However, these efforts have mainly focused on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a sufficiently general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile and remarkable capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many research efforts have leveraged LLMs as the foundation to build AI agents and ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;CiwaGAN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#26080;&#30417;&#30563;&#22768;&#38901;&#23398;&#24314;&#27169;&#21644;&#26080;&#30417;&#30563;&#21548;&#35273;&#27169;&#24577;&#20449;&#24687;&#20132;&#27969;&#65292;&#26159;&#23545;&#20154;&#31867;&#21475;&#35821;&#20064;&#24471;&#26368;&#29616;&#23454;&#30340;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2309.07861</link><description>&lt;p&gt;
CiwaGAN: &#22768;&#38901;&#23398;&#20449;&#24687;&#20132;&#27969;
&lt;/p&gt;
&lt;p&gt;
CiwaGAN: Articulatory information exchange. (arXiv:2309.07861v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CiwaGAN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#26080;&#30417;&#30563;&#22768;&#38901;&#23398;&#24314;&#27169;&#21644;&#26080;&#30417;&#30563;&#21548;&#35273;&#27169;&#24577;&#20449;&#24687;&#20132;&#27969;&#65292;&#26159;&#23545;&#20154;&#31867;&#21475;&#35821;&#20064;&#24471;&#26368;&#29616;&#23454;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#36807;&#25511;&#21046;&#21457;&#38899;&#22120;&#23448;&#23558;&#20449;&#24687;&#32534;&#30721;&#25104;&#22768;&#38899;&#65292;&#24182;&#36890;&#36807;&#21548;&#35273;&#35013;&#32622;&#35299;&#30721;&#22768;&#38899;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CiwaGAN&#65292;&#36825;&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#26080;&#30417;&#30563;&#22768;&#38901;&#23398;&#24314;&#27169;&#21644;&#26080;&#30417;&#30563;&#21548;&#35273;&#27169;&#24577;&#20449;&#24687;&#20132;&#27969;&#30340;&#20154;&#31867;&#21475;&#35821;&#20064;&#24471;&#27169;&#22411;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#30740;&#31350;&#20998;&#21035;&#21253;&#25324;&#20102;&#26080;&#30417;&#30563;&#22768;&#38901;&#23398;&#24314;&#27169;&#21644;&#20449;&#24687;&#20132;&#27969;&#65292;&#20294;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#31532;&#19968;&#20010;&#23558;&#36825;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#32467;&#21512;&#22312;&#19968;&#36215;&#30340;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#22768;&#38901;&#23398;&#27169;&#22411;&#65292;&#20855;&#26377;&#26356;&#21487;&#35299;&#37322;&#30340;&#20869;&#37096;&#34920;&#31034;&#12290;&#25552;&#20986;&#30340;CiwaGAN&#27169;&#22411;&#26159;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;&#20154;&#31867;&#21475;&#35821;&#20064;&#24471;&#36827;&#34892;&#26368;&#29616;&#23454;&#30340;&#36817;&#20284;&#12290;&#22240;&#27492;&#65292;&#23427;&#23545;&#20110;&#35748;&#30693;&#19978;&#21487;&#34892;&#30340;&#20154;&#31867;&#35328;&#35821;&#34892;&#20026;&#27169;&#25311;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans encode information into sounds by controlling articulators and decode information from sounds using the auditory apparatus. This paper introduces CiwaGAN, a model of human spoken language acquisition that combines unsupervised articulatory modeling with an unsupervised model of information exchange through the auditory modality. While prior research includes unsupervised articulatory modeling and information exchange separately, our model is the first to combine the two components. The paper also proposes an improved articulatory model with more interpretable internal representations. The proposed CiwaGAN model is the most realistic approximation of human spoken language acquisition using deep learning. As such, it is useful for cognitively plausible simulations of the human speech act.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;ExpertQA&#65292;&#23427;&#26159;&#19968;&#20010;&#19987;&#23478;&#31574;&#21010;&#30340;&#38382;&#39064;&#21644;&#24102;&#26377;&#23646;&#24615;&#30340;&#31572;&#26696;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#22312;&#39046;&#22495;&#29305;&#23450;&#24773;&#26223;&#20013;&#25552;&#20379;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#24402;&#22240;&#31561;&#26041;&#38754;&#26469;&#30830;&#20445;&#25552;&#20379;&#20934;&#30830;&#30340;&#20449;&#24687;&#12290;&#30740;&#31350;&#36824;&#25910;&#38598;&#20102;&#39046;&#22495;&#19987;&#23478;&#30340;&#38382;&#39064;&#24182;&#35201;&#27714;&#20182;&#20204;&#35780;&#20272;&#29983;&#25104;&#30340;&#31572;&#26696;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#30446;&#30340;&#26159;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#19981;&#20250;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#65292;&#20174;&#32780;&#36991;&#20813;&#19981;&#33391;&#30340;&#31038;&#20250;&#21518;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.07852</link><description>&lt;p&gt;
ExpertQA: &#19987;&#23478;&#31574;&#21010;&#30340;&#38382;&#39064;&#21644;&#24102;&#26377;&#23646;&#24615;&#30340;&#31572;&#26696;
&lt;/p&gt;
&lt;p&gt;
ExpertQA: Expert-Curated Questions and Attributed Answers. (arXiv:2309.07852v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;ExpertQA&#65292;&#23427;&#26159;&#19968;&#20010;&#19987;&#23478;&#31574;&#21010;&#30340;&#38382;&#39064;&#21644;&#24102;&#26377;&#23646;&#24615;&#30340;&#31572;&#26696;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#22312;&#39046;&#22495;&#29305;&#23450;&#24773;&#26223;&#20013;&#25552;&#20379;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#24402;&#22240;&#31561;&#26041;&#38754;&#26469;&#30830;&#20445;&#25552;&#20379;&#20934;&#30830;&#30340;&#20449;&#24687;&#12290;&#30740;&#31350;&#36824;&#25910;&#38598;&#20102;&#39046;&#22495;&#19987;&#23478;&#30340;&#38382;&#39064;&#24182;&#35201;&#27714;&#20182;&#20204;&#35780;&#20272;&#29983;&#25104;&#30340;&#31572;&#26696;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#30446;&#30340;&#26159;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#19981;&#20250;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#65292;&#20174;&#32780;&#36991;&#20813;&#19981;&#33391;&#30340;&#31038;&#20250;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#34987;&#36234;&#26469;&#36234;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#30340;&#29992;&#25143;&#25152;&#37319;&#29992;&#65292;&#30830;&#20445;&#23427;&#20204;&#25552;&#20379;&#22522;&#20110;&#21487;&#39564;&#35777;&#26469;&#28304;&#30340;&#20107;&#23454;&#20934;&#30830;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#32844;&#19994;&#20013;&#37117;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36825;&#29305;&#21035;&#36866;&#29992;&#20110;&#21307;&#23398;&#21644;&#27861;&#24459;&#31561;&#39640;&#39118;&#38505;&#39046;&#22495;&#65292;&#22240;&#20026;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#30340;&#39118;&#38505;&#36739;&#39640;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#33391;&#30340;&#31038;&#20250;&#21518;&#26524;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#20107;&#23454;&#24615;&#21644;&#24402;&#22240;&#26041;&#38754;&#65292;&#24182;&#26410;&#19987;&#27880;&#20110;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#24773;&#26223;&#20013;&#30340;&#36825;&#20123;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#39046;&#22495;&#19987;&#23478;&#32435;&#20837;&#20854;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#30740;&#31350;&#65292;&#20998;&#26512;&#26469;&#33258;&#20960;&#20010;&#31995;&#32479;&#30340;&#21709;&#24212;&#20013;&#25552;&#20379;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#24402;&#22240;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20808;&#20174;32&#20010;&#23398;&#31185;&#39046;&#22495;&#30340;484&#21517;&#21442;&#19982;&#32773;&#20013;&#25910;&#38598;&#30001;&#19987;&#23478;&#31574;&#21010;&#30340;&#38382;&#39064;&#65292;&#28982;&#21518;&#35201;&#27714;&#36825;&#20123;&#19987;&#23478;&#35780;&#20272;&#23545;&#20182;&#20204;&#33258;&#24049;&#38382;&#39064;&#30340;&#20135;&#29983;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#36824;&#35201;&#27714;&#19987;&#23478;&#20462;&#25913;&#20135;&#29983;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models are adapted by a more sophisticated and diverse set of users, the importance of guaranteeing that they provide factually correct information supported by verifiable sources is critical across fields of study &amp; professions. This is especially the case for high-stakes fields, such as medicine and law, where the risk of propagating false information is high and can lead to undesirable societal consequences. Previous work studying factuality and attribution has not focused on analyzing these characteristics of language model outputs in domain-specific scenarios. In this work, we present an evaluation study analyzing various axes of factuality and attribution provided in responses from a few systems, by bringing domain experts in the loop. Specifically, we first collect expert-curated questions from 484 participants across 32 fields of study, and then ask the same experts to evaluate generated responses to their own questions. We also ask experts to revise answers produce
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#22686;&#21152;&#33258;&#21160;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#23454;&#20363;&#65292;&#25552;&#39640;&#20102;&#25688;&#35201;&#38382;&#31572;&#27169;&#22411;&#22312;&#39046;&#22495;&#22806;&#30340;&#24615;&#33021;&#21644;&#27169;&#22411;&#26657;&#20934;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#24615;&#33021;&#25913;&#36827;&#19982;&#21453;&#20107;&#23454;&#23454;&#20363;&#30340;&#22810;&#26679;&#24615;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2309.07822</link><description>&lt;p&gt;
CATfOOD&#65306;&#21453;&#20107;&#23454;&#22686;&#24378;&#35757;&#32451;&#20197;&#25552;&#39640;&#39046;&#22495;&#22806;&#24615;&#33021;&#21644;&#26657;&#20934;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
CATfOOD: Counterfactual Augmented Training for Improving Out-of-Domain Performance and Calibration. (arXiv:2309.07822v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#22686;&#21152;&#33258;&#21160;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#23454;&#20363;&#65292;&#25552;&#39640;&#20102;&#25688;&#35201;&#38382;&#31572;&#27169;&#22411;&#22312;&#39046;&#22495;&#22806;&#30340;&#24615;&#33021;&#21644;&#27169;&#22411;&#26657;&#20934;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#24615;&#33021;&#25913;&#36827;&#19982;&#21453;&#20107;&#23454;&#23454;&#20363;&#30340;&#22810;&#26679;&#24615;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#20960;&#24180;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35268;&#27169;&#26041;&#38754;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#32473;&#23450;&#25552;&#31034;&#30340;&#26465;&#20214;&#19979;&#29983;&#25104;&#25991;&#26412;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;LLM&#26469;&#22686;&#24378;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLM&#65289;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#65288;CF&#65289;&#23454;&#20363;&#65288;&#21363;&#26368;&#23567;&#31243;&#24230;&#30340;&#25913;&#21464;&#36755;&#20837;&#65289;&#65292;&#20197;&#25552;&#39640;SLM&#22312;&#25688;&#35201;&#38382;&#31572;&#65288;QA&#65289;&#35774;&#32622;&#19979;&#30340;&#39046;&#22495;&#22806;&#65288;OOD&#65289;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#21508;&#31181;LLM&#29983;&#25104;&#22120;&#20013;&#65292;&#36825;&#31181;&#25968;&#25454;&#22686;&#24378;&#22987;&#32456;&#33021;&#22815;&#25552;&#39640;OOD&#24615;&#33021;&#65292;&#24182;&#25913;&#36827;&#20102;&#22522;&#20110;&#32622;&#20449;&#24230;&#21644;&#22522;&#20110;&#29702;&#24615;&#22686;&#24378;&#30340;&#26657;&#20934;&#27169;&#22411;&#30340;&#27169;&#22411;&#26657;&#20934;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#24615;&#33021;&#25552;&#21319;&#19982;CF&#23454;&#20363;&#22312;&#22806;&#35266;&#24418;&#24335;&#21644;&#35821;&#20041;&#20869;&#23481;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#21576;&#27491;&#30456;&#20851;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26657;&#20934;&#26356;&#23481;&#26131;&#30340;CF&#22686;&#24378;&#27169;&#22411;&#22312;&#20998;&#37197;&#37325;&#35201;&#24615;&#26102;&#30340;&#29109;&#20063;&#36739;&#20302;&#65292;&#36825;&#34920;&#26126;&#29702;&#24615;&#22686;&#24378;&#30340;&#26657;&#20934;&#22120;&#26356;&#20559;&#22909;&#31616;&#27905;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, large language models (LLMs) have shown remarkable capabilities at scale, particularly at generating text conditioned on a prompt. In our work, we investigate the use of LLMs to augment training data of small language models~(SLMs) with automatically generated counterfactual~(CF) instances -- i.e. minimally altered inputs -- in order to improve out-of-domain~(OOD) performance of SLMs in the extractive question answering~(QA) setup. We show that, across various LLM generators, such data augmentation consistently enhances OOD performance and improves model calibration for both confidence-based and rationale-augmented calibrator models. Furthermore, these performance improvements correlate with higher diversity of CF instances in terms of their surface form and semantic content. Finally, we show that CF augmented models which are easier to calibrate also exhibit much lower entropy when assigning importance, indicating that rationale-augmented calibrators prefer concise ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30284;&#30151;&#20020;&#24202;&#35797;&#39564;&#20013;&#24120;&#35265;&#30340;&#25490;&#38500;&#26631;&#20934;&#65292;&#36890;&#36807;&#24212;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#33258;&#21160;&#20998;&#31867;&#25490;&#38500;&#26631;&#20934;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#19987;&#38376;&#20026;&#20020;&#24202;&#35797;&#39564;&#35774;&#35745;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.07812</link><description>&lt;p&gt;
&#30284;&#30151;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#26631;&#20934;&#30340;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text Classification of Cancer Clinical Trial Eligibility Criteria. (arXiv:2309.07812v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30284;&#30151;&#20020;&#24202;&#35797;&#39564;&#20013;&#24120;&#35265;&#30340;&#25490;&#38500;&#26631;&#20934;&#65292;&#36890;&#36807;&#24212;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#33258;&#21160;&#20998;&#31867;&#25490;&#38500;&#26631;&#20934;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#19987;&#38376;&#20026;&#20020;&#24202;&#35797;&#39564;&#35774;&#35745;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35797;&#39564;&#36164;&#26684;&#26631;&#20934;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#38472;&#36848;&#65292;&#22240;&#27492;&#33258;&#21160;&#30830;&#23450;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#35797;&#39564;&#36164;&#26684;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#19968;&#20010;&#28508;&#22312;&#26041;&#27861;&#26159;&#20351;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#23545;&#24120;&#35265;&#31867;&#22411;&#30340;&#36164;&#26684;&#26631;&#20934;&#36827;&#34892;&#22788;&#29702;&#12290;&#26412;&#30740;&#31350;&#20851;&#27880;&#30284;&#30151;&#35797;&#39564;&#20013;&#30340;&#19971;&#20010;&#24120;&#35265;&#25490;&#38500;&#26631;&#20934;&#65306;&#20808;&#21069;&#24694;&#24615;&#32959;&#30244;&#12289;&#20154;&#31867;&#20813;&#30123;&#32570;&#38519;&#30149;&#27602;&#12289;&#20057;&#32925;&#30149;&#27602;&#12289;&#19993;&#32925;&#30149;&#27602;&#12289;&#31934;&#31070;&#30142;&#30149;&#12289;&#33647;&#29289;/&#29289;&#36136;&#28389;&#29992;&#21644;&#33258;&#36523;&#20813;&#30123;&#30142;&#30149;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;764&#20010;&#24102;&#26377;&#36825;&#20123;&#25490;&#38500;&#26631;&#20934;&#27880;&#37322;&#30340;&#19977;&#26399;&#30284;&#30151;&#35797;&#39564;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#24120;&#35265;&#30340;transformer&#27169;&#22411;&#20197;&#21450;&#19968;&#20010;&#26032;&#30340;&#39044;&#35757;&#32451;&#30340;&#20020;&#24202;&#35797;&#39564;BERT&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#33258;&#21160;&#20998;&#31867;&#24120;&#35265;&#30340;&#25490;&#38500;&#26631;&#20934;&#26159;&#21487;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#20020;&#24202;&#35797;&#39564;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#65292;&#35813;&#27169;&#22411;&#22312;&#25152;&#26377;&#26631;&#20934;&#20013;&#34920;&#29616;&#20986;&#26368;&#39640;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic identification of clinical trials for which a patient is eligible is complicated by the fact that trial eligibility is stated in natural language. A potential solution to this problem is to employ text classification methods for common types of eligibility criteria. In this study, we focus on seven common exclusion criteria in cancer trials: prior malignancy, human immunodeficiency virus, hepatitis B, hepatitis C, psychiatric illness, drug/substance abuse, and autoimmune illness. Our dataset consists of 764 phase III cancer trials with these exclusions annotated at the trial level. We experiment with common transformer models as well as a new pre-trained clinical trial BERT model. Our results demonstrate the feasibility of automatically classifying common exclusion criteria. Additionally, we demonstrate the value of a pre-trained language model specifically for clinical trials, which yields the highest average performance across all criteria.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#20195;&#30721;&#27169;&#22411;&#22312;&#29702;&#35299;API&#21517;&#31216;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#30693;&#35782;&#25506;&#27979;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#20195;&#30721;&#27169;&#22411;&#22312;&#29702;&#35299;API&#30340;&#23436;&#20840;&#38480;&#23450;&#21517;&#26041;&#38754;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2309.07804</link><description>&lt;p&gt;
&#24377;&#20986;&#27979;&#39564;&#65281;&#39044;&#35757;&#32451;&#20195;&#30721;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#27491;&#30830;API&#21517;&#31216;&#30340;&#30693;&#35782;&#65311;
&lt;/p&gt;
&lt;p&gt;
Pop Quiz! Do Pre-trained Code Models Possess Knowledge of Correct API Names?. (arXiv:2309.07804v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07804
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#20195;&#30721;&#27169;&#22411;&#22312;&#29702;&#35299;API&#21517;&#31216;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#30693;&#35782;&#25506;&#27979;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#20195;&#30721;&#27169;&#22411;&#22312;&#29702;&#35299;API&#30340;&#23436;&#20840;&#38480;&#23450;&#21517;&#26041;&#38754;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#39044;&#35757;&#32451;&#20195;&#30721;&#27169;&#22411;&#65288;&#22914;CodeBERT&#21644;Codex&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#36825;&#20123;&#20195;&#30721;&#27169;&#22411;&#23545;&#20110;&#23454;&#29616;&#26399;&#26395;&#30340;&#31243;&#24207;&#21151;&#33021;&#30340;API&#30340;&#27491;&#30830;&#24615;&#21644;&#26126;&#30830;&#24615;&#38750;&#24120;&#20851;&#38190;&#65292;&#35201;&#27714;&#23427;&#20204;&#22312;&#32467;&#26500;&#21644;&#35821;&#20041;&#19978;&#23398;&#20064;&#21508;&#31181;API&#30340;&#23436;&#20840;&#38480;&#23450;&#21517;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#20195;&#30721;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#36807;&#31243;&#20013;&#20063;&#24456;&#38590;&#27491;&#30830;&#24314;&#35758;API&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;API&#20351;&#29992;&#24615;&#33021;&#24046;&#30340;&#21407;&#22240;&#20960;&#20046;&#27809;&#26377;&#36827;&#34892;&#30740;&#31350;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#30693;&#35782;&#25506;&#27979;&#20316;&#20026;&#35299;&#37322;&#20195;&#30721;&#27169;&#22411;&#30340;&#25163;&#27573;&#65292;&#36890;&#36807;&#22635;&#31354;&#24335;&#27979;&#35797;&#26469;&#34913;&#37327;&#27169;&#22411;&#20013;&#23384;&#20648;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#20840;&#38754;&#22320;&#30740;&#31350;&#20102;&#20195;&#30721;&#27169;&#22411;&#29702;&#35299;API&#23436;&#20840;&#38480;&#23450;&#21517;&#30340;&#33021;&#21147;&#65292;&#20174;API&#35843;&#29992;&#21644;API&#23548;&#20837;&#20004;&#20010;&#19981;&#21516;&#30340;&#35282;&#24230;&#26469;&#36827;&#34892;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#20195;&#30721;&#27169;&#22411;&#22312;&#29702;&#35299;API&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent breakthroughs in pre-trained code models, such as CodeBERT and Codex, have shown their superior performance in various downstream tasks. The correctness and unambiguity of API usage among these code models are crucial for achieving desirable program functionalities, requiring them to learn various API fully qualified names structurally and semantically. Recent studies reveal that even state-of-the-art pre-trained code models struggle with suggesting the correct APIs during code generation. However, the reasons for such poor API usage performance are barely investigated. To address this challenge, we propose using knowledge probing as a means of interpreting code models, which uses cloze-style tests to measure the knowledge stored in models. Our comprehensive study examines a code model's capability of understanding API fully qualified names from two different perspectives: API call and API import. Specifically, we reveal that current code models struggle with understanding API n
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#35762;&#25925;&#20107;&#30340;&#24320;&#22836;&#37096;&#20998;&#36890;&#24120;&#20250;&#36981;&#24490;&#21160;&#20316;&#21407;&#21017;&#65292;&#24182;&#19988;&#36825;&#31181;&#39034;&#24207;&#21487;&#33021;&#19982;&#35199;&#26041;&#20256;&#32479;&#35762;&#25925;&#20107;&#30340;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2309.07797</link><description>&lt;p&gt;
&#35762;&#25925;&#20107;&#30340;&#21160;&#24577;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
The Dynamical Principles of Storytelling. (arXiv:2309.07797v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07797
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#35762;&#25925;&#20107;&#30340;&#24320;&#22836;&#37096;&#20998;&#36890;&#24120;&#20250;&#36981;&#24490;&#21160;&#20316;&#21407;&#21017;&#65292;&#24182;&#19988;&#36825;&#31181;&#39034;&#24207;&#21487;&#33021;&#19982;&#35199;&#26041;&#20256;&#32479;&#35762;&#25925;&#20107;&#30340;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32771;&#34385;&#20102;1800&#20010;&#30701;&#31687;&#23567;&#35828;&#30340;&#24320;&#22836;&#37096;&#20998;&#20043;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#24179;&#22343;&#25925;&#20107;&#30340;&#21069;&#21313;&#20960;&#20010;&#27573;&#33853;&#36981;&#24490;&#20102;arXiv:2309.06600&#20013;&#23450;&#20041;&#30340;&#21160;&#20316;&#21407;&#21017;&#12290;&#24403;&#27573;&#33853;&#30340;&#39034;&#24207;&#34987;&#25171;&#20081;&#26102;&#65292;&#24179;&#22343;&#20540;&#19981;&#20877;&#20855;&#22791;&#36825;&#19968;&#29305;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24320;&#22987;&#19968;&#20010;&#25925;&#20107;&#26102;&#65292;&#25105;&#20204;&#22312;&#35821;&#20041;&#31354;&#38388;&#20013;&#26377;&#19968;&#20010;&#20248;&#20808;&#26041;&#21521;&#65292;&#21487;&#33021;&#19982;&#20122;&#37324;&#22763;&#22810;&#24503;&#22312;&#12298;&#35799;&#23398;&#12299;&#20013;&#25152;&#26263;&#31034;&#30340;&#35199;&#26041;&#20256;&#32479;&#35762;&#25925;&#20107;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
When considering the opening part of 1800 short stories, we find that the first dozen paragraphs of the average narrative follow an action principle as defined in arXiv:2309.06600. When the order of the paragraphs is shuffled, the average no longer exhibits this property. The findings show that there is a preferential direction we take in semantic space when starting a story, possibly related to a common Western storytelling tradition as implied by Aristotle in Poetics.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22270;&#20687;-&#25991;&#26412;&#36741;&#21161;&#20219;&#21153;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#65292;&#36890;&#36807;&#20004;&#20010;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#23545;&#22270;&#20687;-&#25991;&#26412;&#34920;&#31034;&#36827;&#34892;&#35843;&#25972;&#65292;&#25429;&#25417;&#24213;&#23618;&#20381;&#36182;&#20851;&#31995;&#21644;&#35821;&#20041;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.07794</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22270;&#20687;-&#25991;&#26412;&#36741;&#21161;&#20219;&#21153;&#25552;&#39640;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Improving Multimodal Classification of Social Media Posts by Leveraging Image-Text Auxiliary tasks. (arXiv:2309.07794v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22270;&#20687;-&#25991;&#26412;&#36741;&#21161;&#20219;&#21153;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#65292;&#36890;&#36807;&#20004;&#20010;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#23545;&#22270;&#20687;-&#25991;&#26412;&#34920;&#31034;&#36827;&#34892;&#35843;&#25972;&#65292;&#25429;&#25417;&#24213;&#23618;&#20381;&#36182;&#20851;&#31995;&#21644;&#35821;&#20041;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#23545;&#24773;&#24863;&#20998;&#26512;&#12289;&#35773;&#21050;&#26816;&#27979;&#21644;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#31561;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21305;&#37197;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#23384;&#22312;&#38544;&#34255;&#25110;&#20114;&#34917;&#20449;&#24687;&#30340;&#29420;&#29305;&#36328;&#27169;&#24577;&#35821;&#20041;&#65292;&#23558;&#25991;&#26412;&#21644;&#22270;&#20687;&#20449;&#24687;&#32467;&#21512;&#36215;&#26469;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#22312;&#24494;&#35843;&#20219;&#20309;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#26102;&#32852;&#21512;&#20351;&#29992;&#20004;&#20010;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#26469;&#30452;&#25509;&#24314;&#27169;&#36825;&#19968;&#38382;&#39064;&#12290;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#65288;ITC&#65289;&#23558;&#19968;&#31687;&#24086;&#23376;&#30340;&#22270;&#20687;-&#25991;&#26412;&#34920;&#31034;&#26356;&#21152;&#38752;&#36817;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#24086;&#23376;&#20998;&#31163;&#24320;&#26469;&#65292;&#25429;&#25417;&#24213;&#23618;&#20381;&#36182;&#20851;&#31995;&#12290;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#65288;ITM&#65289;&#36890;&#36807;&#24809;&#32602;&#19981;&#30456;&#20851;&#30340;&#23545;&#26469;&#20419;&#36827;&#29702;&#35299;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#23545;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#30446;&#26631;&#19982;&#20116;&#20010;&#22810;&#27169;&#24577;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#35777;&#26126;&#20102;&#22312;&#22235;&#20010;&#28909;&#38376;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#38598;&#19978;&#30340;&#19968;&#33268;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effectively leveraging multimodal information from social media posts is essential to various downstream tasks such as sentiment analysis, sarcasm detection and hate speech classification. However, combining text and image information is challenging because of the idiosyncratic cross-modal semantics with hidden or complementary information present in matching image-text pairs. In this work, we aim to directly model this by proposing the use of two auxiliary losses jointly with the main task when fine-tuning any pre-trained multimodal model. Image-Text Contrastive (ITC) brings image-text representations of a post closer together and separates them from different posts, capturing underlying dependencies. Image-Text Matching (ITM) facilitates the understanding of semantic correspondence between images and text by penalizing unrelated pairs. We combine these objectives with five multimodal models, demonstrating consistent improvements across four popular social media datasets. Furthermore,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#35843;&#26597;&#35780;&#20272;&#20102;&#31227;&#21160;&#20005;&#32899;&#28216;&#25103;&#24212;&#29992;&#20013;&#21475;&#35821;&#21270;&#20154;&#24418;&#26426;&#22120;&#20154;&#23545;&#21487;&#29992;&#24615;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#29992;&#25143;&#26356;&#21916;&#27426;&#19982;&#39640;&#20154;&#31867;&#30456;&#20284;&#24230;&#30340;&#26426;&#22120;&#20154;&#36827;&#34892;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2309.07773</link><description>&lt;p&gt;
&#31227;&#21160;&#20005;&#32899;&#28216;&#25103;&#20013;&#21475;&#35821;&#21270;&#20154;&#24418;&#26426;&#22120;&#20154;&#23545;&#21487;&#29992;&#24615;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Usability Evaluation of Spoken Humanoid Embodied Conversational Agents in Mobile Serious Games. (arXiv:2309.07773v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#35843;&#26597;&#35780;&#20272;&#20102;&#31227;&#21160;&#20005;&#32899;&#28216;&#25103;&#24212;&#29992;&#20013;&#21475;&#35821;&#21270;&#20154;&#24418;&#26426;&#22120;&#20154;&#23545;&#21487;&#29992;&#24615;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#29992;&#25143;&#26356;&#21916;&#27426;&#19982;&#39640;&#20154;&#31867;&#30456;&#20284;&#24230;&#30340;&#26426;&#22120;&#20154;&#36827;&#34892;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#31227;&#21160;&#20005;&#32899;&#28216;&#25103;&#24212;&#29992;&#20013;&#21475;&#35821;&#21270;&#20154;&#24418;&#26426;&#22120;&#20154;&#65288;HECA&#65289;&#23545;&#21487;&#29992;&#24615;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#23454;&#35777;&#35843;&#26597;&#12290;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#22810;&#20010;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#20132;&#20114;&#24187;&#35273;&#23545;&#20132;&#20114;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#30740;&#31350;&#20102;&#20004;&#31181;&#26426;&#22120;&#20154;&#21576;&#29616;&#26041;&#24335;&#65306;&#39640;&#20154;&#31867;&#30456;&#20284;&#24230;&#30340;&#26426;&#22120;&#20154;&#65288;HECA&#65289;&#21644;&#20302;&#20154;&#31867;&#30456;&#20284;&#24230;&#30340;&#26426;&#22120;&#20154;&#65288;&#25991;&#26412;&#65289;&#12290;&#23454;&#39564;&#30340;&#30446;&#30340;&#26159;&#35780;&#20272;&#39640;&#20154;&#31867;&#30456;&#20284;&#24230;&#26426;&#22120;&#20154;&#26159;&#21542;&#33021;&#22815;&#24341;&#21457;&#20154;&#31867;&#24187;&#35273;&#24182;&#24433;&#21709;&#21487;&#29992;&#24615;&#12290;&#39640;&#20154;&#31867;&#30456;&#20284;&#24230;&#26426;&#22120;&#20154;&#26159;&#26681;&#25454;ECA&#35774;&#35745;&#27169;&#22411;&#36827;&#34892;&#35774;&#35745;&#30340;&#65292;&#35813;&#27169;&#22411;&#26159;&#19968;&#31181;ECA&#24320;&#21457;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;90&#20301;&#21442;&#19982;&#32773;&#26356;&#21916;&#27426;&#19982;HECA&#36827;&#34892;&#20132;&#20114;&#12290;&#20004;&#20010;&#29256;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#22312;&#32479;&#35745;&#23398;&#19978;&#20855;&#26377;&#26174;&#33879;&#24615;&#65292;&#25928;&#24212;&#22823;&#23567;&#36739;&#22823;&#65288;d=1.01&#65289;&#65292;&#35768;&#22810;&#21442;&#19982;&#32773;&#36890;&#36807;&#35299;&#37322;&#36873;&#25321;&#26469;&#35777;&#26126;&#20182;&#20204;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an empirical investigation of the extent to which spoken Humanoid Embodied Conversational Agents (HECAs) can foster usability in mobile serious game (MSG) applications. The aim of the research is to assess the impact of multiple agents and illusion of humanness on the quality of the interaction. The experiment investigates two styles of agent presentation: an agent of high human-likeness (HECA) and an agent of low human-likeness (text). The purpose of the experiment is to assess whether and how agents of high humanlikeness can evoke the illusion of humanness and affect usability. Agents of high human-likeness were designed by following the ECA design model that is a proposed guide for ECA development. The results of the experiment with 90 participants show that users prefer to interact with the HECAs. The difference between the two versions is statistically significant with a large effect size (d=1.01), with many of the participants justifying their choice by saying
&lt;/p&gt;</description></item><item><title>Echotune&#26159;&#19968;&#20010;&#27169;&#22359;&#21270;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#21033;&#29992;&#35821;&#38899;&#30340;&#21487;&#21464;&#38271;&#24230;&#29305;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;Echo-MSA&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#20174;&#24103;&#21040;&#35805;&#35821;&#30340;&#21508;&#31181;&#39063;&#31890;&#24230;&#30340;&#35821;&#38899;&#29305;&#24449;&#25552;&#21462;&#65292;&#35299;&#20915;&#20102;&#22266;&#23450;&#38271;&#24230;&#27880;&#24847;&#21147;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07765</link><description>&lt;p&gt;
Echotune: &#21033;&#29992;&#35821;&#38899;&#30340;&#21487;&#21464;&#38271;&#24230;&#29305;&#24615;&#30340;&#27169;&#22359;&#21270;&#29305;&#24449;&#25552;&#21462;&#22120;&#22312;ASR&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Echotune: A Modular Extractor Leveraging the Variable-Length Nature of Speech in ASR Tasks. (arXiv:2309.07765v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07765
&lt;/p&gt;
&lt;p&gt;
Echotune&#26159;&#19968;&#20010;&#27169;&#22359;&#21270;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#21033;&#29992;&#35821;&#38899;&#30340;&#21487;&#21464;&#38271;&#24230;&#29305;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;Echo-MSA&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#20174;&#24103;&#21040;&#35805;&#35821;&#30340;&#21508;&#31181;&#39063;&#31890;&#24230;&#30340;&#35821;&#38899;&#29305;&#24449;&#25552;&#21462;&#65292;&#35299;&#20915;&#20102;&#22266;&#23450;&#38271;&#24230;&#27880;&#24847;&#21147;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#24050;&#34987;&#35777;&#26126;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20219;&#21153;&#20013;&#38750;&#24120;&#26377;&#25928;&#65292;&#25104;&#20026;&#35813;&#39046;&#22495;&#20247;&#22810;&#30740;&#31350;&#30340;&#22522;&#30784;&#32452;&#20214;&#12290;&#21382;&#21490;&#19978;&#65292;&#35768;&#22810;&#26041;&#27861;&#20381;&#36182;&#20110;&#22266;&#23450;&#38271;&#24230;&#30340;&#27880;&#24847;&#21147;&#31383;&#21475;&#65292;&#36825;&#23545;&#20110;&#25345;&#32493;&#26102;&#38388;&#21644;&#22797;&#26434;&#24615;&#19981;&#21516;&#30340;&#35821;&#38899;&#26679;&#26412;&#26469;&#35828;&#26159;&#26377;&#38382;&#39064;&#30340;&#65292;&#23548;&#33268;&#25968;&#25454;&#36807;&#24230;&#24179;&#28369;&#21270;&#21644;&#24573;&#35270;&#20102;&#38271;&#26399;&#36830;&#36890;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Echo-MSA&#65292;&#19968;&#20010;&#20855;&#26377;&#21487;&#21464;&#38271;&#24230;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28789;&#27963;&#27169;&#22359;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#22797;&#26434;&#24615;&#21644;&#25345;&#32493;&#26102;&#38388;&#30340;&#35821;&#38899;&#26679;&#26412;&#12290;&#35813;&#27169;&#22359;&#25552;&#20379;&#20102;&#20174;&#24103;&#21644;&#38899;&#32032;&#21040;&#21333;&#35789;&#21644;&#35805;&#35821;&#30340;&#21508;&#31181;&#39063;&#31890;&#24230;&#30340;&#35821;&#38899;&#29305;&#24449;&#25552;&#21462;&#30340;&#28789;&#27963;&#24615;&#12290;&#25552;&#20986;&#30340;&#35774;&#35745;&#25429;&#25417;&#21040;&#20102;&#35821;&#38899;&#30340;&#21487;&#21464;&#38271;&#24230;&#29305;&#24449;&#65292;&#24182;&#35299;&#20915;&#20102;&#22266;&#23450;&#38271;&#24230;&#27880;&#24847;&#21147;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#21033;&#29992;&#20102;&#19968;&#20010;&#24179;&#34892;&#30340;&#27880;&#24847;&#21147;&#26550;&#26500;&#65292;&#24182;&#32467;&#21512;&#20102;&#19968;&#20010;&#21160;&#24577;&#38376;&#25511;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Transformer architecture has proven to be highly effective for Automatic Speech Recognition (ASR) tasks, becoming a foundational component for a plethora of research in the domain. Historically, many approaches have leaned on fixed-length attention windows, which becomes problematic for varied speech samples in duration and complexity, leading to data over-smoothing and neglect of essential long-term connectivity. Addressing this limitation, we introduce Echo-MSA, a nimble module equipped with a variable-length attention mechanism that accommodates a range of speech sample complexities and durations. This module offers the flexibility to extract speech features across various granularities, spanning from frames and phonemes to words and discourse. The proposed design captures the variable length feature of speech and addresses the limitations of fixed-length attention. Our evaluation leverages a parallel attention architecture complemented by a dynamic gating mechanism that amalgam
&lt;/p&gt;</description></item><item><title>PROGrasp&#26159;&#19968;&#20010;&#23454;&#29616;&#29289;&#20307;&#25235;&#21462;&#30340;&#20154;&#26426;&#20132;&#27969;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#38754;&#21521;&#24847;&#22270;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#21644;&#31572;&#26696;&#35299;&#37322;&#27169;&#22359;&#65292;&#26426;&#22120;&#20154;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#30340;&#24847;&#22270;&#26469;&#35782;&#21035;&#21644;&#25235;&#21462;&#30446;&#26631;&#29289;&#20307;&#12290;</title><link>http://arxiv.org/abs/2309.07759</link><description>&lt;p&gt;
PROGrasp:&#23454;&#29616;&#29289;&#20307;&#25235;&#21462;&#30340;&#20154;&#26426;&#20132;&#27969;
&lt;/p&gt;
&lt;p&gt;
PROGrasp: Pragmatic Human-Robot Communication for Object Grasping. (arXiv:2309.07759v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07759
&lt;/p&gt;
&lt;p&gt;
PROGrasp&#26159;&#19968;&#20010;&#23454;&#29616;&#29289;&#20307;&#25235;&#21462;&#30340;&#20154;&#26426;&#20132;&#27969;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#38754;&#21521;&#24847;&#22270;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#21644;&#31572;&#26696;&#35299;&#37322;&#27169;&#22359;&#65292;&#26426;&#22120;&#20154;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#30340;&#24847;&#22270;&#26469;&#35782;&#21035;&#21644;&#25235;&#21462;&#30446;&#26631;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#29289;&#20307;&#25235;&#21462;(IOG)&#26159;&#36890;&#36807;&#20154;&#26426;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#35782;&#21035;&#21644;&#25235;&#21462;&#30446;&#26631;&#29289;&#20307;&#30340;&#20219;&#21153;&#12290;&#24403;&#21069;IOG&#31995;&#32479;&#20551;&#23450;&#20154;&#31867;&#29992;&#25143;&#26368;&#21021;&#25351;&#23450;&#30446;&#26631;&#23545;&#35937;&#30340;&#31867;&#21035;(&#20363;&#22914;&#65292;&#29942;&#23376;)&#12290;&#21463;&#21040;&#35821;&#29992;&#23398;&#30340;&#21551;&#21457;&#65292;&#20154;&#31867;&#24448;&#24448;&#36890;&#36807;&#20381;&#36182;&#19978;&#19979;&#25991;&#26469;&#20256;&#36798;&#24847;&#22270;&#20197;&#23454;&#29616;&#30446;&#26631;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;IOG&#20219;&#21153;&#65292;&#21363;&#23454;&#29992;IOG&#65292;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#65292;&#21363;&#38754;&#21521;&#24847;&#22270;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;(IM-Dial)&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#20219;&#21153;&#22330;&#26223;&#20013;&#65292;&#39318;&#20808;&#32473;&#20986;&#19968;&#20010;&#38754;&#21521;&#24847;&#22270;&#30340;&#35805;&#35821;(&#20363;&#22914;&#65292;&#8220;&#25105;&#28212;&#20102;&#8221;)&#12290;&#28982;&#21518;&#65292;&#26426;&#22120;&#20154;&#24212;&#36890;&#36807;&#19982;&#20154;&#31867;&#29992;&#25143;&#20114;&#21160;&#26469;&#35782;&#21035;&#30446;&#26631;&#23545;&#35937;&#12290;&#22522;&#20110;&#20219;&#21153;&#35774;&#32622;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#21487;&#20197;&#35299;&#37322;&#29992;&#25143;&#30340;&#24847;&#22270;&#24182;&#25441;&#36215;&#30446;&#26631;&#23545;&#35937;&#65292;&#21363;&#23454;&#29992;&#29289;&#20307;&#25235;&#21462;(PROGrasp)&#12290;PROGrasp&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#23450;&#20301;&#12289;&#38382;&#39064;&#25552;&#38382;&#12289;&#29289;&#20307;&#25235;&#21462;&#20197;&#21450;&#26368;&#37325;&#35201;&#30340;&#65292;&#31572;&#26696;&#35299;&#37322;&#27169;&#22359;&#25191;&#34892;&#23454;&#29992;IOG&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactive Object Grasping (IOG) is the task of identifying and grasping the desired object via human-robot natural language interaction. Current IOG systems assume that a human user initially specifies the target object's category (e.g., bottle). Inspired by pragmatics, where humans often convey their intentions by relying on context to achieve goals, we introduce a new IOG task, Pragmatic-IOG, and the corresponding dataset, Intention-oriented Multi-modal Dialogue (IM-Dial). In our proposed task scenario, an intention-oriented utterance (e.g., "I am thirsty") is initially given to the robot. The robot should then identify the target object by interacting with a human user. Based on the task setup, we propose a new robotic system that can interpret the user's intention and pick up the target object, Pragmatic Object Grasping (PROGrasp). PROGrasp performs Pragmatic-IOG by incorporating modules for visual grounding, question asking, object grasping, and most importantly, answer interpre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#38598;&#25104;&#30340;LLM&#26041;&#27861;&#36827;&#34892;&#29983;&#25104;&#24335;AI&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#39044;&#35757;&#32451;LLM&#29983;&#25104;&#27010;&#29575;&#20316;&#20026;&#29305;&#24449;&#26469;&#20934;&#30830;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#35821;&#35328;&#65292;&#24182;&#30830;&#23450;&#29983;&#25104;&#25991;&#26412;&#30340;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#30340;&#24402;&#23646;&#12290;</title><link>http://arxiv.org/abs/2309.07755</link><description>&lt;p&gt;
&#21033;&#29992;&#38598;&#25104;&#30340;LLM&#26041;&#27861;&#36827;&#34892;&#29983;&#25104;&#24335;AI&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Generative AI Text Classification using Ensemble LLM Approaches. (arXiv:2309.07755v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#38598;&#25104;&#30340;LLM&#26041;&#27861;&#36827;&#34892;&#29983;&#25104;&#24335;AI&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#39044;&#35757;&#32451;LLM&#29983;&#25104;&#27010;&#29575;&#20316;&#20026;&#29305;&#24449;&#26469;&#20934;&#30830;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#35821;&#35328;&#65292;&#24182;&#30830;&#23450;&#29983;&#25104;&#25991;&#26412;&#30340;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#30340;&#24402;&#23646;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#22914;&#20869;&#23481;&#21019;&#20316;&#12289;&#25253;&#21578;&#29983;&#25104;&#31561;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#19981;&#21463;&#38480;&#21046;&#30340;&#24694;&#24847;&#24212;&#29992;&#21487;&#33021;&#23548;&#33268;&#19981;&#33391;&#21518;&#26524;&#65292;&#22914;&#34394;&#20551;&#26032;&#38395;&#29983;&#25104;&#12289;&#25220;&#34989;&#31561;&#12290;&#22240;&#27492;&#65292;&#22312;&#36127;&#36131;&#20219;&#22320;&#20351;&#29992;LLM&#26102;&#20934;&#30830;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#35821;&#35328;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;1&#65289;&#26576;&#20010;&#25991;&#26412;&#26159;&#30001;AI&#29983;&#25104;&#36824;&#26159;&#20154;&#31867;&#32534;&#20889;&#30340;&#65307;2&#65289;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#24402;&#23646;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#33521;&#25991;&#21644;&#35199;&#29677;&#29273;&#25991;&#30340;&#25991;&#26412;&#12290;&#26412;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#26159;Automated Text Identification (AuTexTification)&#20849;&#20139;&#20219;&#21153;&#30340;&#19968;&#37096;&#20998;&#12290;&#38024;&#23545;&#19978;&#36848;&#27599;&#20010;&#30740;&#31350;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38598;&#25104;&#31070;&#32463;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;LLM&#29983;&#25104;&#27010;&#29575;&#65292;&#36825;&#20123;&#27010;&#29575;&#34987;&#29992;&#20316;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown impressive performance across a variety of Artificial Intelligence (AI) and natural language processing tasks, such as content creation, report generation, etc. However, unregulated malign application of these models can create undesirable consequences such as generation of fake news, plagiarism, etc. As a result, accurate detection of AI-generated language can be crucial in responsible usage of LLMs. In this work, we explore 1) whether a certain body of text is AI generated or written by human, and 2) attribution of a specific language model in generating a body of text. Texts in both English and Spanish are considered. The datasets used in this study are provided as part of the Automated Text Identification (AuTexTification) shared task. For each of the research objectives stated above, we propose an ensemble neural model that generates probabilities from different pre-trained LLMs which are used as features to a Traditional Machine Learning (T
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#35821;&#38899;&#20998;&#31867;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21333;&#35789;&#32423;&#21035;&#19978;&#35299;&#37322;&#38899;&#39057;&#27573;&#33853;&#21644;&#35821;&#38899;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#36755;&#20837;&#36827;&#34892;&#25200;&#21160;&#65292;&#29983;&#25104;&#26131;&#20110;&#29702;&#35299;&#30340;&#35299;&#37322;&#65292;&#24182;&#22238;&#31572;&#20102;&#22914;&#26524;&#20462;&#25913;&#20102;&#38899;&#39057;&#20449;&#21495;&#20250;&#23545;&#27169;&#22411;&#39044;&#27979;&#20135;&#29983;&#24590;&#26679;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#39564;&#35777;&#65292;&#21457;&#29616;&#36825;&#20123;&#35299;&#37322;&#19982;&#27169;&#22411;&#20869;&#37096;&#24037;&#20316;&#30456;&#31526;&#65292;&#19988;&#23545;&#20154;&#31867;&#32780;&#35328;&#26159;&#21487;&#20449;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.07733</link><description>&lt;p&gt;
&#36890;&#36807;&#21333;&#35789;&#32423;&#38899;&#39057;&#29255;&#27573;&#21644;&#35821;&#38899;&#29305;&#24449;&#35299;&#37322;&#35821;&#38899;&#20998;&#31867;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Explaining Speech Classification Models via Word-Level Audio Segments and Paralinguistic Features. (arXiv:2309.07733v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07733
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#35821;&#38899;&#20998;&#31867;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21333;&#35789;&#32423;&#21035;&#19978;&#35299;&#37322;&#38899;&#39057;&#27573;&#33853;&#21644;&#35821;&#38899;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#36755;&#20837;&#36827;&#34892;&#25200;&#21160;&#65292;&#29983;&#25104;&#26131;&#20110;&#29702;&#35299;&#30340;&#35299;&#37322;&#65292;&#24182;&#22238;&#31572;&#20102;&#22914;&#26524;&#20462;&#25913;&#20102;&#38899;&#39057;&#20449;&#21495;&#20250;&#23545;&#27169;&#22411;&#39044;&#27979;&#20135;&#29983;&#24590;&#26679;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#39564;&#35777;&#65292;&#21457;&#29616;&#36825;&#20123;&#35299;&#37322;&#19982;&#27169;&#22411;&#20869;&#37096;&#24037;&#20316;&#30456;&#31526;&#65292;&#19988;&#23545;&#20154;&#31867;&#32780;&#35328;&#26159;&#21487;&#20449;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#36827;&#23637;&#20026;&#25105;&#20204;&#23545;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#34920;&#26684;&#25968;&#25454;&#27169;&#22411;&#30340;&#36816;&#34892;&#26041;&#24335;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#26041;&#27861;&#29992;&#20110;&#29702;&#35299;&#35821;&#38899;&#27169;&#22411;&#12290;&#29616;&#26377;&#24037;&#20316;&#19987;&#27880;&#20110;&#19968;&#20123;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#65292;&#24182;&#19988;&#23545;&#22823;&#22810;&#25968;&#29992;&#25143;&#26469;&#35828;&#65292;&#35299;&#37322;&#38590;&#20197;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#35821;&#38899;&#20998;&#31867;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20004;&#20010;&#20449;&#24687;&#32423;&#21035;&#19978;&#23545;&#36755;&#20837;&#36827;&#34892;&#25200;&#21160;&#26469;&#29983;&#25104;&#26131;&#20110;&#35299;&#37322;&#30340;&#35299;&#37322;&#12290;1) &#21333;&#35789;&#32423;&#35299;&#37322;&#26174;&#31034;&#27599;&#20010;&#19982;&#21333;&#35789;&#30456;&#20851;&#30340;&#38899;&#39057;&#29255;&#27573;&#22914;&#20309;&#24433;&#21709;&#32467;&#26524;&#12290;2) &#35821;&#38899;&#29305;&#24449;&#65288;&#20363;&#22914;&#38901;&#24459;&#21644;&#32972;&#26223;&#22122;&#38899;&#65289;&#22238;&#31572;&#20102;&#21453;&#20107;&#23454;&#38382;&#39064;&#65306;&#8220;&#22914;&#26524;&#25105;&#20204;&#20197;&#36825;&#31181;&#26041;&#24335;&#32534;&#36753;&#38899;&#39057;&#20449;&#21495;&#65292;&#27169;&#22411;&#30340;&#39044;&#27979;&#20250;&#26159;&#20160;&#20040;&#65311;&#8221;&#25105;&#20204;&#36890;&#36807;&#35299;&#37322;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#33521;&#35821;&#21644;&#24847;&#22823;&#21033;&#35821;&#21475;&#35821;&#29702;&#35299;&#27169;&#22411;&#30340;&#20004;&#20010;&#21475;&#35821;&#20998;&#31867;&#20219;&#21153;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#36825;&#20123;&#35299;&#37322;&#31526;&#21512;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#24182;&#19988;&#23545;&#20154;&#31867;&#32780;&#35328;&#26159;&#21487;&#20449;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in eXplainable AI (XAI) have provided new insights into how models for vision, language, and tabular data operate. However, few approaches exist for understanding speech models. Existing work focuses on a few spoken language understanding (SLU) tasks, and explanations are difficult to interpret for most users. We introduce a new approach to explain speech classification models. We generate easy-to-interpret explanations via input perturbation on two information levels. 1) Word-level explanations reveal how each word-related audio segment impacts the outcome. 2) Paralinguistic features (e.g., prosody and background noise) answer the counterfactual: ``What would the model prediction be if we edited the audio signal in this way?'' We validate our approach by explaining two state-of-the-art SLU models on two speech classification tasks in English and Italian. Our findings demonstrate that the explanations are faithful to the model's inner workings and plausible to humans. O
&lt;/p&gt;</description></item><item><title>PerPLM&#36890;&#36807;&#20010;&#24615;&#21270;&#20013;&#38388;&#23398;&#20064;&#21644;&#25552;&#31034;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#65292;&#24182;&#22312;&#21482;&#26377;&#30446;&#26631;&#20316;&#32773;&#32431;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#23558;&#20854;&#25193;&#23637;&#21040;&#22810;&#29992;&#25143;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#29702;&#35299;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07727</link><description>&lt;p&gt;
PerPLM: &#36890;&#36807;&#20010;&#24615;&#21270;&#30340;&#20013;&#38388;&#23398;&#20064;&#21644;&#25552;&#31034;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
PerPLM: Personalized Fine-tuning of Pretrained Language Models via Writer-specific Intermediate Learning and Prompts. (arXiv:2309.07727v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07727
&lt;/p&gt;
&lt;p&gt;
PerPLM&#36890;&#36807;&#20010;&#24615;&#21270;&#20013;&#38388;&#23398;&#20064;&#21644;&#25552;&#31034;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#65292;&#24182;&#22312;&#21482;&#26377;&#30446;&#26631;&#20316;&#32773;&#32431;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#23558;&#20854;&#25193;&#23637;&#21040;&#22810;&#29992;&#25143;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#29702;&#35299;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#35789;&#21644;&#30701;&#35821;&#30340;&#21547;&#20041;&#19981;&#20165;&#21462;&#20915;&#20110;&#23427;&#20204;&#25152;&#22312;&#30340;&#19978;&#19979;&#25991;&#65292;&#36824;&#21462;&#20915;&#20110;&#20351;&#29992;&#32773;&#12290;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#26159;&#25429;&#25417;&#19978;&#19979;&#25991;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20294;&#36890;&#24120;&#26159;&#20026;&#20102;&#22312;&#19981;&#21516;&#30340;&#20351;&#29992;&#32773;&#20043;&#38388;&#26222;&#36941;&#20351;&#29992;&#32780;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20010;&#24615;&#21270;&#24494;&#35843;PLMs&#26469;&#25552;&#39640;&#25991;&#26412;&#29702;&#35299;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20851;&#27880;&#19968;&#20010;&#21482;&#26377;&#30446;&#26631;&#20316;&#32773;&#30340;&#32431;&#25991;&#26412;&#21487;&#29992;&#20110;&#20010;&#24615;&#21270;&#30340;&#19968;&#33324;&#35774;&#32622;&#12290;&#20026;&#20102;&#36991;&#20813;&#23545;&#19981;&#21516;&#29992;&#25143;&#36827;&#34892;&#24494;&#35843;&#21644;&#23384;&#20648;&#22810;&#20010;PLM&#30340;&#25104;&#26412;&#65292;&#25105;&#20204;&#35814;&#32454;&#25506;&#35752;&#20102;&#20351;&#29992;&#29305;&#23450;&#20316;&#32773;&#25552;&#31034;&#26469;&#20010;&#24615;&#21270;&#32479;&#19968;PLM&#30340;&#21487;&#33021;&#24615;&#12290;&#30001;&#20110;&#36825;&#20123;&#25552;&#31034;&#30340;&#35774;&#35745;&#21644;&#35780;&#20272;&#26159;&#19968;&#20010;&#23578;&#26410;&#21457;&#23637;&#23436;&#21892;&#30340;&#39046;&#22495;&#65292;&#25105;&#20204;&#20171;&#32461;&#24182;&#27604;&#36739;&#20102;&#22312;&#25105;&#20204;&#30340;&#29615;&#22659;&#20013;&#21487;&#33021;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#25552;&#31034;&#12290;&#20026;&#20102;&#26368;&#22823;&#38480;&#24230;&#22320;&#21457;&#25381;&#22522;&#20110;&#25552;&#31034;&#30340;&#20010;&#24615;&#21270;&#24494;&#35843;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25513;&#30721;&#30340;&#20010;&#24615;&#21270;&#20013;&#38388;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The meanings of words and phrases depend not only on where they are used (contexts) but also on who use them (writers). Pretrained language models (PLMs) are powerful tools for capturing context, but they are typically pretrained and fine-tuned for universal use across different writers. This study aims to improve the accuracy of text understanding tasks by personalizing the fine-tuning of PLMs for specific writers. We focus on a general setting where only the plain text from target writers are available for personalization. To avoid the cost of fine-tuning and storing multiple copies of PLMs for different users, we exhaustively explore using writer-specific prompts to personalize a unified PLM. Since the design and evaluation of these prompts is an underdeveloped area, we introduce and compare different types of prompts that are possible in our setting. To maximize the potential of prompt-based personalized fine-tuning, we propose a personalized intermediate learning based on masked l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;L1&#24863;&#30693;&#30340;&#22810;&#35821;&#35328;&#21457;&#38899;&#38169;&#35823;&#26816;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#23545;&#40784;&#36755;&#20837;&#38899;&#39057;&#21644;&#21442;&#32771;&#38899;&#32032;&#24207;&#21015;&#65292;&#24182;&#23558;&#39044;&#35757;&#32451;&#30340;&#36741;&#21161;&#27169;&#22411;&#25552;&#21462;&#30340;L1-L2&#35821;&#38899;&#23884;&#20837;&#19982;&#20027;&#35201;&#32593;&#32476;&#36827;&#34892;&#34701;&#21512;&#12290;&#35813;&#26694;&#26550;&#22312;&#33521;&#25991;&#12289;&#38463;&#25289;&#20271;&#35821;&#21644;&#26222;&#36890;&#35805;&#19978;&#30340;&#32479;&#19968;&#22810;&#35821;&#35328;&#38899;&#32032;&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.07719</link><description>&lt;p&gt;
L1&#24863;&#30693;&#22810;&#35821;&#35328;&#21457;&#38899;&#38169;&#35823;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
L1-aware Multilingual Mispronunciation Detection Framework. (arXiv:2309.07719v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;L1&#24863;&#30693;&#30340;&#22810;&#35821;&#35328;&#21457;&#38899;&#38169;&#35823;&#26816;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#23545;&#40784;&#36755;&#20837;&#38899;&#39057;&#21644;&#21442;&#32771;&#38899;&#32032;&#24207;&#21015;&#65292;&#24182;&#23558;&#39044;&#35757;&#32451;&#30340;&#36741;&#21161;&#27169;&#22411;&#25552;&#21462;&#30340;L1-L2&#35821;&#38899;&#23884;&#20837;&#19982;&#20027;&#35201;&#32593;&#32476;&#36827;&#34892;&#34701;&#21512;&#12290;&#35813;&#26694;&#26550;&#22312;&#33521;&#25991;&#12289;&#38463;&#25289;&#20271;&#35821;&#21644;&#26222;&#36890;&#35805;&#19978;&#30340;&#32479;&#19968;&#22810;&#35821;&#35328;&#38899;&#32032;&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#35805;&#32773;&#30340;&#27597;&#35821;(L1)&#21644;&#38750;&#27597;&#35821;(L2)&#20043;&#38388;&#30340;&#35821;&#38899;&#24046;&#24322;&#26159;&#21457;&#38899;&#38169;&#35823;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#35821;&#35328;MDD&#26550;&#26500;&#8212;&#8212;L1-MultiMDD&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;L1&#24863;&#30693;&#30340;&#35821;&#38899;&#34920;&#31034;&#36827;&#34892;&#22686;&#24378;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#23558;&#36755;&#20837;&#38899;&#39057;&#19982;&#21442;&#32771;&#38899;&#32032;&#24207;&#21015;&#36827;&#34892;&#23545;&#40784;&#12290;&#28982;&#21518;&#65292;&#20174;&#22312;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#39044;&#20808;&#35757;&#32451;&#30340;&#36741;&#21161;&#27169;&#22411;&#20013;&#25552;&#21462;L1-L2&#35821;&#38899;&#23884;&#20837;&#65292;&#24182;&#23558;&#20854;&#19982;&#20027;&#35201;&#32593;&#32476;&#36827;&#34892;&#34701;&#21512;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#36830;&#25509;&#26102;&#24207;&#20998;&#31867;(CTC)&#25439;&#22833;&#20248;&#21270;L1-MultiMDD&#26694;&#26550;&#65292;&#29992;&#20110;&#30446;&#26631;&#35821;&#35328;&#33521;&#25991;&#12289;&#38463;&#25289;&#20271;&#35821;&#21644;&#26222;&#36890;&#35805;&#30340;&#32479;&#19968;&#22810;&#35821;&#35328;&#38899;&#32032;&#35782;&#21035;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;L1-MultiMDD&#26694;&#26550;&#22312;&#24050;&#35265;&#25968;&#25454;&#38598;L2-ARTIC&#12289;LATIC&#21644;AraVoiceL2&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The phonological discrepancies between a speaker's native (L1) and the non-native language (L2) serves as a major factor for mispronunciation. This paper introduces a novel multilingual MDD architecture, L1-MultiMDD, enriched with L1-aware speech representation. An end-to-end speech encoder is trained on the input signal and its corresponding reference phoneme sequence. First, an attention mechanism is deployed to align the input audio with the reference phoneme sequence. Afterwards, the L1-L2-speech embedding are extracted from an auxiliary model, pretrained in a multi-task setup identifying L1 and L2 language, and are infused with the primary network. Finally, the L1-MultiMDD is then optimized for a unified multilingual phoneme recognition task using connectionist temporal classification (CTC) loss for the target languages: English, Arabic, and Mandarin. Our experiments demonstrate the effectiveness of the proposed L1-MultiMDD framework on both seen -- L2-ARTIC, LATIC, and AraVoiceL2
&lt;/p&gt;</description></item><item><title>CoLLD&#26159;&#19968;&#31181;&#29992;&#20110;&#21387;&#32553;&#39044;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#30340;&#23545;&#27604;&#23618;&#19982;&#23618;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#29983;&#27169;&#22411;&#22797;&#21046;&#22823;&#25945;&#24072;&#27169;&#22411;&#30340;&#34892;&#20026;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#35821;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.07707</link><description>&lt;p&gt;
CoLLD: &#23545;&#22810;&#35821;&#31181;&#39044;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#30340;&#23545;&#27604;&#23618;&#19982;&#23618;&#33976;&#39311;&#36827;&#34892;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
CoLLD: Contrastive Layer-to-layer Distillation for Compressing Multilingual Pre-trained Speech Encoders. (arXiv:2309.07707v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07707
&lt;/p&gt;
&lt;p&gt;
CoLLD&#26159;&#19968;&#31181;&#29992;&#20110;&#21387;&#32553;&#39044;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#30340;&#23545;&#27604;&#23618;&#19982;&#23618;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#29983;&#27169;&#22411;&#22797;&#21046;&#22823;&#25945;&#24072;&#27169;&#22411;&#30340;&#34892;&#20026;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#35821;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#22312;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#30001;&#20110;&#24320;&#21457;&#36825;&#20123;&#22823;&#27169;&#22411;&#30340;&#25104;&#26412;&#36739;&#39640;&#65292;&#20026;&#26032;&#20219;&#21153;&#26500;&#24314;&#26032;&#32534;&#30721;&#22120;&#24182;&#23558;&#20854;&#37096;&#32626;&#21040;&#35774;&#22791;&#24212;&#29992;&#20013;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20165;&#38024;&#23545;&#36739;&#23567;&#27169;&#22411;&#21644;&#19981;&#22826;&#23454;&#38469;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#23618;&#19982;&#23618;&#33976;&#39311;&#65288;CoLLD&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25513;&#34109;&#39044;&#27979;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#22797;&#21046;&#22823;&#25945;&#24072;&#27169;&#22411;&#30340;&#34892;&#20026;&#26469;&#21387;&#32553;&#39044;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#12290;CoLLD&#22312;&#22810;&#35821;&#31181;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#21644;&#35782;&#21035;&#22522;&#20934;&#19978;&#32988;&#36807;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24357;&#21512;&#20102;&#23567;&#27169;&#22411;&#21644;&#22823;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale self-supervised pre-trained speech encoders outperform conventional approaches in speech recognition and translation tasks. Due to the high cost of developing these large models, building new encoders for new tasks and deploying them to on-device applications are infeasible. Prior studies propose model compression methods to address this issue, but those works focus on smaller models and less realistic tasks. Thus, we propose Contrastive Layer-to-layer Distillation (CoLLD), a novel knowledge distillation method to compress pre-trained speech encoders by leveraging masked prediction and contrastive learning to train student models to copy the behavior of a large teacher model. CoLLD outperforms prior methods and closes the gap between small and large models on multilingual speech-to-text translation and recognition benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#26694;&#26550;&#8212;&#8212;&#19981;&#30830;&#23450;&#24605;&#32500;&#26641;&#65288;TouT&#65289;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#20002;&#24323;&#26469;&#37327;&#21270;&#20013;&#38388;&#27493;&#39588;&#19978;&#30340;&#26412;&#22320;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#29983;&#25104;&#21709;&#24212;&#30340;&#31934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07694</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24605;&#32500;&#26641;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tree of Uncertain Thoughts Reasoning for Large Language Models. (arXiv:2309.07694v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#26694;&#26550;&#8212;&#8212;&#19981;&#30830;&#23450;&#24605;&#32500;&#26641;&#65288;TouT&#65289;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#20002;&#24323;&#26469;&#37327;&#21270;&#20013;&#38388;&#27493;&#39588;&#19978;&#30340;&#26412;&#22320;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#29983;&#25104;&#21709;&#24212;&#30340;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#24341;&#20837;&#30340;&#19981;&#30830;&#23450;&#24605;&#32500;&#26641;&#65288;Tree of Thoughts, ToT&#65289;&#22312;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#39044;&#35265;&#21644;&#22238;&#28335;&#36827;&#34892;&#20840;&#23616;&#20915;&#31574;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#24573;&#35270;&#20102;&#20013;&#38388;&#20915;&#31574;&#28857;&#25110;&#8220;&#24605;&#32500;&#8221;&#20013;&#30340;&#22266;&#26377;&#23616;&#37096;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#20123;&#22266;&#26377;&#30340;&#23616;&#37096;&#19981;&#30830;&#23450;&#24615;&#65292;&#30001;&#20110;LLMs&#28508;&#22312;&#30340;&#22810;&#26679;&#24615;&#21709;&#24212;&#33021;&#21147;&#65292;&#25104;&#20026;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19981;&#30830;&#23450;&#24605;&#32500;&#26641;&#65288;Tree of Uncertain Thoughts, TouT&#65289;-&#19968;&#31181;&#38024;&#23545;LLMs&#35774;&#35745;&#30340;&#25512;&#29702;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;TouT&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#33945;&#29305;&#21345;&#27931;&#20002;&#24323;(Monte Carlo Dropout)&#26469;&#37327;&#21270;&#19982;LLMs&#22312;&#36825;&#20123;&#20013;&#38388;&#27493;&#39588;&#19978;&#30340;&#19981;&#21516;&#26412;&#22320;&#21709;&#24212;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#24471;&#20998;&#12290;&#36890;&#36807;&#23558;&#36825;&#31181;&#26412;&#22320;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#19982;&#20840;&#23616;&#25628;&#32034;&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;TouT&#25552;&#39640;&#20102;&#27169;&#22411;&#29983;&#25104;&#21709;&#24212;&#30340;&#31934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20004;&#20010;&#33499;&#21051;&#30340;&#35268;&#21010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20005;&#26684;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;24&#28857;&#28216;&#25103;&#21644;&#36855;&#20320;&#22635;&#23383;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the recently introduced Tree of Thoughts (ToT) has heralded advancements in allowing Large Language Models (LLMs) to reason through foresight and backtracking for global decision-making, it has overlooked the inherent local uncertainties in intermediate decision points or "thoughts". These local uncertainties, intrinsic to LLMs given their potential for diverse responses, remain a significant concern in the reasoning process. Addressing this pivotal gap, we introduce the Tree of Uncertain Thoughts (TouT) - a reasoning framework tailored for LLMs. Our TouT effectively leverages Monte Carlo Dropout to quantify uncertainty scores associated with LLMs' diverse local responses at these intermediate steps. By marrying this local uncertainty quantification with global search algorithms, TouT enhances the model's precision in response generation. We substantiate our approach with rigorous experiments on two demanding planning tasks: Game of 24 and Mini Crosswords. The empirical evidence 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35843;&#26597;&#35770;&#25991;&#27010;&#36848;&#20102;&#24403;&#21069;&#29992;&#20110;&#21306;&#20998;ChatGPT&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#24037;&#25991;&#26412;&#30340;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#24635;&#32467;&#20102;&#30740;&#31350;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.07689</link><description>&lt;p&gt;
&#26816;&#27979;ChatGPT&#65306;&#23545;&#26816;&#27979;ChatGPT&#29983;&#25104;&#25991;&#26412;&#29366;&#24577;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Detecting ChatGPT: A Survey of the State of Detecting ChatGPT-Generated Text. (arXiv:2309.07689v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07689
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35843;&#26597;&#35770;&#25991;&#27010;&#36848;&#20102;&#24403;&#21069;&#29992;&#20110;&#21306;&#20998;ChatGPT&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#24037;&#25991;&#26412;&#30340;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#24635;&#32467;&#20102;&#30740;&#31350;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#65288;OpenAI, 2022&#65289;&#31561;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#21644;&#24191;&#27867;&#20351;&#29992;&#30340;&#19981;&#26029;&#36827;&#27493;&#65292;&#23427;&#20204;&#29983;&#25104;&#27969;&#21033;&#30340;&#31867;&#20154;&#25991;&#26412;&#24102;&#26469;&#20102;&#21508;&#31181;&#22909;&#22788;&#65292;&#20294;&#21306;&#20998;&#20154;&#31867;&#29983;&#25104;&#25991;&#26412;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#25991;&#26412;&#30340;&#20219;&#21153;&#20063;&#22240;&#27492;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#36890;&#36807;&#29983;&#25104;&#30475;&#20284;&#30001;&#20154;&#31867;&#29983;&#25104;&#30340;&#20154;&#24037;&#25991;&#26412;&#32780;&#36827;&#34892;&#27450;&#39575;&#12290;&#22312;&#27861;&#24459;&#12289;&#25945;&#32946;&#21644;&#31185;&#23398;&#31561;&#39046;&#22495;&#65292;&#30830;&#20445;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#27492;&#36825;&#20010;&#38382;&#39064;&#23588;&#20026;&#26174;&#33879;&#12290;&#26412;&#35843;&#26597;&#27010;&#36848;&#20102;&#24403;&#21069;&#29992;&#20110;&#21306;&#20998;&#20154;&#24037;&#25991;&#26412;&#21644;ChatGPT&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26500;&#24314;&#29992;&#20110;&#26816;&#27979;ChatGPT&#29983;&#25104;&#25991;&#26412;&#30340;&#19981;&#21516;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#36827;&#34892;&#20102;&#21738;&#20123;&#20851;&#20110;&#20154;&#31867;&#19982;ChatGPT&#29983;&#25104;&#25991;&#26412;&#29305;&#24449;&#30340;&#23450;&#24615;&#20998;&#26512;&#65292;&#24182;&#24635;&#32467;&#20102;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
While recent advancements in the capabilities and widespread accessibility of generative language models, such as ChatGPT (OpenAI, 2022), have brought about various benefits by generating fluent human-like text, the task of distinguishing between human- and large language model (LLM) generated text has emerged as a crucial problem. These models can potentially deceive by generating artificial text that appears to be human-generated. This issue is particularly significant in domains such as law, education, and science, where ensuring the integrity of text is of the utmost importance. This survey provides an overview of the current approaches employed to differentiate between texts generated by humans and ChatGPT. We present an account of the different datasets constructed for detecting ChatGPT-generated text, the various methods utilized, what qualitative analyses into the characteristics of human versus ChatGPT-generated text have been performed, and finally, summarize our findings int
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35780;&#20272;GPT3.5&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20855;&#26377;&#26377;&#36259;&#30340;&#20010;&#24615;&#38382;&#21367;&#22238;&#31572;&#33021;&#21147;&#65292;&#20294;&#19981;&#22826;&#21487;&#33021;&#21457;&#23637;&#20986;&#24847;&#35782;&#65292;&#24182;&#26174;&#31034;&#20986;&#36739;&#22823;&#30340;&#35748;&#30693;&#21644;&#20010;&#24615;&#21464;&#24322;&#12290;</title><link>http://arxiv.org/abs/2309.07683</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#36136;&#65306;&#23545;&#20154;&#31867;&#20013;&#24515;&#20027;&#20041;&#30340;&#35686;&#21578;
&lt;/p&gt;
&lt;p&gt;
Assessing the nature of large language models: A caution against anthropocentrism. (arXiv:2309.07683v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07683
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35780;&#20272;GPT3.5&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20855;&#26377;&#26377;&#36259;&#30340;&#20010;&#24615;&#38382;&#21367;&#22238;&#31572;&#33021;&#21147;&#65292;&#20294;&#19981;&#22826;&#21487;&#33021;&#21457;&#23637;&#20986;&#24847;&#35782;&#65292;&#24182;&#26174;&#31034;&#20986;&#36739;&#22823;&#30340;&#35748;&#30693;&#21644;&#20010;&#24615;&#21464;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36890;&#36807;OpenAI&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;ChatGPT&#30340;&#21457;&#24067;&#24341;&#36215;&#20102;&#20844;&#20247;&#30340;&#20851;&#27880;&#21644;&#29468;&#27979;&#12290;&#30446;&#21069;&#23384;&#22312;&#20004;&#31181;&#24847;&#35265;&#38453;&#33829;&#65306;&#19968;&#26041;&#23545;&#36825;&#20123;&#27169;&#22411;&#20026;&#20154;&#31867;&#20219;&#21153;&#24102;&#26469;&#30340;&#22522;&#26412;&#21464;&#38761;&#30340;&#21487;&#33021;&#24615;&#24863;&#21040;&#20852;&#22859;&#65292;&#21478;&#19968;&#26041;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#24863;&#21040;&#39640;&#24230;&#20851;&#20999;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#20851;&#20999;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26631;&#20934;&#12289;&#35268;&#33539;&#21270;&#21644;&#32463;&#36807;&#39564;&#35777;&#30340;&#35748;&#30693;&#21644;&#20010;&#24615;&#27979;&#37327;&#24037;&#20855;&#26469;&#35780;&#20272;GPT3.5&#12290;&#22312;&#36825;&#20010;&#21021;&#27493;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#27979;&#35797;&#65292;&#21487;&#20197;&#20272;&#35745;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#36793;&#30028;&#65292;&#23427;&#20204;&#22312;&#30701;&#26102;&#38388;&#20869;&#30340;&#31283;&#23450;&#24615;&#20197;&#21450;&#19982;&#20154;&#31867;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GPT 3.5&#24456;&#21487;&#33021;&#27809;&#26377;&#20135;&#29983;&#24847;&#35782;&#65292;&#23613;&#31649;&#23427;&#23545;&#20010;&#24615;&#38382;&#21367;&#30340;&#22238;&#31572;&#33021;&#21147;&#20196;&#20154;&#24863;&#20852;&#36259;&#12290;&#23427;&#22312;&#37325;&#22797;&#35266;&#23519;&#36807;&#31243;&#20013;&#26174;&#31034;&#20986;&#35748;&#30693;&#21644;&#20010;&#24615;&#27979;&#37327;&#26041;&#38754;&#30340;&#22823;&#37327;&#21464;&#24322;&#65292;&#36825;&#19982;&#20855;&#26377;&#20154;&#31867;&#33324;&#20010;&#24615;&#30340;&#27169;&#22411;&#26159;&#19981;&#31526;&#21512;&#39044;&#26399;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI models garnered a large amount of public attention and speculation with the release of OpenAIs chatbot, ChatGPT. At least two opinion camps exist: one excited about possibilities these models offer for fundamental changes to human tasks, and another highly concerned about power these models seem to have. To address these concerns, we assessed GPT3.5 using standard, normed, and validated cognitive and personality measures. For this seedling project, we developed a battery of tests that allowed us to estimate the boundaries of some of these models capabilities, how stable those capabilities are over a short period of time, and how they compare to humans.  Our results indicate that GPT 3.5 is unlikely to have developed sentience, although its ability to respond to personality inventories is interesting. It did display large variability in both cognitive and personality measures over repeated observations, which is not expected if it had a human-like personality. Variability 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#32508;&#21512;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#30495;&#23454;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2309.07682</link><description>&lt;p&gt;
&#19968;&#27425;&#23545;&#35805;&#32988;&#36807;&#21315;&#19975;&#30340;&#25512;&#33616;&#65306;&#32508;&#36848;&#32508;&#21512;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Conversation is Worth A Thousand Recommendations: A Survey of Holistic Conversational Recommender Systems. (arXiv:2309.07682v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07682
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#32508;&#21512;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#30495;&#23454;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#20132;&#20114;&#36807;&#31243;&#29983;&#25104;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#24182;&#38750;&#25152;&#26377;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;&#37117;&#20351;&#29992;&#20154;&#31867;&#23545;&#35805;&#20316;&#20026;&#20132;&#20114;&#25968;&#25454;&#28304;&#65307;&#20808;&#21069;&#30340;&#22823;&#37096;&#20998;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#24037;&#20316;&#36890;&#36807;&#20132;&#25442;&#23454;&#20307;&#32423;&#20449;&#24687;&#26469;&#27169;&#25311;&#20132;&#20114;&#12290;&#22240;&#27492;&#65292;&#20808;&#21069;&#30340;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#24037;&#20316;&#26080;&#27861;&#36866;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#23545;&#35805;&#65292;&#20854;&#20013;&#23545;&#35805;&#20250;&#20986;&#29616;&#24847;&#22806;&#36716;&#21464;&#65292;&#25110;&#32773;&#23545;&#35805;&#21644;&#24847;&#22270;&#29702;&#35299;&#24182;&#38750;&#23436;&#32654;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#30740;&#31350;&#30028;&#24050;&#32463;&#24320;&#22987;&#30740;&#31350;&#32508;&#21512;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#65292;&#36825;&#20123;&#31995;&#32479;&#20351;&#29992;&#20174;&#30495;&#23454;&#22330;&#26223;&#20013;&#25910;&#38598;&#21040;&#30340;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#23613;&#31649;&#36825;&#20123;&#32508;&#21512;&#26041;&#27861;&#24050;&#32463;&#20986;&#29616;&#65292;&#20294;&#20854;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#36890;&#36807;&#32467;&#26500;&#21270;&#26041;&#24335;&#24635;&#32467;&#25991;&#29486;&#65292;&#23545;&#32508;&#21512;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#35843;&#26597;&#12290;&#35843;&#26597;&#23558;&#32508;&#21512;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;&#20998;&#20026;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;1&#65289;&#39592;&#24178;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#36873;&#20351;&#29992;2&#65289;&#22806;&#37096;&#30693;&#35782;&#21644;/&#25110;3&#65289;&#22806;&#37096;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational recommender systems (CRS) generate recommendations through an interactive process. However, not all CRS approaches use human conversations as their source of interaction data; the majority of prior CRS work simulates interactions by exchanging entity-level information. As a result, claims of prior CRS work do not generalise to real-world settings where conversations take unexpected turns, or where conversational and intent understanding is not perfect. To tackle this challenge, the research community has started to examine holistic CRS, which are trained using conversational data collected from real-world scenarios. Despite their emergence, such holistic approaches are under-explored.  We present a comprehensive survey of holistic CRS methods by summarizing the literature in a structured manner. Our survey recognises holistic CRS approaches as having three components: 1) a backbone language model, the optional use of 2) external knowledge, and/or 3) external guidance. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#35828;&#35805;&#20154;&#20998;&#31163;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#22810;&#24207;&#21015;&#23545;&#40784;&#31639;&#27861;&#12290;&#25552;&#20986;&#30340;&#24230;&#37327;&#26631;&#20934;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#25991;&#26412;&#20013;&#32570;&#20047;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#36827;&#34892;&#26356;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;&#35813;&#30740;&#31350;&#20026;&#23545;&#35805;&#31995;&#32479;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#21487;&#35270;&#21270;&#21644;&#35780;&#20272;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2309.07677</link><description>&lt;p&gt;
&#23545;&#21033;&#29992;&#39640;&#25928;&#30340;&#22810;&#24207;&#21015;&#23545;&#40784;&#35780;&#20272;&#21644;&#21487;&#35270;&#21270;&#22522;&#20110;&#25991;&#26412;&#30340;&#35828;&#35805;&#20154;&#20998;&#31163;&#36827;&#34892;&#20102;&#30740;&#31350;&#65288;&#25193;&#23637;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
Aligning Speakers: Evaluating and Visualizing Text-based Diarization Using Efficient Multiple Sequence Alignment (Extended Version). (arXiv:2309.07677v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#35828;&#35805;&#20154;&#20998;&#31163;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#22810;&#24207;&#21015;&#23545;&#40784;&#31639;&#27861;&#12290;&#25552;&#20986;&#30340;&#24230;&#37327;&#26631;&#20934;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#25991;&#26412;&#20013;&#32570;&#20047;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#36827;&#34892;&#26356;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;&#35813;&#30740;&#31350;&#20026;&#23545;&#35805;&#31995;&#32479;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#21487;&#35270;&#21270;&#21644;&#35780;&#20272;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#35828;&#35805;&#20154;&#20998;&#31163;&#65288;SD&#65289;&#35780;&#20272;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#24230;&#37327;&#26041;&#27861;&#22312;&#25991;&#26412;&#20013;&#19981;&#32771;&#34385;&#20219;&#20309;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#23616;&#38480;&#24615;&#12290;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#21363;&#22522;&#20110;&#25991;&#26412;&#30340;&#35828;&#35805;&#20154;&#20998;&#31163;&#38169;&#35823;&#29575;&#21644;&#20998;&#31163;F1&#65292;&#36890;&#36807;&#23545;&#40784;&#21442;&#32771;&#25991;&#26412;&#21644;&#20551;&#35774;&#36716;&#24405;&#20013;&#30340;&#26631;&#35760;&#26469;&#36827;&#34892;&#35805;&#35821;&#21644;&#21333;&#35789;&#32423;&#21035;&#30340;&#35780;&#20272;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#24230;&#37327;&#26631;&#20934;&#21253;&#21547;&#26356;&#22810;&#31867;&#22411;&#30340;&#38169;&#35823;&#65292;&#21487;&#20197;&#22312;SD&#20013;&#36827;&#34892;&#26356;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;&#20026;&#20102;&#23545;&#40784;&#26631;&#35760;&#65292;&#24341;&#20837;&#20102;&#25903;&#25345;&#21442;&#32771;&#24207;&#21015;&#30340;&#22810;&#24207;&#21015;&#23545;&#40784;&#31639;&#27861;&#65292;&#21516;&#26102;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#22788;&#29702;&#23545;&#20551;&#35774;&#30340;&#39640;&#32500;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20998;&#20026;&#20004;&#20010;&#24037;&#20855;&#65292;align4d&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#23545;&#40784;&#31639;&#27861;&#30340;API&#65292;TranscribeView&#29992;&#20110;&#21487;&#35270;&#21270;&#21644;&#35780;&#20272;SD&#38169;&#35823;&#65292;&#21487;&#20197;&#26497;&#22823;&#22320;&#20419;&#36827;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#29983;&#25104;&#65292;&#25512;&#21160;&#23545;&#35805;&#31995;&#32479;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel evaluation approach to text-based speaker diarization (SD), tackling the limitations of traditional metrics that do not account for any contextual information in text. Two new metrics are proposed, Text-based Diarization Error Rate and Diarization F1, which perform utteranceand word-level evaluations by aligning tokens in reference and hypothesis transcripts. Our metrics encompass more types of errors compared to existing ones, allowing us to make a more comprehensive analysis in SD. To align tokens, a multiple sequence alignment algorithm is introduced that supports multiple sequences in the reference while handling high-dimensional alignment to the hypothesis using dynamic programming. Our work is packaged into two tools, align4d providing an API for our alignment algorithm and TranscribeView for visualizing and evaluating SD errors, which can greatly aid in the creation of high-quality data, fostering the advancement of dialogue systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20013;&#25991;&#25991;&#26412;&#21040;&#21487;&#35270;&#21270;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#22810;&#35821;&#35328;BERT&#20316;&#20026;&#32534;&#30721;&#22120;&#20197;&#21450;&#34701;&#20837;n-gram&#20449;&#24687;&#26469;&#35299;&#20915;&#20013;&#25991;&#25991;&#26412;&#21040;&#21487;&#35270;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07650</link><description>&lt;p&gt;
&#20174;&#20013;&#25991;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#20013;&#33258;&#21160;&#29983;&#25104;&#25968;&#25454;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Automatic Data Visualization Generation from Chinese Natural Language Questions. (arXiv:2309.07650v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20013;&#25991;&#25991;&#26412;&#21040;&#21487;&#35270;&#21270;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#22810;&#35821;&#35328;BERT&#20316;&#20026;&#32534;&#30721;&#22120;&#20197;&#21450;&#34701;&#20837;n-gram&#20449;&#24687;&#26469;&#35299;&#20915;&#20013;&#25991;&#25991;&#26412;&#21040;&#21487;&#35270;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21487;&#35270;&#21270;&#24050;&#32463;&#25104;&#20026;&#20174;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#27934;&#35265;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#30001;&#20110;&#25805;&#20316;&#25968;&#25454;&#21487;&#35270;&#21270;&#32534;&#31243;&#35821;&#35328;&#30340;&#22256;&#38590;&#24615;&#65292;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#33258;&#21160;&#29983;&#25104;&#25968;&#25454;&#21487;&#35270;&#21270;&#65288;&#25991;&#26412;&#21040;&#21487;&#35270;&#21270;&#65289;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#23613;&#31649;&#22312;&#33521;&#25991;&#25991;&#26412;&#21040;&#21487;&#35270;&#21270;&#19978;&#26377;&#22823;&#37327;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#20294;&#26159;&#20851;&#20110;&#20174;&#20013;&#25991;&#38382;&#39064;&#29983;&#25104;&#25968;&#25454;&#21487;&#35270;&#21270;&#30340;&#30740;&#31350;&#36824;&#27809;&#26377;&#36827;&#34892;&#12290;&#37492;&#20110;&#27492;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20013;&#25991;&#25991;&#26412;&#21040;&#21487;&#35270;&#21270;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#39318;&#27425;&#23581;&#35797;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#22810;&#35821;&#35328;BERT&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#22686;&#24378;&#20102;&#36328;&#35821;&#35328;&#33021;&#21147;&#65292;&#24182;&#23558;n-gram&#20449;&#24687;&#34701;&#20837;&#21040;&#25105;&#20204;&#30340;&#35789;&#34920;&#31034;&#23398;&#20064;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20540;&#24471;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data visualization has emerged as an effective tool for getting insights from massive datasets. Due to the hardness of manipulating the programming languages of data visualization, automatic data visualization generation from natural languages (Text-to-Vis) is becoming increasingly popular. Despite the plethora of research effort on the English Text-to-Vis, studies have yet to be conducted on data visualization generation from questions in Chinese. Motivated by this, we propose a Chinese Text-to-Vis dataset in the paper and demonstrate our first attempt to tackle this problem. Our model integrates multilingual BERT as the encoder, boosts the cross-lingual ability, and infuses the $n$-gram information into our word representation learning. Our experimental results show that our dataset is challenging and deserves further research.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22312;&#20998;&#35299;&#31070;&#32463;&#20256;&#36755;&#22120;&#20013;&#21152;&#20837;&#20102;&#22522;&#20110;&#31867;&#21035;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#21319;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.07648</link><description>&lt;p&gt;
&#22312;&#20998;&#35299;&#31070;&#32463;&#20256;&#36755;&#22120;&#20013;&#24341;&#20837;&#22522;&#20110;&#31867;&#21035;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Incorporating Class-based Language Model for Named Entity Recognition in Factorized Neural Transducer. (arXiv:2309.07648v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07648
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22312;&#20998;&#35299;&#31070;&#32463;&#20256;&#36755;&#22120;&#20013;&#21152;&#20837;&#20102;&#22522;&#20110;&#31867;&#21035;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#21319;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#20960;&#24180;&#31471;&#21040;&#31471;&#65288;E2E&#65289;&#27169;&#22411;&#22312;&#35821;&#38899;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21448;&#23545;&#35821;&#20041;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#22686;&#24378;E2E&#27169;&#22411;&#20013;&#35782;&#21035;&#21629;&#21517;&#23454;&#20307;&#30340;&#33021;&#21147;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21508;&#31181;&#22522;&#20110;&#35268;&#21017;&#25110;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#19978;&#19979;&#25991;&#20559;&#32622;&#31639;&#27861;&#19978;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#21487;&#33021;&#23545;&#20559;&#32622;&#26435;&#37325;&#25935;&#24863;&#65292;&#25110;&#32773;&#30001;&#20110;&#23545;&#21629;&#21517;&#23454;&#20307;&#21015;&#34920;&#30340;&#36807;&#24230;&#20851;&#27880;&#32780;&#38477;&#20302;&#65292;&#24182;&#19988;&#23384;&#22312;&#35823;&#35302;&#21457;&#30340;&#39118;&#38505;&#12290;&#21463;&#20256;&#32479;&#28151;&#21512;&#31995;&#32479;&#20013;&#22522;&#20110;&#31867;&#21035;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#20197;&#21450;&#22312;&#20998;&#35299;&#31070;&#32463;&#20256;&#36755;&#22120;&#65288;FNT&#65289;&#20013;&#22768;&#23398;&#21644;&#35821;&#35328;&#20449;&#24687;&#30340;&#26377;&#25928;&#35299;&#32806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;E2E&#27169;&#22411;&#26469;&#23558;&#22522;&#20110;&#31867;&#21035;&#30340;LMs&#32435;&#20837;FNT&#20013;&#65292;&#31216;&#20026;C-FNT&#12290;&#22312;C-FNT&#20013;&#65292;&#21629;&#21517;&#23454;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#24471;&#20998;&#21487;&#20197;&#19982;&#20854;&#31867;&#21035;&#20851;&#32852;&#65292;&#32780;&#19981;&#26159;&#19982;&#20854;&#34920;&#38754;&#24418;&#24335;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
In spite of the excellent strides made by end-to-end (E2E) models in speech recognition in recent years, named entity recognition is still challenging but critical for semantic understanding. In order to enhance the ability to recognize named entities in E2E models, previous studies mainly focus on various rule-based or attention-based contextual biasing algorithms. However, their performance might be sensitive to the biasing weight or degraded by excessive attention to the named entity list, along with a risk of false triggering. Inspired by the success of the class-based language model (LM) in named entity recognition in conventional hybrid systems and the effective decoupling of acoustic and linguistic information in the factorized neural Transducer (FNT), we propose a novel E2E model to incorporate class-based LMs into FNT, which is referred as C-FNT. In C-FNT, the language model score of named entities can be associated with the name class instead of its surface form. The experime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27169;&#22359;&#21270;&#25512;&#29702;&#27169;&#22411;MORSE&#65292;&#29992;&#20110;&#25913;&#21892;&#31070;&#32463;&#27169;&#22411;&#30340;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#37319;&#29992;&#27169;&#22359;&#21270;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#36755;&#20837;&#30340;&#21160;&#24577;&#36873;&#25321;&#21644;&#36335;&#30001;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#27169;&#22411;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.07624</link><description>&lt;p&gt;
&#21160;&#24577;&#27169;&#22359;&#21270;&#25512;&#29702;&#29992;&#20110;&#32452;&#21512;&#32467;&#26500;&#21270;&#35299;&#37322;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Dynamic MOdularized Reasoning for Compositional Structured Explanation Generation. (arXiv:2309.07624v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27169;&#22359;&#21270;&#25512;&#29702;&#27169;&#22411;MORSE&#65292;&#29992;&#20110;&#25913;&#21892;&#31070;&#32463;&#27169;&#22411;&#30340;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#37319;&#29992;&#27169;&#22359;&#21270;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#36755;&#20837;&#30340;&#21160;&#24577;&#36873;&#25321;&#21644;&#36335;&#30001;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#27169;&#22411;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#27169;&#22411;&#22312;&#35299;&#20915;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20854;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#21270;&#35299;&#37322;&#29983;&#25104;&#20219;&#21153;&#35774;&#32622;&#65292;&#20197;&#20419;&#36827;&#32452;&#21512;&#25512;&#29702;&#30740;&#31350;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#31526;&#21495;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#25512;&#29702;&#35268;&#21017;&#36827;&#34892;&#21453;&#22797;&#25512;&#29702;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#32452;&#21512;&#24615;&#12290;&#20294;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#33030;&#24369;&#30340;&#31526;&#21495;&#20256;&#36882;&#65292;&#24182;&#19988;&#23616;&#38480;&#20110;&#23450;&#20041;&#26126;&#30830;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27169;&#22359;&#21270;&#25512;&#29702;&#27169;&#22411;MORSE&#65292;&#20197;&#25552;&#39640;&#31070;&#32463;&#27169;&#22411;&#30340;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;MORSE&#23558;&#25512;&#29702;&#36807;&#31243;&#20998;&#35299;&#20026;&#22810;&#20010;&#27169;&#22359;&#30340;&#32452;&#21512;&#65292;&#20854;&#20013;&#27599;&#20010;&#27169;&#22359;&#20195;&#34920;&#19968;&#20010;&#21151;&#33021;&#21333;&#20803;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#27169;&#22359;&#21270;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#21160;&#24577;&#36873;&#25321;&#21644;&#36335;&#30001;&#36755;&#20837;&#21040;&#19987;&#38376;&#30340;&#22836;&#37096;&#65292;&#20351;&#20854;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#27979;&#35797;&#20102;&#25512;&#29702;&#26641;&#30340;&#38271;&#24230;&#21644;&#24418;&#29366;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of neural models in solving reasoning tasks, their compositional generalization capabilities remain unclear. In this work, we propose a new setting of the structured explanation generation task to facilitate compositional reasoning research. Previous works found that symbolic methods achieve superior compositionality by using pre-defined inference rules for iterative reasoning. But these approaches rely on brittle symbolic transfers and are restricted to well-defined tasks. Hence, we propose a dynamic modularized reasoning model, MORSE, to improve the compositional generalization of neural models. MORSE factorizes the inference process into a combination of modules, where each module represents a functional unit. Specifically, we adopt modularized self-attention to dynamically select and route inputs to dedicated heads, which specializes them to specific functions. We conduct experiments for increasing lengths and shapes of reasoning trees on two benchmarks to test 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#22522;&#20110;&#20027;&#39064;&#30340;&#35270;&#39057;&#26816;&#32034;&#24615;&#33021;&#65292;&#26080;&#38656;&#20219;&#20309;&#29305;&#23450;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.07606</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;-shot&#38899;&#39057;&#20027;&#39064;&#37325;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Audio Topic Reranking using Large Language Models. (arXiv:2309.07606v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#22522;&#20110;&#20027;&#39064;&#30340;&#35270;&#39057;&#26816;&#32034;&#24615;&#33021;&#65292;&#26080;&#38656;&#20219;&#20309;&#29305;&#23450;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#35270;&#39057;&#25628;&#32034;&#39033;&#30446;&#36890;&#36807;&#20351;&#29992;&#35270;&#39057;&#29255;&#27573;&#20316;&#20026;&#26597;&#35810;&#39033;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#25991;&#26412;&#26597;&#35810;&#65292;&#26469;&#30740;&#31350;&#20449;&#24687;&#26816;&#32034;&#12290;&#36825;&#20351;&#24471;&#25628;&#32034;&#27169;&#24577;&#26356;&#21152;&#20016;&#23500;&#65292;&#20363;&#22914;&#22270;&#20687;&#12289;&#35828;&#35805;&#32773;&#12289;&#20869;&#23481;&#12289;&#20027;&#39064;&#21644;&#24773;&#24863;&#12290;&#36825;&#20010;&#36807;&#31243;&#30340;&#20851;&#38190;&#35201;&#32032;&#26159;&#23545;&#22823;&#22411;&#23384;&#26723;&#30340;&#39640;&#36895;&#12289;&#28789;&#27963;&#30340;&#25628;&#32034;&#25903;&#25345;&#65292;MVSE&#36890;&#36807;&#29992;&#23884;&#20837;&#34920;&#31034;&#35270;&#39057;&#23646;&#24615;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#26816;&#26597;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#26469;&#20943;&#23569;&#26469;&#33258;&#24555;&#36895;&#23384;&#26723;&#25628;&#32034;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot &#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#65292;&#22240;&#20026;&#36825;&#20123;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#35270;&#39057;&#23384;&#26723;&#38899;&#39057;&#20869;&#23481;&#12290;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#35270;&#39057;&#23384;&#26723;BBC Rewind&#35821;&#26009;&#24211;&#19978;&#35780;&#20272;&#20102;&#22522;&#20110;&#20027;&#39064;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#20219;&#21153;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#37325;&#26032;&#25490;&#24207;&#21487;&#20197;&#23454;&#29616;&#25913;&#36827;&#30340;&#26816;&#32034;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Multimodal Video Search by Examples (MVSE) project investigates using video clips as the query term for information retrieval, rather than the more traditional text query. This enables far richer search modalities such as images, speaker, content, topic, and emotion. A key element for this process is highly rapid, flexible, search to support large archives, which in MVSE is facilitated by representing video attributes by embeddings. This work aims to mitigate any performance loss from this rapid archive search by examining reranking approaches. In particular, zero-shot reranking methods using large language models are investigated as these are applicable to any video archive audio content. Performance is evaluated for topic-based retrieval on a publicly available video archive, the BBC Rewind corpus. Results demonstrate that reranking can achieve improved retrieval ranking without the need for any task-specific training data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24369;&#30417;&#30563;&#30340;&#26041;&#24335;&#26469;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2309.07601</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#20449;&#21495;&#21644;&#24369;&#30417;&#30563;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Detecting Misinformation with LLM-Predicted Credibility Signals and Weak Supervision. (arXiv:2309.07601v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24369;&#30417;&#30563;&#30340;&#26041;&#24335;&#26469;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#24230;&#20449;&#21495;&#20195;&#34920;&#20102;&#35760;&#32773;&#21644;&#20107;&#23454;&#26680;&#26597;&#21592;&#36890;&#24120;&#29992;&#26469;&#35780;&#20272;&#22312;&#32447;&#20869;&#23481;&#30495;&#23454;&#24615;&#30340;&#19968;&#31995;&#21015;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#21270;&#21487;&#20449;&#24230;&#20449;&#21495;&#25552;&#21462;&#30340;&#20219;&#21153;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#35757;&#32451;&#39640;&#20934;&#30830;&#29575;&#30340;&#29305;&#23450;&#20449;&#21495;&#25552;&#21462;&#22120;&#65292;&#32780;&#30446;&#21069;&#27809;&#26377;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#38598;&#23545;&#25152;&#26377;&#21487;&#20449;&#24230;&#20449;&#21495;&#36827;&#34892;&#27880;&#37322;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26159;&#21542;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#19968;&#32452;18&#20010;&#21487;&#20449;&#24230;&#20449;&#21495;&#26469;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20197;&#20135;&#29983;&#27599;&#20010;&#20449;&#21495;&#30340;&#24369;&#26631;&#31614;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#24369;&#30417;&#30563;&#30340;&#26041;&#24335;&#23545;&#36825;&#20123;&#28508;&#22312;&#30340;&#22122;&#22768;&#26631;&#31614;&#36827;&#34892;&#32858;&#21512;&#65292;&#20197;&#39044;&#27979;&#20869;&#23481;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21363;&#32467;&#21512;&#20102;&#38646;-shot LLM&#21487;&#20449;&#24230;&#20449;&#21495;&#26631;&#27880;&#21644;&#24369;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#34394;&#20551;&#20449;&#24687;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#65292;&#32780;&#27809;&#26377;&#20351;&#29992;&#20219;&#20309;&#35757;&#32451;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Credibility signals represent a wide range of heuristics that are typically used by journalists and fact-checkers to assess the veracity of online content. Automating the task of credibility signal extraction, however, is very challenging as it requires high-accuracy signal-specific extractors to be trained, while there are currently no sufficiently large datasets annotated with all credibility signals. This paper investigates whether large language models (LLMs) can be prompted effectively with a set of 18 credibility signals to produce weak labels for each signal. We then aggregate these potentially noisy labels using weak supervision in order to predict content veracity. We demonstrate that our approach, which combines zero-shot LLM credibility signal labeling and weak supervision, outperforms state-of-the-art classifiers on two misinformation datasets without using any ground-truth labels for training. We also analyse the contribution of the individual credibility signals towards p
&lt;/p&gt;</description></item><item><title>C-Pack&#26159;&#19968;&#22871;&#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#39046;&#22495;&#30340;&#36164;&#28304;&#65292;&#21253;&#25324;&#20840;&#38754;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12289;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#38598;&#21644;&#28085;&#30422;&#22810;&#20010;&#23610;&#23544;&#30340;&#23884;&#20837;&#27169;&#22411;&#31995;&#21015;&#12290;&#35813;&#36164;&#28304;&#38598;&#22312;C-MTEB&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;+10%&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#21644;&#20248;&#21270;&#19968;&#22871;&#35757;&#32451;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;C-Pack&#36824;&#21457;&#24067;&#20102;&#33521;&#35821;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#35813;&#36164;&#28304;&#38598;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;</title><link>http://arxiv.org/abs/2309.07597</link><description>&lt;p&gt;
C-Pack: &#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#30340;&#25171;&#21253;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
C-Pack: Packaged Resources To Advance General Chinese Embedding. (arXiv:2309.07597v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07597
&lt;/p&gt;
&lt;p&gt;
C-Pack&#26159;&#19968;&#22871;&#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#39046;&#22495;&#30340;&#36164;&#28304;&#65292;&#21253;&#25324;&#20840;&#38754;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12289;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#38598;&#21644;&#28085;&#30422;&#22810;&#20010;&#23610;&#23544;&#30340;&#23884;&#20837;&#27169;&#22411;&#31995;&#21015;&#12290;&#35813;&#36164;&#28304;&#38598;&#22312;C-MTEB&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;+10%&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#21644;&#20248;&#21270;&#19968;&#22871;&#35757;&#32451;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;C-Pack&#36824;&#21457;&#24067;&#20102;&#33521;&#35821;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#35813;&#36164;&#28304;&#38598;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;C-Pack&#65292;&#36825;&#26159;&#19968;&#22871;&#26174;&#33879;&#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#39046;&#22495;&#30340;&#36164;&#28304;&#12290;C-Pack&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#36164;&#28304;&#12290;1&#65289;C-MTEB&#26159;&#19968;&#20010;&#28085;&#30422;6&#20010;&#20219;&#21153;&#21644;35&#20010;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12290;2&#65289;C-MTP&#26159;&#19968;&#20010;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#27721;&#35821;&#35821;&#26009;&#24211;&#20013;&#31574;&#21010;&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#12290;3&#65289;C-TEM&#26159;&#19968;&#20010;&#28085;&#30422;&#22810;&#20010;&#23610;&#23544;&#30340;&#23884;&#20837;&#27169;&#22411;&#31995;&#21015;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;C-MTEB&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#30340;&#25152;&#26377;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#36798;&#21040;&#20102;&#21457;&#24067;&#26102;&#30340;&#26368;&#39640;+10%&#12290;&#25105;&#20204;&#36824;&#25972;&#21512;&#21644;&#20248;&#21270;&#20102;C-TEM&#30340;&#25972;&#22871;&#35757;&#32451;&#26041;&#27861;&#12290;&#38500;&#20102;&#25105;&#20204;&#20851;&#20110;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#30340;&#36164;&#28304;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#33521;&#35821;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;&#36825;&#20123;&#33521;&#35821;&#27169;&#22411;&#22312;MTEB&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65307;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#24067;&#30340;&#33521;&#35821;&#25968;&#25454;&#27604;&#27721;&#35821;&#25968;&#25454;&#22823;2&#20493;&#12290;&#25152;&#26377;&#36825;&#20123;&#36164;&#28304;&#37117;&#21487;&#20197;&#22312;https://github.com/FlagOpen/FlagEmbedding&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce C-Pack, a package of resources that significantly advance the field of general Chinese embeddings. C-Pack includes three critical resources. 1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6 tasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated from labeled and unlabeled Chinese corpora for training embedding models. 3) C-TEM is a family of embedding models covering multiple sizes. Our models outperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the time of the release. We also integrate and optimize the entire suite of training methods for C-TEM. Along with our resources on general Chinese embedding, we release our data and models for English text embeddings. The English models achieve state-of-the-art performance on MTEB benchmark; meanwhile, our released English data is 2 times larger than the Chinese data. All these resources are made publicly available at https://github.com/FlagOpen/FlagEmbedding.
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;HPSG&#30340;Supertagging&#65292;&#22312;&#39640;&#36136;&#37327;&#27880;&#37322;&#30340;&#26641;&#24211;&#21644;&#22810;&#26679;&#21270;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#65292;&#36890;&#36807;&#20351;&#29992;SVM&#21644;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#20934;&#30830;&#29575;&#12290;&#30456;&#20851;&#25968;&#25454;&#38598;&#24050;&#25972;&#29702;&#20026;&#26631;&#35760;&#20998;&#31867;&#24418;&#24335;&#65292;&#21487;&#20026;&#29616;&#20195;HPSG&#35299;&#26512;&#22120;&#25552;&#20379;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2309.07590</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;HPSG&#30340;Supertagging
&lt;/p&gt;
&lt;p&gt;
Revisiting Supertagging for HPSG. (arXiv:2309.07590v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07590
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;HPSG&#30340;Supertagging&#65292;&#22312;&#39640;&#36136;&#37327;&#27880;&#37322;&#30340;&#26641;&#24211;&#21644;&#22810;&#26679;&#21270;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#65292;&#36890;&#36807;&#20351;&#29992;SVM&#21644;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#20934;&#30830;&#29575;&#12290;&#30456;&#20851;&#25968;&#25454;&#38598;&#24050;&#25972;&#29702;&#20026;&#26631;&#35760;&#20998;&#31867;&#24418;&#24335;&#65292;&#21487;&#20026;&#29616;&#20195;HPSG&#35299;&#26512;&#22120;&#25552;&#20379;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;HPSG&#26641;&#24211;&#35757;&#32451;&#30340;&#26032;&#22411;supertagger&#12290;&#36825;&#20123;&#26641;&#24211;&#22522;&#20110;&#19968;&#20010;&#25104;&#29087;&#30340;&#35821;&#35328;&#23398;&#29702;&#35770;&#65292;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#65292;&#24182;&#19988;&#21253;&#21547;&#20102;&#20016;&#23500;&#22810;&#26679;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#36229;&#20986;&#20102;&#36890;&#24120;&#30340;WSJ&#31532;23&#33410;&#21644;&#32500;&#22522;&#30334;&#31185;&#25968;&#25454;&#12290;&#20043;&#21069;&#30340;HPSG supertagging&#20027;&#35201;&#20381;&#36182;&#20110;&#22522;&#20110;MaxEnt&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;SVM&#21644;&#22522;&#20110;&#31070;&#32463;CRF&#21644;BERT&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20986;SVM&#21644;&#31070;&#32463;supertagger&#30456;&#23545;&#20110;&#22522;&#20934;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#24494;&#35843;&#30340;BERT-based tagger&#22312;&#26469;&#33258;WSJ23&#30340;1000&#20010;&#21477;&#23376;&#19978;&#36798;&#21040;&#20102;97.26%&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#23436;&#20840;&#19981;&#21516;&#39046;&#22495;&#30340;"The Cathedral and the Bazaar"&#19978;&#36798;&#21040;&#20102;93.88%&#30340;&#20934;&#30830;&#29575;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#23558;&#36825;&#20123;&#26032;&#30340;supertagger&#38598;&#25104;&#21040;&#29616;&#20195;HPSG&#35299;&#26512;&#22120;&#20013;&#26159;&#26377;&#24847;&#20041;&#30340;&#65292;&#24182;&#19988;&#25105;&#20204;&#20063;&#24076;&#26395;&#25105;&#20204;&#22312;&#36825;&#37324;&#20351;&#29992;&#30340;&#22810;&#26679;&#19988;&#38590;&#30340;&#25968;&#25454;&#38598;&#22312;&#35813;&#39046;&#22495;&#20013;&#33719;&#24471;&#26356;&#22810;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#36129;&#29486;&#20102;&#37325;&#26032;&#26684;&#24335;&#21270;&#20026;&#26631;&#35760;&#20998;&#31867;&#30340;&#23436;&#25972;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present new supertaggers trained on HPSG-based treebanks. These treebanks feature high-quality annotation based on a well-developed linguistic theory and include diverse and challenging test datasets, beyond the usual WSJ section 23 and Wikipedia data. HPSG supertagging has previously relied on MaxEnt-based models. We use SVM and neural CRF- and BERT-based methods and show that both SVM and neural supertaggers achieve considerably higher accuracy compared to the baseline. Our fine-tuned BERT-based tagger achieves 97.26% accuracy on 1000 sentences from WSJ23 and 93.88% on the completely out-of-domain The Cathedral and the Bazaar (cb)). We conclude that it therefore makes sense to integrate these new supertaggers into modern HPSG parsers, and we also hope that the diverse and difficult datasets we used here will gain more popularity in the field. We contribute the complete dataset reformatted for token classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#29256;&#26412;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;AdaptPrompt&#65292;&#22312;&#38544;&#21547;&#31687;&#31456;&#20851;&#31995;&#35782;&#21035;&#20013;&#24212;&#29992;&#36830;&#36143;&#30693;&#35782;&#33976;&#39311;&#65292;&#36890;&#36807;&#36830;&#32493;&#25552;&#31034;&#21644;&#30693;&#35782;&#20256;&#36882;&#25913;&#36827;&#24615;&#33021;&#65292;&#20943;&#23569;&#25163;&#21160;&#35774;&#35745;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2309.07561</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#25552;&#31034;&#23398;&#20064;&#19982;&#33976;&#39311;&#36830;&#36143;&#30693;&#35782;&#22312;&#38544;&#21547;&#31687;&#31456;&#20851;&#31995;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adaptive Prompt Learning with Distilled Connective Knowledge for Implicit Discourse Relation Recognition. (arXiv:2309.07561v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#29256;&#26412;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;AdaptPrompt&#65292;&#22312;&#38544;&#21547;&#31687;&#31456;&#20851;&#31995;&#35782;&#21035;&#20013;&#24212;&#29992;&#36830;&#36143;&#30693;&#35782;&#33976;&#39311;&#65292;&#36890;&#36807;&#36830;&#32493;&#25552;&#31034;&#21644;&#30693;&#35782;&#20256;&#36882;&#25913;&#36827;&#24615;&#33021;&#65292;&#20943;&#23569;&#25163;&#21160;&#35774;&#35745;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#21547;&#31687;&#31456;&#20851;&#31995;&#35782;&#21035;(IDRR)&#26088;&#22312;&#35782;&#21035;&#20004;&#20010;&#25991;&#26412;&#29255;&#27573;&#20043;&#38388;&#30340;&#31687;&#31456;&#20851;&#31995;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#30340;&#36830;&#25509;&#35789;&#12290;&#26368;&#36817;&#65292;&#25552;&#31034;&#23398;&#20064;&#34987;&#24212;&#29992;&#20110;IDRR&#20219;&#21153;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#25163;&#21160;&#35774;&#35745;&#27169;&#26495;&#21644;&#31572;&#26696;&#65292;&#36825;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#26469;&#35828;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#38556;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#29256;&#26412;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#36830;&#36143;&#30693;&#35782;&#33976;&#39311;&#65292;&#31216;&#20026;AdaptPrompt&#65292;&#36890;&#36807;&#36830;&#32493;&#30340;&#25552;&#31034;&#20943;&#23569;&#25163;&#21160;&#35774;&#35745;&#24037;&#20316;&#65292;&#24182;&#36890;&#36807;&#30693;&#35782;&#20256;&#36882;&#36827;&#19968;&#27493;&#25913;&#36827;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#21644;&#35757;&#32451;&#20102;&#19968;&#20123;&#34394;&#25311;&#26631;&#35760;&#26469;&#24418;&#25104;&#36830;&#32493;&#30340;&#27169;&#26495;&#65292;&#24182;&#36890;&#36807;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#36827;&#34892;&#26799;&#24230;&#25628;&#32034;&#33258;&#21160;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#27169;&#26495;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#31572;&#26696;&#20851;&#31995;&#26144;&#23556;&#35268;&#21017;&#26469;&#29983;&#25104;&#19968;&#20123;&#34394;&#25311;&#31572;&#26696;&#20316;&#20026;&#31572;&#26696;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit discourse relation recognition (IDRR) aims at recognizing the discourse relation between two text segments without an explicit connective. Recently, the prompt learning has just been applied to the IDRR task with great performance improvements over various neural network-based approaches. However, the discrete nature of the state-art-of-art prompting approach requires manual design of templates and answers, a big hurdle for its practical applications. In this paper, we propose a continuous version of prompt learning together with connective knowledge distillation, called AdaptPrompt, to reduce manual design efforts via continuous prompting while further improving performance via knowledge transfer. In particular, we design and train a few virtual tokens to form continuous templates and automatically select the most suitable one by gradient search in the embedding space. We also design an answer-relation mapping rule to generate a few virtual answers as the answer space. Furthe
&lt;/p&gt;</description></item><item><title>DBLPLink&#26159;&#19968;&#20010;&#29992;&#20110;DBLP&#23398;&#26415;&#30693;&#35782;&#22270;&#30340;&#23454;&#20307;&#38142;&#25509;&#22120;&#65292;&#23427;&#20351;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#23454;&#20307;&#23884;&#20837;&#26469;&#36827;&#34892;&#23454;&#20307;&#26631;&#31614;&#29983;&#25104;&#21644;&#25490;&#24207;&#12290;</title><link>http://arxiv.org/abs/2309.07545</link><description>&lt;p&gt;
DBLPLink: &#19968;&#20010;&#29992;&#20110;DBLP&#23398;&#26415;&#30693;&#35782;&#22270;&#30340;&#23454;&#20307;&#38142;&#25509;&#22120;
&lt;/p&gt;
&lt;p&gt;
DBLPLink: An Entity Linker for the DBLP Scholarly Knowledge Graph. (arXiv:2309.07545v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07545
&lt;/p&gt;
&lt;p&gt;
DBLPLink&#26159;&#19968;&#20010;&#29992;&#20110;DBLP&#23398;&#26415;&#30693;&#35782;&#22270;&#30340;&#23454;&#20307;&#38142;&#25509;&#22120;&#65292;&#23427;&#20351;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#23454;&#20307;&#23884;&#20837;&#26469;&#36827;&#34892;&#23454;&#20307;&#26631;&#31614;&#29983;&#25104;&#21644;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;DBLPLink&#30340;&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#65292;&#23427;&#22312;DBLP&#23398;&#26415;&#30693;&#35782;&#22270;&#19978;&#25191;&#34892;&#23454;&#20307;&#38142;&#25509;&#12290;DBLPLink&#20351;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;T5&#65289;&#20174;&#36755;&#20837;&#30340;&#25991;&#26412;&#38382;&#39064;&#20013;&#29983;&#25104;&#23454;&#20307;&#26631;&#31614;&#36328;&#24230;&#12290;&#22522;&#20110;&#36825;&#20123;&#26631;&#31614;&#65292;&#20174;&#25968;&#25454;&#24211;&#20013;&#33719;&#21462;&#23454;&#20307;&#20505;&#36873;&#39033;&#65292;&#24182;&#20351;&#29992;&#23454;&#20307;&#23884;&#20837;&#65288;&#22914;TransE&#12289;DistMult&#21644;ComplEx&#65289;&#23545;&#23427;&#20204;&#36827;&#34892;&#25490;&#24207;&#12290;&#32467;&#26524;&#20197;&#29992;&#25143;&#21487;&#20197;&#27604;&#36739;&#21644;&#23545;&#27604;T5-small&#12289;T5-base&#21644;&#19981;&#21516;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#20043;&#38388;&#30340;&#32467;&#26524;&#12290;&#28436;&#31034;&#21487;&#20197;&#22312;https://ltdemos.informatik.uni-hamburg.de/dblplink/&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a web application named DBLPLink, which performs entity linking over the DBLP scholarly knowledge graph. DBLPLink uses text-to-text pre-trained language models, such as T5, to produce entity label spans from an input text question. Entity candidates are fetched from a database based on the labels, and an entity re-ranker sorts them based on entity embeddings, such as TransE, DistMult and ComplEx. The results are displayed so that users may compare and contrast the results between T5-small, T5-base and the different KG embeddings used. The demo can be accessed at https://ltdemos.informatik.uni-hamburg.de/dblplink/.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31163;&#25955;&#22768;&#23398;&#21333;&#20803;&#30340;&#30452;&#25509;&#25991;&#26412;&#21040;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#38899;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#31639;&#27861;&#25552;&#21462;&#22768;&#23398;&#21333;&#20803;&#65292;&#24182;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#39044;&#27979;&#21644;&#29983;&#25104;&#35821;&#38899;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31995;&#32479;&#22312;&#22810;&#25968;&#35780;&#20272;&#30340;&#35821;&#35328;&#23545;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#19988;&#20351;&#29992;&#26356;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#21021;&#22987;&#21270;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.07478</link><description>&lt;p&gt;
&#20351;&#29992;&#22768;&#23398;&#21333;&#20803;&#30340;&#30452;&#25509;&#25991;&#26412;&#21040;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Direct Text to Speech Translation System using Acoustic Units. (arXiv:2309.07478v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31163;&#25955;&#22768;&#23398;&#21333;&#20803;&#30340;&#30452;&#25509;&#25991;&#26412;&#21040;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#38899;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#31639;&#27861;&#25552;&#21462;&#22768;&#23398;&#21333;&#20803;&#65292;&#24182;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#39044;&#27979;&#21644;&#29983;&#25104;&#35821;&#38899;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31995;&#32479;&#22312;&#22810;&#25968;&#35780;&#20272;&#30340;&#35821;&#35328;&#23545;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#19988;&#20351;&#29992;&#26356;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#21021;&#22987;&#21270;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31163;&#25955;&#22768;&#23398;&#21333;&#20803;&#30340;&#30452;&#25509;&#25991;&#26412;&#21040;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#19981;&#21516;&#28304;&#35821;&#35328;&#30340;&#25991;&#26412;&#20316;&#20026;&#36755;&#20837;&#65292;&#22312;&#19981;&#38656;&#35201;&#35813;&#35821;&#35328;&#30340;&#25991;&#26412;&#36716;&#24405;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#30446;&#26631;&#35821;&#35328;&#30340;&#35821;&#38899;&#12290;&#21463;&#21040;&#20197;&#21069;&#30452;&#25509;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#20013;&#22768;&#23398;&#21333;&#20803;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#30456;&#21516;&#30340;&#27969;&#27700;&#32447;&#36890;&#36807;&#32467;&#21512;&#35821;&#38899;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#31639;&#27861;&#25552;&#21462;&#22768;&#23398;&#21333;&#20803;&#12290;&#19968;&#26086;&#33719;&#24471;&#21333;&#20803;&#65292;&#23601;&#20250;&#35757;&#32451;&#19968;&#20010;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#26469;&#39044;&#27979;&#23427;&#20204;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#22768;&#30721;&#22120;&#20174;&#21333;&#20803;&#29983;&#25104;&#35821;&#38899;&#12290;&#25105;&#20204;&#30340;&#30452;&#25509;&#25991;&#26412;&#21040;&#35821;&#38899;&#32763;&#35793;&#26041;&#27861;&#22312;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#25991;&#26412;mBART&#27169;&#22411;&#20316;&#20026;&#21021;&#22987;&#21270;&#30340;&#26032;CVSS&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#22312;&#22823;&#22810;&#25968;&#35780;&#20272;&#30340;&#35821;&#35328;&#23545;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;&#26356;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#21021;&#22987;&#21270;&#25105;&#20204;&#25552;&#20986;&#30340;&#26550;&#26500;&#26102;&#65292;&#26377;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a direct text to speech translation system using discrete acoustic units. This framework employs text in different source languages as input to generate speech in the target language without the need for text transcriptions in this language. Motivated by the success of acoustic units in previous works for direct speech to speech translation systems, we use the same pipeline to extract the acoustic units using a speech encoder combined with a clustering algorithm. Once units are obtained, an encoder-decoder architecture is trained to predict them. Then a vocoder generates speech from units. Our approach for direct text to speech translation was tested on the new CVSS corpus with two different text mBART models employed as initialisation. The systems presented report competitive performance for most of the language pairs evaluated. Besides, results show a remarkable improvement when initialising our proposed architecture with a model pre-trained with more languages.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#35780;&#20272;&#22120;&#21487;&#20197;&#35299;&#20915;&#24403;&#21069;&#22810;&#35821;&#35328;&#35780;&#20272;&#30340;&#38480;&#21046;&#21644;&#25361;&#25112;&#65292;&#33021;&#22815;&#23545;&#21508;&#31181;&#35821;&#35328;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#36827;&#34892;&#26377;&#25928;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.07462</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#22120;&#26159;&#21542;&#26159;&#25193;&#23637;&#22810;&#35821;&#35328;&#35780;&#20272;&#30340;&#35299;&#20915;&#26041;&#26696;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?. (arXiv:2309.07462v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07462
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#35780;&#20272;&#22120;&#21487;&#20197;&#35299;&#20915;&#24403;&#21069;&#22810;&#35821;&#35328;&#35780;&#20272;&#30340;&#38480;&#21046;&#21644;&#25361;&#25112;&#65292;&#33021;&#22815;&#23545;&#21508;&#31181;&#35821;&#35328;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#36827;&#34892;&#26377;&#25928;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#22914;&#38382;&#31572;&#12289;&#25688;&#35201;&#21644;&#20998;&#31867;&#12290;&#23558;LLMs&#29992;&#20316;&#35780;&#20272;&#22120;&#65292;&#21487;&#20197;&#23545;&#20854;&#20182;&#27169;&#22411;&#65288;&#36890;&#24120;&#20026;LLMs&#65289;&#30340;&#36755;&#20986;&#36827;&#34892;&#25490;&#24207;&#25110;&#35780;&#20998;&#65292;&#22240;&#20026;&#24403;&#21069;&#35780;&#20272;&#25216;&#26415;&#23384;&#22312;&#35768;&#22810;&#38480;&#21046;&#65292;&#21253;&#25324;&#32570;&#20047;&#36866;&#24403;&#30340;&#22522;&#20934;&#12289;&#25351;&#26631;&#12289;&#25104;&#26412;&#21644;&#20154;&#24037;&#26631;&#27880;&#32773;&#30340;&#35775;&#38382;&#24615;&#12290;&#34429;&#28982;LLMs&#33021;&#22815;&#22788;&#29702;&#22823;&#32422;100&#31181;&#35821;&#35328;&#65292;&#20294;&#22823;&#22810;&#25968;&#35821;&#35328;&#22312;&#21508;&#31181;&#20219;&#21153;&#12289;&#25351;&#26631;&#21644;&#22522;&#20934;&#19978;&#32570;&#20047;&#31995;&#32479;&#30340;&#35780;&#20272;&#12290;&#36825;&#23601;&#36843;&#20999;&#38656;&#35201;&#25193;&#23637;&#22810;&#35821;&#35328;&#35780;&#20272;&#65292;&#20197;&#30830;&#20445;&#23545;LLM&#22312;&#21508;&#31181;&#35821;&#35328;&#20013;&#30340;&#24615;&#33021;&#26377;&#20934;&#30830;&#30340;&#29702;&#35299;&#12290;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#22120;&#20284;&#20046;&#26159;&#36825;&#20010;&#38382;&#39064;&#30340;&#23436;&#32654;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#38656;&#35201;&#20154;&#24037;&#26631;&#27880;&#32773;&#12289;&#20154;&#24037;&#21019;&#24314;&#30340;&#21442;&#32771;&#21644;&#22522;&#20934;&#65292;&#24182;&#19988;&#29702;&#35770;&#19978;&#21487;&#20197;&#29992;&#26469;&#35780;&#20272;&#20219;&#20309;&#34987;&#35206;&#30422;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive performance on Natural Language Processing (NLP) tasks, such as Question Answering, Summarization, and Classification. The use of LLMs as evaluators, that can rank or score the output of other models (usually LLMs) has become increasingly popular, due to the limitations of current evaluation techniques including the lack of appropriate benchmarks, metrics, cost, and access to human annotators. While LLMs are capable of handling approximately 100 languages, the majority of languages beyond the top 20 lack systematic evaluation across various tasks, metrics, and benchmarks. This creates an urgent need to scale up multilingual evaluation to ensure a precise understanding of LLM performance across diverse languages. LLM-based evaluators seem like the perfect solution to this problem, as they do not require human annotators, human-created references, or benchmarks and can theoretically be used to evaluate any language covered by the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SIB-200&#25968;&#25454;&#38598;&#65292;&#22312;200&#22810;&#31181;&#35821;&#35328;&#21644;&#26041;&#35328;&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#30340;&#20027;&#39064;&#20998;&#31867;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#22635;&#34917;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#39046;&#22495;&#20013;&#23545;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#65292;&#36890;&#36807;&#20840;&#30417;&#30563;&#12289;&#36328;&#35821;&#35328;&#36801;&#31227;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#24615;&#33021;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2309.07445</link><description>&lt;p&gt;
SIB-200: &#21253;&#25324;200&#22810;&#31181;&#35821;&#35328;&#21644;&#26041;&#35328;&#30340;&#31616;&#21333;&#12289;&#20840;&#38754;&#21644;&#22823;&#22411;&#20027;&#39064;&#20998;&#31867;&#35780;&#20272;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic Classification in 200+ Languages and Dialects. (arXiv:2309.07445v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SIB-200&#25968;&#25454;&#38598;&#65292;&#22312;200&#22810;&#31181;&#35821;&#35328;&#21644;&#26041;&#35328;&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#30340;&#20027;&#39064;&#20998;&#31867;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#22635;&#34917;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#39046;&#22495;&#20013;&#23545;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#65292;&#36890;&#36807;&#20840;&#30417;&#30563;&#12289;&#36328;&#35821;&#35328;&#36801;&#31227;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#24615;&#33021;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#35780;&#20272;&#36890;&#24120;&#20165;&#38480;&#20110;&#19968;&#23567;&#37096;&#20998;&#24102;&#26377;&#21487;&#29992;&#25968;&#25454;&#38598;&#30340;&#35821;&#35328;&#65292;&#25490;&#38500;&#20102;&#35768;&#22810;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#12290;&#26412;&#25991;&#21019;&#24314;&#20102;SIB-200&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20027;&#39064;&#20998;&#31867;&#30340;&#22823;&#35268;&#27169;&#24320;&#25918;&#28304;&#20195;&#30721;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;200&#22810;&#31181;&#35821;&#35328;&#21644;&#26041;&#35328;&#65292;&#20197;&#24357;&#34917;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#32570;&#20047;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;&#23545;&#20110;SIB-200&#20013;&#28085;&#30422;&#30340;&#35768;&#22810;&#35821;&#35328;&#26469;&#35828;&#65292;&#36825;&#26159;&#39318;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;NLU&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#22522;&#20110;Flores-200&#26426;&#22120;&#32763;&#35793;&#35821;&#26009;&#24211;&#65292;&#24182;&#23545;&#35813;&#35821;&#26009;&#24211;&#28085;&#30422;&#30340;&#20854;&#20182;203&#31181;&#35821;&#35328;&#36827;&#34892;&#20102;&#21477;&#23376;&#32423;&#27880;&#37322;&#12290;&#23613;&#31649;&#35813;&#20219;&#21153;&#31616;&#21333;&#65292;&#20294;&#25105;&#20204;&#22312;&#20840;&#30417;&#30563;&#35774;&#32622;&#12289;&#36328;&#35821;&#35328;&#36801;&#31227;&#35774;&#32622;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#35774;&#32622;&#19979;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#24615;&#33021;&#20173;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the progress we have recorded in the last few years in multilingual natural language processing, evaluation is typically limited to a small set of languages with available datasets which excludes a large number of low-resource languages. In this paper, we created SIB-200 -- a large-scale open-sourced benchmark dataset for topic classification in 200 languages and dialects to address the lack of evaluation dataset for Natural Language Understanding (NLU). For many of the languages covered in SIB-200, this is the first publicly available evaluation dataset for NLU. The dataset is based on Flores-200 machine translation corpus. We annotated the English portion of the dataset and extended the sentence-level annotation to the remaining 203 languages covered in the corpus. Despite the simplicity of this task, our evaluation in full-supervised setting, cross-lingual transfer setting and prompting of large language model setting show that there is still a large gap between the performa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#20843;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#25688;&#35201;&#20219;&#21153;&#19978;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#23454;&#39564;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#26368;&#20339;&#36866;&#24212;&#30340;&#27169;&#22411;&#30340;&#25688;&#35201;&#22312;&#23436;&#25972;&#24615;&#21644;&#27491;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20154;&#31867;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.07430</link><description>&lt;p&gt;
&#20020;&#24202;&#25991;&#26412;&#25688;&#35201;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24212;&#29992;&#20248;&#20110;&#20154;&#31867;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
Clinical Text Summarization: Adapting Large Language Models Can Outperform Human Experts. (arXiv:2309.07430v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#20843;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#25688;&#35201;&#20219;&#21153;&#19978;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#23454;&#39564;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#26368;&#20339;&#36866;&#24212;&#30340;&#27169;&#22411;&#30340;&#25688;&#35201;&#22312;&#23436;&#25972;&#24615;&#21644;&#27491;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20154;&#31867;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#24037;&#20316;&#20013;&#65292;&#27983;&#35272;&#22823;&#37327;&#30340;&#25991;&#26412;&#25968;&#25454;&#24182;&#24635;&#32467;&#20851;&#38190;&#20449;&#24687;&#23545;&#20020;&#24202;&#21307;&#29983;&#30340;&#26102;&#38388;&#20998;&#37197;&#36896;&#25104;&#20102;&#24456;&#22823;&#30340;&#36127;&#25285;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#21508;&#31181;&#20020;&#24202;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#23578;&#26410;&#24471;&#21040;&#20005;&#26684;&#30340;&#26816;&#39564;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#20843;&#20010;LLMs&#36827;&#34892;&#20102;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#30340;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#20845;&#20010;&#25968;&#25454;&#38598;&#21644;&#22235;&#20010;&#19981;&#21516;&#30340;&#25688;&#35201;&#20219;&#21153;&#65306;&#25918;&#23556;&#23398;&#25253;&#21578;&#12289;&#24739;&#32773;&#38382;&#39064;&#12289;&#30149;&#21382;&#35760;&#24405;&#21644;&#21307;&#24739;&#23545;&#35805;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#27169;&#22411;&#21644;&#36866;&#24212;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;LLMs&#30340;&#26368;&#26032;&#36827;&#23637;&#21487;&#33021;&#19981;&#20250;&#24102;&#26469;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#19982;&#20845;&#21517;&#21307;&#29983;&#36827;&#34892;&#30340;&#20020;&#24202;&#38405;&#35835;&#32773;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#26368;&#20339;&#36866;&#24212;&#30340;LLM&#30340;&#25688;&#35201;&#22312;&#23436;&#25972;&#24615;&#21644;&#27491;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20154;&#31867;&#25688;&#35201;&#12290;&#25105;&#20204;&#30340;&#36827;&#19968;&#27493;&#23450;&#24615;&#20998;&#26512;&#25581;&#31034;&#20102;LLMs&#21644;&#20154;&#31867;&#22312;&#38754;&#23545;&#30340;&#20849;&#21516;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sifting through vast textual data and summarizing key information imposes a substantial burden on how clinicians allocate their time. Although large language models (LLMs) have shown immense promise in natural language processing (NLP) tasks, their efficacy across diverse clinical summarization tasks has not yet been rigorously examined. In this work, we employ domain adaptation methods on eight LLMs, spanning six datasets and four distinct summarization tasks: radiology reports, patient questions, progress notes, and doctor-patient dialogue. Our thorough quantitative assessment reveals trade-offs between models and adaptation methods in addition to instances where recent advances in LLMs may not lead to improved results. Further, in a clinical reader study with six physicians, we depict that summaries from the best adapted LLM are preferable to human summaries in terms of completeness and correctness. Our ensuing qualitative analysis delineates mutual challenges faced by both LLMs and
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#26377;&#38480;&#36164;&#28304;&#26465;&#20214;&#19979;&#30340;&#35821;&#20041;&#35299;&#26512;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#20351;&#29992;&#33258;&#21160;&#25968;&#25454;&#31579;&#36873;&#12289;&#30693;&#35782;&#36801;&#31227;&#12289;&#20027;&#21160;&#23398;&#20064;&#21644;&#25345;&#32493;&#23398;&#20064;&#12290;&#20855;&#20307;&#26041;&#27861;&#21253;&#25324;&#21512;&#25104;&#35757;&#32451;&#26679;&#26412;&#12289;&#21033;&#29992;&#28304;&#39046;&#22495;&#30693;&#35782;&#25913;&#36827;&#30446;&#26631;&#39046;&#22495;&#35299;&#26512;&#65292;&#20197;&#21450;&#21033;&#29992;&#26377;&#38480;&#30340;&#20154;&#24037;&#32763;&#35793;&#39044;&#31639;&#21644;&#26426;&#22120;&#32763;&#35793;&#26381;&#21153;&#26469;&#35843;&#25972;&#35299;&#26512;&#22120;&#12290;</title><link>http://arxiv.org/abs/2309.07429</link><description>&lt;p&gt;
&#22312;&#26377;&#38480;&#36164;&#28304;&#26465;&#20214;&#19979;&#30340;&#35821;&#20041;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Semantic Parsing in Limited Resource Conditions. (arXiv:2309.07429v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#26377;&#38480;&#36164;&#28304;&#26465;&#20214;&#19979;&#30340;&#35821;&#20041;&#35299;&#26512;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#20351;&#29992;&#33258;&#21160;&#25968;&#25454;&#31579;&#36873;&#12289;&#30693;&#35782;&#36801;&#31227;&#12289;&#20027;&#21160;&#23398;&#20064;&#21644;&#25345;&#32493;&#23398;&#20064;&#12290;&#20855;&#20307;&#26041;&#27861;&#21253;&#25324;&#21512;&#25104;&#35757;&#32451;&#26679;&#26412;&#12289;&#21033;&#29992;&#28304;&#39046;&#22495;&#30693;&#35782;&#25913;&#36827;&#30446;&#26631;&#39046;&#22495;&#35299;&#26512;&#65292;&#20197;&#21450;&#21033;&#29992;&#26377;&#38480;&#30340;&#20154;&#24037;&#32763;&#35793;&#39044;&#31639;&#21644;&#26426;&#22120;&#32763;&#35793;&#26381;&#21153;&#26469;&#35843;&#25972;&#35299;&#26512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#26465;&#20214;&#19979;&#30340;&#35821;&#20041;&#35299;&#26512;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#33258;&#21160;&#25968;&#25454;&#31579;&#36873;&#12289;&#30693;&#35782;&#36801;&#31227;&#12289;&#20027;&#21160;&#23398;&#20064;&#21644;&#25345;&#32493;&#23398;&#20064;&#31561;&#25216;&#26415;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#27809;&#26377;&#24179;&#34892;&#35757;&#32451;&#25968;&#25454;&#30340;&#20219;&#21153;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20174;&#32467;&#26500;&#21270;&#25968;&#25454;&#24211;&#27169;&#24335;&#29983;&#25104;&#21512;&#25104;&#35757;&#32451;&#26679;&#26412;&#30340;&#26041;&#27861;&#12290;&#24403;&#28304;&#39046;&#22495;&#26377;&#22823;&#37327;&#25968;&#25454;&#20294;&#30446;&#26631;&#39046;&#22495;&#21482;&#26377;&#26377;&#38480;&#30340;&#24179;&#34892;&#25968;&#25454;&#26102;&#65292;&#21033;&#29992;&#28304;&#39046;&#22495;&#30340;&#30693;&#35782;&#26469;&#25913;&#36827;&#30446;&#26631;&#39046;&#22495;&#30340;&#35299;&#26512;&#12290;&#23545;&#20110;&#30446;&#26631;&#35821;&#35328;&#20013;&#26377;&#38480;&#25968;&#25454;&#30340;&#22810;&#35821;&#35328;&#24773;&#20917;&#65292;&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#38480;&#30340;&#20154;&#24037;&#32763;&#35793;&#39044;&#31639;&#26469;&#35843;&#25972;&#35299;&#26512;&#22120;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#36873;&#25321;&#28304;&#35821;&#35328;&#26679;&#26412;&#36827;&#34892;&#25163;&#21160;&#32763;&#35793;&#65292;&#26368;&#22823;&#21270;&#30446;&#26631;&#35821;&#35328;&#35299;&#26512;&#22120;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#26412;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#26426;&#22120;&#32763;&#35793;&#26381;&#21153;&#65292;&#24182;&#36741;&#20197;&#20154;&#24037;&#32763;&#35793;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This thesis explores challenges in semantic parsing, specifically focusing on scenarios with limited data and computational resources. It offers solutions using techniques like automatic data curation, knowledge transfer, active learning, and continual learning.  For tasks with no parallel training data, the thesis proposes generating synthetic training examples from structured database schemas. When there is abundant data in a source domain but limited parallel data in a target domain, knowledge from the source is leveraged to improve parsing in the target domain.  For multilingual situations with limited data in the target languages, the thesis introduces a method to adapt parsers using a limited human translation budget. Active learning is applied to select source-language samples for manual translation, maximizing parser performance in the target language. In addition, an alternative method is also proposed to utilize machine translation services, supplemented by human-translated d
&lt;/p&gt;</description></item><item><title>ChatGPT&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#26426;&#22120;&#32763;&#35793;&#33021;&#21147;&#65292;&#20294;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#36164;&#28304;&#27700;&#24179;&#26159;&#20915;&#23450;&#20854;&#32763;&#35793;&#33021;&#21147;&#30340;&#26368;&#37325;&#35201;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2309.07423</link><description>&lt;p&gt;
ChatGPT MT: &#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#65288;&#20294;&#19981;&#26159;&#20302;&#36164;&#28304;&#35821;&#35328;&#65289;&#30340;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
ChatGPT MT: Competitive for High- (but not Low-) Resource Languages. (arXiv:2309.07423v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07423
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#26426;&#22120;&#32763;&#35793;&#33021;&#21147;&#65292;&#20294;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#36164;&#28304;&#27700;&#24179;&#26159;&#20915;&#23450;&#20854;&#32763;&#35793;&#33021;&#21147;&#30340;&#26368;&#37325;&#35201;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38544;&#24335;&#22320;&#23398;&#20064;&#25191;&#34892;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#65292;&#21253;&#25324;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;LLMs&#22312;MT&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#35768;&#22810;&#35821;&#35328;&#65292;&#20854;&#26368;&#36817;&#30340;LLM MT&#24615;&#33021;&#23578;&#26410;&#35780;&#20272;&#12290;&#22312;&#27809;&#26377;&#21457;&#34920;&#30340;&#23454;&#39564;&#35777;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#19990;&#30028;&#19978;&#22810;&#26679;&#21270;&#35821;&#35328;&#30340;&#20351;&#29992;&#32773;&#38590;&#20197;&#30693;&#36947;&#22914;&#20309;&#20197;&#21450;&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;LLMs&#36827;&#34892;&#35813;&#35821;&#35328;&#30340;&#32763;&#35793;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#39318;&#20010;&#23454;&#39564;&#35777;&#25454;&#65292;&#28085;&#30422;&#20102;204&#31181;&#35821;&#35328;&#65292;&#20197;&#21450;&#20351;&#29992;FLORES-200&#22522;&#20934;&#36827;&#34892;&#30340;MT&#25104;&#26412;&#20998;&#26512;&#12290;&#36235;&#21183;&#34920;&#26126;GPT&#27169;&#22411;&#22312;&#26576;&#20123;&#39640;&#36164;&#28304;&#35821;&#35328;&#65288;HRLs&#65289;&#30340;&#24615;&#33021;&#25509;&#36817;&#25110;&#36229;&#36807;&#20256;&#32479;MT&#27169;&#22411;&#65292;&#20294;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;LRLs&#65289;&#26041;&#38754;&#19968;&#30452;&#34920;&#29616;&#19981;&#20339;&#65292;&#25105;&#20204;&#25152;&#30740;&#31350;&#30340;&#35821;&#35328;&#20013;&#26377;84.1%&#30340;&#35821;&#35328;&#22312;&#20256;&#32479;MT&#19979;&#25928;&#26524;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#35821;&#35328;&#30340;&#36164;&#28304;&#27700;&#24179;&#26159;&#30830;&#23450;ChatGPT&#30456;&#23545;&#33021;&#21147;&#30340;&#26368;&#37325;&#35201;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs' MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world's diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language's resource level is the most important feature in determining ChatGPT's relative ability to transl
&lt;/p&gt;</description></item><item><title>PromptASR&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#25552;&#31034;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#21487;&#25511;&#39118;&#26684;&#30340;&#35821;&#22659;&#21270;&#35821;&#38899;&#36716;&#24405;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#20351;&#29992;&#21069;&#19968;&#35805;&#35821;&#30340;&#30495;&#23454;&#25991;&#26412;&#20316;&#20026;&#20869;&#23481;&#25552;&#31034;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;ASR&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#38405;&#35835;&#22270;&#20070;&#25968;&#25454;&#38598;&#21644;&#20869;&#37096;&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#33719;&#24471;&#20102;21.9&#65285;&#21644;6.8&#65285;&#30340;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#12290;&#27492;&#22806;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#37319;&#29992;&#21333;&#35789;&#32423;&#20559;&#32622;&#21015;&#34920;&#20316;&#20026;&#25552;&#31034;&#26469;&#25552;&#39640;&#23545;&#32597;&#35265;&#21333;&#35789;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#31995;&#32479;&#36824;&#21487;&#20197;&#20351;&#29992;&#39069;&#22806;&#30340;&#26679;&#24335;&#25552;&#31034;&#26469;&#24341;&#23548;ASR&#31995;&#32479;&#36755;&#20986;&#19981;&#21516;&#39118;&#26684;&#30340;&#36716;&#24405;&#12290;</title><link>http://arxiv.org/abs/2309.07414</link><description>&lt;p&gt;
&#20855;&#26377;&#21487;&#25511;&#39118;&#26684;&#30340;&#35821;&#22659;&#21270;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;PromptASR
&lt;/p&gt;
&lt;p&gt;
PromptASR for contextualized ASR with controllable style. (arXiv:2309.07414v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07414
&lt;/p&gt;
&lt;p&gt;
PromptASR&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#25552;&#31034;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#21487;&#25511;&#39118;&#26684;&#30340;&#35821;&#22659;&#21270;&#35821;&#38899;&#36716;&#24405;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#20351;&#29992;&#21069;&#19968;&#35805;&#35821;&#30340;&#30495;&#23454;&#25991;&#26412;&#20316;&#20026;&#20869;&#23481;&#25552;&#31034;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;ASR&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#38405;&#35835;&#22270;&#20070;&#25968;&#25454;&#38598;&#21644;&#20869;&#37096;&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#33719;&#24471;&#20102;21.9&#65285;&#21644;6.8&#65285;&#30340;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#12290;&#27492;&#22806;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#37319;&#29992;&#21333;&#35789;&#32423;&#20559;&#32622;&#21015;&#34920;&#20316;&#20026;&#25552;&#31034;&#26469;&#25552;&#39640;&#23545;&#32597;&#35265;&#21333;&#35789;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#31995;&#32479;&#36824;&#21487;&#20197;&#20351;&#29992;&#39069;&#22806;&#30340;&#26679;&#24335;&#25552;&#31034;&#26469;&#24341;&#23548;ASR&#31995;&#32479;&#36755;&#20986;&#19981;&#21516;&#39118;&#26684;&#30340;&#36716;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#65292;&#25552;&#31034;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#20027;&#39064;&#25110;&#36923;&#36753;&#20851;&#31995;&#31561;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PromptASR&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#25552;&#31034;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;E2E ASR&#65289;&#31995;&#32479;&#20013;&#65292;&#20197;&#23454;&#29616;&#20855;&#26377;&#21487;&#25511;&#39118;&#26684;&#30340;&#35821;&#22659;&#21270;&#35821;&#38899;&#36716;&#24405;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#22320;&#65292;&#19987;&#29992;&#25991;&#26412;&#32534;&#30721;&#22120;&#23545;&#25991;&#26412;&#25552;&#31034;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#36890;&#36807;&#36328;&#20004;&#31181;&#27169;&#24577;&#30340;&#29305;&#24449;&#20132;&#20114;&#23558;&#32534;&#30721;&#27880;&#20837;&#21040;&#35821;&#38899;&#32534;&#30721;&#22120;&#20013;&#12290;&#24403;&#20351;&#29992;&#21069;&#38754;&#35805;&#35821;&#30340;&#30495;&#23454;&#25991;&#26412;&#20316;&#20026;&#20869;&#23481;&#25552;&#31034;&#26102;&#65292;&#19982;&#22522;&#32447;ASR&#31995;&#32479;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#22312;&#38405;&#35835;&#22270;&#20070;&#25968;&#25454;&#38598;&#21644;&#20869;&#37096;&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;21.9&#65285;&#21644;6.8&#65285;&#30340;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#12290;&#31995;&#32479;&#36824;&#21487;&#20197;&#37319;&#29992;&#21333;&#35789;&#32423;&#20559;&#32622;&#21015;&#34920;&#20316;&#20026;&#25552;&#31034;&#65292;&#20197;&#25552;&#39640;&#23545;&#32597;&#35265;&#21333;&#35789;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;&#36824;&#21487;&#20197;&#32473;&#25991;&#26412;&#32534;&#30721;&#22120;&#25552;&#20379;&#39069;&#22806;&#30340;&#26679;&#24335;&#25552;&#31034;&#65292;&#24182;&#24341;&#23548;ASR&#31995;&#32479;&#36755;&#20986;&#19981;&#21516;&#39118;&#26684;&#30340;&#36716;&#24405;&#12290;&#20195;&#30721;&#21487;&#20379;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompts are crucial to large language models as they provide context information such as topic or logical relationships. Inspired by this, we propose PromptASR, a framework that integrates prompts in end-to-end automatic speech recognition (E2E ASR) systems to achieve contextualized ASR with controllable style of transcriptions. Specifically, a dedicated text encoder encodes the text prompts and the encodings are injected into the speech encoder by cross-attending the features from two modalities. When using the ground truth text from preceding utterances as content prompt, the proposed system achieves 21.9% and 6.8% relative word error rate reductions on a book reading dataset and an in-house dataset compared to a baseline ASR system. The system can also take word-level biasing lists as prompt to improve recognition accuracy on rare words. An additional style prompt can be given to the text encoder and guide the ASR system to output different styles of transcriptions. The code is avai
&lt;/p&gt;</description></item><item><title>CPPF&#26159;&#19968;&#31181;&#19978;&#19979;&#25991;&#21644;&#21518;&#22788;&#29702;&#26080;&#20851;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#23427;&#25972;&#21512;&#20102;&#22810;&#20010;&#19982;&#35821;&#38899;&#35782;&#21035;&#30456;&#20851;&#30340;ASR&#25991;&#26412;&#22788;&#29702;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#21270;&#27969;&#31243;&#12289;&#36991;&#20813;&#32423;&#32852;&#38169;&#35823;&#20256;&#25773;&#19988;&#35782;&#21035;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#25439;&#22833;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.07413</link><description>&lt;p&gt;
CPPF: &#19968;&#31181;&#19978;&#19979;&#25991;&#21644;&#21518;&#22788;&#29702;&#26080;&#20851;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CPPF: A contextual and post-processing-free model for automatic speech recognition. (arXiv:2309.07413v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07413
&lt;/p&gt;
&lt;p&gt;
CPPF&#26159;&#19968;&#31181;&#19978;&#19979;&#25991;&#21644;&#21518;&#22788;&#29702;&#26080;&#20851;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#23427;&#25972;&#21512;&#20102;&#22810;&#20010;&#19982;&#35821;&#38899;&#35782;&#21035;&#30456;&#20851;&#30340;ASR&#25991;&#26412;&#22788;&#29702;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#21270;&#27969;&#31243;&#12289;&#36991;&#20813;&#32423;&#32852;&#38169;&#35823;&#20256;&#25773;&#19988;&#35782;&#21035;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#25439;&#22833;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;ASR&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25991;&#26412;&#36755;&#20986;&#36890;&#24120;&#38656;&#35201;&#32463;&#36807;&#21518;&#22788;&#29702;&#20219;&#21153;&#25165;&#33021;&#23454;&#38469;&#21033;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;LLMs&#21644;Whisper&#30340;&#22810;&#26041;&#38754;&#33021;&#21147;&#20013;&#33719;&#24471;&#21551;&#21457;&#65292;&#19987;&#27880;&#20110;&#23558;&#19982;&#35821;&#38899;&#35782;&#21035;&#30456;&#20851;&#30340;&#22810;&#20010;ASR&#25991;&#26412;&#22788;&#29702;&#20219;&#21153;&#25972;&#21512;&#21040;ASR&#27169;&#22411;&#20013;&#12290;&#36825;&#31181;&#25972;&#21512;&#19981;&#20165;&#32553;&#30701;&#20102;&#22810;&#38454;&#27573;&#30340;&#27969;&#31243;&#65292;&#36824;&#38450;&#27490;&#20102;&#32423;&#32852;&#38169;&#35823;&#30340;&#20256;&#25773;&#65292;&#20174;&#32780;&#30452;&#25509;&#29983;&#25104;&#21518;&#22788;&#29702;&#36807;&#30340;&#25991;&#26412;&#12290;&#26412;&#30740;&#31350;&#37325;&#28857;&#20851;&#27880;&#19982;ASR&#30456;&#20851;&#30340;&#22788;&#29702;&#20219;&#21153;&#65292;&#21253;&#25324;&#19978;&#19979;&#25991;ASR&#21644;&#22810;&#20010;ASR&#21518;&#22788;&#29702;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CPPF&#27169;&#22411;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#21644;&#39640;&#25928;&#30340;ASR&#22788;&#29702;&#26367;&#20195;&#26041;&#26696;&#12290;CPPF&#26080;&#32541;&#22320;&#25972;&#21512;&#20102;&#36825;&#20123;&#20219;&#21153;&#65292;&#32780;&#19988;&#35782;&#21035;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#26126;&#26174;&#30340;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
ASR systems have become increasingly widespread in recent years. However, their textual outputs often require post-processing tasks before they can be practically utilized. To address this issue, we draw inspiration from the multifaceted capabilities of LLMs and Whisper, and focus on integrating multiple ASR text processing tasks related to speech recognition into the ASR model. This integration not only shortens the multi-stage pipeline, but also prevents the propagation of cascading errors, resulting in direct generation of post-processed text. In this study, we focus on ASR-related processing tasks, including Contextual ASR and multiple ASR post processing tasks. To achieve this objective, we introduce the CPPF model, which offers a versatile and highly effective alternative to ASR processing. CPPF seamlessly integrates these tasks without any significant loss in recognition performance.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#29616;&#26377;&#30340;&#32447;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;LRNN&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LRNN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#37197;&#22791;&#20102;&#19968;&#20010;&#22359;&#23545;&#35282;&#19988;&#36755;&#20837;&#30456;&#20851;&#30340;&#36716;&#31227;&#30697;&#38453;&#65292;&#24182;&#19988;&#26159;&#21807;&#19968;&#19968;&#20010;&#33021;&#22815;&#22312;&#27491;&#21017;&#35821;&#35328;&#20219;&#21153;&#19978;&#36827;&#34892;&#38271;&#24230;&#22806;&#25512;&#30340;LRNN&#12290;</title><link>http://arxiv.org/abs/2309.07412</link><description>&lt;p&gt;
&#22312;&#32447;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#25512;&#36827;&#27491;&#21017;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Advancing Regular Language Reasoning in Linear Recurrent Neural Networks. (arXiv:2309.07412v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07412
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#29616;&#26377;&#30340;&#32447;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;LRNN&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LRNN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#37197;&#22791;&#20102;&#19968;&#20010;&#22359;&#23545;&#35282;&#19988;&#36755;&#20837;&#30456;&#20851;&#30340;&#36716;&#31227;&#30697;&#38453;&#65292;&#24182;&#19988;&#26159;&#21807;&#19968;&#19968;&#20010;&#33021;&#22815;&#22312;&#27491;&#21017;&#35821;&#35328;&#20219;&#21153;&#19978;&#36827;&#34892;&#38271;&#24230;&#22806;&#25512;&#30340;LRNN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#32447;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;LRNN&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#24314;&#27169;&#21644;&#38271;&#31243;&#24314;&#27169;&#20013;&#21462;&#24471;&#20102;&#19982;Transformer&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#24555;&#36895;&#30340;&#24182;&#34892;&#35757;&#32451;&#21644;&#24658;&#23450;&#30340;&#25512;&#29702;&#25104;&#26412;&#12290;&#22312;&#23545;LRNN&#37325;&#26032;&#20135;&#29983;&#20852;&#36259;&#30340;&#21516;&#26102;&#65292;&#25105;&#20204;&#30740;&#31350;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#23398;&#20064;&#35757;&#32451;&#24207;&#21015;&#20013;&#30340;&#38544;&#34255;&#35268;&#21017;&#65292;&#20363;&#22914;&#27491;&#21017;&#35821;&#35328;&#30340;&#35821;&#27861;&#32467;&#26500;&#12290;&#25105;&#20204;&#23545;&#19968;&#20123;&#24050;&#26377;&#30340;LRNN&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#27491;&#21017;&#35821;&#35328;&#19978;&#23384;&#22312;&#38480;&#21046;&#12290;&#22312;&#36825;&#20010;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LRNN&#65292;&#37197;&#22791;&#20102;&#19968;&#20010;&#22359;&#23545;&#35282;&#21644;&#36755;&#20837;&#30456;&#20851;&#30340;&#36716;&#31227;&#30697;&#38453;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#26159;&#21807;&#19968;&#19968;&#20010;&#33021;&#22815;&#22312;&#27491;&#21017;&#35821;&#35328;&#20219;&#21153;&#65288;&#22914;&#27714;&#21644;&#12289;&#20598;&#25968;&#23545;&#12289;&#27169;&#36816;&#31639;&#31561;&#65289;&#19978;&#36827;&#34892;&#38271;&#24230;&#22806;&#25512;&#30340;LRNN&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent studies, linear recurrent neural networks (LRNNs) have achieved Transformer-level performance in natural language modeling and long-range modeling while offering rapid parallel training and constant inference costs. With the resurged interest in LRNNs, we study whether they can learn the hidden rules in training sequences, such as the grammatical structures of regular language. We theoretically analyze some existing LRNNs and discover their limitations on regular language. Motivated by the analysis, we propose a new LRNN equipped with a block-diagonal and input-dependent transition matrix. Experiments suggest that the proposed model is the only LRNN that can perform length extrapolation on regular language tasks such as Sum, Even Pair, and Modular Arithmetic.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#26080;&#30417;&#30563;&#23545;&#27604;&#21477;&#23376;&#23884;&#20837;&#23398;&#20064;&#65292;&#24182;&#20174;&#21435;&#20559;&#35265;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;DebCSE&#26041;&#27861;&#12290;&#36890;&#36807;&#28040;&#38500;&#21508;&#31181;&#20559;&#24046;&#65292;&#21253;&#25324;&#35789;&#39057;&#20559;&#24046;&#12289;&#21477;&#23376;&#38271;&#24230;&#20559;&#24046;&#21644;&#20551;&#36127;&#26679;&#26412;&#20559;&#24046;&#65292;DebCSE&#26088;&#22312;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#21477;&#23376;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2309.07396</link><description>&lt;p&gt;
DebCSE: &#20197;&#21435;&#20559;&#35265;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#26080;&#30417;&#30563;&#23545;&#27604;&#21477;&#23376;&#23884;&#20837;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DebCSE: Rethinking Unsupervised Contrastive Sentence Embedding Learning in the Debiasing Perspective. (arXiv:2309.07396v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#26080;&#30417;&#30563;&#23545;&#27604;&#21477;&#23376;&#23884;&#20837;&#23398;&#20064;&#65292;&#24182;&#20174;&#21435;&#20559;&#35265;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;DebCSE&#26041;&#27861;&#12290;&#36890;&#36807;&#28040;&#38500;&#21508;&#31181;&#20559;&#24046;&#65292;&#21253;&#25324;&#35789;&#39057;&#20559;&#24046;&#12289;&#21477;&#23376;&#38271;&#24230;&#20559;&#24046;&#21644;&#20551;&#36127;&#26679;&#26412;&#20559;&#24046;&#65292;DebCSE&#26088;&#22312;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#21477;&#23376;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#25351;&#20986;&#65292;&#35789;&#39057;&#20559;&#24046;&#20250;&#23548;&#33268;Bert&#27169;&#22411;&#23398;&#20064;&#21040;&#26080;&#27861;&#21306;&#20998;&#30340;&#21477;&#23376;&#23884;&#20837;&#12290;&#19968;&#20123;&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#65292;&#22914;SimCSE&#21644;ConSERT&#65292;&#24050;&#25104;&#21151;&#29992;&#20110;&#25913;&#21892;&#26080;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#30340;&#36136;&#37327;&#65292;&#20943;&#23569;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#20250;&#24341;&#20837;&#26032;&#30340;&#20559;&#24046;&#65292;&#22914;&#21477;&#23376;&#38271;&#24230;&#20559;&#24046;&#21644;&#20551;&#36127;&#26679;&#26412;&#20559;&#24046;&#65292;&#36825;&#20123;&#20559;&#24046;&#38459;&#30861;&#20102;&#27169;&#22411;&#23398;&#20064;&#26356;&#31934;&#32454;&#30340;&#35821;&#20041;&#12290;&#26412;&#25991;&#20174;&#21435;&#20559;&#35265;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#23545;&#27604;&#21477;&#23376;&#23884;&#20837;&#23398;&#20064;&#30340;&#25361;&#25112;&#65292;&#24182;&#35748;&#20026;&#26377;&#25928;&#28040;&#38500;&#21508;&#31181;&#20559;&#24046;&#23545;&#20110;&#23398;&#20064;&#39640;&#36136;&#37327;&#21477;&#23376;&#23884;&#20837;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#25152;&#26377;&#36825;&#20123;&#20559;&#24046;&#37117;&#26159;&#30001;&#20110;&#23545;&#27604;&#23398;&#20064;&#20013;&#26500;&#24314;&#35757;&#32451;&#25968;&#25454;&#30340;&#31616;&#21333;&#35268;&#21017;&#24341;&#20837;&#30340;&#65292;&#23545;&#27604;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#30340;&#20851;&#38190;&#26159;&#22312;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#20013;&#27169;&#25311;&#35757;&#32451;&#25968;&#25454;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several prior studies have suggested that word frequency biases can cause the Bert model to learn indistinguishable sentence embeddings. Contrastive learning schemes such as SimCSE and ConSERT have already been adopted successfully in unsupervised sentence embedding to improve the quality of embeddings by reducing this bias. However, these methods still introduce new biases such as sentence length bias and false negative sample bias, that hinders model's ability to learn more fine-grained semantics. In this paper, we reexamine the challenges of contrastive sentence embedding learning from a debiasing perspective and argue that effectively eliminating the influence of various biases is crucial for learning high-quality sentence embeddings. We think all those biases are introduced by simple rules for constructing training data in contrastive learning and the key for contrastive learning sentence embedding is to mimic the distribution of training data in supervised machine learning in uns
&lt;/p&gt;</description></item><item><title>VDialogUE&#26159;&#19968;&#20010;&#29992;&#20110;&#35270;&#35273;&#23548;&#21521;&#23545;&#35805;&#30340;&#32479;&#19968;&#35780;&#20272;&#22522;&#20934;&#65292;&#23450;&#20041;&#20102;&#20116;&#20010;&#26680;&#24515;&#22810;&#27169;&#24577;&#23545;&#35805;&#20219;&#21153;&#65292;&#24182;&#28085;&#30422;&#20102;&#20845;&#20010;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;VDscore&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;VISIT&#30340;&#22522;&#32447;&#27169;&#22411;&#26469;&#20419;&#36827;&#36890;&#29992;&#22810;&#27169;&#24577;&#23545;&#35805;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.07387</link><description>&lt;p&gt;
VDialogUE: &#19968;&#20010;&#29992;&#20110;&#35270;&#35273;&#23548;&#21521;&#23545;&#35805;&#30340;&#32479;&#19968;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
VDialogUE: A Unified Evaluation Benchmark for Visually-grounded Dialogue. (arXiv:2309.07387v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07387
&lt;/p&gt;
&lt;p&gt;
VDialogUE&#26159;&#19968;&#20010;&#29992;&#20110;&#35270;&#35273;&#23548;&#21521;&#23545;&#35805;&#30340;&#32479;&#19968;&#35780;&#20272;&#22522;&#20934;&#65292;&#23450;&#20041;&#20102;&#20116;&#20010;&#26680;&#24515;&#22810;&#27169;&#24577;&#23545;&#35805;&#20219;&#21153;&#65292;&#24182;&#28085;&#30422;&#20102;&#20845;&#20010;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;VDscore&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;VISIT&#30340;&#22522;&#32447;&#27169;&#22411;&#26469;&#20419;&#36827;&#36890;&#29992;&#22810;&#27169;&#24577;&#23545;&#35805;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#23558;&#22810;&#31181;&#27807;&#36890;&#27169;&#24335;&#65288;&#22914;&#25991;&#26412;&#21644;&#35270;&#35273;&#36755;&#20837;&#65289;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#25104;&#20026;&#19968;&#20010;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#26694;&#26550;&#23545;&#20110;&#35780;&#20272;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VDialogUE&#65292;&#19968;&#20010;&#29992;&#20110;&#35270;&#35273;&#23548;&#21521;&#23545;&#35805;&#30340;&#32479;&#19968;&#35780;&#20272;&#22522;&#20934;&#12290;&#23427;&#23450;&#20041;&#20102;&#20116;&#20010;&#26680;&#24515;&#22810;&#27169;&#24577;&#23545;&#35805;&#20219;&#21153;&#65292;&#24182;&#28085;&#30422;&#20102;&#20845;&#20010;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;VDscore&#65292;&#35813;&#25351;&#26631;&#22522;&#20110;&#23618;&#27425;&#20998;&#26512;&#36807;&#31243;&#65288;Analytic Hierarchy Process&#65292;AHP&#65289;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#22522;&#32447;&#27169;&#22411;VISIT&#65288;VISually-grounded dIalog Transformer&#65289;&#65292;&#20197;&#20419;&#36827;&#36890;&#29992;&#22810;&#27169;&#24577;&#23545;&#35805;&#31995;&#32479;&#30340;&#36827;&#23637;&#12290;&#23427;&#36880;&#27493;&#26500;&#24314;&#23427;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Visually-grounded dialog systems, which integrate multiple modes of communication such as text and visual inputs, have become an increasingly popular area of investigation. However, the absence of a standardized evaluation framework poses a challenge in assessing the development of this field. To this end, we propose \textbf{VDialogUE}, a \textbf{V}isually-grounded \textbf{Dialog}ue benchmark for \textbf{U}nified \textbf{E}valuation. It defines five core multi-modal dialogue tasks and covers six datasets. Furthermore, in order to provide a comprehensive assessment of the model's performance across all tasks, we developed a novel evaluation metric called VDscore, which is based on the Analytic Hierarchy Process~(AHP) method. Additionally, we present a straightforward yet efficient baseline model, named \textbf{VISIT}~(\textbf{VIS}ually-grounded d\textbf{I}alog \textbf{T}ransformer), to promote the advancement of general multi-modal dialogue systems. It progressively builds its multi-mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#26694;&#26550;&#29992;&#20110;&#23545;&#26032;&#38395;&#23186;&#20307;&#26469;&#28304;&#36827;&#34892;&#29305;&#24449;&#20998;&#26512;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#24418;&#20998;&#26512;&#27169;&#22411;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#27934;&#23519;&#21147;&#65292;&#21487;&#20197;&#24555;&#36895;&#26816;&#27979;&#34394;&#20551;&#21644;&#26377;&#20559;&#35265;&#30340;&#26032;&#38395;&#23186;&#20307;&#12290;</title><link>http://arxiv.org/abs/2309.07384</link><description>&lt;p&gt;
&#19968;&#20010;&#20132;&#20114;&#24335;&#26694;&#26550;&#29992;&#20110;&#23545;&#26032;&#38395;&#23186;&#20307;&#26469;&#28304;&#36827;&#34892;&#29305;&#24449;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Interactive Framework for Profiling News Media Sources. (arXiv:2309.07384v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#26694;&#26550;&#29992;&#20110;&#23545;&#26032;&#38395;&#23186;&#20307;&#26469;&#28304;&#36827;&#34892;&#29305;&#24449;&#20998;&#26512;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#24418;&#20998;&#26512;&#27169;&#22411;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#27934;&#23519;&#21147;&#65292;&#21487;&#20197;&#24555;&#36895;&#26816;&#27979;&#34394;&#20551;&#21644;&#26377;&#20559;&#35265;&#30340;&#26032;&#38395;&#23186;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#20852;&#36215;&#23548;&#33268;&#20102;&#22823;&#37327;&#34394;&#20551;&#21644;&#26377;&#20559;&#35265;&#30340;&#26032;&#38395;&#20256;&#25773;&#65292;&#21363;&#20197;&#24433;&#21709;&#20449;&#20208;&#20026;&#30446;&#30340;&#30340;&#20869;&#23481;&#21457;&#24067;&#12290;&#34429;&#28982;&#26816;&#27979;&#21644;&#23545;&#20256;&#25773;&#36825;&#20123;&#26032;&#38395;&#30340;&#26469;&#28304;&#36827;&#34892;&#29305;&#24449;&#20998;&#26512;&#23545;&#20110;&#32500;&#25252;&#19968;&#20010;&#20581;&#24247;&#30340;&#31038;&#20250;&#24456;&#37325;&#35201;&#65292;&#20294;&#23545;&#20110;&#33258;&#21160;&#21270;&#31995;&#32479;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#26694;&#26550;&#29992;&#20110;&#26032;&#38395;&#23186;&#20307;&#30340;&#29305;&#24449;&#20998;&#26512;&#65292;&#23558;&#22522;&#20110;&#22270;&#30340;&#26032;&#38395;&#23186;&#20307;&#29305;&#24449;&#20998;&#26512;&#27169;&#22411;&#12289;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#27934;&#23519;&#21147;&#30456;&#32467;&#21512;&#65292;&#20197;&#34920;&#24449;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#31038;&#20250;&#29615;&#22659;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36890;&#36807;&#20165;&#38656;&#20116;&#27425;&#20154;&#31867;&#20132;&#20114;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#24555;&#36895;&#26816;&#27979;&#20986;&#34394;&#20551;&#21644;&#26377;&#20559;&#35265;&#30340;&#26032;&#38395;&#23186;&#20307;&#65292;&#29978;&#33267;&#22312;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#26032;&#38395;&#20107;&#20214;&#20986;&#29616;&#26102;&#65292;&#20854;&#20013;&#30340;&#27979;&#35797;&#25968;&#25454;&#26159;&#26410;&#30693;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent rise of social media has led to the spread of large amounts of fake and biased news, content published with the intent to sway beliefs. While detecting and profiling the sources that spread this news is important to maintain a healthy society, it is challenging for automated systems.  In this paper, we propose an interactive framework for news media profiling. It combines the strengths of graph based news media profiling models, Pre-trained Large Language Models, and human insight to characterize the social context on social media. Experimental results show that with as little as 5 human interactions, our framework can rapidly detect fake and biased news media, even in the most challenging settings of emerging news events, where test data is unseen.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20808;&#25552;&#21462;&#20851;&#38190;&#21477;&#23376;&#20877;&#36827;&#34892;&#35780;&#20272;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#25991;&#26723;&#25688;&#35201;&#35780;&#20272;&#20013;&#36935;&#21040;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#21644;&#24573;&#35270;&#37325;&#35201;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#26174;&#33879;&#38477;&#20302;&#20102;&#35780;&#20272;&#25104;&#26412;&#65292;&#32780;&#19988;&#19982;&#20154;&#24037;&#35780;&#20272;&#26377;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#26368;&#20339;&#25991;&#26723;&#38271;&#24230;&#21644;&#21477;&#23376;&#25552;&#21462;&#26041;&#27861;&#30340;&#23454;&#29992;&#24314;&#35758;&#65292;&#20026;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2309.07382</link><description>&lt;p&gt;
&#38271;&#25991;&#26412;&#25688;&#35201;&#35780;&#20272;&#20013;&#8220;&#23569;&#21363;&#26159;&#22810;&#8221;&#30340;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Less is More for Long Document Summary Evaluation by LLMs. (arXiv:2309.07382v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07382
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20808;&#25552;&#21462;&#20851;&#38190;&#21477;&#23376;&#20877;&#36827;&#34892;&#35780;&#20272;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#25991;&#26723;&#25688;&#35201;&#35780;&#20272;&#20013;&#36935;&#21040;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#21644;&#24573;&#35270;&#37325;&#35201;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#26174;&#33879;&#38477;&#20302;&#20102;&#35780;&#20272;&#25104;&#26412;&#65292;&#32780;&#19988;&#19982;&#20154;&#24037;&#35780;&#20272;&#26377;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#26368;&#20339;&#25991;&#26723;&#38271;&#24230;&#21644;&#21477;&#23376;&#25552;&#21462;&#26041;&#27861;&#30340;&#23454;&#29992;&#24314;&#35758;&#65292;&#20026;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25688;&#35201;&#35780;&#20272;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#35832;&#22914;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#38271;&#25991;&#26723;&#20013;&#37325;&#35201;&#20449;&#24687;&#34987;&#24573;&#35270;&#30340;&#8220;&#36855;&#22833;&#22312;&#20013;&#38388;&#8221;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#8220;&#20808;&#25552;&#21462;&#20877;&#35780;&#20272;&#8221;&#65292;&#35813;&#26041;&#27861;&#28041;&#21450;&#20174;&#38271;&#25991;&#26412;&#28304;&#25991;&#20214;&#20013;&#25552;&#21462;&#20851;&#38190;&#21477;&#23376;&#65292;&#28982;&#21518;&#36890;&#36807;&#25552;&#38382;LLMs&#26469;&#35780;&#20272;&#25688;&#35201;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#26174;&#33879;&#38477;&#20302;&#20102;&#35780;&#20272;&#25104;&#26412;&#65292;&#32780;&#19988;&#19982;&#20154;&#24037;&#35780;&#20272;&#20043;&#38388;&#23384;&#22312;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#26368;&#20339;&#25991;&#26723;&#38271;&#24230;&#21644;&#21477;&#23376;&#25552;&#21462;&#26041;&#27861;&#30340;&#23454;&#29992;&#24314;&#35758;&#65292;&#20026;&#22522;&#20110;LLMs&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#25104;&#26412;&#25928;&#30410;&#26356;&#39640;&#19988;&#26356;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown promising performance in summary evaluation tasks, yet they face challenges such as high computational costs and the Lost-in-the-Middle problem where important information in the middle of long documents is often overlooked. To address these issues, this paper introduces a novel approach, Extract-then-Evaluate, which involves extracting key sentences from a long source document and then evaluating the summary by prompting LLMs. The results reveal that the proposed method not only significantly reduces evaluation costs but also exhibits a higher correlation with human evaluations. Furthermore, we provide practical recommendations for optimal document length and sentence extraction methods, contributing to the development of cost-effective yet more accurate methods for LLM-based text generation evaluation.
&lt;/p&gt;</description></item><item><title>&#21322;&#28151;&#21512;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#36890;&#36807;&#20998;&#31163;&#22768;&#23398;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#23545;&#20256;&#32479;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#25216;&#26415;&#30340;&#21033;&#29992;&#12290;&#22312;&#20351;&#29992;&#22495;&#22806;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#26102;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#33719;&#24471;21\%&#30340;&#35789;&#38169;&#35823;&#29575;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.07369</link><description>&lt;p&gt;
&#21322;&#28151;&#21512;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#29992;&#20110;&#39640;&#25928;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Hybrid Attention-based Encoder-decoder Model for Efficient Language Model Adaptation. (arXiv:2309.07369v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07369
&lt;/p&gt;
&lt;p&gt;
&#21322;&#28151;&#21512;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#36890;&#36807;&#20998;&#31163;&#22768;&#23398;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#23545;&#20256;&#32479;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#25216;&#26415;&#30340;&#21033;&#29992;&#12290;&#22312;&#20351;&#29992;&#22495;&#22806;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#26102;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#33719;&#24471;21\%&#30340;&#35789;&#38169;&#35823;&#29575;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#24191;&#27867;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#31471;&#21040;&#31471;&#26041;&#24335;&#20013;&#32852;&#21512;&#20248;&#21270;&#22768;&#23398;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25991;&#26412;&#36866;&#24212;&#24615;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#65292;&#26377;&#25928;&#12289;&#24555;&#36895;&#21644;&#24265;&#20215;&#22320;&#36866;&#24212;&#25991;&#26412;&#24050;&#25104;&#20026;&#22312;&#24037;&#19994;&#20013;&#37096;&#32626;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31995;&#32479;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#21363;&#21322;&#28151;&#21512;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#20445;&#30041;&#20102;&#20256;&#32479;&#28151;&#21512;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#27169;&#22359;&#21270;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#21322;&#28151;&#21512;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#23558;&#22768;&#23398;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#20998;&#31163;&#65292;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#20256;&#32479;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#25216;&#26415;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#20351;&#29992;&#22495;&#22806;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#21322;&#28151;&#21512;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#22312;&#35789;&#38169;&#35823;&#29575;&#19978;&#23454;&#29616;&#20102;21\%&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#22312;&#24120;&#35268;&#27979;&#35797;&#38598;&#19978;&#30340;&#35789;&#38169;&#35823;&#29575;&#21482;&#26377;&#36731;&#24494;&#30340;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-based encoder-decoder (AED) speech recognition model has been widely successful in recent years. However, the joint optimization of acoustic model and language model in end-to-end manner has created challenges for text adaptation. In particular, effectively, quickly and inexpensively adapting text has become a primary concern for deploying AED systems in industry. To address this issue, we propose a novel model, the hybrid attention-based encoder-decoder (HAED) speech recognition model that preserves the modularity of conventional hybrid automatic speech recognition systems. Our HAED model separates the acoustic and language models, allowing for the use of conventional text-based language model adaptation techniques. We demonstrate that the proposed HAED model yields 21\% Word Error Rate (WER) improvements in relative when out-of-domain text data is used for language model adaptation, and with only a minor degradation in WER on a general test set compared with conventional AE
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#22312;&#36777;&#35770;&#20889;&#20316;&#20013;&#20998;&#31867;&#29702;&#24819;&#30340;&#25512;&#29702;&#20462;&#25913;&#30340;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#20004;&#31181;&#26041;&#27861;&#26469;&#21033;&#29992;&#36741;&#21161;&#20462;&#35746;&#25968;&#25454;&#30340;&#26469;&#28304;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.07334</link><description>&lt;p&gt;
&#20174;&#36741;&#21161;&#26469;&#28304;&#20013;&#23398;&#20064;&#22312;&#36777;&#35770;&#20462;&#35746;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning from Auxiliary Sources in Argumentative Revision Classification. (arXiv:2309.07334v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07334
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#22312;&#36777;&#35770;&#20889;&#20316;&#20013;&#20998;&#31867;&#29702;&#24819;&#30340;&#25512;&#29702;&#20462;&#25913;&#30340;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#20004;&#31181;&#26041;&#27861;&#26469;&#21033;&#29992;&#36741;&#21161;&#20462;&#35746;&#25968;&#25454;&#30340;&#26469;&#28304;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#27169;&#22411;&#26469;&#20998;&#31867;&#36777;&#35770;&#20889;&#20316;&#20013;&#24076;&#26395;&#25913;&#36827;&#30340;&#25512;&#29702;&#20462;&#35746;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#31181;&#26041;&#27861; - &#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064; - &#21033;&#29992;&#31867;&#20284;&#20219;&#21153;&#30340;&#20462;&#35746;&#25968;&#25454;&#30340;&#36741;&#21161;&#26469;&#28304;&#12290;&#20869;&#22312;&#21644;&#22806;&#22312;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#30830;&#23454;&#21487;&#20197;&#25552;&#39640;&#20998;&#31867;&#22120;&#24615;&#33021;&#65292;&#36229;&#36807;&#22522;&#32447;&#12290;&#23613;&#31649;&#22810;&#20219;&#21153;&#23398;&#20064;&#26174;&#31034;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#26469;&#28304;&#19978;&#36827;&#34892;&#35757;&#32451;&#21487;&#33021;&#20250;&#25552;&#39640;&#24615;&#33021;&#65292;&#36801;&#31227;&#23398;&#20064;&#26356;&#22909;&#22320;&#34920;&#31034;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop models to classify desirable reasoning revisions in argumentative writing. We explore two approaches -- multi-task learning and transfer learning -- to take advantage of auxiliary sources of revision data for similar tasks. Results of intrinsic and extrinsic evaluations show that both approaches can indeed improve classifier performance over baselines. While multi-task learning shows that training on different sources of data at the same time may improve performance, transfer-learning better represents the relationship between the data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#35270;&#35282;&#26469;&#35299;&#37322;&#21464;&#21387;&#22120;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#38416;&#26126;&#20102;&#23618;&#24402;&#19968;&#21270;&#22914;&#20309;&#38480;&#21046;&#28508;&#22312;&#29305;&#24449;&#24182;&#22312;&#36229;&#29699;&#38754;&#19978;&#22609;&#36896;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#25506;&#27979;&#39044;&#35757;&#32451;&#30340;GPT-2&#27169;&#22411;&#39564;&#35777;&#20102;&#35813;&#35270;&#35282;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#21464;&#21387;&#22120;&#30340;&#30452;&#35266;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.07315</link><description>&lt;p&gt;
&#26053;&#34892;&#35789;&#65306;&#19968;&#31181;&#21464;&#21387;&#22120;&#30340;&#20960;&#20309;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traveling Words: A Geometric Interpretation of Transformers. (arXiv:2309.07315v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#35270;&#35282;&#26469;&#35299;&#37322;&#21464;&#21387;&#22120;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#38416;&#26126;&#20102;&#23618;&#24402;&#19968;&#21270;&#22914;&#20309;&#38480;&#21046;&#28508;&#22312;&#29305;&#24449;&#24182;&#22312;&#36229;&#29699;&#38754;&#19978;&#22609;&#36896;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#25506;&#27979;&#39044;&#35757;&#32451;&#30340;GPT-2&#27169;&#22411;&#39564;&#35777;&#20102;&#35813;&#35270;&#35282;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#21464;&#21387;&#22120;&#30340;&#30452;&#35266;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#29702;&#35299;&#20854;&#20869;&#37096;&#26426;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#20309;&#35270;&#35282;&#65292;&#38416;&#26126;&#20102;&#21464;&#21387;&#22120;&#25805;&#20316;&#30340;&#20869;&#37096;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#35828;&#26126;&#20102;&#23618;&#24402;&#19968;&#21270;&#22914;&#20309;&#23558;&#28508;&#22312;&#29305;&#24449;&#38480;&#21046;&#22312;&#19968;&#20010;&#36229;&#29699;&#38754;&#19978;&#65292;&#20174;&#32780;&#20351;&#27880;&#24847;&#21147;&#33021;&#22815;&#22312;&#35813;&#34920;&#38754;&#19978;&#22609;&#36896;&#21333;&#35789;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#36825;&#31181;&#20960;&#20309;&#35270;&#28857;&#26080;&#32541;&#22320;&#36830;&#25509;&#20102;&#36845;&#20195;&#25913;&#36827;&#21644;&#19978;&#19979;&#25991;&#23884;&#20837;&#31561;&#24050;&#30693;&#23646;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25506;&#27979;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;124M&#21442;&#25968;&#30340;GPT-2&#27169;&#22411;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#26089;&#26399;&#23618;&#20013;&#28165;&#26224;&#30340;&#26597;&#35810;-&#38190;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#24182;&#22312;&#26356;&#28145;&#30340;&#23618;&#27425;&#19978;&#24314;&#31435;&#22312;&#20808;&#21069;&#20851;&#20110;&#27880;&#24847;&#22836;&#30340;&#19987;&#38376;&#24615;&#30340;&#35266;&#23519;&#22522;&#30784;&#19978;&#12290;&#21033;&#29992;&#36825;&#20123;&#20960;&#20309;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#21464;&#21387;&#22120;&#30340;&#30452;&#35266;&#29702;&#35299;&#65292;&#23558;&#20854;&#25551;&#32472;&#20026;&#22609;&#36896;&#36712;&#36857;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have significantly advanced the field of natural language processing, but comprehending their internal mechanisms remains a challenge. In this paper, we introduce a novel geometric perspective that elucidates the inner mechanisms of transformer operations. Our primary contribution is illustrating how layer normalization confines the latent features to a hyper-sphere, subsequently enabling attention to mold the semantic representation of words on this surface. This geometric viewpoint seamlessly connects established properties such as iterative refinement and contextual embeddings. We validate our insights by probing a pre-trained 124M parameter GPT-2 model. Our findings reveal clear query-key attention patterns in early layers and build upon prior observations regarding the subject-specific nature of attention heads at deeper layers. Harnessing these geometric insights, we present an intuitive understanding of transformers, depicting them as processes that model the trajec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#27861;&#20064;&#24471;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#35757;&#32451;&#30340;&#19968;&#20010;&#30701;&#26242;&#31383;&#21475;&#20869;&#65292;&#27169;&#22411;&#31361;&#28982;&#33719;&#24471;&#20102;&#35821;&#27861;&#27880;&#24847;&#32467;&#26500;(SAS)&#65292;&#24182;&#20276;&#38543;&#30528;&#25439;&#22833;&#30340;&#38497;&#23789;&#19979;&#38477;&#12290;SAS&#23545;&#38543;&#21518;&#20064;&#24471;&#35821;&#35328;&#33021;&#21147;&#36215;&#21040;&#20102;&#37325;&#35201;&#30340;&#20419;&#36827;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.07311</link><description>&lt;p&gt;
&#25439;&#22833;&#31361;&#28982;&#19979;&#38477;&#65306;&#35821;&#27861;&#20064;&#24471;&#12289;&#30456;&#21464;&#21644;MLM&#20013;&#30340;&#31616;&#21270;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs. (arXiv:2309.07311v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#27861;&#20064;&#24471;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#35757;&#32451;&#30340;&#19968;&#20010;&#30701;&#26242;&#31383;&#21475;&#20869;&#65292;&#27169;&#22411;&#31361;&#28982;&#33719;&#24471;&#20102;&#35821;&#27861;&#27880;&#24847;&#32467;&#26500;(SAS)&#65292;&#24182;&#20276;&#38543;&#30528;&#25439;&#22833;&#30340;&#38497;&#23789;&#19979;&#38477;&#12290;SAS&#23545;&#38543;&#21518;&#20064;&#24471;&#35821;&#35328;&#33021;&#21147;&#36215;&#21040;&#20102;&#37325;&#35201;&#30340;&#20419;&#36827;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20013;&#30340;&#22823;&#22810;&#25968;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#20391;&#37325;&#20110;&#29702;&#35299;&#23436;&#20840;&#35757;&#32451;&#27169;&#22411;&#30340;&#34892;&#20026;&#21644;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#35266;&#23519;&#35757;&#32451;&#36807;&#31243;&#30340;&#36712;&#36857;&#65292;&#21487;&#33021;&#25165;&#33021;&#33719;&#24471;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#26576;&#20123;&#27934;&#23519;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;(MLMs)&#20013;&#30340;&#35821;&#27861;&#20064;&#24471;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20998;&#26512;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#35299;&#37322;&#24615;&#30340;&#28436;&#21270;&#26469;&#21152;&#28145;&#25105;&#20204;&#23545;&#26032;&#20852;&#34892;&#20026;&#30340;&#29702;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#27861;&#27880;&#24847;&#32467;&#26500;(SAS)&#65292;&#36825;&#26159;MLMs&#20013;&#33258;&#28982;&#24418;&#25104;&#30340;&#19968;&#20010;&#29305;&#24615;&#65292;&#20854;&#20013;&#29305;&#23450;&#30340;Transformer&#22836;&#20542;&#21521;&#20110;&#20851;&#27880;&#29305;&#23450;&#30340;&#21477;&#27861;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#35757;&#32451;&#30340;&#19968;&#20010;&#30701;&#26242;&#31383;&#21475;&#20869;&#65292;&#27169;&#22411;&#31361;&#28982;&#33719;&#24471;&#20102;SAS&#65292;&#24182;&#21457;&#29616;&#36825;&#20010;&#31383;&#21475;&#19982;&#25439;&#22833;&#30340;&#38497;&#23789;&#19979;&#38477;&#21516;&#26102;&#21457;&#29983;&#12290;&#27492;&#22806;&#65292;SAS&#20419;&#20351;&#20102;&#38543;&#21518;&#23545;&#35821;&#35328;&#33021;&#21147;&#30340;&#20064;&#24471;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#26469;&#25805;&#32437;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;SAS&#65292;&#26469;&#30740;&#31350;SAS&#30340;&#22240;&#26524;&#20316;&#29992;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process. In this paper, we present a case study of syntax acquisition in masked language models (MLMs). Our findings demonstrate how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure (SAS), a naturally emerging property of MLMs wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in training when models abruptly acquire SAS and find that this window is concurrent with a steep drop in loss. Moreover, SAS precipitates the subsequent acquisition of linguistic capabilities. We then examine the causal role of SAS by introducing a regularizer to manipulate SAS during training, and demonstrate
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#25991;&#26412;&#21069;&#23548;&#35821;&#21644;&#32844;&#19994;&#25551;&#36848;&#21477;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#21629;&#39064;&#27169;&#26495;&#21487;&#20197;&#26377;&#25928;&#25233;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#65292;&#24182;&#19988;&#19981;&#20250;&#23545;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#20135;&#29983;&#26126;&#26174;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.07251</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32972;&#26223;&#20559;&#35265;&#25233;&#21046;
&lt;/p&gt;
&lt;p&gt;
In-Contextual Bias Suppression for Large Language Models. (arXiv:2309.07251v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07251
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#21069;&#23548;&#35821;&#21644;&#32844;&#19994;&#25551;&#36848;&#21477;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#21629;&#39064;&#27169;&#26495;&#21487;&#20197;&#26377;&#25928;&#25233;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#65292;&#24182;&#19988;&#19981;&#20250;&#23545;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#20135;&#29983;&#26126;&#26174;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#24050;&#26377;&#30740;&#31350;&#25253;&#21578;&#31216;&#20854;&#23384;&#22312;&#20196;&#20154;&#25285;&#24551;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#38656;&#35201;&#20154;&#24037;&#26631;&#27880;&#31034;&#20363;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;LLM&#30340;&#24494;&#35843;&#30340;&#21435;&#20559;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#12290;&#27492;&#22806;&#65292;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#36827;&#34892;&#21435;&#20559;&#25152;&#38656;&#30340;&#20869;&#37096;&#21442;&#25968;&#65292;&#22914;&#21830;&#29992;LLM&#65288;&#22914;GPT-4&#65289;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21435;&#20559;&#26367;&#20195;&#26041;&#27861;&#65292;&#31216;&#20026;&#20559;&#35265;&#25233;&#21046;&#65292;&#23427;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#25163;&#21160;&#35774;&#35745;&#30340;&#21453;&#20107;&#23454;&#21629;&#39064;&#27169;&#26495;&#29983;&#25104;&#30340;&#25991;&#26412;&#21069;&#23548;&#35821;&#21487;&#20197;&#20934;&#30830;&#22320;&#25233;&#21046;LLM&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#32844;&#19994;&#30340;&#25551;&#36848;&#21477;&#21487;&#20197;&#36827;&#19968;&#27493;&#25233;&#21046;&#24615;&#21035;&#20559;&#35265;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20559;&#35265;&#25233;&#21046;&#23545;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#19981;&#21033;&#24433;&#21709;&#65292;&#21516;&#26102;&#26377;&#25928;&#32531;&#35299;&#20102;&#24615;&#21035;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their impressive performance in a wide range of NLP tasks, Large Language Models (LLMs) have been reported to encode worrying-levels of gender bias. Prior work has proposed debiasing methods that require human labelled examples, data augmentation and fine-tuning of the LLMs, which are computationally costly. Moreover, one might not even have access to the internal parameters for performing debiasing such as in the case of commercially available LLMs such as GPT-4. To address this challenge we propose bias suppression, a novel alternative to debiasing that does not require access to model parameters. We show that text-based preambles, generated from manually designed templates covering counterfactual statements, can accurately suppress gender biases in LLMs. Moreover, we find that descriptive sentences for occupations can further suppress gender biases. Interestingly, we find that bias suppression has a minimal adverse effect on downstream task performance, while effectively mit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26412;&#20307;&#23545;&#40784;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#26377;&#28508;&#21147;&#22312;&#35880;&#24910;&#30340;&#26694;&#26550;&#21644;&#25552;&#31034;&#35774;&#35745;&#19979;&#36229;&#36234;&#29616;&#26377;&#30340;&#26412;&#20307;&#23545;&#40784;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.07172</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26412;&#20307;&#23545;&#40784;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring Large Language Models for Ontology Alignment. (arXiv:2309.07172v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26412;&#20307;&#23545;&#40784;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#26377;&#28508;&#21147;&#22312;&#35880;&#24910;&#30340;&#26694;&#26550;&#21644;&#25552;&#31034;&#35774;&#35745;&#19979;&#36229;&#36234;&#29616;&#26377;&#30340;&#26412;&#20307;&#23545;&#40784;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#36817;&#30340;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65288;&#22914;GPT&#31995;&#21015;&#21644;Flan-T5&#65289;&#22312;&#26412;&#20307;&#23545;&#40784;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#29992;&#20110;&#35782;&#21035;&#26412;&#20307;&#20043;&#38388;&#30340;&#27010;&#24565;&#31561;&#20215;&#26144;&#23556;&#12290;&#20026;&#20102;&#27979;&#35797;Flan-T5-XXL&#21644;GPT-3.5-turbo&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;OAEI Bio-ML track&#30340;&#20004;&#20010;&#31561;&#20215;&#21305;&#37197;&#25968;&#25454;&#38598;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23376;&#38598;&#65292;&#24182;&#32771;&#34385;&#21040;&#27010;&#24565;&#26631;&#31614;&#21644;&#32467;&#26500;&#19978;&#19979;&#25991;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#26377;&#21487;&#33021;&#22312;&#35880;&#24910;&#30340;&#26694;&#26550;&#21644;&#25552;&#31034;&#35774;&#35745;&#30340;&#24773;&#20917;&#19979;&#65292;&#32988;&#36807;&#29616;&#26377;&#30340;&#26412;&#20307;&#23545;&#40784;&#31995;&#32479;&#22914;BERTMap&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates the applicability of recent generative Large Language Models (LLMs), such as the GPT series and Flan-T5, to ontology alignment for identifying concept equivalence mappings across ontologies. To test the zero-shot performance of Flan-T5-XXL and GPT-3.5-turbo, we leverage challenging subsets from two equivalence matching datasets of the OAEI Bio-ML track, taking into account concept labels and structural contexts. Preliminary findings suggest that LLMs have the potential to outperform existing ontology alignment systems like BERTMap, given careful framework and prompt design.
&lt;/p&gt;</description></item><item><title>&#38543;&#26426;LLMs&#26080;&#27861;&#29702;&#35299;&#35821;&#35328;&#30340;&#21407;&#22240;&#26159;&#23427;&#20204;&#26080;&#27861;&#25552;&#20379;&#21487;&#20197;&#20381;&#36182;&#30340;&#20107;&#23454;&#20449;&#24687;&#65292;&#23427;&#20204;&#23384;&#20648;&#30340;&#35821;&#35328;&#30693;&#35782;&#22475;&#34255;&#22312;&#26080;&#24847;&#20041;&#30340;&#24494;&#29305;&#24449;&#20013;&#65292;&#24182;&#22312;&#26576;&#20123;&#35821;&#35328;&#19978;&#19979;&#25991;&#20013;&#26080;&#27861;&#36827;&#34892;&#27491;&#30830;&#25512;&#29702;&#12290;&#26412;&#25991;&#24314;&#35758;&#22312;&#31526;&#21495;&#21270;&#26041;&#27861;&#20013;&#24212;&#29992;&#26377;&#25928;&#30340;&#33258;&#19979;&#32780;&#19978;&#31574;&#30053;</title><link>http://arxiv.org/abs/2309.05918</link><description>&lt;p&gt;
&#38543;&#26426;LLMs&#26080;&#27861;&#29702;&#35299;&#35821;&#35328;&#65306;&#36208;&#21521;&#31526;&#21495;&#21270;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#26412;&#20307;&#35770;&#22522;&#20110;&#30340;LLMs
&lt;/p&gt;
&lt;p&gt;
Stochastic LLMs do not Understand Language: Towards Symbolic, Explainable and Ontologically Based LLMs. (arXiv:2309.05918v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05918
&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;LLMs&#26080;&#27861;&#29702;&#35299;&#35821;&#35328;&#30340;&#21407;&#22240;&#26159;&#23427;&#20204;&#26080;&#27861;&#25552;&#20379;&#21487;&#20197;&#20381;&#36182;&#30340;&#20107;&#23454;&#20449;&#24687;&#65292;&#23427;&#20204;&#23384;&#20648;&#30340;&#35821;&#35328;&#30693;&#35782;&#22475;&#34255;&#22312;&#26080;&#24847;&#20041;&#30340;&#24494;&#29305;&#24449;&#20013;&#65292;&#24182;&#22312;&#26576;&#20123;&#35821;&#35328;&#19978;&#19979;&#25991;&#20013;&#26080;&#27861;&#36827;&#34892;&#27491;&#30830;&#25512;&#29702;&#12290;&#26412;&#25991;&#24314;&#35758;&#22312;&#31526;&#21495;&#21270;&#26041;&#27861;&#20013;&#24212;&#29992;&#26377;&#25928;&#30340;&#33258;&#19979;&#32780;&#19978;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#30475;&#26469;&#65292;&#22260;&#32469;&#25968;&#25454;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30456;&#23545;&#25104;&#21151;&#30340;&#29378;&#28909;&#26159;&#26377;&#20123;&#35823;&#23548;&#30340;&#65292;&#21407;&#22240;&#22914;&#19979;&#65306;&#65288;i&#65289;LLMs&#19981;&#33021;&#20381;&#36182;&#20110;&#20107;&#23454;&#20449;&#24687;&#65292;&#22240;&#20026;&#23545;&#20110;LLMs&#26469;&#35828;&#65292;&#25668;&#20837;&#30340;&#25152;&#26377;&#25991;&#26412;&#65288;&#20107;&#23454;&#25110;&#38750;&#20107;&#23454;&#65289;&#37117;&#26159;&#24179;&#31561;&#30340;&#65307;&#65288;ii&#65289;&#30001;&#20110;&#23427;&#20204;&#30340;&#20122;&#31526;&#21495;&#24615;&#36136;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#35821;&#35328;&#30340;&#20219;&#20309;&#8220;&#30693;&#35782;&#8221;&#37117;&#23558;&#27704;&#36828;&#22475;&#34255;&#22312;&#25968;&#21313;&#20159;&#20010;&#24494;&#29305;&#24449;&#65288;&#26435;&#37325;&#65289;&#20013;&#65292;&#20854;&#20013;&#27809;&#26377;&#19968;&#20010;&#26412;&#36523;&#26159;&#26377;&#24847;&#20041;&#30340;&#65307;&#20197;&#21450;&#65288;iii&#65289;LLMs&#22312;&#20960;&#31181;&#35821;&#35328;&#19978;&#19979;&#25991;&#20013;&#24120;&#24120;&#26080;&#27861;&#36827;&#34892;&#27491;&#30830;&#25512;&#29702;&#65288;&#22914;&#21517;&#35789;&#22797;&#21512;&#35789;&#12289;&#20849;&#35859;&#35789;&#12289;&#37327;&#35789;&#33539;&#22260;&#27169;&#31946;&#21644;&#24847;&#21521;&#24615;&#19978;&#19979;&#25991;&#65289;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30456;&#23545;&#25104;&#21151;&#19981;&#26159;&#31526;&#21495;&#19982;&#20122;&#31526;&#21495;&#20043;&#36777;&#30340;&#21453;&#26144;&#65292;&#32780;&#26159;&#22312;&#35268;&#27169;&#19978;&#24212;&#29992;&#33258;&#19979;&#32780;&#19978;&#30340;&#36870;&#21521;&#24037;&#31243;&#35821;&#35328;&#30340;&#25104;&#21151;&#31574;&#30053;&#30340;&#21453;&#26144;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#26377;&#25928;&#30340;&#33258;&#19979;&#32780;&#19978;&#31574;&#30053;&#24212;&#29992;&#20110;&#31526;&#21495;&#21270;&#26041;&#27861;&#20013;
&lt;/p&gt;
&lt;p&gt;
In our opinion the exuberance surrounding the relative success of data-driven large language models (LLMs) is slightly misguided and for several reasons (i) LLMs cannot be relied upon for factual information since for LLMs all ingested text (factual or non-factual) was created equal; (ii) due to their subsymbolic na-ture, whatever 'knowledge' these models acquire about language will always be buried in billions of microfeatures (weights), none of which is meaningful on its own; and (iii) LLMs will often fail to make the correct inferences in several linguistic contexts (e.g., nominal compounds, copredication, quantifier scope ambi-guities, intensional contexts. Since we believe the relative success of data-driven large language models (LLMs) is not a reflection on the symbolic vs. subsymbol-ic debate but a reflection on applying the successful strategy of a bottom-up reverse engineering of language at scale, we suggest in this paper applying the effective bottom-up strategy in a symbol
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#26368;&#23567;&#20449;&#24687;&#25552;&#20379;&#32473;GPT&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#38646;-shot&#23398;&#20064;&#22312;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#21644;&#23478;&#26063;&#21490;&#20449;&#24687;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#37319;&#29992;&#20102;&#20004;&#32452;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.05475</link><description>&lt;p&gt;
&#20351;&#29992;GPT&#27169;&#22411;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#21644;&#23478;&#26063;&#21490;&#30340;&#26368;&#23567;&#25351;&#23548;&#30340;&#38646;-shot&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Learning with Minimum Instruction to Extract Social Determinants and Family History from Clinical Notes using GPT Model. (arXiv:2309.05475v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#26368;&#23567;&#20449;&#24687;&#25552;&#20379;&#32473;GPT&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#38646;-shot&#23398;&#20064;&#22312;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#21644;&#23478;&#26063;&#21490;&#20449;&#24687;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#37319;&#29992;&#20102;&#20004;&#32452;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#30740;&#31350;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#12289;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#21644;&#23478;&#26063;&#21490;&#65292;&#20197;&#20102;&#35299;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#19982;&#32467;&#26500;&#21270;&#25968;&#25454;&#30456;&#32467;&#21512;&#26469;&#25913;&#21892;&#21307;&#30103;&#32467;&#26524;&#12290;&#22312;GPT&#27169;&#22411;&#21457;&#24067;&#21518;&#65292;&#35768;&#22810;&#30740;&#31350;&#24212;&#29992;GPT&#27169;&#22411;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#36825;&#20123;&#20449;&#24687;&#12290;&#19982;&#29616;&#26377;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#36890;&#36807;&#23545;GPT&#27169;&#22411;&#25552;&#20379;&#26368;&#23567;&#20449;&#24687;&#26469;&#30740;&#31350;&#38646;-shot&#23398;&#20064;&#22312;&#25552;&#21462;&#36825;&#20123;&#20449;&#24687;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#21033;&#29992;&#21311;&#21517;&#21270;&#30340;&#30495;&#23454;&#20020;&#24202;&#35760;&#24405;&#27880;&#37322;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#12289;&#21508;&#31181;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#21644;&#23478;&#26063;&#21490;&#20449;&#24687;&#12290;&#32771;&#34385;&#21040;GPT&#27169;&#22411;&#21487;&#33021;&#25552;&#20379;&#19982;&#21407;&#22987;&#25968;&#25454;&#20013;&#30340;&#25991;&#26412;&#19981;&#21516;&#30340;&#25991;&#26412;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#32452;&#35780;&#20272;&#25351;&#26631;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#35780;&#20272;&#25351;&#26631;&#21644;&#35821;&#20041;&#30456;&#20284;&#24230;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#23436;&#20840;&#20102;&#35299;&#24615;&#33021;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Demographics, Social determinants of health, and family history documented in the unstructured text within the electronic health records are increasingly being studied to understand how this information can be utilized with the structured data to improve healthcare outcomes. After the GPT models were released, many studies have applied GPT models to extract this information from the narrative clinical notes. Different from the existing work, our research focuses on investigating the zero-shot learning on extracting this information together by providing minimum information to the GPT model. We utilize de-identified real-world clinical notes annotated for demographics, various social determinants, and family history information. Given that the GPT model might provide text different from the text in the original data, we explore two sets of evaluation metrics, including the traditional NER evaluation metrics and semantic similarity evaluation metrics, to completely understand the perform
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#21333;&#35789;&#21644;&#25972;&#20010;&#25991;&#26412;&#20043;&#38388;&#30340;&#20114;&#30456;&#22686;&#24378;&#25928;&#24212;&#26469;&#25552;&#39640;&#24773;&#24863;&#20998;&#26512;&#24615;&#33021;&#30340;&#36890;&#29992;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#65292;&#24182;&#26500;&#24314;&#20102;&#26032;&#30340;&#24773;&#24863;&#25991;&#26412;&#20998;&#31867;&#21644;&#35789;&#24615;&#25968;&#25454;&#38598;&#12290;&#35813;&#27169;&#22411;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36229;&#36234;gpt-3.5-t&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.03787</link><description>&lt;p&gt;
&#32654;&#22269;&#65306;&#36890;&#29992;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#21644;&#26500;&#24314;&#26085;&#35821;&#24773;&#24863;&#25991;&#26412;&#20998;&#31867;&#21644;&#35789;&#24615;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
USA: Universal Sentiment Analysis Model &amp; Construction of Japanese Sentiment Text Classification and Part of Speech Dataset. (arXiv:2309.03787v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#21333;&#35789;&#21644;&#25972;&#20010;&#25991;&#26412;&#20043;&#38388;&#30340;&#20114;&#30456;&#22686;&#24378;&#25928;&#24212;&#26469;&#25552;&#39640;&#24773;&#24863;&#20998;&#26512;&#24615;&#33021;&#30340;&#36890;&#29992;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#65292;&#24182;&#26500;&#24314;&#20102;&#26032;&#30340;&#24773;&#24863;&#25991;&#26412;&#20998;&#31867;&#21644;&#35789;&#24615;&#25968;&#25454;&#38598;&#12290;&#35813;&#27169;&#22411;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36229;&#36234;gpt-3.5-t&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#23427;&#21253;&#25324;&#25991;&#26412;&#32423;&#24773;&#24863;&#26497;&#24615;&#20998;&#31867;&#21644;&#35789;&#32423;&#24773;&#24863;&#26497;&#24615;&#30830;&#23450;&#12290;&#36825;&#31181;&#20998;&#26512;&#25361;&#25112;&#27169;&#22411;&#22312;&#20840;&#38754;&#29702;&#35299;&#25991;&#26412;&#30340;&#21516;&#26102;&#65292;&#25552;&#21462;&#32454;&#24494;&#30340;&#20449;&#24687;&#12290;&#38543;&#30528;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20852;&#36215;&#65292;&#24773;&#24863;&#20998;&#26512;&#30340;&#26032;&#36884;&#24452;&#24471;&#20197;&#24320;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#21033;&#29992;&#21333;&#35789;&#21644;&#25972;&#20010;&#25991;&#26412;&#20043;&#38388;&#30340;&#20114;&#30456;&#22686;&#24378;&#25928;&#24212;&#65288;MRE&#65289;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#23427;&#28145;&#20837;&#25506;&#35752;&#20102;&#21333;&#35789;&#26497;&#24615;&#23545;&#25972;&#20010;&#27573;&#33853;&#30340;&#24773;&#24863;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#25903;&#25345;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26631;&#27880;&#20102;&#22235;&#20010;&#26032;&#39062;&#30340;&#24773;&#24863;&#25991;&#26412;&#20998;&#31867;&#21644;&#35789;&#24615;&#65288;SCPOS&#65289;&#25968;&#25454;&#38598;&#65292;&#22522;&#20110;&#29616;&#26377;&#30340;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#36827;&#34892;&#26500;&#24314;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#25317;&#26377;70&#20159;&#21442;&#25968;&#35268;&#27169;&#30340;&#36890;&#29992;&#24773;&#24863;&#20998;&#26512;&#65288;USA&#65289;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;gpt-3.5-t&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentiment analysis is a pivotal task in the domain of natural language processing. It encompasses both text-level sentiment polarity classification and word-level Part of Speech(POS) sentiment polarity determination. Such analysis challenges models to understand text holistically while also extracting nuanced information. With the rise of Large Language Models(LLMs), new avenues for sentiment analysis have opened. This paper proposes enhancing performance by leveraging the Mutual Reinforcement Effect(MRE) between individual words and the overall text. It delves into how word polarity influences the overarching sentiment of a passage. To support our research, we annotated four novel Sentiment Text Classification and Part of Speech(SCPOS) datasets, building upon existing sentiment classification datasets. Furthermore, we developed a Universal Sentiment Analysis(USA) model, with a 7-billion parameter size. Experimental results revealed that our model surpassed the performance of gpt-3.5-t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#36731;&#37327;&#32423;&#22522;&#20110;&#22270;&#30340;&#23884;&#20837;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#25918;&#23556;&#23398;&#25253;&#21578;&#36827;&#34892;&#22810;&#35821;&#35328;&#32467;&#26500;&#21270;&#34920;&#29616;&#12290;&#36890;&#36807;&#36830;&#25509;&#21307;&#23398;&#26415;&#35821;&#21644;&#22810;&#35821;&#35328;&#30693;&#35782;&#24211;&#65292;&#36825;&#31181;&#23884;&#20837;&#26041;&#27861;&#25581;&#31034;&#20102;&#20020;&#24202;&#26415;&#35821;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25552;&#20379;&#20102;&#23545;&#20020;&#24202;&#21307;&#29983;&#26356;&#26131;&#29702;&#35299;&#12289;&#20020;&#24202;&#26356;&#20934;&#30830;&#30340;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2309.00917</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#29992;&#20110;&#22810;&#35821;&#35328;&#32467;&#26500;&#21270;&#34920;&#29616;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Embeddings for Multi-Lingual Structured Representations of Radiology Reports. (arXiv:2309.00917v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#36731;&#37327;&#32423;&#22522;&#20110;&#22270;&#30340;&#23884;&#20837;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#25918;&#23556;&#23398;&#25253;&#21578;&#36827;&#34892;&#22810;&#35821;&#35328;&#32467;&#26500;&#21270;&#34920;&#29616;&#12290;&#36890;&#36807;&#36830;&#25509;&#21307;&#23398;&#26415;&#35821;&#21644;&#22810;&#35821;&#35328;&#30693;&#35782;&#24211;&#65292;&#36825;&#31181;&#23884;&#20837;&#26041;&#27861;&#25581;&#31034;&#20102;&#20020;&#24202;&#26415;&#35821;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25552;&#20379;&#20102;&#23545;&#20020;&#24202;&#21307;&#29983;&#26356;&#26131;&#29702;&#35299;&#12289;&#20020;&#24202;&#26356;&#20934;&#30830;&#30340;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20020;&#24202;&#25991;&#26412;&#30340;&#26041;&#24335;&#21457;&#29983;&#20102;&#37325;&#22823;&#21464;&#21270;&#12290;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#30340;&#24341;&#20837;&#23548;&#33268;&#20102;&#38024;&#23545;&#65288;&#29983;&#29289;&#65289;&#21307;&#23398;&#39046;&#22495;&#30340;&#35843;&#25972;&#65292;&#22914;PubMedBERT&#21644;ClinicalBERT&#12290;&#36825;&#20123;&#27169;&#22411;&#20381;&#36182;&#20110;&#22823;&#37327;&#23384;&#26723;&#30340;&#21307;&#23398;&#25991;&#26723;&#25968;&#25454;&#24211;&#12290;&#34429;&#28982;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#35299;&#37322;&#33021;&#21147;&#30340;&#32570;&#20047;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#38480;&#21046;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36731;&#37327;&#32423;&#22522;&#20110;&#22270;&#30340;&#23884;&#20837;&#26041;&#27861;&#65292;&#19987;&#38376;&#38024;&#23545;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#23427;&#32771;&#34385;&#20102;&#25253;&#21578;&#30340;&#32467;&#26500;&#21644;&#32452;&#25104;&#65292;&#21516;&#26102;&#36890;&#36807;&#22810;&#35821;&#35328;SNOMED&#20020;&#24202;&#26415;&#35821;&#30693;&#35782;&#24211;&#36830;&#25509;&#25253;&#21578;&#20013;&#30340;&#21307;&#23398;&#26415;&#35821;&#12290;&#29983;&#25104;&#30340;&#22270;&#23884;&#20837;&#25581;&#31034;&#20986;&#20020;&#24202;&#26415;&#35821;&#20043;&#38388;&#30340;&#28508;&#22312;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#23545;&#20020;&#24202;&#21307;&#29983;&#26356;&#26131;&#29702;&#35299;&#12289;&#20020;&#24202;&#26356;&#20934;&#30830;&#30340;&#34920;&#24449;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#23884;&#20837;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The way we analyse clinical texts has undergone major changes over the last years. The introduction of language models such as BERT led to adaptations for the (bio)medical domain like PubMedBERT and ClinicalBERT. These models rely on large databases of archived medical documents. While performing well in terms of accuracy, both the lack of interpretability and limitations to transfer across languages limit their use in clinical setting. We introduce a novel light-weight graph-based embedding method specifically catering radiology reports. It takes into account the structure and composition of the report, while also connecting medical terms in the report through the multi-lingual SNOMED Clinical Terms knowledge base. The resulting graph embedding uncovers the underlying relationships among clinical terms, achieving a representation that is better understandable for clinicians and clinically more accurate, without reliance on large pre-training datasets. We show the use of this embedding
&lt;/p&gt;</description></item><item><title>Qwen-VL&#26159;&#19968;&#31181;&#20855;&#26377;&#22810;&#21151;&#33021;&#33021;&#21147;&#30340;&#21069;&#27839;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#22312;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#12289;&#38382;&#39064;&#22238;&#31572;&#12289;&#35270;&#35273;&#23450;&#20301;&#21644;&#28789;&#27963;&#20132;&#20114;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#22312;&#25512;&#21160;&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#26041;&#38754;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2308.12966</link><description>&lt;p&gt;
Qwen-VL: &#19968;&#31181;&#20855;&#26377;&#22810;&#21151;&#33021;&#33021;&#21147;&#30340;&#21069;&#27839;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities. (arXiv:2308.12966v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12966
&lt;/p&gt;
&lt;p&gt;
Qwen-VL&#26159;&#19968;&#31181;&#20855;&#26377;&#22810;&#21151;&#33021;&#33021;&#21147;&#30340;&#21069;&#27839;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#22312;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#12289;&#38382;&#39064;&#22238;&#31572;&#12289;&#35270;&#35273;&#23450;&#20301;&#21644;&#28789;&#27963;&#20132;&#20114;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#22312;&#25512;&#21160;&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#26041;&#38754;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#21517;&#20026;Qwen-VL&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#24863;&#30693;&#21644;&#29702;&#35299;&#25991;&#26412;&#21644;&#22270;&#20687;&#12290;&#21253;&#25324;Qwen-VL&#21644;Qwen-VL-Chat&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#12289;&#38382;&#39064;&#22238;&#31572;&#12289;&#35270;&#35273;&#23450;&#20301;&#21644;&#28789;&#27963;&#20132;&#20114;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#35780;&#20272;&#33539;&#22260;&#28085;&#30422;&#20102;&#38646;&#26679;&#26412;&#23383;&#24149;&#29983;&#25104;&#12289;&#35270;&#35273;&#25110;&#25991;&#26723;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#21644; grounding &#31561;&#21508;&#31181;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;Qwen-VL&#27604;&#29616;&#26377;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#34920;&#29616;&#26356;&#20248;&#24322;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#26550;&#26500;&#12289;&#35757;&#32451;&#26041;&#27861;&#12289;&#33021;&#21147;&#21644;&#24615;&#33021;&#65292;&#24182;&#31361;&#20986;&#20102;&#23427;&#20204;&#22312;&#25512;&#21160;&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;&#20195;&#30721;&#12289;&#28436;&#31034;&#21644;&#27169;&#22411;&#21487;&#20197;&#22312;https://github.com/QwenLM/Qwen-VL&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the Qwen-VL series, a set of large-scale vision-language models designed to perceive and understand both text and images. Comprising Qwen-VL and Qwen-VL-Chat, these models exhibit remarkable performance in tasks like image captioning, question answering, visual localization, and flexible interaction. The evaluation covers a wide range of tasks including zero-shot captioning, visual or document visual question answering, and grounding. We demonstrate the Qwen-VL outperforms existing Large Vision Language Models (LVLMs). We present their architecture, training, capabilities, and performance, highlighting their contributions to advancing multimodal artificial intelligence. Code, demo and models are available at https://github.com/QwenLM/Qwen-VL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#24320;&#28304;&#24369;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#24182;&#25506;&#32034;&#20102;&#30693;&#35782;&#27880;&#20837;&#21644;&#24072;&#29983;&#26041;&#27861;&#31561;&#25216;&#26415;&#26469;&#20943;&#36731;&#20302;&#21442;&#25968;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25361;&#25112;&#24615;&#39046;&#22495;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24187;&#35273;&#38382;&#39064;&#24471;&#21040;&#20102;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2308.11764</link><description>&lt;p&gt;
Halo&#65306;&#35780;&#20272;&#21644;&#38477;&#20302;&#24320;&#28304;&#24369;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models. (arXiv:2308.11764v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#24320;&#28304;&#24369;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#24182;&#25506;&#32034;&#20102;&#30693;&#35782;&#27880;&#20837;&#21644;&#24072;&#29983;&#26041;&#27861;&#31561;&#25216;&#26415;&#26469;&#20943;&#36731;&#20302;&#21442;&#25968;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25361;&#25112;&#24615;&#39046;&#22495;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24187;&#35273;&#38382;&#39064;&#24471;&#21040;&#20102;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#12290;&#34429;&#28982;&#23545;&#20110;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#26469;&#35828;&#26041;&#20415;&#65292;&#20294;&#26159;&#19982;&#20854;&#26356;&#22823;&#35268;&#27169;&#30340;&#23545;&#24212;&#27169;&#22411;&#30456;&#27604;&#65292;&#24320;&#28304;&#30340;&#21442;&#25968;&#36739;&#23569;&#30340;LLMs&#32463;&#24120;&#20986;&#29616;&#20005;&#37325;&#24187;&#35273;&#38382;&#39064;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#27979;&#37327;&#21644;&#20943;&#23569;BLOOM 7B&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#26159;&#20844;&#24320;&#25552;&#20379;&#32473;&#30740;&#31350;&#21644;&#21830;&#19994;&#24212;&#29992;&#30340;&#24369;&#24320;&#28304;LLMs&#30340;&#20195;&#34920;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;HaloCheck&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26080;&#38656;&#30693;&#35782;&#30340;&#40657;&#30418;&#23376;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;LLMs&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#30693;&#35782;&#27880;&#20837;&#21644;&#24072;&#29983;&#26041;&#27861;&#31561;&#25216;&#26415;&#65292;&#20197;&#20943;&#36731;&#20302;&#21442;&#25968;LLMs&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#36825;&#20123;LLMs&#30340;&#25361;&#25112;&#24615;&#39046;&#22495;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP). Although convenient for research and practical applications, open-source LLMs with fewer parameters often suffer from severe hallucinations compared to their larger counterparts. This paper focuses on measuring and reducing hallucinations in BLOOM 7B, a representative of such weaker open-source LLMs that are publicly available for research and commercial applications. We introduce HaloCheck, a lightweight BlackBox knowledge-free framework designed to quantify the severity of hallucinations in LLMs. Additionally, we explore techniques like knowledge injection and teacher-student approaches to alleviate hallucinations in low-parameter LLMs. Our experiments effectively demonstrate the reduction of hallucinations in challenging domains for these LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#39033;&#36873;&#25321;&#32422;&#40065;&#24052;&#35821;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;YORC&#65292;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#25552;&#20379;&#20102;&#35813;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#32467;&#26524;&#21644;&#26356;&#39640;&#23618;&#27425;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.09768</link><description>&lt;p&gt;
YORC&#65306;&#32422;&#40065;&#24052;&#35821;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
YORC: Yoruba Reading Comprehension dataset. (arXiv:2308.09768v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#39033;&#36873;&#25321;&#32422;&#40065;&#24052;&#35821;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;YORC&#65292;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#25552;&#20379;&#20102;&#35813;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#32467;&#26524;&#21644;&#26356;&#39640;&#23618;&#27425;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;YORC&#65306;&#19968;&#20010;&#22522;&#20110;&#32422;&#40065;&#24052;&#35821;&#39640;&#20013;&#38405;&#35835;&#29702;&#35299;&#32771;&#35797;&#30340;&#26032;&#30340;&#22810;&#39033;&#36873;&#25321;&#32422;&#40065;&#24052;&#35821;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#24050;&#35757;&#32451;&#30340;&#20165;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#26469;&#25552;&#20379;&#22522;&#20934;&#32467;&#26524;&#65292;&#21516;&#26102;&#36824;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#25552;&#20379;&#20102;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we create YORC: a new multi-choice Yoruba Reading Comprehension dataset that is based on Yoruba high-school reading comprehension examination. We provide baseline results by performing cross-lingual transfer using existing English RACE dataset based on a pre-trained encoder-only model. Additionally, we provide results by prompting large language models (LLMs) like GPT-4.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21516;&#21365;&#21644;&#24322;&#21365;&#23545;&#27604;&#23398;&#20064;&#65288;IFTCL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#36866;&#24212;&#19981;&#21516;&#30340;&#27491;&#26679;&#26412;&#23545;&#29983;&#25104;&#26041;&#24335;&#65292;&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#21477;&#23376;&#34920;&#31034;&#20013;&#35299;&#20915;&#20102;&#23545;&#27604;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#35821;&#20041;&#25197;&#26354;&#21644;&#35821;&#20041;&#38388;&#38548;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10932</link><description>&lt;p&gt;
&#21516;&#21365;&#21644;&#24322;&#21365;&#21452;&#32990;&#32974;&#65306;&#21477;&#23376;&#34920;&#31034;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Identical and Fraternal Twins: Fine-Grained Semantic Contrastive Learning of Sentence Representations. (arXiv:2307.10932v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21516;&#21365;&#21644;&#24322;&#21365;&#23545;&#27604;&#23398;&#20064;&#65288;IFTCL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#36866;&#24212;&#19981;&#21516;&#30340;&#27491;&#26679;&#26412;&#23545;&#29983;&#25104;&#26041;&#24335;&#65292;&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#21477;&#23376;&#34920;&#31034;&#20013;&#35299;&#20915;&#20102;&#23545;&#27604;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#35821;&#20041;&#25197;&#26354;&#21644;&#35821;&#20041;&#38388;&#38548;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26174;&#33879;&#25552;&#39640;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#21477;&#23376;&#34920;&#31034;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#27491;&#26679;&#26412;&#19982;&#38170;&#23450;&#26679;&#26412;&#32858;&#31867;&#26469;&#21019;&#24314;&#25152;&#38656;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#20381;&#38752;&#23545;&#27604;&#30446;&#26631;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#32467;&#26524;&#65292;&#22240;&#20026;&#23427;&#26080;&#27861;&#21306;&#20998;&#27491;&#26679;&#26412;&#23545;&#20043;&#38388;&#30340;&#24494;&#23567;&#35821;&#20041;&#21464;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24120;&#35265;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#32463;&#24120;&#24341;&#20837;&#35821;&#20041;&#25197;&#26354;&#65292;&#23548;&#33268;&#27491;&#26679;&#26412;&#23545;&#20043;&#38388;&#23384;&#22312;&#35821;&#20041;&#38388;&#38548;&#12290;&#34429;&#28982;InfoNCE&#25439;&#22833;&#20989;&#25968;&#24573;&#30053;&#20102;&#35821;&#20041;&#38388;&#38548;&#65292;&#24182;&#22312;&#35757;&#32451;&#26399;&#38388;&#20248;&#20808;&#32771;&#34385;&#27491;&#26679;&#26412;&#23545;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26368;&#22823;&#21270;&#65292;&#20174;&#32780;&#23548;&#33268;&#35757;&#32451;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#19981;&#25935;&#24863;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21516;&#21365;&#21644;&#24322;&#21365;&#23545;&#27604;&#23398;&#20064;&#65288;&#31216;&#20026;IFTCL&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#21516;&#26102;&#36866;&#24212;&#19981;&#21516;&#30340;&#27491;&#26679;&#26412;&#23545;&#29983;&#25104;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The enhancement of unsupervised learning of sentence representations has been significantly achieved by the utility of contrastive learning. This approach clusters the augmented positive instance with the anchor instance to create a desired embedding space. However, relying solely on the contrastive objective can result in sub-optimal outcomes due to its inability to differentiate subtle semantic variations between positive pairs. Specifically, common data augmentation techniques frequently introduce semantic distortion, leading to a semantic margin between the positive pair. While the InfoNCE loss function overlooks the semantic margin and prioritizes similarity maximization between positive pairs during training, leading to the insensitive semantic comprehension ability of the trained model. In this paper, we introduce a novel Identical and Fraternal Twins of Contrastive Learning (named IFTCL) framework, capable of simultaneously adapting to various positive pairs generated by differ
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#23545;&#27604;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32763;&#35793;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#21521;&#27169;&#22411;&#21576;&#29616;&#27491;&#30830;&#21644;&#38169;&#35823;&#32763;&#35793;&#30340;&#31034;&#20363;&#24182;&#20351;&#29992;&#20559;&#22909;&#25439;&#22833;&#26469;&#25351;&#23548;&#27169;&#22411;&#23398;&#20064;&#65292;&#25105;&#20204;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#31934;&#35843;LLMs&#29992;&#20110;&#32763;&#35793;&#20219;&#21153;&#26041;&#38754;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2307.04408</link><description>&lt;p&gt;
TIM: &#20351;&#29992;&#23545;&#27604;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
TIM: Teaching Large Language Models to Translate with Comparison. (arXiv:2307.04408v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04408
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#23545;&#27604;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32763;&#35793;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#21521;&#27169;&#22411;&#21576;&#29616;&#27491;&#30830;&#21644;&#38169;&#35823;&#32763;&#35793;&#30340;&#31034;&#20363;&#24182;&#20351;&#29992;&#20559;&#22909;&#25439;&#22833;&#26469;&#25351;&#23548;&#27169;&#22411;&#23398;&#20064;&#65292;&#25105;&#20204;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#31934;&#35843;LLMs&#29992;&#20110;&#32763;&#35793;&#20219;&#21153;&#26041;&#38754;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#26041;&#38754;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#38656;&#35201;&#26356;&#19987;&#19994;&#30693;&#35782;&#30340;&#20219;&#21153;&#65288;&#22914;&#32763;&#35793;&#65289;&#20013;&#26377;&#26102;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#36825;&#31181;&#19981;&#36275;&#30340;&#21487;&#33021;&#21407;&#22240;&#20043;&#19968;&#26159;&#25351;&#20196;&#35843;&#25972;&#26088;&#22312;&#29983;&#25104;&#27969;&#30021;&#12289;&#36830;&#36143;&#30340;&#25991;&#26412;&#65292;&#32780;&#19981;&#21463;&#20219;&#20309;&#20219;&#21153;&#29305;&#23450;&#35201;&#27714;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#35843;&#25972;&#36739;&#23567;&#30340;LLM&#24182;&#20351;&#29992;&#36739;&#20302;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#21487;&#33021;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#20363;&#23376;&#36827;&#34892;&#23545;&#27604;&#25945;&#25480;LLMs&#23398;&#20064;&#32763;&#35793;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#21521;&#27169;&#22411;&#21576;&#29616;&#27491;&#30830;&#21644;&#38169;&#35823;&#32763;&#35793;&#30340;&#31034;&#20363;&#65292;&#24182;&#20351;&#29992;&#20559;&#22909;&#25439;&#22833;&#26469;&#25351;&#23548;&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;WMT2022&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#31934;&#35843;LLMs&#29992;&#20110;&#32763;&#35793;&#20219;&#21153;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-sourced large language models (LLMs) have demonstrated remarkable efficacy in various tasks with instruction tuning. However, these models can sometimes struggle with tasks that require more specialized knowledge such as translation. One possible reason for such deficiency is that instruction tuning aims to generate fluent and coherent text that continues from a given instruction without being constrained by any task-specific requirements. Moreover, it can be more challenging for tuning smaller LLMs with lower-quality training data. To address this issue, we propose a novel framework using examples in comparison to teach LLMs to learn translation. Our approach involves presenting the model with examples of correct and incorrect translations and using a preference loss to guide the model's learning. We evaluate our method on WMT2022 test sets and show that it outperforms existing methods. Our findings offer a new perspective on fine-tuning LLMs for translation tasks and provide a p
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#23637;&#31034;&#30340;&#20154;&#26684;&#29305;&#36136;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#37096;&#20998;LLMs&#22312;&#29305;&#23450;&#25552;&#31034;&#37197;&#32622;&#19979;&#27169;&#25311;&#30340;&#20154;&#26684;&#21487;&#38752;&#19988;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26356;&#22823;&#21644;&#32463;&#36807;&#25351;&#23548;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;LLMs&#30340;&#36755;&#20986;&#20013;&#30340;&#20154;&#26684;&#29305;&#36136;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#36827;&#34892;&#22609;&#36896;&#12290;</title><link>http://arxiv.org/abs/2307.00184</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20154;&#26684;&#29305;&#36136;
&lt;/p&gt;
&lt;p&gt;
Personality Traits in Large Language Models. (arXiv:2307.00184v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00184
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#23637;&#31034;&#30340;&#20154;&#26684;&#29305;&#36136;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#37096;&#20998;LLMs&#22312;&#29305;&#23450;&#25552;&#31034;&#37197;&#32622;&#19979;&#27169;&#25311;&#30340;&#20154;&#26684;&#21487;&#38752;&#19988;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26356;&#22823;&#21644;&#32463;&#36807;&#25351;&#23548;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;LLMs&#30340;&#36755;&#20986;&#20013;&#30340;&#20154;&#26684;&#29305;&#36136;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#36827;&#34892;&#22609;&#36896;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#20351;&#24471;&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#19988;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#25991;&#26412;&#12290;&#38543;&#30528;LLMs&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#39537;&#21160;&#23545;&#35805;&#20195;&#29702;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#35757;&#32451;&#22823;&#37327;&#20154;&#24037;&#29983;&#25104;&#30340;&#25968;&#25454;&#33719;&#24471;&#30340;&#20154;&#26684;&#29305;&#36136;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#30001;&#20110;&#20154;&#26684;&#26159;&#20915;&#23450;&#20132;&#27969;&#25928;&#26524;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#39564;&#35777;&#30340;&#24515;&#29702;&#27979;&#37327;&#27979;&#35797;&#65292;&#24182;&#23545;&#20174;&#24191;&#27867;&#20351;&#29992;&#30340;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#23637;&#31034;&#30340;&#20154;&#26684;&#29305;&#36136;&#36827;&#34892;&#37327;&#21270;&#12289;&#20998;&#26512;&#21644;&#22609;&#36896;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;1&#65289;&#26576;&#20123;LLMs&#30340;&#36755;&#20986;&#20013;&#27169;&#25311;&#30340;&#20154;&#26684;&#65288;&#22312;&#29305;&#23450;&#30340;&#25552;&#31034;&#37197;&#32622;&#19979;&#65289;&#26159;&#21487;&#38752;&#21644;&#26377;&#25928;&#30340;&#65307;2&#65289;LLM&#27169;&#25311;&#30340;&#20154;&#26684;&#30340;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#30340;&#35777;&#25454;&#23545;&#20110;&#26356;&#22823;&#30340;&#21644;&#32463;&#36807;&#25351;&#23548;&#24494;&#35843;&#30340;&#27169;&#22411;&#26356;&#24378;&#65307;3&#65289;LLM&#36755;&#20986;&#20013;&#30340;&#20154;&#26684;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#30340;&#32500;&#24230;&#36827;&#34892;&#22609;&#36896;&#65292;&#20197;&#27169;&#20223;&#29305;&#23450;&#30340;&#20154;&#26684;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant text. As LLMs increasingly power conversational agents, the synthesized personality embedded in these models by virtue of their training on large amounts of human-generated data draws attention. Since personality is an important factor determining the effectiveness of communication, we present a comprehensive method for administering validated psychometric tests and quantifying, analyzing, and shaping personality traits exhibited in text generated from widely-used LLMs. We find that: 1) personality simulated in the outputs of some LLMs (under specific prompting configurations) is reliable and valid; 2) evidence of reliability and validity of LLM-simulated personality is stronger for larger and instruction fine-tuned models; and 3) personality in LLM outputs can be shaped along desired dimensions to mimic specific personality profiles. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24320;&#21457;&#39046;&#22495;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#25972;&#21512;&#29983;&#25104;&#24335;&#29992;&#25143;&#20307;&#39564;&#30740;&#31350;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#39046;&#22495;&#29992;&#25143;&#32435;&#20837;&#21407;&#22411;&#24320;&#21457;&#30340;&#19981;&#21516;&#38454;&#27573;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#29992;&#25143;&#38656;&#27714;&#21644;&#35780;&#20272;&#29992;&#25143;&#20215;&#20540;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.16143</link><description>&lt;p&gt;
&#20026;&#24320;&#21457;&#39046;&#22495;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#32780;&#36827;&#34892;&#30340;&#29983;&#25104;&#24335;&#29992;&#25143;&#20307;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Generative User-Experience Research for Developing Domain-specific Natural Language Processing Applications. (arXiv:2306.16143v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24320;&#21457;&#39046;&#22495;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#25972;&#21512;&#29983;&#25104;&#24335;&#29992;&#25143;&#20307;&#39564;&#30740;&#31350;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#39046;&#22495;&#29992;&#25143;&#32435;&#20837;&#21407;&#22411;&#24320;&#21457;&#30340;&#19981;&#21516;&#38454;&#27573;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#29992;&#25143;&#38656;&#27714;&#21644;&#35780;&#20272;&#29992;&#25143;&#20215;&#20540;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#20307;&#39564;&#65288;UX&#65289;&#26159;&#20154;&#26426;&#20132;&#20114;&#65288;HCI&#65289;&#30740;&#31350;&#30340;&#19968;&#37096;&#20998;&#65292;&#19987;&#27880;&#20110;&#25552;&#39640;&#31995;&#32479;&#29992;&#25143;&#30340;&#30452;&#35266;&#24615;&#12289;&#36879;&#26126;&#24230;&#12289;&#31616;&#27905;&#24615;&#21644;&#20449;&#20219;&#24230;&#12290;&#22823;&#22810;&#25968;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;UX&#30740;&#31350;&#37117;&#37319;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#21363;&#27809;&#26377;&#20851;&#27880;&#29992;&#25143;&#38656;&#27714;&#65292;&#24182;&#20165;&#20165;&#23558;&#39046;&#22495;&#29992;&#25143;&#29992;&#20110;&#21487;&#29992;&#24615;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#26356;&#20856;&#22411;&#30340;UX&#26041;&#27861;&#26159;&#20808;&#38024;&#23545;&#29992;&#25143;&#30340;&#21487;&#29992;&#24615;&#36827;&#34892;&#23450;&#21046;&#65292;&#32780;&#19981;&#26159;&#39318;&#20808;&#20102;&#35299;&#29992;&#25143;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;UX&#30740;&#31350;&#25972;&#21512;&#21040;&#24320;&#21457;&#39046;&#22495;NLP&#24212;&#29992;&#20013;&#30340;&#26041;&#27861;&#12290;&#29983;&#25104;&#24335;UX&#30740;&#31350;&#23558;&#39046;&#22495;&#29992;&#25143;&#32435;&#20837;&#21407;&#22411;&#24320;&#21457;&#30340;&#21021;&#22987;&#38454;&#27573;&#65292;&#21363;&#26500;&#24605;&#21644;&#27010;&#24565;&#35780;&#20272;&#38454;&#27573;&#65292;&#20197;&#21450;&#26368;&#21518;&#19968;&#38454;&#27573;&#35780;&#20272;&#29992;&#25143;&#20215;&#20540;&#30340;&#21464;&#21270;&#12290;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25253;&#36947;&#20102;&#19968;&#20010;&#38024;&#23545;&#36807;&#31243;&#24037;&#19994;&#20013;&#26085;&#24120;&#25805;&#20316;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#20041;&#25628;&#32034;&#30340;&#23436;&#25972;&#21407;&#22411;&#24320;&#21457;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
User experience (UX) is a part of human-computer interaction (HCI) research and focuses on increasing intuitiveness, transparency, simplicity, and trust for system users. Most of the UX research for machine learning (ML) or natural language processing (NLP) focuses on a data-driven methodology, i.e., it fails to focus on users' requirements, and engages domain users mainly for usability evaluation. Moreover, more typical UX methods tailor the systems towards user usability, unlike learning about the user needs first. The paper proposes a methodology for integrating generative UX research into developing domain NLP applications. Generative UX research employs domain users at the initial stages of prototype development, i.e., ideation and concept evaluation, and the last stage for evaluating the change in user value. In the case study, we report the full-cycle prototype development of a domain-specific semantic search for daily operations in the process industry. Our case study shows tha
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#31532;&#19968;&#20010;&#22823;&#22411;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#21253;&#21547;&#27491;&#36127;&#25351;&#20196;&#30340;&#25968;&#25454;&#38598;&#21644;&#25552;&#20986;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#34913;&#37327;&#27169;&#22411;&#20135;&#29983;&#30340;&#24187;&#35273;&#12290;</title><link>http://arxiv.org/abs/2306.14565</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#25351;&#20196;&#35843;&#25972;&#26469;&#20943;&#36731;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning. (arXiv:2306.14565v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#31532;&#19968;&#20010;&#22823;&#22411;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#21253;&#21547;&#27491;&#36127;&#25351;&#20196;&#30340;&#25968;&#25454;&#38598;&#21644;&#25552;&#20986;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#34913;&#37327;&#27169;&#22411;&#20135;&#29983;&#30340;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#27169;&#24577;&#20219;&#21153;&#21462;&#24471;&#20102;&#21487;&#21916;&#30340;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#24456;&#23481;&#26131;&#22312;&#25551;&#36848;&#22270;&#20687;&#21644;&#20154;&#31867;&#25351;&#20196;&#26102;&#20135;&#29983;&#19981;&#19968;&#33268;&#30340;&#24187;&#35273;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#31532;&#19968;&#20010;&#22823;&#22411;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;LRV-Instruction&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#30001;GPT4&#29983;&#25104;&#30340;12&#19975;&#20010;&#35270;&#35273;&#25351;&#20196;&#65292;&#28085;&#30422;&#20102;16&#20010;&#24320;&#25918;&#24335;&#25351;&#20196;&#21644;&#31572;&#26696;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#20219;&#21153;&#12290;&#19982;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#27491;&#25351;&#20196;&#26679;&#26412;&#19981;&#21516;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;LRV-Instruction&#20197;&#21253;&#21547;&#26356;&#22810;&#38024;&#23545;&#26356;&#24378;&#30340;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#30340;&#27491;&#36127;&#25351;&#20196;&#12290;&#25105;&#20204;&#30340;&#36127;&#25351;&#20196;&#22312;&#20004;&#20010;&#35821;&#20041;&#23618;&#27425;&#19978;&#35774;&#35745;&#65306;&#65288;i&#65289;&#19981;&#23384;&#22312;&#20803;&#32032;&#25805;&#20316;&#21644;&#65288;ii&#65289;&#23384;&#22312;&#20803;&#32032;&#25805;&#20316;&#12290;&#20026;&#20102;&#26356;&#26377;&#25928;&#22320;&#34913;&#37327;LMM&#25152;&#20135;&#29983;&#30340;&#24187;&#35273;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;GPT4&#36741;&#21161;&#30340;&#35270;&#35273;&#25351;&#20196;&#35780;&#20272;&#65288;GAVIE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the promising progress in multi-modal tasks, current large multi-modal models (LMM) are prone to hallucinating inconsistent descriptions with respect to the associated image and human instructions. This paper addresses this issue by introducing the first large and diverse visual instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction. Our dataset consists of 120k visual instructions generated by GPT4, covering 16 vision-and-language tasks with open-ended instructions and answers. Unlike existing studies that primarily focus on positive instruction samples, we design LRV-Instruction to include both positive and negative instructions for more robust visual instruction tuning. Our negative instructions are designed at two semantic levels: (i) Nonexistent Element Manipulation and (ii) Existent Element Manipulation. To efficiently measure the hallucination generated by LMMs, we propose GPT4-Assisted Visual Instruction Evaluation (GAVIE), a novel approach to eva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;DSTC 11 Track 4&#20013;&#38024;&#23545;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#36827;&#34892;&#40065;&#26834;&#24615;&#21644;&#22810;&#35821;&#35328;&#33258;&#21160;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#25552;&#20379;&#32473;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#32447;&#65292;&#24182;&#24635;&#32467;&#20102;&#34920;&#29616;&#26368;&#20339;&#30340;&#31995;&#32479;&#21450;&#20854;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.12794</link><description>&lt;p&gt;
DSTC 11 Track 4&#20013;&#29992;&#20110;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#21644;&#22810;&#35821;&#35328;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Overview of Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems at DSTC 11 Track 4. (arXiv:2306.12794v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;DSTC 11 Track 4&#20013;&#38024;&#23545;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#36827;&#34892;&#40065;&#26834;&#24615;&#21644;&#22810;&#35821;&#35328;&#33258;&#21160;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#25552;&#20379;&#32473;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#32447;&#65292;&#24182;&#24635;&#32467;&#20102;&#34920;&#29616;&#26368;&#20339;&#30340;&#31995;&#32479;&#21450;&#20854;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#20986;&#29616;&#21644;&#24555;&#36895;&#21457;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#23545;&#35805;&#31995;&#32479;&#30340;&#30740;&#31350;&#65292;&#24182;&#38543;&#20043;&#24341;&#21457;&#20102;&#20851;&#20110;&#20854;&#33258;&#21160;&#35780;&#20272;&#30340;&#21508;&#31181;&#25361;&#25112;&#12290;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#30340;&#33258;&#21160;&#35780;&#20272;&#20316;&#20026;&#19968;&#20010;&#24320;&#25918;&#24615;&#25361;&#25112;&#24050;&#32463;&#24341;&#36215;&#20102;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#19968;&#30452;&#22312;&#21162;&#21147;&#25552;&#39640;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#19982;&#20154;&#31867;&#35780;&#20272;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#24456;&#23569;&#26377;&#23581;&#35797;&#35780;&#20272;&#23427;&#20204;&#22312;&#22810;&#20010;&#39046;&#22495;&#21644;&#32500;&#24230;&#19978;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#23427;&#20204;&#30340;&#37325;&#28857;&#20027;&#35201;&#38598;&#20013;&#20110;&#33521;&#35821;&#35821;&#35328;&#19978;&#12290;&#25152;&#26377;&#36825;&#20123;&#25361;&#25112;&#20419;&#36827;&#20102;&#24320;&#21457;&#21487;&#38752;&#30340;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#65292;&#22312;&#21508;&#31181;&#39046;&#22495;&#12289;&#32500;&#24230;&#21644;&#35821;&#35328;&#20013;&#37117;&#33021;&#22815;&#20351;&#29992;&#12290;DSTC11&#20013;&#30340;&#36825;&#20010;&#36712;&#36947;&#26159;&#20419;&#36827;&#40065;&#26834;&#21644;&#22810;&#35821;&#35328;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#30340;&#25345;&#32493;&#21162;&#21147;&#30340;&#19968;&#37096;&#20998;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25552;&#20379;&#32473;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#32447;&#65292;&#24182;&#35752;&#35770;&#20102;&#35813;&#36712;&#36947;&#30340;&#25552;&#20132;&#21644;&#32467;&#26524;&#32454;&#33410;&#12290;&#26412;&#25991;&#36824;&#24635;&#32467;&#20102;&#34920;&#29616;&#26368;&#20339;&#30340;&#31995;&#32479;&#21450;&#20854;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have triggered various challenges regarding their automatic evaluation. Automatic evaluation of open-domain dialogue systems as an open challenge has been the center of the attention of many researchers. Despite the consistent efforts to improve automatic metrics' correlations with human evaluation, there have been very few attempts to assess their robustness over multiple domains and dimensions. Also, their focus is mainly on the English language. All of these challenges prompt the development of automatic evaluation metrics that are reliable in various domains, dimensions, and languages. This track in the 11th Dialogue System Technology Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual automatic evaluation metrics. This article describes the datasets and baselines provided to participants and discusses the submission and result det
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#36138;&#24515;&#23545;&#25239;&#25915;&#20987;&#65292;&#38024;&#23545;&#22522;&#20110;&#25552;&#31034;&#30340;&#27169;&#26495;&#22312;PLMs&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#28431;&#27934;&#65292;&#36890;&#36807;&#23383;&#31526;&#32423;&#21644;&#21333;&#35789;&#32423;&#30340;&#30772;&#22351;&#26041;&#27861;&#36827;&#34892;&#25915;&#20987;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.05659</link><description>&lt;p&gt;
COVER&#65306;&#19968;&#31181;&#21551;&#21457;&#24335;&#36138;&#24515;&#23545;&#25239;&#25915;&#20987;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22522;&#20110;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
COVER: A Heuristic Greedy Adversarial Attack on Prompt-based Learning in Language Models. (arXiv:2306.05659v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#36138;&#24515;&#23545;&#25239;&#25915;&#20987;&#65292;&#38024;&#23545;&#22522;&#20110;&#25552;&#31034;&#30340;&#27169;&#26495;&#22312;PLMs&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#28431;&#27934;&#65292;&#36890;&#36807;&#23383;&#31526;&#32423;&#21644;&#21333;&#35789;&#32423;&#30340;&#30772;&#22351;&#26041;&#27861;&#36827;&#34892;&#25915;&#20987;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#26159;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20013;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#24335;&#65292;&#29305;&#21035;&#26159;&#22312;&#20687;&#23569;&#37327;&#26679;&#26412;&#22330;&#26223;&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;PLMs&#30340;&#21487;&#20449;&#24230;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#22312;&#22522;&#20110;&#27169;&#26495;&#30340;&#25552;&#31034;&#20013;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#28508;&#22312;&#30340;&#28431;&#27934;&#65292;&#21487;&#33021;&#20250;&#35823;&#23548;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#24341;&#36215;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#40657;&#30418;&#22330;&#26223;&#20013;&#25552;&#20986;&#22522;&#20110;&#25552;&#31034;&#30340;&#23545;&#25239;&#25915;&#20987;&#25163;&#27573;&#65292;&#25581;&#31034;&#20102;PLMs&#30340;&#19968;&#20123;&#28431;&#27934;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23383;&#31526;&#32423;&#21035;&#21644;&#21333;&#35789;&#32423;&#21035;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#30772;&#22351;&#25163;&#21160;&#27169;&#26495;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#19978;&#36848;&#21551;&#21457;&#24335;&#30772;&#22351;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#36138;&#24515;&#31639;&#27861;&#36827;&#34892;&#25915;&#20987;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;BERT&#31995;&#21015;&#27169;&#22411;&#30340;&#19977;&#20010;&#21464;&#31181;&#21644;&#20843;&#20010;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#20219;&#21153;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-based learning has been proved to be an effective way in pre-trained language models (PLMs), especially in low-resource scenarios like few-shot settings. However, the trustworthiness of PLMs is of paramount significance and potential vulnerabilities have been shown in prompt-based templates that could mislead the predictions of language models, causing serious security concerns. In this paper, we will shed light on some vulnerabilities of PLMs, by proposing a prompt-based adversarial attack on manual templates in black box scenarios. First of all, we design character-level and word-level heuristic approaches to break manual templates separately. Then we present a greedy algorithm for the attack based on the above heuristic destructive approaches. Finally, we evaluate our approach with the classification tasks on three variants of BERT series models and eight datasets. And comprehensive experimental results justify the effectiveness of our approach in terms of attack success rate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;K2&#65292;&#24182;&#24320;&#21457;&#20102;&#21508;&#31181;&#36164;&#28304;&#20197;&#36827;&#19968;&#27493;&#20419;&#36827;&#20854;&#22312;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#65292;&#21253;&#25324;&#31532;&#19968;&#20010;&#22320;&#29699;&#31185;&#23398;&#25945;&#23398;&#35843;&#38899;&#25968;&#25454;&#38598;GeoSignal&#21644;&#31532;&#19968;&#20010;&#22320;&#29699;&#31185;&#23398;&#22522;&#20934;&#27979;&#35797;GeoBenchmark&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#23436;&#25972;&#30340;&#26041;&#27861;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#36890;&#29992;&#39046;&#22495;LLM LLaMA-7B &#27169;&#22411;&#36866;&#24212;&#21040;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2306.05064</link><description>&lt;p&gt;
&#23398;&#20064;&#22320;&#29699;&#31185;&#23398;&#30693;&#35782;&#29702;&#35299;&#21644;&#21033;&#29992;&#30340;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning A Foundation Language Model for Geoscience Knowledge Understanding and Utilization. (arXiv:2306.05064v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;K2&#65292;&#24182;&#24320;&#21457;&#20102;&#21508;&#31181;&#36164;&#28304;&#20197;&#36827;&#19968;&#27493;&#20419;&#36827;&#20854;&#22312;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#65292;&#21253;&#25324;&#31532;&#19968;&#20010;&#22320;&#29699;&#31185;&#23398;&#25945;&#23398;&#35843;&#38899;&#25968;&#25454;&#38598;GeoSignal&#21644;&#31532;&#19968;&#20010;&#22320;&#29699;&#31185;&#23398;&#22522;&#20934;&#27979;&#35797;GeoBenchmark&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#23436;&#25972;&#30340;&#26041;&#27861;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#36890;&#29992;&#39046;&#22495;LLM LLaMA-7B &#27169;&#22411;&#36866;&#24212;&#21040;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#24120;&#35268;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#26412;&#25991;&#23558;LLM&#24341;&#20837;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#65292;&#26088;&#22312;&#25512;&#36827;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#30340;LLM&#65292;&#21629;&#21517;&#20026;K2&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#36164;&#28304;&#65292;&#20197;&#36827;&#19968;&#27493;&#20419;&#36827;LLM&#22312;&#22320;&#29699;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#20026;LLM&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#22320;&#29699;&#31185;&#23398;&#25945;&#23398;&#35843;&#38899;&#25968;&#25454;&#38598;GeoSignal&#65292;&#26088;&#22312;&#23558;LLM&#30456;&#24212;&#19982;&#22320;&#29699;&#31185;&#23398;&#30456;&#20851;&#30340;&#29992;&#25143;&#26597;&#35810;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#31532;&#19968;&#20010;&#22320;&#36136;&#31185;&#23398;&#22522;&#20934;&#27979;&#35797;GeoBenchmark&#65292;&#20197;&#22312;&#22320;&#29699;&#31185;&#23398;&#29615;&#22659;&#20013;&#35780;&#20272;LLM&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#23436;&#25972;&#30340;&#26041;&#27861;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#36890;&#29992;&#39046;&#22495;LLM&#36866;&#24212;&#21040;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#36229;&#36807;100&#19975;&#31687;&#22320;&#29699;&#31185;&#23398;&#25991;&#29486;&#19978;&#36827;&#19968;&#27493;&#35757;&#32451;&#20102;LLaMA-7B&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;GeoSignal&#30340;&#30417;&#30563;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#20139;&#20102;&#19968;&#20010;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#36801;&#31227;LLM&#30340;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs)have achieved great success in general domains of natural language processing. In this paper, we bring LLMs to the realm of geoscience, with the objective of advancing research and applications in this field. To this end, we present the first-ever LLM in geoscience, K2, alongside a suite of resources developed to further promote LLM research within geoscience. For instance, we have curated the first geoscience instruction tuning dataset, GeoSignal, which aims to align LLM responses to geoscience-related user queries. Additionally, we have established the first geoscience benchmark, GeoBenchmark, to evaluate LLMs in the context of geoscience. In this work, we experiment with a complete recipe to adapt a pretrained general-domain LLM to the geoscience domain. Specifically, we further train the LLaMA-7B model on over 1 million pieces of geoscience literature and utilize GeoSignal's supervised data to fine-tune the model. Moreover, we share a protocol that can e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#25506;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#40065;&#26834;&#30340;&#20998;&#31867;&#22120;&#12290;&#36890;&#36807;&#25506;&#27979;&#19978;&#19979;&#25991;&#21270;&#30340;&#34920;&#31034;&#26469;&#39044;&#27979;&#26631;&#31614;&#65292;&#36825;&#31181;&#26041;&#27861;&#23545;&#25351;&#20196;&#21464;&#21270;&#26356;&#21152;&#40065;&#26834;&#65292;&#24182;&#19988;&#22312;&#22810;&#26679;&#21270;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14171</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#25506;&#27979;&#65306;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25506;&#27979;&#26500;&#24314;&#40065;&#26834;&#30340;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Probing in Context: Toward Building Robust Classifiers via Probing Large Language Models. (arXiv:2305.14171v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#25506;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#40065;&#26834;&#30340;&#20998;&#31867;&#22120;&#12290;&#36890;&#36807;&#25506;&#27979;&#19978;&#19979;&#25991;&#21270;&#30340;&#34920;&#31034;&#26469;&#39044;&#27979;&#26631;&#31614;&#65292;&#36825;&#31181;&#26041;&#27861;&#23545;&#25351;&#20196;&#21464;&#21270;&#26356;&#21152;&#40065;&#26834;&#65292;&#24182;&#19988;&#22312;&#22810;&#26679;&#21270;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#22312;&#25552;&#20379;&#25351;&#20196;&#21644;&#23569;&#37327;&#27880;&#37322;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#19988;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#26377;&#24456;&#22823;&#21464;&#21270;&#65292;&#21462;&#20915;&#20110;&#25351;&#20196;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#31181;&#23545;&#19978;&#19979;&#25991;&#30340;&#20381;&#36182;&#21487;&#33021;&#20197;&#19981;&#21487;&#39044;&#27979;&#30340;&#26041;&#24335;&#34920;&#29616;&#65292;&#20363;&#22914;&#65292;&#19968;&#20010;&#30475;&#20284;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#25351;&#20196;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#26356;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#31216;&#20043;&#20026;&#19978;&#19979;&#25991;&#20013;&#30340;&#25506;&#27979;&#12290;&#31867;&#20284;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#25105;&#20204;&#29992;&#25351;&#20196;&#23545;&#36755;&#20837;&#30340;&#34920;&#31034;&#36827;&#34892;&#19978;&#19979;&#25991;&#21270;&#65292;&#20294;&#26159;&#25105;&#20204;&#19981;&#26159;&#35299;&#30721;&#36755;&#20986;&#39044;&#27979;&#65292;&#32780;&#26159;&#25506;&#27979;&#19978;&#19979;&#25991;&#21270;&#30340;&#34920;&#31034;&#26469;&#39044;&#27979;&#26631;&#31614;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#22312;&#22810;&#26679;&#21270;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19978;&#19979;&#25991;&#20013;&#30340;&#25506;&#27979;&#23545;&#25351;&#20196;&#21464;&#21270;&#26356;&#21152;&#40065;&#26834;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25506;&#27979;&#30340;&#24615;&#33021;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#31454;&#20105;&#25110;&#26356;&#32988;&#19968;&#31609;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are able to learn new tasks in context, where they are provided with instructions and a few annotated examples. However, the effectiveness of in-context learning is dependent on the provided context, and the performance on a downstream task can vary considerably, depending on the instruction. Importantly, such dependency on the context can surface in unpredictable ways, e.g., a seemingly more informative instruction might lead to a worse performance. In this paper, we propose an alternative approach, which we term in-context probing. Similar to in-context learning, we contextualize the representation of the input with an instruction, but instead of decoding the output prediction, we probe the contextualized representation to predict the label. Through a series of experiments on a diverse set of classification tasks, we show that in-context probing is significantly more robust to changes in instructions. We further show that probing performs competitive or superior
&lt;/p&gt;</description></item><item><title>PaLM 2 &#26159;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#30340;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#22810;&#35821;&#35328;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#33719;&#24471;&#20102;&#22312;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#26174;&#30528;&#30340;&#25913;&#36827;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;PaLM 2 &#36824;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#31283;&#23450;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26356;&#24191;&#27867;&#22320;&#37096;&#32626;&#65292;&#24182;&#19988;&#21487;&#20197;&#25511;&#21046;&#27602;&#24615;&#25512;&#29702;&#26102;&#38388;&#65292;&#32780;&#19981;&#20250;&#23545;&#20854;&#20182;&#33021;&#21147;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.10403</link><description>&lt;p&gt;
PaLM 2 &#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
PaLM 2 Technical Report. (arXiv:2305.10403v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10403
&lt;/p&gt;
&lt;p&gt;
PaLM 2 &#26159;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#30340;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#22810;&#35821;&#35328;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#33719;&#24471;&#20102;&#22312;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#26174;&#30528;&#30340;&#25913;&#36827;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;PaLM 2 &#36824;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#31283;&#23450;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26356;&#24191;&#27867;&#22320;&#37096;&#32626;&#65292;&#24182;&#19988;&#21487;&#20197;&#25511;&#21046;&#27602;&#24615;&#25512;&#29702;&#26102;&#38388;&#65292;&#32780;&#19981;&#20250;&#23545;&#20854;&#20182;&#33021;&#21147;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; PaLM 2&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#27604;&#20854;&#21069;&#36523; PaLM &#22312;&#22810;&#35821;&#35328;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#26356;&#21152;&#20986;&#33394;&#65292;&#24182;&#19988;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#12290;PaLM 2 &#26159;&#19968;&#31181;&#22522;&#20110; Transformer &#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#22810;&#31181;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#23545;&#33521;&#35821;&#21644;&#22810;&#35821;&#35328;&#35821;&#35328;&#20197;&#21450;&#25512;&#29702;&#20219;&#21153;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; PaLM 2 &#22312;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#20855;&#26377;&#26174;&#30528;&#30340;&#25913;&#36827;&#36136;&#37327;&#65292;&#21516;&#26102;&#23637;&#29616;&#20102;&#27604; PaLM &#26356;&#24555;&#21644;&#26356;&#26377;&#25928;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#31181;&#25913;&#36827;&#30340;&#25928;&#29575;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26356;&#24191;&#27867;&#22320;&#37096;&#32626;&#65292;&#21516;&#26102;&#20063;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26356;&#24555;&#22320;&#21709;&#24212;&#65292;&#20197;&#33719;&#24471;&#26356;&#33258;&#28982;&#30340;&#20132;&#20114;&#33410;&#22863;&#12290;PaLM 2 &#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312; BIG-Bench &#21644;&#20854;&#20182;&#25512;&#29702;&#20219;&#21153;&#19978;&#30456;&#23545;&#20110; PaLM &#26377;&#24040;&#22823;&#30340;&#25913;&#36827;&#12290;PaLM 2 &#22312;&#19968;&#22871;&#36127;&#36131;&#20154;&#30340; AI &#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#31283;&#23450;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#38468;&#21152;&#36816;&#34892;&#24320;&#38144;&#25110;&#23545;&#20854;&#20182;&#33021;&#21147;&#20135;&#29983;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23545;&#27602;&#24615;&#36827;&#34892;&#25512;&#29702;&#26102;&#38388;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture of objectives. Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Over
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#65292;&#35777;&#26126;&#20102;LLMs&#33021;&#22815;&#23436;&#25104;&#35821;&#35328;&#29702;&#35299;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24605;&#36335;&#21551;&#21457;&#20197;&#21450;&#26377;&#25928;&#25351;&#20196;&#24494;&#35843;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.09960</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#28508;&#22312;&#31354;&#38388;&#29702;&#35770;&#23545;&#24212;&#26032;&#20852;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
A Latent Space Theory for Emergent Abilities in Large Language Models. (arXiv:2304.09960v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#65292;&#35777;&#26126;&#20102;LLMs&#33021;&#22815;&#23436;&#25104;&#35821;&#35328;&#29702;&#35299;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24605;&#36335;&#21551;&#21457;&#20197;&#21450;&#26377;&#25928;&#25351;&#20196;&#24494;&#35843;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#24182;&#19981;&#26159;&#38543;&#26426;&#29983;&#25104;&#65292;&#32780;&#26159;&#20026;&#20102;&#20256;&#36882;&#20449;&#24687;&#12290;&#35821;&#35328;&#19982;&#20854;&#24213;&#23618;&#21547;&#20041;&#20043;&#38388;&#23384;&#22312;&#24378;&#28872;&#30340;&#20851;&#32852;&#65292;&#22312;&#20854;&#30456;&#20851;&#24615;&#26041;&#38754;&#26377;&#30528;&#20005;&#37325;&#20559;&#24046;&#30340;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#31232;&#30095;&#24615;&#65292;&#36825;&#20123;&#39640;&#23792;&#20540;&#24688;&#22909;&#19982;&#35821;&#35328;&#30340;&#36793;&#32536;&#20998;&#24067;&#21305;&#37197;&#12290;&#38543;&#30528;&#22823;&#25968;&#25454;&#21644;&#22823;&#27169;&#22411;&#19978;&#35757;&#32451;&#30340;LLMs&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#29616;&#22312;&#21487;&#20197;&#31934;&#30830;&#35780;&#20272;&#35821;&#35328;&#30340;&#36793;&#32536;&#20998;&#24067;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#20415;&#30340;&#25506;&#32034;&#32852;&#21512;&#20998;&#24067;&#31232;&#30095;&#32467;&#26500;&#23454;&#29616;&#26377;&#25928;&#25512;&#29702;&#30340;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#35821;&#35328;&#20998;&#31867;&#20026;&#26126;&#30830;&#19982;{\epsilon}-&#27169;&#31946;&#65292;&#24182;&#25552;&#20986;&#23450;&#37327;&#32467;&#26524;&#65292;&#20197;&#34920;&#26126;LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#65288;&#20363;&#22914;&#35821;&#35328;&#29702;&#35299;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24605;&#36335;&#21551;&#21457;&#20197;&#21450;&#26377;&#25928;&#25351;&#20196;&#24494;&#35843;&#65289;&#37117;&#21487;&#20197;&#24402;&#22240;&#20110;&#23545;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Languages are not created randomly but rather to communicate information. There is a strong association between languages and their underlying meanings, resulting in a sparse joint distribution that is heavily peaked according to their correlations. Moreover, these peak values happen to match with the marginal distribution of languages due to the sparsity. With the advent of LLMs trained on big data and large models, we can now precisely assess the marginal distribution of languages, providing a convenient means of exploring the sparse structures in the joint distribution for effective inferences. In this paper, we categorize languages as either unambiguous or {\epsilon}-ambiguous and present quantitative results to demonstrate that the emergent abilities of LLMs, such as language understanding, in-context learning, chain-of-thought prompting, and effective instruction fine-tuning, can all be attributed to Bayesian inference on the sparse joint distribution of languages.
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#25361;&#25112;&#36187;&#65288;MER 2023&#65289;&#25552;&#20986;&#20102;&#19977;&#20010;&#23376;&#25361;&#25112;&#65306;MER-MULTI&#12289;MER-NOISE&#21644;MER-SEMI&#65292;&#20026;&#20840;&#29699;&#30740;&#31350;&#20154;&#21592;&#26500;&#24314;&#21019;&#26032;&#25216;&#26415;&#25552;&#20379;&#20102;&#28608;&#21169;&#65292;&#24182;&#27979;&#35797;&#20102;&#21508;&#31181;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#25552;&#20379;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#22522;&#32447;&#65292;&#20197;&#20419;&#36827;&#40065;&#26834;&#32780;&#26377;&#25928;&#30340;&#31639;&#27861;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.08981</link><description>&lt;p&gt;
MER 2023: &#22810;&#26631;&#31614;&#23398;&#20064;&#65292;&#27169;&#24577;&#40065;&#26834;&#24615;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MER 2023: Multi-label Learning, Modality Robustness, and Semi-Supervised Learning. (arXiv:2304.08981v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08981
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#25361;&#25112;&#36187;&#65288;MER 2023&#65289;&#25552;&#20986;&#20102;&#19977;&#20010;&#23376;&#25361;&#25112;&#65306;MER-MULTI&#12289;MER-NOISE&#21644;MER-SEMI&#65292;&#20026;&#20840;&#29699;&#30740;&#31350;&#20154;&#21592;&#26500;&#24314;&#21019;&#26032;&#25216;&#26415;&#25552;&#20379;&#20102;&#28608;&#21169;&#65292;&#24182;&#27979;&#35797;&#20102;&#21508;&#31181;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#25552;&#20379;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#22522;&#32447;&#65292;&#20197;&#20419;&#36827;&#40065;&#26834;&#32780;&#26377;&#25928;&#30340;&#31639;&#27861;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#20351;&#24471;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25216;&#26415;&#38590;&#20197;&#28385;&#36275;&#23454;&#38469;&#24212;&#29992;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#21457;&#36215;&#20102;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#25361;&#25112;&#36187;&#65288;MER 2023&#65289;&#65292;&#20197;&#28608;&#21169;&#20840;&#29699;&#30740;&#31350;&#20154;&#21592;&#26500;&#24314;&#21019;&#26032;&#25216;&#26415;&#65292;&#36827;&#19968;&#27493;&#21152;&#36895;&#21644;&#20419;&#36827;&#30740;&#31350;&#12290;&#38024;&#23545;&#20170;&#24180;&#30340;&#25361;&#25112;&#36187;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#23376;&#25361;&#25112;&#65306;&#65288;1&#65289;MER-MULTI&#65292;&#21442;&#36187;&#32773;&#38656;&#35201;&#35782;&#21035;&#31163;&#25955;&#21644;&#32500;&#24230;&#24773;&#24863;&#65307;&#65288;2&#65289;MER-NOISE&#65292;&#22312;&#27979;&#35797;&#35270;&#39057;&#20013;&#28155;&#21152;&#22122;&#22768;&#65292;&#20197;&#35780;&#20272;&#27169;&#24577;&#40065;&#26834;&#24615;&#65307;&#65288;3&#65289;MER-SEMI&#65292;&#25552;&#20379;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#21508;&#31181;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#24182;&#20026;&#27599;&#20010;&#23376;&#25361;&#25112;&#25552;&#20379;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#22522;&#32447;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;MER-MULTI&#19978;&#33719;&#24471;&#20102;77.57&#65285;&#30340;F1&#20998;&#25968;&#21644;0.82&#30340;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#65292;&#22312;MER-NOISE&#19978;&#33719;&#24471;&#20102;69.82&#65285;&#30340;F1&#20998;&#25968;&#21644;0.75&#30340;MSE&#65292;&#22312;MER-SEMI&#19978;&#33719;&#24471;&#20102;69.39&#65285;&#30340;F1&#20998;&#25968;&#21644;0.80&#30340;MSE&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#25361;&#25112;&#36187;&#33021;&#22815;&#28608;&#21457;&#26356;&#22810;&#30340;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#65292;&#24182;&#20419;&#36827;&#40065;&#26834;&#32780;&#26377;&#25928;&#30340;&#31639;&#27861;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few decades, multimodal emotion recognition has made remarkable progress with the development of deep learning. However, existing technologies are difficult to meet the demand for practical applications. To improve the robustness, we launch a Multimodal Emotion Recognition Challenge (MER 2023) to motivate global researchers to build innovative technologies that can further accelerate and foster research. For this year's challenge, we present three distinct sub-challenges: (1) MER-MULTI, in which participants recognize both discrete and dimensional emotions; (2) MER-NOISE, in which noise is added to test videos for modality robustness evaluation; (3) MER-SEMI, which provides large amounts of unlabeled samples for semi-supervised learning. In this paper, we test a variety of multimodal features and provide a competitive baseline for each sub-challenge. Our system achieves 77.57% on the F1 score and 0.82 on the mean squared error (MSE) for MER-MULTI, 69.82% on the F1 score a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2212.09597</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#20316;&#20026;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#37325;&#35201;&#33021;&#21147;&#65292;&#21487;&#20197;&#20026;&#21307;&#30103;&#35786;&#26029;&#12289;&#35848;&#21028;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#21518;&#31471;&#25903;&#25345;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30740;&#31350;&#25104;&#26524;&#30340;&#27604;&#36739;&#21644;&#24635;&#32467;&#65292;&#24182;&#25552;&#20379;&#20102;&#31995;&#32479;&#36164;&#28304;&#20197;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#36164;&#28304;&#21487;&#22312; https://github.com/zjunlp/Prompt4ReasoningPapers &#19978;&#33719;&#21462;&#65288;&#23450;&#26399;&#26356;&#26032;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
&lt;/p&gt;</description></item><item><title>LambdaKG&#26159;&#19968;&#20010;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#24211;&#65292;&#25552;&#20379;&#20102;&#22810;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#25903;&#25345;&#22810;&#31181;&#20219;&#21153;&#65292;&#22914;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#12289;&#38382;&#31572;&#12289;&#25512;&#33616;&#21644;&#30693;&#35782;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2210.00305</link><description>&lt;p&gt;
LambdaKG:&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#24211;
&lt;/p&gt;
&lt;p&gt;
LambdaKG: A Library for Pre-trained Language Model-Based Knowledge Graph Embeddings. (arXiv:2210.00305v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00305
&lt;/p&gt;
&lt;p&gt;
LambdaKG&#26159;&#19968;&#20010;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#24211;&#65292;&#25552;&#20379;&#20102;&#22810;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#25903;&#25345;&#22810;&#31181;&#20219;&#21153;&#65292;&#22914;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#12289;&#38382;&#31572;&#12289;&#25512;&#33616;&#21644;&#30693;&#35782;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#36890;&#24120;&#20855;&#26377;&#24322;&#26500;&#30340;&#22270;&#32467;&#26500;&#21644;&#25991;&#26412;&#20016;&#23500;&#30340;&#23454;&#20307;/&#20851;&#31995;&#20449;&#24687;&#12290;&#22522;&#20110;&#25991;&#26412;&#30340;KG&#23884;&#20837;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#25551;&#36848;&#36827;&#34892;&#32534;&#30721;&#26469;&#34920;&#31034;&#23454;&#20307;&#65292;&#20294;&#30446;&#21069;&#23578;&#26080;&#19987;&#38376;&#20026;PLM&#19982;KG&#35774;&#35745;&#30340;&#24320;&#28304;&#24211;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LambdaKG&#65292;&#19968;&#20010;&#24102;&#26377;&#22810;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65292;BART&#65292;T5&#65292;GPT-3&#65289;&#24182;&#25903;&#25345;&#21508;&#31181;&#20219;&#21153;&#65288;&#22914;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65292;&#38382;&#31572;&#65292;&#25512;&#33616;&#21644;&#30693;&#35782;&#25506;&#32034;&#65289;&#30340;KGE&#24211;&#12290;LambdaKG&#22312;https://github.com/zjunlp/PromptKG/tree/main/lambdaKG&#19978;&#20844;&#24320;&#24320;&#28304;&#65292;&#24182;&#25552;&#20379;&#20102;&#28436;&#31034;&#35270;&#39057;&#21644;&#38271;&#26399;&#32500;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs (KGs) often have two characteristics: heterogeneous graph structure and text-rich entity/relation information. Text-based KG embeddings can represent entities by encoding descriptions with pre-trained language models, but no open-sourced library is specifically designed for KGs with PLMs at present. In this paper, we present LambdaKG, a library for KGE that equips with many pre-trained language models (e.g., BERT, BART, T5, GPT-3), and supports various tasks (e.g., knowledge graph completion, question answering, recommendation, and knowledge probing). LambdaKG is publicly open-sourced at https://github.com/zjunlp/PromptKG/tree/main/lambdaKG, with a demo video at this http URL and long-term maintenance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20174;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#29983;&#25104;SPARQL&#26597;&#35810;&#30340;&#20219;&#21153;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26032;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#24182;&#22312;DBpedia&#21644;Wikidata&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;T5&#27169;&#22411;&#22312;LC-QuAD 1.0&#21644;LC-QuAD 2.0&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#35299;&#26512;&#38656;&#35201;&#23558;&#19968;&#37096;&#20998;&#36755;&#20837;&#22797;&#21046;&#21040;&#36755;&#20986;&#26597;&#35810;&#20013;&#30340;&#38382;&#39064;&#65292;&#36825;&#20026;&#30693;&#35782;&#22270;&#35889;&#35821;&#20041;&#35299;&#26512;&#24102;&#26469;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.12793</link><description>&lt;p&gt;
SPARQL&#35821;&#20041;&#35299;&#26512;&#30340;&#29616;&#20195;&#22522;&#20934;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Modern Baselines for SPARQL Semantic Parsing. (arXiv:2204.12793v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.12793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20174;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#29983;&#25104;SPARQL&#26597;&#35810;&#30340;&#20219;&#21153;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26032;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#24182;&#22312;DBpedia&#21644;Wikidata&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;T5&#27169;&#22411;&#22312;LC-QuAD 1.0&#21644;LC-QuAD 2.0&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#35299;&#26512;&#38656;&#35201;&#23558;&#19968;&#37096;&#20998;&#36755;&#20837;&#22797;&#21046;&#21040;&#36755;&#20986;&#26597;&#35810;&#20013;&#30340;&#38382;&#39064;&#65292;&#36825;&#20026;&#30693;&#35782;&#22270;&#35889;&#35821;&#20041;&#35299;&#26512;&#24102;&#26469;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20110;&#20174;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#29983;&#25104;SPARQL&#26597;&#35810;&#30340;&#20219;&#21153;&#65292;&#36825;&#20123;&#26597;&#35810;&#21487;&#20197;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#25191;&#34892;&#12290;&#25105;&#20204;&#20551;&#35774;&#24050;&#32463;&#25552;&#20379;&#20102;&#40644;&#37329;&#23454;&#20307;&#21644;&#20851;&#31995;&#65292;&#21097;&#19979;&#30340;&#20219;&#21153;&#26159;&#23558;&#23427;&#20204;&#19982;SPARQL&#35789;&#27719;&#21644;&#36755;&#20837;&#26631;&#35760;&#19968;&#36215;&#25353;&#27491;&#30830;&#30340;&#39034;&#24207;&#25490;&#21015;&#65292;&#20197;&#29983;&#25104;&#27491;&#30830;&#30340;SPARQL&#26597;&#35810;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#65292;&#22240;&#27492;&#25105;&#20204;&#23581;&#35797;&#20102;&#20351;&#29992;BART&#12289;T5&#21644;PGNs&#65288;&#25351;&#38024;&#29983;&#25104;&#32593;&#32476;&#65289;&#19982;BERT&#23884;&#20837;&#26469;&#23547;&#25214;&#36825;&#20010;&#20219;&#21153;&#22312;PLM&#26102;&#20195;&#30340;&#26032;&#22522;&#20934;&#65292;&#22312;DBpedia&#21644;Wikidata&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;T5&#38656;&#35201;&#29305;&#27530;&#30340;&#36755;&#20837;&#26631;&#35760;&#21270;&#65292;&#20294;&#22312;LC-QuAD 1.0&#21644;LC-QuAD 2.0&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;&#20102;&#20197;&#21069;&#24037;&#20316;&#20013;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20351;&#24471;&#23545;&#38382;&#39064;&#36827;&#34892;&#35821;&#20041;&#35299;&#26512;&#25104;&#20026;&#21487;&#33021;&#65292;&#20854;&#20013;&#36755;&#20837;&#30340;&#19968;&#37096;&#20998;&#38656;&#35201;&#22797;&#21046;&#21040;&#36755;&#20986;&#26597;&#35810;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#30693;&#35782;&#22270;&#35889;&#35821;&#20041;&#35299;&#26512;&#30340;&#26032;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we focus on the task of generating SPARQL queries from natural language questions, which can then be executed on Knowledge Graphs (KGs). We assume that gold entity and relations have been provided, and the remaining task is to arrange them in the right order along with SPARQL vocabulary, and input tokens to produce the correct SPARQL query. Pre-trained Language Models (PLMs) have not been explored in depth on this task so far, so we experiment with BART, T5 and PGNs (Pointer Generator Networks) with BERT embeddings, looking for new baselines in the PLM era for this task, on DBpedia and Wikidata KGs. We show that T5 requires special input tokenisation, but produces state of the art performance on LC-QuAD 1.0 and LC-QuAD 2.0 datasets, and outperforms task-specific models from previous works. Moreover, the methods enable semantic parsing for questions where a part of the input needs to be copied to the output query, thus enabling a new paradigm in KG semantic parsing.
&lt;/p&gt;</description></item></channel></rss>