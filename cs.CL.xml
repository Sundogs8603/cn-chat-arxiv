<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25512;&#27979;&#35299;&#30721;&#26159;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#30340;&#25216;&#26415;&#65292;&#20294;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36873;&#25321;&#30340;&#33609;&#31295;&#27169;&#22411;&#29983;&#25104;&#30340;&#20196;&#29260;&#34987;&#30446;&#26631;&#27169;&#22411;&#25509;&#21463;&#30340;&#27010;&#29575;&#36234;&#39640;&#65292;&#21534;&#21520;&#37327;&#36234;&#20302;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#21508;&#31181;&#22240;&#32032;&#23545;&#25512;&#27979;&#35299;&#30721;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#27169;&#22411;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01528</link><description>&lt;p&gt;
&#35299;&#30721;&#25512;&#27979;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Decoding Speculative Decoding
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01528
&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#26159;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#30340;&#25216;&#26415;&#65292;&#20294;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36873;&#25321;&#30340;&#33609;&#31295;&#27169;&#22411;&#29983;&#25104;&#30340;&#20196;&#29260;&#34987;&#30446;&#26631;&#27169;&#22411;&#25509;&#21463;&#30340;&#27010;&#29575;&#36234;&#39640;&#65292;&#21534;&#21520;&#37327;&#36234;&#20302;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#21508;&#31181;&#22240;&#32032;&#23545;&#25512;&#27979;&#35299;&#30721;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#27169;&#22411;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#26029;&#65292;&#32780;&#19981;&#20462;&#25913;&#20854;&#32467;&#26524;&#12290;&#22312;&#23545;LLM&#36827;&#34892;&#25512;&#26029;&#26102;&#65292;&#25512;&#27979;&#35299;&#30721;&#20351;&#29992;&#36739;&#23567;&#30340;&#33609;&#31295;&#27169;&#22411;&#29983;&#25104;&#25512;&#27979;&#20196;&#29260;&#65292;&#28982;&#21518;&#20351;&#29992;&#30446;&#26631;LLM&#39564;&#35777;&#36825;&#20123;&#33609;&#31295;&#20196;&#29260;&#12290;&#25512;&#27979;&#35299;&#30721;&#25552;&#20379;&#30340;&#21152;&#36895;&#21462;&#20915;&#20110;&#33609;&#31295;&#27169;&#22411;&#30340;&#36873;&#25321;&#12290;&#26222;&#36941;&#24314;&#35758;&#36873;&#25321;&#19968;&#20010;&#33609;&#31295;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#29983;&#25104;&#30340;&#20196;&#29260;&#34987;LLM&#25509;&#21463;&#30340;&#27010;&#29575;&#24456;&#39640;&#65292;&#20197;&#23454;&#29616;&#26368;&#39640;&#21534;&#21520;&#37327;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#19982;&#20043;&#30456;&#21453;&#65292;&#38543;&#30528;&#29983;&#25104;&#30340;&#20196;&#29260;&#34987;&#30446;&#26631;&#27169;&#22411;&#25509;&#21463;&#30340;&#27010;&#29575;&#22686;&#21152;&#65292;&#21534;&#21520;&#37327;&#20943;&#23569;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#23545;&#24433;&#21709;&#25512;&#27979;&#35299;&#30721;&#30340;&#19981;&#21516;&#22240;&#32032;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#30740;&#31350;&#20102;&#36825;&#20123;&#22240;&#32032;&#22914;&#20309;&#30456;&#20114;&#20316;&#29992;&#21644;&#24433;&#21709;&#21152;&#36895;&#25928;&#26524;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#20998;&#26512;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#29992;&#35813;&#27169;&#22411;&#26469;&#36827;&#34892;&#20915;&#31574;&#65292;&#25552;&#39640;&#25512;&#27979;&#35299;&#30721;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speculative Decoding is a widely used technique to speed up inference for Large Language Models (LLMs) without modifying its outcome. When performing inference on an LLM, speculative decoding uses a smaller draft model which generates speculative tokens and then uses the target LLM to verify those draft tokens. The speedup provided by speculative decoding heavily depends on the choice of the draft model. It has been widely suggested to select a draft model that provides a high probability of the generated token being accepted by the LLM to achieve the highest throughput. However, our experiments indicate the contrary with throughput diminishing as the probability of generated tokens to be accepted by the target model increases. To understand this phenomenon, we perform extensive experiments to characterize the different factors that affect speculative decoding and how those factors interact and affect the speedups. Based on our experiments we describe an analytical model which can be u
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CodeBenchGen&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#20219;&#24847;&#20195;&#30721;&#36716;&#21270;&#20026;&#35780;&#20272;&#31034;&#20363;&#65292;&#21019;&#36896;&#20102;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#20195;&#30721;&#31034;&#20363;&#30340;&#25968;&#25454;&#38598;Exec-CSN&#65292;&#23637;&#31034;&#20102;&#20854;&#21487;&#25193;&#23637;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00566</link><description>&lt;p&gt;
CodeBenchGen: &#21019;&#24314;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CodeBenchGen: Creating Scalable Execution-based Code Generation Benchmarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00566
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CodeBenchGen&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#20219;&#24847;&#20195;&#30721;&#36716;&#21270;&#20026;&#35780;&#20272;&#31034;&#20363;&#65292;&#21019;&#36896;&#20102;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#20195;&#30721;&#31034;&#20363;&#30340;&#25968;&#25454;&#38598;Exec-CSN&#65292;&#23637;&#31034;&#20102;&#20854;&#21487;&#25193;&#23637;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20419;&#36827;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CodeBenchGen&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#21019;&#24314;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#22522;&#20934;&#65292;&#20165;&#38656;&#35201;&#36731;&#24494;&#30340;&#20154;&#31867;&#25351;&#23548;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23558;&#20219;&#24847;&#20195;&#30721;&#29255;&#27573;&#36716;&#21270;&#20026;&#35780;&#20272;&#31034;&#20363;&#65292;&#21253;&#25324;&#29992;&#20110;&#25191;&#34892;&#35780;&#20272;&#30340;&#27979;&#35797;&#29992;&#20363;&#12290;&#25105;&#20204;&#36890;&#36807;&#21019;&#24314;&#21253;&#21547;&#26469;&#33258;CodeSearchNet&#25968;&#25454;&#38598;&#30340;367&#20010;GitHub&#23384;&#20648;&#24211;&#20013;&#30340;&#20195;&#30721;&#20462;&#25913;&#30340;293&#20010;&#24211;&#30340;1,931&#20010;&#20363;&#23376;&#30340;&#25968;&#25454;&#38598;Exec-CSN&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#23637;&#31034;Exec-CSN&#20013;&#31034;&#20363;&#30340;&#22797;&#26434;&#24615;&#21644;&#21487;&#35299;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#20154;&#31867;&#30740;&#31350;&#65292;&#32467;&#26524;&#26174;&#31034;81.3%&#30340;&#20363;&#23376;&#21487;&#20197;&#34987;&#20154;&#31867;&#35299;&#20915;&#65292;61%&#34987;&#35780;&#20026;&#8220;&#38656;&#35201;&#21162;&#21147;&#35299;&#20915;&#8221;&#12290;&#25105;&#20204;&#23545;&#24320;&#28304;&#21644;&#19987;&#26377;&#27169;&#22411;&#36827;&#34892;&#20102;&#20195;&#30721;&#29983;&#25104;&#23454;&#39564;&#65292;&#24182;&#20998;&#26512;&#20102;&#20154;&#31867;&#21644;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00566v1 Announce Type: cross  Abstract: To facilitate evaluation of code generation systems across diverse scenarios, we present CodeBenchGen, a framework to create scalable execution-based benchmarks that only requires light guidance from humans. Specifically, we leverage a large language model (LLM) to convert an arbitrary piece of code into an evaluation example, including test cases for execution-based evaluation. We illustrate the usefulness of our framework by creating a dataset, Exec-CSN, which includes 1,931 examples involving 293 libraries revised from code in 367 GitHub repositories taken from the CodeSearchNet dataset. To demonstrate the complexity and solvability of examples in Exec-CSN, we present a human study demonstrating that 81.3% of the examples can be solved by humans and 61% are rated as ``requires effort to solve''. We conduct code generation experiments on open-source and proprietary models and analyze the performance of both humans and models. We will
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#23398;&#20064;&#26041;&#27861;SOTOPIA-$\pi$&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#34892;&#20026;&#20811;&#38534;&#21644;&#33258;&#25105;&#24378;&#21270;&#35757;&#32451;&#65292;&#25913;&#36827;&#20102;&#35821;&#35328;&#20195;&#29702;&#30340;&#31038;&#20132;&#26234;&#33021;&#65292;&#20351;&#20854;&#36798;&#21040;&#20102;&#19987;&#23478;&#27169;&#22411;&#30340;&#27700;&#24179;&#65292;&#24182;&#25552;&#39640;&#20102;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08715</link><description>&lt;p&gt;
SOTOPIA-$\pi$: &#20132;&#20114;&#24335;&#23398;&#20064;&#31038;&#20132;&#26234;&#33021;&#35821;&#35328;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
SOTOPIA-$\pi$: Interactive Learning of Socially Intelligent Language Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08715
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#23398;&#20064;&#26041;&#27861;SOTOPIA-$\pi$&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#34892;&#20026;&#20811;&#38534;&#21644;&#33258;&#25105;&#24378;&#21270;&#35757;&#32451;&#65292;&#25913;&#36827;&#20102;&#35821;&#35328;&#20195;&#29702;&#30340;&#31038;&#20132;&#26234;&#33021;&#65292;&#20351;&#20854;&#36798;&#21040;&#20102;&#19987;&#23478;&#27169;&#22411;&#30340;&#27700;&#24179;&#65292;&#24182;&#25552;&#39640;&#20102;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#36807;&#27169;&#20223;&#21644;&#31038;&#20132;&#20114;&#21160;&#26469;&#23398;&#20064;&#31038;&#20132;&#25216;&#33021;&#12290;&#29616;&#26377;&#30740;&#31350;&#22312;&#26500;&#24314;&#35821;&#35328;&#20195;&#29702;&#26041;&#38754;&#24456;&#23569;&#28041;&#21450;&#36825;&#31181;&#31038;&#20132;&#23398;&#20064;&#36807;&#31243;&#12290;&#21463;&#21040;&#36825;&#19968;&#31354;&#30333;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#23398;&#20064;&#26041;&#27861;SOTOPIA-$\pi$&#65292;&#25913;&#36827;&#20102;&#35821;&#35328;&#20195;&#29702;&#30340;&#31038;&#20132;&#26234;&#33021;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#34892;&#20026;&#20811;&#38534;&#21644;&#33258;&#25105;&#24378;&#21270;&#35757;&#32451;&#65292;&#26681;&#25454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#35780;&#20998;&#23545;&#32463;&#36807;&#31579;&#36873;&#30340;&#31038;&#20132;&#20114;&#21160;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#27861;&#20351;&#19968;&#20010;7B&#30340;LLM&#36798;&#21040;&#20102;&#19987;&#23478;&#27169;&#22411;(GPT-4-based agent)&#30340;&#31038;&#20132;&#30446;&#26631;&#23436;&#25104;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#35821;&#35328;&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#22312;MMLU&#22522;&#20934;&#19978;&#20445;&#25345;&#20102;&#36890;&#29992;&#30340;&#38382;&#31572;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#36825;&#31181;&#35757;&#32451;&#33539;&#24335;&#25581;&#31034;&#20102;LLM&#35780;&#20272;&#31038;&#20132;&#26234;&#33021;&#30340;&#19968;&#20123;&#22256;&#38590;&#65306;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#32773;&#39640;&#20272;&#20102;&#19987;&#38376;&#38024;&#23545;&#31038;&#20132;&#20114;&#21160;&#35757;&#32451;&#30340;&#35821;&#35328;&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08715v1 Announce Type: new  Abstract: Humans learn social skills through both imitation and social interaction. This social learning process is largely understudied by existing research on building language agents. Motivated by this gap, we propose an interactive learning method, SOTOPIA-$\pi$, improving the social intelligence of language agents. This method leverages behavior cloning and self-reinforcement training on filtered social interaction data according to large language model (LLM) ratings. We show that our training method allows a 7B LLM to reach the social goal completion ability of an expert model (GPT-4-based agent), while improving the safety of language agents and maintaining general QA ability on the MMLU benchmark. We also find that this training paradigm uncovers some difficulties in LLM-based evaluation of social intelligence: LLM-based evaluators overestimate the abilities of the language agents trained specifically for social interaction.
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25903;&#25345;&#29616;&#23454;&#19990;&#30028;&#20107;&#23454;&#26680;&#26597;&#20013;&#23637;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#33021;&#22815;&#35299;&#37322;&#24694;&#24847;&#21644;&#35823;&#23548;&#24615;&#22768;&#26126;&#30340;&#19981;&#21512;&#29702;&#20043;&#22788;&#21644;&#28508;&#22312;&#21160;&#26426;&#12290;</title><link>https://arxiv.org/abs/2403.03627</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#29616;&#23454;&#19990;&#30028;&#20107;&#23454;&#26680;&#26597;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Models to Support Real-World Fact-Checking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03627
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25903;&#25345;&#29616;&#23454;&#19990;&#30028;&#20107;&#23454;&#26680;&#26597;&#20013;&#23637;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#33021;&#22815;&#35299;&#37322;&#24694;&#24847;&#21644;&#35823;&#23548;&#24615;&#22768;&#26126;&#30340;&#19981;&#21512;&#29702;&#20043;&#22788;&#21644;&#28508;&#22312;&#21160;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#20855;&#26377;&#28508;&#21147;&#25903;&#25345;&#20154;&#31867;&#22788;&#29702;&#22823;&#37327;&#20449;&#24687;&#12290;&#34429;&#28982;MLLMs&#24050;&#32463;&#34987;&#29992;&#20316;&#20107;&#23454;&#26680;&#26597;&#24037;&#20855;&#65292;&#20294;&#23601;&#20854;&#22312;&#27492;&#26041;&#38754;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#32780;&#35328;&#65292;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#25105;&#20204;&#26088;&#22312;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#31995;&#32479;&#35780;&#20272;&#24403;&#21069;&#22810;&#27169;&#24577;&#27169;&#22411;&#20419;&#36827;&#29616;&#23454;&#19990;&#30028;&#20107;&#23454;&#26680;&#26597;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#26159;&#26080;&#38656;&#35777;&#25454;&#30340;&#65292;&#20165;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#22266;&#26377;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#35774;&#35745;&#33021;&#22815;&#25552;&#21462;&#27169;&#22411;&#39044;&#27979;&#12289;&#35299;&#37322;&#21644;&#32622;&#20449;&#27700;&#24179;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20851;&#20110;&#27169;&#22411;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#20197;&#21450;&#22833;&#36133;&#21407;&#22240;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#21457;&#29616;&#65292;(1) GPT-4V&#22312;&#35782;&#21035;&#24694;&#24847;&#21644;&#35823;&#23548;&#24615;&#22810;&#27169;&#24577;&#22768;&#26126;&#26041;&#38754;&#34920;&#29616;&#20986;&#36229;&#20961;&#24615;&#33021;&#65292;&#33021;&#22815;&#35299;&#37322;&#19981;&#21512;&#29702;&#30340;&#26041;&#38754;&#21644;&#28508;&#22312;&#21160;&#26426;&#65292;&#20197;&#21450;(2)&#29616;&#26377;&#30340;o
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03627v1 Announce Type: cross  Abstract: Multimodal large language models (MLLMs) carry the potential to support humans in processing vast amounts of information. While MLLMs are already being used as a fact-checking tool, their abilities and limitations in this regard are understudied. Here is aim to bridge this gap. In particular, we propose a framework for systematically assessing the capacity of current multimodal models to facilitate real-world fact-checking. Our methodology is evidence-free, leveraging only these models' intrinsic knowledge and reasoning capabilities. By designing prompts that extract models' predictions, explanations, and confidence levels, we delve into research questions concerning model accuracy, robustness, and reasons for failure. We empirically find that (1) GPT-4V exhibits superior performance in identifying malicious and misleading multimodal claims, with the ability to explain the unreasonable aspects and underlying motives, and (2) existing o
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#21010;&#33021;&#21147;&#26469;&#22686;&#24378;&#38271;&#26399;&#25512;&#33616;&#65292;&#20351;&#27169;&#22411;&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#20013;&#26356;&#26377;&#25928;&#22320;&#29702;&#35299;&#21644;&#24212;&#29992;&#20219;&#21153;&#35299;&#20915;&#21407;&#21017;</title><link>https://arxiv.org/abs/2403.00843</link><description>&lt;p&gt;
&#21033;&#29992;&#21452;&#23618;&#21487;&#23398;&#20064;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35268;&#21010;&#22686;&#24378;&#38271;&#26399;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Enhancing Long-Term Recommendation with Bi-level Learnable Large Language Model Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00843
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#21010;&#33021;&#21147;&#26469;&#22686;&#24378;&#38271;&#26399;&#25512;&#33616;&#65292;&#20351;&#27169;&#22411;&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#20013;&#26356;&#26377;&#25928;&#22320;&#29702;&#35299;&#21644;&#24212;&#29992;&#20219;&#21153;&#35299;&#20915;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#20542;&#21521;&#20110;&#36807;&#20998;&#36814;&#21512;&#29992;&#25143;&#30340;&#21363;&#26102;&#20852;&#36259;&#32780;&#24573;&#35270;&#20182;&#20204;&#30340;&#38271;&#26399;&#21442;&#19982;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#25512;&#33616;&#20915;&#31574;&#36807;&#31243;&#20013;&#21512;&#24182;&#35268;&#21010;&#33021;&#21147;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20197;&#24320;&#21457;&#33021;&#22815;&#21516;&#26102;&#32771;&#34385;&#21363;&#26102;&#20852;&#36259;&#21644;&#38271;&#26399;&#21442;&#19982;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#31232;&#30095;&#25968;&#25454;&#30340;&#26174;&#33879;&#35268;&#21010;&#33021;&#21147;&#29992;&#20110;&#38271;&#26399;&#25512;&#33616;&#12290;&#20851;&#38190;&#22312;&#20110;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#22330;&#26223;&#20013;&#26377;&#25928;&#29702;&#35299;&#21644;&#24212;&#29992;&#20219;&#21153;&#35299;&#20915;&#21407;&#21017;&#65292;&#22240;&#20026;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#21487;&#33021;&#24182;&#26410;&#33258;&#28982;&#21253;&#21547;&#36825;&#20123;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00843v1 Announce Type: cross  Abstract: Traditional recommendation setting tends to excessively cater to users' immediate interests and neglect their long-term engagement. To address it, it is crucial to incorporate planning capabilities into the recommendation decision-making process to develop policies that take into account both immediate interests and long-term engagement. Despite Reinforcement Learning (RL) can learn planning capacity by maximizing cumulative reward, the scarcity of recommendation data presents challenges such as instability and susceptibility to overfitting when training RL models from scratch.   In this context, we propose to leverage the remarkable planning capabilities over sparse data of Large Language Models (LLMs) for long-term recommendation. The key lies in enabling a language model to understand and apply task-solving principles effectively in personalized recommendation scenarios, as the model's pre-training may not naturally encompass these 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Dynamic Experienced Expert Modeling&#65288;DEEM&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#30340;&#32463;&#39564;&#19987;&#23478;&#20351;LLMs&#33021;&#22815;&#20197;&#21322;&#21442;&#25968;&#21270;&#26041;&#24335;&#36827;&#34892;&#25512;&#29702;&#65292;&#25552;&#39640;&#20102;&#22312;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15264</link><description>&lt;p&gt;
DEEM&#65306;&#38754;&#21521;&#31435;&#22330;&#26816;&#27979;&#30340;&#21160;&#24577;&#20307;&#39564;&#19987;&#23478;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
DEEM: Dynamic Experienced Expert Modeling for Stance Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Dynamic Experienced Expert Modeling&#65288;DEEM&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#30340;&#32463;&#39564;&#19987;&#23478;&#20351;LLMs&#33021;&#22815;&#20197;&#21322;&#21442;&#25968;&#21270;&#26041;&#24335;&#36827;&#34892;&#25512;&#29702;&#65292;&#25552;&#39640;&#20102;&#22312;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21021;&#27493;&#23581;&#35797;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35299;&#20915;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#65292;&#23637;&#29616;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#31435;&#22330;&#26816;&#27979;&#36890;&#24120;&#38656;&#35201;&#35814;&#32454;&#30340;&#32972;&#26223;&#30693;&#35782;&#65292;&#20256;&#32479;&#30340;&#25512;&#29702;&#26041;&#27861;&#21487;&#33021;&#20250;&#24573;&#35270;&#39046;&#22495;&#30693;&#35782;&#65292;&#20197;&#36827;&#34892;&#19987;&#19994;&#21644;&#20934;&#30830;&#30340;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;LLMs&#30340;&#25512;&#29702;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#65292;&#23588;&#20854;&#22312;&#21033;&#29992;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#27169;&#25311;&#29305;&#23450;&#19987;&#23478;&#65288;&#21363;&#22810;&#26234;&#33021;&#20307;&#65289;&#26469;&#26816;&#27979;&#31435;&#22330;&#26041;&#38754;&#12290;&#19982;&#29616;&#26377;&#38656;&#35201;&#35814;&#32454;&#25551;&#36848;&#24182;&#20351;&#29992;&#22266;&#23450;&#19987;&#23478;&#30340;&#22810;&#26234;&#33021;&#20307;&#20316;&#21697;&#19981;&#21516;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Dynamic Experienced Expert Modeling&#65288;DEEM&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#29983;&#25104;&#30340;&#32463;&#39564;&#19987;&#23478;&#65292;&#24182;&#35753;LLMs&#20197;&#21322;&#21442;&#25968;&#21270;&#26041;&#24335;&#36827;&#34892;&#25512;&#29702;&#65292;&#20351;&#19987;&#23478;&#26356;&#20855;&#26222;&#36866;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DEEM&#22312;&#19977;&#20010;&#22330;&#26223;&#19978;&#19968;&#30452;&#36798;&#21040;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15264v1 Announce Type: new  Abstract: Recent work has made a preliminary attempt to use large language models (LLMs) to solve the stance detection task, showing promising results. However, considering that stance detection usually requires detailed background knowledge, the vanilla reasoning method may neglect the domain knowledge to make a professional and accurate analysis. Thus, there is still room for improvement of LLMs reasoning, especially in leveraging the generation capability of LLMs to simulate specific experts (i.e., multi-agents) to detect the stance. In this paper, different from existing multi-agent works that require detailed descriptions and use fixed experts, we propose a Dynamic Experienced Expert Modeling (DEEM) method which can leverage the generated experienced experts and let LLMs reason in a semi-parametric way, making the experts more generalizable and reliable. Experimental results demonstrate that DEEM consistently achieves the best results on thre
&lt;/p&gt;</description></item><item><title>&#19987;&#38376;&#27169;&#22411;&#36890;&#24120;&#21482;&#38656;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#65288;100-1000&#20010;&#65289;&#23601;&#33021;&#19982;&#36890;&#29992;&#27169;&#22411;&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#65292;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.12819</link><description>&lt;p&gt;
&#24494;&#35843;&#12289;&#25552;&#31034;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25351;&#23548;&#24494;&#35843;&#65306;&#25105;&#20204;&#38656;&#35201;&#22810;&#23569;&#26631;&#35760;&#26679;&#26412;&#65311;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How Many Labelled Samples Do We Need?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12819
&lt;/p&gt;
&lt;p&gt;
&#19987;&#38376;&#27169;&#22411;&#36890;&#24120;&#21482;&#38656;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#65288;100-1000&#20010;&#65289;&#23601;&#33021;&#19982;&#36890;&#29992;&#27169;&#22411;&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#65292;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35299;&#20915;&#20855;&#26377;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#20219;&#21153;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36873;&#25321;&#20351;&#29992;&#36890;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32780;&#19981;&#36827;&#34892;&#36827;&#19968;&#27493;&#26356;&#26032;&#65292;&#25110;&#32773;&#20351;&#29992;&#23569;&#37327;&#31034;&#20363;&#26469;&#35843;&#25972;&#19987;&#38376;&#30340;&#36739;&#23567;&#27169;&#22411;&#12290; &#24403;&#26377;&#36275;&#22815;&#30340;&#26631;&#35760;&#21487;&#29992;&#26102;&#65292;&#19987;&#38376;&#30340;&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#36890;&#29992;&#27169;&#22411;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#35843;&#26597;&#19987;&#38376;&#27169;&#22411;&#38656;&#35201;&#22810;&#23569;&#26631;&#35760;&#26679;&#26412;&#25165;&#33021;&#23454;&#29616;&#36825;&#31181;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#32771;&#34385;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;&#35266;&#23519;&#25552;&#31034;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24494;&#35843;&#21644;&#25351;&#23548;&#24494;&#35843;&#30340;&#34892;&#20026;&#65292;&#35782;&#21035;&#23427;&#20204;&#22312;&#22686;&#21152;&#19981;&#21516;&#22797;&#26434;&#24615;&#20219;&#21153;&#30340;&#26631;&#35760;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#26102;&#30340;&#25910;&#25903;&#24179;&#34913;&#28857;&#65292;&#25105;&#20204;&#21457;&#29616;&#19987;&#38376;&#27169;&#22411;&#36890;&#24120;&#21482;&#38656;&#23569;&#37327;&#26679;&#26412;&#65288;100-1000&#20010;&#65289;&#23601;&#33021;&#19982;&#36890;&#29992;&#27169;&#22411;&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#12290; &#21516;&#26102;&#65292;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#37327;&#24378;&#28872;&#20381;&#36182;&#20110;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12819v1 Announce Type: cross  Abstract: When solving a task with limited labelled data, researchers can either use a general large language model without further update, or use the few examples to tune a specialised smaller model. When enough labels are available, the specialised models outperform the general ones on many NLP tasks. In this work, we aim to investigate how many labelled samples are required for the specialised models to achieve this superior performance, while taking the results variance into consideration. Observing the behaviour of prompting, in-context learning, fine-tuning and instruction-tuning, identifying their break-even points when increasing number of labelled training samples across three tasks of varying complexity, we find that the specialised models often need only few samples ($100-1000$) to be on par or better than the general ones. At the same time, the amount of required labelled data strongly depends on the task complexity and results varia
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#20219;&#21153;&#29305;&#23450;&#21644;&#29983;&#25104;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;Amharic-LLaMA&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#38463;&#22982;&#21704;&#25289;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20182;&#20204;&#36890;&#36807;&#21019;&#24314;&#38463;&#22982;&#21704;&#25289;&#35821;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#21644;&#24494;&#35843;&#27169;&#22411;&#65292;&#22312;&#19981;&#21516;&#30340;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.08015</link><description>&lt;p&gt;
&#22686;&#24378;Amharic-LLaMA: &#25972;&#21512;&#29305;&#23450;&#20219;&#21153;&#19982;&#29983;&#25104;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Enhancing Amharic-LLaMA: Integrating Task Specific and Generative Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#20219;&#21153;&#29305;&#23450;&#21644;&#29983;&#25104;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;Amharic-LLaMA&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#38463;&#22982;&#21704;&#25289;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20182;&#20204;&#36890;&#36807;&#21019;&#24314;&#38463;&#22982;&#21704;&#25289;&#35821;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#21644;&#24494;&#35843;&#27169;&#22411;&#65292;&#22312;&#19981;&#21516;&#30340;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22240;&#20854;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#20154;&#31867;&#35821;&#35328;&#26041;&#38754;&#30340;&#20986;&#33394;&#34920;&#29616;&#32780;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#20013;&#21463;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#22240;&#32570;&#20047;&#36164;&#28304;&#32780;&#34987;&#33853;&#19979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#36890;&#36807;&#25972;&#21512;&#29305;&#23450;&#20219;&#21153;&#21644;&#29983;&#25104;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;LLaMA-2-Amharic&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#38463;&#22982;&#21704;&#25289;&#35821;&#30340;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#38463;&#22982;&#21704;&#25289;&#35821;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;LLaMA-2-Amharic&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#24320;&#28304;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21019;&#24314;&#27969;&#31243;&#12289;&#25351;&#20196;&#25968;&#25454;&#38598;&#12289;&#35757;&#32451;&#27169;&#22411;&#21644;&#35780;&#20272;&#36755;&#20986;&#65292;&#20197;&#20419;&#36827;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#35821;&#35328;&#29305;&#23450;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have received a lot of attention in natural language processing (NLP) research because of their exceptional performance in understanding and generating human languages. However, low-resource languages are left behind due to the unavailability of resources. In this work, we focus on enhancing the LLaMA-2-Amharic model by integrating task-specific and generative datasets to improve language model performance for Amharic. We compile an Amharic instruction fine-tuning dataset and fine-tuned LLaMA-2-Amharic model. The fine-tuned model shows promising results in different NLP tasks. We open-source our dataset creation pipeline, instruction datasets, trained models, and evaluation outputs to promote language-specific studies on these models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#36275;&#29699;&#27604;&#36187;&#20013;&#19979;&#19968;&#20010;&#20107;&#20214;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#30340;&#21551;&#21457;&#12290;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;WyScout&#25968;&#25454;&#38598;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#26126;&#26174;&#36229;&#36807;&#20102;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#30340;&#24212;&#29992;&#21253;&#25324;&#21338;&#24425;&#21644;&#27604;&#36187;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#25311;&#39592;&#26550;&#29992;&#20110;&#26500;&#24314;&#20998;&#26512;&#27969;&#27700;&#32447;&#12290;</title><link>https://arxiv.org/abs/2402.06820</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#39044;&#27979;&#36275;&#29699;&#27604;&#36187;&#20107;&#20214;
&lt;/p&gt;
&lt;p&gt;
Forecasting Events in Soccer Matches Through Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#36275;&#29699;&#27604;&#36187;&#20013;&#19979;&#19968;&#20010;&#20107;&#20214;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#30340;&#21551;&#21457;&#12290;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;WyScout&#25968;&#25454;&#38598;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#26126;&#26174;&#36229;&#36807;&#20102;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#30340;&#24212;&#29992;&#21253;&#25324;&#21338;&#24425;&#21644;&#27604;&#36187;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#25311;&#39592;&#26550;&#29992;&#20110;&#26500;&#24314;&#20998;&#26512;&#27969;&#27700;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39044;&#27979;&#36275;&#29699;&#27604;&#36187;&#20013;&#19979;&#19968;&#20010;&#20107;&#20214;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38754;&#20020;&#30340;&#38382;&#39064;&#38750;&#24120;&#30456;&#20284;&#30340;&#25361;&#25112;&#12290;&#19982;&#20854;&#20182;&#20005;&#37325;&#38480;&#21046;&#36275;&#29699;&#20107;&#20214;&#21160;&#24577;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#20174;&#24456;&#22810;&#21464;&#37327;&#20013;&#25277;&#35937;&#20986;&#26469;&#25110;&#20381;&#36182;&#20110;&#28151;&#21512;&#39034;&#24207;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#21040;LLMs&#26041;&#27861;&#23398;&#21551;&#21457;&#30340;&#26032;&#25216;&#26415;&#12290;&#36825;&#20123;&#27169;&#22411;&#39044;&#27979;&#20102;&#32452;&#25104;&#19968;&#20010;&#20107;&#20214;&#30340;&#23436;&#25972;&#21464;&#37327;&#38142;&#65292;&#22823;&#22823;&#31616;&#21270;&#20102;&#26500;&#24314;&#36275;&#29699;&#22823;&#20107;&#20214;&#27169;&#22411;&#65288;LEMs&#65289;&#30340;&#36807;&#31243;&#12290;&#21033;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;WyScout&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20851;&#38190;&#39046;&#22495;&#65288;&#22914;&#19979;&#19968;&#20010;&#20107;&#20214;&#31867;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65289;&#26174;&#33879;&#36229;&#36234;&#20102;&#20197;&#24448;LEM&#25552;&#26696;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#31361;&#26174;&#20102;LEM&#22312;&#22810;&#31181;&#24212;&#29992;&#20013;&#30340;&#23454;&#29992;&#24615;&#65292;&#21253;&#25324;&#21338;&#24425;&#21644;&#27604;&#36187;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;LEM&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#25311;&#39592;&#26550;&#65292;&#21487;&#20197;&#26500;&#24314;&#35768;&#22810;&#20998;&#26512;&#27969;&#27700;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an approach to predicting the next event in a soccer match, a challenge bearing remarkable similarities to the problem faced by Large Language Models (LLMs). Unlike other methods that severely limit event dynamics in soccer, often abstracting from many variables or relying on a mix of sequential models, our research proposes a novel technique inspired by the methodologies used in LLMs. These models predict a complete chain of variables that compose an event, significantly simplifying the construction of Large Event Models (LEMs) for soccer. Utilizing deep learning on the publicly available WyScout dataset, the proposed approach notably surpasses the performance of previous LEM proposals in critical areas, such as the prediction accuracy of the next event type. This paper highlights the utility of LEMs in various applications, including betting and match analytics. Moreover, we show that LEMs provide a simulation backbone on which many analytics pipelines can be bu
&lt;/p&gt;</description></item><item><title>PythonSaga&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#65292;&#38024;&#23545;Python&#20195;&#30721;&#29983;&#25104;&#36827;&#34892;&#35780;&#20272;,&#24357;&#34917;&#20102;&#29616;&#26377;&#22522;&#20934;&#23384;&#22312;&#30340;&#32534;&#31243;&#27010;&#24565;&#20559;&#35265;&#21644;&#31616;&#21333;&#20219;&#21153;&#26222;&#36941;&#24615;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2401.03855</link><description>&lt;p&gt;
PythonSaga&#65306;&#37325;&#26032;&#23450;&#20041;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;LLM&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03855
&lt;/p&gt;
&lt;p&gt;
PythonSaga&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#65292;&#38024;&#23545;Python&#20195;&#30721;&#29983;&#25104;&#36827;&#34892;&#35780;&#20272;,&#24357;&#34917;&#20102;&#29616;&#26377;&#22522;&#20934;&#23384;&#22312;&#30340;&#32534;&#31243;&#27010;&#24565;&#20559;&#35265;&#21644;&#31616;&#21333;&#20219;&#21153;&#26222;&#36941;&#24615;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29983;&#25104;&#20195;&#30721;&#28608;&#22686;&#30340;&#25512;&#21160;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;LLMs&#30340;&#21151;&#33021;&#12290;&#25105;&#20204;&#23545;HumanEval&#21644;MBPP&#20004;&#20010;&#27969;&#34892;&#30340;Python&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#20154;&#24037;&#35780;&#20272;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#22810;&#26679;&#24615;&#21644;&#38590;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#23545;&#19968;&#32452;&#26377;&#38480;&#30340;&#32534;&#31243;&#27010;&#24565;&#23384;&#22312;&#20005;&#37325;&#20559;&#35265;&#65292;&#23436;&#20840;&#24573;&#35270;&#20102;&#22823;&#22810;&#25968;&#20854;&#20182;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#22823;&#37327;&#31616;&#21333;&#20219;&#21153;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#21487;&#33021;&#22840;&#22823;&#20102;&#27169;&#22411;&#24615;&#33021;&#30340;&#20272;&#35745;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20934;&#65292;PythonSaga&#65292;&#21253;&#21547;&#20102;185&#20010;&#25163;&#24037;&#21046;&#20316;&#30340;&#25552;&#31034;&#65292;&#28085;&#30422;&#20102;38&#20010;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#30340;&#32534;&#31243;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03855v2 Announce Type: replace-cross  Abstract: Driven by the surge in code generation using large language models (LLMs), numerous benchmarks have emerged to evaluate these LLMs capabilities. We conducted a large-scale human evaluation of HumanEval and MBPP, two popular benchmarks for Python code generation, analyzing their diversity and difficulty. Our findings unveil a critical bias towards a limited set of programming concepts, neglecting most of the other concepts entirely. Furthermore, we uncover a worrying prevalence of easy tasks, potentially inflating model performance estimations. To address these limitations, we propose a novel benchmark, PythonSaga, featuring 185 hand-crafted prompts on a balanced representation of 38 programming concepts across diverse difficulty levels.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Fast LoRA&#65288;FLoRA&#65289;&#26694;&#26550;&#65292;&#20351;&#24471;&#22522;&#30784;&#27169;&#22411;&#30340;&#20302;&#31209;&#35843;&#25972;&#21487;&#20197;&#39640;&#25928;&#25209;&#22788;&#29702;&#24322;&#26500;&#35831;&#27714;&#65292;&#24182;&#22312;&#32489;&#25928;&#19978;&#20445;&#25345;&#31454;&#20105;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.05677</link><description>&lt;p&gt;
&#22522;&#20110;&#25209;&#22788;&#29702;&#30340;&#22522;&#30784;&#27169;&#22411;&#20302;&#31209;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Batched Low-Rank Adaptation of Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05677
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Fast LoRA&#65288;FLoRA&#65289;&#26694;&#26550;&#65292;&#20351;&#24471;&#22522;&#30784;&#27169;&#22411;&#30340;&#20302;&#31209;&#35843;&#25972;&#21487;&#20197;&#39640;&#25928;&#25209;&#22788;&#29702;&#24322;&#26500;&#35831;&#27714;&#65292;&#24182;&#22312;&#32489;&#25928;&#19978;&#20445;&#25345;&#31454;&#20105;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#22240;&#36890;&#36807;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#20302;&#31209;&#30697;&#38453;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#24182;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#32780;&#24341;&#36215;&#20851;&#27880;&#12290;&#34429;&#28982;LoRA&#25552;&#20379;&#20102;&#35768;&#22810;&#20248;&#28857;&#65292;&#20294;&#20854;&#22312;&#23454;&#26102;&#20026;&#21508;&#31181;&#20840;&#29699;&#29992;&#25143;&#25552;&#20379;&#26381;&#21153;&#26102;&#26080;&#27861;&#39640;&#25928;&#22788;&#29702;&#22810;&#20010;&#29305;&#23450;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#33021;&#21147;&#21463;&#21040;&#38480;&#21046;&#12290;&#36825;&#20026;&#38656;&#35201;&#20026;&#27599;&#20010;&#20256;&#20837;&#35831;&#27714;&#20010;&#24615;&#21270;&#12289;&#29305;&#23450;&#20219;&#21153;&#36866;&#24212;&#30340;&#22330;&#26223;&#20013;&#36896;&#25104;&#20102;&#24615;&#33021;&#29942;&#39048;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24555;&#36895;LoRA&#65288;FLoRA&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#25209;&#22788;&#29702;&#20013;&#30340;&#27599;&#20010;&#36755;&#20837;&#31034;&#20363;&#37117;&#21487;&#20197;&#19982;&#20854;&#29420;&#29305;&#30340;&#20302;&#31209;&#36866;&#24212;&#26435;&#37325;&#30456;&#20851;&#32852;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#24322;&#26500;&#35831;&#27714;&#30340;&#39640;&#25928;&#25209;&#22788;&#29702;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#34920;&#26126;&#65292;FLoRA&#20445;&#30041;&#20102;LoRA&#30340;&#32489;&#25928;&#20248;&#28857;&#65292;&#22312;&#36328;&#36234;8&#31181;&#35821;&#35328;&#30340;MultiPL-E&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#19978;&#23637;&#31034;&#20986;&#31454;&#20105;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05677v2 Announce Type: replace-cross  Abstract: Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning foundation models by incorporating trainable low-rank matrices, thereby reducing the number of trainable parameters. While LoRA offers numerous advantages, its applicability for real-time serving to a diverse and global user base is constrained by its incapability to handle multiple task-specific adapters efficiently. This imposes a performance bottleneck in scenarios requiring personalized, task-specific adaptations for each incoming request. To mitigate this constraint, we introduce Fast LoRA (FLoRA), a framework in which each input example in a minibatch can be associated with its unique low-rank adaptation weights, allowing for efficient batching of heterogeneous requests. We empirically demonstrate that FLoRA retains the performance merits of LoRA, showcasing competitive results on the MultiPL-E code generation benchmark spanning over 8 languages and 
&lt;/p&gt;</description></item><item><title>MAIRA-1&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#22312;&#19982;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#23545;&#40784;&#21644;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#20102;CXR&#29305;&#23450;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29983;&#25104;&#20855;&#26377;&#26368;&#20808;&#36827;&#36136;&#37327;&#30340;&#25253;&#21578;&#12290;</title><link>https://arxiv.org/abs/2311.13668</link><description>&lt;p&gt;
MAIRA-1&#65306;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MAIRA-1: A specialised large multimodal model for radiology report generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13668
&lt;/p&gt;
&lt;p&gt;
MAIRA-1&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#22312;&#19982;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#23545;&#40784;&#21644;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#20102;CXR&#29305;&#23450;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29983;&#25104;&#20855;&#26377;&#26368;&#20808;&#36827;&#36136;&#37327;&#30340;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#23556;&#23398;&#29305;&#23450;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#33016;&#37096;X&#20809;&#65288;CXR&#65289;&#29983;&#25104;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22522;&#20110;&#19968;&#20010;&#24605;&#24819;&#65292;&#21363;&#21487;&#20197;&#36890;&#36807;&#19982;&#39044;&#35757;&#32451;&#35270;&#35273;&#32534;&#30721;&#22120;&#23545;&#40784;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#22810;&#27169;&#24577;&#33021;&#21147;&#12290;&#22312;&#33258;&#28982;&#22270;&#20687;&#19978;&#65292;&#36825;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20351;&#22810;&#27169;&#24577;&#27169;&#22411;&#33719;&#24471;&#22270;&#20687;&#29702;&#35299;&#21644;&#25551;&#36848;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#65288;MAIRA-1&#65289;&#21033;&#29992;&#20102;&#19968;&#20010;CXR&#29305;&#23450;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#32467;&#21512;&#22522;&#20110;Vicuna-7B&#30340;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#22522;&#20110;&#25991;&#26412;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#20135;&#29983;&#20855;&#26377;&#26368;&#20808;&#36827;&#36136;&#37327;&#30340;&#25253;&#21578;&#12290;&#29305;&#21035;&#22320;&#65292;MAIRA-1&#22312;&#19982;&#25918;&#23556;&#31185;&#21307;&#29983;&#23545;&#40784;&#30340;RadCliQ&#24230;&#37327;&#21644;&#32771;&#34385;&#30340;&#25152;&#26377;&#35789;&#27719;&#24230;&#37327;&#19978;&#37117;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#25163;&#21160;&#23457;&#26680;&#26174;&#31034;&#20986;&#20102;&#20135;&#29983;&#25253;&#21578;&#30340;&#27969;&#30021;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#25152;&#26410;&#25429;&#25417;&#21040;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;&#26356;&#22810;&#20449;&#24687;&#21644;&#36164;&#28304;&#21487;&#22312;&#39033;&#30446;&#32593;&#31449;&#19978;&#25214;&#21040;&#65306;
&lt;/p&gt;
&lt;p&gt;
We present a radiology-specific multimodal model for the task for generating radiological reports from chest X-rays (CXRs). Our work builds on the idea that large language model(s) can be equipped with multimodal capabilities through alignment with pre-trained vision encoders. On natural images, this has been shown to allow multimodal models to gain image understanding and description capabilities. Our proposed model (MAIRA-1) leverages a CXR-specific image encoder in conjunction with a fine-tuned large language model based on Vicuna-7B, and text-based data augmentation, to produce reports with state-of-the-art quality. In particular, MAIRA-1 significantly improves on the radiologist-aligned RadCliQ metric and across all lexical metrics considered. Manual review of model outputs demonstrates promising fluency and accuracy of generated reports while uncovering failure modes not captured by existing evaluation practices. More information and resources can be found on the project website:
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#30001;&#25551;&#36848;&#36923;&#36753;&#30693;&#35782;&#24211;&#29983;&#25104;&#30340;&#21512;&#25104;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#22522;&#20110;Transformer&#27169;&#22411;&#22312;&#20016;&#23500;&#35821;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.08941</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#25551;&#36848;&#36923;&#36753;&#35821;&#22659;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Reasoning over Description Logic-based Contexts with Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#30001;&#25551;&#36848;&#36923;&#36753;&#30693;&#35782;&#24211;&#29983;&#25104;&#30340;&#21512;&#25104;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#22522;&#20110;Transformer&#27169;&#22411;&#22312;&#20016;&#23500;&#35821;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#34913;&#37327;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#19968;&#31181;&#26041;&#24335;&#26159;&#36890;&#36807;&#35780;&#20272;&#22312;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#21512;&#25104;&#35821;&#22659;&#20013;&#23545;&#36923;&#36753;&#38382;&#39064;&#22238;&#31572;&#25110;&#35777;&#26126;&#29983;&#25104;&#31561;&#19979;&#28216;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#23454;&#38469;&#20351;&#29992;&#30340;&#35821;&#22659;&#38750;&#24120;&#31616;&#21333;&#65307;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#26159;&#30001;&#20165;&#21547;&#26377;&#23569;&#37327;&#36923;&#36753;&#36816;&#31639;&#31526;&#21644;&#37327;&#35789;&#30340;&#30701;&#19968;&#38454;&#36923;&#36753;&#21477;&#23376;&#29983;&#25104;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#22238;&#31572;&#22522;&#20110;Transformer&#27169;&#22411;&#33021;&#22815;&#22312;&#34920;&#36798;&#20016;&#23500;&#35821;&#22659;&#20013;&#25191;&#34892;&#25512;&#29702;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#30001;&#25551;&#36848;&#36923;&#36753;&#30693;&#35782;&#24211;&#29983;&#25104;&#30340;&#21512;&#25104;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#20026;&#29983;&#25104;&#30693;&#35782;&#24211;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#34920;&#36798;&#24335;&#35821;&#35328;$\mathcal{ALCQ$&#12290;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;384K&#20010;&#31034;&#20363;&#65292;&#24182;&#19988;&#22312;&#20004;&#20010;&#32500;&#24230;&#19978;&#22686;&#21152;&#65306;i) &#25512;&#29702;&#28145;&#24230;&#65292;&#21644;ii) &#21477;&#23376;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08941v2 Announce Type: replace-cross  Abstract: One way that the current state of the art measures the reasoning ability of transformer-based models is by evaluating accuracy in downstream tasks like logical question answering or proof generation over synthetic contexts expressed in natural language. However, most of the contexts used are in practice very simple; in most cases, they are generated from short first-order logic sentences with only a few logical operators and quantifiers. In this work, we seek to answer the question how well a transformer-based model will perform reasoning over expressive contexts. For this purpose, we construct a synthetic natural language question-answering dataset, generated by description logic knowledge bases. For the generation of the knowledge bases, we use the expressive language $\mathcal{ALCQ}$. The resulting dataset contains 384K examples, and increases in two dimensions: i) reasoning depth, and ii) length of sentences. We show that t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20154;&#31867;&#21644;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#30340;&#35821;&#35328;&#24046;&#24322;&#65292;&#21457;&#29616;ChatGPT&#22312;&#31038;&#20132;&#12289;&#20998;&#26512;&#12289;&#35748;&#30693;&#12289;&#20851;&#27880;&#28966;&#28857;&#21644;&#31215;&#26497;&#24773;&#32490;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20154;&#31867;&#23545;&#35805;&#26356;&#20855;&#21464;&#24322;&#24615;&#21644;&#30495;&#23454;&#24615;&#65292;&#23613;&#31649;&#22312;&#24773;&#32490;&#26041;&#38754;&#26080;&#26174;&#33879;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#30001;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2401.16587</link><description>&lt;p&gt;
&#20154;&#31867;&#19982;ChatGPT&#29983;&#25104;&#23545;&#35805;&#20043;&#38388;&#30340;&#35821;&#35328;&#23545;&#27604;
&lt;/p&gt;
&lt;p&gt;
A Linguistic Comparison between Human and ChatGPT-Generated Conversations. (arXiv:2401.16587v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20154;&#31867;&#21644;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#30340;&#35821;&#35328;&#24046;&#24322;&#65292;&#21457;&#29616;ChatGPT&#22312;&#31038;&#20132;&#12289;&#20998;&#26512;&#12289;&#35748;&#30693;&#12289;&#20851;&#27880;&#28966;&#28857;&#21644;&#31215;&#26497;&#24773;&#32490;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20154;&#31867;&#23545;&#35805;&#26356;&#20855;&#21464;&#24322;&#24615;&#21644;&#30495;&#23454;&#24615;&#65292;&#23613;&#31649;&#22312;&#24773;&#32490;&#26041;&#38754;&#26080;&#26174;&#33879;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#30001;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#31867;&#21644;LLM&#29983;&#25104;&#30340;&#23545;&#35805;&#20043;&#38388;&#30340;&#35821;&#35328;&#24046;&#24322;&#65292;&#20351;&#29992;&#20102;&#30001;ChatGPT-3.5&#29983;&#25104;&#30340;19.5K&#20010;&#23545;&#35805;&#20316;&#20026;EmpathicDialogues&#25968;&#25454;&#38598;&#30340;&#34917;&#20805;&#12290;&#30740;&#31350;&#37319;&#29992;Linguistic Inquiry and Word Count (LIWC) &#20998;&#26512;&#65292;&#27604;&#36739;&#20102;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#21644;&#20154;&#31867;&#23545;&#35805;&#22312;118&#20010;&#35821;&#35328;&#31867;&#21035;&#19978;&#30340;&#24046;&#24322;&#12290;&#32467;&#26524;&#26174;&#31034;&#20154;&#31867;&#23545;&#35805;&#20855;&#26377;&#26356;&#22823;&#30340;&#21464;&#24322;&#24615;&#21644;&#30495;&#23454;&#24615;&#65292;&#20294;ChatGPT&#22312;&#31038;&#20132;&#36807;&#31243;&#12289;&#20998;&#26512;&#39118;&#26684;&#12289;&#35748;&#30693;&#12289;&#20851;&#27880;&#28966;&#28857;&#21644;&#31215;&#26497;&#24773;&#32490;&#33394;&#24425;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;LLMs&#8220;&#27604;&#30495;&#20154;&#26356;&#20687;&#30495;&#20154;&#8221;&#30340;&#26368;&#26032;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;ChatGPT&#21644;&#20154;&#31867;&#23545;&#35805;&#20043;&#38388;&#27809;&#26377;&#25214;&#21040;&#31215;&#26497;&#25110;&#28040;&#26497;&#24773;&#32490;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#23545;&#35805;&#23884;&#20837;&#30340;&#20998;&#31867;&#22120;&#20998;&#26512;&#34920;&#26126;&#65292;&#23613;&#31649;&#23545;&#35805;&#20013;&#27809;&#26377;&#26126;&#30830;&#25552;&#21450;&#24773;&#32490;&#65292;&#20294;&#23545;&#24773;&#24863;&#20215;&#20540;&#30340;&#38544;&#24615;&#32534;&#30721;&#23384;&#22312;&#12290;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#30001;&#20004;&#20010;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores linguistic differences between human and LLM-generated dialogues, using 19.5K dialogues generated by ChatGPT-3.5 as a companion to the EmpathicDialogues dataset. The research employs Linguistic Inquiry and Word Count (LIWC) analysis, comparing ChatGPT-generated conversations with human conversations across 118 linguistic categories. Results show greater variability and authenticity in human dialogues, but ChatGPT excels in categories such as social processes, analytical style, cognition, attentional focus, and positive emotional tone, reinforcing recent findings of LLMs being "more human than human." However, no significant difference was found in positive or negative affect between ChatGPT and human dialogues. Classifier analysis of dialogue embeddings indicates implicit coding of the valence of affect despite no explicit mention of affect in the conversations. The research also contributes a novel, companion ChatGPT-generated dataset of conversations between two i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#22411;&#30340;&#24369;&#30417;&#30563;&#39640;&#26031;&#23545;&#27604;&#22522;&#30784;&#27169;&#22411;&#26469;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#21644;&#31572;&#26696;&#23545;&#20316;&#20026;&#20107;&#20214;&#25551;&#36848;&#65292;&#25214;&#21040;&#22810;&#20010;&#20851;&#38190;&#24103;&#20316;&#20026;&#30446;&#26631;&#26102;&#21051;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#26102;&#21051;&#20316;&#20026;&#20266;&#26631;&#31614;&#26469;&#24378;&#21046;LMMs&#36827;&#34892;&#25512;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#23545;&#27604;&#22522;&#30784;&#27169;&#22359;&#65288;GCG&#65289;&#26469;&#23398;&#20064;&#26102;&#25928;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2401.10711</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#22411;&#30340;&#24369;&#30417;&#30563;&#39640;&#26031;&#23545;&#27604;&#22522;&#30784;&#27169;&#22411;&#26469;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering. (arXiv:2401.10711v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#22411;&#30340;&#24369;&#30417;&#30563;&#39640;&#26031;&#23545;&#27604;&#22522;&#30784;&#27169;&#22411;&#26469;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#21644;&#31572;&#26696;&#23545;&#20316;&#20026;&#20107;&#20214;&#25551;&#36848;&#65292;&#25214;&#21040;&#22810;&#20010;&#20851;&#38190;&#24103;&#20316;&#20026;&#30446;&#26631;&#26102;&#21051;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#26102;&#21051;&#20316;&#20026;&#20266;&#26631;&#31614;&#26469;&#24378;&#21046;LMMs&#36827;&#34892;&#25512;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#23545;&#27604;&#22522;&#30784;&#27169;&#22359;&#65288;GCG&#65289;&#26469;&#23398;&#20064;&#26102;&#25928;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#38382;&#31572;&#65288;VideoQA&#65289;&#26088;&#22312;&#22522;&#20110;&#35266;&#23519;&#21040;&#30340;&#35270;&#39057;&#20449;&#24687;&#22238;&#31572;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#23613;&#31649;&#22823;&#22411;&#22810;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#22270;&#20687;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#26041;&#38754;&#36824;&#19981;&#36275;&#22815;&#65292;&#20165;&#20165;&#26159;&#23558;&#22343;&#21248;&#37319;&#26679;&#30340;&#24103;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#65292;&#24573;&#30053;&#20102;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#35270;&#35273;&#32447;&#32034;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#35270;&#39057;&#38382;&#31572;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#38024;&#23545;&#38382;&#39064;&#20851;&#38190;&#26102;&#38388;&#25139;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24369;&#30417;&#30563;&#26694;&#26550;&#65292;&#24378;&#21046;LMMs&#20351;&#29992;&#38382;&#39064;&#20851;&#38190;&#26102;&#21051;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#25512;&#29702;&#20986;&#31572;&#26696;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#21644;&#31572;&#26696;&#23545;&#21512;&#24182;&#20026;&#20107;&#20214;&#25551;&#36848;&#65292;&#20197;&#25214;&#21040;&#22810;&#20010;&#20851;&#38190;&#24103;&#20316;&#20026;&#30446;&#26631;&#26102;&#21051;&#65292;&#36825;&#20123;&#26102;&#21051;&#23558;&#20316;&#20026;&#20266;&#26631;&#31614;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#20266;&#26631;&#31614;&#20316;&#20026;&#39069;&#22806;&#30340;&#24369;&#30417;&#30563;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#23545;&#27604;&#22522;&#30784;&#27169;&#22359;&#65288;GCG&#65289;&#12290;GCG&#23398;&#20064;&#22810;&#20010;&#39640;&#26031;&#20989;&#25968;&#26469;&#25551;&#36848;&#26102;&#25928;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video Question Answering (VideoQA) aims to answer natural language questions based on the information observed in videos. Despite the recent success of Large Multimodal Models (LMMs) in image-language understanding and reasoning, they deal with VideoQA insufficiently by simply taking uniformly sampled frames as visual inputs, which ignores question-relevant visual clues. Moreover, there are no human annotations for question-critical timestamps in existing VideoQA datasets. In light of this, we propose a novel weakly supervised framework to enforce the LMMs to reason out the answers with question-critical moments as visual inputs. Specifically, we fuse the question and answer pairs as event descriptions to find multiple keyframes as target moments, which will be pseudo-labels. With these pseudo-labels as additionally weak supervision, we devise a lightweight Gaussian-based Contrastive Grounding (GCG) module. GCG learns multiple Gaussian functions to characterize the temporal structure o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#38024;&#23545;&#25945;&#32946;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#65292;&#21253;&#25324;&#25968;&#23398;&#12289;&#20889;&#20316;&#12289;&#32534;&#31243;&#12289;&#25512;&#29702;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#65292;&#26088;&#22312;&#25506;&#32034;&#20854;&#22312;&#26500;&#24314;&#19979;&#19968;&#20195;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.08664</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#25945;&#32946;&#65306;&#22522;&#26412;&#33021;&#21147;&#12289;&#28508;&#21147;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Adapting Large Language Models for Education: Foundational Capabilities, Potentials, and Challenges. (arXiv:2401.08664v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#38024;&#23545;&#25945;&#32946;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#65292;&#21253;&#25324;&#25968;&#23398;&#12289;&#20889;&#20316;&#12289;&#32534;&#31243;&#12289;&#25512;&#29702;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#65292;&#26088;&#22312;&#25506;&#32034;&#20854;&#22312;&#26500;&#24314;&#19979;&#19968;&#20195;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25945;&#32946;&#24179;&#21488;&#21033;&#29992;&#20114;&#32852;&#32593;&#20998;&#21457;&#25945;&#32946;&#36164;&#28304;&#65292;&#26088;&#22312;&#25552;&#20379;&#20415;&#25463;&#30340;&#25945;&#32946;&#65292;&#20294;&#24448;&#24448;&#22312;&#19982;&#23398;&#29983;&#30340;&#23454;&#26102;&#20132;&#27969;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#30001;&#20110;&#38656;&#35201;&#35299;&#20915;&#23398;&#29983;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#22810;&#26679;&#21270;&#38556;&#30861;&#30340;&#25361;&#25112;&#65292;&#23427;&#20204;&#32463;&#24120;&#38590;&#20197;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#25945;&#32946;&#36164;&#28304;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#25552;&#20379;&#20102;&#36890;&#36807;&#29702;&#35299;&#20010;&#20307;&#35831;&#27714;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;&#12290;&#34429;&#28982;LLMs&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22522;&#20110;LLM&#30340;&#25945;&#32946;&#31995;&#32479;&#30340;&#26500;&#24314;&#20173;&#28982;&#38754;&#20020;&#30528;&#24191;&#27867;&#30340;&#25945;&#32946;&#25216;&#33021;&#35201;&#27714;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#19982;&#25945;&#32946;&#33021;&#21147;&#30456;&#20851;&#30340;&#36817;&#26399;&#20986;&#29616;&#30340;LLM&#30740;&#31350;&#65292;&#21253;&#25324;&#25968;&#23398;&#12289;&#20889;&#20316;&#12289;&#32534;&#31243;&#12289;&#25512;&#29702;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#65292;&#26088;&#22312;&#25506;&#32034;&#23427;&#20204;&#22312;&#26500;&#24314;&#19979;&#19968;&#20195;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online education platforms, leveraging the internet to distribute education resources, seek to provide convenient education but often fall short in real-time communication with students. They often struggle to offer personalized education resources due to the challenge of addressing the diverse obstacles students encounter throughout their learning journey. Recently, the emergence of large language models (LLMs), such as ChatGPT, offers the possibility for resolving this issue by comprehending individual requests. Although LLMs have been successful in various fields, creating an LLM-based education system is still challenging for the wide range of educational skills required. This paper reviews the recently emerged LLM researches related to educational capabilities, including mathematics, writing, programming, reasoning, and knowledge-based question answering, with the aim to explore their potential in constructing the next-generation intelligent education system. Based on the current 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;Stack Overflow&#22238;&#31572;&#20013;&#30340;&#20449;&#24687;&#39640;&#20142;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24320;&#21457;&#20102;&#33258;&#21160;&#25512;&#33616;&#31361;&#20986;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.01472</link><description>&lt;p&gt;
Stack Overflow&#22238;&#31572;&#20013;&#20449;&#24687;&#39640;&#20142;&#30340;&#21021;&#25506;
&lt;/p&gt;
&lt;p&gt;
A First Look at Information Highlighting in Stack Overflow Answers. (arXiv:2401.01472v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;Stack Overflow&#22238;&#31572;&#20013;&#30340;&#20449;&#24687;&#39640;&#20142;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24320;&#21457;&#20102;&#33258;&#21160;&#25512;&#33616;&#31361;&#20986;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#27983;&#35272;Stack Overflow&#65288;SO&#65289;&#30340;&#30693;&#35782;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#20351;&#24086;&#23376;&#23545;&#29992;&#25143;&#26356;&#29983;&#21160;&#65292;SO&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;Markdown&#25110;HTML&#32534;&#20889;&#21644;&#32534;&#36753;&#24086;&#23376;&#65292;&#20197;&#20415;&#29992;&#25143;&#21487;&#20197;&#21033;&#29992;&#21508;&#31181;&#26684;&#24335;&#21270;&#26679;&#24335;&#65288;&#20363;&#22914;&#31895;&#20307;&#12289;&#26012;&#20307;&#21644;&#20195;&#30721;&#65289;&#26469;&#31361;&#20986;&#37325;&#35201;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#31361;&#20986;&#20449;&#24687;&#30340;&#30740;&#31350;&#20173;&#28982;&#26377;&#38480;&#12290;&#30446;&#26631;&#65306;&#25105;&#20204;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;SO&#22238;&#31572;&#20013;&#30340;&#20449;&#24687;&#39640;&#20142;&#12290;&#20026;&#20102;&#25193;&#23637;&#25105;&#20204;&#20043;&#21069;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24320;&#21457;&#20102;&#33258;&#21160;&#25512;&#33616;&#24102;&#26377;&#26684;&#24335;&#21270;&#26679;&#24335;&#30340;&#31361;&#20986;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;&#26412;&#25991;&#30740;&#31350;&#20102;Stack Overflow&#30340;31,169,429&#20010;&#22238;&#31572;&#12290;&#20026;&#20102;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;CNN&#21644;BERT&#27169;&#22411;&#65292;&#38024;&#23545;&#27599;&#31181;&#26684;&#24335;&#21270;&#31867;&#22411;&#65288;&#21363;&#31895;&#20307;&#12289;&#26012;&#20307;&#12289;&#20195;&#30721;&#21644;&#26631;&#39064;&#65289;&#20351;&#29992;&#25105;&#20204;&#20174;SO&#22238;&#31572;&#25910;&#38598;&#30340;&#31361;&#20986;&#20449;&#24687;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context: Navigating the knowledge of Stack Overflow (SO) remains challenging. To make the posts vivid to users, SO allows users to write and edit posts with Markdown or HTML so that users can leverage various formatting styles (e.g., bold, italic, and code) to highlight the important information. Nonetheless, there have been limited studies on the highlighted information. Objective: We carried out the first large-scale exploratory study on the information highlighted in SO answers in our recent study. To extend our previous study, we develop approaches to automatically recommend highlighted content with formatting styles using neural network architectures initially designed for the Named Entity Recognition task. Method: In this paper, we studied 31,169,429 answers of Stack Overflow. For training recommendation models, we choose CNN and BERT models for each type of formatting (i.e., Bold, Italic, Code, and Heading) using the information highlighting dataset we collected from SO answers.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#19978;&#19979;&#25991;&#21033;&#29992;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23558;&#30456;&#20851;&#25991;&#26723;&#32435;&#20837;&#35757;&#32451;&#31034;&#20363;&#20013;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#12290;&#36890;&#36807;&#24341;&#20837;Structured Packing for Long Context (SPLiCe)&#26041;&#27861;&#65292;&#20351;&#29992;&#26816;&#32034;&#26041;&#27861;&#23558;&#26368;&#20114;&#30456;&#20851;&#25991;&#26723;&#27719;&#38598;&#21040;&#21333;&#20010;&#35757;&#32451;&#19978;&#19979;&#25991;&#20013;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.17296</link><description>&lt;p&gt;
LLM&#35757;&#32451;&#20013;&#30340;&#32467;&#26500;&#21270;&#22635;&#20805;&#25913;&#36827;&#20102;&#38271;&#19978;&#19979;&#25991;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Structured Packing in LLM Training Improves Long Context Utilization. (arXiv:2312.17296v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#19978;&#19979;&#25991;&#21033;&#29992;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23558;&#30456;&#20851;&#25991;&#26723;&#32435;&#20837;&#35757;&#32451;&#31034;&#20363;&#20013;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#12290;&#36890;&#36807;&#24341;&#20837;Structured Packing for Long Context (SPLiCe)&#26041;&#27861;&#65292;&#20351;&#29992;&#26816;&#32034;&#26041;&#27861;&#23558;&#26368;&#20114;&#30456;&#20851;&#25991;&#26723;&#27719;&#38598;&#21040;&#21333;&#20010;&#35757;&#32451;&#19978;&#19979;&#25991;&#20013;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LCLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#22312;&#26597;&#35810;&#31185;&#23398;&#30740;&#31350;&#35770;&#25991;&#31561;&#24212;&#29992;&#20013;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#28508;&#21147;&#24448;&#24448;&#21463;&#21040;&#19978;&#19979;&#25991;&#21033;&#29992;&#19981;&#36275;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#30830;&#23450;&#20856;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#32570;&#20047;&#38271;&#31243;&#35821;&#20041;&#20381;&#36182;&#26159;&#20027;&#35201;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#39057;&#32321;&#23558;&#30456;&#20851;&#25991;&#26723;&#32435;&#20837;&#35757;&#32451;&#36755;&#20837;&#30340;&#22909;&#22788;&#12290;&#21033;&#29992;&#20195;&#30721;&#25968;&#25454;&#30340;&#22266;&#26377;&#30446;&#24405;&#32467;&#26500;&#20316;&#20026;&#35757;&#32451;&#31034;&#20363;&#30340;&#26469;&#28304;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#23545;&#20110;&#19982;&#32534;&#30721;&#26080;&#20851;&#30340;&#20219;&#21153;&#65292;&#22218;&#25324;&#30456;&#20851;&#25991;&#26723;&#33021;&#22815;&#25913;&#36827;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#24182;&#19988;&#26356;&#20855;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Structured Packing for Long Context (SPLiCe)&#30340;&#21019;&#26032;&#26041;&#27861;&#12290; SPLiCe&#26159;&#19968;&#31181;&#20351;&#29992;&#26816;&#32034;&#26041;&#27861;&#23558;&#26368;&#20114;&#30456;&#20851;&#25991;&#26723;&#27719;&#38598;&#21040;&#21333;&#20010;&#35757;&#32451;&#19978;&#19979;&#25991;&#20013;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;\method{}&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#29992;&#20110;t
&lt;/p&gt;
&lt;p&gt;
Recent advances in long-context Large Language Models (LCLMs) have generated significant interest, especially in applications such as querying scientific research papers. However, their potential is often limited by inadequate context utilization. We identify the absence of long-range semantic dependencies in typical training data as a primary hindrance. To address this, we delve into the benefits of frequently incorporating related documents into training inputs. Using the inherent directory structure of code data as a source of training examples, we demonstrate improvements in perplexity, even for tasks unrelated to coding. Building on these findings, but with a broader focus, we introduce Structured Packing for Long Context (SPLiCe). SPLiCe is an innovative method for creating training examples by using a retrieval method to collate the most mutually relevant documents into a single training context. Our results indicate that \method{} enhances model performance and can be used to t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Kosmos-G&#65292;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#22270;&#20687;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#27169;&#24577;&#20316;&#20026;&#38170;&#28857;&#65292;&#23558;MLLM&#30340;&#36755;&#20986;&#31354;&#38388;&#19982;CLIP&#23545;&#40784;&#65292;&#24182;&#36827;&#34892;&#32452;&#21512;&#25351;&#20196;&#35843;&#25972;&#12290;Kosmos-G&#23637;&#31034;&#20102;&#38646;&#26679;&#26412;&#22810;&#23454;&#20307;&#20027;&#39064;&#39537;&#21160;&#29983;&#25104;&#30340;&#29420;&#29305;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02992</link><description>&lt;p&gt;
Kosmos-G&#65306;&#20351;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Kosmos-G: Generating Images in Context with Multimodal Large Language Models. (arXiv:2310.02992v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Kosmos-G&#65292;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#22270;&#20687;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#27169;&#24577;&#20316;&#20026;&#38170;&#28857;&#65292;&#23558;MLLM&#30340;&#36755;&#20986;&#31354;&#38388;&#19982;CLIP&#23545;&#40784;&#65292;&#24182;&#36827;&#34892;&#32452;&#21512;&#25351;&#20196;&#35843;&#25972;&#12290;Kosmos-G&#23637;&#31034;&#20102;&#38646;&#26679;&#26412;&#22810;&#23454;&#20307;&#20027;&#39064;&#39537;&#21160;&#29983;&#25104;&#30340;&#29420;&#29305;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#21644;&#35270;&#35273;&#35821;&#35328;&#21040;&#22270;&#20687;&#65288;VL2I&#65289;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20174;&#36890;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#36755;&#20837;&#29983;&#25104;&#22270;&#20687;&#65292;&#29305;&#21035;&#26159;&#28041;&#21450;&#22810;&#20010;&#22270;&#20687;&#30340;&#24773;&#20917;&#65292;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Kosmos-G&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#20808;&#36827;&#24863;&#30693;&#33021;&#21147;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#27169;&#24577;&#20316;&#20026;&#38170;&#28857;&#65292;&#23558;MLLM&#30340;&#36755;&#20986;&#31354;&#38388;&#19982;CLIP&#23545;&#40784;&#65292;&#24182;&#22312;&#31574;&#21010;&#25968;&#25454;&#19978;&#36827;&#34892;&#32452;&#21512;&#25351;&#20196;&#35843;&#25972;&#12290;Kosmos-G&#23637;&#31034;&#20102;&#38646;&#26679;&#26412;&#22810;&#23454;&#20307;&#20027;&#39064;&#39537;&#21160;&#29983;&#25104;&#30340;&#29420;&#29305;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20998;&#25968;&#33976;&#39311;&#25351;&#20196;&#35843;&#25972;&#23545;&#22270;&#20687;&#35299;&#30721;&#22120;&#19981;&#38656;&#35201;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#12290;&#36825;&#20801;&#35768;&#26080;&#32541;&#26367;&#20195;CLIP&#24182;&#36731;&#26494;&#38598;&#25104;&#21508;&#31181;U-Net&#25216;&#26415;&#65292;&#21253;&#25324;&#32454;&#31890;&#24230;&#25511;&#21046;&#21644;&#20010;&#24615;&#21270;&#22270;&#20687;&#35299;&#30721;&#22120;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in text-to-image (T2I) and vision-language-to-image (VL2I) generation have made significant strides. However, the generation from generalized vision-language inputs, especially involving multiple images, remains under-explored. This paper presents Kosmos-G, a model that leverages the advanced perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates a unique capability of zero-shot multi-entity subject-driven generation. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt to
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#33976;&#39311;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#26080;&#30417;&#30563;&#21477;&#27861;&#35299;&#26512;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#36807;&#33976;&#39311;&#23558;&#38598;&#25104;&#30693;&#35782;&#36716;&#31227;&#21040;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#24120;&#35265;&#30340;&#22810;&#25945;&#24072;&#33976;&#39311;&#26041;&#27861;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.01717</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#21477;&#27861;&#20998;&#26512;&#30340;&#38598;&#25104;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Ensemble Distillation for Unsupervised Constituency Parsing. (arXiv:2310.01717v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#33976;&#39311;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#26080;&#30417;&#30563;&#21477;&#27861;&#35299;&#26512;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#36807;&#33976;&#39311;&#23558;&#38598;&#25104;&#30693;&#35782;&#36716;&#31227;&#21040;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#24120;&#35265;&#30340;&#22810;&#25945;&#24072;&#33976;&#39311;&#26041;&#27861;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#21477;&#27861;&#20998;&#26512;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#23558;&#21477;&#23376;&#30340;&#35789;&#21644;&#30701;&#35821;&#32452;&#32455;&#25104;&#19968;&#20010;&#23618;&#27425;&#32467;&#26500;&#65292;&#32780;&#19981;&#20351;&#29992;&#35821;&#35328;&#23398;&#27880;&#37322;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#35299;&#26512;&#22120;&#25429;&#25417;&#21040;&#20102;&#35299;&#26512;&#32467;&#26500;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#26469;&#25552;&#39640;&#26080;&#30417;&#30563;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#26641;&#24179;&#22343;&#8221;&#30340;&#27010;&#24565;&#65292;&#22522;&#20110;&#27492;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#35299;&#26512;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;&#20026;&#20102;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#38598;&#25104;&#30693;&#35782;&#33976;&#39311;&#21040;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#20013;&#65307;&#36825;&#31181;&#38598;&#25104;-&#33976;&#39311;&#30340;&#36807;&#31243;&#26159;&#32531;&#35299;&#24120;&#35265;&#30340;&#22810;&#25945;&#24072;&#33976;&#39311;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36807;&#20102;&#25152;&#26377;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#22987;&#32456;&#34920;&#29616;&#20986;&#20854;&#22312;&#19981;&#21516;&#38598;&#25104;&#32452;&#20214;&#21644;&#39046;&#22495;&#36716;&#31227;&#26465;&#20214;&#19979;&#30340;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the unsupervised constituency parsing task, which organizes words and phrases of a sentence into a hierarchical structure without using linguistically annotated data. We observe that existing unsupervised parsers capture differing aspects of parsing structures, which can be leveraged to enhance unsupervised parsing performance. To this end, we propose a notion of "tree averaging," based on which we further propose a novel ensemble method for unsupervised parsing. To improve inference efficiency, we further distill the ensemble knowledge into a student model; such an ensemble-then-distill process is an effective approach to mitigate the over-smoothing problem existing in common multi-teacher distilling methods. Experiments show that our method surpasses all previous approaches, consistently demonstrating its effectiveness and robustness across various runs, with different ensemble components, and under domain-shift conditions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#27604;&#28436;&#31034;&#21644;&#26174;&#33879;&#24615;&#22270;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#25913;&#21464;&#26631;&#31614;&#23545;&#26174;&#33879;&#24615;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#23588;&#20854;&#23545;&#20110;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#20026;&#26126;&#26174;&#12290;&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23558;&#34920;&#36798;&#24773;&#24863;&#30340;&#26415;&#35821;&#25913;&#20026;&#20013;&#24615;&#35789;&#24182;&#19981;&#20687;&#25913;&#21464;&#26631;&#31614;&#37027;&#26679;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#21478;&#22806;&#65292;&#34917;&#20805;&#35299;&#37322;&#22312;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.05052</link><description>&lt;p&gt;
&#25506;&#32034;&#23545;&#27604;&#28436;&#31034;&#21644;&#26174;&#33879;&#24615;&#22270;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding In-Context Learning with Contrastive Demonstrations and Saliency Maps. (arXiv:2307.05052v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#27604;&#28436;&#31034;&#21644;&#26174;&#33879;&#24615;&#22270;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#25913;&#21464;&#26631;&#31614;&#23545;&#26174;&#33879;&#24615;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#23588;&#20854;&#23545;&#20110;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#20026;&#26126;&#26174;&#12290;&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23558;&#34920;&#36798;&#24773;&#24863;&#30340;&#26415;&#35821;&#25913;&#20026;&#20013;&#24615;&#35789;&#24182;&#19981;&#20687;&#25913;&#21464;&#26631;&#31614;&#37027;&#26679;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#21478;&#22806;&#65292;&#34917;&#20805;&#35299;&#37322;&#22312;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#24615;&#33021;&#20013;&#65292;&#21508;&#31181;&#28436;&#31034;&#32452;&#20214;&#30340;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26631;&#31614;&#12289;&#36755;&#20837;&#20998;&#24067;&#21644;&#34917;&#20805;&#35299;&#37322;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#36825;&#20123;&#22240;&#32032;&#34987;&#20462;&#25913;&#25110;&#25200;&#21160;&#26102;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22522;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#36825;&#20123;&#24037;&#20316;&#23545;&#20110;&#36825;&#20123;&#20803;&#32032;&#22914;&#20309;&#24433;&#21709;ICL&#32473;&#20986;&#20102;&#19981;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#25506;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21487;&#35299;&#37322;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(XNLP)&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#23545;&#27604;&#28436;&#31034;&#30340;&#26174;&#33879;&#24615;&#22270;&#36827;&#34892;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25913;&#21464;&#26631;&#31614;&#23545;&#26174;&#33879;&#24615;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#23588;&#20854;&#23545;&#20110;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#20026;&#26126;&#26174;&#12290;&#25105;&#20204;&#23545;&#36755;&#20837;&#20998;&#24067;&#36827;&#34892;&#20102;&#31890;&#24230;&#32423;&#21035;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23558;&#34920;&#36798;&#24773;&#24863;&#30340;&#26415;&#35821;&#25913;&#20026;&#20013;&#24615;&#35789;&#24182;&#19981;&#20687;&#25913;&#21464;&#26631;&#31614;&#37027;&#26679;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#34917;&#20805;&#35299;&#37322;&#22312;&#25552;&#39640;ICL&#26041;&#38754;&#30340;&#25928;&#26524;&#26159;&#23384;&#22312;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the role of various demonstration components in the in-context learning (ICL) performance of large language models (LLMs). Specifically, we explore the impacts of ground-truth labels, input distribution, and complementary explanations, particularly when these are altered or perturbed. We build on previous work, which offers mixed findings on how these elements influence ICL. To probe these questions, we employ explainable NLP (XNLP) methods and utilize saliency maps of contrastive demonstrations for both qualitative and quantitative analysis. Our findings reveal that flipping ground-truth labels significantly affects the saliency, though it's more noticeable in larger LLMs. Our analysis of the input distribution at a granular level reveals that changing sentiment-indicative terms in a sentiment analysis task to neutral ones does not have as substantial an impact as altering ground-truth labels. Finally, we find that the effectiveness of complementary explanations in boos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#19978;&#28216;&#20559;&#35265;&#12289;&#26679;&#26412;&#20559;&#35265;&#21644;&#36807;&#24230;&#25918;&#22823;&#20559;&#35265;&#19977;&#26041;&#38754;&#20998;&#26512;&#20102;NLP&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#22914;&#20309;&#24433;&#21709;&#25991;&#26412;&#20998;&#31867;&#30340;&#20844;&#24179;&#24615;&#65292;&#24182;&#38024;&#23545;&#36807;&#24230;&#25918;&#22823;&#20559;&#35265;&#36890;&#36807;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20844;&#24179;&#20998;&#31867;&#25928;&#26524;&#12290;&#25552;&#20986;&#20102;&#26500;&#24314;&#20844;&#27491;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#30340;&#23454;&#29992;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2305.12829</link><description>&lt;p&gt;
&#35770;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#65306;&#22914;&#20309;&#26500;&#24314;&#26356;&#20844;&#27491;&#30340;&#25991;&#26412;&#20998;&#31867;&#65311;
&lt;/p&gt;
&lt;p&gt;
On Bias and Fairness in NLP: How to have a fairer text classification?. (arXiv:2305.12829v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#19978;&#28216;&#20559;&#35265;&#12289;&#26679;&#26412;&#20559;&#35265;&#21644;&#36807;&#24230;&#25918;&#22823;&#20559;&#35265;&#19977;&#26041;&#38754;&#20998;&#26512;&#20102;NLP&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#22914;&#20309;&#24433;&#21709;&#25991;&#26412;&#20998;&#31867;&#30340;&#20844;&#24179;&#24615;&#65292;&#24182;&#38024;&#23545;&#36807;&#24230;&#25918;&#22823;&#20559;&#35265;&#36890;&#36807;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20844;&#24179;&#20998;&#31867;&#25928;&#26524;&#12290;&#25552;&#20986;&#20102;&#26500;&#24314;&#20844;&#27491;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#30340;&#23454;&#29992;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#19981;&#21516;&#26469;&#28304;&#30340;&#20559;&#35265;&#65292;&#21363;&#19978;&#28216;&#20559;&#35265;&#12289;&#26679;&#26412;&#20559;&#35265;&#21644;&#36807;&#24230;&#25918;&#22823;&#20559;&#35265;&#65292;&#20197;&#21450;&#23427;&#20204;&#23545;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#21435;&#20559;&#26041;&#27861;&#28040;&#38500;&#36825;&#20123;&#20559;&#35265;&#23545;&#25991;&#26412;&#20998;&#31867;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#36807;&#24230;&#25918;&#22823;&#20559;&#35265;&#23545;&#25991;&#26412;&#20998;&#31867;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#26368;&#22823;&#12290;&#23558;&#35821;&#35328;&#27169;&#22411;&#22312;&#24179;&#34913;&#19981;&#21516;&#31867;&#21035;&#36523;&#20221;&#32676;&#20307;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#21487;&#20197;&#21435;&#38500;&#36807;&#24230;&#25918;&#22823;&#20559;&#35265;&#65292;&#36827;&#32780;&#26500;&#24314;&#26356;&#20844;&#27491;&#30340;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#30740;&#31350;&#21457;&#29616;&#25552;&#20986;&#20102;&#26500;&#24314;&#26356;&#20844;&#27491;&#30340;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#30340;&#23454;&#29992;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide a holistic analysis of the different sources of bias, Upstream, Sample and Overampflication biases, in NLP models. We investigate how they impact the fairness of the task of text classification. We also investigate the impact of removing these biases using different debiasing techniques on the fairness of text classification. We found that overamplification bias is the most impactful bias on the fairness of text classification. And that removing overamplification bias by fine-tuning the LM models on a dataset with balanced representations of the different identity groups leads to fairer text classification models. Finally, we build on our findings and introduce practical guidelines on how to have a fairer text classification model.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#20041;&#26816;&#32034;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25688;&#35201;&#25551;&#36848;&#30340;&#25991;&#26412;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#24403;&#21069;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.12517</link><description>&lt;p&gt;
&#22522;&#20110;&#25688;&#35201;&#25551;&#36848;&#30340;&#25991;&#26412;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Retrieving Texts based on Abstract Descriptions. (arXiv:2305.12517v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#20041;&#26816;&#32034;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25688;&#35201;&#25551;&#36848;&#30340;&#25991;&#26412;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#24403;&#21069;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#38024;&#23545;&#25991;&#26412;&#30340;&#20449;&#24687;&#25552;&#21462;&#65292;&#25351;&#20196;&#20248;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#23545;&#20110;&#22312;&#22823;&#35268;&#27169;&#25991;&#26723;&#38598;&#21512;&#20013;&#23450;&#20301;&#31526;&#21512;&#32473;&#23450;&#25551;&#36848;&#30340;&#25991;&#26412;&#65288;&#35821;&#20041;&#26816;&#32034;&#65289;&#24182;&#19981;&#36866;&#29992;&#12290;&#22522;&#20110;&#23884;&#20837;&#21521;&#37327;&#30340;&#30456;&#20284;&#24230;&#25628;&#32034;&#21487;&#20197;&#36890;&#36807;&#26597;&#35810;&#25191;&#34892;&#26816;&#32034;&#65292;&#20294;&#23884;&#20837;&#20013;&#30340;&#30456;&#20284;&#24230;&#23450;&#20041;&#19981;&#26126;&#30830;&#19988;&#19981;&#19968;&#33268;&#65292;&#24182;&#19988;&#23545;&#20110;&#35768;&#22810;&#29992;&#20363;&#26469;&#35828;&#37117;&#26159;&#27425;&#20248;&#30340;&#12290;&#37027;&#20040;&#65292;&#20160;&#20040;&#26159;&#26377;&#25928;&#26816;&#32034;&#30340;&#22909;&#30340;&#26597;&#35810;&#34920;&#31034;&#65311;&#25105;&#20204;&#30830;&#23450;&#20102;&#26681;&#25454;&#20869;&#23481;&#30340;&#25688;&#35201;&#25551;&#36848;&#26816;&#32034;&#21477;&#23376;&#30340;&#26126;&#30830;&#23450;&#20041;&#19988;&#19968;&#33268;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#21069;&#25991;&#26412;&#23884;&#20837;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#27169;&#22411;&#65292;&#22312;&#26631;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#20013;&#30340;&#34920;&#29616;&#26174;&#33879;&#25552;&#21319;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#36890;&#36807;&#25552;&#31034;LLM&#33719;&#24471;&#30340;&#27491;&#36127;&#26679;&#26412;&#23545;&#36827;&#34892;&#35757;&#32451;&#12290;&#34429;&#28982;&#24456;&#23481;&#26131;&#20174;LLM&#20013;&#33719;&#24471;&#35757;&#32451;&#26448;&#26009;&#65292;&#20294;LLM&#26080;&#27861;&#30452;&#25509;&#25191;&#34892;&#26816;&#32034;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
While instruction-tuned Large Language Models (LLMs) excel at extracting information from text, they are not suitable for locating texts conforming to a given description in a large document collection (semantic retrieval). Similarity search over embedding vectors does allow to perform retrieval by query, but the similarity reflected in the embedding is ill-defined and non-consistent, and is sub-optimal for many use cases. What, then, is a good query representation for effective retrieval?  We identify the well defined and consistent task of retrieving sentences based on abstract descriptions of their content. We demonstrate the inadequacy of current text embeddings and propose an alternative model that significantly improves when used in standard nearest neighbor search. The model is trained using positive and negative pairs sourced through prompting a LLM. While it is easy to source the training material from an LLM, the retrieval task cannot be performed by the LLM directly. This de
&lt;/p&gt;</description></item></channel></rss>