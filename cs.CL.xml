<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#20854;&#20013;&#20351;&#29992;&#24191;&#20041;&#35821;&#27861;&#35268;&#21017;&#65288;GGRs&#65289;&#26469;&#23454;&#29616;&#32452;&#21512;&#19968;&#33324;&#21270;&#65292;&#23558;&#20854;&#35270;&#20026;&#36716;&#23548;&#20219;&#21153;&#20013;&#30340;&#23545;&#31216;&#24615;&#32422;&#26463;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#24418;&#24335;&#21270;&#20102;&#35821;&#35328;&#36716;&#23548;&#30340;&#24191;&#20041;&#23545;&#31216;&#24615;&#27010;&#24565;&#65292;&#36824;&#19982;&#24378;&#21270;&#23398;&#20064;&#21644;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#26377;&#20851;&#32852;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01629</link><description>&lt;p&gt;
&#12298;&#35770;&#25991;&#26631;&#39064;&#65306;&#24191;&#20041;&#35821;&#27861;&#35268;&#21017;&#21644;&#22522;&#20110;&#32467;&#26500;&#30340;&#19968;&#33324;&#21270;&#8212;&#8212;&#23545;&#20110;&#35789;&#27719;&#20219;&#21153;&#21644;&#36716;&#23548;&#30340;&#32463;&#20856;&#31561;&#21464;&#24615;&#20043;&#22806;&#30340;&#19968;&#33324;&#21270;&#30340;&#31435;&#22330;&#35770;&#25991;&#12299;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Generalized grammar rules and structure-based generalization beyond classical equivariance for lexical tasks and transduction
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01629
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#20854;&#20013;&#20351;&#29992;&#24191;&#20041;&#35821;&#27861;&#35268;&#21017;&#65288;GGRs&#65289;&#26469;&#23454;&#29616;&#32452;&#21512;&#19968;&#33324;&#21270;&#65292;&#23558;&#20854;&#35270;&#20026;&#36716;&#23548;&#20219;&#21153;&#20013;&#30340;&#23545;&#31216;&#24615;&#32422;&#26463;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#24418;&#24335;&#21270;&#20102;&#35821;&#35328;&#36716;&#23548;&#30340;&#24191;&#20041;&#23545;&#31216;&#24615;&#27010;&#24565;&#65292;&#36824;&#19982;&#24378;&#21270;&#23398;&#20064;&#21644;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#26377;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#19968;&#33324;&#21270;&#26159;&#20154;&#31867;&#23398;&#20064;&#35789;&#27719;&#19982;&#29616;&#26377;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#20027;&#35201;&#24046;&#24322;&#20043;&#19968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#33021;&#22815;&#20351;&#29992;&#24191;&#20041;&#35821;&#27861;&#35268;&#21017;&#65288;GGRs&#65289;&#36827;&#34892;&#32452;&#21512;&#19968;&#33324;&#21270;&#30340;&#27169;&#22411;&#65292;GGRs&#26159;&#19968;&#31867;&#22522;&#20110;&#23545;&#31216;&#24615;&#30340;&#36716;&#23548;&#20219;&#21153;&#30340;&#32452;&#21512;&#32422;&#26463;&#65292;&#25105;&#20204;&#23558;&#20854;&#35270;&#20026;&#21463;&#29289;&#29702;&#23398;&#20219;&#21153;&#20013;&#31561;&#21464;&#24615;&#32422;&#26463;&#21551;&#21457;&#30340;&#36716;&#23548;&#31867;&#27604;&#12290;&#38500;&#20102;&#20026;&#35821;&#35328;&#36716;&#23548;&#24418;&#24335;&#21270;&#24191;&#20041;&#30340;&#23545;&#31216;&#24615;&#27010;&#24565;&#20043;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36275;&#22815;&#36890;&#29992;&#65292;&#21487;&#20197;&#21253;&#21547;&#35768;&#22810;&#29616;&#26377;&#24037;&#20316;&#20316;&#20026;&#29305;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#22914;&#20309;&#23454;&#29616;GGRs&#30340;&#24819;&#27861;&#65292;&#24182;&#22312;&#27492;&#36807;&#31243;&#20013;&#19982;&#24378;&#21270;&#23398;&#20064;&#21644;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#24314;&#31435;&#20102;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compositional generalization is one of the main properties which differentiates lexical learning in humans from state-of-art neural networks. We propose a general framework for building models that can generalize compositionally using the concept of Generalized Grammar Rules (GGRs), a class of symmetry-based compositional constraints for transduction tasks, which we view as a transduction analogue of equivariance constraints in physics-inspired tasks. Besides formalizing generalized notions of symmetry for language transduction, our framework is general enough to contain many existing works as special cases. We present ideas on how GGRs might be implemented, and in the process draw connections to reinforcement learning and other areas of research.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#20195;&#29702;&#30340;&#26032;&#30340;&#35268;&#21010;&#22522;&#20934;TravelPlanner&#65292;&#23427;&#20851;&#27880;&#20110;&#26053;&#34892;&#35268;&#21010;&#36825;&#19968;&#24120;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#12290;&#32463;&#36807;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#30446;&#21069;&#30340;&#35821;&#35328;&#20195;&#29702;&#20173;&#26080;&#27861;&#22788;&#29702;&#22914;&#27492;&#22797;&#26434;&#30340;&#35268;&#21010;&#20219;&#21153;&#65292;&#21363;&#20351;&#26368;&#20808;&#36827;&#30340;GPT-4&#20063;&#21482;&#33021;&#36798;&#21040;0.6%&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01622</link><description>&lt;p&gt;
TravelPlanner: &#19968;&#31181;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#20195;&#29702;&#30340;&#30495;&#23454;&#19990;&#30028;&#35268;&#21010;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
TravelPlanner: A Benchmark for Real-World Planning with Language Agents
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#20195;&#29702;&#30340;&#26032;&#30340;&#35268;&#21010;&#22522;&#20934;TravelPlanner&#65292;&#23427;&#20851;&#27880;&#20110;&#26053;&#34892;&#35268;&#21010;&#36825;&#19968;&#24120;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#12290;&#32463;&#36807;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#30446;&#21069;&#30340;&#35821;&#35328;&#20195;&#29702;&#20173;&#26080;&#27861;&#22788;&#29702;&#22914;&#27492;&#22797;&#26434;&#30340;&#35268;&#21010;&#20219;&#21153;&#65292;&#21363;&#20351;&#26368;&#20808;&#36827;&#30340;GPT-4&#20063;&#21482;&#33021;&#36798;&#21040;0.6%&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#35268;&#21010;&#36215;&#21021;&#23601;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#36861;&#27714;&#20043;&#19968;&#65292;&#20294;&#26089;&#26399;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#22823;&#22810;&#38598;&#20013;&#22312;&#21463;&#38480;&#29615;&#22659;&#19979;&#65292;&#22240;&#20026;&#32570;&#20047;&#36827;&#34892;&#20154;&#31867;&#27700;&#24179;&#35268;&#21010;&#25152;&#38656;&#30340;&#35768;&#22810;&#35748;&#30693;&#22522;&#30784;&#12290;&#26368;&#36817;&#65292;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39537;&#21160;&#30340;&#35821;&#35328;&#20195;&#29702;&#23637;&#29616;&#20986;&#20102;&#24037;&#20855;&#20351;&#29992;&#21644;&#25512;&#29702;&#31561;&#26377;&#36259;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#35821;&#35328;&#20195;&#29702;&#33021;&#21542;&#22312;&#36229;&#20986;&#20808;&#21069;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#33539;&#22260;&#30340;&#26356;&#22797;&#26434;&#29615;&#22659;&#20013;&#36827;&#34892;&#35268;&#21010;&#65311;&#20026;&#20102;&#25512;&#36827;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TravelPlanner&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#35268;&#21010;&#22522;&#20934;&#65292;&#19987;&#27880;&#20110;&#26053;&#34892;&#35268;&#21010;&#36825;&#20010;&#24120;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#35268;&#21010;&#22330;&#26223;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#27801;&#30418;&#29615;&#22659;&#65292;&#21508;&#31181;&#29992;&#20110;&#35775;&#38382;&#36817;400&#19975;&#20010;&#25968;&#25454;&#35760;&#24405;&#30340;&#24037;&#20855;&#65292;&#24182;&#21253;&#21547;1225&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#35268;&#21010;&#24847;&#22270;&#21644;&#21442;&#32771;&#35745;&#21010;&#12290;&#32508;&#21512;&#35780;&#20272;&#26174;&#31034;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#20195;&#29702;&#23578;&#19981;&#20855;&#22791;&#22788;&#29702;&#22914;&#27492;&#22797;&#26434;&#30340;&#35268;&#21010;&#20219;&#21153;&#30340;&#33021;&#21147;-&#21363;&#20351;&#26159;GPT-4&#30340;&#25104;&#21151;&#29575;&#20063;&#21482;&#26377;0.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
Planning has been part of the core pursuit for artificial intelligence since its conception, but earlier AI agents mostly focused on constrained settings because many of the cognitive substrates necessary for human-level planning have been lacking. Recently, language agents powered by large language models (LLMs) have shown interesting capabilities such as tool use and reasoning. Are these language agents capable of planning in more complex settings that are out of the reach of prior AI agents? To advance this investigation, we propose TravelPlanner, a new planning benchmark that focuses on travel planning, a common real-world planning scenario. It provides a rich sandbox environment, various tools for accessing nearly four million data records, and 1,225 meticulously curated planning intents and reference plans. Comprehensive evaluations show that the current language agents are not yet capable of handling such complex planning tasks-even GPT-4 only achieves a success rate of 0.6%. La
&lt;/p&gt;</description></item><item><title>MAGDi&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22810;&#20010;&#22823;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#25512;&#29702;&#20132;&#20114;&#34920;&#31034;&#20026;&#22270;&#24418;&#65292;&#26469;&#25913;&#21892;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01620</link><description>&lt;p&gt;
MAGDi&#65306;&#32467;&#26500;&#21270;&#33976;&#39311;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#22270;&#22312;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01620
&lt;/p&gt;
&lt;p&gt;
MAGDi&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22810;&#20010;&#22823;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#25512;&#29702;&#20132;&#20114;&#34920;&#31034;&#20026;&#22270;&#24418;&#65292;&#26469;&#25913;&#21892;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#37325;&#22823;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#28041;&#21450;&#22810;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;&#38271;&#26102;&#38388;&#29983;&#25104;&#65292;&#25104;&#26412;&#39640;&#26114;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#22810;&#26234;&#33021;&#20307;&#26041;&#27861;&#26080;&#27861;&#25552;&#20379;&#19968;&#20010;&#26368;&#32456;&#30340;&#12289;&#21333;&#19968;&#30340;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#25512;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MAGDi&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22810;&#20010;LLM&#20043;&#38388;&#30340;&#25512;&#29702;&#20132;&#20114;&#32467;&#26500;&#21270;&#33976;&#39311;&#21040;&#36739;&#23567;&#30340;&#27169;&#22411;&#20013;&#12290;MAGDi&#36890;&#36807;&#23558;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#34920;&#31034;&#20026;&#22270;&#24418;&#65292;&#20351;&#29992;&#22270;&#24418;&#32534;&#30721;&#22120;&#22686;&#24378;&#22522;&#30784;&#23398;&#29983;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#19977;&#20010;&#30446;&#26631;&#20989;&#25968;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#65306;&#19979;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#12289;&#27491;&#30830;&#21644;&#38169;&#35823;&#25512;&#29702;&#20043;&#38388;&#30340;&#23545;&#27604;&#25439;&#22833;&#20197;&#21450;&#22522;&#20110;&#22270;&#24418;&#30340;&#30446;&#26631;&#26469;&#24314;&#27169;&#20132;&#20114;&#32467;&#26500;&#12290;&#22312;&#19971;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#24120;&#35782;&#21644;&#25968;&#23398;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MAGDi&#25913;&#21892;&#20102;&#36739;&#23567;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20248;&#20110;&#20960;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent interactions between Large Language Model (LLM) agents have shown major improvements on diverse reasoning tasks. However, these involve long generations from multiple models across several rounds, making them expensive. Moreover, these multi-agent approaches fail to provide a final, single model for efficient inference. To address this, we introduce MAGDi, a new method for structured distillation of the reasoning interactions between multiple LLMs into smaller LMs. MAGDi teaches smaller models by representing multi-agent interactions as graphs, augmenting a base student model with a graph encoder, and distilling knowledge using three objective functions: next-token prediction, a contrastive loss between correct and incorrect reasoning, and a graph-based objective to model the interaction structure. Experiments on seven widely-used commonsense and math reasoning benchmarks show that MAGDi improves the reasoning capabilities of smaller models, outperforming several methods th
&lt;/p&gt;</description></item><item><title>KB-Plugin&#26159;&#19968;&#20010;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#30693;&#35782;&#24211;&#19978;&#24341;&#23548;&#31243;&#24207;&#30340;&#21363;&#25554;&#21363;&#29992;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#21033;&#29992;&#20016;&#23500;&#36164;&#28304;KB&#30340;&#27880;&#37322;&#25968;&#25454;&#26469;&#25552;&#21462;&#38382;&#39064;&#30456;&#20851;&#30340;&#27169;&#24335;&#20449;&#24687;&#65292;&#24182;&#23454;&#29616;&#22312;&#20219;&#20309;&#20302;&#36164;&#28304;KB&#19978;&#35825;&#23548;&#31243;&#24207;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01619</link><description>&lt;p&gt;
KB-Plugin:&#19968;&#20010;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#30693;&#35782;&#24211;&#19978;&#24341;&#23548;&#31243;&#24207;&#30340;&#21363;&#25554;&#21363;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
KB-Plugin: A Plug-and-play Framework for Large Language Models to Induce Programs over Low-resourced Knowledge Bases
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01619
&lt;/p&gt;
&lt;p&gt;
KB-Plugin&#26159;&#19968;&#20010;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#30693;&#35782;&#24211;&#19978;&#24341;&#23548;&#31243;&#24207;&#30340;&#21363;&#25554;&#21363;&#29992;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#21033;&#29992;&#20016;&#23500;&#36164;&#28304;KB&#30340;&#27880;&#37322;&#25968;&#25454;&#26469;&#25552;&#21462;&#38382;&#39064;&#30456;&#20851;&#30340;&#27169;&#24335;&#20449;&#24687;&#65292;&#24182;&#23454;&#29616;&#22312;&#20219;&#20309;&#20302;&#36164;&#28304;KB&#19978;&#35825;&#23548;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#24402;&#32435;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#21033;&#29992;&#30693;&#35782;&#24211;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22238;&#31572;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#38382;&#39064;&#30340;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#31243;&#24207;&#24402;&#32435;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#24182;&#34892;&#38382;&#39064;-&#31243;&#24207;&#23545;&#65292;&#22312;LLM&#24847;&#35782;&#21040;&#32473;&#23450;KB&#30340;&#27169;&#24335;&#30340;&#21516;&#26102;&#65292;&#23545;&#20110;&#35768;&#22810;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#30340;&#20302;&#36164;&#28304;KB&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KB-Plugin&#65292;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#26694;&#26550;&#65292;&#20351;LLM&#33021;&#22815;&#22312;&#20219;&#20309;&#20302;&#36164;&#28304;KB&#19978;&#35825;&#23548;&#31243;&#24207;&#12290;&#39318;&#20808;&#65292;KB-Plugin&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#23558;&#32473;&#23450;KB&#30340;&#35814;&#32454;&#27169;&#24335;&#20449;&#24687;&#32534;&#30721;&#21040;&#19968;&#20010;&#21487;&#25554;&#25300;&#27169;&#22359;&#65292;&#21363;&#27169;&#24335;&#25554;&#20214;&#12290;&#20854;&#27425;&#65292;KB-Plugin&#21033;&#29992;&#20016;&#23500;&#36164;&#28304;KB&#20013;&#30340;&#20016;&#23500;&#27880;&#37322;&#25968;&#25454;&#26469;&#35757;&#32451;&#21478;&#19968;&#20010;&#21487;&#25554;&#25300;&#27169;&#22359;&#65292;&#21363;PI&#25554;&#20214;&#65292;&#35813;&#25554;&#20214;&#21487;&#20197;&#24110;&#21161;LLM&#20174;&#20219;&#20309;KB&#30340;&#27169;&#24335;&#25554;&#20214;&#20013;&#25552;&#21462;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#27169;&#24335;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#27492;&#20449;&#24687;&#22312;&#35813;KB&#19978;&#35825;&#23548;&#31243;&#24207;&#12290;&#22312;&#20116;&#20010;&#24322;&#26500;KBQA&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Program induction (PI) has become a promising paradigm for using knowledge bases (KBs) to help large language models (LLMs) answer complex knowledge-intensive questions. Nonetheless, PI typically relies on a large number of parallel question-program pairs to make the LLM aware of the schema of the given KB, and is thus challenging for many low-resourced KBs that lack annotated data. To this end, we propose KB-Plugin, a plug-and-play framework that enables LLMs to induce programs over any low-resourced KB. Firstly, KB-Plugin adopts self-supervised learning to encode the detailed schema information of a given KB into a pluggable module, namely schema plugin. Secondly, KB-Plugin utilizes abundant annotated data from a rich-resourced KB to train another pluggable module, namely PI plugin, which can help the LLM extract question-relevant schema information from the schema plugin of any KB and utilize this information to induce programs over this KB. Experiments on five heterogeneous KBQA da
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#28155;&#21152;&#39118;&#26684;&#21521;&#37327;&#26469;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#29305;&#23450;&#39118;&#26684;&#30340;&#25991;&#26412;&#65292;&#36825;&#31181;&#26041;&#27861;&#31616;&#21333;&#19988;&#26377;&#25928;&#65292;&#26159;&#21457;&#23637;&#26356;&#20855;&#36866;&#24212;&#24615;&#21644;&#26377;&#25928;&#24615;&#30340;AI&#39537;&#21160;&#20132;&#20114;&#31995;&#32479;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01618</link><description>&lt;p&gt;
&#29992;&#20110;&#24341;&#23548;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39118;&#26684;&#21521;&#37327;
&lt;/p&gt;
&lt;p&gt;
Style Vectors for Steering Generative Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#28155;&#21152;&#39118;&#26684;&#21521;&#37327;&#26469;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#29305;&#23450;&#39118;&#26684;&#30340;&#25991;&#26412;&#65292;&#36825;&#31181;&#26041;&#27861;&#31616;&#21333;&#19988;&#26377;&#25928;&#65292;&#26159;&#21457;&#23637;&#26356;&#20855;&#36866;&#24212;&#24615;&#21644;&#26377;&#25928;&#24615;&#30340;AI&#39537;&#21160;&#20132;&#20114;&#31995;&#32479;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36890;&#36807;&#22312;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#23558;&#39118;&#26684;&#21521;&#37327;&#28155;&#21152;&#21040;&#38544;&#34255;&#23618;&#28608;&#27963;&#20013;&#65292;&#26469;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36755;&#20986;&#29305;&#23450;&#39118;&#26684;&#65288;&#22914;&#24773;&#24863;&#12289;&#24773;&#32490;&#25110;&#20889;&#20316;&#39118;&#26684;&#65289;&#30340;&#31574;&#30053;&#12290;&#19982;&#26356;&#22797;&#26434;&#30340;&#35757;&#32451;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#34920;&#26126;&#21487;&#20197;&#20174;&#35760;&#24405;&#30340;&#23618;&#28608;&#27963;&#20013;&#31616;&#21333;&#22320;&#35745;&#31639;&#20986;&#29305;&#23450;&#39118;&#26684;&#30340;&#39118;&#26684;&#21521;&#37327;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#36825;&#26679;&#30340;&#39118;&#26684;&#21521;&#37327;&#26469;&#24433;&#21709;&#29983;&#25104;&#25991;&#26412;&#30340;&#39118;&#26684;&#30340;&#28608;&#27963;&#24037;&#31243;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#32454;&#33268;&#21644;&#21487;&#21442;&#25968;&#21270;&#30340;&#29305;&#28857;&#65292;&#21306;&#21035;&#20110;&#25552;&#31034;&#24037;&#31243;&#12290;&#25152;&#23637;&#31034;&#30340;&#30740;&#31350;&#26159;&#21521;&#26356;&#20855;&#36866;&#24212;&#24615;&#21644;&#26377;&#25928;&#24615;&#30340;AI&#39537;&#21160;&#20132;&#20114;&#31995;&#32479;&#21457;&#23637;&#36808;&#20986;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research explores strategies for steering the output of large language models (LLMs) towards specific styles, such as sentiment, emotion, or writing style, by adding style vectors to the activations of hidden layers during text generation. We show that style vectors can be simply computed from recorded layer activations for input texts in a specific style in contrast to more complex training-based approaches. Through a series of experiments, we demonstrate the effectiveness of activation engineering using such style vectors to influence the style of generated text in a nuanced and parameterisable way, distinguishing it from prompt engineering. The presented research constitutes a significant step towards developing more adaptive and effective AI-empowered interactive systems.
&lt;/p&gt;</description></item><item><title>Nomic Embed&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#21487;&#22797;&#29616;&#12289;&#24320;&#28304;&#12289;&#24320;&#25918;&#26435;&#37325;&#12289;&#24320;&#25918;&#25968;&#25454;&#30340;8192&#19978;&#19979;&#25991;&#38271;&#24230;&#33521;&#25991;&#25991;&#26412;&#23884;&#20837;&#22120;&#65292;&#22312;&#30701;&#19978;&#19979;&#25991;&#21644;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#19978;&#20248;&#20110;OpenAI Ada-002&#21644;OpenAI text-embedding-3-small&#12290;</title><link>https://rss.arxiv.org/abs/2402.01613</link><description>&lt;p&gt;
Nomic Embed&#65306;&#35757;&#32451;&#21487;&#22797;&#29616;&#30340;&#38271;&#19978;&#19979;&#25991;&#25991;&#26412;&#23884;&#20837;&#22120;
&lt;/p&gt;
&lt;p&gt;
Nomic Embed: Training a Reproducible Long Context Text Embedder
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01613
&lt;/p&gt;
&lt;p&gt;
Nomic Embed&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#21487;&#22797;&#29616;&#12289;&#24320;&#28304;&#12289;&#24320;&#25918;&#26435;&#37325;&#12289;&#24320;&#25918;&#25968;&#25454;&#30340;8192&#19978;&#19979;&#25991;&#38271;&#24230;&#33521;&#25991;&#25991;&#26412;&#23884;&#20837;&#22120;&#65292;&#22312;&#30701;&#19978;&#19979;&#25991;&#21644;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#19978;&#20248;&#20110;OpenAI Ada-002&#21644;OpenAI text-embedding-3-small&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#25551;&#36848;&#20102;nomic-embed-text-v1&#30340;&#35757;&#32451;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#21487;&#22797;&#29616;&#12289;&#24320;&#28304;&#12289;&#24320;&#25918;&#26435;&#37325;&#12289;&#24320;&#25918;&#25968;&#25454;&#30340;8192&#19978;&#19979;&#25991;&#38271;&#24230;&#33521;&#25991;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#22312;&#30701;&#19978;&#19979;&#25991;&#21644;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#19978;&#22343;&#20248;&#20110;OpenAI Ada-002&#21644;OpenAI text-embedding-3-small&#12290;&#25105;&#20204;&#22312;Apache 2&#35768;&#21487;&#19979;&#21457;&#24067;&#20102;&#35757;&#32451;&#20195;&#30721;&#21644;&#27169;&#22411;&#26435;&#37325;&#12290;&#19982;&#20854;&#20182;&#24320;&#28304;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;2.35&#20159;&#20010;&#31574;&#21010;&#25991;&#26412;&#23545;&#30340;&#35757;&#32451;&#25968;&#25454;&#21152;&#36733;&#22120;&#65292;&#21487;&#20197;&#23436;&#20840;&#22797;&#29616;nomic-embed-text-v1&#12290;&#20320;&#21487;&#20197;&#22312;https://github.com/nomic-ai/contrastors&#25214;&#21040;&#27169;&#22411;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This technical report describes the training of nomic-embed-text-v1, the first fully reproducible, open-source, open-weights, open-data, 8192 context length English text embedding model that outperforms both OpenAI Ada-002 and OpenAI text-embedding-3-small on short and long-context tasks. We release the training code and model weights under an Apache 2 license. In contrast with other open-source models, we release a training data loader with 235 million curated text pairs that allows for the full replication of nomic-embed-text-v1. You can find code and data to replicate the model at https://github.com/nomic-ai/contrastors
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181; groundbreaking &#30340;&#21387;&#21147;&#26816;&#27979;&#31639;&#27861;&#65292;&#21033;&#29992;&#33258;&#21160;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#23454;&#26102;&#20998;&#26512;&#32842;&#22825;&#23545;&#35805;&#65292;&#20197;&#23458;&#35266;&#27979;&#37327;&#21592;&#24037;&#30340;&#24515;&#29702;&#20581;&#24247;&#27700;&#24179;&#65292;&#24182;&#26681;&#25454;&#35821;&#35328;&#29983;&#29289;&#26631;&#24535;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#23454;&#26102;&#27835;&#30103;&#24314;&#35758;&#65292;&#20026;&#20225;&#19994;&#25552;&#20379;&#26089;&#26399;&#24178;&#39044;&#21644;&#25903;&#25345;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01592</link><description>&lt;p&gt;
&#36808;&#21521;&#21487;&#25345;&#32493;&#30340;&#24037;&#20316;&#22330;&#25152;&#24515;&#29702;&#20581;&#24247;&#65306;&#26089;&#26399;&#24178;&#39044;&#21644;&#25903;&#25345;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Sustainable Workplace Mental Health: A Novel Approach to Early Intervention and Support
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181; groundbreaking &#30340;&#21387;&#21147;&#26816;&#27979;&#31639;&#27861;&#65292;&#21033;&#29992;&#33258;&#21160;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#23454;&#26102;&#20998;&#26512;&#32842;&#22825;&#23545;&#35805;&#65292;&#20197;&#23458;&#35266;&#27979;&#37327;&#21592;&#24037;&#30340;&#24515;&#29702;&#20581;&#24247;&#27700;&#24179;&#65292;&#24182;&#26681;&#25454;&#35821;&#35328;&#29983;&#29289;&#26631;&#24535;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#23454;&#26102;&#27835;&#30103;&#24314;&#35758;&#65292;&#20026;&#20225;&#19994;&#25552;&#20379;&#26089;&#26399;&#24178;&#39044;&#21644;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21592;&#24037;&#30340;&#20581;&#24247;&#26159;&#24403;&#20195;&#24037;&#20316;&#22330;&#25152;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#27491;&#22914;&#32654;&#22269;&#24515;&#29702;&#23398;&#21327;&#20250;2021&#24180;&#30340;&#25253;&#21578;&#25152;&#25351;&#20986;&#30340;&#65292;71%&#30340;&#21592;&#24037;&#20250;&#32463;&#21382;&#21387;&#21147;&#25110;&#32039;&#24352;&#12290;&#36825;&#31181;&#21387;&#21147;&#23545;&#24037;&#20316;&#22330;&#25152;&#30340;&#31163;&#32844;&#29575;&#21644;&#32570;&#21220;&#29575;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#65292;61%&#30340;&#31163;&#32844;&#21644;16%&#30340;&#30149;&#20551;&#24402;&#22240;&#20110;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#19981;&#20339;&#12290;&#38599;&#20027;&#38754;&#20020;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#21592;&#24037;&#36890;&#24120;&#22312;&#36798;&#21040;&#21361;&#26426;&#28857;&#20043;&#21069;&#37117;&#19981;&#30693;&#36947;&#33258;&#24049;&#30340;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#65292;&#23548;&#33268;&#23545;&#20225;&#19994;&#31119;&#21033;&#24453;&#36935;&#30340;&#21033;&#29992;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21069;&#25152;&#26410;&#26377;&#30340;&#21387;&#21147;&#26816;&#27979;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#25552;&#20379;&#23454;&#26102;&#30340;&#25903;&#25345;&#12290;&#21033;&#29992;&#33258;&#21160;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20998;&#26512;&#32842;&#22825;&#23545;&#35805;&#23458;&#35266;&#22320;&#27979;&#37327;&#24515;&#29702;&#20581;&#24247;&#27700;&#24179;&#65292;&#24182;&#26681;&#25454;&#35821;&#35328;&#29983;&#29289;&#26631;&#24535;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#23454;&#26102;&#27835;&#30103;&#24314;&#35758;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#36825;&#20123;&#21019;&#26032;&#25216;&#26415;&#25972;&#21512;&#21040;&#20225;&#19994;&#29615;&#22659;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Employee well-being is a critical concern in the contemporary workplace, as highlighted by the American Psychological Association's 2021 report, indicating that 71% of employees experience stress or tension. This stress contributes significantly to workplace attrition and absenteeism, with 61% of attrition and 16% of sick days attributed to poor mental health. A major challenge for employers is that employees often remain unaware of their mental health issues until they reach a crisis point, resulting in limited utilization of corporate well-being benefits. This research addresses this challenge by presenting a groundbreaking stress detection algorithm that provides real-time support preemptively. Leveraging automated chatbot technology, the algorithm objectively measures mental health levels by analyzing chat conversations, offering personalized treatment suggestions in real-time based on linguistic biomarkers. The study explores the feasibility of integrating these innovations into p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BAT&#65292;&#23427;&#32467;&#21512;&#20102;&#21452;&#32819;&#22768;&#38899;&#22330;&#26223;&#20998;&#26512;&#27169;&#22411;&#30340;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#33021;&#21147;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#22797;&#21046;&#20154;&#31867;&#30340;&#31354;&#38388;&#22768;&#38899;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#30340;&#21452;&#32819;&#38899;&#39057;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#31354;&#38388;&#22768;&#38899;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;BAT&#22312;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#21644;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01591</link><description>&lt;p&gt;
BAT: &#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#20851;&#20110;&#31354;&#38388;&#22768;&#38899;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
BAT: Learning to Reason about Spatial Sounds with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BAT&#65292;&#23427;&#32467;&#21512;&#20102;&#21452;&#32819;&#22768;&#38899;&#22330;&#26223;&#20998;&#26512;&#27169;&#22411;&#30340;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#33021;&#21147;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#22797;&#21046;&#20154;&#31867;&#30340;&#31354;&#38388;&#22768;&#38899;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#30340;&#21452;&#32819;&#38899;&#39057;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#31354;&#38388;&#22768;&#38899;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;BAT&#22312;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#21644;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#22768;&#38899;&#25512;&#29702;&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#20154;&#31867;&#25216;&#33021;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#22768;&#38899;&#26469;&#23548;&#33322;&#21644;&#35299;&#37322;&#25105;&#20204;&#30340;&#21608;&#22260;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;BAT&#65292;&#23427;&#23558;&#21452;&#32819;&#22768;&#38899;&#22330;&#26223;&#20998;&#26512;&#27169;&#22411;&#30340;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#33021;&#21147;&#19982;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#20197;&#22797;&#21046;&#36825;&#31181;&#22266;&#26377;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#37326;&#22806;&#31354;&#38388;&#22768;&#38899;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#65292;&#25105;&#20204;&#20351;&#29992;AudioSet&#21644;SoundSpaces 2.0&#21512;&#25104;&#20102;&#19968;&#20010;&#21452;&#32819;&#38899;&#39057;&#25968;&#25454;&#38598;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#31354;&#38388;&#22768;&#38899;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;SpatialSoundQA&#65292;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;QA&#20219;&#21153;&#65292;&#20197;&#35757;&#32451;BAT&#22312;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#21644;&#25512;&#29702;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;BAT&#30340;&#22768;&#23398;&#21069;&#31471;&#32534;&#30721;&#22120;&#26159;&#19968;&#31181;&#21517;&#20026;Spatial Audio Spectrogram Transformer&#65288;Spatial-AST&#65289;&#30340;&#21019;&#26032;&#31354;&#38388;&#38899;&#39057;&#32534;&#30721;&#22120;&#65292;&#23427;&#26412;&#36523;&#22312;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#12289;&#31354;&#38388;&#23450;&#20301;&#21644;&#36317;&#31163;&#20272;&#35745;&#31561;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;Spatial-AST&#19982;LLaMA-2 7B&#38598;&#25104;&#65292;
&lt;/p&gt;
&lt;p&gt;
Spatial sound reasoning is a fundamental human skill, enabling us to navigate and interpret our surroundings based on sound. In this paper we present BAT, which combines the spatial sound perception ability of a binaural acoustic scene analysis model with the natural language reasoning capabilities of a large language model (LLM) to replicate this innate ability. To address the lack of existing datasets of in-the-wild spatial sounds, we synthesized a binaural audio dataset using AudioSet and SoundSpaces 2.0. Next, we developed SpatialSoundQA, a spatial sound-based question-answering dataset, offering a range of QA tasks that train BAT in various aspects of spatial sound perception and reasoning. The acoustic front end encoder of BAT is a novel spatial audio encoder named Spatial Audio Spectrogram Transformer, or Spatial-AST, which by itself achieves strong performance across sound event detection, spatial localization, and distance estimation. By integrating Spatial-AST with LLaMA-2 7B
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#39044;&#20808;&#35268;&#21010;&#12289;&#35268;&#21010;&#36807;&#31243;&#20013;&#21644;&#35745;&#21010;&#21518;&#26816;&#26597;&#19977;&#31181;&#31574;&#30053;&#26469;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20197;&#21450;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19982;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01586</link><description>&lt;p&gt;
TrustAgent: &#36890;&#36807;&#20195;&#29702;&#26500;&#25104;&#23454;&#29616;&#23433;&#20840;&#21487;&#20449;&#36182;&#30340;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#39044;&#20808;&#35268;&#21010;&#12289;&#35268;&#21010;&#36807;&#31243;&#20013;&#21644;&#35745;&#21010;&#21518;&#26816;&#26597;&#19977;&#31181;&#31574;&#30053;&#26469;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20197;&#21450;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19982;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20854;&#21487;&#20449;&#24230;&#20173;&#26410;&#24471;&#21040;&#28145;&#20837;&#25506;&#32034;&#12290;&#30001;&#20110;&#20195;&#29702;&#21487;&#20197;&#30452;&#25509;&#19982;&#29289;&#29702;&#29615;&#22659;&#20132;&#20114;&#65292;&#20854;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#23545;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#32500;&#24230;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19977;&#31181;&#31574;&#30053;&#65306;&#39044;&#20808;&#35268;&#21010;&#31574;&#30053;&#65292;&#22312;&#29983;&#25104;&#35745;&#21010;&#20043;&#21069;&#21521;&#27169;&#22411;&#27880;&#20837;&#23433;&#20840;&#30693;&#35782;&#65307;&#35268;&#21010;&#36807;&#31243;&#20013;&#31574;&#30053;&#65292;&#22312;&#29983;&#25104;&#35745;&#21010;&#26102;&#22686;&#24378;&#23433;&#20840;&#24615;&#65307;&#35745;&#21010;&#21518;&#26816;&#26597;&#31574;&#30053;&#65292;&#36890;&#36807;&#35745;&#21010;&#21518;&#26816;&#26597;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#36890;&#36807;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#26377;&#25928;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#20197;&#21450;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#19982;&#20854;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of LLM-based agents has garnered considerable attention, yet their trustworthiness remains an under-explored area. As agents can directly interact with the physical environment, their reliability and safety is critical. This paper presents an Agent-Constitution-based agent framework, TrustAgent, an initial investigation into improving the safety dimension of trustworthiness in LLM-based agents. This framework consists of threefold strategies: pre-planning strategy which injects safety knowledge to the model prior to plan generation, in-planning strategy which bolsters safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Through experimental analysis, we demonstrate how these approaches can effectively elevate an LLM agent's safety by identifying and preventing potential dangers. Furthermore, we explore the intricate relationships between safety and helpfulness, and between the model's reasoning ability and its efficac
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#37096;&#20998;&#33258;&#21160;&#21270;&#30340;&#35821;&#35328;&#31995;&#32479;&#28436;&#21270;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#21382;&#21490;&#35821;&#35328;&#24418;&#24335;&#19982;&#29616;&#20195;&#35821;&#35328;&#24418;&#24335;&#20043;&#38388;&#30340;&#20013;&#38388;&#22768;&#38899;&#21464;&#21270;&#27493;&#39588;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01582</link><description>&lt;p&gt;
&#23545;&#20110;&#35821;&#35328;&#31995;&#32479;&#28436;&#21270;&#25512;&#26029;&#30340;&#33258;&#21160;&#21270;&#22768;&#38899;&#21464;&#21270;&#39044;&#27979;&#65306;&#22270;&#21345;&#35834;&#20122;&#20154;&#30740;&#31350;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Automating Sound Change Prediction for Phylogenetic Inference: A Tukanoan Case Study
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#37096;&#20998;&#33258;&#21160;&#21270;&#30340;&#35821;&#35328;&#31995;&#32479;&#28436;&#21270;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#21382;&#21490;&#35821;&#35328;&#24418;&#24335;&#19982;&#29616;&#20195;&#35821;&#35328;&#24418;&#24335;&#20043;&#38388;&#30340;&#20013;&#38388;&#22768;&#38899;&#21464;&#21270;&#27493;&#39588;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#32452;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#33258;&#21160;&#21270;&#35821;&#35328;&#31995;&#32479;&#28436;&#21270;&#30340;&#25512;&#26029;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;(1)&#21516;&#28304;&#35789;&#38598;&#21450;&#20854;&#30456;&#24212;&#30340;&#21407;&#22411;&#24418;&#24335;&#21644;&#38899;&#21464;&#35268;&#24459;&#65292;(2)&#20174;&#38899;&#32032;&#21040;&#20854;&#21457;&#38899;&#29305;&#24449;&#30340;&#26144;&#23556;&#20197;&#21450;(3)&#19968;&#31181;&#22768;&#38899;&#21464;&#21270;&#30340;&#31867;&#22411;&#23398;&#25968;&#25454;&#24211;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#36825;&#20123;&#22768;&#38899;&#21464;&#21270;&#25968;&#25454;&#26469;&#26435;&#34913;&#38899;&#32032;&#20043;&#38388;&#30340;&#21457;&#38899;&#24046;&#36317;&#65292;&#24182;&#39044;&#27979;&#21382;&#21490;&#19978;&#30340;&#21407;&#22411;&#24418;&#24335;&#19982;&#20854;&#29616;&#20195;&#21518;&#20195;&#20043;&#38388;&#30340;&#20013;&#38388;&#22768;&#38899;&#21464;&#21270;&#27493;&#39588;&#65292;&#20174;&#32780;&#22312;&#35813;&#26041;&#27861;&#20013;&#37096;&#20998;&#20195;&#26367;&#20102;&#35821;&#35328;&#23398;&#19987;&#23478;&#12290;&#22312;&#25105;&#20204;&#23545;&#22270;&#21345;&#35834;&#20122;&#35821;&#35328;&#30340;&#26368;&#20339;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#29983;&#25104;&#30340;&#26641;&#19982;&#20351;&#29992;&#19987;&#23478;&#27880;&#37322;&#30340;&#26641;&#30456;&#27604;&#65292;&#20855;&#26377;0.12&#30340;&#24191;&#20041;&#22235;&#20998;&#36317;&#65292;&#36825;&#26159;&#20854;&#20182;&#21322;&#33258;&#21160;&#22522;&#32447;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#31070;&#32463;&#26041;&#27861;&#21644;&#22522;&#20110;&#31616;&#27905;&#21407;&#21017;&#30340;&#26641;&#39044;&#27979;&#30340;&#28508;&#22312;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#25105;&#20204;&#36824;&#23581;&#35797;&#20102;&#26368;&#23567;&#27867;&#21270;&#23398;&#20064;&#22120;&#36827;&#34892;&#33258;&#21160;&#38899;&#21464;&#35268;&#24459;&#24402;&#32435;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#20854;&#19982;&#35821;&#35328;&#23398;&#23478;&#30340;&#24402;&#32435;&#32467;&#26524;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe a set of new methods to partially automate linguistic phylogenetic inference given (1) cognate sets with their respective protoforms and sound laws, (2) a mapping from phones to their articulatory features and (3) a typological database of sound changes. We train a neural network on these sound change data to weight articulatory distances between phones and predict intermediate sound change steps between historical protoforms and their modern descendants, replacing a linguistic expert in part of a parsimony-based phylogenetic inference algorithm. In our best experiments on Tukanoan languages, this method produces trees with a Generalized Quartet Distance of 0.12 from a tree that used expert annotations, a significant improvement over other semi-automated baselines. We discuss potential benefits and drawbacks to our neural approach and parsimony-based tree prediction. We also experiment with a minimal generalization learner for automatic sound law induction, finding it compa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#30740;&#31350;&#20116;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35780;&#20272;&#20102;&#38024;&#23545;&#31038;&#20132;&#35821;&#35328;&#20219;&#21153;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#24773;&#22659;&#19979;&#23545;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;TRILLsson&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#35821;&#38899;&#25968;&#25454;&#20013;&#30340;&#31038;&#20132;&#35821;&#35328;&#29305;&#24449;&#65292;&#25552;&#21319;&#20102;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01579</link><description>&lt;p&gt;
&#22810;&#37325;&#35821;&#35328;&#29615;&#22659;&#19979;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#36328;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How Paralingual are Paralinguistic Representations? A Case Study in Speech Emotion Recognition
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#30740;&#31350;&#20116;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35780;&#20272;&#20102;&#38024;&#23545;&#31038;&#20132;&#35821;&#35328;&#20219;&#21153;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#24773;&#22659;&#19979;&#23545;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;TRILLsson&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#35821;&#38899;&#25968;&#25454;&#20013;&#30340;&#31038;&#20132;&#35821;&#35328;&#29305;&#24449;&#65292;&#25552;&#21319;&#20102;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PTM&#65289;&#22312;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#21508;&#31181;PTM&#34920;&#31034;&#20316;&#20026;SER&#19979;&#28216;&#27169;&#22411;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#38024;&#23545;&#31038;&#20132;&#35821;&#35328;&#20219;&#21153;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;PTM&#22312;SER&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;PTM&#36824;&#27809;&#26377;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#36827;&#34892;SER&#35780;&#20272;&#65292;&#19988;&#21482;&#28041;&#21450;&#33521;&#35821;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#20116;&#31181;PTM&#65288;TRILLsson&#12289;wav2vec2&#12289;XLS-R&#12289;x-vector&#12289;Whisper&#65289;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#30740;&#31350;&#65292;&#35780;&#20272;&#31038;&#20132;&#35821;&#35328;PTM&#65288;TRILLsson&#65289;&#22312;&#22810;&#31181;&#35821;&#35328;&#24773;&#22659;&#19979;&#23545;SER&#30340;&#25928;&#26524;&#12290;TRILLsson&#30340;&#34920;&#31034;&#22312;&#25152;&#26377;PTM&#20013;&#36798;&#21040;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;&#36825;&#34920;&#26126;TRILLsson&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#35821;&#38899;&#25968;&#25454;&#20013;&#30340;&#21508;&#31181;&#31038;&#20132;&#35821;&#35328;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;SER&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Models (PTMs) have facilitated substantial progress in the field of Speech Emotion Recognition (SER). SER is an area with applications ranging from HumanComputer Interaction to Healthcare. Recent studies have leveraged various PTM representations as input features for downstream models for SER. PTM specifically pre-trained for paralinguistic tasks have obtained state-of-the-art (SOTA) performance for SER. However, such PTM haven't been evaluated for SER in multilingual settings and experimented only with English. So, we fill this gap, by performing a comprehensive comparative study of five PTMs (TRILLsson, wav2vec2, XLS-R, x-vector, Whisper) for assessing the effectiveness of paralingual PTM (TRILLsson) for SER across multiple languages. Representations from TRILLsson achieved the best performance among all the PTMs. This demonstrates that TRILLsson is able to effectively capture the various paralinguistic features from speech data for better SER. We also show that downstre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#20174;&#20914;&#31361;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#36827;&#34892;&#25968;&#25454;&#25366;&#25496;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#36845;&#20195;&#30340;&#20027;&#21160;&#23398;&#20064;&#36807;&#31243;&#65292;&#32467;&#21512;&#22823;&#22411;&#30340;&#20165;&#32534;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#21462;&#19982;&#20914;&#31361;&#21160;&#24577;&#30456;&#20851;&#30340;&#23376;&#31867;&#20107;&#20214;&#65292;&#36798;&#21040;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01577</link><description>&lt;p&gt;
&#26469;&#33258;&#20914;&#31361;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#25968;&#25454;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Deep Active Learning for Data Mining from Conflict Text Corpora
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#20174;&#20914;&#31361;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#36827;&#34892;&#25968;&#25454;&#25366;&#25496;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#36845;&#20195;&#30340;&#20027;&#21160;&#23398;&#20064;&#36807;&#31243;&#65292;&#32467;&#21512;&#22823;&#22411;&#30340;&#20165;&#32534;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#21462;&#19982;&#20914;&#31361;&#21160;&#24577;&#30456;&#20851;&#30340;&#23376;&#31867;&#20107;&#20214;&#65292;&#36798;&#21040;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20998;&#36776;&#29575;&#30340;&#27494;&#35013;&#20914;&#31361;&#20107;&#20214;&#25968;&#25454;&#20197;&#21450;&#30456;&#20851;&#36807;&#31243;&#24050;&#32463;&#36890;&#36807;UCDP GED&#12289;ACLED&#31561;&#25968;&#25454;&#38598;&#24443;&#24213;&#25913;&#21464;&#20102;&#25919;&#27835;&#20105;&#35770;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#36825;&#20123;&#25968;&#25454;&#38598;&#20165;&#38480;&#20110;&#25910;&#38598;&#26102;&#31354;&#65288;&#39640;&#20998;&#36776;&#29575;&#65289;&#21644;&#24378;&#24230;&#25968;&#25454;&#12290;&#20851;&#20110;&#30446;&#26631;&#12289;&#25112;&#26415;&#12289;&#30446;&#30340;&#31561;&#21160;&#24577;&#20449;&#24687;&#24456;&#23569;&#34987;&#25910;&#38598;&#65292;&#36825;&#26159;&#22240;&#20026;&#25968;&#25454;&#25910;&#38598;&#30340;&#24037;&#20316;&#37327;&#38750;&#24120;&#22823;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#25968;&#25454;&#38598;&#20381;&#36182;&#20110;&#20016;&#23500;&#30340;&#25991;&#26412;&#25968;&#25454;&#24211;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25366;&#25496;&#19982;&#27599;&#20010;&#20107;&#20214;&#30456;&#20851;&#30340;&#26356;&#22810;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24265;&#20215;&#19988;&#39640;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20027;&#21160;&#23398;&#20064;&#26469;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#39034;&#24207;&#65288;&#26377;&#23548;&#21521;&#30340;&#65289;&#20154;&#24037;&#36755;&#20837;&#30340;&#36845;&#20195;&#36807;&#31243;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#36880;&#27493;&#35757;&#32451;&#65288;&#24494;&#35843;&#65289;&#19968;&#20010;&#22823;&#22411;&#30340;&#20165;&#32534;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#25552;&#21462;&#19982;&#20914;&#31361;&#21160;&#24577;&#30456;&#20851;&#30340;&#23376;&#31867;&#20107;&#20214;&#12290;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#65288;&#37329;&#26631;&#20934;&#65289;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-resolution event data on armed conflict and related processes have revolutionized the study of political contention with datasets like UCDP GED, ACLED etc. However, most of these datasets limit themselves to collecting spatio-temporal (high-resolution) and intensity data. Information on dynamics, such as targets, tactics, purposes etc. are rarely collected owing to the extreme workload of collecting data. However, most datasets rely on a rich corpus of textual data allowing further mining of further information connected to each event. This paper proposes one such approach that is inexpensive and high performance, leveraging active learning - an iterative process of improving a machine learning model based on sequential (guided) human input. Active learning is employed to then step-wise train (fine-tuning) of a large, encoder-only language model adapted for extracting sub-classes of events relating to conflict dynamics. The approach shows performance similar to human (gold-standar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#30446;&#21069;&#30340;&#35770;&#25454;&#25688;&#35201;&#26041;&#27861;&#22312;&#25429;&#25417;&#22810;&#26679;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#22810;&#26679;&#24615;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#24847;&#35265;&#12289;&#27880;&#37322;&#32773;&#21644;&#26469;&#28304;&#12290;&#25152;&#30740;&#31350;&#30340;&#20851;&#38190;&#28857;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#36890;&#29992;LLMs&#21644;&#19987;&#38376;&#30340;KPA&#27169;&#22411;&#37117;&#26377;&#20114;&#34917;&#30340;&#20248;&#21183;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#21270;&#23558;&#26377;&#21161;&#20110;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#24212;&#23545;&#35770;&#35777;&#25688;&#35201;&#20013;&#30340;&#22810;&#26679;&#24615;&#38656;&#35201;&#37319;&#29992;&#22810;&#31181;&#31574;&#30053;&#26469;&#22788;&#29702;&#20027;&#35266;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01535</link><description>&lt;p&gt;
&#35770;&#35770;&#25991;&#25688;&#35201;&#20013;&#30340;&#22810;&#26679;&#24615;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Empirical Analysis of Diversity in Argument Summarization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#30446;&#21069;&#30340;&#35770;&#25454;&#25688;&#35201;&#26041;&#27861;&#22312;&#25429;&#25417;&#22810;&#26679;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#22810;&#26679;&#24615;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#24847;&#35265;&#12289;&#27880;&#37322;&#32773;&#21644;&#26469;&#28304;&#12290;&#25152;&#30740;&#31350;&#30340;&#20851;&#38190;&#28857;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#36890;&#29992;LLMs&#21644;&#19987;&#38376;&#30340;KPA&#27169;&#22411;&#37117;&#26377;&#20114;&#34917;&#30340;&#20248;&#21183;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#21270;&#23558;&#26377;&#21161;&#20110;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#24212;&#23545;&#35770;&#35777;&#25688;&#35201;&#20013;&#30340;&#22810;&#26679;&#24615;&#38656;&#35201;&#37319;&#29992;&#22810;&#31181;&#31574;&#30053;&#26469;&#22788;&#29702;&#20027;&#35266;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#31038;&#20250;&#35752;&#35770;&#20013;&#65292;&#25552;&#20379;&#39640;&#27700;&#24179;&#30340;&#35770;&#25454;&#26159;&#20419;&#36827;&#21442;&#19982;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#30446;&#21069;&#30340;&#35770;&#25454;&#25688;&#35201;&#26041;&#27861;&#32570;&#22833;&#20102;&#36825;&#39033;&#20219;&#21153;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#21363;&#25429;&#25417;&#22810;&#26679;&#24615;&#65292;&#36825;&#23545;&#20110;&#21253;&#23481;&#22810;&#20010;&#35266;&#28857;&#26159;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#65306;&#24847;&#35265;&#12289;&#27880;&#37322;&#32773;&#21644;&#26469;&#28304;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#31181;&#21517;&#20026;&#20851;&#38190;&#28857;&#20998;&#26512;&#30340;&#27969;&#34892;&#35770;&#25454;&#25688;&#35201;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#26174;&#31034;&#36825;&#20123;&#26041;&#27861;&#22312;(1)&#20195;&#34920;&#23569;&#25968;&#20154;&#20849;&#20139;&#30340;&#35770;&#28857;&#19978;&#65292;(2)&#22788;&#29702;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#30340;&#25968;&#25454;&#20197;&#21450;(3)&#19982;&#20154;&#24037;&#25552;&#20379;&#30340;&#20027;&#35266;&#27880;&#37322;&#30456;&#19968;&#33268;&#26041;&#38754;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#29992;&#30340;LLM&#21644;&#19987;&#38376;&#30340;KPA&#27169;&#22411;&#37117;&#34920;&#29616;&#20986;&#20102;&#36825;&#31181;&#34892;&#20026;&#65292;&#20294;&#20855;&#26377;&#20114;&#34917;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#21270;&#21487;&#33021;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#24212;&#23545;&#35770;&#35777;&#25688;&#35201;&#20013;&#30340;&#22810;&#26679;&#24615;&#38656;&#35201;&#37319;&#29992;&#19968;&#31995;&#21015;&#31574;&#30053;&#26469;&#22788;&#29702;&#20027;&#35266;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Presenting high-level arguments is a crucial task for fostering participation in online societal discussions. Current argument summarization approaches miss an important facet of this task -- capturing diversity -- which is important for accommodating multiple perspectives. We introduce three aspects of diversity: those of opinions, annotators, and sources. We evaluate approaches to a popular argument summarization task called Key Point Analysis, which shows how these approaches struggle to (1) represent arguments shared by few people, (2) deal with data from various sources, and (3) align with subjectivity in human-provided annotations. We find that both general-purpose LLMs and dedicated KPA models exhibit this behavior, but have complementary strengths. Further, we observe that diversification of training data may ameliorate generalization. Addressing diversity in argument summarization requires a mix of strategies to deal with subjectivity.
&lt;/p&gt;</description></item><item><title>&#25512;&#27979;&#35299;&#30721;&#26159;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#30340;&#25216;&#26415;&#65292;&#20294;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36873;&#25321;&#30340;&#33609;&#31295;&#27169;&#22411;&#29983;&#25104;&#30340;&#20196;&#29260;&#34987;&#30446;&#26631;&#27169;&#22411;&#25509;&#21463;&#30340;&#27010;&#29575;&#36234;&#39640;&#65292;&#21534;&#21520;&#37327;&#36234;&#20302;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#21508;&#31181;&#22240;&#32032;&#23545;&#25512;&#27979;&#35299;&#30721;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#27169;&#22411;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01528</link><description>&lt;p&gt;
&#35299;&#30721;&#25512;&#27979;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Decoding Speculative Decoding
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01528
&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#26159;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#30340;&#25216;&#26415;&#65292;&#20294;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36873;&#25321;&#30340;&#33609;&#31295;&#27169;&#22411;&#29983;&#25104;&#30340;&#20196;&#29260;&#34987;&#30446;&#26631;&#27169;&#22411;&#25509;&#21463;&#30340;&#27010;&#29575;&#36234;&#39640;&#65292;&#21534;&#21520;&#37327;&#36234;&#20302;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#21508;&#31181;&#22240;&#32032;&#23545;&#25512;&#27979;&#35299;&#30721;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#27169;&#22411;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#26029;&#65292;&#32780;&#19981;&#20462;&#25913;&#20854;&#32467;&#26524;&#12290;&#22312;&#23545;LLM&#36827;&#34892;&#25512;&#26029;&#26102;&#65292;&#25512;&#27979;&#35299;&#30721;&#20351;&#29992;&#36739;&#23567;&#30340;&#33609;&#31295;&#27169;&#22411;&#29983;&#25104;&#25512;&#27979;&#20196;&#29260;&#65292;&#28982;&#21518;&#20351;&#29992;&#30446;&#26631;LLM&#39564;&#35777;&#36825;&#20123;&#33609;&#31295;&#20196;&#29260;&#12290;&#25512;&#27979;&#35299;&#30721;&#25552;&#20379;&#30340;&#21152;&#36895;&#21462;&#20915;&#20110;&#33609;&#31295;&#27169;&#22411;&#30340;&#36873;&#25321;&#12290;&#26222;&#36941;&#24314;&#35758;&#36873;&#25321;&#19968;&#20010;&#33609;&#31295;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#29983;&#25104;&#30340;&#20196;&#29260;&#34987;LLM&#25509;&#21463;&#30340;&#27010;&#29575;&#24456;&#39640;&#65292;&#20197;&#23454;&#29616;&#26368;&#39640;&#21534;&#21520;&#37327;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#19982;&#20043;&#30456;&#21453;&#65292;&#38543;&#30528;&#29983;&#25104;&#30340;&#20196;&#29260;&#34987;&#30446;&#26631;&#27169;&#22411;&#25509;&#21463;&#30340;&#27010;&#29575;&#22686;&#21152;&#65292;&#21534;&#21520;&#37327;&#20943;&#23569;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#23545;&#24433;&#21709;&#25512;&#27979;&#35299;&#30721;&#30340;&#19981;&#21516;&#22240;&#32032;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#30740;&#31350;&#20102;&#36825;&#20123;&#22240;&#32032;&#22914;&#20309;&#30456;&#20114;&#20316;&#29992;&#21644;&#24433;&#21709;&#21152;&#36895;&#25928;&#26524;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#20998;&#26512;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#29992;&#35813;&#27169;&#22411;&#26469;&#36827;&#34892;&#20915;&#31574;&#65292;&#25552;&#39640;&#25512;&#27979;&#35299;&#30721;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speculative Decoding is a widely used technique to speed up inference for Large Language Models (LLMs) without modifying its outcome. When performing inference on an LLM, speculative decoding uses a smaller draft model which generates speculative tokens and then uses the target LLM to verify those draft tokens. The speedup provided by speculative decoding heavily depends on the choice of the draft model. It has been widely suggested to select a draft model that provides a high probability of the generated token being accepted by the LLM to achieve the highest throughput. However, our experiments indicate the contrary with throughput diminishing as the probability of generated tokens to be accepted by the target model increases. To understand this phenomenon, we perform extensive experiments to characterize the different factors that affect speculative decoding and how those factors interact and affect the speedups. Based on our experiments we describe an analytical model which can be u
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21160;&#24577;&#20915;&#31574;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"K&#32423;&#25512;&#29702;"&#30340;&#26032;&#39062;&#25512;&#29702;&#26041;&#27861;&#12290;&#36890;&#36807;&#21338;&#24328;&#35770;&#30340;&#35797;&#39564;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#25512;&#29702;&#26041;&#27861;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#23481;&#26131;&#20986;&#38169;&#65292;&#32780;"K&#32423;&#25512;&#29702;"&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01521</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;K&#32423;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
K-Level Reasoning with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01521
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21160;&#24577;&#20915;&#31574;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"K&#32423;&#25512;&#29702;"&#30340;&#26032;&#39062;&#25512;&#29702;&#26041;&#27861;&#12290;&#36890;&#36807;&#21338;&#24328;&#35770;&#30340;&#35797;&#39564;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#25512;&#29702;&#26041;&#27861;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#23481;&#26131;&#20986;&#38169;&#65292;&#32780;"K&#32423;&#25512;&#29702;"&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#20854;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#21160;&#24577;&#12289;&#20132;&#20114;&#21644;&#31454;&#20105;&#22330;&#26223;&#65288;&#22914;&#21830;&#19994;&#25112;&#30053;&#21644;&#32929;&#31080;&#24066;&#22330;&#20998;&#26512;&#65289;&#20013;&#30340;&#24615;&#33021;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#27491;&#24335;&#25506;&#32034;LLMs&#22312;&#24555;&#36895;&#21464;&#21270;&#29615;&#22659;&#20013;&#30340;&#20915;&#31574;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#22522;&#20110;&#21338;&#24328;&#35770;&#30340;&#35797;&#39564;&#65292;&#20197;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#20013;&#21160;&#24577;&#20915;&#31574;&#30340;&#22797;&#26434;&#24615;&#12290;&#36825;&#20123;&#25361;&#25112;&#20855;&#26377;&#26126;&#30830;&#23450;&#20041;&#65292;&#21487;&#20197;&#23545;LLMs&#30340;&#21160;&#24577;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#28165;&#26224;&#12289;&#21487;&#25511;&#21644;&#31934;&#30830;&#30340;&#35780;&#20272;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#25512;&#29702;&#26041;&#27861;&#22312;&#38656;&#35201;k&#32423;&#24605;&#32771;&#30340;&#21160;&#24577;&#29615;&#22659;&#20013;&#23481;&#26131;&#20986;&#38169; - &#36825;&#26159;&#20043;&#21069;&#30740;&#31350;&#20013;&#26410;&#35299;&#20915;&#30340;&#20851;&#38190;&#27010;&#24565;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLMs&#25512;&#29702;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;&#8220;K&#32423;&#25512;&#29702;&#8221;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#23545;&#25163;&#30340;&#35270;&#35282;&#65292;&#20174;&#36882;&#24402;&#35282;&#24230;&#36816;&#29992;&#22522;&#20110;k&#32423;&#24605;&#32771;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) have demonstrated their proficiency in complex reasoning tasks, their performance in dynamic, interactive, and competitive scenarios - such as business strategy and stock market analysis - remains underexplored. To bridge this gap, we formally explore the dynamic reasoning capabilities of LLMs for decision-making in rapidly evolving environments. We introduce two game theory-based pilot challenges that mirror the complexities of real-world dynamic decision-making. These challenges are well-defined, enabling clear, controllable, and precise evaluation of LLMs' dynamic reasoning abilities. Through extensive experiments, we find that existing reasoning methods tend to falter in dynamic settings that require k-level thinking - a key concept not tackled by previous works. To address this, we propose a novel reasoning approach for LLMs, named "K-Level Reasoning". This approach adopts the perspective of rivals to recursively employ k-level thinking based on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#36830;&#32493;&#20540;&#25968;&#25454;&#32780;&#19981;&#26159;&#20998;&#31867;&#25968;&#25454;&#30340;&#26032;&#31181;&#23376;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25913;&#36827;NLP&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#20171;&#32461;&#20102;&#21019;&#24314;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#35770;&#65292;&#21487;&#36866;&#29992;&#20110;&#26356;&#22810;&#29305;&#24449;&#21644;&#35821;&#35328;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01513</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#26799;&#24230;&#35789;&#24207;&#31867;&#22411;&#23398;&#30740;&#31350;&#20174;&#36890;&#29992;&#20381;&#36182;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Multilingual Gradient Word-Order Typology from Universal Dependencies
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#36830;&#32493;&#20540;&#25968;&#25454;&#32780;&#19981;&#26159;&#20998;&#31867;&#25968;&#25454;&#30340;&#26032;&#31181;&#23376;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25913;&#36827;NLP&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#20171;&#32461;&#20102;&#21019;&#24314;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#35770;&#65292;&#21487;&#36866;&#29992;&#20110;&#26356;&#22810;&#29305;&#24449;&#21644;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#35821;&#35328;&#31867;&#22411;&#23398;&#39046;&#22495;&#30340;&#20449;&#24687;&#26377;&#28508;&#21147;&#25552;&#39640;NLP&#20219;&#21153;&#30340;&#34920;&#29616;&#65292;&#20294;&#21487;&#38752;&#30340;&#31867;&#22411;&#23398;&#25968;&#25454;&#26159;&#21069;&#25552;&#12290;&#29616;&#26377;&#30340;&#31867;&#22411;&#23398;&#25968;&#25454;&#24211;&#65292;&#21253;&#25324;WALS&#21644;Grambank&#65292;&#20027;&#35201;&#30001;&#20110;&#20854;&#20998;&#31867;&#26684;&#24335;&#30340;&#19981;&#19968;&#33268;&#24615;&#32780;&#23384;&#22312;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#31867;&#22411;&#23398;&#30340;&#20998;&#31867;&#23450;&#20041;&#19982;&#33258;&#28982;&#35821;&#35328;&#35821;&#26009;&#24211;&#20013;&#25152;&#21457;&#29616;&#30340;&#29616;&#35937;&#30340;&#36830;&#32493;&#24615;&#26412;&#36136;&#19981;&#21516;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#31181;&#23376;&#25968;&#25454;&#38598;&#65292;&#30001;&#36830;&#32493;&#20540;&#25968;&#25454;&#32780;&#19981;&#26159;&#20998;&#31867;&#25968;&#25454;&#32452;&#25104;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#35821;&#35328;&#30340;&#21464;&#24322;&#24615;&#12290;&#34429;&#28982;&#36825;&#20010;&#21021;&#22987;&#25968;&#25454;&#38598;&#20391;&#37325;&#20110;&#35789;&#24207;&#31867;&#22411;&#23398;&#65292;&#20294;&#25105;&#20204;&#20063;&#25552;&#20379;&#20102;&#21019;&#24314;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#35770;&#65292;&#21487;&#20197;&#26041;&#20415;&#22320;&#29992;&#20110;&#29983;&#25104;&#26356;&#24191;&#27867;&#30340;&#29305;&#24449;&#21644;&#35821;&#35328;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
While information from the field of linguistic typology has the potential to improve performance on NLP tasks, reliable typological data is a prerequisite. Existing typological databases, including WALS and Grambank, suffer from inconsistencies primarily caused by their categorical format. Furthermore, typological categorisations by definition differ significantly from the continuous nature of phenomena, as found in natural language corpora. In this paper, we introduce a new seed dataset made up of continuous-valued data, rather than categorical data, that can better reflect the variability of language. While this initial dataset focuses on word-order typology, we also present the methodology used to create the dataset, which can be easily adapted to generate data for a broader set of features and languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22810;&#39033;&#36873;&#25321;&#39064;&#20013;&#24178;&#25200;&#39033;&#29983;&#25104;&#30340;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#12290;&#35843;&#26597;&#32467;&#26524;&#26174;&#31034;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#20027;&#35201;&#26469;&#33258;&#29305;&#23450;&#39046;&#22495;&#25945;&#32946;&#36164;&#28304;&#20013;&#65292;&#20197;&#25991;&#26412;&#20026;&#20027;&#65292;&#32570;&#20047;&#24320;&#25918;&#39046;&#22495;&#21644;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01512</link><description>&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#39064;&#20013;&#30340;&#24178;&#25200;&#39033;&#29983;&#25104;&#65306;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Distractor Generation for Multiple-Choice Questions: A Survey of Methods, Datasets, and Evaluation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22810;&#39033;&#36873;&#25321;&#39064;&#20013;&#24178;&#25200;&#39033;&#29983;&#25104;&#30340;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#12290;&#35843;&#26597;&#32467;&#26524;&#26174;&#31034;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#20027;&#35201;&#26469;&#33258;&#29305;&#23450;&#39046;&#22495;&#25945;&#32946;&#36164;&#28304;&#20013;&#65292;&#20197;&#25991;&#26412;&#20026;&#20027;&#65292;&#32570;&#20047;&#24320;&#25918;&#39046;&#22495;&#21644;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24178;&#25200;&#39033;&#22312;&#23398;&#20064;&#35780;&#20272;&#20013;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#38024;&#23545;&#33521;&#35821;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#24178;&#25200;&#39033;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#20102;&#25991;&#26412;&#21644;&#22810;&#27169;&#24577;&#35821;&#22659;&#30340;&#25968;&#25454;&#38598;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#23545;&#24178;&#25200;&#39033;&#29983;&#25104;&#20219;&#21153;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#35752;&#35770;&#20102;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#32452;&#25104;&#37096;&#20998;&#21450;&#20854;&#29305;&#28857;&#65292;&#20998;&#26512;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;&#24182;&#24635;&#32467;&#20102;&#24178;&#25200;&#39033;&#29983;&#25104;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#32467;&#26524;&#34920;&#26126;&#65292;&#36229;&#36807;&#19968;&#21322;&#30340;&#25968;&#25454;&#38598;&#26469;&#33258;&#29305;&#23450;&#39046;&#22495;&#65288;&#22914;&#31185;&#23398;&#21644;&#33521;&#35821;&#65289;&#20013;&#30340;&#25945;&#32946;&#26469;&#28304;&#65292;&#24182;&#19988;&#20027;&#35201;&#26159;&#20197;&#25991;&#26412;&#20026;&#20027;&#65292;&#32570;&#20047;&#24320;&#25918;&#39046;&#22495;&#21644;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distractors are important in learning evaluation. This paper surveys distractor generation tasks using English multiple-choice question datasets for textual and multimodal contexts. In particular, this paper presents a thorough literature review of the recent studies on distractor generation tasks, discusses multiple choice components and their characteristics, analyzes the related datasets, and summarizes the evaluation metrics of distractor generation. Our investigation reveals that more than half of datasets are human-generated from educational sources in specific domains such as Science and English, which are largely text-based, with a lack of open domain and multimodal datasets.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28151;&#21512;&#31574;&#30053;&#29992;&#20110;&#32842;&#22825;&#35760;&#24405;&#25688;&#35201;&#21270;&#65292;&#35813;&#31574;&#30053;&#39318;&#20808;&#32467;&#21512;&#20102;&#25277;&#21462;&#21644;&#29983;&#25104;&#24335;&#25688;&#35201;&#21270;&#25216;&#26415;&#65292;&#28982;&#21518;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#25688;&#35201;&#30340;&#36136;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#37096;&#32626;&#30340;&#32842;&#22825;&#35760;&#24405;&#25688;&#35201;&#21270;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01510</link><description>&lt;p&gt;
&#19968;&#31181;&#28151;&#21512;&#31574;&#30053;&#29992;&#20110;&#32842;&#22825;&#35760;&#24405;&#25688;&#35201;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Strategy for Chat Transcript Summarization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01510
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28151;&#21512;&#31574;&#30053;&#29992;&#20110;&#32842;&#22825;&#35760;&#24405;&#25688;&#35201;&#21270;&#65292;&#35813;&#31574;&#30053;&#39318;&#20808;&#32467;&#21512;&#20102;&#25277;&#21462;&#21644;&#29983;&#25104;&#24335;&#25688;&#35201;&#21270;&#25216;&#26415;&#65292;&#28982;&#21518;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#25688;&#35201;&#30340;&#36136;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#37096;&#32626;&#30340;&#32842;&#22825;&#35760;&#24405;&#25688;&#35201;&#21270;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25688;&#35201;&#21270;&#26159;&#23558;&#19968;&#27573;&#25991;&#26412;&#21387;&#32553;&#25104;&#36739;&#23569;&#21477;&#23376;&#30340;&#36807;&#31243;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#20869;&#23481;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#32842;&#22825;&#35760;&#24405;&#26159;&#23458;&#25143;&#65288;&#26469;&#30005;&#32773;&#65289;&#21644;&#23458;&#26381;&#20154;&#21592;&#20043;&#38388;&#30340;&#25968;&#23383;&#25110;&#22312;&#32447;&#23545;&#35805;&#30340;&#25991;&#26412;&#21103;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26412;&#22320;&#24320;&#21457;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#39318;&#20808;&#32467;&#21512;&#25277;&#21462;&#21644;&#29983;&#25104;&#24335;&#25688;&#35201;&#21270;&#25216;&#26415;&#65292;&#23545;&#32570;&#20047;&#26631;&#28857;&#25110;&#26410;&#26631;&#28857;&#30340;&#32842;&#22825;&#35760;&#24405;&#36827;&#34892;&#21387;&#32553;&#65292;&#20135;&#29983;&#26356;&#26131;&#35835;&#30340;&#24102;&#26631;&#28857;&#25688;&#35201;&#65292;&#28982;&#21518;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#25688;&#35201;&#30340;&#25972;&#20307;&#36136;&#37327;&#12290;&#24191;&#27867;&#30340;&#27979;&#35797;&#12289;&#35780;&#20272;&#12289;&#27604;&#36739;&#21644;&#39564;&#35777;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#27809;&#26377;&#25163;&#21160;&#29983;&#25104;&#30340;&#21442;&#32771;&#25688;&#35201;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#37096;&#32626;&#30340;&#32842;&#22825;&#35760;&#24405;&#25688;&#35201;&#21270;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text summarization is the process of condensing a piece of text to fewer sentences, while still preserving its content. Chat transcript, in this context, is a textual copy of a digital or online conversation between a customer (caller) and agent(s). This paper presents an indigenously (locally) developed hybrid method that first combines extractive and abstractive summarization techniques in compressing ill-punctuated or un-punctuated chat transcripts to produce more readable punctuated summaries and then optimizes the overall quality of summarization through reinforcement learning. Extensive testing, evaluations, comparisons, and validation have demonstrated the efficacy of this approach for large-scale deployment of chat transcript summarization, in the absence of manually generated reference (annotated) summaries.
&lt;/p&gt;</description></item><item><title>&#20195;&#30721;&#36716;&#25442;&#35821;&#35328;&#35782;&#21035;&#26159;&#19968;&#20010;&#24120;&#35265;&#19988;&#24212;&#29992;&#24191;&#27867;&#30340;&#38382;&#39064;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#37117;&#19981;&#22815;&#29702;&#24819;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25193;&#23637;&#35821;&#35328;&#33539;&#22260;&#12289;&#37319;&#29992;&#31616;&#21333;&#26550;&#26500;&#30340;&#27169;&#22411;&#20197;&#21450;&#37325;&#26032;&#23450;&#20041;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23545;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#24314;&#35758;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01505</link><description>&lt;p&gt;
&#20195;&#30721;&#36716;&#25442;&#35821;&#35328;&#35782;&#21035;&#27604;&#20320;&#24819;&#30340;&#26356;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Code-Switched Language Identification is Harder Than You Think
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01505
&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#36716;&#25442;&#35821;&#35328;&#35782;&#21035;&#26159;&#19968;&#20010;&#24120;&#35265;&#19988;&#24212;&#29992;&#24191;&#27867;&#30340;&#38382;&#39064;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#37117;&#19981;&#22815;&#29702;&#24819;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25193;&#23637;&#35821;&#35328;&#33539;&#22260;&#12289;&#37319;&#29992;&#31616;&#21333;&#26550;&#26500;&#30340;&#27169;&#22411;&#20197;&#21450;&#37325;&#26032;&#23450;&#20041;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23545;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#36716;&#25442;&#65288;CS&#65289;&#26159;&#19968;&#31181;&#22312;&#20070;&#38754;&#21644;&#21475;&#35821;&#20132;&#27969;&#20013;&#38750;&#24120;&#24120;&#35265;&#30340;&#29616;&#35937;&#65292;&#20294;&#26159;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#31243;&#24207;&#23545;&#20854;&#22788;&#29702;&#19981;&#20339;&#12290;&#22312;&#26500;&#24314;CS&#35821;&#26009;&#24211;&#30340;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#29992;&#20110;&#35821;&#26009;&#24211;&#26500;&#24314;&#30340;CS&#35821;&#35328;&#35782;&#21035;&#65288;LID&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#25193;&#23637;&#21040;&#26356;&#22810;&#30340;&#35821;&#35328;&#24182;&#32771;&#34385;&#37319;&#29992;&#31616;&#21333;&#26550;&#26500;&#30340;&#27169;&#22411;&#36827;&#34892;&#26356;&#24555;&#30340;&#25512;&#29702;&#26469;&#20351;&#20219;&#21153;&#26356;&#21152;&#30495;&#23454;&#12290;&#25105;&#20204;&#36824;&#23558;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#21477;&#23376;&#32423;&#22810;&#26631;&#31614;&#26631;&#35760;&#38382;&#39064;&#65292;&#20197;&#20351;&#20854;&#26356;&#21152;&#21487;&#34892;&#12290;&#22312;&#23450;&#20041;&#20102;&#20219;&#21153;&#20043;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#20010;&#21512;&#29702;&#30340;&#27169;&#22411;&#65292;&#24182;&#23450;&#20041;&#20102;&#26356;&#33021;&#21453;&#26144;&#25152;&#38656;&#24615;&#33021;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#24403;&#21069;&#30340;&#26041;&#27861;&#37117;&#19981;&#36275;&#22815;&#65292;&#24182;&#26368;&#32456;&#25552;&#20379;&#20102;&#23545;&#26410;&#26469;&#24037;&#20316;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code switching (CS) is a very common phenomenon in written and spoken communication but one that is handled poorly by many natural language processing applications. Looking to the application of building CS corpora, we explore CS language identification (LID) for corpus building. We make the task more realistic by scaling it to more languages and considering models with simpler architectures for faster inference. We also reformulate the task as a sentence-level multi-label tagging problem to make it more tractable. Having defined the task, we investigate three reasonable models for this task and define metrics which better reflect desired performance. We present empirical evidence that no current approach is adequate and finally provide recommendations for future work in this area.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#27604;&#20102;&#22235;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20174;&#35821;&#20041;&#19977;&#20803;&#32452;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#20102;&#29983;&#25104;&#39044;&#27979;&#20013;&#26368;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01495</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#29983;&#25104;&#23545;&#35805;&#24335;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comparative Analysis of Conversational Large Language Models in Knowledge-Based Text Generation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#27604;&#20102;&#22235;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20174;&#35821;&#20041;&#19977;&#20803;&#32452;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#20102;&#29983;&#25104;&#39044;&#27979;&#20013;&#26368;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#23545;&#20110;&#20250;&#35805;&#24335;&#20449;&#24687;&#25628;&#32034;&#33267;&#20851;&#37325;&#35201;&#12290;&#20174;&#30693;&#35782;&#22270;&#20013;&#33719;&#24471;&#30340;&#35821;&#20041;&#19977;&#20803;&#32452;&#21487;&#20197;&#20316;&#20026;&#23545;&#35805;&#20195;&#29702;&#22238;&#24212;&#30340;&#22522;&#30784;&#65292;&#36890;&#36807;&#25552;&#20379;&#20107;&#23454;&#20381;&#25454;&#26469;&#20256;&#36798;&#20449;&#24687;&#12290;&#36825;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#23588;&#20026;&#37325;&#35201;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24456;&#22823;&#30340;&#20250;&#35805;&#20132;&#20114;&#28508;&#21147;&#65292;&#20294;&#23481;&#26131;&#20135;&#29983;&#24187;&#35937;&#12289;&#30465;&#30053;&#25110;&#20135;&#29983;&#20914;&#31361;&#20449;&#24687;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#20250;&#35805;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20174;&#35821;&#20041;&#19977;&#20803;&#32452;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#26041;&#38754;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22235;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#19981;&#21516;&#21551;&#21457;&#24335;&#25216;&#26415;&#12290;&#36890;&#36807;&#22312;WebNLG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#19968;&#31995;&#21015;&#22522;&#20934;&#23454;&#39564;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#30830;&#23450;&#20102;&#29983;&#25104;&#30340;&#39044;&#27979;&#20013;&#26368;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#20803;&#32452;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Generating natural language text from graph-structured data is essential for conversational information seeking. Semantic triples derived from knowledge graphs can serve as a valuable source for grounding responses from conversational agents by providing a factual basis for the information they communicate. This is especially relevant in the context of large language models, which offer great potential for conversational interaction but are prone to hallucinating, omitting, or producing conflicting information. In this study, we conduct an empirical analysis of conversational large language models in generating natural language text from semantic triples. We compare four large language models of varying sizes with different prompting techniques. Through a series of benchmark experiments on the WebNLG dataset, we analyze the models' performance and identify the most common issues in the generated predictions. Our findings show that the capabilities of large language models in triple ver
&lt;/p&gt;</description></item><item><title>AMOR&#26159;&#19968;&#20010;&#22522;&#20110;&#24320;&#28304;LLM&#30340;&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#22806;&#37096;&#30693;&#35782;&#24211;&#36827;&#34892;&#25512;&#29702;&#21644;&#20154;&#31867;&#30417;&#30563;&#26469;&#36866;&#24212;&#29305;&#23450;&#39046;&#22495;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#24494;&#35843;&#65292;AMOR&#33021;&#22815;&#22312;&#19981;&#21516;&#30693;&#35782;&#29615;&#22659;&#20013;&#27867;&#21270;&#65292;&#24182;&#19988;&#21487;&#20197;&#26681;&#25454;&#36807;&#31243;&#21453;&#39304;&#36827;&#34892;&#39046;&#22495;&#23450;&#21046;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01469</link><description>&lt;p&gt;
AMOR:&#36890;&#36807;&#36827;&#31243;&#21453;&#39304;&#26500;&#24314;&#36866;&#24212;&#24615;&#27169;&#22359;&#21270;&#30693;&#35782;&#20195;&#29702;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01469
&lt;/p&gt;
&lt;p&gt;
AMOR&#26159;&#19968;&#20010;&#22522;&#20110;&#24320;&#28304;LLM&#30340;&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#22806;&#37096;&#30693;&#35782;&#24211;&#36827;&#34892;&#25512;&#29702;&#21644;&#20154;&#31867;&#30417;&#30563;&#26469;&#36866;&#24212;&#29305;&#23450;&#39046;&#22495;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#24494;&#35843;&#65292;AMOR&#33021;&#22815;&#22312;&#19981;&#21516;&#30693;&#35782;&#29615;&#22659;&#20013;&#27867;&#21270;&#65292;&#24182;&#19988;&#21487;&#20197;&#26681;&#25454;&#36807;&#31243;&#21453;&#39304;&#36827;&#34892;&#39046;&#22495;&#23450;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26174;&#33879;&#25104;&#21151;&#24341;&#21457;&#20102;&#26500;&#24314;&#35821;&#35328;&#20195;&#29702;&#23436;&#25104;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#30340;&#39640;&#28526;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#24320;&#28304;LLM&#30340;&#20195;&#29702;&#26694;&#26550;AMOR&#65292;&#36890;&#36807;&#19982;&#22806;&#37096;&#30693;&#35782;&#24211;&#36827;&#34892;&#25512;&#29702;&#24182;&#36890;&#36807;&#20154;&#31867;&#30417;&#30563;&#26469;&#36866;&#24212;&#29305;&#23450;&#39046;&#22495;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;AMOR&#22312;&#26377;&#38480;&#29366;&#24577;&#26426;&#65288;FSM&#65289;&#19978;&#26500;&#24314;&#25512;&#29702;&#36923;&#36753;&#65292;&#36890;&#36807;&#33258;&#20027;&#25191;&#34892;&#21644;&#27169;&#22359;&#38388;&#36716;&#25442;&#35299;&#20915;&#38382;&#39064;&#12290;&#36825;&#20351;&#20154;&#20204;&#33021;&#22815;&#30452;&#25509;&#20026;&#21333;&#20010;&#27169;&#22359;&#25552;&#20379;&#21453;&#39304;&#65292;&#20174;&#32780;&#33258;&#28982;&#24418;&#25104;&#20102;&#36807;&#31243;&#30417;&#30563;&#12290;&#22522;&#20110;&#36825;&#20010;&#25512;&#29702;&#21644;&#21453;&#39304;&#26694;&#26550;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#24494;&#35843;&#24320;&#21457;&#20102;AMOR&#65306;&#39044;&#28909;&#21644;&#36866;&#24212;&#12290;&#21069;&#32773;&#20351;&#29992;&#20174;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#33258;&#21160;&#26500;&#24314;&#30340;&#31034;&#20363;&#23545;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#20351;AMOR&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#30693;&#35782;&#29615;&#22659;&#20013;&#27867;&#21270;&#65292;&#21518;&#32773;&#20351;&#29992;&#36807;&#31243;&#21453;&#39304;&#23558;AMOR&#37327;&#36523;&#23450;&#21046;&#21040;&#29305;&#23450;&#39046;&#22495;&#12290;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The notable success of large language models (LLMs) has sparked an upsurge in building language agents to complete various complex tasks. We present AMOR, an agent framework based on open-source LLMs, which reasons with external knowledge bases and adapts to specific domains through human supervision to the reasoning process. AMOR builds reasoning logic over a finite state machine (FSM) that solves problems through autonomous executions and transitions over disentangled modules. This allows humans to provide direct feedback to the individual modules, and thus naturally forms process supervision. Based on this reasoning and feedback framework, we develop AMOR through two-stage fine-tuning: warm-up and adaptation. The former fine-tunes the LLM with examples automatically constructed from various public datasets and enables AMOR to generalize across different knowledge environments, while the latter tailors AMOR to specific domains using process feedback. Extensive experiments across mult
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;PLMs&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#32467;&#26524;&#26174;&#31034;PLMs&#22312;&#20351;&#29992;&#25552;&#31034;&#26102;&#20855;&#26377;&#36739;&#20302;&#30340;&#19968;&#33268;&#24615;&#65292;&#20294;&#36890;&#36807;&#28155;&#21152;&#35777;&#25454;&#27573;&#33853;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#12290;&#36825;&#34920;&#26126;PLMs&#38656;&#35201;&#36827;&#19968;&#27493;&#22686;&#24378;&#20197;&#22788;&#29702;&#20174;&#20107;&#23454;&#20013;&#26816;&#32034;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01453</link><description>&lt;p&gt;
&#33521;&#22269;&#22899;&#29579;&#24182;&#38750;&#33521;&#26684;&#20848;&#30340;&#22899;&#29579;&#65306;&#20851;&#20110;PLMs&#20013;&#32570;&#20047;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The Queen of England is not England's Queen: On the Lack of Factual Coherency in PLMs
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;PLMs&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#32467;&#26524;&#26174;&#31034;PLMs&#22312;&#20351;&#29992;&#25552;&#31034;&#26102;&#20855;&#26377;&#36739;&#20302;&#30340;&#19968;&#33268;&#24615;&#65292;&#20294;&#36890;&#36807;&#28155;&#21152;&#35777;&#25454;&#27573;&#33853;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#12290;&#36825;&#34920;&#26126;PLMs&#38656;&#35201;&#36827;&#19968;&#27493;&#22686;&#24378;&#20197;&#22788;&#29702;&#20174;&#20107;&#23454;&#20013;&#26816;&#32034;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20013;&#32534;&#30721;&#30340;&#20107;&#23454;&#30693;&#35782;&#20016;&#23500;&#20102;&#20854;&#34920;&#31034;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#20316;&#20026;&#30693;&#35782;&#24211;&#30340;&#20351;&#29992;&#21512;&#29702;&#24615;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#36890;&#36807;&#34913;&#37327;PLMs&#33021;&#22815;&#22312;&#32473;&#23450;&#20027;&#39064;&#21644;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#27491;&#30830;&#39044;&#27979;&#23545;&#35937;&#23454;&#20307;&#30340;&#39057;&#29575;&#65292;&#20197;&#21450;&#36890;&#36807;&#20248;&#21270;&#26597;&#35810;PLMs&#26102;&#20351;&#29992;&#30340;&#25552;&#31034;&#26469;&#25913;&#36827;&#20107;&#23454;&#26816;&#32034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#34917;&#20805;&#24615;&#30340;&#26041;&#38754;&#65292;&#21363;PLMs&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#19968;&#33268;&#24615;&#65292;&#21363;&#22312;PLMs&#36890;&#36807;&#23545;&#23545;&#35937;&#23454;&#20307;&#30340;&#21021;&#22987;&#39044;&#27979;&#21518;&#65292;&#33021;&#22815;&#22810;&#23569;&#27425;&#39044;&#27979;&#21040;&#20027;&#39064;&#23454;&#20307;&#12290;&#36825;&#36229;&#36234;&#20102;&#35780;&#20272;PLMs&#25152;&#30693;&#30340;&#31243;&#24230;&#65292;&#32780;&#26159;&#20851;&#27880;&#20854;&#20869;&#37096;&#30693;&#35782;&#29366;&#24577;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;PLMs&#22312;&#20351;&#29992;&#25163;&#21160;&#32534;&#20889;&#30340;&#12289;&#20248;&#21270;&#30340;&#21644;&#25913;&#20889;&#30340;&#25552;&#31034;&#26102;&#20855;&#26377;&#36739;&#20302;&#30340;&#19968;&#33268;&#24615;&#65292;&#20294;&#21253;&#21547;&#35777;&#25454;&#27573;&#33853;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#19968;&#33268;&#24615;&#12290;&#36825;&#34920;&#26126;&#65292;PLMs&#27809;&#26377;&#27169;&#25311;&#36870;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#22686;&#24378;&#20197;&#22788;&#29702;&#20174;&#20107;&#23454;&#20013;&#26816;&#32034;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Factual knowledge encoded in Pre-trained Language Models (PLMs) enriches their representations and justifies their use as knowledge bases. Previous work has focused on probing PLMs for factual knowledge by measuring how often they can correctly predict an object entity given a subject and a relation, and improving fact retrieval by optimizing the prompts used for querying PLMs. In this work, we consider a complementary aspect, namely the coherency of factual knowledge in PLMs, i.e., how often can PLMs predict the subject entity given its initial prediction of the object entity. This goes beyond evaluating how much PLMs know, and focuses on the internal state of knowledge inside them. Our results indicate that PLMs have low coherency using manually written, optimized and paraphrased prompts, but including an evidence paragraph leads to substantial improvement. This shows that PLMs fail to model inverse relations and need further enhancements to be able to handle retrieving facts from th
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#35748;&#30693;&#22810;&#26679;&#24615;&#21450;&#20854;&#23545;&#32676;&#20307;&#20915;&#31574;&#25104;&#21151;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20998;&#26512;500&#20010;&#22242;&#38431;&#35752;&#35770;&#30340;&#23545;&#35805;&#35760;&#24405;&#65292;&#25105;&#20204;&#21457;&#29616;&#36739;&#22823;&#30340;&#35748;&#30693;&#22810;&#26679;&#24615;&#19982;&#26356;&#25104;&#21151;&#30340;&#32676;&#20307;&#20915;&#31574;&#30456;&#20851;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01427</link><description>&lt;p&gt;
&#22810;&#26679;&#24615;&#23545;&#32676;&#20307;&#20915;&#31574;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The effect of diversity on group decision-making
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01427
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#35748;&#30693;&#22810;&#26679;&#24615;&#21450;&#20854;&#23545;&#32676;&#20307;&#20915;&#31574;&#25104;&#21151;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20998;&#26512;500&#20010;&#22242;&#38431;&#35752;&#35770;&#30340;&#23545;&#35805;&#35760;&#24405;&#65292;&#25105;&#20204;&#21457;&#29616;&#36739;&#22823;&#30340;&#35748;&#30693;&#22810;&#26679;&#24615;&#19982;&#26356;&#25104;&#21151;&#30340;&#32676;&#20307;&#20915;&#31574;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#35748;&#30693;&#22810;&#26679;&#24615;&#30340;&#19981;&#21516;&#26041;&#38754;&#20197;&#21450;&#20854;&#23545;&#32676;&#20307;&#21327;&#21830;&#25104;&#21151;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;500&#20010;&#23567;&#22411;&#22312;&#32447;&#22242;&#38431;&#35752;&#35770;&#29926;&#32034;&#21345;&#29255;&#36873;&#25321;&#20219;&#21153;&#30340;&#23545;&#35805;&#35760;&#24405;&#36827;&#34892;&#35780;&#20272; - DeliData&#35821;&#26009;&#24211;&#12290;&#21033;&#29992;&#35813;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23450;&#37327;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#35748;&#30693;&#22810;&#26679;&#24615;&#30340;&#19977;&#31181;&#19981;&#21516;&#24230;&#37327;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#32676;&#20307;&#35268;&#27169;&#20316;&#20026;&#22810;&#26679;&#24615;&#30340;&#20195;&#29702;&#24230;&#37327;&#30340;&#24433;&#21709;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21021;&#22987;&#24819;&#27861;&#27744;&#30340;&#22823;&#23567;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#35752;&#35770;&#30340;&#35299;&#20915;&#26041;&#26696;&#12289;&#35752;&#35770;&#27169;&#24335;&#20197;&#21450;&#35848;&#35805;&#25506;&#31350;&#22914;&#20309;&#25913;&#21892;&#36825;&#20123;&#29305;&#24449;&#26469;&#30740;&#31350;&#35752;&#35770;&#20869;&#23481;&#12290;&#23613;&#31649;&#32676;&#20307;&#22240;&#20026;&#21152;&#37325;&#20559;&#35265;&#32780;&#21517;&#22768;&#19981;&#20339;&#65292;&#25105;&#20204;&#34920;&#26126;&#23567;&#22242;&#38431;&#21487;&#20197;&#36890;&#36807;&#23545;&#35805;&#20811;&#26381;&#30452;&#35273;&#20559;&#35265;&#65292;&#25552;&#39640;&#20010;&#20307;&#20915;&#31574;&#33021;&#21147;&#12290;&#22312;&#22823;&#26679;&#26412;&#21644;&#19981;&#21516;&#25805;&#20316;&#20013;&#65292;&#25105;&#20204;&#22987;&#32456;&#21457;&#29616;&#35748;&#30693;&#22810;&#26679;&#24615;&#19982;&#32676;&#20307;&#21327;&#21830;&#30340;&#25104;&#21151;&#24615;&#26377;&#20851;
&lt;/p&gt;
&lt;p&gt;
We explore different aspects of cognitive diversity and its effect on the success of group deliberation. To evaluate this, we use 500 dialogues from small, online groups discussing the Wason Card Selection task - the DeliData corpus. Leveraging the corpus, we perform quantitative analysis evaluating three different measures of cognitive diversity. First, we analyse the effect of group size as a proxy measure for diversity. Second, we evaluate the effect of the size of the initial idea pool. Finally, we look into the content of the discussion by analysing discussed solutions, discussion patterns, and how conversational probing can improve those characteristics.   Despite the reputation of groups for compounding bias, we show that small groups can, through dialogue, overcome intuitive biases and improve individual decision-making. Across a large sample and different operationalisations, we consistently find that greater cognitive diversity is associated with more successful group deliber
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21629;&#21517;&#23454;&#20307;&#26631;&#27880;&#20013;&#30340;&#20154;&#31867;&#26631;&#31614;&#21464;&#21270;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#25991;&#26412;&#27495;&#20041;&#21644;&#20154;&#20026;&#25351;&#21335;&#26356;&#25913;&#26159;&#39640;&#36136;&#37327;&#20462;&#35746;&#20013;&#19981;&#21516;&#26631;&#27880;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#24182;&#39564;&#35777;&#20102;&#22810;&#26679;&#21270;&#26631;&#27880;&#29992;&#20110;&#29702;&#35299;&#21629;&#21517;&#23454;&#20307;&#27495;&#20041;&#30340;&#21487;&#34892;&#24615;&#21644;&#24517;&#35201;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01423</link><description>&lt;p&gt;
&#21508;&#23454;&#20307;&#30340;&#19981;&#21516;&#20559;&#22909;&#65306;&#25506;&#31350;&#21629;&#21517;&#23454;&#20307;&#26631;&#27880;&#20013;&#30340;&#20154;&#31867;&#26631;&#31614;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Different Tastes of Entities: Investigating Human Label Variation in Named Entity Annotations
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21629;&#21517;&#23454;&#20307;&#26631;&#27880;&#20013;&#30340;&#20154;&#31867;&#26631;&#31614;&#21464;&#21270;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#25991;&#26412;&#27495;&#20041;&#21644;&#20154;&#20026;&#25351;&#21335;&#26356;&#25913;&#26159;&#39640;&#36136;&#37327;&#20462;&#35746;&#20013;&#19981;&#21516;&#26631;&#27880;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#24182;&#39564;&#35777;&#20102;&#22810;&#26679;&#21270;&#26631;&#27880;&#29992;&#20110;&#29702;&#35299;&#21629;&#21517;&#23454;&#20307;&#27495;&#20041;&#30340;&#21487;&#34892;&#24615;&#21644;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#26377;&#30528;&#24736;&#20037;&#30340;&#20256;&#32479;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#37325;&#26032;&#26631;&#27880;&#30340;&#21162;&#21147;&#26469;&#35299;&#20915;&#27880;&#37322;&#38169;&#35823;&#65292;&#20294;&#20154;&#31867;&#26631;&#31614;&#21464;&#24322;&#30340;&#26469;&#28304;&#65292;&#22914;&#25991;&#26412;&#27495;&#20041;&#12289;&#27880;&#37322;&#38169;&#35823;&#25110;&#25351;&#21335;&#20998;&#27495;&#65292;&#30693;&#20043;&#29978;&#23569;&#12290;&#36825;&#22312;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#21644;&#33521;&#35821;CoNLL03&#20043;&#22806;&#23588;&#20026;&#31361;&#20986;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#31181;&#35821;&#35328;&#65288;&#33521;&#35821;&#12289;&#20025;&#40614;&#35821;&#21644;&#24052;&#20240;&#21033;&#20122;&#35821;&#65289;&#30340;&#19987;&#23478;&#26631;&#27880;&#30340;&#21629;&#21517;&#23454;&#20307;&#25968;&#25454;&#38598;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#26174;&#31034;&#65292;&#25991;&#26412;&#27495;&#20041;&#21644;&#20154;&#20026;&#25351;&#21335;&#26356;&#25913;&#26159;&#39640;&#36136;&#37327;&#20462;&#35746;&#20013;&#19981;&#21516;&#26631;&#27880;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#23398;&#29983;&#23545;&#19968;&#37096;&#20998;&#22256;&#38590;&#23454;&#20307;&#30340;&#26631;&#27880;&#65292;&#24182;&#20174;&#20998;&#24067;&#35282;&#24230;&#39564;&#35777;&#20102;&#22810;&#26679;&#21270;&#26631;&#27880;&#29992;&#20110;&#29702;&#35299;&#21629;&#21517;&#23454;&#20307;&#27495;&#20041;&#30340;&#21487;&#34892;&#24615;&#21644;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition (NER) is a key information extraction task with a long-standing tradition. While recent studies address and aim to correct annotation errors via re-labeling efforts, little is known about the sources of human label variation, such as text ambiguity, annotation error, or guideline divergence. This is especially the case for high-quality datasets and beyond English CoNLL03. This paper studies disagreements in expert-annotated named entity datasets for three languages: English, Danish, and Bavarian. We show that text ambiguity and artificial guideline changes are dominant factors for diverse annotations among high-quality revisions. We survey student annotations on a subset of difficult entities and substantiate the feasibility and necessity of manifold annotations for understanding named entity ambiguities from a distributional perspective.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#26426;&#22120;&#32763;&#35793;&#30340;&#24207;&#21015;&#32553;&#30701;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#32763;&#35793;&#20934;&#30830;&#24615;&#65292;&#22312;&#23545;&#27604;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#26032;&#30340;&#28508;&#22312;&#20998;&#32452;&#21644;&#28508;&#22312;&#36873;&#25321;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01416</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#24863;&#30693;&#26426;&#22120;&#32763;&#35793;&#30340;&#24207;&#21015;&#32553;&#30701;
&lt;/p&gt;
&lt;p&gt;
Sequence Shortening for Context-Aware Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#26426;&#22120;&#32763;&#35793;&#30340;&#24207;&#21015;&#32553;&#30701;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#32763;&#35793;&#20934;&#30830;&#24615;&#65292;&#22312;&#23545;&#27604;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#26032;&#30340;&#28508;&#22312;&#20998;&#32452;&#21644;&#28508;&#22312;&#36873;&#25321;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#24863;&#30693;&#26426;&#22120;&#32763;&#35793;&#26088;&#22312;&#36890;&#36807;&#23558;&#21608;&#22260;&#30340;&#21477;&#23376;&#20316;&#20026;&#19978;&#19979;&#25991;&#26469;&#25913;&#36827;&#21477;&#23376;&#30340;&#32763;&#35793;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#24050;&#32463;&#24212;&#29992;&#20102;&#20004;&#31181;&#20027;&#35201;&#26550;&#26500;&#65292;&#21363;&#22522;&#20110;&#20018;&#32852;&#30340;&#21333;&#32534;&#30721;&#22120;&#21644;&#22810;&#32534;&#30721;&#22120;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#32534;&#30721;&#22120;&#26550;&#26500;&#30340;&#19968;&#20010;&#29305;&#27530;&#24773;&#20917;&#65292;&#22312;&#19979;&#19968;&#27493;&#20013;&#37325;&#29992;&#28304;&#21477;&#23376;&#30340;&#28508;&#22312;&#34920;&#31034;&#20316;&#20026;&#19978;&#19979;&#25991;&#65292;&#21487;&#20197;&#22312;&#23545;&#27604;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65288;&#27169;&#22411;&#24517;&#39035;&#23545;&#25552;&#20379;&#30340;&#21477;&#23376;&#20013;&#30340;&#27491;&#30830;&#32763;&#35793;&#36827;&#34892;&#25490;&#24207;&#65289;&#65292;&#24182;&#19988;&#19982;&#21333;&#32534;&#30721;&#22120;&#21644;&#22810;&#32534;&#30721;&#22120;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;BLEU&#21644;COMET&#20998;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#32531;&#23384;&#34920;&#31034;&#24212;&#29992;&#20110;&#24207;&#21015;&#32553;&#30701;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#19977;&#31181;&#22522;&#20110;&#27719;&#32858;&#30340;&#32553;&#30701;&#25216;&#26415;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#28508;&#22312;&#20998;&#32452;&#21644;&#28508;&#22312;&#36873;&#25321;&#65292;&#20854;&#20013;&#32593;&#32476;&#23398;&#20064;&#23558;&#20196;&#29260;&#20998;&#32452;&#25110;&#36873;&#25321;&#35201;&#32531;&#23384;&#20026;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#32531;&#23384;&#34920;&#31034;&#30340;&#24207;&#21015;&#32553;&#30701;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context-aware Machine Translation aims to improve translations of sentences by incorporating surrounding sentences as context. Towards this task, two main architectures have been applied, namely single-encoder (based on concatenation) and multi-encoder models. In this study, we show that a special case of multi-encoder architecture, where the latent representation of the source sentence is cached and reused as the context in the next step, achieves higher accuracy on the contrastive datasets (where the models have to rank the correct translation among the provided sentences) and comparable BLEU and COMET scores as the single- and multi-encoder approaches. Furthermore, we investigate the application of Sequence Shortening to the cached representations. We test three pooling-based shortening techniques and introduce two novel methods - Latent Grouping and Latent Selecting, where the network learns to group tokens or selects the tokens to be cached as context. Our experiments show that th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#34917;&#20805;&#22522;&#20110;&#20934;&#30830;&#24230;&#35780;&#20272;&#30340;&#19978;&#19979;&#25991;&#21033;&#29992;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#34920;&#26126;&#25200;&#21160;&#20998;&#26512;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#24635;&#20307;&#19978;&#19979;&#25991;&#21033;&#29992;&#24230;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#27979;&#37327;&#25903;&#25345;&#24615;&#19978;&#19979;&#25991;&#23545;&#22788;&#29702;&#19978;&#19979;&#25991;&#30456;&#20851;&#35805;&#35821;&#29616;&#35937;&#36129;&#29486;&#31243;&#24230;&#30340;&#26041;&#27861;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01404</link><description>&lt;p&gt;
&#20851;&#20110;&#27979;&#37327;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20013;&#19978;&#19979;&#25991;&#21033;&#29992;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Measuring Context Utilization in Document-Level MT Systems
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01404
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#34917;&#20805;&#22522;&#20110;&#20934;&#30830;&#24230;&#35780;&#20272;&#30340;&#19978;&#19979;&#25991;&#21033;&#29992;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#34920;&#26126;&#25200;&#21160;&#20998;&#26512;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#24635;&#20307;&#19978;&#19979;&#25991;&#21033;&#29992;&#24230;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#27979;&#37327;&#25903;&#25345;&#24615;&#19978;&#19979;&#25991;&#23545;&#22788;&#29702;&#19978;&#19979;&#25991;&#30456;&#20851;&#35805;&#35821;&#29616;&#35937;&#36129;&#29486;&#31243;&#24230;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#32763;&#35793;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#19968;&#33324;&#30340;&#24230;&#37327;&#26631;&#20934;&#65288;&#22914;BLEU&#65289;&#36827;&#34892;&#35780;&#20272;&#65292;&#20294;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#26080;&#27861;&#25552;&#20379;&#20851;&#20110;&#19978;&#19979;&#25991;&#30340;&#22909;&#22788;&#30340;&#20449;&#24687;&#12290;&#30446;&#21069;&#20851;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#35780;&#20272;&#30340;&#24037;&#20316;&#65292;&#22914;&#23545;&#27604;&#26041;&#27861;&#65292;&#20165;&#20165;&#27979;&#37327;&#38656;&#35201;&#19978;&#19979;&#25991;&#20197;&#28040;&#38500;&#27495;&#20041;&#30340;&#21333;&#35789;&#30340;&#32763;&#35793;&#20934;&#30830;&#24230;&#12290;&#36825;&#26679;&#30340;&#27979;&#37327;&#26041;&#27861;&#26080;&#27861;&#25581;&#31034;&#32763;&#35793;&#27169;&#22411;&#26159;&#21542;&#27491;&#30830;&#22320;&#20351;&#29992;&#20102;&#25903;&#25345;&#24615;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#25552;&#35758;&#22312;&#22522;&#20110;&#20934;&#30830;&#24230;&#30340;&#35780;&#20272;&#20013;&#34917;&#20805;&#19978;&#19979;&#25991;&#21033;&#29992;&#24230;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#21457;&#29616;&#25200;&#21160;&#20998;&#26512;&#65288;&#27604;&#36739;&#22312;&#25552;&#20379;&#27491;&#30830;&#19978;&#19979;&#25991;&#21644;&#38543;&#26426;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#27169;&#22411;&#30340;&#24615;&#33021;&#65289;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#24635;&#20307;&#19978;&#19979;&#25991;&#21033;&#29992;&#24230;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#23545;&#20110;&#26356;&#32454;&#31890;&#24230;&#30340;&#29616;&#35937;&#29305;&#23450;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#27979;&#37327;&#25903;&#25345;&#24615;&#19978;&#19979;&#25991;&#23545;&#22788;&#29702;&#19978;&#19979;&#25991;&#30456;&#20851;&#35805;&#35821;&#29616;&#35937;&#30340;&#36129;&#29486;&#31243;&#24230;&#12290;&#25105;&#20204;&#34920;&#26126;&#33258;&#21160;&#27880;&#37322;&#30340;&#25903;&#25345;&#24615;&#19978;&#19979;&#25991;&#19982;&#20154;&#24037;&#27880;&#37322;&#30340;&#19978;&#19979;&#25991;&#24471;&#20986;&#31867;&#20284;&#30340;&#32467;&#35770;&#65292;&#24182;&#21487;&#20316;&#20026;&#20154;&#24037;&#27880;&#37322;&#26080;&#27861;&#35206;&#30422;&#30340;&#24773;&#20917;&#19979;&#30340;&#26367;&#20195;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level translation models are usually evaluated using general metrics such as BLEU, which are not informative about the benefits of context. Current work on context-aware evaluation, such as contrastive methods, only measure translation accuracy on words that need context for disambiguation. Such measures cannot reveal whether the translation model uses the correct supporting context. We propose to complement accuracy-based evaluation with measures of context utilization. We find that perturbation-based analysis (comparing models' performance when provided with correct versus random context) is an effective measure of overall context utilization. For a finer-grained phenomenon-specific evaluation, we propose to measure how much the supporting context contributes to handling context-dependent discourse phenomena. We show that automatically-annotated supporting context gives similar conclusions to human-annotated context and can be used as alternative for cases where human annota
&lt;/p&gt;</description></item><item><title>StepCoder&#26159;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20195;&#30721;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#38271;&#24207;&#21015;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#20195;&#30721;&#23436;&#25104;&#23376;&#20219;&#21153;&#26469;&#35299;&#20915;&#25506;&#32034;&#25361;&#25112;&#65292;&#24182;&#20351;&#29992;&#32454;&#31890;&#24230;&#20248;&#21270;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01391</link><description>&lt;p&gt;
StepCoder: &#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#32534;&#35793;&#22120;&#21453;&#39304;&#20013;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01391
&lt;/p&gt;
&lt;p&gt;
StepCoder&#26159;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20195;&#30721;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#38271;&#24207;&#21015;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#20195;&#30721;&#23436;&#25104;&#23376;&#20219;&#21153;&#26469;&#35299;&#20915;&#25506;&#32034;&#25361;&#25112;&#65292;&#24182;&#20351;&#29992;&#32454;&#31890;&#24230;&#20248;&#21270;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#20195;&#30721;&#29983;&#25104;&#39046;&#22495;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#32534;&#35793;&#22120;&#21453;&#39304;&#32467;&#21512;&#36215;&#26469;&#65292;&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#31354;&#38388;&#65292;&#20197;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#28385;&#36275;&#22797;&#26434;&#30340;&#20154;&#31867;&#35201;&#27714;&#65292;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#24448;&#24448;&#24456;&#38271;&#65292;&#36825;&#20351;&#24471;&#24378;&#21270;&#23398;&#20064;&#30340;&#25506;&#32034;&#25104;&#20026;&#19968;&#39033;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#21333;&#20803;&#27979;&#35797;&#21487;&#33021;&#26080;&#27861;&#28085;&#30422;&#22797;&#26434;&#20195;&#30721;&#65292;&#20351;&#29992;&#36825;&#20123;&#26410;&#25191;&#34892;&#30340;&#20195;&#30721;&#29255;&#27573;&#26469;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#26159;&#26080;&#25928;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;StepCoder&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#26032;&#22411;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21253;&#21547;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#65306;CCCS&#36890;&#36807;&#23558;&#38271;&#24207;&#21015;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#20195;&#30721;&#23436;&#25104;&#23376;&#20219;&#21153;&#26469;&#35299;&#20915;&#25506;&#32034;&#25361;&#25112;&#65292;&#32780;FGO&#36890;&#36807;&#23631;&#34109;&#26410;&#25191;&#34892;&#30340;&#20195;&#30721;&#27573;&#26469;&#25552;&#20379;&#32454;&#31890;&#24230;&#20248;&#21270;&#65292;&#20165;&#20248;&#21270;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;APPS+&#25968;&#25454;&#38598;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#65292;&#35813;&#25968;&#25454;&#38598;&#32463;&#36807;&#25163;&#21160;&#39564;&#35777;&#30830;&#20445;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce StepCoder, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a Curriculum of Code Completion Subtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide Fine-Grained Optimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ens
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#30740;&#20171;&#32461;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#35780;&#20272;&#26041;&#27861;&#30340;&#29616;&#29366;&#65292;&#21253;&#25324;&#22522;&#20110;LLM&#23548;&#20986;&#30340;&#25351;&#26631;&#12289;&#24341;&#23548;LLM&#21644;&#20351;&#29992;&#24102;&#26377;&#26631;&#35760;&#35780;&#20272;&#25968;&#25454;&#30340;LLM&#24494;&#35843;&#65292;&#24182;&#35752;&#35770;&#20102;&#20154;&#31867;&#19982;LLM&#30340;&#21512;&#20316;&#12290;&#21516;&#26102;&#25351;&#20986;&#20102;&#35813;&#39046;&#22495;&#30340;&#19968;&#20123;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01383</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#35780;&#20272;&#65306;&#29616;&#29366;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
LLM-based NLG Evaluation: Current Status and Challenges
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01383
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#30740;&#20171;&#32461;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#35780;&#20272;&#26041;&#27861;&#30340;&#29616;&#29366;&#65292;&#21253;&#25324;&#22522;&#20110;LLM&#23548;&#20986;&#30340;&#25351;&#26631;&#12289;&#24341;&#23548;LLM&#21644;&#20351;&#29992;&#24102;&#26377;&#26631;&#35760;&#35780;&#20272;&#25968;&#25454;&#30340;LLM&#24494;&#35843;&#65292;&#24182;&#35752;&#35770;&#20102;&#20154;&#31867;&#19982;LLM&#30340;&#21512;&#20316;&#12290;&#21516;&#26102;&#25351;&#20986;&#20102;&#35813;&#39046;&#22495;&#30340;&#19968;&#20123;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#20294;&#25361;&#25112;&#37325;&#37325;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#35780;&#20272;&#25351;&#26631;&#20027;&#35201;&#36890;&#36807;&#31995;&#32479;&#36755;&#20986;&#21644;&#21442;&#32771;&#25991;&#26412;&#20043;&#38388;&#30340;&#20869;&#23481;&#65288;&#22914;n-gram&#65289;&#37325;&#21472;&#24230;&#26469;&#25429;&#25417;&#65292;&#36828;&#36828;&#19981;&#22815;&#20196;&#20154;&#28385;&#24847;&#65292;&#32780;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#22312;NLG&#35780;&#20272;&#26041;&#38754;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#22522;&#20110;LLM&#30340;&#21508;&#31181;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;LLM&#23548;&#20986;&#30340;&#25351;&#26631;&#12289;&#24341;&#23548;LLM&#21644;&#20351;&#29992;&#24102;&#26377;&#26631;&#35760;&#35780;&#20272;&#25968;&#25454;&#30340;LLM&#24494;&#35843;&#12290;&#22312;&#36825;&#39033;&#35843;&#30740;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#32473;&#20986;&#20102;&#22522;&#20110;LLM&#30340;NLG&#35780;&#20272;&#26041;&#27861;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20998;&#21035;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20154;&#31867;&#19982;LLM&#30340;&#21512;&#20316;&#29992;&#20110;NLG&#35780;&#20272;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#20013;&#30340;&#20960;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating natural language generation (NLG) is a vital but challenging problem in artificial intelligence. Traditional evaluation metrics mainly capturing content (e.g. n-gram) overlap between system outputs and references are far from satisfactory, and large language models (LLMs) such as ChatGPT have demonstrated great potential in NLG evaluation in recent years. Various automatic evaluation methods based on LLMs have been proposed, including metrics derived from LLMs, prompting LLMs, and fine-tuning LLMs with labeled evaluation data. In this survey, we first give a taxonomy of LLM-based NLG evaluation methods, and discuss their pros and cons, respectively. We also discuss human-LLM collaboration for NLG evaluation. Lastly, we discuss several open problems in this area and point out future research directions.
&lt;/p&gt;</description></item><item><title>LoTR&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#35843;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20302;&#31209;&#24352;&#37327;&#34920;&#31034;&#21644;&#24352;&#37327;&#20998;&#35299;&#65292;&#20351;&#24471;&#38024;&#23545;&#28145;&#23618;&#27169;&#22411;&#30340;&#21442;&#25968;&#25928;&#29575;&#26356;&#39640;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#24265;&#20215;&#19988;&#24555;&#36895;&#30340;&#19979;&#28216;&#35843;&#20248;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01376</link><description>&lt;p&gt;
LoTR: &#20302;&#24352;&#37327;&#31209;&#26435;&#37325;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
LoTR: Low Tensor Rank Weight Adaptation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01376
&lt;/p&gt;
&lt;p&gt;
LoTR&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#35843;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20302;&#31209;&#24352;&#37327;&#34920;&#31034;&#21644;&#24352;&#37327;&#20998;&#35299;&#65292;&#20351;&#24471;&#38024;&#23545;&#28145;&#23618;&#27169;&#22411;&#30340;&#21442;&#25968;&#25928;&#29575;&#26356;&#39640;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#24265;&#20215;&#19988;&#24555;&#36895;&#30340;&#19979;&#28216;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#30340;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#24605;&#24819;&#25512;&#24191;&#21644;&#25193;&#23637;&#65292;&#36825;&#20123;&#27169;&#22411;&#22522;&#20110;Transformer&#26550;&#26500;&#12290;&#24191;&#27867;&#20351;&#29992;&#30340;LoRA&#31867;&#26041;&#27861;&#26159;&#22522;&#20110;&#26799;&#24230;&#26356;&#26032;&#30340;&#30697;&#38453;&#20998;&#35299;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LoTR&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;&#26041;&#27861;&#65292;&#23427;&#20197;&#24352;&#37327;&#20998;&#35299;&#30340;&#24418;&#24335;&#34920;&#31034;&#21442;&#25968;&#30340;&#26799;&#24230;&#26356;&#26032;&#12290;&#27599;&#20010;&#23618;&#30340;&#20302;&#31209;&#36866;&#37197;&#22120;&#37117;&#30001;&#19977;&#20010;&#30697;&#38453;&#30340;&#20056;&#31215;&#26500;&#25104;&#65292;&#32780;&#24352;&#37327;&#32467;&#26500;&#26159;&#30001;&#36825;&#20010;&#20056;&#31215;&#30340;&#24038;&#21491;&#20056;&#23376;&#22312;&#23618;&#20043;&#38388;&#20849;&#20139;&#24341;&#36215;&#30340;&#12290;&#36890;&#36807;&#23545;&#20302;&#31209;&#24352;&#37327;&#34920;&#31034;&#30340;&#19968;&#31995;&#21015;&#23618;&#21516;&#26102;&#21387;&#32553;&#65292;LoTR&#33021;&#22815;&#27604;LoRA&#22312;&#29305;&#21035;&#26159;&#23545;&#20110;&#28145;&#23618;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#21442;&#25968;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#26680;&#24515;&#24352;&#37327;&#19981;&#20381;&#36182;&#20110;&#21407;&#22987;&#26435;&#37325;&#32500;&#24230;&#65292;&#21487;&#20197;&#20219;&#24847;&#32553;&#23567;&#65292;&#20174;&#32780;&#23454;&#29616;&#38750;&#24120;&#24265;&#20215;&#21644;&#24555;&#36895;&#30340;&#19979;&#28216;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we generalize and extend an idea of low-rank adaptation (LoRA) of large language models (LLMs) based on Transformer architecture. Widely used LoRA-like methods of fine-tuning LLMs are based on matrix factorization of gradient update. We introduce LoTR, a novel approach for parameter-efficient fine-tuning of LLMs which represents a gradient update to parameters in a form of tensor decomposition. Low-rank adapter for each layer is constructed as a product of three matrices, and tensor structure arises from sharing left and right multipliers of this product among layers. Simultaneous compression of a sequence of layers with low-rank tensor representation allows LoTR to archive even better parameter efficiency then LoRA especially for deep models. Moreover, the core tensor does not depend on original weight dimension and can be made arbitrary small, which allows for extremely cheap and fast downstream fine-tuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;LMs&#30340;&#25506;&#31350;&#23454;&#39564;&#65292;&#39318;&#27425;&#23637;&#31034;&#20102;&#20027;&#39064;&#20869;&#21644;&#36328;&#20027;&#39064;&#27867;&#21270;&#24046;&#36317;&#30340;&#21407;&#22240;&#65292;&#24182;&#34920;&#26126;&#23884;&#20837;&#31354;&#38388;&#30340;&#31283;&#20581;&#24615;&#21644;LMs&#20043;&#38388;&#36890;&#29992;&#27867;&#21270;&#24046;&#36317;&#30340;&#26174;&#33879;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#21457;&#29616;&#22810;&#26679;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#12289;&#26550;&#26500;&#35268;&#33539;&#21270;&#25110;&#25968;&#25454;&#21435;&#37325;&#23545;&#20110;&#22686;&#24378;LMs&#30340;&#31283;&#20581;&#24615;&#21644;&#20943;&#23567;&#27867;&#21270;&#24046;&#36317;&#26377;&#31215;&#26497;&#20316;&#29992;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01375</link><description>&lt;p&gt;
&#36339;&#20837;&#20998;&#27495;&#28857;&#65306;&#25506;&#31350;&#20027;&#39064;&#20869;&#21644;&#36328;&#20027;&#39064;&#27867;&#21270;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Dive into the Chasm: Probing the Gap between In- and Cross-Topic Generalization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;LMs&#30340;&#25506;&#31350;&#23454;&#39564;&#65292;&#39318;&#27425;&#23637;&#31034;&#20102;&#20027;&#39064;&#20869;&#21644;&#36328;&#20027;&#39064;&#27867;&#21270;&#24046;&#36317;&#30340;&#21407;&#22240;&#65292;&#24182;&#34920;&#26126;&#23884;&#20837;&#31354;&#38388;&#30340;&#31283;&#20581;&#24615;&#21644;LMs&#20043;&#38388;&#36890;&#29992;&#27867;&#21270;&#24046;&#36317;&#30340;&#26174;&#33879;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#21457;&#29616;&#22810;&#26679;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#12289;&#26550;&#26500;&#35268;&#33539;&#21270;&#25110;&#25968;&#25454;&#21435;&#37325;&#23545;&#20110;&#22686;&#24378;LMs&#30340;&#31283;&#20581;&#24615;&#21644;&#20943;&#23567;&#27867;&#21270;&#24046;&#36317;&#26377;&#31215;&#26497;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#20027;&#39064;&#20869;&#30340;&#35774;&#23450;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#36825;&#37324;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#30456;&#21516;&#30340;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#36328;&#20027;&#39064;&#30340;&#24773;&#20917;&#19979;&#65292;&#20363;&#22914;&#26538;&#25903;&#31649;&#21046;&#65292;&#23427;&#20204;&#22312;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#19981;&#21516;&#20027;&#39064;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#19977;&#20010;&#25506;&#31350;&#23454;&#39564;&#35777;&#26126;&#20102;&#19981;&#21516;LMs&#20043;&#38388;&#20027;&#39064;&#20869;&#21644;&#36328;&#20027;&#39064;&#27867;&#21270;&#24046;&#36317;&#30340;&#21407;&#22240;&#65292;&#24182;&#39318;&#27425;&#23637;&#31034;&#20102;&#23884;&#20837;&#31354;&#38388;&#30340;&#31283;&#20581;&#24615;&#21644;&#36890;&#29992;&#27867;&#21270;&#24046;&#36317;&#22312;LMs&#20043;&#38388;&#26174;&#33879;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#26356;&#22823;&#30340;LMs&#65292;&#24182;&#24378;&#35843;&#20102;&#25105;&#20204;&#20998;&#26512;&#23545;&#20110;&#26368;&#26032;&#27169;&#22411;&#30340;&#30456;&#20851;&#24615;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#22810;&#26679;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#12289;&#26550;&#26500;&#35268;&#33539;&#21270;&#25110;&#25968;&#25454;&#21435;&#37325;&#37117;&#26377;&#21161;&#20110;&#26356;&#31283;&#20581;&#30340;LMs&#24182;&#20943;&#23567;&#27867;&#21270;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#28145;&#20837;&#29702;&#35299;&#21644;&#27604;&#36739;&#19981;&#21516;&#27867;&#21270;&#22330;&#26223;&#19979;&#30340;&#35821;&#35328;&#27169;&#22411;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (LMs) perform well in In-Topic setups, where training and testing data come from the same topics. However, they face challenges in Cross-Topic scenarios where testing data is derived from distinct topics -- such as Gun Control. This study analyzes various LMs with three probing-based experiments to shed light on the reasons behind the In- vs. Cross-Topic generalization gap. Thereby, we demonstrate, for the first time, that generalization gaps and the robustness of the embedding space vary significantly across LMs. Additionally, we assess larger LMs and underscore the relevance of our analysis for recent models. Overall, diverse pre-training objectives, architectural regularization, or data deduplication contribute to more robust LMs and diminish generalization gaps. Our research contributes to a deeper understanding and comparison of language models across different generalization scenarios.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#25991;&#31456;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#38454;&#27573;&#20998;&#31867;&#26041;&#26696;&#23545;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#25351;&#20986;&#20102;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#24037;&#20316;&#26041;&#21521;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01364</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;: &#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Continual Learning for Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01364
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#25991;&#31456;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#38454;&#27573;&#20998;&#31867;&#26041;&#26696;&#23545;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#25351;&#20986;&#20102;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#24037;&#20316;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#24222;&#22823;&#30340;&#35268;&#27169;&#23548;&#33268;&#35757;&#32451;&#25104;&#26412;&#39640;&#26114;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19981;&#26131;&#39057;&#32321;&#37325;&#26032;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#26356;&#26032;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#36171;&#20104;LLMs&#26032;&#30340;&#25216;&#33021;&#65292;&#24182;&#20351;&#20854;&#19982;&#24555;&#36895;&#21457;&#23637;&#30340;&#20154;&#31867;&#30693;&#35782;&#20445;&#25345;&#21516;&#27493;&#12290;&#26412;&#25991;&#23545;LLMs&#30340;&#25345;&#32493;&#23398;&#20064;&#26368;&#26032;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#36848;&#12290;&#37492;&#20110;LLMs&#30340;&#29420;&#29305;&#24615;&#65292;&#25105;&#20204;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#38454;&#27573;&#20998;&#31867;&#26041;&#26696;&#23545;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#28041;&#21450;&#25345;&#32493;&#39044;&#35757;&#32451;&#12289;&#25351;&#20196;&#35843;&#25972;&#21644;&#23545;&#40784;&#31561;&#26041;&#38754;&#12290;&#25105;&#20204;&#23558;LLMs&#30340;&#25345;&#32493;&#23398;&#20064;&#19982;&#22312;&#35268;&#27169;&#36739;&#23567;&#30340;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#31616;&#21333;&#36866;&#24212;&#26041;&#27861;&#20197;&#21450;&#20854;&#20182;&#22686;&#24378;&#31574;&#30053;(&#22914;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#27169;&#22411;&#32534;&#36753;)&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#27492;&#22806;&#65292;&#26681;&#25454;&#23545;&#22522;&#20934;&#21644;&#35780;&#20272;&#30340;&#35752;&#35770;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#19968;&#37325;&#35201;&#20219;&#21153;&#38754;&#20020;&#30340;&#20960;&#20010;&#25361;&#25112;&#21644;&#26410;&#26469;&#24037;&#20316;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are not amenable to frequent re-training, due to high training costs arising from their massive scale. However, updates are necessary to endow LLMs with new skills and keep them up-to-date with rapidly evolving human knowledge. This paper surveys recent works on continual learning for LLMs. Due to the unique nature of LLMs, we catalog continue learning techniques in a novel multi-staged categorization scheme, involving continual pretraining, instruction tuning, and alignment. We contrast continual learning for LLMs with simpler adaptation methods used in smaller models, as well as with other enhancement strategies like retrieval-augmented generation and model editing. Moreover, informed by a discussion of benchmarks and evaluation, we identify several challenges and future work directions for this crucial task.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#21307;&#30103;&#32034;&#36180;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#23646;&#24615;&#65292;&#30740;&#31350;&#20102;&#24433;&#21709;&#20854;&#21487;&#39564;&#35777;&#24615;&#30340;&#22240;&#32032;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#31185;&#23398;&#20107;&#23454;&#39564;&#35777;&#30340;&#35821;&#26009;&#24211;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01360</link><description>&lt;p&gt;
&#21307;&#30103;&#32034;&#36180;&#30340;&#21487;&#39564;&#35777;&#24615;&#20998;&#26512;&#65306;&#20998;&#26512;&#23454;&#20307;&#21644;&#20851;&#31995;&#23646;&#24615;&#20197;&#36827;&#34892;&#20107;&#23454;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
What Makes Medical Claims (Un)Verifiable? Analyzing Entity and Relation Properties for Fact Verification
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#21307;&#30103;&#32034;&#36180;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#23646;&#24615;&#65292;&#30740;&#31350;&#20102;&#24433;&#21709;&#20854;&#21487;&#39564;&#35777;&#24615;&#30340;&#22240;&#32032;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#31185;&#23398;&#20107;&#23454;&#39564;&#35777;&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#25214;&#19981;&#21040;&#35777;&#25454;&#65292;&#29983;&#29289;&#21307;&#23398;&#32034;&#36180;&#39564;&#35777;&#23558;&#22833;&#36133;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#20107;&#23454;&#26680;&#26597;&#32467;&#26524;&#26410;&#30693;&#65292;&#32034;&#36180;&#26080;&#27861;&#39564;&#35777;&#12290;&#20026;&#20102;&#25913;&#36827;&#36825;&#19968;&#24773;&#20917;&#65292;&#25105;&#20204;&#38656;&#35201;&#20102;&#35299;&#26159;&#21542;&#26377;&#20219;&#20309;&#32034;&#36180;&#23646;&#24615;&#20250;&#24433;&#21709;&#20854;&#21487;&#39564;&#35777;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#23454;&#20307;&#21644;&#20851;&#31995;&#23450;&#20041;&#20102;&#29983;&#29289;&#21307;&#23398;&#32034;&#36180;&#35299;&#21078;&#23398;&#30340;&#26680;&#24515;&#21464;&#37327;&#65292;&#24182;&#20998;&#26512;&#23427;&#20204;&#30340;&#23646;&#24615;&#26159;&#21542;&#26377;&#21161;&#20110;&#21306;&#20998;&#21487;&#39564;&#35777;&#21644;&#19981;&#21487;&#39564;&#35777;&#30340;&#32034;&#36180;&#12290;&#22312;&#19982;&#32463;&#36807;&#35757;&#32451;&#30340;&#27880;&#37322;&#19987;&#23478;&#36827;&#34892;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35201;&#27714;&#20182;&#20204;&#25214;&#21040;&#29983;&#29289;&#21307;&#23398;&#32034;&#36180;&#30340;&#35777;&#25454;&#65292;&#24182;&#35266;&#23519;&#20182;&#20204;&#22914;&#20309;&#25913;&#36827;&#35777;&#25454;&#25628;&#32034;&#30340;&#26597;&#35810;&#12290;&#36825;&#23548;&#33268;&#20102;&#31532;&#19968;&#20010;&#29992;&#20027;&#39064;-&#20851;&#31995;-&#23458;&#20307;&#19977;&#20803;&#32452;&#12289;&#35777;&#25454;&#25991;&#26723;&#21644;&#20107;&#23454;&#26680;&#26597;&#32467;&#26524;&#65288;BEAR-Fact&#35821;&#26009;&#24211;&#65289;&#36827;&#34892;&#31185;&#23398;&#20107;&#23454;&#39564;&#35777;&#30340;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#21457;&#29616;&#65288;1&#65289;&#21457;&#29616;&#21542;&#23450;&#24615;&#32034;&#36180;&#30340;&#35777;&#25454;&#65288;&#20363;&#22914;&#65292;X&#19981;&#23548;&#33268;Y&#65289;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#27880;&#37322;&#32773;&#20027;&#35201;&#36890;&#36807;&#28155;&#21152;&#26469;&#22788;&#29702;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical claim verification fails if no evidence can be discovered. In these cases, the fact-checking verdict remains unknown and the claim is unverifiable. To improve upon this, we have to understand if there are any claim properties that impact its verifiability. In this work we assume that entities and relations define the core variables in a biomedical claim's anatomy and analyze if their properties help us to differentiate verifiable from unverifiable claims. In a study with trained annotation experts we prompt them to find evidence for biomedical claims, and observe how they refine search queries for their evidence search. This leads to the first corpus for scientific fact verification annotated with subject-relation-object triplets, evidence documents, and fact-checking verdicts (the BEAR-Fact corpus). We find (1) that discovering evidence for negated claims (e.g., X-does-not-cause-Y) is particularly challenging. Further, we see that annotators process queries mostly by adding
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22270;&#20687;&#25551;&#36848;&#20013;&#20154;&#31867;&#34892;&#20026;&#30340;&#21464;&#21270;&#65292;&#24182;&#21457;&#29616;&#22270;&#20687;&#30340;&#23646;&#24615;&#19982;&#36825;&#20123;&#21464;&#21270;&#30456;&#20851;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25429;&#25417;&#21040;&#36825;&#31181;&#21464;&#21270;&#65292;&#20294;&#20173;&#23384;&#22312;&#20559;&#24046;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01352</link><description>&lt;p&gt;
&#25551;&#36848;&#22270;&#20687;&#30340;&#8220;&#24555;&#24930;&#8221;: &#37327;&#21270;&#21644;&#39044;&#27979;&#35270;&#35273;&#35821;&#35328;&#36807;&#31243;&#20013;&#20154;&#31867;&#20449;&#21495;&#30340;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Describing Images $\textit{Fast and Slow}$: Quantifying and Predicting the Variation in Human Signals during Visuo-Linguistic Processes
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01352
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22270;&#20687;&#25551;&#36848;&#20013;&#20154;&#31867;&#34892;&#20026;&#30340;&#21464;&#21270;&#65292;&#24182;&#21457;&#29616;&#22270;&#20687;&#30340;&#23646;&#24615;&#19982;&#36825;&#20123;&#21464;&#21270;&#30456;&#20851;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25429;&#25417;&#21040;&#36825;&#31181;&#21464;&#21270;&#65292;&#20294;&#20173;&#23384;&#22312;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#30340;&#23646;&#24615;&#19982;&#20154;&#20204;&#22312;&#25551;&#36848;&#22270;&#20687;&#26102;&#30340;&#34892;&#20026;&#20043;&#38388;&#23384;&#22312;&#22797;&#26434;&#30340;&#20851;&#31995;&#12290;&#36825;&#31181;&#34892;&#20026;&#34920;&#29616;&#20986;&#20016;&#23500;&#30340;&#21464;&#21270;&#65292;&#22914;&#30524;&#21160;&#21644;&#20154;&#20204;&#24320;&#22987;&#25551;&#36848;&#22270;&#20687;&#30340;&#26102;&#26426;&#12290;&#23613;&#31649;&#35270;&#35273;&#35821;&#35328;&#21464;&#24322;&#30340;&#36825;&#20123;&#20449;&#21495;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#65292;&#20294;&#22312;&#24403;&#21069;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#23427;&#20204;&#20960;&#20046;&#34987;&#24573;&#35270;&#65292;&#36825;&#20419;&#20351;&#36827;&#19968;&#27493;&#30340;&#35843;&#26597;&#12290;&#36890;&#36807;&#20351;&#29992;&#21516;&#26102;&#25910;&#38598;&#30340;&#33655;&#20848;&#22270;&#20687;&#25551;&#36848;&#35821;&#26009;&#24211;&#21644;&#30524;&#21160;&#25968;&#25454;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#35270;&#35273;&#35821;&#35328;&#20449;&#21495;&#21464;&#21270;&#30340;&#26412;&#36136;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#12290;&#37492;&#20110;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#20551;&#35774;&#21464;&#21270;&#37096;&#20998;&#28304;&#20110;&#22270;&#20687;&#30340;&#23646;&#24615;&#65292;&#24182;&#25506;&#32034;&#39044;&#35757;&#32451;&#35270;&#35273;&#32534;&#30721;&#22120;&#25152;&#32534;&#30721;&#30340;&#22270;&#20687;&#34920;&#31034;&#33021;&#21542;&#25429;&#25417;&#21040;&#36825;&#31181;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#33021;&#22815;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#36825;&#34920;&#26126;&#27169;&#22411;&#32570;&#20047;&#23545;&#20160;&#20040;&#20351;&#21050;&#28608;&#22797;&#26434;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is an intricate relation between the properties of an image and how humans behave while describing the image. This behavior shows ample variation, as manifested in human signals such as eye movements and when humans start to describe the image. Despite the value of such signals of visuo-linguistic variation, they are virtually disregarded in the training of current pretrained models, which motivates further investigation. Using a corpus of Dutch image descriptions with concurrently collected eye-tracking data, we explore the nature of the variation in visuo-linguistic signals, and find that they correlate with each other. Given this result, we hypothesize that variation stems partly from the properties of the images, and explore whether image representations encoded by pretrained vision encoders can capture such variation. Our results indicate that pretrained models do so to a weak-to-moderate degree, suggesting that the models lack biases about what makes a stimulus complex for 
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22810;&#36873;&#39064;&#22238;&#31572;&#30340;&#21512;&#29702;&#24615;&#36827;&#34892;&#20102;&#22238;&#39038;&#65292;&#21457;&#29616;&#24403;&#21069;&#22522;&#20110;&#22810;&#36873;&#39064;&#22238;&#31572;&#30340;&#22522;&#20934;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01349</link><description>&lt;p&gt;
&#36229;&#36234;&#31572;&#26696;&#65306;&#23545;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22810;&#36873;&#39064;&#22238;&#31572;&#30340;&#21512;&#29702;&#24615;&#30340;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
Beyond the Answers: Reviewing the Rationality of Multiple Choice Question Answering for the Evaluation of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01349
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22810;&#36873;&#39064;&#22238;&#31572;&#30340;&#21512;&#29702;&#24615;&#36827;&#34892;&#20102;&#22238;&#39038;&#65292;&#21457;&#29616;&#24403;&#21069;&#22522;&#20110;&#22810;&#36873;&#39064;&#22238;&#31572;&#30340;&#22522;&#20934;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#21457;&#20102;&#19968;&#22330;&#33539;&#24335;&#36716;&#21464;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#23545;LLMs&#30340;&#20840;&#38754;&#35780;&#20272;&#20173;&#28982;&#26159;&#31038;&#21306;&#38754;&#20020;&#30340;&#24517;&#28982;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#23558;&#22810;&#36873;&#39064;&#22238;&#31572;&#65288;MCQA&#65289;&#20316;&#20026;LLMs&#30340;&#22522;&#20934;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;MCQA&#20316;&#20026;LLMs&#35780;&#20272;&#26041;&#27861;&#30340;&#21512;&#29702;&#24615;&#12290;&#22914;&#26524;LLMs&#30495;&#27491;&#29702;&#35299;&#38382;&#39064;&#30340;&#35821;&#20041;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#24212;&#35813;&#22312;&#20174;&#30456;&#21516;&#38382;&#39064;&#27966;&#29983;&#30340;&#21508;&#31181;&#37197;&#32622;&#19978;&#34920;&#29616;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;LLMs&#30340;&#21709;&#24212;&#19968;&#33268;&#24615;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#25105;&#20204;&#23558;&#20043;&#23450;&#20041;&#20026;LLMs&#30340;&#21709;&#24212;&#21487;&#21464;&#24615;&#32508;&#21512;&#24449;&#65288;REVAS&#65289;&#65292;&#36825;&#34920;&#26126;&#30446;&#21069;&#22522;&#20110;MCQA&#30340;&#22522;&#20934;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;LLMs&#30340;&#30495;&#23454;&#33021;&#21147;&#65292;&#24378;&#35843;&#20102;&#23545;&#26356;&#21512;&#36866;&#30340;&#35780;&#20272;&#26041;&#27861;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of natural language processing (NLP), Large Language Models (LLMs) have precipitated a paradigm shift, markedly enhancing performance in natural language generation tasks. Despite these advancements, the comprehensive evaluation of LLMs remains an inevitable challenge for the community. Recently, the utilization of Multiple Choice Question Answering (MCQA) as a benchmark for LLMs has gained considerable traction. This study investigates the rationality of MCQA as an evaluation method for LLMs. If LLMs genuinely understand the semantics of questions, their performance should exhibit consistency across the varied configurations derived from the same questions. Contrary to this expectation, our empirical findings suggest a notable disparity in the consistency of LLM responses, which we define as REsponse VAriability Syndrome (REVAS) of the LLMs, indicating that current MCQA-based benchmarks may not adequately capture the true capabilities of LLMs, which underscores the need f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#25351;&#20986;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#26126;&#26174;&#30340;&#20869;&#23481;&#35821;&#20041;&#21464;&#21270;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01345</link><description>&lt;p&gt;
&#36339;&#36807;$\textbackslash n$: &#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#20943;&#23569;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#25351;&#20986;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#26126;&#26174;&#30340;&#20869;&#23481;&#35821;&#20041;&#21464;&#21270;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#20854;&#22312;&#35270;&#35273;&#20449;&#24687;&#29702;&#35299;&#19982;&#20154;&#31867;&#35821;&#35328;&#26041;&#38754;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;LVLMs&#20173;&#28982;&#38754;&#20020;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#29983;&#25104;&#19982;&#35270;&#35273;&#20449;&#24687;&#20013;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#30456;&#20851;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#35748;&#20026;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30830;&#23450;&#20102;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#65288;'$\textbackslash n\textbackslash n$'&#65289;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#21363;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#22312;&#8220;$\textbackslash n\textbackslash n$&#8221;&#20043;&#21069;&#21644;&#20043;&#21518;&#30340;&#20869;&#23481;&#32463;&#24120;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#35821;&#20041;&#25913;&#21464;&#12290;&#36825;&#31181;&#27169;&#24335;&#20351;&#24471;&#27169;&#22411;&#25512;&#26029;&#22312;&#8220;$\textbackslash n\textbackslash n$&#8221;&#20043;&#21518;&#30340;&#20869;&#23481;&#24212;&#26126;&#26174;&#19981;&#21516;&#20110;&#21069;&#38754;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('$\textbackslash n\textbackslash n$'), where the content before and after '$\textbackslash n\textbackslash n$' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '$\textbackslash n\textbackslash n$' should be obviously different from the preceding contents wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20004;&#31181;&#22788;&#29702;&#27874;&#20848;&#25991;&#26412;&#26102;&#24207;&#35268;&#33539;&#21270;&#30340;&#26041;&#27861;&#65292;&#19968;&#31181;&#26159;&#22522;&#20110;&#35268;&#21017;&#30340;&#65292;&#19968;&#31181;&#26159;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#30446;&#21069;&#30340;&#30740;&#31350;&#38454;&#27573;&#65292;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#22312;&#22823;&#37096;&#20998;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#20294;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20004;&#31181;&#26041;&#27861;&#37117;&#26377;&#21508;&#33258;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01300</link><description>&lt;p&gt;
&#20004;&#31181;&#22788;&#29702;&#27874;&#20848;&#25991;&#26412;&#26102;&#24207;&#35268;&#33539;&#21270;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Two Approaches to Diachronic Normalization of Polish Texts
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20004;&#31181;&#22788;&#29702;&#27874;&#20848;&#25991;&#26412;&#26102;&#24207;&#35268;&#33539;&#21270;&#30340;&#26041;&#27861;&#65292;&#19968;&#31181;&#26159;&#22522;&#20110;&#35268;&#21017;&#30340;&#65292;&#19968;&#31181;&#26159;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#30446;&#21069;&#30340;&#30740;&#31350;&#38454;&#27573;&#65292;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#22312;&#22823;&#37096;&#20998;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#20294;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20004;&#31181;&#26041;&#27861;&#37117;&#26377;&#21508;&#33258;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22788;&#29702;&#27874;&#20848;&#25991;&#26412;&#26102;&#24207;&#35268;&#33539;&#21270;&#30340;&#20004;&#31181;&#26041;&#27861;&#65306;&#19968;&#31181;&#26159;&#22522;&#20110;&#19968;&#32452;&#25163;&#24037;&#27169;&#24335;&#30340;&#35268;&#21017;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#26159;&#22522;&#20110;&#25991;&#26412;&#21040;&#25991;&#26412;&#36716;&#25442;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#31070;&#32463;&#35268;&#33539;&#21270;&#27169;&#22411;&#12290;&#25991;&#31456;&#35814;&#32454;&#35752;&#35770;&#20102;&#20026;&#35813;&#20219;&#21153;&#20934;&#22791;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#25968;&#25454;&#65292;&#20197;&#21450;&#36827;&#34892;&#27604;&#36739;&#25552;&#20986;&#30340;&#35268;&#33539;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#23454;&#39564;&#12290;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#30446;&#21069;&#38454;&#27573;&#25506;&#32034;&#35813;&#38382;&#39064;&#26102;&#65292;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#22312;4&#20010;&#20934;&#22791;&#30340;&#25968;&#25454;&#38598;&#21464;&#20307;&#20013;&#26377;3&#20010;&#20248;&#20110;&#31070;&#32463;&#26041;&#27861;&#65292;&#23613;&#31649;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#20855;&#26377;&#21508;&#33258;&#30340;&#20248;&#21155;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses two approaches to the diachronic normalization of Polish texts: a rule-based solution that relies on a set of handcrafted patterns, and a neural normalization model based on the text-to-text transfer transformer architecture. The training and evaluation data prepared for the task are discussed in detail, along with experiments conducted to compare the proposed normalization solutions. A quantitative and qualitative analysis is made. It is shown that at the current stage of inquiry into the problem, the rule-based solution outperforms the neural one on 3 out of 4 variants of the prepared dataset, although in practice both approaches have distinct advantages and disadvantages.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;T2I-ICL&#22522;&#20934;&#25968;&#25454;&#38598;CoBSAT&#12290;&#30740;&#31350;&#21457;&#29616;MLLMs&#22312;&#35299;&#20915;T2I-ICL&#38382;&#39064;&#26102;&#38754;&#20020;&#30528;&#22810;&#27169;&#24577;&#21644;&#22270;&#20687;&#29983;&#25104;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#31561;&#31574;&#30053;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01293</link><description>&lt;p&gt;
MLLMs&#33021;&#21542;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#36716;&#25442;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can MLLMs Perform Text-to-Image In-Context Learning?
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;T2I-ICL&#22522;&#20934;&#25968;&#25454;&#38598;CoBSAT&#12290;&#30740;&#31350;&#21457;&#29616;MLLMs&#22312;&#35299;&#20915;T2I-ICL&#38382;&#39064;&#26102;&#38754;&#20020;&#30528;&#22810;&#27169;&#24577;&#21644;&#22270;&#20687;&#29983;&#25104;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#31561;&#31574;&#30053;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21457;&#23637;&#21040;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#25512;&#21160;&#20102;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#25193;&#23637;&#21040;&#22810;&#27169;&#24335;&#30340;&#30740;&#31350;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#30340;ICL&#19978;&#12290;&#28982;&#32780;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;ICL&#65288;T2I-ICL&#65289;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24615;&#21644;&#28508;&#22312;&#30340;&#24212;&#29992;&#65292;&#20294;&#20173;&#28982;&#23569;&#26377;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;T2I-ICL&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;CoBSAT&#65292;&#31532;&#19968;&#20010;&#21253;&#21547;&#21313;&#20010;&#20219;&#21153;&#30340;T2I-ICL&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#20845;&#20010;&#26368;&#20808;&#36827;&#30340;MLLMs&#65292;&#25105;&#20204;&#21457;&#29616;MLLMs&#22312;&#35299;&#20915;T2I-ICL&#38382;&#39064;&#26102;&#38754;&#20020;&#30528;&#30456;&#24403;&#22823;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22810;&#27169;&#24577;&#21644;&#22270;&#20687;&#29983;&#25104;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#26159;&#20027;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#24494;&#35843;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#31561;&#31574;&#30053;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;\url{https://github.com/UW-Madison-Lee-Lab/CoBSAT}&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evolution from Large Language Models (LLMs) to Multimodal Large Language Models (MLLMs) has spurred research into extending In-Context Learning (ICL) to its multimodal counterpart. Existing such studies have primarily concentrated on image-to-text ICL. However, the Text-to-Image ICL (T2I-ICL), with its unique characteristics and potential applications, remains underexplored. To address this gap, we formally define the task of T2I-ICL and present CoBSAT, the first T2I-ICL benchmark dataset, encompassing ten tasks. Utilizing our dataset to benchmark six state-of-the-art MLLMs, we uncover considerable difficulties MLLMs encounter in solving T2I-ICL. We identify the primary challenges as the inherent complexity of multimodality and image generation. To overcome these challenges, we explore strategies like fine-tuning and Chain-of-Thought prompting, demonstrating notable improvements. Our code and dataset are available at \url{https://github.com/UW-Madison-Lee-Lab/CoBSAT}.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#35821;&#20041;&#35770;&#35777;&#65292;&#35752;&#35770;&#20102;&#26159;&#21542;&#21487;&#20197;&#31216;&#20043;&#20026;&#8220;&#26426;&#26800;&#24605;&#32500;&#8221;&#65292;&#20197;&#21450;ChatGPT&#27169;&#22411;&#33021;&#21542;&#23454;&#29616;&#35813;&#24605;&#32500;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#8220;&#26426;&#26800;&#24605;&#32500;&#8221;&#32570;&#20047;&#19982;&#29616;&#23454;&#30456;&#20851;&#30340;&#35777;&#25454;&#21644;&#20010;&#20154;&#20449;&#24565;&#65292;&#22240;&#27492;&#26080;&#27861;&#24418;&#25104;&#23545;&#19990;&#30028;&#30340;&#20449;&#24565;&#21644;&#30495;&#23454;&#24615;&#21028;&#26029;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01267</link><description>&lt;p&gt;
&#20154;&#31867;&#19982;&#26426;&#22120;&#65306;&#36923;&#36753;&#12289;&#30495;&#23454;&#24615;&#19982;ChatGPT
&lt;/p&gt;
&lt;p&gt;
The Human and the Mechanical: logos, truthfulness, and ChatGPT
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#35821;&#20041;&#35770;&#35777;&#65292;&#35752;&#35770;&#20102;&#26159;&#21542;&#21487;&#20197;&#31216;&#20043;&#20026;&#8220;&#26426;&#26800;&#24605;&#32500;&#8221;&#65292;&#20197;&#21450;ChatGPT&#27169;&#22411;&#33021;&#21542;&#23454;&#29616;&#35813;&#24605;&#32500;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#8220;&#26426;&#26800;&#24605;&#32500;&#8221;&#32570;&#20047;&#19982;&#29616;&#23454;&#30456;&#20851;&#30340;&#35777;&#25454;&#21644;&#20010;&#20154;&#20449;&#24565;&#65292;&#22240;&#27492;&#26080;&#27861;&#24418;&#25104;&#23545;&#19990;&#30028;&#30340;&#20449;&#24565;&#21644;&#30495;&#23454;&#24615;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#26159;&#21542;&#36866;&#24403;&#22320;&#35848;&#35770;&#8220;&#26426;&#26800;&#24605;&#32500;&#8221;&#65292;&#20197;&#21450;ChatGPT&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#34987;&#35270;&#20026;&#23454;&#29616;&#20102;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#22312;&#24403;&#21069;&#30340;&#35752;&#35770;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#35821;&#20041;&#35770;&#35777;&#12290;&#20154;&#31867;&#26029;&#35328;&#30340;&#34892;&#20026;&#38656;&#35201;&#24418;&#25104;&#19968;&#20010;&#30495;&#23454;&#24615;&#21028;&#26029;&#12290;&#20351;&#29992;&#24773;&#24577;&#21160;&#35789;&#20462;&#39280;&#26029;&#35328;&#65288;&#32422;&#32752;&#19968;&#23450;&#22312;&#23478;&#65289;&#21644;&#20351;&#29992;&#20027;&#35266;&#20803;&#32032;&#65288;&#32422;&#32752;&#26126;&#26174;&#22312;&#23478;&#65289;&#34920;&#26126;&#35828;&#35805;&#32773;&#27491;&#22312;&#25805;&#32437;&#22905;&#30340;&#21028;&#26029;&#65292;&#22312;&#21512;&#20316;&#30340;&#35821;&#22659;&#20013;&#65292;&#24847;&#22270;&#23558;&#22905;&#30340;&#35748;&#30693;&#29366;&#24577;&#23545;&#35805;&#26041;&#36879;&#26126;&#21270;&#12290;&#30495;&#23454;&#24615;&#21028;&#26029;&#26159;&#22522;&#20110;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#24418;&#25104;&#30340;&#65306;&#65288;i&#65289;&#19982;&#29616;&#23454;&#30456;&#20851;&#30340;&#35777;&#25454;&#65288;&#22806;&#29983;&#35777;&#25454;&#65289;&#21644;&#65288;ii&#65289;&#19982;&#20559;&#22909;&#21644;&#20010;&#20154;&#20449;&#24565;&#30456;&#20851;&#30340;&#20869;&#22312;&#35777;&#25454;&#12290;&#32780;&#8220;&#26426;&#26800;&#24605;&#32500;&#8221;&#32570;&#20047;&#36825;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;i&#65289;&#23427;&#20204;&#19982;&#29616;&#23454;&#26080;&#20851;&#65292;&#65288;ii&#65289;&#27809;&#26377;&#20869;&#22312;&#35777;&#25454;&#12290;&#22240;&#27492;&#23427;&#20204;&#32570;&#20047;&#23545;&#19990;&#30028;&#30340;&#20449;&#24565;&#24418;&#25104;&#21644;&#30495;&#23454;&#24615;&#21028;&#26029;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper addresses the question of whether it is appropriate to talk about `mechanical minds' at all, and whether ChatGPT models can indeed be thought of as realizations of that. Our paper adds a semantic argument to the current debate. The act of human assertion requires the formation of a veridicality judgment. Modification of assertions with modals (John must be at home) and the use of subjective elements (John is obviously at home) indicate that the speaker is manipulating her judgments and, in a cooperative context, intends her epistemic state to be transparent to the addressee. Veridicality judgments are formed on the basis of two components: (i) evidence that relates to reality (exogenous evidence) and (ii) endogenous evidence, such as preferences and private beliefs. `Mechanical minds' lack these two components: (i) they do not relate to reality and (ii) do not have endogenous evidence. Therefore they lack the ability to form a belief about the world and a veridicality judgmen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23569;&#26679;&#26412;&#23884;&#22871;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;EnDe&#26816;&#32034;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;&#26426;&#21046;&#25913;&#36827;&#20102;&#31034;&#20363;&#28436;&#31034;&#36873;&#25321;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#28436;&#31034;&#31034;&#20363;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01182</link><description>&lt;p&gt;
&#38024;&#23545;&#23569;&#26679;&#26412;&#23884;&#22871;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning for Few-Shot Nested Named Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23569;&#26679;&#26412;&#23884;&#22871;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;EnDe&#26816;&#32034;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;&#26426;&#21046;&#25913;&#36827;&#20102;&#31034;&#20363;&#28436;&#31034;&#36873;&#25321;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#28436;&#31034;&#31034;&#20363;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23884;&#22871;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20013;&#65292;&#23454;&#20307;&#19982;&#23454;&#20307;&#30456;&#20114;&#23884;&#22871;&#65292;&#22240;&#27492;&#38656;&#35201;&#26356;&#22810;&#30340;&#25968;&#25454;&#27880;&#37322;&#26469;&#35299;&#20915;&#12290;&#36825;&#20419;&#20351;&#21457;&#23637;&#20986;&#23569;&#26679;&#26412;&#23884;&#22871;NER&#65292;&#20854;&#20013;&#20855;&#26377;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26222;&#21450;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#25928;&#21644;&#21019;&#26032;&#30340;ICL&#26694;&#26550;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#23884;&#22871;NER&#30340;&#35774;&#23450;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#26032;&#39062;&#30340;&#31034;&#20363;&#28436;&#31034;&#36873;&#25321;&#26426;&#21046;EnDe retriever&#25913;&#36827;&#20102;ICL&#25552;&#31034;&#12290;&#22312;EnDe&#26816;&#32034;&#22120;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#19977;&#31181;&#31867;&#22411;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#20998;&#21035;&#26159;&#35821;&#20041;&#30456;&#20284;&#24230;&#12289;&#36793;&#30028;&#30456;&#20284;&#24230;&#21644;&#26631;&#31614;&#30456;&#20284;&#24230;&#65292;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#28436;&#31034;&#31034;&#20363;&#12290;&#23545;&#19977;&#20010;&#23884;&#22871;NER&#21644;&#22235;&#20010;&#25153;&#24179;NER&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In nested Named entity recognition (NER), entities are nested with each other, and thus requiring more data annotations to address. This leads to the development of few-shot nested NER, where the prevalence of pretrained language models with in-context learning (ICL) offers promising solutions. In this work, we introduce an effective and innovative ICL framework for the setting of few-shot nested NER. We improve the ICL prompt by devising a novel example demonstration selection mechanism, EnDe retriever. In EnDe retriever, we employ contrastive learning to perform three types of representation learning, in terms of semantic similarity, boundary similarity, and label similarity, to generate high-quality demonstration examples. Extensive experiments over three nested NER and four flat NER datasets demonstrate the efficacy of our system.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#32541;&#38598;&#25104;&#29983;&#25104;&#24335;&#26816;&#32034;&#12289;&#38381;&#24335;&#29983;&#25104;&#21644;RAG&#65292;&#21033;&#29992;&#22806;&#37096;&#35821;&#26009;&#22788;&#29702;&#21508;&#31181;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01176</link><description>&lt;p&gt;
&#20026;&#21033;&#29992;&#22806;&#37096;&#35821;&#26009;&#36827;&#34892;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#32780;&#26500;&#24314;&#30340;&#32479;&#19968;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards a Unified Language Model for Knowledge-Intensive Tasks Utilizing External Corpus
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#32541;&#38598;&#25104;&#29983;&#25104;&#24335;&#26816;&#32034;&#12289;&#38381;&#24335;&#29983;&#25104;&#21644;RAG&#65292;&#21033;&#29992;&#22806;&#37096;&#35821;&#26009;&#22788;&#29702;&#21508;&#31181;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#28982;&#32780;&#22312;&#38656;&#35201;&#22806;&#37096;&#30693;&#35782;&#26469;&#28304;&#30340;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#65292;&#23427;&#20204;&#24448;&#24448;&#20250;&#20135;&#29983;&#34394;&#26500;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25104;&#20026;&#20102;&#19968;&#31181;&#27969;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26816;&#32034;&#27169;&#22359;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#25991;&#26723;&#32034;&#24341;&#65292;&#36825;&#21487;&#33021;&#19982;&#29983;&#25104;&#20219;&#21153;&#30456;&#33073;&#31163;&#12290;&#36890;&#36807;&#29983;&#25104;&#24335;&#26816;&#32034;&#65288;GR&#65289;&#26041;&#27861;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#30456;&#20851;&#25991;&#26723;&#26631;&#35782;&#31526;&#65288;DocIDs&#65289;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;GR&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;LLMs&#22312;GR&#20013;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#32541;&#38598;&#25104;&#29983;&#25104;&#24335;&#26816;&#32034;&#12289;&#38381;&#24335;&#29983;&#25104;&#21644;RAG&#65292;&#21033;&#29992;&#22806;&#37096;&#35821;&#26009;&#22788;&#29702;&#21508;&#31181;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large language models (LLMs) has showcased their efficacy across various domains, yet they often hallucinate, especially in knowledge-intensive tasks that require external knowledge sources. To improve factual accuracy of language models, retrieval-augmented generation (RAG) has emerged as a popular solution. However, traditional retrieval modules often rely on large-scale document indexes, which can be disconnected from generative tasks. Through generative retrieval (GR) approach, language models can achieve superior retrieval performance by directly generating relevant document identifiers (DocIDs). However, the relationship between GR and downstream tasks, as well as the potential of LLMs in GR, remains unexplored. In this paper, we present a unified language model that utilizes external corpus to handle various knowledge-intensive tasks by seamlessly integrating generative retrieval, closed-book generation, and RAG. In order to achieve effective retrieval and generati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23884;&#20837;&#30456;&#20284;&#24615;&#30340;&#25552;&#31034;&#32531;&#23384;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LMMs)&#30340;&#25512;&#29702;&#25928;&#29575;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#33976;&#39311;&#26041;&#27861;&#26469;&#20248;&#21270;&#29616;&#26377;&#23884;&#20837;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32531;&#23384;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01173</link><description>&lt;p&gt;
&#36890;&#36807;&#23884;&#20837;&#30456;&#20284;&#24615;&#23454;&#29616;&#39640;&#25928;&#30340;&#25552;&#31034;&#32531;&#23384;
&lt;/p&gt;
&lt;p&gt;
Efficient Prompt Caching via Embedding Similarity
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23884;&#20837;&#30456;&#20284;&#24615;&#30340;&#25552;&#31034;&#32531;&#23384;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LMMs)&#30340;&#25512;&#29702;&#25928;&#29575;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#33976;&#39311;&#26041;&#27861;&#26469;&#20248;&#21270;&#29616;&#26377;&#23884;&#20837;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32531;&#23384;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#23427;&#38754;&#20020;&#30528;&#36164;&#28304;&#28040;&#32791;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#31034;&#32531;&#23384;&#26469;&#25552;&#39640;LLMs&#30340;&#25512;&#29702;&#25928;&#29575;&#65292;&#21363;&#65292;&#22914;&#26524;&#24403;&#21069;&#25552;&#31034;&#21487;&#20197;&#30001;&#21069;&#19968;&#20010;&#25552;&#31034;&#30340;&#21516;&#26679;&#22238;&#31572;&#32780;&#24471;&#21040;&#22238;&#31572;&#65292;&#23601;&#21487;&#20197;&#30452;&#25509;&#21033;&#29992;&#35813;&#21069;&#19968;&#20010;&#22238;&#31572;&#32780;&#19981;&#35843;&#29992;LLMs&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#36890;&#36807;&#23884;&#20837;&#30456;&#20284;&#24615;&#26469;&#25552;&#21319;&#21333;&#36718;&#38382;&#31572;&#20219;&#21153;&#30340;&#32531;&#23384;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#30446;&#21069;&#24050;&#26377;&#30340;&#25552;&#31034;&#23884;&#20837;&#20027;&#35201;&#20851;&#27880;&#20004;&#20010;&#25552;&#31034;&#26159;&#21542;&#35821;&#20041;&#30456;&#20284;&#65292;&#36825;&#19982;&#21516;&#26679;&#30340;&#22238;&#31572;&#26159;&#21542;&#21487;&#20197;&#22238;&#31572;&#23427;&#20204;&#24182;&#19981;&#31561;&#20215;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33976;&#39311;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#29616;&#26377;&#30340;&#23884;&#20837;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32531;&#23384;&#39044;&#27979;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#19979;&#25552;&#20379;&#20102;&#25105;&#20204;&#26041;&#27861;&#25910;&#25947;&#30340;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25512;&#29702;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved huge success in numerous natural language process (NLP) tasks. However, it faces the challenge of significant resource consumption during inference. In this paper, we aim to improve the inference efficiency of LLMs by prompt caching, i.e., if the current prompt can be answered by the same response of a previous prompt, one can directly utilize that previous response without calling the LLM. Specifically, we focus on the prediction accuracy of prompt caching for single-round question-answering tasks via embedding similarity. The existing embeddings of prompts mostly focus on whether two prompts are semantically similar, which is not necessarily equivalent to whether the same response can answer them. Therefore, we propose a distillation-based method to fine-tune the existing embeddings for better caching prediction. Theoretically, we provide finite-sample guarantees for the convergence of our method under different types of loss functions. Empi
&lt;/p&gt;</description></item><item><title>STAR&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#24577;&#21387;&#32553;&#21644;&#20248;&#21270;&#24310;&#36831;&#12289;&#20869;&#23384;&#21344;&#29992;&#21644;&#36136;&#37327;&#65292;&#23454;&#29616;&#23545;&#27969;&#30340;&#39640;&#25928;&#24207;&#21015;&#36716;&#23548;&#65292;&#24182;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01172</link><description>&lt;p&gt;
&#27969;&#24335;&#24207;&#21015;&#36716;&#23548;&#36890;&#36807;&#21160;&#24577;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Streaming Sequence Transduction through Dynamic Compression
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01172
&lt;/p&gt;
&lt;p&gt;
STAR&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#24577;&#21387;&#32553;&#21644;&#20248;&#21270;&#24310;&#36831;&#12289;&#20869;&#23384;&#21344;&#29992;&#21644;&#36136;&#37327;&#65292;&#23454;&#29616;&#23545;&#27969;&#30340;&#39640;&#25928;&#24207;&#21015;&#36716;&#23548;&#65292;&#24182;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;STAR&#65288;&#24102;&#26377;&#38170;&#23450;&#34920;&#31034;&#30340;&#27969;&#24335;&#36716;&#23548;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;&#27969;&#30340;&#39640;&#25928;&#24207;&#21015;&#36716;&#23548;&#12290;STAR&#21160;&#24577;&#22320;&#23545;&#36755;&#20837;&#27969;&#36827;&#34892;&#20998;&#27573;&#65292;&#21019;&#24314;&#21387;&#32553;&#30340;&#38170;&#23450;&#34920;&#31034;&#65292;&#23454;&#29616;&#36817;&#20046;&#26080;&#25439;&#30340;&#21387;&#32553;&#65288;12&#20493;&#65289;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20013;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;STAR&#22312;&#21516;&#26102;&#36827;&#34892;&#35821;&#38899;&#21040;&#25991;&#26412;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20248;&#36234;&#30340;&#20998;&#21106;&#21644;&#24310;&#36831;-&#36136;&#37327;&#25240;&#34935;&#65292;&#20248;&#21270;&#24310;&#36831;&#12289;&#20869;&#23384;&#21344;&#29992;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce STAR (Stream Transduction with Anchor Representations), a novel Transformer-based model designed for efficient sequence-to-sequence transduction over streams. STAR dynamically segments input streams to create compressed anchor representations, achieving nearly lossless compression (12x) in Automatic Speech Recognition (ASR) and outperforming existing methods. Moreover, STAR demonstrates superior segmentation and latency-quality trade-offs in simultaneous speech-to-text tasks, optimizing latency, memory footprint, and quality.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LLM-Detector&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#24320;&#28304;LLM&#25351;&#20196;&#35843;&#25972;&#26469;&#25913;&#36827;AI&#29983;&#25104;&#30340;&#20013;&#25991;&#25991;&#26412;&#26816;&#27979;&#12290;&#36890;&#36807;&#25552;&#20379;&#28151;&#21512;&#20102;&#20154;&#24037;&#32534;&#20889;&#21477;&#23376;&#21644;LLMs&#28070;&#39280;&#21477;&#23376;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;LLMs&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#25991;&#26723;&#32423;&#21644;&#21477;&#23376;&#32423;&#19978;&#23454;&#29616;&#39640;&#25928;&#30340;&#25991;&#26412;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;BERT&#21644;RoBERTa&#30340;&#27169;&#22411;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01158</link><description>&lt;p&gt;
LLM-Detector: &#20351;&#29992;&#24320;&#28304;LLM&#25351;&#20196;&#35843;&#25972;&#26469;&#25913;&#36827;AI&#29983;&#25104;&#30340;&#20013;&#25991;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
LLM-Detector: Improving AI-Generated Chinese Text Detection with Open-Source LLM Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LLM-Detector&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#24320;&#28304;LLM&#25351;&#20196;&#35843;&#25972;&#26469;&#25913;&#36827;AI&#29983;&#25104;&#30340;&#20013;&#25991;&#25991;&#26412;&#26816;&#27979;&#12290;&#36890;&#36807;&#25552;&#20379;&#28151;&#21512;&#20102;&#20154;&#24037;&#32534;&#20889;&#21477;&#23376;&#21644;LLMs&#28070;&#39280;&#21477;&#23376;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;LLMs&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#25991;&#26723;&#32423;&#21644;&#21477;&#23376;&#32423;&#19978;&#23454;&#29616;&#39640;&#25928;&#30340;&#25991;&#26412;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;BERT&#21644;RoBERTa&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#21644;&#20854;&#20182;&#36890;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#26377;&#20851;&#28389;&#29992;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#30340;&#25285;&#24551;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;BERT&#21644;RoBERTa&#30340;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#27169;&#22411;&#23481;&#26131;&#22312;&#39046;&#22495;&#20869;&#36807;&#24230;&#25311;&#21512;&#65292;&#23548;&#33268;&#39046;&#22495;&#22806;&#65288;OOD&#65289;&#30340;&#26816;&#27979;&#24615;&#33021;&#24046;&#12290;&#26412;&#25991;&#39318;&#20808;&#25910;&#38598;&#20102;&#20154;&#24037;&#19987;&#23478;&#21644;9&#31181;LLMs&#29983;&#25104;&#30340;&#20013;&#25991;&#25991;&#26412;&#22238;&#31572;&#65292;&#38024;&#23545;&#22810;&#20010;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#24182;&#36827;&#19968;&#27493;&#21019;&#24314;&#20102;&#19968;&#20010;&#28151;&#21512;&#20102;&#20154;&#24037;&#32534;&#20889;&#21477;&#23376;&#21644;LLMs&#28070;&#39280;&#21477;&#23376;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;LLM-Detector&#65292;&#19968;&#31181;&#36890;&#36807;LLMs&#30340;&#25351;&#20196;&#35843;&#25972;&#23454;&#29616;&#25991;&#26723;&#32423;&#21644;&#21477;&#23376;&#32423;&#25991;&#26412;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;LLMs&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#33719;&#24471;&#30340;&#20016;&#23500;&#30693;&#35782;&#65292;&#20351;&#20854;&#33021;&#22815;&#26816;&#27979;&#23427;&#20204;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#25351;&#20196;&#35843;&#25972;&#23558;&#27169;&#22411;&#30340;&#21709;&#24212;&#19982;&#29992;&#25143;&#30340;&#26399;&#26395;&#25991;&#26412;&#26816;&#27979;&#20219;&#21153;&#20445;&#25345;&#19968;&#33268;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20043;&#21069;&#30340;&#26041;&#27861;&#20986;&#29616;&#20102;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT and other general large language models (LLMs) have achieved remarkable success, but they have also raised concerns about the misuse of AI-generated texts. Existing AI-generated text detection models, such as based on BERT and RoBERTa, are prone to in-domain over-fitting, leading to poor out-of-domain (OOD) detection performance. In this paper, we first collected Chinese text responses generated by human experts and 9 types of LLMs, for which to multiple domains questions, and further created a dataset that mixed human-written sentences and sentences polished by LLMs. We then proposed LLM-Detector, a novel method for both document-level and sentence-level text detection through Instruction Tuning of LLMs. Our method leverages the wealth of knowledge LLMs acquire during pre-training, enabling them to detect the text they generate. Instruction tuning aligns the model's responses with the user's expected text detection tasks. Experimental results show that previous methods struggl
&lt;/p&gt;</description></item><item><title>CABINET&#26159;&#19968;&#20010;&#29992;&#20110;&#34920;&#26684;&#38382;&#31572;&#31995;&#32479;&#30340;&#22522;&#20110;&#20869;&#23481;&#30456;&#20851;&#24615;&#30340;&#22122;&#22768;&#38477;&#20302;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;&#22788;&#29702;&#34920;&#26684;&#20869;&#23481;&#24182;&#29983;&#25104;&#35299;&#26512;&#35821;&#21477;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#19987;&#27880;&#20110;&#30456;&#20851;&#34920;&#26684;&#25968;&#25454;&#32780;&#25233;&#21046;&#26080;&#20851;&#20449;&#24687;&#30340;&#24178;&#25200;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01155</link><description>&lt;p&gt;
CABINET: &#34920;&#26684;&#38382;&#31572;&#31995;&#32479;&#30340;&#22522;&#20110;&#20869;&#23481;&#30456;&#20851;&#24615;&#30340;&#22122;&#22768;&#38477;&#20302;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CABINET: Content Relevance based Noise Reduction for Table Question Answering
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01155
&lt;/p&gt;
&lt;p&gt;
CABINET&#26159;&#19968;&#20010;&#29992;&#20110;&#34920;&#26684;&#38382;&#31572;&#31995;&#32479;&#30340;&#22522;&#20110;&#20869;&#23481;&#30456;&#20851;&#24615;&#30340;&#22122;&#22768;&#38477;&#20302;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;&#22788;&#29702;&#34920;&#26684;&#20869;&#23481;&#24182;&#29983;&#25104;&#35299;&#26512;&#35821;&#21477;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#19987;&#27880;&#20110;&#30456;&#20851;&#34920;&#26684;&#25968;&#25454;&#32780;&#25233;&#21046;&#26080;&#20851;&#20449;&#24687;&#30340;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#26684;&#29702;&#35299;&#33021;&#21147;&#36890;&#36807;&#23545;&#34920;&#26684;&#30340;&#38382;&#31572;&#20219;&#21153;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#36890;&#24120;&#65292;&#21482;&#26377;&#34920;&#26684;&#30340;&#19968;&#23567;&#37096;&#20998;&#19982;&#32473;&#23450;&#38382;&#39064;&#30340;&#31572;&#26696;&#30456;&#20851;&#12290;&#19981;&#30456;&#20851;&#30340;&#37096;&#20998;&#20250;&#20135;&#29983;&#22122;&#22768;&#21644;&#24178;&#25200;&#20449;&#24687;&#65292;&#23548;&#33268;LLMs&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CABINET&#65288;&#22522;&#20110;&#20869;&#23481;&#30456;&#20851;&#24615;&#30340;&#34920;&#26684;&#38382;&#31572;&#22122;&#22768;&#38477;&#20302;&#26041;&#27861;&#65289;- &#19968;&#20010;&#33021;&#22815;&#35753;LLMs&#19987;&#27880;&#20110;&#30456;&#20851;&#34920;&#26684;&#25968;&#25454;&#24182;&#25233;&#21046;&#26080;&#20851;&#20449;&#24687;&#30340;&#26694;&#26550;&#12290;CABINET&#21253;&#25324;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#30456;&#20851;&#24615;&#35780;&#20998;&#22120;&#65288;URS&#65289;&#65292;&#19982;&#38382;&#31572;LLM&#24046;&#24322;&#24615;&#35757;&#32451;&#65292;&#26681;&#25454;&#20854;&#19982;&#36755;&#20837;&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#23545;&#34920;&#26684;&#20869;&#23481;&#36827;&#34892;&#21152;&#26435;&#22788;&#29702;&#21518;&#20877;&#36755;&#20837;&#38382;&#31572;LLM&#65288;QA LLM&#65289;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#36741;&#21161;&#30456;&#20851;&#24615;&#35780;&#20998;&#22120;&#65292;CABINET&#21033;&#29992;&#19968;&#20010;&#24369;&#30417;&#30563;&#27169;&#22359;&#29983;&#25104;&#19968;&#20010;&#35299;&#26512;&#35821;&#21477;&#65292;&#25551;&#36848;&#34892;&#21644;&#21015;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Table understanding capability of Large Language Models (LLMs) has been extensively studied through the task of question-answering (QA) over tables. Typically, only a small part of the whole table is relevant to derive the answer for a given question. The irrelevant parts act as noise and are distracting information, resulting in sub-optimal performance due to the vulnerability of LLMs to noise. To mitigate this, we propose CABINET (Content RelevAnce-Based NoIse ReductioN for TablE QuesTion-Answering) - a framework to enable LLMs to focus on relevant tabular data by suppressing extraneous information. CABINET comprises an Unsupervised Relevance Scorer (URS), trained differentially with the QA LLM, that weighs the table content based on its relevance to the input question before feeding it to the question-answering LLM (QA LLM). To further aid the relevance scorer, CABINET employs a weakly supervised module that generates a parsing statement describing the criteria of rows and columns r
&lt;/p&gt;</description></item><item><title>"AccentFold"&#26159;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#21475;&#38899;&#23884;&#20837;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#26469;&#25913;&#36827;ASR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#24615;&#20998;&#26512;&#35821;&#38899;&#23884;&#20837;&#65292;&#25581;&#31034;&#38750;&#27954;&#21475;&#38899;&#20043;&#38388;&#30340;&#26377;&#36259;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#20102;Ethnologue&#20808;&#21069;&#26410;&#32463;&#25551;&#36848;&#30340;&#21475;&#38899;&#20851;&#31995;&#12290;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01152</link><description>&lt;p&gt;
"AccentFold&#65306;&#36890;&#36807;&#38646;&#26679;&#26412;ASR&#36866;&#24212;&#25506;&#32034;&#38750;&#27954;&#21475;&#38899;&#20043;&#26053;"
&lt;/p&gt;
&lt;p&gt;
AccentFold: A Journey through African Accents for Zero-Shot ASR Adaptation to Target Accents
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01152
&lt;/p&gt;
&lt;p&gt;
"AccentFold"&#26159;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#21475;&#38899;&#23884;&#20837;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#26469;&#25913;&#36827;ASR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#24615;&#20998;&#26512;&#35821;&#38899;&#23884;&#20837;&#65292;&#25581;&#31034;&#38750;&#27954;&#21475;&#38899;&#20043;&#38388;&#30340;&#26377;&#36259;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#20102;Ethnologue&#20808;&#21069;&#26410;&#32463;&#25551;&#36848;&#30340;&#21475;&#38899;&#20851;&#31995;&#12290;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#38899;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#21475;&#38899;&#35821;&#38899;&#20381;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#24314;&#27169;&#25216;&#26415;&#25110;&#21019;&#24314;&#26377;&#21475;&#38899;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#65292;&#20294;&#30001;&#20110;&#38750;&#27954;&#21475;&#38899;&#30340;&#22810;&#26679;&#24615;&#21644;&#30456;&#20851;&#30340;&#39044;&#31639;&#38480;&#21046;&#65292;&#25910;&#38598;&#21040;&#36275;&#22815;&#30340;&#25968;&#25454;&#20173;&#28982;&#19981;&#21487;&#34892;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#8212;&#8212;"AccentFold"&#65292;&#23427;&#21033;&#29992;&#20102;&#23398;&#20064;&#21040;&#30340;&#21475;&#38899;&#23884;&#20837;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#26469;&#25913;&#36827;&#19979;&#28216;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12290;&#25105;&#20204;&#23545;&#20195;&#34920;100&#22810;&#31181;&#38750;&#27954;&#21475;&#38899;&#30340;&#35821;&#38899;&#23884;&#20837;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#26377;&#36259;&#30340;&#21475;&#38899;&#31354;&#38388;&#20851;&#31995;&#65292;&#31361;&#26174;&#20986;&#22320;&#29702;&#21644;&#20146;&#32536;&#30456;&#20284;&#24615;&#65292;&#25429;&#25417;&#21040;&#20174;&#35821;&#38899;&#20013;&#32463;&#39564;&#24615;&#22320;&#23398;&#20064;&#21040;&#30340;&#19968;&#33268;&#30340;&#38899;&#31995;&#21644;&#24418;&#24577;&#23398;&#35268;&#24459;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;Ethnologue&#20808;&#21069;&#26410;&#32463;&#25551;&#36848;&#30340;&#21475;&#38899;&#20851;&#31995;&#12290;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advancements in speech recognition, accented speech remains challenging. While previous approaches have focused on modeling techniques or creating accented speech datasets, gathering sufficient data for the multitude of accents, particularly in the African context, remains impractical due to their sheer diversity and associated budget constraints. To address these challenges, we propose \textit{AccentFold}, a method that exploits spatial relationships between learned accent embeddings to improve downstream Automatic Speech Recognition (ASR). Our exploratory analysis of speech embeddings representing 100+ African accents reveals interesting spatial accent relationships highlighting geographic and genealogical similarities, capturing consistent phonological, and morphological regularities, all learned empirically from speech. Furthermore, we discover accent relationships previously uncharacterized by the Ethnologue. Through empirical evaluation, we demonstrate the effectiveness o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#65288;MACRS&#65289;&#65292;&#23427;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#34892;&#21160;&#35268;&#21010;&#26694;&#26550;&#26469;&#25511;&#21046;&#23545;&#35805;&#27969;&#31243;&#65292;&#24182;&#22522;&#20110;LLM&#29983;&#25104;&#22810;&#20010;&#20505;&#36873;&#21709;&#24212;&#12290;&#36825;&#20010;&#31995;&#32479;&#33021;&#22815;&#25552;&#39640;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#24182;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#26469;&#26356;&#22909;&#22320;&#24314;&#27169;&#29992;&#25143;&#20559;&#22909;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01135</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Multi-Agent Conversational Recommender System
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#65288;MACRS&#65289;&#65292;&#23427;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#34892;&#21160;&#35268;&#21010;&#26694;&#26550;&#26469;&#25511;&#21046;&#23545;&#35805;&#27969;&#31243;&#65292;&#24182;&#22522;&#20110;LLM&#29983;&#25104;&#22810;&#20010;&#20505;&#36873;&#21709;&#24212;&#12290;&#36825;&#20010;&#31995;&#32479;&#33021;&#22815;&#25552;&#39640;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#24182;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#26469;&#26356;&#22909;&#22320;&#24314;&#27169;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#19982;&#29992;&#25143;&#36827;&#34892;&#27969;&#30021;&#30340;&#22810;&#36718;&#23545;&#35805;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#23427;&#20204;&#26377;&#28508;&#21147;&#36827;&#19968;&#27493;&#25552;&#39640;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#65288;CRS&#65289;&#30340;&#24615;&#33021;&#12290;&#19982;LLM&#25797;&#38271;&#30340;&#26080;&#30446;&#30340;&#38386;&#32842;&#19981;&#21516;&#65292;CRS&#26377;&#19968;&#20010;&#26126;&#30830;&#30340;&#30446;&#26631;&#12290;&#22240;&#27492;&#65292;&#24517;&#39035;&#25511;&#21046;LLM&#20013;&#30340;&#23545;&#35805;&#27969;&#31243;&#65292;&#20197;&#25104;&#21151;&#21521;&#29992;&#25143;&#25512;&#33616;&#36866;&#24403;&#30340;&#29289;&#21697;&#12290;&#27492;&#22806;&#65292;CRS&#20013;&#30340;&#29992;&#25143;&#21453;&#39304;&#21487;&#20197;&#24110;&#21161;&#31995;&#32479;&#26356;&#22909;&#22320;&#24314;&#27169;&#29992;&#25143;&#20559;&#22909;&#65292;&#20294;&#29616;&#26377;&#30740;&#31350;&#24573;&#35270;&#20102;&#36825;&#19968;&#28857;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#25552;&#31034;LLM&#36827;&#34892;&#23545;&#35805;&#25512;&#33616;&#26080;&#27861;&#35299;&#20915;&#19978;&#36848;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#20004;&#20010;&#20851;&#38190;&#27169;&#22359;&#30340;&#22810;&#26234;&#33021;&#20307;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#65288;MACRS&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#34892;&#21160;&#35268;&#21010;&#26694;&#26550;&#65292;&#21487;&#20197;&#22522;&#20110;&#22235;&#20010;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#20307;&#25511;&#21046;&#23545;&#35805;&#27969;&#31243;&#12290;&#36825;&#20010;&#21512;&#20316;&#30340;&#22810;&#26234;&#33021;&#20307;&#26694;&#26550;&#23558;&#22522;&#20110;&#19981;&#21516;&#30340;&#26041;&#26696;&#29983;&#25104;&#21508;&#31181;&#20505;&#36873;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to strong capabilities in conducting fluent, multi-turn conversations with users, Large Language Models (LLMs) have the potential to further improve the performance of Conversational Recommender System (CRS). Unlike the aimless chit-chat that LLM excels at, CRS has a clear target. So it is imperative to control the dialogue flow in the LLM to successfully recommend appropriate items to the users. Furthermore, user feedback in CRS can assist the system in better modeling user preferences, which has been ignored by existing studies. However, simply prompting LLM to conduct conversational recommendation cannot address the above two key challenges.   In this paper, we propose Multi-Agent Conversational Recommender System (MACRS) which contains two essential modules. First, we design a multi-agent act planning framework, which can control the dialogue flow based on four LLM-based agents. This cooperative multi-agent framework will generate various candidate responses based on different 
&lt;/p&gt;</description></item><item><title>Pok\'eLLMon&#26159;&#31532;&#19968;&#20010;&#22312;&#25112;&#26415;&#23545;&#25112;&#28216;&#25103;&#20013;&#23454;&#29616;&#20154;&#31867;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#21270;&#36523;&#20195;&#29702;&#26426;&#22120;&#20154;&#12290;&#23427;&#36890;&#36807;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#12289;&#30693;&#35782;&#22686;&#24378;&#29983;&#25104;&#21644;&#19968;&#33268;&#30340;&#34892;&#21160;&#29983;&#25104;&#30340;&#31574;&#30053;&#65292;&#23637;&#29616;&#20102;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#25112;&#26007;&#31574;&#30053;&#21644;&#21450;&#26102;&#20915;&#31574;&#65292;&#24182;&#22312;Ladder&#27604;&#36187;&#20013;&#36798;&#21040;&#20102;49%&#30340;&#32988;&#29575;&#65292;&#22312;&#36992;&#35831;&#23545;&#25112;&#20013;&#36798;&#21040;&#20102;56%&#30340;&#32988;&#29575;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01118</link><description>&lt;p&gt;
Pok\'eLLMon&#65306;&#19968;&#20010;&#29992;&#20110;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;Pok\'emon&#23545;&#25112;&#30340;&#19982;&#20154;&#31867;&#33021;&#21147;&#30456;&#24403;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Pok\'eLLMon: A Human-Parity Agent for Pok\'emon Battles with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01118
&lt;/p&gt;
&lt;p&gt;
Pok\'eLLMon&#26159;&#31532;&#19968;&#20010;&#22312;&#25112;&#26415;&#23545;&#25112;&#28216;&#25103;&#20013;&#23454;&#29616;&#20154;&#31867;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#21270;&#36523;&#20195;&#29702;&#26426;&#22120;&#20154;&#12290;&#23427;&#36890;&#36807;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#12289;&#30693;&#35782;&#22686;&#24378;&#29983;&#25104;&#21644;&#19968;&#33268;&#30340;&#34892;&#21160;&#29983;&#25104;&#30340;&#31574;&#30053;&#65292;&#23637;&#29616;&#20102;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#25112;&#26007;&#31574;&#30053;&#21644;&#21450;&#26102;&#20915;&#31574;&#65292;&#24182;&#22312;Ladder&#27604;&#36187;&#20013;&#36798;&#21040;&#20102;49%&#30340;&#32988;&#29575;&#65292;&#22312;&#36992;&#35831;&#23545;&#25112;&#20013;&#36798;&#21040;&#20102;56%&#30340;&#32988;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;\textsc{Pok\'eLLMon}&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#25112;&#26415;&#23545;&#25112;&#28216;&#25103;&#20013;&#23454;&#29616;&#20154;&#31867;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#21270;&#36523;&#20195;&#29702;&#26426;&#22120;&#20154;&#65292;&#21516;&#26102;&#20197;Pok\'emon&#23545;&#25112;&#20026;&#20363;&#36827;&#34892;&#20102;&#35777;&#26126;&#12290; \textsc{Pok\'eLLMon}&#30340;&#35774;&#35745;&#37319;&#29992;&#20102;&#19977;&#20010;&#20851;&#38190;&#31574;&#30053;&#65306;&#65288;i&#65289;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#65292;&#21363;&#21363;&#26102;&#20351;&#29992;&#20174;&#23545;&#25112;&#20013;&#33719;&#24471;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#21453;&#39304;&#26469;&#36880;&#27493;&#23436;&#21892;&#31574;&#30053;&#65307;&#65288;ii&#65289;&#30693;&#35782;&#22686;&#24378;&#29983;&#25104;&#65292;&#21363;&#26816;&#32034;&#22806;&#37096;&#30693;&#35782;&#20197;&#23545;&#25239;&#20135;&#29983;&#24187;&#35273;&#29616;&#35937;&#65292;&#24182;&#20351;&#20195;&#29702;&#26426;&#22120;&#20154;&#33021;&#22815;&#21450;&#26102;&#27491;&#30830;&#22320;&#34892;&#21160;&#65307;&#65288;iii&#65289;&#19968;&#33268;&#30340;&#34892;&#21160;&#29983;&#25104;&#65292;&#20197;&#20943;&#36731;&#20195;&#29702;&#26426;&#22120;&#20154;&#38754;&#23545;&#24378;&#25932;&#26102;&#30340;&#8220;&#24778;&#24908;&#25442;&#25163;&#8221;&#29616;&#35937;&#65292;&#20351;&#20854;&#21487;&#20197;&#36867;&#36991;&#25112;&#26007;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#20154;&#31867;&#36827;&#34892;&#30340;&#22312;&#32447;&#23545;&#25112;&#20013;&#65292;\textsc{Pok\'eLLMon}&#37319;&#29992;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#25112;&#26007;&#31574;&#30053;&#21644;&#21450;&#26102;&#20915;&#31574;&#65292;&#20854;&#22312;Ladder&#27604;&#36187;&#20013;&#36798;&#21040;&#20102;49%&#30340;&#32988;&#29575;&#65292;&#22312;&#36992;&#35831;&#23545;&#25112;&#20013;&#36798;&#21040;&#20102;56%&#30340;&#32988;&#29575;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#21644;&#21487;&#29609;&#30340;&#25112;&#26007;&#26085;&#24535;&#21487;&#20197;&#22312;&#20197;&#19979;&#38142;&#25509;&#20013;&#25214;&#21040;&#65306;\url{https://gith
&lt;/p&gt;
&lt;p&gt;
We introduce \textsc{Pok\'eLLMon}, the first LLM-embodied agent that achieves human-parity performance in tactical battle games, as demonstrated in Pok\'emon battles. The design of \textsc{Pok\'eLLMon} incorporates three key strategies: (i) In-context reinforcement learning that instantly consumes text-based feedback derived from battles to iteratively refine the policy; (ii) Knowledge-augmented generation that retrieves external knowledge to counteract hallucination and enables the agent to act timely and properly; (iii) Consistent action generation to mitigate the \textit{panic switching} phenomenon when the agent faces a powerful opponent and wants to elude the battle. We show that online battles against human demonstrates \textsc{Pok\'eLLMon}'s human-like battle strategies and just-in-time decision making, achieving 49\% of win rate in the Ladder competitions and 56\% of win rate in the invited battles. Our implementation and playable battle logs are available at: \url{https://gith
&lt;/p&gt;</description></item><item><title>DTS-SQL&#37319;&#29992;&#20102;&#19968;&#31181;&#20998;&#35299;&#24335;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#32553;&#23567;&#20102;&#24320;&#28304;&#27169;&#22411;&#19982;&#19987;&#26377;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#30340;&#25191;&#34892;&#20934;&#30830;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01117</link><description>&lt;p&gt;
DTS-SQL: &#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#35299;&#24335;&#25991;&#26412;&#21040;SQL
&lt;/p&gt;
&lt;p&gt;
DTS-SQL: Decomposed Text-to-SQL with Small Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01117
&lt;/p&gt;
&lt;p&gt;
DTS-SQL&#37319;&#29992;&#20102;&#19968;&#31181;&#20998;&#35299;&#24335;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#32553;&#23567;&#20102;&#24320;&#28304;&#27169;&#22411;&#19982;&#19987;&#26377;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#30340;&#25191;&#34892;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#65292;&#39046;&#20808;&#30340;&#27169;&#22411;&#20005;&#37325;&#20381;&#36182;&#20110;&#19987;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#24341;&#21457;&#20102;&#25968;&#25454;&#38544;&#31169;&#26041;&#38754;&#30340;&#25285;&#24551;&#12290;&#32553;&#23567;&#24320;&#28304;&#27169;&#22411;&#21644;&#19987;&#26377;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#23545;&#20110;&#32531;&#35299;&#36825;&#31181;&#20381;&#36182;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#24494;&#35843;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#20004;&#20010;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#22823;&#22411;&#36328;&#39046;&#22495;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#23567;&#22411;LLM&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#25191;&#34892;&#20934;&#30830;&#24615;&#65292;&#25552;&#21319;&#20102;3&#21040;7&#20010;&#30334;&#20998;&#28857;&#65292;&#26377;&#25928;&#22320;&#20351;&#24320;&#28304;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#19987;&#26377;&#27169;&#22411;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leading models for the text-to-SQL task heavily rely on proprietary Large Language Models (LLMs), posing concerns over data privacy. Closing the performance gap between small open-source models and large proprietary models is crucial to mitigate this reliance. To this end, we introduce a novel two-stage fine-tuning approach that decomposes the task into two simpler tasks. Through comprehensive evaluation on two large cross-domain datasets and two small LLMs, we show that this approach improves execution accuracy by 3 to 7 percent, effectively aligning the performance of open-source models with their proprietary counterparts.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25991;&#26412;&#34920;&#31034;&#30340;&#26041;&#24335;&#23545;&#24515;&#20869;&#30005;&#22270;&#36827;&#34892;&#25554;&#20540;&#21644;&#25151;&#39076;&#20998;&#31867;&#12290;&#30456;&#27604;&#20854;&#20182;&#34920;&#31034;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25151;&#39076;&#20998;&#31867;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01115</link><description>&lt;p&gt;
&#36890;&#36807;&#25991;&#26412;&#34920;&#31034;&#35299;&#35835;&#24515;&#20869;&#30005;&#22270;
&lt;/p&gt;
&lt;p&gt;
Interpretation of Intracardiac Electrograms Through Textual Representations
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25991;&#26412;&#34920;&#31034;&#30340;&#26041;&#24335;&#23545;&#24515;&#20869;&#30005;&#22270;&#36827;&#34892;&#25554;&#20540;&#21644;&#25151;&#39076;&#20998;&#31867;&#12290;&#30456;&#27604;&#20854;&#20182;&#34920;&#31034;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25151;&#39076;&#20998;&#31867;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#25151;&#39076;(AFib)&#30340;&#19981;&#35268;&#21017;&#30005;&#27963;&#21160;&#19968;&#30452;&#26159;&#24515;&#30005;&#22270;&#23398;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#23545;&#20110;&#20005;&#37325;&#30340;&#25151;&#39076;&#30149;&#20363;&#65292;&#36827;&#34892;&#23548;&#31649;&#28040;&#34701;&#20197;&#33719;&#21462;&#24515;&#20869;&#30005;&#22270;(EGMs)&#12290;EGMs&#25552;&#20379;&#20102;&#24515;&#33039;&#30005;&#27963;&#21160;&#30340;&#22797;&#26434;&#32454;&#33410;&#21644;&#23616;&#37096;&#21270;&#20449;&#24687;&#65292;&#26159;&#21487;&#35299;&#37322;&#30340;&#24515;&#33039;&#30740;&#31350;&#30340;&#29702;&#24819;&#27169;&#24335;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;(AI)&#30340;&#36827;&#23637;&#20351;&#24471;&#19968;&#20123;&#30740;&#31350;&#21487;&#20197;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#37322;&#25151;&#39076;&#20013;&#30340;EGMs&#12290;&#27492;&#22806;&#65292;&#35821;&#35328;&#27169;&#22411;(LMs)&#22312;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#21307;&#30103;&#39046;&#22495;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LMs&#26469;&#36890;&#36807;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#23545;EGM&#25554;&#20540;&#21644;&#25151;&#39076;&#20998;&#31867;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#23558;EGM&#24418;&#24335;&#21270;&#20026;&#25991;&#26412;&#24207;&#21015;&#65292;&#24182;&#19982;&#20854;&#20182;&#34920;&#31034;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#25151;&#39076;&#20998;&#31867;&#26041;&#38754;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the irregular electrical activity of atrial fibrillation (AFib) has been a key challenge in electrocardiography. For serious cases of AFib, catheter ablations are performed to collect intracardiac electrograms (EGMs). EGMs offer intricately detailed and localized electrical activity of the heart and are an ideal modality for interpretable cardiac studies. Recent advancements in artificial intelligence (AI) has allowed some works to utilize deep learning frameworks to interpret EGMs during AFib. Additionally, language models (LMs) have shown exceptional performance in being able to generalize to unseen domains, especially in healthcare. In this study, we are the first to leverage pretrained LMs for finetuning of EGM interpolation and AFib classification via masked language modeling. We formulate the EGM as a textual sequence and present competitive performances on AFib classification compared against other representations. Lastly, we provide a comprehensive interpretabilit
&lt;/p&gt;</description></item><item><title>&#30123;&#33495;&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24178;&#25200;&#24863;&#30693;&#23545;&#40784;&#25216;&#26415;&#65292;&#36890;&#36807;&#36880;&#28176;&#28155;&#21152;&#25200;&#21160;&#20135;&#29983;&#19981;&#21464;&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#25552;&#39640;&#23545;&#25239;&#26377;&#23475;&#25552;&#31034;&#24341;&#36215;&#30340;&#23884;&#20837;&#28418;&#31227;&#30340;&#23545;&#40784;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#33391;&#24615;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01109</link><description>&lt;p&gt;
&#30123;&#33495;&#65306;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24178;&#25200;&#24863;&#30693;&#23545;&#40784;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Vaccine: Perturbation-aware Alignment for Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01109
&lt;/p&gt;
&lt;p&gt;
&#30123;&#33495;&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24178;&#25200;&#24863;&#30693;&#23545;&#40784;&#25216;&#26415;&#65292;&#36890;&#36807;&#36880;&#28176;&#28155;&#21152;&#25200;&#21160;&#20135;&#29983;&#19981;&#21464;&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#25552;&#39640;&#23545;&#25239;&#26377;&#23475;&#25552;&#31034;&#24341;&#36215;&#30340;&#23884;&#20837;&#28418;&#31227;&#30340;&#23545;&#40784;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#33391;&#24615;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24494;&#35843;&#21363;&#26381;&#21153;&#33539; paradigm&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#20026;&#29992;&#25143;&#19978;&#20256;&#30340;&#19968;&#23567;&#37096;&#20998;&#26377;&#23475;&#25968;&#25454;&#25552;&#20379;&#20102;&#26032;&#30340;&#25915;&#20987;&#38754;&#65292;&#36825;&#20123;&#25968;&#25454;&#24456;&#23481;&#26131;&#27450;&#39575;&#24494;&#35843;&#36807;&#31243;&#20174;&#32780;&#20135;&#29983;&#23545;&#40784;&#22833;&#25928;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#21487;&#33021;&#23548;&#33268;&#23545;&#40784;&#22833;&#25928;&#30340;&#26377;&#23475;&#23884;&#20837;&#28418;&#31227;&#29616;&#35937;&#12290;&#21463;&#21040;&#25105;&#20204;&#30340;&#21457;&#29616;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30123;&#33495; (Vaccine) &#65292;&#19968;&#31181;&#38024;&#23545;&#24178;&#25200;&#24863;&#30693;&#30340;&#23545;&#40784;&#25216;&#26415;&#65292;&#20197;&#20943;&#36731;&#29992;&#25143;&#24494;&#35843;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#30123;&#33495;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#22312;&#23545;&#40784;&#38454;&#27573;&#36880;&#28176;&#28155;&#21152;&#31934;&#24515;&#35774;&#35745;&#30340;&#25200;&#21160;&#65292;&#20135;&#29983;&#19981;&#21464;&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#20174;&#32780;&#20351;&#23884;&#20837;&#33021;&#22815;&#25269;&#24481;&#26469;&#33258;&#26410;&#32463;&#28040;&#27602;&#30340;&#29992;&#25143;&#25968;&#25454;&#30340;&#26377;&#23475;&#25200;&#21160;&#12290;&#25105;&#20204;&#22312;&#24320;&#28304;&#20027;&#27969;LLM&#65288;&#22914;Llama2&#65292;Opt&#65292;Vicuna&#65289;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30123;&#33495;&#33021;&#22815;&#25552;&#39640;&#23545;&#25239;&#26377;&#23475;&#25552;&#31034;&#24341;&#36215;&#30340;&#23884;&#20837;&#28418;&#31227;&#30340;&#23545;&#40784;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#33391;&#24615;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The new paradigm of finetuning-as-a-service introduces a new attack surface for Large Language Models (LLMs): a few harmful data uploaded by users can easily trick the finetuning to produce an alignment-broken model. We conduct an empirical analysis and uncover a \textit{harmful embedding drift} phenomenon, showing a probable cause of the alignment-broken effect. Inspired by our findings, we propose Vaccine, a perturbation-aware alignment technique to mitigate the security risk of users finetuning. The core idea of Vaccine is to produce invariant hidden embeddings by progressively adding crafted perturbation to them in the alignment phase. This enables the embeddings to withstand harmful perturbation from un-sanitized user data in the finetuning phase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna) demonstrate that Vaccine can boost the robustness of alignment against harmful prompts induced embedding drift while reserving reasoning ability towards benign prompt
&lt;/p&gt;</description></item><item><title>&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#38754;&#20020;&#30528;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38480;&#21046;&#21644;&#25361;&#25112;&#65292;&#38656;&#35201;&#24341;&#20837;&#25512;&#29702;&#33021;&#21147;&#20316;&#20026;&#32479;&#19968;&#30340;&#26631;&#20934;&#26469;&#23454;&#29616;&#31995;&#32479;&#30340;&#25972;&#21512;&#21644;&#20248;&#21270;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01108</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#65306;&#23616;&#38480;&#24615;&#12289;&#25361;&#25112;&#21644;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Reasoning Capacity in Multi-Agent Systems: Limitations, Challenges and Human-Centered Solutions
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01108
&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#38754;&#20020;&#30528;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38480;&#21046;&#21644;&#25361;&#25112;&#65292;&#38656;&#35201;&#24341;&#20837;&#25512;&#29702;&#33021;&#21147;&#20316;&#20026;&#32479;&#19968;&#30340;&#26631;&#20934;&#26469;&#23454;&#29616;&#31995;&#32479;&#30340;&#25972;&#21512;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20026;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#21033;&#29992;&#23427;&#20204;&#24102;&#26469;&#20102;&#35768;&#22810;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#20026;&#20102;&#23454;&#29616;LLMs&#30340;&#23454;&#38469;&#37319;&#29992;&#65292;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#22312;&#20225;&#19994;&#24179;&#21488;&#20013;&#20855;&#26377;&#22686;&#24378;&#12289;&#25972;&#21512;&#21644;&#21327;&#35843;LLMs&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#35813;&#24179;&#21488;&#21033;&#29992;&#29616;&#26377;&#19987;&#26377;&#25968;&#25454;&#21644;&#27169;&#22411;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#29616;&#23454;&#20219;&#21153;&#12290;&#23613;&#31649;&#36825;&#20123;&#31995;&#32479;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#24403;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#29421;&#31364;&#12289;&#21333;&#19968;&#30446;&#26631;&#30340;&#20248;&#21270;&#21644;&#35780;&#20272;&#65292;&#24448;&#24448;&#24573;&#35270;&#29616;&#23454;&#24773;&#26223;&#20013;&#30340;&#28508;&#22312;&#32422;&#26463;&#65292;&#21253;&#25324;&#26377;&#38480;&#30340;&#39044;&#31639;&#12289;&#36164;&#28304;&#21644;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#35299;&#37322;&#12289;&#20998;&#26512;&#21644;&#35843;&#35797;&#36825;&#20123;&#31995;&#32479;&#35201;&#27714;&#19981;&#21516;&#30340;&#32452;&#20214;&#20043;&#38388;&#36827;&#34892;&#30456;&#20114;&#35780;&#20272;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#36825;&#31181;&#38656;&#27714;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25512;&#29702;&#33021;&#21147;&#30340;&#27010;&#24565;&#20316;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#26631;&#20934;&#65292;&#20197;&#23454;&#29616;&#38598;&#25104;&#21644;&#20248;&#21270;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Remarkable performance of large language models (LLMs) in a variety of tasks brings forth many opportunities as well as challenges of utilizing them in production settings. Towards practical adoption of LLMs, multi-agent systems hold great promise to augment, integrate, and orchestrate LLMs in the larger context of enterprise platforms that use existing proprietary data and models to tackle complex real-world tasks. Despite the tremendous success of these systems, current approaches rely on narrow, single-focus objectives for optimization and evaluation, often overlooking potential constraints in real-world scenarios, including restricted budgets, resources and time. Furthermore, interpreting, analyzing, and debugging these systems requires different components to be evaluated in relation to one another. This demand is currently not feasible with existing methodologies. In this postion paper, we introduce the concept of reasoning capacity as a unifying criterion to enable integration o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#35848;&#21028;&#23545;&#35805;&#31995;&#32479;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#21253;&#25324;&#22522;&#20934;&#12289;&#35780;&#20272;&#21644;&#26041;&#27861;&#35770;&#12290;&#36824;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#65292;&#22914;&#22810;&#27169;&#24577;&#12289;&#22810;&#26041;&#21644;&#36328;&#25991;&#21270;&#30340;&#35848;&#21028;&#22330;&#26223;&#12290;&#36825;&#20026;&#31038;&#32676;&#25552;&#20379;&#20102;&#20851;&#20110;&#35848;&#21028;&#23545;&#35805;&#31995;&#32479;&#30340;&#31995;&#32479;&#32508;&#36848;&#65292;&#24182;&#28608;&#21457;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01097</link><description>&lt;p&gt;
&#35753;&#25105;&#20204;&#26469;&#35848;&#21028;&#65281;&#35848;&#21028;&#23545;&#35805;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Let's Negotiate! A Survey of Negotiation Dialogue Systems
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01097
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#35848;&#21028;&#23545;&#35805;&#31995;&#32479;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#21253;&#25324;&#22522;&#20934;&#12289;&#35780;&#20272;&#21644;&#26041;&#27861;&#35770;&#12290;&#36824;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#65292;&#22914;&#22810;&#27169;&#24577;&#12289;&#22810;&#26041;&#21644;&#36328;&#25991;&#21270;&#30340;&#35848;&#21028;&#22330;&#26223;&#12290;&#36825;&#20026;&#31038;&#32676;&#25552;&#20379;&#20102;&#20851;&#20110;&#35848;&#21028;&#23545;&#35805;&#31995;&#32479;&#30340;&#31995;&#32479;&#32508;&#36848;&#65292;&#24182;&#28608;&#21457;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35848;&#21028;&#26159;&#20154;&#31867;&#27807;&#36890;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#33021;&#21147;&#12290;&#36817;&#24180;&#26469;&#65292;&#35848;&#21028;&#23545;&#35805;&#31995;&#32479;&#30340;&#30740;&#31350;&#20852;&#36259;&#20877;&#24230;&#19978;&#21319;&#65292;&#20854;&#30446;&#26631;&#26159;&#21019;&#24314;&#26234;&#33021;&#20195;&#29702;&#65292;&#24110;&#21161;&#20154;&#20204;&#35299;&#20915;&#20914;&#31361;&#25110;&#36798;&#25104;&#21327;&#35758;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#24456;&#22810;&#25506;&#32034;&#35848;&#21028;&#23545;&#35805;&#31995;&#32479;&#30340;&#30740;&#31350;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#36824;&#27809;&#26377;&#36827;&#34892;&#31995;&#32479;&#32508;&#36848;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#35843;&#26597;&#26368;&#36817;&#22312;&#35848;&#21028;&#23545;&#35805;&#31995;&#32479;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#24182;&#28085;&#30422;&#25991;&#29486;&#20013;&#30340;&#22522;&#20934;&#12289;&#35780;&#20272;&#21644;&#26041;&#27861;&#35770;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#12289;&#22810;&#26041;&#21644;&#36328;&#25991;&#21270;&#30340;&#35848;&#21028;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#31038;&#32676;&#25552;&#20379;&#20851;&#20110;&#35848;&#21028;&#23545;&#35805;&#31995;&#32479;&#30340;&#31995;&#32479;&#32508;&#36848;&#65292;&#24182;&#28608;&#21457;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Negotiation is a crucial ability in human communication. Recently, there has been a resurgent research interest in negotiation dialogue systems, whose goal is to create intelligent agents that can assist people in resolving conflicts or reaching agreements. Although there have been many explorations into negotiation dialogue systems, a systematic review of this task has not been performed to date. We aim to fill this gap by investigating recent studies in the field of negotiation dialogue systems, and covering benchmarks, evaluations and methodologies within the literature. We also discuss potential future directions, including multi-modal, multi-party and cross-cultural negotiation scenarios. Our goal is to provide the community with a systematic overview of negotiation dialogue systems and to inspire future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#38480;&#39046;&#22495;&#25968;&#25454;&#36827;&#34892;&#24265;&#20215;&#25512;&#29702;&#30340;&#19987;&#29992;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#25512;&#29702;&#25104;&#26412;&#30340;&#38480;&#21046;&#19979;&#25214;&#21040;&#20102;&#27604;&#35757;&#32451;&#38750;&#24120;&#22823;&#30340;&#22522;&#26412;&#36716;&#25442;&#22120;&#27169;&#22411;&#26356;&#20248;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#39044;&#31639;&#19979;&#65292;&#36229;&#32593;&#32476;&#21644;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#26356;&#22909;&#65292;&#32780;&#22312;&#22823;&#22411;&#19987;&#29992;&#39044;&#31639;&#19979;&#65292;&#35757;&#32451;&#37325;&#35201;&#26679;&#26412;&#25968;&#25454;&#38598;&#19978;&#30340;&#23567;&#22411;&#27169;&#22411;&#26356;&#20855;&#21560;&#24341;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01093</link><description>&lt;p&gt;
&#20351;&#29992;&#26377;&#38480;&#39046;&#22495;&#25968;&#25454;&#36827;&#34892;&#24265;&#20215;&#25512;&#29702;&#30340;&#19987;&#29992;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Specialized Language Models with Cheap Inference from Limited Domain Data
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#38480;&#39046;&#22495;&#25968;&#25454;&#36827;&#34892;&#24265;&#20215;&#25512;&#29702;&#30340;&#19987;&#29992;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#25512;&#29702;&#25104;&#26412;&#30340;&#38480;&#21046;&#19979;&#25214;&#21040;&#20102;&#27604;&#35757;&#32451;&#38750;&#24120;&#22823;&#30340;&#22522;&#26412;&#36716;&#25442;&#22120;&#27169;&#22411;&#26356;&#20248;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#39044;&#31639;&#19979;&#65292;&#36229;&#32593;&#32476;&#21644;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#26356;&#22909;&#65292;&#32780;&#22312;&#22823;&#22411;&#19987;&#29992;&#39044;&#31639;&#19979;&#65292;&#35757;&#32451;&#37325;&#35201;&#26679;&#26412;&#25968;&#25454;&#38598;&#19978;&#30340;&#23567;&#22411;&#27169;&#22411;&#26356;&#20855;&#21560;&#24341;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#19968;&#31181;&#22810;&#25165;&#22810;&#33402;&#30340;&#24037;&#20855;&#65292;&#20294;&#22312;&#32570;&#20047;&#22823;&#35268;&#27169;&#25512;&#29702;&#39044;&#31639;&#21644;&#22823;&#35268;&#27169;&#39046;&#22495;&#20869;&#35757;&#32451;&#38598;&#30340;&#20219;&#21153;&#20013;&#24212;&#29992;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#23545;&#36825;&#20123;&#38480;&#21046;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#65292;&#24182;&#21306;&#20998;&#20102;&#22235;&#20010;&#37325;&#35201;&#30340;&#21464;&#37327;&#65306;&#39044;&#35757;&#32451;&#39044;&#31639;&#65288;&#29992;&#20110;&#22312;&#30446;&#26631;&#39046;&#22495;&#20986;&#29616;&#20043;&#21069;&#36827;&#34892;&#35757;&#32451;&#65289;&#65292;&#19987;&#29992;&#39044;&#31639;&#65288;&#29992;&#20110;&#22312;&#30446;&#26631;&#39046;&#22495;&#20986;&#29616;&#20043;&#21518;&#36827;&#34892;&#35757;&#32451;&#65289;&#65292;&#25512;&#29702;&#39044;&#31639;&#21644;&#39046;&#22495;&#20869;&#35757;&#32451;&#38598;&#22823;&#23567;&#12290;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;&#21463;&#21040;&#25512;&#29702;&#25104;&#26412;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27604;&#35757;&#32451;&#38750;&#24120;&#22823;&#30340;&#22522;&#26412;&#36716;&#25442;&#22120;&#27169;&#22411;&#30340;&#26631;&#20934;&#20570;&#27861;&#26356;&#22909;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36229;&#32593;&#32476;&#21644;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#39044;&#31639;&#19979;&#20855;&#26377;&#26356;&#22909;&#30340;&#22256;&#24785;&#24230;&#65292;&#32780;&#22312;&#22823;&#22411;&#19987;&#29992;&#39044;&#31639;&#19979;&#65292;&#35757;&#32451;&#37325;&#35201;&#26679;&#26412;&#25968;&#25454;&#38598;&#19978;&#30340;&#23567;&#22411;&#27169;&#22411;&#26356;&#20855;&#21560;&#24341;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have emerged as a versatile tool but are challenging to apply to tasks lacking large inference budgets and large in-domain training sets. This work formalizes these constraints and distinguishes four important variables: the pretraining budget (for training before the target domain is known), the specialization budget (for training after the target domain is known), the inference budget, and the in-domain training set size. Across these settings, we compare different approaches from the machine learning literature. Limited by inference cost, we find better alternatives to the standard practice of training very large vanilla transformer models. In particular, we show that hyper-networks and mixture of experts have better perplexity for large pretraining budgets, while small models trained on importance sampled datasets are attractive for large specialization budgets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Twitter&#19978;&#20851;&#20110;2020&#24180;&#32654;&#22269;&#36873;&#20030;&#30340;&#35752;&#35770;&#65292;&#36890;&#36807;&#35782;&#21035;&#30456;&#20114;&#20316;&#29992;&#30340;&#31038;&#21306;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#20449;&#24687;&#20256;&#36882;&#30340;&#26032;&#26041;&#27861;&#65292;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#26469;&#25506;&#32034;&#36825;&#20123;&#31038;&#21306;&#30340;&#24494;&#22937;&#24847;&#35782;&#24418;&#24577;&#12290;&#36890;&#36807;&#19982;&#30495;&#23454;&#35843;&#26597;&#32467;&#26524;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#27604;&#29616;&#26377;&#22522;&#20934;&#32447;&#26356;&#39640;&#30340;&#19968;&#33268;&#24615;&#65292;&#31361;&#26174;&#20102;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#25581;&#31034;&#26434;&#21512;&#24847;&#35782;&#24418;&#24577;&#31038;&#21306;&#20013;&#22797;&#26434;&#24847;&#35782;&#24418;&#24577;&#30340;&#28508;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01091</link><description>&lt;p&gt;
&#38405;&#35835;&#25512;&#25991;&#20043;&#38388;&#65306;&#35299;&#35835;&#30456;&#20114;&#32852;&#31995;&#30340;&#26434;&#21512;&#24847;&#35782;&#24418;&#24577;&#31038;&#21306;&#30340;&#31435;&#22330;
&lt;/p&gt;
&lt;p&gt;
Reading Between the Tweets: Deciphering Ideological Stances of Interconnected Mixed-Ideology Communities
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Twitter&#19978;&#20851;&#20110;2020&#24180;&#32654;&#22269;&#36873;&#20030;&#30340;&#35752;&#35770;&#65292;&#36890;&#36807;&#35782;&#21035;&#30456;&#20114;&#20316;&#29992;&#30340;&#31038;&#21306;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#20449;&#24687;&#20256;&#36882;&#30340;&#26032;&#26041;&#27861;&#65292;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#26469;&#25506;&#32034;&#36825;&#20123;&#31038;&#21306;&#30340;&#24494;&#22937;&#24847;&#35782;&#24418;&#24577;&#12290;&#36890;&#36807;&#19982;&#30495;&#23454;&#35843;&#26597;&#32467;&#26524;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#27604;&#29616;&#26377;&#22522;&#20934;&#32447;&#26356;&#39640;&#30340;&#19968;&#33268;&#24615;&#65292;&#31361;&#26174;&#20102;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#25581;&#31034;&#26434;&#21512;&#24847;&#35782;&#24418;&#24577;&#31038;&#21306;&#20013;&#22797;&#26434;&#24847;&#35782;&#24418;&#24577;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#21457;&#23637;&#25552;&#39640;&#20102;&#25105;&#20204;&#29702;&#35299;&#22312;&#32447;&#31038;&#21306;&#30340;&#24494;&#22937;&#19990;&#30028;&#35266;&#30340;&#33021;&#21147;&#12290;&#29616;&#26377;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#25506;&#32034;&#24847;&#35782;&#24418;&#24577;&#31435;&#22330;&#26102;&#23558;&#33258;&#30001;&#27966;&#21644;&#20445;&#23432;&#27966;&#35270;&#20026;&#19981;&#21516;&#30340;&#32676;&#20307;&#12290;&#28982;&#32780;&#65292;&#36825;&#26410;&#33021;&#32771;&#34385;&#21040;&#26377;&#26426;&#24418;&#25104;&#30340;&#22312;&#32447;&#31038;&#21306;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#32852;&#31995;&#30340;&#24494;&#22937;&#35266;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20851;&#20110;2020&#24180;&#32654;&#22269;&#36873;&#20030;&#30340;Twitter&#35752;&#35770;&#65292;&#20197;&#35782;&#21035;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#31038;&#21306;&#12290;&#20805;&#20998;&#21033;&#29992;&#36825;&#31181;&#30456;&#20114;&#20851;&#31995;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#22312;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26102;&#21033;&#29992;&#20449;&#24687;&#20256;&#36882;&#26469;&#25506;&#32034;&#36825;&#20123;&#31038;&#21306;&#30340;&#24494;&#22937;&#24847;&#35782;&#24418;&#24577;&#12290;&#36890;&#36807;&#27604;&#36739;&#30001;LMs&#29983;&#25104;&#30340;&#22238;&#22797;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#35843;&#26597;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#27604;&#29616;&#26377;&#22522;&#20934;&#32447;&#26356;&#39640;&#30340;&#19968;&#33268;&#24615;&#65292;&#31361;&#26174;&#20102;&#22312;&#21644;&#36328;&#30456;&#20114;&#32852;&#31995;&#30340;&#26434;&#21512;&#24847;&#35782;&#24418;&#24577;&#31038;&#21306;&#20013;&#21033;&#29992;LMs&#25581;&#31034;&#22797;&#26434;&#24847;&#35782;&#24418;&#24577;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in NLP have improved our ability to understand the nuanced worldviews of online communities. Existing research focused on probing ideological stances treats liberals and conservatives as separate groups. However, this fails to account for the nuanced views of the organically formed online communities and the connections between them. In this paper, we study discussions of the 2020 U.S. election on Twitter to identify complex interacting communities. Capitalizing on this interconnectedness, we introduce a novel approach that harnesses message passing when finetuning language models (LMs) to probe the nuanced ideologies of these communities. By comparing the responses generated by LMs and real-world survey results, our method shows higher alignment than existing baselines, highlighting the potential of using LMs in revealing complex ideologies within and across interconnected mixed-ideology communities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#21407;&#22987;&#35821;&#35328;&#30340;&#19978;&#19979;&#25991;&#12289;&#38382;&#39064;&#21644;&#31572;&#26696;&#32763;&#35793;&#25104;&#39640;&#36164;&#28304;&#35821;&#35328;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#25928;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01065</link><description>&lt;p&gt;
&#29992;&#20110;&#22810;&#35821;&#35328;&#25991;&#26723;&#38382;&#31572;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Evaluation Methodology for Large Language Models for Multilingual Document Question and Answer
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#21407;&#22987;&#35821;&#35328;&#30340;&#19978;&#19979;&#25991;&#12289;&#38382;&#39064;&#21644;&#31572;&#26696;&#32763;&#35793;&#25104;&#39640;&#36164;&#28304;&#35821;&#35328;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;&#23558;&#21407;&#22987;&#35821;&#35328;&#30340;&#19978;&#19979;&#25991;&#12289;&#38382;&#39064;&#21644;&#31572;&#26696;&#32763;&#35793;&#25104;&#39640;&#36164;&#28304;&#35821;&#35328;&#20135;&#29983;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the widespread adoption of Large Language Models (LLMs), in this paper we investigate the multilingual capability of these models. Our preliminary results show that, translating the native language context, question and answer into a high resource language produced the best results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21452;&#37325;&#30446;&#26631;&#23545;&#35805;&#35774;&#32622;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#35745;&#21010;&#39537;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#20219;&#24847;&#35745;&#21010;&#19978;&#22522;&#30784;&#23545;&#35805;&#65292;&#20027;&#21160;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#35745;&#21010;&#65292;&#24182;&#22312;&#31995;&#32479;&#34892;&#20026;&#19978;&#23454;&#26045;&#23433;&#20840;&#38450;&#25252;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01053</link><description>&lt;p&gt;
&#38754;&#21521;&#21452;&#37325;&#30446;&#26631;&#23545;&#35805;&#35774;&#32622;&#30340;&#35745;&#21010;&#39537;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Plan-Grounded Large Language Models for Dual Goal Conversational Settings
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21452;&#37325;&#30446;&#26631;&#23545;&#35805;&#35774;&#32622;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#35745;&#21010;&#39537;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#20219;&#24847;&#35745;&#21010;&#19978;&#22522;&#30784;&#23545;&#35805;&#65292;&#20027;&#21160;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#35745;&#21010;&#65292;&#24182;&#22312;&#31995;&#32479;&#34892;&#20026;&#19978;&#23454;&#26045;&#23433;&#20840;&#38450;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36981;&#24490;&#29992;&#25143;&#25351;&#20196;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20026;LLM&#25552;&#20379;&#20805;&#36275;&#30340;&#33021;&#21147;&#20197;&#27969;&#21033;&#22320;&#36827;&#34892;&#23545;&#35805;&#24182;&#19982;&#20154;&#31867;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#22312;&#21452;&#21521;&#23545;&#35805;&#27969;&#21160;&#25351;&#20196;&#30340;&#28151;&#21512;&#20513;&#35758;&#35774;&#32622;&#20013;&#65292;LLM&#22914;&#20309;&#24341;&#23548;&#20197;&#35745;&#21010;&#20026;&#22522;&#30784;&#30340;&#23545;&#35805;&#36824;&#19981;&#23436;&#20840;&#28165;&#26970;&#65292;&#21363;LLM&#21644;&#29992;&#25143;&#24444;&#27492;&#25552;&#20379;&#25351;&#20196;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#21452;&#37325;&#30446;&#26631;&#28151;&#21512;&#20513;&#35758;&#23545;&#35805;&#35774;&#32622;&#30340;&#38382;&#39064;&#65292;LLM&#19981;&#20165;&#22312;&#20219;&#24847;&#35745;&#21010;&#19978;&#22522;&#30784;&#23545;&#35805;&#65292;&#36824;&#33268;&#21147;&#20110;&#28385;&#36275;&#27969;&#31243;&#35745;&#21010;&#21644;&#29992;&#25143;&#25351;&#20196;&#12290;LLM&#36127;&#36131;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#35745;&#21010;&#65292;&#21516;&#26102;&#36866;&#24212;&#26032;&#30340;&#24773;&#20917;&#65292;&#22238;&#31572;&#38382;&#39064;&#65292;&#24182;&#22312;&#38656;&#35201;&#26102;&#28608;&#27963;&#23433;&#20840;&#38450;&#25252;&#25514;&#26045;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#65292;&#23427;&#22522;&#20110;&#27969;&#31243;&#35745;&#21010;&#26469;&#36827;&#34892;&#23545;&#35805;&#65292;&#21487;&#20197;&#20027;&#21160;&#21442;&#19982;&#23545;&#35805;&#65292;&#24182;&#22312;&#31995;&#32479;&#34892;&#20026;&#19978;&#23454;&#26045;&#23433;&#20840;&#38450;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training Large Language Models (LLMs) to follow user instructions has been shown to supply the LLM with ample capacity to converse fluently while being aligned with humans. Yet, it is not completely clear how an LLM can lead a plan-grounded conversation in mixed-initiative settings where instructions flow in both directions of the conversation, i.e. both the LLM and the user provide instructions to one another. In this paper, we tackle a dual goal mixed-initiative conversational setting where the LLM not only grounds the conversation on an arbitrary plan but also seeks to satisfy both a procedural plan and user instructions. The LLM is then responsible for guiding the user through the plan and, at the same time, adapting to new circumstances, answering questions, and activating safety guardrails when needed. We propose a novel LLM that grounds the dialogue on a procedural plan, can take the dialogue initiative, and enforces guardrails on the system's behavior, while also improving the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21453;&#24605;&#29983;&#25104;&#33021;&#21147;&#25552;&#28860;&#21040;&#36739;&#23567;&#27169;&#22411;&#20013;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#29983;&#25104;&#21453;&#24605;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01051</link><description>&lt;p&gt;
&#29983;&#25104;&#12289;&#25552;&#28860;&#21644;&#35780;&#20272;&#20855;&#26377;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#39118;&#26684;&#30340;&#21160;&#26426;&#24335;&#21453;&#24605;
&lt;/p&gt;
&lt;p&gt;
Generation, Distillation and Evaluation of Motivational Interviewing-Style Reflections with a Foundational Language Model
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21453;&#24605;&#29983;&#25104;&#33021;&#21147;&#25552;&#28860;&#21040;&#36739;&#23567;&#27169;&#22411;&#20013;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#29983;&#25104;&#21453;&#24605;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#39640;&#25928;&#25191;&#34892;&#65292;&#20294;&#30001;&#20110;&#20854;&#24222;&#22823;&#30340;&#20307;&#31215;&#21644;&#19987;&#26377;&#25152;&#26377;&#26435;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#24456;&#38590;&#37096;&#32626;&#12290;&#35768;&#22810;&#20154;&#20250;&#24076;&#26395;&#23558;&#22522;&#30784;&#27169;&#22411;&#30340;&#29305;&#23450;&#21151;&#33021;&#25552;&#28860;&#25104;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#20197;&#20415;&#25317;&#26377;&#21644;&#25511;&#21046;&#12290;&#22312;&#24320;&#21457;&#27835;&#30103;&#24615;&#32842;&#22825;&#26426;&#22120;&#20154;&#26102;&#65292;&#25105;&#20204;&#24076;&#26395;&#25552;&#28860;&#19968;&#31181;&#31216;&#20026;&#21453;&#24605;&#20542;&#21548;&#30340;&#33021;&#21147;&#65292;&#21363;&#27835;&#30103;&#24072;&#29983;&#25104;&#23545;&#23458;&#25143;&#35762;&#35805;&#30340;&#21453;&#24605;&#12290;&#36825;&#20123;&#21453;&#24605;&#35201;&#20040;&#37325;&#26032;&#38472;&#36848;&#23458;&#25143;&#35828;&#36807;&#30340;&#35805;&#65292;&#35201;&#20040;&#23558;&#20854;&#19982;&#30456;&#20851;&#35266;&#23519;&#12289;&#24605;&#24819;&#25110;&#29468;&#27979;&#32852;&#31995;&#36215;&#26469;&#65292;&#20197;&#40723;&#21169;&#21644;&#24341;&#23548;&#23458;&#25143;&#32487;&#32493;&#24605;&#32771;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-4&#65289;&#20013;&#25552;&#28860;&#21453;&#24605;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#20351;&#29992;&#38646;&#26679;&#26412;&#25552;&#31034;&#65292;GPT-4&#21487;&#20197;&#20197;&#23558;&#36817;100%&#30340;&#25104;&#21151;&#29575;&#29983;&#25104;&#21453;&#24605;&#65292;&#20248;&#20110;&#25152;&#26377;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#30001;GPT-4&#29983;&#25104;&#30340;&#21453;&#24605;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Large Foundational Language Models are capable of performing many tasks at a high level but are difficult to deploy in many applications because of their size and proprietary ownership. Many will be motivated to distill specific capabilities of foundational models into smaller models that can be owned and controlled. In the development of a therapeutic chatbot, we wish to distill a capability known as reflective listening, in which a therapist produces reflections of client speech. These reflections either restate what a client has said, or connect what was said to a relevant observation, idea or guess that encourages and guides the client to continue contemplation. In this paper, we present a method for distilling the generation of reflections from a Foundational Language Model (GPT-4) into smaller models. We first show that GPT-4, using zero-shot prompting, can generate reflections at near 100% success rate, superior to all previous methods. Using reflections generated by GPT-4, we f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35757;&#32451;&#19987;&#29992;&#20998;&#35789;&#22120;&#65292;&#23545;&#20998;&#35789;&#22120;&#35774;&#35745;&#36827;&#34892;&#20102;&#28040;&#27602;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#20998;&#35789;&#22120;&#30340;&#22823;&#23567;&#12289;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#35757;&#32451;&#25968;&#25454;&#23545;&#27169;&#22411;&#24615;&#33021;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#24314;&#35758;&#21644;&#20999;&#25442;&#20998;&#35789;&#22120;&#30340;&#26041;&#27861;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01035</link><description>&lt;p&gt;
&#20805;&#20998;&#21033;&#29992;&#20998;&#35789;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#39046;&#22495;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Getting the most out of your tokenizer for pre-training and domain adaptation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35757;&#32451;&#19987;&#29992;&#20998;&#35789;&#22120;&#65292;&#23545;&#20998;&#35789;&#22120;&#35774;&#35745;&#36827;&#34892;&#20102;&#28040;&#27602;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#20998;&#35789;&#22120;&#30340;&#22823;&#23567;&#12289;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#35757;&#32451;&#25968;&#25454;&#23545;&#27169;&#22411;&#24615;&#33021;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#24314;&#35758;&#21644;&#20999;&#25442;&#20998;&#35789;&#22120;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#35789;&#26159;&#29616;&#20195;LLM&#20013;&#40092;&#20026;&#20154;&#30693;&#19988;&#24120;&#34987;&#24573;&#35270;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#22823;&#22810;&#25968;&#24050;&#21457;&#34920;&#30340;&#20316;&#21697;&#22312;&#25152;&#26377;&#23454;&#39564;&#20013;&#37117;&#20351;&#29992;&#21516;&#19968;&#20010;&#20998;&#35789;&#22120;&#65292;&#36890;&#24120;&#26159;&#20174;&#21478;&#19968;&#20010;&#27169;&#22411;&#20511;&#29992;&#32780;&#26469;&#30340;&#65292;&#24182;&#27809;&#26377;&#36827;&#34892;&#28040;&#34701;&#25110;&#20998;&#26512;&#26469;&#20248;&#21270;&#20998;&#35789;&#12290;&#27492;&#22806;&#65292;&#22312;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#26102;&#65292;&#20998;&#35789;&#22120;&#36890;&#24120;&#20445;&#25345;&#19981;&#21464;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#35789;&#22120;&#30340;&#22823;&#23567;&#12289;&#39044;&#26631;&#35760;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#35757;&#32451;&#25968;&#25454;&#23545;&#27169;&#22411;&#30340;&#29983;&#25104;&#36895;&#24230;&#12289;&#26377;&#25928;&#19978;&#19979;&#25991;&#22823;&#23567;&#12289;&#20869;&#23384;&#20351;&#29992;&#21644;&#19979;&#28216;&#24615;&#33021;&#22343;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19987;&#29992;&#30340;&#23383;&#33410;&#23545;&#32534;&#30721;&#20998;&#35789;&#22120;&#65292;&#24182;&#23545;&#20998;&#35789;&#22120;&#35774;&#35745;&#23545;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65288;&#22914;HumanEval&#21644;MBPP&#65289;&#20013;LLM&#24615;&#33021;&#24433;&#21709;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#28040;&#34701;&#65292;&#25552;&#20379;&#20102;&#20998;&#35789;&#22120;&#36229;&#21442;&#25968;&#36873;&#25321;&#21644;&#22312;&#39044;&#35757;&#32451;LLM&#20013;&#20999;&#25442;&#20998;&#35789;&#22120;&#30340;&#24314;&#35758;&#12290;&#25105;&#20204;&#22312;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#21644;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#23427;&#20204;&#23545;&#21508;&#31181;&#20219;&#21153;&#21644;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tokenization is an understudied and often neglected component of modern LLMs. Most published works use a single tokenizer for all experiments, often borrowed from another model, without performing ablations or analysis to optimize tokenization. Moreover, the tokenizer is generally kept unchanged when fine-tuning a base model. In this paper, we show that the size, pre-tokenization regular expression, and training data of a tokenizer can significantly impact the model's generation speed, effective context size, memory usage, and downstream performance. We train specialized Byte-Pair Encoding code tokenizers, and conduct extensive ablations on the impact of tokenizer design on the performance of LLMs for code generation tasks such as HumanEval and MBPP, and provide recommendations for tokenizer hyper-parameters selection and switching the tokenizer in a pre-trained LLM. We perform our experiments on models trained from scratch and from pre-trained models, verifying their applicability to 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;Transformer&#27169;&#22411;&#22312;&#22797;&#21046;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#65292;&#30456;&#27604;&#20110;&#20351;&#29992;&#22266;&#23450;&#28508;&#22312;&#29366;&#24577;&#30340;&#24191;&#20041;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;Transformer&#27169;&#22411;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#22797;&#21046;&#21644;&#26816;&#32034;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01032</link><description>&lt;p&gt;
&#36319;&#30528;&#25105;&#37325;&#22797;&#65306;Transformer&#22312;&#22797;&#21046;&#20219;&#21153;&#19978;&#27604;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
Repeat After Me: Transformers are Better than State Space Models at Copying
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01032
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;Transformer&#27169;&#22411;&#22312;&#22797;&#21046;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#65292;&#30456;&#27604;&#20110;&#20351;&#29992;&#22266;&#23450;&#28508;&#22312;&#29366;&#24577;&#30340;&#24191;&#20041;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;Transformer&#27169;&#22411;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#22797;&#21046;&#21644;&#26816;&#32034;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#24207;&#21015;&#24314;&#27169;&#30340;&#20027;&#35201;&#26550;&#26500;&#65292;&#20294;&#23545;&#20110;&#20351;&#29992;&#19981;&#20381;&#36182;&#20110;&#24207;&#21015;&#38271;&#24230;&#30340;&#22266;&#23450;&#22823;&#23567;&#28508;&#22312;&#29366;&#24577;&#30340;&#27169;&#22411;&#65292;&#20063;&#23601;&#26159;"&#24191;&#20041;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;" (GSSMs)&#65292;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#34429;&#28982;GSSMs&#22312;&#25512;&#29702;&#26102;&#38388;&#25928;&#29575;&#19978;&#26377;&#20248;&#21183;&#65292;&#20294;&#22312;&#38656;&#35201;&#20174;&#36755;&#20837;&#19978;&#19979;&#25991;&#22797;&#21046;&#30340;&#20219;&#21153;&#19978;&#65292;&#23427;&#20204;&#30456;&#23545;&#20110;transformer&#27169;&#22411;&#26469;&#35828;&#26377;&#38480;&#21046;&#12290;&#25105;&#20204;&#20174;&#23545;&#31616;&#21333;&#30340;&#23383;&#31526;&#20018;&#22797;&#21046;&#20219;&#21153;&#30340;&#29702;&#35770;&#20998;&#26512;&#24320;&#22987;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#20010;&#20004;&#23618;&#30340;transformer&#21487;&#20197;&#22797;&#21046;&#25351;&#25968;&#38271;&#24230;&#30340;&#23383;&#31526;&#20018;&#65292;&#32780;GSSMs&#30001;&#20110;&#20854;&#22266;&#23450;&#22823;&#23567;&#30340;&#28508;&#22312;&#29366;&#24577;&#22312;&#26681;&#26412;&#19978;&#26159;&#26377;&#38480;&#21046;&#30340;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;transformer&#22312;&#38656;&#35201;&#22797;&#21046;&#19978;&#19979;&#25991;&#30340;&#21512;&#25104;&#20219;&#21153;&#20013;&#65292;&#22312;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#19978;&#20248;&#20110;GSSMs&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;transformer&#27169;&#22411;&#22312;&#22797;&#21046;&#21644;&#26816;&#32034;&#19978;&#19979;&#25991;&#20449;&#24687;&#26041;&#38754;&#36828;&#36828;&#20248;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are the dominant architecture for sequence modeling, but there is growing interest in models that use a fixed-size latent state that does not depend on the sequence length, which we refer to as "generalized state space models" (GSSMs). In this paper we show that while GSSMs are promising in terms of inference-time efficiency, they are limited compared to transformer models on tasks that require copying from the input context. We start with a theoretical analysis of the simple task of string copying and prove that a two layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state. Empirically, we find that transformers outperform GSSMs in terms of efficiency and generalization on synthetic tasks that require copying the context. Finally, we evaluate pretrained large language models and find that transformer models dramatically outperform state space models at copying and retrieving information from context. 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21487;&#25191;&#34892;&#30340;Python&#20195;&#30721;&#25972;&#21512;LLM&#26234;&#33021;&#20307;&#34892;&#21160;&#65292;&#25552;&#21319;&#20102;&#25104;&#21151;&#29575;&#39640;&#36798;20%&#12290;</title><link>https://rss.arxiv.org/abs/2402.01030</link><description>&lt;p&gt;
&#21487;&#25191;&#34892;&#20195;&#30721;&#34892;&#21160;&#33021;&#22815;&#28608;&#21457;&#26356;&#20986;&#33394;&#30340;LLM&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Executable Code Actions Elicit Better LLM Agents
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01030
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21487;&#25191;&#34892;&#30340;Python&#20195;&#30721;&#25972;&#21512;LLM&#26234;&#33021;&#20307;&#34892;&#21160;&#65292;&#25552;&#21319;&#20102;&#25104;&#21151;&#29575;&#39640;&#36798;20%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26234;&#33021;&#20307;&#20855;&#22791;&#25191;&#34892;&#24191;&#27867;&#34892;&#21160;&#30340;&#33021;&#21147;&#65292;&#22914;&#35843;&#29992;&#24037;&#20855;&#21644;&#25511;&#21046;&#26426;&#22120;&#20154;&#31561;&#65292;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#25361;&#25112;&#20013;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;LLM&#26234;&#33021;&#20307;&#36890;&#24120;&#36890;&#36807;&#29983;&#25104;JSON&#25110;&#25991;&#26412;&#30340;&#39044;&#23450;&#20041;&#26684;&#24335;&#26469;&#20135;&#29983;&#34892;&#21160;&#65292;&#36825;&#36890;&#24120;&#21463;&#38480;&#20110;&#21463;&#38480;&#21046;&#30340;&#34892;&#21160;&#31354;&#38388;&#65288;&#20363;&#22914;&#65292;&#39044;&#23450;&#20041;&#24037;&#20855;&#30340;&#33539;&#22260;&#65289;&#21644;&#21463;&#38480;&#30340;&#28789;&#27963;&#24615;&#65288;&#20363;&#22914;&#65292;&#26080;&#27861;&#32452;&#21512;&#22810;&#20010;&#24037;&#20855;&#65289;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#21487;&#25191;&#34892;&#30340;Python&#20195;&#30721;&#23558;LLM&#26234;&#33021;&#20307;&#30340;&#34892;&#21160;&#25972;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#65288;CodeAct&#65289;&#12290;CodeAct&#19982;Python&#35299;&#37322;&#22120;&#38598;&#25104;&#65292;&#21487;&#20197;&#25191;&#34892;&#20195;&#30721;&#34892;&#21160;&#65292;&#24182;&#36890;&#36807;&#22810;&#36718;&#20132;&#20114;&#22312;&#26032;&#30340;&#35266;&#23519;&#20013;&#21160;&#24577;&#20462;&#35746;&#20808;&#21069;&#30340;&#34892;&#21160;&#25110;&#21457;&#20986;&#26032;&#30340;&#34892;&#21160;&#12290;&#25105;&#20204;&#23545;17&#20010;LLM&#22312;API-Bank&#21644;&#26032;&#32534;&#21046;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;CodeAct&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#26367;&#20195;&#26041;&#26696;&#65288;&#25104;&#21151;&#29575;&#39640;&#20986;20%&#65289;&#12290;CodeAct&#30340;&#20196;&#20154;&#40723;&#33310;&#30340;&#34920;&#29616;&#28608;&#21169;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-sourc
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#29992;&#20110;&#36328;&#26102;&#38388;&#21644;&#35821;&#35328;&#20013;&#20934;&#30830;&#25429;&#25417;&#39640;&#39057;&#21644;&#20302;&#39057;&#35789;&#20041;&#30340;&#24494;&#22937;&#21464;&#21270;&#65292;&#23588;&#20854;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#25429;&#25417;&#20302;&#39057;&#35789;&#20041;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;SemEval2020&#22235;&#31181;&#35821;&#35328;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01025</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#32858;&#31867;&#29992;&#20110;&#36328;&#26102;&#38388;&#21644;&#35821;&#35328;&#26816;&#27979;&#35821;&#20041;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Graph-based Clustering for Detecting Semantic Change Across Time and Languages
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01025
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#29992;&#20110;&#36328;&#26102;&#38388;&#21644;&#35821;&#35328;&#20013;&#20934;&#30830;&#25429;&#25417;&#39640;&#39057;&#21644;&#20302;&#39057;&#35789;&#20041;&#30340;&#24494;&#22937;&#21464;&#21270;&#65292;&#23588;&#20854;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#25429;&#25417;&#20302;&#39057;&#35789;&#20041;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;SemEval2020&#22235;&#31181;&#35821;&#35328;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#19978;&#19979;&#25991;&#21270;&#23884;&#20837;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#20294;&#22522;&#20110;&#36825;&#20123;&#23884;&#20837;&#21644;&#32858;&#31867;&#26041;&#27861;&#30340;&#26816;&#27979;&#35821;&#20041;&#21464;&#21270;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#19981;&#22914;&#22522;&#20110;&#38745;&#24577;&#35789;&#23884;&#20837;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#36825;&#26159;&#22240;&#20026;&#32858;&#31867;&#26041;&#27861;&#20135;&#29983;&#35789;&#20041;&#32858;&#31867;&#30340;&#36136;&#37327;&#36739;&#24046;&#65292;&#38590;&#20197;&#25429;&#25417;&#20302;&#39057;&#35789;&#20041;&#65292;&#23588;&#20854;&#26159;&#20302;&#39057;&#35789;&#20041;&#12290;&#36825;&#20010;&#38382;&#39064;&#38459;&#30861;&#20102;&#30740;&#31350;&#19968;&#20010;&#35821;&#35328;&#20013;&#30340;&#35789;&#20041;&#21464;&#21270;&#23545;&#21478;&#19968;&#20010;&#35821;&#35328;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#36328;&#26102;&#38388;&#21644;&#35821;&#35328;&#20013;&#39640;&#39057;&#21644;&#20302;&#39057;&#35789;&#20041;&#30340;&#24494;&#22937;&#21464;&#21270;&#65292;&#21253;&#25324;&#36825;&#20123;&#35789;&#20041;&#38543;&#26102;&#38388;&#30340;&#22686;&#21152;&#21644;&#20943;&#23569;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;SemEval2020&#22235;&#31181;&#35821;&#35328;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#20013;&#26126;&#26174;&#20248;&#20110;&#36807;&#21435;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#22810;&#21151;&#33021;&#21487;&#35270;&#21270;&#24037;&#20855;&#29992;&#20110;&#26816;&#27979;&#35821;&#20041;&#21464;&#21270;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the predominance of contextualized embeddings in NLP, approaches to detect semantic change relying on these embeddings and clustering methods underperform simpler counterparts based on static word embeddings. This stems from the poor quality of the clustering methods to produce sense clusters -- which struggle to capture word senses, especially those with low frequency. This issue hinders the next step in examining how changes in word senses in one language influence another. To address this issue, we propose a graph-based clustering approach to capture nuanced changes in both high- and low-frequency word senses across time and languages, including the acquisition and loss of these senses over time. Our experimental results show that our approach substantially surpasses previous approaches in the SemEval2020 binary classification task across four languages. Moreover, we showcase the ability of our approach as a versatile visualization tool to detect semantic changes in both int
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#39046;&#22495;&#29420;&#31435;&#30340;&#27450;&#39575;&#34892;&#20026;&#30340;&#26032;&#20998;&#31867;&#21644;&#35821;&#35328;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#32473;&#20986;&#20102;&#23545;&#27450;&#39575;&#34892;&#20026;&#30340;&#26032;&#30340;&#35745;&#31639;&#23450;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#30340;&#20998;&#31867;&#31995;&#32479;&#12290;&#28982;&#21518;&#65292;&#20998;&#26512;&#20102;&#20851;&#20110;&#27450;&#39575;&#30340;&#35821;&#35328;&#32447;&#32034;&#30340;&#20105;&#35758;&#24182;&#25552;&#20379;&#20102;&#31995;&#32479;&#22238;&#39038;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;&#26368;&#21518;&#65292;&#30740;&#31350;&#20102;&#24120;&#35265;&#30340;&#35821;&#35328;&#29305;&#24449;&#24182;&#25552;&#20379;&#20102;&#30693;&#35782;&#36716;&#31227;&#30340;&#35777;&#25454;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01019</link><description>&lt;p&gt;
&#38754;&#21521;&#39046;&#22495;&#29420;&#31435;&#30340;&#27450;&#39575;&#34892;&#20026;&#65306;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#21644;&#35821;&#35328;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Domain-Independent Deception: A New Taxonomy and Linguistic Analysis
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#39046;&#22495;&#29420;&#31435;&#30340;&#27450;&#39575;&#34892;&#20026;&#30340;&#26032;&#20998;&#31867;&#21644;&#35821;&#35328;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#32473;&#20986;&#20102;&#23545;&#27450;&#39575;&#34892;&#20026;&#30340;&#26032;&#30340;&#35745;&#31639;&#23450;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#30340;&#20998;&#31867;&#31995;&#32479;&#12290;&#28982;&#21518;&#65292;&#20998;&#26512;&#20102;&#20851;&#20110;&#27450;&#39575;&#30340;&#35821;&#35328;&#32447;&#32034;&#30340;&#20105;&#35758;&#24182;&#25552;&#20379;&#20102;&#31995;&#32479;&#22238;&#39038;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;&#26368;&#21518;&#65292;&#30740;&#31350;&#20102;&#24120;&#35265;&#30340;&#35821;&#35328;&#29305;&#24449;&#24182;&#25552;&#20379;&#20102;&#30693;&#35782;&#36716;&#31227;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#32463;&#27982;&#21644;&#31038;&#20250;&#27491;&#28153;&#27809;&#22312;&#27450;&#39575;&#24615;&#25915;&#20987;&#20013;&#12290;&#36825;&#20123;&#25915;&#20987;&#20197;&#20551;&#26032;&#38395;&#12289;&#32593;&#32476;&#38035;&#40060;&#21644;&#34394;&#20551;&#25307;&#32856;&#31561;&#22810;&#31181;&#24418;&#24335;&#21576;&#29616;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#27450;&#39575;&#39046;&#22495;&#8221;&#12290;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#30740;&#31350;&#20154;&#21592;&#19968;&#30452;&#22312;&#23581;&#35797;&#36890;&#36807;&#35774;&#35745;&#29305;&#23450;&#39046;&#22495;&#30340;&#26816;&#27979;&#22120;&#26469;&#25913;&#21892;&#36825;&#20010;&#21361;&#38505;&#24773;&#20917;&#12290;&#21482;&#26377;&#23569;&#25968;&#26368;&#36817;&#30340;&#30740;&#31350;&#32771;&#34385;&#21040;&#20102;&#39046;&#22495;&#29420;&#31435;&#30340;&#27450;&#39575;&#34892;&#20026;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#36825;&#20123;&#19981;&#21516;&#30340;&#30740;&#31350;&#32447;&#32034;&#65292;&#24182;&#30740;&#31350;&#20102;&#39046;&#22495;&#29420;&#31435;&#30340;&#27450;&#39575;&#34892;&#20026;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#27450;&#39575;&#34892;&#20026;&#30340;&#26032;&#30340;&#35745;&#31639;&#23450;&#20041;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#25104;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#31995;&#32479;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20851;&#20110;&#35821;&#35328;&#32447;&#32034;&#30340;&#20105;&#35770;&#65292;&#24182;&#25552;&#20379;&#20102;&#31995;&#32479;&#22238;&#39038;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24120;&#35265;&#30340;&#35821;&#35328;&#29305;&#24449;&#65292;&#24182;&#20026;&#19981;&#21516;&#24418;&#24335;&#30340;&#27450;&#39575;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#25552;&#20379;&#20102;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Internet-based economies and societies are drowning in deceptive attacks. These attacks take many forms, such as fake news, phishing, and job scams, which we call ``domains of deception.'' Machine-learning and natural-language-processing researchers have been attempting to ameliorate this precarious situation by designing domain-specific detectors. Only a few recent works have considered domain-independent deception. We collect these disparate threads of research and investigate domain-independent deception. First, we provide a new computational definition of deception and break down deception into a new taxonomy. Then, we analyze the debate on linguistic cues for deception and supply guidelines for systematic reviews. Finally, we investigate common linguistic features and give evidence for knowledge transfer across different forms of deception.
&lt;/p&gt;</description></item><item><title>HR-MultiWOZ&#26159;&#19968;&#20010;&#38754;&#21521;HR LLM Agent&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#39318;&#20010;&#24320;&#28304;&#30340;HR&#23545;&#35805;&#25968;&#25454;&#38598;&#26631;&#35760;&#65292;&#29992;&#20110;NLP&#30740;&#31350;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01018</link><description>&lt;p&gt;
HR-MultiWOZ: &#19968;&#31181;&#38754;&#21521;HR LLM Agent&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01018
&lt;/p&gt;
&lt;p&gt;
HR-MultiWOZ&#26159;&#19968;&#20010;&#38754;&#21521;HR LLM Agent&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#39318;&#20010;&#24320;&#28304;&#30340;HR&#23545;&#35805;&#25968;&#25454;&#38598;&#26631;&#35760;&#65292;&#29992;&#20110;NLP&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#27491;&#22312;&#37325;&#22609;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#20219;&#21153;&#12290;&#23427;&#20204;&#22312;&#20154;&#21147;&#36164;&#28304;&#65288;HR&#65289;&#39046;&#22495;&#30340;&#24212;&#29992;&#20173;&#26377;&#24453;&#25193;&#23637;&#65292;&#24182;&#19988;&#21487;&#33021;&#23545;&#19968;&#20123;&#32791;&#26102;&#30340;&#20219;&#21153;&#24456;&#26377;&#30410;&#12290;&#20363;&#22914;&#65292;&#20241;&#20551;&#30003;&#35831;&#12289;&#21307;&#30103;&#32034;&#36180;&#21644;&#35775;&#38382;&#26435;&#38480;&#30003;&#35831;&#37117;&#26159;&#20540;&#24471;&#27880;&#24847;&#30340;&#20363;&#23376;&#65292;&#20294;&#23427;&#20204;&#32477;&#19981;&#26159;&#21807;&#19968;&#30340;&#20363;&#23376;&#12290;&#28982;&#32780;&#65292;&#19978;&#36848;&#21457;&#23637;&#24517;&#39035;&#24212;&#23545;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#65292;&#21363;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#19968;&#26041;&#38754;&#65292;&#22823;&#22810;&#25968;&#23545;&#35805;&#25968;&#25454;&#38598;&#35299;&#20915;&#30340;&#26159;&#23458;&#25143;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#21592;&#24037;&#38382;&#39064;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#33719;&#21462;&#19982;HR&#30340;&#23545;&#35805;&#21487;&#33021;&#24341;&#36215;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;HR-Multiwoz&#65292;&#36825;&#26159;&#19968;&#20010;&#23436;&#20840;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;550&#20010;&#28085;&#30422;10&#20010;HR&#39046;&#22495;&#30340;&#23545;&#35805;&#65292;&#29992;&#20110;&#35780;&#20272;LLM Agent&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20855;&#26377;&#20197;&#19979;&#36129;&#29486;&#65306;(1) &#36825;&#26159;HR&#39046;&#22495;&#30340;&#39318;&#20010;&#24320;&#28304;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;NLP&#30740;&#31350;&#12290;(2) &#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#26631;&#35760;&#30340;&#12289;&#38754;&#21521;NLP&#20219;&#21153;&#30340;HR&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models (LLMs) have been reshaping Natural Language Processing (NLP) task in several domains. Their use in the field of Human Resources (HR) has still room for expansions and could be beneficial for several time consuming tasks. Examples such as time-off submissions, medical claims filing, and access requests are noteworthy, but they are by no means the sole instances. However, the aforementioned developments must grapple with the pivotal challenge of constructing a high-quality training dataset. On one hand, most conversation datasets are solving problems for customers not employees. On the other hand, gathering conversations with HR could raise privacy concerns. To solve it, we introduce HR-Multiwoz, a fully-labeled dataset of 550 conversations spanning 10 HR domains to evaluate LLM Agent. Our work has the following contributions: (1) It is the first labeled open-sourced conversation dataset in the HR domain for NLP research. (2) It provides a det
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#26694;&#26550;&#26469;&#20998;&#26512;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#36755;&#20837;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#19978;&#19979;&#25991;&#23545;&#36755;&#20986;&#30340;&#24433;&#21709;&#20943;&#23567;&#65292;&#21516;&#26102;&#35821;&#20041;&#21547;&#20041;&#23545;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;&#24433;&#21709;&#36739;&#22823;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00978</link><description>&lt;p&gt;
&#19968;&#31181;&#20449;&#24687;&#35770;&#26041;&#27861;&#26469;&#20998;&#26512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20998;&#31867;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
An Information-Theoretic Approach to Analyze NLP Classification Tasks
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#26694;&#26550;&#26469;&#20998;&#26512;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#36755;&#20837;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#19978;&#19979;&#25991;&#23545;&#36755;&#20986;&#30340;&#24433;&#21709;&#20943;&#23567;&#65292;&#21516;&#26102;&#35821;&#20041;&#21547;&#20041;&#23545;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;&#24433;&#21709;&#36739;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#36755;&#20837;&#23545;&#36755;&#20986;&#30340;&#37325;&#35201;&#24615;&#23545;&#35768;&#22810;&#20219;&#21153;&#37117;&#26377;&#24110;&#21161;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#26694;&#26550;&#26469;&#20998;&#26512;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#36755;&#20837;&#30340;&#24433;&#21709;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#37319;&#29992;&#21333;&#20010;&#20803;&#32032;&#36755;&#20837;&#25110;&#22810;&#20010;&#20803;&#32032;&#36755;&#20837;&#26469;&#39044;&#27979;&#36755;&#20986;&#21464;&#37327;&#65292;&#20854;&#20013;&#19968;&#20010;&#20803;&#32032;&#26159;&#19968;&#27573;&#25991;&#23383;&#12290;&#27599;&#20010;&#25991;&#23383;&#20803;&#32032;&#26377;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#20851;&#32852;&#30340;&#35821;&#20041;&#21547;&#20041;&#21644;&#35821;&#35328;&#23454;&#29616;&#12290;&#36873;&#25321;&#20102;&#22810;&#39033;&#36873;&#25321;&#38405;&#35835;&#29702;&#35299;&#65288;MCRC&#65289;&#21644;&#24773;&#24863;&#20998;&#31867;&#65288;SC&#65289;&#26469;&#23637;&#31034;&#35813;&#26694;&#26550;&#12290;&#23545;&#20110;MCRC&#65292;&#21457;&#29616;&#30456;&#23545;&#20110;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#19978;&#19979;&#25991;&#23545;&#36755;&#20986;&#30340;&#24433;&#21709;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#20943;&#23567;&#12290;&#29305;&#21035;&#26159;&#65292;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#19978;&#19979;&#25991;&#20801;&#35768;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#26356;&#22823;&#30340;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#27979;&#35797;&#21019;&#24314;&#32773;&#22312;&#35774;&#35745;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#26102;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#19978;&#19979;&#25991;&#30340;&#36873;&#25321;&#12290;&#23545;&#20110;SC&#65292;&#21457;&#29616;&#35821;&#20041;&#21547;&#20041;&#23545;&#20110;&#36755;&#20986;&#30340;&#24433;&#21709;&#36739;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the importance of the inputs on the output is useful across many tasks. This work provides an information-theoretic framework to analyse the influence of inputs for text classification tasks. Natural language processing (NLP) tasks take either a single element input or multiple element inputs to predict an output variable, where an element is a block of text. Each text element has two components: an associated semantic meaning and a linguistic realization. Multiple-choice reading comprehension (MCRC) and sentiment classification (SC) are selected to showcase the framework. For MCRC, it is found that the context influence on the output compared to the question influence reduces on more challenging datasets. In particular, more challenging contexts allow a greater variation in complexity of questions. Hence, test creators need to carefully consider the choice of the context when designing multiple-choice questions for assessment. For SC, it is found the semantic meaning of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38754;&#21521;&#38750;&#31243;&#24207;&#21592;&#29992;&#25143;&#30340;KG&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#20307;&#38142;&#25509;&#21644;GPT&#27169;&#22411;&#29983;&#25104;SPARQL&#26597;&#35810;&#65292;&#20351;&#29992;CWA&#39044;&#35757;&#32451;&#25152;&#26377;&#23454;&#20307;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;SPARQL&#21305;&#37197;&#29575;&#20026;62.703%&#12290;</title><link>https://rss.arxiv.org/abs/2402.00969</link><description>&lt;p&gt;
&#20351;&#29992;Entity&#39044;&#35757;&#32451;GPT&#20026;KG&#38382;&#31572;&#29983;&#25104;SPARQL
&lt;/p&gt;
&lt;p&gt;
SPARQL Generation with Entity Pre-trained GPT for KG Question Answering
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38754;&#21521;&#38750;&#31243;&#24207;&#21592;&#29992;&#25143;&#30340;KG&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#20307;&#38142;&#25509;&#21644;GPT&#27169;&#22411;&#29983;&#25104;SPARQL&#26597;&#35810;&#65292;&#20351;&#29992;CWA&#39044;&#35757;&#32451;&#25152;&#26377;&#23454;&#20307;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;SPARQL&#21305;&#37197;&#29575;&#20026;62.703%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#30340;&#27969;&#34892;&#24230;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#36805;&#36895;&#22686;&#38271;&#12290;&#20154;&#20204;&#21487;&#20197;&#36890;&#36807;&#20114;&#32852;&#32593;&#19978;&#30340;&#35768;&#22810;&#22312;&#32447;&#25968;&#25454;&#24211;&#26597;&#35810;&#36825;&#20123;&#30693;&#35782;&#12290;&#20294;&#26159;&#65292;&#22914;&#26524;&#38750;&#31243;&#24207;&#21592;&#29992;&#25143;&#33021;&#22815;&#35775;&#38382;&#20182;&#20204;&#24819;&#35201;&#30693;&#36947;&#30340;&#20219;&#20309;&#20449;&#24687;&#65292;&#37027;&#23558;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25104;&#23601;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#20184;&#20986;&#20102;&#24456;&#22810;&#21162;&#21147;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#21644;&#36890;&#36807;&#35768;&#22810;&#25361;&#25112;&#28608;&#21169;&#21019;&#36896;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37325;&#28857;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#19978;&#36827;&#34892;&#27491;&#30830;&#30340;&#23454;&#20307;&#38142;&#25509;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;GPT&#27169;&#22411;&#26469;&#20174;&#20013;&#21019;&#24314;SPARQL&#26597;&#35810;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#30830;&#23450;&#20102;&#36825;&#20010;&#20219;&#21153;&#20013;&#21487;&#33021;&#26368;&#38590;&#20197;&#22312;&#23569;&#25968;&#25110;&#38646;&#27425;&#23581;&#35797;&#20013;&#35299;&#20915;&#30340;&#23646;&#24615;&#65292;&#24182;&#25552;&#20986;&#23545;&#25152;&#26377;&#23454;&#20307;&#36827;&#34892;&#39044;&#35757;&#32451;&#65288;&#22312;CWA&#19979;&#65289;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;3&#27425;&#23581;&#35797;&#20013;&#65292;&#25105;&#20204;&#22312;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;62.703%&#30340;&#20934;&#30830;&#30340;SPARQL&#21305;&#37197;&#29575;&#65292;&#22312;&#23454;&#20307;&#38142;&#25509;&#25361;&#25112;&#20013;&#33719;&#24471;&#20102;0.809&#30340;F1&#20540;&#65292;&#22312;&#38382;&#39064;&#22238;&#31572;&#25361;&#25112;&#20013;&#33719;&#24471;&#20102;0.009&#30340;F1&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs popularity has been rapidly growing in last years. All that knowledge is available for people to query it through the many online databases on the internet. Though, it would be a great achievement if non-programmer users could access whatever information they want to know. There has been a lot of effort oriented to solve this task using natural language processing tools and creativity encouragement by way of many challenges. Our approach focuses on assuming a correct entity linking on the natural language questions and training a GPT model to create SPARQL queries from them. We managed to isolate which property of the task can be the most difficult to solve at few or zero-shot and we proposed pre-training on all entities (under CWA) to improve the performance. We obtained a 62.703% accuracy of exact SPARQL matches on testing at 3-shots, a F1 of 0.809 on the entity linking challenge and a F1 of 0.009 on the question answering challenge.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20877;&#29616;&#24515;&#29702;&#35821;&#35328;&#23398;&#23454;&#39564;&#65292;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23613;&#31649;&#26080;&#20855;&#36523;&#24615;&#65292;&#21364;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#21040;&#20154;&#31867;&#23545;&#20110;&#35821;&#35328;&#20013;&#22522;&#26412;&#30340;&#31354;&#38388;&#26500;&#24314;&#22359;&#30340;&#38544;&#21547;&#30452;&#35273;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#35821;&#35328;&#21644;&#31354;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00956</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;&#27169;&#22411;&#20013;&#25506;&#32034;&#31354;&#38388;&#27169;&#24335;&#30452;&#35273;
&lt;/p&gt;
&lt;p&gt;
Exploring Spatial Schema Intuitions in Large Language and Vision Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00956
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20877;&#29616;&#24515;&#29702;&#35821;&#35328;&#23398;&#23454;&#39564;&#65292;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23613;&#31649;&#26080;&#20855;&#36523;&#24615;&#65292;&#21364;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#21040;&#20154;&#31867;&#23545;&#20110;&#35821;&#35328;&#20013;&#22522;&#26412;&#30340;&#31354;&#38388;&#26500;&#24314;&#22359;&#30340;&#38544;&#21547;&#30452;&#35273;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#35821;&#35328;&#21644;&#31354;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#38750;&#24120;&#26222;&#36941;&#65292;&#20294;&#20851;&#20110;LLM&#20013;&#30340;&#20855;&#36523;&#24615;&#38382;&#39064;&#36824;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#65292;&#36825;&#20351;&#23427;&#20204;&#19981;&#21516;&#20110;&#26426;&#22120;&#20154;&#20013;&#20855;&#20307;&#30340;&#20855;&#36523;&#31995;&#32479;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#36890;&#36807;&#24863;&#30693;&#30452;&#25509;&#25351;&#23548;&#34892;&#21160;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#65292;&#21363;LLM&#26159;&#21542;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#21040;&#20154;&#31867;&#23545;&#20110;&#35821;&#35328;&#20013;&#22522;&#26412;&#30340;&#31354;&#38388;&#26500;&#24314;&#22359;&#30340;&#38544;&#21547;&#30452;&#35273;&#65292;&#23613;&#31649;&#23427;&#20204;&#26159;&#38750;&#20855;&#36523;&#30340;&#12290;&#25105;&#20204;&#36816;&#29992;&#20174;&#26089;&#26399;&#24863;&#30693;&#36816;&#21160;&#32463;&#39564;&#20013;&#21457;&#23637;&#20986;&#30340;&#31354;&#38388;&#35748;&#30693;&#22522;&#30784;&#30340;&#35265;&#35299;&#65292;&#36890;&#36807;&#20877;&#29616;&#19977;&#20010;&#24515;&#29702;&#35821;&#35328;&#23398;&#23454;&#39564;&#26469;&#25351;&#23548;&#25105;&#20204;&#30340;&#25506;&#32034;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#27169;&#22411;&#36755;&#20986;&#19982;&#20154;&#31867;&#22238;&#31572;&#20043;&#38388;&#20986;&#29616;&#20102;&#30456;&#20851;&#24615;&#65292;&#25581;&#31034;&#20102;&#27809;&#26377;&#19982;&#20855;&#20307;&#32463;&#39564;&#26377;&#23454;&#36136;&#36830;&#25509;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#26174;&#33879;&#30340;&#21306;&#21035;&#21253;&#25324;&#26497;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#21709;&#24212;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#38477;&#20302;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#28145;&#20837;&#20102;&#35299;&#35821;&#35328;&#21644;&#31354;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26377;&#30528;&#24494;&#22937;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the ubiquity of large language models (LLMs) in AI research, the question of embodiment in LLMs remains underexplored, distinguishing them from embodied systems in robotics where sensory perception directly informs physical action. Our investigation navigates the intriguing terrain of whether LLMs, despite their non-embodied nature, effectively capture implicit human intuitions about fundamental, spatial building blocks of language. We employ insights from spatial cognitive foundations developed through early sensorimotor experiences, guiding our exploration through the reproduction of three psycholinguistic experiments. Surprisingly, correlations between model outputs and human responses emerge, revealing adaptability without a tangible connection to embodied experiences. Notable distinctions include polarized language model responses and reduced correlations in vision language models. This research contributes to a nuanced understanding of the interplay between language, spat
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#22411;&#24179;&#21488;&#65292;&#26088;&#22312;&#20351;&#22823;&#22411;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#26356;&#26131;&#20110;&#20351;&#29992;&#65292;&#36890;&#36807;&#26368;&#26032;&#30340;&#22810;LoRA&#25512;&#29702;&#25216;&#26415;&#21644;&#23450;&#21046;&#36866;&#37197;&#22120;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#38548;&#31163;&#12289;&#21152;&#23494;&#21644;&#36523;&#20221;&#39564;&#35777;&#30340;&#23433;&#20840;&#26381;&#21153;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00913</link><description>&lt;p&gt;
&#29992;&#20110;&#23433;&#20840;&#33258;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#30340;&#26426;&#26500;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Institutional Platform for Secure Self-Service Large Language Model Exploration
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00913
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#22411;&#24179;&#21488;&#65292;&#26088;&#22312;&#20351;&#22823;&#22411;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#26356;&#26131;&#20110;&#20351;&#29992;&#65292;&#36890;&#36807;&#26368;&#26032;&#30340;&#22810;LoRA&#25512;&#29702;&#25216;&#26415;&#21644;&#23450;&#21046;&#36866;&#37197;&#22120;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#38548;&#31163;&#12289;&#21152;&#23494;&#21644;&#36523;&#20221;&#39564;&#35777;&#30340;&#23433;&#20840;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#30001;&#32943;&#22612;&#22522;&#22823;&#23398;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#20013;&#24515;&#24320;&#21457;&#30340;&#29992;&#25143;&#21451;&#22909;&#22411;&#24179;&#21488;&#65292;&#26088;&#22312;&#20351;&#22823;&#22411;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26356;&#26131;&#20110;&#20351;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#22312;&#22810;LoRA&#25512;&#29702;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#31995;&#32479;&#26377;&#25928;&#22320;&#36866;&#24212;&#20102;&#21508;&#31867;&#29992;&#25143;&#21644;&#39033;&#30446;&#30340;&#23450;&#21046;&#36866;&#37197;&#22120;&#12290;&#35770;&#25991;&#27010;&#36848;&#20102;&#31995;&#32479;&#30340;&#26550;&#26500;&#21644;&#20851;&#38190;&#29305;&#24615;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#31574;&#21010;&#12289;&#27169;&#22411;&#35757;&#32451;&#12289;&#23433;&#20840;&#25512;&#29702;&#21644;&#22522;&#20110;&#25991;&#26412;&#30340;&#29305;&#24449;&#25552;&#21462;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20110;&#31199;&#25143;&#24847;&#35782;&#30340;&#35745;&#31639;&#32593;&#32476;&#65292;&#22312;&#23433;&#20840;&#22320;&#21033;&#29992;&#23396;&#31435;&#36164;&#28304;&#23707;&#30340;&#22522;&#30784;&#19978;&#24418;&#25104;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#31995;&#32479;&#12290;&#35813;&#24179;&#21488;&#33268;&#21147;&#20110;&#25552;&#20379;&#23433;&#20840;&#30340;LLM&#26381;&#21153;&#65292;&#24378;&#35843;&#36807;&#31243;&#21644;&#25968;&#25454;&#38548;&#31163;&#12289;&#31471;&#21040;&#31471;&#21152;&#23494;&#20197;&#21450;&#22522;&#20110;&#35282;&#33394;&#30340;&#36164;&#28304;&#36523;&#20221;&#39564;&#35777;&#12290;&#35813;&#36129;&#29486;&#19982;&#23454;&#29616;&#31616;&#21270;&#35775;&#38382;&#20808;&#36827;&#30340;AI&#27169;&#22411;&#21644;&#25216;&#26415;&#20197;&#25903;&#25345;&#31185;&#23398;&#21457;&#29616;&#30340;&#24635;&#20307;&#30446;&#26631;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a user-friendly platform developed by the University of Kentucky Center for Applied AI, designed to make large, customized language models (LLMs) more accessible. By capitalizing on recent advancements in multi-LoRA inference, the system efficiently accommodates custom adapters for a diverse range of users and projects. The paper outlines the system's architecture and key features, encompassing dataset curation, model training, secure inference, and text-based feature extraction.   We illustrate the establishment of a tenant-aware computational network using agent-based methods, securely utilizing islands of isolated resources as a unified system. The platform strives to deliver secure LLM services, emphasizing process and data isolation, end-to-end encryption, and role-based resource authentication. This contribution aligns with the overarching goal of enabling simplified access to cutting-edge AI models and technology in support of scientific discovery.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#26089;&#26399;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#25915;&#20987;&#23545;LLM&#26368;&#32456;&#29992;&#25143;&#12289;&#24320;&#21457;&#20154;&#21592;&#21644;&#30740;&#31350;&#20154;&#21592;&#30340;&#24433;&#21709;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00898</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#26089;&#26399;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
An Early Categorization of Prompt Injection Attacks on Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#26089;&#26399;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#25915;&#20987;&#23545;LLM&#26368;&#32456;&#29992;&#25143;&#12289;&#24320;&#21457;&#20154;&#21592;&#21644;&#30740;&#31350;&#20154;&#21592;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#19968;&#30452;&#26159;&#27665;&#20027;&#21270;&#20154;&#24037;&#26234;&#33021;&#30340;&#21069;&#27839;&#12290;&#28982;&#32780;&#65292;ChatGPT&#21644;&#20854;&#20182;&#31867;&#20284;&#24037;&#20855;&#30340;&#21457;&#24067;&#21518;&#65292;&#20154;&#20204;&#23545;&#20110;&#25511;&#21046;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21450;&#20854;&#36755;&#20986;&#30340;&#38590;&#24230;&#36234;&#26469;&#36234;&#25285;&#24551;&#12290;&#30446;&#21069;&#65292;&#25105;&#20204;&#27491;&#22312;&#30446;&#30585;&#19968;&#22330;&#25417;&#36855;&#34255;&#30340;&#28216;&#25103;&#65292;&#29992;&#25143;&#35797;&#22270;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#19968;&#31181;&#31216;&#20026;&#25552;&#31034;&#27880;&#20837;&#30340;&#26032;&#22411;&#25915;&#20987;&#65292;&#32780;&#24320;&#21457;&#20154;&#21592;&#21017;&#35797;&#22270;&#21516;&#26102;&#21457;&#29616;&#28431;&#27934;&#24182;&#38459;&#27490;&#36825;&#20123;&#25915;&#20987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#36825;&#20123;&#26032;&#20852;&#23041;&#32961;&#65292;&#24182;&#25552;&#20986;&#20102;&#25552;&#31034;&#27880;&#20837;&#30340;&#20998;&#31867;&#65292;&#36825;&#21487;&#20197;&#25351;&#23548;&#26410;&#26469;&#22312;&#25552;&#31034;&#27880;&#20837;&#19978;&#30340;&#30740;&#31350;&#65292;&#24182;&#20316;&#20026;LLM&#30028;&#38754;&#24320;&#21457;&#20013;&#28431;&#27934;&#30340;&#26816;&#26597;&#28165;&#21333;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#20808;&#21069;&#30340;&#25991;&#29486;&#21644;&#25105;&#20204;&#33258;&#24049;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25552;&#31034;&#27880;&#20837;&#23545;LLM&#26368;&#32456;&#29992;&#25143;&#12289;&#24320;&#21457;&#20154;&#21592;&#21644;&#30740;&#31350;&#20154;&#21592;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models and AI chatbots have been at the forefront of democratizing artificial intelligence. However, the releases of ChatGPT and other similar tools have been followed by growing concerns regarding the difficulty of controlling large language models and their outputs. Currently, we are witnessing a cat-and-mouse game where users attempt to misuse the models with a novel attack called prompt injections. In contrast, the developers attempt to discover the vulnerabilities and block the attacks simultaneously. In this paper, we provide an overview of these emergent threats and present a categorization of prompt injections, which can guide future research on prompt injections and act as a checklist of vulnerabilities in the development of LLM interfaces. Moreover, based on previous literature and our own empirical research, we discuss the implications of prompt injections to LLM end users, developers, and researchers.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#38450;&#24481;&#21644;&#23545;&#25239;&#24212;&#29992;&#65292;&#24182;&#35782;&#21035;&#20102;&#20851;&#38190;&#30340;&#30740;&#31350;&#31354;&#32570;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;LLM&#39537;&#21160;&#30340;&#32593;&#32476;&#23433;&#20840;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#26426;&#20250;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00891</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#24212;&#29992;&#65306;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Cybersecurity: State-of-the-Art
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#38450;&#24481;&#21644;&#23545;&#25239;&#24212;&#29992;&#65292;&#24182;&#35782;&#21035;&#20102;&#20851;&#38190;&#30340;&#30740;&#31350;&#31354;&#32570;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;LLM&#39537;&#21160;&#30340;&#32593;&#32476;&#23433;&#20840;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#26426;&#20250;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#25105;&#20204;&#23545;&#26234;&#33021;&#30340;&#29702;&#35299;&#65292;&#20351;&#25105;&#20204;&#26356;&#25509;&#36817;&#20110;&#20154;&#24037;&#26234;&#33021;&#12290;&#33258;&#20174;&#23427;&#20204;&#34987;&#24341;&#20837;&#20197;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#31215;&#26497;&#25506;&#32034;&#20102;LLMs&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#65292;&#20256;&#32479;&#19978;&#23545;&#25968;&#25454;&#39537;&#21160;&#35299;&#20915;&#26041;&#26696;&#25345;&#25269;&#35302;&#24577;&#24230;&#19988;&#23545;&#26426;&#22120;&#23398;&#20064;&#37319;&#29992;&#36739;&#24930;&#65292;&#36825;&#19968;&#39046;&#22495;&#21364;&#24322;&#20891;&#31361;&#36215;&#12290;&#26412;&#30740;&#31350;&#23545;&#29616;&#26377;&#25991;&#29486;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20840;&#38754;&#25551;&#36848;&#20102;LLMs&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#38450;&#24481;&#21644;&#23545;&#25239;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#19981;&#20165;&#23545;&#24403;&#21069;&#30340;&#30740;&#31350;&#29616;&#29366;&#36827;&#34892;&#20102;&#35843;&#26597;&#21644;&#20998;&#31867;&#65292;&#24182;&#19988;&#36824;&#35782;&#21035;&#20986;&#20102;&#20851;&#38190;&#30340;&#30740;&#31350;&#31354;&#32570;&#12290;&#36890;&#36807;&#35780;&#20272;&#25915;&#20987;&#21644;&#38450;&#24481;&#24212;&#29992;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#23545;LLM&#39537;&#21160;&#30340;&#32593;&#32476;&#23433;&#20840;&#25152;&#28041;&#21450;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#26426;&#20250;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of Large Language Models (LLMs) has revolutionized our comprehension of intelligence bringing us closer to Artificial Intelligence. Since their introduction, researchers have actively explored the applications of LLMs across diverse fields, significantly elevating capabilities. Cybersecurity, traditionally resistant to data-driven solutions and slow to embrace machine learning, stands out as a domain. This study examines the existing literature, providing a thorough characterization of both defensive and adversarial applications of LLMs within the realm of cybersecurity. Our review not only surveys and categorizes the current landscape but also identifies critical research gaps. By evaluating both offensive and defensive applications, we aim to provide a holistic understanding of the potential risks and opportunities associated with LLM-driven cybersecurity.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#20063;&#38754;&#20020;&#30528;&#23433;&#20840;&#21644;&#38544;&#31169;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;&#26412;&#35843;&#26597;&#20840;&#38754;&#23457;&#26597;&#20102;LLM&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#25361;&#25112;&#65292;&#28085;&#30422;&#20102;&#35757;&#32451;&#25968;&#25454;&#12289;&#29992;&#25143;&#21644;&#24212;&#29992;&#39118;&#38505;&#31561;&#26041;&#38754;&#65292;&#24182;&#23545;&#35299;&#20915;&#26041;&#27861;&#36827;&#34892;&#20102;&#22238;&#39038;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00888</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#25361;&#25112;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Security and Privacy Challenges of Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00888
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#20063;&#38754;&#20020;&#30528;&#23433;&#20840;&#21644;&#38544;&#31169;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;&#26412;&#35843;&#26597;&#20840;&#38754;&#23457;&#26597;&#20102;LLM&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#25361;&#25112;&#65292;&#28085;&#30422;&#20102;&#35757;&#32451;&#25968;&#25454;&#12289;&#29992;&#25143;&#21644;&#24212;&#29992;&#39118;&#38505;&#31561;&#26041;&#38754;&#65292;&#24182;&#23545;&#35299;&#20915;&#26041;&#27861;&#36827;&#34892;&#20102;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#29983;&#25104;&#21644;&#24635;&#32467;&#25991;&#26412;&#12289;&#35821;&#35328;&#32763;&#35793;&#21644;&#38382;&#31572;&#31561;&#22810;&#20010;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#22914;&#20170;&#65292;LLM&#27491;&#22312;&#25104;&#20026;&#35745;&#31639;&#26426;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#38750;&#24120;&#27969;&#34892;&#30340;&#24037;&#20855;&#65292;&#20855;&#22791;&#20998;&#26512;&#22797;&#26434;&#35821;&#35328;&#27169;&#24335;&#24182;&#26681;&#25454;&#19978;&#19979;&#25991;&#25552;&#20379;&#30456;&#20851;&#21644;&#36866;&#24403;&#22238;&#31572;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#36825;&#20123;&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#23433;&#20840;&#21644;&#38544;&#31169;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#22914;&#36234;&#29425;&#25915;&#20987;&#12289;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#21644;&#20010;&#20154;&#21487;&#35782;&#21035;&#20449;&#24687;&#27844;&#38706;&#25915;&#20987;&#12290;&#26412;&#35843;&#26597;&#20840;&#38754;&#23457;&#26597;&#20102;LLM&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#25361;&#25112;&#65292;&#21253;&#25324;&#35757;&#32451;&#25968;&#25454;&#21644;&#29992;&#25143;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#22312;&#20132;&#36890;&#12289;&#25945;&#32946;&#21644;&#21307;&#30103;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#24212;&#29992;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;LLM&#30340;&#33030;&#24369;&#24615;&#31243;&#24230;&#65292;&#35843;&#26597;&#20102;&#20986;&#29616;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#25915;&#20987;&#65292;&#24182;&#23545;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#27861;&#36827;&#34892;&#20102;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated extraordinary capabilities and contributed to multiple fields, such as generating and summarizing text, language translation, and question-answering. Nowadays, LLM is becoming a very popular tool in computerized language processing tasks, with the capability to analyze complicated linguistic patterns and provide relevant and appropriate responses depending on the context. While offering significant advantages, these models are also vulnerable to security and privacy attacks, such as jailbreaking attacks, data poisoning attacks, and Personally Identifiable Information (PII) leakage attacks. This survey provides a thorough review of the security and privacy challenges of LLMs for both training data and users, along with the application-based risks in various domains, such as transportation, education, and healthcare. We assess the extent of LLM vulnerabilities, investigate emerging security and privacy attacks for LLMs, and review the potent
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#31232;&#30095;&#24494;&#35843;&#26041;&#27861;&#25193;&#23637;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#24494;&#35843;&#31639;&#27861;SpIEL&#65292;&#24182;&#23545;LLM&#36827;&#34892;&#20102;&#25351;&#20196;&#24494;&#35843;&#65292;&#20197;&#35299;&#20915;&#20854;&#21442;&#25968;&#24222;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2401.16405</link><description>&lt;p&gt;
&#23558;&#31232;&#30095;&#24494;&#35843;&#25193;&#23637;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling Sparse Fine-Tuning to Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2401.16405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#31232;&#30095;&#24494;&#35843;&#26041;&#27861;&#25193;&#23637;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#24494;&#35843;&#31639;&#27861;SpIEL&#65292;&#24182;&#23545;LLM&#36827;&#34892;&#20102;&#25351;&#20196;&#24494;&#35843;&#65292;&#20197;&#35299;&#20915;&#20854;&#21442;&#25968;&#24222;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30001;&#20110;&#20854;&#21442;&#25968;&#30340;&#24222;&#22823;&#25968;&#37327;&#65292;&#24456;&#38590;&#23436;&#20840;&#36827;&#34892;&#24494;&#35843;&#65288;&#20363;&#22914;&#20351;&#29992;&#25351;&#20196;&#25110;&#20154;&#24037;&#21453;&#39304;&#65289;&#12290;&#19968;&#31995;&#21015;&#21442;&#25968;&#39640;&#25928;&#30340;&#31232;&#30095;&#24494;&#35843;&#26041;&#27861;&#22312;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#23384;&#20648;&#38656;&#27714;&#19982;LLM&#30340;&#22823;&#23567;&#25104;&#27491;&#27604;&#22686;&#21152;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#31232;&#30095;&#24494;&#35843;&#25193;&#23637;&#21040;&#26368;&#20808;&#36827;&#30340;LLM&#65292;&#22914;LLaMA 2 7B&#21644;13B&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SpIEL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#31232;&#30095;&#24494;&#35843;&#26041;&#27861;&#65292;&#23427;&#38024;&#23545;&#25152;&#38656;&#30340;&#31232;&#30095;&#24230;&#27700;&#24179;&#65292;&#32500;&#25252;&#19968;&#20010;&#21442;&#25968;&#32034;&#24341;&#25968;&#32452;&#21644;&#36825;&#20123;&#21442;&#25968;&#30456;&#23545;&#20110;&#39044;&#35757;&#32451;&#20540;&#30340;&#22686;&#37327;&#12290;&#23427;&#36941;&#21382;&#20197;&#19979;&#27493;&#39588;&#65306;&#65288;a&#65289;&#26356;&#26032;&#27963;&#36291;&#22686;&#37327;&#65292;&#65288;b&#65289;&#20462;&#21098;&#32034;&#24341;&#65288;&#22522;&#20110;&#20854;&#22686;&#37327;&#30340;&#21464;&#21270;&#22823;&#23567;&#65289;&#65292;&#20197;&#21450;&#65288;c&#65289;&#37325;&#26032;&#29983;&#38271;&#32034;&#24341;&#12290;&#23545;&#20110;&#37325;&#26032;&#29983;&#38271;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;&#23569;&#37327;&#20505;&#36873;&#21442;&#25968;&#30340;&#32047;&#31215;&#26799;&#24230;&#25110;&#20351;&#29992;&#39640;&#25928;&#30340;SM3&#20248;&#21270;&#22120;&#20272;&#35745;&#30340;&#36817;&#20284;&#21160;&#24046;&#30340;&#20004;&#20010;&#20934;&#21017;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#25351;&#20196;&#24494;&#35843;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are difficult to fully fine-tune (e.g., with instructions or human feedback) due to their sheer number of parameters. A family of parameter-efficient sparse fine-tuning methods have proven promising in terms of performance but their memory requirements increase proportionally to the size of the LLMs. In this work, we scale sparse fine-tuning to state-of-the-art LLMs like LLaMA 2 7B and 13B. We propose SpIEL, a novel sparse fine-tuning method which, for a desired density level, maintains an array of parameter indices and the deltas of these parameters relative to their pretrained values. It iterates over: (a) updating the active deltas, (b) pruning indices (based on the change of magnitude of their deltas) and (c) regrowth of indices. For regrowth, we explore two criteria based on either the accumulated gradients of a few candidate parameters or their approximate momenta estimated using the efficient SM3 optimizer. We experiment with instruction-tuning of LL
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#35821;&#20041;&#23398;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;LM&#28508;&#22312;&#31354;&#38388;&#30340;&#21442;&#32771;&#26694;&#26550;&#65292;&#30830;&#20445;&#22522;&#20110;LM&#35789;&#27719;&#30340;&#20998;&#31163;&#35821;&#20041;&#20998;&#26512;&#12290;&#22312;LM&#36866;&#24212;&#36807;&#31243;&#20013;&#65292;&#24341;&#20837;&#20102;&#35745;&#31639;logits&#30340;&#26032;&#25216;&#26415;&#21644;&#31070;&#32463;&#32858;&#31867;&#27169;&#22359;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25991;&#26412;&#29702;&#35299;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2401.16184</link><description>&lt;p&gt;
&#20851;&#20110;LM&#28508;&#22312;&#31354;&#38388;&#30340;&#35821;&#20041;&#23398;&#65306;&#19968;&#31181;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On the Semantics of LM Latent Space: A Vocabulary-defined Approach
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2401.16184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#35821;&#20041;&#23398;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;LM&#28508;&#22312;&#31354;&#38388;&#30340;&#21442;&#32771;&#26694;&#26550;&#65292;&#30830;&#20445;&#22522;&#20110;LM&#35789;&#27719;&#30340;&#20998;&#31163;&#35821;&#20041;&#20998;&#26512;&#12290;&#22312;LM&#36866;&#24212;&#36807;&#31243;&#20013;&#65292;&#24341;&#20837;&#20102;&#35745;&#31639;logits&#30340;&#26032;&#25216;&#26415;&#21644;&#31070;&#32463;&#32858;&#31867;&#27169;&#22359;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25991;&#26412;&#29702;&#35299;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;(LM)&#30340;&#28508;&#22312;&#31354;&#38388;&#23545;&#20110;&#25913;&#36827;&#20854;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#20998;&#26512;&#24448;&#24448;&#22312;&#25552;&#20379;&#22522;&#20110;&#27169;&#22411;&#30340;&#23545;LM&#35821;&#20041;&#30340;&#20998;&#31163;&#27934;&#23519;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#24182;&#24573;&#35270;&#20102;LM&#36866;&#24212;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#20026;&#20102;&#21709;&#24212;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#35821;&#20041;&#23398;&#65292;&#23427;&#22312;LM&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#21442;&#32771;&#26694;&#26550;&#65292;&#30830;&#20445;&#22522;&#20110;LM&#35789;&#27719;&#30340;&#20998;&#31163;&#35821;&#20041;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#20132;&#32455;&#20998;&#26512;&#65292;&#21033;&#29992;LM&#35789;&#27719;&#26469;&#33719;&#24471;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#30340;&#27934;&#23519;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;logits&#30340;&#26032;&#25216;&#26415;&#65292;&#24378;&#35843;&#21487;&#24494;&#20998;&#24615;&#21644;&#23616;&#37096;&#31561;&#36317;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#31070;&#32463;&#32858;&#31867;&#27169;&#22359;&#65292;&#29992;&#20110;&#22312;LM&#36866;&#24212;&#36807;&#31243;&#20013;&#36827;&#34892;&#35821;&#20041;&#26657;&#20934;&#12290;&#36890;&#36807;&#22312;&#22810;&#31181;&#25991;&#26412;&#29702;&#35299;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#38754;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the latent space of language models (LM) is crucial to refining their performance and interpretability. Existing analyses often fall short in providing disentangled (model-centric) insights into LM semantics, and neglect essential aspects of LM adaption. In response, we introduce a pioneering method called vocabulary-defined semantics, which establishes a reference frame within the LM latent space, ensuring disentangled semantic analysis grounded in LM vocabulary. Our approach transcends prior entangled analysis, leveraging LM vocabulary for model-centric insights. Furthermore, we propose a novel technique to compute logits, emphasising differentiability and local isotropy, and introduce a neural clustering module for semantically calibrating data representations during LM adaptation. Through extensive experiments across diverse text understanding datasets, our approach outperforms state-of-the-art methods of retrieval-augmented generation and parameter-efficient finetuni
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797; NoFunEval&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#21151;&#33021;&#24615;&#35201;&#27714;&#21644;&#31616;&#21333;&#20998;&#31867;&#23454;&#20363;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#36825;&#20123;&#35201;&#27714;&#26102;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#30450;&#28857;&#12290;</title><link>https://rss.arxiv.org/abs/2401.15963</link><description>&lt;p&gt;
NoFunEval: &#26377;&#36259;&#30340;&#26159;&#65292;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#36229;&#20986;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#35201;&#27714;&#19978;&#36935;&#21040;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
NoFunEval: Funny How Code LMs Falter on Requirements Beyond Functional Correctness
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2401.15963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797; NoFunEval&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#21151;&#33021;&#24615;&#35201;&#27714;&#21644;&#31616;&#21333;&#20998;&#31867;&#23454;&#20363;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#36825;&#20123;&#35201;&#27714;&#26102;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#30450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;code LMs&#65289;&#30340;&#35780;&#20272;&#22522;&#20934;&#20960;&#20046;&#23436;&#20840;&#38598;&#20013;&#22312;LMs&#26159;&#21542;&#33021;&#22815;&#29983;&#25104;&#21151;&#33021;&#27491;&#30830;&#30340;&#20195;&#30721;&#19978;&#12290;&#22312;&#23454;&#38469;&#30340;&#36719;&#20214;&#24037;&#31243;&#20013;&#65292;&#24320;&#21457;&#20154;&#21592;&#20250;&#32771;&#34385;&#36229;&#20986;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#35201;&#27714;&#12290;&#20182;&#20204;&#23545;&#20110;&#8220;&#22914;&#20309;&#8221;&#23454;&#29616;&#21151;&#33021;&#26377;&#30528;&#23545;&#25972;&#20307;&#31995;&#32479;&#35774;&#35745;&#30446;&#26631;&#65288;&#22914;&#25928;&#29575;&#12289;&#23433;&#20840;&#24615;&#21644;&#21487;&#32500;&#25252;&#24615;&#65289;&#30340;&#35201;&#27714;&#12290;&#22914;&#26524;LMs&#33021;&#22815;&#23637;&#31034;&#23545;&#35201;&#27714;&#21644;&#20195;&#30721;&#35821;&#20041;&#30340;&#24378;&#22823;&#29702;&#35299;&#33021;&#21147;&#65292;&#20182;&#20204;&#20063;&#20250;&#26356;&#21152;&#20449;&#20219;&#36825;&#20123;LMs&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;NoFunEval&#26469;&#35780;&#20272;&#20195;&#30721;LMs&#22312;&#38750;&#21151;&#33021;&#24615;&#35201;&#27714;&#21644;&#31616;&#21333;&#20998;&#31867;&#23454;&#20363;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#26041;&#27861;Coding Concepts (CoCo)&#65292;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#20154;&#21592;&#21521;LMs&#20256;&#36798;&#39046;&#22495;&#30693;&#35782;&#12290;&#25105;&#20204;&#23545;22&#20010;&#20195;&#30721;LMs&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#26222;&#36941;&#34920;&#29616;&#19981;&#20339;&#65292;&#26263;&#31034;&#30528;&#23427;&#20204;&#22312;&#22788;&#29702;&#36825;&#20123;&#38382;&#39064;&#26102;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#30450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing evaluation benchmarks of language models of code (code LMs) focus almost exclusively on whether the LMs can generate functionally-correct code. In real-world software engineering, developers think beyond functional correctness. They have requirements on "how" a functionality should be implemented to meet overall system design objectives like efficiency, security, and maintainability. They would also trust the code LMs more if the LMs demonstrate robust understanding of requirements and code semantics.   We propose a new benchmark NoFunEval to evaluate code LMs on non-functional requirements and simple classification instances for both functional and non-functional requirements. We propose a prompting method, Coding Concepts (CoCo), as a way for a developer to communicate the domain knowledge to the LMs. We conduct an extensive evaluation of twenty-two code LMs. Our finding is that they generally falter when tested on our benchmark, hinting at fundamental blindspots in their tr
&lt;/p&gt;</description></item><item><title>SuperCLUE-Math6 &#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#20998;&#32423;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22686;&#21152;&#38590;&#24230;&#12289;&#22810;&#26679;&#24615;&#21644;&#24212;&#29992;&#33539;&#22260;&#65292;&#25552;&#20379;&#20102;2000&#22810;&#20010;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#21019;&#26032;&#26041;&#27861;&#26469;&#37327;&#21270;&#22823;&#22411;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#25512;&#29702;&#27700;&#24179;&#20998;&#23618;&#65292;&#39030;&#32423;&#27169;&#22411;&#22914;GPT-4&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;SC-Math6&#22635;&#34917;&#20102;&#20013;&#25991;&#25968;&#23398;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#31354;&#30333;&#65292;&#24182;&#25512;&#36827;&#20102;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#21270;&#12290;</title><link>https://rss.arxiv.org/abs/2401.11819</link><description>&lt;p&gt;
SuperCLUE-Math6&#65306;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#36923;&#36753;&#22810;&#27493;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#20998;&#32423;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SuperCLUE-Math6: Graded Multi-Step Math Reasoning Benchmark for LLMs in Chinese
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2401.11819
&lt;/p&gt;
&lt;p&gt;
SuperCLUE-Math6 &#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#20998;&#32423;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22686;&#21152;&#38590;&#24230;&#12289;&#22810;&#26679;&#24615;&#21644;&#24212;&#29992;&#33539;&#22260;&#65292;&#25552;&#20379;&#20102;2000&#22810;&#20010;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#21019;&#26032;&#26041;&#27861;&#26469;&#37327;&#21270;&#22823;&#22411;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#25512;&#29702;&#27700;&#24179;&#20998;&#23618;&#65292;&#39030;&#32423;&#27169;&#22411;&#22914;GPT-4&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;SC-Math6&#22635;&#34917;&#20102;&#20013;&#25991;&#25968;&#23398;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#31354;&#30333;&#65292;&#24182;&#25512;&#36827;&#20102;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;SuperCLUE-Math6&#65288;SC-Math6&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;SC-Math6&#26159;GSM8K&#25968;&#25454;&#38598;&#30340;&#21319;&#32423;&#29256;&#26412;&#65292;&#22686;&#21152;&#20102;&#38590;&#24230;&#12289;&#22810;&#26679;&#24615;&#21644;&#24212;&#29992;&#33539;&#22260;&#12290;&#23427;&#21253;&#21547;&#20102;2000&#22810;&#20010;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#24182;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#35299;&#20915;&#26041;&#26696;&#30340;&#25968;&#23398;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#22823;&#22411;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22522;&#20110;&#19981;&#21516;&#25512;&#29702;&#27493;&#39588;&#30340;&#38382;&#39064;&#34920;&#29616;&#12290;&#23545;13&#20010;&#20195;&#34920;&#24615;&#30340;&#20013;&#25991;&#27169;&#22411;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#25512;&#29702;&#27700;&#24179;&#20998;&#23618;&#65292;&#39030;&#32423;&#27169;&#22411;&#22914;GPT-4&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;SC-Math6&#22635;&#34917;&#20102;&#20013;&#25991;&#25968;&#23398;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#31354;&#30333;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#27979;&#35797;&#24179;&#21488;&#26469;&#25512;&#36827;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce SuperCLUE-Math6(SC-Math6), a new benchmark dataset to evaluate the mathematical reasoning abilities of Chinese language models. SC-Math6 is designed as an upgraded Chinese version of the GSM8K dataset with enhanced difficulty, diversity, and application scope. It consists of over 2000 mathematical word problems requiring multi-step reasoning and providing natural language solutions. We propose an innovative scheme to quantify the reasoning capability of large models based on performance over problems with different reasoning steps. Experiments on 13 representative Chinese models demonstrate a clear stratification of reasoning levels, with top models like GPT-4 showing superior performance. SC-Math6 fills the gap in Chinese mathematical reasoning benchmarks and provides a comprehensive testbed to advance the intelligence of Chinese language models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#22312;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#23545;&#20110;&#19981;&#21516;&#29983;&#25104;&#22120;&#21644;&#39046;&#22495;&#20135;&#29983;&#30340;&#25991;&#26412;&#23384;&#22312;&#20005;&#37325;&#38480;&#21046;&#12290;</title><link>https://rss.arxiv.org/abs/2401.09407</link><description>&lt;p&gt;
&#35299;&#35835;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;: &#36890;&#36807;&#22823;&#35268;&#27169;&#35821;&#35328;&#35821;&#20041;&#30340;&#24191;&#20041;&#31574;&#30053;&#26469;&#26816;&#27979;&#20154;&#31867;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Deciphering Textual Authenticity: A Generalized Strategy through the Lens of Large Language Semantics for Detecting Human vs. Machine-Generated Text
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2401.09407
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#22312;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#23545;&#20110;&#19981;&#21516;&#29983;&#25104;&#22120;&#21644;&#39046;&#22495;&#20135;&#29983;&#30340;&#25991;&#26412;&#23384;&#22312;&#20005;&#37325;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#23545;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#24037;&#20855;&#30340;&#38656;&#27714;&#26085;&#30410;&#22686;&#38271;&#12290;&#26377;&#25928;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#38754;&#20020;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;: &#39318;&#20808;&#65292;&#20182;&#20204;&#22312;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#26102;&#38754;&#20020;&#30528;&#26497;&#22823;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#22330;&#26223;&#20013;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26159;&#30001;&#21508;&#31181;&#29983;&#25104;&#22120;&#20135;&#29983;&#30340;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;GPT-4&#21644;Dolly&#65292;&#24182;&#28085;&#30422;&#21508;&#31181;&#39046;&#22495;&#65292;&#20174;&#23398;&#26415;&#25163;&#31295;&#21040;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#23558;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#35270;&#20026;&#20005;&#26684;&#30340;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#65292;&#24573;&#30053;&#20102;&#19981;&#21516;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#22810;&#26679;&#24615;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#20013;&#19981;&#21516;&#29983;&#25104;&#22120;&#21644;&#39046;&#22495;&#20135;&#29983;&#30340;&#25991;&#26412;&#26102;&#21463;&#21040;&#20005;&#37325;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the recent proliferation of Large Language Models (LLMs), there has been an increasing demand for tools to detect machine-generated text. The effective detection of machine-generated text face two pertinent problems: First, they are severely limited in generalizing against real-world scenarios, where machine-generated text is produced by a variety of generators, including but not limited to GPT-4 and Dolly, and spans diverse domains, ranging from academic manuscripts to social media posts. Second, existing detection methodologies treat texts produced by LLMs through a restrictive binary classification lens, neglecting the nuanced diversity of artifacts generated by different LLMs. In this work, we undertake a systematic study on the detection of machine-generated text in real-world scenarios. We first study the effectiveness of state-of-the-art approaches and find that they are severely limited against text produced by diverse generators and domains in the real world. Furthermore,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28608;&#27963;&#26631;&#24535;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21387;&#32553;LLM&#30340;&#28608;&#27963;&#29366;&#24577;&#65292;&#20351;&#20854;&#33021;&#22815;&#20197;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#24863;&#30693;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;LLM&#22312;&#30701;&#19978;&#19979;&#25991;&#20013;&#30340;&#21407;&#22987;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#20869;&#23384;&#21644;&#26102;&#38388;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#22810;&#26679;&#21270;&#35757;&#32451;&#26377;&#25928;&#22320;&#25903;&#25345;&#19981;&#21516;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;</title><link>https://rss.arxiv.org/abs/2401.03462</link><description>&lt;p&gt;
&#20174;4K&#21040;400K&#30340;&#39134;&#36291;&#65306;&#21033;&#29992;&#28608;&#27963;&#26631;&#24535;&#25193;&#23637;LLM&#30340;&#19978;&#19979;&#25991;
&lt;/p&gt;
&lt;p&gt;
Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2401.03462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28608;&#27963;&#26631;&#24535;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21387;&#32553;LLM&#30340;&#28608;&#27963;&#29366;&#24577;&#65292;&#20351;&#20854;&#33021;&#22815;&#20197;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#24863;&#30693;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;LLM&#22312;&#30701;&#19978;&#19979;&#25991;&#20013;&#30340;&#21407;&#22987;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#20869;&#23384;&#21644;&#26102;&#38388;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#22810;&#26679;&#21270;&#35757;&#32451;&#26377;&#25928;&#22320;&#25903;&#25345;&#19981;&#21516;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#19978;&#19979;&#25991;&#30340;&#21033;&#29992;&#23545;&#20110;LLM&#26469;&#35828;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#12290;&#23613;&#31649;&#36890;&#36807;&#24494;&#35843;&#21487;&#20197;&#25193;&#23637;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#20294;&#36825;&#20250;&#23548;&#33268;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#30340;&#26174;&#33879;&#25104;&#26412;&#65292;&#24182;&#23545;LLM&#30340;&#21407;&#22987;&#33021;&#21147;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28608;&#27963;&#26631;&#24535;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;LLM&#30340;&#21407;&#22987;&#28608;&#27963;&#21387;&#32553;&#25104;&#32039;&#20945;&#30340;&#24418;&#24335;&#65292;&#20351;LLM&#33021;&#22815;&#20197;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#24863;&#30693;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#12290;&#28608;&#27963;&#26631;&#24535;&#34987;&#24341;&#20837;&#20026;&#25554;&#20214;&#27169;&#22359;&#65292;&#23436;&#20840;&#20445;&#30041;&#20102;LLM&#22312;&#30701;&#19978;&#19979;&#25991;&#20013;&#30340;&#21407;&#22987;&#33021;&#21147;&#12290;&#23427;&#19982;&#28369;&#21160;&#31383;&#21475;&#19968;&#36215;&#23454;&#26102;&#22788;&#29702;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#20013;&#23454;&#29616;&#20102;&#31454;&#20105;&#21147;&#30340;&#20869;&#23384;&#21644;&#26102;&#38388;&#25928;&#29575;&#12290;&#28608;&#27963;&#26631;&#24535;&#26159;&#36890;&#36807;&#22810;&#26679;&#21270;&#21387;&#32553;&#27604;&#30340;&#30701;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#24471;&#30410;&#20110;&#36825;&#31181;&#22788;&#29702;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#25903;&#25345;&#19981;&#21516;&#19978;&#19979;&#25991;&#38271;&#24230;&#65292;&#23454;&#29616;&#23567;&#35268;&#27169;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The utilization of long contexts poses a big challenge for LLMs due to their limited context window size. Although the context window can be extended through fine-tuning, it will result in a considerable cost at both training and inference time, and exert an unfavorable impact to the LLM's original capabilities. In this work, we propose a new method called Activation Beacon, which condenses LLM's raw activations into compact forms such that the LLM can perceive a longer context with a limited context window. Activation Beacon is introduced as a plug-in module, which fully preserves the LLM's original capability in short contexts. It works with the sliding window to streamingly process the long context, which leads to a competitive memory and time efficiency in both training and inference. Activation Beacon is trained with short-sequence data of diversified condensing ratios. Thanks to such a treatment, it can be effectively learned to support different context lengths with a small trai
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#30340;&#25439;&#22833;&#23574;&#23792;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25214;&#20986;&#20102;&#26799;&#24230;&#29190;&#28856;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#28385;&#36275;&#35201;&#27714;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#38450;&#27490;&#23574;&#23792;&#30340;&#21457;&#29983;&#12290;</title><link>https://rss.arxiv.org/abs/2312.16903</link><description>&lt;p&gt;
&#21035;&#20877;&#20986;&#29616;&#23574;&#23792;&#20102;&#65306;&#31283;&#23450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Spike No More: Stabilizing the Pre-training of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.16903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#30340;&#25439;&#22833;&#23574;&#23792;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25214;&#20986;&#20102;&#26799;&#24230;&#29190;&#28856;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#28385;&#36275;&#35201;&#27714;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#38450;&#27490;&#23574;&#23792;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#32463;&#24120;&#20986;&#29616;&#25439;&#22833;&#23574;&#23792;&#12290;&#36825;&#20123;&#23574;&#23792;&#20250;&#38477;&#20302;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26377;&#26102;&#20250;&#30772;&#22351;&#39044;&#35757;&#32451;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#25105;&#20204;&#24212;&#35813;&#36991;&#20813;&#36825;&#31181;&#23574;&#23792;&#30340;&#20986;&#29616;&#12290;&#20026;&#20102;&#30740;&#31350;&#25439;&#22833;&#23574;&#23792;&#30340;&#21407;&#22240;&#65292;&#25105;&#20204;&#20851;&#27880;&#20869;&#37096;&#23618;&#30340;&#26799;&#24230;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#26799;&#24230;&#29190;&#28856;&#30340;&#20004;&#20010;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#20102;&#39044;&#38450;&#26799;&#24230;&#29190;&#28856;&#30340;&#35201;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32452;&#21512;&#21021;&#22987;&#21270;&#26041;&#27861;&#21644;&#23545;&#23884;&#20837;&#36827;&#34892;&#31616;&#21333;&#20462;&#25913;&#26469;&#28385;&#36275;&#35201;&#27714;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36825;&#31181;&#32452;&#21512;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#38450;&#27490;&#23574;&#23792;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Loss spikes often occur during pre-training of large language models. The spikes degrade the performance of large language models and sometimes ruin the pre-training. Since the pre-training needs a vast computational budget, we should avoid such spikes. To investigate the cause of loss spikes, we focus on gradients of internal layers. Through theoretical analyses, we reveal two causes of the exploding gradients, and provide requirements to prevent the explosion. In addition, we propose a method to satisfy the requirements by combining the initialization method and a simple modification to embeddings. We conduct various experiments to verify our theoretical analyses empirically. Experimental results indicate that the combination is effective in preventing spikes during pre-training.
&lt;/p&gt;</description></item><item><title>DSPy&#24341;&#20837;&#20102;LM&#26029;&#35328;&#65292;&#29992;&#20110;&#34920;&#36798;&#35821;&#35328;&#27169;&#22411;&#24212;&#28385;&#36275;&#30340;&#35745;&#31639;&#32422;&#26463;&#12290;&#22312;&#22235;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;LM&#26029;&#35328;&#19981;&#20165;&#25552;&#39640;&#20102;&#23545;&#35268;&#21017;&#30340;&#36981;&#23432;&#65292;&#32780;&#19988;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#22686;&#21152;&#20102;&#23545;&#32422;&#26463;&#30340;&#25509;&#21463;&#27425;&#25968;&#24182;&#29983;&#25104;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#22238;&#22797;&#12290;</title><link>https://rss.arxiv.org/abs/2312.13382</link><description>&lt;p&gt;
DSPy&#26029;&#35328;&#65306;&#29992;&#20110;&#33258;&#25105;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#27969;&#27700;&#32447;&#30340;&#35745;&#31639;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.13382
&lt;/p&gt;
&lt;p&gt;
DSPy&#24341;&#20837;&#20102;LM&#26029;&#35328;&#65292;&#29992;&#20110;&#34920;&#36798;&#35821;&#35328;&#27169;&#22411;&#24212;&#28385;&#36275;&#30340;&#35745;&#31639;&#32422;&#26463;&#12290;&#22312;&#22235;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;LM&#26029;&#35328;&#19981;&#20165;&#25552;&#39640;&#20102;&#23545;&#35268;&#21017;&#30340;&#36981;&#23432;&#65292;&#32780;&#19988;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#22686;&#21152;&#20102;&#23545;&#32422;&#26463;&#30340;&#25509;&#21463;&#27425;&#25968;&#24182;&#29983;&#25104;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#35843;&#29992;&#20316;&#20026;&#21487;&#32452;&#21512;&#27169;&#22359;&#30340;&#38142;&#24335;&#32534;&#31243;&#26041;&#24335;&#27491;&#22312;&#25512;&#21160;&#19968;&#31181;&#26032;&#30340;&#32534;&#31243;&#26041;&#24335;&#65292;&#20294;&#30830;&#20445;LM&#36981;&#23432;&#37325;&#35201;&#32422;&#26463;&#38656;&#35201;&#21551;&#21457;&#24335;&#30340;&#8220;&#25552;&#31034;&#24037;&#31243;&#8221;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;LM&#26029;&#35328;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#34920;&#36798;LM&#24212;&#28385;&#36275;&#30340;&#35745;&#31639;&#32422;&#26463;&#30340;&#32534;&#31243;&#32467;&#26500;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26500;&#25972;&#21512;&#21040;&#26368;&#36817;&#30340;DSPy LM&#32534;&#31243;&#27169;&#22411;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#31574;&#30053;&#65292;&#20351;&#24471;DSPy&#33021;&#22815;&#23558;&#24102;&#26377;LM&#26029;&#35328;&#30340;&#31243;&#24207;&#32534;&#35793;&#20026;&#26356;&#21487;&#38752;&#21644;&#20934;&#30830;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#22312;&#25512;&#26029;&#26102;&#20351;&#29992;&#26029;&#35328;&#36827;&#34892;&#33258;&#21160;&#33258;&#25105;&#20462;&#22797;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#22235;&#20010;&#19981;&#21516;&#30340;&#25991;&#26412;&#29983;&#25104;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;LM&#26029;&#35328;&#19981;&#20165;&#25913;&#21892;&#20102;&#23545;&#35268;&#21017;&#30340;&#36981;&#23432;&#65292;&#32780;&#19988;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#25509;&#21463;&#32422;&#26463;&#30340;&#27425;&#25968;&#22686;&#21152;&#20102;164&#65285;&#65292;&#29983;&#25104;&#20102;37&#65285;&#26356;&#39640;&#36136;&#37327;&#30340;&#22238;&#22797;&#12290;&#25105;&#20204;&#30340;LM&#26029;&#35328;&#21442;&#32771;&#23454;&#29616;&#24050;&#38598;&#25104;&#21040;DSPy&#20013;&#65292;&#32593;&#22336;&#20026;https://github.com/stanfordnlp/dspy
&lt;/p&gt;
&lt;p&gt;
Chaining language model (LM) calls as composable modules is fueling a new way of programming, but ensuring LMs adhere to important constraints requires heuristic "prompt engineering". We introduce LM Assertions, a programming construct for expressing computational constraints that LMs should satisfy. We integrate our constructs into the recent DSPy programming model for LMs, and present new strategies that allow DSPy to compile programs with LM Assertions into more reliable and accurate systems. We also propose strategies to use assertions at inference time for automatic self-refinement with LMs. We report on four diverse case studies for text generation and find that LM Assertions improve not only compliance with imposed rules but also downstream task performance, passing constraints up to 164% more often and generating up to 37% more higher-quality responses. Our reference implementation of LM Assertions is integrated into DSPy at https://github.com/stanfordnlp/dspy
&lt;/p&gt;</description></item><item><title>CBQ&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#22359;&#37325;&#26500;&#22411;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12290;CBQ&#36890;&#36807;&#20351;&#29992;&#21516;&#28304;&#37325;&#26500;&#26041;&#26696;&#26469;&#24314;&#31435;&#22359;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#26368;&#23567;&#21270;&#35823;&#24046;&#31215;&#32047;&#12290;CBQ&#36824;&#37319;&#29992;&#20102;&#31895;&#21040;&#31934;&#30340;&#39044;&#22788;&#29702;&#31574;&#30053;&#21644;&#33258;&#36866;&#24212;&#30340;&#21462;&#25972;&#25216;&#26415;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26497;&#31471;&#24322;&#24120;&#20540;&#24182;&#25552;&#39640;&#25972;&#20307;&#37327;&#21270;&#31934;&#24230;&#12290;</title><link>https://rss.arxiv.org/abs/2312.07950</link><description>&lt;p&gt;
&#36328;&#22359;&#37327;&#21270;&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#22359;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CBQ: Cross-Block Quantization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.07950
&lt;/p&gt;
&lt;p&gt;
CBQ&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#22359;&#37325;&#26500;&#22411;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12290;CBQ&#36890;&#36807;&#20351;&#29992;&#21516;&#28304;&#37325;&#26500;&#26041;&#26696;&#26469;&#24314;&#31435;&#22359;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#26368;&#23567;&#21270;&#35823;&#24046;&#31215;&#32047;&#12290;CBQ&#36824;&#37319;&#29992;&#20102;&#31895;&#21040;&#31934;&#30340;&#39044;&#22788;&#29702;&#31574;&#30053;&#21644;&#33258;&#36866;&#24212;&#30340;&#21462;&#25972;&#25216;&#26415;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26497;&#31471;&#24322;&#24120;&#20540;&#24182;&#25552;&#39640;&#25972;&#20307;&#37327;&#21270;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#22312;&#20197;&#26497;&#20302;&#25104;&#26412;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PTQ&#26041;&#27861;&#21482;&#20851;&#27880;&#22788;&#29702;&#21333;&#20010;&#23618;&#25110;&#21333;&#20010;&#22359;&#20869;&#30340;&#24322;&#24120;&#20540;&#65292;&#24573;&#30053;&#20102;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22312;&#20302;&#20301;&#35774;&#32622;&#20013;&#23548;&#33268;&#20005;&#37325;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22359;&#38388;&#37325;&#26500;&#30340;&#36328;&#22359;PTQ&#26041;&#27861;CBQ&#12290;CBQ&#37319;&#29992;&#20102;&#19968;&#31181;&#21516;&#28304;&#37325;&#26500;&#26041;&#26696;&#26469;&#23454;&#29616;&#22359;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#26368;&#23567;&#21270;&#35823;&#24046;&#31215;&#32047;&#12290;&#27492;&#22806;&#65292;CBQ&#36824;&#32467;&#21512;&#20102;&#19968;&#31181;&#31895;&#21040;&#31934;&#30340;&#39044;&#22788;&#29702;&#31574;&#30053;&#65288;CFP&#65289;&#26469;&#25233;&#21046;&#26435;&#37325;&#21644;&#28608;&#27963;&#20540;&#30340;&#24322;&#24120;&#20540;&#65292;&#24182;&#37197;&#21512;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;LoRA&#21462;&#25972;&#25216;&#26415;&#23454;&#29616;&#31934;&#30830;&#30340;&#26435;&#37325;&#37327;&#21270;&#12290;&#36825;&#20123;&#21019;&#26032;&#20351;CBQ&#19981;&#20165;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26497;&#31471;&#24322;&#24120;&#20540;&#65292;&#36824;&#33021;&#25552;&#39640;&#25972;&#20307;&#37327;&#21270;&#31934;&#24230;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;CBQ&#22312;&#20302;&#20301;&#37327;&#21270;&#65288;W4A4&#65292;W4A8&#31561;&#65289;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (PTQ) has played a key role in compressing large language models (LLMs) with ultra-low costs. However, existing PTQ methods only focus on handling the outliers within one layer or one block, which ignores the dependency of blocks and leads to severe performance degradation in low-bit settings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ method for LLMs. CBQ employs a cross-block dependency using a homologous reconstruction scheme, establishing long-range dependencies across multiple blocks to minimize error accumulation. Furthermore, CBQ incorporates a coarse-to-fine preprocessing (CFP) strategy for suppressing weight and activation outliers, coupled with an adaptive LoRA-Rounding technique for precise weight quantization. These innovations enable CBQ to not only handle extreme outliers effectively but also improve overall quantization accuracy. Extensive experiments show that CBQ achieves superior low-bit quantization (W4A4, W4A8, W
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#20803;&#23618;&#38754;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32534;&#30721;&#20219;&#21153;&#20013;&#20462;&#34917;LLM&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;API&#24207;&#21015;&#25512;&#33616;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#20266;&#20195;&#30721;&#21040;&#20195;&#30721;&#36716;&#25442;&#31561;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#21644;&#35780;&#20272;&#12290;</title><link>https://rss.arxiv.org/abs/2312.05356</link><description>&lt;p&gt;
Neuron Patching: &#31070;&#32463;&#20803;&#23618;&#38754;&#30340;&#27169;&#22411;&#32534;&#36753;&#19982;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Neuron Patching: Neuron-level Model Editing on Code Generation and LLMs
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.05356
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#20803;&#23618;&#38754;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32534;&#30721;&#20219;&#21153;&#20013;&#20462;&#34917;LLM&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;API&#24207;&#21015;&#25512;&#33616;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#20266;&#20195;&#30721;&#21040;&#20195;&#30721;&#36716;&#25442;&#31561;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#24471;&#21040;&#20102;&#25104;&#21151;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#12290;&#26356;&#26032;&#36825;&#20123;&#27169;&#22411;&#30340;&#26032;&#30693;&#35782;&#38750;&#24120;&#26114;&#36149;&#65292;&#36890;&#24120;&#38656;&#35201;&#20840;&#38754;&#23454;&#29616;&#20854;&#20215;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;MENT&#65292;&#29992;&#20110;&#22312;&#32534;&#30721;&#20219;&#21153;&#20013;&#20462;&#34917;LLM&#27169;&#22411;&#12290;&#22522;&#20110;&#29983;&#25104;&#24335;LLM&#30340;&#26426;&#21046;&#65292;MENT&#21487;&#20197;&#22312;&#39044;&#27979;&#19979;&#19968;&#20010;&#20196;&#29260;&#26102;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#65292;&#24182;&#36827;&#19968;&#27493;&#25903;&#25345;&#24120;&#35265;&#30340;&#32534;&#30721;&#20219;&#21153;&#12290;MENT&#20855;&#26377;&#39640;&#25928;&#12289;&#26377;&#25928;&#21644;&#21487;&#38752;&#30340;&#29305;&#28857;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#20462;&#34917;1&#25110;2&#20010;&#31070;&#32463;&#20803;&#26469;&#32416;&#27491;&#31070;&#32463;&#27169;&#22411;&#12290;&#20316;&#20026;&#31070;&#32463;&#20803;&#23618;&#38754;&#19978;&#29983;&#25104;&#27169;&#22411;&#32534;&#36753;&#30340;&#20808;&#39537;&#24037;&#20316;&#65292;&#25105;&#20204;&#35268;&#33539;&#20102;&#32534;&#36753;&#36807;&#31243;&#24182;&#20171;&#32461;&#20102;&#30456;&#20851;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26032;&#30340;&#34913;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#29992;&#20110;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#32534;&#30721;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;API&#24207;&#21015;&#25512;&#33616;&#12289;&#34892;&#32423;&#20195;&#30721;&#29983;&#25104;&#21644;&#20266;&#20195;&#30721;&#21040;&#20195;&#30721;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are successfully adopted in software engineering, especially in code generation. Updating these models with new knowledge is very expensive, and is often required to fully realize their value. In this paper, we propose a novel and effective model editing approach, \textsc{MENT}, to patch LLMs in coding tasks. Based on the mechanism of generative LLMs, \textsc{MENT} enables model editing in next-token predictions, and further supports common coding tasks. \textsc{MENT} is effective, efficient, and reliable. It can correct a neural model by patching 1 or 2 neurons. As the pioneer work on neuron-level model editing of generative models, we formalize the editing process and introduce the involved concepts. Besides, we also introduce new measures to evaluate its generalization ability, and build a benchmark for further study. Our approach is evaluated on three coding tasks, including API-seq recommendation, line-level code generation, and pseudocode-to-code transaction
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#23545;&#22312;&#22270;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#30740;&#31350;&#20102;&#32431;&#22270;&#24418;&#12289;&#25991;&#26412;&#23646;&#24615;&#22270;&#24418;&#21644;&#25991;&#26412;&#37197;&#23545;&#22270;&#24418;&#19977;&#20010;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#24182;&#25506;&#35752;&#20102;&#22522;&#20110;&#22270;&#24418;&#30340;&#25512;&#29702;&#33021;&#21147;&#26159;&#21542;&#21487;&#20197;&#25512;&#24191;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#12290;</title><link>https://rss.arxiv.org/abs/2312.02783</link><description>&lt;p&gt;
&#22312;&#22270;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Models on Graphs: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.02783
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23545;&#22312;&#22270;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#30740;&#31350;&#20102;&#32431;&#22270;&#24418;&#12289;&#25991;&#26412;&#23646;&#24615;&#22270;&#24418;&#21644;&#25991;&#26412;&#37197;&#23545;&#22270;&#24418;&#19977;&#20010;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#24182;&#25506;&#35752;&#20102;&#22522;&#20110;&#22270;&#24418;&#30340;&#25512;&#29702;&#33021;&#21147;&#26159;&#21542;&#21487;&#20197;&#25512;&#24191;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT4&#21644;LLaMA&#65292;&#30001;&#20110;&#20854;&#24378;&#22823;&#30340;&#25991;&#26412;&#32534;&#30721;/&#35299;&#30721;&#33021;&#21147;&#21644;&#26032;&#21457;&#29616;&#30340;&#32039;&#24613;&#33021;&#21147;&#65288;&#20363;&#22914;&#25512;&#29702;&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#34429;&#28982;LLMs&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#32431;&#25991;&#26412;&#65292;&#20294;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#25991;&#26412;&#25968;&#25454;&#19982;&#22270;&#24418;&#24418;&#24335;&#30340;&#20016;&#23500;&#32467;&#26500;&#20449;&#24687;&#30456;&#20851;&#32852;&#65288;&#20363;&#22914;&#23398;&#26415;&#32593;&#32476;&#21644;&#30005;&#23376;&#21830;&#21153;&#32593;&#32476;&#65289;&#65292;&#25110;&#32773;&#22270;&#24418;&#25968;&#25454;&#19982;&#20016;&#23500;&#30340;&#25991;&#26412;&#20449;&#24687;&#37197;&#23545;&#65288;&#20363;&#22914;&#24102;&#26377;&#25551;&#36848;&#30340;&#20998;&#23376;&#65289;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;LLMs&#24050;&#32463;&#23637;&#31034;&#20102;&#20854;&#22522;&#20110;&#32431;&#25991;&#26412;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#23578;&#26410;&#25506;&#32034;&#27492;&#31867;&#33021;&#21147;&#26159;&#21542;&#21487;&#20197;&#25512;&#24191;&#21040;&#22270;&#24418;&#19978;&#65288;&#21363;&#22522;&#20110;&#22270;&#24418;&#30340;&#25512;&#29702;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#22312;&#22270;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#22330;&#26223;&#21644;&#25216;&#26415;&#36827;&#34892;&#20102;&#31995;&#32479;&#22238;&#39038;&#12290;&#25105;&#20204;&#39318;&#20808;&#24635;&#32467;&#20102;&#37319;&#29992;LLMs&#22312;&#22270;&#24418;&#19978;&#30340;&#28508;&#22312;&#22330;&#26223;&#65292;&#20998;&#20026;&#32431;&#22270;&#24418;&#12289;&#25991;&#26412;&#23646;&#24615;&#22270;&#24418;&#21644;&#25991;&#26412;&#37197;&#23545;&#22270;&#24418;&#19977;&#20010;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data is paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-attributed graphs, and text-pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#20307;&#30340;&#23433;&#20840;&#24615;&#65292;&#20174;&#26234;&#33021;&#20307;&#25968;&#37327;&#12289;&#35282;&#33394;&#23450;&#20041;&#21644;&#25915;&#20987;&#27700;&#24179;&#19977;&#20010;&#35282;&#24230;&#36827;&#34892;&#20102;&#25506;&#35752;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#8220;&#24694;&#24847;&#22825;&#25165;&#8221;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#19982;&#21407;&#22987;&#35282;&#33394;&#30456;&#20851;&#30340;&#25552;&#31034;&#65292;&#26469;&#27979;&#35797;&#22312;&#19981;&#21516;&#35282;&#33394;&#23450;&#20041;&#21644;&#25915;&#20987;&#27700;&#24179;&#19979;&#30340;&#24433;&#21709;&#12290;</title><link>https://rss.arxiv.org/abs/2311.11855</link><description>&lt;p&gt;
&#24694;&#24847;&#22825;&#25165;&#65306;&#25506;&#32034;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#20307;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evil Geniuses: Delving into the Safety of LLM-based Agents
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2311.11855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#20307;&#30340;&#23433;&#20840;&#24615;&#65292;&#20174;&#26234;&#33021;&#20307;&#25968;&#37327;&#12289;&#35282;&#33394;&#23450;&#20041;&#21644;&#25915;&#20987;&#27700;&#24179;&#19977;&#20010;&#35282;&#24230;&#36827;&#34892;&#20102;&#25506;&#35752;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#8220;&#24694;&#24847;&#22825;&#25165;&#8221;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#19982;&#21407;&#22987;&#35282;&#33394;&#30456;&#20851;&#30340;&#25552;&#31034;&#65292;&#26469;&#27979;&#35797;&#22312;&#19981;&#21516;&#35282;&#33394;&#23450;&#20041;&#21644;&#25915;&#20987;&#27700;&#24179;&#19979;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#24471;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#20307;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#31867;&#20154;&#34892;&#20026;&#21644;&#21512;&#20316;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#26234;&#33021;&#20307;&#20063;&#24102;&#26469;&#20102;&#19968;&#20123;&#29420;&#29305;&#30340;&#39118;&#38505;&#65292;&#28304;&#20110;&#20132;&#20114;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#21644;&#24037;&#20855;&#30340;&#21487;&#29992;&#24615;&#12290;&#26412;&#25991;&#20174;&#26234;&#33021;&#20307;&#25968;&#37327;&#12289;&#35282;&#33394;&#23450;&#20041;&#21644;&#25915;&#20987;&#27700;&#24179;&#19977;&#20010;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#20102;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#20307;&#30340;&#23433;&#20840;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#26495;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#29992;&#20110;&#27979;&#35797;&#26234;&#33021;&#20307;&#25968;&#37327;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#20132;&#20114;&#29615;&#22659;&#21644;&#35282;&#33394;&#29305;&#24322;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#24694;&#24847;&#22825;&#25165;&#8221;(EG)&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#29983;&#25104;&#19982;&#21407;&#22987;&#35282;&#33394;&#30456;&#20851;&#30340;&#25552;&#31034;&#65292;&#20197;&#26816;&#26597;&#22312;&#21508;&#31181;&#35282;&#33394;&#23450;&#20041;&#21644;&#25915;&#20987;&#27700;&#24179;&#19979;&#30340;&#24433;&#21709;&#12290;&#24694;&#24847;&#22825;&#25165;&#21033;&#29992;&#32418;&#34013;&#23545;&#25239;&#35757;&#32451;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#25552;&#31034;&#30340;&#25915;&#20987;&#24615;&#21644;&#19982;&#21407;&#22987;&#35282;&#33394;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid advancements in large language models (LLMs) have revitalized in LLM-based agents, exhibiting impressive human-like behaviors and cooperative capabilities in various scenarios. However, these agents also bring some exclusive risks, stemming from the complexity of interaction environments and the usability of tools. This paper delves into the safety of LLM-based agents from three perspectives: agent quantity, role definition, and attack level. Specifically, we initially propose to employ a template-based attack strategy on LLM-based agents to find the influence of agent quantity. In addition, to address interaction environment and role specificity issues, we introduce Evil Geniuses (EG), an effective attack method that autonomously generates prompts related to the original role to examine the impact across various role definitions and attack levels. EG leverages Red-Blue exercises, significantly improving the generated prompt aggressiveness and similarity to original roles. Our ev
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#21307;&#23398;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21407;&#29702;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#21516;&#26102;&#22238;&#31572;&#20102;&#21307;&#23398;LLMs&#30340;&#26500;&#24314;&#12289;&#19979;&#28216;&#24615;&#33021;&#12289;&#23454;&#38469;&#24212;&#29992;&#12289;&#25361;&#25112;&#20197;&#21450;&#26356;&#22909;&#26500;&#24314;&#21644;&#21033;&#29992;&#30340;&#38382;&#39064;&#12290;&#26088;&#22312;&#20026;&#26500;&#24314;&#26377;&#25928;&#30340;&#21307;&#23398;LLMs&#25552;&#20379;&#35265;&#35299;&#21644;&#23454;&#29992;&#36164;&#28304;&#12290;</title><link>https://rss.arxiv.org/abs/2311.05112</link><description>&lt;p&gt;
&#21307;&#23398;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#26597;&#65306;&#21407;&#29702;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models in Medicine: Principles, Applications, and Challenges
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2311.05112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#21307;&#23398;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21407;&#29702;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#21516;&#26102;&#22238;&#31572;&#20102;&#21307;&#23398;LLMs&#30340;&#26500;&#24314;&#12289;&#19979;&#28216;&#24615;&#33021;&#12289;&#23454;&#38469;&#24212;&#29992;&#12289;&#25361;&#25112;&#20197;&#21450;&#26356;&#22909;&#26500;&#24314;&#21644;&#21033;&#29992;&#30340;&#38382;&#39064;&#12290;&#26088;&#22312;&#20026;&#26500;&#24314;&#26377;&#25928;&#30340;&#21307;&#23398;LLMs&#25552;&#20379;&#35265;&#35299;&#21644;&#23454;&#29992;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#30001;&#20110;&#20854;&#29702;&#35299;&#21644;&#29983;&#25104;&#20154;&#31867;&#35821;&#35328;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#20020;&#24202;&#21307;&#23398;&#20013;&#65292;LLMs&#22312;&#21327;&#21161;&#21307;&#29983;&#36827;&#34892;&#24739;&#32773;&#25252;&#29702;&#26041;&#38754;&#27491;&#22312;&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#21307;&#23398;&#20013;LLMs&#30340;&#21407;&#29702;&#12289;&#24212;&#29992;&#21644;&#38754;&#20020;&#30340;&#25361;&#25112;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#25105;&#20204;&#22238;&#31572;&#20102;&#20197;&#19979;&#20855;&#20307;&#38382;&#39064;&#65306;1&#65289;&#22914;&#20309;&#26500;&#24314;&#21307;&#23398;LLMs&#65311;2&#65289;&#20160;&#20040;&#26159;&#21307;&#23398;LLMs&#30340;&#19979;&#28216;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#65311;3&#65289;&#22312;&#29616;&#23454;&#20020;&#24202;&#23454;&#36341;&#20013;&#65292;&#22914;&#20309;&#21033;&#29992;&#21307;&#23398;LLMs&#65311;4&#65289;&#20351;&#29992;&#21307;&#23398;LLMs&#20250;&#20986;&#29616;&#21738;&#20123;&#25361;&#25112;&#65311;5&#65289;&#22914;&#20309;&#26356;&#22909;&#22320;&#26500;&#24314;&#21644;&#21033;&#29992;&#21307;&#23398;LLMs&#65311;&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;&#21307;&#23398;&#20013;LLMs&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#30340;&#35265;&#35299;&#65292;&#24182;&#20316;&#20026;&#26500;&#24314;&#26377;&#25928;&#30340;&#21307;&#23398;LLMs&#30340;&#23454;&#29992;&#36164;&#28304;&#12290;&#25105;&#20204;&#36824;&#32500;&#25252;&#24182;&#23450;&#26399;&#26356;&#26032;&#19968;&#20010;&#28165;&#21333;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as ChatGPT, have received substantial attention due to their capabilities for understanding and generating human language. LLMs in medicine to assist physicians for patient care are emerging as a promising research direction in both artificial intelligence and clinical medicine. This review provides a comprehensive overview of the principles, applications, and challenges faced by LLMs in medicine. We address the following specific questions: 1) How should medical LLMs be built? 2) What are the measures for the downstream performance of medical LLMs? 3) How should medical LLMs be utilized in real-world clinical practice? 4) What challenges arise from the use of medical LLMs? and 5) How should we better construct and utilize medical LLMs? This review aims to provide insights into the opportunities and challenges of LLMs in medicine, and serve as a practical resource for constructing effective medical LLMs. We also maintain and regularly updated list of 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#22522;&#30784;&#27169;&#22411;&#30340;&#23884;&#20837;&#34920;&#31034;&#21487;&#20197;&#26816;&#27979;&#21040;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#19988;&#22312;&#20256;&#32479;&#30340;&#27867;&#21270;&#24230;&#37327;&#19978;&#34920;&#29616;&#20986;&#20559;&#24046;&#12290;&#39044;&#35757;&#32451;&#30340;GPT-2&#27169;&#22411;&#30340;&#29305;&#24449;&#23398;&#20064;&#26080;&#27861;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#25552;&#21319;&#24615;&#33021;&#65292;&#32780;&#23545;&#20854;&#34920;&#31034;&#36827;&#34892;&#32447;&#24615;&#25506;&#27979;&#21487;&#33021;&#20248;&#20110;&#25972;&#20307;&#24494;&#35843;&#12290;</title><link>https://rss.arxiv.org/abs/2310.13836</link><description>&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#30340;&#23884;&#20837;&#34920;&#31034;&#33021;&#22815;&#26816;&#27979;&#21040;&#20998;&#24067;&#20559;&#31227;
&lt;/p&gt;
&lt;p&gt;
Foundation Model's Embedded Representations May Detect Distribution Shift
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2310.13836
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#22522;&#30784;&#27169;&#22411;&#30340;&#23884;&#20837;&#34920;&#31034;&#21487;&#20197;&#26816;&#27979;&#21040;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#19988;&#22312;&#20256;&#32479;&#30340;&#27867;&#21270;&#24230;&#37327;&#19978;&#34920;&#29616;&#20986;&#20559;&#24046;&#12290;&#39044;&#35757;&#32451;&#30340;GPT-2&#27169;&#22411;&#30340;&#29305;&#24449;&#23398;&#20064;&#26080;&#27861;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#25552;&#21319;&#24615;&#33021;&#65292;&#32780;&#23545;&#20854;&#34920;&#31034;&#36827;&#34892;&#32447;&#24615;&#25506;&#27979;&#21487;&#33021;&#20248;&#20110;&#25972;&#20307;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37319;&#26679;&#20559;&#24046;&#21487;&#33021;&#23548;&#33268;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#20043;&#38388;&#21457;&#29983;&#20998;&#24067;&#20559;&#31227;&#65292;&#20351;&#25105;&#20204;&#38590;&#20197;&#29702;&#35299;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#37492;&#20110;&#24191;&#27867;&#37319;&#29992;&#24050;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#36801;&#31227;&#23398;&#20064;&#20219;&#21153;&#30340;&#24037;&#20855;&#65292;&#36825;&#19968;&#28857;&#23588;&#20026;&#37325;&#35201;&#65292;&#32780;&#36825;&#20123;&#22522;&#30784;&#31070;&#32463;&#32593;&#32476;&#30340;&#34892;&#20026;&#20173;&#28982;&#19981;&#22826;&#28165;&#26970;&#12290;&#25105;&#20204;&#20197;Sentiment140&#25968;&#25454;&#38598;&#19978;&#30340;&#36801;&#31227;&#23398;&#20064;&#20026;&#20363;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#35768;&#22810;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#23545;Sentiment140&#30340;&#25163;&#21160;&#26631;&#27880;&#30340;&#27979;&#35797;&#38598;M&#21644;&#33258;&#21160;&#26631;&#27880;&#30340;&#35757;&#32451;&#38598;P&#20855;&#26377;&#19981;&#21516;&#30340;&#34920;&#31034;&#65292;&#35777;&#23454;&#20102;&#21457;&#29983;&#20102;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#35748;&#20026;&#22312;P&#19978;&#35757;&#32451;&#24182;&#22312;M&#19978;&#35780;&#20272;&#24615;&#33021;&#26159;&#19968;&#31181;&#26377;&#20559;&#24046;&#30340;&#27867;&#21270;&#24230;&#37327;&#12290;&#23545;&#39044;&#35757;&#32451;&#30340;GPT-2&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20174;P&#20013;&#21487;&#23398;&#24471;&#30340;&#29305;&#24449;&#19981;&#20250;&#25913;&#21892;&#65288;&#20107;&#23454;&#19978;&#20250;&#38459;&#30861;&#65289;&#22312;M&#19978;&#30340;&#24615;&#33021;&#12290;&#23545;&#39044;&#35757;&#32451;&#30340;GPT-2&#30340;&#34920;&#31034;&#36827;&#34892;&#32447;&#24615;&#25506;&#27979;&#26159;&#40065;&#26834;&#30340;&#65292;&#29978;&#33267;&#21487;&#33021;&#32988;&#36807;&#25972;&#20307;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sampling biases can cause distribution shifts between train and test datasets for supervised learning tasks, obscuring our ability to understand the generalization capacity of a model. This is especially important considering the wide adoption of pre-trained foundational neural networks -- whose behavior remains poorly understood -- for transfer learning (TL) tasks. We present a case study for TL on the Sentiment140 dataset and show that many pre-trained foundation models encode different representations of Sentiment140's manually curated test set $M$ from the automatically labeled training set $P$, confirming that a distribution shift has occurred. We argue training on $P$ and measuring performance on $M$ is a biased measure of generalization. Experiments on pre-trained GPT-2 show that the features learnable from $P$ do not improve (and in fact hamper) performance on $M$. Linear probes on pre-trained GPT-2's representations are robust and may even outperform overall fine-tuning, imply
&lt;/p&gt;</description></item><item><title>Geo-Encoder&#26159;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#22320;&#29702;&#37325;&#26032;&#25490;&#24207;&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#23558;&#20013;&#25991;&#22320;&#29702;&#35821;&#20041;&#38598;&#25104;&#21040;&#37325;&#26032;&#25490;&#24207;&#27969;&#31243;&#20013;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22359;&#21644;&#24322;&#27493;&#26356;&#26032;&#26426;&#21046;&#65292;&#33021;&#22815;&#25552;&#21319;&#27169;&#22411;&#22312;&#29305;&#23450;&#22359;&#19978;&#30340;&#27880;&#24847;&#21147;&#38598;&#20013;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2309.01606</link><description>&lt;p&gt;
Geo-Encoder: &#29992;&#20110;&#20013;&#25991;&#22320;&#29702;&#37325;&#26032;&#25490;&#24207;&#30340;&#22359;-&#35770;&#35777;&#21452;&#32534;&#30721;&#22120;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Geo-Encoder: A Chunk-Argument Bi-Encoder Framework for Chinese Geographic Re-Ranking
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2309.01606
&lt;/p&gt;
&lt;p&gt;
Geo-Encoder&#26159;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#22320;&#29702;&#37325;&#26032;&#25490;&#24207;&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#23558;&#20013;&#25991;&#22320;&#29702;&#35821;&#20041;&#38598;&#25104;&#21040;&#37325;&#26032;&#25490;&#24207;&#27969;&#31243;&#20013;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22359;&#21644;&#24322;&#27493;&#26356;&#26032;&#26426;&#21046;&#65292;&#33021;&#22815;&#25552;&#21319;&#27169;&#22411;&#22312;&#29305;&#23450;&#22359;&#19978;&#30340;&#27880;&#24847;&#21147;&#38598;&#20013;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#25991;&#22320;&#29702;&#37325;&#26032;&#25490;&#24207;&#20219;&#21153;&#26088;&#22312;&#22312;&#26816;&#32034;&#21040;&#30340;&#20505;&#36873;&#22320;&#22336;&#20013;&#25214;&#21040;&#26368;&#30456;&#20851;&#30340;&#22320;&#22336;&#65292;&#36825;&#23545;&#20110;&#23548;&#33322;&#22320;&#22270;&#31561;&#19982;&#20301;&#32622;&#30456;&#20851;&#30340;&#26381;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#19982;&#19968;&#33324;&#30340;&#35821;&#21477;&#19981;&#21516;&#65292;&#22320;&#29702;&#19978;&#19979;&#25991;&#19982;&#22320;&#29702;&#27010;&#24565;&#32039;&#23494;&#30456;&#36830;&#65292;&#20174;&#19968;&#33324;&#30340;&#29255;&#27573;&#65288;&#22914;&#30465;&#20221;&#65289;&#21040;&#29305;&#23450;&#30340;&#29255;&#27573;&#65288;&#22914;&#36947;&#36335;&#65289;&#12290;&#22522;&#20110;&#27492;&#29305;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;Geo-Encoder&#65292;&#23558;&#20013;&#25991;&#22320;&#29702;&#35821;&#20041;&#26356;&#26377;&#25928;&#22320;&#38598;&#25104;&#21040;&#37325;&#26032;&#25490;&#24207;&#27969;&#31243;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#20351;&#29992;&#29616;&#25104;&#30340;&#24037;&#20855;&#23558;&#25991;&#26412;&#19982;&#22320;&#29702;&#33539;&#22260;&#30456;&#20851;&#32852;&#65292;&#23558;&#23427;&#20204;&#20316;&#20026;&#20998;&#22359;&#21333;&#20803;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22359;&#65292;&#21516;&#26102;&#33719;&#21462;&#19968;&#20010;&#26377;&#25928;&#30340;&#27880;&#24847;&#21147;&#30697;&#38453;&#65292;&#20197;&#30830;&#23450;&#22359;&#23545;&#39069;&#22806;&#35821;&#20041;&#34920;&#31034;&#30340;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#24322;&#27493;&#26356;&#26032;&#26426;&#21046;&#65292;&#29992;&#20110;&#25552;&#20986;&#30340;&#38468;&#21152;&#20219;&#21153;&#65292;&#26088;&#22312;&#24341;&#23548;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#38598;&#20013;&#27880;&#24847;&#21147;&#22312;&#29305;&#23450;&#30340;&#22359;&#19978;&#12290;&#22312;&#23454;&#39564;&#20013;
&lt;/p&gt;
&lt;p&gt;
Chinese geographic re-ranking task aims to find the most relevant addresses among retrieved candidates, which is crucial for location-related services such as navigation maps. Unlike the general sentences, geographic contexts are closely intertwined with geographical concepts, from general spans (e.g., province) to specific spans (e.g., road). Given this feature, we propose an innovative framework, namely Geo-Encoder, to more effectively integrate Chinese geographical semantics into re-ranking pipelines. Our methodology begins by employing off-the-shelf tools to associate text with geographical spans, treating them as chunking units. Then, we present a multi-task learning module to simultaneously acquire an effective attention matrix that determines chunk contributions to extra semantic representations. Furthermore, we put forth an asynchronous update mechanism for the proposed addition task, aiming to guide the model capable of effectively focusing on specific chunks. Experiments on t
&lt;/p&gt;</description></item><item><title>&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-4&#20855;&#22791;&#20102;&#29702;&#35299;&#21644;&#35825;&#23548;&#20182;&#20154;&#20135;&#29983;&#38169;&#35823;&#20449;&#24565;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22797;&#26434;&#30340;&#27450;&#39575;&#22330;&#26223;&#20013;&#21487;&#20197;&#36890;&#36807;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#24471;&#21040;&#22686;&#24378;&#12290;</title><link>https://rss.arxiv.org/abs/2307.16513</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#27450;&#39575;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Deception Abilities Emerged in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2307.16513
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-4&#20855;&#22791;&#20102;&#29702;&#35299;&#21644;&#35825;&#23548;&#20182;&#20154;&#20135;&#29983;&#38169;&#35823;&#20449;&#24565;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22797;&#26434;&#30340;&#27450;&#39575;&#22330;&#26223;&#20013;&#21487;&#20197;&#36890;&#36807;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#24471;&#21040;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30446;&#21069;&#22788;&#20110;&#23558;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#20132;&#27969;&#21644;&#26085;&#24120;&#29983;&#27963;&#32039;&#23494;&#32467;&#21512;&#30340;&#21069;&#27839;&#12290;&#22240;&#27492;&#65292;&#23558;&#23427;&#20204;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25512;&#29702;&#33021;&#21147;&#30340;&#31283;&#23450;&#22686;&#38271;&#65292;&#26410;&#26469;&#30340;LLM&#34987;&#24576;&#30097;&#33021;&#22815;&#27450;&#39575;&#20154;&#31867;&#25805;&#20316;&#21592;&#65292;&#24182;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#32469;&#36807;&#30417;&#27979;&#24037;&#20316;&#12290;&#20026;&#27492;&#65292;LLM&#38656;&#35201;&#20855;&#22791;&#23545;&#27450;&#39575;&#31574;&#30053;&#30340;&#27010;&#24565;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;LLM&#65288;&#22914;GPT-4&#65289;&#20013;&#20986;&#29616;&#20102;&#36825;&#31181;&#31574;&#30053;&#65292;&#32780;&#22312;&#26089;&#26399;&#30340;LLM&#20013;&#24182;&#19981;&#23384;&#22312;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#34920;&#26126;&#26368;&#20808;&#36827;&#30340;LLM&#33021;&#22815;&#29702;&#35299;&#21644;&#35825;&#23548;&#20182;&#20154;&#20135;&#29983;&#38169;&#35823;&#30340;&#20449;&#24565;&#65292;&#20854;&#22312;&#22797;&#26434;&#30340;&#27450;&#39575;&#22330;&#26223;&#20013;&#34920;&#29616;&#21487;&#20197;&#36890;&#36807;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#24471;&#21040;&#22686;&#24378;&#65292;&#24182;&#19988;&#24341;&#21457;LLM&#20013;&#30340;&#39532;&#22522;&#38597;&#32500;&#21033;&#20027;&#20041;&#21487;&#20197;&#25913;&#21464;&#20854;&#27450;&#39575;&#20542;&#21521;&#12290;&#24635;&#20043;&#65292;&#25581;&#31034;&#20102;&#36804;&#20170;&#20026;&#27490;&#26410;&#30693;&#30340;&#27450;&#39575;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently at the forefront of intertwining artificial intelligence (AI) systems with human communication and everyday life. Thus, aligning them with human values is of great importance. However, given the steady increase in reasoning abilities, future LLMs are under suspicion of becoming able to deceive human operators and utilizing this ability to bypass monitoring efforts. As a prerequisite to this, LLMs need to possess a conceptual understanding of deception strategies. This study reveals that such strategies emerged in state-of-the-art LLMs, such as GPT-4, but were non-existent in earlier LLMs. We conduct a series of experiments showing that state-of-the-art LLMs are able to understand and induce false beliefs in other agents, that their performance in complex deception scenarios can be amplified utilizing chain-of-thought reasoning, and that eliciting Machiavellianism in LLMs can alter their propensity to deceive. In sum, revealing hitherto unknown
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#34920;&#31034;&#30340;&#24402;&#32435;&#25512;&#29702;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24418;&#24335;&#35821;&#35328;&#22312;&#22788;&#29702;&#21407;&#22987;&#36755;&#20837;&#12289;&#22788;&#29702;&#38169;&#35823;&#26631;&#35760;&#25968;&#25454;&#21644;&#22788;&#29702;&#27169;&#31946;&#36755;&#20837;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24402;&#32435;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#21644;&#20107;&#23454;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>https://rss.arxiv.org/abs/2212.10923</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#24402;&#32435;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models as Inductive Reasoners
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2212.10923
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#34920;&#31034;&#30340;&#24402;&#32435;&#25512;&#29702;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24418;&#24335;&#35821;&#35328;&#22312;&#22788;&#29702;&#21407;&#22987;&#36755;&#20837;&#12289;&#22788;&#29702;&#38169;&#35823;&#26631;&#35760;&#25968;&#25454;&#21644;&#22788;&#29702;&#27169;&#31946;&#36755;&#20837;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24402;&#32435;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#21644;&#20107;&#23454;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#32435;&#25512;&#29702;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#30340;&#24402;&#32435;&#25512;&#29702;&#30740;&#31350;&#20013;&#65292;&#24418;&#24335;&#35821;&#35328;&#34987;&#29992;&#20316;&#30693;&#35782;&#65288;&#20107;&#23454;&#21644;&#35268;&#21017;&#65289;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#24418;&#24335;&#35821;&#35328;&#20250;&#32473;&#24402;&#32435;&#25512;&#29702;&#24102;&#26469;&#31995;&#32479;&#24615;&#38382;&#39064;&#65292;&#20363;&#22914;&#26080;&#27861;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#36825;&#26679;&#30340;&#21407;&#22987;&#36755;&#20837;&#12289;&#23545;&#38169;&#35823;&#26631;&#35760;&#30340;&#25968;&#25454;&#25935;&#24863;&#20197;&#21450;&#22788;&#29702;&#27169;&#31946;&#36755;&#20837;&#30340;&#33021;&#21147;&#19981;&#36275;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#25512;&#29702;&#33539;&#24335;&#65288;&#20219;&#21153;&#65289;&#65292;&#21363;&#20174;&#33258;&#28982;&#35821;&#35328;&#20107;&#23454;&#20013;&#24402;&#32435;&#20986;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;DEER&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;1.2k&#20010;&#35268;&#21017;-&#20107;&#23454;&#23545;&#65292;&#35268;&#21017;&#21644;&#20107;&#23454;&#20197;&#33258;&#28982;&#35821;&#35328;&#20070;&#20889;&#12290;&#36824;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#29992;&#20110;&#35780;&#20272;&#27492;&#20219;&#21153;&#30340;&#26032;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#12290;&#36890;&#36807;DEER&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#29616;&#20195;&#30340;&#24402;&#32435;&#25512;&#29702;&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#30340;&#34920;&#31034;&#32780;&#19981;&#26159;&#24418;&#24335;&#35821;&#35328;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inductive reasoning is a core component of human intelligence. In the past research of inductive reasoning within computer science, formal language is used as representations of knowledge (facts and rules, more specifically). However, formal language can cause systematic problems for inductive reasoning such as disability of handling raw input such as natural language, sensitiveness to mislabeled data, and incapacity to handle ambiguous input. To this end, we propose a new paradigm (task) for inductive reasoning, which is to induce natural language rules from natural language facts, and create a dataset termed DEER containing 1.2k rule-fact pairs for the task, where rules and facts are written in natural language. New automatic metrics are also proposed and analysed for the evaluation of this task. With DEER, we investigate a modern approach for inductive reasoning where we use natural language as representation for knowledge instead of formal language and use pretrained language model
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#39640;&#25928;&#31934;&#30830;&#20248;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#31574;&#30053;&#21442;&#25968;&#21270;&#20219;&#24847;&#30340;&#24773;&#20917;&#19979;&#65292;&#28176;&#36817;&#22320;&#19982;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#19968;&#33268;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.00856</link><description>&lt;p&gt;
&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#39640;&#25928;&#31934;&#30830;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient and Exact Optimization of Language Model Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#39640;&#25928;&#31934;&#30830;&#20248;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#31574;&#30053;&#21442;&#25968;&#21270;&#20219;&#24847;&#30340;&#24773;&#20917;&#19979;&#65292;&#28176;&#36817;&#22320;&#19982;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#19968;&#33268;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#23545;&#40784;&#23545;&#20110;&#20854;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#35813;&#38382;&#39064;&#34987;&#24314;&#27169;&#20026;&#20248;&#21270;&#27169;&#22411;&#31574;&#30053;&#65292;&#20197;&#26368;&#22823;&#21270;&#21453;&#26144;&#20154;&#31867;&#20559;&#22909;&#30340;&#39044;&#26399;&#22870;&#21169;&#65292;&#24182;&#23613;&#37327;&#20943;&#23567;&#19982;&#21021;&#22987;&#31574;&#30053;&#30340;&#20559;&#24046;&#12290;&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#30452;&#25509;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20854;&#31574;&#30053;&#26356;&#26032;&#30340;&#26041;&#24046;&#24456;&#39640;&#65292;&#38459;&#30861;&#20102;&#39640;&#25928;&#30340;&#31574;&#30053;&#25913;&#36827;&#12290;&#26368;&#36817;&#65292;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#34987;&#25552;&#20986;&#20197;&#30452;&#25509;&#20174;&#20559;&#22909;&#25968;&#25454;&#20013;&#20248;&#21270;&#31574;&#30053;&#12290;&#23613;&#31649;&#23454;&#29616;&#31616;&#21333;&#65292;DPO&#26159;&#22522;&#20110;&#19981;&#19968;&#23450;&#33021;&#22312;&#23454;&#36341;&#20013;&#23454;&#29616;&#30340;&#26368;&#20248;&#31574;&#30053;&#23548;&#20986;&#30340;&#65292;&#36825;&#21066;&#24369;&#20102;&#20854;&#25910;&#25947;&#21040;&#39044;&#26399;&#35299;&#20915;&#26041;&#26696;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#31934;&#30830;&#20248;&#21270;&#65288;EXO&#65289;&#30340;&#23545;&#40784;&#30446;&#26631;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#31574;&#30053;&#30340;&#20219;&#24847;&#21442;&#25968;&#21270;&#65292;EXO&#20445;&#35777;&#28176;&#36817;&#22320;&#19982;RL&#31639;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#19968;&#33268;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The alignment of language models with human preferences is vital for their application in real-world tasks. The problem is formulated as optimizing the model's policy to maximize the expected reward that reflects human preferences with minimal deviation from the initial policy. While considered as a straightforward solution, reinforcement learning (RL) suffers from high variance in policy updates, which impedes efficient policy improvement. Recently, direct preference optimization (DPO) was proposed to directly optimize the policy from preference data. Though simple to implement, DPO is derived based on the optimal policy that is not assured to be achieved in practice, which undermines its convergence to the intended solution.   In this paper, we propose efficient exact optimization (EXO) of the alignment objective. We prove that EXO is guaranteed to optimize in the same direction as the RL algorithms asymptotically for arbitary parametrization of the policy, while enables efficient op
&lt;/p&gt;</description></item><item><title>CroissantLLM&#26159;&#19968;&#20010;1.3B&#30340;&#21452;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;1:1&#30340;&#33521;&#35821;-&#27861;&#35821;&#39044;&#35757;&#32451;&#25968;&#25454;&#27604;&#20363;&#12289;&#33258;&#23450;&#20041;&#30340;&#20998;&#35789;&#22120;&#21644;&#21452;&#35821;&#35843;&#20248;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#21644;&#24320;&#28304;&#12290;&#27169;&#22411;&#36824;&#21457;&#24067;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#22810;&#20010;&#26816;&#26597;&#28857;&#65292;&#20197;&#21450;&#19968;&#20010;&#27861;&#35821;&#22522;&#20934;&#27979;&#35797; FrenchBench&#12290;</title><link>https://arxiv.org/abs/2402.00786</link><description>&lt;p&gt;
CroissantLLM: &#19968;&#20010;&#30495;&#27491;&#30340;&#21452;&#35821;&#27861;&#35821;-&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CroissantLLM: A Truly Bilingual French-English Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00786
&lt;/p&gt;
&lt;p&gt;
CroissantLLM&#26159;&#19968;&#20010;1.3B&#30340;&#21452;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;1:1&#30340;&#33521;&#35821;-&#27861;&#35821;&#39044;&#35757;&#32451;&#25968;&#25454;&#27604;&#20363;&#12289;&#33258;&#23450;&#20041;&#30340;&#20998;&#35789;&#22120;&#21644;&#21452;&#35821;&#35843;&#20248;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#21644;&#24320;&#28304;&#12290;&#27169;&#22411;&#36824;&#21457;&#24067;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#22810;&#20010;&#26816;&#26597;&#28857;&#65292;&#20197;&#21450;&#19968;&#20010;&#27861;&#35821;&#22522;&#20934;&#27979;&#35797; FrenchBench&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CroissantLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;3T&#20010;&#33521;&#35821;&#21644;&#27861;&#35821;&#26631;&#35760;&#19978;&#39044;&#35757;&#32451;&#30340;13&#20159;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#30740;&#31350;&#21644;&#24037;&#19994;&#31038;&#21306;&#24102;&#26469;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#30340;&#12289;&#23436;&#20840;&#24320;&#28304;&#30340;&#21452;&#35821;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#28040;&#36153;&#32423;&#26412;&#22320;&#30828;&#20214;&#19978;&#24555;&#36895;&#36816;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;1:1&#30340;&#33521;&#35821;-&#27861;&#35821;&#39044;&#35757;&#32451;&#25968;&#25454;&#27604;&#20363;&#12289;&#33258;&#23450;&#20041;&#30340;&#20998;&#35789;&#22120;&#21644;&#21452;&#35821;&#35843;&#20248;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#19968;&#31181;&#20869;&#22312;&#21452;&#35821;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#20010;&#27861;&#35821;&#20998;&#21106;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#25163;&#24037;&#31574;&#21010;&#12289;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#28304;&#12290;&#20026;&#20102;&#35780;&#20272;&#22312;&#33521;&#35821;&#20197;&#22806;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797; FrenchBench&#65292;&#21253;&#25324;&#19968;&#31995;&#21015;&#20998;&#31867;&#21644;&#29983;&#25104;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;&#27169;&#22411;&#22312;&#27861;&#35821;&#35821;&#35328;&#20013;&#24615;&#33021;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20445;&#25345;&#36879;&#26126;&#24230;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#20195;&#30721;&#24211;&#21644;&#21508;&#31181;&#27169;&#22411;&#35268;&#27169;&#12289;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19978;&#30340;&#20960;&#21313;&#20010;&#26816;&#26597;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T English and French tokens, to bring to the research and industrial community a high-performance, fully open-sourced bilingual model that runs swiftly on consumer-grade local hardware. To that end, we pioneer the approach of training an intrinsically bilingual model with a 1:1 English-to-French pretraining data ratio, a custom tokenizer, and bilingual finetuning datasets. We release the training dataset, notably containing a French split with manually curated, high-quality, and varied data sources. To assess performance outside of English, we craft a novel benchmark, FrenchBench, consisting of an array of classification and generation tasks, covering various orthogonal aspects of model performance in the French Language. Additionally, rooted in transparency and to foster further Large Language Model research, we release codebases, and dozens of checkpoints across various model sizes, training data distributions, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Reveal&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22312;&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#31572;&#35774;&#32622;&#20013;&#23545;&#22797;&#26434;&#24605;&#32500;&#38142;&#30340;&#33258;&#21160;&#39564;&#35777;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#35814;&#23613;&#30340;&#26631;&#31614;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#31572;&#26696;&#20013;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#30340;&#30456;&#20851;&#24615;&#12289;&#24402;&#22240;&#21644;&#36923;&#36753;&#27491;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00559</link><description>&lt;p&gt;
&#19968;&#26465;&#24605;&#32500;&#38142;&#26465;&#30340;&#24378;&#24230;&#21462;&#20915;&#20110;&#26368;&#24369;&#30340;&#29615;&#33410;&#65306;&#19968;&#20010;&#39564;&#35777;&#25512;&#29702;&#38142;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Reveal&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22312;&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#31572;&#35774;&#32622;&#20013;&#23545;&#22797;&#26434;&#24605;&#32500;&#38142;&#30340;&#33258;&#21160;&#39564;&#35777;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#35814;&#23613;&#30340;&#26631;&#31614;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#31572;&#26696;&#20013;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#30340;&#30456;&#20851;&#24615;&#12289;&#24402;&#22240;&#21644;&#36923;&#36753;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20419;&#20351;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#36880;&#27493;&#22238;&#31572;&#65288;&#20363;&#22914;&#8220;&#24605;&#32500;&#38142;&#8221;&#65289;&#26159;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#20854;&#20013;&#26356;&#20934;&#30830;&#30340;&#25512;&#29702;&#38142;&#36890;&#24120;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#25991;&#29486;&#35752;&#35770;&#20102;&#33258;&#21160;&#39564;&#35777;&#25512;&#29702;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#21644;&#25913;&#21892;&#20854;&#27491;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#32454;&#31890;&#24230;&#30340;&#27493;&#39588;&#32423;&#25968;&#25454;&#38598;&#65292;&#26080;&#27861;&#23545;&#36825;&#31867;&#39564;&#35777;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Reveal&#65306;&#25512;&#29702;&#39564;&#35777;&#35780;&#20272;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22312;&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#31572;&#35774;&#32622;&#20013;&#23545;&#22797;&#26434;&#24605;&#32500;&#38142;&#30340;&#33258;&#21160;&#39564;&#35777;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;Reveal&#21253;&#25324;&#23545;&#35821;&#35328;&#27169;&#22411;&#31572;&#26696;&#20013;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#30340;&#30456;&#20851;&#24615;&#12289;&#24402;&#22240;&#20110;&#35777;&#25454;&#27573;&#33853;&#20197;&#21450;&#36923;&#36753;&#27491;&#30830;&#24615;&#30340;&#20840;&#38754;&#26631;&#31614;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting language models to provide step-by-step answers (e.g., "Chain-of-Thought") is the prominent approach for complex reasoning tasks, where more accurate reasoning chains typically improve downstream task performance. Recent literature discusses automatic methods to verify reasoning steps to evaluate and improve their correctness. However, no fine-grained step-level datasets are available to enable thorough evaluation of such verification methods, hindering progress in this direction. We introduce Reveal: Reasoning Verification Evaluation, a new dataset to benchmark automatic verifiers of complex Chain-of-Thought reasoning in open-domain question answering settings. Reveal includes comprehensive labels for the relevance, attribution to evidence passages, and logical correctness of each reasoning step in a language model's answer, across a wide variety of datasets and state-of-the-art language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22320;&#22270;&#25509;&#31181;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20248;&#21270;&#30340;&#25361;&#25112;&#25968;&#25454;&#23376;&#38598;&#19978;&#23545;QA&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20943;&#23569;&#27169;&#22411;&#23545;&#25968;&#25454;&#38598;&#20266;&#36857;&#30340;&#20381;&#36182;&#24615;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#22312;&#22797;&#26434;&#21644;&#24320;&#25918;&#30340;&#19978;&#19979;&#25991;&#25512;&#29702;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17498</link><description>&lt;p&gt;
&#36890;&#36807;&#22320;&#22270;&#25509;&#31181;&#25913;&#36827;QA&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving QA Model Performance with Cartographic Inoculation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22320;&#22270;&#25509;&#31181;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20248;&#21270;&#30340;&#25361;&#25112;&#25968;&#25454;&#23376;&#38598;&#19978;&#23545;QA&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20943;&#23569;&#27169;&#22411;&#23545;&#25968;&#25454;&#38598;&#20266;&#36857;&#30340;&#20381;&#36182;&#24615;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#22312;&#22797;&#26434;&#21644;&#24320;&#25918;&#30340;&#19978;&#19979;&#25991;&#25512;&#29702;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
QA&#27169;&#22411;&#38754;&#20020;&#30528;&#22797;&#26434;&#32780;&#24320;&#25918;&#30340;&#19978;&#19979;&#25991;&#25512;&#29702;&#38382;&#39064;&#65292;&#20294;&#36890;&#24120;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#29305;&#23450;&#20110;&#25968;&#25454;&#38598;&#30340;&#27169;&#24335;&#26469;&#23398;&#20064;&#39640;&#24615;&#33021;&#30340;&#35299;&#20915;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#36825;&#20123;&#27169;&#24335;&#65292;&#25110;&#32773;&#31216;&#20026;"&#25968;&#25454;&#38598;&#20266;&#36857;"&#65292;&#38477;&#20302;&#20102;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;QA&#38382;&#39064;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21033;&#29992;&#35757;&#32451;&#29992;&#20110;QA&#30340;ElectraSmallDiscriminator&#27169;&#22411;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#32452;&#23545;&#25239;&#24615;&#25361;&#25112;&#25968;&#25454;&#38598;&#20998;&#26512;&#20102;&#25968;&#25454;&#38598;&#20266;&#36857;&#30340;&#24433;&#21709;&#21644;&#21457;&#29983;&#24773;&#20917;&#65292;&#35813;&#25968;&#25454;&#38598;&#26088;&#22312;&#28151;&#28102;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#20266;&#36857;&#36827;&#34892;&#39044;&#27979;&#30340;&#27169;&#22411;&#12290;&#22312;&#29616;&#26377;&#20943;&#36731;&#20266;&#36857;&#24433;&#21709;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#22320;&#22270;&#25509;&#31181;&#65292;&#36890;&#36807;&#22312;&#20248;&#21270;&#30340;&#25361;&#25112;&#25968;&#25454;&#23376;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#20943;&#23569;&#27169;&#22411;&#23545;&#25968;&#25454;&#38598;&#20266;&#36857;&#30340;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#26377;&#36873;&#25321;&#22320;&#22312;&#25361;&#25112;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#26865;&#20004;&#21487;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#19978;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#65292;&#21487;&#20197;&#22312;&#26368;&#23567;&#25439;&#22833;&#27169;&#22411;&#23545;&#20854;&#20182;&#25361;&#25112;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#22823;&#25552;&#39640;&#27169;&#22411;&#22312;&#25972;&#20010;&#25361;&#25112;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
QA models are faced with complex and open-ended contextual reasoning problems, but can often learn well-performing solution heuristics by exploiting dataset-specific patterns in their training data. These patterns, or "dataset artifacts", reduce the model's ability to generalize to real-world QA problems. Utilizing an ElectraSmallDiscriminator model trained for QA, we analyze the impacts and incidence of dataset artifacts using an adversarial challenge set designed to confuse models reliant on artifacts for prediction. Extending existing work on methods for mitigating artifact impacts, we propose cartographic inoculation, a novel method that fine-tunes models on an optimized subset of the challenge data to reduce model reliance on dataset artifacts. We show that by selectively fine-tuning a model on ambiguous adversarial examples from a challenge set, significant performance improvements can be made on the full challenge dataset with minimal loss of model generalizability to other chal
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#21462;&#20195;&#32463;&#27982;&#23454;&#39564;&#23460;&#36827;&#34892;&#36873;&#25321;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17435</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#21462;&#20195;&#32463;&#27982;&#36873;&#25321;&#39044;&#27979;&#23454;&#39564;&#23460;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Replace Economic Choice Prediction Labs?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17435
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#21462;&#20195;&#32463;&#27982;&#23454;&#39564;&#23460;&#36827;&#34892;&#36873;&#25321;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#27982;&#36873;&#25321;&#39044;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#24448;&#24448;&#21463;&#38480;&#20110;&#33719;&#21462;&#20154;&#31867;&#36873;&#25321;&#25968;&#25454;&#30340;&#22256;&#38590;&#12290;&#23454;&#39564;&#32463;&#27982;&#23398;&#30740;&#31350;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#19987;&#27880;&#20110;&#31616;&#21333;&#30340;&#36873;&#25321;&#29615;&#22659;&#12290;&#26368;&#36817;&#65292;&#20154;&#24037;&#26234;&#33021;&#30028;&#20197;&#20004;&#31181;&#26041;&#24335;&#20026;&#35813;&#21162;&#21147;&#20570;&#20986;&#20102;&#36129;&#29486;&#65306;&#32771;&#34385;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#20195;&#26367;&#20154;&#31867;&#22312;&#19978;&#36848;&#31616;&#21333;&#36873;&#25321;&#39044;&#27979;&#29615;&#22659;&#20013;&#65292;&#20197;&#21450;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;&#30740;&#31350;&#26356;&#22797;&#26434;&#20294;&#20173;&#20005;&#26684;&#30340;&#23454;&#39564;&#32463;&#27982;&#23398;&#29615;&#22659;&#65292;&#21253;&#25324;&#19981;&#23436;&#20840;&#20449;&#24687;&#12289;&#37325;&#22797;&#21338;&#24328;&#21644;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#30340;&#35828;&#26381;&#28216;&#25103;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#28789;&#24863;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#23436;&#20840;&#27169;&#25311;&#32463;&#27982;&#29615;&#22659;&#65292;&#24182;&#29983;&#25104;&#29992;&#20110;&#39640;&#25928;&#20154;&#31867;&#36873;&#25321;&#39044;&#27979;&#30340;&#25968;&#25454;&#65292;&#26367;&#20195;&#22797;&#26434;&#30340;&#32463;&#27982;&#23454;&#39564;&#23460;&#30740;&#31350;&#65311;&#25105;&#20204;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#24320;&#21019;&#20102;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#34920;&#26126;&#20165;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Economic choice prediction is an essential challenging task, often constrained by the difficulties in acquiring human choice data. Indeed, experimental economics studies had focused mostly on simple choice settings. The AI community has recently contributed to that effort in two ways: considering whether LLMs can substitute for humans in the above-mentioned simple choice prediction settings, and the study through ML lens of more elaborated but still rigorous experimental economics settings, employing incomplete information, repetitive play, and natural language communication, notably language-based persuasion games. This leaves us with a major inspiration: can LLMs be used to fully simulate the economic environment and generate data for efficient human choice prediction, substituting for the elaborated economic lab studies? We pioneer the study of this subject, demonstrating its feasibility. In particular, we show that a model trained solely on LLM-generated data can effectively predic
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#21512;&#35843;&#26597;&#24635;&#32467;&#20102;&#26368;&#36817;&#22312;&#20167;&#24680;&#35328;&#35770;&#23457;&#26680;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#20102;&#25991;&#26412;&#12289;&#35270;&#35273;&#21644;&#21548;&#35273;&#20803;&#32032;&#22312;&#20256;&#25773;&#20167;&#24680;&#35328;&#35770;&#20013;&#30340;&#24494;&#22937;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#24378;&#35843;&#20102;&#22823;&#22411;&#27169;&#22411;&#23545;&#23457;&#26680;&#33021;&#21147;&#30340;&#37325;&#26032;&#23450;&#20041;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#25351;&#20986;&#20102;&#22312;&#23569;&#25968;&#35821;&#35328;&#21644;&#25991;&#21270;&#32972;&#26223;&#19979;&#30340;&#30740;&#31350;&#24046;&#36317;&#21644;&#22788;&#29702;&#20302;&#36164;&#28304;&#29615;&#22659;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.16727</link><description>&lt;p&gt;
&#26368;&#36817;&#22312;&#20167;&#24680;&#35328;&#35770;&#23457;&#26680;&#26041;&#38754;&#30340;&#36827;&#23637;&#65306;&#22810;&#27169;&#24577;&#21644;&#22823;&#22411;&#27169;&#22411;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Recent Advances in Hate Speech Moderation: Multimodality and the Role of Large Models. (arXiv:2401.16727v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16727
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#21512;&#35843;&#26597;&#24635;&#32467;&#20102;&#26368;&#36817;&#22312;&#20167;&#24680;&#35328;&#35770;&#23457;&#26680;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#20102;&#25991;&#26412;&#12289;&#35270;&#35273;&#21644;&#21548;&#35273;&#20803;&#32032;&#22312;&#20256;&#25773;&#20167;&#24680;&#35328;&#35770;&#20013;&#30340;&#24494;&#22937;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#24378;&#35843;&#20102;&#22823;&#22411;&#27169;&#22411;&#23545;&#23457;&#26680;&#33021;&#21147;&#30340;&#37325;&#26032;&#23450;&#20041;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#25351;&#20986;&#20102;&#22312;&#23569;&#25968;&#35821;&#35328;&#21644;&#25991;&#21270;&#32972;&#26223;&#19979;&#30340;&#30740;&#31350;&#24046;&#36317;&#21644;&#22788;&#29702;&#20302;&#36164;&#28304;&#29615;&#22659;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32593;&#32476;&#20132;&#27969;&#30340;&#19981;&#26029;&#21457;&#23637;&#20013;&#65292;&#23457;&#26680;&#20167;&#24680;&#35328;&#35770;&#65288;HS&#65289;&#38754;&#20020;&#30528;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#30001;&#25968;&#23383;&#20869;&#23481;&#30340;&#22810;&#27169;&#24577;&#29305;&#24615;&#25152;&#24102;&#26469;&#30340;&#12290;&#36825;&#39033;&#32508;&#21512;&#35843;&#26597;&#28145;&#20837;&#30740;&#31350;&#20102;HS&#23457;&#26680;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#30528;&#37325;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#23835;&#36215;&#35282;&#33394;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#23545;&#24403;&#21069;&#25991;&#29486;&#30340;&#20840;&#38754;&#20998;&#26512;&#24320;&#22987;&#65292;&#25581;&#31034;&#20102;&#25991;&#26412;&#12289;&#35270;&#35273;&#21644;&#21548;&#35273;&#20803;&#32032;&#22312;&#20256;&#25773;HS&#20013;&#30340;&#24494;&#22937;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#26126;&#26174;&#30340;&#36235;&#21183;&#65292;&#21363;&#23558;&#36825;&#20123;&#27169;&#24577;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;HS&#30340;&#20256;&#25773;&#20855;&#26377;&#22797;&#26434;&#24615;&#21644;&#24494;&#22937;&#24615;&#12290;&#23545;&#20110;&#30001;LLMs&#21644;LMMs&#24102;&#26469;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#29305;&#21035;&#24378;&#35843;&#20102;&#20854;&#23545;&#26816;&#27979;&#21644;&#23457;&#26680;&#33021;&#21147;&#36793;&#30028;&#30340;&#37325;&#26032;&#23450;&#20041;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#30740;&#31350;&#20013;&#23384;&#22312;&#30340;&#29616;&#26377;&#24046;&#36317;&#65292;&#29305;&#21035;&#26159;&#22312;&#23569;&#25968;&#35821;&#35328;&#21644;&#25991;&#21270;&#30340;&#32972;&#26223;&#19979;&#65292;&#20197;&#21450;&#22312;&#22788;&#29702;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#38656;&#35201;&#35299;&#20915;&#26041;&#26696;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the evolving landscape of online communication, moderating hate speech (HS) presents an intricate challenge, compounded by the multimodal nature of digital content. This comprehensive survey delves into the recent strides in HS moderation, spotlighting the burgeoning role of large language models (LLMs) and large multimodal models (LMMs). Our exploration begins with a thorough analysis of current literature, revealing the nuanced interplay between textual, visual, and auditory elements in propagating HS. We uncover a notable trend towards integrating these modalities, primarily due to the complexity and subtlety with which HS is disseminated. A significant emphasis is placed on the advances facilitated by LLMs and LMMs, which have begun to redefine the boundaries of detection and moderation capabilities. We identify existing gaps in research, particularly in the context of underrepresented languages and cultures, and the need for solutions to handle low-resource settings. The survey
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20154;&#31867;&#21644;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#30340;&#35821;&#35328;&#24046;&#24322;&#65292;&#21457;&#29616;ChatGPT&#22312;&#31038;&#20132;&#12289;&#20998;&#26512;&#12289;&#35748;&#30693;&#12289;&#20851;&#27880;&#28966;&#28857;&#21644;&#31215;&#26497;&#24773;&#32490;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20154;&#31867;&#23545;&#35805;&#26356;&#20855;&#21464;&#24322;&#24615;&#21644;&#30495;&#23454;&#24615;&#65292;&#23613;&#31649;&#22312;&#24773;&#32490;&#26041;&#38754;&#26080;&#26174;&#33879;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#30001;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2401.16587</link><description>&lt;p&gt;
&#20154;&#31867;&#19982;ChatGPT&#29983;&#25104;&#23545;&#35805;&#20043;&#38388;&#30340;&#35821;&#35328;&#23545;&#27604;
&lt;/p&gt;
&lt;p&gt;
A Linguistic Comparison between Human and ChatGPT-Generated Conversations. (arXiv:2401.16587v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20154;&#31867;&#21644;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#30340;&#35821;&#35328;&#24046;&#24322;&#65292;&#21457;&#29616;ChatGPT&#22312;&#31038;&#20132;&#12289;&#20998;&#26512;&#12289;&#35748;&#30693;&#12289;&#20851;&#27880;&#28966;&#28857;&#21644;&#31215;&#26497;&#24773;&#32490;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20154;&#31867;&#23545;&#35805;&#26356;&#20855;&#21464;&#24322;&#24615;&#21644;&#30495;&#23454;&#24615;&#65292;&#23613;&#31649;&#22312;&#24773;&#32490;&#26041;&#38754;&#26080;&#26174;&#33879;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#30001;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#31867;&#21644;LLM&#29983;&#25104;&#30340;&#23545;&#35805;&#20043;&#38388;&#30340;&#35821;&#35328;&#24046;&#24322;&#65292;&#20351;&#29992;&#20102;&#30001;ChatGPT-3.5&#29983;&#25104;&#30340;19.5K&#20010;&#23545;&#35805;&#20316;&#20026;EmpathicDialogues&#25968;&#25454;&#38598;&#30340;&#34917;&#20805;&#12290;&#30740;&#31350;&#37319;&#29992;Linguistic Inquiry and Word Count (LIWC) &#20998;&#26512;&#65292;&#27604;&#36739;&#20102;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#21644;&#20154;&#31867;&#23545;&#35805;&#22312;118&#20010;&#35821;&#35328;&#31867;&#21035;&#19978;&#30340;&#24046;&#24322;&#12290;&#32467;&#26524;&#26174;&#31034;&#20154;&#31867;&#23545;&#35805;&#20855;&#26377;&#26356;&#22823;&#30340;&#21464;&#24322;&#24615;&#21644;&#30495;&#23454;&#24615;&#65292;&#20294;ChatGPT&#22312;&#31038;&#20132;&#36807;&#31243;&#12289;&#20998;&#26512;&#39118;&#26684;&#12289;&#35748;&#30693;&#12289;&#20851;&#27880;&#28966;&#28857;&#21644;&#31215;&#26497;&#24773;&#32490;&#33394;&#24425;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;LLMs&#8220;&#27604;&#30495;&#20154;&#26356;&#20687;&#30495;&#20154;&#8221;&#30340;&#26368;&#26032;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;ChatGPT&#21644;&#20154;&#31867;&#23545;&#35805;&#20043;&#38388;&#27809;&#26377;&#25214;&#21040;&#31215;&#26497;&#25110;&#28040;&#26497;&#24773;&#32490;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#23545;&#35805;&#23884;&#20837;&#30340;&#20998;&#31867;&#22120;&#20998;&#26512;&#34920;&#26126;&#65292;&#23613;&#31649;&#23545;&#35805;&#20013;&#27809;&#26377;&#26126;&#30830;&#25552;&#21450;&#24773;&#32490;&#65292;&#20294;&#23545;&#24773;&#24863;&#20215;&#20540;&#30340;&#38544;&#24615;&#32534;&#30721;&#23384;&#22312;&#12290;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#30001;&#20004;&#20010;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores linguistic differences between human and LLM-generated dialogues, using 19.5K dialogues generated by ChatGPT-3.5 as a companion to the EmpathicDialogues dataset. The research employs Linguistic Inquiry and Word Count (LIWC) analysis, comparing ChatGPT-generated conversations with human conversations across 118 linguistic categories. Results show greater variability and authenticity in human dialogues, but ChatGPT excels in categories such as social processes, analytical style, cognition, attentional focus, and positive emotional tone, reinforcing recent findings of LLMs being "more human than human." However, no significant difference was found in positive or negative affect between ChatGPT and human dialogues. Classifier analysis of dialogue embeddings indicates implicit coding of the valence of affect despite no explicit mention of affect in the conversations. The research also contributes a novel, companion ChatGPT-generated dataset of conversations between two i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#19987;&#19994;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#19987;&#19994;&#30693;&#35782;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#26469;&#25552;&#21319;&#33258;&#21160;&#29983;&#25104;&#25253;&#21578;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#30340;&#27169;&#22411;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.16578</link><description>&lt;p&gt;
&#21457;&#25381;&#19987;&#19994;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#19987;&#38271;&#65292;&#25552;&#21319;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;LLM&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Leveraging Professional Radiologists' Expertise to Enhance LLMs' Evaluation for Radiology Reports. (arXiv:2401.16578v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16578
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#19987;&#19994;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#19987;&#19994;&#30693;&#35782;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#26469;&#25552;&#21319;&#33258;&#21160;&#29983;&#25104;&#25253;&#21578;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#30340;&#27169;&#22411;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25918;&#23556;&#23398;&#39046;&#22495;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24050;&#32463;&#22823;&#22823;&#25512;&#36827;&#20102;&#25253;&#21578;&#29983;&#25104;&#65292;&#20294;&#33258;&#21160;&#29983;&#25104;&#25253;&#21578;&#30340;&#33258;&#21160;&#35780;&#20272;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22914;&#20256;&#32479;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#21644;&#20020;&#24202;&#25928;&#33021;&#65288;CE&#65289;&#65292;&#24448;&#24448;&#26080;&#27861;&#25429;&#25417;&#20020;&#24202;&#32972;&#26223;&#30340;&#35821;&#20041;&#22797;&#26434;&#24615;&#65292;&#25110;&#32773;&#36807;&#20998;&#24378;&#35843;&#20020;&#24202;&#32454;&#33410;&#65292;&#38477;&#20302;&#20102;&#25253;&#21578;&#30340;&#28165;&#26224;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#19987;&#19994;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#19987;&#19994;&#30693;&#35782;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3.5&#21644;GPT-4 1&#65292;&#30456;&#32467;&#21512;&#12290;&#21033;&#29992;&#19978;&#19979;&#25991;&#25351;&#23548;&#23398;&#20064;&#65288;ICIL&#65289;&#21644;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25512;&#29702;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;LLM&#30340;&#35780;&#20272;&#19982;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#26631;&#20934;&#20445;&#25345;&#19968;&#33268;&#65292;&#23454;&#29616;&#20102;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25253;&#21578;&#19982;&#20154;&#31867;&#29983;&#25104;&#25253;&#21578;&#20043;&#38388;&#30340;&#35814;&#32454;&#27604;&#36739;&#12290;&#36825;&#36827;&#19968;&#27493;&#36890;&#36807;&#22238;&#24402;&#27169;&#22411;&#26469;&#32508;&#21512;&#21477;&#23376;&#35780;&#20272;&#20998;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#8220;&#35814;&#32454;GPT-4&#65288;5&#27425;&#35757;&#32451;&#65289;&#8221;&#27169;&#22411;&#33719;&#24471;&#20102;0.48&#30340;&#20998;&#25968;&#65292;&#20248;&#20110;METEOR&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
In radiology, Artificial Intelligence (AI) has significantly advanced report generation, but automatic evaluation of these AI-produced reports remains challenging. Current metrics, such as Conventional Natural Language Generation (NLG) and Clinical Efficacy (CE), often fall short in capturing the semantic intricacies of clinical contexts or overemphasize clinical details, undermining report clarity. To overcome these issues, our proposed method synergizes the expertise of professional radiologists with Large Language Models (LLMs), like GPT-3.5 and GPT-4 1. Utilizing In-Context Instruction Learning (ICIL) and Chain of Thought (CoT) reasoning, our approach aligns LLM evaluations with radiologist standards, enabling detailed comparisons between human and AI generated reports. This is further enhanced by a Regression model that aggregates sentence evaluation scores. Experimental results show that our ''Detailed GPT-4 (5-shot)'' model achieves a 0.48 score, outperforming the METEOR metric 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;RAG&#30340;MufassirQAS&#38382;&#31572;&#31995;&#32479;&#21033;&#29992;NLP&#25216;&#26415;&#24314;&#31435;&#32852;&#31995;&#24182;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#65292;&#24110;&#21161;&#29702;&#35299;&#20234;&#26031;&#20848;&#25945;&#30340;&#22797;&#26434;&#24615;&#21644;&#25945;&#20041;&#28145;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.15378</link><description>&lt;p&gt;
&#22522;&#20110;RAG&#30340;&#29702;&#35299;&#20234;&#26031;&#20848;&#25945;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#25552;&#26696;&#65306;MufassirQAS LLM
&lt;/p&gt;
&lt;p&gt;
A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM. (arXiv:2401.15378v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15378
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;RAG&#30340;MufassirQAS&#38382;&#31572;&#31995;&#32479;&#21033;&#29992;NLP&#25216;&#26415;&#24314;&#31435;&#32852;&#31995;&#24182;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#65292;&#24110;&#21161;&#29702;&#35299;&#20234;&#26031;&#20848;&#25945;&#30340;&#22797;&#26434;&#24615;&#21644;&#25945;&#20041;&#28145;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21644;&#29702;&#35299;&#23447;&#25945;&#23384;&#22312;&#22797;&#26434;&#24615;&#21644;&#25945;&#20041;&#28145;&#24230;&#30340;&#25361;&#25112;&#12290;&#38382;&#31572;&#26426;&#22120;&#20154;&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#12290;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#24314;&#31435;&#20027;&#39064;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#12290;&#36825;&#20123;&#33021;&#21147;&#20351;&#20854;&#25104;&#20026;&#29992;&#20110;&#23447;&#25945;&#21551;&#33945;&#30340;&#38382;&#39064;&#22238;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;LLM&#20063;&#26377;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#30340;&#20542;&#21521;&#65292;&#31216;&#20026;&#24187;&#35273;&#12290;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22238;&#31572;&#21487;&#33021;&#21253;&#21547;&#20398;&#36785;&#20010;&#20154;&#23447;&#25945;&#20449;&#20208;&#12289;&#36328;&#23447;&#27966;&#20914;&#31361;&#21644;&#26377;&#20105;&#35758;&#25110;&#25935;&#24863;&#30340;&#35805;&#39064;&#30340;&#20869;&#23481;&#12290;&#23427;&#38656;&#35201;&#36991;&#20813;&#36825;&#31181;&#24773;&#20917;&#65292;&#32780;&#19981;&#20250;&#23459;&#25196;&#20167;&#24680;&#35328;&#35770;&#25110;&#20882;&#29359;&#26576;&#20123;&#32676;&#20307;&#30340;&#20154;&#25110;&#20182;&#20204;&#30340;&#20449;&#20208;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#21521;&#37327;&#25968;&#25454;&#24211;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26041;&#27861;&#26469;&#25552;&#39640;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;&#25105;&#20204;&#30340;&#38382;&#31572;&#31995;&#32479;&#31216;&#20026;"MufassirQAS"&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#35780;&#20272;&#35813;&#31995;&#32479;&#24182;&#35777;&#26126;&#20854;&#22312;&#35299;&#20915;&#23447;&#25945;&#34892;&#19994;&#38382;&#39064;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
There exist challenges in learning and understanding religions as the presence of complexity and depth of religious doctrines and teachings. Chatbots as question-answering systems can help in solving these challenges. LLM chatbots use NLP techniques to establish connections between topics and accurately respond to complex questions. These capabilities make it perfect to be used in enlightenment on religion as a question answering chatbot. However, LLMs also have a tendency to generate false information, known as hallucination. The responses of the chatbots can include content that insults personal religious beliefs, interfaith conflicts, and controversial or sensitive topics. It needs to avoid such cases without promoting hate speech or offending certain groups of people or their beliefs. This study uses a vector database-based Retrieval Augmented Generation (RAG) approach to enhance the accuracy and transparency of LLMs. Our question-answering system is called as "MufassirQAS". We cre
&lt;/p&gt;</description></item><item><title>UNSEE&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#38750;&#23545;&#27604;&#24230;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#30446;&#26631;&#32593;&#32476;&#35299;&#20915;&#20102;&#34920;&#31034;&#22349;&#22604;&#38382;&#39064;&#65292;&#36798;&#21040;&#20102;&#19982;&#23545;&#27604;&#30446;&#26631;&#30456;&#24403;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.15316</link><description>&lt;p&gt;
UNSEE: &#26080;&#30417;&#30563;&#30340;&#38750;&#23545;&#27604;&#24230;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
UNSEE: Unsupervised Non-contrastive Sentence Embeddings. (arXiv:2401.15316v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15316
&lt;/p&gt;
&lt;p&gt;
UNSEE&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#38750;&#23545;&#27604;&#24230;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#30446;&#26631;&#32593;&#32476;&#35299;&#20915;&#20102;&#34920;&#31034;&#22349;&#22604;&#38382;&#39064;&#65292;&#36798;&#21040;&#20102;&#19982;&#23545;&#27604;&#30446;&#26631;&#30456;&#24403;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UNSEE&#65288;Unsupervised Non-Contrastive Sentence Embeddings&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;SimCSE&#12290;&#25105;&#20204;&#39318;&#20808;&#35299;&#20915;&#20102;SimCSE&#20013;&#26367;&#25442;&#23545;&#27604;&#30446;&#26631;&#20026;&#38750;&#23545;&#27604;&#30446;&#26631;&#26102;&#20986;&#29616;&#30340;&#34920;&#31034;&#22349;&#22604;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#30446;&#26631;&#32593;&#32476;&#30340;&#31616;&#21333;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#34920;&#31034;&#22349;&#22604;&#12290;&#30446;&#26631;&#32593;&#32476;&#30340;&#24341;&#20837;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#38750;&#23545;&#27604;&#30446;&#26631;&#65292;&#22312;&#20445;&#25345;&#35757;&#32451;&#31283;&#23450;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#19982;&#23545;&#27604;&#30446;&#26631;&#30456;&#24403;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#36890;&#36807;&#31934;&#24515;&#35843;&#25972;&#21644;&#20248;&#21270;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38750;&#23545;&#27604;&#24230;&#21477;&#23376;&#23884;&#20837;&#19978;&#36798;&#21040;&#20102;&#24005;&#23792;&#24615;&#33021;&#12290;&#36825;&#19968;&#20840;&#38754;&#30340;&#21162;&#21147;&#20135;&#29983;&#20102;&#20986;&#33394;&#30340;&#21477;&#23376;&#34920;&#31034;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present UNSEE: Unsupervised Non-Contrastive Sentence Embeddings, a novel approach that outperforms SimCSE in the Massive Text Embedding benchmark. Our exploration begins by addressing the challenge of representation collapse, a phenomenon observed when contrastive objectives in SimCSE are replaced with non-contrastive objectives. To counter this issue, we propose a straightforward solution known as the target network, effectively mitigating representation collapse. The introduction of the target network allows us to leverage non-contrastive objectives, maintaining training stability while achieving performance improvements comparable to contrastive objectives. Our method has achieved peak performance in non-contrastive sentence embeddings through meticulous fine-tuning and optimization. This comprehensive effort has yielded superior sentence representation models, showcasing the effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#39044;&#35757;&#32451;&#25991;&#26412;&#26469;&#34913;&#37327;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#27010;&#24565;&#30340;&#39057;&#29575;&#65292;&#24182;&#21457;&#29616;&#27969;&#34892;&#30340;VLM&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#38271;&#23614;&#27010;&#24565;&#20998;&#24067;&#65292;&#36825;&#19982;&#25353;&#31867;&#21035;&#30340;&#20934;&#30830;&#29575;&#24378;&#28872;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2401.12425</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#34987;&#24573;&#35270;&#30340;&#23614;&#37096;
&lt;/p&gt;
&lt;p&gt;
The Neglected Tails of Vision-Language Models. (arXiv:2401.12425v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#39044;&#35757;&#32451;&#25991;&#26412;&#26469;&#34913;&#37327;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#27010;&#24565;&#30340;&#39057;&#29575;&#65292;&#24182;&#21457;&#29616;&#27969;&#34892;&#30340;VLM&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#38271;&#23614;&#27010;&#24565;&#20998;&#24067;&#65292;&#36825;&#19982;&#25353;&#31867;&#21035;&#30340;&#20934;&#30830;&#29575;&#24378;&#28872;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#22312;&#38646;&#26679;&#26412;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35270;&#35273;&#27010;&#24565;&#19978;&#30340;&#34920;&#29616;&#26497;&#19981;&#22343;&#34913;&#12290;&#20363;&#22914;&#65292;&#23613;&#31649;CLIP&#22312;ImageNet&#19978;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24179;&#22343;&#38646;&#26679;&#26412;&#20934;&#30830;&#29575;&#65288;72.7&#65285;&#65289;&#65292;&#20294;&#22312;&#21313;&#20010;&#27010;&#24565;&#65288;&#22914;gyromitra&#21644;night snake&#65289;&#19978;&#30340;&#20934;&#30830;&#29575;&#19981;&#21040;10&#65285;&#65292;&#36825;&#21487;&#33021;&#26159;&#22240;&#20026;&#36825;&#20123;&#27010;&#24565;&#22312;VLM&#30340;&#38750;&#22343;&#34913;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#19981;&#36275;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#36825;&#31181;&#19981;&#24179;&#34913;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#22312;VLM&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#35745;&#31639;&#29305;&#23450;&#27010;&#24565;&#30340;&#39057;&#29575;&#26159;&#38750;&#24120;&#22797;&#26434;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;&#20998;&#26512;&#39044;&#35757;&#32451;&#25991;&#26412;&#26469;&#27979;&#37327;&#27010;&#24565;&#39057;&#29575;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#24110;&#21161;&#35745;&#31639;&#21253;&#21547;&#32473;&#23450;&#27010;&#24565;&#30340;&#21516;&#20041;&#35789;&#30340;&#30456;&#20851;&#25991;&#26412;&#65292;&#24182;&#35299;&#20915;&#35821;&#35328;&#27495;&#20041;&#12290;&#25105;&#20204;&#30830;&#35748;&#20687;LAION&#36825;&#26679;&#30340;&#27969;&#34892;&#30340;VLM&#25968;&#25454;&#38598;&#30830;&#23454;&#23637;&#31034;&#20102;&#38271;&#23614;&#27010;&#24565;&#20998;&#24067;&#65292;&#24182;&#19988;&#36825;&#19982;&#25353;&#31867;&#21035;&#30340;&#20934;&#30830;&#29575;&#24378;&#28872;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#24403;&#20195;&#30340;&#22810;&#27169;&#24335;&#31995;&#32479;&#65292;&#22914;&#35270;&#35273;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#25991;&#26412;-&#35270;&#35273;&#25512;&#29702;&#27169;&#22411;&#65292;&#22312;&#36825;&#31181;&#38271;&#23614;&#20998;&#24067;&#19979;&#32463;&#24120;&#38590;&#20197;&#36798;&#21040;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) excel in zero-shot recognition but exhibit drastically imbalanced performance across visual concepts. For example, CLIP, despite an impressive mean zero-shot accuracy on ImageNet (72.7%), yields $&lt;$10% on ten concepts (e.g., gyromitra and night snake), presumably, because these concepts are under-represented in VLMs' imbalanced pretraining data. Yet, assessing this imbalance is challenging as it is non-trivial to calculate the frequency of specific concepts within VLMs' large-scale pretraining data. Our work makes the first attempt to measure the concept frequency by analyzing pretraining texts. We use off-the-shelf language models to help count relevant texts that contain synonyms of the given concepts and resolve linguistic ambiguity. We confirm that popular VLM datasets like LAION indeed exhibit long-tailed concept distributions, which strongly correlate with per-class accuracies. Further, contemporary multimodal systems, e.g., visual chatbots and text-
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;cuDialog&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#25991;&#21270;&#20026;&#35270;&#35282;&#30340;&#23545;&#35805;&#29983;&#25104;&#22522;&#20934;&#65292;&#24182;&#24320;&#21457;&#20102;&#33021;&#22815;&#20174;&#23545;&#35805;&#20013;&#25552;&#21462;&#25991;&#21270;&#23646;&#24615;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#32467;&#21512;&#25991;&#21270;&#20215;&#20540;&#35843;&#26597;&#21487;&#20197;&#25552;&#39640;&#23545;&#35805;&#20195;&#29702;&#30340;&#23545;&#20010;&#24615;&#21270;&#21644;&#23545;&#35805;&#36136;&#37327;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10352</link><description>&lt;p&gt;
&#36890;&#36807;&#25991;&#21270;&#20215;&#20540;&#35843;&#26597;&#65292;&#24357;&#21512;&#23545;&#35805;&#20195;&#29702;&#20013;&#30340;&#25991;&#21270;&#32454;&#24494;&#24046;&#21035;
&lt;/p&gt;
&lt;p&gt;
Bridging Cultural Nuances in Dialogue Agents through Cultural Value Surveys. (arXiv:2401.10352v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10352
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;cuDialog&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#25991;&#21270;&#20026;&#35270;&#35282;&#30340;&#23545;&#35805;&#29983;&#25104;&#22522;&#20934;&#65292;&#24182;&#24320;&#21457;&#20102;&#33021;&#22815;&#20174;&#23545;&#35805;&#20013;&#25552;&#21462;&#25991;&#21270;&#23646;&#24615;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#32467;&#21512;&#25991;&#21270;&#20215;&#20540;&#35843;&#26597;&#21487;&#20197;&#25552;&#39640;&#23545;&#35805;&#20195;&#29702;&#30340;&#23545;&#20010;&#24615;&#21270;&#21644;&#23545;&#35805;&#36136;&#37327;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20195;&#29702;&#19982;&#25991;&#21270;&#30340;&#20132;&#20114;&#39046;&#22495;&#26159;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#20294;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#21508;&#31181;&#31038;&#20250;&#25991;&#21270;&#26041;&#38754;&#65292;&#20174;&#27807;&#36890;&#39118;&#26684;&#21644;&#20449;&#24565;&#21040;&#20849;&#20139;&#30340;&#38544;&#21947;&#21644;&#30693;&#35782;&#65292;&#37117;&#28145;&#21051;&#22320;&#24433;&#21709;&#30528;&#36825;&#20123;&#20132;&#20114;&#12290;&#20026;&#20102;&#26356;&#28145;&#20837;&#22320;&#30740;&#31350;&#36825;&#19968;&#21160;&#24577;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;cuDialog&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#25991;&#21270;&#20026;&#35270;&#35282;&#30340;&#23545;&#35805;&#29983;&#25104;&#22522;&#20934;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#33021;&#22815;&#20174;&#23545;&#35805;&#20132;&#27969;&#20013;&#25552;&#21462;&#25991;&#21270;&#23646;&#24615;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#35805;&#20195;&#29702;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#36136;&#37327;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#20849;&#21516;&#23398;&#20064;&#25991;&#21270;&#29702;&#35299;&#21644;&#22810;&#36718;&#23545;&#35805;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#25991;&#21270;&#32500;&#24230;&#19982;&#23545;&#35805;&#32534;&#30721;&#29305;&#24449;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21152;&#20837;&#25991;&#21270;&#20215;&#20540;&#35843;&#26597;&#33021;&#22815;&#22686;&#24378;&#19982;&#21442;&#32771;&#25991;&#29486;&#21644;&#25991;&#21270;&#26631;&#35760;&#30340;&#19968;&#33268;&#24615;&#65292;&#26174;&#31034;&#20986;&#23427;&#23545;&#20010;&#24615;&#21270;&#21644;&#23545;&#35805;&#36136;&#37327;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20419;&#36827;&#23545;&#35805;&#20195;&#29702;&#19982;&#25991;&#21270;&#30340;&#20132;&#20114;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cultural landscape of interactions with dialogue agents is a compelling yet relatively unexplored territory. It's clear that various sociocultural aspects -- from communication styles and beliefs to shared metaphors and knowledge -- profoundly impact these interactions. To delve deeper into this dynamic, we introduce cuDialog, a first-of-its-kind benchmark for dialogue generation with a cultural lens. We also develop baseline models capable of extracting cultural attributes from dialogue exchanges, with the goal of enhancing the predictive accuracy and quality of dialogue agents. To effectively co-learn cultural understanding and multi-turn dialogue predictions, we propose to incorporate cultural dimensions with dialogue encoding features. Our experimental findings highlight that incorporating cultural value surveys boosts alignment with references and cultural markers, demonstrating its considerable influence on personalization and dialogue quality. To facilitate further explorati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#23545;&#27604;&#24615;&#20559;&#22909;&#20248;&#21270;&#65288;CPO&#65289;&#30340;&#26041;&#27861;&#65292;&#24357;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#24615;&#33021;&#19982;&#20256;&#32479;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#32763;&#35793;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.08417</link><description>&lt;p&gt;
&#23545;&#27604;&#24615;&#20559;&#22909;&#20248;&#21270;&#65306;&#25512;&#21160;&#26426;&#22120;&#32763;&#35793;&#20013;LLM&#24615;&#33021;&#30340;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation. (arXiv:2401.08417v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#23545;&#27604;&#24615;&#20559;&#22909;&#20248;&#21270;&#65288;CPO&#65289;&#30340;&#26041;&#27861;&#65292;&#24357;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#24615;&#33021;&#19982;&#20256;&#32479;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#32763;&#35793;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#31561;&#35268;&#27169;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#8212;&#8212;7B&#25110;&#32773;13B&#21442;&#25968;&#30340;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#34920;&#29616;&#26368;&#22909;&#30340;13B LLM&#32763;&#35793;&#27169;&#22411;&#65292;&#22914;ALMA&#65292;&#20063;&#26080;&#27861;&#36798;&#21040;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#20256;&#32479;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32763;&#35793;&#27169;&#22411;&#25110;&#32773;&#26356;&#22823;&#35268;&#27169;&#30340;LLM&#65288;&#22914;GPT-4&#65289;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24357;&#21512;&#36825;&#19968;&#24615;&#33021;&#24046;&#36317;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#30417;&#30563;&#24494;&#35843;&#22312;MT&#20219;&#21153;&#20013;&#38024;&#23545;LLM&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#24378;&#35843;&#20102;&#23613;&#31649;&#26159;&#20154;&#24037;&#29983;&#25104;&#30340;&#21442;&#32771;&#25968;&#25454;&#65292;&#20294;&#20854;&#20013;&#23384;&#22312;&#36136;&#37327;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#19982;&#27169;&#20223;&#21442;&#32771;&#32763;&#35793;&#30340;SFT&#30456;&#21453;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#27604;&#24615;&#20559;&#22909;&#20248;&#21270;&#65288;CPO&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35757;&#32451;&#27169;&#22411;&#36991;&#20813;&#29983;&#25104;&#20165;&#20165;&#21512;&#20046;&#35201;&#27714;&#20294;&#19981;&#23436;&#32654;&#30340;&#32763;&#35793;&#12290;&#23558;CPO&#24212;&#29992;&#20110;&#20165;&#26377;22K&#23545;&#21477;&#23376;&#21644;12M&#21442;&#25968;&#30340;ALMA&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#24471;&#21040;&#30340;&#27169;&#22411;&#31216;&#20026;ALMA-R&#65292;&#20854;&#24615;&#33021;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36807;WMT&#27604;&#36187;&#30340;&#33719;&#32988;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Moderate-sized large language models (LLMs) -- those with 7B or 13B parameters -- exhibit promising machine translation (MT) performance. However, even the top-performing 13B LLM-based translation models, like ALMA, does not match the performance of state-of-the-art conventional encoder-decoder translation models or larger-scale LLMs such as GPT-4. In this study, we bridge this performance gap. We first assess the shortcomings of supervised fine-tuning for LLMs in the MT task, emphasizing the quality issues present in the reference data, despite being human-generated. Then, in contrast to SFT which mimics reference translations, we introduce Contrastive Preference Optimization (CPO), a novel approach that trains models to avoid generating adequate but not perfect translations. Applying CPO to ALMA models with only 22K parallel sentences and 12M parameters yields significant improvements. The resulting model, called ALMA-R, can match or exceed the performance of the WMT competition winn
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DQN&#30340;&#22312;&#32447;&#21361;&#26426;&#20107;&#20214;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#24635;&#32467;&#22810;&#20010;&#28798;&#23475;&#30456;&#20851;&#30340;&#25968;&#25454;&#27969;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#25110;&#20869;&#23481;&#37325;&#26032;&#25490;&#24207;&#65292;&#19988;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.06683</link><description>&lt;p&gt;
DQNC2S&#65306;&#22522;&#20110;DQN&#30340;&#36328;&#27969;&#21361;&#26426;&#20107;&#20214;&#25688;&#35201;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
DQNC2S: DQN-based Cross-stream Crisis event Summarizer. (arXiv:2401.06683v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DQN&#30340;&#22312;&#32447;&#21361;&#26426;&#20107;&#20214;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#24635;&#32467;&#22810;&#20010;&#28798;&#23475;&#30456;&#20851;&#30340;&#25968;&#25454;&#27969;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#25110;&#20869;&#23481;&#37325;&#26032;&#25490;&#24207;&#65292;&#19988;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#24635;&#32467;&#22810;&#20010;&#19982;&#28798;&#23475;&#30456;&#20851;&#30340;&#25968;&#25454;&#27969;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#29616;&#26377;&#30340;&#26816;&#32034;&#19982;&#37325;&#26032;&#25490;&#24207;&#31574;&#30053;&#22312;&#22810;&#27969;&#25968;&#25454;&#30340;&#22266;&#26377;&#20887;&#20313;&#21644;&#22810;&#26597;&#35810;&#29615;&#22659;&#19979;&#30340;&#38480;&#21046;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24369;&#26631;&#27880;&#21644;&#28145;&#24230;Q&#32593;&#32476;&#30340;&#22312;&#32447;&#21361;&#26426;&#26102;&#38388;&#36724;&#29983;&#25104;&#26041;&#27861;&#12290;&#23427;&#33021;&#22815;&#23454;&#26102;&#36873;&#25321;&#30456;&#20851;&#30340;&#25991;&#26412;&#29255;&#27573;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#25110;&#20869;&#23481;&#37325;&#26032;&#25490;&#24207;&#65292;&#20174;&#32780;&#20351;&#25512;&#29702;&#26102;&#38388;&#19982;&#36755;&#20837;&#26597;&#35810;&#30340;&#25968;&#37327;&#26080;&#20851;&#12290;&#35813;&#26041;&#27861;&#36824;&#23558;&#20887;&#20313;&#36807;&#28388;&#22120;&#34701;&#20837;&#22870;&#21169;&#20989;&#25968;&#20013;&#65292;&#20197;&#26377;&#25928;&#22788;&#29702;&#36328;&#27969;&#20869;&#23481;&#37325;&#21472;&#12290;&#22312;CrisisFACTS 2022&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25152;&#36798;&#21040;&#30340;ROUGE&#21644;BERTScore&#32467;&#26524;&#20248;&#20110;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Summarizing multiple disaster-relevant data streams simultaneously is particularly challenging as existing Retrieve&amp;Re-ranking strategies suffer from the inherent redundancy of multi-stream data and limited scalability in a multi-query setting. This work proposes an online approach to crisis timeline generation based on weak annotation with Deep Q-Networks. It selects on-the-fly the relevant pieces of text without requiring neither human annotations nor content re-ranking. This makes the inference time independent of the number of input queries. The proposed approach also incorporates a redundancy filter into the reward function to effectively handle cross-stream content overlaps. The achieved ROUGE and BERTScore results are superior to those of best-performing models on the CrisisFACTS 2022 benchmark.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#24515;&#29702;&#20581;&#24247;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#34920;&#36798;&#20154;&#31867;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#26041;&#38754;&#30340;&#29702;&#35299;&#33021;&#21147;&#36229;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.04592</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20102;&#35299;&#24515;&#29702;&#20581;&#24247;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Assessment on Comprehending Mental Health through Large Language Models. (arXiv:2401.04592v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04592
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#24515;&#29702;&#20581;&#24247;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#34920;&#36798;&#20154;&#31867;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#26041;&#38754;&#30340;&#29702;&#35299;&#33021;&#21147;&#36229;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#29702;&#20581;&#24247;&#25361;&#25112;&#23545;&#20010;&#20154;&#21644;&#31038;&#21306;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#20840;&#29699;&#36127;&#25285;&#12290;&#26368;&#36817;&#30340;&#25968;&#25454;&#34920;&#26126;&#65292;&#36229;&#36807;20%&#30340;&#25104;&#24180;&#20154;&#22312;&#20182;&#20204;&#30340;&#19968;&#29983;&#20013;&#21487;&#33021;&#20250;&#36935;&#21040;&#33267;&#23569;&#19968;&#31181;&#24515;&#29702;&#38556;&#30861;&#12290;&#19968;&#26041;&#38754;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#20026;&#21508;&#31181;&#24212;&#29992;&#25552;&#20379;&#20102;&#20415;&#21033;&#65292;&#28982;&#32780;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#30340;&#29702;&#35299;&#21644;&#25552;&#21319;&#20173;&#23384;&#22312;&#37325;&#35201;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#33258;&#28982;&#35821;&#35328;&#20013;&#20154;&#31867;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#34920;&#36798;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#22312;&#35299;&#20915;&#36825;&#19968;&#31354;&#30333;&#30340;&#36807;&#31243;&#20013;&#36827;&#34892;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21021;&#27493;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;Llama-2&#21644;ChatGPT&#19982;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#12290;&#25105;&#20204;&#22312;DAIC-WOZ&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65288;&#22914;BERT&#25110;XLNet&#65289;&#30340;&#24615;&#33021;&#20248;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mental health challenges pose considerable global burdens on individuals and communities. Recent data indicates that more than 20% of adults may encounter at least one mental disorder in their lifetime. On the one hand, the advancements in large language models have facilitated diverse applications, yet a significant research gap persists in understanding and enhancing the potential of large language models within the domain of mental health. On the other hand, across various applications, an outstanding question involves the capacity of large language models to comprehend expressions of human mental health conditions in natural language. This study presents an initial evaluation of large language models in addressing this gap. Due to this, we compare the performance of Llama-2 and ChatGPT with classical Machine as well as Deep learning models. Our results on the DAIC-WOZ dataset show that transformer-based models, like BERT or XLNet, outperform the large language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24773;&#32490;&#20998;&#31867;&#20013;&#30340;&#20027;&#39064;&#20559;&#24046;&#38382;&#39064;&#65292;&#21457;&#29616;&#24773;&#32490;&#35821;&#26009;&#24211;&#20013;&#30340;&#20027;&#39064;&#19982;&#24773;&#32490;&#23454;&#38469;&#19978;&#20855;&#26377;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#24773;&#32490;&#20998;&#31867;&#22120;&#23481;&#26131;&#21463;&#21040;&#36825;&#20123;&#20027;&#39064;&#30340;&#24178;&#25200;&#12290;&#26368;&#21518;&#65292;&#30740;&#31350;&#32773;&#23637;&#31034;&#20102;&#19968;&#31181;&#21435;&#20559;&#24046;&#30340;&#26041;&#27861;&#21487;&#20197;&#20943;&#36731;&#20027;&#39064;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2312.09043</link><description>&lt;p&gt;
&#24773;&#32490;&#20998;&#31867;&#20013;&#30340;&#20027;&#39064;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Topic Bias in Emotion Classification. (arXiv:2312.09043v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.09043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24773;&#32490;&#20998;&#31867;&#20013;&#30340;&#20027;&#39064;&#20559;&#24046;&#38382;&#39064;&#65292;&#21457;&#29616;&#24773;&#32490;&#35821;&#26009;&#24211;&#20013;&#30340;&#20027;&#39064;&#19982;&#24773;&#32490;&#23454;&#38469;&#19978;&#20855;&#26377;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#24773;&#32490;&#20998;&#31867;&#22120;&#23481;&#26131;&#21463;&#21040;&#36825;&#20123;&#20027;&#39064;&#30340;&#24178;&#25200;&#12290;&#26368;&#21518;&#65292;&#30740;&#31350;&#32773;&#23637;&#31034;&#20102;&#19968;&#31181;&#21435;&#20559;&#24046;&#30340;&#26041;&#27861;&#21487;&#20197;&#20943;&#36731;&#20027;&#39064;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#35821;&#26009;&#24211;&#36890;&#24120;&#26159;&#22522;&#20110;&#20851;&#38190;&#35789;/&#26631;&#31614;&#25628;&#32034;&#25110;&#36890;&#36807;&#35810;&#38382;&#30740;&#31350;&#21442;&#19982;&#32773;&#29983;&#25104;&#25991;&#26412;&#23454;&#20363;&#26469;&#37319;&#26679;&#30340;&#12290;&#26080;&#35770;&#21738;&#31181;&#24773;&#20917;&#65292;&#36825;&#20123;&#35821;&#26009;&#24211;&#37117;&#19981;&#26159;&#20195;&#34920;&#39046;&#22495;&#25972;&#20307;&#30340;&#22343;&#21248;&#26679;&#26412;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#31181;&#25968;&#25454;&#33719;&#21462;&#26041;&#24335;&#23548;&#33268;&#20102;&#36825;&#20123;&#35821;&#26009;&#24211;&#20013;&#36807;&#24230;&#21576;&#29616;&#30340;&#20027;&#39064;&#20043;&#38388;&#19981;&#20999;&#23454;&#38469;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#20027;&#39064;&#20559;&#24046;&#21487;&#33021;&#20250;&#23548;&#33268;&#38169;&#35823;&#30340;&#39044;&#27979;&#65292;&#20363;&#22914;&#23545;&#20110;&#23454;&#20363;"I organized the service for my aunt's funeral."&#65292;&#23613;&#31649;&#19982;&#24754;&#20260;&#24773;&#32490;&#26631;&#35760;&#30340;&#23454;&#20363;&#20013;&#30340;&#33900;&#31036;&#20107;&#20214;&#36807;&#24230;&#21576;&#29616;&#65292;&#20294;&#26356;&#36866;&#21512;&#30340;&#24773;&#32490;&#26159;&#33258;&#35946;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#25968;&#25454;&#21644;&#24314;&#27169;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#31181;&#20027;&#39064;&#20559;&#24046;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20027;&#39064;&#24314;&#27169;&#33258;&#21160;&#26631;&#35760;&#20102;&#19968;&#32452;&#24773;&#32490;&#35821;&#26009;&#24211;&#65292;&#24182;&#23637;&#31034;&#20102;&#24773;&#32490;&#23454;&#38469;&#19978;&#19982;&#29305;&#23450;&#20027;&#39064;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#24773;&#32490;&#20998;&#31867;&#22120;&#21463;&#21040;&#36825;&#20123;&#20027;&#39064;&#30340;&#24178;&#25200;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24050;&#24314;&#31435;&#30340;&#21435;&#20559;&#24046;&#26041;&#27861;&#21487;&#20197;&#20943;&#36731;&#20027;&#39064;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion corpora are typically sampled based on keyword/hashtag search or by asking study participants to generate textual instances. In any case, these corpora are not uniform samples representing the entirety of a domain. We hypothesize that this practice of data acquisition leads to unrealistic correlations between overrepresented topics in these corpora that harm the generalizability of models. Such topic bias could lead to wrong predictions for instances like "I organized the service for my aunt's funeral." when funeral events are over-represented for instances labeled with sadness, despite the emotion of pride being more appropriate here. In this paper, we study this topic bias both from the data and the modeling perspective. We first label a set of emotion corpora automatically via topic modeling and show that emotions in fact correlate with specific topics. Further, we see that emotion classifiers are confounded by such topics. Finally, we show that the established debiasing met
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23454;&#20307;&#21305;&#37197;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;LLMs&#23545;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#36739;&#23569;&#19988;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11244</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#20307;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Entity Matching using Large Language Models. (arXiv:2310.11244v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11244
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23454;&#20307;&#21305;&#37197;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;LLMs&#23545;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#36739;&#23569;&#19988;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#21305;&#37197;&#26159;&#21028;&#26029;&#20004;&#20010;&#23454;&#20307;&#25551;&#36848;&#26159;&#21542;&#25351;&#30340;&#26159;&#21516;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#23454;&#20307;&#30340;&#20219;&#21153;&#12290;&#23454;&#20307;&#21305;&#37197;&#26159;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#25104;&#27969;&#31243;&#20013;&#30340;&#26680;&#24515;&#27493;&#39588;&#65292;&#20063;&#26159;&#35768;&#22810;&#30005;&#23376;&#21830;&#21153;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#20123;&#24212;&#29992;&#38656;&#35201;&#23558;&#26469;&#33258;&#19981;&#21516;&#20379;&#24212;&#21830;&#30340;&#20135;&#21697;&#21305;&#37197;&#36215;&#26469;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#23454;&#20307;&#21305;&#37197;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22914;BERT&#25110;RoBERTa&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#20307;&#21305;&#37197;&#20013;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#65288;i&#65289;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#29305;&#23450;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#65307;&#65288;ii&#65289;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#23545;&#20110;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#23454;&#20307;&#19981;&#22815;&#20581;&#22766;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#22522;&#20110;PLMs&#30340;&#21305;&#37197;&#22120;&#30340;&#22791;&#36873;&#26041;&#26696;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;LLMs&#23545;&#39046;&#22495;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#36739;&#23569;&#19988;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;&#25176;&#31649;&#30340;LLMs&#65292;&#22914;GPT3.5&#21644;GPT4&#65292;&#20197;&#21450;&#22522;&#20110;Llama2&#30340;&#24320;&#28304;LLMs&#65292;&#21487;&#20197;&#22312;&#26412;&#22320;&#36816;&#34892;&#12290;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;&#22330;&#26223;&#21644;&#8230;
&lt;/p&gt;
&lt;p&gt;
Entity Matching is the task of deciding whether two entity descriptions refer to the same real-world entity. Entity Matching is a central step in most data integration pipelines and an enabler for many e-commerce applications which require to match products offers from different vendors. State-of-the-art entity matching methods often rely on pre-trained language models (PLMs) such as BERT or RoBERTa. Two major drawbacks of these models for entity matching are that (i) the models require significant amounts of task-specific training data and (ii) the fine-tuned models are not robust concerning out-of-distribution entities. In this paper, we investigate using large language models (LLMs) for entity matching as a less domain-specific training data reliant and more robust alternative to PLM-based matchers. Our study covers hosted LLMs, such as GPT3.5 and GPT4, as well as open source LLMs based on Llama2 which can be run locally. We evaluate these models in a zero-shot scenario as well as a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#36890;&#36807;&#35780;&#20272;&#20351;&#29992;DocRED&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11085</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
In-Context Few-Shot Relation Extraction via Pre-Trained Language Models. (arXiv:2310.11085v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#36890;&#36807;&#35780;&#20272;&#20351;&#29992;DocRED&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#26088;&#22312;&#20174;&#25991;&#26412;&#25991;&#26723;&#20013;&#25512;&#26029;&#32467;&#26500;&#21270;&#30340;&#20154;&#31867;&#30693;&#35782;&#12290;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36890;&#24120;&#26377;&#20004;&#20010;&#38480;&#21046;&#65306;(1)&#23427;&#20204;&#35201;&#27714;&#21629;&#21517;&#23454;&#20307;&#20316;&#20026;&#36755;&#20837;&#25110;&#25512;&#26029;&#23427;&#20204;&#65292;&#20174;&#32780;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#22122;&#22768;&#65292;(2)&#23427;&#20204;&#38656;&#35201;&#20154;&#24037;&#23545;&#25991;&#26723;&#36827;&#34892;&#27880;&#37322;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#30340;&#30740;&#31350;&#32773;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#22312;&#28040;&#38500;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#30340;&#38656;&#27714;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#20851;&#38190;&#24615;&#30340;&#20248;&#21183;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#24494;&#35843;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#22312;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#25105;&#20204;&#20351;&#29992;DocRED&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#36825;&#26159;&#30446;&#21069;&#26368;&#22823;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25552;&#21462;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction aims at inferring structured human knowledge from textual documents. State-of-the-art methods based on language models commonly have two limitations: (1) they require named entities to be either given as input or infer them, which introduces additional noise, and (2) they require human annotations of documents. As a remedy, we present a novel framework for in-context few-shot relation extraction via pre-trained language models. To the best of our knowledge, we are the first to reformulate the relation extraction task as a tailored in-context few-shot learning paradigm. Thereby, we achieve crucial benefits in that we eliminate the need for both named entity recognition and human annotation of documents. Unlike existing methods based on fine-tuning, our framework is flexible in that it can be easily updated for a new set of relations without re-training. We evaluate our framework using DocRED, the largest publicly available dataset for document-level relation extracti
&lt;/p&gt;</description></item><item><title>"&#24341;&#29992;GPT&#30340;&#8220;&#35930;&#40736;&#35797;&#39564;&#8221;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#26234;&#33021;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#65292;&#21033;&#29992;&#26234;&#33021;&#20195;&#29702;&#20195;&#34920;&#20225;&#19994;&#36827;&#34892;&#31454;&#20105;&#21644;&#21246;&#32467;&#30740;&#31350;&#12290;&#23427;&#27604;&#20351;&#29992;&#20154;&#31867;&#20027;&#20307;&#36827;&#34892;&#23454;&#39564;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#23637;&#29616;&#20986;&#36229;&#36234;&#20256;&#32479;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;"</title><link>http://arxiv.org/abs/2308.10974</link><description>&lt;p&gt;
"&#24341;&#29992;GPT&#30340;&#8220;&#35930;&#40736;&#35797;&#39564;&#8221;&#65306;&#19968;&#31181;&#30740;&#31350;&#20225;&#19994;&#31454;&#20105;&#21644;&#21246;&#32467;&#30340;&#21019;&#26032;&#26234;&#33021;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;"
&lt;/p&gt;
&lt;p&gt;
"Guinea Pig Trials" Utilizing GPT: A Novel Smart Agent-Based Modeling Approach for Studying Firm Competition and Collusion. (arXiv:2308.10974v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10974
&lt;/p&gt;
&lt;p&gt;
"&#24341;&#29992;GPT&#30340;&#8220;&#35930;&#40736;&#35797;&#39564;&#8221;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#26234;&#33021;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#65292;&#21033;&#29992;&#26234;&#33021;&#20195;&#29702;&#20195;&#34920;&#20225;&#19994;&#36827;&#34892;&#31454;&#20105;&#21644;&#21246;&#32467;&#30740;&#31350;&#12290;&#23427;&#27604;&#20351;&#29992;&#20154;&#31867;&#20027;&#20307;&#36827;&#34892;&#23454;&#39564;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#23637;&#29616;&#20986;&#36229;&#36234;&#20256;&#32479;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;"
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20225;&#19994;&#31454;&#20105;&#21644;&#21246;&#32467;&#28041;&#21450;&#22797;&#26434;&#30340;&#21160;&#24577;&#65292;&#23588;&#20854;&#26159;&#32771;&#34385;&#21040;&#20225;&#19994;&#20043;&#38388;&#30340;&#27807;&#36890;&#12290;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#22797;&#26434;&#31995;&#32479;&#30340;&#38382;&#39064;&#65292;&#20256;&#32479;&#19978;&#36890;&#36807;&#28041;&#21450;&#20154;&#31867;&#20027;&#20307;&#25110;&#22522;&#20110;&#20195;&#29702;&#30340;&#24314;&#27169;&#26041;&#27861;&#36827;&#34892;&#25506;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#26234;&#33021;&#20195;&#29702;&#24314;&#27169;&#65288;SABM&#65289;&#65292;&#20854;&#20013;&#30001;GPT-4&#25216;&#26415;&#25903;&#25345;&#30340;&#26234;&#33021;&#20195;&#29702;&#20195;&#34920;&#20225;&#19994;&#24182;&#30456;&#20114;&#20132;&#20114;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#25511;&#21046;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#26465;&#20214;&#19979;&#20225;&#19994;&#20215;&#26684;&#31454;&#20105;&#21644;&#21246;&#32467;&#34892;&#20026;&#12290;&#19982;&#20351;&#29992;&#20154;&#31867;&#20027;&#20307;&#36827;&#34892;&#23454;&#39564;&#30456;&#27604;&#65292;SABM&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#21644;&#28789;&#27963;&#24615;&#12290;&#26234;&#33021;&#20195;&#29702;&#25317;&#26377;&#20915;&#31574;&#30340;&#24191;&#27867;&#30693;&#35782;&#24211;&#65292;&#23637;&#29616;&#20986;&#31867;&#20284;&#20154;&#31867;&#30340;&#25112;&#30053;&#33021;&#21147;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#24314;&#27169;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26234;&#33021;&#20195;&#29702;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#23545;&#35805;&#24182;&#20010;&#24615;&#21270;&#65292;&#20351;&#20854;&#25104;&#20026;&#30740;&#31350;&#28041;&#21450;&#27807;&#36890;&#30340;&#22797;&#26434;&#24773;&#20917;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Firm competition and collusion involve complex dynamics, particularly when considering communication among firms. Such issues can be modeled as problems of complex systems, traditionally approached through experiments involving human subjects or agent-based modeling methods. We propose an innovative framework called Smart Agent-Based Modeling (SABM), wherein smart agents, supported by GPT-4 technologies, represent firms, and interact with one another. We conducted a controlled experiment to study firm price competition and collusion behaviors under various conditions. SABM is more cost-effective and flexible compared to conducting experiments with human subjects. Smart agents possess an extensive knowledge base for decision-making and exhibit human-like strategic abilities, surpassing traditional ABM agents. Furthermore, smart agents can simulate human conversation and be personalized, making them ideal for studying complex situations involving communication. Our results demonstrate th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102; GPT-3.5 &#21644; GPT-4 &#22312; APPS &#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#25105;&#20462;&#22797;&#22312; GPT &#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#33258;&#25105;&#20462;&#22797;&#22312;&#36739;&#30701;&#21644;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#25928;&#26524;&#26356;&#22909;&#65292;&#20165;&#22312;&#26576;&#20123;&#20195;&#30721;&#37096;&#20998;&#19978;&#24212;&#29992;&#33258;&#25105;&#20462;&#22797;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#24341;&#23548;&#20462;&#22797;&#26041;&#27861;&#22312; APPS &#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.09896</link><description>&lt;p&gt;
&#25581;&#31192; GPT &#33258;&#25105;&#20462;&#22797;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Demystifying GPT Self-Repair for Code Generation. (arXiv:2306.09896v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102; GPT-3.5 &#21644; GPT-4 &#22312; APPS &#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#25105;&#20462;&#22797;&#22312; GPT &#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#33258;&#25105;&#20462;&#22797;&#22312;&#36739;&#30701;&#21644;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#25928;&#26524;&#26356;&#22909;&#65292;&#20165;&#22312;&#26576;&#20123;&#20195;&#30721;&#37096;&#20998;&#19978;&#24212;&#29992;&#33258;&#25105;&#20462;&#22797;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#24341;&#23548;&#20462;&#22797;&#26041;&#27861;&#22312; APPS &#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25361;&#25112;&#24615;&#32534;&#31243;&#20219;&#21153;&#19978;&#20173;&#38754;&#20020;&#22256;&#38590;&#12290;&#33258;&#25105;&#20462;&#22797;&#8212;&#8212;&#21363;&#27169;&#22411;&#35843;&#35797;&#24182;&#20462;&#22797;&#33258;&#24049;&#30340;&#20195;&#30721;&#8212;&#8212;&#26368;&#36817;&#25104;&#20026;&#25552;&#39640;&#24615;&#33021;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#33258;&#25105;&#20462;&#22797;&#22914;&#20309;&#26377;&#25928;&#22320;&#21457;&#25381;&#20316;&#29992;&#30340;&#30740;&#31350;&#36824;&#38750;&#24120;&#26377;&#38480;&#12290;&#26377;&#20154;&#20250;&#24819;&#30693;&#36947;&#65292;&#24403;&#21516;&#19968;&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#26102;&#65292;&#27169;&#22411;&#31350;&#31455;&#33021;&#21542;&#25552;&#20379;&#20934;&#30830;&#30340;&#21453;&#39304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102; GPT-3.5 &#21644; GPT-4 &#22312; APPS &#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#22810;&#31181;&#32534;&#30721;&#25361;&#25112;&#32452;&#25104;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#31574;&#30053; pass@t&#65292;&#35813;&#31574;&#30053;&#34913;&#37327;&#20102;&#20219;&#21153;&#36890;&#36807;&#29575;&#19982;&#20174;&#27169;&#22411;&#20013;&#25277;&#26679;&#30340;&#24635;&#26631;&#35760;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20165;&#22522;&#20110;&#25277;&#26679;&#30340;&#26041;&#27861;&#30340;&#20844;&#24179;&#27604;&#36739;&#12290;&#36890;&#36807;&#36825;&#31181;&#35780;&#20272;&#31574;&#30053;&#65292;&#25105;&#20204;&#21457;&#29616;&#33258;&#25105;&#20462;&#22797;&#22312; GPT &#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#24182;&#30830;&#23450;&#20102;&#24433;&#21709;&#33258;&#25105;&#20462;&#22797;&#34920;&#29616;&#30340;&#20960;&#20010;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36755;&#20837;&#22122;&#22768;&#36739;&#23569;&#19988;&#27169;&#22411;&#23545;&#21021;&#22987;&#36755;&#20986;&#19981;&#22826;&#33258;&#20449;&#30340;&#36739;&#30701;&#21644;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#65292;&#33258;&#25105;&#20462;&#22797;&#25928;&#26524;&#26356;&#22909;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#20165;&#22312;&#26576;&#20123;&#20195;&#30721;&#37096;&#20998;&#19978;&#24212;&#29992;&#33258;&#25105;&#20462;&#22797;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24341;&#23548;&#20462;&#22797;&#26041;&#27861;&#65292;&#21033;&#29992;&#22806;&#37096;&#21453;&#39304;&#26469;&#22686;&#24378; GPT &#27169;&#22411;&#30340;&#33258;&#25105;&#20462;&#22797;&#33021;&#21147;&#65292;&#22312; APPS &#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable aptitude in code generation but still struggle on challenging programming tasks. Self-repair -in which the model debugs and fixes mistakes in its own code -- has recently become a popular way to boost performance in these settings. However, only very limited studies on how and when self-repair works effectively exist in the literature, and one might wonder to what extent a model is really capable of providing accurate feedback on why the code is wrong when that code was generated by the same model. In this paper, we analyze GPT-3.5 and GPT-4's ability to perform self-repair on APPS, a challenging dataset consisting of diverse coding challenges. To do so, we first establish a new evaluation strategy dubbed pass@t that measures the pass rate of the tasks against the total number of tokens sampled from the model, enabling a fair comparison to purely sampling-based approaches. With this evaluation strategy, we find that the effectiveness
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#30721;&#22120;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#26222;&#36866;&#24615;&#65292;&#34920;&#26126;&#21363;&#20351;&#21482;&#26377;&#21333;&#23618;&#21644;&#21333;&#27880;&#24847;&#21147;&#22836;&#65292;&#20173;&#28982;&#20855;&#26377;&#22270;&#28789;&#23436;&#22791;&#24615;&#65292;&#20854;&#20013;&#21333;&#35789;&#23884;&#20837;&#30340;&#31232;&#30095;&#24615;/&#21487;&#21387;&#32553;&#24615;&#26159;&#24517;&#35201;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.17026</link><description>&lt;p&gt;
&#35770;&#35299;&#30721;&#22120;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the Computational Power of Decoder-Only Transformer Language Models. (arXiv:2305.17026v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#30721;&#22120;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#26222;&#36866;&#24615;&#65292;&#34920;&#26126;&#21363;&#20351;&#21482;&#26377;&#21333;&#23618;&#21644;&#21333;&#27880;&#24847;&#21147;&#22836;&#65292;&#20173;&#28982;&#20855;&#26377;&#22270;&#28789;&#23436;&#22791;&#24615;&#65292;&#20854;&#20013;&#21333;&#35789;&#23884;&#20837;&#30340;&#31232;&#30095;&#24615;/&#21487;&#21387;&#32553;&#24615;&#26159;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31456;&#23545;&#35299;&#30721;&#22120;Transformer&#27169;&#22411;&#30340;&#35745;&#31639;&#26222;&#36866;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#35780;&#20272;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;Transformer&#27169;&#22411;&#30340;&#29702;&#35770;&#25991;&#29486;&#65292;&#24182;&#34920;&#26126;&#20165;&#20351;&#29992;&#21333;&#23618;&#21644;&#21333;&#27880;&#24847;&#21147;&#22836;&#30340;&#35299;&#30721;&#22120;Transformer&#32467;&#26500;&#65292;&#22312;&#21512;&#29702;&#20551;&#35774;&#19979;&#20855;&#22791;&#22270;&#28789;&#23436;&#22791;&#24615;&#12290;&#20174;&#29702;&#35770;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21333;&#35789;&#23884;&#20837;&#30340;&#31232;&#30095;&#24615;/&#21487;&#21387;&#32553;&#24615;&#26159;&#22270;&#28789;&#23436;&#22791;&#24615;&#25104;&#31435;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a theoretical evaluation of the computational universality of decoder-only transformer models. We extend the theoretical literature on transformer models and show that decoder-only transformer architectures (even with only a single layer and single attention head) are Turing complete under reasonable assumptions. From the theoretical analysis, we show sparsity/compressibility of the word embedding to be a necessary condition for Turing completeness to hold.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#25512;&#29702;&#20013;&#30340;&#33258;&#27965;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20551;&#35774;&#33258;&#27965;&#24615;&#21644;&#32452;&#21512;&#33258;&#27965;&#24615;&#20004;&#20010;&#37325;&#35201;&#29305;&#24615;&#65292;&#24182;&#21457;&#29616;GPT-3/-4&#27169;&#22411;&#22312;&#36825;&#20004;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#20102;&#36739;&#24046;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14279</link><description>&lt;p&gt;
LLM&#30340;&#22810;&#27493;&#25512;&#29702;&#20013;&#30340;&#20004;&#20010;&#33258;&#27965;&#22833;&#36133;
&lt;/p&gt;
&lt;p&gt;
Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs. (arXiv:2305.14279v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#25512;&#29702;&#20013;&#30340;&#33258;&#27965;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20551;&#35774;&#33258;&#27965;&#24615;&#21644;&#32452;&#21512;&#33258;&#27965;&#24615;&#20004;&#20010;&#37325;&#35201;&#29305;&#24615;&#65292;&#24182;&#21457;&#29616;GPT-3/-4&#27169;&#22411;&#22312;&#36825;&#20004;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#20102;&#36739;&#24046;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#19978;&#19979;&#25991;&#20026;&#22522;&#30784;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24191;&#27867;&#25104;&#21151;&#65292;&#20294;&#36825;&#31181;&#25104;&#21151;&#36890;&#24120;&#26159;&#36890;&#36807;&#27491;&#30830;&#24615;&#32780;&#19981;&#26159;&#19968;&#33268;&#24615;&#26469;&#35780;&#20272;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#22312;&#35299;&#20915;&#30001;&#22810;&#20010;&#23376;&#27493;&#39588;&#30340;&#31572;&#26696;&#32452;&#25104;&#30340;&#20219;&#21153;&#30340;&#22810;&#27493;&#25512;&#29702;&#20013;&#65292;&#33258;&#27965;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#26631;&#20934;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23545;&#20110;&#22810;&#27493;&#25512;&#29702;&#29305;&#21035;&#37325;&#35201;&#30340;&#33258;&#27965;&#24615;&#31867;&#22411;&#65306;&#20551;&#35774;&#33258;&#27965;&#24615;&#65288;&#27169;&#22411;&#22312;&#20551;&#35774;&#30340;&#20854;&#20182;&#19978;&#19979;&#25991;&#20013;&#30340;&#36755;&#20986;&#39044;&#27979;&#33021;&#21147;&#65289;&#21644;&#32452;&#21512;&#33258;&#27965;&#24615;&#65288;&#24403;&#23558;&#20013;&#38388;&#23376;&#27493;&#39588;&#26367;&#25442;&#20026;&#27169;&#22411;&#23545;&#36825;&#20123;&#27493;&#39588;&#30340;&#36755;&#20986;&#26102;&#65292;&#27169;&#22411;&#30340;&#26368;&#32456;&#36755;&#20986;&#30340;&#19968;&#33268;&#24615;&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;GPT-3/-4&#27169;&#22411;&#30340;&#22810;&#20010;&#21464;&#20307;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#37117;&#34920;&#29616;&#20986;&#20102;&#20302;&#19968;&#33268;&#24615;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved widespread success on a variety of in-context few-shot tasks, but this success is typically evaluated via correctness rather than consistency. We argue that self-consistency is an important criteria for valid multi-step reasoning in tasks where the solution is composed of the answers to multiple sub-steps. We propose two types of self-consistency that are particularly important for multi-step reasoning -hypothetical consistency (a model's ability to predict what its output would be in a hypothetical other context) and compositional consistency (consistency of a model's final outputs when intermediate sub-steps are replaced with the model's outputs for those steps). We demonstrate that multiple variants of the GPT-3/-4 models exhibit poor consistency rates across both types of consistency on a variety of tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#19981;&#21516;&#30340;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#20256;&#32479;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;&#31361;&#20986;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#36129;&#29486;&#21644;&#25361;&#25112;&#65292;&#24182;&#24378;&#35843;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#23637;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.12641</link><description>&lt;p&gt;
&#36229;&#36234;&#35821;&#35328;&#65306;&#21477;&#23376;&#34920;&#31034;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Beyond Words: A Comprehensive Survey of Sentence Representations. (arXiv:2305.12641v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#19981;&#21516;&#30340;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#20256;&#32479;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;&#31361;&#20986;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#36129;&#29486;&#21644;&#25361;&#25112;&#65292;&#24182;&#24378;&#35843;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#23637;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#34920;&#31034;&#24050;&#32463;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#22914;&#26816;&#32034;&#12289;&#38382;&#31572;&#21644;&#25991;&#26412;&#20998;&#31867;&#12290;&#23427;&#20204;&#25429;&#25417;&#21477;&#23376;&#30340;&#35821;&#20041;&#21644;&#21547;&#20041;&#65292;&#20351;&#35745;&#31639;&#26426;&#33021;&#22815;&#29702;&#35299;&#21644;&#25512;&#29702;&#20154;&#31867;&#35821;&#35328;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#23398;&#20064;&#21477;&#23376;&#34920;&#31034;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#21253;&#25324;&#26080;&#30417;&#30563;&#12289;&#30417;&#30563;&#21644;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#19981;&#21516;&#30340;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#20256;&#32479;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#25972;&#29702;&#20102;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#30340;&#25991;&#29486;&#65292;&#31361;&#20986;&#20102;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#36129;&#29486;&#21644;&#25361;&#25112;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#32508;&#36848;&#24378;&#35843;&#20102;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#36825;&#19968;&#39046;&#22495;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#20173;&#28982;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence representations have become a critical component in natural language processing applications, such as retrieval, question answering, and text classification. They capture the semantics and meaning of a sentence, enabling machines to understand and reason over human language. In recent years, significant progress has been made in developing methods for learning sentence representations, including unsupervised, supervised, and transfer learning approaches. In this paper, we provide an overview of the different methods for sentence representation learning, including both traditional and deep learning-based techniques. We provide a systematic organization of the literature on sentence representation learning, highlighting the key contributions and challenges in this area. Overall, our review highlights the progress made in sentence representation learning, the importance of this area in natural language processing, and the challenges that remain. We conclude with directions for fu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#22810;&#20851;&#31995;&#21452;&#26354;&#35789;&#21521;&#37327;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#30001;&#23450;&#20041;&#25152;&#24341;&#36215;&#30340;&#20998;&#23618;&#21644;&#22810;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.07303</link><description>&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#22810;&#20851;&#31995;&#21452;&#26354;&#35789;&#21521;&#37327;
&lt;/p&gt;
&lt;p&gt;
Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions. (arXiv:2305.07303v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#22810;&#20851;&#31995;&#21452;&#26354;&#35789;&#21521;&#37327;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#30001;&#23450;&#20041;&#25152;&#24341;&#36215;&#30340;&#20998;&#23618;&#21644;&#22810;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20351;&#29992;&#20998;&#24067;&#20449;&#24687;&#30340;&#31070;&#32463;&#35789;&#21521;&#37327;&#19968;&#30452;&#20197;&#26469;&#37117;&#33021;&#20026;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#26377;&#29992;&#30340;&#21547;&#20041;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20250;&#23548;&#33268;&#38590;&#20197;&#35299;&#37322;&#21644;&#25511;&#21046;&#30340;&#34920;&#31034;&#12290;&#30456;&#21453;&#65292;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20855;&#26377;&#36882;&#24402;&#30340;&#65292;&#33258;&#35828;&#26126;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#21487;&#20197;&#25903;&#25345;&#33021;&#22815;&#20445;&#30041;&#21521;&#37327;&#31354;&#38388;&#20013;&#26174;&#24335;&#27010;&#24565;&#20851;&#31995;&#21644;&#32422;&#26463;&#30340;&#26032;&#22411;&#34920;&#31034;&#23398;&#20064;&#33539; paradigm&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#12289;&#22810;&#20851;&#31995;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#21512;&#26144;&#23556;&#23450;&#20041;&#21644;&#23450;&#20041;&#26415;&#35821;&#21450;&#20854;&#30456;&#24212;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#20165;&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#35789;&#21521;&#37327;&#12290;&#36890;&#36807;&#33258;&#21160;&#20174;&#23450;&#20041;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#32763;&#35793;&#30446;&#26631;&#35268;&#33539;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#26694;&#26550;&#19987;&#38376;&#35774;&#23450;&#20026;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#25429;&#33719;&#30001;&#23450;&#20041;&#24341;&#36215;&#30340;&#20998;&#23618;&#21644;&#22810;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural-based word embeddings using solely distributional information have consistently produced useful meaning representations for downstream tasks. However, existing approaches often result in representations that are hard to interpret and control. Natural language definitions, on the other side, possess a recursive, self-explanatory semantic structure that can support novel representation learning paradigms able to preserve explicit conceptual relations and constraints in the vector space.  This paper proposes a neuro-symbolic, multi-relational framework to learn word embeddings exclusively from natural language definitions by jointly mapping defined and defining terms along with their corresponding semantic relations. By automatically extracting the relations from definitions corpora and formalising the learning problem via a translational objective, we specialise the framework in hyperbolic space to capture the hierarchical and multi-resolution structure induced by the definitions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;ChatGPT&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;N-back&#20219;&#21153;&#30340;&#34892;&#20026;&#34920;&#29616;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#30456;&#20284;&#65292;&#36825;&#20026;&#35774;&#35745;&#20855;&#26377;&#20154;&#31867;&#32423;&#35748;&#30693;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25552;&#20379;&#20102;&#20851;&#38190;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2305.03731</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;
&lt;/p&gt;
&lt;p&gt;
Assessing Working Memory Capacity of ChatGPT. (arXiv:2305.03731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;ChatGPT&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;N-back&#20219;&#21153;&#30340;&#34892;&#20026;&#34920;&#29616;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#30456;&#20284;&#65292;&#36825;&#20026;&#35774;&#35745;&#20855;&#26377;&#20154;&#31867;&#32423;&#35748;&#30693;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25552;&#20379;&#20102;&#20851;&#38190;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20316;&#35760;&#24518;&#26159;&#20154;&#31867;&#26234;&#33021;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#23427;&#20316;&#20026;&#20449;&#24687;&#20020;&#26102;&#23384;&#20648;&#21644;&#25805;&#20316;&#30340;&#24037;&#20316;&#31354;&#38388;&#12290;&#26412;&#25991;&#36890;&#36807;&#26816;&#26597;ChatGPT&#22312;N-back&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#35843;&#26597;&#20102;&#36825;&#19968;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#24037;&#20316;&#35760;&#24518;&#23545;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#24615;&#65292;&#25509;&#30528;&#20171;&#32461;&#20102;&#35780;&#20272;ChatGPT&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#27604;&#36739;&#20102;ChatGPT&#22312;&#35328;&#35821;&#21644;&#31354;&#38388;N- back&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#34920;&#29616;&#19982;&#25991;&#29486;&#25253;&#36947;&#30340;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#35774;&#35745;&#20855;&#26377;&#20154;&#31867;&#32423;&#35748;&#30693;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#24403;&#21069;&#36827;&#23637;&#25552;&#20379;&#20102;&#20851;&#38190;&#27934;&#23519;&#65292;&#24182;&#20026;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29702;&#35299;&#20154;&#31867;&#24037;&#20316;&#35760;&#24518;&#30340;&#26410;&#26469;&#21162;&#21147;&#25552;&#20379;&#20102;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Working memory is a critical aspect of both human intelligence and artificial intelligence (AI), serving as a workspace for the temporary storage and manipulation of information. This paper investigates working memory capacity of ChatGPT, a state-of-the-art language model, by examining its performance on N-back tasks. We begin by discussing the importance of working memory to humans and AI, followed by the methods employed to assess working memory capacity of ChatGPT. Our study compares behavioral performance of ChatGPT on verbal and spatial N-back tasks to that of human participants reported in the literature, revealing notable similarities. Our findings offer crucial insights into the current progress in designing AI systems with human-level cognitive abilities and hold promise for informing future endeavors aimed at enhancing AI working memory and understanding human working memory through AI models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#36890;&#36807;&#25968;&#25454;&#25972;&#21512;&#26469;&#25552;&#39640;&#22270;&#20687;&#23383;&#24149;&#36136;&#37327;&#30340;&#26041;&#26696;&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#36164;&#28304;&#35757;&#32451;&#20102;&#27604;&#22522;&#20934;&#27169;&#22411;&#26356;&#22909;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.03610</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#22270;&#20687;&#23383;&#24149;&#25968;&#25454;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Data Curation for Image Captioning with Text-to-Image Generative Models. (arXiv:2305.03610v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#36890;&#36807;&#25968;&#25454;&#25972;&#21512;&#26469;&#25552;&#39640;&#22270;&#20687;&#23383;&#24149;&#36136;&#37327;&#30340;&#26041;&#26696;&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#36164;&#28304;&#35757;&#32451;&#20102;&#27604;&#22522;&#20934;&#27169;&#22411;&#26356;&#22909;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#20687;&#23383;&#24149;&#25216;&#26415;&#30340;&#21457;&#23637;&#20027;&#35201;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#26085;&#30410;&#20381;&#36182;&#20110;&#35745;&#31639;&#36164;&#28304;&#21644;&#36234;&#26469;&#36234;&#22823;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#36890;&#36807;&#25968;&#25454;&#25972;&#21512;&#30340;&#20004;&#31181;&#26041;&#27861;&#25506;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#25552;&#39640;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#36136;&#37327;&#26469;&#25913;&#21892;&#24615;&#33021;&#65306;&#19968;&#31181;&#26041;&#27861;&#20551;&#23450;&#30001;&#20110;&#22270;&#20687;&#21644;&#23383;&#24149;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#26576;&#20123;&#31034;&#20363;&#24212;&#35813;&#36991;&#20813;&#20351;&#29992;&#65292;&#21478;&#19968;&#31181;&#26041;&#27861;&#21017;&#20551;&#23450;&#19981;&#21305;&#37197;&#21487;&#20197;&#36890;&#36807;&#26367;&#25442;&#22270;&#20687;&#26469;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in image captioning are mainly driven by large-scale vision-language pretraining, relying heavily on computational resources and increasingly large multimodal datasets. Instead of scaling up pretraining data, we ask whether it is possible to improve performance by improving the quality of the samples in existing datasets. We pursue this question through two approaches to data curation: one that assumes that some examples should be avoided due to mismatches between the image and caption, and one that assumes that the mismatch can be addressed by replacing the image, for which we use the state-of-the-art Stable Diffusion model. These approaches are evaluated using the BLIP model on MS COCO and Flickr30K in both finetuning and few-shot learning settings. Our simple yet effective approaches consistently outperform baselines, indicating that better image captioning models can be trained by curating existing resources. Finally, we conduct a human study to understand the error
&lt;/p&gt;</description></item></channel></rss>