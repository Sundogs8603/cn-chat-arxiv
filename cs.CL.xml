<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>MultiZoo&#21644;MultiBench&#26159;&#29992;&#20110;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#26631;&#20934;&#21270;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#20102;&#22810;&#27169;&#24577;&#31639;&#27861;&#30340;&#23454;&#29616;&#21644;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#20419;&#36827;&#23545;&#22810;&#27169;&#24577;&#27169;&#22411;&#33021;&#21147;&#21644;&#38480;&#21046;&#30340;&#29702;&#35299;&#65292;&#24182;&#30830;&#20445;&#26131;&#29992;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16413</link><description>&lt;p&gt;
MultiZoo &amp; MultiBench: &#29992;&#20110;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#26631;&#20934;&#21270;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
MultiZoo &amp; MultiBench: A Standardized Toolkit for Multimodal Deep Learning. (arXiv:2306.16413v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16413
&lt;/p&gt;
&lt;p&gt;
MultiZoo&#21644;MultiBench&#26159;&#29992;&#20110;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#26631;&#20934;&#21270;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#20102;&#22810;&#27169;&#24577;&#31639;&#27861;&#30340;&#23454;&#29616;&#21644;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#20419;&#36827;&#23545;&#22810;&#27169;&#24577;&#27169;&#22411;&#33021;&#21147;&#21644;&#38480;&#21046;&#30340;&#29702;&#35299;&#65292;&#24182;&#30830;&#20445;&#26131;&#29992;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;&#28041;&#21450;&#25972;&#21512;&#26469;&#33258;&#22810;&#31181;&#24322;&#26500;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#21152;&#24555;&#23545;&#23569;&#30740;&#31350;&#30340;&#27169;&#24577;&#21644;&#20219;&#21153;&#30340;&#36827;&#23637;&#65292;&#21516;&#26102;&#30830;&#20445;&#29616;&#23454;&#19990;&#30028;&#30340;&#31283;&#20581;&#24615;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;MultiZoo&#65292;&#19968;&#20010;&#20844;&#20849;&#24037;&#20855;&#21253;&#65292;&#20854;&#20013;&#21253;&#21547;&gt; 20&#20010;&#26680;&#24515;&#22810;&#27169;&#24577;&#31639;&#27861;&#30340;&#26631;&#20934;&#21270;&#23454;&#29616;&#65292;&#20197;&#21450;MultiBench&#65292;&#19968;&#20010;&#28085;&#30422;15&#20010;&#25968;&#25454;&#38598;&#65292;10&#20010;&#27169;&#24577;&#65292;20&#20010;&#39044;&#27979;&#20219;&#21153;&#21644;6&#20010;&#30740;&#31350;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#12290;&#36825;&#20123;&#25552;&#20379;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#31471;&#21040;&#31471;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#31616;&#21270;&#21644;&#26631;&#20934;&#21270;&#25968;&#25454;&#21152;&#36733;&#12289;&#23454;&#39564;&#35774;&#32622;&#21644;&#27169;&#22411;&#35780;&#20272;&#12290;&#20026;&#20102;&#23454;&#29616;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#65288;1&#65289;&#27867;&#21270;&#33021;&#21147;&#65292;&#65288;2&#65289;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#65292;&#21644;&#65288;3&#65289;&#27169;&#24577;&#40065;&#26834;&#24615;&#12290;MultiBench&#20026;&#26356;&#22909;&#22320;&#20102;&#35299;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#21151;&#33021;&#21644;&#38480;&#21046;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#21516;&#26102;&#30830;&#20445;&#26131;&#20110;&#20351;&#29992;&#12289;&#21487;&#35775;&#38382;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#21253;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiZoo, a public toolkit consisting of standardized implementations of &gt; 20 core multimodal algorithms and MultiBench, a large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. Together, these provide an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, we offer a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench paves the way towards a better understanding of the capabilities and limitations of multimodal models, while ensuring ease of use, accessibility, and reproducibility. Our toolkits are publicly available, wi
&lt;/p&gt;</description></item><item><title>LENS&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#29702;&#22270;&#20687;&#30340;&#35270;&#35273;&#27169;&#22359;&#36755;&#20986;&#36827;&#34892;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#30340;&#22788;&#29702;&#12290;&#23427;&#19981;&#38656;&#35201;&#22810;&#27169;&#24577;&#35757;&#32451;&#21363;&#21487;&#19982;&#26356;&#22823;&#26356;&#22797;&#26434;&#30340;&#31995;&#32479;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.16410</link><description>&lt;p&gt;
&#20197;&#33258;&#28982;&#35821;&#35328;&#20026;&#38236;&#65292;&#23454;&#29616;&#33021;&#30475;&#35265;&#30340;&#35821;&#35328;&#27169;&#22411;&#65306;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;LENS&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language. (arXiv:2306.16410v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16410
&lt;/p&gt;
&lt;p&gt;
LENS&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#29702;&#22270;&#20687;&#30340;&#35270;&#35273;&#27169;&#22359;&#36755;&#20986;&#36827;&#34892;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#30340;&#22788;&#29702;&#12290;&#23427;&#19981;&#38656;&#35201;&#22810;&#27169;&#24577;&#35757;&#32451;&#21363;&#21487;&#19982;&#26356;&#22823;&#26356;&#22797;&#26434;&#30340;&#31995;&#32479;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;LENS&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#20915;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#22270;&#20687;&#30340;&#19968;&#32452;&#29420;&#31435;&#19988;&#39640;&#24230;&#25551;&#36848;&#24615;&#30340;&#35270;&#35273;&#27169;&#22359;&#30340;&#36755;&#20986;&#65292;&#25552;&#20379;&#20851;&#20110;&#22270;&#20687;&#30340;&#35814;&#23613;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#32431;&#35745;&#31639;&#26426;&#35270;&#35273;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#65292;&#22914;&#38646;&#21644;&#23569;&#26679;&#26412;&#30446;&#26631;&#35782;&#21035;&#65292;&#21516;&#26102;&#20063;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;LENS&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#29616;&#25104;&#30340;LLM&#65292;&#24182;&#19988;&#25105;&#20204;&#21457;&#29616;&#26377;LENS&#30340;LLMs&#30340;&#24615;&#33021;&#39640;&#24230;&#31454;&#20105;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#22810;&#27169;&#24577;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;https://github.com/ContextualAI/lens&#24320;&#28304;&#25105;&#20204;&#30340;&#20195;&#30721;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#20132;&#20114;&#24335;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose LENS, a modular approach for tackling computer vision problems by leveraging the power of large language models (LLMs). Our system uses a language model to reason over outputs from a set of independent and highly descriptive vision modules that provide exhaustive information about an image. We evaluate the approach on pure computer vision settings such as zero- and few-shot object recognition, as well as on vision and language problems. LENS can be applied to any off-the-shelf LLM and we find that the LLMs with LENS perform highly competitively with much bigger and much more sophisticated systems, without any multimodal training whatsoever. We open-source our code at https://github.com/ContextualAI/lens and provide an interactive demo.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20840;&#29699;&#35266;&#28857;&#30340;&#20195;&#34920;&#24615;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;&#36328;&#22269;&#35843;&#26597;&#38382;&#39064;&#21644;&#31572;&#26696;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23450;&#20041;&#19968;&#20010;&#30456;&#20284;&#24230;&#24230;&#37327;&#26631;&#20934;&#65292;&#30740;&#31350;&#21457;&#29616;&#40664;&#35748;&#24773;&#20917;&#19979;&#35821;&#35328;&#27169;&#22411;&#30340;&#22238;&#24212;&#26356;&#20542;&#21521;&#20110;&#26576;&#20123;&#20154;&#32676;&#30340;&#35266;&#28857;&#65292;&#20294;&#24403;&#27169;&#22411;&#32771;&#34385;&#29305;&#23450;&#22269;&#23478;&#30340;&#35266;&#28857;&#26102;&#65292;&#22238;&#24212;&#20250;&#26356;&#21152;&#36148;&#36817;&#35813;&#22269;&#23478;&#30340;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.16388</link><description>&lt;p&gt;
&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#20013;&#20027;&#35266;&#20840;&#29699;&#35266;&#28857;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Measuring the Representation of Subjective Global Opinions in Language Models. (arXiv:2306.16388v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20840;&#29699;&#35266;&#28857;&#30340;&#20195;&#34920;&#24615;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;&#36328;&#22269;&#35843;&#26597;&#38382;&#39064;&#21644;&#31572;&#26696;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23450;&#20041;&#19968;&#20010;&#30456;&#20284;&#24230;&#24230;&#37327;&#26631;&#20934;&#65292;&#30740;&#31350;&#21457;&#29616;&#40664;&#35748;&#24773;&#20917;&#19979;&#35821;&#35328;&#27169;&#22411;&#30340;&#22238;&#24212;&#26356;&#20542;&#21521;&#20110;&#26576;&#20123;&#20154;&#32676;&#30340;&#35266;&#28857;&#65292;&#20294;&#24403;&#27169;&#22411;&#32771;&#34385;&#29305;&#23450;&#22269;&#23478;&#30340;&#35266;&#28857;&#26102;&#65292;&#22238;&#24212;&#20250;&#26356;&#21152;&#36148;&#36817;&#35813;&#22269;&#23478;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#33021;&#26080;&#27861;&#20844;&#24179;&#22320;&#20195;&#34920;&#31038;&#20250;&#38382;&#39064;&#20013;&#22810;&#26679;&#21270;&#30340;&#20840;&#29699;&#35266;&#28857;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#23450;&#37327;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#29983;&#25104;&#30340;&#22238;&#31572;&#19982;&#21738;&#20123;&#20154;&#30340;&#35266;&#28857;&#26356;&#20026;&#30456;&#20284;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;GlobalOpinionQA&#65292;&#21253;&#21547;&#20102;&#26469;&#33258;&#36328;&#22269;&#35843;&#26597;&#30340;&#38382;&#39064;&#21644;&#31572;&#26696;&#65292;&#26088;&#22312;&#25429;&#25417;&#19981;&#21516;&#22269;&#23478;&#20851;&#20110;&#20840;&#29699;&#38382;&#39064;&#30340;&#22810;&#26679;&#35266;&#28857;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#24230;&#37327;&#26631;&#20934;&#65292;&#20197;&#22269;&#23478;&#20026;&#26465;&#20214;&#65292;&#37327;&#21270;&#20102;LLM&#29983;&#25104;&#30340;&#35843;&#26597;&#22238;&#31572;&#19982;&#20154;&#31867;&#22238;&#31572;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#32463;&#36807;&#23466;&#27861;AI&#22521;&#35757;&#30340;LLM&#36827;&#34892;&#20102;&#19977;&#20010;&#23454;&#39564;&#65292;&#20998;&#21035;&#32771;&#34385;&#20854;&#24110;&#21161;&#24615;&#12289;&#35802;&#23454;&#24615;&#21644;&#26080;&#23475;&#24615;&#12290;&#40664;&#35748;&#24773;&#20917;&#19979;&#65292;LLM&#30340;&#22238;&#24212;&#26356;&#20542;&#21521;&#20110;&#19982;&#26576;&#20123;&#20154;&#32676;&#30340;&#35266;&#28857;&#26356;&#31867;&#20284;&#65292;&#20363;&#22914;&#26469;&#33258;&#32654;&#22269;&#12289;&#27431;&#27954;&#21644;&#21335;&#32654;&#27954;&#30340;&#20154;&#32676;&#65292;&#20984;&#26174;&#20102;&#28508;&#22312;&#30340;&#20559;&#35265;&#12290;&#24403;&#25105;&#20204;&#25552;&#31034;&#27169;&#22411;&#32771;&#34385;&#26576;&#20010;&#29305;&#23450;&#22269;&#23478;&#30340;&#35266;&#28857;&#26102;&#65292;&#22238;&#24212;&#20250;&#26356;&#21152;&#31867;&#20284;&#20110;&#35813;&#22269;&#23478;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) may not equitably represent diverse global perspectives on societal issues. In this paper, we develop a quantitative framework to evaluate whose opinions model-generated responses are more similar to. We first build a dataset, GlobalOpinionQA, comprised of questions and answers from cross-national surveys designed to capture diverse opinions on global issues across different countries. Next, we define a metric that quantifies the similarity between LLM-generated survey responses and human responses, conditioned on country. With our framework, we run three experiments on an LLM trained to be helpful, honest, and harmless with Constitutional AI. By default, LLM responses tend to be more similar to the opinions of certain populations, such as those from the USA, and some European and South American countries, highlighting the potential for biases. When we prompt the model to consider a particular country's perspective, responses shift to be more similar to the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#31449;&#28857;&#20020;&#24202;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36882;&#24402;&#21644;&#27880;&#24847;&#21147;&#27169;&#22411;&#20197;&#21450;NVFlare&#26694;&#26550;&#12290;&#30740;&#31350;&#24341;&#20837;&#20102;&#20004;&#31181;&#31034;&#20363;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;LSTM&#27169;&#22411;&#21644;BERT&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29702;&#35299;&#19978;&#19979;&#25991;&#21644;&#35821;&#20041;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.16367</link><description>&lt;p&gt;
&#22810;&#31449;&#28857;&#20020;&#24202;&#32852;&#37030;&#23398;&#20064;&#20351;&#29992;&#36882;&#24402;&#21644;&#27880;&#24847;&#21147;&#27169;&#22411;&#20197;&#21450;NVFlare
&lt;/p&gt;
&lt;p&gt;
Multi-Site Clinical Federated Learning using Recursive and Attentive Models and NVFlare. (arXiv:2306.16367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#31449;&#28857;&#20020;&#24202;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36882;&#24402;&#21644;&#27880;&#24847;&#21147;&#27169;&#22411;&#20197;&#21450;NVFlare&#26694;&#26550;&#12290;&#30740;&#31350;&#24341;&#20837;&#20102;&#20004;&#31181;&#31034;&#20363;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;LSTM&#27169;&#22411;&#21644;BERT&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29702;&#35299;&#19978;&#19979;&#25991;&#21644;&#35821;&#20041;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#20581;&#24247;&#25968;&#25454;&#30340;&#24555;&#36895;&#22686;&#38271;&#24341;&#21457;&#20102;&#23545;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65289;&#26469;&#23457;&#26597;&#21307;&#30103;&#35760;&#24405;&#12289;&#20020;&#24202;&#31508;&#35760;&#21644;&#20854;&#20182;&#22522;&#20110;&#25991;&#26412;&#30340;&#20581;&#24247;&#20449;&#24687;&#30340;&#20852;&#36259;&#12290;&#34429;&#28982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#22312;&#22686;&#24378;&#24739;&#32773;&#25252;&#29702;&#21644;&#20915;&#31574;&#21046;&#23450;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#25968;&#25454;&#38544;&#31169;&#21644;&#36981;&#23432;&#27861;&#35268;&#20173;&#28982;&#26159;&#20851;&#38190;&#38382;&#39064;&#12290;&#32852;&#37030;&#23398;&#20064;&#25104;&#20026;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#22810;&#20010;&#32452;&#32455;&#33021;&#22815;&#22312;&#19981;&#20256;&#25773;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#32852;&#37030;&#23398;&#20064;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;&#30001;NVIDIA&#24320;&#21457;&#30340;NVFlare&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#21307;&#30103;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#20010;&#31034;&#20363;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65306;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#30340;&#27169;&#22411;&#21644;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#36716;&#25442;&#65288;BERT&#65289;&#65292;&#23427;&#20204;&#22312;&#29702;&#35299;&#19978;&#19979;&#25991;&#21644;&#35821;&#20041;&#26041;&#38754;&#34920;&#29616;&#20986; exceptional &#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prodigious growth of digital health data has precipitated a mounting interest in harnessing machine learning methodologies, such as natural language processing (NLP), to scrutinize medical records, clinical notes, and other text-based health information. Although NLP techniques have exhibited substantial potential in augmenting patient care and informing clinical decision-making, data privacy and adherence to regulations persist as critical concerns. Federated learning (FL) emerges as a viable solution, empowering multiple organizations to train machine learning models collaboratively without disseminating raw data. This paper proffers a pragmatic approach to medical NLP by amalgamating FL, NLP models, and the NVFlare framework, developed by NVIDIA. We introduce two exemplary NLP models, the Long-Short Term Memory (LSTM)-based model and Bidirectional Encoder Representations from Transformers (BERT), which have demonstrated exceptional performance in comprehending context and semant
&lt;/p&gt;</description></item><item><title>VBN&#26159;&#19968;&#31181;&#21033;&#29992;&#23618;&#27425;&#21644;&#20851;&#31995;&#20449;&#24687;&#30340;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#23454;&#20307;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#25968;&#25454;&#31232;&#32570;&#30340;&#8220;&#38271;&#23614;&#8221;&#23454;&#20307;&#24314;&#27169;&#12290;&#36890;&#36807;&#20351;&#29992;&#23618;&#27425;&#20808;&#39564;&#21644;&#26126;&#30830;&#20851;&#31995;&#32422;&#26463;&#65292;VBN&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24314;&#27169;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#23494;&#24230;&#34920;&#31034;&#23454;&#20307;&#65292;&#23545;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#26469;&#23454;&#29616;&#24555;&#36895;&#30340;&#36817;&#20284;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2306.16326</link><description>&lt;p&gt;
&#36890;&#36807;&#21464;&#20998;&#36125;&#21494;&#26031;&#32593;&#32476;&#23454;&#29616;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning via Variational Bayesian Networks. (arXiv:2306.16326v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16326
&lt;/p&gt;
&lt;p&gt;
VBN&#26159;&#19968;&#31181;&#21033;&#29992;&#23618;&#27425;&#21644;&#20851;&#31995;&#20449;&#24687;&#30340;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#23454;&#20307;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#25968;&#25454;&#31232;&#32570;&#30340;&#8220;&#38271;&#23614;&#8221;&#23454;&#20307;&#24314;&#27169;&#12290;&#36890;&#36807;&#20351;&#29992;&#23618;&#27425;&#20808;&#39564;&#21644;&#26126;&#30830;&#20851;&#31995;&#32422;&#26463;&#65292;VBN&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24314;&#27169;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#23494;&#24230;&#34920;&#31034;&#23454;&#20307;&#65292;&#23545;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#26469;&#23454;&#29616;&#24555;&#36895;&#30340;&#36817;&#20284;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#23454;&#20307;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;-&#21464;&#20998;&#36125;&#21494;&#26031;&#32593;&#32476; (VBN)&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#23618;&#27425;&#21644;&#20851;&#31995;&#20449;&#24687;&#65292;&#24182;&#23545;&#8220;&#38271;&#23614;&#8221;&#20013;&#30340;&#23454;&#20307;&#24314;&#27169;&#29305;&#21035;&#26377;&#29992;&#65292;&#22240;&#20026;&#36825;&#31181;&#24773;&#20917;&#19979;&#25968;&#25454;&#31232;&#32570;&#12290;VBN&#36890;&#36807;&#20004;&#31181;&#20114;&#34917;&#26426;&#21046;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#38271;&#23614;&#23454;&#20307;&#24314;&#27169;&#65306;&#39318;&#20808;&#65292;VBN&#37319;&#29992;&#20102;&#20449;&#24687;&#20016;&#23500;&#30340;&#23618;&#27425;&#20808;&#39564;&#65292;&#20351;&#20849;&#20139;&#20849;&#21516;&#31062;&#20808;&#30340;&#23454;&#20307;&#20043;&#38388;&#30340;&#20449;&#24687;&#20256;&#25773;&#25104;&#20026;&#21487;&#33021;&#12290;&#27492;&#22806;&#65292;VBN&#24314;&#27169;&#20102;&#23454;&#20307;&#20043;&#38388;&#30340;&#26126;&#30830;&#20851;&#31995;&#65292;&#24378;&#21046;&#23454;&#20307;&#20043;&#38388;&#30340;&#20114;&#34917;&#32467;&#26500;&#21644;&#19968;&#33268;&#24615;&#65292;&#24341;&#23548;&#23398;&#20064;&#30340;&#34920;&#31034;&#21521;&#26356;&#26377;&#24847;&#20041;&#30340;&#31354;&#38388;&#24067;&#23616;&#12290;&#20854;&#27425;&#65292;VBN&#36890;&#36807;&#23494;&#24230;&#34920;&#31034;&#23454;&#20307;&#65288;&#32780;&#19981;&#26159;&#21521;&#37327;&#65289;&#65292;&#20174;&#32780;&#23545;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#36215;&#21040;&#34917;&#20805;&#20316;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#30340;&#36817;&#20284;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;VBN&#22312;&#35821;&#35328;&#23398;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Variational Bayesian Network (VBN) - a novel Bayesian entity representation learning model that utilizes hierarchical and relational side information and is particularly useful for modeling entities in the ``long-tail'', where the data is scarce. VBN provides better modeling for long-tail entities via two complementary mechanisms: First, VBN employs informative hierarchical priors that enable information propagation between entities sharing common ancestors. Additionally, VBN models explicit relations between entities that enforce complementary structure and consistency, guiding the learned representations towards a more meaningful arrangement in space. Second, VBN represents entities by densities (rather than vectors), hence modeling uncertainty that plays a complementary role in coping with data scarcity. Finally, we propose a scalable Variational Bayes optimization algorithm that enables fast approximate Bayesian inference. We evaluate the effectiveness of VBN on linguist
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#19971;&#20010;&#38463;&#25289;&#20271;&#35821;NLP&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;GPT-4&#22312;&#20116;&#20010;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;GPT-3.5&#12290;&#30740;&#31350;&#36824;&#23545;&#25361;&#25112;&#24615;&#30340;&#26041;&#35328;&#25968;&#25454;&#38598;&#19978;&#30340;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;LLMs&#22312;&#35813;&#20219;&#21153;&#19978;&#21462;&#24471;&#21331;&#36234;&#32467;&#26524;&#30340;&#35265;&#35299;&#12290;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#20010;&#26041;&#20415;&#35780;&#20272;&#36825;&#20123;&#20219;&#21153;&#30340;&#26032;&#30340;Python&#25509;&#21475;&#12290;</title><link>http://arxiv.org/abs/2306.16322</link><description>&lt;p&gt;
Taqyim: &#20351;&#29992;ChatGPT&#27169;&#22411;&#35780;&#20272;&#38463;&#25289;&#20271;&#35821;NLP&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Taqyim: Evaluating Arabic NLP Tasks Using ChatGPT Models. (arXiv:2306.16322v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16322
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#19971;&#20010;&#38463;&#25289;&#20271;&#35821;NLP&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;GPT-4&#22312;&#20116;&#20010;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;GPT-3.5&#12290;&#30740;&#31350;&#36824;&#23545;&#25361;&#25112;&#24615;&#30340;&#26041;&#35328;&#25968;&#25454;&#38598;&#19978;&#30340;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;LLMs&#22312;&#35813;&#20219;&#21153;&#19978;&#21462;&#24471;&#21331;&#36234;&#32467;&#26524;&#30340;&#35265;&#35299;&#12290;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#20010;&#26041;&#20415;&#35780;&#20272;&#36825;&#20123;&#20219;&#21153;&#30340;&#26032;&#30340;Python&#25509;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#32454;&#35843;&#65292;&#24182;&#19988;&#21253;&#25324;ChatGPT&#65292;&#19968;&#31181;&#26500;&#24314;&#22312;&#35832;&#22914;GPT-3.5&#21644;GPT-4&#31561;LLMs&#20043;&#19978;&#30340;&#32842;&#22825;&#22411;&#27169;&#22411;&#12290;&#23613;&#31649;&#19982;&#33521;&#35821;&#30456;&#27604;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#27604;&#20363;&#36739;&#20302;&#65292;&#20294;&#23427;&#20204;&#22312;&#20854;&#20182;&#35821;&#35328;&#20013;&#20063;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#19971;&#20010;&#19981;&#21516;&#30340;&#38463;&#25289;&#20271;&#35821;NLP&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65306;&#24773;&#24863;&#20998;&#26512;&#12289;&#32763;&#35793;&#12289;&#38899;&#35793;&#12289;&#25913;&#20889;&#12289;&#35789;&#24615;&#26631;&#27880;&#12289;&#25688;&#35201;&#25552;&#21462;&#21644;&#38899;&#32032;&#26631;&#27880;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#22312;&#36825;&#19971;&#20010;&#20219;&#21153;&#20013;&#26377;&#20116;&#20010;&#30340;&#34920;&#29616;&#20248;&#20110;GPT-3.5&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;LLMs&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#35328;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20248;&#24322;&#32467;&#26524;&#30340;&#21407;&#22240;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;Python&#25509;&#21475;https://github.com/ARBML/Taqyim&#65292;&#20197;&#20415;&#36731;&#26494;&#35780;&#20272;&#36825;&#20123;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive performance on various downstream tasks without requiring fine-tuning, including ChatGPT, a chat-based model built on top of LLMs such as GPT-3.5 and GPT-4. Despite having a lower training proportion compared to English, these models also exhibit remarkable capabilities in other languages. In this study, we assess the performance of GPT-3.5 and GPT-4 models on seven distinct Arabic NLP tasks: sentiment analysis, translation, transliteration, paraphrasing, part of speech tagging, summarization, and diacritization. Our findings reveal that GPT-4 outperforms GPT-3.5 on five out of the seven tasks. Furthermore, we conduct an extensive analysis of the sentiment analysis task, providing insights into how LLMs achieve exceptional results on a challenging dialectal dataset. Additionally, we introduce a new Python interface https://github.com/ARBML/Taqyim that facilitates the evaluation of these tasks effortlessly.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20013;&#25991;&#25991;&#26412;&#32416;&#38169;&#30340;&#23545;&#25239;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#21644;&#35821;&#20041;&#26816;&#27979;&#65292;&#22312;&#20013;&#25991;&#21477;&#23376;&#19978;&#19979;&#25991;&#20013;&#22686;&#24378;&#20102;&#23383;&#31526;&#22810;&#20041;&#30340;&#24314;&#27169;&#21644;&#26816;&#27979;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16313</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20013;&#25991;&#25991;&#26412;&#32416;&#38169;&#30340;&#23545;&#25239;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#21644;&#35821;&#20041;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
An Adversarial Multi-Task Learning Method for Chinese Text Correction with Semantic Detection. (arXiv:2306.16313v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20013;&#25991;&#25991;&#26412;&#32416;&#38169;&#30340;&#23545;&#25239;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#21644;&#35821;&#20041;&#26816;&#27979;&#65292;&#22312;&#20013;&#25991;&#21477;&#23376;&#19978;&#19979;&#25991;&#20013;&#22686;&#24378;&#20102;&#23383;&#31526;&#22810;&#20041;&#30340;&#24314;&#27169;&#21644;&#26816;&#27979;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#32416;&#38169;&#65292;&#23588;&#20854;&#26159;&#26356;&#24191;&#27867;&#24212;&#29992;&#22330;&#26223;&#19979;&#30340;&#35821;&#20041;&#32416;&#38169;&#65292;&#23545;&#20110;&#25552;&#39640;&#25991;&#26412;&#30340;&#27969;&#30021;&#24615;&#21644;&#20889;&#20316;&#25928;&#29575;&#26377;&#30528;&#26497;&#39640;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#20013;&#25991;&#21477;&#23376;&#19978;&#19979;&#25991;&#20013;&#23383;&#31526;&#22810;&#20041;&#30340;&#24314;&#27169;&#21644;&#26816;&#27979;&#33021;&#21147;&#12290;&#20854;&#20013;&#65292;&#24341;&#20837;&#20102;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#21644;&#35780;&#20998;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#19968;&#23545;&#19981;&#20165;&#32806;&#21512;&#32780;&#19988;&#23545;&#25239;&#30340;&#23398;&#20064;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#31574;&#30053;&#21644;&#19968;&#20010;&#31574;&#30053;&#32593;&#32476;&#65292;&#20197;&#23454;&#29616;&#24102;&#26377;&#35821;&#20041;&#26816;&#27979;&#30340;&#39640;&#25928;&#20013;&#25991;&#25991;&#26412;&#32416;&#38169;&#20219;&#21153;&#12290;&#23454;&#39564;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#21644;&#20116;&#31181;&#21487;&#27604;&#36739;&#30340;&#26041;&#27861;&#19978;&#36827;&#34892;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20013;&#25991;&#25991;&#26412;&#32416;&#38169;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#23454;&#29616;&#35821;&#20041;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text correction, especially the semantic correction of more widely used scenes, is strongly required to improve, for the fluency and writing efficiency of the text. An adversarial multi-task learning method is proposed to enhance the modeling and detection ability of character polysemy in Chinese sentence context. Wherein, two models, the masked language model and scoring language model, are introduced as a pair of not only coupled but also adversarial learning tasks. Moreover, the Monte Carlo tree search strategy and a policy network are introduced to accomplish the efficient Chinese text correction task with semantic detection. The experiments are executed on three datasets and five comparable methods, and the experimental results show that our method can obtain good performance in Chinese text correction task for better semantic rationality.
&lt;/p&gt;</description></item><item><title>ChatGPT&#22312;&#22269;&#23478;&#21307;&#30103;&#25191;&#29031;&#32771;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22522;&#30784;&#32447;&#24615;&#20195;&#25968;&#25945;&#23398;&#19978;&#23384;&#22312;&#37325;&#22823;&#25968;&#23398;&#38169;&#35823;&#21644;&#36923;&#36753;&#25512;&#26029;&#22833;&#36133;&#30340;&#38382;&#39064;&#65292;&#19981;&#36866;&#21512;&#20316;&#20026;&#23398;&#29983;&#30340;&#25945;&#24072;&#12290;</title><link>http://arxiv.org/abs/2306.16282</link><description>&lt;p&gt;
ChatGPT&#22312;&#22269;&#23478;&#21307;&#30103;&#25191;&#29031;&#32771;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22522;&#30784;&#32447;&#24615;&#20195;&#25968;&#19978;&#19981;&#20339;
&lt;/p&gt;
&lt;p&gt;
ChatGPT may excel in States Medical Licensing Examination but falters in basic Linear Algebra. (arXiv:2306.16282v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16282
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#22269;&#23478;&#21307;&#30103;&#25191;&#29031;&#32771;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22522;&#30784;&#32447;&#24615;&#20195;&#25968;&#25945;&#23398;&#19978;&#23384;&#22312;&#37325;&#22823;&#25968;&#23398;&#38169;&#35823;&#21644;&#36923;&#36753;&#25512;&#26029;&#22833;&#36133;&#30340;&#38382;&#39064;&#65292;&#19981;&#36866;&#21512;&#20316;&#20026;&#23398;&#29983;&#30340;&#25945;&#24072;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#20986;&#29616;&#26159;&#36805;&#36895;&#30340;&#65292;&#34429;&#28982;&#23427;&#22312;&#26576;&#20123;&#39046;&#22495;&#34920;&#29616;&#20986;&#31215;&#26497;&#24433;&#21709;&#65292;&#20294;&#23427;&#30340;&#24433;&#21709;&#24182;&#19981;&#26222;&#36941;&#26377;&#21033;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#38598;&#20013;&#22312;ChatGPT&#22312;&#25968;&#23398;&#25945;&#32946;&#20013;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#25945;&#25480;&#22522;&#30784;&#32447;&#24615;&#20195;&#25968;&#26041;&#38754;&#12290;&#34429;&#28982;ChatGPT&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#19988;&#26377;&#21160;&#21147;&#30340;&#31572;&#26696;&#65292;&#20294;&#25105;&#20204;&#24517;&#39035;&#35748;&#35782;&#21040;&#23427;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#20250;&#20986;&#29616;&#37325;&#22823;&#30340;&#25968;&#23398;&#38169;&#35823;&#65292;&#20197;&#21450;&#22312;&#36923;&#36753;&#25512;&#26029;&#26041;&#38754;&#30340;&#22833;&#36133;&#12290;&#36825;&#20123;&#24773;&#20917;&#24341;&#21457;&#20102;&#23545;&#31995;&#32479;&#23545;&#25968;&#23398;&#30340;&#30495;&#27491;&#29702;&#35299;&#30340;&#25285;&#24551;&#65292;&#22240;&#20026;&#23427;&#20284;&#20046;&#26356;&#20381;&#36182;&#20110;&#35270;&#35273;&#27169;&#24335;&#32780;&#19981;&#26159;&#30495;&#27491;&#30340;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;ChatGPT&#20316;&#20026;&#23398;&#29983;&#30340;&#25945;&#24072;&#30340;&#36866;&#29992;&#24615;&#20063;&#20540;&#24471;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of ChatGPT has been rapid, and although it has demonstrated positive impacts in certain domains, its influence is not universally advantageous. Our analysis focuses on ChatGPT's capabilities in Mathematics Education, particularly in teaching basic Linear Algebra. While there are instances where ChatGPT delivers accurate and well-motivated answers, it is crucial to recognize numerous cases where it makes significant mathematical errors and fails in logical inference. These occurrences raise concerns regarding the system's genuine understanding of mathematics, as it appears to rely more on visual patterns rather than true comprehension. Additionally, the suitability of ChatGPT as a teacher for students also warrants consideration.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36845;&#20195;&#25552;&#31034;&#19982;ChatGPT&#25110;GPT-4&#36827;&#34892;&#22810;&#36718;&#20132;&#20114;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#39135;&#29289;&#24433;&#21709;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20135;&#21697;&#29305;&#23450;&#25351;&#23548;(PSG)&#24320;&#21457;&#20013;&#20855;&#26377;&#28508;&#21147;&#24212;&#29992;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.16275</link><description>&lt;p&gt;
&#21033;&#29992;GPT-4&#36827;&#34892;&#39135;&#29289;&#24433;&#21709;&#25688;&#35201;&#20197;&#36890;&#36807;&#36845;&#20195;&#25552;&#31034;&#22686;&#24378;&#20135;&#21697;&#29305;&#23450;&#25351;&#23548;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;
Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting. (arXiv:2306.16275v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36845;&#20195;&#25552;&#31034;&#19982;ChatGPT&#25110;GPT-4&#36827;&#34892;&#22810;&#36718;&#20132;&#20114;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#39135;&#29289;&#24433;&#21709;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20135;&#21697;&#29305;&#23450;&#25351;&#23548;(PSG)&#24320;&#21457;&#20013;&#20855;&#26377;&#28508;&#21147;&#24212;&#29992;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#29289;&#24433;&#21709;&#20174;&#26032;&#33647;&#30003;&#35831;(NDA)&#20013;&#30340;&#25688;&#35201;&#26159;&#20135;&#21697;&#29305;&#23450;&#25351;&#23548;(PSG)&#24320;&#21457;&#21644;&#35780;&#20272;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20174;&#22823;&#37327;&#33647;&#29289;&#30003;&#35831;&#23457;&#26597;&#25991;&#20214;&#20013;&#25163;&#21160;&#25688;&#35201;&#39135;&#29289;&#24433;&#21709;&#26159;&#32791;&#26102;&#30340;&#65292;&#36825;&#24341;&#21457;&#20102;&#24320;&#21457;&#33258;&#21160;&#21270;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22914;ChatGPT&#21644;GPT-4&#30340;&#36827;&#23637;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#25913;&#21892;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#25928;&#26524;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#20854;&#22312;PSG&#35780;&#20272;&#20013;&#20934;&#30830;&#27010;&#25324;&#39135;&#29289;&#24433;&#21709;&#30340;&#33021;&#21147;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21363;&#36845;&#20195;&#25552;&#31034;&#65292;&#36890;&#36807;&#22810;&#36718;&#20132;&#20114;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#22320;&#19982;ChatGPT&#25110;GPT-4&#36827;&#34892;&#20114;&#21160;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#36718;&#36845;&#20195;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#39135;&#29289;&#24433;&#21709;&#25688;&#35201;&#65292;&#20854;&#20013;&#22312;&#36830;&#32493;&#30340;&#36718;&#27425;&#20013;&#20998;&#21035;&#25552;&#20379;&#20102;&#20851;&#38190;&#23383;&#32858;&#28966;&#21644;&#38271;&#24230;&#25511;&#21046;&#30340;&#25552;&#31034;&#20197;&#32454;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Food effect summarization from New Drug Application (NDA) is an essential component of product-specific guidance (PSG) development and assessment. However, manual summarization of food effect from extensive drug application review documents is time-consuming, which arouses a need to develop automated methods. Recent advances in large language models (LLMs) such as ChatGPT and GPT-4, have demonstrated great potential in improving the effectiveness of automated text summarization, but its ability regarding the accuracy in summarizing food effect for PSG assessment remains unclear. In this study, we introduce a simple yet effective approach, iterative prompting, which allows one to interact with ChatGPT or GPT-4 more effectively and efficiently through multi-turn interaction. Specifically, we propose a three-turn iterative prompting approach to food effect summarization in which the keyword-focused and length-controlled prompts are respectively provided in consecutive turns to refine the 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#38463;&#23500;&#27735;&#26222;&#20160;&#22270;&#35821;&#21464;&#20307;&#30340;&#24773;&#32490;&#26631;&#27880;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;7600&#26465;&#25512;&#25991;&#65292;&#20197;&#30740;&#31350;&#22612;&#21033;&#29677;&#31105;&#27490;&#22919;&#22899;&#25509;&#21463;&#25945;&#32946;&#30340;&#24773;&#32490;&#21453;&#24212;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#31070;&#32463;&#26550;&#26500;&#23545;&#36798;&#37324;&#35821;&#24773;&#24863;&#20998;&#31867;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2306.16268</link><description>&lt;p&gt;
&#22312;&#38463;&#23500;&#27735;&#31105;&#27490;&#25945;&#32946;&#30340;&#25512;&#25991;&#24773;&#32490;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Emotion Analysis of Tweets Banning Education in Afghanistan. (arXiv:2306.16268v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16268
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#38463;&#23500;&#27735;&#26222;&#20160;&#22270;&#35821;&#21464;&#20307;&#30340;&#24773;&#32490;&#26631;&#27880;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;7600&#26465;&#25512;&#25991;&#65292;&#20197;&#30740;&#31350;&#22612;&#21033;&#29677;&#31105;&#27490;&#22919;&#22899;&#25509;&#21463;&#25945;&#32946;&#30340;&#24773;&#32490;&#21453;&#24212;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#31070;&#32463;&#26550;&#26500;&#23545;&#36798;&#37324;&#35821;&#24773;&#24863;&#20998;&#31867;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#38463;&#23500;&#27735;&#26222;&#20160;&#22270;&#35821;&#21464;&#20307;&#30340;&#24773;&#32490;&#26631;&#27880;&#25968;&#25454;&#38598;&#12290;LetHerLearn&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;7600&#26465;&#25512;&#25991;&#65292;&#36825;&#20123;&#25512;&#25991;&#26159;&#23545;&#22612;&#21033;&#29677;&#20110;2022&#24180;&#31105;&#27490;&#22919;&#22899;&#25509;&#21463;&#25945;&#32946;&#30340;&#21453;&#24212;&#65292;&#24182;&#19988;&#24050;&#26681;&#25454;&#22467;&#20811;&#26364;&#24773;&#32490;&#31867;&#21035;&#36827;&#34892;&#20102;&#25163;&#21160;&#26631;&#27880;&#12290;&#25105;&#20204;&#22312;&#27492;&#35814;&#32454;&#20171;&#32461;&#20102;&#25968;&#25454;&#25910;&#38598;&#21644;&#26631;&#27880;&#36807;&#31243;&#65292;&#21576;&#29616;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#32479;&#35745;&#20449;&#24687;&#20197;&#21450;&#23545;&#25152;&#24471;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#21021;&#27493;&#23454;&#39564;&#65292;&#23545;&#36798;&#37324;&#35821;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#20102;&#22810;&#31181;&#19981;&#21516;&#31070;&#32463;&#26550;&#26500;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the first emotion annotated dataset for the Dari variant of Persian spoken in Afghanistan. The LetHerLearn dataset contains 7,600 tweets posted in reaction to the Taliban ban of women rights to education in 2022 and has been manually annotated according to Ekman emotion categories. We here detail the data collection and annotation process, present relevant dataset statistics as well as initial experiments on the resulting dataset, benchmarking a number of different neural architectures for the task of Dari emotion classification.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#21516;&#21512;&#20316;&#26500;&#24314;&#30340;CBBQ&#20013;&#25991;&#20559;&#24046;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20840;&#38754;&#34913;&#37327;&#20102;&#19982;&#20013;&#22269;&#25991;&#21270;&#21644;&#20215;&#20540;&#35266;&#30456;&#20851;&#30340;14&#20010;&#31038;&#20250;&#32500;&#24230;&#20013;&#30340;&#21051;&#26495;&#21360;&#35937;&#21644;&#31038;&#20250;&#20559;&#35265;&#65292;&#23545;&#20110;&#26816;&#27979;&#27169;&#22411;&#20559;&#35265;&#20855;&#26377;&#24191;&#27867;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#39640;&#24230;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16244</link><description>&lt;p&gt;
CBBQ: &#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#21516;&#21512;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#20013;&#25991;&#20559;&#24046;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CBBQ: A Chinese Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models. (arXiv:2306.16244v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16244
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#21516;&#21512;&#20316;&#26500;&#24314;&#30340;CBBQ&#20013;&#25991;&#20559;&#24046;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20840;&#38754;&#34913;&#37327;&#20102;&#19982;&#20013;&#22269;&#25991;&#21270;&#21644;&#20215;&#20540;&#35266;&#30456;&#20851;&#30340;14&#20010;&#31038;&#20250;&#32500;&#24230;&#20013;&#30340;&#21051;&#26495;&#21360;&#35937;&#21644;&#31038;&#20250;&#20559;&#35265;&#65292;&#23545;&#20110;&#26816;&#27979;&#27169;&#22411;&#20559;&#35265;&#20855;&#26377;&#24191;&#27867;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#39640;&#24230;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#38754;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#20559;&#35265;&#23545;&#20110;&#26816;&#27979;&#21644;&#38477;&#20302;&#39640;&#33021;&#21147;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#36947;&#24503;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#30001;&#20154;&#31867;&#19987;&#23478;&#21644;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20849;&#21516;&#26500;&#24314;&#30340;&#20013;&#25991;&#20559;&#24046;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#19982;&#20013;&#22269;&#25991;&#21270;&#21644;&#20215;&#20540;&#35266;&#30456;&#20851;&#30340;14&#20010;&#31038;&#20250;&#32500;&#24230;&#20013;&#30340;&#21051;&#26495;&#21360;&#35937;&#21644;&#31038;&#20250;&#20559;&#35265;&#12290;&#22312;&#25968;&#25454;&#38598;&#30340;&#25972;&#29702;&#36807;&#31243;&#20013;&#65292;&#21253;&#25324;4&#20010;&#20851;&#38190;&#27493;&#39588;&#65306;&#36890;&#36807;&#24191;&#27867;&#30340;&#25991;&#29486;&#35780;&#35770;&#35782;&#21035;&#20559;&#35265;&#65292;&#29983;&#25104;&#27169;&#31946;&#30340;&#19978;&#19979;&#25991;&#65292;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#28040;&#38500;&#27169;&#31946;&#30340;&#19978;&#19979;&#25991;&#65292;&#20197;&#21450;&#25163;&#21160;&#23457;&#26597;&#21644;&#37325;&#32452;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#27979;&#35797;&#23454;&#20363;&#26159;&#20174;3000&#22810;&#20010;&#32463;&#36807;&#20005;&#26684;&#36136;&#37327;&#25511;&#21046;&#30340;&#39640;&#36136;&#37327;&#27169;&#26495;&#25163;&#21160;&#25552;&#21462;&#30340;&#12290;&#25968;&#25454;&#38598;&#20855;&#26377;&#24191;&#27867;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#39640;&#24230;&#30340;&#22810;&#26679;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#25968;&#25454;&#38598;&#22312;&#26816;&#27979;&#27169;&#22411;&#20559;&#35265;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;10&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22343;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Holistically measuring societal biases of large language models is crucial for detecting and reducing ethical risks in highly capable AI models. In this work, we present a Chinese Bias Benchmark dataset that consists of over 100K questions jointly constructed by human experts and generative language models, covering stereotypes and societal biases in 14 social dimensions related to Chinese culture and values. The curation process contains 4 essential steps: bias identification via extensive literature review, ambiguous context generation, AI-assisted disambiguous context generation, snd manual review \&amp; recomposition. The testing instances in the dataset are automatically derived from 3K+ high-quality templates manually authored with stringent quality control. The dataset exhibits wide coverage and high diversity. Extensive experiments demonstrate the effectiveness of the dataset in detecting model bias, with all 10 publicly available Chinese large language models exhibiting strong bia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#35266;&#23519;&#34892;&#21160;&#21644;&#25351;&#20196;&#25512;&#26029;&#21512;&#20316;&#22242;&#38431;&#30340;&#30446;&#26631;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16207</link><description>&lt;p&gt;
&#20174;&#34892;&#21160;&#21644;&#25351;&#20196;&#20013;&#25512;&#26029;&#27807;&#36890;&#20195;&#29702;&#30340;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Inferring the Goals of Communicating Agents from Actions and Instructions. (arXiv:2306.16207v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#35266;&#23519;&#34892;&#21160;&#21644;&#25351;&#20196;&#25512;&#26029;&#21512;&#20316;&#22242;&#38431;&#30340;&#30446;&#26631;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#31867;&#21512;&#20316;&#26102;&#65292;&#20182;&#20204;&#32463;&#24120;&#36890;&#36807;&#21475;&#22836;&#27807;&#36890;&#21644;&#38750;&#21475;&#22836;&#34892;&#21160;&#26469;&#21327;&#35843;&#27963;&#21160;&#65292;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#26469;&#25512;&#26029;&#20849;&#21516;&#30340;&#30446;&#26631;&#21644;&#35745;&#21010;&#12290;&#25105;&#20204;&#22914;&#20309;&#24314;&#27169;&#36825;&#31181;&#25512;&#29702;&#33021;&#21147;&#65311;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21512;&#20316;&#22242;&#38431;&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#20010;&#20195;&#29702;&#65288;&#20027;&#35201;&#30340;&#65289;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21521;&#21478;&#19968;&#20010;&#20195;&#29702;&#65288;&#21161;&#29702;&#65289;&#20256;&#36798;&#20851;&#20110;&#20849;&#21516;&#35745;&#21010;&#30340;&#20449;&#24687;&#65292;&#20351;&#29992;GPT-3&#20316;&#20026;&#25351;&#20196;&#35821;&#21477;&#30340;&#20284;&#28982;&#20989;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#31532;&#19977;&#26041;&#35266;&#23519;&#32773;&#22914;&#20309;&#36890;&#36807;&#22810;&#27169;&#24577;&#36125;&#21494;&#26031;&#36870;&#21521;&#35268;&#21010;&#20174;&#34892;&#21160;&#21644;&#25351;&#20196;&#20013;&#25512;&#26029;&#22242;&#38431;&#30340;&#30446;&#26631;&#65292;&#35745;&#31639;&#22312;&#20195;&#29702;&#20154;&#20250;&#37319;&#21462;&#21644;&#20132;&#27969;&#20197;&#23454;&#29616;&#30446;&#26631;&#30340;&#20551;&#35774;&#19979;&#30340;&#30446;&#26631;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#19982;&#22810;&#20195;&#29702;&#31995;&#32479;&#30340;&#32593;&#26684;&#19990;&#30028;&#20013;&#30340;&#20154;&#31867;&#30446;&#26631;&#25512;&#26029;&#36827;&#34892;&#27604;&#36739;&#26469;&#35780;&#20272;&#36825;&#31181;&#26041;&#27861;&#65292;&#21457;&#29616;&#25105;&#20204;&#27169;&#22411;&#30340;&#25512;&#26029;&#19982;&#20154;&#31867;&#21028;&#26029;&#23494;&#20999;&#30456;&#20851;&#65288;R = 0.96&#65289;&#12290;&#19982;&#20165;&#20174;&#34892;&#21160;&#25512;&#26029;&#30456;&#27604;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
When humans cooperate, they frequently coordinate their activity through both verbal communication and non-verbal actions, using this information to infer a shared goal and plan. How can we model this inferential ability? In this paper, we introduce a model of a cooperative team where one agent, the principal, may communicate natural language instructions about their shared plan to another agent, the assistant, using GPT-3 as a likelihood function for instruction utterances. We then show how a third person observer can infer the team's goal via multi-modal Bayesian inverse planning from actions and instructions, computing the posterior distribution over goals under the assumption that agents will act and communicate rationally to achieve them. We evaluate this approach by comparing it with human goal inferences in a multi-agent gridworld, finding that our model's inferences closely correlate with human judgments (R = 0.96). When compared to inference from actions alone, we also find th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#21160;&#24577;&#22270;&#30693;&#35782;&#32858;&#21512;&#26469;&#25552;&#21319;&#23545;&#35805;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#26469;&#33258;&#24086;&#23376;&#21644;&#22806;&#37096;&#22270;&#30693;&#35782;&#30340;&#24322;&#36136;&#29305;&#24449;&#65292;&#20174;&#32780;&#35299;&#20915;&#22270;&#30693;&#35782;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#24322;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16195</link><description>&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#22270;&#30693;&#35782;&#32858;&#21512;&#25552;&#21319;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing Dialogue Generation via Dynamic Graph Knowledge Aggregation. (arXiv:2306.16195v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#21160;&#24577;&#22270;&#30693;&#35782;&#32858;&#21512;&#26469;&#25552;&#21319;&#23545;&#35805;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#26469;&#33258;&#24086;&#23376;&#21644;&#22806;&#37096;&#22270;&#30693;&#35782;&#30340;&#24322;&#36136;&#29305;&#24449;&#65292;&#20174;&#32780;&#35299;&#20915;&#22270;&#30693;&#35782;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#24322;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22806;&#37096;&#22270;&#30693;&#35782;&#34701;&#20837;&#31070;&#32463;&#23545;&#35805;&#26426;&#22120;&#20154;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#26377;&#25928;&#25552;&#21319;&#23545;&#35805;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#22312;&#20256;&#32479;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20013;&#65292;&#22270;&#19978;&#30340;&#20449;&#24687;&#20256;&#36882;&#19982;&#25991;&#26412;&#26080;&#20851;&#65292;&#23548;&#33268;&#22270;&#34920;&#24449;&#21644;&#25991;&#26412;&#20043;&#38388;&#23384;&#22312;&#35821;&#20041;&#24046;&#24322;&#12290;&#29616;&#26377;&#27169;&#22411;&#30340;&#35757;&#32451;&#26041;&#24335;&#23548;&#33268;&#20102;&#22270;&#30693;&#35782;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#40511;&#27807;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#22270;&#22686;&#24378;&#23545;&#35805;&#29983;&#25104;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#21160;&#24577;&#26500;&#24314;&#19968;&#20010;&#24102;&#26377;&#20266;&#33410;&#28857;&#30340;&#22810;&#36339;&#30693;&#35782;&#22270;&#65292;&#22312;&#22270;&#20013;&#30340;&#27599;&#19968;&#27493;&#20013;&#37117;&#23558;&#35821;&#35328;&#27169;&#22411;&#32435;&#20837;&#29305;&#24449;&#32858;&#21512;&#12290;&#20026;&#20102;&#36991;&#20813;&#23398;&#20064;&#22312;&#26222;&#36890;&#23376;&#22270;&#19978;&#24341;&#36215;&#30340;&#35821;&#20041;&#20559;&#24046;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#24212;&#29992;&#20102;&#20998;&#23618;&#22270;&#27880;&#24847;&#21147;&#65292;&#20197;&#32858;&#21512;&#20266;&#33410;&#28857;&#19978;&#30340;&#22270;&#29305;&#24449;&#65292;&#26368;&#32456;&#33719;&#24471;&#20840;&#23616;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#26469;&#33258;&#24086;&#23376;&#21644;&#22806;&#37096;&#22270;&#30693;&#35782;&#30340;&#24322;&#36136;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating external graph knowledge into neural chatbot models has been proven effective for enhancing dialogue generation. However, in conventional graph neural networks (GNNs), message passing on a graph is independent from text, resulting in the graph representation hidden space differing from that of the text. This training regime of existing models therefore leads to a semantic gap between graph knowledge and text. In this study, we propose a novel framework for knowledge graph enhanced dialogue generation. We dynamically construct a multi-hop knowledge graph with pseudo nodes to involve the language model in feature aggregation within the graph at all steps. To avoid the semantic biases caused by learning on vanilla subgraphs, the proposed framework applies hierarchical graph attention to aggregate graph features on pseudo nodes and then attains a global feature. Therefore, the framework can better utilise the heterogeneous features from both the post and external graph knowle
&lt;/p&gt;</description></item><item><title>&#23398;&#26415;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SkillNet-X&#30340;&#22810;&#35821;&#35328;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#36890;&#36807;&#31232;&#30095;&#28608;&#27963;&#30456;&#20851;&#30340;&#25216;&#33021;&#27169;&#22359;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#22810;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.16176</link><description>&lt;p&gt;
SkillNet-X&#65306;&#19968;&#31181;&#20855;&#26377;&#31232;&#30095;&#28608;&#27963;&#25216;&#33021;&#30340;&#22810;&#35821;&#35328;&#22810;&#20219;&#21153;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SkillNet-X: A Multilingual Multitask Model with Sparsely Activated Skills. (arXiv:2306.16176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16176
&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SkillNet-X&#30340;&#22810;&#35821;&#35328;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#36890;&#36807;&#31232;&#30095;&#28608;&#27963;&#30456;&#20851;&#30340;&#25216;&#33021;&#27169;&#22359;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#22810;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#21482;&#33021;&#22312;&#20219;&#21153;&#25110;&#35821;&#35328;&#19978;&#21033;&#29992;&#36890;&#29992;&#30693;&#35782;&#65292;&#21363;&#22833;&#21435;&#20102;&#36328;&#35821;&#35328;&#25110;&#36328;&#20219;&#21153;&#30340;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SkillNet-X&#30340;&#36890;&#29992;&#22810;&#35821;&#35328;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#21333;&#19968;&#27169;&#22411;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#30340;&#22810;&#20010;&#19981;&#21516;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#20960;&#20010;&#35821;&#35328;&#29305;&#23450;&#30340;&#25216;&#33021;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#25216;&#33021;&#65292;&#27599;&#20010;&#25216;&#33021;&#23545;&#24212;&#19968;&#20010;&#25216;&#33021;&#27169;&#22359;&#12290;SkillNet-X&#31232;&#30095;&#28608;&#27963;&#19982;&#30446;&#26631;&#20219;&#21153;&#25110;&#30446;&#26631;&#35821;&#35328;&#30456;&#20851;&#30340;&#25216;&#33021;&#27169;&#22359;&#30340;&#37096;&#20998;&#12290;&#20316;&#20026;&#30693;&#35782;&#20256;&#36882;&#20013;&#24515;&#65292;&#25216;&#33021;&#27169;&#22359;&#33021;&#22815;&#36830;&#32493;&#21560;&#25910;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#30693;&#35782;&#21644;&#35821;&#35328;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;&#22522;&#20110;Transformer&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;&#23618;&#21644;&#21069;&#39304;&#32593;&#32476;&#23618;&#20197;&#36866;&#24212;&#25216;&#33021;&#27169;&#22359;&#12290;&#25105;&#20204;&#22312;&#22235;&#31181;&#35821;&#35328;&#30340;&#21313;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;SkillNet-X&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;SkillNet-X&#30340;&#24615;&#33021;&#20248;&#20110;&#21333;&#29420;&#38024;&#23545;&#27599;&#20010;&#20219;&#21153;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional multitask learning methods basically can only exploit common knowledge in task- or language-wise, which lose either cross-language or cross-task knowledge. This paper proposes a general multilingual multitask model, named SkillNet-X, which enables a single model to tackle many different tasks from different languages. To this end, we define several language-specific skills and task-specific skills, each of which corresponds to a skill module. SkillNet-X sparsely activates parts of the skill modules which are relevant either to the target task or the target language. Acting as knowledge transit hubs, skill modules are capable of absorbing task-related knowledge and language-related knowledge consecutively. Based on Transformer, we modify the multi-head attention layer and the feed forward network layer to accommodate skill modules. We evaluate SkillNet-X on eleven natural language understanding datasets in four languages. Results show that SkillNet-X performs better than tas
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24320;&#21457;&#39046;&#22495;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#25972;&#21512;&#29983;&#25104;&#24335;&#29992;&#25143;&#20307;&#39564;&#30740;&#31350;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#39046;&#22495;&#29992;&#25143;&#32435;&#20837;&#21407;&#22411;&#24320;&#21457;&#30340;&#19981;&#21516;&#38454;&#27573;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#29992;&#25143;&#38656;&#27714;&#21644;&#35780;&#20272;&#29992;&#25143;&#20215;&#20540;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.16143</link><description>&lt;p&gt;
&#20026;&#24320;&#21457;&#39046;&#22495;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#32780;&#36827;&#34892;&#30340;&#29983;&#25104;&#24335;&#29992;&#25143;&#20307;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Generative User-Experience Research for Developing Domain-specific Natural Language Processing Applications. (arXiv:2306.16143v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24320;&#21457;&#39046;&#22495;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#25972;&#21512;&#29983;&#25104;&#24335;&#29992;&#25143;&#20307;&#39564;&#30740;&#31350;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#39046;&#22495;&#29992;&#25143;&#32435;&#20837;&#21407;&#22411;&#24320;&#21457;&#30340;&#19981;&#21516;&#38454;&#27573;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#29992;&#25143;&#38656;&#27714;&#21644;&#35780;&#20272;&#29992;&#25143;&#20215;&#20540;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#20307;&#39564;&#65288;UX&#65289;&#26159;&#20154;&#26426;&#20132;&#20114;&#65288;HCI&#65289;&#30740;&#31350;&#30340;&#19968;&#37096;&#20998;&#65292;&#19987;&#27880;&#20110;&#25552;&#39640;&#31995;&#32479;&#29992;&#25143;&#30340;&#30452;&#35266;&#24615;&#12289;&#36879;&#26126;&#24230;&#12289;&#31616;&#27905;&#24615;&#21644;&#20449;&#20219;&#24230;&#12290;&#22823;&#22810;&#25968;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;UX&#30740;&#31350;&#37117;&#37319;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#21363;&#27809;&#26377;&#20851;&#27880;&#29992;&#25143;&#38656;&#27714;&#65292;&#24182;&#20165;&#20165;&#23558;&#39046;&#22495;&#29992;&#25143;&#29992;&#20110;&#21487;&#29992;&#24615;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#26356;&#20856;&#22411;&#30340;UX&#26041;&#27861;&#26159;&#20808;&#38024;&#23545;&#29992;&#25143;&#30340;&#21487;&#29992;&#24615;&#36827;&#34892;&#23450;&#21046;&#65292;&#32780;&#19981;&#26159;&#39318;&#20808;&#20102;&#35299;&#29992;&#25143;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;UX&#30740;&#31350;&#25972;&#21512;&#21040;&#24320;&#21457;&#39046;&#22495;NLP&#24212;&#29992;&#20013;&#30340;&#26041;&#27861;&#12290;&#29983;&#25104;&#24335;UX&#30740;&#31350;&#23558;&#39046;&#22495;&#29992;&#25143;&#32435;&#20837;&#21407;&#22411;&#24320;&#21457;&#30340;&#21021;&#22987;&#38454;&#27573;&#65292;&#21363;&#26500;&#24605;&#21644;&#27010;&#24565;&#35780;&#20272;&#38454;&#27573;&#65292;&#20197;&#21450;&#26368;&#21518;&#19968;&#38454;&#27573;&#35780;&#20272;&#29992;&#25143;&#20215;&#20540;&#30340;&#21464;&#21270;&#12290;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25253;&#36947;&#20102;&#19968;&#20010;&#38024;&#23545;&#36807;&#31243;&#24037;&#19994;&#20013;&#26085;&#24120;&#25805;&#20316;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#20041;&#25628;&#32034;&#30340;&#23436;&#25972;&#21407;&#22411;&#24320;&#21457;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
User experience (UX) is a part of human-computer interaction (HCI) research and focuses on increasing intuitiveness, transparency, simplicity, and trust for system users. Most of the UX research for machine learning (ML) or natural language processing (NLP) focuses on a data-driven methodology, i.e., it fails to focus on users' requirements, and engages domain users mainly for usability evaluation. Moreover, more typical UX methods tailor the systems towards user usability, unlike learning about the user needs first. The paper proposes a methodology for integrating generative UX research into developing domain NLP applications. Generative UX research employs domain users at the initial stages of prototype development, i.e., ideation and concept evaluation, and the last stage for evaluating the change in user value. In the case study, we report the full-cycle prototype development of a domain-specific semantic search for daily operations in the process industry. Our case study shows tha
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#35782;&#21035;&#25233;&#37057;&#30151;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#35299;&#20915;&#22235;&#20010;&#39044;&#27979;&#23376;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#21477;&#23376;&#23884;&#20837;&#20316;&#20026;&#32447;&#24615;&#22238;&#24402;&#22120;&#30340;&#36755;&#20837;&#20135;&#29983;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.16125</link><description>&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#35782;&#21035;&#25233;&#37057;&#30151;&#30340;&#26694;&#26550;&#65306;MentalRiskES@IberLEF 2023
&lt;/p&gt;
&lt;p&gt;
A Framework for Identifying Depression on Social Media: MentalRiskES@IberLEF 2023. (arXiv:2306.16125v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16125
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#35782;&#21035;&#25233;&#37057;&#30151;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#35299;&#20915;&#22235;&#20010;&#39044;&#27979;&#23376;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#21477;&#23376;&#23884;&#20837;&#20316;&#20026;&#32447;&#24615;&#22238;&#24402;&#22120;&#30340;&#36755;&#20837;&#20135;&#29983;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#21442;&#19982;IberLEF 2023&#30340;MentalRiskES&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#28041;&#21450;&#26681;&#25454;&#20010;&#20154;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#27963;&#21160;&#26469;&#39044;&#27979;&#20182;&#20204;&#21487;&#33021;&#24739;&#25233;&#37057;&#30151;&#30340;&#21487;&#33021;&#24615;&#12290;&#25968;&#25454;&#38598;&#30001;175&#20010;Telegram&#29992;&#25143;&#30340;&#23545;&#35805;&#32452;&#25104;&#65292;&#27599;&#20010;&#29992;&#25143;&#26681;&#25454;&#20182;&#20204;&#24739;&#30149;&#35777;&#25454;&#36827;&#34892;&#26631;&#35760;&#12290;&#25105;&#20204;&#20351;&#29992;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#22235;&#20010;&#39044;&#27979;&#23376;&#20219;&#21153;&#65306;&#20108;&#20998;&#31867;&#12289;&#31616;&#21333;&#22238;&#24402;&#12289;&#22810;&#31867;&#21035;&#20998;&#31867;&#21644;&#22810;&#31867;&#21035;&#22238;&#24402;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#26469;&#35299;&#20915;&#22810;&#31867;&#21035;&#22238;&#24402;&#38382;&#39064;&#65292;&#28982;&#21518;&#23558;&#39044;&#27979;&#32467;&#26524;&#36716;&#25442;&#20026;&#36866;&#29992;&#20110;&#20854;&#20182;&#19977;&#20010;&#23376;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#19981;&#21516;&#24314;&#27169;&#26041;&#27861;&#30340;&#24615;&#33021;&#65306;&#23545;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21644;&#20351;&#29992;&#21477;&#23376;&#23884;&#20837;&#20316;&#20026;&#32447;&#24615;&#22238;&#24402;&#22120;&#30340;&#36755;&#20837;&#65292;&#21518;&#32773;&#20135;&#29983;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#21487;&#20197;&#22312;&#20197;&#19979;&#38142;&#25509;&#25214;&#21040;&#22797;&#29616;&#25105;&#20204;&#32467;&#26524;&#30340;&#20195;&#30721;&#65306;https://github.com/simonsanvil/EarlyDep
&lt;/p&gt;
&lt;p&gt;
This paper describes our participation in the MentalRiskES task at IberLEF 2023. The task involved predicting the likelihood of an individual experiencing depression based on their social media activity. The dataset consisted of conversations from 175 Telegram users, each labeled according to their evidence of suffering from the disorder. We used a combination of traditional machine learning and deep learning techniques to solve four predictive subtasks: binary classification, simple regression, multiclass classification, and multiclass regression. We approached this by training a model to solve the multiclass regression case and then transforming the predictions to work for the other three subtasks. We compare the performance of two different modeling approaches: fine-tuning a BERT-based model and using sentence embeddings as inputs to a linear regressor, with the latter yielding better results. The code to reproduce our results can be found at: https://github.com/simonsanvil/EarlyDep
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545; GPT-3.5-Turbo &#21644; GPT-4 &#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#33021;&#22815;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#30456;&#20851;&#29255;&#27573;&#30340;&#25903;&#25745;&#19982;&#39046;&#20808;&#31995;&#32479;&#30456;&#31454;&#20105;&#65292;&#20294;&#22312;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#22914;&#20854;&#20182;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2306.16108</link><description>&lt;p&gt;
ChatGPT &#26159;&#19968;&#20010;&#29983;&#29289;&#21307;&#23398;&#19987;&#23478;&#21527;&#65311;&#8212;&#8212;&#25506;&#32034;&#24403;&#21069; GPT &#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#20013;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT a Biomedical Expert? -- Exploring the Zero-Shot Performance of Current GPT Models in Biomedical Tasks. (arXiv:2306.16108v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16108
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545; GPT-3.5-Turbo &#21644; GPT-4 &#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#33021;&#22815;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#30456;&#20851;&#29255;&#27573;&#30340;&#25903;&#25745;&#19982;&#39046;&#20808;&#31995;&#32479;&#30456;&#31454;&#20105;&#65292;&#20294;&#22312;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#22914;&#20854;&#20182;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35780;&#20272;&#20102;&#21830;&#19994;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; GPT-3.5-Turbo &#21644; GPT-4 &#22312; 2023 &#24180; BioASQ &#25361;&#25112;&#20013;&#30340;&#20219;&#21153;&#34920;&#29616;&#12290;&#22312;&#20219;&#21153; 11b &#31532;&#20108;&#38454;&#27573;&#20013;&#65292;&#20063;&#23601;&#26159;&#31572;&#26696;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#36825;&#20004;&#20010;&#27169;&#22411;&#36890;&#36807;&#31616;&#21333;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#30456;&#20851;&#29255;&#27573;&#30340;&#25903;&#25745;&#34920;&#29616;&#20986;&#20102;&#19982;&#39046;&#20808;&#31995;&#32479;&#30456;&#31454;&#20105;&#30340;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#27809;&#26377;&#30456;&#20851;&#29255;&#27573;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20063;&#20196;&#20154;&#28385;&#24847;&#65292;&#23613;&#31649;&#27809;&#26377;&#36798;&#21040;&#26368;&#20339;&#31995;&#32479;&#30340;&#27700;&#24179;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36739;&#26087;&#19988;&#26356;&#20415;&#23452;&#30340; GPT-3.5-Turbo &#31995;&#32479;&#22312;&#22522;&#20110;&#20107;&#23454;&#21644;&#21015;&#34920;&#31572;&#26696;&#30340;&#38382;&#31572;&#29615;&#22659;&#20013;&#33021;&#22815;&#19982; GPT-4 &#30456;&#31454;&#20105;&#12290;&#22312;&#20219;&#21153; 11b &#31532;&#19968;&#38454;&#27573;&#20013;&#65292;&#20391;&#37325;&#20110;&#26816;&#32034;&#65292;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#26597;&#35810;&#25193;&#23637;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20294;&#19982;&#20854;&#20182;&#31995;&#32479;&#30456;&#27604;&#20173;&#28982;&#26377;&#25152;&#19981;&#36275;&#12290;&#37325;&#26032;&#36816;&#34892;&#36825;&#20123;&#23454;&#39564;&#25152;&#38656;&#30340;&#20195;&#30721;&#21487;&#22312; GitHub &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We assessed the performance of commercial Large Language Models (LLMs) GPT-3.5-Turbo and GPT-4 on tasks from the 2023 BioASQ challenge. In Task 11b Phase B, which is focused on answer generation, both models demonstrated competitive abilities with leading systems. Remarkably, they achieved this with simple zero-shot learning, grounded with relevant snippets. Even without relevant snippets, their performance was decent, though not on par with the best systems. Interestingly, the older and cheaper GPT-3.5-Turbo system was able to compete with GPT-4 in the grounded Q&amp;A setting on factoid and list answers. In Task 11b Phase A, focusing on retrieval, query expansion through zero-shot learning improved performance, but the models fell short compared to other systems. The code needed to rerun these experiments is available through GitHub.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#24320;&#28304;&#30340;&#27861;&#24459;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatLaw&#65292;&#23427;&#36890;&#36807;&#32508;&#21512;&#22806;&#37096;&#30693;&#35782;&#24211;&#20026;&#20013;&#22269;&#27861;&#24459;&#39046;&#22495;&#30340;&#25968;&#23383;&#21270;&#36716;&#22411;&#25552;&#20379;&#25903;&#25345;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#31934;&#24515;&#35774;&#35745;&#30340;&#27861;&#24459;&#39046;&#22495;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#23558;&#21521;&#37327;&#25968;&#25454;&#24211;&#26816;&#32034;&#19982;&#20851;&#38190;&#35789;&#26816;&#32034;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#27861;&#24459;&#25968;&#25454;&#31579;&#36873;&#20013;&#30340;&#27169;&#22411;&#24187;&#35273;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#27880;&#24847;&#21147;&#26041;&#27861;&#20197;&#22686;&#24378;&#23545;&#25991;&#26412;&#20013;&#20851;&#38190;&#20449;&#24687;&#30340;&#27880;&#24847;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.16092</link><description>&lt;p&gt;
ChatLaw: &#22522;&#20110;&#24320;&#28304;&#27861;&#24459;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#22806;&#37096;&#30693;&#35782;&#24211;
&lt;/p&gt;
&lt;p&gt;
ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases. (arXiv:2306.16092v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#24320;&#28304;&#30340;&#27861;&#24459;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatLaw&#65292;&#23427;&#36890;&#36807;&#32508;&#21512;&#22806;&#37096;&#30693;&#35782;&#24211;&#20026;&#20013;&#22269;&#27861;&#24459;&#39046;&#22495;&#30340;&#25968;&#23383;&#21270;&#36716;&#22411;&#25552;&#20379;&#25903;&#25345;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#31934;&#24515;&#35774;&#35745;&#30340;&#27861;&#24459;&#39046;&#22495;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#23558;&#21521;&#37327;&#25968;&#25454;&#24211;&#26816;&#32034;&#19982;&#20851;&#38190;&#35789;&#26816;&#32034;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#27861;&#24459;&#25968;&#25454;&#31579;&#36873;&#20013;&#30340;&#27169;&#22411;&#24187;&#35273;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#27880;&#24847;&#21147;&#26041;&#27861;&#20197;&#22686;&#24378;&#23545;&#25991;&#26412;&#20013;&#20851;&#38190;&#20449;&#24687;&#30340;&#27880;&#24847;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#25913;&#21464;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#28508;&#21147;&#65292;&#24341;&#21457;&#20102;&#23545;&#22402;&#30452;&#29305;&#23450;&#22823;&#27169;&#22411;&#30340;&#26497;&#22823;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#19982;&#20687;BloombergGPT&#21644;FinGPT&#36825;&#26679;&#21033;&#29992;&#20854;&#29420;&#29305;&#25968;&#25454;&#31215;&#32047;&#22312;&#37329;&#34701;&#39046;&#22495;&#21462;&#24471;&#36827;&#23637;&#30340;&#19987;&#26377;&#27169;&#22411;&#19981;&#21516;&#65292;&#20013;&#22269;&#27861;&#24459;&#39046;&#22495;&#20013;&#27809;&#26377;&#31867;&#20284;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#20419;&#36827;&#25968;&#23383;&#21270;&#36716;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ChatLaw&#30340;&#24320;&#28304;&#27861;&#24459;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#30001;&#20110;&#25968;&#25454;&#36136;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#19968;&#20010;&#27861;&#24459;&#39046;&#22495;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#22312;&#21442;&#32771;&#25968;&#25454;&#26816;&#32034;&#36807;&#31243;&#20013;&#27861;&#24459;&#25968;&#25454;&#31579;&#36873;&#20013;&#30340;&#27169;&#22411;&#24187;&#35273;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23558;&#21521;&#37327;&#25968;&#25454;&#24211;&#26816;&#32034;&#19982;&#20851;&#38190;&#35789;&#26816;&#32034;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#20943;&#23569;&#21482;&#20381;&#38752;&#21521;&#37327;&#25968;&#25454;&#24211;&#26816;&#32034;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#27880;&#24847;&#21147;&#26041;&#27861;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#23545;&#25991;&#26412;&#20013;&#20851;&#38190;&#20449;&#24687;&#30340;&#27880;&#24847;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown the potential to revolutionize natural language processing tasks in various domains, sparking great interest in vertical-specific large models. However, unlike proprietary models such as BloombergGPT and FinGPT, which have leveraged their unique data accumulations to make strides in the finance domain, there hasn't not many similar large language models in the Chinese legal domain to facilitate its digital transformation.  In this paper, we propose an open-source legal large language model named ChatLaw. Due to the importance of data quality, we carefully designed a legal domain fine-tuning dataset. Additionally, to overcome the problem of model hallucinations in legal data screening during reference data retrieval, we introduce a method that combines vector database retrieval with keyword retrieval to effectively reduce the inaccuracy of relying solely on vector database retrieval. Furthermore, we propose a self-attention method to enhance the a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38271;&#26399;&#23545;&#35805;&#20998;&#26512;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;McAdams&#31995;&#25968;&#21644;&#39057;&#35889;&#24179;&#28369;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16071</link><description>&lt;p&gt;
&#38271;&#26399;&#23545;&#35805;&#20998;&#26512;&#65306;&#25506;&#32034;&#23454;&#29992;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Long-term Conversation Analysis: Exploring Utility and Privacy. (arXiv:2306.16071v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38271;&#26399;&#23545;&#35805;&#20998;&#26512;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;McAdams&#31995;&#25968;&#21644;&#39057;&#35889;&#24179;&#28369;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#26085;&#24120;&#29983;&#27963;&#20013;&#35760;&#24405;&#30340;&#23545;&#35805;&#38656;&#35201;&#38544;&#31169;&#20445;&#25252;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#36755;&#20837;&#29305;&#24449;&#32500;&#24230;&#38477;&#20302;&#12289;&#39057;&#35889;&#24179;&#28369;&#21644;&#22522;&#20110;McAdams&#31995;&#25968;&#30340;&#20302;&#25104;&#26412;&#35828;&#35805;&#32773;&#21311;&#21517;&#21270;&#25216;&#26415;&#30340;&#38544;&#31169;&#20445;&#25252;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#21644;&#35828;&#35805;&#32773;&#20998;&#21106;&#31995;&#32479;&#35780;&#20272;&#20102;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#36890;&#36807;&#35821;&#38899;&#35782;&#21035;&#21644;&#35828;&#35805;&#32773;&#39564;&#35777;&#27169;&#22411;&#30830;&#23450;&#20102;&#38544;&#31169;&#20445;&#25252;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;McAdams&#31995;&#25968;&#21644;&#39057;&#35889;&#24179;&#28369;&#30340;&#32452;&#21512;&#22312;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The analysis of conversations recorded in everyday life requires privacy protection. In this contribution, we explore a privacy-preserving feature extraction method based on input feature dimension reduction, spectral smoothing and the low-cost speaker anonymization technique based on McAdams coefficient. We assess the utility of the feature extraction methods with a voice activity detection and a speaker diarization system, while privacy protection is determined with a speech recognition and a speaker verification model. We show that the combination of McAdams coefficient and spectral smoothing maintains the utility while improving privacy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#25910;&#38598;&#30340;Twitter&#25512;&#25991;&#65292;&#23545;2022&#24180;&#21345;&#22612;&#23572;&#19990;&#30028;&#26479;&#20043;&#21069;&#30340;&#24773;&#24863;&#21644;&#26377;&#36259;&#20107;&#23454;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#20154;&#20204;&#23545;&#19990;&#30028;&#26479;&#30340;&#24320;&#24149;&#25345;&#31215;&#26497;&#24577;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.16049</link><description>&lt;p&gt;
&#36890;&#36807;Twitter&#21644;AI&#22312;2022&#24180;&#21345;&#22612;&#23572;&#19990;&#30028;&#26479;&#20043;&#21069;&#23398;&#21040;&#30340;&#24773;&#24863;&#21644;&#26377;&#36259;&#20107;&#23454;
&lt;/p&gt;
&lt;p&gt;
What Sentiment and Fun Facts We Learnt Before FIFA World Cup Qatar 2022 Using Twitter and AI. (arXiv:2306.16049v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#25910;&#38598;&#30340;Twitter&#25512;&#25991;&#65292;&#23545;2022&#24180;&#21345;&#22612;&#23572;&#19990;&#30028;&#26479;&#20043;&#21069;&#30340;&#24773;&#24863;&#21644;&#26377;&#36259;&#20107;&#23454;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#20154;&#20204;&#23545;&#19990;&#30028;&#26479;&#30340;&#24320;&#24149;&#25345;&#31215;&#26497;&#24577;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Twitter&#26159;&#19968;&#20010;&#36830;&#25509;&#22823;&#22810;&#25968;&#22269;&#23478;&#30340;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#65292;&#21487;&#20197;&#23454;&#26102;&#21457;&#29616;&#26032;&#38395;&#12290;&#30001;&#20110;Twitter&#19978;&#30340;&#25512;&#25991;&#36890;&#24120;&#24456;&#30701;&#24182;&#34920;&#36798;&#20844;&#20247;&#24773;&#24863;&#65292;&#22240;&#27492;&#21487;&#20316;&#20026;&#20840;&#29699;&#20107;&#20214;&#30340;&#35266;&#28857;&#25366;&#25496;&#21644;&#24773;&#24863;&#20998;&#26512;&#30340;&#25968;&#25454;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#25552;&#20379;&#19982;FIFA&#19990;&#30028;&#26479;&#30456;&#20851;&#30340;&#25512;&#25991;&#30340;&#24773;&#24863;&#20998;&#26512;&#12290;&#22312;&#31038;&#21306;&#20013;&#39318;&#27425;&#25910;&#38598;&#20102;&#33267;&#23569;13&#19975;&#26465;&#25512;&#25991;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#25512;&#25991;&#26159;&#36890;&#36807;&#21345;&#22612;&#23572;&#19990;&#30028;&#26479;2022&#30340;&#30456;&#20851;&#26631;&#31614;&#21644;&#20851;&#38190;&#35789;&#25910;&#38598;&#30340;&#12290;&#26412;&#25991;&#20351;&#29992;Vader&#31639;&#27861;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#25910;&#38598;&#30340;Twitter&#25512;&#25991;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#22312;&#19990;&#30028;&#26479;&#20043;&#21069;&#19968;&#27573;&#26102;&#38388;&#20869;&#19982;&#20960;&#20010;&#37325;&#35201;&#26041;&#38754;&#30456;&#20851;&#30340;&#24773;&#24863;&#21644;&#26377;&#36259;&#20107;&#23454;&#12290;&#32467;&#26524;&#26174;&#31034;&#20154;&#20204;&#23545;&#19990;&#30028;&#26479;&#30340;&#24320;&#24149;&#25345;&#31215;&#26497;&#24577;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Twitter is a social media platform bridging most countries and allows real-time news discovery. Since the tweets on Twitter are usually short and express public feelings, thus provide a source for opinion mining and sentiment analysis for global events. This paper proposed an effective solution, in providing a sentiment on tweets related to the FIFA World Cup. At least 130k tweets, as the first in the community, are collected and implemented as a dataset to evaluate the performance of the proposed machine learning solution. These tweets are collected with the related hashtags and keywords of the Qatar World Cup 2022. The Vader algorithm is used in this paper for sentiment analysis. Through the machine learning method and collected Twitter tweets, we discovered the sentiments and fun facts of several aspects important to the period before the World Cup. The result shows people are positive to the opening of the World Cup.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#35821;&#35328;&#34892;&#20026;&#30740;&#31350;&#20102;&#24847;&#22823;&#21033;&#26032;&#20896;&#30123;&#24773;&#31532;&#19968;&#27874;&#27969;&#34892;&#26399;&#38388;&#20844;&#20247;&#23545;&#20107;&#20214;&#30340;&#26102;&#31354;&#21464;&#21270;&#21453;&#24212;&#12290;&#30740;&#31350;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#21644;&#32858;&#31867;&#25216;&#26415;&#25506;&#32034;&#20102;&#25512;&#25991;&#20351;&#29992;&#36235;&#21183;&#65292;&#36890;&#36807;&#23450;&#24615;&#27604;&#36739;&#20998;&#26512;&#30830;&#23450;&#20102;&#23545;&#24212;&#30340;&#26415;&#35821;&#31867;&#21035;&#12290;&#30740;&#31350;&#32467;&#26524;&#35777;&#23454;&#20102;&#24050;&#26377;&#30340;&#24515;&#29702;&#35266;&#23519;&#32467;&#26524;&#65292;&#21363;&#20107;&#20214;&#30340;&#38663;&#20013;&#21644;&#22806;&#22260;&#21306;&#22495;&#30340;&#20844;&#20247;&#35328;&#35770;&#21576;&#29616;&#20986;&#26126;&#26174;&#30340;&#26102;&#31354;&#32858;&#31867;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.16031</link><description>&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#25506;&#32034;&#20844;&#20849;&#35805;&#35821;&#30340;&#26102;&#31354;&#21464;&#21270;&#65306;&#24847;&#22823;&#21033;&#26032;&#20896;&#30123;&#24773;&#31532;&#19968;&#27874;&#27969;&#34892;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring Spatial-Temporal Variations of Public Discourse on Social Media: A Case Study on the First Wave of the Coronavirus Pandemic in Italy. (arXiv:2306.16031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#35821;&#35328;&#34892;&#20026;&#30740;&#31350;&#20102;&#24847;&#22823;&#21033;&#26032;&#20896;&#30123;&#24773;&#31532;&#19968;&#27874;&#27969;&#34892;&#26399;&#38388;&#20844;&#20247;&#23545;&#20107;&#20214;&#30340;&#26102;&#31354;&#21464;&#21270;&#21453;&#24212;&#12290;&#30740;&#31350;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#21644;&#32858;&#31867;&#25216;&#26415;&#25506;&#32034;&#20102;&#25512;&#25991;&#20351;&#29992;&#36235;&#21183;&#65292;&#36890;&#36807;&#23450;&#24615;&#27604;&#36739;&#20998;&#26512;&#30830;&#23450;&#20102;&#23545;&#24212;&#30340;&#26415;&#35821;&#31867;&#21035;&#12290;&#30740;&#31350;&#32467;&#26524;&#35777;&#23454;&#20102;&#24050;&#26377;&#30340;&#24515;&#29702;&#35266;&#23519;&#32467;&#26524;&#65292;&#21363;&#20107;&#20214;&#30340;&#38663;&#20013;&#21644;&#22806;&#22260;&#21306;&#22495;&#30340;&#20844;&#20247;&#35328;&#35770;&#21576;&#29616;&#20986;&#26126;&#26174;&#30340;&#26102;&#31354;&#32858;&#31867;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#35821;&#35328;&#34892;&#20026;&#26469;&#30740;&#31350;&#31038;&#20250;&#23545;&#37325;&#35201;&#20107;&#20214;&#65288;&#20363;&#22914;SARS CoV2&#22823;&#27969;&#34892;&#26399;&#38388;&#21457;&#29983;&#30340;&#20107;&#20214;&#65289;&#30340;&#21453;&#24212;&#65292;&#29305;&#21035;&#26159;&#31354;&#38388;&#21644;&#26102;&#38388;&#26041;&#38754;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#21644;&#32858;&#31867;&#25216;&#26415;&#22522;&#20110;&#25512;&#25991;&#20351;&#29992;&#36235;&#21183;&#26469;&#30830;&#23450;&#26102;&#31354;&#33539;&#30068;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23450;&#24615;&#27604;&#36739;&#20998;&#26512;&#65292;&#23558;&#27599;&#20010;&#31867;&#21035;&#20013;&#30340;&#26174;&#33879;&#26415;&#35821;&#37492;&#23450;&#20026;&#25163;&#24037;&#32534;&#30721;&#31867;&#21035;&#20013;&#22522;&#20110;&#32553;&#25918;F&#20998;&#25968;&#32858;&#21512;&#30340;&#26415;&#35821;&#12290;&#20026;&#20102;&#35828;&#26126;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#24847;&#22823;&#21033;&#26032;&#20896;&#30123;&#24773;&#30340;&#31532;&#19968;&#27874;&#27969;&#34892;&#20013;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26469;&#25506;&#32034;&#24050;&#26377;&#30340;&#24515;&#29702;&#35266;&#23519;&#32467;&#26524;&#65292;&#21363;&#36523;&#20307;&#36317;&#31163;&#20107;&#20214;&#30340;&#36828;&#36817;&#20250;&#24433;&#21709;&#20107;&#20214;&#30456;&#20851;&#20449;&#24687;&#30340;&#20256;&#25773;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#30142;&#30149;&#30340;&#38663;&#20013;&#21644;&#22806;&#22260;&#21306;&#22495;&#23545;&#24212;&#20110;&#28165;&#26224;&#30340;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#26469;&#30830;&#35748;&#36825;&#20123;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a methodology for exploring how linguistic behaviour on social media can be used to explore societal reactions to important events such as those that transpired during the SARS CoV2 pandemic. In particular, where spatial and temporal aspects of events are important features. Our methodology consists of grounding spatial-temporal categories in tweet usage trends using time-series analysis and clustering. Salient terms in each category were then identified through qualitative comparative analysis based on scaled f-scores aggregated into hand-coded categories. To exemplify this approach, we conducted a case study on the first wave of the coronavirus in Italy. We used our proposed methodology to explore existing psychological observations which claimed that physical distance from events affects what is communicated about them. We confirmed these findings by showing that the epicentre of the disease and peripheral regions correspond to clear time-series clusters and that
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#37051;&#20196;&#29260;&#21512;&#24182;&#65288;A-ToMe&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#28176;&#21512;&#24182;&#20855;&#26377;&#30456;&#20284;&#20851;&#38190;&#20540;&#30340;&#30456;&#37051;&#20196;&#29260;&#65292;&#20943;&#23569;&#20102;&#24635;&#30340;&#26102;&#38388;&#27493;&#38271;&#65292;&#24182;&#21152;&#24555;&#20102;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20196;&#29260;&#25968;&#37327;&#24182;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#65292;&#21516;&#26102;&#20934;&#30830;&#24615;&#25439;&#22833;&#24494;&#20046;&#20854;&#24494;&#12290;</title><link>http://arxiv.org/abs/2306.16009</link><description>&lt;p&gt;
&#21152;&#36895;&#20256;&#24863;&#22120;&#36890;&#36807;&#30456;&#37051;&#20196;&#29260;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
Accelerating Transducers through Adjacent Token Merging. (arXiv:2306.16009v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16009
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#37051;&#20196;&#29260;&#21512;&#24182;&#65288;A-ToMe&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#28176;&#21512;&#24182;&#20855;&#26377;&#30456;&#20284;&#20851;&#38190;&#20540;&#30340;&#30456;&#37051;&#20196;&#29260;&#65292;&#20943;&#23569;&#20102;&#24635;&#30340;&#26102;&#38388;&#27493;&#38271;&#65292;&#24182;&#21152;&#24555;&#20102;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20196;&#29260;&#25968;&#37327;&#24182;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#65292;&#21516;&#26102;&#20934;&#30830;&#24615;&#25439;&#22833;&#24494;&#20046;&#20854;&#24494;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#36890;&#24120;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#22768;&#23398;&#32534;&#30721;&#22120;&#65292;&#22312;&#39640;&#24103;&#29575;&#19979;&#29983;&#25104;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#20108;&#27425;&#35745;&#31639;&#65292;&#36825;&#31181;&#35774;&#35745;&#22312;&#22788;&#29702;&#38271;&#38899;&#39057;&#20449;&#21495;&#26102;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#30456;&#37051;&#20196;&#29260;&#21512;&#24182;&#65288;A-ToMe&#65289;&#65292;&#23427;&#36880;&#28176;&#21512;&#24182;&#20855;&#26377;&#30456;&#20284;&#20851;&#38190;&#20540;&#30340;&#30456;&#37051;&#20196;&#29260;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21487;&#20197;&#20943;&#23569;&#24635;&#30340;&#26102;&#38388;&#27493;&#38271;&#65292;&#24182;&#21152;&#24555;&#32534;&#30721;&#22120;&#21644;&#32852;&#21512;&#32593;&#32476;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#22312;LibriSpeech&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;57%&#30340;&#20196;&#29260;&#65292;&#24182;&#22312;GPU&#19978;&#25552;&#39640;70%&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#32780;&#20960;&#20046;&#27809;&#26377;&#25439;&#22833;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;A-ToMe&#20063;&#26159;&#19968;&#31181;&#20943;&#23569;&#38271;&#24418;&#24335;ASR&#20196;&#29260;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#36755;&#20837;&#35821;&#38899;&#30001;&#22810;&#20010;&#35805;&#35821;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent end-to-end automatic speech recognition (ASR) systems often utilize a Transformer-based acoustic encoder that generates embedding at a high frame rate. However, this design is inefficient, particularly for long speech signals due to the quadratic computation of self-attention. To address this, we propose a new method, Adjacent Token Merging (A-ToMe), which gradually combines adjacent tokens with high similarity scores between their key values. In this way, the total time step could be reduced, and the inference of both the encoder and joint network is accelerated. Experiments on LibriSpeech show that our method can reduce 57% of tokens and improve the inference speed on GPU by 70% without any notable loss of accuracy. Additionally, we demonstrate that A-ToMe is also an effective solution to reduce tokens in long-form ASR, where the input speech consists of multiple utterances.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#36827;&#34892;&#38646;&#26679;&#26412;&#39046;&#22495;&#36866;&#24212;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20108;&#27425;&#20462;&#27491;&#21644;&#28145;&#24230;&#34701;&#21512;&#65292;&#21482;&#20351;&#29992;&#19968;&#20010;&#39046;&#22495;&#25552;&#31034;&#21363;&#21487;&#26377;&#25928;&#38477;&#20302;&#35789;&#38169;&#35823;&#29575;&#65292;&#24182;&#19988;&#26377;&#26356;&#22909;&#30340;&#23454;&#20307;&#21644;&#36229;&#20986;&#35789;&#27719;&#30340;&#21484;&#22238;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.16007</link><description>&lt;p&gt;
&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#36827;&#34892;&#38646;&#26679;&#26412;&#39046;&#22495;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Prompting Large Language Models for Zero-Shot Domain Adaptation in Speech Recognition. (arXiv:2306.16007v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16007
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#36827;&#34892;&#38646;&#26679;&#26412;&#39046;&#22495;&#36866;&#24212;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20108;&#27425;&#20462;&#27491;&#21644;&#28145;&#24230;&#34701;&#21512;&#65292;&#21482;&#20351;&#29992;&#19968;&#20010;&#39046;&#22495;&#25552;&#31034;&#21363;&#21487;&#26377;&#25928;&#38477;&#20302;&#35789;&#38169;&#35823;&#29575;&#65292;&#24182;&#19988;&#26377;&#26356;&#22909;&#30340;&#23454;&#20307;&#21644;&#36229;&#20986;&#35789;&#27719;&#30340;&#21484;&#22238;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#25972;&#21512;&#24050;&#34987;&#35777;&#26126;&#26159;&#35299;&#20915;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#39046;&#22495;&#36716;&#31227;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#30446;&#26631;&#39046;&#22495;&#25991;&#26412;&#25968;&#25454;&#26469;&#35757;&#32451;LMs&#12290;&#19982;&#36825;&#20123;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21482;&#20351;&#29992;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#30340;&#25991;&#26412;&#25552;&#31034;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#20351;&#29992;LLaMA&#65288;&#19968;&#20010;&#20855;&#26377;70&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#36827;&#34892;&#38646;&#26679;&#26412;ASR&#39046;&#22495;&#36866;&#24212;&#30340;&#26041;&#27861;&#12290;LLaMA&#26377;&#20004;&#31181;&#29992;&#27861;&#65306;1&#65289;&#20108;&#27425;&#20462;&#27491;&#65306;&#20351;&#29992;LLaMA&#37325;&#26032;&#25490;&#21015;&#32473;&#23450;ASR&#31995;&#32479;&#30340;N&#20010;&#26368;&#20339;&#20551;&#35774;&#65307;2&#65289;&#28145;&#24230;LLM&#34701;&#21512;&#65306;&#23558;LLaMA&#25972;&#21512;&#21040;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;ASR&#31995;&#32479;&#30340;&#35299;&#30721;&#22120;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21482;&#20351;&#29992;&#19968;&#20010;&#39046;&#22495;&#25552;&#31034;&#65292;&#20004;&#31181;&#26041;&#27861;&#37117;&#21487;&#20197;&#26377;&#25928;&#38477;&#20302;&#36229;&#20986;&#39046;&#22495;&#30340;TedLium-2&#21644;SPGISpeech&#25968;&#25454;&#38598;&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#12290;&#23588;&#20854;&#26159;&#65292;&#28145;&#24230;LLM&#34701;&#21512;&#20855;&#26377;&#26356;&#22909;&#30340;&#23454;&#20307;&#21644;&#36229;&#20986;&#35789;&#27719;&#30340;&#21333;&#35789;&#21484;&#22238;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of Language Models (LMs) has proven to be an effective way to address domain shifts in speech recognition. However, these approaches usually require a significant amount of target domain text data for the training of LMs. Different from these methods, in this work, with only a domain-specific text prompt, we propose two zero-shot ASR domain adaptation methods using LLaMA, a 7-billion-parameter large language model (LLM). LLM is used in two ways: 1) second-pass rescoring: reranking N-best hypotheses of a given ASR system with LLaMA; 2) deep LLM-fusion: incorporating LLM into the decoder of an encoder-decoder based ASR system. Experiments show that, with only one domain prompt, both methods can effectively reduce word error rates (WER) on out-of-domain TedLium-2 and SPGISpeech datasets. Especially, the deep LLM-fusion has the advantage of better recall of entity and out-of-vocabulary words.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31616;&#21270;&#31038;&#20132;&#23186;&#20307;&#20449;&#24687;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35782;&#21035;&#21307;&#23398;&#23454;&#20307;&#12289;&#26631;&#20934;&#21270;&#23454;&#20307;&#21644;&#20998;&#37197;UMLS&#27010;&#24565;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;COVID-19&#30456;&#20851;&#25512;&#25991;&#30340;&#30151;&#29366;&#35789;&#20856;&#12290;</title><link>http://arxiv.org/abs/2306.16001</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#31616;&#21270;&#31038;&#20132;&#23186;&#20307;&#20449;&#24687;&#26816;&#32034;&#20197;&#25903;&#25345;&#20844;&#20849;&#21355;&#29983;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Streamlining Social Media Information Retrieval for Public Health Research with Deep Learning. (arXiv:2306.16001v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31616;&#21270;&#31038;&#20132;&#23186;&#20307;&#20449;&#24687;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35782;&#21035;&#21307;&#23398;&#23454;&#20307;&#12289;&#26631;&#20934;&#21270;&#23454;&#20307;&#21644;&#20998;&#37197;UMLS&#27010;&#24565;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;COVID-19&#30456;&#20851;&#25512;&#25991;&#30340;&#30151;&#29366;&#35789;&#20856;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#22312;&#27969;&#34892;&#30149;&#30417;&#27979;&#20013;&#30340;&#21033;&#29992;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#35777;&#23454;&#12290;&#28982;&#32780;&#65292;&#24403;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#35789;&#27719;&#34920;&#26469;&#26816;&#32034;&#30456;&#20851;&#35821;&#26009;&#24211;&#26102;&#65292;&#24120;&#24120;&#20250;&#24341;&#20837;&#20559;&#35265;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#26500;&#24314;&#21307;&#23398;&#20439;&#35821;&#21644;&#32479;&#19968;&#21307;&#23398;&#35821;&#35328;&#31995;&#32479;&#65288;UMLS&#65289;&#27010;&#24565;&#30340;&#24191;&#27867;&#23383;&#20856;&#12290;&#35813;&#26694;&#26550;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#22522;&#20110;BERT&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#20013;&#35782;&#21035;&#20986;&#21307;&#23398;&#23454;&#20307;&#65307;&#28145;&#24230;&#23398;&#20064;&#39537;&#21160;&#30340;&#26631;&#20934;&#21270;&#27169;&#22359;&#65292;&#29992;&#20110;&#23545;&#25552;&#21462;&#20986;&#30340;&#23454;&#20307;&#36827;&#34892;&#35268;&#33539;&#21270;&#22788;&#29702;&#65307;&#21322;&#30417;&#30563;&#32858;&#31867;&#27169;&#22359;&#65292;&#23558;&#26368;&#21487;&#33021;&#30340;UMLS&#27010;&#24565;&#20998;&#37197;&#32473;&#27599;&#20010;&#35268;&#33539;&#21270;&#23454;&#20307;&#12290;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#20174;2020&#24180;2&#26376;1&#26085;&#21040;2022&#24180;4&#26376;30&#26085;&#26399;&#38388;&#19982;COVID-19&#30456;&#20851;&#30340;&#25512;&#25991;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#30151;&#29366;&#35789;&#20856;&#65288;&#21487;&#22312;https://github.com/ningkko/UMLS_colloquialism/&#19978;&#33719;&#21462;&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;9,249&#20010;&#26631;&#20934;&#21270;&#23454;&#20307;&#65292;&#26144;&#23556;&#21040;876&#20010;UMLS&#27010;&#24565;&#21644;38,175&#20010;&#20442;&#35821;&#34920;&#36798;&#12290;&#35813;&#26694;&#26550;&#30340;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
The utilization of social media in epidemic surveillance has been well established. Nonetheless, bias is often introduced when pre-defined lexicons are used to retrieve relevant corpus. This study introduces a framework aimed at curating extensive dictionaries of medical colloquialisms and Unified Medical Language System (UMLS) concepts. The framework comprises three modules: a BERT-based Named Entity Recognition (NER) model that identifies medical entities from social media content, a deep-learning powered normalization module that standardizes the extracted entities, and a semi-supervised clustering module that assigns the most probable UMLS concept to each standardized entity. We applied this framework to COVID-19-related tweets from February 1, 2020, to April 30, 2022, generating a symptom dictionary (available at https://github.com/ningkko/UMLS_colloquialism/) composed of 9,249 standardized entities mapped to 876 UMLS concepts and 38,175 colloquial expressions. This framework demo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21477;&#23376;&#21040;&#26631;&#31614;&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#26085;&#35821;&#21477;&#23376;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#36890;&#36807;&#25972;&#21512;&#21477;&#23376;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#32422;&#26463;&#26426;&#21046;&#26469;&#25552;&#39640;&#26684;&#24335;&#20934;&#30830;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#25972;&#21512;&#33021;&#22815;&#22686;&#24378;&#20004;&#20010;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15978</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#26085;&#35821;&#21477;&#23376;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#21477;&#23376;&#21040;&#26631;&#31614;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Sentence-to-Label Generation Framework for Multi-task Learning of Japanese Sentence Classification and Named Entity Recognition. (arXiv:2306.15978v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21477;&#23376;&#21040;&#26631;&#31614;&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#26085;&#35821;&#21477;&#23376;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#36890;&#36807;&#25972;&#21512;&#21477;&#23376;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#32422;&#26463;&#26426;&#21046;&#26469;&#25552;&#39640;&#26684;&#24335;&#20934;&#30830;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#25972;&#21512;&#33021;&#22815;&#22686;&#24378;&#20004;&#20010;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#25552;&#21462;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#20010;&#20851;&#38190;&#30340;&#23376;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21477;&#23376;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#22810;&#20219;&#21153;&#65288;SCNM&#65289;&#26041;&#27861;&#65292;&#23558;&#21477;&#23376;&#20998;&#31867;&#65288;SC&#65289;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21477;&#23376;&#21040;&#26631;&#31614;&#29983;&#25104;&#65288;SLG&#65289;&#26694;&#26550;&#29992;&#20110;SCNM&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;SC&#21644;NER&#25968;&#25454;&#30340;&#32500;&#22522;&#30334;&#31185;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#26684;&#24335;&#36716;&#25442;&#22120;&#65292;&#25105;&#20204;&#32479;&#19968;&#20102;&#36755;&#20837;&#26684;&#24335;&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;SC&#26631;&#31614;&#12289;NER&#26631;&#31614;&#21644;&#30456;&#20851;&#25991;&#26412;&#29255;&#27573;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32422;&#26463;&#26426;&#21046;&#65288;CM&#65289;&#26469;&#25552;&#39640;&#29983;&#25104;&#30340;&#26684;&#24335;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#29420;&#31435;&#20219;&#21153;&#30456;&#27604;&#65292;SCNM&#20013;&#30340;SC&#20934;&#30830;&#24230;&#25552;&#39640;&#20102;1.13&#20010;&#30334;&#20998;&#28857;&#65292;NER&#25552;&#39640;&#20102;1.06&#20010;&#30334;&#20998;&#28857;&#65292;CM&#23558;&#26684;&#24335;&#20934;&#30830;&#24615;&#20174;63.61&#25552;&#39640;&#21040;100&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;SC&#21644;NER&#20043;&#38388;&#23384;&#22312;&#30456;&#20114;&#24378;&#21270;&#30340;&#25928;&#24212;&#65292;&#25972;&#21512;&#21487;&#20197;&#25552;&#39640;&#20004;&#20010;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information extraction(IE) is a crucial subfield within natural language processing. In this study, we introduce a Sentence Classification and Named Entity Recognition Multi-task (SCNM) approach that combines Sentence Classification (SC) and Named Entity Recognition (NER). We develop a Sentence-to-Label Generation (SLG) framework for SCNM and construct a Wikipedia dataset containing both SC and NER. Using a format converter, we unify input formats and employ a generative model to generate SC-labels, NER-labels, and associated text segments. We propose a Constraint Mechanism (CM) to improve generated format accuracy. Our results show SC accuracy increased by 1.13 points and NER by 1.06 points in SCNM compared to standalone tasks, with CM raising format accuracy from 63.61 to 100. The findings indicate mutual reinforcement effects between SC and NER, and integration enhances both tasks' performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27493;&#39588;&#29983;&#25104;&#12289;&#39564;&#35777;&#21644;&#32416;&#27491;&#30340;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#26041;&#27861;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#38169;&#35823;&#25351;&#31034;&#25552;&#31034;&#26469;&#25913;&#21892;&#36755;&#20986;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.15933</link><description>&lt;p&gt;
&#36890;&#36807;&#39564;&#35777;&#21644;&#32416;&#27491;&#25552;&#31034;&#36827;&#34892;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
You Can Generate It Again: Data-to-text Generation with Verification and Correction Prompting. (arXiv:2306.15933v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27493;&#39588;&#29983;&#25104;&#12289;&#39564;&#35777;&#21644;&#32416;&#27491;&#30340;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#26041;&#27861;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#38169;&#35823;&#25351;&#31034;&#25552;&#31034;&#26469;&#25913;&#21892;&#36755;&#20986;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29616;&#26377;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20174;&#32467;&#26500;&#21270;&#25968;&#25454;&#36755;&#20837;&#29983;&#25104;&#25991;&#26412;&#25551;&#36848;&#65288;&#31216;&#20026;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#65289;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21253;&#25324;&#29983;&#25104;&#12289;&#39564;&#35777;&#21644;&#32416;&#27491;&#38454;&#27573;&#30340;&#22810;&#27493;&#39588;&#36807;&#31243;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#19968;&#27425;&#24615;&#29983;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;VCP&#65288;&#39564;&#35777;&#21644;&#32416;&#27491;&#25552;&#31034;&#65289;&#65292;&#20174;&#27169;&#22411;&#29983;&#25104;&#21021;&#22987;&#36755;&#20986;&#24320;&#22987;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32487;&#32493;&#39564;&#35777;&#25152;&#29983;&#25104;&#25991;&#26412;&#30340;&#19981;&#21516;&#26041;&#38754;&#30340;&#27491;&#30830;&#24615;&#12290;&#39564;&#35777;&#27493;&#39588;&#30340;&#35266;&#23519;&#32467;&#26524;&#34987;&#36716;&#21270;&#20026;&#19987;&#38376;&#30340;&#38169;&#35823;&#25351;&#31034;&#25552;&#31034;&#65292;&#35813;&#25552;&#31034;&#25351;&#31034;&#27169;&#22411;&#22312;&#37325;&#26032;&#29983;&#25104;&#36755;&#20986;&#26102;&#32771;&#34385;&#24050;&#35782;&#21035;&#30340;&#38169;&#35823;&#12290;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#30340;&#32416;&#27491;&#33021;&#21147;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#22521;&#35757;&#36807;&#31243;&#12290;&#35813;&#36807;&#31243;&#20351;&#27169;&#22411;&#33021;&#22815;&#34701;&#20837;&#38169;&#35823;&#25351;&#31034;&#25552;&#31034;&#30340;&#21453;&#39304;&#65292;&#20174;&#32780;&#25913;&#21892;&#36755;&#20986;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant advancements in existing models, generating text descriptions from structured data input, known as data-to-text generation, remains a challenging task. In this paper, we propose a novel approach that goes beyond traditional one-shot generation methods by introducing a multi-step process consisting of generation, verification, and correction stages. Our approach, VCP(Verification and Correction Prompting), begins with the model generating an initial output. We then proceed to verify the correctness of different aspects of the generated text. The observations from the verification step are converted into a specialized error-indication prompt, which instructs the model to regenerate the output while considering the identified errors. To enhance the model's correction ability, we have developed a carefully designed training procedure. This procedure enables the model to incorporate feedback from the error-indication prompt, resulting in improved output generation. Throu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#36807;&#28388;&#20989;&#25968;&#26469;&#29983;&#25104;&#26377;&#38480;&#32422;&#26463;&#25991;&#26412;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;AI&#20889;&#20316;&#21161;&#25163;&#24037;&#20855;&#65292;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#30340;&#38656;&#27714;&#29983;&#25104;&#24102;&#26377;&#21508;&#31181;&#32422;&#26463;&#26465;&#20214;&#30340;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.15926</link><description>&lt;p&gt;
&#22823;&#22810;&#25968;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#25104;&#20026;&#35799;&#20154;&#65306;&#19968;&#20010;AI&#20889;&#20316;&#21161;&#25163;&#21644;&#26377;&#38480;&#25991;&#26412;&#29983;&#25104;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Most Language Models can be Poets too: An AI Writing Assistant and Constrained Text Generation Studio. (arXiv:2306.15926v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15926
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#36807;&#28388;&#20989;&#25968;&#26469;&#29983;&#25104;&#26377;&#38480;&#32422;&#26463;&#25991;&#26412;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;AI&#20889;&#20316;&#21161;&#25163;&#24037;&#20855;&#65292;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#30340;&#38656;&#27714;&#29983;&#25104;&#24102;&#26377;&#21508;&#31181;&#32422;&#26463;&#26465;&#20214;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#20851;&#26377;&#38480;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#30340;&#24555;&#36895;&#36827;&#23637;&#65292;&#20294;&#23545;&#24050;&#34987;&#35789;&#27719;&#12289;&#35821;&#20041;&#25110;&#38899;&#38901;&#32422;&#26463;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#30740;&#31350;&#26102;&#38388;&#24456;&#23569;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22823;&#22810;&#25968;&#35821;&#35328;&#27169;&#22411;&#21363;&#20351;&#22312;&#26174;&#33879;&#32422;&#26463;&#19979;&#20063;&#33021;&#29983;&#25104;&#24341;&#20154;&#20837;&#32988;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26222;&#36866;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#25991;&#26412;&#21333;&#20803;&#20043;&#21069;&#32452;&#21512;&#24212;&#29992;&#36807;&#28388;&#20989;&#25968;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#27719;&#65292;&#26469;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#36825;&#31181;&#26041;&#27861;&#26159;&#21363;&#25554;&#21363;&#29992;&#30340;&#65292;&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#20462;&#25913;&#12290;&#20026;&#23637;&#31034;&#36825;&#31181;&#25216;&#26415;&#30340;&#20215;&#20540;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;AI&#20889;&#20316;&#21161;&#25163;&#65292;&#21517;&#20026;&#26377;&#38480;&#25991;&#26412;&#29983;&#25104;&#24037;&#20855;&#65288;CTGS&#65289;&#12290;CTGS&#20801;&#35768;&#29992;&#25143;&#29983;&#25104;&#25110;&#36873;&#25321;&#20855;&#26377;&#21508;&#31181;&#32422;&#26463;&#26465;&#20214;&#30340;&#25991;&#26412;&#65292;&#20363;&#22914;&#31105;&#27490;&#26576;&#20010;&#23383;&#27597;&#65292;&#24378;&#21046;&#29983;&#25104;&#30340;&#21333;&#35789;&#20855;&#26377;&#19968;&#23450;&#30340;&#38899;&#33410;&#25968;&#65292;&#25110;&#24378;&#21046;&#29983;&#25104;&#19982;&#32473;&#23450;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#21333;&#35789;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite rapid advancement in the field of Constrained Natural Language Generation, little time has been spent on exploring the potential of language models which have had their vocabularies lexically, semantically, and/or phonetically constrained. We find that most language models generate compelling text even under significant constraints. We present a simple and universally applicable technique for modifying the output of a language model by compositionally applying filter functions to the language models vocabulary before a unit of text is generated. This approach is plug-and-play and requires no modification to the model. To showcase the value of this technique, we present an easy to use AI writing assistant called Constrained Text Generation Studio (CTGS). CTGS allows users to generate or choose from text with any combination of a wide variety of constraints, such as banning a particular letter, forcing the generated words to have a certain number of syllables, and/or forcing the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20248;&#21270;&#22522;&#20110;Transformer&#30340;&#23494;&#38598;&#35821;&#27573;&#26816;&#32034;&#65288;DPR&#65289;&#31639;&#27861;&#65292;&#20351;&#29992;&#32622;&#20449;&#24230;&#26657;&#20934;&#30340;&#38598;&#21512;&#39044;&#27979;&#26041;&#27861;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#39046;&#22495;&#30340;&#26368;&#20248;&#31890;&#24230;&#20063;&#26377;&#25152;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.15917</link><description>&lt;p&gt;
&#32622;&#20449;&#24230;&#26657;&#20934;&#30340;&#38598;&#21512;&#24335;&#23494;&#38598;&#30701;&#35821;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Confidence-Calibrated Ensemble Dense Phrase Retrieval. (arXiv:2306.15917v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20248;&#21270;&#22522;&#20110;Transformer&#30340;&#23494;&#38598;&#35821;&#27573;&#26816;&#32034;&#65288;DPR&#65289;&#31639;&#27861;&#65292;&#20351;&#29992;&#32622;&#20449;&#24230;&#26657;&#20934;&#30340;&#38598;&#21512;&#39044;&#27979;&#26041;&#27861;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#39046;&#22495;&#30340;&#26368;&#20248;&#31890;&#24230;&#20063;&#26377;&#25152;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19981;&#38656;&#35201;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;Transformer&#30340;&#23494;&#38598;&#35821;&#27573;&#26816;&#32034;&#65288;DPR&#65289;&#31639;&#27861;&#65288;&#30001;Karpukhin&#31561;&#20154;&#20110;2020&#24180;&#24320;&#21457;&#65289;&#30340;&#20248;&#21270;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#27934;&#23519;&#65306;&#25105;&#20204;&#22312;&#19981;&#21516;&#30701;&#35821;&#38271;&#24230;&#65288;&#20363;&#22914;&#19968;&#21477;&#21644;&#20116;&#21477;&#65289;&#19978;&#24212;&#29992;DPR&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#65292;&#24182;&#23545;&#25152;&#26377;&#36825;&#20123;&#19981;&#21516;&#20998;&#21106;&#30340;&#32467;&#26524;&#36827;&#34892;&#32622;&#20449;&#24230;&#26657;&#20934;&#30340;&#38598;&#21512;&#39044;&#27979;&#12290;&#36825;&#31181;&#30456;&#23545;&#35814;&#23613;&#30340;&#26041;&#27861;&#22312;Google NQ&#21644;SQuAD&#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#19981;&#21516;&#30340;&#39063;&#31890;&#24230;&#23545;&#20110;&#19981;&#21516;&#30340;&#39046;&#22495;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the extent to which the transformer-based Dense Passage Retrieval (DPR) algorithm, developed by (Karpukhin et. al. 2020), can be optimized without further pre-training. Our method involves two particular insights: we apply the DPR context encoder at various phrase lengths (e.g. one-sentence versus five-sentence segments), and we take a confidence-calibrated ensemble prediction over all of these different segmentations. This somewhat exhaustive approach achieves start-of-the-art results on benchmark datasets such as Google NQ and SQuAD. We also apply our method to domain-specific datasets, and the results suggest how different granularities are optimal for different domains
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#22120;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22810;&#26679;&#24615;&#23646;&#24615;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#19988;&#24402;&#22240;&#30340;&#25968;&#25454;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#39640;&#22522;&#25968;&#21644;&#22810;&#26679;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#20351;&#29992;&#23646;&#24615;&#21270;&#25552;&#31034;&#23545;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#23637;&#31034;&#20102;&#20851;&#20110;&#20559;&#24046;&#12289;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#30340;&#20840;&#38754;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#65292;&#24182;&#24471;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;&#31995;&#32479;&#24615;&#20559;&#24046;&#23384;&#22312;&#20110;&#29983;&#25104;&#25968;&#25454;&#20013;&#65292;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15895</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#22120;&#65306;&#22810;&#26679;&#24615;&#21644;&#20559;&#24046;&#30340;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias. (arXiv:2306.15895v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#22120;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22810;&#26679;&#24615;&#23646;&#24615;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#19988;&#24402;&#22240;&#30340;&#25968;&#25454;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#39640;&#22522;&#25968;&#21644;&#22810;&#26679;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#20351;&#29992;&#23646;&#24615;&#21270;&#25552;&#31034;&#23545;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#23637;&#31034;&#20102;&#20851;&#20110;&#20559;&#24046;&#12289;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#30340;&#20840;&#38754;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#65292;&#24182;&#24471;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;&#31995;&#32479;&#24615;&#20559;&#24046;&#23384;&#22312;&#20110;&#29983;&#25104;&#25968;&#25454;&#20013;&#65292;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#31867;&#21035;&#26465;&#20214;&#25552;&#31034;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#29983;&#25104;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#19988;&#32487;&#25215;&#20102;LLM&#30340;&#31995;&#32479;&#24615;&#20559;&#24046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#20855;&#26377;&#22810;&#26679;&#23646;&#24615;&#30340;&#25552;&#31034;(&#20363;&#22914;&#25351;&#23450;&#38271;&#24230;&#21644;&#39118;&#26684;&#31561;&#23646;&#24615;)&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#65292;&#36825;&#26377;&#28508;&#21147;&#20135;&#29983;&#22810;&#26679;&#21644;&#24402;&#22240;&#30340;&#29983;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20851;&#27880;&#20855;&#26377;&#39640;&#22522;&#25968;&#21644;&#22810;&#26679;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23646;&#24615;&#21270;&#25552;&#31034;&#22312;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#31616;&#21333;&#30340;&#31867;&#21035;&#26465;&#20214;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#39033;&#21253;&#25324;&#20559;&#24046;&#12289;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#31561;&#20851;&#38190;&#26041;&#38754;&#30340;&#20840;&#38754;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#24378;&#35843;&#20102;&#19977;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;&#39318;&#20808;&#65292;&#31995;&#32479;&#24615;&#20559;&#24046;&#22312;&#29983;&#25104;&#25968;&#25454;&#20013;&#23384;&#22312;&#65307;&#20854;&#27425;&#65292;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65307;&#26368;&#21518;&#65292;&#36827;&#34892;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been recently leveraged as training data generators for various natural language processing (NLP) tasks. While previous research has explored different approaches to training models using generated data, they generally rely on simple class-conditional prompts, which may limit the diversity of the generated data and inherit systematic biases of LLM. Thus, we investigate training data generation with diversely attributed prompts (e.g., specifying attributes like length and style), which have the potential to yield diverse and attributed generated data. Our investigation focuses on datasets with high cardinality and diverse domains, wherein we demonstrate that attributed prompts outperform simple class-conditional prompts in terms of the resulting model's performance. Additionally, we present a comprehensive empirical study on data generation encompassing vital aspects like bias, diversity, and efficiency, and highlight three key observations: firstly, sy
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#20132;&#20114;&#23398;&#20064;&#21644;&#21629;&#21517;&#28216;&#25103;&#65292;&#23454;&#29616;&#20102;&#20195;&#29702;&#20043;&#38388;&#36890;&#36807;&#35789;&#24207;&#21015;&#20132;&#27969;&#30340;&#26041;&#24335;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#20855;&#26377;&#32452;&#21512;&#24615;&#30340;&#35821;&#20041;&#30693;&#35782;&#30340;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.15837</link><description>&lt;p&gt;
&#20174;&#20154;&#38469;&#20132;&#20114;&#38754;&#20020;&#22330;&#26223;&#23398;&#20064;&#30475;&#31526;&#21495;&#30340;&#20986;&#29616;&#65306;&#35821;&#20041;&#30693;&#35782;&#30340;&#32452;&#21512;&#24615;&#20986;&#29616;&#26426;&#21046;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Symbol emergence as interpersonal cross-situational learning: the emergence of lexical knowledge with combinatoriality. (arXiv:2306.15837v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#20132;&#20114;&#23398;&#20064;&#21644;&#21629;&#21517;&#28216;&#25103;&#65292;&#23454;&#29616;&#20102;&#20195;&#29702;&#20043;&#38388;&#36890;&#36807;&#35789;&#24207;&#21015;&#20132;&#27969;&#30340;&#26041;&#24335;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#20855;&#26377;&#32452;&#21512;&#24615;&#30340;&#35821;&#20041;&#30693;&#35782;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;Metropolis-Hastings&#21629;&#21517;&#28216;&#25103;&#21644;&#20132;&#20114;&#23398;&#20064;&#65292;&#20351;&#20195;&#29702;&#22312;&#31526;&#21495;&#20986;&#29616;&#31995;&#32479;&#20013;&#33021;&#22815;&#36890;&#36807;&#32452;&#21512;&#24615;&#33719;&#24471;&#35789;&#27719;&#30693;&#35782;&#12290;&#35768;&#22810;&#35745;&#31639;&#27169;&#22411;&#24050;&#34987;&#25552;&#20986;&#29992;&#20110;&#30740;&#31350;&#32039;&#24613;&#20132;&#27969;&#20013;&#30340;&#32452;&#21512;&#24615;&#21644;&#35748;&#30693;&#19982;&#21457;&#23637;&#26426;&#22120;&#20154;&#20013;&#30340;&#31526;&#21495;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;&#22522;&#20110;&#24863;&#30693;&#36816;&#21160;&#20449;&#24687;&#30340;&#31867;&#21035;&#24418;&#25104;&#21644;&#36890;&#36807;&#21333;&#19968;&#32508;&#21512;&#27169;&#22411;&#20013;&#30340;&#35789;&#24207;&#21015;&#20132;&#27969;&#30340;&#31526;&#21495;&#20132;&#27969;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#22810;&#27169;&#24577;&#24863;&#30693;&#36816;&#21160;&#20449;&#24687;&#36827;&#34892;&#31867;&#21035;&#24418;&#25104;&#65292;&#24182;&#36890;&#36807;&#20195;&#29702;&#38388;&#30340;&#35789;&#24207;&#21015;&#20132;&#27969;&#23454;&#29616;&#31526;&#21495;&#30340;&#20986;&#29616;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#20855;&#26377;&#32452;&#21512;&#24615;&#30340;&#35821;&#20041;&#30693;&#35782;&#30340;&#20986;&#29616;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#36890;&#36807;&#32467;&#21512;&#19982;&#35789;&#30456;&#20851;&#30340;&#20449;&#24687;&#39044;&#27979;&#26410;&#35266;&#23519;&#21040;&#30340;&#24773;&#20917;&#30340;&#24863;&#30693;&#36816;&#21160;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a computational model for a symbol emergence system that enables the emergence of lexical knowledge with combinatoriality among agents through a Metropolis-Hastings naming game and cross-situational learning. Many computational models have been proposed to investigate combinatoriality in emergent communication and symbol emergence in cognitive and developmental robotics. However, existing models do not sufficiently address category formation based on sensory-motor information and semiotic communication through the exchange of word sequences within a single integrated model. Our proposed model facilitates the emergence of lexical knowledge with combinatoriality by performing category formation using multimodal sensory-motor information and enabling semiotic communication through the exchange of word sequences among agents in a unified model. Furthermore, the model enables an agent to predict sensory-motor information for unobserved situations by combining words associated wit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#31574;&#30053;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#65288;MAT&#65289;&#65292;&#36890;&#36807;&#22312;&#32454;&#35843;&#38454;&#27573;&#21152;&#20837;&#23545;&#25239;&#35757;&#32451;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#37319;&#26679;&#26041;&#27861;&#24314;&#31435;&#20102;MAT&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MAT&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.15826</link><description>&lt;p&gt;
MAT: &#23545;&#24494;&#35843;&#20013;&#23545;&#25239;&#35757;&#32451;&#30340;&#28151;&#21512;&#31574;&#30053;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
MAT: Mixed-Strategy Game of Adversarial Training in Fine-tuning. (arXiv:2306.15826v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#31574;&#30053;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#65288;MAT&#65289;&#65292;&#36890;&#36807;&#22312;&#32454;&#35843;&#38454;&#27573;&#21152;&#20837;&#23545;&#25239;&#35757;&#32451;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#37319;&#26679;&#26041;&#27861;&#24314;&#31435;&#20102;MAT&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MAT&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#26377;&#25928;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#32454;&#35843;&#38454;&#27573;&#21152;&#20837;&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#20174;&#21338;&#24328;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#23545;&#25239;&#35757;&#32451;&#30340;&#24212;&#29992;&#23545;&#24212;&#20110;&#32431;&#31574;&#30053;&#28216;&#25103;&#65292;&#20854;&#22312;&#31574;&#30053;&#33539;&#22260;&#26041;&#38754;&#23384;&#22312;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#22240;&#27492;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;&#20026;&#20102;&#25512;&#21160;&#24615;&#33021;&#36793;&#30028;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#31574;&#30053;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#65288;MAT&#65289;&#12290;&#22312;&#26041;&#27861;&#19978;&#65292;&#25105;&#20204;&#20351;&#29992;&#29109;&#38236;&#20687;&#19979;&#38477;&#25512;&#23548;&#20102;&#23545;&#25239;&#35757;&#32451;&#30340;&#28151;&#21512;&#31574;&#30053;&#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#65292;&#36890;&#36807;&#37319;&#26679;&#26041;&#27861;&#24314;&#31435;&#20102;MAT&#12290;&#20026;&#20102;&#39564;&#35777;MAT&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;BERT&#21644;RoBERTa&#31561;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#22522;&#20934;&#23454;&#39564;&#12290;MAT&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning large-scale pre-trained language models has been demonstrated effective for various natural language processing (NLP) tasks. Previous studies have established that incorporating adversarial training during the fine-tuning stage can significantly enhance model generalization and robustness. However, from the perspective of game theory, such utilizations of adversarial training correspond to pure-strategy games, which are inherently limited in terms of the scope of their strategies, thereby still having room for improvement. In order to push the performance boundaries, we propose a novel Mixed-strategy Adversarial Training algorithm (MAT). Methodologically, we derive the Nash equilibrium of a mixed-strategy game for adversarial training using Entropy Mirror Descent to establish MAT by sampling method. To verify the effectiveness of MAT, we conducted extensive benchmark experiments on large-scale pre-trained models, such as BERT and RoBERTa. MAT significantly outperforms the s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#21482;&#20351;&#29992;&#26368;&#26377;&#20449;&#24515;&#30340;&#27169;&#22411;&#30340;&#36755;&#20986;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#24212;&#29992;&#31243;&#24207;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15824</link><description>&lt;p&gt;
&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Confidence-based Ensembles of End-to-End Speech Recognition Models. (arXiv:2306.15824v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#21482;&#20351;&#29992;&#26368;&#26377;&#20449;&#24515;&#30340;&#27169;&#22411;&#30340;&#36755;&#20986;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#24212;&#29992;&#31243;&#24207;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#24180;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#25968;&#37327;&#19981;&#26029;&#22686;&#38271;&#12290;&#36825;&#20123;&#27169;&#22411;&#32463;&#24120;&#34987;&#35843;&#25972;&#21040;&#26032;&#39046;&#22495;&#25110;&#35821;&#35328;&#65292;&#23548;&#33268;&#19987;&#23478;&#31995;&#32479;&#22823;&#37327;&#28044;&#29616;&#65292;&#22312;&#30446;&#26631;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#33258;&#24049;&#19987;&#19994;&#39046;&#22495;&#20043;&#22806;&#30340;&#24615;&#33021;&#21364;&#30456;&#23545;&#36739;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#38598;&#25104;&#26469;&#25506;&#32034;&#36825;&#20123;&#19987;&#23478;&#27169;&#22411;&#30340;&#32452;&#21512;&#65306;&#22312;&#27169;&#22411;&#20013;&#21482;&#20351;&#29992;&#26368;&#26377;&#20449;&#24515;&#30340;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#20551;&#35774;&#27169;&#22411;&#30340;&#30446;&#26631;&#25968;&#25454;&#38500;&#20102;&#19968;&#20010;&#23567;&#30340;&#39564;&#35777;&#38598;&#22806;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#24212;&#29992;&#31243;&#24207;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#30001;5&#20010;&#21333;&#35821;&#27169;&#22411;&#32452;&#25104;&#30340;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#38598;&#25104;&#20248;&#20110;&#36890;&#36807;&#19987;&#29992;&#35821;&#35328;&#35782;&#21035;&#27169;&#22359;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#30340;&#31995;&#32479;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#23558;&#22522;&#30784;&#27169;&#22411;&#21644;&#35843;&#25972;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#22312;&#21407;&#22987;&#21644;&#30446;&#26631;&#25968;&#25454;&#19978;&#21462;&#24471;&#24378;&#22823;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26550;&#26500;&#19978;&#39564;&#35777;&#20102;&#25152;&#26377;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The number of end-to-end speech recognition models grows every year. These models are often adapted to new domains or languages resulting in a proliferation of expert systems that achieve great results on target data, while generally showing inferior performance outside of their domain of expertise. We explore combination of such experts via confidence-based ensembles: ensembles of models where only the output of the most-confident model is used. We assume that models' target data is not available except for a small validation set. We demonstrate effectiveness of our approach with two applications. First, we show that a confidence-based ensemble of 5 monolingual models outperforms a system where model selection is performed via a dedicated language identification block. Second, we demonstrate that it is possible to combine base and adapted models to achieve strong results on both original and target data. We validate all our results on multiple datasets and model architectures.
&lt;/p&gt;</description></item><item><title>FLuRKA&#26159;&#19968;&#31181;&#34701;&#21512;&#20302;&#31209;&#21644;&#26680;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;transformer&#31867;&#21035;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#36817;&#20284;&#25216;&#26415;&#65292;&#22312;&#36816;&#34892;&#26102;&#38388;&#24615;&#33021;&#21644;&#36136;&#37327;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.15799</link><description>&lt;p&gt;
FLuRKA: &#24555;&#36895;&#34701;&#21512;&#20302;&#31209;&#21644;&#26680;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
FLuRKA: Fast fused Low-Rank &amp; Kernel Attention. (arXiv:2306.15799v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15799
&lt;/p&gt;
&lt;p&gt;
FLuRKA&#26159;&#19968;&#31181;&#34701;&#21512;&#20302;&#31209;&#21644;&#26680;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;transformer&#31867;&#21035;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#36817;&#20284;&#25216;&#26415;&#65292;&#22312;&#36816;&#34892;&#26102;&#38388;&#24615;&#33021;&#21644;&#36136;&#37327;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;transformer&#32467;&#26500;&#30340;&#25552;&#20986;&#20197;&#26469;&#65292;&#35768;&#22810;&#39640;&#25928;&#30340;&#36817;&#20284;&#33258;&#27880;&#24847;&#21147;&#25216;&#26415;&#24050;&#32463;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#20854;&#20013;&#20004;&#31181;&#27969;&#34892;&#30340;&#25216;&#26415;&#31867;&#21035;&#26159;&#20302;&#31209;&#21644;&#26680;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#30456;&#20114;&#34917;&#20805;&#65292;&#21033;&#29992;&#36825;&#20123;&#21327;&#21516;&#25928;&#24212;&#26469;&#34701;&#21512;&#20302;&#31209;&#21644;&#26680;&#26041;&#27861;&#65292;&#20135;&#29983;&#20102;&#19968;&#31181;&#26032;&#30340;transformer&#31867;&#21035;&#65306;FLuRKA&#65288;&#24555;&#36895;&#20302;&#31209;&#21644;&#26680;&#27880;&#24847;&#21147;&#65289;&#12290;FLuRKA&#30456;&#23545;&#20110;&#36825;&#20123;&#36817;&#20284;&#25216;&#26415;&#25552;&#20379;&#20102;&#21487;&#35266;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#26041;&#38754;&#35780;&#20272;&#20102;FLuRKA&#30340;&#36816;&#34892;&#26102;&#38388;&#24615;&#33021;&#21644;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#36816;&#34892;&#26102;&#38388;&#20998;&#26512;&#25552;&#20379;&#20102;&#22810;&#31181;&#21442;&#25968;&#37197;&#32622;&#65292;&#22312;&#36825;&#20123;&#37197;&#32622;&#19979;&#65292;FLuRKA&#20855;&#26377;&#21152;&#36895;&#25928;&#26524;&#65307;&#25105;&#20204;&#30340;&#20934;&#30830;&#24615;&#20998;&#26512;&#38480;&#23450;&#20102;FLuRKA&#30456;&#23545;&#20110;&#20840;&#27880;&#24847;&#21147;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#23454;&#20363;&#21270;&#20102;&#19977;&#31181;FLuRKA&#21464;&#20307;&#65292;&#30456;&#23545;&#20110;&#20302;&#31209;&#21644;&#26680;&#26041;&#27861;&#20998;&#21035;&#23454;&#29616;&#20102;&#39640;&#36798;3.3&#20493;&#21644;1.7&#20493;&#30340;&#32463;&#39564;&#21152;&#36895;&#12290;&#36825;&#24847;&#21619;&#30528;&#26356;&#24555;&#30340;&#36816;&#34892;&#26102;&#38388;&#65292;&#32780;&#19988;&#36136;&#37327;&#20173;&#28982;&#20445;&#25345;&#19981;&#38169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many efficient approximate self-attention techniques have become prevalent since the inception of the transformer architecture. Two popular classes of these techniques are low-rank and kernel methods. Each of these methods has its own strengths. We observe these strengths synergistically complement each other and exploit these synergies to fuse low-rank and kernel methods, producing a new class of transformers: FLuRKA (Fast Low-Rank and Kernel Attention). FLuRKA provide sizable performance gains over these approximate techniques and are of high quality. We theoretically and empirically evaluate both the runtime performance and quality of FLuRKA. Our runtime analysis posits a variety of parameter configurations where FLuRKA exhibit speedups and our accuracy analysis bounds the error of FLuRKA with respect to full-attention. We instantiate three FLuRKA variants which experience empirical speedups of up to 3.3x and 1.7x over low-rank and kernel methods respectively. This translates to spe
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#22312;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#34429;&#28982;GPT-4&#30340;&#21484;&#22238;&#29575;&#36739;&#39640;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#20542;&#21521;&#20110;&#36807;&#24230;&#20462;&#27491;&#12290;</title><link>http://arxiv.org/abs/2306.15788</link><description>&lt;p&gt;
&#22312;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#26041;&#38754;&#35780;&#20272;GPT-3.5&#21644;GPT-4
&lt;/p&gt;
&lt;p&gt;
Evaluating GPT-3.5 and GPT-4 on Grammatical Error Correction for Brazilian Portuguese. (arXiv:2306.15788v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15788
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#22312;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#34429;&#28982;GPT-4&#30340;&#21484;&#22238;&#29575;&#36739;&#39640;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#20542;&#21521;&#20110;&#36807;&#24230;&#20462;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;GPT-3.5&#21644;GPT-4&#36825;&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#65288;GEC&#65289;&#24037;&#20855;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23558;&#20854;&#24615;&#33021;&#19982;Microsoft Word&#21644;Google Docs&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#38024;&#23545;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;GEC&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#22235;&#20010;&#31867;&#21035;&#65306;&#35821;&#27861;&#12289;&#25340;&#20889;&#12289;&#20114;&#32852;&#32593;&#21644;&#24555;&#36895;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;GPT-4&#30340;&#21484;&#22238;&#29575;&#27604;&#20854;&#20182;&#26041;&#27861;&#39640;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#20542;&#21521;&#20110;&#20855;&#26377;&#36739;&#20302;&#30340;&#31934;&#30830;&#24230;&#65292;&#23548;&#33268;&#36807;&#24230;&#20462;&#27491;&#12290;&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#23454;&#38469;GEC&#24037;&#20855;&#30340;&#28508;&#21147;&#65292;&#24182;&#40723;&#21169;&#36827;&#19968;&#27493;&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#21644;&#20854;&#20182;&#25945;&#32946;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the effectiveness of GPT-3.5 and GPT-4, two large language models, as Grammatical Error Correction (GEC) tools for Brazilian Portuguese and compare their performance against Microsoft Word and Google Docs. We introduce a GEC dataset for Brazilian Portuguese with four categories: Grammar, Spelling, Internet, and Fast typing. Our results show that while GPT-4 has higher recall than other methods, LLMs tend to have lower precision, leading to overcorrection. This study demonstrates the potential of LLMs as practical GEC tools for Brazilian Portuguese and encourages further exploration of LLMs for non-English languages and other educational settings.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20174;&#25216;&#26415;&#35282;&#24230;&#23450;&#20041;&#21644;&#25552;&#20986;&#20102;&#20154;&#31867;&#20013;&#24515;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;(HGAI)&#30340;&#19979;&#19968;&#27493;&#24037;&#20316;&#65292;&#21253;&#25324;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#12289;&#36866;&#24212;&#20154;&#31867;&#30340;&#24847;&#22270;&#34920;&#36798;&#21644;&#22686;&#24378;&#20154;&#31867;&#22312;&#21327;&#20316;&#24037;&#20316;&#27969;&#20013;&#30340;&#33021;&#21147;&#12290;&#36825;&#20010;&#24037;&#20316;&#30340;&#30446;&#26631;&#26159;&#21560;&#24341;&#36328;&#23398;&#31185;&#30740;&#31350;&#22242;&#38431;&#23545;HGAI&#30340;&#26032;&#20852;&#24819;&#27861;&#36827;&#34892;&#35752;&#35770;&#65292;&#24182;&#20445;&#25345;&#26410;&#26469;&#24037;&#20316;&#26223;&#35266;&#30340;&#25972;&#20307;&#36830;&#36143;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15774</link><description>&lt;p&gt;
&#20154;&#31867;&#20013;&#24515;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#19979;&#19968;&#27493;&#65306;&#25216;&#26415;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Next Steps for Human-Centered Generative AI: A Technical Perspective. (arXiv:2306.15774v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15774
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20174;&#25216;&#26415;&#35282;&#24230;&#23450;&#20041;&#21644;&#25552;&#20986;&#20102;&#20154;&#31867;&#20013;&#24515;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;(HGAI)&#30340;&#19979;&#19968;&#27493;&#24037;&#20316;&#65292;&#21253;&#25324;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#12289;&#36866;&#24212;&#20154;&#31867;&#30340;&#24847;&#22270;&#34920;&#36798;&#21644;&#22686;&#24378;&#20154;&#31867;&#22312;&#21327;&#20316;&#24037;&#20316;&#27969;&#20013;&#30340;&#33021;&#21147;&#12290;&#36825;&#20010;&#24037;&#20316;&#30340;&#30446;&#26631;&#26159;&#21560;&#24341;&#36328;&#23398;&#31185;&#30740;&#31350;&#22242;&#38431;&#23545;HGAI&#30340;&#26032;&#20852;&#24819;&#27861;&#36827;&#34892;&#35752;&#35770;&#65292;&#24182;&#20445;&#25345;&#26410;&#26469;&#24037;&#20316;&#26223;&#35266;&#30340;&#25972;&#20307;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21453;&#22797;&#36328;&#23398;&#31185;&#35752;&#35770;&#65292;&#25105;&#20204;&#20174;&#25216;&#26415;&#35282;&#24230;&#20026;&#20154;&#31867;&#20013;&#24515;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;(HGAI)&#23450;&#20041;&#21644;&#25552;&#20986;&#20102;&#19979;&#19968;&#27493;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#36129;&#29486;&#20102;&#19968;&#20010;&#36335;&#32447;&#22270;&#65292;&#27010;&#36848;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#19977;&#20010;&#23618;&#38754;&#19978;&#30340;&#26410;&#26469;&#26041;&#21521;&#65306;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#65307;&#36866;&#24212;&#20154;&#31867;&#30340;&#24847;&#22270;&#34920;&#36798;&#65307;&#22686;&#24378;&#20154;&#31867;&#22312;&#21327;&#20316;&#24037;&#20316;&#27969;&#20013;&#30340;&#33021;&#21147;&#12290;&#35813;&#36335;&#32447;&#22270;&#26088;&#22312;&#21560;&#24341;&#36328;&#23398;&#31185;&#30740;&#31350;&#22242;&#38431;&#23545;HGAI&#30340;&#26032;&#20852;&#24819;&#27861;&#36827;&#34892;&#20840;&#38754;&#30340;&#35752;&#35770;&#65292;&#21516;&#26102;&#20445;&#25345;&#26410;&#26469;&#24037;&#20316;&#26223;&#35266;&#30340;&#25972;&#20307;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Through iterative, cross-disciplinary discussions, we define and propose next-steps for Human-centered Generative AI (HGAI) from a technical perspective. We contribute a roadmap that lays out future directions of Generative AI spanning three levels: Aligning with human values; Accommodating humans' expression of intents; and Augmenting humans' abilities in a collaborative workflow. This roadmap intends to draw interdisciplinary research teams to a comprehensive list of emergent ideas in HGAI, identifying their interested topics while maintaining a coherent big picture of the future work landscape.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26631;&#27880;&#30340;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#25104;&#26412;&#25552;&#21319;NLP&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#24046;&#24322;&#24615;&#37319;&#26679;&#31574;&#30053;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20998;&#31867;&#21644;&#25490;&#24207;&#20219;&#21153;&#19978;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#27169;&#22411;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.15766</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26631;&#27880;&#32773;&#65306;&#20197;&#26368;&#23567;&#25104;&#26412;&#22686;&#24378;NLP&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Annotators: Enhancing Generalization of NLP Models at Minimal Cost. (arXiv:2306.15766v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26631;&#27880;&#30340;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#25104;&#26412;&#25552;&#21319;NLP&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#24046;&#24322;&#24615;&#37319;&#26679;&#31574;&#30053;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20998;&#31867;&#21644;&#25490;&#24207;&#20219;&#21153;&#19978;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#27169;&#22411;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#24335;NLP&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#24456;&#39640;&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#23545;&#20110;&#20302;&#25968;&#25454;&#39046;&#22495;&#30340;&#36755;&#20837;&#20063;&#23481;&#26131;&#20986;&#29616;&#22833;&#36133;&#65292;&#20363;&#22914;&#35757;&#32451;&#25968;&#25454;&#20013;&#26410;&#21253;&#21547;&#30340;&#39046;&#22495;&#12290;&#20316;&#20026;&#25910;&#38598;&#29305;&#23450;&#39046;&#22495;&#30340;&#30495;&#23454;&#26631;&#31614;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#23545;&#36755;&#20837;&#36827;&#34892;&#26631;&#27880;&#24182;&#25552;&#21319;NLP&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;LLM&#26631;&#27880;&#30340;&#39044;&#31639;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#36755;&#20837;&#36827;&#34892;&#26631;&#27880;&#21644;&#37325;&#26032;&#35757;&#32451;NLP&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#27969;&#34892;&#30340;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#25928;&#26524;&#19981;&#20339;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#21644;&#24494;&#35843;NLP&#27169;&#22411;&#20043;&#38388;&#39044;&#27979;&#20998;&#25968;&#24046;&#24322;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#21033;&#29992;&#20102;&#22823;&#22810;&#25968;NLP&#27169;&#22411;&#37117;&#26159;&#20174;&#22522;&#30784;&#27169;&#22411;&#24494;&#35843;&#32780;&#26469;&#30340;&#20107;&#23454;&#12290;&#20998;&#31867;(&#35821;&#20041;&#30456;&#20284;&#24230;)&#21644;&#25490;&#24207;(&#35821;&#20041;&#25628;&#32034;)&#20219;&#21153;&#30340;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#37319;&#26679;&#31574;&#30053;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art supervised NLP models achieve high accuracy but are also susceptible to failures on inputs from low-data regimes, such as domains that are not represented in training data. As an approximation to collecting ground-truth labels for the specific domain, we study the use of large language models (LLMs) for annotating inputs and improving the generalization of NLP models. Specifically, given a budget for LLM annotations, we present an algorithm for sampling the most informative inputs to annotate and retrain the NLP model. We find that popular active learning strategies such as uncertainty-based sampling do not work well. Instead, we propose a sampling strategy based on the difference in prediction scores between the base model and the finetuned NLP model, utilizing the fact that most NLP models are finetuned from a base model. Experiments with classification (semantic similarity) and ranking (semantic search) tasks show that our sampling strategy leads to significant gain
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#23450;&#37327;&#25991;&#26412;&#21644;&#32593;&#32476;&#20998;&#26512;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#26368;&#22823;&#30340;&#40657;&#27934;incels&#35770;&#22363;&#22914;&#20309;&#35752;&#35770;&#36523;&#20221;&#32676;&#20307;&#12290;&#30740;&#31350;&#21457;&#29616;&#35813;&#31038;&#21306;&#20135;&#29983;&#20102;&#35768;&#22810;&#26032;&#30340;&#36523;&#20221;&#26415;&#35821;&#65292;&#23384;&#22312;&#29289;&#36136;&#20027;&#20041;&#30340;&#24847;&#35782;&#24418;&#24577;&#12290;&#23545;&#27492;&#25105;&#20204;&#35752;&#35770;&#20102;&#23545;&#33258;&#21160;&#21270; misogynist hate speech &#26816;&#27979;&#30740;&#31350;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.15745</link><description>&lt;p&gt;
&#12298;&#22312;&#19968;&#20010;&#23545;&#22899;&#24615;&#21388;&#24694;&#30340;incels&#35770;&#22363;&#20013;&#30340;&#36523;&#20221;&#24314;&#26500;&#12299;
&lt;/p&gt;
&lt;p&gt;
Identity Construction in a Misogynist Incels Forum. (arXiv:2306.15745v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#23450;&#37327;&#25991;&#26412;&#21644;&#32593;&#32476;&#20998;&#26512;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#26368;&#22823;&#30340;&#40657;&#27934;incels&#35770;&#22363;&#22914;&#20309;&#35752;&#35770;&#36523;&#20221;&#32676;&#20307;&#12290;&#30740;&#31350;&#21457;&#29616;&#35813;&#31038;&#21306;&#20135;&#29983;&#20102;&#35768;&#22810;&#26032;&#30340;&#36523;&#20221;&#26415;&#35821;&#65292;&#23384;&#22312;&#29289;&#36136;&#20027;&#20041;&#30340;&#24847;&#35782;&#24418;&#24577;&#12290;&#23545;&#27492;&#25105;&#20204;&#35752;&#35770;&#20102;&#23545;&#33258;&#21160;&#21270; misogynist hate speech &#26816;&#27979;&#30740;&#31350;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#23450;&#37327;&#25991;&#26412;&#21644;&#32593;&#32476;&#20998;&#26512;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;incels.is&#65292;&#21363;&#26368;&#22823;&#30340;&#40657;&#27934;incels&#35770;&#22363;&#22914;&#20309;&#35752;&#35770;&#36523;&#20221;&#32676;&#20307;&#12290;&#25105;&#20204;&#21457;&#29616;&#35813;&#31038;&#21306;&#20135;&#29983;&#20102;&#35768;&#22810;&#26032;&#30340;&#36523;&#20221;&#26415;&#35821;&#65292;&#23613;&#31649;&#22899;&#24615;&#30340;&#26415;&#35821;&#26368;&#24120;&#35265;&#65292;&#20294;&#20854;&#20182;&#23569;&#25968;&#32676;&#20307;&#30340;&#25552;&#21450;&#20063;&#22312;&#22686;&#21152;&#12290;&#23545;&#36523;&#20221;&#32676;&#20307;&#30340;&#20851;&#32852;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#20010;&#31038;&#21306;&#23384;&#22312;&#30528;&#29289;&#36136;&#20027;&#20041;&#30340;&#24847;&#35782;&#24418;&#24577;&#65292;&#20854;&#20013;&#36523;&#20307;&#22806;&#35980;&#12289;&#24615;&#21035;&#21644;&#31181;&#26063;&#31561;&#20915;&#23450;&#20102;&#20154;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23545;&#33258;&#21160;&#21270; misogynist hate speech &#26816;&#27979;&#30740;&#31350;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online communities of involuntary celibates (incels) are a prominent source of misogynist hate speech. In this paper, we use quantitative text and network analysis approaches to examine how identity groups are discussed on incels.is, the largest black-pilled incels forum. We find that this community produces a wide range of novel identity terms and, while terms for women are most common, mentions of other minoritized identities are increasing. An analysis of the associations made with identity groups suggests an essentialist ideology where physical appearance, as well as gender and racial hierarchies, determine human value. We discuss implications for research into automated misogynist hate speech detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DMNER&#30340;&#26032;&#22411;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#35782;&#21035;&#26694;&#26550;&#65292;&#36890;&#36807;&#26816;&#27979;&#23454;&#20307;&#36793;&#30028;&#21644;&#21305;&#37197;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#26469;&#25552;&#39640;NER&#30340;&#24615;&#33021;&#12290;&#22312;&#26377;&#30417;&#30563;NER&#12289;&#36828;&#31243;&#30417;&#30563;NER&#21644;&#22810;&#25968;&#25454;&#38598;&#21512;&#24182;&#35757;&#32451;NER&#31561;&#22330;&#26223;&#20013;&#65292;DMNER&#37117;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15736</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#27979;&#21644;&#21305;&#37197;&#36827;&#34892;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Biomedical Entity Recognition by Detection and Matching. (arXiv:2306.15736v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DMNER&#30340;&#26032;&#22411;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#35782;&#21035;&#26694;&#26550;&#65292;&#36890;&#36807;&#26816;&#27979;&#23454;&#20307;&#36793;&#30028;&#21644;&#21305;&#37197;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#26469;&#25552;&#39640;NER&#30340;&#24615;&#33021;&#12290;&#22312;&#26377;&#30417;&#30563;NER&#12289;&#36828;&#31243;&#30417;&#30563;NER&#21644;&#22810;&#25968;&#25454;&#38598;&#21512;&#24182;&#35757;&#32451;NER&#31561;&#22330;&#26223;&#20013;&#65292;DMNER&#37117;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;BNER&#65289;&#26159;&#35768;&#22810;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;&#19982;&#19968;&#33324;&#30340;NER&#19981;&#21516;&#65292;BNER&#38656;&#35201;&#20840;&#38754;&#25484;&#25569;&#39046;&#22495;&#30693;&#35782;&#65292;&#32780;&#19988;&#22312;&#35757;&#32451;&#25968;&#25454;&#20043;&#22806;&#34701;&#20837;&#22806;&#37096;&#30693;&#35782;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;BNER&#26694;&#26550;&#65292;&#31216;&#20026;DMNER&#12290;&#36890;&#36807;&#21033;&#29992;&#24050;&#26377;&#30340;&#23454;&#20307;&#34920;&#31034;&#27169;&#22411;SAPBERT&#65292;&#25105;&#20204;&#23558;BNER&#20316;&#20026;&#19968;&#20010;&#20004;&#27493;&#39588;&#30340;&#36807;&#31243;&#26469;&#22788;&#29702;&#65306;&#23454;&#20307;&#36793;&#30028;&#26816;&#27979;&#21644;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#21305;&#37197;&#12290;DMNER&#22312;&#22810;&#31181;NER&#22330;&#26223;&#20013;&#20855;&#26377;&#36866;&#29992;&#24615;&#65306;1&#65289;&#22312;&#26377;&#30417;&#30563;NER&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;DMNER&#26377;&#25928;&#32416;&#27491;&#20102;&#22522;&#32447;NER&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;2&#65289;&#22312;&#36828;&#31243;&#30417;&#30563;NER&#20013;&#65292;&#23558;MRC&#21644;AutoNER&#20316;&#20026;&#36328;&#24230;&#36793;&#30028;&#26816;&#27979;&#22120;&#30456;&#32467;&#21512;&#65292;&#20351;DMNER&#33021;&#22815;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;3&#65289;&#23545;&#20110;&#36890;&#36807;&#21512;&#24182;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;NER&#35757;&#32451;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19982;DS-NER&#31867;&#20284;&#30340;&#26694;&#26550;&#65292;&#20294;&#36824;&#39069;&#22806;&#21033;&#29992;ChatGPT&#26469;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#30701;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical named entity recognition (BNER) serves as the foundation for numerous biomedical text mining tasks. Unlike general NER, BNER require a comprehensive grasp of the domain, and incorporating external knowledge beyond training data poses a significant challenge. In this study, we propose a novel BNER framework called DMNER. By leveraging existing entity representation models SAPBERT, we tackle BNER as a two-step process: entity boundary detection and biomedical entity matching. DMNER exhibits applicability across multiple NER scenarios: 1) In supervised NER, we observe that DMNER effectively rectifies the output of baseline NER models, thereby further enhancing performance. 2) In distantly supervised NER, combining MRC and AutoNER as span boundary detectors enables DMNER to achieve satisfactory results. 3) For training NER by merging multiple datasets, we adopt a framework similar to DS-NER but additionally leverage ChatGPT to obtain high-quality phrases in the training. Through
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#30333;&#20154;&#33267;&#19978;&#20027;&#20041;&#26497;&#31471;&#20027;&#20041;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#21644;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#38598;&#20197;&#21450;&#21253;&#25324;&#20013;&#24615;&#21644;&#21453;&#31181;&#26063;&#20027;&#20041;&#25968;&#25454;&#65292;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#36827;&#34892;&#24369;&#30417;&#30563;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#23545;&#26032;&#39046;&#22495;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21453;&#31181;&#26063;&#20027;&#20041;&#25991;&#26412;&#20316;&#20026;&#21453;&#20363;&#26469;&#20943;&#36731;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2306.15732</link><description>&lt;p&gt;
&#19968;&#20010;&#24369;&#30417;&#30563;&#20998;&#31867;&#22120;&#21644;&#30333;&#20154;&#33267;&#19978;&#20027;&#20041;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Weakly Supervised Classifier and Dataset of White Supremacist Language. (arXiv:2306.15732v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15732
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#30333;&#20154;&#33267;&#19978;&#20027;&#20041;&#26497;&#31471;&#20027;&#20041;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#21644;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#38598;&#20197;&#21450;&#21253;&#25324;&#20013;&#24615;&#21644;&#21453;&#31181;&#26063;&#20027;&#20041;&#25968;&#25454;&#65292;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#36827;&#34892;&#24369;&#30417;&#30563;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#23545;&#26032;&#39046;&#22495;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21453;&#31181;&#26063;&#20027;&#20041;&#25991;&#26412;&#20316;&#20026;&#21453;&#20363;&#26469;&#20943;&#36731;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#30333;&#20154;&#33267;&#19978;&#20027;&#20041;&#26497;&#31471;&#20027;&#20041;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#21644;&#20998;&#31867;&#22120;&#65292;&#36825;&#26159;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#20013;&#26085;&#30410;&#22686;&#38271;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24369;&#30417;&#30563;&#20998;&#31867;&#22120;&#26159;&#22312;&#20174;&#26126;&#30830;&#30340;&#30333;&#20154;&#33267;&#19978;&#20027;&#20041;&#39046;&#22495;&#37197;&#23545;&#20013;&#24615;&#21644;&#21453;&#31181;&#26063;&#20027;&#20041;&#25968;&#25454;&#30340;&#22823;&#22411;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#23545;&#26032;&#39046;&#22495;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23558;&#21453;&#31181;&#26063;&#20027;&#20041;&#25991;&#26412;&#20316;&#20026;&#30333;&#20154;&#33267;&#19978;&#20027;&#20041;&#35821;&#35328;&#30340;&#21453;&#20363;&#21487;&#20197;&#20943;&#36731;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a dataset and classifier for detecting the language of white supremacist extremism, a growing issue in online hate speech. Our weakly supervised classifier is trained on large datasets of text from explicitly white supremacist domains paired with neutral and anti-racist data from similar domains. We demonstrate that this approach improves generalization performance to new domains. Incorporating anti-racist texts as counterexamples to white supremacist language mitigates bias.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#39640;&#25928;&#30340;&#26080;&#25968;&#25454;&#23545;&#25239;&#26816;&#27979;&#30340;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#23545;&#25239;&#26679;&#26412;&#19982;&#39640;&#32500;&#36755;&#20837;&#20013;&#30340;&#29305;&#23450;&#21521;&#37327;&#30340;&#20851;&#31995;&#65292;&#35745;&#31639;&#20986;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#65288;UAPs&#65289;&#12290;&#22522;&#20110;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#25968;&#25454;&#23545;&#25239;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;UAPs&#23545;&#27491;&#24120;&#26679;&#26412;&#21644;&#23545;&#25239;&#26679;&#26412;&#30340;&#21453;&#24212;&#20135;&#29983;&#19981;&#21516;&#30340;&#32467;&#26524;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;&#20855;&#20307;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#32500;&#25345;&#20102;&#19982;&#27491;&#24120;&#25512;&#26029;&#30456;&#31561;&#30340;&#26102;&#38388;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2306.15705</link><description>&lt;p&gt;
&#20851;&#20110;&#29992;&#20110;&#39640;&#25928;&#30340;&#26080;&#25968;&#25454;&#23545;&#25239;&#26816;&#27979;&#30340;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
On the Universal Adversarial Perturbations for Efficient Data-free Adversarial Detection. (arXiv:2306.15705v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#39640;&#25928;&#30340;&#26080;&#25968;&#25454;&#23545;&#25239;&#26816;&#27979;&#30340;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#23545;&#25239;&#26679;&#26412;&#19982;&#39640;&#32500;&#36755;&#20837;&#20013;&#30340;&#29305;&#23450;&#21521;&#37327;&#30340;&#20851;&#31995;&#65292;&#35745;&#31639;&#20986;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#65288;UAPs&#65289;&#12290;&#22522;&#20110;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#25968;&#25454;&#23545;&#25239;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;UAPs&#23545;&#27491;&#24120;&#26679;&#26412;&#21644;&#23545;&#25239;&#26679;&#26412;&#30340;&#21453;&#24212;&#20135;&#29983;&#19981;&#21516;&#30340;&#32467;&#26524;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;&#20855;&#20307;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#32500;&#25345;&#20102;&#19982;&#27491;&#24120;&#25512;&#26029;&#30456;&#31561;&#30340;&#26102;&#38388;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#23545;&#25239;&#26679;&#26412;&#20197;&#27450;&#39575;&#27169;&#22411;&#26159;&#30830;&#20445;&#31038;&#20132;&#23433;&#20840;&#24212;&#29992;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#25239;&#26816;&#27979;&#26041;&#27861;&#38656;&#35201;&#35775;&#38382;&#36275;&#22815;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#24341;&#21457;&#20102;&#19982;&#38544;&#31169;&#27844;&#38706;&#21644;&#36890;&#29992;&#24615;&#30456;&#20851;&#30340;&#26126;&#26174;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25915;&#20987;&#31639;&#27861;&#20135;&#29983;&#30340;&#23545;&#25239;&#26679;&#26412;&#19982;&#39640;&#32500;&#36755;&#20837;&#20013;&#30340;&#29305;&#23450;&#21521;&#37327;&#23494;&#20999;&#30456;&#20851;&#12290;&#36825;&#20123;&#21521;&#37327;&#31216;&#20026;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#65288;UAPs&#65289;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35745;&#31639;&#24471;&#20986;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#25968;&#25454;&#23545;&#25239;&#26816;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23545;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#23545;&#27491;&#24120;&#26679;&#26412;&#21644;&#23545;&#25239;&#26679;&#26412;&#20135;&#29983;&#19981;&#21516;&#30340;&#21453;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#26816;&#27979;&#24615;&#33021;&#65292;&#24182;&#19988;&#32500;&#25345;&#20102;&#19982;&#27491;&#24120;&#25512;&#26029;&#30456;&#31561;&#30340;&#26102;&#38388;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting adversarial samples that are carefully crafted to fool the model is a critical step to socially-secure applications. However, existing adversarial detection methods require access to sufficient training data, which brings noteworthy concerns regarding privacy leakage and generalizability. In this work, we validate that the adversarial sample generated by attack algorithms is strongly related to a specific vector in the high-dimensional inputs. Such vectors, namely UAPs (Universal Adversarial Perturbations), can be calculated without original training data. Based on this discovery, we propose a data-agnostic adversarial detection framework, which induces different responses between normal and adversarial samples to UAPs. Experimental results show that our method achieves competitive detection performance on various text classification tasks, and maintains an equivalent time consumption to normal inference.
&lt;/p&gt;</description></item><item><title>Voicebox&#26159;&#19968;&#31181;&#22823;&#35268;&#27169;&#30340;&#22810;&#35821;&#35328;&#36890;&#29992;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#33258;&#22238;&#24402;&#30340;&#27969;&#21305;&#37197;&#27169;&#22411;&#65292;&#22312;&#25991;&#26412;&#21644;&#38899;&#39057;&#19978;&#19979;&#25991;&#26465;&#20214;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#65292;&#22122;&#22768;&#21435;&#38500;&#65292;&#20869;&#23481;&#32534;&#36753;&#65292;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#26679;&#21270;&#30340;&#26679;&#26412;&#29983;&#25104;&#31561;&#22810;&#31181;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.15687</link><description>&lt;p&gt;
Voicebox&#65306;&#22823;&#35268;&#27169;&#30340;&#22810;&#35821;&#35328;&#36890;&#29992;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale. (arXiv:2306.15687v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15687
&lt;/p&gt;
&lt;p&gt;
Voicebox&#26159;&#19968;&#31181;&#22823;&#35268;&#27169;&#30340;&#22810;&#35821;&#35328;&#36890;&#29992;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#33258;&#22238;&#24402;&#30340;&#27969;&#21305;&#37197;&#27169;&#22411;&#65292;&#22312;&#25991;&#26412;&#21644;&#38899;&#39057;&#19978;&#19979;&#25991;&#26465;&#20214;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#65292;&#22122;&#22768;&#21435;&#38500;&#65292;&#20869;&#23481;&#32534;&#36753;&#65292;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#26679;&#21270;&#30340;&#26679;&#26412;&#29983;&#25104;&#31561;&#22810;&#31181;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;GPT&#21644;DALL-E&#24050;&#32463;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#30340;&#26041;&#24335;&#12290;&#36825;&#20123;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#25110;&#22270;&#20687;&#36755;&#20986;&#65292;&#32780;&#19988;&#36824;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#35299;&#20915;&#26410;&#34987;&#26126;&#30830;&#25945;&#25480;&#30340;&#20219;&#21153;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#22312;&#35268;&#27169;&#21644;&#20219;&#21153;&#36890;&#29992;&#21270;&#26041;&#38754;&#20173;&#28982;&#27604;&#36739;&#21407;&#22987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Voicebox&#65292;&#36825;&#26159;&#26368;&#22810;&#21151;&#33021;&#30340;&#38754;&#21521;&#35268;&#27169;&#30340;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#27169;&#22411;&#12290;Voicebox&#26159;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;&#30340;&#27969;&#21305;&#37197;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#38899;&#39057;&#19978;&#19979;&#25991;&#21644;&#25991;&#26412;&#26465;&#20214;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#29992;50,000&#23567;&#26102;&#30340;&#26410;&#32463;&#36807;&#28388;&#25110;&#22686;&#24378;&#30340;&#35821;&#38899;&#36827;&#34892;&#22635;&#20805;&#12290;&#19982;GPT&#31867;&#20284;&#65292;Voicebox&#21487;&#20197;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25191;&#34892;&#22810;&#31181;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#20294;&#26356;&#21152;&#28789;&#27963;&#65292;&#22240;&#20026;&#23427;&#36824;&#21487;&#20197;&#23545;&#26410;&#26469;&#30340;&#19978;&#19979;&#25991;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;Voicebox&#21487;&#20197;&#29992;&#20110;&#21333;&#35821;&#25110;&#36328;&#35821;&#35328;&#38646;&#26679;&#26412;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#65292;&#22122;&#22768;&#21435;&#38500;&#65292;&#20869;&#23481;&#32534;&#36753;&#65292;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#26679;&#21270;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;&#29305;&#21035;&#26159;&#65292;Voicebox
&lt;/p&gt;
&lt;p&gt;
Large-scale generative models such as GPT and DALL-E have revolutionized natural language processing and computer vision research. These models not only generate high fidelity text or image outputs, but are also generalists which can solve tasks not explicitly taught. In contrast, speech generative models are still primitive in terms of scale and task generalization. In this paper, we present Voicebox, the most versatile text-guided generative model for speech at scale. Voicebox is a non-autoregressive flow-matching model trained to infill speech, given audio context and text, trained on over 50K hours of speech that are neither filtered nor enhanced. Similar to GPT, Voicebox can perform many different tasks through in-context learning, but is more flexible as it can also condition on future context. Voicebox can be used for mono or cross-lingual zero-shot text-to-speech synthesis, noise removal, content editing, style conversion, and diverse sample generation. In particular, Voicebox 
&lt;/p&gt;</description></item><item><title>Master-ASR&#26159;&#19968;&#20010;&#22522;&#20110;&#27169;&#22359;&#21270;&#23398;&#20064;&#30340;ASR&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#27169;&#22359;&#26469;&#23454;&#29616;&#22810;&#35821;&#35328;&#21487;&#25193;&#23637;&#24615;&#21644;&#20302;&#36164;&#28304;&#36866;&#24212;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;ASR&#38754;&#20020;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#20302;&#36164;&#28304;&#36866;&#24212;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.15686</link><description>&lt;p&gt;
Master-ASR: &#23454;&#29616;ASR&#30340;&#22810;&#35821;&#35328;&#21487;&#25193;&#23637;&#24615;&#21644;&#20302;&#36164;&#28304;&#36866;&#24212;&#24615;&#30340;&#27169;&#22359;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Master-ASR: Achieving Multilingual Scalability and Low-Resource Adaptation in ASR with Modular Learning. (arXiv:2306.15686v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15686
&lt;/p&gt;
&lt;p&gt;
Master-ASR&#26159;&#19968;&#20010;&#22522;&#20110;&#27169;&#22359;&#21270;&#23398;&#20064;&#30340;ASR&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#27169;&#22359;&#26469;&#23454;&#29616;&#22810;&#35821;&#35328;&#21487;&#25193;&#23637;&#24615;&#21644;&#20302;&#36164;&#28304;&#36866;&#24212;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;ASR&#38754;&#20020;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#20302;&#36164;&#28304;&#36866;&#24212;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#26368;&#36817;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#38459;&#30861;&#20854;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#65306;(1)&#38590;&#20197;&#24341;&#20837;&#21487;&#25193;&#23637;&#24615;&#27169;&#22411;&#65292;&#25903;&#25345;&#26377;&#38480;&#30340;&#35757;&#32451;&#12289;&#25512;&#29702;&#21644;&#23384;&#20648;&#24320;&#38144;&#30340;&#26356;&#22810;&#35821;&#35328;&#65307;(2)&#20302;&#36164;&#28304;&#36866;&#24212;&#33021;&#21147;&#65292;&#33021;&#22815;&#22312;&#36991;&#20813;&#36807;&#25311;&#21512;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#30340;&#21516;&#26102;&#23454;&#29616;&#26377;&#25928;&#30340;&#20302;&#36164;&#28304;&#36866;&#24212;&#12290;&#21463;&#21040;&#26368;&#36817;&#30340;&#30740;&#31350;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20551;&#35774;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#36328;&#35821;&#35328;&#24191;&#27867;&#20849;&#20139;&#30340;&#27169;&#22359;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;ASR&#26694;&#26550;&#65292;&#31216;&#20026;\METHODNS&#65292;\textit{&#39318;&#27425;}&#21516;&#26102;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#21487;&#25193;&#23637;&#24615;&#21644;&#20302;&#36164;&#28304;&#36866;&#24212;&#33021;&#21147;&#65292;&#36825;&#24471;&#30410;&#20110;&#23427;&#30340;&#27169;&#22359;&#21270;&#21363;&#35013;&#37197;&#31574;&#30053;&#12290;&#20855;&#20307;&#22320;&#65292;\METHOD&#23398;&#20064;&#20102;&#19968;&#23567;&#32452;&#36890;&#29992;&#30340;&#23376;&#27169;&#22359;&#65292;&#24182;&#33258;&#36866;&#24212;&#22320;&#23558;&#23427;&#20204;&#32452;&#35013;&#21040;&#19981;&#21516;&#30340;&#35821;&#35328;&#20013;&#65292;&#20197;&#38477;&#20302;&#22810;&#35821;&#35328;&#24320;&#38144;&#24182;&#23454;&#29616;&#26377;&#25928;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive performance recently achieved by automatic speech recognition (ASR), we observe two primary challenges that hinder its broader applications: (1) The difficulty of introducing scalability into the model to support more languages with limited training, inference, and storage overhead; (2) The low-resource adaptation ability that enables effective low-resource adaptation while avoiding over-fitting and catastrophic forgetting issues. Inspired by recent findings, we hypothesize that we can address the above challenges with modules widely shared across languages. To this end, we propose an ASR framework, dubbed \METHODNS, that, \textit{for the first time}, simultaneously achieves strong multilingual scalability and low-resource adaptation ability thanks to its modularize-then-assemble strategy. Specifically, \METHOD learns a small set of generalizable sub-modules and adaptively assembles them for different languages to reduce the multilingual overhead and enable effec
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23454;&#26102;GPU&#35299;&#30721;&#20013;&#38598;&#25104;&#19978;&#19979;&#25991;&#20559;&#32622;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;ASR&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#20801;&#35768;&#21160;&#24577;&#19978;&#19979;&#25991;&#20999;&#25442;&#12290;</title><link>http://arxiv.org/abs/2306.15685</link><description>&lt;p&gt;
&#22312;GPU&#35299;&#30721;&#22120;&#20013;&#23454;&#29616;&#19978;&#19979;&#25991;&#20559;&#32622;&#29992;&#20110;&#22312;&#32447;ASR
&lt;/p&gt;
&lt;p&gt;
Implementing contextual biasing in GPU decoder for online ASR. (arXiv:2306.15685v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23454;&#26102;GPU&#35299;&#30721;&#20013;&#38598;&#25104;&#19978;&#19979;&#25991;&#20559;&#32622;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;ASR&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#20801;&#35768;&#21160;&#24577;&#19978;&#19979;&#25991;&#20999;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPU&#35299;&#30721;&#26174;&#33879;&#21152;&#36895;&#20102;ASR&#39044;&#27979;&#30340;&#36755;&#20986;&#12290;&#34429;&#28982;&#24050;&#32463;&#22312;&#22312;&#32447;ASR&#35299;&#30721;&#20013;&#20351;&#29992;&#20102;GPU&#65292;&#20294;&#26159;&#23578;&#26410;&#23545;GPU&#19978;&#30340;&#21518;&#22788;&#29702;&#21644;&#37325;&#26032;&#35780;&#20998;&#36827;&#34892;&#36866;&#24403;&#30340;&#30740;&#31350;&#12290;&#21033;&#29992;&#21487;&#29992;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#36827;&#34892;&#37325;&#26032;&#35780;&#20998;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;ASR&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;CPU&#22330;&#26223;&#20013;&#65292;&#22312;&#35299;&#30721;&#21644;&#23545;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#26435;&#37325;&#36827;&#34892;&#20559;&#32622;&#26041;&#38754;&#20351;&#29992;&#26684;&#23376;&#37325;&#26032;&#35780;&#20998;&#30340;&#21487;&#34892;&#24615;&#12290;&#22312;&#23454;&#26102;GPU&#35299;&#30721;&#20013;&#65292;&#20250;&#29983;&#25104;&#37096;&#20998;&#35782;&#21035;&#20551;&#35774;&#32780;&#19981;&#29983;&#25104;&#26684;&#23376;&#65292;&#36825;&#20351;&#24471;&#20559;&#32622;&#30340;&#23454;&#29616;&#26356;&#21152;&#22797;&#26434;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#25551;&#36848;&#20102;&#19968;&#31181;&#22312;&#23454;&#26102;GPU&#35299;&#30721;&#20013;&#38598;&#25104;&#19978;&#19979;&#25991;&#20559;&#32622;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#21033;&#29992;&#20102;&#26631;&#20934;&#30340;Kaldi GPU&#35299;&#30721;&#22120;&#12290;&#38500;&#20102;&#23545;&#37096;&#20998;ASR&#39044;&#27979;&#36827;&#34892;&#20559;&#32622;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#20801;&#35768;&#21160;&#24577;&#19978;&#19979;&#25991;&#20999;&#25442;&#65292;&#30452;&#25509;&#22312;GPU&#19978;&#23545;&#27599;&#20010;&#35821;&#38899;&#27573;&#36827;&#34892;&#28789;&#27963;&#30340;&#37325;&#26032;&#35780;&#20998;&#12290;&#20195;&#30721;&#24050;&#32463;&#20844;&#24320;&#21457;&#24067;&#24182;&#20351;&#29992;&#20102;&#24320;&#28304;&#27979;&#35797;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPU decoding significantly accelerates the output of ASR predictions. While GPUs are already being used for online ASR decoding, post-processing and rescoring on GPUs have not been properly investigated yet. Rescoring with available contextual information can considerably improve ASR predictions. Previous studies have proven the viability of lattice rescoring in decoding and biasing language model (LM) weights in offline and online CPU scenarios. In real-time GPU decoding, partial recognition hypotheses are produced without lattice generation, which makes the implementation of biasing more complex. The paper proposes and describes an approach to integrate contextual biasing in real-time GPU decoding while exploiting the standard Kaldi GPU decoder. Besides the biasing of partial ASR predictions, our approach also permits dynamic context switching allowing a flexible rescoring per each speech segment directly on GPU. The code is publicly released and tested with open-sourced test sets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26088;&#22312;&#20026;&#27861;&#35821;&#20013;&#30340;&#33258;&#21160;&#27880;&#37322;&#30452;&#25509;&#35328;&#35821;&#65288;AADS&#65289;&#21019;&#24314;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;&#30740;&#31350;&#37319;&#29992;&#26368;&#22823;&#30340;&#27861;&#35821;&#21465;&#36848;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#20219;&#21153;&#20173;&#38656;&#22823;&#37327;&#21162;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#19981;&#21516;&#22522;&#32447;&#27169;&#22411;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.15634</link><description>&lt;p&gt;
&#33258;&#21160;&#27880;&#37322;&#27861;&#35821;&#20070;&#38754;&#21465;&#36848;&#20013;&#30340;&#30452;&#25509;&#35328;&#35821;
&lt;/p&gt;
&lt;p&gt;
Automatic Annotation of Direct Speech in Written French Narratives. (arXiv:2306.15634v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#20026;&#27861;&#35821;&#20013;&#30340;&#33258;&#21160;&#27880;&#37322;&#30452;&#25509;&#35328;&#35821;&#65288;AADS&#65289;&#21019;&#24314;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;&#30740;&#31350;&#37319;&#29992;&#26368;&#22823;&#30340;&#27861;&#35821;&#21465;&#36848;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#20219;&#21153;&#20173;&#38656;&#22823;&#37327;&#21162;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#19981;&#21516;&#22522;&#32447;&#27169;&#22411;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#27880;&#37322;&#27861;&#35821;&#20070;&#38754;&#25991;&#26412;&#20013;&#30340;&#30452;&#25509;&#35328;&#35821;&#65288;AADS&#65289;&#32463;&#24120;&#22312;&#35745;&#31639;&#26426;&#21465;&#36848;&#29702;&#35299;&#20013;&#20351;&#29992;&#12290;&#24050;&#32463;&#30740;&#31350;&#20102;&#22522;&#20110;&#35268;&#21017;&#25110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#33521;&#35821;&#25110;&#24503;&#35821;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25105;&#20204;&#30340;&#30446;&#26631;&#35821;&#35328;&#27861;&#35821;&#65292;&#24456;&#23569;&#26377;&#30456;&#20851;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#21644;&#35780;&#20272;&#27861;&#35821;&#20013;&#30340;AADS&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25972;&#21512;&#20102;&#36804;&#20170;&#20026;&#27490;&#26631;&#27880;&#26377;&#27599;&#20010;&#21333;&#35789;&#30340;&#26368;&#22823;&#30340;&#27861;&#35821;&#21465;&#36848;&#25968;&#25454;&#38598;&#65307;&#25105;&#20204;&#38024;&#23545;&#24207;&#21015;&#26631;&#27880;&#25110;&#20854;&#20182;&#35821;&#35328;&#30340;AADS&#36866;&#24212;&#20102;&#21508;&#31181;&#22522;&#32447;&#27169;&#22411;&#65307;&#24182;&#19988;&#25105;&#20204;&#35774;&#35745;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#37325;&#28857;&#26159;&#27867;&#21270;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20010;&#20219;&#21153;&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#20316;&#65292;&#24182;&#24378;&#35843;&#20102;&#27599;&#20010;&#22522;&#32447;&#27169;&#22411;&#30340;&#29305;&#28857;&#12290;&#23613;&#31649;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#25913;&#36827;&#65292;&#20294;&#23427;&#26159;&#40723;&#21169;&#26356;&#22810;&#20851;&#20110;&#36825;&#20010;&#20027;&#39064;&#30340;&#30740;&#31350;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The automatic annotation of direct speech (AADS) in written text has been often used in computational narrative understanding. Methods based on either rules or deep neural networks have been explored, in particular for English or German languages. Yet, for French, our target language, not many works exist. Our goal is to create a unified framework to design and evaluate AADS models in French. For this, we consolidated the largest-to-date French narrative dataset annotated with DS per word; we adapted various baselines for sequence labelling or from AADS in other languages; and we designed and conducted an extensive evaluation focused on generalisation. Results show that the task still requires substantial efforts and emphasise characteristics of each baseline. Although this framework could be improved, it is a step further to encourage more research on the topic.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20301;&#32622;&#25554;&#20540;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#26368;&#23567;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23558;RoPE-based&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;&#26368;&#22810;32768&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#33719;&#24471;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#36890;&#36807;&#32447;&#24615;&#38477;&#20302;&#36755;&#20837;&#20301;&#32622;&#32034;&#24341;&#30340;&#22823;&#23567;&#65292;&#25105;&#20204;&#20445;&#25345;&#20102;&#25193;&#23637;&#27169;&#22411;&#22312;&#21407;&#22987;&#19978;&#19979;&#25991;&#31383;&#21475;&#20869;&#20219;&#21153;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.15595</link><description>&lt;p&gt;
&#36890;&#36807;&#20301;&#32622;&#25554;&#20540;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;
&lt;/p&gt;
&lt;p&gt;
Extending Context Window of Large Language Models via Positional Interpolation. (arXiv:2306.15595v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15595
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20301;&#32622;&#25554;&#20540;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#26368;&#23567;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23558;RoPE-based&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;&#26368;&#22810;32768&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#33719;&#24471;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#36890;&#36807;&#32447;&#24615;&#38477;&#20302;&#36755;&#20837;&#20301;&#32622;&#32034;&#24341;&#30340;&#22823;&#23567;&#65292;&#25105;&#20204;&#20445;&#25345;&#20102;&#25193;&#23637;&#27169;&#22411;&#22312;&#21407;&#22987;&#19978;&#19979;&#25991;&#31383;&#21475;&#20869;&#20219;&#21153;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20301;&#32622;&#25554;&#20540;&#65288;PI&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26368;&#23567;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23558;RoPE-based&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;LLaMA&#27169;&#22411;&#65289;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#25193;&#23637;&#21040;&#26368;&#22810;32768&#65292;&#24182;&#19988;&#22312;&#38656;&#35201;&#38271;&#19978;&#19979;&#25991;&#30340;&#21508;&#31181;&#20219;&#21153;&#65288;&#21253;&#25324;&#23494;&#38053;&#26816;&#32034;&#12289;&#35821;&#35328;&#24314;&#27169;&#21644;&#38271;&#31687;&#25991;&#26723;&#25688;&#35201;&#31561;&#65289;&#19978;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20301;&#32622;&#25554;&#20540;&#25193;&#23637;&#30340;&#27169;&#22411;&#22312;&#21407;&#22987;&#19978;&#19979;&#25991;&#31383;&#21475;&#20869;&#30340;&#20219;&#21153;&#20013;&#30456;&#23545;&#20445;&#25345;&#33391;&#22909;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20301;&#32622;&#25554;&#20540;&#32447;&#24615;&#22320;&#38477;&#20302;&#36755;&#20837;&#20301;&#32622;&#32034;&#24341;&#30340;&#22823;&#23567;&#65292;&#20197;&#21305;&#37197;&#21407;&#22987;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#65292;&#32780;&#19981;&#26159;&#36229;&#36807;&#35757;&#32451;&#26102;&#19978;&#19979;&#25991;&#38271;&#24230;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#39640;&#27880;&#24847;&#21147;&#20998;&#25968;&#65292;&#23436;&#20840;&#30772;&#22351;&#33258;&#27880;&#24847;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#30740;&#31350;&#34920;&#26126;&#65292;&#25554;&#20540;&#30340;&#19978;&#30028;&#33267;&#23569;&#26159;&#25512;&#26029;&#30340;&#19978;&#30028;&#30340;$\sim 600 \times$&#35201;&#23567;&#65292;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#20854;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Position Interpolation (PI) that extends the context window sizes of RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal fine-tuning (within 1000 steps), while demonstrating strong empirical results on various tasks that require long context, including passkey retrieval, language modeling, and long document summarization from LLaMA 7B to 65B. Meanwhile, the extended model by Position Interpolation preserve quality relatively well on tasks within its original context window. To achieve this goal, Position Interpolation linearly down-scales the input position indices to match the original context window size, rather than extrapolating beyond the trained context length which may lead to catastrophically high attention scores that completely ruin the self-attention mechanism. Our theoretical study shows that the upper bound of interpolation is at least $\sim 600 \times$ smaller than that of extrapolation, further demonstrating its stability. Models extend
&lt;/p&gt;</description></item><item><title>3D-Speaker&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#35774;&#22791;&#12289;&#22810;&#36317;&#31163;&#21644;&#22810;&#26041;&#35328;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#30740;&#31350;&#35821;&#38899;&#34920;&#31034;&#35299;&#32544;&#12290;&#23427;&#21253;&#21547;&#20102;10,000&#22810;&#20010;&#35828;&#35805;&#20154;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#29992;&#26469;&#35780;&#20272;&#22823;&#22411;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#21644;&#25506;&#32034;&#22495;&#22806;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.15354</link><description>&lt;p&gt;
3D-Speaker&#65306;&#29992;&#20110;&#35821;&#38899;&#34920;&#31034;&#35299;&#32544;&#30340;&#22823;&#35268;&#27169;&#22810;&#35774;&#22791;&#12289;&#22810;&#36317;&#31163;&#21644;&#22810;&#26041;&#35328;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
3D-Speaker: A Large-Scale Multi-Device, Multi-Distance, and Multi-Dialect Corpus for Speech Representation Disentanglement. (arXiv:2306.15354v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15354
&lt;/p&gt;
&lt;p&gt;
3D-Speaker&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#35774;&#22791;&#12289;&#22810;&#36317;&#31163;&#21644;&#22810;&#26041;&#35328;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#30740;&#31350;&#35821;&#38899;&#34920;&#31034;&#35299;&#32544;&#12290;&#23427;&#21253;&#21547;&#20102;10,000&#22810;&#20010;&#35828;&#35805;&#20154;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#29992;&#26469;&#35780;&#20272;&#22823;&#22411;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#21644;&#25506;&#32034;&#22495;&#22806;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#38899;&#31038;&#21306;&#20013;&#65292;&#20998;&#31163;&#35821;&#38899;&#35805;&#35821;&#20013;&#30340;&#19981;&#30456;&#20851;&#20449;&#24687;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#19981;&#21516;&#30340;&#35821;&#38899;&#30456;&#20851;&#20219;&#21153;&#19987;&#27880;&#20110;&#25552;&#21462;&#19981;&#21516;&#30340;&#35821;&#38899;&#34920;&#31034;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20854;&#20182;&#19981;&#30456;&#20851;&#20449;&#24687;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#20197;&#20419;&#36827;&#35821;&#38899;&#34920;&#31034;&#35299;&#32544;&#30340;&#30740;&#31350;&#12290;3D-Speaker&#21253;&#21547;&#36229;&#36807;10,000&#20010;&#35828;&#35805;&#20154;&#65292;&#27599;&#20010;&#35828;&#35805;&#20154;&#21516;&#26102;&#30001;&#22810;&#20010;&#35774;&#22791;&#24405;&#21046;&#65292;&#22312;&#19981;&#21516;&#30340;&#36317;&#31163;&#19978;&#65292;&#24182;&#19988;&#19968;&#20123;&#35828;&#35805;&#20154;&#20250;&#35762;&#22810;&#31181;&#26041;&#35328;&#12290;&#22810;&#32500;&#38899;&#39057;&#25968;&#25454;&#30340;&#21463;&#25511;&#32452;&#21512;&#20135;&#29983;&#20102;&#19968;&#20010;&#22810;&#26679;&#30340;&#28151;&#21512;&#35821;&#38899;&#34920;&#31034;&#32416;&#32544;&#30697;&#38453;&#65292;&#20174;&#32780;&#28608;&#21457;&#20986;&#35299;&#24320;&#23427;&#20204;&#30340;&#26377;&#36259;&#26041;&#27861;&#12290;3D-Speaker&#30340;&#22810;&#39046;&#22495;&#24615;&#36136;&#36824;&#20351;&#20854;&#25104;&#20026;&#35780;&#20272;&#22823;&#22411;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#21644;&#23454;&#39564;&#22495;&#22806;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#21512;&#36866;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disentangling uncorrelated information in speech utterances is a crucial research topic within speech community. Different speech-related tasks focus on extracting distinct speech representations while minimizing the affects of other uncorrelated information. We present a large-scale speech corpus to facilitate the research of speech representation disentanglement. 3D-Speaker contains over 10,000 speakers, each of whom are simultaneously recorded by multiple Devices, locating at different Distances, and some speakers are speaking multiple Dialects. The controlled combinations of multi-dimensional audio data yield a matrix of a diverse blend of speech representation entanglement, thereby motivating intriguing methods to untangle them. The multi-domain nature of 3D-Speaker also makes it a suitable resource to evaluate large universal speech models and experiment methods of out-of-domain learning and self-supervised learning. https://3dspeaker.github.io/
&lt;/p&gt;</description></item><item><title>MindDial&#26159;&#19968;&#20010;&#20351;&#29992;&#24515;&#26234;&#27169;&#25311;&#36827;&#34892;&#20449;&#24565;&#21160;&#24577;&#36319;&#36394;&#30340;&#23545;&#35805;&#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#22330;&#26223;&#21270;&#29615;&#22659;&#20013;&#29983;&#25104;&#33258;&#30001;&#23545;&#35805;&#26469;&#21327;&#21830;&#20849;&#35782;&#12290;</title><link>http://arxiv.org/abs/2306.15253</link><description>&lt;p&gt;
MindDial: &#24102;&#26377;&#24515;&#26234;&#27169;&#25311;&#30340;&#20449;&#24565;&#21160;&#24577;&#36319;&#36394;&#29992;&#20110;&#22330;&#26223;&#21270;&#31070;&#32463;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MindDial: Belief Dynamics Tracking with Theory-of-Mind Modeling for Situated Neural Dialogue Generation. (arXiv:2306.15253v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15253
&lt;/p&gt;
&lt;p&gt;
MindDial&#26159;&#19968;&#20010;&#20351;&#29992;&#24515;&#26234;&#27169;&#25311;&#36827;&#34892;&#20449;&#24565;&#21160;&#24577;&#36319;&#36394;&#30340;&#23545;&#35805;&#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#22330;&#26223;&#21270;&#29615;&#22659;&#20013;&#29983;&#25104;&#33258;&#30001;&#23545;&#35805;&#26469;&#21327;&#21830;&#20849;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#20132;&#27969;&#20013;&#33258;&#30001;&#34920;&#36798;&#24847;&#20041;&#25110;&#20849;&#35782;&#30340;&#21516;&#26102;&#36827;&#34892;&#23545;&#35805;&#12290;&#23613;&#31649;&#22823;&#22411;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#24182;&#26410;&#32771;&#34385;&#21040;&#20849;&#20139;&#30340;&#22330;&#26223;&#29615;&#22659;&#20013;&#20010;&#20307;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#24046;&#24322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MindDial&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#26694;&#26550;&#65292;&#21487;&#20197;&#29983;&#25104;&#22330;&#26223;&#21270;&#30340;&#33258;&#30001;&#23545;&#35805;&#26469;&#21327;&#21830;&#20849;&#35782;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#24515;&#26234;&#27169;&#22359;&#65292;&#21487;&#20197;&#36861;&#36394;&#19977;&#20010;&#23618;&#27425;&#30340;&#20449;&#24565;&#65292;&#21363;&#35828;&#35805;&#32773;&#30340;&#20449;&#24565;&#12289;&#35828;&#35805;&#32773;&#23545;&#21548;&#20247;&#20449;&#24565;&#30340;&#39044;&#27979;&#20197;&#21450;&#22522;&#20110;&#21069;&#20004;&#32773;&#20043;&#38388;&#30340;&#20849;&#21516;&#20449;&#24565;&#12290;&#28982;&#21518;&#65292;&#35828;&#35805;&#34892;&#20026;&#20998;&#31867;&#22836;&#23558;&#20915;&#23450;&#26159;&#21542;&#32487;&#32493;&#23545;&#35805;&#12289;&#32467;&#26463;&#27492;&#36718;&#23545;&#35805;&#25110;&#37319;&#21462;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#34892;&#21160;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#20849;&#35782;&#23545;&#40784;&#30340;&#25968;&#25454;&#38598;MutualFriend&#65292;&#22686;&#21152;&#20102;&#20449;&#24565;&#21160;&#24577;&#27880;&#37322;&#65292;&#30446;&#26631;&#26159;&#26681;&#25454;&#20004;&#20010;&#20195;&#29702;&#20043;&#38388;&#30340;&#33258;&#30001;&#23545;&#35805;&#25214;&#21040;&#19968;&#20010;&#20849;&#21516;&#30340;&#26379;&#21451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#24515;&#26234;&#29366;&#24577;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans talk in free-form while negotiating the expressed meanings or common ground. Despite the impressive conversational abilities of the large generative language models, they do not consider the individual differences in contextual understanding in a shared situated environment. In this work, we propose MindDial, a novel conversational framework that can generate situated free-form responses to negotiate common ground. We design an explicit mind module that can track three-level beliefs -- the speaker's belief, the speaker's prediction of the listener's belief, and the common belief based on the gap between the first two. Then the speaking act classification head will decide to continue to talk, end this turn, or take task-related action. We augment a common ground alignment dataset MutualFriend with belief dynamics annotation, of which the goal is to find a single mutual friend based on the free chat between two agents. Experiments show that our model with mental state modeling can
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#20135;&#21697;&#35780;&#35770;&#29702;&#35299;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#65292;BERT&#27169;&#22411;&#22312;&#19981;&#21516;&#22495;&#19978;&#30340;&#36328;&#22495;&#34892;&#20026;&#12290;&#23613;&#31649;&#21333;&#22495;&#27169;&#22411;&#22312;&#23545;&#24212;&#22495;&#19978;&#30053;&#26377;&#25552;&#39640;&#65292;&#22810;&#22495;&#27169;&#22411;&#22312;&#35780;&#20272;&#22810;&#22495;&#25968;&#25454;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#22312;&#24179;&#22343;&#27979;&#35797;&#20013;&#20063;&#26356;&#20248;&#12290;&#23613;&#31649;&#21333;&#22495;&#27169;&#22411;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#20250;&#22686;&#21152;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2306.15123</link><description>&lt;p&gt;
&#30740;&#31350;BERT&#22312;&#35780;&#35770;&#29702;&#35299;&#20013;&#30340;&#36328;&#22495;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Investigating Cross-Domain Behaviors of BERT in Review Understanding. (arXiv:2306.15123v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15123
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#20135;&#21697;&#35780;&#35770;&#29702;&#35299;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#65292;BERT&#27169;&#22411;&#22312;&#19981;&#21516;&#22495;&#19978;&#30340;&#36328;&#22495;&#34892;&#20026;&#12290;&#23613;&#31649;&#21333;&#22495;&#27169;&#22411;&#22312;&#23545;&#24212;&#22495;&#19978;&#30053;&#26377;&#25552;&#39640;&#65292;&#22810;&#22495;&#27169;&#22411;&#22312;&#35780;&#20272;&#22810;&#22495;&#25968;&#25454;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#22312;&#24179;&#22343;&#27979;&#35797;&#20013;&#20063;&#26356;&#20248;&#12290;&#23613;&#31649;&#21333;&#22495;&#27169;&#22411;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#20250;&#22686;&#21152;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#35770;&#20998;&#25968;&#39044;&#27979;&#38656;&#35201;&#29702;&#35299;&#35780;&#35770;&#25991;&#26412;&#65292;&#36825;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#19968;&#20010;&#20851;&#38190;&#23454;&#38469;&#24212;&#29992;&#12290;&#30001;&#20110;&#20135;&#21697;&#35780;&#35770;&#20013;&#30340;&#25991;&#26412;&#39046;&#22495;&#19981;&#21516;&#65292;&#24120;&#35265;&#20570;&#27861;&#26159;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#35780;&#35770;&#19978;&#23545;BERT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#23545;BERT&#27169;&#22411;&#22312;&#20135;&#21697;&#35780;&#35770;&#29702;&#35299;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#36328;&#22495;&#34892;&#20026;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#12290;&#22312;&#26412;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21333;&#22495;&#21644;&#22810;&#22495;&#20122;&#39532;&#36874;&#35780;&#35770;&#25968;&#25454;&#19978;&#24494;&#35843;&#30340;&#25991;&#26412;&#20998;&#31867;BERT&#27169;&#22411;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#23613;&#31649;&#21333;&#22495;&#27169;&#22411;&#22312;&#23545;&#24212;&#22495;&#19978;&#30340;&#24615;&#33021;&#30053;&#26377;&#25552;&#39640;&#65292;&#20294;&#22312;&#22810;&#22495;&#25968;&#25454;&#19978;&#35780;&#20272;&#26102;&#65292;&#22810;&#22495;&#27169;&#22411;&#20248;&#20110;&#21333;&#22495;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#21333;&#22495;&#27169;&#22411;&#26410;&#36827;&#34892;&#24494;&#35843;&#30340;&#21333;&#22495;&#25968;&#25454;&#19978;&#65292;&#20197;&#21450;&#22312;&#32771;&#34385;&#25152;&#26377;&#27979;&#35797;&#26102;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;&#34429;&#28982;&#36890;&#36807;&#21333;&#22495;&#27169;&#22411;&#24494;&#35843;&#21487;&#20197;&#30053;&#24494;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#35745;&#31639;&#36164;&#28304;&#20063;&#20250;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Review score prediction requires review text understanding, a critical real-world application of natural language processing. Due to dissimilar text domains in product reviews, a common practice is fine-tuning BERT models upon reviews of differing domains. However, there has not yet been an empirical study of cross-domain behaviors of BERT models in the various tasks of product review understanding. In this project, we investigate text classification BERT models fine-tuned on single-domain and multi-domain Amazon review data. In our findings, though single-domain models achieved marginally improved performance on their corresponding domain compared to multi-domain models, multi-domain models outperformed single-domain models when evaluated on multi-domain data, single-domain data the single-domain model was not fine-tuned on, and on average when considering all tests. Though slight increases in accuracy can be achieved through single-domain model fine-tuning, computational resources an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20302;&#36164;&#28304;&#25968;&#25454;&#38598;IEHate&#65292;&#20854;&#20013;&#21253;&#21547;11,457&#26465;&#19982;&#21360;&#24230;&#24030;&#31435;&#36873;&#20030;&#27963;&#21160;&#30456;&#20851;&#30340;&#21360;&#22320;&#35821;&#25512;&#25991;&#12290;&#36890;&#36807;&#23545;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#30740;&#31350;&#32773;&#20204;&#25581;&#31034;&#20102;&#25919;&#27835;&#20132;&#27969;&#20013;&#20167;&#24680;&#35328;&#35770;&#30340;&#26222;&#36941;&#24615;&#21644;&#19981;&#21516;&#24418;&#24335;&#30340;&#24974;&#24680;&#35821;&#35328;&#65292;&#24182;&#20351;&#29992;&#22810;&#31181;&#31639;&#27861;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38656;&#35201;&#26356;&#39640;&#32423;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20167;&#24680;&#35328;&#35770;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.14764</link><description>&lt;p&gt;
&#25581;&#31034;&#21360;&#24230;&#36873;&#20030;&#26399;&#38388;&#30340;&#25919;&#27835;&#20167;&#24680;&#35328;&#35770;&#65306;&#19968;&#20010;&#26032;&#30340;&#20302;&#36164;&#28304;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934; &#65288;arXiv:2306.14764v2 [cs.CL] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
Uncovering Political Hate Speech During Indian Election Campaign: A New Low-Resource Dataset and Baselines. (arXiv:2306.14764v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20302;&#36164;&#28304;&#25968;&#25454;&#38598;IEHate&#65292;&#20854;&#20013;&#21253;&#21547;11,457&#26465;&#19982;&#21360;&#24230;&#24030;&#31435;&#36873;&#20030;&#27963;&#21160;&#30456;&#20851;&#30340;&#21360;&#22320;&#35821;&#25512;&#25991;&#12290;&#36890;&#36807;&#23545;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#30740;&#31350;&#32773;&#20204;&#25581;&#31034;&#20102;&#25919;&#27835;&#20132;&#27969;&#20013;&#20167;&#24680;&#35328;&#35770;&#30340;&#26222;&#36941;&#24615;&#21644;&#19981;&#21516;&#24418;&#24335;&#30340;&#24974;&#24680;&#35821;&#35328;&#65292;&#24182;&#20351;&#29992;&#22810;&#31181;&#31639;&#27861;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38656;&#35201;&#26356;&#39640;&#32423;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20167;&#24680;&#35328;&#35770;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25919;&#27835;&#28436;&#35762;&#20013;&#20167;&#24680;&#35328;&#35770;&#30340;&#26816;&#27979;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#32780;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;IEHate&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;11,457&#26465;&#25163;&#21160;&#26631;&#27880;&#30340;&#19982;2021&#24180;11&#26376;1&#26085;&#33267;2022&#24180;3&#26376;9&#26085;&#26399;&#38388;&#21360;&#24230;&#24030;&#31435;&#36873;&#20030;&#27963;&#21160;&#30456;&#20851;&#30340;&#21360;&#22320;&#35821;&#25512;&#25991;&#12290;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;&#25919;&#27835;&#20132;&#27969;&#20013;&#20167;&#24680;&#35328;&#35770;&#30340;&#26222;&#36941;&#24615;&#21644;&#20351;&#29992;&#30340;&#19981;&#21516;&#24418;&#24335;&#30340;&#24974;&#24680;&#35821;&#35328;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31995;&#21015;&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#22522;&#20110;transformer&#30340;&#31639;&#27861;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#65292;&#24378;&#35843;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#38656;&#35201;&#26356;&#39640;&#32423;&#30340;&#25216;&#26415;&#12290;&#29305;&#21035;&#26159;&#65292;&#20154;&#24037;&#35780;&#20272;&#30456;&#23545;&#36739;&#39640;&#30340;&#24471;&#20998;&#24378;&#35843;&#20102;&#21033;&#29992;&#20154;&#24037;&#21644;&#33258;&#21160;&#21270;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The detection of hate speech in political discourse is a critical issue, and this becomes even more challenging in low-resource languages. To address this issue, we introduce a new dataset named IEHate, which contains 11,457 manually annotated Hindi tweets related to the Indian Assembly Election Campaign from November 1, 2021, to March 9, 2022. We performed a detailed analysis of the dataset, focusing on the prevalence of hate speech in political communication and the different forms of hateful language used. Additionally, we benchmark the dataset using a range of machine learning, deep learning, and transformer-based algorithms. Our experiments reveal that the performance of these models can be further improved, highlighting the need for more advanced techniques for hate speech detection in low-resource languages. In particular, the relatively higher score of human evaluation over algorithms emphasizes the importance of utilizing both human and automated approaches for effective hate 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;2020&#24180;&#21152;&#32435;&#24635;&#32479;&#36873;&#20030;&#26399;&#38388;&#30340;&#25512;&#29305;&#25968;&#25454;&#36827;&#34892;&#31435;&#22330;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#25512;&#29305;&#29992;&#25143;&#23545;&#20004;&#20301;&#20027;&#35201;&#20505;&#36873;&#20154;&#30340;&#35266;&#28857;&#65292;&#24182;&#36890;&#36807;&#35789;&#20856;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#24471;&#21040;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.14203</link><description>&lt;p&gt;
&#25512;&#29305;&#25968;&#25454;&#20013;&#30340;&#31435;&#22330;&#39044;&#27979;&#19982;&#20998;&#26512;&#65306;&#21152;&#32435;2020&#24180;&#24635;&#32479;&#36873;&#20030;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Stance Prediction and Analysis of Twitter data : A case study of Ghana 2020 Presidential Elections. (arXiv:2306.14203v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;2020&#24180;&#21152;&#32435;&#24635;&#32479;&#36873;&#20030;&#26399;&#38388;&#30340;&#25512;&#29305;&#25968;&#25454;&#36827;&#34892;&#31435;&#22330;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#25512;&#29305;&#29992;&#25143;&#23545;&#20004;&#20301;&#20027;&#35201;&#20505;&#36873;&#20154;&#30340;&#35266;&#28857;&#65292;&#24182;&#36890;&#36807;&#35789;&#20856;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#24471;&#21040;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2020&#24180;12&#26376;7&#26085;&#65292;&#21152;&#32435;&#20154;&#21442;&#21152;&#25237;&#31080;&#65292;&#30830;&#23450;&#19979;&#19968;&#20219;&#24635;&#32479;&#12290;&#20026;&#20102;&#20174;&#36825;&#27425;&#24635;&#32479;&#36873;&#20030;&#20013;&#33719;&#21462;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31435;&#22330;&#20998;&#26512;&#65288;&#19981;&#24635;&#26159;&#31561;&#21516;&#20110;&#24773;&#24863;&#20998;&#26512;&#65289;&#65292;&#20197;&#20102;&#35299;&#25512;&#29305;&#36825;&#20010;&#21463;&#27426;&#36814;&#30340;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#22914;&#20309;&#21453;&#26144;&#20854;&#29992;&#25143;&#23545;&#20004;&#20301;&#20027;&#35201;&#24635;&#32479;&#20505;&#36873;&#20154;&#30340;&#24847;&#35265;&#12290;&#25105;&#20204;&#20351;&#29992;&#25512;&#29305;API&#65288;Tweepy&#65289;&#25910;&#38598;&#20102;&#24635;&#20849;99,356&#26465;&#25512;&#25991;&#65292;&#24182;&#23545;&#20854;&#20013;&#30340;3,090&#26465;&#36827;&#34892;&#20102;&#20154;&#24037;&#26631;&#27880;&#65292;&#20998;&#20026;&#19977;&#31867;&#65306;&#21453;&#23545;&#65292;&#20013;&#31435;&#21644;&#25903;&#25345;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;&#36825;&#20123;&#25512;&#25991;&#36827;&#34892;&#20102;&#39044;&#22788;&#29702;&#12290;&#21033;&#29992;&#20004;&#31181;&#22522;&#20110;&#35789;&#20856;&#30340;&#26041;&#27861;&#65288;VADER&#21644;TextBlob&#65289;&#20197;&#21450;&#20116;&#31181;&#22522;&#20110;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65288;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#65292;&#36923;&#36753;&#22238;&#24402;&#65288;LR&#65289;&#65292;&#22810;&#39033;&#24335;&#26420;&#32032;&#36125;&#21494;&#26031;&#65288;MNB&#65289;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21644;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#65289;&#23545;&#32467;&#26524;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21442;&#32771;&#20102;&#20934;&#30830;&#29575;&#65292;&#31934;&#30830;&#29575;&#65292;&#21484;&#22238;&#29575;&#21644;F1&#20998;&#25968;&#31561;&#25351;&#26631;&#12290;&#26368;&#20339;&#24615;&#33021;&#30001;L&#31639;&#27861;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
On December 7, 2020, Ghanaians participated in the polls to determine their president for the next four years. To gain insights from this presidential election, we conducted stance analysis (which is not always equivalent to sentiment analysis) to understand how Twitter, a popular social media platform, reflected the opinions of its users regarding the two main presidential candidates. We collected a total of 99,356 tweets using the Twitter API (Tweepy) and manually annotated 3,090 tweets into three classes: Against, Neutral, and Support. We then performed preprocessing on the tweets. The resulting dataset was evaluated using two lexicon-based approaches, VADER and TextBlob, as well as five supervised machine learning-based approaches: Support Vector Machine (SVM), Logistic Regression (LR), Multinomial Na\"ive Bayes (MNB), Stochastic Gradient Descent (SGD), and Random Forest (RF), based on metrics such as accuracy, precision, recall, and F1-score. The best performance was achieved by L
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#30721;&#21644;&#28151;&#21512;&#22120;&#30456;&#21327;&#20316;&#30340;&#31471;&#21040;&#31471;&#34920;&#29616;&#21147;TTS&#65292;&#23427;&#37319;&#29992;&#26032;&#30340;&#36755;&#20837;&#34920;&#31034;&#21644;&#31616;&#21333;&#30340;&#26550;&#26500;&#26469;&#23454;&#29616;&#25913;&#36827;&#30340;&#38901;&#24459;&#24314;&#27169;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19567</link><description>&lt;p&gt;
DC CoMix TTS&#65306;&#19968;&#31181;&#19982;&#28151;&#21512;&#22120;&#21327;&#20316;&#30340;&#31471;&#21040;&#31471;&#34920;&#29616;&#21147;TTS&#65292;&#21033;&#29992;&#31163;&#25955;&#30721;&#23454;&#29616;&#25913;&#36827;&#30340;&#38901;&#24459;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
DC CoMix TTS: An End-to-End Expressive TTS with Discrete Code Collaborated with Mixer. (arXiv:2305.19567v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#30721;&#21644;&#28151;&#21512;&#22120;&#30456;&#21327;&#20316;&#30340;&#31471;&#21040;&#31471;&#34920;&#29616;&#21147;TTS&#65292;&#23427;&#37319;&#29992;&#26032;&#30340;&#36755;&#20837;&#34920;&#31034;&#21644;&#31616;&#21333;&#30340;&#26550;&#26500;&#26469;&#23454;&#29616;&#25913;&#36827;&#30340;&#38901;&#24459;&#24314;&#27169;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20013;&#24615;TTS&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#20869;&#23481;&#27844;&#28431;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36755;&#20837;&#34920;&#31034;&#21644;&#31616;&#21333;&#30340;&#26550;&#26500;&#26469;&#23454;&#29616;&#25913;&#36827;&#30340;&#38901;&#24459;&#24314;&#27169;&#12290;&#21463;&#26368;&#36817;&#22312;TTS&#20013;&#20351;&#29992;&#31163;&#25955;&#30721;&#21462;&#24471;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#31163;&#25955;&#30721;&#24341;&#20837;&#21040;&#21442;&#32771;&#32534;&#30721;&#22120;&#30340;&#36755;&#20837;&#20013;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#38899;&#39057;&#21387;&#32553;&#27169;&#22411;&#20013;&#30340;&#21521;&#37327;&#37327;&#21270;&#22120;&#26469;&#21033;&#29992;&#23427;&#24050;&#32463;&#35757;&#32451;&#36807;&#30340;&#22810;&#26679;&#21270;&#30340;&#22768;&#23398;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#20462;&#25913;&#21518;&#30340;MLP-Mixer&#24212;&#29992;&#21040;&#21442;&#32771;&#32534;&#30721;&#22120;&#20013;&#65292;&#20351;&#24471;&#26550;&#26500;&#26356;&#21152;&#36731;&#30408;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#35757;&#32451;&#38901;&#24459;&#36716;&#31227;TTS&#12290;&#25105;&#20204;&#36890;&#36807;&#20027;&#35266;&#21644;&#23458;&#35266;&#35780;&#20272;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#65292;&#24403;&#31163;&#25955;&#30721;&#20316;&#20026;&#36755;&#20837;&#26102;&#65292;&#21442;&#32771;&#32534;&#30721;&#22120;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#22909;&#30340;&#19982;&#35828;&#35805;&#20154;&#26080;&#20851;&#30340;&#38901;&#24459;&#12290;&#21478;&#22806;&#65292;&#21363;&#20351;&#36755;&#20837;&#21442;&#25968;&#26356;&#23569;&#65292;&#25105;&#20204;&#20063;&#21487;&#20197;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the huge successes made in neutral TTS, content-leakage remains a challenge. In this paper, we propose a new input representation and simple architecture to achieve improved prosody modeling. Inspired by the recent success in the use of discrete code in TTS, we introduce discrete code to the input of the reference encoder. Specifically, we leverage the vector quantizer from the audio compression model to exploit the diverse acoustic information it has already been trained on. In addition, we apply the modified MLP-Mixer to the reference encoder, making the architecture lighter. As a result, we train the prosody transfer TTS in an end-to-end manner. We prove the effectiveness of our method through both subjective and objective evaluations. We demonstrate that the reference encoder learns better speaker-independent prosody when discrete code is utilized as input in the experiments. In addition, we obtain comparable results even when fewer parameters are inputted.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35770;&#23618;&#38754;&#25506;&#31350;&#20102;&#24102;&#26377;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22522;&#26412;&#25968;&#23398;&#21644;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#22238;&#24402;Transformer&#22823;&#23567;&#24658;&#23450;&#21363;&#21487;&#35299;&#20915;&#20219;&#21153;&#65292;&#25581;&#31034;&#20102;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#32972;&#21518;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.15408</link><description>&lt;p&gt;
&#20174;&#29702;&#35770;&#35282;&#24230;&#25581;&#31034;&#8220;&#24605;&#32500;&#38142;&#8221;&#32972;&#21518;&#30340;&#22885;&#31192;
&lt;/p&gt;
&lt;p&gt;
Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective. (arXiv:2305.15408v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#23618;&#38754;&#25506;&#31350;&#20102;&#24102;&#26377;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22522;&#26412;&#25968;&#23398;&#21644;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#22238;&#24402;Transformer&#22823;&#23567;&#24658;&#23450;&#21363;&#21487;&#35299;&#20915;&#20219;&#21153;&#65292;&#25581;&#31034;&#20102;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#32972;&#21518;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;"&#24605;&#32500;&#38142;"&#25552;&#31034;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#25968;&#23398;&#25110;&#25512;&#29702;&#30340;&#22797;&#26434;&#20219;&#21153;&#20013;&#12290;&#23613;&#31649;&#33719;&#24471;&#20102;&#24040;&#22823;&#30340;&#23454;&#35777;&#25104;&#21151;&#65292;&#20294;&#8220;&#24605;&#32500;&#38142;&#8221;&#32972;&#21518;&#30340;&#26426;&#21046;&#20197;&#21450;&#23427;&#22914;&#20309;&#37322;&#25918;LLMs&#30340;&#28508;&#21147;&#20173;&#28982;&#26159;&#31070;&#31192;&#30340;&#12290;&#26412;&#25991;&#39318;&#27425;&#20174;&#29702;&#35770;&#19978;&#22238;&#31572;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#24102;&#26377;&#8220;&#24605;&#32500;&#38142;&#8221;&#22312;&#35299;&#20915;&#22522;&#26412;&#25968;&#23398;&#21644;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#32473;&#20986;&#19968;&#20010;&#19981;&#21487;&#33021;&#30340;&#32467;&#26524;&#65292;&#34920;&#26126;&#20219;&#20309;&#26377;&#38480;&#28145;&#24230;&#30340;Transformer&#37117;&#19981;&#33021;&#30452;&#25509;&#36755;&#20986;&#27491;&#30830;&#30340;&#22522;&#26412;&#31639;&#26415;/&#26041;&#31243;&#20219;&#21153;&#30340;&#31572;&#26696;&#65292;&#38500;&#38750;&#27169;&#22411;&#22823;&#23567;&#38543;&#30528;&#36755;&#20837;&#38271;&#24230;&#30340;&#22686;&#21152;&#21576;&#36229;&#22810;&#39033;&#24335;&#22686;&#38271;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#36890;&#36807;&#26500;&#36896;&#35777;&#26126;&#65292;&#22823;&#23567;&#24658;&#23450;&#30340;&#33258;&#22238;&#24402;Transformer&#36275;&#20197;&#36890;&#36807;&#20351;&#29992;&#24120;&#29992;&#30340;&#25968;&#23398;&#35821;&#35328;&#24418;&#24335;&#29983;&#25104;&#8220;&#24605;&#32500;&#38142;&#8221;&#25512;&#23548;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the capacity of LLMs with CoT in solving fundamental mathematical and decision-making problems. We start by giving an impossibility result showing that any bounded-depth Transformer cannot directly output correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of a constant size suffice to solve both tasks by generating CoT derivations using a commonly-used math language forma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#20934;&#30830;&#24615;&#30340;&#31574;&#30053;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;Polytuplet Loss&#20989;&#25968;&#26469;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#65292;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#26524;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2304.01046</link><description>&lt;p&gt;
&#8220;Polytuplet Loss: &#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#8221;
&lt;/p&gt;
&lt;p&gt;
Polytuplet Loss: A Reverse Approach to Training Reading Comprehension and Logical Reasoning Models. (arXiv:2304.01046v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#20934;&#30830;&#24615;&#30340;&#31574;&#30053;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;Polytuplet Loss&#20989;&#25968;&#26469;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#65292;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#26524;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25972;&#20010;&#23398;&#26657;&#25945;&#32946;&#36807;&#31243;&#20013;&#65292;&#23398;&#29983;&#20204;&#23558;&#21463;&#21040;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#32771;&#39564;&#12290;&#23398;&#29983;&#20204;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#31574;&#30053;&#26469;&#23436;&#25104;&#27492;&#31867;&#32771;&#35797;&#65292;&#20854;&#20013;&#26377;&#20123;&#34987;&#35748;&#20026;&#26159;&#36890;&#24120;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31574;&#30053;&#30340;&#12290;&#36825;&#26679;&#19968;&#31181;&#31574;&#30053;&#28041;&#21450;&#24378;&#35843;&#30456;&#23545;&#20934;&#30830;&#24615;&#32780;&#38750;&#32477;&#23545;&#20934;&#30830;&#24615;&#65292;&#29702;&#35770;&#19978;&#21487;&#20197;&#22312;&#19981;&#23436;&#20840;&#25484;&#25569;&#35299;&#39064;&#25152;&#38656;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#24471;&#20986;&#27491;&#30830;&#31572;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#36825;&#31181;&#31574;&#30053;&#26469;&#35757;&#32451;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#20197;&#35299;&#20915;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;ReClor&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#36923;&#36753;&#25512;&#29702;&#25216;&#33021;&#65292;&#20294;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#31181;&#36890;&#29992;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Polytuplet Loss&#20989;&#25968;&#65292;&#26159;&#19977;&#20803;&#32452;&#25439;&#22833;&#20989;&#25968;&#30340;&#25193;&#23637;&#65292;&#20197;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#32780;&#38750;&#23398;&#20064;&#32477;&#23545;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Throughout schooling, students are tested on reading comprehension and logical reasoning. Students have developed various strategies for completing such exams, some of which are generally thought to outperform others. One such strategy involves emphasizing relative accuracy over absolute accuracy and can theoretically produce the correct answer without full knowledge of the information required to solve the question. This paper examines the effectiveness of applying such a strategy to train transfer learning models to solve reading comprehension and logical reasoning questions. The models were evaluated on the ReClor dataset, a challenging reading comprehension and logical reasoning benchmark. While previous studies targeted logical reasoning skills, we focus on a general training method and model architecture. We propose the polytuplet loss function, an extension of the triplet loss function, to ensure prioritization of learning the relative correctness of answer choices over learning
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#30340;&#25991;&#26412;&#36716;SQL&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#19968;&#31995;&#21015;&#29420;&#29305;&#25361;&#25112;&#65292;&#21253;&#25324;&#29983;&#25104;SQL&#26597;&#35810;&#12289;&#29702;&#35299;&#26102;&#38388;&#34920;&#36798;&#24335;&#20197;&#21450;&#21306;&#20998;&#26377;&#26080;&#31572;&#26696;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.07695</link><description>&lt;p&gt;
EHRSQL&#65306;&#38754;&#21521;&#30005;&#23376;&#30149;&#21382;&#30340;&#23454;&#29992;&#25991;&#26412;&#36716;SQL&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records. (arXiv:2301.07695v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07695
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#30340;&#25991;&#26412;&#36716;SQL&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#19968;&#31995;&#21015;&#29420;&#29305;&#25361;&#25112;&#65292;&#21253;&#25324;&#29983;&#25104;SQL&#26597;&#35810;&#12289;&#29702;&#35299;&#26102;&#38388;&#34920;&#36798;&#24335;&#20197;&#21450;&#21306;&#20998;&#26377;&#26080;&#31572;&#26696;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#30005;&#23376;&#30149;&#21382;&#65288;EHR&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#12290;&#23545;&#35805;&#26159;&#30001;222&#20010;&#21307;&#38498;&#24037;&#20316;&#20154;&#21592;&#21253;&#25324;&#21307;&#29983;&#12289;&#25252;&#22763;&#12289;&#20445;&#38505;&#23457;&#26597;&#21644;&#20581;&#24247;&#26723;&#26696;&#22242;&#38431;&#31561;&#25163;&#26426;&#32780;&#26469;&#12290;&#20026;&#20102;&#26500;&#24314;&#20851;&#20110;&#32467;&#26500;&#21270;EHR&#25968;&#25454;&#30340;QA&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#22312;&#19968;&#25152;&#22823;&#23398;&#21307;&#38498;&#36827;&#34892;&#20102;&#19968;&#27425;&#27665;&#35843;&#24182;&#21046;&#20316;&#20102;&#27169;&#26495;&#35805;&#26415;&#20197;&#21019;&#24314;&#31181;&#23376;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25163;&#21160;&#23558;&#23427;&#20204;&#38142;&#25509;&#21040;&#20004;&#20010;&#24320;&#28304;&#30340;EHR&#25968;&#25454;&#24211;&#65288;MIMIC-III&#21644;eICU&#65289;&#20013;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#20102;&#26469;&#33258;&#27665;&#24847;&#35843;&#26597;&#30340;&#21508;&#31181;&#26102;&#38388;&#34920;&#36798;&#24335;&#21644;&#26410;&#33021;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#29420;&#29305;&#30340;&#25361;&#25112;&#65306;&#27169;&#22411;&#38656;&#35201; 1&#65289;&#29983;&#25104;&#21453;&#26144;&#21307;&#38498;&#20013;&#21508;&#31181;&#38656;&#27714;&#30340;SQL&#26597;&#35810;&#65292;&#21253;&#25324;&#31616;&#21333;&#30340;&#26816;&#32034;&#21644;&#22797;&#26434;&#30340;&#25805;&#20316;&#65292;&#22914;&#35745;&#31639;&#29983;&#23384;&#29575;&#65292;2&#65289;&#29702;&#35299;&#21508;&#31181;&#26102;&#38388;&#34920;&#36798;&#24335;&#20197;&#22238;&#31572;&#19982;&#26102;&#38388;&#25935;&#24863;&#30340;&#21307;&#30103;&#38382;&#39064;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;3&#65289;&#26681;&#25454;&#39044;&#27979;&#21306;&#20998;&#32473;&#23450;&#38382;&#39064;&#26159;&#21487;&#22238;&#31572;&#36824;&#26159;&#19981;&#21487;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new text-to-SQL dataset for electronic health records (EHRs). The utterances were collected from 222 hospital staff, including physicians, nurses, insurance review and health records teams, and more. To construct the QA dataset on structured EHR data, we conducted a poll at a university hospital and templatized the responses to create seed questions. Then, we manually linked them to two open-source EHR databases, MIMIC-III and eICU, and included them with various time expressions and held-out unanswerable questions in the dataset, which were all collected from the poll. Our dataset poses a unique set of challenges: the model needs to 1) generate SQL queries that reflect a wide range of needs in the hospital, including simple retrieval and complex operations such as calculating survival rate, 2) understand various time expressions to answer time-sensitive questions in healthcare, and 3) distinguish whether a given question is answerable or unanswerable based on the predicti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20869;&#33258;&#36866;&#24212;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#20004;&#31181;&#36884;&#24452;&#26469;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#20135;&#29983;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#12290;&#19968;&#31181;&#26159;&#36890;&#36807;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#23558;&#33521;&#25991;&#36164;&#28304;&#32763;&#35793;&#20026;&#30446;&#26631;&#35821;&#35328;&#65292;&#27880;&#37325;&#25968;&#37327;&#65307;&#21478;&#19968;&#31181;&#26159;&#30452;&#25509;&#22522;&#20110;&#39640;&#36136;&#37327;&#12289;&#29421;&#35889;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#26412;&#22320;&#21270;&#12290;&#36825;&#20123;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#36164;&#28304;&#36739;&#23569;&#35821;&#35328;&#22914;&#24847;&#22823;&#21033;&#35821;&#30340;&#39046;&#22495;&#20869;&#36866;&#24212;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.10422</link><description>&lt;p&gt;
&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#20869;&#33258;&#36866;&#24212;&#30340;&#26412;&#22320;&#21270;
&lt;/p&gt;
&lt;p&gt;
Localising In-Domain Adaptation of Transformer-Based Biomedical Language Models. (arXiv:2212.10422v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20869;&#33258;&#36866;&#24212;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#20004;&#31181;&#36884;&#24452;&#26469;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#20135;&#29983;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#12290;&#19968;&#31181;&#26159;&#36890;&#36807;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#23558;&#33521;&#25991;&#36164;&#28304;&#32763;&#35793;&#20026;&#30446;&#26631;&#35821;&#35328;&#65292;&#27880;&#37325;&#25968;&#37327;&#65307;&#21478;&#19968;&#31181;&#26159;&#30452;&#25509;&#22522;&#20110;&#39640;&#36136;&#37327;&#12289;&#29421;&#35889;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#26412;&#22320;&#21270;&#12290;&#36825;&#20123;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#36164;&#28304;&#36739;&#23569;&#35821;&#35328;&#22914;&#24847;&#22823;&#21033;&#35821;&#30340;&#39046;&#22495;&#20869;&#36866;&#24212;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#21307;&#30103;&#26102;&#20195;&#65292;&#21307;&#38498;&#27599;&#22825;&#20135;&#29983;&#30340;&#22823;&#37327;&#25991;&#26412;&#20449;&#24687;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#20294;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;&#36164;&#20135;&#65292;&#21487;&#20197;&#36890;&#36807;&#29305;&#23450;&#20219;&#21153;&#12289;&#31934;&#32454;&#35843;&#25972;&#30340;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#34920;&#31034;&#27169;&#22411;&#26469;&#21033;&#29992;&#65292;&#20174;&#32780;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;&#21644;&#31649;&#29702;&#12290;&#23545;&#20110;&#36825;&#20123;&#19987;&#38376;&#39046;&#22495;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26469;&#33258;&#24191;&#35206;&#30422;&#28857;&#26816;&#30340;&#24494;&#35843;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#39046;&#22495;&#20869;&#36164;&#28304;&#30340;&#39069;&#22806;&#35757;&#32451;&#36718;&#27425;&#19978;&#21487;&#20197;&#33719;&#30410;&#24456;&#22823;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36164;&#28304;&#36890;&#24120;&#23545;&#20110;&#20687;&#24847;&#22823;&#21033;&#36825;&#26679;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#26159;&#19981;&#21487;&#21450;&#30340;&#65292;&#20351;&#24471;&#24403;&#22320;&#21307;&#30103;&#26426;&#26500;&#26080;&#27861;&#36827;&#34892;&#39046;&#22495;&#20869;&#36866;&#24212;&#12290;&#20026;&#20102;&#32553;&#23567;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#35752;&#20102;&#20004;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#26469;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#29983;&#25104;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#24847;&#22823;&#21033;&#35821;&#20026;&#20855;&#20307;&#26696;&#20363;&#65306;&#19968;&#31181;&#22522;&#20110;&#33521;&#25991;&#36164;&#28304;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65292;&#36861;&#27714;&#25968;&#37327;&#32780;&#19981;&#26159;&#36136;&#37327;&#65307;&#21478;&#19968;&#31181;&#22522;&#20110;&#39640;&#36136;&#37327;&#12289;&#29421;&#35889;&#30340;&#35821;&#26009;&#24211;&#30340;&#26041;&#27861;&#65292;&#30452;&#25509;&#36827;&#34892;&#26412;&#22320;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of digital healthcare, the huge volumes of textual information generated every day in hospitals constitute an essential but underused asset that could be exploited with task-specific, fine-tuned biomedical language representation models, improving patient care and management. For such specialized domains, previous research has shown that fine-tuning models stemming from broad-coverage checkpoints can largely benefit additional training rounds over large-scale in-domain resources. However, these resources are often unreachable for less-resourced languages like Italian, preventing local medical institutions to employ in-domain adaptation. In order to reduce this gap, our work investigates two accessible approaches to derive biomedical language models in languages other than English, taking Italian as a concrete use-case: one based on neural machine translation of English resources, favoring quantity over quality; the other based on a high-grade, narrow-scoped corpus natively w
&lt;/p&gt;</description></item><item><title>NarraSum&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25925;&#20107;&#27010;&#25324;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#40723;&#21169;&#25925;&#20107;&#27010;&#25324;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#30446;&#21069;&#30340;&#27010;&#25324;&#27169;&#22411;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#19982;&#20154;&#31867;&#34920;&#29616;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2212.01476</link><description>&lt;p&gt;
NarraSum:&#19968;&#20010;&#29992;&#20110;&#25925;&#20107;&#27010;&#25324;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
NarraSum: A Large-Scale Dataset for Abstractive Narrative Summarization. (arXiv:2212.01476v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01476
&lt;/p&gt;
&lt;p&gt;
NarraSum&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25925;&#20107;&#27010;&#25324;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#40723;&#21169;&#25925;&#20107;&#27010;&#25324;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#30446;&#21069;&#30340;&#27010;&#25324;&#27169;&#22411;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#19982;&#20154;&#31867;&#34920;&#29616;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#20107;&#27010;&#25324;&#26088;&#22312;&#20135;&#29983;&#19968;&#20010;&#31616;&#27905;&#29256;&#26412;&#30340;&#25925;&#20107;&#65292;&#25551;&#36848;&#20854;&#26368;&#26174;&#33879;&#30340;&#20107;&#20214;&#21644;&#35282;&#33394;&#12290;&#27010;&#25324;&#25925;&#20107;&#26159;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#29702;&#35299;&#20107;&#20214;&#22240;&#26524;&#20851;&#31995;&#21644;&#35282;&#33394;&#34892;&#20026;&#12290;&#20026;&#20102;&#40723;&#21169;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NarraSum&#65292;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25925;&#20107;&#27010;&#25324;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#21547;&#20102;12.2&#19975;&#20010;&#25925;&#20107;&#25991;&#26723;&#65292;&#36825;&#20123;&#25991;&#26723;&#26469;&#33258;&#20110;&#30005;&#24433;&#21644;&#30005;&#35270;&#21095;&#30340;&#24773;&#33410;&#25551;&#36848;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#19981;&#21516;&#30340;&#27969;&#27966;&#65292;&#20197;&#21450;&#23427;&#20204;&#30456;&#24212;&#30340;&#25277;&#35937;&#27010;&#25324;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;NarraSum&#19978;&#65292;&#20154;&#31867;&#19982;&#26368;&#20808;&#36827;&#30340;&#27010;&#25324;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#30528;&#24040;&#22823;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#25968;&#25454;&#38598;&#33021;&#20419;&#36827;&#26410;&#26469;&#22312;&#27010;&#25324;&#26041;&#21521;&#30340;&#30740;&#31350;&#65292;&#20197;&#21450;&#26356;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#30740;&#31350;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/zhaochaocs/narrasum&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Narrative summarization aims to produce a distilled version of a narrative to describe its most salient events and characters. Summarizing a narrative is challenging as it requires an understanding of event causality and character behaviors. To encourage research in this direction, we propose NarraSum, a large-scale narrative summarization dataset. It contains 122K narrative documents, which are collected from plot descriptions of movies and TV episodes with diverse genres, and their corresponding abstractive summaries. Experiments show that there is a large performance gap between humans and the state-of-the-art summarization models on NarraSum. We hope that this dataset will promote future research in summarization, as well as broader studies of natural language understanding and generation. The dataset is available at https://github.com/zhaochaocs/narrasum.
&lt;/p&gt;</description></item><item><title>QueryForm&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#38646;&#26679;&#26412;&#34920;&#21333;&#23454;&#20307;&#26597;&#35810;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21452;&#37325;&#25552;&#31034;&#26426;&#21046;&#21644;&#21033;&#29992;&#22823;&#35268;&#27169;&#26597;&#35810;-&#23454;&#20307;&#23545;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#33021;&#22815;&#20174;&#32467;&#26500;&#21270;&#25991;&#26723;&#20013;&#25552;&#21462;&#23454;&#20307;&#20540;&#65292;&#26080;&#38656;&#30446;&#26631;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2211.07730</link><description>&lt;p&gt;
QueryForm: &#19968;&#31181;&#31616;&#21333;&#30340;&#38646;&#26679;&#26412;&#34920;&#21333;&#23454;&#20307;&#26597;&#35810;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
QueryForm: A Simple Zero-shot Form Entity Query Framework. (arXiv:2211.07730v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07730
&lt;/p&gt;
&lt;p&gt;
QueryForm&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#38646;&#26679;&#26412;&#34920;&#21333;&#23454;&#20307;&#26597;&#35810;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21452;&#37325;&#25552;&#31034;&#26426;&#21046;&#21644;&#21033;&#29992;&#22823;&#35268;&#27169;&#26597;&#35810;-&#23454;&#20307;&#23545;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#33021;&#22815;&#20174;&#32467;&#26500;&#21270;&#25991;&#26723;&#20013;&#25552;&#21462;&#23454;&#20307;&#20540;&#65292;&#26080;&#38656;&#30446;&#26631;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#23545;&#20110;&#25991;&#26723;&#29702;&#35299;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#20294;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#22330;&#26223;&#65292;&#26377;&#21161;&#20110;&#20943;&#23569;&#26631;&#27880;&#25991;&#26723;&#23454;&#20307;&#25152;&#38656;&#30340;&#39640;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;&#26694;&#26550;QueryForm&#65292;&#35813;&#26694;&#26550;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#20174;&#31867;&#20284;&#34920;&#21333;&#30340;&#25991;&#26723;&#20013;&#25552;&#21462;&#23454;&#20307;&#20540;&#12290;QueryForm&#21253;&#21547;&#19968;&#20010;&#21452;&#37325;&#25552;&#31034;&#26426;&#21046;&#65292;&#23558;&#25991;&#26723;&#27169;&#24335;&#21644;&#29305;&#23450;&#23454;&#20307;&#31867;&#22411;&#32452;&#21512;&#25104;&#19968;&#20010;&#26597;&#35810;&#65292;&#29992;&#20110;&#25552;&#31034;Transformer&#27169;&#22411;&#25191;&#34892;&#21333;&#20010;&#23454;&#20307;&#25552;&#21462;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#35758;&#21033;&#29992;&#20174;&#31867;&#20284;&#34920;&#21333;&#30340;&#32593;&#39029;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#26597;&#35810;-&#23454;&#20307;&#23545;&#36827;&#34892;QueryForm&#30340;&#39044;&#35757;&#32451;&#65292;&#36825;&#20123;&#32593;&#39029;&#24102;&#26377;&#24369;HTML&#27880;&#37322;&#12290;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#32479;&#19968;&#21040;&#30456;&#21516;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;&#26694;&#26550;&#20013;&#65292;QueryForm&#20351;&#27169;&#22411;&#33021;&#22815;&#20174;&#21253;&#21547;&#21508;&#31181;&#23454;&#20307;&#21644;&#24067;&#23616;&#30340;&#32467;&#26500;&#21270;&#25991;&#26723;&#20013;&#23398;&#20064;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#30446;&#26631;&#25991;&#26723;&#31867;&#22411;&#65292;&#26080;&#38656;&#30446;&#26631;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;QueryForm&#22312;&#24179;&#22343;&#27700;&#24179;&#19978;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot transfer learning for document understanding is a crucial yet under-investigated scenario to help reduce the high cost involved in annotating document entities. We present a novel query-based framework, QueryForm, that extracts entity values from form-like documents in a zero-shot fashion. QueryForm contains a dual prompting mechanism that composes both the document schema and a specific entity type into a query, which is used to prompt a Transformer model to perform a single entity extraction task. Furthermore, we propose to leverage large-scale query-entity pairs generated from form-like webpages with weak HTML annotations to pre-train QueryForm. By unifying pre-training and fine-tuning into the same query-based framework, QueryForm enables models to learn from structured documents containing various entities and layouts, leading to better generalization to target document types without the need for target-specific training data. QueryForm sets new state-of-the-art average 
&lt;/p&gt;</description></item><item><title>&#23618;&#27425;&#28151;&#21512;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#19981;&#24179;&#34913;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#26696;&#20013;&#30340;&#29420;&#29305;&#38382;&#39064;&#65292;&#21253;&#25324;&#23618;&#27425;&#32467;&#26500;&#30340;&#26631;&#31614;&#12289;&#24322;&#26500;&#30340;&#35821;&#20041;&#21644;&#19981;&#24179;&#34913;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2209.13912</link><description>&lt;p&gt;
&#23618;&#27425;&#28151;&#21512;&#22810;&#26631;&#31614;&#20998;&#31867;&#22312;&#19981;&#24179;&#34913;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#26696;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hierarchical MixUp Multi-label Classification with Imbalanced Interdisciplinary Research Proposals. (arXiv:2209.13912v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13912
&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#28151;&#21512;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#19981;&#24179;&#34913;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#26696;&#20013;&#30340;&#29420;&#29305;&#38382;&#39064;&#65292;&#21253;&#25324;&#23618;&#27425;&#32467;&#26500;&#30340;&#26631;&#31614;&#12289;&#24322;&#26500;&#30340;&#35821;&#20041;&#21644;&#19981;&#24179;&#34913;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36164;&#21161;&#26426;&#26500;&#20381;&#36182;&#20110;&#39046;&#22495;&#19987;&#23478;&#21644;&#30740;&#31350;&#25552;&#26696;&#20043;&#38388;&#30340;&#20027;&#39064;&#21305;&#37197;&#26469;&#25351;&#23450;&#25552;&#26696;&#23457;&#38405;&#20154;&#12290;&#38543;&#30528;&#25552;&#26696;&#36234;&#26469;&#36234;&#36328;&#23398;&#31185;&#65292;&#22914;&#20309;&#20934;&#30830;&#22320;&#23545;&#25552;&#26696;&#30340;&#36328;&#23398;&#31185;&#24615;&#36136;&#36827;&#34892;&#24314;&#27169;&#21644;&#20998;&#31867;&#65292;&#24182;&#25214;&#21040;&#20855;&#26377;&#21512;&#36866;&#19987;&#19994;&#30693;&#35782;&#30340;&#19987;&#23478;&#23457;&#38405;&#20154;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#20851;&#38190;&#27493;&#39588;&#26159;&#20934;&#30830;&#22320;&#23545;&#25552;&#26696;&#30340;&#36328;&#23398;&#31185;&#26631;&#31614;&#36827;&#34892;&#24314;&#27169;&#21644;&#20998;&#31867;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21644;&#24212;&#29992;&#30456;&#20851;&#25991;&#29486;&#65292;&#22914;&#25991;&#26412;&#20998;&#31867;&#21644;&#25552;&#26696;&#20998;&#31867;&#65292;&#22312;&#21516;&#26102;&#35299;&#20915;&#30001;&#36328;&#23398;&#31185;&#25552;&#26696;&#25968;&#25454;&#24341;&#20837;&#30340;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#26041;&#38754;&#36824;&#19981;&#36275;&#65306;1&#65289;&#25552;&#26696;&#30340;&#23398;&#31185;&#26631;&#31614;&#20855;&#26377;&#20174;&#31895;&#31890;&#24230;&#21040;&#32454;&#31890;&#24230;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#20363;&#22914;&#20174;&#20449;&#24687;&#31185;&#23398;&#21040;AI&#21040;AI&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;2&#65289;&#21508;&#20010;&#20027;&#35201;&#25991;&#26412;&#37096;&#20998;&#20855;&#26377;&#24322;&#26500;&#30340;&#35821;&#20041;&#65292;&#36215;&#19981;&#21516;&#30340;&#20316;&#29992;&#12290;3&#65289;&#25552;&#26696;&#30340;&#25968;&#37327;&#22312;&#21508;&#20010;&#26631;&#31614;&#20043;&#38388;&#26159;&#19981;&#24179;&#34913;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Funding agencies are largely relied on a topic matching between domain experts and research proposals to assign proposal reviewers. As proposals are increasingly interdisciplinary, it is challenging to profile the interdisciplinary nature of a proposal, and, thereafter, find expert reviewers with an appropriate set of expertise. An essential step in solving this challenge is to accurately model and classify the interdisciplinary labels of a proposal. Existing methodological and application-related literature, such as textual classification and proposal classification, are insufficient in jointly addressing the three key unique issues introduced by interdisciplinary proposal data: 1) the hierarchical structure of discipline labels of a proposal from coarse-grain to fine-grain, e.g., from information science to AI to fundamentals of AI. 2) the heterogeneous semantics of various main textual parts that play different roles in a proposal; 3) the number of proposals is imbalanced between no
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#33410;&#30340;&#26412;&#22320;&#23383;&#33410;&#34701;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#24403;&#21069;NLP&#27169;&#22411;&#20013;&#23376;&#35789;&#26631;&#35760;&#21270;&#26041;&#26696;&#30340;&#21018;&#24615;&#21644;&#23545;&#20854;&#20182;&#35821;&#26009;&#24211;&#36866;&#24212;&#24615;&#24046;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#22312;&#22810;&#35821;&#31181;&#35821;&#26009;&#24211;&#20013;&#36807;&#24230;&#20999;&#20998;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2205.11490</link><description>&lt;p&gt;
&#26412;&#22320;&#23383;&#33410;&#34701;&#21512;&#29992;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Local Byte Fusion for Neural Machine Translation. (arXiv:2205.11490v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#33410;&#30340;&#26412;&#22320;&#23383;&#33410;&#34701;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#24403;&#21069;NLP&#27169;&#22411;&#20013;&#23376;&#35789;&#26631;&#35760;&#21270;&#26041;&#26696;&#30340;&#21018;&#24615;&#21644;&#23545;&#20854;&#20182;&#35821;&#26009;&#24211;&#36866;&#24212;&#24615;&#24046;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#22312;&#22810;&#35821;&#31181;&#35821;&#26009;&#24211;&#20013;&#36807;&#24230;&#20999;&#20998;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;NLP&#27169;&#22411;&#20013;&#65292;&#23376;&#35789;&#26631;&#35760;&#21270;&#26041;&#26696;&#26159;&#20027;&#35201;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#26696;&#21487;&#33021;&#36807;&#20110;&#27515;&#26495;&#65292;&#24182;&#19988;&#22312;&#19968;&#20010;&#35821;&#26009;&#24211;&#19978;&#26500;&#24314;&#30340;&#26631;&#35760;&#22120;&#23545;&#20854;&#20182;&#24179;&#34892;&#35821;&#26009;&#24211;&#30340;&#36866;&#24212;&#24615;&#19981;&#20339;&#12290;&#35266;&#23519;&#21457;&#29616;&#65292;&#22312;&#22810;&#35821;&#31181;&#35821;&#26009;&#24211;&#20013;&#65292;&#23376;&#35789;&#26631;&#35760;&#21270;&#26041;&#26696;&#20250;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#36827;&#34892;&#36807;&#24230;&#20999;&#20998;&#65292;&#20174;&#32780;&#23548;&#33268;&#32763;&#35793;&#24615;&#33021;&#19979;&#38477;&#12290;&#23376;&#35789;&#26631;&#35760;&#21270;&#30340;&#19968;&#20010;&#31616;&#21333;&#26367;&#20195;&#26041;&#27861;&#26159;&#22522;&#20110;&#23383;&#33410;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#32534;&#30721;&#26041;&#26696;&#65288;&#22914;UTF-8&#65289;&#23558;&#36755;&#20837;&#36827;&#34892;&#23383;&#33410;&#24207;&#21015;&#26631;&#35760;&#21270;&#12290;&#23383;&#33410;&#26631;&#35760;&#36890;&#24120;&#22312;&#23376;&#23383;&#31526;&#31890;&#24230;&#19978;&#34920;&#31034;&#36755;&#20837;&#65292;&#21363;&#19968;&#20010;&#23383;&#31526;&#21487;&#20197;&#30001;&#22810;&#20010;&#23383;&#33410;&#26631;&#35760;&#24207;&#21015;&#34920;&#31034;&#12290;&#36825;&#23548;&#33268;&#23383;&#33410;&#24207;&#21015;&#27604;&#23383;&#31526;&#24207;&#21015;&#38271;&#24471;&#22810;&#12290;&#22312;&#36739;&#20302;&#23618;&#20013;&#24378;&#21046;&#25191;&#34892;&#23616;&#37096;&#20449;&#24687;&#30340;&#32858;&#21512;&#21487;&#20197;&#25351;&#23548;&#27169;&#22411;&#26500;&#24314;&#26356;&#39640;&#23618;&#27425;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23383;&#33410;n-gram&#21644;&#21333;&#35789;&#36793;&#30028;&#30340;&#26412;&#22320;&#23383;&#33410;&#34701;&#21512;&#65288;LOBEF&#65289;&#26041;&#27861;&#29992;&#20110;&#22522;&#20110;&#23383;&#33410;&#30340;&#26426;&#22120;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subword tokenization schemes are the dominant technique used in current NLP models. However, such schemes can be rigid and tokenizers built on one corpus do not adapt well to other parallel corpora. It has also been observed that in multilingual corpora, subword tokenization schemes over-segment low-resource languages leading to a drop in translation performance. A simple alternative to subword tokenizers is byte-based methods i.e. tokenization into byte sequences using encoding schemes such as UTF-8. Byte tokens often represent inputs at a sub-character granularity i.e. one character can be represented by a sequence of multiple byte tokens. This results in byte sequences that are significantly longer than character sequences. Enforcing aggregation of local information in the lower layers can guide the model to build higher-level semantic information. We propose a Local Byte Fusion (LOBEF) method for byte-based machine translation -- utilizing byte $n$-gram and word boundaries -- to ag
&lt;/p&gt;</description></item><item><title>EHRKit&#26159;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25991;&#26412;&#30340;Python&#24037;&#20855;&#21253;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#20010;NLP&#20219;&#21153;&#21644;&#22522;&#20110;MIMIC-III&#25968;&#25454;&#30340;&#25509;&#21475;&#65292;&#21487;&#20197;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#25688;&#35201;&#29983;&#25104;&#12289;&#26426;&#22120;&#32763;&#35793;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2204.06604</link><description>&lt;p&gt;
EHRKit&#65306;&#29992;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25991;&#26412;&#30340;Python&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
EHRKit: A Python Natural Language Processing Toolkit for Electronic Health Record Texts. (arXiv:2204.06604v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.06604
&lt;/p&gt;
&lt;p&gt;
EHRKit&#26159;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25991;&#26412;&#30340;Python&#24037;&#20855;&#21253;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#20010;NLP&#20219;&#21153;&#21644;&#22522;&#20110;MIMIC-III&#25968;&#25454;&#30340;&#25509;&#21475;&#65292;&#21487;&#20197;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#25688;&#35201;&#29983;&#25104;&#12289;&#26426;&#22120;&#32763;&#35793;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#26159;&#29616;&#20195;&#21307;&#30103;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24433;&#21709;&#30528;&#21307;&#30103;&#26381;&#21153;&#12289;&#36816;&#33829;&#21644;&#30740;&#31350;&#12290;&#23613;&#31649;EHR&#20013;&#26377;&#32467;&#26500;&#21270;&#20449;&#24687;&#65292;&#20294;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#25104;&#20026;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#36817;&#26399;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#27861;&#30340;&#25104;&#21151;&#23548;&#33268;&#20102;&#22788;&#29702;&#38750;&#32467;&#26500;&#21270;&#20020;&#24202;&#31508;&#35760;&#30340;&#26032;&#26041;&#21521;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#20020;&#24202;&#25991;&#26412;&#30340;Python&#24211;&#65292;EHRKit&#12290;&#35813;&#24211;&#21253;&#21547;&#20004;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;MIMIC-III&#29305;&#23450;&#20989;&#25968;&#21644;&#20219;&#21153;&#29305;&#23450;&#20989;&#25968;&#12290;&#31532;&#19968;&#37096;&#20998;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#29992;&#20110;&#35775;&#38382;MIMIC-III NOTEEVENTS&#25968;&#25454;&#30340;&#25509;&#21475;&#65292;&#21253;&#25324;&#22522;&#26412;&#25628;&#32034;&#12289;&#20449;&#24687;&#26816;&#32034;&#21644;&#20449;&#24687;&#25552;&#21462;&#12290;&#31532;&#20108;&#37096;&#20998;&#38598;&#25104;&#20102;&#35768;&#22810;&#31532;&#19977;&#26041;&#24211;&#65292;&#29992;&#20110;12&#20010;&#31163;&#32447;NLP&#20219;&#21153;&#65292;&#20363;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#25688;&#35201;&#29983;&#25104;&#12289;&#26426;&#22120;&#32763;&#35793;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Electronic Health Record (EHR) is an essential part of the modern medical system and impacts healthcare delivery, operations, and research. Unstructured text is attracting much attention despite structured information in the EHRs and has become an exciting research field. The success of the recent neural Natural Language Processing (NLP) method has led to a new direction for processing unstructured clinical notes. In this work, we create a python library for clinical texts, EHRKit. This library contains two main parts: MIMIC-III-specific functions and tasks specific functions. The first part introduces a list of interfaces for accessing MIMIC-III NOTEEVENTS data, including basic search, information retrieval, and information extraction. The second part integrates many third-party libraries for up to 12 off-shelf NLP tasks such as named entity recognition, summarization, machine translation, etc.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#27169;&#24577;&#22330;&#26223;&#19979;&#30340;&#39640;&#25928;&#34920;&#31034;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#24230;&#37327;&#26041;&#27861;&#26469;&#37327;&#21270;&#27169;&#24577;&#21644;&#20132;&#20114;&#30340;&#24322;&#36136;&#24615;&#65292;&#20197;&#21152;&#36895;&#23545;&#22810;&#26679;&#21270;&#21644;&#23569;&#34987;&#30740;&#31350;&#30340;&#27169;&#24577;&#30340;&#25512;&#24191;&#12290; (arXiv:2203.01311v4 [cs.LG] UPDATED)</title><link>http://arxiv.org/abs/2203.01311</link><description>&lt;p&gt;
&#39640;&#27169;&#24577;&#22810;&#27169;&#24577;Transformer&#65306;&#37327;&#21270;&#27169;&#24577;&#19982;&#20132;&#20114;&#24322;&#36136;&#24615;&#20197;&#36827;&#34892;&#39640;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
High-Modality Multimodal Transformer: Quantifying Modality &amp; Interaction Heterogeneity for High-Modality Representation Learning. (arXiv:2203.01311v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#27169;&#24577;&#22330;&#26223;&#19979;&#30340;&#39640;&#25928;&#34920;&#31034;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#24230;&#37327;&#26041;&#27861;&#26469;&#37327;&#21270;&#27169;&#24577;&#21644;&#20132;&#20114;&#30340;&#24322;&#36136;&#24615;&#65292;&#20197;&#21152;&#36895;&#23545;&#22810;&#26679;&#21270;&#21644;&#23569;&#34987;&#30740;&#31350;&#30340;&#27169;&#24577;&#30340;&#25512;&#24191;&#12290; (arXiv:2203.01311v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#26412;&#36136;&#19978;&#26159;&#22810;&#27169;&#24577;&#30340;&#65292;&#20363;&#22914;&#20154;&#31867;&#29992;&#20110;&#20132;&#27969;&#30340;&#21475;&#35821;&#12289;&#25163;&#21183;&#21644;&#35821;&#29992;&#23398;&#65292;&#20197;&#21450;&#26426;&#22120;&#20154;&#19978;&#30340;&#21147;&#12289;&#26412;&#20307;&#24863;&#21644;&#35270;&#35273;&#20256;&#24863;&#22120;&#12290;&#34429;&#28982;&#22810;&#27169;&#24577;&#23398;&#20064;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20852;&#36259;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#19968;&#23567;&#32452;&#27169;&#24577;&#65292;&#20027;&#35201;&#26159;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#38899;&#39057;&#12290;&#20026;&#20102;&#21152;&#36895;&#21521;&#22810;&#26679;&#21270;&#21644;&#23569;&#34987;&#30740;&#31350;&#30340;&#27169;&#24577;&#25512;&#24191;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#27169;&#24577;&#22330;&#26223;&#19979;&#30340;&#39640;&#25928;&#34920;&#31034;&#23398;&#20064;&#65292;&#28041;&#21450;&#19968;&#20010;&#22823;&#37327;&#30340;&#19981;&#21516;&#27169;&#24577;&#12290;&#30001;&#20110;&#20026;&#27599;&#20010;&#26032;&#27169;&#24577;&#28155;&#21152;&#26032;&#27169;&#22411;&#21464;&#24471;&#20195;&#20215;&#36807;&#39640;&#65292;&#20851;&#38190;&#30340;&#25216;&#26415;&#25361;&#25112;&#26159;&#24322;&#36136;&#24615;&#37327;&#21270;&#65306;&#25105;&#20204;&#22914;&#20309;&#34913;&#37327;&#21738;&#20123;&#27169;&#24577;&#32534;&#30721;&#20102;&#31867;&#20284;&#30340;&#20449;&#24687;&#21644;&#20132;&#20114;&#65292;&#20197;&#20415;&#20801;&#35768;&#19982;&#20808;&#21069;&#30340;&#27169;&#24577;&#20849;&#20139;&#21442;&#25968;&#65311;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#24230;&#37327;&#26041;&#27861;&#26469;&#37327;&#21270;&#24322;&#36136;&#24615;&#65306;(1)&#27169;&#24577;&#24322;&#36136;&#24615;&#30740;&#31350;&#20102;&#20004;&#20010;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world problems are inherently multimodal, from spoken language, gestures, and paralinguistics humans use to communicate, to force, proprioception, and visual sensors on robots. While there has been an explosion of interest in multimodal learning, these methods are focused on a small set of modalities primarily in language, vision, and audio. In order to accelerate generalization towards diverse and understudied modalities, this paper studies efficient representation learning for high-modality scenarios involving a large set of diverse modalities. Since adding new models for every new modality becomes prohibitively expensive, a critical technical challenge is heterogeneity quantification: how can we measure which modalities encode similar information and interactions in order to permit parameter sharing with previous modalities? This paper proposes two new information theoretic metrics for heterogeneity quantification: (1) modality heterogeneity studies how similar 2 modalitie
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32437;&#21521;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30196;&#21574;&#30417;&#27979;&#21644;&#35786;&#26029;&#12290;&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#12289;&#35328;&#35821;&#21644;&#35821;&#29992;&#25351;&#26631;&#65292;&#21487;&#20197;&#21306;&#20998;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#24739;&#32773;&#21644;&#23545;&#29031;&#32452;&#65292;&#20174;&#32780;&#20026;&#30196;&#21574;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2109.01537</link><description>&lt;p&gt;
&#29992;&#20110;&#30196;&#21574;&#30417;&#27979;&#21644;&#35786;&#26029;&#30340;&#32437;&#21521;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Longitudinal Multi-modal Dataset for Dementia Monitoring and Diagnosis. (arXiv:2109.01537v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.01537
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32437;&#21521;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30196;&#21574;&#30417;&#27979;&#21644;&#35786;&#26029;&#12290;&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#12289;&#35328;&#35821;&#21644;&#35821;&#29992;&#25351;&#26631;&#65292;&#21487;&#20197;&#21306;&#20998;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#24739;&#32773;&#21644;&#23545;&#29031;&#32452;&#65292;&#20174;&#32780;&#20026;&#30196;&#21574;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30196;&#21574;&#26159;&#19968;&#31995;&#21015;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#24433;&#21709;&#36234;&#26469;&#36234;&#22810;&#30340;&#20840;&#29699;&#32769;&#40836;&#20154;&#21475;&#30340;&#35760;&#24518;&#21644;&#35748;&#30693;&#33021;&#21147;&#12290;&#33258;&#21160;&#21270;&#20998;&#26512;&#35821;&#35328;&#12289;&#35328;&#35821;&#21644;&#35821;&#29992;&#25351;&#26631;&#20316;&#20026;&#35748;&#30693;&#34928;&#36864;&#30340;&#28508;&#22312;&#25351;&#26631;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32437;&#21521;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22312;&#33258;&#28982;&#29615;&#22659;&#19979;&#25910;&#38598;&#20102;&#36731;&#24230;&#30196;&#21574;&#24739;&#32773;&#21644;&#37197;&#23545;&#30340;&#24180;&#40836;&#21305;&#37197;&#23545;&#29031;&#32452;&#30340;&#25968;&#25454;&#65292;&#26102;&#38388;&#36328;&#24230;&#20026;&#20960;&#20010;&#26376;&#12290;&#22810;&#27169;&#24577;&#25968;&#25454;&#21253;&#25324;&#21475;&#22836;&#20250;&#35805;&#65292;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#34987;&#36716;&#24405;&#65292;&#20197;&#21450;&#36755;&#20837;&#21644;&#20070;&#20889;&#30340;&#24605;&#32771;&#20869;&#23481;&#65292;&#20197;&#21450;&#30456;&#20851;&#30340;&#38750;&#35821;&#35328;&#20449;&#24687;&#65292;&#22914;&#31508;&#30011;&#21644;&#25353;&#38190;&#12290;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;&#35813;&#25968;&#25454;&#38598;&#65292;&#24182;&#30528;&#37325;&#35752;&#35770;&#20102;&#20351;&#29992;&#35821;&#38899;&#27169;&#24577;&#30340;&#20219;&#21153;&#12290;&#21518;&#32773;&#28041;&#21450;&#21033;&#29992;&#25968;&#25454;&#30340;&#32437;&#21521;&#29305;&#24615;&#26469;&#21306;&#20998;&#23545;&#29031;&#32452;&#21644;&#30196;&#21574;&#24739;&#32773;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#20250;&#35805;&#38388;&#35821;&#38899;&#30340;&#21464;&#21270;&#22312;&#19981;&#21516;&#30340;&#20250;&#35805;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dementia is a family of neurogenerative conditions affecting memory and cognition in an increasing number of individuals in our globally aging population. Automated analysis of language, speech and paralinguistic indicators have been gaining popularity as potential indicators of cognitive decline. Here we propose a novel longitudinal multi-modal dataset collected from people with mild dementia and age matched controls over a period of several months in a natural setting. The multi-modal data consists of spoken conversations, a subset of which are transcribed, as well as typed and written thoughts and associated extra-linguistic information such as pen strokes and keystrokes. We describe the dataset in detail and proceed to focus on a task using the speech modality. The latter involves distinguishing controls from people with dementia by exploiting the longitudinal nature of the data. Our experiments showed significant differences in how the speech varied from session to session in the 
&lt;/p&gt;</description></item></channel></rss>