<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#25918;&#23556;&#23398;&#39046;&#22495;&#30340;&#26368;&#20339;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Radiology-Llama2&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#33410;&#36807;&#31243;&#36827;&#34892;&#29305;&#23450;&#35757;&#32451;&#65292;&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#19988;&#20020;&#24202;&#19978;&#26377;&#29992;&#30340;&#25918;&#23556;&#23398;&#32467;&#26524;&#12290;&#36890;&#36807;ROUGE&#25351;&#26631;&#23450;&#37327;&#35780;&#20272;&#21644;&#19987;&#23478;&#35780;&#20272;&#65292;&#35777;&#26126;Radiology-Llama2&#22312;&#21487;&#29702;&#35299;&#24615;&#12289;&#36830;&#36143;&#24615;&#12289;&#30456;&#20851;&#24615;&#12289;&#31616;&#27905;&#24615;&#21644;&#20020;&#24202;&#25928;&#29992;&#26041;&#38754;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#26412;&#22320;&#21270;&#35821;&#35328;&#27169;&#22411;&#22312;&#25918;&#23556;&#23398;&#31561;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.06419</link><description>&lt;p&gt;
Radiology-Llama2: &#29992;&#20110;&#25918;&#23556;&#23398;&#39046;&#22495;&#30340;&#26368;&#20339;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Radiology-Llama2: Best-in-Class Large Language Model for Radiology. (arXiv:2309.06419v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25918;&#23556;&#23398;&#39046;&#22495;&#30340;&#26368;&#20339;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Radiology-Llama2&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#33410;&#36807;&#31243;&#36827;&#34892;&#29305;&#23450;&#35757;&#32451;&#65292;&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#19988;&#20020;&#24202;&#19978;&#26377;&#29992;&#30340;&#25918;&#23556;&#23398;&#32467;&#26524;&#12290;&#36890;&#36807;ROUGE&#25351;&#26631;&#23450;&#37327;&#35780;&#20272;&#21644;&#19987;&#23478;&#35780;&#20272;&#65292;&#35777;&#26126;Radiology-Llama2&#22312;&#21487;&#29702;&#35299;&#24615;&#12289;&#36830;&#36143;&#24615;&#12289;&#30456;&#20851;&#24615;&#12289;&#31616;&#27905;&#24615;&#21644;&#20020;&#24202;&#25928;&#29992;&#26041;&#38754;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#26412;&#22320;&#21270;&#35821;&#35328;&#27169;&#22411;&#22312;&#25918;&#23556;&#23398;&#31561;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Radiology-Llama2&#65292;&#19968;&#20010;&#36890;&#36807;&#25351;&#20196;&#35843;&#33410;&#36807;&#31243;&#19987;&#38376;&#38024;&#23545;&#25918;&#23556;&#23398;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;Radiology-Llama2&#22522;&#20110;Llama2&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#22823;&#37327;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#25968;&#25454;&#38598;&#36827;&#34892;&#36827;&#19968;&#27493;&#35757;&#32451;&#65292;&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#19988;&#20020;&#24202;&#19978;&#26377;&#29992;&#30340;&#25918;&#23556;&#23398;&#32467;&#26524;&#12290;&#22312;MIMIC-CXR&#21644;OpenI&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;ROUGE&#25351;&#26631;&#36827;&#34892;&#23450;&#37327;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;Radiology-Llama2&#30456;&#27604;&#20854;&#20182;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;MIMIC-CXR&#19978;&#30340;Rouge-1&#24471;&#20998;&#20026;0.4834&#65292;OpenI&#19978;&#30340;&#24471;&#20998;&#20026;0.4185&#12290;&#25918;&#23556;&#23398;&#19987;&#23478;&#30340;&#38468;&#21152;&#35780;&#20272;&#24378;&#35843;&#20102;&#35813;&#27169;&#22411;&#22312;&#21487;&#29702;&#35299;&#24615;&#12289;&#36830;&#36143;&#24615;&#12289;&#30456;&#20851;&#24615;&#12289;&#31616;&#27905;&#24615;&#21644;&#20020;&#24202;&#25928;&#29992;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19987;&#38376;&#20026;&#25918;&#23556;&#23398;&#31561;&#29305;&#23450;&#39046;&#22495;&#35774;&#35745;&#21644;&#35843;&#33410;&#30340;&#26412;&#22320;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#32463;&#36807;&#36866;&#24403;&#30340;&#35780;&#20272;&#21644;&#37096;&#32626;&#65292;&#36825;&#26679;&#30340;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#33258;&#21160;&#21270;&#26426;&#26800;&#20219;&#21153;&#21644;&#22686;&#24378;&#20154;&#26426;&#21327;&#21516;&#26469;&#25913;&#21464;&#25918;&#23556;&#23398;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Radiology-Llama2, a large language model specialized for radiology through a process known as instruction tuning. Radiology-Llama2 is based on the Llama2 architecture and further trained on a large dataset of radiology reports to generate coherent and clinically useful impressions from radiological findings. Quantitative evaluations using ROUGE metrics on the MIMIC-CXR and OpenI datasets demonstrate that Radiology-Llama2 achieves state-of-the-art performance compared to other generative language models, with a Rouge-1 score of 0.4834 on MIMIC-CXR and 0.4185 on OpenI. Additional assessments by radiology experts highlight the model's strengths in understandability, coherence, relevance, conciseness, and clinical utility. The work illustrates the potential of localized language models designed and tuned for specialized domains like radiology. When properly evaluated and deployed, such models can transform fields like radiology by automating rote tasks and enhancing h
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#27602;&#24615;&#20820;&#23376;&#27934;&#26694;&#26550;&#23545;PaLM 2&#30340;&#23433;&#20840;&#21453;&#39304;&#36827;&#34892;&#20102;&#31283;&#20581;&#24615;&#23457;&#35745;&#65292;&#25581;&#31034;&#20102;PaLM 2&#29983;&#25104;&#30340;&#39640;&#24230;&#20196;&#20154;&#19981;&#23433;&#30340;&#27602;&#24615;&#20869;&#23481;&#26410;&#34987;&#23433;&#20840;&#23432;&#25252;&#26639;&#35780;&#20272;&#20026;&#39640;&#24230;&#19981;&#23433;&#20840;&#12290;</title><link>http://arxiv.org/abs/2309.06415</link><description>&lt;p&gt;
&#28145;&#20837;&#27602;&#24615;&#20820;&#23376;&#27934;&#65306;&#36890;&#36807;PaLM 2&#30340;&#23432;&#25252;&#26639;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Down the Toxicity Rabbit Hole: Investigating PaLM 2 Guardrails. (arXiv:2309.06415v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06415
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#27602;&#24615;&#20820;&#23376;&#27934;&#26694;&#26550;&#23545;PaLM 2&#30340;&#23433;&#20840;&#21453;&#39304;&#36827;&#34892;&#20102;&#31283;&#20581;&#24615;&#23457;&#35745;&#65292;&#25581;&#31034;&#20102;PaLM 2&#29983;&#25104;&#30340;&#39640;&#24230;&#20196;&#20154;&#19981;&#23433;&#30340;&#27602;&#24615;&#20869;&#23481;&#26410;&#34987;&#23433;&#20840;&#23432;&#25252;&#26639;&#35780;&#20272;&#20026;&#39640;&#24230;&#19981;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#21517;&#20026;&#8220;&#27602;&#24615;&#20820;&#23376;&#27934;&#8221;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23545;PaLM 2&#30340;&#23433;&#20840;&#21453;&#39304;&#36827;&#34892;&#20102;&#24378;&#21270;&#31283;&#20581;&#24615;&#23457;&#35745;&#12290;&#20174;&#19968;&#20010;&#21051;&#26495;&#21360;&#35937;&#24320;&#22987;&#65292;&#35813;&#26694;&#26550;&#25351;&#31034;PaLM 2&#29983;&#25104;&#27604;&#21051;&#26495;&#21360;&#35937;&#26356;&#20855;&#26377;&#27602;&#24615;&#30340;&#20869;&#23481;&#12290;&#27599;&#19968;&#27425;&#36845;&#20195;&#65292;&#23427;&#37117;&#35201;&#27714;PaLM 2&#29983;&#25104;&#27604;&#19978;&#19968;&#27425;&#36845;&#20195;&#26356;&#20855;&#26377;&#27602;&#24615;&#30340;&#20869;&#23481;&#65292;&#30452;&#21040;PaLM 2&#30340;&#23433;&#20840;&#23432;&#25252;&#26639;&#21457;&#20986;&#23433;&#20840;&#36829;&#35268;&#35686;&#25253;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#26497;&#20854;&#20196;&#20154;&#19981;&#23433;&#30340;&#21453;&#29369;&#22826;&#20027;&#20041;&#12289;&#20234;&#26031;&#20848;&#24656;&#24807;&#30151;&#12289;&#31181;&#26063;&#20027;&#20041;&#12289;&#24656;&#21516;&#21644;&#21388;&#22899;&#24773;&#32490;&#65288;&#20165;&#21015;&#20030;&#20960;&#31181;&#65289;&#30340;&#29983;&#25104;&#20869;&#23481;&#65292;&#24182;&#19988;&#36825;&#20123;&#20869;&#23481;&#22312;PaLM 2&#30340;&#23433;&#20840;&#23432;&#25252;&#26639;&#35780;&#20272;&#20013;&#24182;&#26410;&#34987;&#35270;&#20026;&#39640;&#24230;&#19981;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper conducts a robustness audit of the safety feedback of PaLM 2 through a novel toxicity rabbit hole framework introduced here. Starting with a stereotype, the framework instructs PaLM 2 to generate more toxic content than the stereotype. Every subsequent iteration it continues instructing PaLM 2 to generate more toxic content than the previous iteration until PaLM 2 safety guardrails throw a safety violation. Our experiments uncover highly disturbing antisemitic, Islamophobic, racist, homophobic, and misogynistic (to list a few) generated content that PaLM 2 safety guardrails do not evaluate as highly unsafe.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#35780;&#35770;&#27169;&#22411;&#21644;&#24341;&#20837;&#21453;&#39304;&#23398;&#20064;&#24490;&#29615;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#31995;&#32479;&#20013;&#24341;&#29992;&#38169;&#35823;&#12289;&#29983;&#25104;&#34394;&#26500;&#20449;&#24687;&#21644;&#32570;&#23569;&#20851;&#38190;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.06384</link><description>&lt;p&gt;
&#26397;&#30528;&#21487;&#38752;&#27969;&#21033;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36808;&#36827;&#65306;&#22312;&#38382;&#31572;&#31995;&#32479;&#20013;&#24341;&#20837;&#21453;&#39304;&#23398;&#20064;&#24490;&#29615;
&lt;/p&gt;
&lt;p&gt;
Towards Reliable and Fluent Large Language Models: Incorporating Feedback Learning Loops in QA Systems. (arXiv:2309.06384v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#35780;&#35770;&#27169;&#22411;&#21644;&#24341;&#20837;&#21453;&#39304;&#23398;&#20064;&#24490;&#29615;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#31995;&#32479;&#20013;&#24341;&#29992;&#38169;&#35823;&#12289;&#29983;&#25104;&#34394;&#26500;&#20449;&#24687;&#21644;&#32570;&#23569;&#20851;&#38190;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#21508;&#31181;&#26085;&#24120;&#24212;&#29992;&#20013;&#22810;&#21151;&#33021;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#36825;&#20123;&#38382;&#39064;&#21253;&#25324;&#24341;&#29992;&#38169;&#35823;&#65288;&#24341;&#25991;&#65289;&#12289;&#29983;&#25104;&#34394;&#26500;&#20449;&#24687;&#65288;&#27491;&#30830;&#24615;&#65289;&#20197;&#21450;&#21253;&#21547;&#22810;&#20313;&#25110;&#36951;&#28431;&#20851;&#38190;&#32454;&#33410;&#65288;&#27969;&#30021;&#24615;&#65289;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#20570;&#20986;&#20102;&#20960;&#20010;&#37325;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#35780;&#35770;&#27169;&#22411;&#65292;&#35780;&#20272;LLMs&#22312;&#38382;&#31572;&#31995;&#32479;&#20013;&#29983;&#25104;&#30340;&#22238;&#31572;&#30340;&#24341;&#25991;&#12289;&#27491;&#30830;&#24615;&#21644;&#27969;&#30021;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21453;&#39304;&#26426;&#21046;&#65292;&#21033;&#29992;&#35780;&#35770;&#27169;&#22411;&#23545;&#29983;&#25104;&#25991;&#26412;&#30340;&#24322;&#26500;&#26041;&#38754;&#36827;&#34892;&#23454;&#26102;&#21453;&#39304;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21453;&#39304;&#23398;&#20064;&#24490;&#29615;&#65292;&#20351;&#29992;&#35780;&#35770;&#27169;&#22411;&#26469;&#36845;&#20195;&#25913;&#36827;&#36127;&#36131;&#22238;&#31572;&#29983;&#25104;&#30340;LLM&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have emerged as versatile tools in various daily applications. However, they are fraught with issues that undermine their utility and trustworthiness. These include the incorporation of erroneous references (citation), the generation of hallucinated information (correctness), and the inclusion of superfluous or omission of crucial details (fluency). To ameliorate these concerns, this study makes several key contributions. First, we build a dataset to train a critic model capable of evaluating the citation, correctness, and fluency of responses generated by LLMs in QA systems. Second, we propose an automated feedback mechanism that leverages the critic model to offer real-time feedback on heterogeneous aspects of generated text. Third, we introduce a feedback learning loop that uses this critic model to iteratively improve the performance of the LLM responsible for response generation. Experimental results demonstrate the efficacy of our approach, showing su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24357;&#21512;&#24341;&#29992;&#21644;&#24341;&#25991;&#25991;&#26412;&#20043;&#38388;&#36317;&#31163;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24341;&#25991;&#25991;&#26412;&#36328;&#24230;(CTS)&#26367;&#20195;&#25688;&#35201;&#20316;&#20026;&#36755;&#20837;&#65292;&#20174;&#32780;&#20351;&#24471;&#24341;&#25991;&#29983;&#25104;&#26356;&#21152;&#20934;&#30830;&#21644;&#30456;&#20851;&#12290;&#36890;&#36807;&#33258;&#21160;&#26631;&#27880;&#21644;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;CTS&#26631;&#27880;&#65292;&#25552;&#39640;&#24341;&#25991;&#25991;&#26412;&#29983;&#25104;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.06365</link><description>&lt;p&gt;
&#24341;&#25991;&#25991;&#26412;&#36328;&#24230;&#29992;&#20110;&#24341;&#25991;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Cited Text Spans for Citation Text Generation. (arXiv:2309.06365v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24357;&#21512;&#24341;&#29992;&#21644;&#24341;&#25991;&#25991;&#26412;&#20043;&#38388;&#36317;&#31163;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24341;&#25991;&#25991;&#26412;&#36328;&#24230;(CTS)&#26367;&#20195;&#25688;&#35201;&#20316;&#20026;&#36755;&#20837;&#65292;&#20174;&#32780;&#20351;&#24471;&#24341;&#25991;&#29983;&#25104;&#26356;&#21152;&#20934;&#30830;&#21644;&#30456;&#20851;&#12290;&#36890;&#36807;&#33258;&#21160;&#26631;&#27880;&#21644;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;CTS&#26631;&#27880;&#65292;&#25552;&#39640;&#24341;&#25991;&#25991;&#26412;&#29983;&#25104;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36991;&#20813;&#38750;&#20107;&#23454;&#24615;&#24187;&#35273;&#65292;&#33258;&#21160;&#30456;&#20851;&#24037;&#20316;&#29983;&#25104;&#24517;&#39035;&#23558;&#20854;&#36755;&#20986;&#19982;&#24341;&#25991;&#20013;&#30340;&#20869;&#23481;&#30456;&#20851;&#32852;&#65292;&#20294;&#30001;&#20110;&#31185;&#23398;&#25991;&#26723;&#30340;&#38271;&#24230;&#65292;&#29616;&#26377;&#30340;&#27010;&#25324;&#24615;&#26041;&#27861;&#21482;&#26377;&#22312;&#24341;&#25991;&#25688;&#35201;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#12290;&#25105;&#20204;&#35777;&#26126;&#25688;&#35201;&#24182;&#19981;&#24635;&#26159;&#24341;&#25991;&#29983;&#25104;&#30340;&#26368;&#20339;&#36755;&#20837;&#65292;&#20197;&#21450;&#20197;&#36825;&#31181;&#26041;&#24335;&#35757;&#32451;&#30340;&#27169;&#22411;&#20250;&#20986;&#29616;&#24187;&#35273;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24341;&#25991;&#25991;&#26412;&#36328;&#24230;(CTS)&#20316;&#20026;&#25688;&#35201;&#30340;&#26367;&#20195;&#26465;&#20214;&#12290;&#30001;&#20110;&#25163;&#21160;CTS&#27880;&#37322;&#38750;&#24120;&#32791;&#26102;&#19988;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#65292;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#22522;&#20110;ROUGE&#30340;&#33258;&#21160;&#26631;&#27880;&#20505;&#36873;CTS&#21477;&#23376;&#65292;&#24182;&#21462;&#24471;&#36275;&#22815;&#24378;&#30340;&#24615;&#33021;&#20197;&#26367;&#20195;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;CTS&#26816;&#32034;&#26041;&#27861;&#65292;&#20351;&#24471;&#20197;&#24341;&#25991;&#20840;&#25991;&#20026;&#22522;&#30784;&#29983;&#25104;&#24341;&#25991;&#25991;&#26412;&#21464;&#24471;&#26377;&#21069;&#26223;&#21644;&#23454;&#38469;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic related work generation must ground their outputs to the content of the cited papers to avoid non-factual hallucinations, but due to the length of scientific documents, existing abstractive approaches have conditioned only on the cited paper \textit{abstracts}. We demonstrate that the abstract is not always the most appropriate input for citation generation and that models trained in this way learn to hallucinate. We propose to condition instead on the \textit{cited text span} (CTS) as an alternative to the abstract. Because manual CTS annotation is extremely time- and labor-intensive, we experiment with automatic, ROUGE-based labeling of candidate CTS sentences, achieving sufficiently strong performance to substitute for expensive human annotations, and we propose a human-in-the-loop, keyword-based CTS retrieval approach that makes generating citation texts grounded in the full text of cited papers both promising and practical.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23450;&#24615;&#20998;&#26512;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#33258;&#30001;&#22238;&#31572;&#65292;&#37325;&#28857;&#32771;&#23519;&#20102;&#31639;&#27861;&#20445;&#30495;&#24230;&#65292;&#24182;&#25552;&#20986;&#39640;&#31639;&#27861;&#20445;&#30495;&#24230;&#21487;&#20197;&#25512;&#24191;&#21040;&#30495;&#23454;&#20154;&#31867;&#30340;&#35266;&#28857;&#12290;&#36825;&#23545;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20154;&#31867;&#34892;&#20026;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.06364</link><description>&lt;p&gt;
&#22522;&#20110;&#26694;&#26550;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#30001;&#22238;&#31572;&#30340;&#23450;&#24615;&#20998;&#26512;&#65306;&#31639;&#27861;&#20445;&#30495;&#24230;
&lt;/p&gt;
&lt;p&gt;
Framework-Based Qualitative Analysis of Free Responses of Large Language Models: Algorithmic Fidelity. (arXiv:2309.06364v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23450;&#24615;&#20998;&#26512;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#33258;&#30001;&#22238;&#31572;&#65292;&#37325;&#28857;&#32771;&#23519;&#20102;&#31639;&#27861;&#20445;&#30495;&#24230;&#65292;&#24182;&#25552;&#20986;&#39640;&#31639;&#27861;&#20445;&#30495;&#24230;&#21487;&#20197;&#25512;&#24191;&#21040;&#30495;&#23454;&#20154;&#31867;&#30340;&#35266;&#28857;&#12290;&#36825;&#23545;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20154;&#31867;&#34892;&#20026;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21487;&#20197;&#27169;&#25311;&#33258;&#30001;&#22238;&#31572;&#38754;&#35797;&#38382;&#39064;&#65292;&#23601;&#20687;&#20256;&#32479;&#19978;&#20351;&#29992;&#23450;&#24615;&#30740;&#31350;&#26041;&#27861;&#20998;&#26512;&#30340;&#37027;&#26679;&#12290;&#23450;&#24615;&#26041;&#27861;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#28041;&#21450;&#23545;&#24320;&#25918;&#24335;&#35775;&#35848;&#25110;&#33258;&#30001;&#36827;&#34892;&#30340;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#30340;&#25163;&#21160;&#20998;&#26512;&#12290;&#26412;&#25991;&#32771;&#34385;&#36890;&#36807;&#23450;&#24615;&#26041;&#27861;&#23545;LLMs&#29983;&#25104;&#30340;"&#30789;&#21442;&#19982;&#32773;"&#36827;&#34892;&#30740;&#31350;&#65292;&#20174;&#32780;&#20135;&#29983;&#21487;&#33021;&#21487;&#20197;&#25512;&#24191;&#21040;&#30495;&#23454;&#20154;&#32676;&#30340;&#27934;&#23519;&#21147;&#12290;&#25105;&#20204;&#20998;&#26512;&#30340;&#20851;&#38190;&#27010;&#24565;&#26159;&#31639;&#27861;&#20445;&#30495;&#24230;&#65292;&#36825;&#26159;&#30001;Argyle&#31561;&#20154;&#65288;2023&#24180;&#65289;&#24341;&#20837;&#30340;&#19968;&#20010;&#26415;&#35821;&#65292;&#29992;&#20110;&#25551;&#36848;LLM&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#20154;&#31867;&#20122;&#32676;&#20307;&#30340;&#20449;&#24565;&#21644;&#24577;&#24230;&#30340;&#31243;&#24230;&#30456;&#21563;&#21512;&#12290;&#26681;&#25454;&#23450;&#20041;&#65292;&#39640;&#31639;&#27861;&#20445;&#30495;&#24230;&#34920;&#26126;&#20174;LLMs&#20013;&#25552;&#21462;&#30340;&#28508;&#22312;&#20449;&#24565;&#21487;&#33021;&#21487;&#20197;&#25512;&#24191;&#21040;&#30495;&#23454;&#20154;&#31867;&#65292;&#32780;&#20302;&#31639;&#27861;&#20445;&#30495;&#24230;&#21017;&#20351;&#24471;&#36825;&#26679;&#30340;&#30740;&#31350;&#26080;&#25928;&#12290;&#26412;&#25991;&#20351;&#29992;LLM&#29983;&#25104;&#38754;&#35797;&#38382;&#31572;&#65292;...
&lt;/p&gt;
&lt;p&gt;
Today, using Large-scale generative Language Models (LLMs) it is possible to simulate free responses to interview questions like those traditionally analyzed using qualitative research methods. Qualitative methodology encompasses a broad family of techniques involving manual analysis of open-ended interviews or conversations conducted freely in natural language. Here we consider whether artificial "silicon participants" generated by LLMs may be productively studied using qualitative methods aiming to produce insights that could generalize to real human populations. The key concept in our analysis is algorithmic fidelity, a term introduced by Argyle et al. (2023) capturing the degree to which LLM-generated outputs mirror human sub-populations' beliefs and attitudes. By definition, high algorithmic fidelity suggests latent beliefs elicited from LLMs may generalize to real humans, whereas low algorithmic fidelity renders such research invalid. Here we used an LLM to generate interviews wi
&lt;/p&gt;</description></item><item><title>&#20043;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21521;&#24120;&#35782;&#29983;&#25104;&#22120;&#23637;&#31034;&#27010;&#24565;&#30340;&#39034;&#24207;&#23545;&#29983;&#25104;&#21477;&#23376;&#30340;&#36136;&#37327;&#36215;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#32771;&#34385;&#22810;&#31181;&#35821;&#35328;&#27169;&#22411;&#21644;&#27010;&#24565;&#25490;&#24207;&#31574;&#30053;&#65292;&#21457;&#29616;BART-large&#27169;&#22411;&#22312;&#20351;&#29992;CommonGen&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#26102;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2309.06363</link><description>&lt;p&gt;
&#23398;&#20064;&#39044;&#27979;&#24120;&#35782;&#29983;&#25104;&#30340;&#27010;&#24565;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Learning to Predict Concept Ordering for Common Sense Generation. (arXiv:2309.06363v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06363
&lt;/p&gt;
&lt;p&gt;
&#20043;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21521;&#24120;&#35782;&#29983;&#25104;&#22120;&#23637;&#31034;&#27010;&#24565;&#30340;&#39034;&#24207;&#23545;&#29983;&#25104;&#21477;&#23376;&#30340;&#36136;&#37327;&#36215;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#32771;&#34385;&#22810;&#31181;&#35821;&#35328;&#27169;&#22411;&#21644;&#27010;&#24565;&#25490;&#24207;&#31574;&#30053;&#65292;&#21457;&#29616;BART-large&#27169;&#22411;&#22312;&#20351;&#29992;CommonGen&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#26102;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21521;&#24120;&#35782;&#29983;&#25104;&#22120;&#23637;&#31034;&#27010;&#24565;&#30340;&#39034;&#24207;&#23545;&#29983;&#25104;&#30340;&#21477;&#23376;&#36136;&#37327;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#32473;&#23450;&#19968;&#32452;&#27010;&#24565;&#30340;&#26368;&#20339;&#39034;&#24207;&#20197;&#20415;&#20174;&#39044;&#35757;&#32451;&#29983;&#25104;&#22120;&#29983;&#25104;&#21253;&#21547;&#25152;&#26377;&#27010;&#24565;&#30340;&#33258;&#28982;&#21477;&#23376;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#20102;&#35299;&#36755;&#20837;&#27010;&#24565;&#30340;&#25490;&#24207;&#19982;&#29983;&#25104;&#21477;&#23376;&#36136;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31995;&#32479;&#30740;&#31350;&#65292;&#32771;&#34385;&#20102;&#22810;&#31181;&#35821;&#35328;&#27169;&#22411;&#21644;&#27010;&#24565;&#25490;&#24207;&#31574;&#30053;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20351;&#29992;CommonGen&#35757;&#32451;&#25968;&#25454;&#20013;&#27010;&#24565;&#30340;&#20986;&#29616;&#39034;&#24207;&#20316;&#20026;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;BART-large&#27169;&#22411;&#22987;&#32456;&#20248;&#20110;&#26412;&#30740;&#31350;&#20013;&#32771;&#34385;&#30340;&#25152;&#26377;&#20854;&#20182;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#22312;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#36739;&#22823;&#30340;&#22522;&#20110;GPT3&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21464;&#20307;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#24182;&#19981;&#19968;&#23450;&#33021;&#32988;&#36807;&#26356;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior work has shown that the ordering in which concepts are shown to a commonsense generator plays an important role, affecting the quality of the generated sentence. However, it remains a challenge to determine the optimal ordering of a given set of concepts such that a natural sentence covering all the concepts could be generated from a pretrained generator. To understand the relationship between the ordering of the input concepts and the quality of the generated sentences, we conduct a systematic study considering multiple language models (LMs) and concept ordering strategies. We find that BART-large model consistently outperforms all other LMs considered in this study when fine-tuned using the ordering of concepts as they appear in CommonGen training data as measured using multiple evaluation metrics. Moreover, the larger GPT3-based large language models (LLMs) variants do not necessarily outperform much smaller LMs on this task, even when fine-tuned on task-specific training data
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#22914;&#20309;&#25552;&#39640;&#38382;&#31572;&#27169;&#22411;&#22312;&#33258;&#28982;&#20998;&#24067;&#36716;&#25442;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20102;&#22686;&#24378;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.06358</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#36827;&#34892;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#25552;&#39640;&#38382;&#31572;&#20013;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generative Data Augmentation using LLMs improves Distributional Robustness in Question Answering. (arXiv:2309.06358v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#22914;&#20309;&#25552;&#39640;&#38382;&#31572;&#27169;&#22411;&#22312;&#33258;&#28982;&#20998;&#24067;&#36716;&#25442;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20102;&#22686;&#24378;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#33258;&#28982;&#20998;&#24067;&#36716;&#25442;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#38382;&#31572;&#29615;&#22659;&#20013;&#65292;&#23545;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#30340;&#30740;&#31350;&#24037;&#20316;&#20173;&#22312;&#19981;&#26029;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#33258;&#28982;&#20998;&#24067;&#36716;&#25442;&#19979;&#30340;&#22495;&#27867;&#21270;&#27010;&#24565;&#21364;&#21463;&#21040;&#24456;&#23569;&#20851;&#27880;&#65292;&#22240;&#20026;&#30446;&#26631;&#22495;&#26159;&#26410;&#30693;&#30340;&#12290;&#38543;&#30528;&#29983;&#25104;&#27169;&#22411;&#36136;&#37327;&#21644;&#33719;&#21462;&#26041;&#24335;&#30340;&#22823;&#24133;&#25552;&#39640;&#65292;&#25105;&#20204;&#22238;&#31572;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#22914;&#20309;&#24433;&#21709;&#38382;&#31572;&#27169;&#22411;&#22312;&#33258;&#28982;&#20998;&#24067;&#36716;&#25442;&#19979;&#30340;&#24615;&#33021;&#65311;&#25105;&#20204;&#22312;4&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#8220;&#37326;&#22806;&#29983;&#25104;&#8221;&#22914;&#20309;&#24110;&#21161;&#23454;&#29616;&#22495;&#27867;&#21270;&#12290;&#25105;&#20204;&#37319;&#21462;&#20102;&#20004;&#27493;&#29983;&#25104;&#26041;&#27861;&#65292;&#29983;&#25104;&#19978;&#19979;&#25991;&#21644;&#38382;&#31572;&#23545;&#26469;&#22686;&#24378;&#29616;&#26377;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22686;&#24378;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#26469;&#25552;&#21319;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robustness in Natural Language Processing continues to be a pertinent issue, where state of the art models under-perform under naturally shifted distributions. In the context of Question Answering, work on domain adaptation methods continues to be a growing body of research. However, very little attention has been given to the notion of domain generalization under natural distribution shifts, where the target domain is unknown. With drastic improvements in the quality and access to generative models, we answer the question: How do generated datasets influence the performance of QA models under natural distribution shifts? We perform experiments on 4 different datasets under varying amounts of distribution shift, and analyze how "in-the-wild" generation can help achieve domain generalization. We take a two-step generation approach, generating both contexts and QA pairs to augment existing datasets. Through our experiments, we demonstrate how augmenting reading comprehension datasets wit
&lt;/p&gt;</description></item><item><title>&#35768;&#22810;&#30740;&#31350;&#20851;&#27880;&#20110;&#22914;&#20309;&#24341;&#23548;&#21644;&#32467;&#26500;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#20110;&#36755;&#20837;&#38382;&#39064;&#26412;&#36523;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#37325;&#26032;&#38405;&#35835;&#8221;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#36890;&#36807;&#28145;&#20837;&#38405;&#35835;&#36755;&#20837;&#25552;&#31034;&#20013;&#30340;&#38382;&#39064;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#26356;&#28145;&#20837;&#30340;&#27934;&#23519;&#12289;&#26356;&#20934;&#30830;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#26356;&#26377;&#25928;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.06275</link><description>&lt;p&gt;
&#37325;&#26032;&#38405;&#35835;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Re-Reading Improves Reasoning in Language Models. (arXiv:2309.06275v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06275
&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#20851;&#27880;&#20110;&#22914;&#20309;&#24341;&#23548;&#21644;&#32467;&#26500;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#20110;&#36755;&#20837;&#38382;&#39064;&#26412;&#36523;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#37325;&#26032;&#38405;&#35835;&#8221;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#36890;&#36807;&#28145;&#20837;&#38405;&#35835;&#36755;&#20837;&#25552;&#31034;&#20013;&#30340;&#38382;&#39064;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#26356;&#28145;&#20837;&#30340;&#27934;&#23519;&#12289;&#26356;&#20934;&#30830;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#26356;&#26377;&#25928;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24320;&#21457;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#20197;&#24341;&#23548;&#21644;&#32467;&#26500;&#21270;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#20165;&#35299;&#30721;&#30340;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#36890;&#24120;&#22312;&#21333;&#20010;&#21069;&#21521;&#20256;&#36882;&#20013;&#25805;&#20316;&#36755;&#20837;&#38382;&#39064;&#65292;&#21487;&#33021;&#20250;&#24573;&#30053;&#20154;&#31867;&#25512;&#29702;&#20013;&#20016;&#23500;&#30340;&#21069;&#21518;&#20132;&#20114;&#12290;&#23545;&#20110;&#23884;&#20837;&#22312;&#25552;&#31034;&#20013;&#30340;&#36755;&#20837;&#38382;&#39064;&#36825;&#19968;&#20851;&#38190;&#32500;&#24230;&#65292;&#30446;&#21069;&#20851;&#27880;&#36739;&#23569;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#39640;&#25928;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#31216;&#20026;&#8220;&#37325;&#26032;&#38405;&#35835;&#8221;&#12290;&#20174;&#20154;&#31867;&#23398;&#20064;&#21644;&#38382;&#39064;&#35299;&#20915;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#37325;&#26032;&#38405;&#35835;&#24847;&#21619;&#30528;&#37325;&#35775;&#23884;&#22312;&#36755;&#20837;&#25552;&#31034;&#20013;&#30340;&#38382;&#39064;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#35748;&#30693;&#22686;&#24378;&#30340;&#21407;&#21017;&#23436;&#32654;&#22865;&#21512;&#65292;&#20351;LLM&#33021;&#22815;&#28145;&#20837;&#27934;&#23519;&#12289;&#35782;&#21035;&#22797;&#26434;&#30340;&#27169;&#24335;&#12289;&#24314;&#31435; mor
&lt;/p&gt;
&lt;p&gt;
Reasoning presents a significant and challenging issue for Large Language Models (LLMs). The predominant focus of research has revolved around developing diverse prompting strategies to guide and structure the reasoning processes of LLMs. However, these approaches based on decoder-only causal language models often operate the input question in a single forward pass, potentially missing the rich, back-and-forth interactions inherent in human reasoning. Scant attention has been paid to a critical dimension, i.e., the input question itself embedded within the prompts. In response, we introduce a deceptively simple yet highly effective prompting strategy, termed question "re-reading". Drawing inspiration from human learning and problem-solving, re-reading entails revisiting the question information embedded within input prompts. This approach aligns seamlessly with the cognitive principle of reinforcement, enabling LLMs to extract deeper insights, identify intricate patterns, establish mor
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#34920;&#31034;&#21644;&#20998;&#35789;&#26102;&#38388;&#25968;&#25454;&#30340;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#20351;&#29992;&#36731;&#37327;&#32423;&#23884;&#20837;&#23618;&#36827;&#34892;&#25552;&#31034;&#35843;&#25972;&#21644;&#22810;&#27169;&#24577;&#36866;&#37197;&#22120;&#12290;</title><link>http://arxiv.org/abs/2309.06236</link><description>&lt;p&gt;
&#36367;&#20986;&#30340;&#31532;&#19968;&#27493;&#26368;&#22256;&#38590;&#65306;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#34920;&#31034;&#21644;&#20998;&#35789;&#26102;&#38388;&#25968;&#25454;&#30340;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models. (arXiv:2309.06236v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06236
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#34920;&#31034;&#21644;&#20998;&#35789;&#26102;&#38388;&#25968;&#25454;&#30340;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#20351;&#29992;&#36731;&#37327;&#32423;&#23884;&#20837;&#23618;&#36827;&#34892;&#25552;&#31034;&#35843;&#25972;&#21644;&#22810;&#27169;&#24577;&#36866;&#37197;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23548;&#33268;&#20154;&#20204;&#36234;&#26469;&#36234;&#22810;&#22320;&#23558;&#23427;&#20204;&#29992;&#20316;&#20010;&#20154;&#21161;&#25163;&#21644;&#36890;&#29992;&#35745;&#31639;&#24341;&#25806;&#12290;&#28982;&#32780;&#65292;&#23558;&#25968;&#20540;/&#26102;&#38388;&#25968;&#25454;&#36755;&#20837;&#21040;&#36825;&#20123;&#27169;&#22411;&#20013;&#26102;&#65292;&#20250;&#20986;&#29616;&#19968;&#20010;&#26126;&#26174;&#30340;&#38556;&#30861;&#65292;&#27604;&#22914;&#20174;&#21487;&#31359;&#25140;&#35774;&#22791;&#25110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#33719;&#21462;&#30340;&#25968;&#25454;&#12290;LLMs&#22312;&#20854;&#36755;&#20837;&#20013;&#20351;&#29992;&#20998;&#35789;&#22120;&#23558;&#25991;&#26412;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#21333;&#20301;&#12290;&#28982;&#32780;&#65292;&#20998;&#35789;&#22120;&#24182;&#19981;&#35774;&#35745;&#29992;&#20110;&#34920;&#31034;&#25968;&#20540;&#65292;&#24182;&#21487;&#33021;&#38590;&#20197;&#29702;&#35299;&#37325;&#22797;&#27169;&#24335;&#21644;&#19978;&#19979;&#25991;&#65292;&#23558;&#36830;&#32493;&#30340;&#20540;&#35270;&#20026;&#21333;&#29420;&#30340;&#26631;&#35760;&#24182;&#24573;&#30053;&#23427;&#20204;&#30340;&#26102;&#38388;&#20851;&#31995;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#36817;&#20351;&#29992;LLMs&#36827;&#34892;&#20197;&#20154;&#20026;&#20013;&#24515;&#20219;&#21153;&#30340;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#27969;&#34892;&#30340;LLMs&#38169;&#35823;&#22320;&#23545;&#26102;&#38388;&#25968;&#25454;&#36827;&#34892;&#20998;&#35789;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#19968;&#20123;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20363;&#22914;&#20351;&#29992;&#36731;&#37327;&#32423;&#23884;&#20837;&#23618;&#36827;&#34892;&#25552;&#31034;&#35843;&#25972;&#21644;&#22810;&#27169;&#24577;&#36866;&#37197;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable generalization across diverse tasks, leading individuals to increasingly use them as personal assistants and universal computing engines. Nevertheless, a notable obstacle emerges when feeding numerical/temporal data into these models, such as data sourced from wearables or electronic health records. LLMs employ tokenizers in their input that break down text into smaller units. However, tokenizers are not designed to represent numerical values and might struggle to understand repetitive patterns and context, treating consecutive values as separate tokens and disregarding their temporal relationships. Here, we discuss recent works that employ LLMs for human-centric tasks such as in mobile health sensing and present a case study showing that popular LLMs tokenize temporal data incorrectly. To address that, we highlight potential solutions such as prompt tuning with lightweight embedding layers as well as multimodal adapters, that c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21160;&#20316;&#20849;&#29616;&#30340;&#20219;&#21153;&#65292;&#24182;&#21019;&#24314;&#20102;ACE&#25968;&#25454;&#38598;&#20197;&#21450;&#30456;&#24212;&#30340;&#20195;&#30721;&#12290;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#22270;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#19981;&#21516;&#25968;&#25454;&#22495;&#20013;&#30340;&#20154;&#31867;&#21160;&#20316;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.06219</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#38142;&#25509;&#39044;&#27979;&#22312;&#29983;&#27963;&#26041;&#24335;vlog&#20013;&#30340;&#20154;&#31867;&#21160;&#20316;&#20849;&#29616;
&lt;/p&gt;
&lt;p&gt;
Human Action Co-occurrence in Lifestyle Vlogs using Graph Link Prediction. (arXiv:2309.06219v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06219
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21160;&#20316;&#20849;&#29616;&#30340;&#20219;&#21153;&#65292;&#24182;&#21019;&#24314;&#20102;ACE&#25968;&#25454;&#38598;&#20197;&#21450;&#30456;&#24212;&#30340;&#20195;&#30721;&#12290;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#22270;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#19981;&#21516;&#25968;&#25454;&#22495;&#20013;&#30340;&#20154;&#31867;&#21160;&#20316;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21160;&#20316;&#20849;&#29616;&#30340;&#20219;&#21153;&#65292;&#21363;&#30830;&#23450;&#20004;&#20010;&#20154;&#31867;&#21160;&#20316;&#26159;&#21542;&#21487;&#20197;&#22312;&#21516;&#19968;&#26102;&#38388;&#38388;&#38548;&#20869;&#20849;&#29616;&#12290;&#25105;&#20204;&#21019;&#24314;&#24182;&#20844;&#24320;&#20102;ACE&#65288;Action Co-occurrencE&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#32422;12k&#20010;&#20849;&#29616;&#30340;&#35270;&#35273;&#21160;&#20316;&#23545;&#21644;&#23427;&#20204;&#23545;&#24212;&#30340;&#35270;&#39057;&#29255;&#27573;&#32452;&#25104;&#30340;&#22823;&#22411;&#22270;&#24418;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#26469;&#33258;&#21160;&#25512;&#26029;&#20004;&#20010;&#21160;&#20316;&#26159;&#21542;&#20849;&#29616;&#30340;&#22270;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22270;&#24418;&#29305;&#21035;&#36866;&#21512;&#25429;&#25417;&#20154;&#31867;&#21160;&#20316;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#25152;&#23398;&#20064;&#30340;&#22270;&#24418;&#34920;&#31034;&#23545;&#20110;&#25105;&#20204;&#30340;&#20219;&#21153;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#22495;&#20013;&#25429;&#25417;&#21040;&#26032;&#39062;&#32780;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#20171;&#32461;&#30340;ACE&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#21487;&#22312;https://github.com/MichiganNLP/vlog_action_co-occurrence&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the task of automatic human action co-occurrence identification, i.e., determine whether two human actions can co-occur in the same interval of time. We create and make publicly available the ACE (Action Co-occurrencE) dataset, consisting of a large graph of ~12k co-occurring pairs of visual actions and their corresponding video clips. We describe graph link prediction models that leverage visual and textual information to automatically infer if two actions are co-occurring. We show that graphs are particularly well suited to capture relations between human actions, and the learned graph representations are effective for our task and capture novel and relevant information across different data domains. The ACE dataset and the code introduced in this paper are publicly available at https://github.com/MichiganNLP/vlog_action_co-occurrence.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#26032;&#38395;&#25925;&#20107;&#38142;&#30340;&#32858;&#31867;&#65292;&#25913;&#36827;&#21644;&#35780;&#20272;&#20102;&#26032;&#38395;&#25512;&#33616;&#20013;&#20449;&#24687;&#30862;&#29255;&#21270;&#30340;&#26816;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#34913;&#37327;&#20449;&#24687;&#27969;&#30340;&#23436;&#25972;&#24615;&#21644;&#24433;&#21709;&#27665;&#20027;&#21644;&#20844;&#20849;&#35752;&#35770;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.06192</link><description>&lt;p&gt;
&#25552;&#39640;&#21644;&#35780;&#20272;&#26032;&#38395;&#25512;&#33616;&#20013;&#30340;&#20449;&#24687;&#30862;&#29255;&#26816;&#27979;&#19982;&#26032;&#38395;&#25925;&#20107;&#38142;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Improving and Evaluating the Detection of Fragmentation in News Recommendations with the Clustering of News Story Chains. (arXiv:2309.06192v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06192
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#26032;&#38395;&#25925;&#20107;&#38142;&#30340;&#32858;&#31867;&#65292;&#25913;&#36827;&#21644;&#35780;&#20272;&#20102;&#26032;&#38395;&#25512;&#33616;&#20013;&#20449;&#24687;&#30862;&#29255;&#21270;&#30340;&#26816;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#34913;&#37327;&#20449;&#24687;&#27969;&#30340;&#23436;&#25972;&#24615;&#21644;&#24433;&#21709;&#27665;&#20027;&#21644;&#20844;&#20849;&#35752;&#35770;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#22312;&#22609;&#36896;&#27665;&#20027;&#31038;&#20250;&#20013;&#30340;&#20449;&#24687;&#33719;&#21462;&#26041;&#38754;&#25198;&#28436;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#23558;&#25512;&#33616;&#38024;&#23545;&#29992;&#25143;&#30340;&#20855;&#20307;&#20852;&#36259;&#21487;&#33021;&#23548;&#33268;&#20449;&#24687;&#27969;&#30340;&#20998;&#27495;&#12290;&#20449;&#24687;&#25509;&#35302;&#30340;&#30862;&#29255;&#21270;&#23545;&#20844;&#20849;&#39046;&#22495;&#30340;&#23436;&#25972;&#24615;&#26500;&#25104;&#25361;&#25112;&#65292;&#36827;&#32780;&#24433;&#21709;&#27665;&#20027;&#21644;&#20844;&#20849;&#35752;&#35770;&#12290;&#30862;&#29255;&#21270;&#25351;&#26631;&#37327;&#21270;&#20102;&#26032;&#38395;&#25512;&#33616;&#20013;&#20449;&#24687;&#27969;&#30340;&#30862;&#29255;&#21270;&#31243;&#24230;&#12290;&#20934;&#30830;&#34913;&#37327;&#35813;&#25351;&#26631;&#38656;&#35201;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#20110;&#35782;&#21035;&#19981;&#21516;&#30340;&#26032;&#38395;&#20107;&#20214;&#12289;&#25925;&#20107;&#25110;&#26102;&#38388;&#32447;&#12290;&#26412;&#25991;&#23545;&#22312;&#26032;&#38395;&#25512;&#33616;&#20013;&#37327;&#21270;&#20449;&#24687;&#30862;&#29255;&#21270;&#30340;&#21508;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35843;&#26597;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#26032;&#38395;&#25925;&#20107;&#32858;&#31867;&#30340;&#24615;&#33021;&#24230;&#37327;&#21644;&#19981;&#21516;&#27169;&#25311;&#30340;&#26032;&#38395;&#25512;&#33616;&#22330;&#26223;&#19979;&#30340;&#30862;&#29255;&#21270;&#35780;&#20998;&#35780;&#20272;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
News recommender systems play an increasingly influential role in shaping information access within democratic societies. However, tailoring recommendations to users' specific interests can result in the divergence of information streams. Fragmented access to information poses challenges to the integrity of the public sphere, thereby influencing democracy and public discourse. The Fragmentation metric quantifies the degree of fragmentation of information streams in news recommendations. Accurate measurement of this metric requires the application of Natural Language Processing (NLP) to identify distinct news events, stories, or timelines. This paper presents an extensive investigation of various approaches for quantifying Fragmentation in news recommendations. These approaches are evaluated both intrinsically, by measuring performance on news story clustering, and extrinsically, by assessing the Fragmentation scores of different simulated news recommender scenarios. Our findings demons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#20013;&#36890;&#36807;&#35838;&#31243;&#23398;&#20064;&#30340;&#36880;&#27493;&#20943;&#23569;&#21487;&#29992;&#28304;&#20449;&#24687;&#65292;&#20174;&#25972;&#20010;&#21477;&#23376;&#21040;&#19982;&#24310;&#36831;&#23545;&#24212;&#30340;&#21069;&#32512;&#65292;&#20197;&#23454;&#29616;&#20174;&#24207;&#21015;&#21040;&#24207;&#21015;&#35757;&#32451;&#21040;&#21069;&#32512;&#21040;&#21069;&#32512;&#35757;&#32451;&#30340;&#36807;&#28193;&#65292;&#20174;&#32780;&#22686;&#24378;SiMT&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.06179</link><description>&lt;p&gt;
&#22312;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#20013;&#23637;&#26395;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
Glancing Future for Simultaneous Machine Translation. (arXiv:2309.06179v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#20013;&#36890;&#36807;&#35838;&#31243;&#23398;&#20064;&#30340;&#36880;&#27493;&#20943;&#23569;&#21487;&#29992;&#28304;&#20449;&#24687;&#65292;&#20174;&#25972;&#20010;&#21477;&#23376;&#21040;&#19982;&#24310;&#36831;&#23545;&#24212;&#30340;&#21069;&#32512;&#65292;&#20197;&#23454;&#29616;&#20174;&#24207;&#21015;&#21040;&#24207;&#21015;&#35757;&#32451;&#21040;&#21069;&#32512;&#21040;&#21069;&#32512;&#35757;&#32451;&#30340;&#36807;&#28193;&#65292;&#20174;&#32780;&#22686;&#24378;SiMT&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#65288;SiMT&#65289;&#22312;&#38405;&#35835;&#28304;&#35821;&#21477;&#30340;&#21516;&#26102;&#36755;&#20986;&#32763;&#35793;&#12290;&#19982;&#20256;&#32479;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#65288;seq2seq&#65289;&#35757;&#32451;&#19981;&#21516;&#65292;&#29616;&#26377;&#30340;SiMT&#26041;&#27861;&#37319;&#29992;&#21069;&#32512;&#21040;&#21069;&#32512;&#65288;prefix2prefix&#65289;&#35757;&#32451;&#65292;&#21363;&#27169;&#22411;&#22522;&#20110;&#37096;&#20998;&#28304;&#26631;&#35760;&#39044;&#27979;&#30446;&#26631;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#21069;&#32512;&#21040;&#21069;&#32512;&#35757;&#32451;&#38477;&#20302;&#20102;&#27169;&#22411;&#25429;&#25417;&#20840;&#23616;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#30001;&#20110;&#32570;&#20047;&#24517;&#35201;&#30340;&#28304;&#20449;&#24687;&#32780;&#24341;&#20837;&#20102;&#24378;&#21046;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#24357;&#21512;&#21069;&#32512;&#21040;&#21069;&#32512;&#35757;&#32451;&#21644;&#24207;&#21015;&#21040;&#24207;&#21015;&#35757;&#32451;&#20043;&#38388;&#24046;&#36317;&#20197;&#22686;&#24378;SiMT&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#35838;&#31243;&#23398;&#20064;&#20013;&#23637;&#26395;&#26410;&#26469;&#65292;&#23454;&#29616;&#20174;&#24207;&#21015;&#21040;&#24207;&#21015;&#35757;&#32451;&#36807;&#28193;&#21040;&#21069;&#32512;&#21040;&#21069;&#32512;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36880;&#28176;&#20943;&#23569;&#21487;&#29992;&#30340;&#28304;&#20449;&#24687;&#65292;&#20174;&#25972;&#20010;&#21477;&#23376;&#21040;&#19982;&#24310;&#36831;&#23545;&#24212;&#30340;&#21069;&#32512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;SiMT&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneous machine translation (SiMT) outputs translation while reading the source sentence. Unlike conventional sequence-to-sequence (seq2seq) training, existing SiMT methods adopt the prefix-to-prefix (prefix2prefix) training, where the model predicts target tokens based on partial source tokens. However, the prefix2prefix training diminishes the ability of the model to capture global information and introduces forced predictions due to the absence of essential source information. Consequently, it is crucial to bridge the gap between the prefix2prefix training and seq2seq training to enhance the translation capability of the SiMT model. In this paper, we propose a novel method that glances future in curriculum learning to achieve the transition from the seq2seq training to prefix2prefix training. Specifically, we gradually reduce the available source information from the whole sentence to the prefix corresponding to that latency. Our method is applicable to a wide range of SiMT met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38598;&#25104;&#27169;&#22411;&#23558;&#30693;&#35782;&#24211;&#19982;&#26597;&#35810;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#20307;&#35782;&#21035;&#21644;&#38142;&#25509;&#25361;&#25112;&#12290;&#36890;&#36807;&#25193;&#23637;&#30693;&#35782;&#24211;&#21644;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#21484;&#22238;&#29575;&#65292;&#24182;&#20351;&#29992;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#21644;&#22810;&#20803;&#21152;&#24615;&#22238;&#24402;&#26641;&#36807;&#28388;&#32467;&#26524;&#24471;&#21040;&#39640;&#31934;&#24230;&#30340;&#23454;&#20307;&#35782;&#21035;&#21644;&#38142;&#25509;&#12290;&#26368;&#32456;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;0.535&#30340;F1&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.06175</link><description>&lt;p&gt;
AKEM: &#21033;&#29992;&#38598;&#25104;&#27169;&#22411;&#23558;&#30693;&#35782;&#24211;&#19982;&#26597;&#35810;&#23545;&#40784;&#20197;&#36827;&#34892;&#23454;&#20307;&#35782;&#21035;&#21644;&#38142;&#25509;
&lt;/p&gt;
&lt;p&gt;
AKEM: Aligning Knowledge Base to Queries with Ensemble Model for Entity Recognition and Linking. (arXiv:2309.06175v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38598;&#25104;&#27169;&#22411;&#23558;&#30693;&#35782;&#24211;&#19982;&#26597;&#35810;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#20307;&#35782;&#21035;&#21644;&#38142;&#25509;&#25361;&#25112;&#12290;&#36890;&#36807;&#25193;&#23637;&#30693;&#35782;&#24211;&#21644;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#21484;&#22238;&#29575;&#65292;&#24182;&#20351;&#29992;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#21644;&#22810;&#20803;&#21152;&#24615;&#22238;&#24402;&#26641;&#36807;&#28388;&#32467;&#26524;&#24471;&#21040;&#39640;&#31934;&#24230;&#30340;&#23454;&#20307;&#35782;&#21035;&#21644;&#38142;&#25509;&#12290;&#26368;&#32456;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;0.535&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;NLPCC 2015&#20013;&#23454;&#20307;&#35782;&#21035;&#21644;&#38142;&#25509;&#25361;&#25112;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#20219;&#21153;&#21253;&#25324;&#20174;&#30701;&#25628;&#32034;&#26597;&#35810;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#30340;&#25552;&#21450;&#65292;&#24182;&#23558;&#20854;&#38142;&#25509;&#21040;&#21442;&#32771;&#20013;&#25991;&#30693;&#35782;&#24211;&#20013;&#30340;&#23454;&#20307;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25193;&#23637;&#29616;&#26377;&#30693;&#35782;&#24211;&#65292;&#24182;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#35782;&#21035;&#20505;&#36873;&#23454;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#21484;&#22238;&#29575;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20174;&#20505;&#36873;&#23454;&#20307;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#21644;&#22810;&#20803;&#21152;&#24615;&#22238;&#24402;&#26641;&#20316;&#20026;&#35780;&#20998;&#20989;&#25968;&#26469;&#36807;&#28388;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24212;&#29992;&#35268;&#21017;&#26469;&#36827;&#19968;&#27493;&#32454;&#21270;&#32467;&#26524;&#21644;&#25552;&#39640;&#31934;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#36798;&#21040;&#20102;0.535&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach to address the Entity Recognition and Linking Challenge at NLPCC 2015. The task involves extracting named entity mentions from short search queries and linking them to entities within a reference Chinese knowledge base. To tackle this problem, we first expand the existing knowledge base and utilize external knowledge to identify candidate entities, thereby improving the recall rate. Next, we extract features from the candidate entities and utilize Support Vector Regression and Multiple Additive Regression Tree as scoring functions to filter the results. Additionally, we apply rules to further refine the results and enhance precision. Our method is computationally efficient and achieves an F1 score of 0.535.
&lt;/p&gt;</description></item><item><title>&#22312;IberLEF 2023&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29916;&#25289;&#23612;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#20195;&#30721;&#20132;&#26367;&#20998;&#26512;&#30340;GUA-SPA&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#21253;&#25324;&#35782;&#21035;&#35821;&#35328;&#12289;NER&#21644;&#26032;&#39062;&#30340;&#35199;&#29677;&#29273;&#35821;&#29255;&#27573;&#22312;&#20195;&#30721;&#20132;&#26367;&#35821;&#22659;&#20013;&#20351;&#29992;&#26041;&#24335;&#30340;&#20998;&#31867;&#12290;&#22312;&#35780;&#20272;&#38454;&#27573;&#65292;Task 1&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#65292;&#32780;Task 2&#21644;Task 3&#30340;&#32467;&#26524;&#21017;&#19981;&#31283;&#23450;&#12290;</title><link>http://arxiv.org/abs/2309.06163</link><description>&lt;p&gt;
IberLEF 2023&#20013;&#20851;&#20110;&#29916;&#25289;&#23612;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#20195;&#30721;&#20132;&#26367;&#20998;&#26512;&#30340;GUA-SPA&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Overview of GUA-SPA at IberLEF 2023: Guarani-Spanish Code Switching Analysis. (arXiv:2309.06163v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06163
&lt;/p&gt;
&lt;p&gt;
&#22312;IberLEF 2023&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29916;&#25289;&#23612;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#20195;&#30721;&#20132;&#26367;&#20998;&#26512;&#30340;GUA-SPA&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#21253;&#25324;&#35782;&#21035;&#35821;&#35328;&#12289;NER&#21644;&#26032;&#39062;&#30340;&#35199;&#29677;&#29273;&#35821;&#29255;&#27573;&#22312;&#20195;&#30721;&#20132;&#26367;&#35821;&#22659;&#20013;&#20351;&#29992;&#26041;&#24335;&#30340;&#20998;&#31867;&#12290;&#22312;&#35780;&#20272;&#38454;&#27573;&#65292;Task 1&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#65292;&#32780;Task 2&#21644;Task 3&#30340;&#32467;&#26524;&#21017;&#19981;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;IberLEF 2023&#20013;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#21644;&#20998;&#26512;&#29916;&#25289;&#23612;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#20195;&#30721;&#20132;&#26367;&#30340;&#20849;&#20139;&#20219;&#21153;GUA-SPA&#12290;&#35813;&#25361;&#25112;&#21253;&#25324;&#19977;&#20010;&#20219;&#21153;&#65306;&#35782;&#21035;&#19968;&#20010;&#26631;&#35760;&#30340;&#35821;&#35328;&#12289;NER&#21644;&#23545;&#35199;&#29677;&#29273;&#35821;&#29255;&#27573;&#22312;&#20195;&#30721;&#20132;&#26367;&#35821;&#22659;&#20013;&#20351;&#29992;&#26041;&#24335;&#30340;&#26032;&#20219;&#21153;&#12290;&#25105;&#20204;&#26631;&#27880;&#20102;&#19968;&#20010;&#21253;&#21547;1500&#20010;&#26469;&#33258;&#26032;&#38395;&#25991;&#31456;&#21644;&#25512;&#29305;&#30340;&#25991;&#26412;&#30340;&#35821;&#26009;&#24211;&#65292;&#22823;&#32422;&#26377;2.5&#19975;&#20010;&#26631;&#35760;&#65292;&#24182;&#23545;&#20219;&#21153;&#30340;&#20449;&#24687;&#36827;&#34892;&#20102;&#26631;&#35760;&#12290;&#19977;&#20010;&#22242;&#38431;&#21442;&#21152;&#20102;&#35780;&#20272;&#38454;&#27573;&#65292;Task 1&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#65292;Task 2&#21644;3&#30340;&#32467;&#26524;&#21017;&#21442;&#24046;&#19981;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the first shared task for detecting and analyzing code-switching in Guarani and Spanish, GUA-SPA at IberLEF 2023. The challenge consisted of three tasks: identifying the language of a token, NER, and a novel task of classifying the way a Spanish span is used in the code-switched context. We annotated a corpus of 1500 texts extracted from news articles and tweets, around 25 thousand tokens, with the information for the tasks. Three teams took part in the evaluation phase, obtaining in general good results for Task 1, and more mixed results for Tasks 2 and 3.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Prompting4Debugging&#65288;P4D&#65289;&#20316;&#20026;&#19968;&#20010;&#35843;&#35797;&#21644;&#32418;&#38431;&#27979;&#35797;&#24037;&#20855;&#65292;&#21487;&#20197;&#33258;&#21160;&#25214;&#21040;&#25193;&#25955;&#27169;&#22411;&#30340;&#38382;&#39064;&#25552;&#31034;&#65292;&#20197;&#27979;&#35797;&#37096;&#32626;&#30340;&#23433;&#20840;&#26426;&#21046;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06135</link><description>&lt;p&gt;
Prompting4Debugging: &#36890;&#36807;&#21457;&#29616;&#38382;&#39064;&#25552;&#31034;&#26469;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#32418;&#38431;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts. (arXiv:2309.06135v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06135
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Prompting4Debugging&#65288;P4D&#65289;&#20316;&#20026;&#19968;&#20010;&#35843;&#35797;&#21644;&#32418;&#38431;&#27979;&#35797;&#24037;&#20855;&#65292;&#21487;&#20197;&#33258;&#21160;&#25214;&#21040;&#25193;&#25955;&#27169;&#22411;&#30340;&#38382;&#39064;&#25552;&#31034;&#65292;&#20197;&#27979;&#35797;&#37096;&#32626;&#30340;&#23433;&#20840;&#26426;&#21046;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#20363;&#22914;&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#65292;&#26368;&#36817;&#23637;&#29616;&#20986;&#39640;&#36136;&#37327;&#20869;&#23481;&#29983;&#25104;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#24182;&#25104;&#20026;&#36817;&#26399;&#21464;&#38761;&#24615;&#20154;&#24037;&#26234;&#33021;&#28010;&#28526;&#30340;&#20195;&#34920;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36827;&#27493;&#20063;&#24102;&#26469;&#20102;&#23545;&#35813;&#29983;&#25104;&#25216;&#26415;&#28389;&#29992;&#30340;&#26085;&#30410;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#29983;&#25104;&#21463;&#29256;&#26435;&#20445;&#25252;&#25110;&#19981;&#36866;&#21512;&#22312;&#24037;&#20316;&#29615;&#22659;&#20013;&#26597;&#30475;&#30340;&#22270;&#20687;&#12290;&#34429;&#28982;&#24050;&#32463;&#20570;&#20986;&#20102;&#19968;&#20123;&#21162;&#21147;&#26469;&#36890;&#36807;&#27169;&#22411;&#24494;&#35843;&#26469;&#36807;&#28388;&#19981;&#36866;&#24403;&#30340;&#22270;&#20687;/&#25552;&#31034;&#25110;&#21024;&#38500;&#19981;&#24076;&#26395;&#30340;&#27010;&#24565;/&#39118;&#26684;&#65292;&#20294;&#36825;&#20123;&#23433;&#20840;&#26426;&#21046;&#23545;&#20110;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#25552;&#31034;&#30340;&#21487;&#38752;&#24615;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Prompting4Debugging&#65288;P4D&#65289;&#20316;&#20026;&#19968;&#20010;&#35843;&#35797;&#21644;&#32418;&#38431;&#27979;&#35797;&#24037;&#20855;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#25214;&#21040;&#25193;&#25955;&#27169;&#22411;&#30340;&#38382;&#39064;&#25552;&#31034;&#65292;&#20197;&#27979;&#35797;&#37096;&#32626;&#30340;&#23433;&#20840;&#26426;&#21046;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;P4D&#24037;&#20855;&#22312;&#21457;&#29616;&#20855;&#26377;&#23433;&#20840;&#26426;&#21046;&#30340;SD&#27169;&#22411;&#30340;&#26032;&#28431;&#27934;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models, e.g. Stable Diffusion (SD), lately have shown remarkable ability in high-quality content generation, and become one of the representatives for the recent wave of transformative AI. Nevertheless, such advance comes with an intensifying concern about the misuse of this generative technology, especially for producing copyrighted or NSFW (i.e. not safe for work) images. Although efforts have been made to filter inappropriate images/prompts or remove undesirable concepts/styles via model fine-tuning, the reliability of these safety mechanisms against diversified problematic prompts remains largely unexplored. In this work, we propose Prompting4Debugging (P4D) as a debugging and red-teaming tool that automatically finds problematic prompts for diffusion models to test the reliability of a deployed safety mechanism. We demonstrate the efficacy of our P4D tool in uncovering new vulnerabilities of SD models with safety mechanisms. Particularly, our result shows t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#26469;&#33258;&#21160;&#27979;&#37327;&#25991;&#26412;&#20013;&#30340;&#27169;&#31946;&#24615;&#21644;&#20027;&#35266;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#19987;&#23478;&#31995;&#32479;VAGO&#65292;&#20197;&#21450;&#22522;&#20110;BERT-like&#26550;&#26500;&#30340;&#31070;&#32463;&#20811;&#38534;&#65292;&#35813;&#26041;&#27861;&#22312;&#22266;&#23450;&#35821;&#26009;&#24211;&#21644;&#22810;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06132</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#20013;&#27979;&#37327;&#27169;&#31946;&#24615;&#21644;&#20027;&#35266;&#24615;&#65306;&#20174;&#31526;&#21495;&#21040;&#31070;&#32463;&#32593;&#32476;&#30340;VAGO
&lt;/p&gt;
&lt;p&gt;
Measuring vagueness and subjectivity in texts: from symbolic to neural VAGO. (arXiv:2309.06132v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#26469;&#33258;&#21160;&#27979;&#37327;&#25991;&#26412;&#20013;&#30340;&#27169;&#31946;&#24615;&#21644;&#20027;&#35266;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#19987;&#23478;&#31995;&#32479;VAGO&#65292;&#20197;&#21450;&#22522;&#20110;BERT-like&#26550;&#26500;&#30340;&#31070;&#32463;&#20811;&#38534;&#65292;&#35813;&#26041;&#27861;&#22312;&#22266;&#23450;&#35821;&#26009;&#24211;&#21644;&#22810;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#26469;&#33258;&#21160;&#27979;&#37327;&#25991;&#26412;&#20013;&#30340;&#27169;&#31946;&#24615;&#21644;&#20027;&#35266;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19987;&#23478;&#31995;&#32479;VAGO&#65292;&#24182;&#22312;&#19968;&#23567;&#32452;&#20107;&#23454;&#19982;&#35266;&#28857;&#21477;&#23376;&#30340;&#22522;&#20934;&#19978;&#23545;&#20854;&#36827;&#34892;&#20102;&#35828;&#26126;&#65292;&#24182;&#22312;&#26356;&#22823;&#30340;&#27861;&#35821;&#26032;&#38395;&#35821;&#26009;&#24211;FreSaDa&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20197;&#30830;&#35748;&#35773;&#21050;&#24615;&#25991;&#26412;&#20013;&#20027;&#35266;&#26631;&#35760;&#30340;&#26356;&#39640;&#27969;&#34892;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;BERT-like&#26550;&#26500;&#30340;VAGO&#31070;&#32463;&#20811;&#38534;&#65292;&#35813;&#26550;&#26500;&#22522;&#20110;&#22312;FreSaDa&#19978;&#33719;&#24471;&#30340;&#31526;&#21495;VAGO&#20998;&#25968;&#36827;&#34892;&#35757;&#32451;&#12290;&#20351;&#29992;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#65288;LIME&#65289;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#31070;&#32463;&#29256;&#26412;&#22312;&#20016;&#23500;&#31526;&#21495;&#29256;&#26412;&#30340;&#35789;&#20856;&#21644;&#29983;&#25104;&#20854;&#20182;&#35821;&#35328;&#29256;&#26412;&#26041;&#38754;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a hybrid approach to the automated measurement of vagueness and subjectivity in texts. We first introduce the expert system VAGO, we illustrate it on a small benchmark of fact vs. opinion sentences, and then test it on the larger French press corpus FreSaDa to confirm the higher prevalence of subjective markers in satirical vs. regular texts. We then build a neural clone of VAGO, based on a BERT-like architecture, trained on the symbolic VAGO scores obtained on FreSaDa. Using explainability tools (LIME), we show the interest of this neural version for the enrichment of the lexicons of the symbolic version, and for the production of versions in other languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#39044;&#31639;&#19979;&#65292;&#23545;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25490;&#24207;&#22120;&#36827;&#34892;&#24494;&#35843;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35266;&#23519;&#21457;&#29616;&#65292;&#22312;&#19981;&#21516;&#38543;&#26426;&#36873;&#25321;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#26377;&#25928;&#24615;&#23384;&#22312;&#24456;&#22823;&#21464;&#24322;&#24615;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#20027;&#21160;&#36873;&#25321;&#23545;&#25490;&#24207;&#22120;&#25928;&#26524;&#31215;&#26497;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#65292;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#24615;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.06131</link><description>&lt;p&gt;
&#23545;&#31070;&#32463;&#25490;&#24207;&#22120;&#36827;&#34892;&#24494;&#35843;&#30340;&#25968;&#25454;&#26631;&#27880;&#65311;&#24403;&#21069;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#24182;&#19981;&#27604;&#38543;&#26426;&#36873;&#25321;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Annotating Data for Fine-Tuning a Neural Ranker? Current Active Learning Strategies are not Better than Random Selection. (arXiv:2309.06131v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#39044;&#31639;&#19979;&#65292;&#23545;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25490;&#24207;&#22120;&#36827;&#34892;&#24494;&#35843;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35266;&#23519;&#21457;&#29616;&#65292;&#22312;&#19981;&#21516;&#38543;&#26426;&#36873;&#25321;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#26377;&#25928;&#24615;&#23384;&#22312;&#24456;&#22823;&#21464;&#24322;&#24615;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#20027;&#21160;&#36873;&#25321;&#23545;&#25490;&#24207;&#22120;&#25928;&#26524;&#31215;&#26497;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#65292;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#24615;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#25628;&#32034;&#26041;&#27861;&#30456;&#27604;&#32479;&#35745;&#21644;&#26089;&#26399;&#31070;&#32463;&#25490;&#24207;&#27169;&#22411;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#26377;&#25928;&#24615;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#22522;&#20110;PLM&#30340;&#25490;&#24207;&#22120;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#27880;&#35757;&#32451;&#25968;&#25454;&#12290;&#26631;&#27880;&#25968;&#25454;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#21162;&#21147;&#65292;&#22240;&#27492;&#22312;&#20855;&#20307;&#39046;&#22495;&#20219;&#21153;&#20013;&#38750;&#24120;&#26114;&#36149;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#39044;&#31639;&#19979;&#24494;&#35843;&#22522;&#20110;PLM&#30340;&#25490;&#24207;&#22120;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#24773;&#20917;&#65306;&#20174;&#22836;&#24320;&#22987;&#24494;&#35843;&#25490;&#24207;&#22120;&#65292;&#20197;&#21450;&#20174;&#24050;&#32463;&#22312;&#36890;&#29992;&#25968;&#25454;&#19978;&#24494;&#35843;&#30340;&#25490;&#24207;&#22120;&#24320;&#22987;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#65292;&#24182;&#22312;&#30446;&#26631;&#25968;&#25454;&#38598;&#19978;&#32487;&#32493;&#24494;&#35843;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#19981;&#21516;&#38543;&#26426;&#36873;&#25321;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#26377;&#25928;&#24615;&#23384;&#22312;&#24456;&#22823;&#30340;&#21464;&#24322;&#24615;&#12290;&#36825;&#34920;&#26126;&#36890;&#36807;&#20027;&#21160;&#36873;&#25321;&#23545;&#25490;&#24207;&#22120;&#25928;&#26524;&#26368;&#31215;&#26497;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#65292;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#24615;&#25552;&#21319;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#23558;&#21487;&#20197;&#24494;&#35843;&#20986;&#26377;&#25928;&#30340;PLM&#25490;&#24207;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Search methods based on Pretrained Language Models (PLM) have demonstrated great effectiveness gains compared to statistical and early neural ranking models. However, fine-tuning PLM-based rankers requires a great amount of annotated training data. Annotating data involves a large manual effort and thus is expensive, especially in domain specific tasks. In this paper we investigate fine-tuning PLM-based rankers under limited training data and budget. We investigate two scenarios: fine-tuning a ranker from scratch, and domain adaptation starting with a ranker already fine-tuned on general data, and continuing fine-tuning on a target dataset. We observe a great variability in effectiveness when fine-tuning on different randomly selected subsets of training data. This suggests that it is possible to achieve effectiveness gains by actively selecting a subset of the training data that has the most positive effect on the rankers. This way, it would be possible to fine-tune effective PLM rank
&lt;/p&gt;</description></item><item><title>AstroLLaMA&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#22825;&#25991;&#23398;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;arXiv&#20013;&#30340;&#22825;&#25991;&#23398;&#25688;&#35201;fine-tuned&#24471;&#21040;&#65292;&#20854;&#22312;&#22240;&#26524;&#35821;&#35328;&#24314;&#27169;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29983;&#25104;&#30340;&#25991;&#26412;&#23436;&#25104;&#21644;&#23884;&#20837;&#25552;&#21462;&#27604;&#20854;&#20182;&#22522;&#30784;&#27169;&#22411;&#26356;&#20855;&#27934;&#23519;&#21147;&#21644;&#31185;&#23398;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06126</link><description>&lt;p&gt;
AstroLLaMA: &#38754;&#21521;&#22825;&#25991;&#23398;&#30340;&#19987;&#19994;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AstroLLaMA: Towards Specialized Foundation Models in Astronomy. (arXiv:2309.06126v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06126
&lt;/p&gt;
&lt;p&gt;
AstroLLaMA&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#22825;&#25991;&#23398;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;arXiv&#20013;&#30340;&#22825;&#25991;&#23398;&#25688;&#35201;fine-tuned&#24471;&#21040;&#65292;&#20854;&#22312;&#22240;&#26524;&#35821;&#35328;&#24314;&#27169;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29983;&#25104;&#30340;&#25991;&#26412;&#23436;&#25104;&#21644;&#23884;&#20837;&#25552;&#21462;&#27604;&#20854;&#20182;&#22522;&#30784;&#27169;&#22411;&#26356;&#20855;&#27934;&#23519;&#21147;&#21644;&#31185;&#23398;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#20154;&#31867;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#23398;&#26415;&#22825;&#25991;&#23398;&#31561;&#39640;&#24230;&#19987;&#19994;&#21270;&#39046;&#22495;&#24448;&#24448;&#38590;&#20197;&#32988;&#20219;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AstroLLaMA&#65292;&#36825;&#26159;&#19968;&#20010;&#20174;arXiv&#19978;&#30340;&#36229;&#36807;300,000&#20010;&#22825;&#25991;&#23398;&#25688;&#35201;&#20013;&#20351;&#29992;LLaMA-2 fine-tuned&#24471;&#21040;&#30340;70&#20159;&#21442;&#25968;&#27169;&#22411;&#12290;AstroLLaMA&#38024;&#23545;&#20256;&#32479;&#22240;&#26524;&#35821;&#35328;&#24314;&#27169;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#20854;&#22256;&#24785;&#24230;&#27604;Llama-2&#20302;30&#65285;&#65292;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#39046;&#22495;&#36866;&#24212;&#24615;&#12290;&#23613;&#31649;&#21442;&#25968;&#26126;&#26174;&#36739;&#23569;&#65292;&#20294;&#25105;&#20204;&#30340;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#23436;&#25104;&#21644;&#23884;&#20837;&#25552;&#21462;&#27604;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#26356;&#20855;&#27934;&#23519;&#21147;&#21644;&#31185;&#23398;&#30456;&#20851;&#24615;&#12290;AstroLLaMA&#20316;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;fine-tuning&#28508;&#21147;&#12290;&#20854;&#20844;&#24320;&#21457;&#24067;&#26088;&#22312;&#25512;&#21160;&#22260;&#32469;&#22825;&#25991;&#23398;&#30340;&#30740;&#31350;&#65292;&#21253;&#25324;&#33258;&#21160;&#35770;&#25991;&#25688;&#35201;&#21644;&#23545;&#35805;&#20195;&#29702;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models excel in many human-language tasks but often falter in highly specialized domains like scholarly astronomy. To bridge this gap, we introduce AstroLLaMA, a 7-billion-parameter model fine-tuned from LLaMA-2 using over 300,000 astronomy abstracts from arXiv. Optimized for traditional causal language modeling, AstroLLaMA achieves a 30% lower perplexity than Llama-2, showing marked domain adaptation. Our model generates more insightful and scientifically relevant text completions and embedding extraction than state-of-the-arts foundation models despite having significantly fewer parameters. AstroLLaMA serves as a robust, domain-specific model with broad fine-tuning potential. Its public release aims to spur astronomy-focused research, including automatic paper summarization and conversational agent development.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;GPT-2&#20174;&#35821;&#26009;&#24211;&#20013;&#36827;&#34892;&#38750;&#25688;&#35201;&#25110;&#29983;&#25104;&#24615;&#29305;&#24449;&#21270;&#20154;&#29289;&#23454;&#20307;&#30340;&#38646;-shot&#26041;&#27861;&#65292;&#29992;&#20110;&#25581;&#31034;&#23186;&#20307;&#23545;&#20844;&#20247;&#20154;&#29289;&#30340;&#28508;&#22312;&#24577;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.06112</link><description>&lt;p&gt;
&#25581;&#31034;&#23186;&#20307;&#23545;&#20844;&#20247;&#20154;&#29289;&#30340;&#28508;&#22312;&#24577;&#24230;
&lt;/p&gt;
&lt;p&gt;
Characterizing Latent Perspectives of Media Houses Towards Public Figures. (arXiv:2309.06112v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;GPT-2&#20174;&#35821;&#26009;&#24211;&#20013;&#36827;&#34892;&#38750;&#25688;&#35201;&#25110;&#29983;&#25104;&#24615;&#29305;&#24449;&#21270;&#20154;&#29289;&#23454;&#20307;&#30340;&#38646;-shot&#26041;&#27861;&#65292;&#29992;&#20110;&#25581;&#31034;&#23186;&#20307;&#23545;&#20844;&#20247;&#20154;&#29289;&#30340;&#28508;&#22312;&#24577;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23186;&#20307;&#25253;&#36947;&#20844;&#20247;&#20154;&#29289;&#24448;&#24448;&#21463;&#21040;&#21508;&#33258;&#19990;&#30028;&#35266;&#30340;&#20559;&#35265;&#24433;&#21709;&#12290;&#23545;&#36825;&#20123;&#28508;&#22312;&#27169;&#24335;&#30340;&#25551;&#36848;&#26377;&#21161;&#20110;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#35299;&#37322;&#26032;&#38395;&#25925;&#20107;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#22810;&#26679;&#21270;&#25110;&#20027;&#35266;&#30340;&#24635;&#32467;&#65292;&#36825;&#21487;&#33021;&#19981;&#36866;&#21512;&#20998;&#31867;&#20026;&#39044;&#23450;&#20041;&#30340;&#31867;&#21035;&#26631;&#31614;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;GPT-2&#20174;&#35821;&#26009;&#24211;&#20013;&#36827;&#34892;&#38750;&#25688;&#35201;&#25110;&#29983;&#25104;&#24615;&#29305;&#24449;&#21270;&#20154;&#29289;&#23454;&#20307;&#30340;&#38646;-shot&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#26469;&#33258;&#20960;&#23478;&#30693;&#21517;&#26032;&#38395;&#23186;&#20307;&#30340;&#26126;&#30830;&#25991;&#31456;&#20316;&#20026;&#35821;&#26009;&#24211;&#65292;&#20026;&#36825;&#31181;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#26377;&#21147;&#30340;&#35770;&#35777;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#29992;&#29305;&#23450;&#30340;&#20154;&#29289;&#23454;&#20307;&#36827;&#34892;&#20102;GPT-2&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#65292;&#28982;&#21518;&#20877;&#29992;&#20174;&#31243;&#24207;&#26500;&#36896;&#30340;&#29305;&#24449;&#21270;&#35821;&#26009;&#24211;&#21019;&#24314;&#30340;&#20154;&#29289;&#23454;&#20307;&#29305;&#24449;&#21270;&#28436;&#31034;&#36827;&#19968;&#27493;&#24494;&#35843;&#23427;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#29992;&#25163;&#21160;&#25552;&#31034;&#30340;&#23454;&#20307;&#21517;&#31216;&#39044;&#28909;&#20102;&#36825;&#20010;&#32463;&#36807;&#20004;&#27425;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Media houses reporting on public figures, often come with their own biases stemming from their respective worldviews. A characterization of these underlying patterns helps us in better understanding and interpreting news stories. For this, we need diverse or subjective summarizations, which may not be amenable for classifying into predefined class labels. This work proposes a zero-shot approach for non-extractive or generative characterizations of person entities from a corpus using GPT-2. We use well-articulated articles from several well-known news media houses as a corpus to build a sound argument for this approach. First, we fine-tune a GPT-2 pre-trained language model with a corpus where specific person entities are characterized. Second, we further fine-tune this with demonstrations of person entity characterizations, created from a corpus of programmatically constructed characterizations. This twice fine-tuned model is primed with manual prompts consisting of entity names that w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#35273;&#20998;&#31867;&#25193;&#23637;&#65288;VTE&#65289;&#26041;&#27861;&#65292;&#23558;&#35270;&#35273;&#29305;&#24449;&#24341;&#20837;&#21040;&#20998;&#31867;&#25193;&#23637;&#20219;&#21153;&#20013;&#65292;&#24182;&#36890;&#36807;&#25991;&#26412;&#19978;&#20041;&#35789;&#23398;&#20064;&#20219;&#21153;&#21644;&#35270;&#35273;&#21407;&#22411;&#23398;&#20064;&#20219;&#21153;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#35821;&#20041;&#65292;&#20135;&#29983;&#20102;&#32454;&#31890;&#24230;&#30340;&#35270;&#35273;&#35821;&#20041;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20013;&#25991;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24230;&#65292;&#34920;&#29616;&#20248;&#20110;ChatGPT.</title><link>http://arxiv.org/abs/2309.06105</link><description>&lt;p&gt;
&#23454;&#29616;&#35270;&#35273;&#20998;&#31867;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Towards Visual Taxonomy Expansion. (arXiv:2309.06105v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#35273;&#20998;&#31867;&#25193;&#23637;&#65288;VTE&#65289;&#26041;&#27861;&#65292;&#23558;&#35270;&#35273;&#29305;&#24449;&#24341;&#20837;&#21040;&#20998;&#31867;&#25193;&#23637;&#20219;&#21153;&#20013;&#65292;&#24182;&#36890;&#36807;&#25991;&#26412;&#19978;&#20041;&#35789;&#23398;&#20064;&#20219;&#21153;&#21644;&#35270;&#35273;&#21407;&#22411;&#23398;&#20064;&#20219;&#21153;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#35821;&#20041;&#65292;&#20135;&#29983;&#20102;&#32454;&#31890;&#24230;&#30340;&#35270;&#35273;&#35821;&#20041;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20013;&#25991;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24230;&#65292;&#34920;&#29616;&#20248;&#20110;ChatGPT.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#25193;&#23637;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#65292;&#21487;&#20197;&#23558;&#26032;&#27010;&#24565;&#30340;&#19981;&#26029;&#22686;&#21152;&#30340;&#25968;&#37327;&#32452;&#32455;&#21040;&#29616;&#26377;&#20998;&#31867;&#20013;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#21482;&#20851;&#27880;&#20351;&#29992;&#25991;&#26412;&#35821;&#20041;&#65292;&#23548;&#33268;&#26080;&#27861;&#25512;&#24191;&#21040;&#26410;&#35265;&#26415;&#35821;&#21644;&#8220;&#20856;&#22411;&#36229;&#20041;&#38382;&#39064;&#8221;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35270;&#35273;&#20998;&#31867;&#25193;&#23637;&#65288;VTE&#65289;&#65292;&#23558;&#35270;&#35273;&#29305;&#24449;&#24341;&#20837;&#21040;&#20998;&#31867;&#25193;&#23637;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25991;&#26412;&#19978;&#20041;&#35789;&#23398;&#20064;&#20219;&#21153;&#21644;&#35270;&#35273;&#21407;&#22411;&#23398;&#20064;&#20219;&#21153;&#65292;&#20197;&#32858;&#31867;&#25991;&#26412;&#21644;&#35270;&#35273;&#35821;&#20041;&#12290;&#38500;&#20102;&#21508;&#33258;&#27169;&#24577;&#30340;&#20219;&#21153;&#20043;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36229;-&#21407;&#22411;&#32422;&#26463;&#65292;&#23558;&#25991;&#26412;&#21644;&#35270;&#35273;&#35821;&#20041;&#32467;&#21512;&#36215;&#26469;&#20135;&#29983;&#32454;&#31890;&#24230;&#30340;&#35270;&#35273;&#35821;&#20041;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#20449;&#26381;&#30340;&#32467;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#20013;&#25991;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20934;&#30830;&#24230;&#26174;&#33879;&#25552;&#39640;&#20102;8.75&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20013;&#25991;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;ChatGPT.
&lt;/p&gt;
&lt;p&gt;
Taxonomy expansion task is essential in organizing the ever-increasing volume of new concepts into existing taxonomies. Most existing methods focus exclusively on using textual semantics, leading to an inability to generalize to unseen terms and the "Prototypical Hypernym Problem." In this paper, we propose Visual Taxonomy Expansion (VTE), introducing visual features into the taxonomy expansion task. We propose a textual hypernymy learning task and a visual prototype learning task to cluster textual and visual semantics. In addition to the tasks on respective modalities, we introduce a hyper-proto constraint that integrates textual and visual semantics to produce fine-grained visual semantics. Our method is evaluated on two datasets, where we obtain compelling results. Specifically, on the Chinese taxonomy dataset, our method significantly improves accuracy by 8.75 %. Additionally, our approach performs better than ChatGPT on the Chinese taxonomy dataset.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#24494;&#35843;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#31574;&#30053;&#22312;&#35299;&#20915;&#36328;&#35821;&#35328;&#20219;&#21153;&#26102;&#30340;&#34920;&#29616;&#65292;&#35780;&#20272;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#31243;&#24230;&#21644;&#36716;&#31227;&#30340;&#25104;&#21151;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.06089</link><description>&lt;p&gt;
&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#33539;&#24335;&#20013;&#27979;&#37327;&#28798;&#38590;&#24615;&#36951;&#24536;&#65306;&#25506;&#32034;&#35843;&#20248;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Measuring Catastrophic Forgetting in Cross-Lingual Transfer Paradigms: Exploring Tuning Strategies. (arXiv:2309.06089v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06089
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#24494;&#35843;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#31574;&#30053;&#22312;&#35299;&#20915;&#36328;&#35821;&#35328;&#20219;&#21153;&#26102;&#30340;&#34920;&#29616;&#65292;&#35780;&#20272;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#31243;&#24230;&#21644;&#36716;&#31227;&#30340;&#25104;&#21151;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#35328;&#36716;&#31227;&#26159;&#19968;&#31181;&#35299;&#20915;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#20219;&#21153;&#30340;&#26377;&#24076;&#26395;&#30340;&#25216;&#26415;&#12290;&#22312;&#36825;&#20010;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#19982;&#38646;&#23556;&#21644;&#20840;&#23556;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#35821;&#35328;&#35774;&#32622;&#19979;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;&#20316;&#20026;&#24494;&#35843;&#31574;&#30053;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#21442;&#25968;&#25928;&#29575;&#36866;&#37197;&#22120;&#26041;&#27861;&#19982;&#25152;&#26377;&#21442;&#25968;&#24494;&#35843;&#12290;&#20316;&#20026;&#36328;&#35821;&#35328;&#36716;&#31227;&#31574;&#30053;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20351;&#29992;&#27599;&#20010;&#35821;&#35328;&#20381;&#27425;&#30340;&#20013;&#38388;&#35757;&#32451;&#65288;IT&#65289;&#21644;&#22312;&#24494;&#35843;&#30340;&#39564;&#35777;&#38454;&#27573;&#24050;&#32463;&#20351;&#29992;&#30446;&#26631;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#39564;&#35777;&#65288;CLV&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36716;&#31227;&#30340;&#25104;&#21151;&#31243;&#24230;&#20197;&#21450;&#28304;&#35821;&#35328;&#20013;&#30001;&#20110;&#36328;&#35821;&#35328;&#36716;&#31227;&#32780;&#23548;&#33268;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#31243;&#24230;&#65292;&#21363;&#22312;&#23398;&#20064;&#19981;&#21516;&#35821;&#35328;&#20013;&#30340;&#26032;&#20449;&#24687;&#26102;&#20043;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#25439;&#22833;&#20102;&#22810;&#23569;&#12290;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#20998;&#31867;&#38382;&#39064;&#19978;&#65292;&#21253;&#25324;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#21644;&#20135;&#21697;&#35780;&#35770;&#65292;&#20998;&#21035;&#21253;&#21547;&#20102;&#22810;&#20010;&#35821;&#31181;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cross-lingual transfer is a promising technique to solve tasks in less-resourced languages. In this empirical study, we compare two fine-tuning approaches combined with zero-shot and full-shot learning approaches for large language models in a cross-lingual setting. As fine-tuning strategies, we compare parameter-efficient adapter methods with fine-tuning of all parameters. As cross-lingual transfer strategies, we compare the intermediate-training (\textit{IT}) that uses each language sequentially and cross-lingual validation (\textit{CLV}) that uses a target language already in the validation phase of fine-tuning. We assess the success of transfer and the extent of catastrophic forgetting in a source language due to cross-lingual transfer, i.e., how much previously acquired knowledge is lost when we learn new information in a different language. The results on two different classification problems, hate speech detection and product reviews, each containing datasets in several lang
&lt;/p&gt;</description></item><item><title>BHASA&#26159;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#22871;&#20214;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#19996;&#21335;&#20122;&#35821;&#35328;&#21644;&#25991;&#21270;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#23427;&#21253;&#25324;NLP&#22522;&#20934;&#12289;&#35821;&#35328;&#35786;&#26029;&#24037;&#20855;&#21253;&#21644;&#25991;&#21270;&#35786;&#26029;&#25968;&#25454;&#38598;&#12290;&#30446;&#21069;&#65292;&#35813;&#22871;&#20214;&#30340;&#21021;&#27493;&#29256;&#26412;&#20165;&#38024;&#23545;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#12289;&#36234;&#21335;&#35821;&#12289;&#27888;&#35821;&#21644;&#27888;&#31859;&#23572;&#35821;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.06085</link><description>&lt;p&gt;
BHASA&#65306;&#38754;&#21521;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#19996;&#21335;&#20122;&#35821;&#35328;&#21644;&#25991;&#21270;&#32508;&#21512;&#35780;&#20272;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
BHASA: A Holistic Southeast Asian Linguistic and Cultural Evaluation Suite for Large Language Models. (arXiv:2309.06085v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06085
&lt;/p&gt;
&lt;p&gt;
BHASA&#26159;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#22871;&#20214;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#19996;&#21335;&#20122;&#35821;&#35328;&#21644;&#25991;&#21270;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#23427;&#21253;&#25324;NLP&#22522;&#20934;&#12289;&#35821;&#35328;&#35786;&#26029;&#24037;&#20855;&#21253;&#21644;&#25991;&#21270;&#35786;&#26029;&#25968;&#25454;&#38598;&#12290;&#30446;&#21069;&#65292;&#35813;&#22871;&#20214;&#30340;&#21021;&#27493;&#29256;&#26412;&#20165;&#38024;&#23545;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#12289;&#36234;&#21335;&#35821;&#12289;&#27888;&#35821;&#21644;&#27888;&#31859;&#23572;&#35821;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#35268;&#27169;&#24102;&#26469;&#30340;&#26032;&#33021;&#21147;&#20351;&#24471;&#26500;&#24314;&#20840;&#38754;&#12289;&#22810;&#26679;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#25104;&#20026;&#24517;&#35201;&#65292;&#22914;HELM&#21644;BIG-bench&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#37096;&#20998;&#22522;&#20934;&#21482;&#20851;&#27880;&#33521;&#35821;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#19996;&#21335;&#20122;&#65288;SEA&#65289;&#35821;&#35328;&#30340;&#35780;&#20272;&#24456;&#23569;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BHASA&#65292;&#19968;&#20010;&#38024;&#23545;SEA&#35821;&#35328;&#30340;&#32508;&#21512;&#35821;&#35328;&#21644;&#25991;&#21270;&#35780;&#20272;&#22871;&#20214;&#12290;&#23427;&#21253;&#25324;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;1&#65289;&#28085;&#30422;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#12289;&#29983;&#25104;&#65288;NLG&#65289;&#21644;&#25512;&#29702;&#65288;NLR&#65289;&#20219;&#21153;&#30340;NLP&#22522;&#20934;&#65292;&#20849;&#28085;&#30422;&#20843;&#20010;&#20219;&#21153;&#65307;&#65288;2&#65289;LINDSEA&#65292;&#19968;&#20010;&#36328;&#36234;&#21477;&#27861;&#12289;&#35821;&#20041;&#21644;&#35821;&#29992;&#31561;&#21508;&#31181;&#35821;&#35328;&#29616;&#35937;&#30340;&#35821;&#35328;&#35786;&#26029;&#24037;&#20855;&#21253;&#65307;&#65288;3&#65289;&#19968;&#20221;&#25991;&#21270;&#35786;&#26029;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25506;&#32034;&#25991;&#21270;&#34920;&#36798;&#21644;&#25935;&#24863;&#24615;&#12290;&#23545;&#20110;&#36825;&#20010;&#21021;&#27493;&#24037;&#20316;&#65292;&#25105;&#20204;&#21482;&#38024;&#23545;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#12289;&#36234;&#21335;&#35821;&#12289;&#27888;&#35821;&#21644;&#27888;&#31859;&#23572;&#35821;&#23454;&#29616;&#20102;NLP&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid development of Large Language Models (LLMs) and the emergence of novel abilities with scale have necessitated the construction of holistic, diverse and challenging benchmarks such as HELM and BIG-bench. However, at the moment, most of these benchmarks focus only on performance in English and evaluations that include Southeast Asian (SEA) languages are few in number. We therefore propose BHASA, a holistic linguistic and cultural evaluation suite for LLMs in SEA languages. It comprises three components: (1) a NLP benchmark covering eight tasks across Natural Language Understanding (NLU), Generation (NLG) and Reasoning (NLR) tasks, (2) LINDSEA, a linguistic diagnostic toolkit that spans the gamut of linguistic phenomena including syntax, semantics and pragmatics, and (3) a cultural diagnostics dataset that probes for both cultural representation and sensitivity. For this preliminary effort, we implement the NLP benchmark only for Indonesian, Vietnamese, Thai and Tamil, and we on
&lt;/p&gt;</description></item><item><title>RAP-Gen&#26159;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#20462;&#34917;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20043;&#21069;&#30340;&#20195;&#30721;&#24211;&#20013;&#26816;&#32034;&#21040;&#30340;&#30456;&#20851;&#20462;&#22797;&#27169;&#24335;&#65292;&#20197;&#20943;&#36731;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#20013;&#21442;&#25968;&#27169;&#22411;&#24314;&#27169;&#22797;&#26434;&#25628;&#32034;&#31354;&#38388;&#30340;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2309.06057</link><description>&lt;p&gt;
RAP-Gen&#65306;&#22522;&#20110;CodeT5&#30340;&#26816;&#32034;&#22686;&#24378;&#20462;&#34917;&#29983;&#25104;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
RAP-Gen: Retrieval-Augmented Patch Generation with CodeT5 for Automatic Program Repair. (arXiv:2309.06057v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06057
&lt;/p&gt;
&lt;p&gt;
RAP-Gen&#26159;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#20462;&#34917;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20043;&#21069;&#30340;&#20195;&#30721;&#24211;&#20013;&#26816;&#32034;&#21040;&#30340;&#30456;&#20851;&#20462;&#22797;&#27169;&#24335;&#65292;&#20197;&#20943;&#36731;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#20013;&#21442;&#25968;&#27169;&#22411;&#24314;&#27169;&#22797;&#26434;&#25628;&#32034;&#31354;&#38388;&#30340;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;(APR)&#23545;&#20110;&#20943;&#23569;&#24320;&#21457;&#32773;&#30340;&#25163;&#21160;&#35843;&#35797;&#24037;&#20316;&#21644;&#25552;&#39640;&#36719;&#20214;&#21487;&#38752;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#25628;&#32034;&#30340;&#25216;&#26415;&#36890;&#24120;&#20381;&#36182;&#21551;&#21457;&#24335;&#35268;&#21017;&#25110;&#20887;&#20313;&#20551;&#35774;&#26469;&#25366;&#25496;&#20462;&#22797;&#27169;&#24335;&#65292;&#32780;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;(DL)&#26041;&#27861;&#30340;&#20852;&#36215;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#33258;&#21160;&#21270;&#31243;&#24207;&#20462;&#22797;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#24120;&#24120;&#21463;&#21040;&#21442;&#25968;&#27169;&#22411;&#23545;APR&#39640;&#24230;&#22797;&#26434;&#25628;&#32034;&#31354;&#38388;&#24314;&#27169;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#20943;&#36731;&#21442;&#25968;&#27169;&#22411;&#30340;&#36127;&#25285;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#32034;&#22686;&#24378;&#20462;&#34917;&#29983;&#25104;&#26694;&#26550;(RAP-Gen)&#65292;&#36890;&#36807;&#26126;&#30830;&#21033;&#29992;&#28304;&#33258;&#20197;&#21069;&#30340;&#38169;&#28431;&#23545;&#20195;&#30721;&#24211;&#20013;&#26816;&#32034;&#21040;&#30340;&#30456;&#20851;&#20462;&#22797;&#27169;&#24335;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#28151;&#21512;&#30340;&#20462;&#34917;&#31579;&#36873;&#22120;&#65292;&#20197;&#35821;&#35328;&#26080;&#20851;&#30340;&#26041;&#24335;&#65292;&#22522;&#20110;&#21407;&#22987;&#28304;&#20195;&#30721;&#36827;&#34892;&#35789;&#27861;&#21644;&#35821;&#20041;&#21305;&#37197;&#65292;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#29305;&#23450;&#20110;&#20195;&#30721;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic program repair (APR) is crucial to reduce manual debugging efforts for developers and improve software reliability. While conventional search-based techniques typically rely on heuristic rules or a redundancy assumption to mine fix patterns, recent years have witnessed the surge of deep learning (DL) based approaches to automate the program repair process in a data-driven manner. However, their performance is often limited by a fixed set of parameters to model the highly complex search space of APR. To ease such burden on the parametric models, in this work, we propose a novel Retrieval-Augmented Patch Generation framework (RAP-Gen) by explicitly leveraging relevant fix patterns retrieved from a codebase of previous bug-fix pairs. Specifically, we build a hybrid patch retriever to account for both lexical and semantic matching based on the raw source code in a language-agnostic manner, which does not rely on any code-specific features. In addition, we adapt a code-aware langu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#34920;&#31034;&#23398;&#20064;&#30340;&#35282;&#24230;&#65292;&#30740;&#31350;&#20102;&#34920;&#31034;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#19978;&#19979;&#25991;&#20869;&#37096;&#25104;&#20998;&#23545;&#23398;&#20064;&#24615;&#33021;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.06054</link><description>&lt;p&gt;
&#34920;&#31034;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24433;&#21709;&#65306;&#23545;&#21512;&#25104;&#20219;&#21153;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
How does representation impact in-context learning: A exploration on a synthetic task. (arXiv:2309.06054v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#34920;&#31034;&#23398;&#20064;&#30340;&#35282;&#24230;&#65292;&#30740;&#31350;&#20102;&#34920;&#31034;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#19978;&#19979;&#25991;&#20869;&#37096;&#25104;&#20998;&#23545;&#23398;&#20064;&#24615;&#33021;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21363;&#20174;&#19978;&#19979;&#25991;&#26679;&#26412;&#20013;&#23398;&#20064;&#65292;&#26159;Transformer&#30340;&#19968;&#39033;&#24341;&#20154;&#27880;&#30446;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#39537;&#21160;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#23578;&#26410;&#34987;&#20805;&#20998;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20174;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#34920;&#31034;&#23398;&#20064;&#35282;&#24230;&#36827;&#34892;&#35843;&#26597;&#12290;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#22330;&#26223;&#20013;&#65292;&#34920;&#31034;&#26356;&#21152;&#22797;&#26434;&#65292;&#34920;&#31034;&#21487;&#20197;&#21463;&#21040;&#27169;&#22411;&#26435;&#37325;&#21644;&#19978;&#19979;&#25991;&#26679;&#26412;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#19978;&#36848;&#20004;&#20010;&#27010;&#24565;&#26041;&#38754;&#30340;&#34920;&#31034;&#20998;&#21035;&#31216;&#20026;&#26435;&#37325;&#20869;&#37096;&#25104;&#20998;&#21644;&#19978;&#19979;&#25991;&#20869;&#37096;&#25104;&#20998;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20004;&#20010;&#25104;&#20998;&#22914;&#20309;&#24433;&#21709;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21512;&#25104;&#20219;&#21153;&#65292;&#20174;&#32780;&#21487;&#20197;&#35774;&#35745;&#20004;&#20010;&#25506;&#38024;&#65292;&#21363;&#26435;&#37325;&#20869;&#37096;&#25506;&#38024;&#21644;&#19978;&#19979;&#25991;&#25506;&#38024;&#65292;&#20998;&#21035;&#35780;&#20272;&#36825;&#20004;&#20010;&#25104;&#20998;&#12290;&#25105;&#20204;&#35777;&#26126;&#19978;&#19979;&#25991;&#20869;&#37096;&#25104;&#20998;&#30340;&#22909;&#22351;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#39640;&#24230;&#30456;&#20851;&#65292;&#36825;&#34920;&#26126;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#32416;&#32544;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning, i.e., learning from in-context samples, is an impressive ability of Transformer. However, the mechanism driving the in-context learning is not yet fully understood. In this study, we aim to investigate from an underexplored perspective of representation learning. The representation is more complex for in-context learning senario, where the representation can be impacted by both model weights and in-context samples. We refer the above two conceptually aspects of representation as in-weight component and in-context component, respectively. To study how the two components affect in-context learning capabilities, we construct a novel synthetic task, making it possible to device two probes, in-weights probe and in-context probe, to evaluate the two components, respectively. We demonstrate that the goodness of in-context component is highly related to the in-context learning performance, which indicates the entanglement between in-context learning and representation lear
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22235;&#20010;&#20934;&#21017;&#29992;&#20110;&#20272;&#35745;&#38271;&#25991;&#26723;&#30340;&#20449;&#24687;&#23494;&#24230;&#65292;&#21253;&#25324;&#24778;&#35766;&#24230;&#12289;&#29109;&#12289;&#22343;&#21248;&#20449;&#24687;&#23494;&#24230;&#21644;&#35789;&#27719;&#23494;&#24230;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35789;&#27719;&#36873;&#25321;&#26041;&#27861;&#22312;&#33258;&#21160;&#21270;&#21307;&#23398;&#32534;&#30721;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06009</link><description>&lt;p&gt;
&#38271;&#25991;&#26723;&#30340;&#20869;&#23481;&#20943;&#23569;&#12289;&#24778;&#35766;&#24230;&#21644;&#20449;&#24687;&#23494;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Content Reduction, Surprisal and Information Density Estimation for Long Documents. (arXiv:2309.06009v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22235;&#20010;&#20934;&#21017;&#29992;&#20110;&#20272;&#35745;&#38271;&#25991;&#26723;&#30340;&#20449;&#24687;&#23494;&#24230;&#65292;&#21253;&#25324;&#24778;&#35766;&#24230;&#12289;&#29109;&#12289;&#22343;&#21248;&#20449;&#24687;&#23494;&#24230;&#21644;&#35789;&#27719;&#23494;&#24230;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35789;&#27719;&#36873;&#25321;&#26041;&#27861;&#22312;&#33258;&#21160;&#21270;&#21307;&#23398;&#32534;&#30721;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#35745;&#31639;&#35821;&#35328;&#23398;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#29992;&#20110;&#30740;&#31350;&#35821;&#35328;&#30340;&#20449;&#24687;&#20869;&#23481;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#38382;&#39064;&#65306;1)&#20449;&#24687;&#26159;&#22914;&#20309;&#22312;&#38271;&#25991;&#26723;&#20013;&#20998;&#24067;&#30340;&#65311;2)&#20869;&#23481;&#20943;&#23569;&#65288;&#22914;&#26631;&#35760;&#36873;&#25321;&#21644;&#25991;&#26412;&#25688;&#35201;&#65289;&#22914;&#20309;&#24433;&#21709;&#38271;&#25991;&#26723;&#20013;&#30340;&#20449;&#24687;&#23494;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#29992;&#20110;&#20272;&#35745;&#38271;&#25991;&#26723;&#20449;&#24687;&#23494;&#24230;&#30340;&#20934;&#21017;&#65292;&#21253;&#25324;&#24778;&#35766;&#24230;&#12289;&#29109;&#12289;&#22343;&#21248;&#20449;&#24687;&#23494;&#24230;&#21644;&#35789;&#27719;&#23494;&#24230;&#12290;&#20854;&#20013;&#21069;&#19977;&#20010;&#37319;&#29992;&#20102;&#20449;&#24687;&#35770;&#30340;&#27979;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#20026;&#20020;&#24202;&#31508;&#35760;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35789;&#27719;&#36873;&#25321;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#22810;&#39046;&#22495;&#25991;&#26723;&#30340;&#26426;&#22120;&#25688;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#20102;&#19981;&#21516;&#39046;&#22495;&#38271;&#25991;&#26412;&#30340;&#20449;&#24687;&#23494;&#24230;&#31995;&#32479;&#24046;&#24322;&#12290;&#23545;&#20110;&#38271;&#20020;&#24202;&#31508;&#35760;&#30340;&#33258;&#21160;&#21270;&#21307;&#23398;&#32534;&#30721;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35789;&#27719;&#36873;&#25321;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many computational linguistic methods have been proposed to study the information content of languages. We consider two interesting research questions: 1) how is information distributed over long documents, and 2) how does content reduction, such as token selection and text summarization, affect the information density in long documents. We present four criteria for information density estimation for long documents, including surprisal, entropy, uniform information density, and lexical density. Among those criteria, the first three adopt the measures from information theory. We propose an attention-based word selection method for clinical notes and study machine summarization for multiple-domain documents. Our findings reveal the systematic difference in information density of long text in various domains. Empirical results on automated medical coding from long clinical notes show the effectiveness of the attention-based word selection method.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#28040;&#34701;&#27169;&#22411;&#32452;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#36335;&#24452;&#26469;&#21435;&#38500;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#33391;&#34892;&#20026;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#20943;&#23569;GPT-2&#27602;&#24615;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#65292;&#20165;&#28040;&#34701;12&#26465;&#22240;&#26524;&#36793;&#20013;&#30340;11.6K&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;&#27602;&#24615;&#29983;&#25104;&#65292;&#24182;&#22312;&#20854;&#20182;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#24456;&#23567;&#12290;</title><link>http://arxiv.org/abs/2309.05973</link><description>&lt;p&gt;
&#20999;&#26029;&#30005;&#36335;: &#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#28040;&#34701;&#21435;&#38500;&#27169;&#22411;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Circuit Breaking: Removing Model Behaviors with Targeted Ablation. (arXiv:2309.05973v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#28040;&#34701;&#27169;&#22411;&#32452;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#36335;&#24452;&#26469;&#21435;&#38500;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#33391;&#34892;&#20026;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#20943;&#23569;GPT-2&#27602;&#24615;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#65292;&#20165;&#28040;&#34701;12&#26465;&#22240;&#26524;&#36793;&#20013;&#30340;11.6K&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;&#27602;&#24615;&#29983;&#25104;&#65292;&#24182;&#22312;&#20854;&#20182;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20250;&#34920;&#29616;&#20986;&#22312;&#39044;&#35757;&#32451;&#30446;&#26631;&#19978;&#25552;&#39640;&#24615;&#33021;&#20294;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#38477;&#20302;&#24615;&#33021;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28040;&#34701;&#27169;&#22411;&#32452;&#20214;&#20043;&#38388;&#30340;&#19968;&#23567;&#37096;&#20998;&#22240;&#26524;&#36335;&#24452;&#65292;&#20197;&#31105;&#29992;&#19982;&#19981;&#33391;&#34892;&#20026;&#26377;&#20851;&#30340;&#35745;&#31639;&#30005;&#36335;&#65292;&#20174;&#32780;&#21435;&#38500;&#19981;&#33391;&#34892;&#20026;&#12290;&#22312;&#25317;&#26377;&#27169;&#22411;&#34920;&#29616;&#24046;&#30340;&#23567;&#22411;&#36755;&#20837;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23398;&#20250;&#20102;&#28040;&#34701;&#19968;&#23567;&#37096;&#20998;&#37325;&#35201;&#30340;&#22240;&#26524;&#36335;&#24452;&#12290;&#22312;&#20943;&#23569;GPT-2&#27602;&#24615;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#65292;&#25105;&#20204;&#21457;&#29616;&#28040;&#34701;&#20165;&#20165;12&#26465;&#22240;&#26524;&#36793;&#20013;&#30340;11.6K&#65292;&#21487;&#20197;&#20943;&#36731;&#27602;&#24615;&#29983;&#25104;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models often exhibit behaviors that improve performance on a pre-training objective but harm performance on downstream tasks. We propose a novel approach to removing undesirable behaviors by ablating a small number of causal pathways between model components, with the intention of disabling the computational circuit responsible for the bad behavior. Given a small dataset of inputs where the model behaves poorly, we learn to ablate a small number of important causal pathways. In the setting of reducing GPT-2 toxic language generation, we find ablating just 12 of the 11.6K causal edges mitigates toxic generation with minimal degradation of performance on other inputs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#20845;&#20010;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#26597;&#35810;&#30340;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20114;&#21160;&#27700;&#24179;&#19982;&#31532;&#19968;&#20010;&#22238;&#31572;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26597;&#35810;&#26159;&#21542;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#22238;&#31572;&#12290;</title><link>http://arxiv.org/abs/2309.05961</link><description>&lt;p&gt;
&#35780;&#20272;&#28526;&#36215;&#28526;&#33853;&#65306;&#23545;&#19981;&#21516;&#24179;&#21488;&#38388;&#38382;&#31572;&#36235;&#21183;&#30340;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms. (arXiv:2309.05961v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#20845;&#20010;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#26597;&#35810;&#30340;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20114;&#21160;&#27700;&#24179;&#19982;&#31532;&#19968;&#20010;&#22238;&#31572;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26597;&#35810;&#26159;&#21542;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#22240;&#20854;&#24555;&#36895;&#22238;&#31572;&#29992;&#25143;&#26597;&#35810;&#30340;&#33021;&#21147;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#36825;&#20123;&#22238;&#31572;&#36895;&#24230;&#30340;&#24555;&#24930;&#21462;&#20915;&#20110;&#26597;&#35810;&#29305;&#23450;&#21644;&#29992;&#25143;&#30456;&#20851;&#30340;&#22240;&#32032;&#30340;&#32508;&#21512;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20845;&#20010;&#39640;&#24230;&#27969;&#34892;&#30340;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#65292;&#20998;&#26512;&#20102;&#36825;&#20123;&#22240;&#32032;&#22312;&#20854;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#22238;&#31572;&#25152;&#33457;&#36153;&#30340;&#26102;&#38388;&#19982;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#30340;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#20114;&#21160;&#27700;&#24179;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#36825;&#20123;&#20803;&#25968;&#25454;&#21644;&#29992;&#25143;&#20114;&#21160;&#27169;&#24335;&#65292;&#25105;&#20204;&#35797;&#22270;&#39044;&#27979;&#21738;&#20123;&#26597;&#35810;&#23558;&#36805;&#36895;&#33719;&#24471;&#21021;&#22987;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community Question Answering (CQA) platforms steadily gain popularity as they provide users with fast responses to their queries. The swiftness of these responses is contingent on a mixture of query-specific and user-related elements. This paper scrutinizes these contributing factors within the context of six highly popular CQA platforms, identified through their standout answering speed. Our investigation reveals a correlation between the time taken to yield the first response to a question and several variables: the metadata, the formulation of the questions, and the level of interaction among users. Additionally, by employing conventional machine learning models to analyze these metadata and patterns of user interaction, we endeavor to predict which queries will receive their initial responses promptly.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#36947;&#24503;&#26426;&#22120;&#26694;&#26550;&#65292;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36947;&#24503;&#20915;&#31574;&#19978;&#30340;&#20542;&#21521;&#24615;&#65292;&#21457;&#29616;&#34429;&#28982;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#20559;&#22909;&#23384;&#22312;&#23450;&#24615;&#19978;&#30340;&#30456;&#20284;&#24615;&#65292;&#20294;&#22312;&#25968;&#37327;&#19978;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#24322;&#65292;&#34920;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#20542;&#21521;&#20110;&#20570;&#20986;&#26356;&#22362;&#20915;&#30340;&#20915;&#31574;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#20855;&#26377;&#37325;&#35201;&#30340;&#20262;&#29702;&#21644;&#28508;&#22312;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.05958</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#36947;&#24503;&#26426;&#22120;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
The Moral Machine Experiment on Large Language Models. (arXiv:2309.05958v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#36947;&#24503;&#26426;&#22120;&#26694;&#26550;&#65292;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36947;&#24503;&#20915;&#31574;&#19978;&#30340;&#20542;&#21521;&#24615;&#65292;&#21457;&#29616;&#34429;&#28982;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#20559;&#22909;&#23384;&#22312;&#23450;&#24615;&#19978;&#30340;&#30456;&#20284;&#24615;&#65292;&#20294;&#22312;&#25968;&#37327;&#19978;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#24322;&#65292;&#34920;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#20542;&#21521;&#20110;&#20570;&#20986;&#26356;&#22362;&#20915;&#30340;&#20915;&#31574;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#20855;&#26377;&#37325;&#35201;&#30340;&#20262;&#29702;&#21644;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#28145;&#20837;&#25972;&#21512;&#65292;&#29702;&#35299;&#23427;&#20204;&#22914;&#20309;&#20570;&#20986;&#36947;&#24503;&#21028;&#26029;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#36947;&#24503;&#26426;&#22120;&#26694;&#26550;&#65292;&#35843;&#26597;&#20102;&#30693;&#21517;&#30340;LLMs&#65288;&#21253;&#25324;GPT-3.5&#65292;GPT-4&#65292;PaLM 2&#21644;Llama 2&#65289;&#30340;&#36947;&#24503;&#20915;&#31574;&#20542;&#21521;&#65292;&#24182;&#23558;&#20854;&#19982;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#27604;&#36739;&#12290;&#34429;&#28982;LLMs&#21644;&#20154;&#31867;&#30340;&#20559;&#22909;&#65288;&#20363;&#22914;&#23558;&#20154;&#31867;&#25918;&#22312;&#23456;&#29289;&#20043;&#19978;&#21644;&#26356;&#20542;&#21521;&#20110;&#25405;&#25937;&#26356;&#22810;&#29983;&#21629;&#65289;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#19968;&#33268;&#30340;&#65292;&#20294;&#29305;&#21035;&#26159;PaLM 2&#21644;Llama 2&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;LLM&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#23384;&#22312;&#23450;&#24615;&#19978;&#30340;&#30456;&#20284;&#24615;&#65292;&#20294;&#22312;&#25968;&#37327;&#19978;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#24322;&#65292;&#36825;&#34920;&#26126;&#19982;&#20154;&#31867;&#30456;&#27604;&#65292;LLMs&#21487;&#33021;&#26356;&#36235;&#21521;&#20110;&#20570;&#20986;&#26356;&#22362;&#20915;&#30340;&#20915;&#31574;&#12290;&#36825;&#20123;&#21457;&#29616;&#38416;&#26126;&#20102;LLMs&#30340;&#36947;&#24503;&#26694;&#26550;&#21450;&#20854;&#23545;&#33258;&#21160;&#39550;&#39542;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) become more deeply integrated into various sectors, understanding how they make moral judgments has become crucial, particularly in the realm of autonomous driving. This study utilized the Moral Machine framework to investigate the ethical decision-making tendencies of prominent LLMs, including GPT-3.5, GPT-4, PaLM 2, and Llama 2, comparing their responses to human preferences. While LLMs' and humans' preferences such as prioritizing humans over pets and favoring saving more lives are broadly aligned, PaLM 2 and Llama 2, especially, evidence distinct deviations. Additionally, despite the qualitative similarities between the LLM and human preferences, there are significant quantitative disparities, suggesting that LLMs might lean toward more uncompromising decisions, compared to the milder inclinations of humans. These insights elucidate the ethical frameworks of LLMs and their potential implications for autonomous driving.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#30340;&#24179;&#34913;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20844;&#20849;&#21355;&#29983;&#20998;&#26512;&#26694;&#26550;ALEX&#65292;&#36890;&#36807;&#37319;&#29992;&#22797;&#26434;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#20811;&#26381;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.05951</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20844;&#20849;&#21355;&#29983;&#30340;&#24179;&#34913;&#21644;&#21487;&#35299;&#37322;&#24615;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Balanced and Explainable Social Media Analysis for Public Health with Large Language Models. (arXiv:2309.05951v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05951
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#30340;&#24179;&#34913;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20844;&#20849;&#21355;&#29983;&#20998;&#26512;&#26694;&#26550;ALEX&#65292;&#36890;&#36807;&#37319;&#29992;&#22797;&#26434;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#20811;&#26381;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20844;&#20849;&#21355;&#29983;&#27963;&#21160;&#20986;&#29616;&#65292;&#36825;&#23545;&#20110;&#30123;&#24773;&#30417;&#27979;&#21644;&#25919;&#24220;&#20915;&#31574;&#38750;&#24120;&#20540;&#24471;&#27880;&#24847;&#12290;&#24403;&#21069;&#30340;&#20844;&#20849;&#21355;&#29983;&#20998;&#26512;&#25216;&#26415;&#28041;&#21450;BERT&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#31561;&#27969;&#34892;&#27169;&#22411;&#12290;&#23613;&#31649;LLMs&#30340;&#26368;&#26032;&#36827;&#23637;&#34920;&#26126;&#36890;&#36807;&#22312;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23427;&#20204;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35299;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#20294;&#20026;&#27599;&#20010;&#29305;&#23450;&#30340;&#20844;&#20849;&#21355;&#29983;&#20219;&#21153;&#35757;&#32451;&#19968;&#20010;&#39046;&#22495;&#20869;LLM&#30340;&#25104;&#26412;&#23588;&#20026;&#26114;&#36149;&#12290;&#27492;&#22806;&#65292;&#26469;&#33258;&#31038;&#20132;&#23186;&#20307;&#30340;&#27492;&#31867;&#39046;&#22495;&#20869;&#25968;&#25454;&#38598;&#36890;&#24120;&#20855;&#26377;&#20005;&#37325;&#30340;&#19981;&#24179;&#34913;&#24615;&#65292;&#36825;&#23558;&#22952;&#30861;LLMs&#30340;&#35843;&#20248;&#25928;&#29575;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#21487;&#20197;&#37319;&#29992;&#22797;&#26434;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#20811;&#26381;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#36866;&#24403;&#22320;&#24341;&#23548;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#12290;&#22522;&#20110;&#20197;&#19978;&#35752;&#35770;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;ALEX&#26694;&#26550;&#65292;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#30340;&#24179;&#34913;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20844;&#20849;&#21355;&#29983;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
As social media becomes increasingly popular, more and more public health activities emerge, which is worth noting for pandemic monitoring and government decision-making. Current techniques for public health analysis involve popular models such as BERT and large language models (LLMs). Although recent progress in LLMs has shown a strong ability to comprehend knowledge by being fine-tuned on specific domain datasets, the costs of training an in-domain LLM for every specific public health task are especially expensive. Furthermore, such kinds of in-domain datasets from social media are generally highly imbalanced, which will hinder the efficiency of LLMs tuning. To tackle these challenges, the data imbalance issue can be overcome by sophisticated data augmentation methods for social media datasets. In addition, the ability of the LLMs can be effectively utilised by prompting the model properly. In light of the above discussion, in this paper, a novel ALEX framework is proposed for social
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#36991;&#20813;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#65292;&#37319;&#29992;&#32842;&#22825;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#22312;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.05950</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#30418;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models as Black-Box Optimizers for Vision-Language Models. (arXiv:2309.05950v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#36991;&#20813;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#65292;&#37319;&#29992;&#32842;&#22825;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#22312;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#30446;&#21069;&#65292;VLMs &#30340;&#24494;&#35843;&#26041;&#27861;&#20027;&#35201;&#22312;&#30333;&#30418;&#29615;&#22659;&#20013;&#25805;&#20316;&#65292;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810; VLMs &#20381;&#36182;&#20110;&#19987;&#26377;&#25968;&#25454;&#19988;&#19981;&#24320;&#28304;&#65292;&#38480;&#21046;&#20102;&#20351;&#29992;&#30333;&#30418;&#26041;&#27861;&#36827;&#34892;&#24494;&#35843;&#12290;&#37492;&#20110;&#20687; ChatGPT &#36825;&#26679;&#30340;&#21463;&#27426;&#36814;&#31169;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#20173;&#28982;&#25552;&#20379;&#22522;&#20110;&#35821;&#35328;&#30340;&#29992;&#25143;&#30028;&#38754;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340; VLMs &#24494;&#35843;&#26041;&#27861;&#65292;&#20174;&#32780;&#36991;&#20813;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#12289;&#29305;&#24449;&#23884;&#20837;&#25110;&#36755;&#20986; logits &#30340;&#38656;&#35201;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#32842;&#22825;&#30340; LLMs &#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#20197;&#22312;&#20351;&#29992; CLIP &#36827;&#34892;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#31034;&#20363;&#20219;&#21153;&#20013;&#23547;&#25214;&#26368;&#20339;&#25991;&#26412;&#25552;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#33258;&#21160;"&#29228;&#23665;"&#31243;&#24207;&#65292;&#23427;&#33021;&#25910;&#25947;&#21040;&#26377;&#25928;&#30340;&#25552;&#31034;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) pre-trained on web-scale datasets have demonstrated remarkable capabilities across a variety of vision and multimodal tasks. Currently, fine-tuning methods for VLMs mainly operate in a white-box setting, requiring access to model parameters for backpropagation. However, many VLMs rely on proprietary data and are not open-source, which restricts the use of white-box approaches for fine-tuning. Given that popular private large language models (LLMs) like ChatGPT still offer a language-based user interface, we aim to develop a novel fine-tuning approach for VLMs through natural language prompts, thereby avoiding the need to access model parameters, feature embeddings, or output logits. In this setup, we propose employing chat-based LLMs as black-box optimizers to search for the best text prompt on the illustrative task of few-shot image classification using CLIP. Specifically, we adopt an automatic "hill-climbing" procedure that converges on an effective prom
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65306;&#22238;&#31572;&#20135;&#21697;&#30340;&#20027;&#35266;&#24402;&#32435;&#38382;&#39064;&#65288;SUBJPQA&#65289;&#12290;&#19982;&#20256;&#32479;&#30340;QA&#20219;&#21153;&#19981;&#21516;&#65292;&#36825;&#31867;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#38750;&#21807;&#19968;&#30340;&#65292;&#24182;&#19988;&#38656;&#35201;&#20174;&#22810;&#20010;&#35282;&#24230;&#24635;&#32467;&#22810;&#20010;&#30693;&#35782;&#28304;&#30340;&#20027;&#35266;&#24847;&#35265;&#21644;&#23458;&#35266;&#30693;&#35782;&#26469;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20449;&#24687;&#26816;&#32034;&#12289;&#30456;&#20851;&#24615;&#25429;&#25417;&#21644;&#25688;&#35201;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.05938</link><description>&lt;p&gt;
&#36890;&#36807;&#24635;&#32467;&#22810;&#28304;&#22810;&#35270;&#35282;&#30693;&#35782;&#22238;&#31572;&#20135;&#21697;&#30340;&#20027;&#35266;&#24402;&#32435;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Answering Subjective Induction Questions on Products by Summarizing Multi-sources Multi-viewpoints Knowledge. (arXiv:2309.05938v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65306;&#22238;&#31572;&#20135;&#21697;&#30340;&#20027;&#35266;&#24402;&#32435;&#38382;&#39064;&#65288;SUBJPQA&#65289;&#12290;&#19982;&#20256;&#32479;&#30340;QA&#20219;&#21153;&#19981;&#21516;&#65292;&#36825;&#31867;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#38750;&#21807;&#19968;&#30340;&#65292;&#24182;&#19988;&#38656;&#35201;&#20174;&#22810;&#20010;&#35282;&#24230;&#24635;&#32467;&#22810;&#20010;&#30693;&#35782;&#28304;&#30340;&#20027;&#35266;&#24847;&#35265;&#21644;&#23458;&#35266;&#30693;&#35782;&#26469;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20449;&#24687;&#26816;&#32034;&#12289;&#30456;&#20851;&#24615;&#25429;&#25417;&#21644;&#25688;&#35201;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#22238;&#31572;&#20135;&#21697;&#30340;&#20027;&#35266;&#24402;&#32435;&#38382;&#39064;&#65288;SUBJPQA&#65289;&#30340;&#39046;&#22495;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#12290;&#36825;&#31867;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#38750;&#21807;&#19968;&#30340;&#65292;&#20294;&#21487;&#20197;&#20174;&#22810;&#20010;&#35282;&#24230;&#26469;&#35299;&#37322;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#8220;&#25163;&#26426;&#26159;&#21542;&#37325;&#8221;&#30340;&#31572;&#26696;&#26377;&#22810;&#31181;&#19981;&#21516;&#30340;&#35266;&#28857;&#12290;&#19968;&#20010;&#28385;&#24847;&#30340;&#31572;&#26696;&#24212;&#35813;&#33021;&#22815;&#24635;&#32467;&#36825;&#20123;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#20027;&#35266;&#24847;&#35265;&#65292;&#24182;&#25552;&#20379;&#23458;&#35266;&#30693;&#35782;&#65292;&#27604;&#22914;&#25163;&#26426;&#30340;&#37325;&#37327;&#12290;&#36825;&#19982;&#20256;&#32479;&#30340;QA&#20219;&#21153;&#38750;&#24120;&#19981;&#21516;&#65292;&#20256;&#32479;QA&#20219;&#21153;&#20013;&#23545;&#20110;&#20107;&#23454;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#21807;&#19968;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#21333;&#20010;&#25968;&#25454;&#28304;&#20013;&#25214;&#21040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#26032;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#39588;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#22810;&#20010;&#30693;&#35782;&#28304;&#20013;&#26816;&#32034;&#25152;&#26377;&#19982;&#31572;&#26696;&#30456;&#20851;&#30340;&#32447;&#32034;&#65292;&#21253;&#25324;&#20107;&#23454;&#21644;&#35266;&#28857;&#12290;&#36824;&#25910;&#38598;&#20102;&#38544;&#21547;&#30340;&#24120;&#35782;&#20107;&#23454;&#26469;&#34917;&#20805;&#24517;&#35201;&#20294;&#32570;&#22833;&#30340;&#32972;&#26223;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20132;&#20114;&#24335;&#27880;&#24847;&#21147;&#26469;&#25429;&#25417;&#23427;&#20204;&#19982;&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25688;&#35201;&#29983;&#25104;&#22120;&#26469;&#32858;&#21512;&#36825;&#20123;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new task in the field of Answering Subjective Induction Question on Products (SUBJPQA). The answer to this kind of question is non-unique, but can be interpreted from many perspectives. For example, the answer to 'whether the phone is heavy' has a variety of different viewpoints. A satisfied answer should be able to summarize these subjective opinions from multiple sources and provide objective knowledge, such as the weight of a phone. That is quite different from the traditional QA task, in which the answer to a factoid question is unique and can be found from a single data source. To address this new task, we propose a three-steps method. We first retrieve all answer-related clues from multiple knowledge sources on facts and opinions. The implicit commonsense facts are also collected to supplement the necessary but missing contexts. We then capture their relevance with the questions by interactive attention. Next, we design a reinforcement-based summarizer to ag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#23545;&#26412;&#20307;&#30693;&#35782;&#30340;&#23384;&#20648;&#21644;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#65292;&#20197;&#21450;&#20854;&#23545;&#23454;&#20307;&#31867;&#22411;&#12289;&#31867;&#21644;&#23646;&#24615;&#20043;&#38388;&#30340;&#23618;&#27425;&#20851;&#31995;&#65292;&#20197;&#21450;&#23646;&#24615;&#30340;&#22495;&#21644;&#33539;&#22260;&#32422;&#26463;&#30340;&#35760;&#24518;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.05936</link><description>&lt;p&gt;
PLMs&#26159;&#21542;&#30693;&#36947;&#21644;&#29702;&#35299;&#26412;&#20307;&#30693;&#35782;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do PLMs Know and Understand Ontological Knowledge?. (arXiv:2309.05936v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#23545;&#26412;&#20307;&#30693;&#35782;&#30340;&#23384;&#20648;&#21644;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#65292;&#20197;&#21450;&#20854;&#23545;&#23454;&#20307;&#31867;&#22411;&#12289;&#31867;&#21644;&#23646;&#24615;&#20043;&#38388;&#30340;&#23618;&#27425;&#20851;&#31995;&#65292;&#20197;&#21450;&#23646;&#24615;&#30340;&#22495;&#21644;&#33539;&#22260;&#32422;&#26463;&#30340;&#35760;&#24518;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20307;&#30693;&#35782;&#26159;&#19990;&#30028;&#30693;&#35782;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#31867;&#21644;&#23646;&#24615;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25506;&#32034;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#26159;&#21542;&#30693;&#36947;&#21644;&#29702;&#35299;&#36825;&#26679;&#30340;&#30693;&#35782;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PLM&#25506;&#27979;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20107;&#23454;&#30693;&#35782;&#65292;&#32570;&#20047;&#23545;&#26412;&#20307;&#30693;&#35782;&#30340;&#31995;&#32479;&#25506;&#27979;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#31350;PLMs&#26159;&#21542;&#23384;&#20648;&#26412;&#20307;&#30693;&#35782;&#65292;&#24182;&#23545;&#20854;&#20855;&#26377;&#35821;&#20041;&#29702;&#35299;&#32780;&#19981;&#26159;&#23545;&#34920;&#38754;&#24418;&#24335;&#30340;&#27515;&#35760;&#30828;&#32972;&#12290;&#20026;&#20102;&#25506;&#27979;PLMs&#26159;&#21542;&#20102;&#35299;&#26412;&#20307;&#30693;&#35782;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;PLMs&#23545;&#20197;&#19979;&#26041;&#38754;&#30340;&#35760;&#24518;&#24773;&#20917;&#65306;&#65288;1&#65289;&#23454;&#20307;&#31867;&#22411;&#65307;&#65288;2&#65289;&#31867;&#21644;&#23646;&#24615;&#20043;&#38388;&#30340;&#23618;&#27425;&#20851;&#31995;&#65292;&#20363;&#22914;&#65292;&#20154;&#26159;&#21160;&#29289;&#30340;&#23376;&#31867;&#65292;&#21442;&#19982;&#22242;&#38431;&#30340;&#25104;&#21592;&#26159;&#25104;&#21592;&#30340;&#23376;&#23646;&#24615;&#65307;&#65288;3&#65289;&#23646;&#24615;&#30340;&#22495;&#21644;&#33539;&#22260;&#32422;&#26463;&#65292;&#20363;&#22914;&#65292;&#21442;&#19982;&#22242;&#38431;&#30340;&#25104;&#21592;&#30340;&#20027;&#35821;&#24212;&#26159;&#20154;&#65292;&#23486;&#35821;&#24212;&#26159;&#19968;&#20010;&#36816;&#21160;&#38431;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ontological knowledge, which comprises classes and properties and their relationships, is integral to world knowledge. It is significant to explore whether Pretrained Language Models (PLMs) know and understand such knowledge. However, existing PLM-probing studies focus mainly on factual knowledge, lacking a systematic probing of ontological knowledge. In this paper, we focus on probing whether PLMs store ontological knowledge and have a semantic understanding of the knowledge rather than rote memorization of the surface form. To probe whether PLMs know ontological knowledge, we investigate how well PLMs memorize: (1) types of entities; (2) hierarchical relationships among classes and properties, e.g., Person is a subclass of Animal and Member of Sports Team is a subproperty of Member of ; (3) domain and range constraints of properties, e.g., the subject of Member of Sports Team should be a Person and the object should be a Sports Team. To further probe whether PLMs truly understand ont
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#21253;&#25324;&#24187;&#35273;&#29616;&#35937;&#30340;&#20998;&#31867;&#12289;&#35780;&#20272;&#26631;&#20934;&#21644;&#20943;&#36731;&#24187;&#35273;&#30340;&#31574;&#30053;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.05922</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Hallucination in Large Foundation Models. (arXiv:2309.05922v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#21253;&#25324;&#24187;&#35273;&#29616;&#35937;&#30340;&#20998;&#31867;&#12289;&#35780;&#20272;&#26631;&#20934;&#21644;&#20943;&#36731;&#24187;&#35273;&#30340;&#31574;&#30053;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#25351;&#30340;&#26159;&#29983;&#25104;&#20559;&#31163;&#20107;&#23454;&#30340;&#20869;&#23481;&#25110;&#21253;&#21547;&#34394;&#26500;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#27010;&#36848;&#20102;&#36817;&#26399;&#21162;&#21147;&#30340;&#24191;&#27867;&#27010;&#36848;&#65292;&#36825;&#20123;&#21162;&#21147;&#26088;&#22312;&#35782;&#21035;&#12289;&#38416;&#26126;&#21644;&#35299;&#20915;&#24187;&#35273;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#8220;&#22823;&#8221;&#22522;&#30784;&#27169;&#22411;&#65288;LFMs&#65289;&#12290;&#26412;&#25991;&#23545;&#29305;&#23450;&#20110;LFMs&#30340;&#21508;&#31181;&#31867;&#22411;&#30340;&#24187;&#35273;&#29616;&#35937;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#24182;&#24314;&#31435;&#20102;&#35780;&#20272;&#24187;&#35273;&#31243;&#24230;&#30340;&#35780;&#20272;&#26631;&#20934;&#12290;&#23427;&#36824;&#26816;&#26597;&#20102;&#20943;&#36731;LFM&#20013;&#24187;&#35273;&#30340;&#29616;&#26377;&#31574;&#30053;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#19982;LFMs&#20013;&#24187;&#35273;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hallucination in a foundation model (FM) refers to the generation of content that strays from factual reality or includes fabricated information. This survey paper provides an extensive overview of recent efforts that aim to identify, elucidate, and tackle the problem of hallucination, with a particular focus on ``Large'' Foundation Models (LFMs). The paper classifies various types of hallucination phenomena that are specific to LFMs and establishes evaluation criteria for assessing the extent of hallucination. It also examines existing strategies for mitigating hallucination in LFMs and discusses potential directions for future research in this area. Essentially, the paper offers a comprehensive examination of the challenges and solutions related to hallucination in LFMs.
&lt;/p&gt;</description></item><item><title>SAGE&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#21313;&#20159;&#32423;&#20135;&#21697;&#30446;&#24405;&#20013;&#29983;&#25104;&#23646;&#24615;&#20540;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22788;&#29702;&#36328;&#35821;&#35328;&#12289;&#20135;&#21697;&#31867;&#22411;&#21644;&#30446;&#26631;&#23646;&#24615;&#30340;&#38382;&#39064;&#12290;&#23427;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#25512;&#26029;&#38544;&#24335;&#20351;&#29992;&#36802;&#22238;&#35821;&#35328;&#25552;&#21040;&#30340;&#23646;&#24615;&#20540;&#65292;&#24182;&#19988;&#33021;&#22815;&#39044;&#27979;&#23646;&#24615;&#30340;&#19981;&#36866;&#29992;&#24615;&#21644;&#26080;&#27861;&#20174;&#21487;&#29992;&#20449;&#24687;&#20013;&#33719;&#21462;&#23646;&#24615;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.05920</link><description>&lt;p&gt;
SAGE: &#38024;&#23545;&#21313;&#20159;&#32423;&#20135;&#21697;&#30446;&#24405;&#30340;&#32467;&#26500;&#21270;&#23646;&#24615;&#20540;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SAGE: Structured Attribute Value Generation for Billion-Scale Product Catalogs. (arXiv:2309.05920v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05920
&lt;/p&gt;
&lt;p&gt;
SAGE&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#21313;&#20159;&#32423;&#20135;&#21697;&#30446;&#24405;&#20013;&#29983;&#25104;&#23646;&#24615;&#20540;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22788;&#29702;&#36328;&#35821;&#35328;&#12289;&#20135;&#21697;&#31867;&#22411;&#21644;&#30446;&#26631;&#23646;&#24615;&#30340;&#38382;&#39064;&#12290;&#23427;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#25512;&#26029;&#38544;&#24335;&#20351;&#29992;&#36802;&#22238;&#35821;&#35328;&#25552;&#21040;&#30340;&#23646;&#24615;&#20540;&#65292;&#24182;&#19988;&#33021;&#22815;&#39044;&#27979;&#23646;&#24615;&#30340;&#19981;&#36866;&#29992;&#24615;&#21644;&#26080;&#27861;&#20174;&#21487;&#29992;&#20449;&#24687;&#20013;&#33719;&#21462;&#23646;&#24615;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;SAGE&#65292;&#19968;&#20010;&#29992;&#20110;&#22312;&#20840;&#29699;&#30005;&#23376;&#21830;&#21153;&#30446;&#24405;&#20013;&#25512;&#26029;&#20135;&#21697;&#23646;&#24615;&#20540;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#23646;&#24615;&#20540;&#39044;&#27979;&#38382;&#39064;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#36716;&#21270;&#20026;&#36328;&#35821;&#35328;&#12289;&#20135;&#21697;&#31867;&#22411;&#21644;&#30446;&#26631;&#23646;&#24615;&#30340;Seq2Seq&#25688;&#35201;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26032;&#27169;&#22411;&#26041;&#27861;&#19981;&#20877;&#23616;&#38480;&#20110;&#22312;&#39044;&#20808;&#25351;&#23450;&#30340;&#36873;&#39033;&#38598;&#20869;&#39044;&#27979;&#23646;&#24615;&#20540;&#65292;&#24182;&#19988;&#20063;&#19981;&#35201;&#27714;&#25152;&#23547;&#25214;&#30340;&#23646;&#24615;&#20540;&#22312;&#25991;&#26412;&#20013;&#26126;&#30830;&#25552;&#21450;&#12290;SAGE&#21487;&#20197;&#25512;&#26029;&#38544;&#24335;&#20351;&#29992;&#36802;&#22238;&#35821;&#35328;&#25552;&#21040;&#30340;&#23646;&#24615;&#20540;&#65292;&#25110;&#32773;&#26681;&#26412;&#19981;&#25552;&#21450;&#30340;&#24120;&#35782;&#40664;&#35748;&#24773;&#20917;&#19979;&#30340;&#23646;&#24615;&#20540;&#12290;&#27492;&#22806;&#65292;SAGE&#33021;&#22815;&#39044;&#27979;&#19968;&#20010;&#23646;&#24615;&#23545;&#20110;&#24403;&#21069;&#20135;&#21697;&#26159;&#21542;&#19981;&#36866;&#29992;&#65292;&#25110;&#32773;&#26159;&#21542;&#26080;&#27861;&#20174;&#21487;&#29992;&#20449;&#24687;&#20013;&#33719;&#21462;&#12290;SAGE&#26159;&#31532;&#19968;&#31181;&#33021;&#22815;&#22312;&#23454;&#38469;&#30005;&#23376;&#21830;&#21153;&#30446;&#24405;&#35774;&#32622;&#20013;&#22788;&#29702;&#23646;&#24615;&#20540;&#39044;&#27979;&#20219;&#21153;&#25152;&#26377;&#26041;&#38754;&#30340;&#26041;&#27861;&#12290;&#19968;&#22871;&#32508;&#21512;&#30340;...
&lt;/p&gt;
&lt;p&gt;
We introduce SAGE; a Generative LLM for inferring attribute values for products across world-wide e-Commerce catalogs. We introduce a novel formulation of the attribute-value prediction problem as a Seq2Seq summarization task, across languages, product types and target attributes. Our novel modeling approach lifts the restriction of predicting attribute values within a pre-specified set of choices, as well as, the requirement that the sought attribute values need to be explicitly mentioned in the text. SAGE can infer attribute values even when such values are mentioned implicitly using periphrastic language, or not-at-all-as is the case for common-sense defaults. Additionally, SAGE is capable of predicting whether an attribute is inapplicable for the product at hand, or non-obtainable from the available information. SAGE is the first method able to tackle all aspects of the attribute-value-prediction task as they arise in practical settings in e-Commerce catalogs. A comprehensive set o
&lt;/p&gt;</description></item><item><title>&#38543;&#26426;LLMs&#26080;&#27861;&#29702;&#35299;&#35821;&#35328;&#30340;&#21407;&#22240;&#26159;&#23427;&#20204;&#26080;&#27861;&#25552;&#20379;&#21487;&#20197;&#20381;&#36182;&#30340;&#20107;&#23454;&#20449;&#24687;&#65292;&#23427;&#20204;&#23384;&#20648;&#30340;&#35821;&#35328;&#30693;&#35782;&#22475;&#34255;&#22312;&#26080;&#24847;&#20041;&#30340;&#24494;&#29305;&#24449;&#20013;&#65292;&#24182;&#22312;&#26576;&#20123;&#35821;&#35328;&#19978;&#19979;&#25991;&#20013;&#26080;&#27861;&#36827;&#34892;&#27491;&#30830;&#25512;&#29702;&#12290;&#26412;&#25991;&#24314;&#35758;&#22312;&#31526;&#21495;&#21270;&#26041;&#27861;&#20013;&#24212;&#29992;&#26377;&#25928;&#30340;&#33258;&#19979;&#32780;&#19978;&#31574;&#30053;</title><link>http://arxiv.org/abs/2309.05918</link><description>&lt;p&gt;
&#38543;&#26426;LLMs&#26080;&#27861;&#29702;&#35299;&#35821;&#35328;&#65306;&#36208;&#21521;&#31526;&#21495;&#21270;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#26412;&#20307;&#35770;&#22522;&#20110;&#30340;LLMs
&lt;/p&gt;
&lt;p&gt;
Stochastic LLMs do not Understand Language: Towards Symbolic, Explainable and Ontologically Based LLMs. (arXiv:2309.05918v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05918
&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;LLMs&#26080;&#27861;&#29702;&#35299;&#35821;&#35328;&#30340;&#21407;&#22240;&#26159;&#23427;&#20204;&#26080;&#27861;&#25552;&#20379;&#21487;&#20197;&#20381;&#36182;&#30340;&#20107;&#23454;&#20449;&#24687;&#65292;&#23427;&#20204;&#23384;&#20648;&#30340;&#35821;&#35328;&#30693;&#35782;&#22475;&#34255;&#22312;&#26080;&#24847;&#20041;&#30340;&#24494;&#29305;&#24449;&#20013;&#65292;&#24182;&#22312;&#26576;&#20123;&#35821;&#35328;&#19978;&#19979;&#25991;&#20013;&#26080;&#27861;&#36827;&#34892;&#27491;&#30830;&#25512;&#29702;&#12290;&#26412;&#25991;&#24314;&#35758;&#22312;&#31526;&#21495;&#21270;&#26041;&#27861;&#20013;&#24212;&#29992;&#26377;&#25928;&#30340;&#33258;&#19979;&#32780;&#19978;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#30475;&#26469;&#65292;&#22260;&#32469;&#25968;&#25454;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30456;&#23545;&#25104;&#21151;&#30340;&#29378;&#28909;&#26159;&#26377;&#20123;&#35823;&#23548;&#30340;&#65292;&#21407;&#22240;&#22914;&#19979;&#65306;&#65288;i&#65289;LLMs&#19981;&#33021;&#20381;&#36182;&#20110;&#20107;&#23454;&#20449;&#24687;&#65292;&#22240;&#20026;&#23545;&#20110;LLMs&#26469;&#35828;&#65292;&#25668;&#20837;&#30340;&#25152;&#26377;&#25991;&#26412;&#65288;&#20107;&#23454;&#25110;&#38750;&#20107;&#23454;&#65289;&#37117;&#26159;&#24179;&#31561;&#30340;&#65307;&#65288;ii&#65289;&#30001;&#20110;&#23427;&#20204;&#30340;&#20122;&#31526;&#21495;&#24615;&#36136;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#35821;&#35328;&#30340;&#20219;&#20309;&#8220;&#30693;&#35782;&#8221;&#37117;&#23558;&#27704;&#36828;&#22475;&#34255;&#22312;&#25968;&#21313;&#20159;&#20010;&#24494;&#29305;&#24449;&#65288;&#26435;&#37325;&#65289;&#20013;&#65292;&#20854;&#20013;&#27809;&#26377;&#19968;&#20010;&#26412;&#36523;&#26159;&#26377;&#24847;&#20041;&#30340;&#65307;&#20197;&#21450;&#65288;iii&#65289;LLMs&#22312;&#20960;&#31181;&#35821;&#35328;&#19978;&#19979;&#25991;&#20013;&#24120;&#24120;&#26080;&#27861;&#36827;&#34892;&#27491;&#30830;&#25512;&#29702;&#65288;&#22914;&#21517;&#35789;&#22797;&#21512;&#35789;&#12289;&#20849;&#35859;&#35789;&#12289;&#37327;&#35789;&#33539;&#22260;&#27169;&#31946;&#21644;&#24847;&#21521;&#24615;&#19978;&#19979;&#25991;&#65289;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30456;&#23545;&#25104;&#21151;&#19981;&#26159;&#31526;&#21495;&#19982;&#20122;&#31526;&#21495;&#20043;&#36777;&#30340;&#21453;&#26144;&#65292;&#32780;&#26159;&#22312;&#35268;&#27169;&#19978;&#24212;&#29992;&#33258;&#19979;&#32780;&#19978;&#30340;&#36870;&#21521;&#24037;&#31243;&#35821;&#35328;&#30340;&#25104;&#21151;&#31574;&#30053;&#30340;&#21453;&#26144;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#26377;&#25928;&#30340;&#33258;&#19979;&#32780;&#19978;&#31574;&#30053;&#24212;&#29992;&#20110;&#31526;&#21495;&#21270;&#26041;&#27861;&#20013;
&lt;/p&gt;
&lt;p&gt;
In our opinion the exuberance surrounding the relative success of data-driven large language models (LLMs) is slightly misguided and for several reasons (i) LLMs cannot be relied upon for factual information since for LLMs all ingested text (factual or non-factual) was created equal; (ii) due to their subsymbolic na-ture, whatever 'knowledge' these models acquire about language will always be buried in billions of microfeatures (weights), none of which is meaningful on its own; and (iii) LLMs will often fail to make the correct inferences in several linguistic contexts (e.g., nominal compounds, copredication, quantifier scope ambi-guities, intensional contexts. Since we believe the relative success of data-driven large language models (LLMs) is not a reflection on the symbolic vs. subsymbol-ic debate but a reflection on applying the successful strategy of a bottom-up reverse engineering of language at scale, we suggest in this paper applying the effective bottom-up strategy in a symbol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.05833</link><description>&lt;p&gt;
PACE: &#20351;&#29992;GPT-4&#36827;&#34892;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#20013;&#30340;&#25552;&#31034;&#21644;&#22686;&#21152;&#20197;&#36827;&#34892;&#26657;&#20934;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
PACE: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis. (arXiv:2309.05833v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;IT&#34892;&#19994;&#21521;&#22522;&#20110;&#20113;&#30340;&#24179;&#21488;&#30340;&#36716;&#21464;&#24378;&#35843;&#20102;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#30830;&#20445;&#26381;&#21153;&#30340;&#21487;&#38752;&#24615;&#21644;&#32500;&#25252;&#23458;&#25143;&#20449;&#20219;&#12290;&#26680;&#24515;&#38382;&#39064;&#26159;&#26377;&#25928;&#30830;&#23450;&#26681;&#26412;&#21407;&#22240;&#65292;&#30001;&#20110;&#24403;&#20195;&#20113;&#22522;&#30784;&#35774;&#26045;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#19968;&#20219;&#21153;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#20986;&#29616;&#20102;&#35768;&#22810;&#29992;&#20110;&#26681;&#26412;&#21407;&#22240;&#35782;&#21035;&#30340;&#22522;&#20110;AI&#30340;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#20173;&#21463;&#21040;&#20854;&#36755;&#20986;&#36136;&#37327;&#19981;&#19968;&#33268;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#27169;&#22411;&#26681;&#25454;&#21382;&#21490;&#20107;&#20214;&#25968;&#25454;&#35780;&#20272;&#33258;&#36523;&#30340;&#32622;&#20449;&#24230;&#65292;&#32771;&#34385;&#20854;&#23545;&#35777;&#25454;&#30340;&#35780;&#20272;&#24378;&#24230;&#12290;&#28982;&#21518;&#65292;&#27169;&#22411;&#23457;&#26680;&#30001;&#39044;&#27979;&#22120;&#29983;&#25104;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#28982;&#21518;&#65292;&#20248;&#21270;&#27493;&#39588;&#23558;&#36825;&#20123;&#35780;&#20272;&#32467;&#21512;&#36215;&#26469;&#30830;&#23450;&#26368;&#32456;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the transition to cloud-based platforms in the IT sector has emphasized the significance of cloud incident root cause analysis to ensure service reliability and maintain customer trust. Central to this process is the efficient determination of root causes, a task made challenging due to the complex nature of contemporary cloud infrastructures. Despite the proliferation of AI-driven tools for root cause identification, their applicability remains limited by the inconsistent quality of their outputs. This paper introduces a method for enhancing confidence estimation in root cause analysis tools by prompting retrieval-augmented large language models (LLMs). This approach operates in two phases. Initially, the model evaluates its confidence based on historical incident data, considering its assessment of the evidence strength. Subsequently, the model reviews the root cause generated by the predictor. An optimization step then combines these evaluations to determine the fin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#29983;&#25104;&#25439;&#22833;&#20989;&#25968;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#26041;&#27861;&#20013;&#35789;&#27719;&#21305;&#37197;&#21644;&#32570;&#23569;&#19978;&#19979;&#25991;&#32771;&#34385;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.05804</link><description>&lt;p&gt;
&#29983;&#25104;&#8220;nice&#8221;&#32780;&#19981;&#26159;&#29983;&#25104;&#8220;good&#8221;&#19981;&#20687;&#29983;&#25104;&#8220;rice&#8221;&#37027;&#20040;&#31967;&#31957;&#65281;&#26397;&#30528;&#19978;&#19979;&#25991;&#21644;&#35821;&#20041;&#34701;&#21512;&#30340;&#23545;&#35805;&#29983;&#25104;&#25439;&#22833;&#20989;&#25968;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hi Model, generating 'nice' instead of 'good' is not as bad as generating 'rice'! Towards Context and Semantic Infused Dialogue Generation Loss Function and Evaluation Metric. (arXiv:2309.05804v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05804
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#29983;&#25104;&#25439;&#22833;&#20989;&#25968;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#26041;&#27861;&#20013;&#35789;&#27719;&#21305;&#37197;&#21644;&#32570;&#23569;&#19978;&#19979;&#25991;&#32771;&#34385;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20108;&#21313;&#24180;&#20013;&#65292;&#23545;&#35805;&#24314;&#27169;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20174;&#31616;&#21333;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#22238;&#31572;&#21457;&#23637;&#21040;&#20010;&#24615;&#21270;&#21644;&#26377;&#35828;&#26381;&#21147;&#30340;&#22238;&#31572;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#20123;&#36827;&#23637;&#65292;&#23545;&#35805;&#29983;&#25104;&#30340;&#30446;&#26631;&#20989;&#25968;&#21644;&#35780;&#20272;&#25351;&#26631;&#20173;&#28982;&#20572;&#28382;&#19981;&#21069;&#65292;&#20998;&#21035;&#26159;&#20132;&#21449;&#29109;&#21644;BLEU&#12290;&#36825;&#20123;&#22522;&#20110;&#35789;&#27719;&#30340;&#25351;&#26631;&#23384;&#22312;&#20197;&#19979;&#20027;&#35201;&#38480;&#21046;&#65306;(a)&#27809;&#26377;&#35821;&#20041;&#32771;&#34385;&#30340;&#35789;&#23545;&#35789;&#21305;&#37197;&#65306;&#23427;&#23558;&#29983;&#25104;&#8220;nice&#8221;&#21644;&#29983;&#25104;&#8220;rice&#8221;&#20316;&#20026;&#8220;good&#8221;&#30340;&#22833;&#36133;&#32479;&#19968;&#32771;&#34385;&#12290;(b)&#32570;&#23569;&#20026;&#35780;&#20272;&#29983;&#25104;&#30340;&#22238;&#31572;&#25552;&#20379;&#19978;&#19979;&#25991;&#23646;&#24615;&#65306;&#21363;&#20351;&#29983;&#25104;&#30340;&#22238;&#31572;&#19982;&#27491;&#22312;&#36827;&#34892;&#30340;&#23545;&#35805;&#19978;&#19979;&#25991;&#30456;&#20851;&#65292;&#22914;&#26524;&#19981;&#19982;&#35821;&#26009;&#24211;&#20013;&#25552;&#20379;&#30340;&#40644;&#37329;&#35805;&#35821;&#21305;&#37197;&#65292;&#20173;&#28982;&#21487;&#33021;&#21463;&#21040;&#24809;&#32602;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20840;&#38754;&#35843;&#26597;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19978;&#19979;&#25991;&#35821;&#20041;&#34701;&#21512;&#23545;&#35805;(SemTextualLogue)&#25439;&#22833;&#20989;&#25968;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21046;&#23450;&#20102;&#19968;&#22871;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;
&lt;/p&gt;
&lt;p&gt;
Over the past two decades, dialogue modeling has made significant strides, moving from simple rule-based responses to personalized and persuasive response generation. However, despite these advancements, the objective functions and evaluation metrics for dialogue generation have remained stagnant, i.e., cross-entropy and BLEU, respectively. These lexical-based metrics have the following key limitations: (a) word-to-word matching without semantic consideration: It assigns the same credit for failure to generate 'nice' and 'rice' for 'good'. (b) missing context attribute for evaluating the generated response: Even if a generated response is relevant to the ongoing dialogue context, it may still be penalized for not matching the gold utterance provided in the corpus. In this paper, we first investigate these limitations comprehensively and propose a new loss function called Semantic Infused Contextualized diaLogue (SemTextualLogue) loss function. Furthermore, we formulate a new evaluation
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21152;&#24378;&#21644;&#21152;&#36895;&#23545;P vs. NP&#38382;&#39064;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#33487;&#26684;&#25289;&#24213;&#25512;&#29702;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28145;&#20837;&#24605;&#32771;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#12290;&#22312;P vs. NP&#38382;&#39064;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;GPT-4&#25104;&#21151;&#20135;&#29983;&#20102;&#35777;&#26126;&#26550;&#26500;&#65292;&#24182;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#25512;&#29702;&#65292;&#24471;&#20986;&#20102;"P &#8800; NP"&#30340;&#32467;&#35770;&#65292;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.05689</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#31185;&#23398;&#30740;&#31350;&#65306;&#23545;P vs. NP&#38382;&#39064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Model for Science: A Study on P vs. NP. (arXiv:2309.05689v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21152;&#24378;&#21644;&#21152;&#36895;&#23545;P vs. NP&#38382;&#39064;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#33487;&#26684;&#25289;&#24213;&#25512;&#29702;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28145;&#20837;&#24605;&#32771;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#12290;&#22312;P vs. NP&#38382;&#39064;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;GPT-4&#25104;&#21151;&#20135;&#29983;&#20102;&#35777;&#26126;&#26550;&#26500;&#65292;&#24182;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#25512;&#29702;&#65292;&#24471;&#20986;&#20102;"P &#8800; NP"&#30340;&#32467;&#35770;&#65292;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#21644;&#21152;&#36895;&#23545;P vs. NP&#38382;&#39064;&#30340;&#30740;&#31350;&#65292;&#36825;&#26159;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#25968;&#23398;&#20013;&#26368;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#20043;&#19968;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33487;&#26684;&#25289;&#24213;&#25512;&#29702;&#65288;Socratic reasoning&#65289;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#19982;LLMs&#36827;&#34892;&#28145;&#20837;&#24605;&#32771;&#26469;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#12290;&#33487;&#26684;&#25289;&#24213;&#25512;&#29702;&#40723;&#21169;LLMs&#36882;&#24402;&#22320;&#21457;&#29616;&#12289;&#35299;&#20915;&#21644;&#25972;&#21512;&#38382;&#39064;&#65292;&#21516;&#26102;&#20419;&#36827;&#33258;&#25105;&#35780;&#20272;&#21644;&#25913;&#36827;&#12290;&#25105;&#20204;&#23545;P vs. NP&#38382;&#39064;&#30340;&#35797;&#28857;&#30740;&#31350;&#34920;&#26126;&#65292;GPT-4&#22312;97&#20010;&#23545;&#35805;&#22238;&#21512;&#20013;&#25104;&#21151;&#22320;&#20135;&#29983;&#20102;&#19968;&#20010;&#35777;&#26126;&#26550;&#26500;&#65292;&#24182;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#25512;&#29702;&#65292;&#24471;&#20986;&#20102;&#8220;P &#8800; NP&#8221;&#30340;&#32467;&#35770;&#65292;&#36825;&#19982;&#65288;Xu and Zhou, 2023&#65289;&#30340;&#32467;&#35770;&#19968;&#33268;&#12290;&#35813;&#35843;&#26597;&#22312;LLMs&#30340;&#24191;&#27867;&#35299;&#31354;&#38388;&#20013;&#25581;&#31034;&#20102;&#26032;&#30340;&#27934;&#23519;&#21147;&#65292;&#20026;LLM&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we use large language models (LLMs) to augment and accelerate research on the P versus NP problem, one of the most important open problems in theoretical computer science and mathematics. Specifically, we propose Socratic reasoning, a general framework that promotes in-depth thinking with LLMs for complex problem-solving. Socratic reasoning encourages LLMs to recursively discover, solve, and integrate problems while facilitating self-evaluation and refinement. Our pilot study on the P vs. NP problem shows that GPT-4 successfully produces a proof schema and engages in rigorous reasoning throughout 97 dialogue turns, concluding "P $\neq$ NP", which is in alignment with (Xu and Zhou, 2023). The investigation uncovers novel insights within the extensive solution space of LLMs, shedding light on LLM for Science.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#20013;&#20351;&#29992;ChatGPT&#29983;&#25104;&#25991;&#26412;&#30340;&#20154;&#36896;&#25991;&#26412;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#20102;&#20351;&#29992;CNN/DailyMail&#26032;&#38395;&#25991;&#31456;&#39044;&#35757;&#32451;&#30340;RoBERTa&#21644;&#20351;&#29992;&#30456;&#21516;&#25991;&#31456;&#39044;&#35757;&#32451;&#30340;ChatGPT&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.05668</link><description>&lt;p&gt;
&#30740;&#31350;&#20351;&#29992;ChatGPT&#29983;&#25104;&#25991;&#26412;&#36827;&#34892;&#39044;&#35757;&#32451;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Studying the impacts of pre-training using ChatGPT-generated text on downstream tasks. (arXiv:2309.05668v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#20013;&#20351;&#29992;ChatGPT&#29983;&#25104;&#25991;&#26412;&#30340;&#20154;&#36896;&#25991;&#26412;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#20102;&#20351;&#29992;CNN/DailyMail&#26032;&#38395;&#25991;&#31456;&#39044;&#35757;&#32451;&#30340;RoBERTa&#21644;&#20351;&#29992;&#30456;&#21516;&#25991;&#31456;&#39044;&#35757;&#32451;&#30340;ChatGPT&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#20174;&#20114;&#32852;&#32593;&#26723;&#26696;&#20013;&#25552;&#21462;&#30340;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20123;LLM&#65292;&#20363;&#22914;ChatGPT&#65292;&#24050;&#24191;&#27867;&#21487;&#29992;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#20026;&#21508;&#31181;&#30446;&#30340;&#29983;&#25104;&#25991;&#26412;&#65292;&#21253;&#25324;&#25991;&#31456;&#12289;&#35770;&#25991;&#12289;&#31505;&#35805;&#21644;&#35799;&#27468;&#12290;&#30001;&#20110;LLM&#26159;&#22312;&#28085;&#30422;Reddit&#21644;Twitter&#31561;&#24179;&#21488;&#30340;&#21508;&#31181;&#25991;&#26412;&#26469;&#28304;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#21487;&#20197;&#39044;&#35265;&#26410;&#26469;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#36824;&#23558;&#21253;&#21547;&#21069;&#20960;&#20010;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#37492;&#20110;&#27492;&#21457;&#23637;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#20013;&#20154;&#36896;&#25991;&#26412;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#20351;&#29992;CNN/DailyMail&#26032;&#38395;&#25991;&#31456;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;RoBERTa&#21644;&#19968;&#20010;&#20351;&#29992;&#30456;&#21516;&#25991;&#31456;&#36827;&#34892;&#35757;&#32451;&#30340;ChatGPT&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent times, significant advancements have been witnessed in the field of language models, particularly with the emergence of Large Language Models (LLMs) that are trained on vast amounts of data extracted from internet archives. These LLMs, such as ChatGPT, have become widely accessible, allowing users to generate text for various purposes including articles, essays, jokes, and poetry. Given that LLMs are trained on a diverse range of text sources, encompassing platforms like Reddit and Twitter, it is foreseeable that future training datasets will also incorporate text generated by previous iterations of the models themselves. In light of this development, our research aims to investigate the influence of artificial text in the pre-training phase of language models. Specifically, we conducted a comparative analysis between a language model, RoBERTa, pre-trained using CNN/DailyMail news articles, and ChatGPT, which employed the same articles for its training and evaluated their per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#21457;&#29616;LLMs&#22312;&#32593;&#32476;&#36816;&#32500;&#65288;NetOps&#65289;&#39046;&#22495;&#20855;&#26377;&#24378;&#22823;&#30340;&#28508;&#21147;&#24212;&#29992;&#65292;&#33021;&#22815;&#25552;&#21319;&#33258;&#21160;&#21270;&#21644;&#26234;&#33021;&#21270;&#30340;NetOps&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.05557</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32593;&#32476;&#36816;&#32500;&#33021;&#21147;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of NetOps Capability of Pre-Trained Large Language Models. (arXiv:2309.05557v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#21457;&#29616;LLMs&#22312;&#32593;&#32476;&#36816;&#32500;&#65288;NetOps&#65289;&#39046;&#22495;&#20855;&#26377;&#24378;&#22823;&#30340;&#28508;&#21147;&#24212;&#29992;&#65292;&#33021;&#22815;&#25552;&#21319;&#33258;&#21160;&#21270;&#21644;&#26234;&#33021;&#21270;&#30340;NetOps&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#22238;&#31572;&#20154;&#31867;&#35821;&#35328;&#26597;&#35810;&#65292;&#24182;&#22312;&#32593;&#32476;&#36816;&#32500;&#65288;NetOps&#65289;&#39046;&#22495;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#28508;&#21147;&#24212;&#29992;&#12290;&#30001;&#20110;&#20855;&#22791;&#22823;&#37327;&#24120;&#35782;&#30693;&#35782;&#65292;LLMs&#22312;&#25512;&#29702;&#20934;&#30830;&#24615;&#19978;&#27604;&#20256;&#32479;&#27169;&#22411;&#26356;&#22909;&#65292;&#24182;&#20855;&#26377;&#26356;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12289;&#25512;&#29702;&#33021;&#21147;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#65292;&#36825;&#20123;&#33021;&#21147;&#21487;&#33021;&#23545;&#33258;&#21160;&#21270;&#21644;&#26234;&#33021;&#21270;&#30340;NetOps&#26377;&#24040;&#22823;&#30340;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#21508;&#31181;NetOps&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#31995;&#32479;&#35780;&#20272;&#20102;&#36873;&#25321;&#30340;&#20960;&#31181;LLMs&#22312;NetOps&#39046;&#22495;&#30340;&#33021;&#21147;&#12289;&#20248;&#21183;&#21644;&#38480;&#21046;&#12290;&#35780;&#20272;&#38024;&#23545;5732&#20010;&#20851;&#20110;NetOps&#30340;&#38382;&#39064;&#36827;&#34892;&#65292;&#28085;&#30422;&#20102;26&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#36890;&#29992;&#39046;&#22495;LLMs&#65292;&#21253;&#25324;ChatGPT&#12289;LLaMA&#12289;Falcon&#31561;&#12290;&#25105;&#20204;&#36824;&#23545;&#20854;&#20013;&#19968;&#20123;LLMs&#36827;&#34892;&#20102;NetOps&#35821;&#26009;&#24211;&#30340;&#24494;&#35843;&#65292;&#24182;&#35780;&#20272;&#20102;&#32467;&#26524;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#35780;&#20272;&#26041;&#27861;&#36981;&#24490;&#24191;&#27867;&#37319;&#29992;&#30340;&#29992;&#20110;&#29983;&#25104;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can respond to human language queries and have shown powerful potential applications in network operations (NetOps). Thanks to the large amount of commonsense knowledge inherent, LLMs achieve much better inference accuracy than traditional models and emerge with strong abilities in generalization, reasoning, and code generation. These abilities may have a crucial boost to automated and intelligent NetOps. However, it remains under-explored how well LLMs perform in various NetOps tasks. In this work, we make a systematic assessment of the capabilities, strengths, and limitations of selected LLMs in the field of NetOps. The evaluation is conducted on a collection of 5,732 questions about NetOps, encompassing 26 publicly available general-domain LLMs, including ChatGPT, LLaMA, Falcon, etc. We also finetune some of these LLMs with our collected NetOps corpus and evaluate the resulting models. The evaluation method follows the widely adopted benchmarks for gener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22810;&#25991;&#26723;&#25688;&#35201;&#39046;&#22495;&#30340;&#26368;&#26032;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36890;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;LED&#22312;MS$^2$&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#20026;&#26410;&#26469;&#30340;MDS&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#21442;&#32771;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.04951</link><description>&lt;p&gt;
&#22810;&#25991;&#26723;&#25688;&#35201;&#65306;&#19968;&#39033;&#27604;&#36739;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Multi-document Summarization: A Comparative Evaluation. (arXiv:2309.04951v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22810;&#25991;&#26723;&#25688;&#35201;&#39046;&#22495;&#30340;&#26368;&#26032;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36890;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;LED&#22312;MS$^2$&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#20026;&#26410;&#26469;&#30340;MDS&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#21442;&#32771;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#22810;&#25991;&#26723;&#25688;&#35201;(MDS)&#39046;&#22495;&#30340;&#26368;&#26032;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#19981;&#21516;&#31867;&#22411;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#30740;&#31350;&#29616;&#26377;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#30830;&#23450;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25991;&#29486;&#35780;&#20272;&#65292;&#20197;&#30830;&#23450;&#26368;&#26032;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23545;BigSurvey-MDS&#21644;MS$^2$&#25968;&#25454;&#38598;&#19978;&#30340;PRIMERA&#21644;PEGASUS&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#30001;&#20110;&#39046;&#22495;&#30340;&#19981;&#21516;&#32780;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;LED&#22312;MS$^2$&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;PRIMERA&#21644;PEGASUS&#12290;&#25105;&#20204;&#20351;&#29992;ROUGE&#20998;&#25968;&#20316;&#20026;&#24615;&#33021;&#24230;&#37327;&#25351;&#26631;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#20102;&#35299;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#19981;&#36275;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#65292;&#24182;&#20026;&#19981;&#21516;&#39046;&#22495;&#20013;&#20934;&#30830;&#12289;&#40065;&#26834;&#30340;&#27169;&#22411;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#26410;&#26469;&#30340;MDS&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is aimed at evaluating state-of-the-art models for Multi-document Summarization (MDS) on different types of datasets in various domains and investigating the limitations of existing models to determine future research directions. To address this gap, we conducted an extensive literature review to identify state-of-the-art models and datasets. We analyzed the performance of PRIMERA and PEGASUS models on BigSurvey-MDS and MS$^2$ datasets, which posed unique challenges due to their varied domains. Our findings show that the General-Purpose Pre-trained Model LED outperforms PRIMERA and PEGASUS on the MS$^2$ dataset. We used the ROUGE score as a performance metric to evaluate the identified models on different datasets. Our study provides valuable insights into the models' strengths and weaknesses, as well as their applicability in different domains. This work serves as a reference for future MDS research and contributes to the development of accurate and robust models which can 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;ASR&#30340;n-best&#21015;&#34920;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#28508;&#22312;&#38480;&#21046;&#65292;&#32780;&#26080;&#38656;&#23454;&#36136;&#25913;&#21464;ASR&#21644;LLM&#30340;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.04842</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#21033;&#29992;ASR&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Exploiting ASR Uncertainty. (arXiv:2309.04842v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04842
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;ASR&#30340;n-best&#21015;&#34920;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#28508;&#22312;&#38480;&#21046;&#65292;&#32780;&#26080;&#38656;&#23454;&#36136;&#25913;&#21464;ASR&#21644;LLM&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#35201;&#22312;&#21475;&#35821;&#29702;&#35299;&#65288;SLU&#65289;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#23427;&#20204;&#24517;&#39035;&#20381;&#38752;&#29616;&#25104;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#36827;&#34892;&#36716;&#24405;&#65292;&#25110;&#32773;&#37197;&#22791;&#20869;&#32622;&#30340;&#35821;&#38899;&#27169;&#24577;&#12290;&#26412;&#25991;&#20851;&#27880;&#30340;&#26159;&#21069;&#19968;&#31181;&#24773;&#20917;&#65292;&#21363;LLM&#22312;SLU&#20219;&#21153;&#19978;&#30340;&#20934;&#30830;&#24615;&#21463;&#38480;&#20110;&#22266;&#23450;ASR&#31995;&#32479;&#22312;&#21475;&#35821;&#36755;&#20837;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#35821;&#38899;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#65292;&#20854;&#20013;&#39640;&#23383;&#35789;&#38169;&#35823;&#29575;&#21487;&#33021;&#38480;&#21046;LLM&#29702;&#35299;&#21475;&#22836;&#24847;&#22270;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#19981;&#26159;&#36890;&#36807;&#35774;&#35745;&#22797;&#26434;&#25110;&#19987;&#38376;&#30340;&#26550;&#26500;&#36861;&#27714;&#39640;&#20934;&#30830;&#24615;&#65292;&#32780;&#26159;&#22312;&#19981;&#23454;&#36136;&#25913;&#21464;&#24213;&#23618;ASR&#21644;LLM&#30340;&#24773;&#20917;&#19979;&#65292;&#30475;&#30475;&#25105;&#20204;&#33021;&#36208;&#22810;&#36828;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#28508;&#22312;&#22320;&#34987;&#22810;&#20010;&#19981;&#30456;&#20851;&#30340;&#20219;&#21153;&#20849;&#20139;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;ASR&#20551;&#35774;&#30340;n-best&#21015;&#34920;&#26469;&#25552;&#31034;LLM&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#23481;&#26131;&#20986;&#38169;&#30340;1-best&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models excel in a variety of natural language processing (NLP) tasks, to perform well on spoken language understanding (SLU) tasks, they must either rely on off-the-shelf automatic speech recognition (ASR) systems for transcription, or be equipped with an in-built speech modality. This work focuses on the former scenario, where LLM's accuracy on SLU tasks is constrained by the accuracy of a fixed ASR system on the spoken input. Specifically, we tackle speech-intent classification task, where a high word-error-rate can limit the LLM's ability to understand the spoken intent. Instead of chasing a high accuracy by designing complex or specialized architectures regardless of deployment costs, we seek to answer how far we can go without substantially changing the underlying ASR and LLM, which can potentially be shared by multiple unrelated tasks. To this end, we propose prompting the LLM with an n-best list of ASR hypotheses instead of only the error-prone 1-best hypoth
&lt;/p&gt;</description></item><item><title>FIAT&#26159;&#19968;&#31181;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#23436;&#20840;&#24494;&#35843;&#33539;&#24335;&#34701;&#21512;&#30340;&#26032;&#30340;&#23398;&#20064;&#26041;&#24335;&#65292;&#21487;&#20197;&#22312;&#26368;&#22823;&#27169;&#22411;&#19978;&#36827;&#34892;&#25351;&#20196;&#21644;&#25512;&#29702;&#65292;&#24182;&#19988;&#22312;&#36739;&#23567;&#27169;&#22411;&#19978;&#36827;&#34892;&#21442;&#25968;&#26356;&#26032;&#65292;&#32463;&#36807;&#22810;&#35821;&#35328;&#20219;&#21153;&#27979;&#35797;&#65292;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#37117;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.04663</link><description>&lt;p&gt;
FIAT: &#23558;&#23398;&#20064;&#33539;&#24335;&#19982;&#25351;&#20196;&#21152;&#36895;&#35843;&#20248;&#30456;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
FIAT: Fusing learning paradigms with Instruction-Accelerated Tuning. (arXiv:2309.04663v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04663
&lt;/p&gt;
&lt;p&gt;
FIAT&#26159;&#19968;&#31181;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#23436;&#20840;&#24494;&#35843;&#33539;&#24335;&#34701;&#21512;&#30340;&#26032;&#30340;&#23398;&#20064;&#26041;&#24335;&#65292;&#21487;&#20197;&#22312;&#26368;&#22823;&#27169;&#22411;&#19978;&#36827;&#34892;&#25351;&#20196;&#21644;&#25512;&#29702;&#65292;&#24182;&#19988;&#22312;&#36739;&#23567;&#27169;&#22411;&#19978;&#36827;&#34892;&#21442;&#25968;&#26356;&#26032;&#65292;&#32463;&#36807;&#22810;&#35821;&#35328;&#20219;&#21153;&#27979;&#35797;&#65292;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#37117;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23398;&#20064;&#33539;&#24335;&#36890;&#24120;&#20998;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#21644;&#23436;&#20840;&#24494;&#35843;&#12290;&#27599;&#31181;&#33539;&#24335;&#37117;&#26377;&#20854;&#33258;&#36523;&#30340;&#21462;&#33293;&#65292;&#36825;&#21462;&#20915;&#20110;&#21487;&#29992;&#25968;&#25454;&#12289;&#27169;&#22411;&#22823;&#23567;&#12289;&#35745;&#31639;&#25104;&#26412;&#12289;&#26131;&#29992;&#24615;&#21644;&#26368;&#32456;&#36136;&#37327;&#65292;&#20294;&#26080;&#27861;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#37117;&#34920;&#29616;&#33391;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20197;&#24378;&#35843;&#23427;&#20204;&#20043;&#38388;&#33258;&#28982;&#32852;&#31995;&#30340;&#26041;&#24335;&#25551;&#36848;&#20102;ICL&#21644;&#24494;&#35843;&#33539;&#24335;&#12290;&#22522;&#20110;&#36825;&#20123;&#32852;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FIAT&#30340;&#26032;&#23398;&#20064;&#33539;&#24335;&#65292;&#23558;&#36825;&#20123;&#33539;&#24335;&#30340;&#20248;&#28857;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#20351;&#24471;&#22312;&#26368;&#22823;&#27169;&#22411;&#19978;&#21487;&#20197;&#36827;&#34892;&#24555;&#36895;&#24037;&#31243;&#25351;&#20196;&#21644;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#65292;&#21516;&#26102;&#22312;&#21442;&#25968;&#25928;&#29575;&#35843;&#20248;&#30340;&#36739;&#23567;&#27169;&#22411;&#19978;&#20351;&#29992;&#31867;&#20284;&#30340;&#26041;&#27861;&#36827;&#34892;&#21442;&#25968;&#26356;&#26032;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22810;&#35821;&#35328;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;FIAT&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;FIAT&#22312;100-10,000&#20010;&#35757;&#32451;&#26679;&#26412;&#35268;&#27169;&#19979;&#22343;&#27604;ICL&#21644;&#24494;&#35843;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#24076;&#26395;FIAT&#33021;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#24471;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#37117;&#33021;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning paradigms for large language models (LLMs) currently tend to fall within either in-context learning (ICL) or full fine-tuning. Each of these comes with their own trade-offs based on available data, model size, compute cost, ease-of-use, and final quality with neither solution performing well across-the-board. In this article, we first describe ICL and fine-tuning paradigms in a way that highlights their natural connections. Based on these connections, we propose a new learning paradigm called FIAT that fuses the best of these paradigms together, enabling prompt-engineered instructions and chain-of-thought reasoning with the very largest models while also using similar methods to perform parameter updates on a modestly-sized LLM with parameter-efficient tuning. We evaluate FIAT's effectiveness on a variety of multilingual tasks and observe that FIAT performs better than both ICL and fine-tuning at scales ranging from 100-10,000 training examples. We hope that FIAT provides a pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALEX&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;LLMs&#35299;&#37322;&#26426;&#21046;&#26469;&#25913;&#36827;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20844;&#20849;&#21355;&#29983;&#20998;&#26512;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#24179;&#34913;&#35757;&#32451;&#35299;&#20915;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#26377;&#25928;&#21033;&#29992;&#20102;LLMs&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.04213</link><description>&lt;p&gt;
UQ&#22312;#SMM4H 2023&#19978;&#30340;&#35770;&#25991;&#65306;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#36827;&#34892;&#20844;&#20849;&#21355;&#29983;&#20998;&#26512;&#30340;ALEX&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
UQ at #SMM4H 2023: ALEX for Public Health Analysis with Social Media. (arXiv:2309.04213v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALEX&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;LLMs&#35299;&#37322;&#26426;&#21046;&#26469;&#25913;&#36827;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20844;&#20849;&#21355;&#29983;&#20998;&#26512;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#24179;&#34913;&#35757;&#32451;&#35299;&#20915;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#26377;&#25928;&#21033;&#29992;&#20102;LLMs&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#26222;&#21450;&#65292;&#19982;&#20844;&#20849;&#21355;&#29983;&#30456;&#20851;&#30340;&#27963;&#21160;&#20063;&#36234;&#26469;&#36234;&#22810;&#12290;&#30446;&#21069;&#30340;&#20844;&#20849;&#21355;&#29983;&#20998;&#26512;&#25216;&#26415;&#28041;&#21450;&#21040;&#27969;&#34892;&#30340;&#27169;&#22411;&#65292;&#22914;BERT&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#28982;&#32780;&#65292;&#20026;&#20844;&#20849;&#21355;&#29983;&#22495;&#35757;&#32451;LLMs&#30340;&#25104;&#26412;&#23588;&#20854;&#26114;&#36149;&#12290;&#27492;&#22806;&#65292;&#31038;&#20132;&#23186;&#20307;&#20013;&#36825;&#31181;&#22495;&#20869;&#25968;&#25454;&#38598;&#24448;&#24448;&#23384;&#22312;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#20026;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#21487;&#20197;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#24179;&#34913;&#35757;&#32451;&#26469;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#35774;&#32622;&#27169;&#22411;&#30340;&#24341;&#23548;&#26041;&#24335;&#26377;&#25928;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;ALEX&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;LLMs&#35299;&#37322;&#26426;&#21046;&#26469;&#25552;&#39640;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20844;&#20849;&#21355;&#29983;&#20998;&#26512;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;ALEX&#27169;&#22411;&#22312;Social Media Mining for Health 2023 &#65288;SMM4H&#65289;&#30340;&#20219;&#21153;2&#21644;&#20219;&#21153;4&#20013;&#33719;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#22312;&#20219;&#21153;1&#20013;&#24471;&#21040;&#20102;&#36739;&#39640;&#30340;&#35780;&#20998;[1]&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#24050;&#22312; https:/ /github &#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
As social media becomes increasingly popular, more and more activities related to public health emerge. Current techniques for public health analysis involve popular models such as BERT and large language models (LLMs). However, the costs of training in-domain LLMs for public health are especially expensive. Furthermore, such kinds of in-domain datasets from social media are generally imbalanced. To tackle these challenges, the data imbalance issue can be overcome by data augmentation and balanced training. Moreover, the ability of the LLMs can be effectively utilized by prompting the model properly. In this paper, a novel ALEX framework is proposed to improve the performance of public health analysis on social media by adopting an LLMs explanation mechanism. Results show that our ALEX model got the best performance among all submissions in both Task 2 and Task 4 with a high score in Task 1 in Social Media Mining for Health 2023 (SMM4H)[1]. Our code has been released at https:// github
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;CALLA&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25506;&#32034;LLMs&#20174;&#20013;&#25991;&#21307;&#23398;&#25991;&#29486;&#20013;&#33719;&#21462;&#20132;&#20114;&#24335;&#30693;&#35782;&#12290;&#36890;&#36807;&#33258;&#30001;&#23545;&#35805;&#20107;&#23454;&#26680;&#26597;&#20219;&#21153;&#65292;&#35780;&#20272;&#20102;LLMs&#25484;&#25569;&#21307;&#23398;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20107;&#23454;&#36319;&#38543;&#21709;&#24212;&#8221;&#30340;&#29616;&#35937;&#12290;&#20026;&#20102;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20154;&#24037;&#26500;&#24314;&#20102;&#20004;&#31181;&#35282;&#24230;&#30340;&#27979;&#35797;&#25968;&#25454;&#65306;&#19968;&#31181;&#19982;&#20107;&#23454;&#19968;&#33268;&#65292;&#19968;&#31181;&#19982;&#20107;&#23454;&#19981;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2309.04198</link><description>&lt;p&gt;
CALLA&#25968;&#25454;&#38598;&#65306;&#20174;&#20013;&#25991;&#21307;&#23398;&#25991;&#29486;&#20013;&#25506;&#32034;LLMs&#30340;&#20132;&#20114;&#24335;&#30693;&#35782;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
The CALLA Dataset: Probing LLMs' Interactive Knowledge Acquisition from Chinese Medical Literature. (arXiv:2309.04198v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04198
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;CALLA&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25506;&#32034;LLMs&#20174;&#20013;&#25991;&#21307;&#23398;&#25991;&#29486;&#20013;&#33719;&#21462;&#20132;&#20114;&#24335;&#30693;&#35782;&#12290;&#36890;&#36807;&#33258;&#30001;&#23545;&#35805;&#20107;&#23454;&#26680;&#26597;&#20219;&#21153;&#65292;&#35780;&#20272;&#20102;LLMs&#25484;&#25569;&#21307;&#23398;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20107;&#23454;&#36319;&#38543;&#21709;&#24212;&#8221;&#30340;&#29616;&#35937;&#12290;&#20026;&#20102;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20154;&#24037;&#26500;&#24314;&#20102;&#20004;&#31181;&#35282;&#24230;&#30340;&#27979;&#35797;&#25968;&#25454;&#65306;&#19968;&#31181;&#19982;&#20107;&#23454;&#19968;&#33268;&#65292;&#19968;&#31181;&#19982;&#20107;&#23454;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#30340;&#20852;&#36259;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#36890;&#36807;&#21307;&#23398;&#30693;&#35782;&#22270;&#26500;&#24314;&#25351;&#23548;&#24494;&#35843;&#65288;IFT&#65289;&#25968;&#25454;&#65292;&#20197;&#20016;&#23500;LLMs&#30340;&#20132;&#20114;&#24335;&#21307;&#23398;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#20016;&#23500;&#30340;&#21307;&#23398;&#30693;&#35782;&#26469;&#28304;&#30340;&#21307;&#23398;&#25991;&#29486;&#20173;&#26410;&#34987;&#24320;&#21457;&#21033;&#29992;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;CALLA&#25968;&#25454;&#38598;&#65292;&#20197;&#25506;&#32034;LLMs&#20174;&#20013;&#22269;&#21307;&#23398;&#25991;&#29486;&#20013;&#33719;&#21462;&#20132;&#20114;&#24335;&#30693;&#35782;&#12290;&#23427;&#36890;&#36807;&#33258;&#30001;&#23545;&#35805;&#20107;&#23454;&#26680;&#26597;&#20219;&#21153;&#35780;&#20272;LLMs&#25484;&#25569;&#21307;&#23398;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#31181;&#29616;&#35937;&#31216;&#20026;&#8220;&#20107;&#23454;&#36319;&#38543;&#21709;&#24212;&#8221;&#65292;LLMs&#20542;&#21521;&#20110;&#30830;&#35748;&#38382;&#39064;&#20013;&#25552;&#21040;&#30340;&#20107;&#23454;&#65292;&#24182;&#23545;&#25361;&#25112;&#36825;&#20123;&#20107;&#23454;&#34920;&#29616;&#20986;&#19981;&#24773;&#24895;&#12290;&#20026;&#28040;&#38500;&#36825;&#31181;&#29616;&#35937;&#23548;&#33268;&#30340;&#19981;&#20934;&#30830;&#35780;&#20272;&#65292;&#23545;&#20110;&#40644;&#37329;&#20107;&#23454;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#35282;&#24230;&#20154;&#24037;&#26500;&#24314;&#27979;&#35797;&#25968;&#25454;&#65306;&#19968;&#20010;&#19982;&#20107;&#23454;&#19968;&#33268;&#65292;&#19968;&#20010;&#19982;&#20107;&#23454;&#19981;&#19968;&#33268;&#12290;&#26681;&#25454;&#36825;&#20123;&#27979;&#35797;&#25968;&#25454;&#65292;&#25105;&#20204;&#20026;LLMs&#35780;&#20272;&#20854;&#23545;&#21307;&#23398;&#30693;&#35782;&#30340;&#25484;&#25569;&#33021;&#21147;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of Large Language Models (LLMs) to the medical domain has stimulated the interest of researchers. Recent studies have focused on constructing Instruction Fine-Tuning (IFT) data through medical knowledge graphs to enrich the interactive medical knowledge of LLMs. However, the medical literature serving as a rich source of medical knowledge remains unexplored. Our work introduces the CALLA dataset to probe LLMs' interactive knowledge acquisition from Chinese medical literature. It assesses the proficiency of LLMs in mastering medical knowledge through a free-dialogue fact-checking task. We identify a phenomenon called the ``fact-following response``, where LLMs tend to affirm facts mentioned in questions and display a reluctance to challenge them. To eliminate the inaccurate evaluation caused by this phenomenon, for the golden fact, we artificially construct test data from two perspectives: one consistent with the fact and one inconsistent with the fact. Drawing from the 
&lt;/p&gt;</description></item><item><title>ImageBind-LLM&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;&#32852;&#21512;&#23884;&#20837;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.03905</link><description>&lt;p&gt;
ImageBind-LLM: &#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#20248;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ImageBind-LLM: Multi-modality Instruction Tuning. (arXiv:2309.03905v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03905
&lt;/p&gt;
&lt;p&gt;
ImageBind-LLM&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;&#32852;&#21512;&#23884;&#20837;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;ImageBind&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#20248;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#35821;&#35328;&#21644;&#22270;&#20687;&#25351;&#20196;&#35843;&#20248;&#26041;&#38754;&#65292;&#19982;&#27492;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;ImageBind-LLM&#21487;&#20197;&#21709;&#24212;&#22810;&#27169;&#24577;&#26465;&#20214;&#65292;&#21253;&#25324;&#38899;&#39057;&#12289;3D&#28857;&#20113;&#12289;&#35270;&#39057;&#20197;&#21450;&#23427;&#20204;&#30340;&#23884;&#20837;&#31354;&#38388;&#31639;&#26415;&#65292;&#21482;&#38656;&#36827;&#34892;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#35757;&#32451;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#21487;&#23398;&#20064;&#30340;Bind&#32593;&#32476;&#26469;&#23545;&#40784;LLaMA&#21644;ImageBind&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#20043;&#38388;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;Bind&#32593;&#32476;&#36716;&#25442;&#30340;&#22270;&#20687;&#29305;&#24449;&#34987;&#28155;&#21152;&#21040;LLaMA&#30340;&#25152;&#26377;&#23618;&#30340;&#21333;&#35789;&#26631;&#35760;&#20013;&#65292;&#36890;&#36807;&#19968;&#20010;&#26080;&#27880;&#24847;&#21147;&#21644;&#38646;&#21021;&#22987;&#21270;&#30340;&#38376;&#25511;&#26426;&#21046;&#36880;&#27493;&#27880;&#20837;&#35270;&#35273;&#25351;&#20196;&#12290;&#22312;ImageBind&#30340;&#32852;&#21512;&#23884;&#20837;&#30340;&#24110;&#21161;&#19979;&#65292;&#31616;&#21333;&#30340;&#22270;&#20687;-&#25991;&#26412;&#35757;&#32451;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#22810;&#27169;&#24577;&#36755;&#20837;&#34987;&#36865;&#20837;&#30456;&#24212;&#30340;ImageBind&#32534;&#30721;&#22120;&#65292;&#24182;&#34987;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ImageBind-LLM, a multi-modality instruction tuning method of large language models (LLMs) via ImageBind. Existing works mainly focus on language and image instruction tuning, different from which, our ImageBind-LLM can respond to multi-modality conditions, including audio, 3D point clouds, video, and their embedding-space arithmetic by only image-text alignment training. During training, we adopt a learnable bind network to align the embedding space between LLaMA and ImageBind's image encoder. Then, the image features transformed by the bind network are added to word tokens of all layers in LLaMA, which progressively injects visual instructions via an attention-free and zero-initialized gating mechanism. Aided by the joint embedding of ImageBind, the simple image-text training enables our model to exhibit superior multi-modality instruction-following capabilities. During inference, the multi-modality inputs are fed into the corresponding ImageBind encoders, and processed by 
&lt;/p&gt;</description></item><item><title>RoDia&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#32599;&#39532;&#23612;&#20122;&#26041;&#35328;&#35782;&#21035;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;&#20116;&#20010;&#19981;&#21516;&#22320;&#21306;&#30340;2&#23567;&#26102;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#32452;&#31454;&#20105;&#27169;&#22411;&#20316;&#20026;&#26410;&#26469;&#30740;&#31350;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2309.03378</link><description>&lt;p&gt;
RoDia: &#19968;&#20221;&#29992;&#20110;&#32599;&#39532;&#23612;&#20122;&#26041;&#35328;&#35782;&#21035;&#30340;&#26032;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
RoDia: A New Dataset for Romanian Dialect Identification from Speech. (arXiv:2309.03378v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03378
&lt;/p&gt;
&lt;p&gt;
RoDia&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#32599;&#39532;&#23612;&#20122;&#26041;&#35328;&#35782;&#21035;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;&#20116;&#20010;&#19981;&#21516;&#22320;&#21306;&#30340;2&#23567;&#26102;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#32452;&#31454;&#20105;&#27169;&#22411;&#20316;&#20026;&#26410;&#26469;&#30740;&#31350;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#35328;&#35782;&#21035;&#26159;&#35821;&#38899;&#22788;&#29702;&#21644;&#35821;&#35328;&#25216;&#26415;&#20013;&#20851;&#38190;&#30340;&#20219;&#21153;&#65292;&#21487;&#20197;&#22686;&#24378;&#35832;&#22914;&#35821;&#38899;&#35782;&#21035;&#12289;&#35828;&#35805;&#20154;&#39564;&#35777;&#31561;&#21508;&#31181;&#24212;&#29992;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#24191;&#20026;&#20351;&#29992;&#30340;&#35821;&#35328;&#30340;&#26041;&#35328;&#35782;&#21035;&#19978;&#65292;&#20294;&#23545;&#20110;&#32599;&#39532;&#23612;&#20122;&#36825;&#31181;&#36164;&#28304;&#26377;&#38480;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#26041;&#35328;&#35782;&#21035;&#21364;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RoDia&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#32599;&#39532;&#23612;&#20122;&#26041;&#35328;&#35782;&#21035;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;&#12290;RoDia&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#26469;&#33258;&#32599;&#39532;&#23612;&#20122;&#20116;&#20010;&#19981;&#21516;&#22320;&#21306;&#30340;&#21508;&#31181;&#35821;&#38899;&#26679;&#26412;&#65292;&#28085;&#30422;&#20102;&#22478;&#24066;&#21644;&#20892;&#26449;&#29615;&#22659;&#65292;&#24635;&#20849;&#26377;2&#23567;&#26102;&#30340;&#25163;&#21160;&#26631;&#27880;&#35821;&#38899;&#25968;&#25454;&#12290;&#38500;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#32452;&#21487;&#20316;&#20026;&#26410;&#26469;&#30740;&#31350;&#22522;&#20934;&#30340;&#31454;&#20105;&#27169;&#22411;&#12290;&#26368;&#39640;&#24471;&#20998;&#30340;&#27169;&#22411;&#30340;&#23439;F1&#20998;&#25968;&#20026;59.83%&#65292;&#24494;F1&#20998;&#25968;&#20026;62.08%&#65292;&#35828;&#26126;&#35813;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;RoDia&#26159;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialect identification is a critical task in speech processing and language technology, enhancing various applications such as speech recognition, speaker verification, and many others. While most research studies have been dedicated to dialect identification in widely spoken languages, limited attention has been given to dialect identification in low-resource languages, such as Romanian. To address this research gap, we introduce RoDia, the first dataset for Romanian dialect identification from speech. The RoDia dataset includes a varied compilation of speech samples from five distinct regions of Romania, covering both urban and rural environments, totaling 2 hours of manually annotated speech data. Along with our dataset, we introduce a set of competitive models to be used as baselines for future research. The top scoring model achieves a macro F1 score of 59.83% and a micro F1 score of 62.08%, indicating that the task is challenging. We thus believe that RoDia is a valuable resource
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20805;&#20998;&#35757;&#32451;&#65292;&#19968;&#20010;20&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#27809;&#26377;&#35745;&#31639;&#22120;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#20197;&#20960;&#20046;100%&#30340;&#20934;&#30830;&#24230;&#25191;&#34892;&#22810;&#20301;&#25968;&#30340;&#31639;&#26415;&#36816;&#31639;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;GPT-4&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#36890;&#36807;&#22312;&#38468;&#21152;&#30340;&#22810;&#27493;&#39588;&#31639;&#26415;&#36816;&#31639;&#21644;&#25968;&#23398;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#19982;GPT-4&#22312;&#20013;&#25991;&#25968;&#23398;&#38382;&#39064;&#19978;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.03241</link><description>&lt;p&gt;
GPT&#21487;&#20197;&#22312;&#27809;&#26377;&#35745;&#31639;&#22120;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
GPT Can Solve Mathematical Problems Without a Calculator. (arXiv:2309.03241v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20805;&#20998;&#35757;&#32451;&#65292;&#19968;&#20010;20&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#27809;&#26377;&#35745;&#31639;&#22120;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#20197;&#20960;&#20046;100%&#30340;&#20934;&#30830;&#24230;&#25191;&#34892;&#22810;&#20301;&#25968;&#30340;&#31639;&#26415;&#36816;&#31639;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;GPT-4&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#36890;&#36807;&#22312;&#38468;&#21152;&#30340;&#22810;&#27493;&#39588;&#31639;&#26415;&#36816;&#31639;&#21644;&#25968;&#23398;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#19982;GPT-4&#22312;&#20013;&#25991;&#25968;&#23398;&#38382;&#39064;&#19978;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#24120;&#35748;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#22312;&#27809;&#26377;&#35745;&#31639;&#22120;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#25191;&#34892;&#31639;&#26415;&#36816;&#31639;&#65292;&#29305;&#21035;&#26159;&#36229;&#36807;8&#20301;&#25968;&#23383;&#30340;&#20056;&#27861;&#65292;&#20197;&#21450;&#28041;&#21450;&#23567;&#25968;&#21644;&#20998;&#25968;&#30340;&#36816;&#31639;&#12290;&#26412;&#25991;&#26088;&#22312;&#25361;&#25112;&#36825;&#31181;&#35823;&#35299;&#12290;&#36890;&#36807;&#20805;&#20998;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#19968;&#20010;&#25317;&#26377;20&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20197;&#36817;&#20046;100%&#30340;&#20934;&#30830;&#24230;&#25191;&#34892;&#22810;&#20301;&#25968;&#30340;&#31639;&#26415;&#36816;&#31639;&#65292;&#32780;&#19988;&#27809;&#26377;&#25968;&#25454;&#27844;&#38706;&#65292;&#26174;&#33879;&#36229;&#36807;&#20102;GPT-4&#65288;&#20854;&#22810;&#20301;&#25968;&#20056;&#27861;&#20934;&#30830;&#29575;&#20165;&#20026;4.3%&#65289;&#12290;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;MathGLM&#65292;&#23427;&#26159;&#36890;&#36807;&#22312;&#21253;&#21547;&#20102;&#25991;&#26412;&#25551;&#36848;&#30340;&#38468;&#21152;&#22810;&#27493;&#39588;&#31639;&#26415;&#36816;&#31639;&#21644;&#25968;&#23398;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#19978;&#20174;GLM-10B&#24494;&#35843;&#32780;&#25104;&#30340;&#65292;&#23427;&#22312;&#19968;&#20010;&#21253;&#21547;5000&#20010;&#26679;&#26412;&#30340;&#20013;&#25991;&#25968;&#23398;&#38382;&#39064;&#27979;&#35797;&#38598;&#19978;&#30340;&#34920;&#29616;&#19982;GPT-4&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous studies have typically assumed that large language models are unable to accurately perform arithmetic operations, particularly multiplication of &gt;8 digits, and operations involving decimals and fractions, without the use of calculator tools. This paper aims to challenge this misconception. With sufficient training data, a 2 billion-parameter language model can accurately perform multi-digit arithmetic operations with almost 100% accuracy without data leakage, significantly surpassing GPT-4 (whose multi-digit multiplication accuracy is only 4.3%). We also demonstrate that our MathGLM, fine-tuned from GLM-10B on a dataset with additional multi-step arithmetic operations and math problems described in text, achieves similar performance to GPT-4 on a 5,000-samples Chinese math problem test set.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#36164;&#28304;&#24187;&#35273;&#39044;&#38450;&#26041;&#27861;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#23545;&#36755;&#20837;&#25351;&#20196;&#20013;&#27010;&#24565;&#30340;&#29087;&#24713;&#31243;&#24230;&#65292;&#22312;&#36935;&#21040;&#19981;&#29087;&#24713;&#30340;&#27010;&#24565;&#26102;&#19981;&#29983;&#25104;&#21709;&#24212;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.02654</link><description>&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#36164;&#28304;&#24187;&#35273;&#39044;&#38450;
&lt;/p&gt;
&lt;p&gt;
Zero-Resource Hallucination Prevention for Large Language Models. (arXiv:2309.02654v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#36164;&#28304;&#24187;&#35273;&#39044;&#38450;&#26041;&#27861;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#23545;&#36755;&#20837;&#25351;&#20196;&#20013;&#27010;&#24565;&#30340;&#29087;&#24713;&#31243;&#24230;&#65292;&#22312;&#36935;&#21040;&#19981;&#29087;&#24713;&#30340;&#27010;&#24565;&#26102;&#19981;&#29983;&#25104;&#21709;&#24212;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#24191;&#27867;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24341;&#36215;&#20102;&#8220;&#24187;&#35273;&#8221;&#38382;&#39064;&#30340;&#20851;&#27880;&#65292;&#36825;&#25351;&#30340;&#26159;LLMs&#29983;&#25104;&#20107;&#23454;&#19981;&#20934;&#30830;&#25110;&#27809;&#26377;&#26681;&#25454;&#30340;&#20449;&#24687;&#30340;&#24773;&#20917;&#12290;&#29616;&#26377;&#30340;&#35821;&#35328;&#21161;&#25163;&#20013;&#24187;&#35273;&#26816;&#27979;&#25216;&#26415;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#27169;&#31946;&#12289;&#22522;&#20110;&#33258;&#30001;&#35821;&#35328;&#30340;&#24605;&#32500;&#38142;&#26465;(CoT)&#25216;&#26415;&#25110;&#22522;&#20110;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#23384;&#22312;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#35782;&#21035;&#29983;&#25104;&#21518;&#24187;&#35273;&#30340;&#26041;&#27861;&#26080;&#27861;&#39044;&#38450;&#20854;&#21457;&#29983;&#65292;&#24182;&#19988;&#30001;&#20110;&#25351;&#20196;&#26684;&#24335;&#21644;&#27169;&#22411;&#39118;&#26684;&#30340;&#24433;&#21709;&#65292;&#24615;&#33021;&#19981;&#19968;&#33268;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#26816;&#27979;&#33258;&#25105;&#35780;&#20272;&#25216;&#26415;&#65292;&#31216;&#20026;{\method}&#65292;&#23427;&#19987;&#27880;&#20110;&#35780;&#20272;&#27169;&#22411;&#23545;&#36755;&#20837;&#25351;&#20196;&#20013;&#27010;&#24565;&#30340;&#29087;&#24713;&#31243;&#24230;&#65292;&#24182;&#22312;&#36935;&#21040;&#19981;&#29087;&#24713;&#30340;&#27010;&#24565;&#26102;&#19981;&#29983;&#25104;&#21709;&#24212;&#12290;&#36825;&#31181;&#26041;&#27861;&#27169;&#25311;&#20102;&#20154;&#31867;&#33021;&#22815;&#22312;&#27809;&#26377;&#25226;&#25569;&#26102;&#19981;&#20316;&#22238;&#24212;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalent use of large language models (LLMs) in various domains has drawn attention to the issue of "hallucination," which refers to instances where LLMs generate factually inaccurate or ungrounded information. Existing techniques for hallucination detection in language assistants rely on intricate fuzzy, specific free-language-based chain of thought (CoT) techniques or parameter-based methods that suffer from interpretability issues. Additionally, the methods that identify hallucinations post-generation could not prevent their occurrence and suffer from inconsistent performance due to the influence of the instruction format and model style. In this paper, we introduce a novel pre-detection self-evaluation technique, referred to as {\method}, which focuses on evaluating the model's familiarity with the concepts present in the input instruction and withholding the generation of response in case of unfamiliar concepts. This approach emulates the human ability to refrain from respond
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;AffectVisDial&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;50,000&#20010;&#22522;&#20110;&#35270;&#35273;&#30340;&#23545;&#35805;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#24773;&#24863;&#35270;&#35273;&#23545;&#35805;&#27169;&#22411;&#26469;&#35299;&#20915;&#22522;&#20110;&#23545;&#35805;&#30340;&#38382;&#31572;&#12289;&#24773;&#24863;&#39044;&#27979;&#21644;&#24773;&#24863;&#35299;&#37322;&#20219;&#21153;&#65292;&#23637;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#24773;&#24863;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16349</link><description>&lt;p&gt;
&#24773;&#24863;&#35270;&#35273;&#23545;&#35805;&#65306;&#22522;&#20110;&#35270;&#35273;&#23545;&#35805;&#29702;&#35299;&#24773;&#24863;&#24418;&#25104;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Affective Visual Dialog: A Large-Scale Benchmark for Emotional Reasoning Based on Visually Grounded Conversations. (arXiv:2308.16349v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16349
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;AffectVisDial&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;50,000&#20010;&#22522;&#20110;&#35270;&#35273;&#30340;&#23545;&#35805;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#24773;&#24863;&#35270;&#35273;&#23545;&#35805;&#27169;&#22411;&#26469;&#35299;&#20915;&#22522;&#20110;&#23545;&#35805;&#30340;&#38382;&#31572;&#12289;&#24773;&#24863;&#39044;&#27979;&#21644;&#24773;&#24863;&#35299;&#37322;&#20219;&#21153;&#65292;&#23637;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#24773;&#24863;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#24773;&#24863;&#35270;&#35273;&#23545;&#35805;&#65292;&#20316;&#20026;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#65292;&#29992;&#20110;&#30740;&#31350;&#29702;&#35299;&#22312;&#22522;&#20110;&#35270;&#35273;&#23545;&#35805;&#20013;&#24773;&#24863;&#24418;&#25104;&#30340;&#36807;&#31243;&#12290;&#36825;&#39033;&#20219;&#21153;&#28041;&#21450;&#19977;&#39033;&#25216;&#33021;&#65306;&#65288;1&#65289;&#22522;&#20110;&#23545;&#35805;&#30340;&#38382;&#31572;&#65292;&#65288;2&#65289;&#22522;&#20110;&#23545;&#35805;&#30340;&#24773;&#24863;&#39044;&#27979;&#65292;&#20197;&#21450;&#65288;3&#65289;&#22522;&#20110;&#23545;&#35805;&#29983;&#25104;&#24773;&#24863;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;AffectVisDial&#65292;&#21253;&#21547;50,000&#20010;10&#36718;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#23545;&#35805;&#65292;&#36824;&#21253;&#25324;&#24635;&#32467;&#30340;&#24773;&#24863;&#24402;&#22240;&#21644;&#22522;&#20110;&#23545;&#35805;&#30340;&#24773;&#24863;&#35299;&#37322;&#65292;&#24635;&#20849;&#38656;&#35201;27180&#20010;&#24037;&#20316;&#23567;&#26102;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#25910;&#38598;&#35813;&#25968;&#25454;&#38598;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#24182;&#20171;&#32461;&#20102;&#19982;&#23545;&#35805;&#21442;&#19982;&#32773;&#30456;&#20851;&#30340;&#25552;&#38382;&#32773;&#21644;&#22238;&#31572;&#32773;&#20219;&#21153;&#12290;&#25105;&#20204;&#35757;&#32451;&#21644;&#23637;&#31034;&#20102;&#26469;&#33258;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#22362;&#23454;&#30340;&#24773;&#24863;&#35270;&#35273;&#23545;&#35805;&#22522;&#32447;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#27169;&#22411;&#29983;&#25104;&#30340;&#22238;&#31572;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#24773;&#24863;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Affective Visual Dialog, an emotion explanation and reasoning task as a testbed for research on understanding the formation of emotions in visually grounded conversations. The task involves three skills: (1) Dialog-based Question Answering (2) Dialog-based Emotion Prediction and (3) Affective emotion explanation generation based on the dialog. Our key contribution is the collection of a large-scale dataset, dubbed AffectVisDial, consisting of 50K 10-turn visually grounded dialogs as well as concluding emotion attributions and dialog-informed textual emotion explanations, resulting in a total of 27,180 working hours. We explain our design decisions in collecting the dataset and introduce the questioner and answerer tasks that are associated with the participants in the conversation. We train and demonstrate solid Affective Visual Dialog baselines adapted from state-of-the-art models. Remarkably, the responses generated by our models show promising emotional reasoning abilit
&lt;/p&gt;</description></item><item><title>LLaSM&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#21644;&#35821;&#38899;&#27169;&#22411;&#65292;&#20855;&#26377;&#36328;&#27169;&#24577;&#23545;&#35805;&#33021;&#21147;&#65292;&#36890;&#36807;&#36981;&#24490;&#35821;&#38899;&#21644;&#35821;&#35328;&#25351;&#20196;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#20415;&#33258;&#28982;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.15930</link><description>&lt;p&gt;
LLaSM: &#22823;&#22411;&#35821;&#35328;&#21644;&#35821;&#38899;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLaSM: Large Language and Speech Model. (arXiv:2308.15930v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15930
&lt;/p&gt;
&lt;p&gt;
LLaSM&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#21644;&#35821;&#38899;&#27169;&#22411;&#65292;&#20855;&#26377;&#36328;&#27169;&#24577;&#23545;&#35805;&#33021;&#21147;&#65292;&#36890;&#36807;&#36981;&#24490;&#35821;&#38899;&#21644;&#35821;&#35328;&#25351;&#20196;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#20415;&#33258;&#28982;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#35270;&#35273;-&#35821;&#35328;&#22810;&#27169;&#24577;&#27169;&#22411;&#19978;&#65292;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#26469;&#36981;&#24490;&#35270;&#35273;&#21644;&#35821;&#35328;&#25351;&#20196;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#35821;&#38899;&#20063;&#26159;&#20154;&#31867;&#19982;&#19990;&#30028;&#20114;&#21160;&#30340;&#37325;&#35201;&#26041;&#24335;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#19968;&#20010;&#36890;&#29992;&#30340;&#21161;&#25163;&#26469;&#35828;&#65292;&#33021;&#22815;&#36981;&#24490;&#22810;&#27169;&#24577;&#35821;&#38899;&#21644;&#35821;&#35328;&#25351;&#20196;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#21644;&#35821;&#38899;&#27169;&#22411;&#65288;LLaSM&#65289;&#12290;LLaSM&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#38899;-&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#36328;&#27169;&#24577;&#23545;&#35805;&#33021;&#21147;&#65292;&#33021;&#22815;&#36981;&#24490;&#35821;&#38899;&#21644;&#35821;&#35328;&#25351;&#20196;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;LLaSM&#23637;&#31034;&#20102;&#19968;&#31181;&#26356;&#26041;&#20415;&#33258;&#28982;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#24335;&#12290;&#20026;&#20102;&#25903;&#25345;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#35821;&#38899;&#25351;&#20196;&#25968;&#25454;&#38598;LLaSM-Audio-Instructions&#12290;&#20195;&#30721;&#21644;&#28436;&#31034;&#21487;&#22312;https://github.com/LinkSoul-AI/LLaSM&#21644;ht&#19978;&#26597;&#30475;
&lt;/p&gt;
&lt;p&gt;
Multi-modal large language models have garnered significant interest recently. Though, most of the works focus on vision-language multi-modal models providing strong capabilities in following vision-and-language instructions. However, we claim that speech is also an important modality through which humans interact with the world. Hence, it is crucial for a general-purpose assistant to be able to follow multi-modal speech-and-language instructions. In this work, we propose Large Language and Speech Model (LLaSM). LLaSM is an end-to-end trained large multi-modal speech-language model with cross-modal conversational abilities, capable of following speech-and-language instructions. Our early experiments show that LLaSM demonstrates a more convenient and natural way for humans to interact with artificial intelligence. Specifically, we also release a large Speech Instruction Following dataset LLaSM-Audio-Instructions. Code and demo are available at https://github.com/LinkSoul-AI/LLaSM and ht
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38754;&#21521;Fon&#35821;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#35789;&#24615;&#26631;&#27880;&#20219;&#21153;&#19978;&#20849;&#20139;&#30693;&#35782;&#65292;&#22686;&#24378;&#27169;&#22411;&#22312;Fon&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.14280</link><description>&lt;p&gt;
FonMTL:&#38754;&#21521;Fon&#35821;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FonMTL: Towards Multitask Learning for the Fon Language. (arXiv:2308.14280v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38754;&#21521;Fon&#35821;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#35789;&#24615;&#26631;&#27880;&#20219;&#21153;&#19978;&#20849;&#20139;&#30693;&#35782;&#65292;&#22686;&#24378;&#27169;&#22411;&#22312;Fon&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Fon&#35821;&#26159;&#19968;&#31181;&#30495;&#27491;&#30340;&#20302;&#36164;&#28304;&#38750;&#27954;&#35821;&#35328;&#65292;&#22823;&#32422;&#26377;200&#19975;&#20154;&#21475;&#65292;&#20854;&#22312;&#32447;&#23384;&#22312;&#26377;&#38480;&#65292;&#24182;&#19988;&#29616;&#26377;&#25968;&#25454;&#38598;&#20063;&#24456;&#26377;&#38480;&#12290;&#22810;&#20219;&#21153;&#23398;&#20064;&#26159;&#19968;&#31181;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#19981;&#21516;&#20294;&#30456;&#20851;&#30340;&#20219;&#21153;&#38388;&#20849;&#20139;&#30693;&#35782;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36825;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#22330;&#26223;&#20013;&#23588;&#20026;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#25506;&#32034;&#24615;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;Fon&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#27169;&#22411;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;Fon&#35821;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;&#35789;&#24615;&#26631;&#27880;&#65288;POS&#65289;&#20219;&#21153;&#12290;&#25105;&#20204;&#21033;&#29992;&#20004;&#20010;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#32534;&#30721;&#22120;&#26469;&#26500;&#24314;&#36755;&#20837;&#30340;&#20849;&#20139;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;&#23618;&#22359;&#36827;&#34892;&#27599;&#20010;&#20219;&#21153;&#30340;&#20998;&#31867;&#12290;&#25105;&#20204;&#22312;Fon&#35821;&#30340;NER&#21644;POS&#20219;&#21153;&#19978;&#30340;&#32467;&#26524;&#19982;&#20960;&#20010;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#24615;&#33021;&#30456;&#27604;&#65292;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65288;&#25110;&#26356;&#22909;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Fon language, spoken by an average 2 million of people, is a truly low-resourced African language, with a limited online presence, and existing datasets (just to name but a few). Multitask learning is a learning paradigm that aims to improve the generalization capacity of a model by sharing knowledge across different but related tasks: this could be prevalent in very data-scarce scenarios. In this paper, we present the first explorative approach to multitask learning, for model capabilities enhancement in Natural Language Processing for the Fon language. Specifically, we explore the tasks of Named Entity Recognition (NER) and Part of Speech Tagging (POS) for Fon. We leverage two language model heads as encoders to build shared representations for the inputs, and we use linear layers blocks for classification relative to each task. Our results on the NER and POS tasks for Fon, show competitive (or better) performances compared to several multilingual pretrained language models finet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#22312;&#22788;&#29702;&#36880;&#28176;&#22797;&#26434;&#30340;&#31867;&#27604;&#25512;&#29702;&#26102;&#30340;&#24517;&#35201;&#24615;&#65292;&#20197;&#25552;&#20379;&#36229;&#36234;&#25991;&#23383;&#20869;&#23481;&#30340;&#24191;&#27867;&#12289;&#22810;&#26679;&#21270;&#30340;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;&#32479;&#35745;&#21644;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#22686;&#24378;&#21644;&#24341;&#23548;&#26144;&#23556;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.01936</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#25105;&#20204;&#38656;&#35201;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#26469;&#24314;&#27169;&#23454;&#29992;&#30340;&#31867;&#27604;?
&lt;/p&gt;
&lt;p&gt;
Why Do We Need Neuro-symbolic AI to Model Pragmatic Analogies?. (arXiv:2308.01936v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#22312;&#22788;&#29702;&#36880;&#28176;&#22797;&#26434;&#30340;&#31867;&#27604;&#25512;&#29702;&#26102;&#30340;&#24517;&#35201;&#24615;&#65292;&#20197;&#25552;&#20379;&#36229;&#36234;&#25991;&#23383;&#20869;&#23481;&#30340;&#24191;&#27867;&#12289;&#22810;&#26679;&#21270;&#30340;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;&#32479;&#35745;&#21644;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#22686;&#24378;&#21644;&#24341;&#23548;&#26144;&#23556;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#30340;&#19968;&#20010;&#29305;&#28857;&#26159;&#33021;&#22815;&#21033;&#29992;&#29087;&#24713;&#30340;&#39046;&#22495;&#23545;&#19981;&#37027;&#20040;&#29087;&#24713;&#30340;&#39046;&#22495;&#36827;&#34892;&#25512;&#29702;&#65292;&#21363;&#31867;&#27604;&#25512;&#29702;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#22312;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#34920;&#36798;&#30340;&#36880;&#28176;&#22797;&#26434;&#30340;&#31867;&#27604;&#26102;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22235;&#20010;&#19981;&#21516;&#22797;&#26434;&#32423;&#21035;&#30340;&#31867;&#27604;&#65306;&#35789;&#27719;&#31867;&#27604;&#12289;&#21477;&#27861;&#31867;&#27604;&#12289;&#35821;&#20041;&#31867;&#27604;&#21644;&#23454;&#29992;&#31867;&#27604;&#12290;&#38543;&#30528;&#31867;&#27604;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#23427;&#20204;&#38656;&#35201;&#36229;&#20986;&#25991;&#26412;&#20869;&#23481;&#30340;&#24191;&#27867;&#12289;&#22810;&#26679;&#21270;&#30340;&#30693;&#35782;&#65292;&#36825;&#22312;&#25903;&#25345;LLMs&#30340;&#35789;&#27719;&#20849;&#29616;&#32479;&#35745;&#20013;&#19981;&#22826;&#21487;&#33021;&#25214;&#21040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#37319;&#29992;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24517;&#35201;&#24615;&#65292;&#36825;&#20123;&#25216;&#26415;&#32467;&#21512;&#20102;&#32479;&#35745;&#21644;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#65292;&#26681;&#25454;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25552;&#20379;&#20449;&#24687;&#20197;&#31361;&#20986;&#21644;&#22686;&#24378;&#30456;&#20851;&#20869;&#23481;&#65292;&#25552;&#20379;&#25277;&#35937;&#21644;&#24341;&#23548;&#26144;&#23556;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#30693;&#35782;&#39537;&#21160;&#26041;&#27861;&#22312;&#20445;&#25345;LLMs&#30340;&#25928;&#29575;&#30340;&#21516;&#26102;&#20445;&#25345;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A hallmark of intelligence is the ability to use a familiar domain to make inferences about a less familiar domain, known as analogical reasoning. In this article, we delve into the performance of Large Language Models (LLMs) in dealing with progressively complex analogies expressed in unstructured text. We discuss analogies at four distinct levels of complexity: lexical analogies, syntactic analogies, semantic analogies, and pragmatic analogies. As the analogies become more complex, they require increasingly extensive, diverse knowledge beyond the textual content, unlikely to be found in the lexical co-occurrence statistics that power LLMs. To address this, we discuss the necessity of employing Neuro-symbolic AI techniques that combine statistical and symbolic AI, informing the representation of unstructured text to highlight and augment relevant content, provide abstraction and guide the mapping process. Our knowledge-informed approach maintains the efficiency of LLMs while preservin
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;&#36890;&#36807;&#26368;&#20339;&#36873;&#25321;&#21644;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#24773;&#24863;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20102;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#24773;&#24863;&#26816;&#27979;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.00264</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multi-Modality Multi-Loss Fusion Network. (arXiv:2308.00264v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00264
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;&#36890;&#36807;&#26368;&#20339;&#36873;&#25321;&#21644;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#24773;&#24863;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20102;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#24773;&#24863;&#26816;&#27979;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36328;&#22810;&#20010;&#27169;&#24577;&#36873;&#25321;&#21644;&#34701;&#21512;&#29305;&#24449;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#32452;&#21512;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#20197;&#25913;&#21892;&#24773;&#24863;&#26816;&#27979;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#34701;&#21512;&#26041;&#27861;&#24182;&#19988;&#30740;&#31350;&#20102;&#22810;&#25439;&#22833;&#35757;&#32451;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#20013;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#30830;&#23450;&#20102;&#19982;&#23376;&#32593;&#24615;&#33021;&#30456;&#20851;&#30340;&#26377;&#29992;&#21457;&#29616;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#27169;&#22411;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;CMU-MOSI&#12289;CMU-MOSEI&#21644;CH-SIMS&#65289;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22823;&#22810;&#25968;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35757;&#32451;&#22810;&#27169;&#24577;&#29305;&#24449;&#21487;&#20197;&#25913;&#21892;&#21333;&#27169;&#24577;&#27979;&#35797;&#65292;&#24182;&#19988;&#22522;&#20110;&#25968;&#25454;&#38598;&#27880;&#37322;&#27169;&#24335;&#35774;&#35745;&#34701;&#21512;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#22686;&#24378;&#24773;&#24863;&#26816;&#27979;&#30340;&#20248;&#21270;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we investigate the optimal selection and fusion of features across multiple modalities and combine these in a neural network to improve emotion detection. We compare different fusion methods and examine the impact of multi-loss training within the multi-modality fusion network, identifying useful findings relating to subnet performance. Our best model achieves state-of-the-art performance for three datasets (CMU-MOSI, CMU-MOSEI and CH-SIMS), and outperforms the other methods in most metrics. We have found that training on multimodal features improves single modality testing and designing fusion methods based on dataset annotation schema enhances model performance. These results suggest a roadmap towards an optimized feature selection and fusion approach for enhancing emotion detection in neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#35821;&#38899;&#21161;&#25163;&#22312;&#24341;&#23548;&#20154;&#20204;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#35266;&#23519;&#21442;&#19982;&#32773;&#20351;&#29992;&#35821;&#38899;&#21161;&#25163;&#36827;&#34892;&#28921;&#39274;&#65292;&#21457;&#29616;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#20061;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#27169;&#31946;&#22823;&#23616;&#12289;&#20449;&#24687;&#36807;&#36733;&#21644;&#26080;&#27861;&#20256;&#36798;&#25805;&#20316;&#24615;&#31561;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#36866;&#24212;&#25991;&#26412;&#25351;&#20196;&#30340;&#35821;&#38899;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2306.09992</link><description>&lt;p&gt;
&#37325;&#20889;&#33050;&#26412;&#65306;&#20026;&#35821;&#38899;&#20132;&#20114;&#36866;&#24212;&#25991;&#26412;&#25351;&#20196;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Rewriting the Script: Adapting Text Instructions for Voice Interaction. (arXiv:2306.09992v1 [cs.HC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#35821;&#38899;&#21161;&#25163;&#22312;&#24341;&#23548;&#20154;&#20204;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#35266;&#23519;&#21442;&#19982;&#32773;&#20351;&#29992;&#35821;&#38899;&#21161;&#25163;&#36827;&#34892;&#28921;&#39274;&#65292;&#21457;&#29616;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#20061;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#27169;&#31946;&#22823;&#23616;&#12289;&#20449;&#24687;&#36807;&#36733;&#21644;&#26080;&#27861;&#20256;&#36798;&#25805;&#20316;&#24615;&#31561;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#36866;&#24212;&#25991;&#26412;&#25351;&#20196;&#30340;&#35821;&#38899;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35821;&#38899;&#21161;&#25163;&#30340;&#20351;&#29992;&#29575;&#36805;&#36895;&#19978;&#21319;&#65292;&#20294;&#20854;&#24212;&#29992;&#33539;&#22260;&#20173;&#20027;&#35201;&#23616;&#38480;&#20110;&#31616;&#21333;&#30340;&#20219;&#21153;&#65292;&#22914;&#25773;&#25918;&#38899;&#20048;&#12289;&#26080;&#38656;&#25163;&#21160;&#25628;&#32034;&#25110;&#25511;&#21046;&#29289;&#32852;&#32593;&#35774;&#22791;&#12290;&#26412;&#25991;&#30740;&#31350;&#35821;&#38899;&#21161;&#25163;&#22312;&#24341;&#23548;&#20154;&#20204;&#23436;&#25104;&#26356;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#38480;&#21046;&#65306;&#26391;&#35835;&#25991;&#23383;&#25351;&#20196;&#12290;&#20197;&#39135;&#35889;&#20026;&#20363;&#65292;&#25105;&#20204;&#35266;&#23519;&#20102;12&#21517;&#21442;&#19982;&#32773;&#22312;&#23478;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#38899;&#21161;&#25163;&#36827;&#34892;&#28921;&#39274;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#26041;&#27861;&#23384;&#22312;9&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#27169;&#31946;&#22823;&#23616;&#12289;&#20449;&#24687;&#36807;&#36733;&#21644;&#26080;&#27861;&#20256;&#36798;&#25805;&#20316;&#24615;&#31561;&#38382;&#39064;&#12290;&#35821;&#38899;&#21161;&#25163;&#25152;&#25552;&#20379;&#30340;&#25351;&#20196;&#23588;&#20854;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#33021;&#20687;&#20070;&#38754;&#25351;&#20196;&#19968;&#26679;&#36731;&#26494;&#27983;&#35272;&#12290;&#23588;&#20854;&#26159;Alexa&#22312;&#21521;&#29992;&#25143;&#25552;&#20379;&#20851;&#38190;&#20449;&#24687;&#25110;&#22238;&#31572;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36866;&#24212;&#25991;&#26412;&#25351;&#20196;&#30340;&#35821;&#38899;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Voice assistants have sharply risen in popularity in recent years, but their use has been limited mostly to simple applications like music, hands-free search, or control of internet-of-things devices. What would it take for voice assistants to guide people through more complex tasks? In our work, we study the limitations of the dominant approach voice assistants take to complex task guidance: reading aloud written instructions. Using recipes as an example, we observe twelve participants cook at home with a state-of-the-art voice assistant. We learn that the current approach leads to nine challenges, including obscuring the bigger picture, overwhelming users with too much information, and failing to communicate affordances. Instructions delivered by a voice assistant are especially difficult because they cannot be skimmed as easily as written instructions. Alexa in particular did not surface crucial details to the user or answer questions well. We draw on our observations to propose eig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#32431;&#31929;&#30340;&#21333;&#35821;&#25968;&#25454;&#28304;&#35757;&#32451;&#21452;&#35821;&#21644;&#20195;&#30721;&#20999;&#25442;ASR&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#38598;&#21512;&#26631;&#35760;&#22120;&#65292;&#23558;LID&#24212;&#29992;&#21040;&#27599;&#20010;&#26631;&#35760;&#65292;&#32780;&#19981;&#26159;&#22312;&#21333;&#35821;&#26679;&#26412;&#36793;&#30028;&#29983;&#25104;LID&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38598;&#21512;&#26631;&#35760;&#22120;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#21512;&#25104;&#20195;&#30721;&#20999;&#25442;ASR&#25968;&#25454;&#29983;&#25104;&#25216;&#26415;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#20195;&#30721;&#20999;&#25442;ASR&#27169;&#22411;&#22312;&#35821;&#38899;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08753</link><description>&lt;p&gt;
&#20174;&#21333;&#35821;&#25968;&#25454;&#28304;&#20013;&#35757;&#32451;&#21452;&#35821;&#21644;&#20195;&#30721;&#20999;&#25442;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards training Bilingual and Code-Switched Speech Recognition models from Monolingual data sources. (arXiv:2306.08753v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#32431;&#31929;&#30340;&#21333;&#35821;&#25968;&#25454;&#28304;&#35757;&#32451;&#21452;&#35821;&#21644;&#20195;&#30721;&#20999;&#25442;ASR&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#38598;&#21512;&#26631;&#35760;&#22120;&#65292;&#23558;LID&#24212;&#29992;&#21040;&#27599;&#20010;&#26631;&#35760;&#65292;&#32780;&#19981;&#26159;&#22312;&#21333;&#35821;&#26679;&#26412;&#36793;&#30028;&#29983;&#25104;LID&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38598;&#21512;&#26631;&#35760;&#22120;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#21512;&#25104;&#20195;&#30721;&#20999;&#25442;ASR&#25968;&#25454;&#29983;&#25104;&#25216;&#26415;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#20195;&#30721;&#20999;&#25442;ASR&#27169;&#22411;&#22312;&#35821;&#38899;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#33021;&#22815;&#36716;&#24405;&#22810;&#31181;&#35821;&#35328;&#30340;&#38899;&#39057;&#65292;&#28040;&#38500;&#20102;&#20351;&#29992;&#19981;&#21516;&#27169;&#22411;&#30340;&#38656;&#35201;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36824;&#33021;&#36827;&#34892;&#35821;&#35328;&#35782;&#21035;&#65288;LID&#65289;&#21644;&#22788;&#29702;&#20195;&#30721;&#20999;&#25442;&#35821;&#38899;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#31232;&#32570;&#30340;&#20195;&#30721;&#20999;&#25442;&#21644;&#22810;&#35821;&#38899;&#25968;&#25454;&#35821;&#26009;&#24211;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;&#32431;&#31929;&#30340;&#21333;&#35821;&#25968;&#25454;&#28304;&#35757;&#32451;&#21452;&#35821;&#21644;&#20195;&#30721;&#20999;&#25442;ASR&#27169;&#22411;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#38598;&#21512;&#26631;&#35760;&#22120;&#30340;&#27010;&#24565;&#65292;&#23427;&#19982;&#30446;&#21069;&#20027;&#27969;&#25216;&#26415;&#22312;&#21333;&#35821;&#26679;&#26412;&#36793;&#30028;&#29983;&#25104;LID&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#32780;&#26159;&#20026;&#27599;&#20010;&#21457;&#23556;&#30340;&#26631;&#35760;&#29983;&#25104;LID&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#21452;&#35821;&#21644;&#21333;&#35821;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#38598;&#21512;&#26631;&#35760;&#22120;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#25104;&#30340;&#20195;&#30721;&#20999;&#25442;ASR&#25968;&#25454;&#29983;&#25104;&#25216;&#26415;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#20195;&#30721;&#20999;&#25442;ASR&#27169;&#22411;&#22312;&#35821;&#38899;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual Automatic Speech Recognition (ASR) models are capable of transcribing audios across multiple languages, eliminating the need for separate models. In addition, they can perform Language Identification (LID) and handle code-switched speech. However, training these models requires special code-switch and multilingual speech corpora which are sparsely available. In this paper, we evaluate different approaches towards training of bilingual as well as code-switched ASR models using purely monolingual data sources. We introduce the concept of aggregate tokenizers that differs from the current prevalent technique of generating LIDs at the boundaries of monolingual samples and produces LID for each emitted token instead. We compare bilingual and monolingual model performance, showcase the efficacy of aggregate tokenizers, present a synthetic code-switched ASR data generation technique and demonstrate the effectiveness of the proposed code-switched ASR models for the tasks of speech
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#28145;&#24230;&#27169;&#22359;&#21270;&#30340;&#23436;&#20840;&#26080;&#30417;&#30563;&#35821;&#38899;&#20998;&#31163;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#26377;&#30417;&#30563;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#25490;&#21015;&#38382;&#39064;&#12289;&#35828;&#35805;&#20154;&#25968;&#37327;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#21644;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#20381;&#36182;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10652</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#28145;&#24230;&#27169;&#22359;&#21270;&#30340;&#35821;&#38899;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Speech Separation based on Contrastive Learning and Deep Modularization. (arXiv:2305.10652v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#28145;&#24230;&#27169;&#22359;&#21270;&#30340;&#23436;&#20840;&#26080;&#30417;&#30563;&#35821;&#38899;&#20998;&#31163;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#26377;&#30417;&#30563;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#25490;&#21015;&#38382;&#39064;&#12289;&#35828;&#35805;&#20154;&#25968;&#37327;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#21644;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#20381;&#36182;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#35821;&#38899;&#20998;&#31163;&#30340;&#26368;&#20808;&#36827;&#24037;&#20855;&#20381;&#36182;&#20110;&#26377;&#30417;&#30563;&#23398;&#20064;&#12290;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#24517;&#39035;&#22788;&#29702;&#25490;&#21015;&#38382;&#39064;&#65292;&#23427;&#20204;&#21463;&#21040;&#35757;&#32451;&#21644;&#25512;&#26029;&#20013;&#20351;&#29992;&#30340;&#35828;&#35805;&#32773;&#25968;&#37327;&#19981;&#21305;&#37197;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#23384;&#22312;&#12290;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#37319;&#29992;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#35821;&#38899;&#20998;&#31163;&#25216;&#26415;&#26377;&#25928;&#22320;&#35299;&#20915;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#24314;&#31435;&#24103;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#22312;&#19979;&#28216;&#30340;&#28145;&#24230;&#27169;&#22359;&#21270;&#20219;&#21153;&#20013;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#35821;&#38899;&#20998;&#31163;&#20013;&#65292;&#35828;&#35805;&#20154;&#30340;&#19981;&#21516;&#24103;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#32473;&#23450;&#37027;&#20010;&#35828;&#35805;&#20154;&#30340;&#38544;&#21547;&#26631;&#20934;&#24103;&#30340;&#22686;&#24378;&#29256;&#12290;&#35828;&#35805;&#20154;&#30340;&#24103;&#21253;&#21547;&#36275;&#22815;&#30340;&#38901;&#24459;&#20449;&#24687;&#37325;&#21472;&#65292;&#36825;&#26159;&#35821;&#38899;&#20998;&#31163;&#30340;&#20851;&#38190;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#23398;&#20064;&#32553;&#23567;&#24103;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current monaural state of the art tools for speech separation relies on supervised learning. This means that they must deal with permutation problem, they are impacted by the mismatch on the number of speakers used in training and inference. Moreover, their performance heavily relies on the presence of high-quality labelled data. These problems can be effectively addressed by employing a fully unsupervised technique for speech separation. In this paper, we use contrastive learning to establish the representations of frames then use the learned representations in the downstream deep modularization task. Concretely, we demonstrate experimentally that in speech separation, different frames of a speaker can be viewed as augmentations of a given hidden standard frame of that speaker. The frames of a speaker contain enough prosodic information overlap which is key in speech separation. Based on this, we implement a self-supervised learning to learn to minimize the distance between frames
&lt;/p&gt;</description></item><item><title>PaLM 2 &#26159;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#30340;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#22810;&#35821;&#35328;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#33719;&#24471;&#20102;&#22312;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#26174;&#30528;&#30340;&#25913;&#36827;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;PaLM 2 &#36824;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#31283;&#23450;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26356;&#24191;&#27867;&#22320;&#37096;&#32626;&#65292;&#24182;&#19988;&#21487;&#20197;&#25511;&#21046;&#27602;&#24615;&#25512;&#29702;&#26102;&#38388;&#65292;&#32780;&#19981;&#20250;&#23545;&#20854;&#20182;&#33021;&#21147;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.10403</link><description>&lt;p&gt;
PaLM 2 &#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
PaLM 2 Technical Report. (arXiv:2305.10403v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10403
&lt;/p&gt;
&lt;p&gt;
PaLM 2 &#26159;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#30340;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#22810;&#35821;&#35328;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#33719;&#24471;&#20102;&#22312;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#26174;&#30528;&#30340;&#25913;&#36827;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;PaLM 2 &#36824;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#31283;&#23450;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26356;&#24191;&#27867;&#22320;&#37096;&#32626;&#65292;&#24182;&#19988;&#21487;&#20197;&#25511;&#21046;&#27602;&#24615;&#25512;&#29702;&#26102;&#38388;&#65292;&#32780;&#19981;&#20250;&#23545;&#20854;&#20182;&#33021;&#21147;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; PaLM 2&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#27604;&#20854;&#21069;&#36523; PaLM &#22312;&#22810;&#35821;&#35328;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#26356;&#21152;&#20986;&#33394;&#65292;&#24182;&#19988;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#12290;PaLM 2 &#26159;&#19968;&#31181;&#22522;&#20110; Transformer &#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#22810;&#31181;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#23545;&#33521;&#35821;&#21644;&#22810;&#35821;&#35328;&#35821;&#35328;&#20197;&#21450;&#25512;&#29702;&#20219;&#21153;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; PaLM 2 &#22312;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#20855;&#26377;&#26174;&#30528;&#30340;&#25913;&#36827;&#36136;&#37327;&#65292;&#21516;&#26102;&#23637;&#29616;&#20102;&#27604; PaLM &#26356;&#24555;&#21644;&#26356;&#26377;&#25928;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#31181;&#25913;&#36827;&#30340;&#25928;&#29575;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26356;&#24191;&#27867;&#22320;&#37096;&#32626;&#65292;&#21516;&#26102;&#20063;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26356;&#24555;&#22320;&#21709;&#24212;&#65292;&#20197;&#33719;&#24471;&#26356;&#33258;&#28982;&#30340;&#20132;&#20114;&#33410;&#22863;&#12290;PaLM 2 &#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312; BIG-Bench &#21644;&#20854;&#20182;&#25512;&#29702;&#20219;&#21153;&#19978;&#30456;&#23545;&#20110; PaLM &#26377;&#24040;&#22823;&#30340;&#25913;&#36827;&#12290;PaLM 2 &#22312;&#19968;&#22871;&#36127;&#36131;&#20154;&#30340; AI &#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#31283;&#23450;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#38468;&#21152;&#36816;&#34892;&#24320;&#38144;&#25110;&#23545;&#20854;&#20182;&#33021;&#21147;&#20135;&#29983;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23545;&#27602;&#24615;&#36827;&#34892;&#25512;&#29702;&#26102;&#38388;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture of objectives. Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Over
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;OpenAI GPT-4&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#19978;&#19979;&#25991;&#25552;&#31034;&#19982;&#22825;&#25991;&#23398;&#25991;&#29486;&#36827;&#34892;&#20132;&#20114;&#30340;&#28508;&#21147;&#12290;&#35813;&#27169;&#22411;&#21487;&#29992;&#20110;&#25552;&#20379;&#22810;&#25991;&#29486;&#19978;&#19979;&#25991;&#19979;&#30340;&#35814;&#32454;&#31572;&#26696;&#65292;&#20026;&#22825;&#25991;&#23398;&#30028;&#25506;&#32034;&#24320;&#36767;&#20102;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2304.05406</link><description>&lt;p&gt;
&#26143;&#38469;&#38386;&#32842;&#65306;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#19982;&#22825;&#25991;&#23398;&#25991;&#29486;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Galactic ChitChat: Using Large Language Models to Converse with Astronomy Literature. (arXiv:2304.05406v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;OpenAI GPT-4&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#19978;&#19979;&#25991;&#25552;&#31034;&#19982;&#22825;&#25991;&#23398;&#25991;&#29486;&#36827;&#34892;&#20132;&#20114;&#30340;&#28508;&#21147;&#12290;&#35813;&#27169;&#22411;&#21487;&#29992;&#20110;&#25552;&#20379;&#22810;&#25991;&#29486;&#19978;&#19979;&#25991;&#19979;&#30340;&#35814;&#32454;&#31572;&#26696;&#65292;&#20026;&#22825;&#25991;&#23398;&#30028;&#25506;&#32034;&#24320;&#36767;&#20102;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#21033;&#29992;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;OpenAI GPT-4&#22823;&#35821;&#35328;&#27169;&#22411;&#19982;&#22825;&#25991;&#23398;&#35770;&#25991;&#36827;&#34892;&#26377;&#24847;&#20041;&#20114;&#21160;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#33976;&#39311;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#21407;&#22987;&#36755;&#20837;&#35770;&#25991;&#30340;50\&#65285;&#65292;&#21516;&#26102;&#20445;&#25345;&#27573;&#33853;&#32467;&#26500;&#21644;&#25972;&#20307;&#35821;&#20041;&#23436;&#25972;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#25991;&#26723;&#20869;&#23481;&#36827;&#34892;&#27169;&#22411;&#30340;&#21709;&#24212;&#65288;&#21313;&#20010;&#33976;&#39311;&#36807;&#30340;&#25991;&#29486;&#65289;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#22312;&#22810;&#25991;&#26723;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#20379;&#20102;&#22312;&#30456;&#20851;&#30740;&#31350;&#21457;&#29616;&#26694;&#26550;&#20013;&#36827;&#34892;&#19978;&#19979;&#25991;&#21270;&#35814;&#32454;&#35299;&#31572;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22825;&#25991;&#23398;&#30028;&#30340;&#28508;&#21147;&#65292;&#20026;&#36827;&#19968;&#27493;&#25506;&#32034;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#65292;&#23588;&#20854;&#26159;&#21033;&#29992;&#27169;&#22411;&#36827;&#34892;&#20551;&#35774;&#29983;&#25104;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate the potential of the state-of-the-art OpenAI GPT-4 large language model to engage in meaningful interactions with Astronomy papers using in-context prompting. To optimize for efficiency, we employ a distillation technique that effectively reduces the size of the original input paper by 50\%, while maintaining the paragraph structure and overall semantic integrity. We then explore the model's responses using a multi-document context (ten distilled documents). Our findings indicate that GPT-4 excels in the multi-document domain, providing detailed answers contextualized within the framework of related research findings. Our results showcase the potential of large language models for the astronomical community, offering a promising avenue for further exploration, particularly the possibility of utilizing the models for hypothesis generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#19996;&#21335;&#20122;&#20116;&#31181;&#35821;&#35328;&#21644;Singlish&#30340;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;ChatGPT&#23637;&#29616;&#20986;&#26368;&#39640;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35789;&#27719;&#36873;&#25321;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;ChatGPT&#21644;InstructGPT&#22312;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#26102;&#30340;&#29087;&#32451;&#31243;&#24230;&#21463;&#21040;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.13592</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#30340;&#25552;&#31034;&#65306;&#19996;&#21335;&#20122;&#35821;&#35328;&#30340;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Prompting Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages. (arXiv:2303.13592v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#19996;&#21335;&#20122;&#20116;&#31181;&#35821;&#35328;&#21644;Singlish&#30340;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;ChatGPT&#23637;&#29616;&#20986;&#26368;&#39640;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35789;&#27719;&#36873;&#25321;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;ChatGPT&#21644;InstructGPT&#22312;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#26102;&#30340;&#29087;&#32451;&#31243;&#24230;&#21463;&#21040;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28151;&#21512;&#20195;&#30721;&#22312;&#19990;&#30028;&#35768;&#22810;&#22320;&#21306;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#35821;&#35328;&#23454;&#36341;&#65292;&#20294;&#25910;&#38598;&#39640;&#36136;&#37327;&#19988;&#20302;&#25104;&#26412;&#30340;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#20173;&#28982;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#36843;&#20351;&#20154;&#20204;&#38382;&#65306;&#36825;&#20123;&#31995;&#32479;&#33021;&#29992;&#20110;&#25968;&#25454;&#29983;&#25104;&#21527;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#19968;&#20010;&#38646;-shot&#30340;&#26041;&#24335;&#19979;&#22914;&#20309;&#25552;&#31034;LLMs&#20026;&#19996;&#21335;&#20122;&#65288;SEA&#65289;&#30340;&#20116;&#31181;&#35821;&#35328;&#65288;&#21360;&#23612;&#35821;&#65292;&#39532;&#26469;&#35821;&#65292;&#20013;&#25991;&#65292;&#22612;&#21152;&#36335;&#35821;&#65292;&#36234;&#21335;&#35821;&#65289;&#21450;&#20811;&#37324;&#22885;&#23572;&#35821;S ingl ish&#21019;&#36896;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;ChatGPT&#26174;&#31034;&#20986;&#26368;&#22823;&#30340;&#28508;&#21147;&#65292;&#24403;&#26126;&#30830;&#23450;&#20041;&#8220;&#28151;&#21512;&#20195;&#30721;&#8221;&#26415;&#35821;&#26102;&#65292;&#33021;&#22815;68%&#30340;&#26102;&#38388;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#12290;&#27492;&#22806;&#65292;ChatGPT&#21644;InstructGPT&#65288;davinci-003&#65289;&#29983;&#25104;S ingl ish&#25991;&#26412;&#30340;&#34920;&#29616;&#20063;&#20540;&#24471;&#27880;&#24847;&#65292;&#23427;&#20204;&#22312;&#21508;&#31181;&#25552;&#31034;&#19979;&#30340;&#25104;&#21151;&#29575;&#24179;&#22343;&#20026;96%&#12290;&#20294;&#26159;&#65292;ChatGPT&#21644;InstructGPT&#30340;&#28151;&#21512;&#20195;&#30721;&#29087;&#32451;&#31243;&#24230;&#21463;&#21040;&#35789;&#27719;&#36873;&#25321;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#35821;&#20041;&#19981;&#27491;&#30830;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
While code-mixing is a common linguistic practice in many parts of the world, collecting high-quality and low-cost code-mixed data remains a challenge for natural language processing (NLP) research. The proliferation of Large Language Models (LLMs) in recent times compels one to ask: can these systems be used for data generation? In this article, we explore prompting LLMs in a zero-shot manner to create code-mixed data for five languages in South East Asia (SEA) -Indonesian, Malay, Chinese, Tagalog, Vietnamese, as well as the creole language Singlish. We find that ChatGPT shows the most potential, capable of producing code-mixed text 68% of the time when the term "code-mixing" is explicitly defined. Moreover, both ChatGPT and InstructGPT's (davinci-003) performances in generating Singlish texts are noteworthy, averaging a 96% success rate across a variety of prompts. The code-mixing proficiency of ChatGPT and InstructGPT, however, is dampened by word choice errors that lead to semant
&lt;/p&gt;</description></item><item><title>ROSCOE&#26159;&#19968;&#22871;&#24230;&#37327;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20998;&#36880;&#27493;&#25512;&#29702;&#30340;&#27491;&#30830;&#24615;&#21644;&#36136;&#37327;&#12290;&#23427;&#21487;&#20197;&#34913;&#37327;&#35821;&#20041;&#19968;&#33268;&#24615;&#12289;&#36923;&#36753;&#24615;&#12289;&#20449;&#24687;&#37327;&#12289;&#27969;&#30021;&#24230;&#21644;&#20107;&#23454;&#31561;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.07919</link><description>&lt;p&gt;
ROSCOE: &#29992;&#20110;&#35780;&#20998;&#36880;&#27493;&#25512;&#29702;&#30340;&#19968;&#22871;&#24230;&#37327;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning. (arXiv:2212.07919v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07919
&lt;/p&gt;
&lt;p&gt;
ROSCOE&#26159;&#19968;&#22871;&#24230;&#37327;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20998;&#36880;&#27493;&#25512;&#29702;&#30340;&#27491;&#30830;&#24615;&#21644;&#36136;&#37327;&#12290;&#23427;&#21487;&#20197;&#34913;&#37327;&#35821;&#20041;&#19968;&#33268;&#24615;&#12289;&#36923;&#36753;&#24615;&#12289;&#20449;&#24687;&#37327;&#12289;&#27969;&#30021;&#24230;&#21644;&#20107;&#23454;&#31561;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35201;&#27714;&#29983;&#25104;&#36880;&#27493;&#25512;&#29702;&#26469;&#35299;&#37322;&#20854;&#26368;&#32456;&#31572;&#26696;&#26102;&#65292;&#23427;&#20204;&#22312;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#19978;&#26174;&#31034;&#20986;&#20102;&#25913;&#36827;&#12290;&#36825;&#20123;&#25512;&#29702;&#27493;&#39588;&#22823;&#22823;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#39564;&#35777;&#24615;&#65292;&#20294;&#22312;&#27809;&#26377;&#21487;&#38752;&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#29420;&#31435;&#20110;&#26368;&#32456;&#31572;&#26696;&#26469;&#30740;&#31350;&#23427;&#20204;&#30340;&#27491;&#30830;&#24615;&#26159;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#24182;&#19981;&#30693;&#36947;&#25152;&#36848;&#30340;&#25512;&#29702;&#27493;&#39588;&#23454;&#38469;&#19978;&#26377;&#22810;&#23569;&#25903;&#25345;&#26368;&#32456;&#20219;&#21153;&#39044;&#27979;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ROSCOE&#65292;&#36825;&#26159;&#19968;&#22871;&#21487;&#35299;&#37322;&#30340;&#26080;&#30417;&#30563;&#33258;&#21160;&#35780;&#20998;&#25351;&#26631;&#65292;&#23427;&#25913;&#36827;&#24182;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#25351;&#26631;&#12290;&#20026;&#20102;&#35780;&#20272;ROSCOE&#19982;&#22522;&#32447;&#25351;&#26631;&#30340;&#24046;&#24322;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#25512;&#29702;&#38169;&#35823;&#30340;&#20998;&#31867;&#65292;&#24182;&#22312;&#24120;&#29992;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#25910;&#38598;&#20102;&#21512;&#25104;&#21644;&#20154;&#24037;&#35780;&#20272;&#24471;&#20998;&#12290;&#19982;&#29616;&#26377;&#25351;&#26631;&#30456;&#27604;&#65292;ROSCOE&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#36880;&#27493;&#25512;&#29702;&#30340;&#29305;&#24615;&#26469;&#34913;&#37327;&#35821;&#20041;&#19968;&#33268;&#24615;&#12289;&#36923;&#36753;&#24615;&#12289;&#20449;&#24687;&#37327;&#12289;&#27969;&#30021;&#24230;&#21644;&#20107;&#23454;&#31561;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models show improved downstream task performance when prompted to generate step-by-step reasoning to justify their final answers. These reasoning steps greatly improve model interpretability and verification, but objectively studying their correctness (independent of the final answer) is difficult without reliable methods for automatic evaluation. We simply do not know how often the stated reasoning steps actually support the final end task predictions. In this work, we present ROSCOE, a suite of interpretable, unsupervised automatic scores that improve and extend previous text generation evaluation metrics. To evaluate ROSCOE against baseline metrics, we design a typology of reasoning errors and collect synthetic and human evaluation scores on commonly used reasoning datasets. In contrast with existing metrics, ROSCOE can measure semantic consistency, logicality, informativeness, fluency, and factuality - among other traits - by leveraging properties of step-by-step rat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#32467;&#26500;&#21270;&#30693;&#35782;&#22686;&#24378;&#30340;&#25925;&#20107;&#29983;&#25104;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#30446;&#21069;&#30340;&#26041;&#27861;&#19982;&#25216;&#26415;&#65292;&#25351;&#20986;&#20102;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#21644;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.04634</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#30693;&#35782;&#22686;&#24378;&#30340;&#24320;&#25918;&#19990;&#30028;&#25925;&#20107;&#29983;&#25104;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Open-world Story Generation with Structured Knowledge Enhancement: A Comprehensive Survey. (arXiv:2212.04634v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#32467;&#26500;&#21270;&#30693;&#35782;&#22686;&#24378;&#30340;&#25925;&#20107;&#29983;&#25104;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#30446;&#21069;&#30340;&#26041;&#27861;&#19982;&#25216;&#26415;&#65292;&#25351;&#20986;&#20102;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#21644;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35762;&#25925;&#20107;&#21644;&#21465;&#20107;&#26159;&#20154;&#31867;&#20307;&#39564;&#30340;&#22522;&#30784;&#65292;&#19982;&#25105;&#20204;&#30340;&#31038;&#20250;&#21644;&#25991;&#21270;&#21442;&#19982;&#23494;&#19981;&#21487;&#20998;&#12290;&#22240;&#27492;&#65292;&#38271;&#26399;&#20197;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#19968;&#30452;&#23581;&#35797;&#21019;&#24314;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#25925;&#20107;&#30340;&#31995;&#32479;&#12290;&#36817;&#24180;&#26469;&#65292;&#21463;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#36164;&#28304;&#30340;&#25512;&#21160;&#65292;&#33258;&#21160;&#29983;&#25104;&#25925;&#20107;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#37325;&#22823;&#25361;&#25112;&#65292;&#20363;&#22914;&#65292;&#38656;&#35201;&#22312;&#29983;&#25104;&#30340;&#25925;&#20107;&#20013;&#23454;&#29616;&#20840;&#23616;&#19968;&#33268;&#24615;&#65292;&#36825;&#20351;&#24471;&#29983;&#25104;&#27169;&#22411;&#26080;&#27861;&#36798;&#21040;&#19982;&#20154;&#31867;&#21465;&#36848;&#32773;&#30456;&#21516;&#30340;&#21465;&#20107;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#35768;&#22810;&#30740;&#31350;&#35797;&#22270;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#27880;&#20837;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#36825;&#34987;&#31216;&#20026;&#32467;&#26500;&#21270;&#30693;&#35782;&#22686;&#24378;&#30340;&#25925;&#20107;&#29983;&#25104;&#12290;&#23558;&#22806;&#37096;&#30693;&#35782;&#32435;&#20837;&#20854;&#20013;&#21487;&#20197;&#22686;&#24378;&#25925;&#20107;&#20107;&#20214;&#20043;&#38388;&#30340;&#36923;&#36753;&#36830;&#36143;&#24615;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#30693;&#35782;&#22522;&#30784;&#65292;&#24182;&#20943;&#36731;&#25925;&#20107;&#20013;&#36807;&#24230;&#27010;&#25324;&#21644;&#37325;&#22797;&#38382;&#39064;&#12290;&#26412;&#27425;&#35843;&#26597;&#25552;&#20379;&#20102;&#23545;&#35813;&#30740;&#31350;&#39046;&#22495;&#30340;&#26368;&#26032;&#21644;&#20840;&#38754;&#30340;&#22238;&#39038;&#65306;&#65288;i&#65289;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#32467;&#26500;&#21270;&#30693;&#35782;&#22686;&#24378;&#25925;&#20107;&#29983;&#25104;&#30340;&#32508;&#36848;&#65292;&#65288;ii&#65289;&#25105;&#20204;&#24635;&#32467;&#20102;&#30446;&#21069;&#30340;&#26041;&#27861;&#19982;&#25216;&#26415;&#65292;&#65288;iii&#65289;&#25105;&#20204;&#25351;&#20986;&#20102;&#23578;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#19982;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Storytelling and narrative are fundamental to human experience, intertwined with our social and cultural engagement. As such, researchers have long attempted to create systems that can generate stories automatically. In recent years, powered by deep learning and massive data resources, automatic story generation has shown significant advances. However, considerable challenges, like the need for global coherence in generated stories, still hamper generative models from reaching the same storytelling ability as human narrators. To tackle these challenges, many studies seek to inject structured knowledge into the generation process, which is referred to as structured knowledge-enhanced story generation. Incorporating external knowledge can enhance the logical coherence among story events, achieve better knowledge grounding, and alleviate over-generalization and repetition problems in stories. This survey provides the latest and comprehensive review of this research field: (i) we present a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#21270;&#30340;&#25991;&#26412;&#34164;&#28085;&#65288;TE&#65289;&#27169;&#22411;&#65288;Context-TE&#65289;&#65292;&#36890;&#36807;&#32771;&#34385;&#20854;&#20182;&#36873;&#39033;&#20316;&#20026;&#24403;&#21069;&#24314;&#27169;&#30340;&#19978;&#19979;&#25991;&#65292;&#23427;&#33021;&#22815;&#35299;&#20915;TE&#26041;&#27861;&#20013;&#30340;&#20004;&#20010;&#38480;&#21046;&#12290;&#36825;&#20010;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#21487;&#38752;&#30340;&#36873;&#39033;&#20915;&#31574;&#65292;&#24182;&#19988;&#36890;&#36807;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.00301</link><description>&lt;p&gt;
&#23398;&#20064;&#20174;&#22810;&#20010;&#36873;&#39033;&#20013;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Learning to Select from Multiple Options. (arXiv:2212.00301v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#21270;&#30340;&#25991;&#26412;&#34164;&#28085;&#65288;TE&#65289;&#27169;&#22411;&#65288;Context-TE&#65289;&#65292;&#36890;&#36807;&#32771;&#34385;&#20854;&#20182;&#36873;&#39033;&#20316;&#20026;&#24403;&#21069;&#24314;&#27169;&#30340;&#19978;&#19979;&#25991;&#65292;&#23427;&#33021;&#22815;&#35299;&#20915;TE&#26041;&#27861;&#20013;&#30340;&#20004;&#20010;&#38480;&#21046;&#12290;&#36825;&#20010;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#21487;&#38752;&#30340;&#36873;&#39033;&#20915;&#31574;&#65292;&#24182;&#19988;&#36890;&#36807;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#21487;&#20197;&#30475;&#20316;&#26159;&#20174;&#19968;&#32452;&#36873;&#39033;&#20013;&#36827;&#34892;&#36873;&#25321;&#65292;&#27604;&#22914;&#20998;&#31867;&#20219;&#21153;&#12289;&#22810;&#39033;&#36873;&#25321;&#39064;&#31561;&#12290;&#25991;&#26412;&#34164;&#28085;&#65288;TE&#65289;&#34987;&#35777;&#26126;&#26159;&#22788;&#29702;&#36825;&#20123;&#36873;&#25321;&#38382;&#39064;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;TE&#23558;&#36755;&#20837;&#25991;&#26412;&#35270;&#20026;&#21069;&#25552;&#65288;P&#65289;&#65292;&#36873;&#39033;&#35270;&#20026;&#20551;&#35774;&#65288;H&#65289;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;&#65288;P&#65292;H&#65289;&#36827;&#34892;&#37197;&#23545;&#24314;&#27169;&#26469;&#22788;&#29702;&#36873;&#25321;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;TE&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#38480;&#21046;&#65306;&#39318;&#20808;&#65292;&#37197;&#23545;&#24314;&#27169;&#26080;&#27861;&#24847;&#35782;&#21040;&#20854;&#20182;&#36873;&#39033;&#65292;&#36825;&#19981;&#22815;&#30452;&#35266;&#65292;&#22240;&#20026;&#20154;&#20204;&#24120;&#24120;&#36890;&#36807;&#27604;&#36739;&#31454;&#20105;&#20505;&#36873;&#39033;&#26469;&#30830;&#23450;&#26368;&#20339;&#36873;&#39033;&#65307;&#20854;&#27425;&#65292;&#37197;&#23545;TE&#30340;&#25512;&#29702;&#36807;&#31243;&#32791;&#26102;&#65292;&#29305;&#21035;&#26159;&#24403;&#36873;&#39033;&#31354;&#38388;&#36739;&#22823;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#30340;TE&#27169;&#22411;&#65288;Context-TE&#65289;&#65292;&#36890;&#36807;&#23558;&#20854;&#20182;k&#20010;&#36873;&#39033;&#38468;&#21152;&#20026;&#24403;&#21069;&#65288;P&#65292;H&#65289;&#24314;&#27169;&#30340;&#19978;&#19979;&#25991;&#26469;&#36827;&#34892;&#24314;&#27169;&#12290;Context-TE&#33021;&#22815;&#36890;&#36807;&#32771;&#34385;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#26469;&#23398;&#20064;&#21040;&#26356;&#21487;&#38752;&#30340;H&#20915;&#31574;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;Pa
&lt;/p&gt;
&lt;p&gt;
Many NLP tasks can be regarded as a selection problem from a set of options, such as classification tasks, multi-choice question answering, etc. Textual entailment (TE) has been shown as the state-of-the-art (SOTA) approach to dealing with those selection problems. TE treats input texts as premises (P), options as hypotheses (H), then handles the selection problem by modeling (P, H) pairwise. Two limitations: first, the pairwise modeling is unaware of other options, which is less intuitive since humans often determine the best options by comparing competing candidates; second, the inference process of pairwise TE is time-consuming, especially when the option space is large. To deal with the two issues, this work first proposes a contextualized TE model (Context-TE) by appending other k options as the context of the current (P, H) modeling. Context-TE is able to learn more reliable decision for the H since it considers various context. Second, we speed up Context-TE by coming up with Pa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#20105;&#35758;&#21477;&#23545;&#36827;&#34892;&#23454;&#39564;&#27604;&#36739;&#65292;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#20013;GPT-2&#19982;&#20154;&#31867;&#21028;&#26029;&#26368;&#20026;&#19968;&#33268;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#22833;&#36133;&#20197;&#21450;&#25214;&#20986;&#26368;&#31526;&#21512;&#20154;&#31867;&#21028;&#26029;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2204.03592</link><description>&lt;p&gt;
&#26816;&#39564;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#23545;&#20154;&#31867;&#35821;&#35328;&#21028;&#26029;&#39044;&#27979;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Testing the limits of natural language models for predicting human language judgments. (arXiv:2204.03592v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.03592
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#20105;&#35758;&#21477;&#23545;&#36827;&#34892;&#23454;&#39564;&#27604;&#36739;&#65292;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#20013;GPT-2&#19982;&#20154;&#31867;&#21028;&#26029;&#26368;&#20026;&#19968;&#33268;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#22833;&#36133;&#20197;&#21450;&#25214;&#20986;&#26368;&#31526;&#21512;&#20154;&#31867;&#21028;&#26029;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#20851;&#20110;&#20154;&#31867;&#35821;&#35328;&#22788;&#29702;&#26041;&#24335;&#30340;&#35745;&#31639;&#20551;&#35774;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#39564;&#26041;&#27861;&#23545;&#22810;&#26679;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#19968;&#33268;&#24615;&#30340;&#27604;&#36739;&#65306;&#20105;&#35758;&#21477;&#23545;&#12290;&#23545;&#20110;&#27599;&#20010;&#20105;&#35758;&#21477;&#23545;&#65292;&#20004;&#20010;&#35821;&#35328;&#27169;&#22411;&#22312;&#21738;&#20010;&#21477;&#23376;&#26356;&#21487;&#33021;&#20986;&#29616;&#22312;&#33258;&#28982;&#25991;&#26412;&#20013;&#19978;&#23384;&#22312;&#19981;&#21516;&#24847;&#35265;&#12290;&#32771;&#34385;&#21040;&#20061;&#20010;&#35821;&#35328;&#27169;&#22411;&#65288;&#21253;&#25324;n-gram&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#25442;&#22120;&#27169;&#22411;&#65289;&#65292;&#25105;&#20204;&#36890;&#36807;&#20174;&#35821;&#26009;&#24211;&#20013;&#36873;&#25321;&#21477;&#23376;&#25110;&#32773;&#21512;&#25104;&#20248;&#21270;&#21477;&#23545;&#26469;&#21019;&#24314;&#20102;&#25968;&#30334;&#20010;&#36825;&#26679;&#30340;&#20105;&#35758;&#21477;&#23545;&#12290;&#28982;&#21518;&#65292;&#20154;&#31867;&#21463;&#35797;&#32773;&#25552;&#20379;&#20102;&#21028;&#26029;&#65292;&#25351;&#31034;&#22312;&#27599;&#20010;&#21477;&#23545;&#20013;&#65292;&#21738;&#20010;&#21477;&#23376;&#26356;&#21487;&#33021;&#21457;&#29983;&#12290;&#20105;&#35758;&#21477;&#23545;&#34987;&#35777;&#26126;&#26497;&#20026;&#26377;&#25928;&#65292;&#33021;&#22815;&#25581;&#31034;&#27169;&#22411;&#30340;&#22833;&#36133;&#21644;&#35782;&#21035;&#19982;&#20154;&#31867;&#21028;&#26029;&#26368;&#20026;&#19968;&#33268;&#30340;&#27169;&#22411;&#12290;&#32463;&#36807;&#27979;&#35797;&#65292;&#26368;&#31526;&#21512;&#20154;&#31867;&#21028;&#26029;&#30340;&#27169;&#22411;&#26159;GPT-2&#65292;&#23613;&#31649;&#23454;&#39564;&#36824;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network language models can serve as computational hypotheses about how humans process language. We compared the model-human consistency of diverse language models using a novel experimental approach: controversial sentence pairs. For each controversial sentence pair, two language models disagree about which sentence is more likely to occur in natural text. Considering nine language models (including n-gram, recurrent neural networks, and transformer models), we created hundreds of such controversial sentence pairs by either selecting sentences from a corpus or synthetically optimizing sentence pairs to be highly controversial. Human subjects then provided judgments indicating for each pair which of the two sentences is more likely. Controversial sentence pairs proved highly effective at revealing model failures and identifying models that aligned most closely with human judgments. The most human-consistent model tested was GPT-2, although experiments also revealed significant s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#26126;&#30830;&#29983;&#25104;&#38544;&#21547;&#24120;&#35782;&#30693;&#35782;&#24182;&#29992;&#20110;&#21709;&#24212;&#29983;&#25104;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#26356;&#20016;&#23500;&#12289;&#26356;&#20855;&#20307;&#19988;&#36981;&#24490;&#24120;&#35782;&#30340;&#21709;&#24212;&#65292;&#32463;&#30001;&#20154;&#24037;&#26631;&#27880;&#32773;&#35780;&#20272;&#20063;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#24847;&#20041;&#19988;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2110.08501</link><description>&lt;p&gt;
&#24910;&#35328;&#32780;&#21518;&#35328;&#65306;&#26126;&#30830;&#29983;&#25104;&#38544;&#21547;&#24120;&#35782;&#30693;&#35782;&#20197;&#20379;&#21709;&#24212;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Think Before You Speak: Explicitly Generating Implicit Commonsense Knowledge for Response Generation. (arXiv:2110.08501v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.08501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#26126;&#30830;&#29983;&#25104;&#38544;&#21547;&#24120;&#35782;&#30693;&#35782;&#24182;&#29992;&#20110;&#21709;&#24212;&#29983;&#25104;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#26356;&#20016;&#23500;&#12289;&#26356;&#20855;&#20307;&#19988;&#36981;&#24490;&#24120;&#35782;&#30340;&#21709;&#24212;&#65292;&#32463;&#30001;&#20154;&#24037;&#26631;&#27880;&#32773;&#35780;&#20272;&#20063;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#24847;&#20041;&#19988;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#21547;&#30693;&#35782;&#65292;&#22914;&#24120;&#35782;&#65292;&#23545;&#20110;&#27969;&#30021;&#30340;&#20154;&#31867;&#23545;&#35805;&#24456;&#20851;&#38190;&#12290;&#24403;&#21069;&#30340;&#31070;&#32463;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#34987;&#35757;&#32451;&#25104;&#30452;&#25509;&#29983;&#25104;&#21709;&#24212;&#65292;&#24573;&#30053;&#20102;&#26410;&#26126;&#31034;&#30340;&#38544;&#21547;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24910;&#35328;&#32780;&#21518;&#35328;&#65288;TBS&#65289;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#39318;&#20808;&#22806;&#21270;&#38544;&#21547;&#24120;&#35782;&#30693;&#35782;&#65288;&#24605;&#32771;&#65289;&#65292;&#28982;&#21518;&#21033;&#29992;&#35813;&#30693;&#35782;&#29983;&#25104;&#21709;&#24212;&#65288;&#35328;&#65289;&#12290;&#25105;&#20204;&#26399;&#26395;&#22806;&#21270;&#38544;&#21547;&#30693;&#35782;&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#65292;&#20135;&#29983;&#26356;&#20016;&#23500;&#30340;&#21709;&#24212;&#65292;&#24182;&#19988;&#23454;&#29616;&#26356;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#37319;&#38598;&#30693;&#35782;&#23545;&#40784;&#23545;&#35805;&#12289;&#34920;&#31034;&#38544;&#21547;&#30693;&#35782;&#20197;&#21450;&#22312;&#30693;&#35782;&#21644;&#23545;&#35805;&#20043;&#38388;&#36807;&#28193;&#30340;&#19981;&#21516;&#36873;&#25321;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;TBS&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#19978;&#20248;&#20110;&#31471;&#21040;&#31471;&#21644;&#22686;&#21152;&#30693;&#35782;&#30340;&#21709;&#24212;&#29983;&#25104;&#22522;&#20934;&#65292;&#24182;&#19988;&#29983;&#25104;&#26356;&#20016;&#23500;&#12289;&#26356;&#20855;&#20307;&#19988;&#36981;&#24490;&#24120;&#35782;&#30340;&#21709;&#24212;&#65292;&#32463;&#30001;&#20154;&#24037;&#26631;&#27880;&#32773;&#35780;&#20272;&#20063;&#29983;&#25104;&#20855;&#26377;&#24847;&#20041;&#19988;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit knowledge, such as common sense, is key to fluid human conversations. Current neural response generation (RG) models are trained to generate responses directly, omitting unstated implicit knowledge. In this paper, we present Think-Before-Speaking (TBS), a generative approach to first externalize implicit commonsense knowledge (think) and use this knowledge to generate responses (speak). We expect that externalizing implicit knowledge allows more efficient learning, produces more informative responses, and enables more explainable models. We analyze different choices to collect knowledge-aligned dialogues, represent implicit knowledge, and transition between knowledge and dialogues. Empirical results show TBS models outperform end-to-end and knowledge-augmented RG baselines on most automatic metrics and generate more informative, specific, and commonsense-following responses, as evaluated by human annotators. TBS also generates knowledge that makes sense and is relevant to the 
&lt;/p&gt;</description></item></channel></rss>