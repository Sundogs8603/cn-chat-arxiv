<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#24247;&#31185;&#36842;&#20122;&#26694;&#26550;&#26159;&#19968;&#20010;&#25903;&#25345;&#21508;&#31181;&#27010;&#29575;&#29702;&#35770;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#38480;&#21046;&#30340;&#24182;&#34892;&#30340;&#31070;&#32463;&#31526;&#21495;&#32467;&#26500;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#38598;&#21512;&#27963;&#21160;&#26816;&#27979;&#12289;&#23454;&#20307;&#38142;&#25509;&#21644;&#25512;&#33616;&#20219;&#21153;&#65292;&#25552;&#39640;&#20102;&#26368;&#26032;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00480</link><description>&lt;p&gt;
&#19982;&#24247;&#31185;&#36842;&#20122;&#24182;&#34892;&#30340;&#31070;&#32463;&#31526;&#21495;&#19968;&#20307;&#21270;
&lt;/p&gt;
&lt;p&gt;
Parallel Neurosymbolic Integration with Concordia. (arXiv:2306.00480v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00480
&lt;/p&gt;
&lt;p&gt;
&#24247;&#31185;&#36842;&#20122;&#26694;&#26550;&#26159;&#19968;&#20010;&#25903;&#25345;&#21508;&#31181;&#27010;&#29575;&#29702;&#35770;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#38480;&#21046;&#30340;&#24182;&#34892;&#30340;&#31070;&#32463;&#31526;&#21495;&#32467;&#26500;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#38598;&#21512;&#27963;&#21160;&#26816;&#27979;&#12289;&#23454;&#20307;&#38142;&#25509;&#21644;&#25512;&#33616;&#20219;&#21153;&#65292;&#25552;&#39640;&#20102;&#26368;&#26032;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24182;&#34892;&#30340;&#31070;&#32463;&#31526;&#21495;&#32467;&#26500;&#36890;&#36807;&#23558;&#36923;&#36753;&#29702;&#35770;&#30340;&#30693;&#35782;&#25552;&#21462;&#21040;&#28145;&#24230;&#27169;&#22411;&#20013;&#65292;&#22312;NLP&#20013;&#24471;&#21040;&#26377;&#25928;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25216;&#26415;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#25903;&#25345;&#21463;&#38480;&#24418;&#24335;&#30340;&#36923;&#36753;&#29702;&#35770;&#65292;&#24182;&#20381;&#36182;&#20110;&#36923;&#36753;&#21644;&#28145;&#24230;&#32593;&#32476;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#20551;&#35774;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#24247;&#31185;&#36842;&#20122;&#26694;&#26550;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#38480;&#21046;&#12290;&#24247;&#31185;&#36842;&#20122;&#26694;&#26550;&#26082;&#19981;&#20851;&#27880;&#28145;&#24230;&#32593;&#32476;&#65292;&#20063;&#19981;&#20851;&#27880;&#36923;&#36753;&#29702;&#35770;&#65292;&#25903;&#25345;&#21508;&#31181;&#27010;&#29575;&#29702;&#35770;&#12290;&#26694;&#26550;&#21487;&#20197;&#25903;&#25345;&#20004;&#20010;&#32452;&#20214;&#30340;&#30417;&#30563;&#35757;&#32451;&#21644;&#31070;&#32463;&#32452;&#20214;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#12290;&#24247;&#31185;&#36842;&#20122;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;NLP&#21644;&#25968;&#25454;&#20998;&#31867;&#20197;&#22806;&#30340;&#20219;&#21153;&#65292;&#25552;&#39640;&#20102;&#38598;&#21512;&#27963;&#21160;&#26816;&#27979;&#12289;&#23454;&#20307;&#38142;&#25509;&#21644;&#25512;&#33616;&#20219;&#21153;&#30340;&#26368;&#26032;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parallel neurosymbolic architectures have been applied effectively in NLP by distilling knowledge from a logic theory into a deep model.However, prior art faces several limitations including supporting restricted forms of logic theories and relying on the assumption of independence between the logic and the deep network. We present Concordia, a framework overcoming the limitations of prior art. Concordia is agnostic both to the deep network and the logic theory offering support for a wide range of probabilistic theories. Our framework can support supervised training of both components and unsupervised training of the neural component. Concordia has been successfully applied to tasks beyond NLP and data classification, improving the accuracy of state-of-the-art on collective activity detection, entity linking and recommendation tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23581;&#35797;&#23454;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#21487;&#36870;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24494;&#35843;&#65292;&#24182;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#24494;&#35843;&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2306.00477</link><description>&lt;p&gt;
&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#21487;&#36870;&#24615;&#65306;&#20174;&#21442;&#25968;&#21040;&#20869;&#23384;&#39640;&#25928;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning. (arXiv:2306.00477v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23581;&#35797;&#23454;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#21487;&#36870;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24494;&#35843;&#65292;&#24182;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#24494;&#35843;&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#38750;&#24120;&#25104;&#21151;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35757;&#32451;&#23569;&#37327;&#21442;&#25968;&#32780;&#19981;&#20250;&#38477;&#20302;&#24615;&#33021;&#65292;&#24182;&#38543;&#30528;PLM&#36234;&#26469;&#36234;&#22823;&#32780;&#25104;&#20026;&#20107;&#23454;&#19978;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PEFT&#26041;&#27861;&#19981;&#20855;&#22791;&#20869;&#23384;&#25928;&#29575;&#65292;&#22240;&#20026;&#23427;&#20204;&#20173;&#38656;&#35201;&#23384;&#20648;&#22823;&#37096;&#20998;&#20013;&#38388;&#28608;&#27963;&#20540;&#20197;&#20415;&#35745;&#31639;&#26799;&#24230;&#65292;&#31867;&#20284;&#20110;&#24494;&#35843;&#12290;&#19968;&#20010;&#20943;&#23569;&#28608;&#27963;&#20869;&#23384;&#30340;&#26377;&#25928;&#26041;&#27861;&#26159;&#24212;&#29992;&#21487;&#36870;&#27169;&#22411;&#65292;&#36825;&#26679;&#20013;&#38388;&#28608;&#27963;&#20540;&#23601;&#26080;&#38656;&#32531;&#23384;&#65292;&#21487;&#20197;&#37325;&#26032;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#23558;PLM&#20462;&#25913;&#20026;&#23427;&#30340;&#21487;&#36870;&#21464;&#20307;&#24182;&#36827;&#34892;PEFT&#24182;&#19981;&#26159;&#19968;&#20214;&#23481;&#26131;&#30340;&#20107;&#65292;&#22240;&#20026;&#21487;&#36870;&#27169;&#22411;&#20855;&#26377;&#19982;&#24403;&#21069;&#21457;&#24067;&#30340;PLM&#19981;&#21516;&#30340;&#20307;&#31995;&#32467;&#26500;&#12290;&#26412;&#25991;&#39318;&#20808;&#35843;&#26597;&#29616;&#26377;PEFT&#26041;&#27861;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#35748;&#35782;&#21040;&#22312;&#21021;&#22987;&#21270;PEFT&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs) has emerged as a highly successful approach, with training only a small number of parameters without sacrificing performance and becoming the de-facto learning paradigm with the increasing size of PLMs. However, existing PEFT methods are not memory-efficient, because they still require caching most of the intermediate activations for the gradient calculation, akin to fine-tuning. One effective way to reduce the activation memory is to apply a reversible model, so the intermediate activations are not necessary to be cached and can be recomputed. Nevertheless, modifying a PLM to its reversible variant with PEFT is not straightforward, since the reversible model has a distinct architecture from the currently released PLMs. In this paper, we first investigate what is a key factor for the success of existing PEFT methods, and realize that it's essential to preserve the PLM's starting point when initializing a PEFT 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#21508;&#21521;&#24322;&#24615;&#21644;&#24322;&#24120;&#20540;&#32500;&#24230;&#65292;&#38024;&#23545;&#36328;&#35821;&#35328;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#12290;&#21457;&#29616;&#36328;&#35821;&#35328;&#30456;&#20284;&#24615;&#30446;&#26631;&#19978;&#30340;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;&#34920;&#31034;&#30340;&#21508;&#21521;&#21516;&#24615;&#65292;&#20294;&#19981;&#20250;&#32479;&#19968;&#20943;&#23569;&#24322;&#24120;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.00458</link><description>&lt;p&gt;
&#25506;&#31350;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21508;&#21521;&#24322;&#24615;&#21644;&#24322;&#24120;&#20540;&#65292;&#38024;&#23545;&#36328;&#35821;&#35328;&#35821;&#20041;&#21477;&#23376;&#30456;&#20284;&#24230;&#36827;&#34892;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring Anisotropy and Outliers in Multilingual Language Models for Cross-Lingual Semantic Sentence Similarity. (arXiv:2306.00458v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00458
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#21508;&#21521;&#24322;&#24615;&#21644;&#24322;&#24120;&#20540;&#32500;&#24230;&#65292;&#38024;&#23545;&#36328;&#35821;&#35328;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#12290;&#21457;&#29616;&#36328;&#35821;&#35328;&#30456;&#20284;&#24615;&#30446;&#26631;&#19978;&#30340;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;&#34920;&#31034;&#30340;&#21508;&#21521;&#21516;&#24615;&#65292;&#20294;&#19981;&#20250;&#32479;&#19968;&#20943;&#23569;&#24322;&#24120;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#34920;&#31034;&#27604;&#38745;&#24577;&#31867;&#22411;&#23884;&#20837;&#26356;&#20855;&#21508;&#21521;&#24322;&#24615;&#65292;&#24182;&#19988;&#36890;&#24120;&#26174;&#31034;&#24322;&#24120;&#20540;&#32500;&#24230;&#12290;&#34429;&#28982;&#23545;&#20110;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#37117;&#26159;&#22914;&#27492;&#65292;&#20294;&#22312;&#22810;&#35821;&#35328;&#24773;&#22659;&#19979;&#30340;&#30740;&#31350;&#36824;&#36828;&#19981;&#22815;&#12290;&#20026;&#20160;&#20040;&#20250;&#20986;&#29616;&#36825;&#20123;&#24322;&#24120;&#20540;&#24182;&#19988;&#23427;&#20204;&#22914;&#20309;&#24433;&#21709;&#34920;&#31034;&#20173;&#26159;&#30740;&#31350;&#30340;&#27963;&#36291;&#39046;&#22495;&#12290;&#25105;&#20204;&#35843;&#26597;&#22810;&#20010;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#24322;&#24120;&#20540;&#32500;&#24230;&#21450;&#20854;&#19982;&#21508;&#21521;&#24322;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#36328;&#35821;&#35328;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#65292;&#22240;&#20026;&#36825;&#20123;&#26159;&#35780;&#20272;&#22810;&#35821;&#35328;&#34920;&#31034;&#33258;&#28982;&#30340;&#20219;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#21477;&#23376;&#34920;&#31034;&#12290;&#22312;&#24179;&#34892;&#36164;&#28304;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#21477;&#23376;&#36716;&#25442;&#22120;&#22312;&#27492;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#25105;&#20204;&#23637;&#31034;&#23427;&#20204;&#30340;&#34920;&#31034;&#26356;&#21508;&#21521;&#21516;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24635;&#20307;&#25913;&#21892;&#22810;&#35821;&#35328;&#34920;&#31034;&#12290;&#25105;&#20204;&#35843;&#26597;&#36890;&#36807;&#22312;&#36328;&#35821;&#35328;&#30446;&#26631;&#19978;&#35757;&#32451;&#21487;&#20943;&#23569;&#22810;&#23569;&#21508;&#21521;&#24322;&#24615;&#21644;&#24322;&#24120;&#20540;&#65292;&#20197;&#21450;&#36825;&#26679;&#20570;&#22914;&#20309;&#24433;&#21709;&#36328;&#35821;&#35328;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36328;&#35821;&#35328;&#30456;&#20284;&#24615;&#30446;&#26631;&#19978;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;&#34920;&#31034;&#30340;&#21508;&#21521;&#21516;&#24615;&#65292;&#20294;&#19981;&#20250;&#32479;&#19968;&#20943;&#23569;&#24322;&#24120;&#20540;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#20854;&#20013;&#19968;&#20123;&#35821;&#35328;&#26174;&#31034;&#20986;&#27604;&#20854;&#20182;&#35821;&#35328;&#26356;&#22823;&#30340;&#24322;&#24120;&#20540;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous work has shown that the representations output by contextual language models are more anisotropic than static type embeddings, and typically display outlier dimensions. This seems to be true for both monolingual and multilingual models, although much less work has been done on the multilingual context. Why these outliers occur and how they affect the representations is still an active area of research. We investigate outlier dimensions and their relationship to anisotropy in multiple pre-trained multilingual language models. We focus on cross-lingual semantic similarity tasks, as these are natural tasks for evaluating multilingual representations. Specifically, we examine sentence representations. Sentence transformers which are fine-tuned on parallel resources (that are not always available) perform better on this task, and we show that their representations are more isotropic. However, we aim to improve multilingual representations in general. We investigate how much of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#20420;&#35821;&#25991;&#26412;&#30340;&#22823;&#25968;&#25454;&#26041;&#27861;&#65292;&#26088;&#22312;&#26816;&#27979;&#25991;&#26412;&#20013;&#30340;&#35773;&#21050;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#32452;&#21487;&#29992;&#20110;&#23624;&#25240;&#21644;&#25991;&#26412;&#21512;&#25104;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#32593;&#32476;&#26381;&#21153;&#20013;&#23454;&#29616;&#12290;&#23545;&#36825;&#20123;&#31639;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27979;&#35797;&#21644;&#27604;&#36739;&#65292;&#21516;&#26102;&#29992;&#20110;&#20272;&#35745;&#20420;&#35821;&#19981;&#21516;&#35789;&#24615;&#30340;&#24418;&#24577;&#21464;&#24322;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00445</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#22823;&#25968;&#25454;&#30340;&#20420;&#35821;&#35773;&#21050;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A big data approach towards sarcasm detection in Russian. (arXiv:2306.00445v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#20420;&#35821;&#25991;&#26412;&#30340;&#22823;&#25968;&#25454;&#26041;&#27861;&#65292;&#26088;&#22312;&#26816;&#27979;&#25991;&#26412;&#20013;&#30340;&#35773;&#21050;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#32452;&#21487;&#29992;&#20110;&#23624;&#25240;&#21644;&#25991;&#26412;&#21512;&#25104;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#32593;&#32476;&#26381;&#21153;&#20013;&#23454;&#29616;&#12290;&#23545;&#36825;&#20123;&#31639;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27979;&#35797;&#21644;&#27604;&#36739;&#65292;&#21516;&#26102;&#29992;&#20110;&#20272;&#35745;&#20420;&#35821;&#19981;&#21516;&#35789;&#24615;&#30340;&#24418;&#24577;&#21464;&#24322;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#38024;&#23545;&#20420;&#32599;&#26031;&#35821;&#35328;&#23624;&#25240;&#21644;&#33258;&#21160;&#25991;&#26412;&#21512;&#25104;&#30340;&#30830;&#23450;&#24615;&#31639;&#27861;&#65292;&#24182;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#32593;&#32476;&#26381;&#21153;www.passare.ru&#20013;&#23454;&#29616;&#12290;&#35813;&#26381;&#21153;&#25552;&#20379;&#20102;&#21333;&#35789;&#23624;&#25240;&#12289;&#35789;&#21305;&#37197;&#21644;&#29983;&#25104;&#35821;&#27861;&#27491;&#30830;&#30340;&#20420;&#35821;&#25991;&#26412;&#30340;&#21151;&#33021;&#12290;&#25152;&#36873;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/passare-ru/PassareFunctions/&#20013;&#33719;&#24471;&#12290;&#38024;&#23545;OpenCorpora&#27880;&#37322;&#35821;&#26009;&#24211;&#23545;&#23624;&#25240;&#21151;&#33021;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#19982;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#29992;&#20110;&#20272;&#35745;&#20420;&#35821;&#19981;&#21516;&#35789;&#24615;&#30340;&#24418;&#24577;&#21464;&#24322;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a set of deterministic algorithms for Russian inflection and automated text synthesis. These algorithms are implemented in a publicly available web-service www.passare.ru. This service provides functions for inflection of single words, word matching and synthesis of grammatically correct Russian text. Selected code and datasets are available at https://github.com/passare-ru/PassareFunctions/ Performance of the inflectional functions has been tested against the annotated corpus of Russian language OpenCorpora, compared with that of other solutions, and used for estimating the morphological variability and complexity of different parts of speech in Russian.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26032;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#21160;&#20462;&#25913;&#24615;&#21035;&#26292;&#21147;&#25551;&#36848;&#26469;&#25913;&#21464;&#35835;&#32773;&#23545;&#20110;&#26045;&#26292;&#32773;&#36131;&#20219;&#31243;&#24230;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#22810;&#31181;&#26041;&#27861;&#36827;&#34892;&#21477;&#23376;&#37325;&#20889;&#65292;&#24182;&#37319;&#29992;&#38382;&#21367;&#30740;&#31350;&#21644;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#23545;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.00437</link><description>&lt;p&gt;
&#24847;&#22823;&#21033;&#22899;&#24615;&#34987;&#23475;&#26032;&#38395;&#30340;&#36131;&#20219;&#35270;&#35282;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Responsibility Perspective Transfer for Italian Femicide News. (arXiv:2306.00437v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26032;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#21160;&#20462;&#25913;&#24615;&#21035;&#26292;&#21147;&#25551;&#36848;&#26469;&#25913;&#21464;&#35835;&#32773;&#23545;&#20110;&#26045;&#26292;&#32773;&#36131;&#20219;&#31243;&#24230;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#22810;&#31181;&#26041;&#27861;&#36827;&#34892;&#21477;&#23376;&#37325;&#20889;&#65292;&#24182;&#37319;&#29992;&#38382;&#21367;&#30740;&#31350;&#21644;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#23545;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#30340;&#35821;&#35328;&#34920;&#36848;&#21487;&#20197;&#23548;&#33268;&#23545;&#30495;&#23454;&#19990;&#30028;&#20107;&#20214;&#30340;&#19981;&#21516;&#29702;&#35299;&#12290;&#20197;&#24448;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#24615;&#21035;&#26292;&#21147;&#25551;&#36848;&#20250;&#24433;&#21709;&#35835;&#32773;&#23545;&#20110;&#26292;&#21147;&#34892;&#20026;&#35841;&#24212;&#25215;&#25285;&#36131;&#20219;&#30340;&#29702;&#35299;&#65292;&#36827;&#32780;&#24378;&#21270;&#23558;&#21463;&#23475;&#20154;&#35270;&#20026;&#37096;&#20998;&#26377;&#36131;&#20219;&#30340;&#21051;&#26495;&#21360;&#35937;&#12290;&#20026;&#20102;&#25552;&#39640;&#20154;&#20204;&#23545;&#20110;&#20889;&#20316;&#35270;&#35282;&#30340;&#24847;&#35782;&#65292;&#20419;&#36827;&#23545;&#20110;&#19981;&#21516;&#35270;&#35282;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#39033;&#26032;&#20219;&#21153;&#65292;&#21363;&#33258;&#21160;&#20462;&#25913;&#24615;&#21035;&#26292;&#21147;&#25551;&#36848;&#65292;&#20197;&#25913;&#21464;&#23545;&#20110;&#26045;&#26292;&#32773;&#36131;&#20219;&#31243;&#24230;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20934;&#24179;&#34892;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#23545;&#20110;&#26045;&#26292;&#32773;&#36131;&#20219;&#24863;&#20302;&#21644;&#39640;&#30340;&#21477;&#23376;&#65292;&#24182;&#37319;&#29992;&#26080;&#30417;&#30563;&#65288;&#22522;&#20110;mBART&#65289;&#12289;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#65288;&#22522;&#20110;GPT3&#65289;&#26041;&#27861;&#36827;&#34892;&#21477;&#23376;&#37325;&#20889;&#12290;&#25105;&#20204;&#20351;&#29992;&#38382;&#21367;&#30740;&#31350;&#21644;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Different ways of linguistically expressing the same real-world event can lead to different perceptions of what happened. Previous work has shown that different descriptions of gender-based violence (GBV) influence the reader's perception of who is to blame for the violence, possibly reinforcing stereotypes which see the victim as partly responsible, too. As a contribution to raise awareness on perspective-based writing, and to facilitate access to alternative perspectives, we introduce the novel task of automatically rewriting GBV descriptions as a means to alter the perceived level of responsibility on the perpetrator. We present a quasi-parallel dataset of sentences with low and high perceived responsibility levels for the perpetrator, and experiment with unsupervised (mBART-based), zero-shot and few-shot (GPT3-based) methods for rewriting sentences. We evaluate our models using a questionnaire study and a suite of automatic metrics.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35774;&#35745;&#20998;&#31867;&#27861;&#26469;&#20998;&#31867;&#24120;&#35265;&#22810;&#31572;&#26696;&#38405;&#35835;&#29702;&#35299;&#23454;&#20363;&#65292;&#20998;&#26512;&#27169;&#22411;&#33539;&#20363;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#22810;&#31572;&#26696;&#23454;&#20363;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#26576;&#20123;&#27169;&#22411;&#33539;&#20363;&#26356;&#25797;&#38271;&#25429;&#33719;&#38382;&#39064;&#20851;&#38190;&#20449;&#24687;&#65292;&#32780;&#20854;&#20182;&#33539;&#20363;&#21017;&#26356;&#36866;&#21512;&#24314;&#31435;&#38382;&#39064;&#21644;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#29983;&#25104;&#27169;&#22411;&#25972;&#21512;&#19981;&#21516;&#33539;&#20363;&#30340;&#20248;&#21183;&#65292;&#20855;&#26377;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.00435</link><description>&lt;p&gt;
&#25105;&#24212;&#35813;&#32473;&#20960;&#20010;&#31572;&#26696;&#65311;&#22810;&#31572;&#26696;&#38405;&#35835;&#29702;&#35299;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How Many Answers Should I Give? An Empirical Study of Multi-Answer Reading Comprehension. (arXiv:2306.00435v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00435
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35774;&#35745;&#20998;&#31867;&#27861;&#26469;&#20998;&#31867;&#24120;&#35265;&#22810;&#31572;&#26696;&#38405;&#35835;&#29702;&#35299;&#23454;&#20363;&#65292;&#20998;&#26512;&#27169;&#22411;&#33539;&#20363;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#22810;&#31572;&#26696;&#23454;&#20363;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#26576;&#20123;&#27169;&#22411;&#33539;&#20363;&#26356;&#25797;&#38271;&#25429;&#33719;&#38382;&#39064;&#20851;&#38190;&#20449;&#24687;&#65292;&#32780;&#20854;&#20182;&#33539;&#20363;&#21017;&#26356;&#36866;&#21512;&#24314;&#31435;&#38382;&#39064;&#21644;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#29983;&#25104;&#27169;&#22411;&#25972;&#21512;&#19981;&#21516;&#33539;&#20363;&#30340;&#20248;&#21183;&#65292;&#20855;&#26377;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#31572;&#26696;&#29616;&#35937;&#24847;&#21619;&#30528;&#38382;&#39064;&#21487;&#33021;&#22312;&#25991;&#26723;&#20013;&#26377;&#22810;&#20010;&#31572;&#26696;&#65292;&#20154;&#31867;&#21487;&#20197;&#22788;&#29702;&#24471;&#24456;&#22909;&#65292;&#20294;&#23545;&#20110;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#31995;&#32479;&#26469;&#35828;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#26469;&#20998;&#31867;&#24120;&#35265;&#30340;&#22810;&#31572;&#26696;&#38405;&#35835;&#29702;&#35299;&#23454;&#20363;&#65292;&#24182;&#20998;&#26512;&#22810;&#31572;&#26696;&#25361;&#25112;&#30340;&#26469;&#28304;&#65292;&#21516;&#26102;&#20998;&#26512;&#20102;&#19981;&#21516;&#27169;&#22411;&#33539;&#20363;&#22914;&#20309;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#22810;&#31572;&#26696;&#23454;&#20363;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26576;&#20123;&#33539;&#20363;&#24456;&#22909;&#22320;&#25429;&#33719;&#20102;&#38382;&#39064;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#32780;&#21478;&#19968;&#20123;&#33539;&#20363;&#21017;&#26356;&#22909;&#22320;&#24314;&#31435;&#20102;&#38382;&#39064;&#21644;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21033;&#29992;&#19981;&#21516;&#27169;&#22411;&#33539;&#20363;&#30340;&#20248;&#21183;&#30340;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#25104;&#27169;&#22411;&#26159;&#25972;&#21512;&#19981;&#21516;&#27169;&#22411;&#33539;&#20363;&#30340;&#26377;&#21069;&#36884;&#30340;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
The multi-answer phenomenon, where a question may have multiple answers scattered in the document, can be well handled by humans but is challenging enough for machine reading comprehension (MRC) systems. Despite recent progress in multi-answer MRC, there lacks a systematic analysis of how this phenomenon arises and how to better address it. In this work, we design a taxonomy to categorize commonly-seen multi-answer MRC instances, with which we inspect three multi-answer datasets and analyze where the multi-answer challenge comes from. We further analyze how well different paradigms of current multi-answer MRC models deal with different types of multi-answer instances. We find that some paradigms capture well the key information in the questions while others better model the relationship between questions and contexts. We thus explore strategies to make the best of the strengths of different paradigms. Experiments show that generation models can be a promising platform to incorporate di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#8220;&#20998;&#35299;&#12289;&#24449;&#26381;&#21644;&#32452;&#21512;&#8221;&#35299;&#32806;&#35821;&#20041;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19987;&#23478;&#28151;&#21512;&#26426;&#21046;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#26174;&#33879;&#24182;&#19968;&#33268;&#22320;&#25552;&#39640;&#20102;&#38754;&#21521;&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00434</link><description>&lt;p&gt;
&#20998;&#35299;&#12289;&#24449;&#26381;&#21644;&#32452;&#21512;&#65306;&#38754;&#21521;&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#30340;&#28151;&#21512;&#26080;&#35821;&#20041;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
Divide, Conquer, and Combine: Mixture of Semantic-Independent Experts for Zero-Shot Dialogue State Tracking. (arXiv:2306.00434v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#8220;&#20998;&#35299;&#12289;&#24449;&#26381;&#21644;&#32452;&#21512;&#8221;&#35299;&#32806;&#35821;&#20041;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19987;&#23478;&#28151;&#21512;&#26426;&#21046;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#26174;&#33879;&#24182;&#19968;&#33268;&#22320;&#25552;&#39640;&#20102;&#38754;&#21521;&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#65288;DST&#65289;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#26377;&#21161;&#20110;&#22788;&#29702;&#21508;&#31181;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#39046;&#22495;&#65292;&#32780;&#26080;&#38656;&#25910;&#38598;&#39046;&#22495;&#20869;&#25968;&#25454;&#30340;&#25104;&#26412;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#30740;&#31350;&#24120;&#35265;&#30340;&#25968;&#25454;&#25110;&#27169;&#22411;&#32423;&#21035;&#30340;&#22686;&#24378;&#26041;&#27861;&#26469;&#22686;&#24378;&#27867;&#21270;&#24615;&#33021;&#65292;&#20294;&#26410;&#33021;&#26377;&#25928;&#22320;&#35299;&#32806;&#26679;&#26412;&#35821;&#20041;&#65292;&#38480;&#21046;&#20102;DST&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#8220;&#20998;&#35299;&#12289;&#24449;&#26381;&#21644;&#32452;&#21512;&#8221;&#35299;&#20915;&#26041;&#26696;&#65292;&#26126;&#30830;&#22320;&#35299;&#32806;&#20102;&#24050;&#30693;&#25968;&#25454;&#30340;&#35821;&#20041;&#65292;&#24182;&#21033;&#29992;&#19987;&#23478;&#28151;&#21512;&#26426;&#21046;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;&#24050;&#30693;&#25968;&#25454;&#21010;&#20998;&#20026;&#35821;&#20041;&#29420;&#31435;&#23376;&#38598;&#65292;&#24182;&#35757;&#32451;&#30456;&#24212;&#30340;&#19987;&#23478;&#65292;&#22312;&#25105;&#20204;&#35774;&#35745;&#30340;&#38598;&#21512;&#25512;&#29702;&#19979;&#65292;&#23558;&#26032;&#30340;&#26410;&#35265;&#26679;&#26412;&#26144;&#23556;&#24182;&#25512;&#26029;&#20986;&#28151;&#21512;&#22411;&#19987;&#23478;&#12290;&#22312;T5-Adapter&#19978;&#23545;MultiWOZ2.1&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#26696;&#26174;&#33879;&#19988;&#19968;&#33268;&#22320;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#22312;&#35774;&#32622;&#19978;&#23454;&#29616;&#20102;SOTA&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot transfer learning for Dialogue State Tracking (DST) helps to handle a variety of task-oriented dialogue domains without the cost of collecting in-domain data. Existing works mainly study common data- or model-level augmentation methods to enhance the generalization but fail to effectively decouple the semantics of samples, limiting the zero-shot performance of DST. In this paper, we present a simple and effective "divide, conquer and combine" solution, which explicitly disentangles the semantics of seen data, and leverages the performance and robustness with the mixture-of-experts mechanism. Specifically, we divide the seen data into semantically independent subsets and train corresponding experts, the newly unseen samples are mapped and inferred with mixture-of-experts with our designed ensemble inference. Extensive experiments on MultiWOZ2.1 upon the T5-Adapter show our schema significantly and consistently improves the zero-shot performance, achieving the SOTA on settings 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;ReViz&#8221;&#30340;&#26816;&#32034;&#27169;&#22411;&#65292;&#21487;&#20197;&#30452;&#25509;&#22788;&#29702;&#25991;&#26412;&#21644;&#22270;&#20687;&#36755;&#20837;&#20197;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#30693;&#35782;&#26816;&#32034;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00424</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#26597;&#35810;&#30340;&#31471;&#21040;&#31471;&#30693;&#35782;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
End-to-end Knowledge Retrieval with Multi-modal Queries. (arXiv:2306.00424v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00424
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;ReViz&#8221;&#30340;&#26816;&#32034;&#27169;&#22411;&#65292;&#21487;&#20197;&#30452;&#25509;&#22788;&#29702;&#25991;&#26412;&#21644;&#22270;&#20687;&#36755;&#20837;&#20197;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#30693;&#35782;&#26816;&#32034;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#26597;&#35810;&#19979;&#30340;&#30693;&#35782;&#26816;&#32034;&#65292;&#21363;&#21253;&#21547;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#26597;&#35810;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#19982;&#20043;&#21069;&#30340;&#36328;&#27169;&#24577;&#26816;&#32034;&#30740;&#31350;&#19981;&#21516;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;ReMuQ&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#36825;&#20010;&#20219;&#21153;&#30340;&#36827;&#23637;&#12290;ReMuQ&#38656;&#35201;&#19968;&#20010;&#31995;&#32479;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#25991;&#26412;&#21644;&#22270;&#20687;&#26597;&#35810;&#30340;&#20869;&#23481;&#26469;&#26816;&#32034;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#20013;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21483;&#20570;&#8220;ReViz&#8221;&#30340;&#26816;&#32034;&#27169;&#22411;&#65292;&#36825;&#20010;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#22788;&#29702;&#36755;&#20837;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#65292;&#20197;&#31471;&#21040;&#31471;&#26041;&#24335;&#26816;&#32034;&#30456;&#20851;&#30693;&#35782;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20687;&#30446;&#26631;&#26816;&#27979;&#22120;&#25110;&#26631;&#39064;&#29983;&#25104;&#22120;&#31561;&#20013;&#38388;&#27169;&#22359;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#22810;&#27169;&#24577;&#26597;&#35810;&#19979;&#30340;&#30693;&#35782;&#26816;&#32034;&#65292;&#24182;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#38646;-shot&#35774;&#32622;&#19979;&#65292;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;ReMuQ&#21644;OK-VQA&#65289;&#19978;&#30340;&#26816;&#32034;&#24615;&#33021;&#20248;&#36234;&#20197;&#21450;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#21518;&#36827;&#19968;&#27493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate knowledge retrieval with multi-modal queries, i.e. queries containing information split across image and text inputs, a challenging task that differs from previous work on cross-modal retrieval. We curate a new dataset called ReMuQ for benchmarking progress on this task. ReMuQ requires a system to retrieve knowledge from a large corpus by integrating contents from both text and image queries. We introduce a retriever model ``ReViz'' that can directly process input text and images to retrieve relevant knowledge in an end-to-end fashion without being dependent on intermediate modules such as object detectors or caption generators. We introduce a new pretraining task that is effective for learning knowledge retrieval with multimodal queries and also improves performance on downstream tasks. We demonstrate superior performance in retrieval on two datasets (ReMuQ and OK-VQA) under zero-shot settings as well as further improvements when finetuned on these datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;&#26631;&#35760;&#32423;&#30340;&#29983;&#25104;&#12289;&#25552;&#39640;&#21407;&#22987;&#23398;&#20064;&#21644;&#20943;&#23569;&#38169;&#35823;&#65292;&#20854;&#20013;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;dropout&#12289;&#36793;&#32536;&#38750;&#20284;&#28982;&#23398;&#20064;&#21644;&#26368;&#23567;&#21270;&#29109;&#12290;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#24773;&#24863;&#22235;&#20803;&#32452;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00418</link><description>&lt;p&gt;
&#8220;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#38750;&#20284;&#28982;&#23398;&#20064;&#25552;&#39640;&#29983;&#25104;&#24335;&#24773;&#24863;&#22235;&#20803;&#32452;&#39044;&#27979;&#8221;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Unlikelihood Learning Improves Generative Aspect Sentiment Quad Prediction. (arXiv:2306.00418v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;&#26631;&#35760;&#32423;&#30340;&#29983;&#25104;&#12289;&#25552;&#39640;&#21407;&#22987;&#23398;&#20064;&#21644;&#20943;&#23569;&#38169;&#35823;&#65292;&#20854;&#20013;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;dropout&#12289;&#36793;&#32536;&#38750;&#20284;&#28982;&#23398;&#20064;&#21644;&#26368;&#23567;&#21270;&#29109;&#12290;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#24773;&#24863;&#22235;&#20803;&#32452;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#24191;&#27867;&#20851;&#27880;&#20102;&#24773;&#24863;&#22235;&#20803;&#32452;&#39044;&#27979;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#20986;&#22235;&#20803;&#32452;&#65292;&#23558;&#21407;&#22987;&#21477;&#23376;&#36716;&#21270;&#20026;&#27169;&#26495;&#21270;&#30340;&#30446;&#26631;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#21482;&#20851;&#27880;&#29983;&#25104;&#20160;&#20040;&#65292;&#32780;&#24573;&#30053;&#20102;&#19981;&#38656;&#35201;&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#35748;&#20026;&#32771;&#34385;&#36127;&#26679;&#26412;&#20063;&#20250;&#24102;&#26469;&#28508;&#22312;&#30340;&#22909;&#22788;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#26495;&#26080;&#20851;&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;&#26631;&#35760;&#32423;&#30340;&#29983;&#25104;&#65292;&#21516;&#26102;&#25552;&#39640;&#21407;&#22987;&#23398;&#20064;&#21644;&#20943;&#23569;&#38169;&#35823;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33945;&#29305;&#21345;&#27931;dropout&#26469;&#29702;&#35299;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#32622;&#19981;&#30830;&#23450;&#24615;&#65292;&#33719;&#21462;&#22122;&#22768;&#21644;&#38169;&#35823;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#36793;&#32536;&#38750;&#20284;&#28982;&#23398;&#20064;&#26469;&#25233;&#21046;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#38169;&#35823;&#26631;&#35760;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26368;&#23567;&#21270;&#29109;&#26469;&#24179;&#34913;&#36793;&#32536;&#38750;&#20284;&#28982;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25552;&#39640;&#24773;&#24863;&#22235;&#20803;&#32452;&#39044;&#27979;&#30340;&#24615;&#33021;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, aspect sentiment quad prediction has received widespread attention in the field of aspect-based sentiment analysis. Existing studies extract quadruplets via pre-trained generative language models to paraphrase the original sentence into a templated target sequence. However, previous works only focus on what to generate but ignore what not to generate. We argue that considering the negative samples also leads to potential benefits. In this work, we propose a template-agnostic method to control the token-level generation, which boosts original learning and reduces mistakes simultaneously. Specifically, we introduce Monte Carlo dropout to understand the built-in uncertainty of pre-trained language models, acquiring the noises and errors. We further propose marginalized unlikelihood learning to suppress the uncertainty-aware mistake tokens. Finally, we introduce minimization entropy to balance the effects of marginalized unlikelihood learning. Extensive experiments on four public
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#36827;&#34892;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#22312;&#25511;&#21046;&#23454;&#39564;&#20013;&#65292;ASR&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;AWE&#27169;&#22411;&#34920;&#29616;&#26356;&#20026;&#40065;&#26834;&#12290;</title><link>http://arxiv.org/abs/2306.00410</link><description>&lt;p&gt;
&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#36827;&#34892;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#65306;&#27604;&#36739;ASR&#21644;Wolof&#20197;&#21450;Swahili&#20013;&#22522;&#20110;&#22768;&#23398;&#35789;&#23884;&#20837;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards hate speech detection in low-resource languages: Comparing ASR to acoustic word embeddings on Wolof and Swahili. (arXiv:2306.00410v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#36827;&#34892;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#22312;&#25511;&#21046;&#23454;&#39564;&#20013;&#65292;ASR&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;AWE&#27169;&#22411;&#34920;&#29616;&#26356;&#20026;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#36890;&#36807;&#20851;&#38190;&#35789;&#35782;&#21035;&#22312;&#24191;&#25773;&#20013;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#12290;&#19968;&#31181;&#26041;&#27861;&#26159;&#20026;&#30446;&#26631;&#20302;&#36164;&#28304;&#35821;&#35328;&#24314;&#31435;&#19968;&#20010;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#12290;&#25105;&#20204;&#23558;&#27492;&#19982;&#20351;&#29992;&#22522;&#20110;&#22768;&#23398;&#35789;&#23884;&#20837;&#65288;AWE&#65289;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#35813;&#27169;&#22411;&#23558;&#35821;&#38899;&#27573;&#26144;&#23556;&#21040;&#19968;&#20010;&#31354;&#38388;&#65292;&#20854;&#20013;&#21305;&#37197;&#30340;&#35789;&#26377;&#30456;&#20284;&#30340;&#21521;&#37327;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#22312;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#20013;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;AWE&#27169;&#22411;&#65292;&#22312;&#26410;&#35265;&#36807;&#30340;&#30446;&#26631;&#35821;&#35328;&#25968;&#25454;&#20013;&#26816;&#27979;&#20851;&#38190;&#35789;&#12290;&#19982;ASR&#30456;&#27604;&#65292;AWE&#26041;&#27861;&#20165;&#38656;&#35201;&#23569;&#37327;&#30340;&#20851;&#38190;&#35789;&#26679;&#26412;&#12290;&#22312;Wolof&#21644;Swahili&#30340;&#25511;&#21046;&#23454;&#39564;&#20013;&#65292;&#30001;&#20165;&#20116;&#20998;&#38047;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;ASR&#27169;&#22411;&#20248;&#20110;AWE&#26041;&#27861;&#12290;&#20294;&#26159;&#22312;&#23454;&#38469;&#37326;&#22806;&#27979;&#35797;&#30340;Swahili&#24191;&#25773;&#20013;&#65292;&#20351;&#29992;&#19968;&#20998;&#38047;&#30340;&#27169;&#26495;&#25968;&#25454;&#30340;AWE&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#65292;&#25552;&#20379;&#31867;&#20284;&#20110;30&#23567;&#26102;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#30340;ASR&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider hate speech detection through keyword spotting on radio broadcasts. One approach is to build an automatic speech recognition (ASR) system for the target low-resource language. We compare this to using acoustic word embedding (AWE) models that map speech segments to a space where matching words have similar vectors. We specifically use a multilingual AWE model trained on labelled data from well-resourced languages to spot keywords in data in the unseen target language. In contrast to ASR, the AWE approach only requires a few keyword exemplars. In controlled experiments on Wolof and Swahili where training and test data are from the same domain, an ASR model trained on just five minutes of data outperforms the AWE approach. But in an in-the-wild test on Swahili radio broadcasts with actual hate speech keywords, the AWE model (using one minute of template data) is more robust, giving similar performance to an ASR system trained on 30 hours of labelled data.
&lt;/p&gt;</description></item><item><title>BiSync&#26159;&#19968;&#27454;&#21452;&#35821;&#20889;&#20316;&#21161;&#25163;&#65292;&#20801;&#35768;&#29992;&#25143;&#33258;&#30001;&#22320;&#20351;&#29992;&#20004;&#31181;&#35821;&#35328;&#25776;&#20889;&#25991;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#20004;&#31181;&#21333;&#35821;&#25991;&#26412;&#21516;&#27493;&#65292;&#36824;&#21253;&#25324;&#20854;&#20182;&#21151;&#33021;&#65292;&#20363;&#22914;&#26174;&#31034;&#26367;&#20195;&#21069;&#32512;&#32763;&#35793;&#21644;&#37322;&#20041;&#65292;&#26088;&#22312;&#20419;&#36827;&#25776;&#20889;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2306.00400</link><description>&lt;p&gt;
BiSync&#65306;&#29992;&#20110;&#21516;&#27493;&#21333;&#35821;&#25991;&#26412;&#30340;&#21452;&#35821;&#32534;&#36753;&#22120;
&lt;/p&gt;
&lt;p&gt;
BiSync: A Bilingual Editor for Synchronized Monolingual Texts. (arXiv:2306.00400v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00400
&lt;/p&gt;
&lt;p&gt;
BiSync&#26159;&#19968;&#27454;&#21452;&#35821;&#20889;&#20316;&#21161;&#25163;&#65292;&#20801;&#35768;&#29992;&#25143;&#33258;&#30001;&#22320;&#20351;&#29992;&#20004;&#31181;&#35821;&#35328;&#25776;&#20889;&#25991;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#20004;&#31181;&#21333;&#35821;&#25991;&#26412;&#21516;&#27493;&#65292;&#36824;&#21253;&#25324;&#20854;&#20182;&#21151;&#33021;&#65292;&#20363;&#22914;&#26174;&#31034;&#26367;&#20195;&#21069;&#32512;&#32763;&#35793;&#21644;&#37322;&#20041;&#65292;&#26088;&#22312;&#20419;&#36827;&#25776;&#20889;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#20840;&#29699;&#21270;&#30340;&#19990;&#30028;&#20013;&#65292;&#36234;&#26469;&#36234;&#22810;&#24773;&#20917;&#38656;&#35201;&#20154;&#20204;&#20351;&#29992;&#19968;&#31181;&#25110;&#22810;&#31181;&#22806;&#35821;&#36827;&#34892;&#27807;&#36890;&#12290;&#22312;&#20070;&#38754;&#27807;&#36890;&#30340;&#24773;&#20917;&#19979;&#65292;&#29087;&#32451;&#25484;&#25569;&#22806;&#35821;&#30340;&#29992;&#25143;&#21487;&#20197;&#20511;&#21161;&#35745;&#31639;&#26426;&#36741;&#21161;&#32763;&#35793;&#65288;CAT&#65289;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#24448;&#24448;&#38656;&#35201;&#29992;&#25143;&#35775;&#38382;&#22806;&#37096;&#36164;&#28304;&#65292;&#22914;&#35789;&#20856;&#12289;&#26415;&#35821;&#34920;&#12289;&#21452;&#35821;&#35821;&#26009;&#24211;&#31561;&#65292;&#20174;&#32780;&#20013;&#26029;&#24182;&#20005;&#37325;&#38459;&#30861;&#20854;&#20889;&#20316;&#36807;&#31243;&#12290;&#20026;&#20102;&#20351;&#20889;&#20316;&#36807;&#31243;&#26356;&#21152;&#24179;&#31283;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; BiSync&#65292;&#19968;&#31181;&#21452;&#35821;&#20889;&#20316;&#21161;&#25163;&#65292;&#20801;&#35768;&#29992;&#25143;&#33258;&#30001;&#22320;&#20351;&#29992;&#20004;&#31181;&#35821;&#35328;&#25776;&#20889;&#25991;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#20004;&#31181;&#21333;&#35821;&#25991;&#26412;&#21516;&#27493;&#12290;&#25105;&#20204;&#36824;&#21253;&#25324;&#20854;&#20182;&#21151;&#33021;&#65292;&#20363;&#22914;&#26174;&#31034;&#26367;&#20195;&#21069;&#32512;&#32763;&#35793;&#21644;&#37322;&#20041;&#65292;&#26088;&#22312;&#20419;&#36827;&#25776;&#20889;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
In our globalized world, a growing number of situations arise where people are required to communicate in one or several foreign languages. In the case of written communication, users with a good command of a foreign language may find assistance from computer-aided translation (CAT) technologies. These technologies often allow users to access external resources, such as dictionaries, terminologies or bilingual concordancers, thereby interrupting and considerably hindering the writing process. In addition, CAT systems assume that the source sentence is fixed and also restrict the possible changes on the target side. In order to make the writing process smoother, we present BiSync, a bilingual writing assistant that allows users to freely compose text in two languages, while maintaining the two monolingual texts synchronized. We also include additional functionalities, such as the display of alternative prefix translations and paraphrases, which are intended to facilitate the authoring o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#20132;&#26367;&#35757;&#32451;&#36807;&#31243;&#65292;&#22312;&#24207;&#21015;&#32423;&#21035;&#20559;&#22909;&#19982;&#26631;&#35760;&#32423;&#21035;&#35757;&#32451;&#25351;&#23548;&#20043;&#38388;&#36827;&#34892;&#36845;&#20195;&#65292;&#24182;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#25351;&#23548;&#25913;&#36827;LM&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.00398</link><description>&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#26631;&#35760;&#32423;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
Preference-grounded Token-level Guidance for Language Model Fine-tuning. (arXiv:2306.00398v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#20132;&#26367;&#35757;&#32451;&#36807;&#31243;&#65292;&#22312;&#24207;&#21015;&#32423;&#21035;&#20559;&#22909;&#19982;&#26631;&#35760;&#32423;&#21035;&#35757;&#32451;&#25351;&#23548;&#20043;&#38388;&#36827;&#34892;&#36845;&#20195;&#65292;&#24182;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#25351;&#23548;&#25913;&#36827;LM&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20559;&#22909;&#19982;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30456;&#21305;&#37197;&#26159;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#20851;&#38190;&#25361;&#25112;&#26159;&#20559;&#22909;&#36890;&#24120;&#22312;&#24207;&#21015;&#32423;&#21035;&#19978;&#25552;&#20379;&#65292;&#32780;LM&#35757;&#32451;&#21644;&#29983;&#25104;&#37117;&#21457;&#29983;&#22312;&#26631;&#35760;&#32423;&#21035;&#19978;&#12290;&#22240;&#27492;&#65292;&#20559;&#22909;&#21644;LM&#35757;&#32451;&#25439;&#22833;&#20043;&#38388;&#23384;&#22312;&#39063;&#31890;&#24230;&#19981;&#21305;&#37197;&#65292;&#36825;&#21487;&#33021;&#20250;&#22797;&#26434;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#20132;&#26367;&#35757;&#32451;&#36807;&#31243;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#24207;&#21015;&#32423;&#21035;&#20559;&#22909;&#19982;&#26631;&#35760;&#32423;&#21035;&#35757;&#32451;&#25351;&#23548;&#20043;&#38388;&#36827;&#34892;&#36845;&#20195;&#65292;&#24182;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#25351;&#23548;&#25913;&#36827;LM&#12290;&#20026;&#20102;&#23398;&#20064;&#25351;&#23548;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#25104;&#23545;&#20559;&#22909;&#23398;&#20064;&#25193;&#23637;&#21040;&#21487;&#21464;&#38271;&#24230;LM&#29983;&#25104;&#21644;&#21033;&#29992;&#22810;&#20010;&#29983;&#25104;&#20043;&#38388;&#30340;&#20559;&#22909;&#12290;&#23545;&#20110;LM&#35757;&#32451;&#65292;&#22522;&#20110;&#30417;&#30563;&#25968;&#25454;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#25351;&#23548;&#30340;&#26497;&#31616;&#20027;&#20041;&#23398;&#20064;&#30446;&#26631;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#25351;&#23548;&#25552;&#39640;&#20102;&#29983;&#25104;&#24207;&#21015;&#30340;&#36136;&#37327;&#65292;&#20801;&#35768;&#26356;&#31934;&#32454;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning language models (LMs) with preferences is an important problem in natural language generation. A key challenge is that preferences are typically provided at the sequence level while LM training and generation both occur at the token level. There is, therefore, a granularity mismatch between the preference and the LM training losses, which may complicate the learning problem. In this paper, we address this issue by developing an alternate training process, where we iterate between grounding the sequence-level preference into token-level training guidance, and improving the LM with the learned guidance. For guidance learning, we design a framework that extends the pairwise-preference learning in imitation learning to both variable-length LM generation and utilizing the preference among multiple generations. For LM training, based on the amount of supervised data, we present two minimalist learning objectives that utilize the learned guidance. In experiments, our method performs 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#26412;&#20307;&#35770;&#65292;&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#26377;&#20851;&#32593;&#32476;&#23433;&#20840;&#26412;&#20307;&#35770;&#39046;&#22495;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.00377</link><description>&lt;p&gt;
&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#24320;&#21457;&#21644;&#26500;&#24314;&#26412;&#20307;&#35770;
&lt;/p&gt;
&lt;p&gt;
Developing and Building Ontologies in Cyber Security. (arXiv:2306.00377v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00377
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#26412;&#20307;&#35770;&#65292;&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#26377;&#20851;&#32593;&#32476;&#23433;&#20840;&#26412;&#20307;&#35770;&#39046;&#22495;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23433;&#20840;&#26159;&#25105;&#20204;&#29616;&#20195;&#31038;&#20250;&#20013;&#26368;&#21463;&#20851;&#27880;&#30340;&#23398;&#31185;&#20043;&#19968;&#12290;&#25105;&#20204;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#24037;&#20316;&#65292;&#36873;&#25321;&#20102;&#32593;&#32476;&#23433;&#20840;&#26412;&#20307;&#35770;&#20316;&#20026;&#30740;&#31350;&#20027;&#39064;&#12290;&#25105;&#20204;&#27719;&#38598;&#20102;&#26368;&#26032;&#21644;&#20197;&#21069;&#30340;&#26412;&#20307;&#35770;&#65292;&#24182;&#26681;&#25454;&#19981;&#21516;&#30340;&#20998;&#26512;&#22240;&#32032;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#25214;&#21040;&#26368;&#20339;&#26041;&#26696;&#12290;&#36873;&#25321;&#27492;&#20027;&#39064;&#30340;&#21407;&#22240;&#26159;&#27719;&#38598;&#26469;&#33258;&#19981;&#21516;&#26102;&#20195;&#30340;&#19981;&#21516;&#26412;&#20307;&#35770;&#12290;&#22240;&#20026;&#30740;&#31350;&#20013;&#21253;&#25324;&#30340;SLR&#22823;&#22810;&#30740;&#31350;&#21333;&#20010;&#26412;&#20307;&#35770;&#12290;&#22914;&#26524;&#20219;&#20309;&#30740;&#31350;&#20154;&#21592;&#24819;&#35201;&#30740;&#31350;&#26412;&#20307;&#35770;&#65292;&#20182;&#24517;&#39035;&#30740;&#31350;&#27599;&#20010;&#21333;&#29420;&#30340;&#26412;&#20307;&#35770;&#24182;&#36873;&#25321;&#26368;&#36866;&#21512;&#20182;&#30740;&#31350;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#27719;&#38598;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#26412;&#20307;&#35770;&#65292;&#24182;&#30456;&#20114;&#27604;&#36739;&#20197;&#24471;&#21040;&#26368;&#20339;&#26041;&#26696;&#12290;&#36890;&#36807;&#31995;&#32479;&#24615;&#36807;&#31243;&#35748;&#30495;&#36873;&#25321;&#20102;2010&#24180;&#33267;2020&#24180;&#38388;&#30340;24&#31687;&#35770;&#25991;&#65292;&#24182;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#26368;&#21518;&#65292;&#26412;SLR&#21576;&#29616;&#32473;&#30740;&#31350;&#20154;&#21592;&#26377;&#20851;&#32593;&#32476;&#23433;&#20840;&#26412;&#20307;&#35770;&#39046;&#22495;&#30340;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cyber Security is one of the most arising disciplines in our modern society. We work on Cybersecurity domain and in this the topic we chose is Cyber Security Ontologies. In this we gather all latest and previous ontologies and compare them on the basis of different analyzing factors to get best of them. Reason to select this topic is to assemble different ontologies from different era of time. Because, researches that included in this SLR is mostly studied single ontology. If any researcher wants to study ontologies, he has to study every single ontology and select which one is best for his research. So, we assemble different types of ontology and compare them against each other to get best of them. A total 24 papers between years 2010-2020 are carefully selected through systematic process and classified accordingly. Lastly, this SLR have been presented to provide the researchers promising future directions in the domain of cybersecurity ontologies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22240;&#26524;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#20998;&#25968;&#21644;&#21453;&#20107;&#23454;&#22686;&#24378;&#26041;&#27861;&#30340;CFL&#20307;&#31995;&#26550;&#26500;&#65292;&#36890;&#36807;&#25554;&#20837;&#24335;&#26041;&#24335;&#23545;&#39044;&#35757;&#32451;&#30340;LM&#36827;&#34892;&#35299;&#27602;&#65292;&#23454;&#29616;&#20102;&#22240;&#26524;&#20844;&#24179;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#26041;&#27861;&#19981;&#20250;&#23545;&#27169;&#22411;&#22256;&#24785;&#20135;&#29983;&#22826;&#22823;&#24433;&#21709;&#65292;&#36824;&#20943;&#36731;&#20102;&#24847;&#22806;&#30340;&#20559;&#32622;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.00374</link><description>&lt;p&gt;
CFL&#65306;&#36890;&#36807;&#22522;&#20110;Token&#32423;&#23646;&#24615;&#25511;&#21046;&#30340;&#29983;&#25104;&#23454;&#29616;&#22240;&#26524;&#20844;&#24179;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CFL: Causally Fair Language Models Through Token-level Attribute Controlled Generation. (arXiv:2306.00374v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22240;&#26524;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#20998;&#25968;&#21644;&#21453;&#20107;&#23454;&#22686;&#24378;&#26041;&#27861;&#30340;CFL&#20307;&#31995;&#26550;&#26500;&#65292;&#36890;&#36807;&#25554;&#20837;&#24335;&#26041;&#24335;&#23545;&#39044;&#35757;&#32451;&#30340;LM&#36827;&#34892;&#35299;&#27602;&#65292;&#23454;&#29616;&#20102;&#22240;&#26524;&#20844;&#24179;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#26041;&#27861;&#19981;&#20250;&#23545;&#27169;&#22411;&#22256;&#24785;&#20135;&#29983;&#22826;&#22823;&#24433;&#21709;&#65292;&#36824;&#20943;&#36731;&#20102;&#24847;&#22806;&#30340;&#20559;&#32622;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#23646;&#24615;&#65292;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22240;&#26524;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#20998;&#25968;&#21644;&#21453;&#20107;&#23454;&#22686;&#24378;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#22312;LM&#35299;&#27602;&#30340;&#32972;&#26223;&#19979;&#25552;&#20986;&#20102;CFL&#65288;causally fair language&#65289;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#25554;&#20837;&#24335;&#26041;&#24335;&#23545;&#39044;&#35757;&#32451;&#30340;LM&#36827;&#34892;&#35299;&#27602;&#12290;&#25105;&#20204;&#30340;&#20307;&#31995;&#32467;&#26500;&#22522;&#20110;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#65292;&#25968;&#23398;&#36879;&#26126;&#19988;&#19982;&#35768;&#22810;&#29616;&#26377;&#35299;&#27602;&#25216;&#26415;&#30456;&#27604;&#35745;&#31639;&#25928;&#29575;&#39640;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20960;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#29702;&#35299;LM&#22312;&#26377;&#23475;&#25991;&#26412;&#29983;&#25104;&#29615;&#22659;&#20013;&#30340;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;\RTP(RTP)&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#27602;&#24615;&#36864;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CFL&#23454;&#29616;&#20102;&#36825;&#31181;&#35299;&#27602;&#65292;&#23545;&#27169;&#22411;&#22256;&#24785;&#27809;&#26377;&#22826;&#22823;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23545;BOLD&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;CFL&#30340;&#20943;&#36731;&#24847;&#22806;&#20559;&#32622;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method to control the attributes of Language Models (LMs) for the text generation task using Causal Average Treatment Effect (ATE) scores and counterfactual augmentation. We explore this method, in the context of LM detoxification, and propose the Causally Fair Language (CFL) architecture for detoxifying pre-trained LMs in a plug-and-play manner. Our architecture is based on a Structural Causal Model (SCM) that is mathematically transparent and computationally efficient as compared with many existing detoxification techniques. We also propose several new metrics that aim to better understand the behaviour of LMs in the context of toxic text generation. Further, we achieve state of the art performance for toxic degeneration, which are computed using \RTP (RTP) benchmark. Our experiments show that CFL achieves such a detoxification without much impact on the model perplexity. We also show that CFL mitigates the unintended bias problem through experiments on the BOLD dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#30340;&#28966;&#28857;&#21069;&#32512;&#35843;&#25972;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#21333;&#23646;&#24615;&#25511;&#21046;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25511;&#21046;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#27969;&#30021;&#24230;&#65292;&#22312;&#22810;&#23646;&#24615;&#25511;&#21046;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#25511;&#21046;&#20934;&#30830;&#24615;&#65292;&#24182;&#20445;&#25345;&#20102;&#25511;&#21046;&#26032;&#23646;&#24615;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#29616;&#26377;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00369</link><description>&lt;p&gt;
&#38024;&#23545;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#30340;&#28966;&#28857;&#21069;&#32512;&#35843;&#25972;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Focused Prefix Tuning for Controllable Text Generation. (arXiv:2306.00369v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#30340;&#28966;&#28857;&#21069;&#32512;&#35843;&#25972;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#21333;&#23646;&#24615;&#25511;&#21046;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25511;&#21046;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#27969;&#30021;&#24230;&#65292;&#22312;&#22810;&#23646;&#24615;&#25511;&#21046;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#25511;&#21046;&#20934;&#30830;&#24615;&#65292;&#24182;&#20445;&#25345;&#20102;&#25511;&#21046;&#26032;&#23646;&#24615;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#29616;&#26377;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#25968;&#25454;&#38598;&#20013;&#65292;&#23384;&#22312;&#26410;&#26631;&#27880;&#23646;&#24615;&#65292;&#21487;&#33021;&#20250;&#20026;&#20351;&#29992;&#20854;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#25552;&#20379;&#26080;&#20851;&#30340;&#23398;&#20064;&#20449;&#21495;&#65292;&#20174;&#32780;&#38477;&#20302;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#28966;&#28857;&#21069;&#32512;&#35843;&#25972;&#65288;FPT&#65289;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#20351;&#25511;&#21046;&#33021;&#22815;&#19987;&#27880;&#20110;&#25152;&#38656;&#23646;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#21333;&#23646;&#24615;&#25511;&#21046;&#20219;&#21153;&#20013;&#65292;FPT&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25511;&#21046;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#27969;&#30021;&#24230;&#12290;&#22312;&#22810;&#23646;&#24615;&#25511;&#21046;&#20219;&#21153;&#20013;&#65292;FPT&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#25511;&#21046;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#25511;&#21046;&#26032;&#23646;&#24615;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#29616;&#26377;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a controllable text generation dataset, there exist unannotated attributes that could provide irrelevant learning signals to models that use it for training and thus degrade their performance. We propose focused prefix tuning(FPT) to mitigate the problem and to enable the control to focus on the desired attribute. Experimental results show that FPT can achieve better control accuracy and text fluency than baseline models in single-attribute control tasks. In multi-attribute control tasks, FPT achieves comparable control accuracy with the state-of-the-art approach while keeping the flexibility to control new attributes without retraining existing models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#26032;&#39062;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24191;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21160;&#35789;&#26367;&#25442;&#36827;&#34892;&#65292;&#29992;&#20110;&#21307;&#23398;&#22240;&#26524;&#20851;&#31995;&#25552;&#21462;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;3&#31181;&#25968;&#25454;&#22686;&#24191;&#25216;&#26415;&#30456;&#27604;&#36739;&#65292;&#22312;&#23569;&#25968;&#31867;&#26041;&#38754;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.00346</link><description>&lt;p&gt;
SemEval2023&#20219;&#21153;8&#20013;&#30340;CAISA&#65306;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24191;&#29992;&#20110;&#20943;&#36731;&#22240;&#26524;&#20851;&#31995;&#25552;&#21462;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
CAISA at SemEval-2023 Task 8: Counterfactual Data Augmentation for Mitigating Class Imbalance in Causal Claim Identification. (arXiv:2306.00346v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00346
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#26032;&#39062;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24191;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21160;&#35789;&#26367;&#25442;&#36827;&#34892;&#65292;&#29992;&#20110;&#21307;&#23398;&#22240;&#26524;&#20851;&#31995;&#25552;&#21462;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;3&#31181;&#25968;&#25454;&#22686;&#24191;&#25216;&#26415;&#30456;&#27604;&#36739;&#65292;&#22312;&#23569;&#25968;&#31867;&#26041;&#38754;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#20250;&#23548;&#33268;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#23569;&#25968;&#31867;&#21644;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#20135;&#29983;&#19981;&#29702;&#24819;&#30340;&#34920;&#29616;&#12290;&#20351;&#29992;&#25968;&#25454;&#22686;&#24191;&#25216;&#26415;&#26469;&#22686;&#21152;&#26679;&#26412;&#25968;&#37327;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36807;&#21160;&#35789;&#26367;&#25442;&#36827;&#34892;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24191;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21307;&#23398;&#22240;&#26524;&#20851;&#31995;&#25552;&#21462;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#35813;&#26041;&#27861;&#30340;&#24433;&#21709;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;3&#31181;&#25968;&#25454;&#22686;&#24191;&#25216;&#26415;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#65288;&#30456;&#23545;&#65289;&#25552;&#39640;&#23569;&#25968;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The class imbalance problem can cause machine learning models to produce an undesirable performance on the minority class as well as the whole dataset. Using data augmentation techniques to increase the number of samples is one way to tackle this problem. We introduce a novel counterfactual data augmentation by verb replacement for the identification of medical claims. In addition, we investigate the impact of this method and compare it with 3 other data augmentation techniques, showing that the proposed method can result in a significant (relative) improvement in the minority class.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#20174;&#25991;&#26412;&#25551;&#36848;&#21644;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#23383;&#24149;&#65292;&#32780;&#19981;&#30452;&#25509;&#22788;&#29702;&#22270;&#20687;&#12290;&#22312;CIDEr&#25351;&#26631;&#19978;&#65292;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.00301</link><description>&lt;p&gt;
CapText: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#20687;&#20869;&#23481;&#21644;&#25551;&#36848;&#29983;&#25104;&#23383;&#24149;
&lt;/p&gt;
&lt;p&gt;
CapText: Large Language Model-based Caption Generation From Image Context and Description. (arXiv:2306.00301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00301
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#20174;&#25991;&#26412;&#25551;&#36848;&#21644;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#23383;&#24149;&#65292;&#32780;&#19981;&#30452;&#25509;&#22788;&#29702;&#22270;&#20687;&#12290;&#22312;CIDEr&#25351;&#26631;&#19978;&#65292;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#38590;&#20197;&#29992;&#20110;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#22270;&#29255;&#23383;&#24149;&#24448;&#24448;&#26159;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#65292;&#24182;&#19988;&#25552;&#20379;&#26377;&#20851;&#22270;&#20687;&#30340;&#34917;&#20805;&#20449;&#24687;&#65292;&#32780;&#27169;&#22411;&#24448;&#24448;&#29983;&#25104;&#25551;&#36848;&#22270;&#20687;&#35270;&#35273;&#29305;&#24449;&#30340;&#8220;&#25551;&#36848;&#8221;&#12290;&#22312;&#23383;&#24149;&#29983;&#25104;&#26041;&#38754;&#30340;&#30740;&#31350;&#24050;&#25506;&#32034;&#20102;&#20351;&#29992;&#27169;&#22411;&#22312;&#25552;&#20379;&#23545;&#24212;&#30340;&#25551;&#36848;&#25110;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#23383;&#24149;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#25991;&#26412;&#25551;&#36848;&#21644;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#23383;&#24149;&#65292;&#32780;&#19981;&#30452;&#25509;&#22788;&#29702;&#22270;&#20687;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#24494;&#35843;&#21518;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312; CIDEr &#25351;&#26631;&#19978;&#32988;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#27169;&#22411;&#65292;&#22914; OSCAR-VinVL&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep-learning models have been shown to perform well on image-to-text datasets, it is difficult to use them in practice for captioning images. This is because \textit{captions} traditionally tend to be context-dependent and offer complementary information about an image, while models tend to produce \textit{descriptions} that describe the visual features of the image. Prior research in caption generation has explored the use of models that generate captions when provided with the images alongside their respective descriptions or contexts. We propose and evaluate a new approach, which leverages existing large language models to generate captions from textual descriptions and context alone, without ever processing the image directly. We demonstrate that after fine-tuning, our approach outperforms current state-of-the-art image-text alignment models like OSCAR-VinVL on this task on the CIDEr metric.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#20219;&#21153;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#30340;&#26080;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#24615;&#33021;&#39044;&#27979;&#25351;&#26631;hidden covariance&#65292;&#21487;&#26174;&#33879;&#20248;&#21270;&#29616;&#26377;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2306.00288</link><description>&lt;p&gt;
&#26080;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#22312;RNN&#21644;Transformer&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Training-free Neural Architecture Search for RNNs and Transformers. (arXiv:2306.00288v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#20219;&#21153;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#30340;&#26080;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#24615;&#33021;&#39044;&#27979;&#25351;&#26631;hidden covariance&#65292;&#21487;&#26174;&#33879;&#20248;&#21270;&#29616;&#26377;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;(NAS)&#21487;&#20197;&#33258;&#21160;&#21019;&#24314;&#26032;&#30340;&#26377;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20026;&#25163;&#21160;&#35774;&#35745;&#22797;&#26434;&#26550;&#26500;&#25552;&#20379;&#20102;&#19968;&#31181;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;NAS&#31639;&#27861;&#36895;&#24230;&#36739;&#24930;&#65292;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#26550;&#26500;&#30340;&#26080;&#35757;&#32451;NAS&#25351;&#26631;&#65292;&#26497;&#22823;&#22320;&#21152;&#36895;&#20102;&#25628;&#32034;&#31639;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#38024;&#23545;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNN)&#21644;BERT-based transformer&#26550;&#26500;&#30340;&#26080;&#35757;&#32451;NAS&#25351;&#26631;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;hidden covariance&#30340;&#26032;&#30340;&#26080;&#35757;&#32451;&#25351;&#26631;&#65292;&#21487;&#20197;&#39044;&#27979;&#35757;&#32451;&#21518;RNN&#26550;&#26500;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26080;&#35757;&#32451;&#25351;&#26631;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#30340;&#36716;&#25442;&#22120;&#25628;&#32034;&#31354;&#38388;&#33539;&#20363;
&lt;/p&gt;
&lt;p&gt;
Neural architecture search (NAS) has allowed for the automatic creation of new and effective neural network architectures, offering an alternative to the laborious process of manually designing complex architectures. However, traditional NAS algorithms are slow and require immense amounts of computing power. Recent research has investigated training-free NAS metrics for image classification architectures, drastically speeding up search algorithms. In this paper, we investigate training-free NAS metrics for recurrent neural network (RNN) and BERT-based transformer architectures, targeted towards language modeling tasks. First, we develop a new training-free metric, named hidden covariance, that predicts the trained performance of an RNN architecture and significantly outperforms existing training-free metrics. We experimentally evaluate the effectiveness of the hidden covariance metric on the NAS-Bench-NLP benchmark. Second, we find that the current search space paradigm for transformer
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#22312;&#22788;&#29702;&#38750;&#27954;&#21517;&#23383;&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#22686;&#24378;&#31561;&#31574;&#30053;&#65292;&#36890;&#36807;&#24494;&#35843;ASR&#27169;&#22411;&#22312;&#22810;&#20010;&#38750;&#27954;&#21475;&#38899;&#19978;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#27169;&#22411;&#35823;&#24046;&#65292;&#30456;&#23545;WER&#25552;&#39640;&#20102;81.5&#65285;&#12290;</title><link>http://arxiv.org/abs/2306.00253</link><description>&lt;p&gt;
AfriNames&#65306;&#22823;&#22810;&#25968;ASR&#27169;&#22411;&#8220;&#23648;&#25134;&#8221;&#38750;&#27954;&#20154;&#30340;&#21517;&#23383;
&lt;/p&gt;
&lt;p&gt;
AfriNames: Most ASR models "butcher" African Names. (arXiv:2306.00253v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00253
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#22312;&#22788;&#29702;&#38750;&#27954;&#21517;&#23383;&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#22686;&#24378;&#31561;&#31574;&#30053;&#65292;&#36890;&#36807;&#24494;&#35843;ASR&#27169;&#22411;&#22312;&#22810;&#20010;&#38750;&#27954;&#21475;&#38899;&#19978;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#27169;&#22411;&#35823;&#24046;&#65292;&#30456;&#23545;WER&#25552;&#39640;&#20102;81.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#23545;&#35805;&#20195;&#29702;&#24517;&#39035;&#20934;&#30830;&#25429;&#25417;&#21629;&#21517;&#23454;&#20307;&#65292;&#20197;&#26368;&#23567;&#21270;&#19979;&#28216;&#20219;&#21153;&#30340;&#38169;&#35823;&#65292;&#20363;&#22914;&#65292;&#35201;&#27714;&#35821;&#38899;&#21161;&#25163;&#25773;&#25918;&#29305;&#23450;&#33402;&#26415;&#23478;&#30340;&#38899;&#36712;&#65292;&#21551;&#21160;&#23548;&#33322;&#21040;&#29305;&#23450;&#20301;&#32622;&#65292;&#25110;&#20026;&#24739;&#32773;&#35760;&#24405;&#23454;&#39564;&#23460;&#32467;&#26524;&#12290;&#20294;&#26159;&#65292;&#24403;&#20986;&#29616;&#35832;&#22914;&#8220;Ukachukwu&#8221;&#65288;&#20234;&#21338;&#35821;&#65289;&#12289;&#8220;Lakicia&#8221;&#65288;&#26031;&#29926;&#24076;&#37324;&#35821;&#65289;&#25110;&#8220;Ingabire&#8221;&#65288;&#21346;&#26106;&#36798;&#35821;&#65289;&#31561;&#21629;&#21517;&#23454;&#20307;&#26102;&#65292;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#30340;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#65292;&#23558;&#38169;&#35823;&#20256;&#36882;&#21040;&#19979;&#28216;&#31995;&#32479;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24314;&#27169;&#20026;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#28436;&#31034;&#20102;&#36890;&#36807;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#12289;&#26234;&#33021;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26469;&#22686;&#21152;&#38750;&#27954;&#21629;&#21517;&#23454;&#20307;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#22810;&#20010;&#38750;&#27954;&#21475;&#38899;&#19978;&#24494;&#35843;&#22810;&#35821;&#35328;ASR&#27169;&#22411;&#21487;&#20197;&#20943;&#36731;&#27169;&#22411;&#20559;&#24046;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#21253;&#21547;&#38750;&#27954;&#21629;&#21517;&#23454;&#20307;&#30340;&#26679;&#26412;&#19978;&#30456;&#23545;WER&#25913;&#36827;&#20102;81.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Useful conversational agents must accurately capture named entities to minimize error for downstream tasks, for example, asking a voice assistant to play a track from a certain artist, initiating navigation to a specific location, or documenting a laboratory result for a patient. However, where named entities such as ``Ukachukwu`` (Igbo), ``Lakicia`` (Swahili), or ``Ingabire`` (Rwandan) are spoken, automatic speech recognition (ASR) models' performance degrades significantly, propagating errors to downstream systems. We model this problem as a distribution shift and demonstrate that such model bias can be mitigated through multilingual pre-training, intelligent data augmentation strategies to increase the representation of African-named entities, and fine-tuning multilingual ASR models on multiple African accents. The resulting fine-tuned models show an 81.5\% relative WER improvement compared with the baseline on samples with African-named entities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20687;&#32032;&#32423;&#21035;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#27169;&#25311;&#20154;&#31867;&#27010;&#24565;&#30028;&#38754;&#21644;&#28151;&#21512;&#21160;&#20316;&#31354;&#38388;&#30340;&#20195;&#29702;&#65292;&#23454;&#29616;&#20102;&#22312;GUI&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#30340;MiniWob++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20154;&#31867;&#24037;&#20316;&#32773;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2306.00245</link><description>&lt;p&gt;
&#20174;&#20687;&#32032;&#21040;&#29992;&#25143;&#30028;&#38754;&#25805;&#20316;&#65306;&#36890;&#36807;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#23398;&#20064;&#36981;&#24490;&#25351;&#20196;
&lt;/p&gt;
&lt;p&gt;
From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces. (arXiv:2306.00245v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20687;&#32032;&#32423;&#21035;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#27169;&#25311;&#20154;&#31867;&#27010;&#24565;&#30028;&#38754;&#21644;&#28151;&#21512;&#21160;&#20316;&#31354;&#38388;&#30340;&#20195;&#29702;&#65292;&#23454;&#29616;&#20102;&#22312;GUI&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#30340;MiniWob++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20154;&#31867;&#24037;&#20316;&#32773;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20026;&#20102;&#26500;&#24314;&#25805;&#20316;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65288;GUI&#65289;&#30340;&#25968;&#23383;&#21270;&#20195;&#29702;&#65292;&#22823;&#22810;&#25968;&#24037;&#20316;&#37117;&#20381;&#36182;&#22522;&#20110;&#25991;&#26412;&#30340;&#34920;&#31034;&#65288;&#20174;HTML&#25110;&#20854;&#20182;&#32467;&#26500;&#21270;&#25968;&#25454;&#28304;&#27966;&#29983;&#65289;&#65292;&#36825;&#20123;&#34920;&#31034;&#24182;&#19981;&#24635;&#26159;&#23481;&#26131;&#33719;&#21462;&#12290;&#36825;&#20123;&#36755;&#20837;&#34920;&#31034;&#36890;&#24120;&#19982;&#33258;&#23450;&#20041;&#30340;&#20219;&#21153;&#29305;&#23450;&#21160;&#20316;&#31354;&#38388;&#30456;&#20851;&#32852;&#12290;&#26412;&#25991;&#26088;&#22312;&#21019;&#24314;&#20351;&#29992;&#19982;&#20154;&#31867;&#36890;&#24120;&#20351;&#29992;&#30340;&#30456;&#21516;&#27010;&#24565;&#30028;&#38754;-&#36890;&#36807;&#22522;&#20110;&#20687;&#32032;&#30340;&#23631;&#24149;&#25130;&#22270;&#21644;&#23545;&#24212;&#20110;&#38190;&#30424;&#21644;&#40736;&#26631;&#25805;&#20316;&#30340;&#36890;&#29992;&#21160;&#20316;&#31354;&#38388;&#19982;&#25968;&#23383;&#19990;&#30028;&#20132;&#20114;&#30340;&#20195;&#29702;&#12290;&#22312;&#36817;&#26399;&#20851;&#20110;&#20687;&#32032;&#32423;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#36825;&#26679;&#30340;&#20195;&#29702;&#22312;GUI&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#30340;MiniWob ++&#22522;&#20934;&#27979;&#35797;&#20013;&#33021;&#22815;&#36229;&#36234;&#20154;&#31867;&#24037;&#20316;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Much of the previous work towards digital agents for graphical user interfaces (GUIs) has relied on text-based representations (derived from HTML or other structured data sources), which are not always readily available. These input representations have been often coupled with custom, task-specific action spaces. This paper focuses on creating agents that interact with the digital world using the same conceptual interface that humans commonly use -via pixel-based screenshots and a generic action space corresponding to keyboard and mouse actions. Building upon recent progress in pixel-based pretraining, we show, for the first time, that it is possible for such agents to outperform human crowdworkers on the MiniWob++ benchmark of GUI-based instruction following tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;&#35009;&#21098;&#26469;&#25552;&#39640;BLIP&#27169;&#22411;&#22312;&#32454;&#33410;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.00228</link><description>&lt;p&gt;
&#20351;&#29992;&#35270;&#35273;&#35009;&#21098;&#26469;&#22686;&#24378;BLIP&#27169;&#22411;&#32454;&#33410;&#38382;&#39064;&#22238;&#31572;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Using Visual Cropping to Enhance Fine-Detail Question Answering of BLIP-Family Models. (arXiv:2306.00228v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;&#35009;&#21098;&#26469;&#25552;&#39640;BLIP&#27169;&#22411;&#22312;&#32454;&#33410;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#24863;&#30693;&#12289;&#35821;&#35328;&#21644;&#32972;&#26223;&#30693;&#35782;&#31995;&#32479;&#20043;&#38388;&#30340;&#26080;&#32541;&#20132;&#20114;&#12290;&#34429;&#28982;&#26368;&#36817;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#22914;BLIP&#24050;&#32463;&#25552;&#39640;&#20102;&#36825;&#39033;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#32570;&#20047;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#38382;&#39064;&#21644;&#25512;&#29702;&#31867;&#22411;&#19978;&#34920;&#29616;&#30340;&#33021;&#21147;&#12290;&#37492;&#20110;&#25105;&#20204;&#23545;BLIP&#23478;&#26063;&#27169;&#22411;&#36827;&#34892;&#30340;&#21021;&#27493;&#20998;&#26512;&#26174;&#31034;&#20986;&#22238;&#31572;&#32454;&#33410;&#38382;&#39064;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#25105;&#20204;&#30740;&#31350;&#20197;&#19979;&#38382;&#39064;&#65306;&#33021;&#21542;&#20351;&#29992;&#35270;&#35273;&#35009;&#21098;&#26469;&#25552;&#39640;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#22312;&#32454;&#33410;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#65311;&#32771;&#34385;&#21040;BLIP&#23478;&#26063;&#27169;&#22411;&#30340;&#26368;&#36817;&#25104;&#21151;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#38646;&#26679;&#26412;&#21644;&#19968;&#20010;&#24494;&#35843;&#30340;BLIP&#27169;&#22411;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19977;&#20010;&#21463;&#25511;&#30340;&#23376;&#38598;&#65292;&#20197;&#34913;&#37327;&#35009;&#21098;&#26159;&#21542;&#26377;&#21161;&#20110;&#27169;&#22411;&#24615;&#33021;&#12290;&#38500;&#20102;&#20154;&#24037;&#35009;&#21098;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#20004;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#23884;&#20837;&#30340;&#33258;&#21160;&#35009;&#21098;&#31574;&#30053;&#65292;&#36825;&#20123;&#31574;&#30053;&#20197;CLIP&#20026;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Question Answering is a challenging task, as it requires seamless interaction between perceptual, linguistic, and background knowledge systems. While the recent progress of visual and natural language models like BLIP has led to improved performance on this task, we lack understanding of the ability of such models to perform on different kinds of questions and reasoning types. As our initial analysis of BLIP-family models revealed difficulty with answering fine-detail questions, we investigate the following question: Can visual cropping be employed to improve the performance of state-of-the-art visual question answering models on fine-detail questions? Given the recent success of the BLIP-family models, we study a zero-shot and a fine-tuned BLIP model. We define three controlled subsets of the popular VQA-v2 benchmark to measure whether cropping can help model performance. Besides human cropping, we devise two automatic cropping strategies based on multi-modal embedding by CLIP 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#25955;&#30011;&#31508;&#30340;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#22270;&#20687;&#24494;&#35843;&#24037;&#20855;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26681;&#25454;&#30446;&#26631;&#21306;&#22495;&#20462;&#25913;AI&#21512;&#25104;&#22270;&#20687;&#24182;&#20445;&#30041;&#21407;&#22987;&#19978;&#19979;&#25991;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20462;&#22797;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#65292;&#35813;&#26041;&#27861;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#21487;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00219</link><description>&lt;p&gt;
&#25193;&#25955;&#30011;&#31508;&#65306;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#29983;&#25104;&#22270;&#20687;&#32534;&#36753;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Diffusion Brush: A Latent Diffusion Model-based Editing Tool for AI-generated Images. (arXiv:2306.00219v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#25955;&#30011;&#31508;&#30340;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#22270;&#20687;&#24494;&#35843;&#24037;&#20855;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26681;&#25454;&#30446;&#26631;&#21306;&#22495;&#20462;&#25913;AI&#21512;&#25104;&#22270;&#20687;&#24182;&#20445;&#30041;&#21407;&#22987;&#19978;&#19979;&#25991;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20462;&#22797;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#65292;&#35813;&#26041;&#27861;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#21487;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#38480;&#21046;&#65292;&#29983;&#25104;&#30340;&#22270;&#20687;&#32463;&#24120;&#21253;&#21547;&#19981;&#33391;&#30340;&#20266;&#24433;&#25110;&#20854;&#20182;&#38169;&#35823;&#12290;&#29616;&#26377;&#30340;&#24494;&#35843;&#29983;&#25104;&#22270;&#20687;&#30340;&#25216;&#26415;&#35201;&#20040;&#32791;&#26102;&#65288;&#25163;&#21160;&#32534;&#36753;&#65289;&#65292;&#35201;&#20040;&#20135;&#29983;&#19981;&#22815;&#23436;&#32654;&#30340;&#32467;&#26524;&#65288;&#20462;&#34917;&#65289;&#65292;&#35201;&#20040;&#20250;&#23548;&#33268;&#25972;&#20307;&#22270;&#20687;&#20135;&#29983;&#24847;&#24819;&#19981;&#21040;&#30340;&#21464;&#21270;&#65288;&#21464;&#20307;&#36873;&#25321;&#21644;&#25552;&#31034;&#24494;&#35843;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#25955;&#30011;&#31508;&#30340;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#22270;&#20687;&#24494;&#35843;&#24037;&#20855;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24494;&#35843;AI&#21512;&#25104;&#22270;&#20687;&#20013;&#25152;&#38656;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#20013;&#22312;&#30446;&#26631;&#21306;&#22495;&#24341;&#20837;&#20102;&#26032;&#30340;&#38543;&#26426;&#22122;&#22768;&#27169;&#24335;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#20445;&#30041;&#20854;&#20182;&#21306;&#22495;&#21407;&#22987;&#19978;&#19979;&#25991;&#30340;&#21516;&#26102;&#65292;&#39640;&#25928;&#22320;&#23545;&#25351;&#23450;&#21306;&#22495;&#36827;&#34892;&#26356;&#25913;&#12290;&#25105;&#20204;&#36890;&#36807;&#33402;&#26415;&#23478;&#36827;&#34892;&#30340;&#29992;&#25143;&#30740;&#31350;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#23558;&#25105;&#20204;&#30340;&#25216;&#26415;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20462;&#22797;&#25216;&#26415;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generative models have made remarkable advancements in generating high-quality images. However, generated images often contain undesirable artifacts or other errors due to model limitations. Existing techniques to fine-tune generated images are time-consuming (manual editing), produce poorly-integrated results (inpainting), or result in unexpected changes across the entire image (variation selection and prompt fine-tuning). In this work, we present Diffusion Brush, a Latent Diffusion Model-based (LDM) tool to efficiently fine-tune desired regions within an AI-synthesized image. Our method introduces new random noise patterns at targeted regions during the reverse diffusion process, enabling the model to efficiently make changes to the specified regions while preserving the original context for the rest of the image. We evaluate our method's usability and effectiveness through a user study with artists, comparing our technique against other state-of-the-art image inpaintin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#28508;&#22312;&#22996;&#23113;&#35821;&#28040;&#27495;&#20219;&#21153;&#65292;&#39318;&#20808;&#27880;&#37322;&#20102;&#27169;&#31946;&#24615;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#20013;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#22996;&#23113;&#35821;&#35821;&#26009;&#24211;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#35821;&#35328;Transformer&#27169;&#22411;&#65292;&#35813;&#30740;&#31350;&#24314;&#31435;&#20102;&#21021;&#27493;&#30340;&#32467;&#26524;&#65292;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2306.00217</link><description>&lt;p&gt;
FEED PETs&#65306;&#20851;&#20110;&#28508;&#22312;&#22996;&#23113;&#35828;&#35821;&#30340;&#28040;&#27495;&#26356;&#22810;&#23454;&#39564;&#19982;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
FEED PETs: Further Experimentation and Expansion on the Disambiguation of Potentially Euphemistic Terms. (arXiv:2306.00217v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#28508;&#22312;&#22996;&#23113;&#35821;&#28040;&#27495;&#20219;&#21153;&#65292;&#39318;&#20808;&#27880;&#37322;&#20102;&#27169;&#31946;&#24615;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#20013;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#22996;&#23113;&#35821;&#35821;&#26009;&#24211;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#35821;&#35328;Transformer&#27169;&#22411;&#65292;&#35813;&#30740;&#31350;&#24314;&#31435;&#20102;&#21021;&#27493;&#30340;&#32467;&#26524;&#65292;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;Transformer&#22312;&#33521;&#35821;&#22996;&#23113;&#35821;&#28040;&#27495;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#21363;&#22312;&#29305;&#23450;&#35821;&#22659;&#20013;&#23558;&#28508;&#22312;&#30340;&#22996;&#23113;&#35821;&#65288;PET&#65289;&#20998;&#31867;&#20026;&#22996;&#23113;&#35821;&#25110;&#38750;&#22996;&#23113;&#35821;&#12290;&#26412;&#30740;&#31350;&#20174;&#20004;&#20010;&#26041;&#38754;&#25193;&#23637;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20026;&#27169;&#31946;&#24615;&#27880;&#37322;PET&#65292;&#36825;&#26159;&#19982;&#22996;&#23113;&#35821;&#30456;&#20851;&#30340;&#35821;&#35328;&#23646;&#24615;&#65292;&#24182;&#21457;&#29616;Transformer&#36890;&#24120;&#26356;&#25797;&#38271;&#20998;&#31867;&#27169;&#31946;&#30340;PET&#65292;&#36825;&#34920;&#26126;&#24433;&#21709;&#24615;&#33021;&#30340;&#25968;&#25454;&#20013;&#23384;&#22312;&#35821;&#35328;&#24046;&#24322;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19977;&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#26032;&#39062;&#22996;&#23113;&#35821;&#35821;&#26009;&#24211;&#65306;&#32422;&#40065;&#24052;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#20013;&#25991;&#26222;&#36890;&#35805;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#35821;&#35328;Transformer&#27169;&#22411;mBERT&#21644;XLM-RoBERTa&#22312;&#27599;&#31181;&#35821;&#35328;&#20013;&#25191;&#34892;&#22996;&#23113;&#35821;&#28040;&#27495;&#23454;&#39564;&#65292;&#24314;&#31435;&#21021;&#27493;&#32467;&#26524;&#65292;&#20026;&#26410;&#26469;&#24037;&#20316;&#25552;&#20379;&#20102;&#22522;&#30784;&#21644;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have been shown to work well for the task of English euphemism disambiguation, in which a potentially euphemistic term (PET) is classified as euphemistic or non-euphemistic in a particular context. In this study, we expand on the task in two ways. First, we annotate PETs for vagueness, a linguistic property associated with euphemisms, and find that transformers are generally better at classifying vague PETs, suggesting linguistic differences in the data that impact performance. Second, we present novel euphemism corpora in three different languages: Yoruba, Spanish, and Mandarin Chinese. We perform euphemism disambiguation experiments in each language using multilingual transformer models mBERT and XLM-RoBERTa, establishing preliminary results from which to launch future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#38899;&#36716;&#25991;&#26412;&#32763;&#35793;&#30340;&#25913;&#36827;&#31574;&#30053;&#65292;&#21033;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#20197;&#21450;&#22810;&#35821;&#31181;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#21021;&#27493;&#25552;&#21319;&#24615;&#33021;&#65292;&#22312;&#35757;&#32451;&#21644;&#35299;&#30721;&#26102;&#20351;&#29992;CTC&#20316;&#20026;&#39069;&#22806;&#30340;&#30446;&#26631;&#36827;&#19968;&#27493;&#20248;&#21270;&#32467;&#26524;&#65292;&#22312;Tamasheq-&#27861;&#35821;&#25968;&#25454;&#19978;&#36798;&#21040;&#20102;7.3&#30340;BLEU&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2306.00208</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;ASR&#27169;&#22411;&#30340;&#20302;&#36164;&#28304;&#35821;&#38899;&#36716;&#25991;&#26412;&#32763;&#35793;&#30340;&#25913;&#36827;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Strategies for improving low resource speech to text translation relying on pre-trained ASR models. (arXiv:2306.00208v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#38899;&#36716;&#25991;&#26412;&#32763;&#35793;&#30340;&#25913;&#36827;&#31574;&#30053;&#65292;&#21033;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#20197;&#21450;&#22810;&#35821;&#31181;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#21021;&#27493;&#25552;&#21319;&#24615;&#33021;&#65292;&#22312;&#35757;&#32451;&#21644;&#35299;&#30721;&#26102;&#20351;&#29992;CTC&#20316;&#20026;&#39069;&#22806;&#30340;&#30446;&#26631;&#36827;&#19968;&#27493;&#20248;&#21270;&#32467;&#26524;&#65292;&#22312;Tamasheq-&#27861;&#35821;&#25968;&#25454;&#19978;&#36798;&#21040;&#20102;7.3&#30340;BLEU&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25913;&#36827;&#20302;&#36164;&#28304;&#35821;&#38899;&#36716;&#25991;&#26412;&#32763;&#35793;&#30340;&#25216;&#26415;&#21644;&#21457;&#29616;&#12290;&#25105;&#20204;&#22312;&#33521;&#35821;-&#33889;&#33796;&#29273;&#35821;&#21644;Tamasheq-&#27861;&#35821;&#20004;&#31181;&#35821;&#35328;&#23545;&#30340;&#27169;&#25311;&#21644;&#30495;&#23454;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#26469;&#36827;&#34892;&#32763;&#35793;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#35821;&#31181;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#20316;&#20026;&#33391;&#22909;&#30340;&#21021;&#22987;&#21270;&#12290;&#27492;&#22806;&#65292;&#22312;&#35757;&#32451;&#21644;&#35299;&#30721;&#26102;&#20351;&#29992;CTC&#20316;&#20026;&#39069;&#22806;&#30340;&#32763;&#35793;&#30446;&#26631;&#26377;&#21161;&#20110;&#37325;&#26032;&#25490;&#24207;&#20869;&#37096;&#34920;&#31034;&#24182;&#25913;&#36827;&#26368;&#32456;&#30340;&#32763;&#35793;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35797;&#22270;&#30830;&#23450;&#23545;&#20110;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#25913;&#36827;&#26368;&#26377;&#36129;&#29486;&#30340;&#21508;&#31181;&#22240;&#32032;&#65288;&#21021;&#22987;&#21270;&#65292;&#30446;&#26631;&#21644;&#36229;&#21442;&#25968;&#65289;&#12290;&#21482;&#20351;&#29992;300&#20010;&#23567;&#26102;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;Tamasheq-&#27861;&#35821;&#25968;&#25454;&#19978;&#36798;&#21040;&#20102;7.3&#30340;BLEU&#24471;&#20998;&#65292;&#20248;&#20110;IWSLT 2022&#30340;&#20043;&#21069;&#21457;&#34920;&#30340;&#20316;&#21697;1.6&#20010;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents techniques and findings for improving the performance of low-resource speech to text translation (ST). We conducted experiments on both simulated and real-low resource setups, on language pairs English - Portuguese, and Tamasheq - French respectively. Using the encoder-decoder framework for ST, our results show that a multilingual automatic speech recognition system acts as a good initialization under low-resource scenarios. Furthermore, using the CTC as an additional objective for translation during training and decoding helps to reorder the internal representations and improves the final translation. Through our experiments, we try to identify various factors (initializations, objectives, and hyper-parameters) that contribute the most for improvements in low-resource setups. With only 300 hours of pre-training data, our model achieved 7.3 BLEU score on Tamasheq - French data, outperforming prior published works from IWSLT 2022 by 1.6 points.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#30340;&#19981;&#21464;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29983;&#25104;&#30340;&#25991;&#26412;&#20998;&#24067;&#36716;&#31227;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.00198</link><description>&lt;p&gt;
&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#30340;&#19981;&#21464;&#23398;&#20064;&#29305;&#24449;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
An Invariant Learning Characterization of Controlled Text Generation. (arXiv:2306.00198v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#30340;&#19981;&#21464;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29983;&#25104;&#30340;&#25991;&#26412;&#20998;&#24067;&#36716;&#31227;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#29983;&#25104;&#26159;&#25351;&#21019;&#24314;&#21253;&#21547;&#25152;&#38656;&#26679;&#24335;&#25110;&#35821;&#20041;&#23646;&#24615;&#30340;&#25991;&#26412;&#30340;&#38382;&#39064;&#12290;&#35768;&#22810;&#26041;&#27861;&#23558;&#36825;&#20010;&#38382;&#39064;&#24402;&#32467;&#20026;&#35757;&#32451;&#25152;&#38656;&#23646;&#24615;&#30340;&#39044;&#27979;&#22120;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#33021;&#26469;&#33258;&#21508;&#31181;&#20998;&#24067;&#12290;&#26412;&#25991;&#36890;&#36807;&#19981;&#21464;&#23398;&#20064;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#25511;&#21046;&#29983;&#25104;&#20013;&#20998;&#24067;&#36716;&#31227;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controlled generation refers to the problem of creating text that contains stylistic or semantic attributes of interest. Many approaches reduce this problem to training a predictor of the desired attribute. For example, researchers hoping to deploy a large language model to produce non-toxic content may use a toxicity classifier to filter generated text. In practice, the generated text to classify, which is determined by user prompts, may come from a wide range of distributions. In this paper, we show that the performance of controlled generation may be poor if the distributions of text in response to user prompts differ from the distribution the predictor was trained on. To address this problem, we cast controlled generation under distribution shift as an invariant learning problem: the most effective predictor should be invariant across multiple text environments. We then discuss a natural solution that arises from this characterization and propose heuristics for selecting natural en
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#24102;&#26377;&#25991;&#26412;&#34164;&#21547;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#26469;&#23454;&#29616;&#25277;&#35937;&#25688;&#35201;&#31995;&#32479;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;&#24544;&#23454;&#24230;&#12289;&#26126;&#26174;&#24615;&#21644;&#31616;&#27905;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00186</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#34164;&#21547;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20107;&#23454;&#19968;&#33268;&#24615;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Factually Consistent Summarization via Reinforcement Learning with Textual Entailment Feedback. (arXiv:2306.00186v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#24102;&#26377;&#25991;&#26412;&#34164;&#21547;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#26469;&#23454;&#29616;&#25277;&#35937;&#25688;&#35201;&#31995;&#32479;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;&#24544;&#23454;&#24230;&#12289;&#26126;&#26174;&#24615;&#21644;&#31616;&#27905;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24403;&#20195;&#30340;&#25991;&#26412;&#29983;&#25104;&#31995;&#32479;&#34920;&#29616;&#20986;&#34920;&#38754;&#19978;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#20250;&#29983;&#25104;&#19982;&#20854;&#36755;&#20837;&#19981;&#30456;&#31526;&#30340;&#38169;&#35823;&#25991;&#26412;&#12290;&#36825;&#31181;&#29616;&#35937;&#22312;&#25688;&#35201;&#31561;&#20219;&#21153;&#20013;&#26356;&#21152;&#31361;&#20986;&#65292;&#22240;&#20026;&#29983;&#25104;&#30340;&#25688;&#35201;&#24212;&#35813;&#24471;&#21040;&#20854;&#26469;&#28304;&#25991;&#31456;&#30340;&#35777;&#23454;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#26368;&#36817;&#22312;&#25991;&#26412;&#34164;&#21547;&#27169;&#22411;&#19978;&#30340;&#36827;&#23637;&#26469;&#30452;&#25509;&#35299;&#20915;&#25277;&#35937;&#25688;&#35201;&#31995;&#32479;&#20013;&#30340;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#19982;&#26080;&#38656;&#21442;&#32771;&#30340;&#25991;&#26412;&#34164;&#21547;&#22870;&#21169;&#26469;&#20248;&#21270;&#20107;&#23454;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#25506;&#32034;&#38543;&#20043;&#32780;&#26469;&#30340;&#26435;&#34913;&#65292;&#22240;&#20026;&#25913;&#36827;&#30340;&#19968;&#33268;&#24615;&#21487;&#33021;&#20250;&#20197;&#25688;&#35201;&#30340;&#20449;&#24687;&#37327;&#23569;&#25110;&#26356;&#25552;&#21462;&#20026;&#20195;&#20215;&#12290;&#26681;&#25454;&#33258;&#21160;&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#30528;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;&#24544;&#23454;&#24230;&#12289;&#26126;&#26174;&#24615;&#21644;&#31616;&#27905;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the seeming success of contemporary grounded text generation systems, they often tend to generate factually inconsistent text with respect to their input. This phenomenon is emphasized in tasks like summarization, in which the generated summaries should be corroborated by their source article. In this work, we leverage recent progress on textual entailment models to directly address this problem for abstractive summarization systems. We use reinforcement learning with reference-free, textual entailment rewards to optimize for factual consistency and explore the ensuing trade-offs, as improved consistency may come at the cost of less informative or more extractive summaries. Our results, according to both automatic metrics and human evaluation, show that our method considerably improves the faithfulness, salience, and conciseness of the generated summaries.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CHANGES&#65292;&#19968;&#31181;&#23545;&#27604;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#25552;&#21462;&#31185;&#23398;&#35770;&#25991;&#30340;&#25688;&#35201;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20351;&#29992;&#20998;&#23618;&#35805;&#35821;&#22270;&#21644;&#22270;&#23545;&#27604;&#23398;&#20064;&#27169;&#22359;&#65292;&#23398;&#20064;&#26377;&#25928;&#30340;&#21477;&#23376;&#34920;&#31034;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#22312;&#24314;&#27169;&#31185;&#23398;&#35770;&#25991;&#26102;&#25429;&#25417;&#20998;&#23618;&#32467;&#26500;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00177</link><description>&lt;p&gt;
&#38754;&#21521;&#31185;&#23398;&#25991;&#29486;&#25688;&#35201;&#30340;&#23545;&#27604;&#20998;&#23618;&#35805;&#35821;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Contrastive Hierarchical Discourse Graph for Scientific Document Summarization. (arXiv:2306.00177v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CHANGES&#65292;&#19968;&#31181;&#23545;&#27604;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#25552;&#21462;&#31185;&#23398;&#35770;&#25991;&#30340;&#25688;&#35201;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20351;&#29992;&#20998;&#23618;&#35805;&#35821;&#22270;&#21644;&#22270;&#23545;&#27604;&#23398;&#20064;&#27169;&#22359;&#65292;&#23398;&#20064;&#26377;&#25928;&#30340;&#21477;&#23376;&#34920;&#31034;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#22312;&#24314;&#27169;&#31185;&#23398;&#35770;&#25991;&#26102;&#25429;&#25417;&#20998;&#23618;&#32467;&#26500;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25193;&#23637;&#30340;&#32467;&#26500;&#19978;&#19979;&#25991;&#26465;&#20214;&#19979;&#65292;&#31185;&#23398;&#35770;&#25991;&#25688;&#35201;&#25104;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHANGES&#30340;&#23545;&#27604;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#25552;&#21462;&#31185;&#23398;&#35770;&#25991;&#25688;&#35201;&#12290;&#35813;&#27169;&#22411;&#23558;&#31185;&#23398;&#35770;&#25991;&#34920;&#31034;&#20026;&#19968;&#20010;&#20998;&#23618;&#30340;&#35805;&#35821;&#22270;&#65292;&#24182;&#20351;&#29992;&#19987;&#38376;&#35774;&#35745;&#30340;&#20998;&#23618;&#22270;&#20449;&#24687;&#32858;&#21512;&#26041;&#27861;&#23398;&#20064;&#26377;&#25928;&#30340;&#21477;&#23376;&#34920;&#31034;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22270;&#23545;&#27604;&#23398;&#20064;&#27169;&#22359;&#65292;&#20197;&#23398;&#20064;&#20840;&#23616;&#30340;&#20027;&#39064;&#24863;&#30693;&#21477;&#23376;&#34920;&#31034;&#12290;&#22312;PubMed&#21644;arXiv&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;CHANGES&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#22312;&#24314;&#27169;&#31185;&#23398;&#35770;&#25991;&#26102;&#25429;&#25417;&#20998;&#23618;&#32467;&#26500;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The extended structural context has made scientific paper summarization a challenging task. This paper proposes CHANGES, a contrastive hierarchical graph neural network for extractive scientific paper summarization. CHANGES represents a scientific paper with a hierarchical discourse graph and learns effective sentence representations with dedicated designed hierarchical graph information aggregation. We also propose a graph contrastive learning module to learn global theme-aware sentence representations. Extensive experiments on the PubMed and arXiv benchmark datasets prove the effectiveness of CHANGES and the importance of capturing hierarchical structure information in modeling scientific papers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#27880;&#37322;&#30340;&#33258;&#21160;&#21270;&#27969;&#31243;&#65292;&#24378;&#35843;&#24517;&#39035;&#38024;&#23545;&#20154;&#31867;&#29983;&#25104;&#30340;&#26631;&#31614;&#39564;&#35777;&#23427;&#20204;&#30340;&#24615;&#33021;&#65292;&#39564;&#35777;&#32467;&#26524;&#34920;&#26126;LLM&#30340;&#24615;&#33021;&#24456;&#26377;&#21069;&#36884;&#65292;&#20294;&#39640;&#24230;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#21644;&#27880;&#37322;&#20219;&#21153;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.00176</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#33258;&#21160;&#26631;&#27880;&#38656;&#35201;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Automated Annotation with Generative AI Requires Validation. (arXiv:2306.00176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#27880;&#37322;&#30340;&#33258;&#21160;&#21270;&#27969;&#31243;&#65292;&#24378;&#35843;&#24517;&#39035;&#38024;&#23545;&#20154;&#31867;&#29983;&#25104;&#30340;&#26631;&#31614;&#39564;&#35777;&#23427;&#20204;&#30340;&#24615;&#33021;&#65292;&#39564;&#35777;&#32467;&#26524;&#34920;&#26126;LLM&#30340;&#24615;&#33021;&#24456;&#26377;&#21069;&#36884;&#65292;&#20294;&#39640;&#24230;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#21644;&#27880;&#37322;&#20219;&#21153;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20197;&#25104;&#20026;&#25991;&#26412;&#27880;&#37322;&#36807;&#31243;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20294;&#30001;&#20110;&#25552;&#31034;&#36136;&#37327;&#65292;&#25991;&#26412;&#25968;&#25454;&#29305;&#23450;&#24615;&#21644;&#27010;&#24565;&#38590;&#24230;&#31561;&#21407;&#22240;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#22312;&#27880;&#37322;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#12290;&#22240;&#20026;&#21363;&#20351;LLM&#25216;&#26415;&#24471;&#21040;&#25913;&#36827;&#65292;&#36825;&#20123;&#25361;&#25112;&#20173;&#23558;&#23384;&#22312;&#65292;&#25152;&#20197;&#25105;&#20204;&#35748;&#20026;&#20351;&#29992;LLM&#30340;&#20219;&#20309;&#33258;&#21160;&#26631;&#27880;&#36807;&#31243;&#37117;&#24517;&#39035;&#38024;&#23545;&#20154;&#31867;&#29983;&#25104;&#30340;&#26631;&#31614;&#39564;&#35777;LLM&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#20010;&#24037;&#20316;&#27969;&#31243;&#65292;&#20197;&#19968;&#31181;&#21407;&#21017;&#24615;&#21644;&#26377;&#25928;&#29575;&#30340;&#26041;&#24335;&#21033;&#29992;LLM&#30340;&#27880;&#37322;&#28508;&#21147;&#12290;&#20351;&#29992;GPT-4&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#39640;&#24433;&#21709;&#26399;&#21002;&#30340;&#26368;&#26032;&#31038;&#20250;&#31185;&#23398;&#25991;&#31456;&#20013;&#22797;&#21046;11&#20010;&#25968;&#25454;&#38598;&#30340;27&#20010;&#26631;&#27880;&#20219;&#21153;&#26469;&#39564;&#35777;&#36825;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#25991;&#26412;&#27880;&#37322;&#65292;LLM&#30340;&#24615;&#33021;&#24456;&#26377;&#21069;&#36884;&#65292;&#20294;&#39640;&#24230;&#21462;&#20915;&#20110;&#25968;&#25454;&#38598;&#21644;&#27880;&#37322;&#20219;&#21153;&#31867;&#22411;&#65292;&#36825;&#24378;&#35843;&#20102;&#25353;&#20219;&#21153;&#39564;&#35777;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#26131;&#20110;&#20351;&#29992;&#30340;&#36719;&#20214;&#65292;&#26088;&#22312;&#23454;&#29616;&#25105;&#20204;&#30340;&#24037;&#20316;&#27969;&#31243;&#24182;&#31616;&#21270;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative large language models (LLMs) can be a powerful tool for augmenting text annotation procedures, but their performance varies across annotation tasks due to prompt quality, text data idiosyncrasies, and conceptual difficulty. Because these challenges will persist even as LLM technology improves, we argue that any automated annotation process using an LLM must validate the LLM's performance against labels generated by humans. To this end, we outline a workflow to harness the annotation potential of LLMs in a principled, efficient way. Using GPT-4, we validate this approach by replicating 27 annotation tasks across 11 datasets from recent social science articles in high-impact journals. We find that LLM performance for text annotation is promising but highly contingent on both the dataset and the type of annotation task, which reinforces the necessity to validate on a task-by-task basis. We make available easy-to-use software designed to implement our workflow and streamline the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#33258;&#28982;&#39046;&#22495;&#36716;&#31227;&#35774;&#32622;&#19979;&#24494;&#35843;&#21644;&#23567;&#26679;&#26412;&#23398;&#20064;&#27169;&#22411;&#30340;DR&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;DR&#22522;&#20934;&#65292;&#25552;&#20986;&#20102;DR&#25361;&#25112;&#30340;&#20004;&#20010;&#35270;&#35282;&#65306;&#28304;&#22495;&#38477;&#20302;&#65288;SD&#65289;&#21644;&#30446;&#26631;&#22495;&#38477;&#20302;&#65288;TD&#65289;&#65292;&#24182;&#21457;&#29616;&#20004;&#32773;&#20043;&#19968;&#36890;&#24120;&#26159;&#27491;&#20540;&#65292;&#24378;&#35843;&#20102;&#35780;&#20272;DR&#25361;&#25112;&#30340;&#20004;&#20010;&#35270;&#35282;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00168</link><description>&lt;p&gt;
&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#38754;&#23545;&#39046;&#22495;&#36716;&#31227;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Measuring the Robustness of Natural Language Processing Models to Domain Shifts. (arXiv:2306.00168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#33258;&#28982;&#39046;&#22495;&#36716;&#31227;&#35774;&#32622;&#19979;&#24494;&#35843;&#21644;&#23567;&#26679;&#26412;&#23398;&#20064;&#27169;&#22411;&#30340;DR&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;DR&#22522;&#20934;&#65292;&#25552;&#20986;&#20102;DR&#25361;&#25112;&#30340;&#20004;&#20010;&#35270;&#35282;&#65306;&#28304;&#22495;&#38477;&#20302;&#65288;SD&#65289;&#21644;&#30446;&#26631;&#22495;&#38477;&#20302;&#65288;TD&#65289;&#65292;&#24182;&#21457;&#29616;&#20004;&#32773;&#20043;&#19968;&#36890;&#24120;&#26159;&#27491;&#20540;&#65292;&#24378;&#35843;&#20102;&#35780;&#20272;DR&#25361;&#25112;&#30340;&#20004;&#20010;&#35270;&#35282;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#23567;&#26679;&#26412;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#20173;&#28982;&#33853;&#21518;&#20110;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#39046;&#22495;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#39046;&#22495;&#40065;&#26834;&#24615;&#65288;DR&#65289;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;DR&#30740;&#31350;&#23384;&#22312;&#19981;&#19968;&#33268;&#30340;&#35774;&#32622;&#12289;&#32570;&#20047;&#35780;&#20272;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#21644;&#36807;&#22810;&#20381;&#38752;&#25361;&#25112;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#33258;&#28982;&#39046;&#22495;&#36716;&#31227;&#35774;&#32622;&#19979;&#24494;&#35843;&#21644;&#23567;&#26679;&#26412;&#23398;&#20064;&#27169;&#22411;&#30340;DR&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;DR&#22522;&#20934;&#65292;&#21253;&#25324;&#22810;&#26679;&#21270;&#30340;NLP&#20219;&#21153;&#65292;&#21253;&#25324;&#21477;&#23376;&#21644;&#26631;&#35760;&#32423;&#20998;&#31867;&#12289;&#38382;&#31572;&#21644;&#29983;&#25104;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#30001;&#20960;&#20010;&#39046;&#22495;&#32452;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DR&#25361;&#25112;&#30340;&#20004;&#20010;&#35270;&#35282;&#65306;&#28304;&#22495;&#38477;&#20302;&#65288;SD&#65289;&#21644;&#30446;&#26631;&#22495;&#38477;&#20302;&#65288;TD&#65289;&#65292;&#23427;&#20204;&#20132;&#26367;&#20316;&#20026;&#21442;&#32771;&#28857;&#26469;&#27604;&#36739;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#37325;&#22823;&#27604;&#20363;&#30340;&#39046;&#22495;&#36716;&#31227;&#20013;&#65292;SD&#25110;TD&#20043;&#19968;&#26159;&#27491;&#30340;&#65292;&#20294;&#19981;&#26159;&#20004;&#32773;&#37117;&#27491;&#65292;&#24378;&#35843;&#20102;&#35780;&#20272;DR&#25361;&#25112;&#30340;&#20004;&#20010;&#35270;&#35282;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#20801;&#35768;&#22312;&#27169;&#22411;&#12289;&#20219;&#21153;&#21644;&#35774;&#32622;&#19978;&#20844;&#24179;&#27604;&#36739;DR&#65292;&#24182;&#25552;&#20379;&#26377;&#20851;NLP&#27169;&#22411;DR&#24615;&#36136;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have shown promising performance on various tasks, including fine-tuning, few-shot learning, and zero-shot learning. However, their performance on domains without labeled data still lags behind those with labeled data, which we refer as the Domain Robustness (DR) challenge. Existing research on DR suffers from disparate setups, lack of evaluation task variety, and reliance on challenge sets. In this paper, we explore the DR challenge of both fine-tuned and few-shot learning models in natural domain shift settings. We introduce a DR benchmark comprising diverse NLP tasks, including sentence and token-level classification, QA, and generation, each task consists of several domains. We propose two views of the DR challenge: Source Drop (SD) and Target Drop (TD), which alternate between the source and target in-domain performance as reference points. We find that in significant proportions of domain shifts, either SD or TD is positive, but not both, emphasizing the imp
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24207;&#21015;&#12289;&#24207;&#21015;&#21644;&#38598;&#21512;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#25991;&#26412;&#21040;&#34920;&#26684;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#20004;&#20010;&#24120;&#35265;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#26356;&#20855;&#28789;&#27963;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#21487;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.00137</link><description>&lt;p&gt;
&#19968;&#31181;&#25991;&#26412;&#21040;&#34920;&#26684;&#29983;&#25104;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&amp;&#38598;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Sequence-to-Sequence&amp;Set Model for Text-to-Table Generation. (arXiv:2306.00137v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00137
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24207;&#21015;&#12289;&#24207;&#21015;&#21644;&#38598;&#21512;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#25991;&#26412;&#21040;&#34920;&#26684;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#20004;&#20010;&#24120;&#35265;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#26356;&#20855;&#28789;&#27963;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#21487;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#21040;&#34920;&#26684;&#29983;&#25104;&#20219;&#21153;&#22240;&#20854;&#24191;&#27867;&#24212;&#29992;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#20010;&#26041;&#38754;&#65292;&#20027;&#23548;&#27169;&#22411;&#23558;&#35813;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#24207;&#21015;&#21040;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#23558;&#27599;&#20010;&#34920;&#26684;&#20018;&#34892;&#21270;&#20026;&#20196;&#29260;&#24207;&#21015;&#65292;&#36890;&#36807;&#20174;&#19978;&#21040;&#19979;&#36830;&#25509;&#25152;&#26377;&#34892;&#12290;&#28982;&#32780;&#65292;&#23427;&#23384;&#22312;&#20004;&#20010;&#20005;&#37325;&#32570;&#38519;&#65306;1&#65289;&#39044;&#23450;&#20041;&#30340;&#39034;&#24207;&#22312;&#35757;&#32451;&#26399;&#38388;&#24341;&#20837;&#20102;&#38169;&#35823;&#30340;&#20559;&#24046;&#65292;&#39640;&#24230;&#24809;&#32602;&#34892;&#20043;&#38388;&#30340;&#39034;&#24207;&#31227;&#21160;&#65307;2&#65289;&#24403;&#27169;&#22411;&#36755;&#20986;&#38271;&#26631;&#35760;&#24207;&#21015;&#26102;&#65292;&#38169;&#35823;&#20256;&#25773;&#38382;&#39064;&#21464;&#24471;&#20005;&#37325;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#65292;&#20197;&#35777;&#26126;&#22823;&#22810;&#25968;&#34892;&#30340;&#29983;&#25104;&#26159;&#21644;&#39034;&#24207;&#26080;&#20851;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&amp;&#38598;&#21512;&#25991;&#26412;&#21040;&#34920;&#26684;&#29983;&#25104;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#38500;&#20102;&#25991;&#26412;&#32534;&#30721;&#22120;&#20043;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#37197;&#22791;&#20102;&#19968;&#20010;&#34920;&#22836;&#29983;&#25104;&#22120;&#65292;&#20197;&#39034;&#24207;&#26041;&#24335;&#39318;&#20808;&#36755;&#20986;&#34920;&#22836;&#65292;&#21363;&#34920;&#26684;&#30340;&#31532;&#19968;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the text-to-table generation task has attracted increasing attention due to its wide applications. In this aspect, the dominant model formalizes this task as a sequence-to-sequence generation task and serializes each table into a token sequence during training by concatenating all rows in a top-down order. However, it suffers from two serious defects: 1) the predefined order introduces a wrong bias during training, which highly penalizes shifts in the order between rows; 2) the error propagation problem becomes serious when the model outputs a long token sequence. In this paper, we first conduct a preliminary study to demonstrate the generation of most rows is order-insensitive. Furthermore, we propose a novel sequence-to-sequence&amp;set text-to-table generation model. Specifically, in addition to a text encoder encoding the input text, our model is equipped with a table header generator to first output a table header, i.e., the first row of the table, in the manner of sequence 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DRS&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;-&#35821;&#20041;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#21644;&#24847;&#20041;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#38750;&#33521;&#35821;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;DRS&#35299;&#26512;&#21644;DRS&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#19988;&#21487;&#20197;&#22823;&#22823;&#38477;&#20302;&#22810;&#35821;&#35328;&#35821;&#20041;&#27880;&#37322;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.00124</link><description>&lt;p&gt;
&#38754;&#21521;&#22810;&#35821;&#35328;&#35299;&#26512;&#21644;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;-&#35821;&#20041;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pre-Trained Language-Meaning Models for Multilingual Parsing and Generation. (arXiv:2306.00124v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DRS&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;-&#35821;&#20041;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#21644;&#24847;&#20041;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#38750;&#33521;&#35821;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;DRS&#35299;&#26512;&#21644;DRS&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#19988;&#21487;&#20197;&#22823;&#22823;&#38477;&#20302;&#22810;&#35821;&#35328;&#35821;&#20041;&#27880;&#37322;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#24182;&#36817;&#26399;&#34987;&#29992;&#20110;&#35745;&#31639;&#35821;&#20041;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20219;&#21153;&#24182;&#27809;&#26377;&#20805;&#20998;&#21463;&#30410;&#20110;PLMs&#65292;&#22240;&#20026;&#24847;&#20041;&#34920;&#31034;&#24182;&#26410;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#26174;&#24335;&#21253;&#25324;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;&#35821;&#31687;&#34920;&#31034;&#32467;&#26500;&#65288;DRSs&#65289;&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;-&#35821;&#20041;&#27169;&#22411;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#21644;&#24847;&#20041;&#34920;&#31034;&#21516;&#26102;&#21253;&#21547;&#22312;&#19968;&#20010;&#27169;&#22411;&#20013;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#26469;&#20943;&#23569;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#30001;&#20110;DRS&#26159;&#35821;&#35328;&#20013;&#31435;&#30340;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#36827;&#19968;&#27493;&#25552;&#39640;&#38750;&#33521;&#35821;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#33258;&#21160;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#35821;&#35328;DRS&#35299;&#26512;&#21644;DRS&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;&#20851;&#20110;&#29983;&#25104;&#20219;&#21153;&#30340;&#33258;&#21160;&#24230;&#37327;&#21644;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20998;&#26512;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#20154;&#31867;&#35780;&#20272;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22823;&#22823;&#38477;&#20302;&#22810;&#35821;&#35328;&#35821;&#20041;&#27880;&#37322;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) have achieved great success in NLP and have recently been used for tasks in computational semantics. However, these tasks do not fully benefit from PLMs since meaning representations are not explicitly included in the pre-training stage. We introduce multilingual pre-trained language-meaning models based on Discourse Representation Structures (DRSs), including meaning representations besides natural language texts in the same model, and design a new strategy to reduce the gap between the pre-training and fine-tuning objectives. Since DRSs are language neutral, cross-lingual transfer learning is adopted to further improve the performance of non-English tasks. Automatic evaluation results show that our approach achieves the best performance on both the multilingual DRS parsing and DRS-to-text generation tasks. Correlation analysis between automatic metrics and human judgements on the generation task further validates the effectiveness of our model. Huma
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22810;&#35821;&#35328;&#22810;&#27604;&#21947;&#35821;&#35328;&#24314;&#27169;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20934;&#21477;&#23376;&#32423;&#27604;&#21947;&#35821;&#35328;&#26816;&#27979;&#12290;&#36890;&#36807;&#22522;&#20110;&#27169;&#26495;&#30340;&#25552;&#31034;&#23398;&#20064;&#65292;&#32479;&#19968;&#20102;&#36328;&#22810;&#20010;&#20462;&#36766;&#25163;&#27861;&#21644;&#35821;&#35328;&#30456;&#20851;&#30340;&#22810;&#20010;&#26816;&#27979;&#20219;&#21153;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#27604;&#24378;&#22823;&#30340;&#22522;&#32447;&#27169;&#22411;&#26356;&#21152;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2306.00121</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#22810;&#27604;&#21947;&#35821;&#35328;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multilingual Multi-Figurative Language Detection. (arXiv:2306.00121v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00121
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22810;&#35821;&#35328;&#22810;&#27604;&#21947;&#35821;&#35328;&#24314;&#27169;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20934;&#21477;&#23376;&#32423;&#27604;&#21947;&#35821;&#35328;&#26816;&#27979;&#12290;&#36890;&#36807;&#22522;&#20110;&#27169;&#26495;&#30340;&#25552;&#31034;&#23398;&#20064;&#65292;&#32479;&#19968;&#20102;&#36328;&#22810;&#20010;&#20462;&#36766;&#25163;&#27861;&#21644;&#35821;&#35328;&#30456;&#20851;&#30340;&#22810;&#20010;&#26816;&#27979;&#20219;&#21153;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#27604;&#24378;&#22823;&#30340;&#22522;&#32447;&#27169;&#22411;&#26356;&#21152;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27604;&#21947;&#35821;&#35328;&#26377;&#21161;&#20110;&#20154;&#20204;&#34920;&#36798;&#25277;&#35937;&#27010;&#24565;&#12289;&#21796;&#36215;&#27604;&#23383;&#38754;&#34920;&#36798;&#26356;&#24378;&#30340;&#24773;&#24863;&#65292;&#20174;&#32780;&#20351;&#25991;&#26412;&#26356;&#21152;&#26377;&#21019;&#36896;&#21147;&#21644;&#21560;&#24341;&#21147;&#12290;&#23613;&#31649;&#27604;&#21947;&#35821;&#35328;&#29702;&#35299;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#21644;&#32771;&#34385;&#22810;&#20010;&#20462;&#36766;&#25163;&#27861;&#26102;&#65292;&#20854;&#30740;&#31350;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#35821;&#35328;&#22810;&#27604;&#21947;&#35821;&#35328;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#21477;&#23376;&#32423;&#27604;&#21947;&#35821;&#35328;&#26816;&#27979;&#65292;&#28085;&#30422;&#20102;&#19977;&#31181;&#24120;&#35265;&#30340;&#20462;&#36766;&#25163;&#27861;&#21644;&#19971;&#31181;&#35821;&#35328;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22522;&#20110;&#22522;&#20110;&#27169;&#26495;&#30340;&#25552;&#31034;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#20010;&#27604;&#21947;&#35821;&#35328;&#26816;&#27979;&#26694;&#26550;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#32479;&#19968;&#20102;&#36328;&#22810;&#20010;&#20462;&#36766;&#25163;&#27861;&#21644;&#35821;&#35328;&#30456;&#20851;&#30340;&#22810;&#20010;&#26816;&#27979;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#29305;&#23450;&#20110;&#20219;&#21153;&#25110;&#35821;&#35328;&#30340;&#27169;&#22359;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20248;&#20110;&#20960;&#20010;&#24378;&#22823;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Figures of speech help people express abstract concepts and evoke stronger emotions than literal expressions, thereby making texts more creative and engaging. Due to its pervasive and fundamental character, figurative language understanding has been addressed in Natural Language Processing, but it's highly understudied in a multilingual setting and when considering more than one figure of speech at the same time. To bridge this gap, we introduce multilingual multi-figurative language modelling, and provide a benchmark for sentence-level figurative language detection, covering three common figures of speech and seven languages. Specifically, we develop a framework for figurative language detection based on template-based prompt learning. In so doing, we unify multiple detection tasks that are interrelated across multiple figures of speech and languages, without requiring task- or language-specific modules. Experimental results show that our framework outperforms several strong baselines
&lt;/p&gt;</description></item><item><title>MuseCoco&#26159;&#19968;&#31181;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#31526;&#21495;&#38899;&#20048;&#30340;&#31995;&#32479;&#65292;&#20855;&#22791;&#39640;&#25928;&#21644;&#28789;&#27963;&#31561;&#29305;&#28857;&#65292;&#20026;&#38899;&#20048;&#23478;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#38899;&#20048;&#29983;&#25104;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.00110</link><description>&lt;p&gt;
MuseCoco: &#20174;&#25991;&#26412;&#29983;&#25104;&#31526;&#21495;&#38899;&#20048;
&lt;/p&gt;
&lt;p&gt;
MuseCoco: Generating Symbolic Music from Text. (arXiv:2306.00110v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00110
&lt;/p&gt;
&lt;p&gt;
MuseCoco&#26159;&#19968;&#31181;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#31526;&#21495;&#38899;&#20048;&#30340;&#31995;&#32479;&#65292;&#20855;&#22791;&#39640;&#25928;&#21644;&#28789;&#27963;&#31561;&#29305;&#28857;&#65292;&#20026;&#38899;&#20048;&#23478;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#38899;&#20048;&#29983;&#25104;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#38899;&#20048;&#26159;&#19968;&#31181;&#29992;&#25143;&#21451;&#22909;&#30340;&#26041;&#24335;&#65292;&#22240;&#20026;&#25991;&#26412;&#26159;&#30456;&#23545;&#26131;&#20110;&#29992;&#25143;&#21442;&#19982;&#30340;&#30028;&#38754;&#12290;&#32780;&#26377;&#20123;&#26041;&#27861;&#21033;&#29992;&#25991;&#26412;&#26469;&#25511;&#21046;&#38899;&#20048;&#38899;&#39057;&#30340;&#29983;&#25104;&#65292;&#20294;&#26159;&#32534;&#36753;&#29983;&#25104;&#38899;&#39057;&#30340;&#38899;&#20048;&#20803;&#32032;&#23545;&#20110;&#29992;&#25143;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#31526;&#21495;&#38899;&#20048;&#20855;&#26377;&#26131;&#20110;&#32534;&#36753;&#30340;&#20248;&#28857;&#65292;&#20351;&#29992;&#25143;&#26356;&#23481;&#26131;&#25805;&#20316;&#29305;&#23450;&#30340;&#38899;&#20048;&#20803;&#32032;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MuseCoco&#65292;&#23427;&#21033;&#29992;&#38899;&#20048;&#23646;&#24615;&#20316;&#20026;&#26725;&#26753;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#25991;&#26412;&#21040;&#23646;&#24615;&#29702;&#35299;&#21644;&#23646;&#24615;&#21040;&#38899;&#20048;&#29983;&#25104;&#30340;&#20004;&#20010;&#38454;&#27573;&#65292;&#20174;&#32780;&#29983;&#25104;&#31526;&#21495;&#38899;&#20048;&#12290;MuseCoCo&#20195;&#34920;&#38899;&#20048;&#20316;&#26354;&#21103;&#39550;&#39542;&#65292;&#20351;&#38899;&#20048;&#23478;&#21487;&#20197;&#30452;&#25509;&#20174;&#32473;&#23450;&#30340;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#38899;&#20048;&#65292;&#19982;&#20174;&#22836;&#24320;&#22987;&#21019;&#20316;&#30456;&#27604;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;&#35813;&#31995;&#32479;&#20855;&#26377;&#20004;&#20010;&#20027;&#35201;&#20248;&#28857;&#65306;&#25968;&#25454;&#39640;&#25928;&#12290;&#22312;&#23646;&#24615;&#21040;&#38899;&#20048;&#29983;&#25104;&#38454;&#27573;&#65292;&#23646;&#24615;&#21487;&#20197;&#30452;&#25509;&#36827;&#34892;&#32534;&#30721;&#65292;&#32780;&#19981;&#38656;&#35201;&#22823;&#37327;&#30340;&#38899;&#20048;&#25968;&#25454;&#12290;&#20854;&#27425;&#65292;&#27492;&#31995;&#32479;&#20855;&#26377;&#39640;&#32423;&#21035;&#30340;&#28789;&#27963;&#24615;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#36890;&#36807;&#21464;&#26356;&#25991;&#26412;&#36755;&#20837;&#26469;&#29983;&#25104;&#22810;&#26679;&#21270;&#19988;&#26377;&#20010;&#24615;&#30340;&#31526;&#21495;&#38899;&#20048;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating music from text descriptions is a user-friendly mode since the text is a relatively easy interface for user engagement. While some approaches utilize texts to control music audio generation, editing musical elements in generated audio is challenging for users. In contrast, symbolic music offers ease of editing, making it more accessible for users to manipulate specific musical elements. In this paper, we propose MuseCoco, which generates symbolic music from text descriptions with musical attributes as the bridge to break down the task into text-to-attribute understanding and attribute-to-music generation stages. MuseCoCo stands for Music Composition Copilot that empowers musicians to generate music directly from given text descriptions, offering a significant improvement in efficiency compared to creating music entirely from scratch. The system has two main advantages: Firstly, it is data efficient. In the attribute-to-music generation stage, the attributes can be directly e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;MERT&#65292;&#21033;&#29992;&#20102;&#25945;&#24072;&#27169;&#22411;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#30340;&#32452;&#21512;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.00107</link><description>&lt;p&gt;
MERT:&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22768;&#23398;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training. (arXiv:2306.00107v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00107
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;MERT&#65292;&#21033;&#29992;&#20102;&#25945;&#24072;&#27169;&#22411;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#30340;&#32452;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26368;&#36817;&#22312;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#35821;&#38899;&#39046;&#22495;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#35757;&#32451;&#36890;&#29992;&#27169;&#22411;&#30340;&#19968;&#31181;&#24456;&#26377;&#21069;&#26223;&#30340;&#33539;&#20363;&#65292;&#23545;&#20110;&#36328;&#36234;&#38899;&#20048;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#35843;&#24615;&#21644;&#38899;&#39640;&#36825;&#26679;&#30340;&#29305;&#27530;&#38899;&#20048;&#30693;&#35782;&#30340;&#24314;&#27169;&#39047;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22768;&#23398;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;&#65292;&#21363;MERT&#12290;&#22312;&#25105;&#20204;&#30340;&#25506;&#32034;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26356;&#20248;&#31168;&#30340;&#25945;&#24072;&#27169;&#22411;&#32452;&#21512;&#65292;&#36825;&#31181;&#32452;&#21512;&#26041;&#27861;&#22312;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is primarily due to the distinctive challenges associated with modelling musical knowledge, particularly its tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified a superior combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ManagerTower&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;VL&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#65292;&#21487;&#20197;&#38598;&#21512;&#24182;&#32452;&#21512;&#19981;&#21516;&#32423;&#21035;&#30340;&#39044;&#20808;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#19987;&#23478;&#30340;&#35265;&#35299;&#65292;&#24182;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#32858;&#21512;&#21333;&#27169;&#24577;&#35821;&#20041;&#30693;&#35782;&#20197;&#20419;&#36827;&#26356;&#20840;&#38754;&#30340;&#36328;&#27169;&#24577;&#23545;&#40784;&#21644;&#34701;&#21512;&#12290;&#20165;&#20351;&#29992;4M VLP&#25968;&#25454;&#65292;ManagerTower&#22312;&#21508;&#31181;&#19979;&#28216;VL&#20219;&#21153;&#20013;&#37117;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;VQAv2&#27979;&#35797;&#26631;&#20934;&#19979;&#36798;&#21040;&#20102;79.15%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;Flickr30K&#19978;&#30340;IR@1&#20026;86.56% TR@1&#20026;95.64%&#12290;</title><link>http://arxiv.org/abs/2306.00103</link><description>&lt;p&gt;
ManagerTower&#65306;&#32858;&#21512;&#21333;&#27169;&#24577;&#19987;&#23478;&#35265;&#35299;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning. (arXiv:2306.00103v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00103
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ManagerTower&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;VL&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#65292;&#21487;&#20197;&#38598;&#21512;&#24182;&#32452;&#21512;&#19981;&#21516;&#32423;&#21035;&#30340;&#39044;&#20808;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#19987;&#23478;&#30340;&#35265;&#35299;&#65292;&#24182;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#32858;&#21512;&#21333;&#27169;&#24577;&#35821;&#20041;&#30693;&#35782;&#20197;&#20419;&#36827;&#26356;&#20840;&#38754;&#30340;&#36328;&#27169;&#24577;&#23545;&#40784;&#21644;&#34701;&#21512;&#12290;&#20165;&#20351;&#29992;4M VLP&#25968;&#25454;&#65292;ManagerTower&#22312;&#21508;&#31181;&#19979;&#28216;VL&#20219;&#21153;&#20013;&#37117;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;VQAv2&#27979;&#35797;&#26631;&#20934;&#19979;&#36798;&#21040;&#20102;79.15%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;Flickr30K&#19978;&#30340;IR@1&#20026;86.56% TR@1&#20026;95.64%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20004;&#22612;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#19979;&#28216;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#25913;&#36827;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#24037;&#20316;&#36890;&#36807;&#26500;&#24314;&#32534;&#30721;&#22120;&#20043;&#38388;&#30340;&#26725;&#26753;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#23427;&#36973;&#21463;&#20102;&#21333;&#27169;&#24577;&#34920;&#31034;&#30340;&#36880;&#23618;&#21033;&#29992;&#25928;&#26524;&#19981;&#20339;&#30340;&#22256;&#22659;&#65292;&#24182;&#19988;&#19981;&#33021;&#28789;&#27963;&#22320;&#21033;&#29992;&#19981;&#21516;&#32423;&#21035;&#30340;&#21333;&#27169;&#24577;&#35821;&#20041;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ManagerTower&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;VL&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#65292;&#23427;&#38598;&#21512;&#24182;&#32452;&#21512;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#19981;&#21516;&#32423;&#21035;&#30340;&#21333;&#27169;&#24577;&#19987;&#23478;&#30340;&#35265;&#35299;&#12290;&#22312;&#27599;&#20010;&#36328;&#27169;&#24577;&#23618;&#20013;&#24341;&#20837;&#30340;&#31649;&#29702;&#22120;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#32858;&#21512;&#21333;&#27169;&#24577;&#35821;&#20041;&#30693;&#35782;&#65292;&#20197;&#20419;&#36827;&#26356;&#20840;&#38754;&#30340;&#36328;&#27169;&#24577;&#23545;&#40784;&#21644;&#34701;&#21512;&#12290;ManagerTower&#22312;&#26377;&#21644;&#27809;&#26377;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;(VLP)&#30340;&#24773;&#20917;&#19979;&#37117;&#20248;&#20110;&#20197;&#21069;&#30340;&#24378;&#22522;&#32447;&#12290;&#20165;&#20351;&#29992;4M VLP&#25968;&#25454;&#65292;ManagerTower&#22312;&#21508;&#31181;&#19979;&#28216;VL&#20219;&#21153;&#20013;&#37117;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;VQAv2&#27979;&#35797;&#26631;&#20934;&#19979;&#36798;&#21040;&#20102;79.15%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;Flickr30K&#19978;&#30340;IR@1&#20026;86.56% TR@1&#20026;95.64%&#12290;Code and check
&lt;/p&gt;
&lt;p&gt;
Two-Tower Vision-Language (VL) models have shown promising improvements on various downstream VL tasks. Although the most advanced work improves performance by building bridges between encoders, it suffers from ineffective layer-by-layer utilization of uni-modal representations and cannot flexibly exploit different levels of uni-modal semantic knowledge. In this work, we propose ManagerTower, a novel VL model architecture that gathers and combines the insights of pre-trained uni-modal experts at different levels. The managers introduced in each cross-modal layer can adaptively aggregate uni-modal semantic knowledge to facilitate more comprehensive cross-modal alignment and fusion. ManagerTower outperforms previous strong baselines both with and without Vision-Language Pre-training (VLP). With only 4M VLP data, ManagerTower achieves superior performances on various downstream VL tasks, especially 79.15% accuracy on VQAv2 Test-Std, 86.56% IR@1 and 95.64% TR@1 on Flickr30K. Code and check
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#28151;&#21512;&#35821;&#35328;&#20803;&#34920;&#31034;&#36716;&#25442;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#28304;&#35821;&#35328;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#31639;&#27861;&#23454;&#29616;&#20102;&#23545;&#20110;&#26497;&#24230;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#21516;&#26102;&#22312;&#30456;&#21516;&#25968;&#37327;&#30340;&#25968;&#25454;&#19979;&#65292;&#34920;&#31034;&#26356;&#22909;&#22320;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.00100</link><description>&lt;p&gt;
MetaXLR&#8212;&#8212;&#22522;&#20110;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#20302;&#36164;&#28304;&#36328;&#35821;&#35328;&#23398;&#20064;&#30340;&#28151;&#21512;&#35821;&#35328;&#20803;&#34920;&#31034;&#36716;&#25442;(arXiv:2306.00100v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
MetaXLR -- Mixed Language Meta Representation Transformation for Low-resource Cross-lingual Learning based on Multi-Armed Bandit. (arXiv:2306.00100v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00100
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#28151;&#21512;&#35821;&#35328;&#20803;&#34920;&#31034;&#36716;&#25442;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#28304;&#35821;&#35328;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#31639;&#27861;&#23454;&#29616;&#20102;&#23545;&#20110;&#26497;&#24230;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#21516;&#26102;&#22312;&#30456;&#21516;&#25968;&#37327;&#30340;&#25968;&#25454;&#19979;&#65292;&#34920;&#31034;&#26356;&#22909;&#22320;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26497;&#24230;&#20302;&#36164;&#28304;&#30340;&#35821;&#35328;&#32780;&#35328;&#65292;&#36801;&#31227;&#23398;&#20064;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#27809;&#26377;&#22823;&#35268;&#27169;&#30340;&#21333;&#35821;&#35821;&#26009;&#24211;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20063;&#27809;&#26377;&#36275;&#22815;&#30340;&#26631;&#27880;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#32487;&#25215;&#20102;MetaXL&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24314;&#35758;&#20351;&#29992;&#20803;&#23398;&#20064;&#20174;&#21333;&#20010;&#28304;&#35821;&#35328;&#36716;&#31227;&#21040;&#26497;&#24230;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#36873;&#25321;&#22810;&#20010;&#28304;&#35821;&#35328;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#65292;&#20351;&#29992;&#22810;&#33218;&#36172;&#21338;&#26426;&#31639;&#27861;&#26469;&#21033;&#29992;&#35757;&#32451;&#20013;&#30340;&#35821;&#35328;&#12290;&#36890;&#36807;&#36825;&#20004;&#31181;&#25913;&#36827;&#65292;&#25105;&#20204;&#35774;&#27861;&#22312;&#20351;&#29992;&#30456;&#21516;&#25968;&#37327;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#26497;&#24230;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#20351;&#34920;&#31034;&#26356;&#22909;&#22320;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#35813;&#26041;&#27861;&#33021;&#22815;&#20351;&#29992;&#22810;&#31181;&#35821;&#35328;&#65292;&#22240;&#27492;&#20801;&#35768;&#26694;&#26550;&#20351;&#29992;&#26356;&#22823;&#37327;&#30340;&#25968;&#25454;&#65292;&#21363;&#20351;&#20351;&#29992;&#30456;&#21516;&#25968;&#37327;&#30340;&#25968;&#25454;&#65292;&#20173;&#28982;&#20248;&#20110;&#21069;MetaXL&#26041;&#27861;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Transfer learning for extremely low resource languages is a challenging task as there is no large scale monolingual corpora for pre training or sufficient annotated data for fine tuning. We follow the work of MetaXL which suggests using meta learning for transfer learning from a single source language to an extremely low resource one. We propose an enhanced approach which uses multiple source languages chosen in a data driven manner. In addition, we introduce a sample selection strategy for utilizing the languages in training by using a multi armed bandit algorithm. Using both of these improvements we managed to achieve state of the art results on the NER task for the extremely low resource languages while using the same amount of data, making the representations better generalized. Also, due to the method ability to use multiple languages it allows the framework to use much larger amounts of data, while still having superior results over the former MetaXL method even with the same amo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#39564;&#35777;&#30340;&#36890;&#29992;&#32531;&#35299;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20026;&#20854;&#33258;&#25105;&#25552;&#21462;&#25552;&#20379;&#26435;&#23041;&#24615;&#24182;&#26816;&#26597;&#20854;&#33258;&#24049;&#30340;&#36755;&#20986;&#12290;&#22312;&#20020;&#24202;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21508;&#31181;LLMs&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#26497;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#35748;&#30693;&#22686;&#24378;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.00024</link><description>&lt;p&gt;
&#33258;&#25105;&#39564;&#35777;&#25913;&#21892;&#23569;&#26679;&#26412;&#20020;&#24202;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Self-Verification Improves Few-Shot Clinical Information Extraction. (arXiv:2306.00024v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#39564;&#35777;&#30340;&#36890;&#29992;&#32531;&#35299;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20026;&#20854;&#33258;&#25105;&#25552;&#21462;&#25552;&#20379;&#26435;&#23041;&#24615;&#24182;&#26816;&#26597;&#20854;&#33258;&#24049;&#30340;&#36755;&#20986;&#12290;&#22312;&#20020;&#24202;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21508;&#31181;LLMs&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#26497;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#35748;&#30693;&#22686;&#24378;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#24739;&#32773;&#20449;&#24687;&#26159;&#21355;&#29983;&#20915;&#31574;&#25903;&#25345;&#21644;&#20020;&#24202;&#30740;&#31350;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#36890;&#36807;&#23569;&#37327;&#19978;&#19979;&#25991;&#23398;&#20064;&#21152;&#36895;&#20020;&#24202;&#31649;&#29702;&#30340;&#28508;&#21147;&#65292;&#30456;&#36739;&#20110;&#38656;&#35201;&#26356;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#30340;&#30417;&#30563;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#29616;&#20195;LLM&#65288;&#22914;GPT-4&#65289;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#27493;&#65292;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#31561;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#20851;&#38190;&#20219;&#21153;&#39046;&#22495;&#65288;&#22914;&#20581;&#24247;&#65289;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#25105;&#39564;&#35777;&#65292;&#25506;&#32034;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#32531;&#35299;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;LLM&#20026;&#20854;&#33258;&#25105;&#25552;&#21462;&#25552;&#20379;&#26435;&#23041;&#24615;&#24182;&#26816;&#26597;&#33258;&#24049;&#30340;&#36755;&#20986;&#12290;&#36825;&#24471;&#30410;&#20110;&#39564;&#35777;&#21644;&#29983;&#25104;&#20043;&#38388;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#21518;&#32773;&#36890;&#24120;&#27604;&#21069;&#32773;&#23481;&#26131;&#24471;&#22810;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26631;&#20934;&#20020;&#24202;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#65292;&#23545;&#20110;&#21508;&#31181;LLM&#37117;&#33021;&#25345;&#32493;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#33258;&#25105;&#39564;&#35777;&#20135;&#29983;&#20102;&#26497;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#35748;&#30693;&#22686;&#24378;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting patient information from unstructured text is a critical task in health decision-support and clinical research. Large language models (LLMs) have shown the potential to accelerate clinical curation via few-shot in-context learning, in contrast to supervised learning which requires much more costly human annotations. However, despite drastic advances in modern LLMs such as GPT-4, they still struggle with issues regarding accuracy and interpretability, especially in mission-critical domains such as health. Here, we explore a general mitigation framework using self-verification, which leverages the LLM to provide provenance for its own extraction and check its own outputs. This is made possible by the asymmetry between verification and generation, where the latter is often much easier than the former. Experimental results show that our method consistently improves accuracy for various LLMs in standard clinical information extraction tasks. Additionally, self-verification yields
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#35299;&#37322;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#65292;&#37319;&#29992;&#21452;&#21521;&#21464;&#24418;&#22120;&#27169;&#22411;BERT&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#25552;&#20379;&#21518;&#39564;&#30340;&#27169;&#22411;&#26080;&#20851;&#21644;&#20195;&#29702;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#20197;&#25903;&#25345;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#24182;&#38450;&#27490;&#27169;&#22411;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.00021</link><description>&lt;p&gt;
&#37319;&#29992;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#35299;&#37322;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Explaining Hate Speech Classification with Model Agnostic Methods. (arXiv:2306.00021v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35299;&#37322;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#65292;&#37319;&#29992;&#21452;&#21521;&#21464;&#24418;&#22120;&#27169;&#22411;BERT&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#25552;&#20379;&#21518;&#39564;&#30340;&#27169;&#22411;&#26080;&#20851;&#21644;&#20195;&#29702;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#20197;&#25903;&#25345;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#24182;&#38450;&#27490;&#27169;&#22411;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#31361;&#30772;&#12290;&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#23545;&#35805;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20063;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20154;&#21592;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;&#26368;&#36817;&#30340;&#36235;&#21183;&#25152;&#34920;&#26126;&#30340;&#37027;&#26679;&#65292;&#22312;AI&#27169;&#22411;&#20013;&#21152;&#20837;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#32500;&#24230;&#30340;&#38656;&#27714;&#24050;&#32463;&#24471;&#21040;&#28145;&#21051;&#30340;&#35748;&#35782;&#12290;&#26412;&#25991;&#30340;&#30740;&#31350;&#30446;&#26631;&#26159;&#35201;&#22312;&#20167;&#24680;&#35328;&#35770;&#39044;&#27979;&#19982;&#31995;&#32479;&#29983;&#25104;&#30340;&#35299;&#37322;&#20043;&#38388;&#25645;&#24314;&#26725;&#26753;&#12290;&#36890;&#36807;&#39318;&#20808;&#39044;&#27979;&#25991;&#26412;&#30340;&#20998;&#31867;&#65292;&#28982;&#21518;&#25552;&#20379;&#19968;&#20010;&#21518;&#39564;&#30340;&#27169;&#22411;&#26080;&#20851;&#21644;&#20195;&#29702;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#26469;&#25903;&#25345;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#24182;&#38450;&#27490;&#27169;&#22411;&#20559;&#24046;&#12290;&#37319;&#29992;&#21452;&#21521;&#21464;&#24418;&#22120;&#27169;&#22411;BERT&#36827;&#34892;&#39044;&#27979;&#65292;&#22240;&#20026;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
There have been remarkable breakthroughs in Machine Learning and Artificial Intelligence, notably in the areas of Natural Language Processing and Deep Learning. Additionally, hate speech detection in dialogues has been gaining popularity among Natural Language Processing researchers with the increased use of social media. However, as evidenced by the recent trends, the need for the dimensions of explainability and interpretability in AI models has been deeply realised. Taking note of the factors above, the research goal of this paper is to bridge the gap between hate speech prediction and the explanations generated by the system to support its decision. This has been achieved by first predicting the classification of a text and then providing a posthoc, model agnostic and surrogate interpretability approach for explainability and to prevent model bias. The bidirectional transformer model BERT has been used for prediction because of its state of the art efficiency over other Machine Lea
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35843;&#26597;&#20102;GPT-4&#30340;&#22320;&#29702;&#30693;&#35782;&#27700;&#24179;&#65292;&#24182;&#25506;&#35752;&#20854;&#22312;&#22320;&#29702;&#25968;&#25454;&#20998;&#26512;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.00020</link><description>&lt;p&gt;
GPT4GEO&#65306;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#30475;&#24453;&#19990;&#30028;&#22320;&#29702;
&lt;/p&gt;
&lt;p&gt;
GPT4GEO: How a Language Model Sees the World's Geography. (arXiv:2306.00020v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00020
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35843;&#26597;&#20102;GPT-4&#30340;&#22320;&#29702;&#30693;&#35782;&#27700;&#24179;&#65292;&#24182;&#25506;&#35752;&#20854;&#22312;&#22320;&#29702;&#25968;&#25454;&#20998;&#26512;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#28041;&#21450;&#38382;&#39064;&#22238;&#31572;&#12289;&#29983;&#25104;&#36830;&#36143;&#25991;&#26412;&#21644;&#20195;&#30721;&#30340;&#24191;&#27867;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#20840;&#38754;&#29702;&#35299;LLM&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#23545;&#20110;&#23433;&#20840;&#12289;&#19979;&#28216;&#24212;&#29992;&#21644;&#24615;&#33021;&#25913;&#36827;&#37117;&#26377;&#30410;&#22788;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;GPT-4&#33719;&#24471;&#20107;&#23454;&#22320;&#29702;&#30693;&#35782;&#30340;&#31243;&#24230;&#65292;&#24182;&#33021;&#21542;&#23558;&#36825;&#20123;&#30693;&#35782;&#29992;&#20110;&#35299;&#37322;&#24615;&#25512;&#29702;&#65292;&#36825;&#23545;&#20110;&#28041;&#21450;&#22320;&#29702;&#25968;&#25454;&#30340;&#24212;&#29992;&#65288;&#22914;&#22320;&#29702;&#31354;&#38388;&#20998;&#26512;&#12289;&#20379;&#24212;&#38142;&#31649;&#29702;&#21644;&#28798;&#38590;&#21709;&#24212;&#65289;&#23588;&#20854;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#22810;&#26679;&#21270;&#30340;&#23454;&#39564;&#65292;&#20174;&#23450;&#20301;&#12289;&#36317;&#31163;&#21644;&#39640;&#24230;&#20272;&#35745;&#31561;&#20107;&#23454;&#20219;&#21153;&#24320;&#22987;&#65292;&#21040;&#29983;&#25104;&#22269;&#23478;&#36718;&#24275;&#21644;&#26053;&#28216;&#32593;&#32476;&#12289;&#22312;&#32422;&#26463;&#26465;&#20214;&#19979;&#23547;&#25214;&#36335;&#32447;&#21644;&#20379;&#24212;&#38142;&#20998;&#26512;&#31561;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;GPT-4&#65288;&#27809;&#26377;&#25554;&#20214;&#25110;Internet&#35775;&#38382;&#65289;&#20102;&#35299;&#21644;&#19981;&#20102;&#35299;&#19990;&#30028;&#22320;&#29702;&#30340;&#24191;&#27867;&#25551;&#36848;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#22320;&#29702;&#31354;&#38388;&#39046;&#22495;&#24212;&#29992;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown remarkable capabilities across a broad range of tasks involving question answering and the generation of coherent text and code. Comprehensively understanding the strengths and weaknesses of LLMs is beneficial for safety, downstream applications and improving performance. In this work, we investigate the degree to which GPT-4 has acquired factual geographic knowledge and is capable of using this knowledge for interpretative reasoning, which is especially important for applications that involve geographic data, such as geospatial analysis, supply chain management, and disaster response. To this end, we design and conduct a series of diverse experiments, starting from factual tasks such as location, distance and elevation estimation to more complex questions such as generating country outlines and travel networks, route finding under constraints and supply chain analysis. We provide a broad characterisation of what GPT-4 (without plugins or Interne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22810;&#39033;&#24335;&#26420;&#32032;&#36125;&#21494;&#26031;&#31639;&#27861;&#21644;TF-IDF&#21521;&#37327;&#21270;&#22120;&#26816;&#26597;&#33778;&#24459;&#23486;&#26032;&#38395;&#25512;&#25991;&#30340;&#21487;&#20449;&#24230;&#65292;&#22312;&#35757;&#32451;&#21644;&#39044;&#27979;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#24314;&#35758;&#36827;&#19968;&#27493;&#25913;&#36827;&#35821;&#26009;&#24211;&#25910;&#38598;&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#26469;&#21152;&#24378;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.00018</link><description>&lt;p&gt;
&#21033;&#29992;&#22810;&#39033;&#24335;&#26420;&#32032;&#36125;&#21494;&#26031;&#31639;&#27861;&#21644;Term Frequency Inverse Document Frequency (TF-IDF&#21521;&#37327;&#21270;&#22120;)&#26816;&#26597;&#33778;&#24459;&#23486;&#26032;&#38395;&#25512;&#25991;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Utilization of Multinomial Naive Bayes Algorithm and Term Frequency Inverse Document Frequency (TF-IDF Vectorizer) in Checking the Credibility of News Tweet in the Philippines. (arXiv:2306.00018v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22810;&#39033;&#24335;&#26420;&#32032;&#36125;&#21494;&#26031;&#31639;&#27861;&#21644;TF-IDF&#21521;&#37327;&#21270;&#22120;&#26816;&#26597;&#33778;&#24459;&#23486;&#26032;&#38395;&#25512;&#25991;&#30340;&#21487;&#20449;&#24230;&#65292;&#22312;&#35757;&#32451;&#21644;&#39044;&#27979;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#24314;&#35758;&#36827;&#19968;&#27493;&#25913;&#36827;&#35821;&#26009;&#24211;&#25910;&#38598;&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#26469;&#21152;&#24378;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#23186;&#20307;&#30340;&#25968;&#23383;&#21270;&#25104;&#20026;&#36827;&#27493;&#30340;&#33391;&#22909;&#25351;&#26631;&#65292;&#20063;&#26159;&#26356;&#22810;&#23041;&#32961;&#30340;&#20449;&#21495;&#12290;&#34394;&#20551;&#20449;&#24687;&#25110;&#20551;&#26032;&#38395;&#26159;&#20854;&#20013;&#20043;&#19968;&#65292;&#26377;&#24517;&#35201;&#37319;&#21462;&#34892;&#21160;&#26469;&#25171;&#20987;&#34394;&#20551;&#20449;&#24687;&#12290;&#26412;&#25991;&#21033;&#29992;&#22522;&#20110;&#30495;&#23454;&#26631;&#27880;&#30340;&#27880;&#37322;&#21644;TF-IDF&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#26032;&#38395;&#25991;&#31456;&#65292;&#28982;&#21518;&#29992;&#20316;Multinomial Naive Bayes&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#35813;&#27169;&#22411;&#22312;&#35757;&#32451;&#26102;&#30340;&#20934;&#30830;&#29575;&#20026;99.46&#65285;&#65292;&#22312;&#39044;&#27979;&#26410;&#35265;&#25968;&#25454;&#26102;&#20026;88.98&#65285;&#12290;&#23558;&#20551;&#26032;&#38395;&#26631;&#35760;&#20026;&#30495;&#26032;&#38395;&#26159;&#39044;&#27979;&#20013;&#30340;&#19968;&#20010;&#20851;&#27880;&#28857;&#65292;&#22312;F1&#24471;&#20998;&#20013;&#34920;&#29616;&#20026;89.68&#65285;&#12290;&#36825;&#21487;&#33021;&#20250;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#38450;&#27490;&#36825;&#31181;&#24773;&#20917;&#21457;&#29983;&#65292;&#24314;&#35758;&#36827;&#19968;&#27493;&#25913;&#36827;&#35821;&#26009;&#24211;&#25910;&#38598;&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#26469;&#21152;&#24378;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The digitalization of news media become a good indicator of progress and signal to more threats. Media disinformation or fake news is one of these threats, and it is necessary to take any action in fighting disinformation. This paper utilizes ground truth-based annotations and TF-IDF as feature extraction for the news articles which is then used as a training data set for Multinomial Naive Bayes. The model has an accuracy of 99.46% in training and 88.98% in predicting unseen data. Tagging fake news as real news is a concerning point on the prediction that is indicated in the F1 score of 89.68%. This could lead to a negative impact. To prevent this to happen it is suggested to further improve the corpus collection, and use an ensemble machine learning to reinforce the prediction
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#32467;&#21512;&#31526;&#21495;&#34920;&#31034;&#21644;&#33258;&#19979;&#32780;&#19978;&#30340;&#36870;&#21521;&#24037;&#31243;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#27491;&#35821;&#35328;&#29702;&#35299;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#12289;&#35821;&#35328;&#26080;&#20851;&#30340;LLMs&#12290;</title><link>http://arxiv.org/abs/2306.00017</link><description>&lt;p&gt;
&#21521;&#21487;&#35299;&#37322;&#30340;&#12289;&#35821;&#35328;&#26080;&#20851;&#30340;LLMs&#36808;&#36827;&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#31526;&#21495;&#36870;&#21521;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale. (arXiv:2306.00017v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#32467;&#21512;&#31526;&#21495;&#34920;&#31034;&#21644;&#33258;&#19979;&#32780;&#19978;&#30340;&#36870;&#21521;&#24037;&#31243;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#27491;&#35821;&#35328;&#29702;&#35299;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#12289;&#35821;&#35328;&#26080;&#20851;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#19968;&#20010;&#37324;&#31243;&#30865;&#65292;&#26080;&#21487;&#21542;&#35748;&#22320;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20013;&#35768;&#22810;&#20449;&#20208;&#12290;&#28982;&#32780;&#65292;&#24403;&#28041;&#21450;&#30495;&#27491;&#30340;&#35821;&#35328;&#29702;&#35299;&#26102;&#65292;&#36825;&#20123;LLM&#30340;&#35768;&#22810;&#38480;&#21046;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#20123;&#38480;&#21046;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24213;&#23618;&#26550;&#26500;&#30340;&#21103;&#20135;&#21697;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#20122;&#31526;&#21495;&#24615;&#36136;&#65292;&#36825;&#20123;&#27169;&#22411;&#33719;&#24471;&#26377;&#20851;&#35821;&#35328;&#22914;&#20309;&#36816;&#20316;&#30340;&#20219;&#20309;&#30693;&#35782;&#37117;&#23558;&#34987;&#22475;&#22312;&#25968;&#21313;&#20159;&#20010;&#24494;&#29305;&#24449;&#65288;&#26435;&#37325;&#65289;&#20013;&#65292;&#20854;&#20013;&#27809;&#26377;&#19968;&#20010;&#21333;&#29420;&#30340;&#29305;&#24449;&#26377;&#24847;&#20041;&#65292;&#20351;&#24471;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#31526;&#21495;&#34920;&#31034;&#30340;&#24378;&#24230;&#19982;&#25105;&#20204;&#35748;&#20026;&#26159;LLMs&#25104;&#21151;&#30340;&#20851;&#38190;&#32467;&#21512;&#36215;&#26469;&#65292;&#21363;&#22312;&#35268;&#27169;&#19978;&#25104;&#21151;&#22320;&#36827;&#34892;&#33258;&#19979;&#32780;&#19978;&#30340;&#35821;&#35328;&#36870;&#21521;&#24037;&#31243;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20027;&#24352;&#22312;&#31526;&#21495;&#35774;&#32622;&#19979;&#23545;&#35821;&#35328;&#36827;&#34892;&#33258;&#19979;&#32780;&#19978;&#30340;&#36870;&#21521;&#24037;&#31243;&#12290;&#19968;&#20123;&#20316;&#32773;&#25552;&#20986;&#20102;&#36825;&#20010;&#39033;&#30446;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#23558;&#36827;&#34892;&#35814;&#32454;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved a milestone that undenia-bly changed many held beliefs in artificial intelligence (AI). However, there remains many limitations of these LLMs when it comes to true language understanding, limitations that are a byproduct of the under-lying architecture of deep neural networks. Moreover, and due to their subsymbolic nature, whatever knowledge these models acquire about how language works will always be buried in billions of microfeatures (weights), none of which is meaningful on its own, making such models hopelessly unexplainable. To address these limitations, we suggest com-bining the strength of symbolic representations with what we believe to be the key to the success of LLMs, namely a successful bottom-up re-verse engineering of language at scale. As such we argue for a bottom-up reverse engineering of language in a symbolic setting. Hints on what this project amounts to have been suggested by several authors, and we discuss in some detail
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#37327;&#21270;&#21069;&#24494;&#35843;&#8221;&#26694;&#26550; PreQuant&#65292;&#21487;&#29992;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#37327;&#21270;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#21644;&#20351;&#29992;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.00014</link><description>&lt;p&gt;
&#38754;&#21521;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#37327;&#21270;&#26041;&#27861;PreQuant
&lt;/p&gt;
&lt;p&gt;
PreQuant: A Task-agnostic Quantization Approach for Pre-trained Language Models. (arXiv:2306.00014v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#37327;&#21270;&#21069;&#24494;&#35843;&#8221;&#26694;&#26550; PreQuant&#65292;&#21487;&#29992;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#37327;&#21270;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#21644;&#20351;&#29992;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24050;&#32463;&#22312;&#35768;&#22810;NLP&#24212;&#29992;&#20013;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#37096;&#32626;&#22797;&#26434;&#12289;&#20351;&#29992;&#26114;&#36149;&#65292;&#22240;&#27492;&#26377;&#25928;&#21387;&#32553;&#22823;&#35268;&#27169;PLMs&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#37327;&#21270;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#29992;&#20302;&#27604;&#29305;&#23450;&#28857;&#26684;&#24335;&#34920;&#31034;&#39640;&#31934;&#24230;&#24352;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#37327;&#21270;&#21069;&#24494;&#35843;&#8221;&#26694;&#26550;PreQuant&#65292;&#23427;&#19982;&#21508;&#31181;&#37327;&#21270;&#31574;&#30053;&#20860;&#23481;&#65292;&#24182;&#32467;&#21512;&#8220;&#24322;&#24120;&#20540;&#24863;&#30693;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#8221;&#36827;&#34892;&#26657;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;
While transformer-based pre-trained language models (PLMs) have dominated a number of NLP applications, these models are heavy to deploy and expensive to use. Therefore, effectively compressing large-scale PLMs becomes an increasingly important problem. Quantization, which represents high-precision tensors with low-bit fix-point format, is a viable solution. However, most existing quantization methods are task-specific, requiring customized training and quantization with a large number of trainable parameters on each individual task. Inspired by the observation that the over-parameterization nature of PLMs makes it possible to freeze most of the parameters during the fine-tuning stage, in this work, we propose a novel ``quantize before fine-tuning'' framework, PreQuant, that differs from both quantization-aware training and post-training quantization. PreQuant is compatible with various quantization strategies, with outlier-aware parameter-efficient fine-tuning incorporated to correct 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26412;&#20998;&#31867;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#33258;&#21160;&#20174;&#22823;&#37327;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#30284;&#30151;&#30456;&#20851;&#23454;&#20307;&#21644;&#23454;&#20307;&#38388;&#20851;&#31995;&#65292;&#20197;&#24110;&#21161;&#25512;&#36827;&#30284;&#30151;&#30740;&#31350;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.00013</link><description>&lt;p&gt;
&#30284;&#30151;&#23454;&#20307;&#30340;&#20851;&#32852;&#21644;&#20998;&#31867;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Approach for Cancer Entities Association and Classification. (arXiv:2306.00013v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26412;&#20998;&#31867;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#33258;&#21160;&#20174;&#22823;&#37327;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#30284;&#30151;&#30456;&#20851;&#23454;&#20307;&#21644;&#23454;&#20307;&#38388;&#20851;&#31995;&#65292;&#20197;&#24110;&#21161;&#25512;&#36827;&#30284;&#30151;&#30740;&#31350;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#19990;&#30028;&#21355;&#29983;&#32452;&#32455;&#65288;WHO&#65289;&#30340;&#25968;&#25454;&#65292;&#30284;&#30151;&#26159;&#20840;&#29699;&#31532;&#20108;&#22823;&#27515;&#22240;&#12290;&#19981;&#21516;&#31867;&#22411;&#30284;&#30151;&#30340;&#31185;&#23398;&#30740;&#31350;&#20197;&#27599;&#24180;&#21457;&#24067;&#22823;&#37327;&#30340;&#30740;&#31350;&#25991;&#31456;&#30340;&#36895;&#24230;&#19981;&#26029;&#22686;&#38271;&#12290;&#19982;&#22522;&#22240;&#30456;&#20851;&#30340;&#33647;&#29289;&#12289;&#35786;&#26029;&#12289;&#39118;&#38505;&#12289;&#30151;&#29366;&#12289;&#27835;&#30103;&#31561;&#30340;&#20449;&#24687;&#21644;&#30693;&#35782;&#26159;&#24110;&#21161;&#25506;&#32034;&#21644;&#25512;&#36827;&#30284;&#30151;&#30740;&#31350;&#36827;&#23637;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#25163;&#21160;&#31579;&#36873;&#36825;&#20040;&#22823;&#37327;&#30340;&#25991;&#31456;&#38750;&#24120;&#36153;&#26102;&#36153;&#21147;&#65292;&#24456;&#38590;&#21046;&#23450;&#20219;&#20309;&#20551;&#35774;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#20004;&#31181;&#26368;&#20026;&#37325;&#35201;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21151;&#33021;&#65292;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26412;&#20998;&#31867;&#65292;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#21457;&#29616;&#30693;&#35782;&#12290;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20511;&#21161;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#21644;&#20869;&#32622;&#23383;&#20856;&#35782;&#21035;&#24182;&#25552;&#21462;&#19982;&#30284;&#30151;&#30456;&#20851;&#30340;&#39044;&#23450;&#20041;&#23454;&#20307;&#12290;&#25991;&#26412;&#20998;&#31867;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24110;&#21161;&#25506;&#31350;&#30284;&#30151;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
According to the World Health Organization (WHO), cancer is the second leading cause of death globally. Scientific research on different types of cancers grows at an ever-increasing rate, publishing large volumes of research articles every year. The insight information and the knowledge of the drug, diagnostics, risk, symptoms, treatments, etc., related to genes are significant factors that help explore and advance the cancer research progression. Manual screening of such a large volume of articles is very laborious and time-consuming to formulate any hypothesis. The study uses the two most non-trivial NLP, Natural Language Processing functions, Entity Recognition, and text classification to discover knowledge from biomedical literature. Named Entity Recognition (NER) recognizes and extracts the predefined entities related to cancer from unstructured text with the support of a user-friendly interface and built-in dictionaries. Text classification helps to explore the insights into the 
&lt;/p&gt;</description></item><item><title>Brainformers &#26159;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#23618;&#32423;&#30340;&#32467;&#26500;&#23436;&#21892;&#20102; Transformer &#30340;&#35774;&#35745;&#32570;&#38519;&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#25910;&#25947;&#21644;&#26356;&#24555;&#30340;&#27493;&#39588;&#26102;&#38388;&#65292;&#34920;&#29616;&#20986;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00008</link><description>&lt;p&gt;
Brainformers&#65306;&#20197;&#25928;&#29575;&#25442;&#21462;&#31616;&#27905;&#24615;
&lt;/p&gt;
&lt;p&gt;
Brainformers: Trading Simplicity for Efficiency. (arXiv:2306.00008v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00008
&lt;/p&gt;
&lt;p&gt;
Brainformers &#26159;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#23618;&#32423;&#30340;&#32467;&#26500;&#23436;&#21892;&#20102; Transformer &#30340;&#35774;&#35745;&#32570;&#38519;&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#25910;&#25947;&#21644;&#26356;&#24555;&#30340;&#27493;&#39588;&#26102;&#38388;&#65292;&#34920;&#29616;&#20986;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer &#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26368;&#36817;&#25104;&#21151;&#30340;&#26680;&#24515;&#25216;&#26415;&#12290;Transformer &#20855;&#26377;&#19968;&#20010;&#20960;&#20046;&#32479;&#19968;&#30340;&#39592;&#26550;&#65292;&#20854;&#20013;&#23618;&#27425;&#22312;&#21069;&#39304;&#21644;&#33258;&#27880;&#24847;&#21147;&#20043;&#38388;&#20132;&#26367;&#20197;&#24314;&#31435;&#28145;&#24230;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#35774;&#35745;&#36873;&#25321;&#65292;&#24182;&#21457;&#29616;&#26356;&#22797;&#26434;&#30340;&#22359;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#23436;&#25104;&#20219;&#21153;&#12290;&#26681;&#25454;&#36825;&#20010;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#22359;&#65292;&#31216;&#20026; Brainformer&#65292;&#23427;&#30001;&#21508;&#31181;&#24418;&#24335;&#30340;&#23618;&#24402;&#19968;&#21270;&#21644;&#28608;&#27963;&#20989;&#25968;&#12289;&#31232;&#30095;&#38376;&#25511;&#21069;&#39304;&#23618;&#12289;&#23494;&#38598;&#21069;&#39304;&#23618;&#12289;&#27880;&#24847;&#21147;&#23618;&#31561;&#22810;&#26679;&#23618;&#32423;&#32452;&#25104;&#12290;&#22312;&#36136;&#37327;&#21644;&#25928;&#29575;&#26041;&#38754;&#65292;Brainformer &#24635;&#26159;&#20248;&#20110;&#29616;&#26377;&#30340;&#31264;&#23494;&#21644;&#31232;&#30095; Transformer&#12290;&#19968;&#20010;&#20855;&#26377; 80 &#20159;&#20010;&#27599;&#20010;&#26631;&#35760;&#28608;&#27963;&#21442;&#25968;&#30340; Brainformer &#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#20854; GLaM &#23545;&#24212;&#29289;&#65292;&#34920;&#29616;&#20986; 2 &#20493;&#26356;&#24555;&#30340;&#35757;&#32451;&#25910;&#25947;&#21644; 5 &#20493;&#26356;&#24555;&#30340;&#27493;&#39588;&#26102;&#38388;&#12290;&#22312;&#19979;&#28216;&#20219;&#21153;&#35780;&#20272;&#20013;&#65292;Brainformer &#20063;&#34920;&#29616;&#24471;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are central to recent successes in natural language processing and computer vision. Transformers have a mostly uniform backbone where layers alternate between feed-forward and self-attention in order to build a deep network. Here we investigate this design choice and find that more complex blocks that have different permutations of layer primitives can be more efficient. Using this insight, we develop a complex block, named Brainformer, that consists of a diverse sets of layers such as sparsely gated feed-forward layers, dense feed-forward layers, attention layers, and various forms of layer normalization and activation functions. Brainformer consistently outperforms the state-of-the-art dense and sparse Transformers, in terms of both quality and efficiency. A Brainformer model with 8 billion activated parameters per token demonstrates 2x faster training convergence and 5x faster step time compared to its GLaM counterpart. In downstream task evaluation, Brainformer also de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#22235;&#20010;&#27861;&#24459;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#20004;&#20010;&#26410;&#26631;&#35760;&#65292;&#21478;&#22806;&#20004;&#20010;&#20351;&#29992;&#21551;&#21457;&#24335;&#26631;&#31614;&#36827;&#34892;&#20102;&#26631;&#35760;&#65292;&#26088;&#22312;&#29992;&#20110;&#25991;&#26412;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#12290;&#30740;&#31350;&#27604;&#36739;&#20102;&#24369;&#30417;&#30563;&#21644;&#20256;&#32479;&#27880;&#37322;&#36807;&#31243;&#26041;&#27861;&#65292;&#34920;&#26126;&#24369;&#30417;&#30563;&#21487;&#20197;&#36798;&#21040;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00007</link><description>&lt;p&gt;
&#33889;&#35821;&#27861;&#24459;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#25968;&#25454;&#38598;&#65306;&#27604;&#36739;&#24369;&#30417;&#30563;&#21644;&#27880;&#37322;&#36807;&#31243;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Datasets for Portuguese Legal Semantic Textual Similarity: Comparing weak supervision and an annotation process approaches. (arXiv:2306.00007v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#22235;&#20010;&#27861;&#24459;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#20004;&#20010;&#26410;&#26631;&#35760;&#65292;&#21478;&#22806;&#20004;&#20010;&#20351;&#29992;&#21551;&#21457;&#24335;&#26631;&#31614;&#36827;&#34892;&#20102;&#26631;&#35760;&#65292;&#26088;&#22312;&#29992;&#20110;&#25991;&#26412;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#12290;&#30740;&#31350;&#27604;&#36739;&#20102;&#24369;&#30417;&#30563;&#21644;&#20256;&#32479;&#27880;&#37322;&#36807;&#31243;&#26041;&#27861;&#65292;&#34920;&#26126;&#24369;&#30417;&#30563;&#21487;&#20197;&#36798;&#21040;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24052;&#35199;&#21496;&#27861;&#26426;&#20851;&#24037;&#20316;&#37327;&#22823;&#65292;&#23548;&#33268;&#21496;&#27861;&#31243;&#24207;&#26102;&#38388;&#38271;&#12290;&#24052;&#35199;&#22269;&#23478;&#21496;&#27861;&#22996;&#21592;&#20250;&#22312;469/2022&#21495;&#20915;&#35758;&#20013;&#21046;&#23450;&#20102;&#25968;&#23383;&#21270;&#25991;&#20214;&#21644;&#27969;&#31243;&#30340;&#27491;&#24335;&#25351;&#23548;&#26041;&#38024;&#65292;&#24320;&#25299;&#20102;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#20351;&#29992;&#33258;&#21160;&#25216;&#26415;&#20197;&#24110;&#21161;&#26085;&#24120;&#20219;&#21153;&#30340;&#21487;&#33021;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#22823;&#37327;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#12290;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20801;&#35768;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#22788;&#29702;&#21644;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#65292;&#28508;&#22312;&#22320;&#21152;&#24555;&#35813;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#22810;&#20010;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#25152;&#38656;&#30340;&#27861;&#24459;&#39046;&#22495;&#25968;&#25454;&#38598;&#24456;&#23569;&#19988;&#38590;&#20197;&#33719;&#24471;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#19987;&#23478;&#30340;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#20026;&#27861;&#24459;&#39046;&#22495;&#36129;&#29486;&#20102;&#22235;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#20004;&#20010;&#20855;&#26377;&#25991;&#26723;&#21644;&#20803;&#25968;&#25454;&#20294;&#26410;&#26631;&#35760;&#65292;&#21478;&#22806;&#20004;&#20010;&#21017;&#26631;&#35760;&#20102;&#21551;&#21457;&#24335;&#26631;&#31614;&#65292;&#26088;&#22312;&#29992;&#20110;&#25991;&#26412;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35780;&#20272;&#20351;&#29992;&#24369;&#30417;&#30563;&#21019;&#24314;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#20256;&#32479;&#30340;&#27880;&#37322;&#36807;&#31243;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24369;&#30417;&#30563;&#21487;&#20197;&#26159;&#19968;&#20010;&#21487;&#34892;&#30340;&#36873;&#25321;&#65292;&#36798;&#21040;&#20102;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Brazilian judiciary has a large workload, resulting in a long time to finish legal proceedings. Brazilian National Council of Justice has established in Resolution 469/2022 formal guidance for document and process digitalization opening up the possibility of using automatic techniques to help with everyday tasks in the legal field, particularly in a large number of texts yielded on the routine of law procedures. Notably, Artificial Intelligence (AI) techniques allow for processing and extracting useful information from textual data, potentially speeding up the process. However, datasets from the legal domain required by several AI techniques are scarce and difficult to obtain as they need labels from experts. To address this challenge, this article contributes with four datasets from the legal domain, two with documents and metadata but unlabeled, and another two labeled with a heuristic aiming at its use in textual semantic similarity tasks. Also, to evaluate the effectiveness of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20004;&#38454;&#27573;&#35299;&#30721;&#26426;&#21046;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#20195;&#30721;&#30340;&#23618;&#27425;&#23646;&#24615;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#20004;&#20010;&#27493;&#39588;&#65292;&#23454;&#29616;&#23545;ICD&#20195;&#30721;&#30340;&#39640;&#25928;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.00005</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;ICD&#32534;&#30721;&#30340;&#20004;&#38454;&#27573;&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Two-Stage Decoder for Efficient ICD Coding. (arXiv:2306.00005v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20004;&#38454;&#27573;&#35299;&#30721;&#26426;&#21046;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#20195;&#30721;&#30340;&#23618;&#27425;&#23646;&#24615;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#20004;&#20010;&#27493;&#39588;&#65292;&#23454;&#29616;&#23545;ICD&#20195;&#30721;&#30340;&#39640;&#25928;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#20445;&#20581;&#26426;&#26500;&#20013;&#30340;&#20020;&#24202;&#35760;&#24405;&#23558;&#20351;&#29992;&#22269;&#38469;&#30142;&#30149;&#20998;&#31867;&#65288;ICD&#65289;&#32534;&#30721;&#26631;&#35760;; &#36825;&#26159;&#19968;&#32452;&#29992;&#20110;&#21307;&#30103;&#35786;&#26029;&#21644;&#25163;&#26415;&#30340;&#20998;&#31867;&#32534;&#30721;&#12290;&#30001;&#20110;&#22024;&#26434;&#30340;&#20020;&#24202;&#25991;&#26723;&#36755;&#20837;&#21644;&#38271;&#23614;&#24335;&#26631;&#31614;&#20998;&#24067;&#65292;ICD&#32534;&#30721;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#33258;&#21160;ICD&#32534;&#30721;&#26041;&#27861;&#36890;&#36807;&#39069;&#22806;&#30340;&#25968;&#25454;&#21644;&#30693;&#35782;&#24211;&#23545;&#21307;&#30103;&#31508;&#35760;&#21644;&#20195;&#30721;&#36827;&#34892;&#32534;&#30721;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#22823;&#22810;&#25968;&#19981;&#21453;&#26144;&#20986;&#20154;&#31867;&#32534;&#30721;&#32773;&#22914;&#20309;&#29983;&#25104;&#20195;&#30721;&#65306;&#39318;&#20808;&#65292;&#32534;&#30721;&#21592;&#36873;&#25321;&#36890;&#29992;&#20195;&#30721;&#31867;&#21035;&#65292;&#28982;&#21518;&#26597;&#25214;&#19982;&#24739;&#32773;&#24773;&#20917;&#30456;&#20851;&#30340;&#20855;&#20307;&#23376;&#31867;&#21035;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#35299;&#30721;&#26426;&#21046;&#26469;&#39044;&#27979;ICD&#20195;&#30721;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#20195;&#30721;&#30340;&#23618;&#27425;&#23646;&#24615;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#20004;&#20010;&#27493;&#39588;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#39044;&#27979;&#29238;&#20195;&#30721;&#65292;&#28982;&#21518;&#26681;&#25454;&#20043;&#21069;&#30340;&#39044;&#27979;&#39044;&#27979;&#23376;&#20195;&#30721;&#12290;&#20844;&#20849;MIMIC-III&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical notes in healthcare facilities are tagged with the International Classification of Diseases (ICD) code; a list of classification codes for medical diagnoses and procedures. ICD coding is a challenging multilabel text classification problem due to noisy clinical document inputs and long-tailed label distribution. Recent automated ICD coding efforts improve performance by encoding medical notes and codes with additional data and knowledge bases. However, most of them do not reflect how human coders generate the code: first, the coders select general code categories and then look for specific subcategories that are relevant to a patient's condition. Inspired by this, we propose a two-stage decoding mechanism to predict ICD codes. Our model uses the hierarchical properties of the codes to split the prediction into two steps: At first, we predict the parent code and then predict the child code based on the previous prediction. Experiments on the public MIMIC-III data set show that 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31867;&#20197;&#20915;&#31574;&#20026;&#23548;&#21521;&#30340;&#20154;&#26426;&#23545;&#35805;&#20219;&#21153;&#65292;&#20197;&#21450;&#22312;&#20250;&#35758;&#35770;&#25991;&#23457;&#31295;&#20154;&#20998;&#37197;&#12289;&#22478;&#24066;&#22810;&#27493;&#34892;&#31243;&#35268;&#21010;&#21644;&#26053;&#34892;&#35745;&#21010;&#21327;&#21830;&#31561;&#22330;&#26223;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#21644;&#29992;&#25143;&#19981;&#21516;&#30340;&#33021;&#21147;&#22914;&#20309;&#32467;&#21512;&#20197;&#36798;&#21040;&#26368;&#20339;&#20915;&#31574;&#12290;&#35770;&#25991;&#36890;&#36807;&#26500;&#24314;&#23545;&#35805;&#29615;&#22659;&#24182;&#36827;&#34892;&#20154;&#26426;&#23545;&#35805;&#25910;&#38598;&#25968;&#25454;&#65292;&#21457;&#29616;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#22312;&#27492;&#31867;&#20219;&#21153;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.20076</link><description>&lt;p&gt;
&#20197;&#20915;&#31574;&#20026;&#23548;&#21521;&#30340;&#20154;&#26426;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Decision-Oriented Dialogue for Human-AI Collaboration. (arXiv:2305.20076v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.20076
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31867;&#20197;&#20915;&#31574;&#20026;&#23548;&#21521;&#30340;&#20154;&#26426;&#23545;&#35805;&#20219;&#21153;&#65292;&#20197;&#21450;&#22312;&#20250;&#35758;&#35770;&#25991;&#23457;&#31295;&#20154;&#20998;&#37197;&#12289;&#22478;&#24066;&#22810;&#27493;&#34892;&#31243;&#35268;&#21010;&#21644;&#26053;&#34892;&#35745;&#21010;&#21327;&#21830;&#31561;&#22330;&#26223;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#21644;&#29992;&#25143;&#19981;&#21516;&#30340;&#33021;&#21147;&#22914;&#20309;&#32467;&#21512;&#20197;&#36798;&#21040;&#26368;&#20339;&#20915;&#31574;&#12290;&#35770;&#25991;&#36890;&#36807;&#26500;&#24314;&#23545;&#35805;&#29615;&#22659;&#24182;&#36827;&#34892;&#20154;&#26426;&#23545;&#35805;&#25910;&#38598;&#25968;&#25454;&#65292;&#21457;&#29616;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#22312;&#27492;&#31867;&#20219;&#21153;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31867;&#20219;&#21153;&#65292;&#31216;&#20026;&#20197;&#20915;&#31574;&#20026;&#23548;&#21521;&#30340;&#23545;&#35805;&#65292;&#20854;&#20013;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#24517;&#39035;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#19982;&#19968;&#21517;&#25110;&#22810;&#21517;&#20154;&#31867;&#21512;&#20316;&#65292;&#24110;&#21161;&#20182;&#20204;&#20570;&#20986;&#22797;&#26434;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#39046;&#22495;&#20013;&#24418;&#24335;&#21270;&#29992;&#25143;&#38754;&#20020;&#26085;&#24120;&#20915;&#31574;&#30340;&#36807;&#31243;&#65306;&#65288;1&#65289;&#36873;&#25321;&#20250;&#35758;&#35770;&#25991;&#30340;&#23457;&#31295;&#20154;&#20998;&#37197;&#65292;&#65288;2&#65289;&#22312;&#22478;&#24066;&#20013;&#35268;&#21010;&#22810;&#27493;&#34892;&#31243;&#65292;&#20197;&#21450;&#65288;3&#65289;&#20026;&#19968;&#32676;&#26379;&#21451;&#21327;&#21830;&#26053;&#34892;&#35745;&#21010;&#12290;&#22312;&#27599;&#20010;&#35774;&#32622;&#20013;&#65292;AI&#21161;&#25163;&#21644;&#29992;&#25143;&#20855;&#26377;&#19981;&#21516;&#30340;&#33021;&#21147;&#65292;&#20182;&#20204;&#24517;&#39035;&#32467;&#21512;&#36215;&#26469;&#24471;&#20986;&#26368;&#20339;&#20915;&#31574;&#65306;&#21161;&#25163;&#21487;&#20197;&#35775;&#38382;&#21644;&#22788;&#29702;&#22823;&#37327;&#20449;&#24687;&#65292;&#32780;&#29992;&#25143;&#20855;&#26377;&#31995;&#32479;&#22806;&#30340;&#20559;&#22909;&#21644;&#38480;&#21046;&#12290;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#23545;&#35805;&#29615;&#22659;&#65292;&#20854;&#20013;&#20195;&#29702;&#26681;&#25454;&#20182;&#20204;&#36798;&#21040;&#30340;&#26368;&#32456;&#20915;&#31574;&#30340;&#36136;&#37327;&#33719;&#24471;&#22870;&#21169;&#12290;&#20351;&#29992;&#36825;&#20123;&#29615;&#22659;&#65292;&#25105;&#20204;&#19982;&#20154;&#20204;&#25198;&#28436;&#21161;&#25163;&#30340;&#20154;&#36827;&#34892;&#20102;&#20154;&#26426;&#23545;&#35805;&#12290;&#20026;&#20102;&#27604;&#36739;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#30340;&#20132;&#27969;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#19982;&#20154;&#31867;-&#20154;&#31867;&#23545;&#35805;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#20915;&#31574;&#23548;&#21521;&#23545;&#35805;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#20984;&#26174;&#20102;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe a class of tasks called decision-oriented dialogues, in which AI assistants must collaborate with one or more humans via natural language to help them make complex decisions. We formalize three domains in which users face everyday decisions: (1) choosing an assignment of reviewers to conference papers, (2) planning a multi-step itinerary in a city, and (3) negotiating travel plans for a group of friends. In each of these settings, AI assistants and users have disparate abilities that they must combine to arrive at the best decision: assistants can access and process large amounts of information, while users have preferences and constraints external to the system. For each task, we build a dialogue environment where agents receive a reward based on the quality of the final decision they reach. Using these environments, we collect human-human dialogues with humans playing the role of assistant. To compare how current AI assistants communicate in these settings, we present bas
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#21453;&#21521;&#20256;&#25773;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#8212;&#8212;&#26463;&#25628;&#32034;&#36882;&#24402;&#21333;&#20803;&#65288;BT-Cell&#65289;&#65292;&#29992;&#20110;&#25193;&#23637;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#23545;&#28508;&#22312;&#32467;&#26500;&#30340;&#24863;&#30693;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#26494;&#26463;&#25628;&#32034;&#20013;&#30828;&#21069;k&#31639;&#23376;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26799;&#24230;&#20449;&#21495;&#20256;&#36882;&#12290;&#22312;&#35780;&#20272;&#20013;&#21457;&#29616;&#65292;BT-Cell&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#22810;&#20010;&#20855;&#26377;&#32467;&#26500;&#25935;&#24863;&#24615;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.19999</link><description>&lt;p&gt;
&#26463;&#25628;&#32034;&#36882;&#24402;&#21333;&#20803;&#65306;&#19968;&#31181;&#25903;&#25345;&#21453;&#21521;&#20256;&#25773;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Beam Tree Recursive Cells. (arXiv:2305.19999v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#21453;&#21521;&#20256;&#25773;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#8212;&#8212;&#26463;&#25628;&#32034;&#36882;&#24402;&#21333;&#20803;&#65288;BT-Cell&#65289;&#65292;&#29992;&#20110;&#25193;&#23637;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#23545;&#28508;&#22312;&#32467;&#26500;&#30340;&#24863;&#30693;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#26494;&#26463;&#25628;&#32034;&#20013;&#30828;&#21069;k&#31639;&#23376;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26799;&#24230;&#20449;&#21495;&#20256;&#36882;&#12290;&#22312;&#35780;&#20272;&#20013;&#21457;&#29616;&#65292;BT-Cell&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#22810;&#20010;&#20855;&#26377;&#32467;&#26500;&#25935;&#24863;&#24615;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#26463;&#25628;&#32034;&#36882;&#24402;&#21333;&#20803;&#65288;BT-Cell&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25193;&#23637;&#25903;&#25345;&#20351;&#29992;&#26463;&#25628;&#32034;&#36827;&#34892;&#28508;&#22312;&#32467;&#26500;&#24863;&#30693;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RvNN&#65289;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#25552;&#20986;&#22312;&#26463;&#25628;&#32034;&#20013;&#23545;&#30828;&#24615;&#21069;k&#31639;&#23376;&#30340;&#25918;&#26494;&#26469;&#25193;&#23637;&#27492;&#26694;&#26550;&#65292;&#20197;&#26356;&#22909;&#22320;&#20256;&#36882;&#26799;&#24230;&#20449;&#21495;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#19981;&#21516;&#20195;&#34920;&#24615;&#20998;&#24067;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BT-Cell&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20307;&#29616;&#32467;&#26500;&#25935;&#24863;&#24615;&#30340;&#20219;&#21153;&#65288;&#22914;ListOps&#21644;&#36923;&#36753;&#25512;&#29702;&#65289;&#19978;&#36798;&#21040;&#20102;&#20960;&#20046;&#23436;&#32654;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#19982;&#20854;&#20182;&#22522;&#20110;RvNN&#30340;&#27169;&#22411;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;ListOps&#20013;&#30830;&#23450;&#20102;&#31070;&#32463;&#27169;&#22411;&#22312;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#21442;&#25968;&#25968;&#37327;&#19978;&#30340;&#26410;&#30693;&#22833;&#25928;&#26696;&#20363;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/JRC1995/BeamTreeRecursiveCells&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Beam Tree Recursive Cell (BT-Cell) - a backpropagation-friendly framework to extend Recursive Neural Networks (RvNNs) with beam search for latent structure induction. We further extend this framework by proposing a relaxation of the hard top-k operators in beam search for better propagation of gradient signals. We evaluate our proposed models in different out-of-distribution splits in both synthetic and realistic data. Our experiments show that BTCell achieves near-perfect performance on several challenging structure-sensitive synthetic tasks like ListOps and logical inference while maintaining comparable performance in realistic data against other RvNN-based models. Additionally, we identify a previously unknown failure case for neural models in generalization to unseen number of arguments in ListOps. The code is available at: https://github.com/JRC1995/BeamTreeRecursiveCells.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#30721;&#21644;&#28151;&#21512;&#22120;&#30456;&#21327;&#20316;&#30340;&#31471;&#21040;&#31471;&#34920;&#29616;&#21147;TTS&#65292;&#23427;&#37319;&#29992;&#26032;&#30340;&#36755;&#20837;&#34920;&#31034;&#21644;&#31616;&#21333;&#30340;&#26550;&#26500;&#26469;&#23454;&#29616;&#25913;&#36827;&#30340;&#38901;&#24459;&#24314;&#27169;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19567</link><description>&lt;p&gt;
DC CoMix TTS&#65306;&#19968;&#31181;&#19982;&#28151;&#21512;&#22120;&#21327;&#20316;&#30340;&#31471;&#21040;&#31471;&#34920;&#29616;&#21147;TTS&#65292;&#21033;&#29992;&#31163;&#25955;&#30721;&#23454;&#29616;&#25913;&#36827;&#30340;&#38901;&#24459;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
DC CoMix TTS: An End-to-End Expressive TTS with Discrete Code Collaborated with Mixer. (arXiv:2305.19567v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#30721;&#21644;&#28151;&#21512;&#22120;&#30456;&#21327;&#20316;&#30340;&#31471;&#21040;&#31471;&#34920;&#29616;&#21147;TTS&#65292;&#23427;&#37319;&#29992;&#26032;&#30340;&#36755;&#20837;&#34920;&#31034;&#21644;&#31616;&#21333;&#30340;&#26550;&#26500;&#26469;&#23454;&#29616;&#25913;&#36827;&#30340;&#38901;&#24459;&#24314;&#27169;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20013;&#24615;TTS&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#20869;&#23481;&#27844;&#28431;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36755;&#20837;&#34920;&#31034;&#21644;&#31616;&#21333;&#30340;&#26550;&#26500;&#26469;&#23454;&#29616;&#25913;&#36827;&#30340;&#38901;&#24459;&#24314;&#27169;&#12290;&#21463;&#26368;&#36817;&#22312;TTS&#20013;&#20351;&#29992;&#31163;&#25955;&#30721;&#21462;&#24471;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#31163;&#25955;&#30721;&#24341;&#20837;&#21040;&#21442;&#32771;&#32534;&#30721;&#22120;&#30340;&#36755;&#20837;&#20013;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#38899;&#39057;&#21387;&#32553;&#27169;&#22411;&#20013;&#30340;&#21521;&#37327;&#37327;&#21270;&#22120;&#26469;&#21033;&#29992;&#23427;&#24050;&#32463;&#35757;&#32451;&#36807;&#30340;&#22810;&#26679;&#21270;&#30340;&#22768;&#23398;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#20462;&#25913;&#21518;&#30340;MLP-Mixer&#24212;&#29992;&#21040;&#21442;&#32771;&#32534;&#30721;&#22120;&#20013;&#65292;&#20351;&#24471;&#26550;&#26500;&#26356;&#21152;&#36731;&#30408;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#35757;&#32451;&#38901;&#24459;&#36716;&#31227;TTS&#12290;&#25105;&#20204;&#36890;&#36807;&#20027;&#35266;&#21644;&#23458;&#35266;&#35780;&#20272;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#65292;&#24403;&#31163;&#25955;&#30721;&#20316;&#20026;&#36755;&#20837;&#26102;&#65292;&#21442;&#32771;&#32534;&#30721;&#22120;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#22909;&#30340;&#19982;&#35828;&#35805;&#20154;&#26080;&#20851;&#30340;&#38901;&#24459;&#12290;&#21478;&#22806;&#65292;&#21363;&#20351;&#36755;&#20837;&#21442;&#25968;&#26356;&#23569;&#65292;&#25105;&#20204;&#20063;&#21487;&#20197;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the huge successes made in neutral TTS, content-leakage remains a challenge. In this paper, we propose a new input representation and simple architecture to achieve improved prosody modeling. Inspired by the recent success in the use of discrete code in TTS, we introduce discrete code to the input of the reference encoder. Specifically, we leverage the vector quantizer from the audio compression model to exploit the diverse acoustic information it has already been trained on. In addition, we apply the modified MLP-Mixer to the reference encoder, making the architecture lighter. As a result, we train the prosody transfer TTS in an end-to-end manner. We prove the effectiveness of our method through both subjective and objective evaluations. We demonstrate that the reference encoder learns better speaker-independent prosody when discrete code is utilized as input in the experiments. In addition, we obtain comparable results even when fewer parameters are inputted.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#35789;&#27719;&#25991;&#20307;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#20165;&#20351;&#29992;&#23569;&#37327;&#31181;&#23376;&#23545;&#27599;&#20010;&#25991;&#20307;&#27010;&#24565;&#24471;&#20986;&#21521;&#37327;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#38745;&#24577;&#23884;&#20837;&#31354;&#38388;&#26356;&#20934;&#30830;&#22320;&#32534;&#30721;&#20102;&#21333;&#35789;&#21644;&#30701;&#35821;&#30340;&#29305;&#24449;&#65292;&#32780;&#19978;&#19979;&#25991;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#21017;&#22312;&#21477;&#23376;&#32423;&#21035;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.18657</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#35789;&#27719;&#25991;&#20307;&#29305;&#24449;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Representation Of Lexical Stylistic Features In Language Models' Embedding Space. (arXiv:2305.18657v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#35789;&#27719;&#25991;&#20307;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#20165;&#20351;&#29992;&#23569;&#37327;&#31181;&#23376;&#23545;&#27599;&#20010;&#25991;&#20307;&#27010;&#24565;&#24471;&#20986;&#21521;&#37327;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#38745;&#24577;&#23884;&#20837;&#31354;&#38388;&#26356;&#20934;&#30830;&#22320;&#32534;&#30721;&#20102;&#21333;&#35789;&#21644;&#30701;&#35821;&#30340;&#29305;&#24449;&#65292;&#32780;&#19978;&#19979;&#25991;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#21017;&#22312;&#21477;&#23376;&#32423;&#21035;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#31354;&#38388;&#32534;&#30721;&#20102;&#20851;&#20110;&#21333;&#35789;&#21450;&#20854;&#20851;&#31995;&#65288;&#22914;&#30456;&#20284;&#24615;&#12289;&#19978;&#20041;&#35789;&#12289;&#19968;&#35789;&#22810;&#20041;&#65289;&#20197;&#21450;&#25277;&#35937;&#30340;&#35821;&#20041;&#27010;&#24565;&#65288;&#22914;&#24378;&#24230;&#65289;&#20016;&#23500;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#27492;&#31354;&#38388;&#20013;&#30340;&#35789;&#27719;&#25991;&#20307;&#29305;&#24449;&#65292;&#22914;&#22797;&#26434;&#24615;&#12289;&#27491;&#24335;&#31243;&#24230;&#21644;&#27604;&#21947;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20165;&#20351;&#29992;&#23569;&#37327;&#31181;&#23376;&#23545;&#21487;&#20197;&#24471;&#20986;&#27599;&#20010;&#25991;&#20307;&#27010;&#24565;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;&#21033;&#29992;&#36825;&#20123;&#21521;&#37327;&#65292;&#22312;&#30456;&#24212;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#36827;&#34892;&#31616;&#21333;&#35745;&#31639;&#65292;&#21363;&#21487;&#36890;&#36807;&#36825;&#20123;&#32500;&#24230;&#34920;&#24449;&#26032;&#25991;&#26412;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#38745;&#24577;&#23884;&#20837;&#31354;&#38388;&#26356;&#20934;&#30830;&#22320;&#32534;&#30721;&#20102;&#21333;&#35789;&#21644;&#30701;&#35821;&#30340;&#29305;&#24449;&#65292;&#32780;&#19978;&#19979;&#25991;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#21017;&#22312;&#21477;&#23376;&#32423;&#21035;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;&#19978;&#19979;&#25991;&#21270;&#34920;&#31034;&#22312;&#21333;&#35789;&#32423;&#21035;&#19978;&#30340;&#36739;&#20302;&#24615;&#33021;&#37096;&#20998;&#24402;&#22240;&#20110;&#20854;&#21521;&#37327;&#31354;&#38388;&#30340;&#21508;&#21521;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The representation space of pretrained Language Models (LMs) encodes rich information about words and their relationships (e.g., similarity, hypernymy, polysemy) as well as abstract semantic notions (e.g., intensity). In this paper, we demonstrate that lexical stylistic notions such as complexity, formality, and figurativeness, can also be identified in this space. We show that it is possible to derive a vector representation for each of these stylistic notions from only a small number of seed pairs. Using these vectors, we can characterize new texts in terms of these dimensions by performing simple calculations in the corresponding embedding space. We conduct experiments on five datasets and find that static embeddings encode these features more accurately at the level of words and phrases, whereas contextualized LMs perform better on sentences. The lower performance of contextualized representations at the word level is partially attributable to the anisotropy of their vector space, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#25216;&#26415;&#65292;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#19982;&#39044;&#27979;&#20934;&#30830;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.18404</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#31572;&#26696;&#30830;&#35748;&#39044;&#27979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Conformal Prediction with Large Language Models for Multi-Choice Question Answering. (arXiv:2305.18404v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#25216;&#26415;&#65292;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#19982;&#39044;&#27979;&#20934;&#30830;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#24320;&#21457;&#65292;&#23545;&#23427;&#20204;&#36827;&#34892;&#20581;&#22766;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25216;&#26415;&#23558;&#25104;&#20026;&#23427;&#20204;&#22312;&#39640;&#39118;&#38505;&#22330;&#26223;&#19979;&#23433;&#20840;&#37096;&#32626;&#30340;&#20851;&#38190;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#25216;&#26415;&#65292;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#19982;&#39044;&#27979;&#20934;&#30830;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;&#36825;&#31181;&#35266;&#23519;&#23545;&#20110;&#19979;&#28216;&#24212;&#29992;&#65292;&#22914;&#36873;&#25321;&#24615;&#20998;&#31867;&#21644;&#36807;&#28388;&#20302;&#36136;&#37327;&#39044;&#27979;&#65292;&#21487;&#33021;&#20250;&#26377;&#29992;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#31526;&#21512;&#24615;&#39044;&#27979;&#23545;&#20110;&#36229;&#20986;&#20027;&#39064;&#30340;&#38382;&#39064;&#30340;&#20132;&#25442;&#24615;&#20551;&#35774;&#65292;&#36825;&#21487;&#33021;&#26159;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#30340;&#26356;&#20026;&#29616;&#23454;&#30340;&#22330;&#26223;&#12290;&#26412;&#30740;&#31350;&#20026;&#22312;&#38656;&#35201;&#21487;&#38752;&#20445;&#35777;&#38169;&#35823;&#29575;&#30340;&#23433;&#20840;&#20851;&#38190;&#24773;&#20917;&#19979;&#26356;&#21152;&#20540;&#24471;&#20449;&#36182;&#21644;&#21487;&#38752;&#22320;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models continue to be widely developed, robust uncertainty quantification techniques will become crucial for their safe deployment in high-stakes scenarios. In this work, we explore how conformal prediction can be used to provide uncertainty quantification in language models for the specific task of multiple-choice question-answering. We find that the uncertainty estimates from conformal prediction are tightly correlated with prediction accuracy. This observation can be useful for downstream applications such as selective classification and filtering out low-quality predictions. We also investigate the exchangeability assumption required by conformal prediction to out-of-subject questions, which may be a more realistic scenario for many practical applications. Our work contributes towards more trustworthy and reliable usage of large language models in safety-critical situations, where robust guarantees of error rate are required.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#30340;&#21487;&#35270;&#21270;&#32534;&#31243;&#20219;&#21153;&#21512;&#25104;&#26041;&#27861;NeurTaskSyn&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#38024;&#23545;&#35268;&#33539;&#20013;&#32473;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#25152;&#38656;&#35201;&#30340;&#32534;&#31243;&#27010;&#24565;&#21644;&#23545;&#21487;&#35270;&#21270;&#20219;&#21153;&#30340;&#38480;&#21046;&#65292;&#33258;&#21160;&#29983;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.18342</link><description>&lt;p&gt;
&#21487;&#35270;&#21270;&#32534;&#31243;&#20013;&#31070;&#32463;&#20219;&#21153;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Neural Task Synthesis for Visual Programming. (arXiv:2305.18342v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18342
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#30340;&#21487;&#35270;&#21270;&#32534;&#31243;&#20219;&#21153;&#21512;&#25104;&#26041;&#27861;NeurTaskSyn&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#38024;&#23545;&#35268;&#33539;&#20013;&#32473;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#25152;&#38656;&#35201;&#30340;&#32534;&#31243;&#27010;&#24565;&#21644;&#23545;&#21487;&#35270;&#21270;&#20219;&#21153;&#30340;&#38480;&#21046;&#65292;&#33258;&#21160;&#29983;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#26032;&#30340;&#20869;&#23481;&#65292;&#29983;&#25104;&#24335;&#31070;&#32463;&#27169;&#22411;&#22312;&#22686;&#24378;&#32534;&#31243;&#25945;&#32946;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#26088;&#22312;&#35774;&#35745;&#31070;&#32463;&#27169;&#22411;&#65292;&#33021;&#22815;&#26681;&#25454;&#21487;&#35270;&#21270;&#32534;&#31243;&#29615;&#22659;&#19979;&#32473;&#23450;&#30340;&#35268;&#33539;&#33258;&#21160;&#29983;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#20687; GPT-4 &#36825;&#26679;&#30340;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#33719;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#21512;&#25104;&#21487;&#35270;&#21270;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#65292;&#24182;&#19988;&#22312;&#36923;&#36753;&#21644;&#31354;&#38388;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415; NeurTaskSyn&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#38024;&#23545;&#35268;&#33539;&#20013;&#32473;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#25152;&#38656;&#35201;&#30340;&#32534;&#31243;&#27010;&#24565;&#21644;&#23545;&#21487;&#35270;&#21270;&#20219;&#21153;&#30340;&#38480;&#21046;&#65292;&#21512;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;NeurTaskSyn &#30001;&#20004;&#20010;&#37096;&#20998;&#26500;&#25104;&#65306;&#31532;&#19968;&#20010;&#37096;&#20998;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#31243;&#24207;&#36827;&#34892;&#35757;&#32451;&#65292;&#29983;&#25104;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#65292;&#31532;&#20108;&#20010;&#37096;&#20998;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;&#36827;&#34892;&#35757;&#32451;&#65292;&#25351;&#23548;&#24213;&#23618;&#31526;&#21495;&#25191;&#34892;&#24341;&#25806;&#29983;&#25104;&#21487;&#35270;&#21270;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative neural models hold great promise in enhancing programming education by synthesizing new content for students. We seek to design neural models that can automatically generate programming tasks for a given specification in the context of visual programming domains. Despite the recent successes of large generative models like GPT-4, our initial results show that these models are ineffective in synthesizing visual programming tasks and struggle with logical and spatial reasoning. We propose a novel neuro-symbolic technique, NeurTaskSyn, that can synthesize programming tasks for a specification given in the form of desired programming concepts exercised by its solution code and constraints on the visual task. NeurTaskSyn has two components: the first component is trained via imitation learning procedure to generate possible solution codes, and the second component is trained via reinforcement learning procedure to guide an underlying symbolic execution engine that generates visua
&lt;/p&gt;</description></item><item><title>Translatotron 3&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#21333;&#35821;&#25968;&#25454;&#36827;&#34892;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#65292;&#26080;&#38656;&#37197;&#23545;&#30340;&#25968;&#25454;&#25110;&#19987;&#19994;&#24314;&#27169;&#65292;&#23637;&#31034;&#20102;&#20445;&#30041;&#35821;&#35328;/&#38750;&#35821;&#35328;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.17547</link><description>&lt;p&gt;
Translatotron 3: &#20351;&#29992;&#21333;&#35821;&#25968;&#25454;&#36827;&#34892;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Translatotron 3: Speech to Speech Translation with Monolingual Data. (arXiv:2305.17547v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17547
&lt;/p&gt;
&lt;p&gt;
Translatotron 3&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#21333;&#35821;&#25968;&#25454;&#36827;&#34892;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#65292;&#26080;&#38656;&#37197;&#23545;&#30340;&#25968;&#25454;&#25110;&#19987;&#19994;&#24314;&#27169;&#65292;&#23637;&#31034;&#20102;&#20445;&#30041;&#35821;&#35328;/&#38750;&#35821;&#35328;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Translatotron 3&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#12289;&#26080;&#30417;&#30563;&#30340;&#23884;&#20837;&#26144;&#23556;&#21644;&#22238;&#35793;&#23558;&#30452;&#25509;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#20174;&#21333;&#22768;&#36947;&#35821;&#38899;-&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#23436;&#20840;&#26080;&#30417;&#30563;&#22320;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#35199;&#29677;&#29273;&#35821;&#21644;&#33521;&#35821;&#20043;&#38388;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#20219;&#21153;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Translatotron 3&#20248;&#20110;&#22522;&#20934;&#32423;&#32852;&#31995;&#32479;&#65292;&#22312; synthesized Unpaired-Conversational &#25968;&#25454;&#38598;&#19978;&#25253;&#21578;&#20102;18.14 BLEU&#20998;&#25968;&#30340;&#25552;&#39640;&#12290;&#19982;&#38656;&#35201;&#30495;&#23454;&#37197;&#23545;&#25968;&#25454;&#25110;&#19987;&#19994;&#24314;&#27169;&#26469;&#22797;&#21046;&#35821;&#35328;/&#38750;&#35821;&#35328;&#20449;&#24687;&#30340;&#30417;&#30563;&#26041;&#27861;&#19981;&#21516;&#65292;Translatotron 3&#23637;&#31034;&#20102;&#23427;&#20445;&#30041;&#20102;&#20687;&#26242;&#20572;&#12289;&#35828;&#35805;&#36895;&#24230;&#21644;&#35828;&#35805;&#20154;&#36523;&#20221;&#31561;&#35821;&#35328;/&#38750;&#35821;&#35328;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents Translatotron 3, a novel approach to train a direct speech-to-speech translation model from monolingual speech-text datasets only in a fully unsupervised manner. Translatotron 3 combines masked autoencoder, unsupervised embedding mapping, and back-translation to achieve this goal. Experimental results in speech-to-speech translation tasks between Spanish and English show that Translatotron 3 outperforms a baseline cascade system, reporting 18.14 BLEU points improvement on the synthesized Unpaired-Conversational dataset. In contrast to supervised approaches that necessitate real paired data, which is unavailable, or specialized modeling to replicate para-/non-linguistic information, Translatotron 3 showcases its capability to retain para-/non-linguistic such as pauses, speaking rates, and speaker identity. Audio samples can be found in our website this http URL
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;FACTUAL-MR&#65292;&#35813;&#25968;&#25454;&#38598;&#29992;&#20110;&#20934;&#30830;&#25991;&#26412;&#22330;&#26223;&#22270;&#35299;&#26512;&#65292;&#36825;&#26679;&#21487;&#20197;&#36991;&#20813;&#29616;&#26377;&#35299;&#26512;&#22120;&#20986;&#29616;&#30340;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.17497</link><description>&lt;p&gt;
FACTUAL: &#19968;&#20010;&#29992;&#20110;&#20934;&#30830;&#25991;&#26412;&#22330;&#26223;&#22270;&#35299;&#26512;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
FACTUAL: A Benchmark for Faithful and Consistent Textual Scene Graph Parsing. (arXiv:2305.17497v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17497
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;FACTUAL-MR&#65292;&#35813;&#25968;&#25454;&#38598;&#29992;&#20110;&#20934;&#30830;&#25991;&#26412;&#22330;&#26223;&#22270;&#35299;&#26512;&#65292;&#36825;&#26679;&#21487;&#20197;&#36991;&#20813;&#29616;&#26377;&#35299;&#26512;&#22120;&#20986;&#29616;&#30340;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#22330;&#26223;&#22270;&#35299;&#26512;&#22312;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#21253;&#25324;&#22270;&#20687;&#25551;&#36848;&#35780;&#20272;&#21644;&#22270;&#20687;&#26816;&#32034;&#12290;&#28982;&#32780;&#65292;&#23558;&#22270;&#20687;&#26631;&#39064;&#36716;&#25442;&#20026;&#22330;&#26223;&#22270;&#30340;&#29616;&#26377;&#35299;&#26512;&#22120;&#32463;&#24120;&#36973;&#21463;&#20004;&#31181;&#31867;&#22411;&#30340;&#38169;&#35823;&#12290;&#39318;&#20808;&#65292;&#29983;&#25104;&#30340;&#22330;&#26223;&#22270;&#26410;&#33021;&#25429;&#25417;&#26631;&#39064;&#25110;&#30456;&#24212;&#22270;&#20687;&#30340;&#30495;&#23454;&#35821;&#20041;&#65292;&#23548;&#33268;&#32570;&#20047;&#20934;&#30830;&#24615;&#12290;&#20854;&#27425;&#65292;&#29983;&#25104;&#30340;&#22330;&#26223;&#22270;&#20855;&#26377;&#39640;&#24230;&#19981;&#19968;&#33268;&#24615;&#65292;&#30456;&#21516;&#30340;&#35821;&#20041;&#30001;&#19981;&#21516;&#30340;&#27880;&#37322;&#34920;&#31034;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#37319;&#29992;&#31216;&#20026;FACTUAL-MR&#30340;&#26032;&#20013;&#38388;&#34920;&#31034;&#23545;Visual Genome&#65288;VG&#65289;&#20013;&#30340;&#26631;&#39064;&#36827;&#34892;&#37325;&#26032;&#27880;&#37322;&#12290; FACTUAL-MR &#21487;&#20197;&#30452;&#25509;&#36716;&#25442;&#20026;&#20934;&#30830;&#21644;&#19968;&#33268;&#30340;&#22330;&#26223;&#22270;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#28165;&#26970;&#22320;&#34920;&#26126;&#65292;&#22312;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#26041;&#38754;&#65292;&#25105;&#20204;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#35299;&#26512;&#22120;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#36825;&#31181;&#25913;&#36827;&#23545;&#20110;&#38656;&#35201;&#31934;&#30830;&#30340;&#25991;&#26412;&#22330;&#26223;&#22270;&#20998;&#26512;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual scene graph parsing has become increasingly important in various vision-language applications, including image caption evaluation and image retrieval. However, existing scene graph parsers that convert image captions into scene graphs often suffer from two types of errors. First, the generated scene graphs fail to capture the true semantics of the captions or the corresponding images, resulting in a lack of faithfulness. Second, the generated scene graphs have high inconsistency, with the same semantics represented by different annotations.  To address these challenges, we propose a novel dataset, which involves re-annotating the captions in Visual Genome (VG) using a new intermediate representation called FACTUAL-MR. FACTUAL-MR can be directly converted into faithful and consistent scene graph annotations. Our experimental results clearly demonstrate that the parser trained on our dataset outperforms existing approaches in terms of faithfulness and consistency. This improvemen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;A2EHV&#65292;&#37319;&#29992;&#24322;&#36136;&#20215;&#20540;&#31995;&#32479;&#65292;&#24182;&#22522;&#20110;&#20215;&#20540;&#21512;&#29702;&#24615;&#21644;&#31038;&#20250;&#20215;&#20540;&#23450;&#21521;&#26694;&#26550;&#35780;&#20272;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#20559;&#22909;&#65292;&#32467;&#26524;&#34920;&#26126;&#27604;&#20256;&#32479;&#23545;&#40784;&#26041;&#27861;&#26356;&#21512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.17147</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24322;&#36136;&#20215;&#20540;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Value Evaluation for Large Language Models. (arXiv:2305.17147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;A2EHV&#65292;&#37319;&#29992;&#24322;&#36136;&#20215;&#20540;&#31995;&#32479;&#65292;&#24182;&#22522;&#20110;&#20215;&#20540;&#21512;&#29702;&#24615;&#21644;&#31038;&#20250;&#20215;&#20540;&#23450;&#21521;&#26694;&#26550;&#35780;&#20272;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#20559;&#22909;&#65292;&#32467;&#26524;&#34920;&#26126;&#27604;&#20256;&#32479;&#23545;&#40784;&#26041;&#27861;&#26356;&#21512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#20351;&#24471;&#23558;&#23427;&#20204;&#30340;&#20215;&#20540;&#19982;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#23581;&#35797;&#23558;&#20854;&#19982;&#19968;&#31181;&#21516;&#36136;&#30340;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#65292;&#24182;&#38656;&#35201;&#20154;&#31867;&#39564;&#35777;&#65292;&#20294;&#32570;&#20047;&#23545;&#23545;&#40784;&#25152;&#38656;&#26041;&#38754;&#21644;&#28145;&#24230;&#30340;&#20849;&#35782;&#20197;&#21450;&#36896;&#25104;&#30340;&#20154;&#31867;&#20559;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;A2EHV&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#24322;&#36136;&#20215;&#20540;&#31995;&#32479;&#65292;&#65288;1&#65289;&#26159;&#33258;&#21160;&#21270;&#30340;&#65292;&#20197;&#26368;&#23567;&#21270;&#21333;&#20010;&#20154;&#31867;&#20559;&#35265;&#65292;&#24182;&#19988;&#65288;2&#65289;&#20801;&#35768;&#35780;&#20272;&#38024;&#23545;&#21508;&#31181;&#30446;&#26631;&#20540;&#30340;&#24322;&#36136;&#20195;&#29702;&#20154;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20215;&#20540;&#21512;&#29702;&#24615;&#30340;&#27010;&#24565;&#65292;&#23427;&#20195;&#34920;&#20102;&#20195;&#29702;&#20154;&#25191;&#34892;&#26368;&#33021;&#28385;&#36275;&#30446;&#26631;&#20215;&#20540;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#20215;&#20540;&#21512;&#29702;&#24615;&#30340;&#37327;&#21270;&#26159;&#36890;&#36807;&#31038;&#20250;&#24515;&#29702;&#23398;&#20013;&#30340;&#31038;&#20250;&#20215;&#20540;&#23450;&#21521;&#26694;&#26550;&#36827;&#34892;&#30340;&#65292;&#35813;&#26694;&#26550;&#23558;&#20215;&#20540;&#31354;&#38388;&#20998;&#20026;&#22235;&#20010;&#31867;&#21035;&#65292;&#20197;&#35780;&#20272;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#20559;&#22909;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#20010;&#27169;&#22411;&#30340;&#20215;&#20540;&#21512;&#29702;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;A2EHV&#26041;&#27861;&#27604;&#20256;&#32479;&#23545;&#40784;&#26041;&#27861;&#26356;&#21512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergent capabilities of Large Language Models (LLMs) have made it crucial to align their values with those of humans. Current methodologies typically attempt alignment with a homogeneous human value and requires human verification, yet lack consensus on the desired aspect and depth of alignment and resulting human biases. In this paper, we propose A2EHV, an Automated Alignment Evaluation with a Heterogeneous Value system that (1) is automated to minimize individual human biases, and (2) allows assessments against various target values to foster heterogeneous agents. Our approach pivots on the concept of value rationality, which represents the ability for agents to execute behaviors that satisfy a target value the most. The quantification of value rationality is facilitated by the Social Value Orientation framework from social psychology, which partitions the value space into four categories to assess social preferences from agents' behaviors. We evaluate the value rationality of e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Ghost in the Minecraft (GITM)&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22522;&#20110;&#25991;&#26412;&#30340;&#30693;&#35782;&#21644;&#35760;&#24518;&#65292;&#21019;&#36896;&#20102;&#19968;&#31181;&#22312;Minecraft&#20013;&#20855;&#22791;&#36890;&#29992;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#65292;&#21487;&#22312;&#20197;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#22797;&#26434;&#32534;&#31243;&#29615;&#22659;&#20013;&#29087;&#32451;&#23548;&#33322;&#12290;</title><link>http://arxiv.org/abs/2305.17144</link><description>&lt;p&gt;
Minecraft&#20013;&#30340;&#24189;&#28789;&#65306;&#21033;&#29992;&#22522;&#20110;&#25991;&#26412;&#30693;&#35782;&#21644;&#35760;&#24518;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#36890;&#29992;&#33021;&#21147;&#26234;&#33021;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ghost in the Minecraft: Generally Capable Agents for Open-World Enviroments via Large Language Models with Text-based Knowledge and Memory. (arXiv:2305.17144v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Ghost in the Minecraft (GITM)&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22522;&#20110;&#25991;&#26412;&#30340;&#30693;&#35782;&#21644;&#35760;&#24518;&#65292;&#21019;&#36896;&#20102;&#19968;&#31181;&#22312;Minecraft&#20013;&#20855;&#22791;&#36890;&#29992;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#65292;&#21487;&#22312;&#20197;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#22797;&#26434;&#32534;&#31243;&#29615;&#22659;&#20013;&#29087;&#32451;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Minecraft&#29609;&#27861;&#21560;&#24341;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#25104;&#20026;&#24320;&#21457;&#33021;&#22815;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#26234;&#33021;&#20307;&#30340;&#20016;&#23500;&#24179;&#21488;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#29305;&#23450;&#30340;&#30446;&#26631;&#19978;&#65292;&#20363;&#22914;&#27969;&#34892;&#30340;&#8220;ObtainDiamond&#8221;&#20219;&#21153;&#65292;&#24182;&#19988;&#36824;&#27809;&#26377;&#26174;&#31034;&#20986;&#26377;&#25928;&#22320;&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#8220;ObtainDiamond&#8221;&#20219;&#21153;&#30340;&#30446;&#21069;&#26368;&#39640;&#25104;&#21151;&#29575;&#21482;&#26377;&#32422;20&#65285;&#65292;&#20984;&#26174;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25511;&#21046;&#22120;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Ghost in the Minecraft (GITM)&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22522;&#20110;&#25991;&#26412;&#30340;&#30693;&#35782;&#21644;&#35760;&#24518;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#21019;&#24314;Minecraft&#20013;&#30340;&#36890;&#29992;&#33021;&#21147;&#26234;&#33021;&#20307;&#12290;&#36825;&#20123;&#20855;&#22791;LLM&#20013;&#30340;&#36923;&#36753;&#21644;&#24120;&#35782;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#29087;&#32451;&#22320;&#22312;&#20197;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#22797;&#26434;&#32534;&#31243;&#29615;&#22659;&#20013;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The captivating realm of Minecraft has attracted substantial research interest in recent years, serving as a rich platform for developing intelligent agents capable of functioning in open-world environments. However, the current research landscape predominantly focuses on specific objectives, such as the popular "ObtainDiamond" task, and has not yet shown effective generalization to a broader spectrum of tasks. Furthermore, the current leading success rate for the "ObtainDiamond" task stands at around 20%, highlighting the limitations of Reinforcement Learning (RL) based controllers used in existing methods. To tackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel framework integrates Large Language Models (LLMs) with text-based knowledge and memory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These agents, equipped with the logic and common sense capabilities of LLMs, can skillfully navigate complex, sparse-reward environments with text-based 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#24773;&#24863;&#20307;&#39564;&#32773;&#24182;&#20026;&#20854;&#20998;&#37197;&#24773;&#24863;&#30340;&#33258;&#21160;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#30456;&#20851;&#30340;&#23454;&#39564;&#12290;&#35813;&#26041;&#27861;&#30340;&#23454;&#29616;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20294;&#23637;&#31034;&#20102;&#22312;&#25991;&#26412;&#20013;&#26816;&#27979;&#24773;&#24863;&#20307;&#39564;&#32773;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16731</link><description>&lt;p&gt;
&#35782;&#21035;&#24773;&#24863;&#20307;&#39564;&#32773;&#20316;&#20026;&#24773;&#24863;&#20998;&#26512;&#30340;&#20808;&#20915;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Emotion Experiencer Recognition as a Prerequisite for Experiencer-Specific Emotion Analysis. (arXiv:2305.16731v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#24773;&#24863;&#20307;&#39564;&#32773;&#24182;&#20026;&#20854;&#20998;&#37197;&#24773;&#24863;&#30340;&#33258;&#21160;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#30456;&#20851;&#30340;&#23454;&#39564;&#12290;&#35813;&#26041;&#27861;&#30340;&#23454;&#29616;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20294;&#23637;&#31034;&#20102;&#22312;&#25991;&#26412;&#20013;&#26816;&#27979;&#24773;&#24863;&#20307;&#39564;&#32773;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#35282;&#33394;&#26631;&#27880;&#26088;&#22312;&#25552;&#21462;&#25991;&#26412;&#20013;&#25551;&#36848;&#35841;&#32463;&#21382;&#24773;&#24863;&#12289;&#20026;&#20160;&#20040;&#20197;&#21450;&#23545;&#35841;&#30340;&#20449;&#24687;&#12290;&#36825;&#36890;&#24120;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24314;&#27169;&#20219;&#21153;&#65292;&#22914;&#26524;&#35201;&#22238;&#31572;&#30340;&#20027;&#35201;&#38382;&#39064;&#26159;&#35841;&#24863;&#21463;&#21040;&#20102;&#21738;&#31181;&#24773;&#24863;&#65292;&#36825;&#21487;&#33021;&#20250;&#36807;&#20110;&#22797;&#26434;&#12290;&#26412;&#25991;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#33258;&#21160;&#26816;&#27979;&#25991;&#26412;&#20013;&#30340;&#24773;&#24863;&#20307;&#39564;&#32773;&#24182;&#38543;&#21518;&#20026;&#20854;&#20998;&#37197;&#24773;&#24863;&#65292;&#23637;&#31034;&#20102;&#22312;&#25991;&#26412;&#20013;&#26816;&#27979;&#24773;&#24863;&#20307;&#39564;&#32773;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24182;&#21576;&#29616;&#20102;&#30456;&#20851;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion role labeling aims at extracting who is described in text to experience an emotion, why, and towards whom. This is often a challenging modelling task which might be overly sophisticated if the main question to answer is who feels which emotion. Recently, Troiano et al. (2022) proposed a data set that focuses on assigning emotion labels and appraisal labels to individual entities in text and Wegge et al. (2022) presented the first modelling experiments. Their experiencer-specific emotion prediction model has, however, only been evaluated on gold-annotated experiencers, due to the unavailability of an automatic experiencer detection approach. We fill this gap with the first experiments to automatically detect emotion experiencers in text and, subsequently, assign them emotions. We show that experiencer detection in text is a challenging task, with a precision of .82 and a recall of .56 (F1 =.66). Consequently, the performance of the experiencer-specific emotion detection pipeline
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;93&#21517;NLP&#21021;&#23398;&#32773;&#30340;&#35843;&#26597;&#65292;&#21457;&#29616;&#30740;&#31350;&#20316;&#32773;&#25552;&#20379;&#23436;&#25972;&#25991;&#26723;&#12289;&#26356;&#22909;&#30340;&#20195;&#30721;&#23454;&#36341;&#21644;&#26356;&#26131;&#20110;&#33719;&#21462;&#30340;&#25968;&#25454;&#25991;&#20214;&#26159;&#21021;&#23398;&#32773;&#25104;&#21151;&#22797;&#29616;&#26368;&#36817;NLP&#35770;&#25991;&#32467;&#26524;&#30340;&#20851;&#38190;&#65292;&#24314;&#35758;NLP&#30740;&#31350;&#20154;&#21592;&#27880;&#37325;&#36825;&#20123;&#26041;&#38754;&#65292;&#26356;&#22909;&#22320;&#25903;&#25345;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2305.16579</link><description>&lt;p&gt;
&#20154;&#20154;&#21487;&#22797;&#29616;&#30340;NLP&#30740;&#31350;&#65306;&#21021;&#23398;&#32773;&#30340;&#38656;&#27714;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
NLP Reproducibility For All: Understanding Experiences of Beginners. (arXiv:2305.16579v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16579
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;93&#21517;NLP&#21021;&#23398;&#32773;&#30340;&#35843;&#26597;&#65292;&#21457;&#29616;&#30740;&#31350;&#20316;&#32773;&#25552;&#20379;&#23436;&#25972;&#25991;&#26723;&#12289;&#26356;&#22909;&#30340;&#20195;&#30721;&#23454;&#36341;&#21644;&#26356;&#26131;&#20110;&#33719;&#21462;&#30340;&#25968;&#25454;&#25991;&#20214;&#26159;&#21021;&#23398;&#32773;&#25104;&#21151;&#22797;&#29616;&#26368;&#36817;NLP&#35770;&#25991;&#32467;&#26524;&#30340;&#20851;&#38190;&#65292;&#24314;&#35758;NLP&#30740;&#31350;&#20154;&#21592;&#27880;&#37325;&#36825;&#20123;&#26041;&#38754;&#65292;&#26356;&#22909;&#22320;&#25903;&#25345;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#36817;&#24180;&#26469;&#24322;&#24120;&#28779;&#29190;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24613;&#20110;&#36827;&#20837;&#35813;&#39046;&#22495;&#65292;&#20294;&#30446;&#21069;&#30340;&#30740;&#31350;&#22797;&#29616;&#21162;&#21147;&#26159;&#21542;&#36275;&#20197;&#35753;&#36825;&#20123;&#21021;&#23398;&#32773;&#24212;&#29992;&#26368;&#26032;&#30340;&#36827;&#23637;&#36824;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#20102;&#35299;&#21021;&#23398;&#32773;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#20171;&#32461;&#24615;&#30340;NLP&#35838;&#31243;&#20013;&#24320;&#23637;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#35753;&#23398;&#29983;&#22797;&#29616;&#26368;&#36817;NLP&#35770;&#25991;&#30340;&#32467;&#26524;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20182;&#20204;&#30340;&#32534;&#31243;&#25216;&#33021;&#21644;&#23545;&#30740;&#31350;&#35770;&#25991;&#30340;&#29702;&#35299;&#23545;&#23436;&#25104;&#32451;&#20064;&#30340;&#20184;&#20986;&#20165;&#26377;&#38480;&#30340;&#24433;&#21709;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#30740;&#31350;&#20316;&#32773;&#30340;&#21487;&#35775;&#38382;&#24615;&#21162;&#21147;&#26159;&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#21253;&#25324;&#23436;&#25972;&#30340;&#25991;&#26723;&#12289;&#26356;&#22909;&#30340;&#32534;&#30721;&#23454;&#36341;&#21644;&#26356;&#23481;&#26131;&#33719;&#21462;&#30340;&#25968;&#25454;&#25991;&#20214;&#12290;&#21069;&#36827;&#26102;&#65292;&#25105;&#20204;&#24314;&#35758;NLP&#30740;&#31350;&#20154;&#21592;&#23494;&#20999;&#20851;&#27880;&#36825;&#20123;&#24320;&#28304;&#24037;&#20316;&#30340;&#31616;&#21333;&#26041;&#38754;&#65292;&#24182;&#20351;&#29992;&#21021;&#23398;&#32773;&#30340;&#21453;&#39304;&#35265;&#35299;&#25552;&#20379;&#21487;&#25805;&#20316;&#30340;&#24819;&#27861;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#20182;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
As natural language processing (NLP) has recently seen an unprecedented level of excitement, and more people are eager to enter the field, it is unclear whether current research reproducibility efforts are sufficient for this group of beginners to apply the latest developments. To understand their needs, we conducted a study with 93 students in an introductory NLP course, where students reproduced the results of recent NLP papers. Surprisingly, we find that their programming skill and comprehension of research papers have a limited impact on their effort spent completing the exercise. Instead, we find accessibility efforts by research authors to be the key to success, including complete documentation, better coding practice, and easier access to data files. Going forward, we recommend that NLP researchers pay close attention to these simple aspects of open-sourcing their work, and use insights from beginners' feedback to provide actionable ideas on how to better support them.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31616;&#35201;&#27010;&#36848;&#20102;&#38271;&#25991;&#26412;&#30340;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#29616;&#29366;&#65292;&#20027;&#35201;&#21253;&#25324;&#25991;&#26723;&#20998;&#31867;&#21644;&#25688;&#35201;&#65292;&#28085;&#30422;&#20102;&#24773;&#24863;&#20998;&#26512;&#65292;&#21516;&#26102;&#36824;&#25506;&#35752;&#20102;&#38271;&#25991;&#26412;NLP&#30340;&#20027;&#35201;&#25361;&#25112;&#12289;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.16259</link><description>&lt;p&gt;
&#38271;&#25991;&#26412;&#30340;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65306;&#29616;&#29366;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Neural Natural Language Processing for Long Texts: A Survey of the State-of-the-Art. (arXiv:2305.16259v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31616;&#35201;&#27010;&#36848;&#20102;&#38271;&#25991;&#26412;&#30340;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#29616;&#29366;&#65292;&#20027;&#35201;&#21253;&#25324;&#25991;&#26723;&#20998;&#31867;&#21644;&#25688;&#35201;&#65292;&#28085;&#30422;&#20102;&#24773;&#24863;&#20998;&#26512;&#65292;&#21516;&#26102;&#36824;&#25506;&#35752;&#20102;&#38271;&#25991;&#26412;NLP&#30340;&#20027;&#35201;&#25361;&#25112;&#12289;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#37319;&#29992;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#38271;&#25991;&#26412;&#20998;&#26512;&#30340;&#38656;&#27714;&#19982;&#30701;&#25991;&#26412;&#26377;&#24456;&#22823;&#19981;&#21516;&#65292;&#32780;&#32593;&#32476;&#19978;&#20256;&#36755;&#30340;&#25991;&#26723;&#22823;&#23567;&#19981;&#26029;&#22686;&#21152;&#65292;&#20351;&#38271;&#25991;&#26412;&#30340;&#33258;&#21160;&#29702;&#35299;&#25104;&#20026;&#19968;&#39033;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#30340;&#20004;&#20010;&#30446;&#26631;&#26159;&#65306;a&#65289;&#27010;&#36848;&#30456;&#20851;&#30340;&#31070;&#32463;&#26500;&#24314;&#27169;&#22359;&#65292;&#20316;&#20026;&#30701;&#25945;&#31243;&#65307;b&#65289;&#24635;&#32467;&#38271;&#25991;&#26412;NLP&#30340;&#29616;&#29366;&#65292;&#20027;&#35201;&#20851;&#27880;&#20004;&#20010;&#26680;&#24515;&#20219;&#21153;&#65306;&#25991;&#26723;&#20998;&#31867;&#21644;&#25991;&#26723;&#25688;&#35201;&#12290;&#24773;&#24863;&#20998;&#26512;&#20063;&#28085;&#30422;&#22312;&#20869;&#65292;&#22240;&#20026;&#23427;&#36890;&#24120;&#34987;&#35270;&#20026;&#25991;&#26723;&#20998;&#31867;&#30340;&#29305;&#20363;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#38271;&#25991;&#26412;NLP&#30456;&#20851;&#30340;&#20027;&#35201;&#25361;&#25112;&#12289;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#20171;&#32461;&#20102;&#30456;&#20851;&#30340;&#20844;&#24320;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#20419;&#36827;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The adoption of Deep Neural Networks (DNNs) has greatly benefited Natural Language Processing (NLP) during the past decade. However, the demands of long document analysis are quite different from those of shorter texts, while the ever increasing size of documents uploaded on-line renders automated understanding of long texts a critical area of research. This article has two goals: a) it overviews the relevant neural building blocks, thus serving as a short tutorial, and b) it surveys the state-of-the-art in long document NLP, mainly focusing on two central tasks: document classification and document summarization. Sentiment analysis for long texts is also covered, since it is typically treated as a particular case of document classification. Additionally, this article discusses the main challenges, issues and current solutions related to long document NLP. Finally, the relevant, publicly available, annotated datasets are presented, in order to facilitate further research.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25910;&#38598;&#21644;&#20998;&#31867;&#20102;&#36229;&#36807;220&#20010;&#21463;&#27426;&#36814;&#30340;&#25163;&#24037;&#35821;&#35328;&#29305;&#24449;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#30340;&#25163;&#24037;&#35821;&#35328;&#29305;&#24449;&#25552;&#21462;&#31995;&#32479;&#65292;&#20197;&#31995;&#32479;&#24615;&#30340;&#21487;&#25193;&#23637;&#26041;&#24335;&#23454;&#29616;&#65292;&#24182;&#22312;&#20960;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#30456;&#20851;&#24615;&#20998;&#26512;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.15878</link><description>&lt;p&gt;
LFTK: &#35745;&#31639;&#35821;&#35328;&#23398;&#20013;&#30340;&#25163;&#24037;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
LFTK: Handcrafted Features in Computational Linguistics. (arXiv:2305.15878v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15878
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25910;&#38598;&#21644;&#20998;&#31867;&#20102;&#36229;&#36807;220&#20010;&#21463;&#27426;&#36814;&#30340;&#25163;&#24037;&#35821;&#35328;&#29305;&#24449;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#30340;&#25163;&#24037;&#35821;&#35328;&#29305;&#24449;&#25552;&#21462;&#31995;&#32479;&#65292;&#20197;&#31995;&#32479;&#24615;&#30340;&#21487;&#25193;&#23637;&#26041;&#24335;&#23454;&#29616;&#65292;&#24182;&#22312;&#20960;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#30456;&#20851;&#24615;&#20998;&#26512;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#30340;&#30740;&#31350;&#24050;&#32463;&#37492;&#23450;&#20986;&#20102;&#19968;&#32452;&#20016;&#23500;&#30340;&#25163;&#24037;&#35821;&#35328;&#29305;&#24449;&#65292;&#21487;&#20197;&#28508;&#22312;&#22320;&#24110;&#21161;&#21508;&#31181;&#20219;&#21153;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#36825;&#20123;&#29305;&#24449;&#25968;&#37327;&#24222;&#22823;&#65292;&#22240;&#27492;&#38590;&#20197;&#26377;&#25928;&#22320;&#36873;&#25321;&#21644;&#21033;&#29992;&#29616;&#26377;&#30340;&#25163;&#24037;&#29305;&#24449;&#12290;&#21152;&#19978;&#22312;&#30740;&#31350;&#24037;&#20316;&#20013;&#23454;&#29616;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#30446;&#21069;&#36824;&#19981;&#23384;&#22312;&#20998;&#31867;&#26041;&#26696;&#25110;&#32773;&#32479;&#19968;&#25509;&#21463;&#30340;&#29305;&#24449;&#21517;&#31216;&#65292;&#36825;&#36896;&#25104;&#20102;&#19981;&#24517;&#35201;&#30340;&#28151;&#20081;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25163;&#24037;&#29305;&#24449;&#25552;&#21462;&#24211;&#37117;&#19981;&#26159;&#24320;&#28304;&#30340;&#65292;&#25110;&#32773;&#27809;&#26377;&#24471;&#21040;&#31215;&#26497;&#30340;&#32500;&#25252;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#38656;&#35201;&#20174;&#38646;&#24320;&#22987;&#26500;&#24314;&#36825;&#26679;&#30340;&#25552;&#21462;&#31995;&#32479;&#12290;&#25105;&#20204;&#36890;&#36807;&#36807;&#21435;&#30340;&#25991;&#29486;&#25910;&#38598;&#21644;&#20998;&#31867;&#20102;&#36229;&#36807;220&#20010;&#21463;&#27426;&#36814;&#30340;&#25163;&#24037;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#30456;&#20851;&#24615;&#20998;&#26512;&#30740;&#31350;&#65292;&#24182;&#25253;&#21578;&#20102;&#27599;&#20010;&#29305;&#24449;&#30340;&#28508;&#22312;&#29992;&#36884;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#30340;&#25163;&#24037;&#35821;&#35328;&#29305;&#24449;&#25552;&#21462;&#31995;&#32479;&#65292;&#20197;&#31995;&#32479;&#24615;&#30340;&#21487;&#25193;&#23637;&#26041;&#24335;&#23454;&#29616;&#12290;&#25105;&#20204;&#24320;&#28304;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Past research has identified a rich set of handcrafted linguistic features that can potentially assist various tasks. However, their extensive number makes it difficult to effectively select and utilize existing handcrafted features. Coupled with the problem of inconsistent implementation across research works, there has been no categorization scheme or generally-accepted feature names. This creates unwanted confusion. Also, most existing handcrafted feature extraction libraries are not open-source or not actively maintained. As a result, a researcher often has to build such an extraction system from the ground up.  We collect and categorize more than 220 popular handcrafted features grounded on past literature. Then, we conduct a correlation analysis study on several task-specific datasets and report the potential use cases of each feature. Lastly, we devise a multilingual handcrafted linguistic feature extraction system in a systematically expandable manner. We open-source our system
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35770;&#23618;&#38754;&#25506;&#31350;&#20102;&#24102;&#26377;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22522;&#26412;&#25968;&#23398;&#21644;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#22238;&#24402;Transformer&#22823;&#23567;&#24658;&#23450;&#21363;&#21487;&#35299;&#20915;&#20219;&#21153;&#65292;&#25581;&#31034;&#20102;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#32972;&#21518;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.15408</link><description>&lt;p&gt;
&#20174;&#29702;&#35770;&#35282;&#24230;&#25581;&#31034;&#8220;&#24605;&#32500;&#38142;&#8221;&#32972;&#21518;&#30340;&#22885;&#31192;
&lt;/p&gt;
&lt;p&gt;
Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective. (arXiv:2305.15408v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#23618;&#38754;&#25506;&#31350;&#20102;&#24102;&#26377;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22522;&#26412;&#25968;&#23398;&#21644;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#22238;&#24402;Transformer&#22823;&#23567;&#24658;&#23450;&#21363;&#21487;&#35299;&#20915;&#20219;&#21153;&#65292;&#25581;&#31034;&#20102;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#32972;&#21518;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;"&#24605;&#32500;&#38142;"&#25552;&#31034;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#25968;&#23398;&#25110;&#25512;&#29702;&#30340;&#22797;&#26434;&#20219;&#21153;&#20013;&#12290;&#23613;&#31649;&#33719;&#24471;&#20102;&#24040;&#22823;&#30340;&#23454;&#35777;&#25104;&#21151;&#65292;&#20294;&#8220;&#24605;&#32500;&#38142;&#8221;&#32972;&#21518;&#30340;&#26426;&#21046;&#20197;&#21450;&#23427;&#22914;&#20309;&#37322;&#25918;LLMs&#30340;&#28508;&#21147;&#20173;&#28982;&#26159;&#31070;&#31192;&#30340;&#12290;&#26412;&#25991;&#39318;&#27425;&#20174;&#29702;&#35770;&#19978;&#22238;&#31572;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#24102;&#26377;&#8220;&#24605;&#32500;&#38142;&#8221;&#22312;&#35299;&#20915;&#22522;&#26412;&#25968;&#23398;&#21644;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#32473;&#20986;&#19968;&#20010;&#19981;&#21487;&#33021;&#30340;&#32467;&#26524;&#65292;&#34920;&#26126;&#20219;&#20309;&#26377;&#38480;&#28145;&#24230;&#30340;Transformer&#37117;&#19981;&#33021;&#30452;&#25509;&#36755;&#20986;&#27491;&#30830;&#30340;&#22522;&#26412;&#31639;&#26415;/&#26041;&#31243;&#20219;&#21153;&#30340;&#31572;&#26696;&#65292;&#38500;&#38750;&#27169;&#22411;&#22823;&#23567;&#38543;&#30528;&#36755;&#20837;&#38271;&#24230;&#30340;&#22686;&#21152;&#21576;&#36229;&#22810;&#39033;&#24335;&#22686;&#38271;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#36890;&#36807;&#26500;&#36896;&#35777;&#26126;&#65292;&#22823;&#23567;&#24658;&#23450;&#30340;&#33258;&#22238;&#24402;Transformer&#36275;&#20197;&#36890;&#36807;&#20351;&#29992;&#24120;&#29992;&#30340;&#25968;&#23398;&#35821;&#35328;&#24418;&#24335;&#29983;&#25104;&#8220;&#24605;&#32500;&#38142;&#8221;&#25512;&#23548;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the capacity of LLMs with CoT in solving fundamental mathematical and decision-making problems. We start by giving an impossibility result showing that any bounded-depth Transformer cannot directly output correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of a constant size suffice to solve both tasks by generating CoT derivations using a commonly-used math language forma
&lt;/p&gt;</description></item><item><title>SPECTRON&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#38899;&#24310;&#32493;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#38899;&#32534;&#30721;&#22120;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#26469;&#29983;&#25104;&#25991;&#26412;&#21644;&#35821;&#38899;&#36755;&#20986;&#65292;&#22312;&#35821;&#20041;&#20869;&#23481;&#21644;&#35762;&#35805;&#32773;&#20445;&#25252;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.15255</link><description>&lt;p&gt;
&#24102;&#26377;&#35821;&#38899;&#30340;LM&#65306;&#36229;&#36234;&#35821;&#38899;&#20196;&#29260;&#30340;&#21475;&#35821;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
LMs with a Voice: Spoken Language Modeling beyond Speech Tokens. (arXiv:2305.15255v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15255
&lt;/p&gt;
&lt;p&gt;
SPECTRON&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#38899;&#24310;&#32493;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#38899;&#32534;&#30721;&#22120;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#26469;&#29983;&#25104;&#25991;&#26412;&#21644;&#35821;&#38899;&#36755;&#20986;&#65292;&#22312;&#35821;&#20041;&#20869;&#23481;&#21644;&#35762;&#35805;&#32773;&#20445;&#25252;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SPECTRON&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20197;&#25191;&#34892;&#35821;&#38899;&#24310;&#32493;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#25991;&#26412;&#21644;&#35821;&#38899;&#36755;&#20986;&#65292;&#25972;&#20010;&#31995;&#32479;&#37117;&#22312;&#39057;&#35889;&#22270;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#12290;&#22312;&#39057;&#35889;&#22270;&#39046;&#22495;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#30456;&#23545;&#20110;&#20351;&#29992;&#31163;&#25955;&#35821;&#38899;&#34920;&#31034;&#30340;&#29616;&#26377;&#32423;&#32852;&#26041;&#27861;&#31616;&#21270;&#20102;&#25105;&#20204;&#30340;&#35821;&#38899;&#24310;&#32493;&#31995;&#32479;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35821;&#20041;&#20869;&#23481;&#21644;&#35762;&#35805;&#32773;&#20445;&#25252;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#21516;&#26102;&#20063;&#20174;&#39044;&#20808;&#23384;&#22312;&#30340;&#27169;&#22411;&#20013;&#33719;&#24471;&#20102;&#30693;&#35782;&#20256;&#36882;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#32593;&#31449;https://michelleramanovich.github.io/spectron/spectron&#19978;&#21487;&#20197;&#25214;&#21040;&#38899;&#39057;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SPECTRON, a novel approach to adapting pre-trained language models (LMs) to perform speech continuation. By leveraging pre-trained speech encoders, our model generates both text and speech outputs with the entire system being trained end-to-end operating directly on spectrograms. Training the entire model in the spectrogram domain simplifies our speech continuation system versus existing cascade methods which use discrete speech representations. We further show our method surpasses existing spoken language models both in semantic content and speaker preservation while also benefiting from the knowledge transferred from pre-existing models. Audio samples can be found in our website https://michelleramanovich.github.io/spectron/spectron
&lt;/p&gt;</description></item><item><title>BeamSearchQA&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36845;&#20195;&#24335;&#29983;&#25104;&#38382;&#39064;&#65292;&#20197;&#25429;&#25417;&#38544;&#21547;&#30693;&#35782;&#24182;&#20248;&#21270;&#38382;&#31572;&#36807;&#31243;&#65292;&#22312;NQ&#21644;WebQ&#27979;&#35797;&#38598;&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;71.7&#65285;&#21644;46.7&#65285;&#30340;F1&#20998;&#25968;&#65292;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.14766</link><description>&lt;p&gt;
BeamSearchQA: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#38646;-shot QA&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
BeamSearchQA: Large Language Models are Strong Zero-Shot QA Solver. (arXiv:2305.14766v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14766
&lt;/p&gt;
&lt;p&gt;
BeamSearchQA&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36845;&#20195;&#24335;&#29983;&#25104;&#38382;&#39064;&#65292;&#20197;&#25429;&#25417;&#38544;&#21547;&#30693;&#35782;&#24182;&#20248;&#21270;&#38382;&#31572;&#36807;&#31243;&#65292;&#22312;NQ&#21644;WebQ&#27979;&#35797;&#38598;&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;71.7&#65285;&#21644;46.7&#65285;&#30340;F1&#20998;&#25968;&#65292;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#31572;&#26159;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#36890;&#24120;&#38656;&#35201;&#35775;&#38382;&#22806;&#37096;&#20449;&#24687;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#21333;&#36718;&#26816;&#32034;-&#38405;&#35835;&#26041;&#27861;&#65292;&#39318;&#20808;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#65292;&#28982;&#21518;&#22522;&#20110;&#26816;&#32034;&#30340;&#20449;&#24687;&#22238;&#31572;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#22238;&#31572;&#38382;&#39064;&#38656;&#35201;&#38544;&#21547;&#30340;&#30693;&#35782;&#65292;&#36825;&#20123;&#30693;&#35782;&#19981;&#30452;&#25509;&#20174;&#38382;&#39064;&#26412;&#36523;&#20013;&#33719;&#24471;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#31572;&#27969;&#31243;&#65292;&#31216;&#20026;BeamSearchQA&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36845;&#20195;&#29983;&#25104;&#20851;&#20110;&#21407;&#22987;&#38382;&#39064;&#30340;&#26032;&#38382;&#39064;&#65292;&#23454;&#29616;&#36845;&#20195;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#36845;&#20195;&#32454;&#21270;&#21644;&#25193;&#23637;&#38382;&#39064;&#30340;&#33539;&#22260;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#25429;&#25417;&#24182;&#21033;&#29992;&#21487;&#33021;&#26080;&#27861;&#36890;&#36807;&#26816;&#32034;&#30452;&#25509;&#33719;&#21462;&#30340;&#38544;&#34255;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#24320;&#25918;&#39046;&#22495;NQ&#21644;WebQ&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BeamSearchQA&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22312;NQ&#21644;WebQ&#27979;&#35797;&#38598;&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;71.7&#65285;&#21644;46.7&#65285;&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-domain question answering is a crucial task that often requires accessing external information. Existing methods typically adopt a single-turn retrieve-then-read approach, where relevant documents are first retrieved, and questions are then answered based on the retrieved information. However, there are cases where answering a question requires implicit knowledge that is not directly retrievable from the question itself. In this work, we propose a novel question-answering pipeline called eamSearchQA. Our approach leverages large language models(LLMs) to iteratively generate new questions about the original question, enabling an iterative reasoning process. By iteratively refining and expanding the scope of the question, our method aims to capture and utilize hidden knowledge that may not be directly obtainable through retrieval. We evaluate our approach on the widely-used open-domain NQ and WebQ datasets. The experimental results demonstrate that BeamSearchQA significantly outperf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;DirecT2V&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23548;&#28436;&#65292;&#20174;&#19968;&#20010;&#25277;&#35937;&#30340;&#29992;&#25143;&#25552;&#31034;&#20013;&#29983;&#25104;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#36830;&#36143;&#19988;&#36830;&#36143;&#30340;&#35270;&#39057;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;LLM&#23548;&#28436;&#23558;&#29992;&#25143;&#36755;&#20837;&#20998;&#20026;&#27599;&#19968;&#24103;&#30340;&#25552;&#31034;&#65292;&#36890;&#36807;&#20540;&#26144;&#23556;&#21644;&#21452;softmax&#36807;&#28388;&#22120;&#26469;&#20445;&#25345;&#26102;&#38388;&#19968;&#33268;&#21644;&#38450;&#27490;&#23545;&#35937;&#25240;&#21472;&#12290;</title><link>http://arxiv.org/abs/2305.14330</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#24103;&#32423;&#23548;&#28436;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Frame-level Directors for Zero-shot Text-to-Video Generation. (arXiv:2305.14330v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;DirecT2V&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23548;&#28436;&#65292;&#20174;&#19968;&#20010;&#25277;&#35937;&#30340;&#29992;&#25143;&#25552;&#31034;&#20013;&#29983;&#25104;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#36830;&#36143;&#19988;&#36830;&#36143;&#30340;&#35270;&#39057;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;LLM&#23548;&#28436;&#23558;&#29992;&#25143;&#36755;&#20837;&#20998;&#20026;&#27599;&#19968;&#24103;&#30340;&#25552;&#31034;&#65292;&#36890;&#36807;&#20540;&#26144;&#23556;&#21644;&#21452;softmax&#36807;&#28388;&#22120;&#26469;&#20445;&#25345;&#26102;&#38388;&#19968;&#33268;&#21644;&#38450;&#27490;&#23545;&#35937;&#25240;&#21472;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#30340;&#33539;&#24335;&#20013;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#28857;&#25918;&#22312;&#23558;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#25193;&#23637;&#21040;&#25991;&#26412;&#21040;&#35270;&#39057;&#65288;T2V&#65289;&#29983;&#25104;&#19978;&#12290;&#23613;&#31649;&#36825;&#20123;&#26694;&#26550;&#24456;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#30528;&#32500;&#25252;&#19968;&#33268;&#30340;&#21465;&#36848;&#21644;&#22788;&#29702;&#20174;&#21333;&#20010;&#29992;&#25143;&#25552;&#31034;&#20013;&#30340;&#24555;&#36895;&#22330;&#26223;&#32452;&#21512;&#25110;&#23545;&#35937;&#20301;&#32622;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;DirecT2V&#65292;&#23427;&#21033;&#29992;&#38024;&#23545;&#25351;&#20196;&#26657;&#20934;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#21333;&#20010;&#25277;&#35937;&#29992;&#25143;&#25552;&#31034;&#29983;&#25104;&#36880;&#24103;&#25551;&#36848;&#12290;DirecT2V&#21033;&#29992;LLM&#23548;&#28436;&#23558;&#29992;&#25143;&#36755;&#20837;&#20998;&#20026;&#27599;&#20010;&#24103;&#30340;&#21333;&#29420;&#25552;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#21253;&#21547;&#26102;&#38388;&#21464;&#21270;&#30340;&#20869;&#23481;&#21644;&#20415;&#20110;&#19968;&#33268;&#30340;&#35270;&#39057;&#29983;&#25104;&#12290;&#20026;&#20102;&#20445;&#25345;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#24615;&#21644;&#38450;&#27490;&#23545;&#35937;&#25240;&#21472;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20540;&#26144;&#23556;&#26041;&#27861;&#21644;&#21452;softmax&#36807;&#28388;&#22120;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;DirecT2V&#26694;&#26550;&#22312;&#38646;&#26679;&#26412;T2V&#29983;&#25104;&#20013;&#20135;&#29983;&#30340;&#35270;&#35273;&#36830;&#36143;&#21644;&#19968;&#33268;&#30340;&#35270;&#39057;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the paradigm of AI-generated content (AIGC), there has been increasing attention in extending pre-trained text-to-image (T2I) models to text-to-video (T2V) generation. Despite their effectiveness, these frameworks face challenges in maintaining consistent narratives and handling rapid shifts in scene composition or object placement from a single user prompt. This paper introduces a new framework, dubbed DirecT2V, which leverages instruction-tuned large language models (LLMs) to generate frame-by-frame descriptions from a single abstract user prompt. DirecT2V utilizes LLM directors to divide user inputs into separate prompts for each frame, enabling the inclusion of time-varying content and facilitating consistent video generation. To maintain temporal consistency and prevent object collapse, we propose a novel value mapping method and dual-softmax filtering. Extensive experimental results validate the effectiveness of the DirecT2V framework in producing visually coherent and consist
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20013;&#25991;&#23545;&#35805;&#32423;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#30340;&#21021;&#27493;&#24037;&#20316;&#65292;&#22312;&#26032;&#26500;&#24314;&#20102;&#30340;&#39640;&#36136;&#37327;&#20154;&#24037;&#26631;&#27880;&#35821;&#26009;&#24211;&#19978;&#65292;&#36890;&#36807;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#30340;&#25968;&#25454;&#23398;&#20064;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#20449;&#21495;&#30340;&#23558;&#35265;&#21040;&#30340;&#21477;&#27861;&#20381;&#23384;&#36716;&#25442;&#25104;&#20803;&#32032;&#35805;&#35821;&#21333;&#20301;&#20043;&#38388;&#30340;&#26410;&#35265;&#20381;&#23384;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21333;&#35270;&#35282;&#21644;&#22810;&#35270;&#35282;&#30340;&#25968;&#25454;&#36873;&#25321;&#26469;&#35775;&#38382;&#21487;&#38752;&#30340;&#20266;&#26631;&#35760;&#23454;&#20363;&#65292;&#21462;&#24471;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.12441</link><description>&lt;p&gt;
&#20013;&#25991;&#23545;&#35805;&#32423;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#30340;&#21021;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Pilot Study on Dialogue-Level Dependency Parsing for Chinese. (arXiv:2305.12441v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12441
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20013;&#25991;&#23545;&#35805;&#32423;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#30340;&#21021;&#27493;&#24037;&#20316;&#65292;&#22312;&#26032;&#26500;&#24314;&#20102;&#30340;&#39640;&#36136;&#37327;&#20154;&#24037;&#26631;&#27880;&#35821;&#26009;&#24211;&#19978;&#65292;&#36890;&#36807;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#30340;&#25968;&#25454;&#23398;&#20064;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#20449;&#21495;&#30340;&#23558;&#35265;&#21040;&#30340;&#21477;&#27861;&#20381;&#23384;&#36716;&#25442;&#25104;&#20803;&#32032;&#35805;&#35821;&#21333;&#20301;&#20043;&#38388;&#30340;&#26410;&#35265;&#20381;&#23384;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21333;&#35270;&#35282;&#21644;&#22810;&#35270;&#35282;&#30340;&#25968;&#25454;&#36873;&#25321;&#26469;&#35775;&#38382;&#21487;&#38752;&#30340;&#20266;&#26631;&#35760;&#23454;&#20363;&#65292;&#21462;&#24471;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#32423;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#23588;&#20854;&#26159;&#20013;&#25991;&#26041;&#38754;&#30340;&#30740;&#31350;&#36824;&#19981;&#36275;&#12290;&#22240;&#27492;&#25105;&#20204;&#20511;&#37492;&#20102;&#21477;&#27861;&#20381;&#23384;&#29702;&#35770;&#21644;&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#65288;RST&#65289;&#65292;&#26500;&#24314;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#26631;&#27880;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;850&#20010;&#23545;&#35805;&#21644;199,803&#20010;&#20381;&#23384;&#20851;&#31995;&#12290;&#37492;&#20110;&#27492;&#31867;&#20219;&#21153;&#30340;&#39640;&#26114;&#26631;&#27880;&#25104;&#26412;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#24773;&#20917;&#12290;&#22522;&#20110;&#29616;&#26377;&#30340;&#21477;&#27861;&#26641;&#24211;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#21495;&#30340;&#26041;&#27861;&#23558;&#35265;&#21040;&#30340;&#21477;&#27861;&#20381;&#23384;&#36716;&#25442;&#25104;&#20803;&#32032;&#35805;&#35821;&#21333;&#20301;&#20043;&#38388;&#30340;&#26410;&#35265;&#20381;&#23384;&#65292;&#20854;&#20013;&#20449;&#21495;&#26159;&#36890;&#36807;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#26816;&#27979;&#24471;&#21040;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#21333;&#35270;&#35282;&#21644;&#22810;&#35270;&#35282;&#30340;&#25968;&#25454;&#36873;&#25321;&#26469;&#35775;&#38382;&#21487;&#38752;&#30340;&#20266;&#26631;&#35760;&#23454;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#22522;&#32447;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26377;&#20851;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#30340;&#20960;&#20010;&#20851;&#38190;&#35201;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue-level dependency parsing has received insufficient attention, especially for Chinese. To this end, we draw on ideas from syntactic dependency and rhetorical structure theory (RST), developing a high-quality human-annotated corpus, which contains 850 dialogues and 199,803 dependencies. Considering that such tasks suffer from high annotation costs, we investigate zero-shot and few-shot scenarios. Based on an existing syntactic treebank, we adopt a signal-based method to transform seen syntactic dependencies into unseen ones between elementary discourse units (EDUs), where the signals are detected by masked language modeling. Besides, we apply single-view and multi-view data selection to access reliable pseudo-labeled instances. Experimental results show the effectiveness of these baselines. Moreover, we discuss several crucial points about our dataset and approach.
&lt;/p&gt;</description></item><item><title>&#38646;&#26679;&#26412;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#23481;&#26131;&#20986;&#29616;&#8220;&#31163;&#35889;&#38382;&#39064;&#8221;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#31639;&#27861;LAVS&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#35821;&#35328;&#20043;&#38388;&#30340;KL&#20998;&#27495;&#26174;&#33879;&#38477;&#20302;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10930</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#8220;&#31163;&#35889;&#38382;&#39064;&#8221;
&lt;/p&gt;
&lt;p&gt;
On the Off-Target Problem of Zero-Shot Multilingual Neural Machine Translation. (arXiv:2305.10930v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10930
&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#23481;&#26131;&#20986;&#29616;&#8220;&#31163;&#35889;&#38382;&#39064;&#8221;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#31639;&#27861;LAVS&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#35821;&#35328;&#20043;&#38388;&#30340;KL&#20998;&#27495;&#26174;&#33879;&#38477;&#20302;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#20173;&#28982;&#23384;&#22312;&#8220;&#31163;&#35889;&#38382;&#39064;&#8221;&#65292;&#21363;&#23558;&#32763;&#35793;&#36755;&#20986;&#21040;&#38169;&#35823;&#30340;&#35821;&#35328;&#20013;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#38646;&#26679;&#26412;&#32763;&#35793;&#20219;&#21153;&#20013;&#26356;&#21152;&#26126;&#26174;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#24403;&#32534;&#30721;&#30446;&#26631;&#35821;&#35328;&#20449;&#21495;&#26102;&#22833;&#25928;&#65292;&#20250;&#23548;&#33268;&#31163;&#35889;&#38382;&#39064;&#65292;&#24182;&#19988;&#20004;&#31181;&#35821;&#35328;&#35789;&#27719;&#20043;&#38388;&#26356;&#25509;&#36817;&#30340;&#35789;&#27719;&#36317;&#31163;&#65288;&#21363;KL&#20998;&#27495;&#65289;&#19982;&#26356;&#39640;&#30340;&#31163;&#35889;&#29575;&#26377;&#20851;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#21457;&#29616;&#65292;&#20165;&#38548;&#31163;&#35299;&#30721;&#22120;&#20013;&#19981;&#21516;&#35821;&#35328;&#30340;&#35789;&#27719;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#31639;&#27861;Language Aware Vocabulary Sharing (LAVS)&#26469;&#26500;&#24314;&#22810;&#35821;&#35328;&#35789;&#27719;&#34920;&#65292;&#36890;&#36807;&#22686;&#21152;&#35821;&#35328;&#20043;&#38388;&#30340;KL&#20998;&#27495;&#65292;&#22823;&#22823;&#20943;&#36731;&#20102;&#32763;&#35793;&#27169;&#22411;&#30340;&#31163;&#35889;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;11&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#32763;&#35793;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;90&#20010;&#32763;&#35793;&#20219;&#21153;&#65292;&#37319;&#29992;LAVS&#30340;&#31163;&#35889;&#29575;&#38477;&#20302;&#20102;37&#65285;&#33267;90&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
While multilingual neural machine translation has achieved great success, it suffers from the off-target issue, where the translation is in the wrong language. This problem is more pronounced on zero-shot translation tasks. In this work, we find that failing in encoding discriminative target language signal will lead to off-target and a closer lexical distance (i.e., KL-divergence) between two languages' vocabularies is related with a higher off-target rate. We also find that solely isolating the vocab of different languages in the decoder can alleviate the problem. Motivated by the findings, we propose Language Aware Vocabulary Sharing (LAVS), a simple and effective algorithm to construct the multilingual vocabulary, that greatly alleviates the off-target problem of the translation model by increasing the KL-divergence between languages. We conduct experiments on a multilingual machine translation benchmark in 11 languages. Experiments show that the off-target rate for 90 translation 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#23545;&#35805;&#20013;&#22522;&#20110;&#38544;&#21947;&#30340;&#33457;&#21644;&#26893;&#29289;&#21517;&#31216;&#65292;&#37492;&#21035;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;GPT-3.5&#65292;&#26368;&#22909;&#30340;&#34920;&#29616;&#22120;&#22312;&#20219;&#21153;&#20013;&#25253;&#21578;&#20102;92.2349&#65285;&#30340;F1&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.10833</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#25552;&#21462;&#33457;&#21644;&#26893;&#29289;&#30340;&#38544;&#21947;&#24615;&#21517;&#31216;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Methods for Extracting Metaphorical Names of Flowers and Plants. (arXiv:2305.10833v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#23545;&#35805;&#20013;&#22522;&#20110;&#38544;&#21947;&#30340;&#33457;&#21644;&#26893;&#29289;&#21517;&#31216;&#65292;&#37492;&#21035;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;GPT-3.5&#65292;&#26368;&#22909;&#30340;&#34920;&#29616;&#22120;&#22312;&#20219;&#21153;&#20013;&#25253;&#21578;&#20102;92.2349&#65285;&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26893;&#29289;&#23398;&#39046;&#22495;&#20805;&#28385;&#20102;&#38544;&#21947;&#24615;&#26415;&#35821;&#65292;&#36825;&#20123;&#26415;&#35821;&#22312;&#25551;&#36848;&#21644;&#35782;&#21035;&#33457;&#21644;&#26893;&#29289;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20294;&#26159;&#65292;&#22312;&#23545;&#35805;&#20013;&#35782;&#21035;&#36825;&#20123;&#26415;&#35821;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;&#22312;&#32763;&#35793;&#36807;&#31243;&#21644;&#35789;&#20856;&#32534;&#32386;&#20219;&#21153;&#20013;&#65292;&#36825;&#24448;&#24448;&#23548;&#33268;&#38169;&#35823;&#30340;&#21457;&#29983;&#12290;&#24403;&#28041;&#21450;&#21040;&#21333;&#35789;&#21644;&#30701;&#35821;&#26102;&#65292;&#22312;&#26426;&#22120;&#32763;&#35793;&#26041;&#38754;&#36825;&#20010;&#36807;&#31243;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#21644;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#25216;&#26415;&#30340;&#26368;&#26032;&#20851;&#27880;&#28857;&#20043;&#19968;&#26159;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#33258;&#21160;&#35782;&#21035;&#23545;&#35805;&#20013;&#22522;&#20110;&#38544;&#21947;&#30340;&#21333;&#35789;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#21313;&#19977;&#31181;&#27969;&#34892;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#20197;&#21450;ChatGPT&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#24182;&#19988;&#36890;&#36807;F1&#24471;&#20998;&#35777;&#26126;&#20102;&#37492;&#21035;&#27169;&#22411;&#20248;&#20110;GPT-3.5&#27169;&#22411;&#65292;&#25105;&#20204;&#26368;&#22909;&#30340;&#34920;&#29616;&#22120;&#22312;&#38544;&#21947;&#33457;&#21321;&#21644;&#26893;&#29289;&#21517;&#31216;&#35782;&#21035;&#20219;&#21153;&#20013;&#25253;&#21578;&#20102;92.2349&#65285;&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The domain of Botany is rich with metaphorical terms. Those terms play an important role in the description and identification of flowers and plants. However, the identification of such terms in discourse is an arduous task. This leads in some cases to committing errors during translation processes and lexicographic tasks. The process is even more challenging when it comes to machine translation, both in the cases of single-word terms and multi-word terms. One of the recent concerns of Natural Language Processing (NLP) applications and Machine Translation (MT) technologies is the automatic identification of metaphor-based words in discourse through Deep Learning (DL). In this study, we seek to fill this gap through the use of thirteen popular transformer based models, as well as ChatGPT, and we show that discriminative models perform better than GPT-3.5 model with our best performer reporting 92.2349% F1 score in metaphoric flower and plant names identification task.
&lt;/p&gt;</description></item><item><title>ChatGPT&#20316;&#20026;&#25991;&#26412;&#31616;&#21270;&#24037;&#20855;&#21487;&#20197;&#21435;&#38500;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#26576;&#20123;&#29305;&#23450;&#32676;&#20307;&#30340;&#20559;&#35265;&#65292;&#20943;&#23569;&#27169;&#22411;&#30340;&#27495;&#35270;&#24615;&#12290;&#65288;&#27880;&#65306;ChatGPT&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65289;</title><link>http://arxiv.org/abs/2305.06166</link><description>&lt;p&gt;
ChatGPT&#20316;&#20026;&#21435;&#38500;&#20559;&#35265;&#30340;&#25991;&#26412;&#31616;&#21270;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
ChatGPT as a Text Simplification Tool to Remove Bias. (arXiv:2305.06166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06166
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20316;&#20026;&#25991;&#26412;&#31616;&#21270;&#24037;&#20855;&#21487;&#20197;&#21435;&#38500;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#26576;&#20123;&#29305;&#23450;&#32676;&#20307;&#30340;&#20559;&#35265;&#65292;&#20943;&#23569;&#27169;&#22411;&#30340;&#27495;&#35270;&#24615;&#12290;&#65288;&#27880;&#65306;ChatGPT&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65289;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25429;&#25417;&#21040;&#29305;&#23450;&#23376;&#32676;&#20307;&#30340;&#29305;&#23450;&#35821;&#35328;&#20449;&#21495;&#65292;&#22914;&#26524;&#27169;&#22411;&#23398;&#20064;&#20102;&#25429;&#25417;&#26576;&#20010;&#32676;&#20307;&#30340;&#35821;&#35328;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#27495;&#35270;&#12290;&#22914;&#26524;&#27169;&#22411;&#24320;&#22987;&#23558;&#29305;&#23450;&#35821;&#35328;&#19982;&#26576;&#20010;&#29305;&#23450;&#32676;&#20307;&#32852;&#31995;&#36215;&#26469;&#65292;&#22522;&#20110;&#27492;&#35821;&#35328;&#20570;&#20986;&#30340;&#20219;&#20309;&#20915;&#31574;&#37117;&#23558;&#19982;&#20854;&#21463;&#20445;&#25252;&#29305;&#24449;&#26377;&#30528;&#24378;&#28872;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#21487;&#33021;&#30340;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#65292;&#21363;&#25991;&#26412;&#31616;&#21270;&#12290;&#36825;&#20010;&#24819;&#27861;&#30340;&#39537;&#21160;&#21147;&#26159;&#31616;&#21270;&#25991;&#26412;&#24212;&#35813;&#26631;&#20934;&#21270;&#35821;&#35328;&#65292;&#20351;&#20854;&#20197;&#19968;&#31181;&#26041;&#24335;&#35828;&#35805;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#21547;&#20041;&#12290;&#23454;&#39564;&#26174;&#31034;&#65292;&#38024;&#23545;&#25935;&#24863;&#23646;&#24615;&#39044;&#27979;&#30340;&#20998;&#31867;&#22120;&#31934;&#24230;&#20250;&#22240;&#20351;&#29992;&#31616;&#21270;&#25968;&#25454;&#32780;&#19979;&#38477;&#39640;&#36798;17%&#12290;
&lt;/p&gt;
&lt;p&gt;
The presence of specific linguistic signals particular to a certain sub-group of people can be picked up by language models during training. This may lead to discrimination if the model has learnt to pick up on a certain group's language. If the model begins to associate specific language with a distinct group, any decisions made based upon this language would hold a strong correlation to a decision based on their protected characteristic. We explore a possible technique for bias mitigation in the form of simplification of text. The driving force of this idea is that simplifying text should standardise language to one way of speaking while keeping the same meaning. The experiment shows promising results as the classifier accuracy for predicting the sensitive attribute drops by up to 17% for the simplified data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#23454;&#29616;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01555</link><description>&lt;p&gt;
&#22914;&#20309;&#21457;&#25381;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?. (arXiv:2305.01555v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#23454;&#29616;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20294;&#26159;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#36824;&#27809;&#26377;&#24471;&#21040;&#20840;&#38754;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#35814;&#32454;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#20351;&#29992;GPT-3.5&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#30340;&#22522;&#26412;&#26041;&#27861;&#8212;&#8212;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25968;&#25454;&#29983;&#25104;&#12290;&#20026;&#20102;&#22686;&#24378;&#23569;&#26679;&#26412;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#19982;&#20197;&#21069;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#32780;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25512;&#21160;&#20197;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#22312;&#22235;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#28608;&#21457;&#26410;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312; \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm} &#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling language models have revolutionized widespread NLP tasks, yet little comprehensively explored few-shot relation extraction with large language models. In this paper, we investigate principal methodologies, in-context learning and data generation, for few-shot relation extraction via GPT-3.5 through exhaustive experiments. To enhance few-shot performance, we further propose task-related instructions and schema-constrained data generation. We observe that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets. We hope our work can inspire future research for the capabilities of large language models in few-shot relation extraction. Code is available in \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#30740;&#31350;&#20102;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#25913;&#36827;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#26368;&#36817;&#30740;&#31350;&#65292;&#21253;&#25324;&#23545;&#21453;&#39304;&#30340;&#20840;&#38754;&#24418;&#24335;&#21270;&#12289;&#21453;&#39304;&#30340;&#26684;&#24335;&#21644;&#30446;&#30340;&#30340;&#25551;&#36848;&#65292;&#21644;&#25552;&#20986;&#20351;&#29992;&#21453;&#39304;&#30340;&#20004;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#29992;&#20110;&#20154;&#31867;&#21453;&#39304;&#25968;&#25454;&#25910;&#38598;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#21644;&#30456;&#20851;&#30340;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2305.00955</link><description>&lt;p&gt;
&#26550;&#36215;&#26725;&#26753;&#65306;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#25972;&#21512;&#65288;&#20154;&#31867;&#65289;&#21453;&#39304;&#30340;&#35843;&#26597;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation. (arXiv:2305.00955v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#30740;&#31350;&#20102;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#25913;&#36827;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#26368;&#36817;&#30740;&#31350;&#65292;&#21253;&#25324;&#23545;&#21453;&#39304;&#30340;&#20840;&#38754;&#24418;&#24335;&#21270;&#12289;&#21453;&#39304;&#30340;&#26684;&#24335;&#21644;&#30446;&#30340;&#30340;&#25551;&#36848;&#65292;&#21644;&#25552;&#20986;&#20351;&#29992;&#21453;&#39304;&#30340;&#20004;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#29992;&#20110;&#20154;&#31867;&#21453;&#39304;&#25968;&#25454;&#25910;&#38598;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#21644;&#30456;&#20851;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#37117;&#26159;&#22522;&#20110;&#21033;&#29992;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#25968;&#25454;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33539;&#24335;&#21487;&#33021;&#20250;&#23548;&#33268;&#29983;&#25104;&#26377;&#23475;&#12289;&#19981;&#20934;&#30830;&#21644;&#26080;&#29992;&#20869;&#23481;&#30340;&#27169;&#22411;&#65292;&#32780;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#36890;&#24120;&#26080;&#27861;&#35782;&#21035;&#36825;&#20123;&#34892;&#20026;&#12290;&#38543;&#30528;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#24378;&#22823;&#65292;&#20154;&#31867;&#21453;&#39304;&#25104;&#20026;&#35780;&#20215;&#21644;&#25913;&#36827;&#27169;&#22411;&#30340;&#23453;&#36149;&#20449;&#21495;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#27010;&#36848;&#26368;&#36817;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#25913;&#36827;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#23545;&#21453;&#39304;&#30340;&#20840;&#38754;&#24418;&#24335;&#21270;&#65292;&#24182;&#26681;&#25454;&#36825;&#31181;&#24418;&#24335;&#21270;&#23558;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20998;&#31867;&#21644;&#32452;&#32455;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21453;&#39304;&#21487;&#20197;&#36890;&#36807;&#20854;&#26684;&#24335;&#21644;&#30446;&#30340;&#26469;&#25551;&#36848;&#65292;&#24182;&#28085;&#30422;&#20102;&#25552;&#20986;&#20351;&#29992;&#21453;&#39304;&#30340;&#20004;&#31181;&#26041;&#27861;&#65288;&#29992;&#20110;&#35757;&#32451;&#25110;&#35299;&#30721;&#65289;&#65306;&#30452;&#25509;&#20351;&#29992;&#21453;&#39304;&#25110;&#35757;&#32451;&#21453;&#39304;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#29992;&#20110;&#20154;&#31867;&#21453;&#39304;&#25968;&#25454;&#25910;&#38598;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#30456;&#20851;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent advances in natural language generation have been fueled by training large language models on internet-scale data. However, this paradigm can lead to models that generate toxic, inaccurate, and unhelpful content, and automatic evaluation metrics often fail to identify these behaviors. As models become more capable, human feedback is an invaluable signal for evaluating and improving models. This survey aims to provide an overview of the recent research that has leveraged human feedback to improve natural language generation. First, we introduce an encompassing formalization of feedback, and identify and organize existing research into a taxonomy following this formalization. Next, we discuss how feedback can be described by its format and objective, and cover the two approaches proposed to use feedback (either for training or decoding): directly using the feedback or training feedback models. We also discuss existing datasets for human-feedback data collection, and concerns 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#22522;&#20110;BERT&#30340;NLP&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65306;&#34429;&#28982;DistilBERT&#21644;TinyBERT&#31561;&#36731;&#37327;&#32423;&#27169;&#22411;&#30456;&#23545;&#21344;&#29992;&#26356;&#23569;&#20869;&#23384;&#65292;&#20294;&#23427;&#20204;&#22312;&#22797;&#26434;&#30340;NLP&#20219;&#21153;&#19978;&#34920;&#29616;&#36739;&#24046;&#65307;ResNet-based BERT&#27169;&#22411;&#21487;&#20197;&#22312;&#31934;&#24230;&#21644;&#36164;&#28304;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#30340;&#24179;&#34913;&#65292;&#36866;&#21512;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#12290;</title><link>http://arxiv.org/abs/2304.11520</link><description>&lt;p&gt;
&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#22522;&#20110;BERT&#30340;NLP&#27169;&#22411;&#30340;&#25361;&#25112;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring Challenges of Deploying BERT-based NLP Models in Resource-Constrained Embedded Devices. (arXiv:2304.11520v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#22522;&#20110;BERT&#30340;NLP&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65306;&#34429;&#28982;DistilBERT&#21644;TinyBERT&#31561;&#36731;&#37327;&#32423;&#27169;&#22411;&#30456;&#23545;&#21344;&#29992;&#26356;&#23569;&#20869;&#23384;&#65292;&#20294;&#23427;&#20204;&#22312;&#22797;&#26434;&#30340;NLP&#20219;&#21153;&#19978;&#34920;&#29616;&#36739;&#24046;&#65307;ResNet-based BERT&#27169;&#22411;&#21487;&#20197;&#22312;&#31934;&#24230;&#21644;&#36164;&#28304;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#30340;&#24179;&#34913;&#65292;&#36866;&#21512;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;BERT&#30340;&#31070;&#32463;&#26550;&#26500;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#19979;&#28216;NLP&#20219;&#21153;&#30340;&#27969;&#34892;&#20808;&#36827;&#25216;&#26415;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26550;&#26500;&#23545;&#25968;&#25454;&#20381;&#36182;&#24615;&#24378;&#65292;&#21344;&#29992;&#22823;&#37327;&#20869;&#23384;&#21644;&#33021;&#37327;&#65292;&#32463;&#24120;&#38459;&#30861;&#23427;&#20204;&#22312;&#35768;&#22810;&#23454;&#26102;&#12289;&#36164;&#28304;&#21463;&#38480;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#37096;&#32626;&#12290;&#29616;&#26377;&#30340;BERT&#36731;&#37327;&#32423;&#29256;&#26412;&#65288;&#20363;&#22914;DistilBERT&#21644;TinyBERT&#65289;&#36890;&#24120;&#22312;&#22797;&#26434;&#30340;NLP&#20219;&#21153;&#19978;&#26080;&#27861;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#20174;&#35774;&#35745;&#24072;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#35201;&#20026;&#29305;&#23450;&#30340;NLP&#20219;&#21153;&#20351;&#29992;&#20309;&#31181;&#8220;&#27491;&#30830;&#30340;&#8221;&#22522;&#20110;BERT&#30340;&#26550;&#26500;&#65292;&#20197;&#22312;&#36164;&#28304;&#21487;&#29992;&#24615;&#21644;&#26368;&#32456;&#29992;&#25143;&#38656;&#27714;&#30340;&#26368;&#23567;&#31934;&#24230;&#20043;&#38388;&#23454;&#29616;&#26368;&#20339;&#26435;&#34913;&#65292;&#23578;&#19981;&#30830;&#23450;&#12290;&#31995;&#32479;&#24037;&#31243;&#24072;&#24517;&#39035;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#36827;&#34892;&#35797;&#38169;&#23454;&#39564;&#65292;&#20197;&#25214;&#21040;&#21512;&#36866;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#22312;&#19981;&#21516;&#30340;&#36164;&#28304;&#38480;&#21046;&#21644;&#31934;&#24230;&#39044;&#31639;&#19979;&#23545;BERT-based&#27169;&#22411;&#36827;&#34892;&#20102;&#25506;&#31350;&#24615;&#30740;&#31350;&#65292;&#20197;&#24471;&#20986;&#26377;&#20851;&#27492;&#36164;&#28304;/&#31934;&#24230;&#26435;&#34913;&#30340;&#32463;&#39564;&#24615;&#35266;&#23519;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;DistilBERT&#21644;TinyBERT&#31561;&#26356;&#36731;&#37327;&#32423;&#30340;&#27169;&#22411;&#30456;&#23545;BERT-base&#21344;&#29992;&#30340;&#20869;&#23384;&#35201;&#23569;&#24471;&#22810;&#65292;&#20294;&#23427;&#20204;&#22312;&#22797;&#26434;&#30340;NLP&#20219;&#21153;&#20013;&#31934;&#24230;&#30340;&#19979;&#38477;&#26159;&#26126;&#26174;&#30340;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;ResNet&#30340;BERT&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20934;&#30830;&#24615;&#21644;&#36164;&#28304;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#30340;&#24179;&#34913;&#65292;&#20351;&#20854;&#25104;&#20026;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#20013;&#37096;&#32626;&#30340;&#33391;&#22909;&#20505;&#36873;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
BERT-based neural architectures have established themselves as popular state-of-the-art baselines for many downstream NLP tasks. However, these architectures are data-hungry and consume a lot of memory and energy, often hindering their deployment in many real-time, resource-constrained applications. Existing lighter versions of BERT (eg. DistilBERT and TinyBERT) often cannot perform well on complex NLP tasks. More importantly, from a designer's perspective, it is unclear what is the "right" BERT-based architecture to use for a given NLP task that can strike the optimal trade-off between the resources available and the minimum accuracy desired by the end user. System engineers have to spend a lot of time conducting trial-and-error experiments to find a suitable answer to this question. This paper presents an exploratory study of BERT-based models under different resource constraints and accuracy budgets to derive empirical observations about this resource/accuracy trade-offs. Our findin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30340;&#30740;&#31350;&#21457;&#29616;&#20013;&#38388;&#26816;&#26597;&#28857;&#27604;&#23436;&#20840;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#22909;&#22320;&#39044;&#27979;&#27169;&#22411;&#30340;&#35760;&#24518;&#21270;&#34892;&#20026;&#65292;&#24182;&#19988;&#21457;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35760;&#24518;&#21270;&#24471;&#20998;&#30340;&#20998;&#24067;&#35268;&#24459;&#12290;</title><link>http://arxiv.org/abs/2304.11158</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31361;&#29616;&#21644;&#21487;&#39044;&#30693;&#24615;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Emergent and Predictable Memorization in Large Language Models. (arXiv:2304.11158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11158
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30340;&#30740;&#31350;&#21457;&#29616;&#20013;&#38388;&#26816;&#26597;&#28857;&#27604;&#23436;&#20840;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#22909;&#22320;&#39044;&#27979;&#27169;&#22411;&#30340;&#35760;&#24518;&#21270;&#34892;&#20026;&#65292;&#24182;&#19988;&#21457;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35760;&#24518;&#21270;&#24471;&#20998;&#30340;&#20998;&#24067;&#35268;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35760;&#24518;&#21270;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36755;&#20986;&#20854;&#35757;&#32451;&#25968;&#25454;&#23436;&#20840;&#30456;&#21516;&#24207;&#21015;&#30340;&#20542;&#21521;&#65292;&#36825;&#26159;&#23433;&#20840;&#37096;&#32626;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#38382;&#39064;&#20043;&#19968;&#12290;&#29305;&#21035;&#22320;&#65292;&#26368;&#23567;&#21270;&#27169;&#22411;&#23545;&#21253;&#21547;&#20010;&#20154;&#21487;&#35782;&#21035;&#20449;&#24687;&#65288;PII&#65289;&#31561;&#25935;&#24863;&#25968;&#25454;&#28857;&#30340;&#35760;&#24518;&#21270;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36825;&#31181;&#19981;&#33391;&#30340;&#35760;&#24518;&#21270;&#30340;&#26222;&#21450;&#21487;&#33021;&#20250;&#32473;&#27169;&#22411;&#35757;&#32451;&#32773;&#24102;&#26469;&#38382;&#39064;&#65292;&#29978;&#33267;&#21487;&#33021;&#38656;&#35201;&#20002;&#24323;&#21542;&#21017;&#21151;&#33021;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#25512;&#26029;&#20302;&#35745;&#31639;&#21147;&#35797;&#39564;&#36816;&#34892;&#30340;&#35760;&#24518;&#21270;&#34892;&#20026;&#26469;&#39044;&#27979;&#21738;&#20123;&#24207;&#21015;&#23558;&#22312;&#22823;&#22411;&#27169;&#22411;&#30340;&#20840;&#23616;&#22521;&#35757;&#26399;&#38388;&#36827;&#34892;&#35760;&#24518;&#21270;&#12290;&#25105;&#20204;&#27979;&#37327;&#20102;Pythia&#27169;&#22411;&#22871;&#20214;&#30340;&#35760;&#24518;&#21270;&#65292;&#21457;&#29616;&#20013;&#38388;&#26816;&#26597;&#28857;&#27604;&#36739;&#23567;&#30340;&#24050;&#23436;&#20840;&#35757;&#32451;&#27169;&#22411;&#26356;&#22909;&#22320;&#39044;&#27979;&#20102;&#27169;&#22411;&#30340;&#35760;&#24518;&#21270;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#26377;&#20851;&#27169;&#22411;&#21644;&#25968;&#25454;&#35760;&#24518;&#21270;&#20998;&#25968;&#20998;&#24067;&#30340;&#36827;&#19968;&#27493;&#26032;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memorization, or the tendency of large language models (LLMs) to output entire sequences from their training data verbatim, is a key concern for safely deploying language models. In particular, it is vital to minimize a model's memorization of sensitive datapoints such as those containing personal identifiable information (PII). The prevalence of such undesirable memorization can pose issues for model trainers, and may even require discarding an otherwise functional model. We therefore seek to predict which sequences will be memorized before a large model's full train-time by extrapolating the memorization behavior of lower-compute trial runs. We measure memorization of the Pythia model suite, and find that intermediate checkpoints are better predictors of a model's memorization behavior than smaller fully-trained models. We additionally provide further novel discoveries on the distribution of memorization scores across models and data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23450;&#37327;&#23454;&#39564;&#26126;&#30830;&#20102;&#26426;&#22120;&#29983;&#25104;&#21644;&#20154;&#31867;&#25776;&#20889;&#30340;&#31185;&#25216;&#35770;&#25991;&#20043;&#38388;&#30340;&#20851;&#38190;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26234;&#33021;&#24037;&#20316;&#27969;&#65292;&#26088;&#22312;&#26377;&#25928;&#35299;&#20915;&#31185;&#25216;&#35770;&#25991;&#26816;&#27979;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.05011</link><description>&lt;p&gt;
&#28151;&#21512;&#26234;&#33021;&#31185;&#25216;&#35770;&#25991;&#26816;&#27979;&#30340;&#29702;&#35299;&#19982;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Towards an Understanding and Explanation for Mixed-Initiative Artificial Scientific Text Detection. (arXiv:2304.05011v1 [cs.HC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23450;&#37327;&#23454;&#39564;&#26126;&#30830;&#20102;&#26426;&#22120;&#29983;&#25104;&#21644;&#20154;&#31867;&#25776;&#20889;&#30340;&#31185;&#25216;&#35770;&#25991;&#20043;&#38388;&#30340;&#20851;&#38190;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26234;&#33021;&#24037;&#20316;&#27969;&#65292;&#26088;&#22312;&#26377;&#25928;&#35299;&#20915;&#31185;&#25216;&#35770;&#25991;&#26816;&#27979;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22240;&#20854;&#20986;&#33394;&#30340;&#29983;&#25104;&#20154;&#31867;&#21270;&#25991;&#26412;&#30340;&#33021;&#21147;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#24191;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#20854;&#28508;&#22312;&#35823;&#29992;&#24341;&#36215;&#20102;&#31038;&#20250;&#20851;&#27880;&#65292;&#20027;&#35201;&#26159;&#20851;&#20110;&#23398;&#26415;&#25220;&#34989;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20960;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#65306;1&#65289;&#32570;&#20047;&#23545;&#26426;&#22120;&#29983;&#25104;&#21644;&#20154;&#31867;&#25776;&#20889;&#30340;&#31185;&#25216;&#35770;&#25991;&#24046;&#24322;&#30340;&#26126;&#30830;&#29702;&#35299;&#12289;2&#65289;&#30001;&#20110;&#20998;&#24067;&#38382;&#39064;&#23548;&#33268;&#29616;&#26377;&#26041;&#27861;&#30340;&#24046;&#24378;&#20154;&#24847;&#27867;&#21270;&#34920;&#29616;&#12289;&#20197;&#21450;3&#65289;&#26816;&#27979;&#36807;&#31243;&#20013;&#23545;&#20154;&#26426;&#21327;&#20316;&#30340;&#25903;&#25345;&#21450;&#36275;&#22815;&#35299;&#37322;&#24615;&#30340;&#26377;&#38480;&#65292;&#20351;&#24471;&#26377;&#25928;&#30340;&#31185;&#25216;&#35770;&#25991;&#26816;&#27979;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#39318;&#20808;&#36890;&#36807;&#19968;&#20010;&#23450;&#37327;&#23454;&#39564;&#8203;&#8203;&#8203;&#26126;&#30830;&#20102;&#26426;&#22120;&#29983;&#25104;&#21644;&#20154;&#31867;&#25776;&#20889;&#30340;&#31185;&#25216;&#35770;&#25991;&#20043;&#38388;&#30340;&#20851;&#38190;&#24046;&#24322;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26234;&#33021;&#24037;&#20316;&#27969;&#65292;&#32467;&#21512;&#20154;&#31867;&#19987;&#23478;&#30340;&#20808;&#39564;&#30693;&#35782;&#21644;&#26426;&#22120;&#26234;&#33021;&#65292;&#20197;&#21450;&#19968;&#20010;&#21487;&#35270;&#21270;&#20998;&#26512;&#21407;&#22411;&#26469;&#20419;&#36827;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have gained popularity in various fields for their exceptional capability of generating human-like text. Their potential misuse has raised social concerns about plagiarism in academic contexts. However, effective artificial scientific text detection is a non-trivial task due to several challenges, including 1) the lack of a clear understanding of the differences between machine-generated and human-written scientific text, 2) the poor generalization performance of existing methods caused by out-of-distribution issues, and 3) the limited support for human-machine collaboration with sufficient interpretability during the detection process. In this paper, we first identify the critical distinctions between machine-generated and human-written scientific text through a quantitative experiment. Then, we propose a mixed-initiative workflow that combines human experts' prior knowledge with machine intelligence, along with a visual analytics prototype to facilitate e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;-&#35805;&#35821;&#27880;&#24847;&#21147;&#21644;&#32852;&#21512;&#24314;&#27169;&#30340;&#26597;&#35810;&#24863;&#30693;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#27169;&#22359;&#35745;&#31639;&#35805;&#35821;&#32423;&#21035;&#19982;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#23558;&#26631;&#35760;&#32423;&#21035;&#30340;&#26597;&#35810;&#20851;&#32852;&#24615;&#21644;&#35805;&#35821;&#32423;&#21035;&#30340;&#26597;&#35810;&#20851;&#32852;&#24615;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#29983;&#25104;&#19968;&#20010;&#26356;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#25688;&#35201;&#12290;&#32463;&#36807;&#23545;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;QFMS&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.04487</link><description>&lt;p&gt;
&#22522;&#20110;&#26597;&#35810;-&#35805;&#35821;&#27880;&#24847;&#21147;&#21644;&#32852;&#21512;&#24314;&#27169;&#30340;&#26597;&#35810;&#28966;&#28857;&#20250;&#35758;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Query-Utterance Attention with Joint modeling for Query-Focused Meeting Summarization. (arXiv:2303.04487v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;-&#35805;&#35821;&#27880;&#24847;&#21147;&#21644;&#32852;&#21512;&#24314;&#27169;&#30340;&#26597;&#35810;&#24863;&#30693;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#27169;&#22359;&#35745;&#31639;&#35805;&#35821;&#32423;&#21035;&#19982;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#23558;&#26631;&#35760;&#32423;&#21035;&#30340;&#26597;&#35810;&#20851;&#32852;&#24615;&#21644;&#35805;&#35821;&#32423;&#21035;&#30340;&#26597;&#35810;&#20851;&#32852;&#24615;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#29983;&#25104;&#19968;&#20010;&#26356;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#25688;&#35201;&#12290;&#32463;&#36807;&#23545;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;QFMS&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#28966;&#28857;&#20250;&#35758;&#25688;&#35201;&#65288;QFMS&#65289;&#26088;&#22312;&#26681;&#25454;&#32473;&#23450;&#30340;&#26597;&#35810;&#65292;&#20174;&#20250;&#35758;&#35760;&#24405;&#20013;&#29983;&#25104;&#25688;&#35201;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;&#26597;&#35810;&#19982;&#20250;&#35758;&#35760;&#24405;&#25340;&#25509;&#36215;&#26469;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#38544;&#24335;&#22320;&#23545;&#26631;&#35760;&#32423;&#21035;&#30340;&#26597;&#35810;&#30456;&#20851;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38271;&#26102;&#38388;&#30340;&#20250;&#35758;&#35760;&#24405;&#23548;&#33268;&#20851;&#38190;&#30340;&#26597;&#35810;&#30456;&#20851;&#20449;&#24687;&#34987;&#31232;&#37322;&#65292;&#22240;&#27492;&#21407;&#22987;&#30340;&#22522;&#20110;&#36716;&#25442;&#30340;&#27169;&#22411;&#19981;&#36275;&#20197;&#31361;&#20986;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#20851;&#38190;&#37096;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;-&#35805;&#35821;&#27880;&#24847;&#21147;&#21644;&#32852;&#21512;&#24314;&#27169;&#30340;&#26597;&#35810;&#24863;&#30693;&#26694;&#26550;&#12290;&#23427;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#27169;&#22359;&#35745;&#31639;&#35805;&#35821;&#32423;&#21035;&#19982;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#21518;&#65292;&#23558;&#26631;&#35760;&#32423;&#21035;&#30340;&#26597;&#35810;&#20851;&#32852;&#24615;&#21644;&#35805;&#35821;&#32423;&#21035;&#30340;&#26597;&#35810;&#20851;&#32852;&#24615;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#26126;&#30830;&#30340;&#27880;&#24847;&#26426;&#21046;&#25972;&#21512;&#21040;&#29983;&#25104;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#19981;&#21516;&#39063;&#31890;&#24230;&#30340;&#26597;&#35810;&#30456;&#20851;&#24615;&#26377;&#21161;&#20110;&#29983;&#25104;&#19968;&#20010;&#26356;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#25688;&#35201;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;QFMS&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Query-focused meeting summarization (QFMS) aims to generate summaries from meeting transcripts in response to a given query. Previous works typically concatenate the query with meeting transcripts and implicitly model the query relevance only at the token level with attention mechanism. However, due to the dilution of key query-relevant information caused by long meeting transcripts, the original transformer-based model is insufficient to highlight the key parts related to the query. In this paper, we propose a query-aware framework with joint modeling token and utterance based on Query-Utterance Attention. It calculates the utterance-level relevance to the query with a dense retrieval module. Then both token-level query relevance and utterance-level query relevance are combined and incorporated into the generation process with attention mechanism explicitly. We show that the query relevance of different granularities contributes to generating a summary more related to the query. Exper
&lt;/p&gt;</description></item><item><title>Almanac&#26159;&#19968;&#20010;&#24102;&#26377;&#26816;&#32034;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#20026;&#20020;&#24202;&#21307;&#23398;&#30340;&#25351;&#21335;&#21644;&#27835;&#30103;&#24314;&#35758;&#25552;&#20379;&#20102;&#20107;&#23454;&#24615;&#12289;&#23436;&#22791;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#22686;&#24378;&#12290;&#35813;&#27169;&#22411;&#26377;&#26395;&#25104;&#20026;&#20020;&#24202;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2303.01229</link><description>&lt;p&gt;
Almanac: &#26816;&#32034;&#22686;&#24378;&#30340;&#20020;&#24202;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Almanac: Retrieval-Augmented Language Models for Clinical Medicine. (arXiv:2303.01229v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01229
&lt;/p&gt;
&lt;p&gt;
Almanac&#26159;&#19968;&#20010;&#24102;&#26377;&#26816;&#32034;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#20026;&#20020;&#24202;&#21307;&#23398;&#30340;&#25351;&#21335;&#21644;&#27835;&#30103;&#24314;&#35758;&#25552;&#20379;&#20102;&#20107;&#23454;&#24615;&#12289;&#23436;&#22791;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#22686;&#24378;&#12290;&#35813;&#27169;&#22411;&#26377;&#26395;&#25104;&#20026;&#20020;&#24202;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;-shot&#33021;&#21147;&#65292;&#20363;&#22914;&#25688;&#35201;&#12289;&#23545;&#35805;&#29983;&#25104;&#21644;&#38382;&#31572;&#12290;&#34429;&#28982;&#22312;&#20020;&#24202;&#21307;&#23398;&#20013;&#26377;&#35768;&#22810;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#24448;&#24448;&#21463;&#21040;&#23427;&#20204;&#29983;&#25104;&#38169;&#35823;&#29978;&#33267;&#26377;&#26102;&#26159;&#26377;&#23475;&#35328;&#35770;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Almanac&#65292;&#36825;&#26159;&#19968;&#20010;&#24102;&#26377;&#26816;&#32034;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#21307;&#30103;&#25351;&#21335;&#21644;&#27835;&#30103;&#24314;&#35758;&#12290;&#30001;5&#21517;&#33891;&#20107;&#20250;&#35748;&#35777;&#21644;&#20303;&#38498;&#21307;&#24072;&#35780;&#20272;&#30340;130&#20010;&#20020;&#24202;&#22330;&#26223;&#30340;&#26032;&#25968;&#25454;&#38598;&#30340;&#34920;&#29616;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#19987;&#19994;&#39046;&#22495;&#20013;&#65292;&#20107;&#23454;&#24615;&#26174;&#33879;&#25552;&#39640;&#65288;&#22312;p&#20540;&lt;0.05&#30340;&#24773;&#20917;&#19979;&#22686;&#21152;&#20102;18%&#30340;&#24179;&#22343;&#20540;&#65289;&#65292;&#24182;&#25913;&#21892;&#20102;&#23436;&#22791;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#28508;&#21147;&#25104;&#20026;&#20020;&#24202;&#20915;&#31574;&#36807;&#31243;&#20013;&#26377;&#25928;&#30340;&#24037;&#20855;&#65292;&#21516;&#26102;&#20063;&#24378;&#35843;&#20102;&#27491;&#30830;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-language models have recently demonstrated impressive zero-shot capabilities in a variety of natural language tasks such as summarization, dialogue generation, and question-answering. Despite many promising applications in clinical medicine, adoption of these models in real-world settings has been largely limited by their tendency to generate incorrect and sometimes even toxic statements. In this study, we develop Almanac, a large language model framework augmented with retrieval capabilities for medical guideline and treatment recommendations. Performance on a novel dataset of clinical scenarios (n = 130) evaluated by a panel of 5 board-certified and resident physicians demonstrates significant increases in factuality (mean of 18% at p-value &lt; 0.05) across all specialties, with improvements in completeness and safety. Our results demonstrate the potential for large language models to be effective tools in the clinical decision-making process, while also emphasizing the importanc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;GPT&#27169;&#22411;&#30340;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;GEMBA&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#38646;-shot&#25552;&#31034;&#12289;&#22810;&#31181;&#27169;&#24335;&#19979;&#19982;WMT22&#24230;&#37327;&#20849;&#20139;&#20219;&#21153;&#30340;MQM&#20154;&#31867;&#26631;&#31614;&#30456;&#27604;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#20013;&#30340;&#23454;&#29992;&#24615;&#25552;&#20379;&#20102;&#39318;&#27425;&#30340;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2302.14520</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#30340;&#26368;&#20339;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are State-of-the-Art Evaluators of Translation Quality. (arXiv:2302.14520v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14520
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;GPT&#27169;&#22411;&#30340;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;GEMBA&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#38646;-shot&#25552;&#31034;&#12289;&#22810;&#31181;&#27169;&#24335;&#19979;&#19982;WMT22&#24230;&#37327;&#20849;&#20139;&#20219;&#21153;&#30340;MQM&#20154;&#31867;&#26631;&#31614;&#30456;&#27604;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#20013;&#30340;&#23454;&#29992;&#24615;&#25552;&#20379;&#20102;&#39318;&#27425;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;GPT&#30340;GEMBA&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#32763;&#35793;&#36136;&#37327;&#65292;&#21487;&#20197;&#22312;&#26377;&#25110;&#27809;&#26377;&#21442;&#32771;&#32763;&#35793;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#38646;-shot&#25552;&#31034;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#27604;&#36739;&#20102;&#20004;&#31181;&#22522;&#20110;&#21442;&#32771;&#32763;&#35793;&#21487;&#29992;&#24615;&#30340;&#27169;&#24335;&#19979;&#30340;&#22235;&#31181;&#25552;&#31034;&#21464;&#37327;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;9&#20010;GPT&#27169;&#22411;&#29256;&#26412;&#65292;&#21253;&#25324;ChatGPT&#21644;GPT-4&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#25105;&#20204;&#30340;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#21482;&#36866;&#29992;&#20110;GPT 3.5&#21450;&#26356;&#22823;&#30340;&#27169;&#22411;&#12290;&#19982;WMT22&#24230;&#37327;&#20849;&#20139;&#20219;&#21153;&#30340;&#32467;&#26524;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#31181;&#27169;&#24335;&#19979;&#19982;MQM&#20154;&#31867;&#26631;&#31614;&#30456;&#27604;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#25152;&#26377;&#19977;&#31181;WMT22&#24230;&#37327;&#20849;&#20139;&#20219;&#21153;&#30340;&#35821;&#35328;&#23545;&#65288;&#21363;&#33521;&#35821;&#21040;&#24503;&#35821;&#12289;&#33521;&#35821;&#21040;&#20420;&#35821;&#21644;&#20013;&#25991;&#21040;&#33521;&#35821;&#65289;&#37117;&#36866;&#29992;&#12290;&#36825;&#20026;&#39044;&#35757;&#32451;&#12289;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#20013;&#30340;&#23454;&#29992;&#24615;&#25552;&#20379;&#20102;&#39318;&#27425;&#30340;&#23637;&#31034;&#12290;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#25152;&#26377;&#20195;&#30721;&#21644;&#29992;&#20110;&#23454;&#39564;&#30340;&#25552;&#31034;&#27169;&#26495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe GEMBA, a GPT-based metric for assessment of translation quality, which works both with a reference translation and without. In our evaluation, we focus on zero-shot prompting, comparing four prompt variants in two modes, based on the availability of the reference. We investigate nine versions of GPT models, including ChatGPT and GPT-4. We show that our method for translation quality assessment only works with GPT~3.5 and larger models. Comparing to results from WMT22's Metrics shared task, our method achieves state-of-the-art accuracy in both modes when compared to MQM-based human labels. Our results are valid on the system level for all three WMT22 Metrics shared task language pairs, namely English into German, English into Russian, and Chinese into English. This provides a first glimpse into the usefulness of pre-trained, generative large language models for quality assessment of translations. We publicly release all our code and prompt templates used for the experiments 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22312;&#35821;&#35328;&#24341;&#23548;&#30340;&#21512;&#36523;&#20195;&#29702;&#20013;&#20351;&#29992;&#22810;&#27169;&#24577;ASR&#27169;&#22411;&#26469;&#20943;&#23569;&#35821;&#38899;&#25351;&#20196;&#30340;&#36716;&#24405;&#38169;&#35823;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#35270;&#35273;&#19978;&#19979;&#25991;&#26469;&#25552;&#39640;&#36716;&#24405;&#32467;&#26524;&#12290;&#27169;&#22411;&#33021;&#22815;&#24674;&#22797;&#22810;&#36798;30&#65285;&#30340;&#34987;&#23631;&#34109;&#21333;&#35789;&#65292;&#24182;&#19988;&#35757;&#32451;&#22522;&#20110;&#25991;&#26412;&#30340;&#21512;&#36523;&#20195;&#29702;&#22312;&#36981;&#24490;&#22810;&#27169;&#24577;ASR&#27169;&#22411;&#36716;&#24405;&#30340;&#25351;&#20196;&#19979;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#23436;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2302.14030</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#35821;&#38899;&#35782;&#21035;&#29992;&#20110;&#35821;&#35328;&#24341;&#23548;&#30340;&#21512;&#36523;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Multimodal Speech Recognition for Language-Guided Embodied Agents. (arXiv:2302.14030v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22312;&#35821;&#35328;&#24341;&#23548;&#30340;&#21512;&#36523;&#20195;&#29702;&#20013;&#20351;&#29992;&#22810;&#27169;&#24577;ASR&#27169;&#22411;&#26469;&#20943;&#23569;&#35821;&#38899;&#25351;&#20196;&#30340;&#36716;&#24405;&#38169;&#35823;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#35270;&#35273;&#19978;&#19979;&#25991;&#26469;&#25552;&#39640;&#36716;&#24405;&#32467;&#26524;&#12290;&#27169;&#22411;&#33021;&#22815;&#24674;&#22797;&#22810;&#36798;30&#65285;&#30340;&#34987;&#23631;&#34109;&#21333;&#35789;&#65292;&#24182;&#19988;&#35757;&#32451;&#22522;&#20110;&#25991;&#26412;&#30340;&#21512;&#36523;&#20195;&#29702;&#22312;&#36981;&#24490;&#22810;&#27169;&#24577;ASR&#27169;&#22411;&#36716;&#24405;&#30340;&#25351;&#20196;&#19979;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#24341;&#23548;&#30340;&#21512;&#36523;&#20195;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;&#36890;&#24120;&#20551;&#35774;&#22522;&#20110;&#25991;&#26412;&#30340;&#25351;&#20196;&#65292;&#20294;&#23454;&#38469;&#37096;&#32626;&#30340;&#20195;&#29702;&#20250;&#36935;&#21040;&#21475;&#22836;&#25351;&#20196;&#12290;&#23613;&#31649;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#21487;&#20197;&#24357;&#21512;&#35821;&#38899;&#36755;&#20837;&#24046;&#36317;&#65292;&#20294;&#38169;&#35823;&#30340;ASR&#36716;&#24405;&#20250;&#25439;&#23475;&#20195;&#29702;&#23436;&#25104;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#22810;&#27169;&#24577;ASR&#27169;&#22411;&#35757;&#32451;&#26469;&#32771;&#34385;&#20276;&#38543;&#30340;&#35270;&#35273;&#29615;&#22659;&#65292;&#20174;&#32780;&#20943;&#23569;&#35821;&#38899;&#25351;&#20196;&#30340;&#36716;&#24405;&#38169;&#35823;&#12290;&#25105;&#20204;&#22312;ALFRED&#20219;&#21153;&#23436;&#25104;&#25968;&#25454;&#38598;&#19978;&#21512;&#25104;&#20102;&#21475;&#22836;&#25351;&#20196;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#31995;&#32479;&#24615;&#22320;&#23631;&#34109;&#21475;&#22836;&#21333;&#35789;&#26469;&#27169;&#25311;&#22768;&#23398;&#22122;&#22768;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21033;&#29992;&#35270;&#35273;&#35266;&#23519;&#21487;&#20197;&#26377;&#21161;&#20110;&#24674;&#22797;&#23631;&#34109;&#30340;&#21333;&#35789;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;ASR&#27169;&#22411;&#65292;&#24674;&#22797;&#30340;&#23631;&#34109;&#21333;&#35789;&#27604;&#21333;&#27169;&#24577;&#22522;&#32447;&#22810;&#36798;30&#65285;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#22522;&#20110;&#25991;&#26412;&#35757;&#32451;&#30340;&#21512;&#36523;&#20195;&#29702;&#36890;&#36807;&#36981;&#24490;&#22810;&#27169;&#24577;ASR&#27169;&#22411;&#30340;&#36716;&#24405;&#25351;&#20196;&#26356;&#32463;&#24120;&#22320;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Benchmarks for language-guided embodied agents typically assume text-based instructions, but deployed agents will encounter spoken instructions. While Automatic Speech Recognition (ASR) models can bridge the input gap, erroneous ASR transcripts can hurt the agents' ability to complete tasks. In this work, we propose training a multimodal ASR model to reduce errors in transcribing spoken instructions by considering the accompanying visual context. We train our model on a dataset of spoken instructions, synthesized from the ALFRED task completion dataset, where we simulate acoustic noise by systematically masking spoken words. We find that utilizing visual observations facilitates masked word recovery, with multimodal ASR models recovering up to 30% more masked words than unimodal baselines. We also find that a text-trained embodied agent successfully completes tasks more often by following transcribed instructions from multimodal ASR models. github.com/Cylumn/embodied-multimodal-asr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21382;&#21490;&#27010;&#36848;&#20197;&#21450;ChatGPT&#32972;&#21518;&#30340;&#25216;&#26415;&#65292;&#24378;&#35843;&#20102;&#23427;&#22312;&#21307;&#30103;&#20445;&#20581;&#12289;&#25945;&#32946;&#21644;&#30740;&#31350;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#24182;&#25351;&#20986;&#20102;&#20854;&#38544;&#31169;&#21644;&#36947;&#24503;&#26041;&#38754;&#30340;&#25285;&#24551;&#20197;&#21450;&#24403;&#21069;&#29256;&#26412;&#30340;&#37325;&#35201;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2302.13817</link><description>&lt;p&gt;
&#35753;&#25105;&#20204;&#26469;&#32842;&#32842;&#21543;&#65281;&#19982;ChatGPT&#30340;&#23545;&#35805;&#65306;&#25216;&#26415;&#65292;&#24212;&#29992;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Let's have a chat! A Conversation with ChatGPT: Technology, Applications, and Limitations. (arXiv:2302.13817v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21382;&#21490;&#27010;&#36848;&#20197;&#21450;ChatGPT&#32972;&#21518;&#30340;&#25216;&#26415;&#65292;&#24378;&#35843;&#20102;&#23427;&#22312;&#21307;&#30103;&#20445;&#20581;&#12289;&#25945;&#32946;&#21644;&#30740;&#31350;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#24182;&#25351;&#20986;&#20102;&#20854;&#38544;&#31169;&#21644;&#36947;&#24503;&#26041;&#38754;&#30340;&#25285;&#24551;&#20197;&#21450;&#24403;&#21069;&#29256;&#26412;&#30340;&#37325;&#35201;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#27454;&#33021;&#22815;&#29983;&#25104;&#20687;&#20154;&#31867;&#19968;&#26679;&#30340;&#21477;&#23376;&#21644;&#20889;&#20986;&#36830;&#36143;&#25991;&#31456;&#30340;&#20154;&#24037;&#26234;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;ChatGPT&#30340;&#20986;&#29616;&#24341;&#36215;&#20102;&#19990;&#30028;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21382;&#21490;&#27010;&#36848;&#20197;&#21450;ChatGPT&#32972;&#21518;&#30340;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#36824;&#24378;&#35843;&#20102;ChatGPT&#22312;&#21508;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#65292;&#25945;&#32946;&#21644;&#30740;&#31350;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#23613;&#31649;&#26377;&#30528;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#65292;&#20294;&#26159;ChatGPT&#21608;&#22260;&#23384;&#22312;&#30528;&#19968;&#20123;&#38544;&#31169;&#21644;&#36947;&#24503;&#26041;&#38754;&#30340;&#25285;&#24551;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#24403;&#21069;&#29256;&#26412;ChatGPT&#30340;&#19968;&#20123;&#37325;&#35201;&#38480;&#21046;&#12290;&#25105;&#20204;&#36824;&#21521;ChatGPT&#25552;&#20986;&#20102;&#19968;&#20123;&#38382;&#39064;&#65292;&#20197;&#20415;&#23427;&#34920;&#36798;&#33258;&#24049;&#30340;&#30475;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of an AI-powered chatbot that can generate human-like sentences and write coherent essays has caught the world's attention. This paper discusses the historical overview of chatbots and the technology behind Chat Generative Pre-trained Transformer, better known as ChatGPT. Moreover, potential applications of ChatGPT in various domains, including healthcare, education, and research, are highlighted. Despite promising results, there are several privacy and ethical concerns surrounding ChatGPT. In addition, we highlight some of the important limitations of the current version of ChatGPT. We also ask ChatGPT to provide its point of view and present its responses to several questions we attempt to answer.
&lt;/p&gt;</description></item><item><title>ProsAudit&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#38901;&#24459;&#22522;&#20934;&#27979;&#35797;&#65292;&#30001;&#21407;&#22411;&#21477;&#27861;&#20219;&#21153;&#21644;&#35789;&#27719;&#20219;&#21153;&#20004;&#37096;&#20998;&#32452;&#25104;&#12290;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#23545;&#34920;&#29616;&#26377;&#26126;&#26174;&#24433;&#21709;&#65292;&#35789;&#27719;&#20219;&#21153;&#20013;&#27597;&#35821;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#38750;&#27597;&#35821;&#27169;&#22411;&#34920;&#29616;&#36739;&#24046;&#12290;</title><link>http://arxiv.org/abs/2302.12057</link><description>&lt;p&gt;
ProsAudit&#65306;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#38901;&#24459;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
ProsAudit, a prosodic benchmark for self-supervised speech models. (arXiv:2302.12057v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12057
&lt;/p&gt;
&lt;p&gt;
ProsAudit&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#38901;&#24459;&#22522;&#20934;&#27979;&#35797;&#65292;&#30001;&#21407;&#22411;&#21477;&#27861;&#20219;&#21153;&#21644;&#35789;&#27719;&#20219;&#21153;&#20004;&#37096;&#20998;&#32452;&#25104;&#12290;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#23545;&#34920;&#29616;&#26377;&#26126;&#26174;&#24433;&#21709;&#65292;&#35789;&#27719;&#20219;&#21153;&#20013;&#27597;&#35821;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#38750;&#27597;&#35821;&#27169;&#22411;&#34920;&#29616;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986; ProsAudit&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#35821;&#38899;&#27169;&#22411;&#20013;&#32467;&#26500;&#38901;&#24459;&#30693;&#35782;&#30340;&#33521;&#35821;&#22522;&#20934;&#27979;&#35797;&#12290;&#23427;&#30001;&#20004;&#20010;&#23376;&#20219;&#21153;&#12289;&#30456;&#24212;&#30340;&#25351;&#26631;&#21644;&#19968;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#32452;&#25104;&#12290;&#22312;&#21407;&#22411;&#21477;&#27861;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#24517;&#39035;&#27491;&#30830;&#35782;&#21035;&#24378;&#35843;&#21644;&#24369;&#35843;&#30340;&#38901;&#24459;&#36793;&#30028;&#12290;&#22312;&#35789;&#27719;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#38656;&#35201;&#27491;&#30830;&#21306;&#20998;&#25554;&#20837;&#21333;&#35789;&#21644;&#20869;&#37096;&#30340;&#20572;&#39039;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23545;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#30340;&#20154;&#20026;&#35780;&#20272;&#20998;&#25968;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015; SSL &#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#20004;&#20010;&#20219;&#21153;&#19978;&#37117;&#33021;&#22815;&#22312;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#38750;&#27597;&#35821;&#27169;&#22411;&#22312;&#35789;&#27719;&#20219;&#21153;&#19978;&#34920;&#29616;&#26126;&#26174;&#36739;&#24046;&#65292;&#31361;&#26174;&#20102;&#35813;&#20219;&#21153;&#20013;&#35789;&#27719;&#30693;&#35782;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#27169;&#22411;&#30340;&#22823;&#23567;&#23545;&#34920;&#29616;&#26377;&#26126;&#26174;&#30340;&#24433;&#21709;&#65292;&#35757;&#32451;&#25968;&#25454;&#26356;&#22810;&#30340;&#27169;&#22411;&#22312;&#36825;&#20004;&#20010;&#23376;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ProsAudit, a benchmark in English to assess structural prosodic knowledge in self-supervised learning (SSL) speech models. It consists of two subtasks, their corresponding metrics, and an evaluation dataset. In the protosyntax task, the model must correctly identify strong versus weak prosodic boundaries. In the lexical task, the model needs to correctly distinguish between pauses inserted between words and within words. We also provide human evaluation scores on this benchmark. We evaluated a series of SSL models and found that they were all able to perform above chance on both tasks, even when evaluated on an unseen language. However, non-native models performed significantly worse than native ones on the lexical task, highlighting the importance of lexical knowledge in this task. We also found a clear effect of size with models trained on more data performing better in the two subtasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29983;&#25104;Sokoban&#28216;&#25103;&#20851;&#21345;&#65292;&#24182;&#21457;&#29616;&#38543;&#30528;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;LLMs&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#24456;&#22823;&#30340;&#25552;&#39640;&#12290;&#26410;&#26469;&#24037;&#20316;&#30340;&#21069;&#26223;&#20063;&#34987;&#35752;&#35770;&#20102;&#12290;</title><link>http://arxiv.org/abs/2302.05817</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20851;&#21345;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Level Generation Through Large Language Models. (arXiv:2302.05817v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05817
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29983;&#25104;Sokoban&#28216;&#25103;&#20851;&#21345;&#65292;&#24182;&#21457;&#29616;&#38543;&#30528;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;LLMs&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#24456;&#22823;&#30340;&#25552;&#39640;&#12290;&#26410;&#26469;&#24037;&#20316;&#30340;&#21069;&#26223;&#20063;&#34987;&#35752;&#35770;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26159;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#35757;&#32451;&#20889;&#25925;&#20107;&#12289;&#29983;&#25104;&#20195;&#30721;&#21644;&#22238;&#31572;&#38382;&#39064;&#12290;&#20294;&#23427;&#20204;&#33021;&#21542;&#29983;&#25104;&#21151;&#33021;&#24615;&#30340;&#35270;&#39057;&#28216;&#25103;&#20851;&#21345;&#21602;&#65311;&#28216;&#25103;&#20851;&#21345;&#30001;&#20110;&#21151;&#33021;&#32422;&#26463;&#21644;&#22810;&#32500;&#31354;&#38388;&#20851;&#31995;&#30340;&#22797;&#26434;&#24615;&#65292;&#19982;LLM&#22312;&#35757;&#32451;&#26399;&#38388;&#36890;&#24120;&#30475;&#21040;&#30340;&#25968;&#25454;&#31181;&#31867;&#38750;&#24120;&#19981;&#21516;&#12290;&#28216;&#25103;&#20851;&#21345;&#30340;&#25968;&#25454;&#38598;&#20063;&#24456;&#38590;&#33719;&#24471;&#65292;&#21487;&#33021;&#20250;&#32791;&#23613;&#36825;&#20123;&#23545;&#25968;&#25454;&#26377;&#24378;&#28872;&#38656;&#27714;&#30340;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;LLMs&#29983;&#25104;Sokoban&#28216;&#25103;&#20851;&#21345;&#65292;&#24182;&#21457;&#29616;LLMs&#30830;&#23454;&#33021;&#22815;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#24182;&#19988;&#23427;&#30340;&#24615;&#33021;&#38543;&#30528;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#22823;&#24133;&#25552;&#39640;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#21021;&#27493;&#30340;&#23454;&#39564;&#26469;&#25511;&#21046;LLM&#20851;&#21345;&#29983;&#25104;&#22120;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#26377;&#21069;&#36884;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are powerful tools, capable of leveraging their training on natural language to write stories, generate code, and answer questions. But can they generate functional video game levels? Game levels, with their complex functional constraints and spatial relationships in more than one dimension, are very different from the kinds of data an LLM typically sees during training. Datasets of game levels are also hard to come by, potentially taxing the abilities of these data-hungry models. We investigate the use of LLMs to generate levels for the game Sokoban, finding that LLMs are indeed capable of doing so, and that their performance scales dramatically with dataset size. We also perform preliminary experiments on controlling LLM level generators and discuss promising areas for future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#19968;&#20123;&#25216;&#24039;&#29992;&#20110;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#65292;&#25552;&#20986;&#20102;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#25991;&#26412;&#25490;&#21517;&#20013;&#21487;&#20197;&#20351;&#29992;&#30340;&#25216;&#24039;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#25216;&#24039;&#23545;&#20110;&#25552;&#39640;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#30340;&#25928;&#26524;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2302.04460</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#30340;&#25216;&#24039;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
Bag of Tricks for Training Data Extraction from Language Models. (arXiv:2302.04460v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#19968;&#20123;&#25216;&#24039;&#29992;&#20110;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#65292;&#25552;&#20986;&#20102;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#25991;&#26412;&#25490;&#21517;&#20013;&#21487;&#20197;&#20351;&#29992;&#30340;&#25216;&#24039;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#25216;&#24039;&#23545;&#20110;&#25552;&#39640;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#30340;&#25928;&#26524;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#26029;&#30740;&#31350;&#65292;&#38544;&#31169;&#20445;&#25252;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#20316;&#20026;&#28508;&#22312;&#30340;&#35780;&#20272;&#38544;&#31169;&#27844;&#38706;&#30340;&#24037;&#20855;&#21464;&#24471;&#38750;&#24120;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#39033;&#20219;&#21153;&#30340;&#22256;&#38590;&#31243;&#24230;&#65292;&#30446;&#21069;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#20173;&#28982;&#19981;&#22815;&#26377;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#25216;&#24039;&#29992;&#20110;&#25913;&#36827;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#65292;&#25105;&#20204;&#23545;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25552;&#21462;&#26041;&#27861;&#20351;&#29992;&#29983;&#25104;&#28982;&#21518;&#25490;&#24207;&#30340;&#27969;&#31243;&#65288;&#20363;&#22914;&#65292;&#29983;&#25104;&#28508;&#22312;&#30340;&#35757;&#32451;&#25968;&#25454;&#25991;&#26412;&#65292;&#28982;&#21518;&#26681;&#25454;&#29305;&#23450;&#30340;&#26631;&#20934;&#23545;&#23427;&#20204;&#36827;&#34892;&#25490;&#24207;&#65289;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#25991;&#26412;&#29983;&#25104;&#21644;&#25991;&#26412;&#25490;&#21517;&#30340;&#25216;&#24039;&#12290;&#65288;&#20363;&#22914;&#65292;&#37319;&#26679;&#31574;&#30053;&#21644;&#20196;&#29260;&#32423;&#26631;&#20934;&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20123;&#20043;&#21069;&#34987;&#24573;&#35270;&#30340;&#25216;&#24039;&#23545;&#20110;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#30340;&#25104;&#21151;&#38750;&#24120;&#20851;&#38190;&#12290;&#22522;&#20110;GPT-Neo 1.3B&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#24039;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advance of language models, privacy protection is receiving more attention. Training data extraction is therefore of great importance, as it can serve as a potential tool to assess privacy leakage. However, due to the difficulty of this task, most of the existing methods are proof-of-concept and still not effective enough. In this paper, we investigate and benchmark tricks for improving training data extraction using a publicly available dataset. Because most existing extraction methods use a pipeline of generating-then-ranking, i.e., generating text candidates as potential training data and then ranking them based on specific criteria, our research focuses on the tricks for both text generation (e.g., sampling strategy) and text ranking (e.g., token-level criteria). The experimental results show that several previously overlooked tricks can be crucial to the success of training data extraction. Based on the GPT-Neo 1.3B evaluation results, our proposed tricks outperform the b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#31163;&#25955;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#25991;&#26412;&#25552;&#31034;&#65292;&#36827;&#32780;&#25511;&#21046;&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20248;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#30340;&#24212;&#29992;&#65292;&#20026;API&#29992;&#25143;&#25552;&#20379;&#20102;&#36731;&#26494;&#29983;&#25104;&#12289;&#21457;&#29616;&#12289;&#28151;&#21512;&#21644;&#21305;&#37197;&#22270;&#20687;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#33258;&#21160;&#21457;&#29616;&#30828;&#25552;&#31034;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.03668</link><description>&lt;p&gt;
&#30828;&#25552;&#31034;&#21464;&#31616;&#21333;&#65306;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#31163;&#25955;&#20248;&#21270;&#26041;&#27861;&#36827;&#34892;&#25552;&#31034;&#35843;&#33410;&#21644;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery. (arXiv:2302.03668v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03668
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#31163;&#25955;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#25991;&#26412;&#25552;&#31034;&#65292;&#36827;&#32780;&#25511;&#21046;&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20248;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#30340;&#24212;&#29992;&#65292;&#20026;API&#29992;&#25143;&#25552;&#20379;&#20102;&#36731;&#26494;&#29983;&#25104;&#12289;&#21457;&#29616;&#12289;&#28151;&#21512;&#21644;&#21305;&#37197;&#22270;&#20687;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#33258;&#21160;&#21457;&#29616;&#30828;&#25552;&#31034;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#30340;&#20248;&#28857;&#22312;&#20110;&#21487;&#20197;&#36890;&#36807;&#22522;&#20110;&#25991;&#26412;&#30340;&#25552;&#31034;&#36827;&#34892;&#25511;&#21046;&#12290;&#20256;&#32479;&#30340;&#8220;&#30828;&#8221;&#25552;&#31034;&#26159;&#30001;&#21487;&#35299;&#37322;&#30340;&#35789;&#27719;&#21644;&#26631;&#35760;&#26500;&#25104;&#65292;&#24517;&#39035;&#30001;&#20154;&#25163;&#24037;&#21046;&#20316;&#12290;&#27492;&#22806;&#36824;&#26377;&#8220;&#36719;&#8221;&#25552;&#31034;&#65292;&#23427;&#20204;&#30001;&#36830;&#32493;&#30340;&#29305;&#24449;&#21521;&#37327;&#32452;&#25104;&#65292;&#21487;&#20197;&#36890;&#36807;&#24378;&#22823;&#30340;&#20248;&#21270;&#26041;&#27861;&#21457;&#29616;&#65292;&#20294;&#23427;&#20204;&#19981;&#33021;&#24456;&#23481;&#26131;&#22320;&#35299;&#37322;&#65292;&#19981;&#33021;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#37325;&#22797;&#20351;&#29992;&#65292;&#20063;&#19981;&#33021;&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#25509;&#21475;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#25928;&#26799;&#24230;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#26469;&#31283;&#20581;&#22320;&#20248;&#21270;&#30828;&#25991;&#26412;&#25552;&#31034;&#12290;&#35813;&#26041;&#27861;&#33258;&#21160;&#20026;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#24212;&#29992;&#29983;&#25104;&#30828;&#25991;&#26412;&#25552;&#31034;&#12290;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#35774;&#32622;&#20013;&#65292;&#35813;&#26041;&#27861;&#20026;&#25193;&#25955;&#27169;&#22411;&#21019;&#24314;&#30828;&#25552;&#31034;&#65292;&#20351;API&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#29983;&#25104;&#12289;&#21457;&#29616;&#12289;&#28151;&#21512;&#21644;&#21305;&#37197;&#22270;&#20687;&#27010;&#24565;&#65292;&#32780;&#19981;&#38656;&#35201;&#20107;&#20808;&#20102;&#35299;&#22914;&#20309;&#25552;&#31034;&#27169;&#22411;&#12290;&#22312;&#25991;&#26412;&#21040;&#25991;&#26412;&#30340;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#21160;&#21457;&#29616;&#30828;&#25552;&#31034;&#21487;&#20197;&#26377;&#25928;&#22320;&#20248;&#21270;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The strength of modern generative models lies in their ability to be controlled through text-based prompts. Typical "hard" prompts are made from interpretable words and tokens, and must be hand-crafted by humans. There are also "soft" prompts, which consist of continuous feature vectors. These can be discovered using powerful optimization methods, but they cannot be easily interpreted, re-used across models, or plugged into a text-based interface.  We describe an approach to robustly optimize hard text prompts through efficient gradient-based optimization. Our approach automatically generates hard text-based prompts for both text-to-image and text-to-text applications. In the text-to-image setting, the method creates hard prompts for diffusion models, allowing API users to easily generate, discover, and mix and match image concepts without prior knowledge on how to prompt the model. In the text-to-text setting, we show that hard prompts can be automatically discovered that are effectiv
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#20165;&#22788;&#29702;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#20687;&#36827;&#34892;&#32852;&#31995;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#12290;&#35813;&#26041;&#27861;&#22312;&#29615;&#22659;&#30456;&#20851;&#30340;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#21313;&#20998;&#20248;&#24322;&#65292;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#35270;&#35273;&#22330;&#26223;&#19979;&#20132;&#20114;&#38382;&#39064;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2301.13823</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#20687;&#36827;&#34892;&#32852;&#31995;&#20197;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Grounding Language Models to Images for Multimodal Inputs and Outputs. (arXiv:2301.13823v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13823
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#20165;&#22788;&#29702;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#20687;&#36827;&#34892;&#32852;&#31995;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#12290;&#35813;&#26041;&#27861;&#22312;&#29615;&#22659;&#30456;&#20851;&#30340;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#21313;&#20998;&#20248;&#24322;&#65292;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#35270;&#35273;&#22330;&#26223;&#19979;&#20132;&#20114;&#38382;&#39064;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#20165;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#19982;&#35270;&#35273;&#39046;&#22495;&#32852;&#31995;&#36215;&#26469;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#21033;&#29992;&#20174;&#22823;&#35268;&#27169;&#25991;&#26412;&#39044;&#35757;&#32451;&#20013;&#23398;&#21040;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#29983;&#25104;&#12290;&#25105;&#20204;&#20445;&#25345;&#35821;&#35328;&#27169;&#22411;&#20923;&#32467;&#65292;&#24182;&#24494;&#35843;&#36755;&#20837;&#21644;&#36755;&#20986;&#32447;&#24615;&#23618;&#20197;&#23454;&#29616;&#36328;&#27169;&#24577;&#20132;&#20114;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#12290;&#25105;&#20204;&#22312;&#29615;&#22659;&#30456;&#20851;&#30340;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#34920;&#29616;&#65292;&#24182;&#23637;&#31034;&#20102;&#24341;&#20154;&#20837;&#32988;&#30340;&#20132;&#20114;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#29616;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#22312;&#35270;&#35273;&#22330;&#26223;&#19979;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#19988;&#36890;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#20195;&#29702;&#35821;&#35328;&#23398;&#20064;&#19982;&#36890;&#20449;&#26694;&#26550;&#65292;&#25104;&#21151;&#22797;&#21046;&#20102;&#35821;&#24207;/&#26684;&#26631;&#20132;&#25442;&#36825;&#19968;&#24191;&#27867;&#23384;&#22312;&#30340;&#35821;&#35328;&#26222;&#36941;&#35268;&#24459;&#65292;&#23454;&#29616;&#20102;&#26356;&#30495;&#23454;&#30340;&#35821;&#35328;&#28436;&#21464;&#27169;&#25311;&#65292;&#24182;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#24433;&#21709;&#35821;&#35328;&#26222;&#36941;&#35268;&#24459;&#30340;&#35748;&#30693;&#21644;&#31038;&#20250;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2301.13083</link><description>&lt;p&gt;
&#31070;&#32463;&#20195;&#29702;&#36890;&#20449;&#25512;&#21160;&#35821;&#35328;&#26222;&#36941;&#35268;&#24459;&#30340;&#20986;&#29616;&#65306;&#20197;&#35821;&#24207;/&#26684;&#26631;&#20132;&#25442;&#20026;&#20363;&#35777;
&lt;/p&gt;
&lt;p&gt;
Communication Drives the Emergence of Language Universals in Neural Agents: Evidence from the Word-order/Case-marking Trade-off. (arXiv:2301.13083v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#20195;&#29702;&#35821;&#35328;&#23398;&#20064;&#19982;&#36890;&#20449;&#26694;&#26550;&#65292;&#25104;&#21151;&#22797;&#21046;&#20102;&#35821;&#24207;/&#26684;&#26631;&#20132;&#25442;&#36825;&#19968;&#24191;&#27867;&#23384;&#22312;&#30340;&#35821;&#35328;&#26222;&#36941;&#35268;&#24459;&#65292;&#23454;&#29616;&#20102;&#26356;&#30495;&#23454;&#30340;&#35821;&#35328;&#28436;&#21464;&#27169;&#25311;&#65292;&#24182;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#24433;&#21709;&#35821;&#35328;&#26222;&#36941;&#35268;&#24459;&#30340;&#35748;&#30693;&#21644;&#31038;&#20250;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#31070;&#32463;&#20195;&#29702;&#30340;&#35821;&#35328;&#28436;&#21464;&#21644;&#21464;&#21270;&#30340;&#27169;&#25311;&#20013;&#65292;&#20154;&#24037;&#23398;&#20064;&#32773;&#30340;&#34892;&#20026;&#36890;&#24120;&#19982;&#20154;&#31867;&#23398;&#20064;&#32773;&#19981;&#21516;&#65292;&#36825;&#24120;&#34987;&#24402;&#22240;&#20110;&#36825;&#20123;&#23398;&#20064;&#32773;&#32570;&#20047;&#36866;&#24403;&#30340;&#35748;&#30693;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#20063;&#26377;&#20154;&#25552;&#20986;&#26356;&#33258;&#28982;&#30340;&#35821;&#35328;&#23398;&#20064;&#21644;&#20351;&#29992;&#29615;&#22659;&#21487;&#33021;&#23548;&#33268;&#26356;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#31181;&#21518;&#19968;&#31181;&#35828;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#35821;&#24207;/&#26684;&#26631;&#20132;&#25442;&#65292;&#19968;&#31181;&#34987;&#24191;&#27867;&#35777;&#26126;&#30340;&#35821;&#35328;&#26222;&#36941;&#35268;&#24459;&#65292;&#36825;&#31181;&#35268;&#24459;&#22312;&#27169;&#25311;&#20013;&#34987;&#35777;&#26126;&#23588;&#20854;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#20195;&#29702;&#35821;&#35328;&#23398;&#20064;&#21644;&#36890;&#20449;&#26694;&#26550;&#65288;NeLLCom&#65289;&#65292;&#20854;&#20013;&#35828;&#35805;&#21644;&#21548;&#21462;&#30340;&#20195;&#29702;&#39318;&#20808;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#23398;&#20064;&#19968;&#31181;&#23567;&#35821;&#35328;&#65292;&#28982;&#21518;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23545;&#20854;&#36827;&#34892;&#20248;&#21270;&#20197;&#36827;&#34892;&#27807;&#36890;&#12290;&#32039;&#23494;&#36981;&#24490;&#26089;&#26399;&#20154;&#31867;&#23454;&#39564;&#30340;&#35774;&#32622;&#65292;&#25105;&#20204;&#25104;&#21151;&#22797;&#21046;&#20102;&#36825;&#31181;&#26032;&#26694;&#26550;&#19979;&#30340;&#20132;&#25442;&#65292;&#32780;&#19981;&#26159;&#22312;&#20195;&#29702;&#20013;&#30828;&#32534;&#30721;&#29305;&#23450;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#26159;&#21457;&#23637;&#26356;&#30495;&#23454;&#30340;&#35821;&#35328;&#28436;&#21464;&#27169;&#25311;&#21644;&#26356;&#22909;&#22320;&#29702;&#35299;&#24433;&#21709;&#35821;&#35328;&#26222;&#36941;&#35268;&#24459;&#30340;&#35748;&#30693;&#21644;&#31038;&#20250;&#22240;&#32032;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial learners often behave differently from human learners in the context of neural agent-based simulations of language emergence and change. A common explanation is the lack of appropriate cognitive biases in these learners. However, it has also been proposed that more naturalistic settings of language learning and use could lead to more human-like results. We investigate this latter account focusing on the word-order/case-marking trade-off, a widely attested language universal that has proven particularly hard to simulate. We propose a new Neural-agent Language Learning and Communication framework (NeLLCom) where pairs of speaking and listening agents first learn a miniature language via supervised learning, and then optimize it for communication via reinforcement learning. Following closely the setup of earlier human experiments, we succeed in replicating the trade-off with the new framework without hard-coding specific biases in the agents. We see this as an essential step to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#21644;&#24635;&#32467;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#30340;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;ICL&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#26032;&#33539;&#24335;&#65292;&#25506;&#32034;ICL&#20197;&#35780;&#20272;&#21644;&#25512;&#24191;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#33021;&#21147;&#24050;&#25104;&#20026;&#19968;&#31181;&#26032;&#36235;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ICL&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#24635;&#32467;&#20102;&#39640;&#32423;&#25216;&#26415;&#65292;&#26368;&#21518;&#35752;&#35770;&#20102;ICL&#30340;&#25361;&#25112;&#20197;&#21450;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2301.00234</link><description>&lt;p&gt;
&#20851;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on In-context Learning. (arXiv:2301.00234v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#21644;&#24635;&#32467;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#30340;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;ICL&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#26032;&#33539;&#24335;&#65292;&#25506;&#32034;ICL&#20197;&#35780;&#20272;&#21644;&#25512;&#24191;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#33021;&#21147;&#24050;&#25104;&#20026;&#19968;&#31181;&#26032;&#36235;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ICL&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#24635;&#32467;&#20102;&#39640;&#32423;&#25216;&#26415;&#65292;&#26368;&#21518;&#35752;&#35770;&#20102;ICL&#30340;&#25361;&#25112;&#20197;&#21450;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#19981;&#26029;&#22686;&#24378;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#22312;&#20854;&#20013;LLM&#20165;&#22522;&#20110;&#21152;&#20837;&#23569;&#37327;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#36827;&#34892;&#39044;&#27979;&#12290;&#25506;&#32034;ICL&#20197;&#35780;&#20272;&#21644;&#25512;&#24191;LLM&#30340;&#33021;&#21147;&#24050;&#25104;&#20026;&#19968;&#31181;&#26032;&#36235;&#21183;&#12290;&#26412;&#25991;&#26088;&#22312;&#35843;&#26597;&#21644;&#24635;&#32467;ICL&#30340;&#36827;&#23637;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;ICL&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#28548;&#28165;&#20854;&#19982;&#30456;&#20851;&#30740;&#31350;&#30340;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32452;&#32455;&#21644;&#35752;&#35770;&#39640;&#32423;&#25216;&#26415;&#65292;&#21253;&#25324;&#35757;&#32451;&#31574;&#30053;&#12289;&#28436;&#31034;&#35774;&#35745;&#31574;&#30053;&#20197;&#21450;&#30456;&#20851;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;ICL&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#40723;&#21169;&#26356;&#22810;&#30340;&#30740;&#31350;&#65292;&#25581;&#31034;ICL&#30340;&#24037;&#20316;&#21407;&#29702;&#24182;&#25913;&#36827;ICL&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing ability of large language models (LLMs), in-context learning (ICL) has become a new paradigm for natural language processing (NLP), where LLMs make predictions only based on contexts augmented with a few examples. It has been a new trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis. Finally, we discuss the challenges of ICL and provide potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISCO&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#21487;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#20107;&#23454;&#25968;&#25454;&#65292;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#31561;&#20219;&#21153;&#30340;&#22240;&#26524;&#25512;&#29702;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#25968;&#25454;&#35757;&#32451;&#26041;&#27861;&#65292;&#25928;&#26524;&#26356;&#22909;&#19988;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2212.10534</link><description>&lt;p&gt;
DISCO: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#28860;&#30701;&#35821;&#21453;&#20107;&#23454;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
DISCO: Distilling Phrasal Counterfactuals with Large Language Models. (arXiv:2212.10534v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISCO&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#21487;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#20107;&#23454;&#25968;&#25454;&#65292;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#31561;&#20219;&#21153;&#30340;&#22240;&#26524;&#25512;&#29702;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#25968;&#25454;&#35757;&#32451;&#26041;&#27861;&#65292;&#25928;&#26524;&#26356;&#22909;&#19988;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#21453;&#20107;&#23454;&#22686;&#24191;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#20219;&#21153;&#30340;&#22240;&#26524;&#32467;&#26500;&#34920;&#36798;&#65292;&#20174;&#32780;&#23454;&#29616;&#31283;&#20581;&#30340;&#27867;&#21270;&#12290;&#20294;&#23545;&#20110;&#22823;&#22810;&#25968;&#20219;&#21153;&#32780;&#35328;&#65292;&#39640;&#36136;&#37327;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#24456;&#23569;&#19988;&#38590;&#20197;&#22823;&#35268;&#27169;&#29983;&#25104;&#12290;&#24403;&#20351;&#29992;&#20247;&#21253;&#26041;&#27861;&#36827;&#34892;&#29983;&#25104;&#26102;&#65292;&#36890;&#24120;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#37117;&#26377;&#38480;&#12290;&#24403;&#20351;&#29992;&#26377;&#30417;&#30563;&#26041;&#27861;&#26102;&#65292;&#35201;&#23558;&#20854;&#25193;&#23637;&#21040;&#26032;&#30340;&#21453;&#20107;&#23454;&#32500;&#24230;&#26159;&#35745;&#31639;&#19978;&#26114;&#36149;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DISCO&#65288;DIStilled COunterfactual Data&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#35268;&#27169;&#19978;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#12290;DISCO&#24037;&#31243;&#24072;&#20351;&#29992;&#22823;&#22411;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25552;&#31034;&#20197;&#29983;&#25104;&#30701;&#35821;&#25200;&#21160;&#12290;&#28982;&#21518;&#65292;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#25945;&#24072;&#27169;&#22411;&#36807;&#28388;&#36825;&#20123;&#29983;&#25104;&#65292;&#20197;&#25552;&#21462;&#39640;&#36136;&#37327;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#12290;&#34429;&#28982;&#26159;&#38754;&#21521;&#20219;&#21153;&#30340;&#65292;&#25105;&#20204;&#24212;&#29992;&#25105;&#20204;&#30340;&#27969;&#31243;&#26469;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#22312;&#20687;NLI&#21387;&#21147;&#27979;&#35797;&#36825;&#26679;&#30340;&#25361;&#25112;&#24615;&#35780;&#20272;&#20013;&#65292;&#29992;DISCO&#29983;&#25104;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#30456;&#23545;&#36739;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#27604;&#20351;&#29992;&#20256;&#32479;&#65288;&#38750;&#21453;&#20107;&#23454;&#22686;&#24378;&#65289;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#22823;&#27169;&#22411;&#25928;&#26524;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#29983;&#25104;&#21453;&#20107;&#23454;&#25968;&#25454;&#65292;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#30340;&#22240;&#26524;&#25512;&#29702;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models trained with counterfactually augmented data learn representations of the causal structure of tasks, enabling robust generalization. However, high-quality counterfactual data is scarce for most tasks and not easily generated at scale. When crowdsourced, such data is typically limited in scale and diversity; when generated using supervised methods, it is computationally expensive to extend to new counterfactual dimensions. In this work, we introduce DISCO (DIStilled COunterfactual Data), a new method for automatically generating high quality counterfactual data at scale. DISCO engineers prompts to generate phrasal perturbations with a large general language model. Then, a task-specific teacher model filters these generations to distill high-quality counterfactual data. While task-agnostic, we apply our pipeline to the task of natural language inference (NLI) and find that on challenging evaluations such as the NLI stress test, comparatively smaller student models trained with DIS
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#20351;&#29992;&#26080;&#25928;&#25512;&#29702;&#27493;&#39588;&#30340;&#24773;&#20917;&#19979;&#65292;CoT&#25552;&#31034;&#20063;&#21487;&#20197;&#23454;&#29616;CoT&#25512;&#29702;&#65292;&#24182;&#26174;&#31034;&#20986;&#20854;&#20182;&#26041;&#38754;&#23545;&#20110;CoT&#30340;&#26377;&#25928;&#24615;&#26356;&#20026;&#20851;&#38190;&#65292;&#36825;&#26377;&#21161;&#20110;&#28145;&#20837;&#20102;&#35299;CoT&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#24182;&#25552;&#20986;&#25913;&#36827;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2212.10001</link><description>&lt;p&gt;
&#25506;&#32034;&#8220;Chain-of-Thought&#8221;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#65306;&#19968;&#20010;&#20851;&#20110;&#37325;&#28857;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters. (arXiv:2212.10001v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#20351;&#29992;&#26080;&#25928;&#25512;&#29702;&#27493;&#39588;&#30340;&#24773;&#20917;&#19979;&#65292;CoT&#25552;&#31034;&#20063;&#21487;&#20197;&#23454;&#29616;CoT&#25512;&#29702;&#65292;&#24182;&#26174;&#31034;&#20986;&#20854;&#20182;&#26041;&#38754;&#23545;&#20110;CoT&#30340;&#26377;&#25928;&#24615;&#26356;&#20026;&#20851;&#38190;&#65292;&#36825;&#26377;&#21161;&#20110;&#28145;&#20837;&#20102;&#35299;CoT&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#24182;&#25552;&#20986;&#25913;&#36827;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;Chain-of-Thought&#8221;&#65288;CoT&#65289;&#25552;&#31034;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;CoT&#36890;&#36807;&#22312;&#28436;&#31034;&#20013;&#25552;&#20379;&#19968;&#31995;&#21015;&#30340;&#25512;&#29702;&#27493;&#39588;&#65292;&#26126;&#30830;&#40723;&#21169;LLM&#29983;&#25104;&#35299;&#20915;&#38382;&#39064;&#30340;&#20013;&#38388;&#29702;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#12290;&#23613;&#31649;CoT&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30446;&#21069;&#20173;&#24456;&#23569;&#20102;&#35299;&#20160;&#20040;&#20351;CoT&#25552;&#31034;&#26377;&#25928;&#65292;&#20197;&#21450;&#28436;&#31034;&#30340;&#25512;&#29702;&#27493;&#39588;&#30340;&#21738;&#20123;&#26041;&#38754;&#23545;&#20854;&#24615;&#33021;&#36215;&#21040;&#36129;&#29486;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#20351;&#29992;&#26080;&#25928;&#25512;&#29702;&#27493;&#39588;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#31034;&#20063;&#21487;&#20197;&#23454;&#29616;CoT&#25512;&#29702;&#65292;&#32780;&#20351;&#29992;&#21508;&#31181;&#24230;&#37327;&#26041;&#27861;&#65292;&#25552;&#31034;&#30340;&#24615;&#33021;&#20063;&#21487;&#20197;&#36798;&#21040;&#20351;&#29992;CoT&#26102;&#30340;80-90&#65285;&#20197;&#19978;&#65292;&#21516;&#26102;&#22312;&#25512;&#29702;&#26399;&#38388;&#20173;&#20250;&#29983;&#25104;&#36830;&#36143;&#30340;&#25512;&#29702;&#38142;&#26465;&#12290;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#29702;&#24615;&#30340;&#20854;&#20182;&#26041;&#38754;&#65292;&#27604;&#22914;&#19982;&#26597;&#35810;&#30456;&#20851;&#21644;&#27491;&#30830;&#25490;&#24207;&#30340;&#25512;&#29702;&#27493;&#39588;&#65292;&#23545;&#20110;&#26377;&#25928;&#30340;CoT&#25512;&#29702;&#26356;&#20026;&#37325;&#35201;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#20123;&#21457;&#29616;&#28145;&#21270;&#20102;&#25105;&#20204;&#23545;CoT&#25552;&#31034;&#30340;&#29702;&#35299;&#65292;&#24182;&#20026;&#25552;&#39640;&#20854;&#25928;&#26524;&#25552;&#20986;&#20102;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought (CoT) prompting can dramatically improve the multi-step reasoning abilities of large language models (LLMs). CoT explicitly encourages the LLM to generate intermediate rationales for solving a problem, by providing a series of reasoning steps in the demonstrations. Despite its success, there is still little understanding of what makes CoT prompting effective and which aspects of the demonstrated reasoning steps contribute to its performance. In this paper, we show that CoT reasoning is possible even with invalid demonstrations - prompting with invalid reasoning steps can achieve over 80-90% of the performance obtained using CoT under various metrics, while still generating coherent lines of reasoning during inference. Further experiments show that other aspects of the rationales, such as being relevant to the query and correctly ordering the reasoning steps, are much more important for effective CoT reasoning. Overall, these findings both deepen our understanding of Co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2212.09597</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#20316;&#20026;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#37325;&#35201;&#33021;&#21147;&#65292;&#21487;&#20197;&#20026;&#21307;&#30103;&#35786;&#26029;&#12289;&#35848;&#21028;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#21518;&#31471;&#25903;&#25345;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30740;&#31350;&#25104;&#26524;&#30340;&#27604;&#36739;&#21644;&#24635;&#32467;&#65292;&#24182;&#25552;&#20379;&#20102;&#31995;&#32479;&#36164;&#28304;&#20197;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#36164;&#28304;&#21487;&#22312; https://github.com/zjunlp/Prompt4ReasoningPapers &#19978;&#33719;&#21462;&#65288;&#23450;&#26399;&#26356;&#26032;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#20256;&#36882;&#21040;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#24182;&#35777;&#26126;&#36825;&#26679;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#23567;&#22411;&#27169;&#22411;&#22312;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#31526;&#21495;&#25512;&#29702;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.08410</link><description>&lt;p&gt;
&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Teaching Small Language Models to Reason. (arXiv:2212.08410v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#20256;&#36882;&#21040;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#24182;&#35777;&#26126;&#36825;&#26679;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#23567;&#22411;&#27169;&#22411;&#22312;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#31526;&#21495;&#25512;&#29702;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24605;&#32500;&#38142;&#30340;&#21551;&#21457;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#19968;&#31995;&#21015;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25512;&#29702;&#33021;&#21147;&#20284;&#20046;&#20165;&#22312;&#25317;&#26377;&#36229;&#36807;1000&#20159;&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#20013;&#20986;&#29616;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#23558;&#36825;&#31181;&#25512;&#29702;&#33021;&#21147;&#20256;&#36882;&#21040;&#23567;&#20110;1000&#20159;&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#20013;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#36739;&#22823;&#30340;&#25945;&#24072;&#27169;&#22411;&#29983;&#25104;&#30340;&#24605;&#32500;&#38142;&#36755;&#20986;&#36827;&#34892;&#24494;&#35843;&#65292;&#23545;&#23398;&#29983;&#27169;&#22411;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#31526;&#21495;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;T5 XXL&#22312;GSM8K&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#20174;8.11%&#25552;&#39640;&#21040;21.99%&#65292;&#24403;&#23427;&#34987;PaLM-540B&#29983;&#25104;&#30340;&#24605;&#32500;&#38142;&#36827;&#34892;&#24494;&#35843;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets. However, these reasoning capabilities only appear to emerge in models with a size of over 100 billion parameters. In this paper, we explore the transfer of such reasoning capabilities to models with less than 100 billion parameters via knowledge distillation. Specifically, we finetune a student model on the chain of thought outputs generated by a larger teacher model. Our experiments show that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets. For example, the accuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% when finetuned on PaLM-540B generated chains of thought.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34394;&#25311;&#22522;&#20934;Super-CLEVR&#65292;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#20998;&#31163;&#35270;&#35273;&#38382;&#39064;&#24212;&#23545;&#39046;&#22495;&#36716;&#31227;&#20013;&#30340;&#21508;&#31181;&#22240;&#32032;&#65292;&#25506;&#31350;VQA&#26041;&#27861;&#30340;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;&#20316;&#32773;&#22312;&#22235;&#20010;VQA&#39046;&#22495;&#21464;&#37327;&#19978;&#27979;&#35797;&#20102;&#22235;&#31181;&#26041;&#27861;&#21644;&#19968;&#31181;&#26032;&#26041;&#27861;P-NSVQA&#65292;&#32467;&#26524;&#34920;&#26126;P-NSVQA&#27604;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#26356;&#22909;&#65292;Super-CLEVR&#26159;&#20998;&#26512;VQA&#39046;&#22495;&#27867;&#21270;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2212.00259</link><description>&lt;p&gt;
&#36229;&#32423;CLEVR&#65306;&#35786;&#26029;&#35270;&#35273;&#25512;&#29702;&#39046;&#22495;&#31283;&#20581;&#24615;&#30340;&#34394;&#25311;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Super-CLEVR: A Virtual Benchmark to Diagnose Domain Robustness in Visual Reasoning. (arXiv:2212.00259v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34394;&#25311;&#22522;&#20934;Super-CLEVR&#65292;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#20998;&#31163;&#35270;&#35273;&#38382;&#39064;&#24212;&#23545;&#39046;&#22495;&#36716;&#31227;&#20013;&#30340;&#21508;&#31181;&#22240;&#32032;&#65292;&#25506;&#31350;VQA&#26041;&#27861;&#30340;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;&#20316;&#32773;&#22312;&#22235;&#20010;VQA&#39046;&#22495;&#21464;&#37327;&#19978;&#27979;&#35797;&#20102;&#22235;&#31181;&#26041;&#27861;&#21644;&#19968;&#31181;&#26032;&#26041;&#27861;P-NSVQA&#65292;&#32467;&#26524;&#34920;&#26126;P-NSVQA&#27604;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#26356;&#22909;&#65292;Super-CLEVR&#26159;&#20998;&#26512;VQA&#39046;&#22495;&#27867;&#21270;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#27169;&#22411;&#32463;&#24120;&#22312;&#20998;&#24067;&#22806;&#25968;&#25454;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#24182;&#19988;&#22312;&#39046;&#22495;&#27867;&#21270;&#19978;&#36935;&#21040;&#22256;&#38590;&#12290;&#30001;&#20110;&#36825;&#20010;&#20219;&#21153;&#30340;&#22810;&#27169;&#24577;&#24615;&#36136;&#65292;&#22810;&#20010;&#21464;&#21270;&#22240;&#32032;&#30456;&#20114;&#20132;&#32455;&#65292;&#20351;&#24471;&#27867;&#21270;&#20998;&#26512;&#38590;&#20197;&#20998;&#26512;&#12290;&#36825;&#28608;&#21169;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#34394;&#25311;&#22522;&#20934;&#65292;&#36229;&#32423;CLEVR&#65292;&#20854;&#20013;&#21487;&#20197;&#20998;&#31163;VQA&#39046;&#22495;&#36716;&#31227;&#20013;&#30340;&#19981;&#21516;&#22240;&#32032;&#65292;&#20197;&#20415;&#33021;&#22815;&#23558;&#23427;&#20204;&#30340;&#25928;&#26524;&#29420;&#31435;&#22320;&#36827;&#34892;&#30740;&#31350;&#12290;&#32771;&#34385;&#20102;&#22235;&#20010;&#22240;&#32032;&#65306;&#35270;&#35273;&#22797;&#26434;&#24230;&#12289;&#38382;&#39064;&#20887;&#20313;&#12289;&#27010;&#24565;&#20998;&#24067;&#21644;&#27010;&#24565;&#32452;&#25104;&#24615;&#12290;&#36890;&#36807;&#21487;&#25511;&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#36229;&#32423;CLEVR&#20351;&#25105;&#20204;&#33021;&#22815;&#27979;&#35797;VQA&#26041;&#27861;&#65292;&#22312;&#20854;&#20013;&#27979;&#35797;&#25968;&#25454;&#27839;&#30528;&#27599;&#20010;&#36724;&#19982;&#35757;&#32451;&#25968;&#25454;&#26377;&#25152;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22235;&#31181;&#29616;&#26377;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;NSCL&#21644;NSVQA&#65292;&#20197;&#21450;&#20004;&#31181;&#38750;&#31526;&#21495;&#26041;&#27861;FiLM&#21644;mDETR&#65307;&#20197;&#21450;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#27010;&#29575;NSVQA&#65288;P-NSVQA&#65289;&#65292;&#23427;&#23558;&#19981;&#30830;&#23450;&#24615;&#25512;&#29702;&#19982;NSVQA&#25193;&#23637;&#12290;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#65292;P-NSVQA&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#65292;&#36229;&#32423;CLEVR&#21487;&#20197;&#25104;&#20026;&#20998;&#26512;VQA&#20013;&#39046;&#22495;&#27867;&#21270;&#30340;&#26377;&#29992;&#35786;&#26029;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Question Answering (VQA) models often perform poorly on out-of-distribution data and struggle on domain generalization. Due to the multi-modal nature of this task, multiple factors of variation are intertwined, making generalization difficult to analyze. This motivates us to introduce a virtual benchmark, Super-CLEVR, where different factors in VQA domain shifts can be isolated in order that their effects can be studied independently. Four factors are considered: visual complexity, question redundancy, concept distribution and concept compositionality. With controllably generated data, Super-CLEVR enables us to test VQA methods in situations where the test data differs from the training data along each of these axes. We study four existing methods, including two neural symbolic methods NSCL and NSVQA, and two non-symbolic methods FiLM and mDETR; and our proposed method, probabilistic NSVQA (P-NSVQA), which extends NSVQA with uncertainty reasoning. P-NSVQA outperforms other metho
&lt;/p&gt;</description></item><item><title>&#22312;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#20013;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#22122;&#22768;&#12289;&#33258;&#28982;&#21457;&#29983;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#21644;&#21327;&#21464;&#37327;&#28418;&#31227;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#33391;&#27169;&#24335;&#34987;&#38169;&#35823;&#22320;&#36171;&#20104;&#39640;&#22870;&#21169;&#20540;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#22870;&#21169;&#21338;&#24328;&#65292;&#38656;&#35201;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2211.08714</link><description>&lt;p&gt;
&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#22870;&#21169;&#21338;&#24328;
&lt;/p&gt;
&lt;p&gt;
Reward Gaming in Conditional Text Generation. (arXiv:2211.08714v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08714
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#20013;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#22122;&#22768;&#12289;&#33258;&#28982;&#21457;&#29983;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#21644;&#21327;&#21464;&#37327;&#28418;&#31227;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#33391;&#27169;&#24335;&#34987;&#38169;&#35823;&#22320;&#36171;&#20104;&#39640;&#22870;&#21169;&#20540;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#22870;&#21169;&#21338;&#24328;&#65292;&#38656;&#35201;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#19982;&#25152;&#38656;&#34892;&#20026;&#30456;&#19968;&#33268;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#28857;&#22312;&#20110;&#20351;&#29992;&#20174;&#20154;&#31867;&#27880;&#37322;&#20013;&#23398;&#20064;&#30340;&#22870;&#21169;&#20989;&#25968;&#20197;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19977;&#31181;&#24120;&#35265;&#24773;&#20917;&#65292;&#21363;&#30001;&#22122;&#22768;&#24341;&#36215;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12289;&#33258;&#28982;&#21457;&#29983;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#21644;&#21327;&#21464;&#37327;&#28418;&#31227;&#65292;&#20854;&#20013;&#39640;&#22870;&#21169;&#34987;&#38169;&#35823;&#22320;&#20998;&#37197;&#32473;&#19981;&#33391;&#27169;&#24335;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#21363;&#20351;&#23398;&#20064;&#21040;&#30340;&#24230;&#37327;&#22312;&#35757;&#32451;&#22870;&#21169;&#20989;&#25968;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#20998;&#24067;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#19981;&#33391;&#27169;&#24335;&#22312;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#30340;RL&#35757;&#32451;&#36807;&#31243;&#20013;&#20173;&#26377;&#21487;&#33021;&#34987;&#25918;&#22823;&#12290;&#23613;&#31649;RL&#25110;&#23433;&#20840;&#31038;&#21306;&#24050;&#32463;&#24320;&#22987;&#35752;&#35770;&#22870;&#21169;&#21338;&#24328;&#65292;&#20294;&#22312;&#36825;&#31687;&#35752;&#35770;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#20351;&#29992;&#20855;&#20307;&#30340;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#31034;&#20363;&#65292;&#37325;&#28857;&#20171;&#32461;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#31038;&#21306;&#20013;&#30340;&#22870;&#21169;&#21338;&#24328;&#65292;&#24182;&#35752;&#35770;&#21487;&#33021;&#30340;&#20462;&#22797;&#25514;&#26045;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
To align conditional text generation model outputs with desired behaviors, there has been an increasing focus on training the model using reinforcement learning (RL) with reward functions learned from human annotations. Under this framework, we identify three common cases where high rewards are incorrectly assigned to undesirable patterns: noise-induced spurious correlation, naturally occurring spurious correlation, and covariate shift. We show that even though learned metrics achieve high performance on the distribution of the data used to train the reward function, the undesirable patterns may be amplified during RL training of the text generation model. While there has been discussion about reward gaming in the RL or safety community, in this discussion piece, we would like to highlight reward gaming in the natural language generation (NLG) community using concrete conditional text generation examples and discuss potential fixes and areas for future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#34920;&#26126;&#20102;&#31163;&#32447;&#25351;&#26631;&#19982;&#20154;&#24037;&#36830;&#32493;&#35780;&#20998;&#20043;&#38388;&#23384;&#22312;&#33391;&#22909;&#30340;&#30456;&#20851;&#24615;&#65292;&#21487;&#20197;&#22312;&#21516;&#26102;&#32763;&#35793;&#27169;&#24335;&#20013;&#21487;&#38752;&#22320;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#30340;&#35780;&#20272;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#22823;&#35268;&#27169;&#20154;&#24037;&#35780;&#20272;&#30340;&#38656;&#35201;&#12290;</title><link>http://arxiv.org/abs/2211.08633</link><description>&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#30340;&#34913;&#37327;&#25351;&#26631;&#19982;&#20154;&#24037;&#35780;&#20998;&#30456;&#31526;&#65292;&#22312;&#21516;&#26102;&#32763;&#35793;&#20013;&#20063;&#36866;&#29992;
&lt;/p&gt;
&lt;p&gt;
MT Metrics Correlate with Human Ratings of Simultaneous Speech Translation. (arXiv:2211.08633v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#20102;&#31163;&#32447;&#25351;&#26631;&#19982;&#20154;&#24037;&#36830;&#32493;&#35780;&#20998;&#20043;&#38388;&#23384;&#22312;&#33391;&#22909;&#30340;&#30456;&#20851;&#24615;&#65292;&#21487;&#20197;&#22312;&#21516;&#26102;&#32763;&#35793;&#27169;&#24335;&#20013;&#21487;&#38752;&#22320;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#30340;&#35780;&#20272;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#22823;&#35268;&#27169;&#20154;&#24037;&#35780;&#20272;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#35780;&#20998;&#21644;&#31163;&#32447;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#25351;&#26631;&#65288;&#22914;BLEU&#12289;chrF2&#12289;BertScore&#21644;COMET&#65289;&#20043;&#38388;&#26377;&#20960;&#20010;&#20803;&#35780;&#20272;&#30740;&#31350;&#12290;&#36825;&#20123;&#25351;&#26631;&#24050;&#32463;&#29992;&#20110;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#65288;SST&#65289;&#30340;&#35780;&#20272;&#65292;&#20294;&#23427;&#20204;&#19982;&#26368;&#36817;&#25910;&#38598;&#30340;&#36830;&#32493;&#35780;&#20998;&#65288;CR&#65289;&#30340;SST&#30340;&#20154;&#24037;&#35780;&#20998;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#25552;&#20132;&#32473;IWSLT 2022&#24180;&#33521;&#24503;SST&#20219;&#21153;&#30340;&#20505;&#36873;&#31995;&#32479;&#30340;&#35780;&#20272;&#65292;&#24182;&#23545;CR&#21644;&#19978;&#36848;&#25351;&#26631;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30456;&#20851;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31163;&#32447;&#25351;&#26631;&#19982;CR&#23384;&#22312;&#33391;&#22909;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#21487;&#20197;&#21487;&#38752;&#22320;&#29992;&#20110;&#35780;&#20272;&#21516;&#26102;&#32763;&#35793;&#27169;&#24335;&#19979;&#30340;&#26426;&#22120;&#32763;&#35793;&#65292;&#20294;&#23545;&#27979;&#35797;&#38598;&#22823;&#23567;&#26377;&#19968;&#23450;&#38480;&#21046;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#31216;&#65292;&#37492;&#20110;&#24403;&#21069;SST&#30340;&#36136;&#37327;&#27700;&#24179;&#65292;&#36825;&#20123;&#25351;&#26631;&#21487;&#20197;&#29992;&#20316;CR&#30340;&#20195;&#29702;&#65292;&#20943;&#36731;&#20102;&#22823;&#35268;&#27169;&#20154;&#24037;&#35780;&#20272;&#30340;&#38656;&#35201;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#35768;&#22810;&#25351;&#26631;&#30340;&#30456;&#20851;&#24615;&#22312;&#19981;&#21516;&#30340;&#27979;&#35797;&#38598;&#19979;&#21457;&#29983;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
There have been several meta-evaluation studies on the correlation between human ratings and offline machine translation (MT) evaluation metrics such as BLEU, chrF2, BertScore and COMET. These metrics have been used to evaluate simultaneous speech translation (SST) but their correlations with human ratings of SST, which has been recently collected as Continuous Ratings (CR), are unclear. In this paper, we leverage the evaluations of candidate systems submitted to the English-German SST task at IWSLT 2022 and conduct an extensive correlation analysis of CR and the aforementioned metrics. Our study reveals that the offline metrics are well correlated with CR and can be reliably used for evaluating machine translation in simultaneous mode, with some limitations on the test set size. We conclude that given the current quality levels of SST, these metrics can be used as proxies for CR, alleviating the need for large scale human evaluation. Additionally, we observe that correlations of the m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#36947;&#24503;&#20559;&#35265;&#65292;&#33521;&#35821;&#26041;&#38754;&#30340;&#20559;&#35265;&#27604;&#20854;&#20182;&#35821;&#35328;&#26356;&#24378;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#33521;&#35821;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#26356;&#20542;&#21521;&#20110;&#20248;&#20808;&#32771;&#34385;&#20010;&#20307;&#21270;&#30340;&#36947;&#24503;&#22522;&#30784;&#32780;&#38750;&#26463;&#32538;&#24615;&#30340;&#36947;&#24503;&#22522;&#30784;&#65292;&#36825;&#23545;&#36328;&#35821;&#35328;&#36716;&#31227;&#21644;&#35774;&#35745;&#20262;&#29702;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#20855;&#26377;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2211.07733</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#36947;&#24503;&#20559;&#35265;&#20135;&#29983;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Speaking Multiple Languages Affects the Moral Bias of Language Models. (arXiv:2211.07733v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#36947;&#24503;&#20559;&#35265;&#65292;&#33521;&#35821;&#26041;&#38754;&#30340;&#20559;&#35265;&#27604;&#20854;&#20182;&#35821;&#35328;&#26356;&#24378;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#33521;&#35821;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#26356;&#20542;&#21521;&#20110;&#20248;&#20808;&#32771;&#34385;&#20010;&#20307;&#21270;&#30340;&#36947;&#24503;&#22522;&#30784;&#32780;&#38750;&#26463;&#32538;&#24615;&#30340;&#36947;&#24503;&#22522;&#30784;&#65292;&#36825;&#23545;&#36328;&#35821;&#35328;&#36716;&#31227;&#21644;&#35774;&#35745;&#20262;&#29702;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#20855;&#26377;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#24191;&#27867;&#24212;&#29992;&#20110;&#22788;&#29702;&#22810;&#31181;&#35821;&#35328;&#30340;&#25968;&#25454;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#27599;&#31181;&#35821;&#35328;&#30340;&#25968;&#25454;&#35757;&#32451;&#37327;&#19981;&#21516;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#33521;&#35821;&#26041;&#38754;&#30340;&#34920;&#29616;&#24448;&#24448;&#35201;&#27604;&#20854;&#20182;&#35821;&#35328;&#22909;&#24471;&#22810;&#12290;&#25105;&#20204;&#30740;&#31350;&#36825;&#26159;&#21542;&#24433;&#21709;&#20102;&#36825;&#20123;&#27169;&#22411;&#23545;&#36947;&#24503;&#20934;&#21017;&#30340;&#25429;&#25417;&#21644;&#36816;&#29992;&#12290;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#20174;&#33521;&#35821;&#20013;&#25429;&#25417;&#21040;&#20102;&#36947;&#24503;&#20934;&#21017;&#24182;&#23558;&#20854;&#26045;&#21152;&#21040;&#20854;&#20182;&#35821;&#35328;&#20013;&#65311;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#22312;&#26576;&#20123;&#35821;&#35328;&#20013;&#34920;&#29616;&#20986;&#38543;&#24847;&#30340;&#12289;&#28508;&#22312;&#26377;&#23475;&#30340;&#20449;&#24565;&#65311;&#36825;&#20004;&#20010;&#38382;&#39064;&#37117;&#21487;&#33021;&#23545;&#36328;&#35821;&#35328;&#36716;&#31227;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#33021;&#23548;&#33268;&#26377;&#23475;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36816;&#29992;&#8220;&#36947;&#24503;&#26041;&#21521;&#8221;&#26694;&#26550;&#27604;&#36739;&#20102;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;&#24503;&#35821;&#12289;&#25463;&#20811;&#35821;&#12289;&#38463;&#25289;&#20271;&#35821;&#12289;&#27721;&#35821;&#21644;&#33521;&#35821;&#20013;&#30340;&#32467;&#26524;&#65292;&#20998;&#26512;&#20102;&#27169;&#22411;&#22312;&#36807;&#28388;&#21518;&#30340;&#24179;&#34892;&#23383;&#24149;&#35821;&#26009;&#24211;&#19978;&#30340;&#34892;&#20026;&#65292;&#24182;&#23558;&#27169;&#22411;&#24212;&#29992;&#20110;&#36947;&#24503;&#22522;&#30784;&#38382;&#21367;&#65292;&#19982;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#30340;&#20154;&#31867;&#22238;&#31572;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#36947;&#24503;&#20559;&#35265;&#65292;&#19988;&#33521;&#35821;&#26041;&#38754;&#30340;&#20559;&#35265;&#27604;&#20854;&#20182;&#35821;&#35328;&#26356;&#24378;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#33521;&#35821;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#26356;&#20542;&#21521;&#20110;&#20248;&#20808;&#32771;&#34385;&#20010;&#20307;&#21270;&#30340;&#36947;&#24503;&#22522;&#30784;&#32780;&#38750;&#26463;&#32538;&#24615;&#30340;&#36947;&#24503;&#22522;&#30784;&#12290;&#36825;&#23545;&#36328;&#35821;&#35328;&#36716;&#31227;&#21644;&#35774;&#35745;&#20262;&#29702;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#20855;&#26377;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained multilingual language models (PMLMs) are commonly used when dealing with data from multiple languages and cross-lingual transfer. However, PMLMs are trained on varying amounts of data for each language. In practice this means their performance is often much better on English than many other languages. We explore to what extent this also applies to moral norms. Do the models capture moral norms from English and impose them on other languages? Do the models exhibit random and thus potentially harmful beliefs in certain languages? Both these issues could negatively impact cross-lingual transfer and potentially lead to harmful outcomes. In this paper, we (1) apply the MoralDirection framework to multilingual models, comparing results in German, Czech, Arabic, Chinese, and English, (2) analyse model behaviour on filtered parallel subtitles corpora, and (3) apply the models to a Moral Foundations Questionnaire, comparing with human responses from different countries. Our experime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#35821;&#27861;&#30340;&#34920;&#36848;&#30340;&#19968;&#33268;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#36890;&#36807;&#22810;&#39033;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#20013; emergent &#35821;&#35328;&#32467;&#26500;&#26159;&#33030;&#24369;&#30340;&#12290;</title><link>http://arxiv.org/abs/2210.17406</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340; emergent &#35821;&#35328;&#32467;&#26500;&#26159;&#33030;&#24369;&#30340;
&lt;/p&gt;
&lt;p&gt;
Emergent Linguistic Structures in Neural Networks are Fragile. (arXiv:2210.17406v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#35821;&#27861;&#30340;&#34920;&#36848;&#30340;&#19968;&#33268;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#36890;&#36807;&#22810;&#39033;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#20013; emergent &#35821;&#35328;&#32467;&#26500;&#26159;&#33030;&#24369;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#24378;&#21170;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#24230;&#31561;&#24615;&#33021;&#25351;&#26631;&#24182;&#19981;&#33021;&#34913;&#37327;&#27169;&#22411;&#22312;&#20195;&#34920;&#22797;&#26434;&#35821;&#35328;&#32467;&#26500;&#26041;&#38754;&#30340;&#36136;&#37327;&#12290;&#26412;&#25991;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#20195;&#34920;&#35821;&#27861;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#35821;&#35328;&#34920;&#36848;&#30340;&#19968;&#33268;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#26694;&#26550;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20123;&#31283;&#20581;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24230;&#37327;&#26041;&#24335;&#65292;&#36825;&#20123;&#24230;&#37327;&#26041;&#24335;&#21033;&#29992;&#26368;&#36817;&#22312;&#36890;&#36807;&#25506;&#27979;&#20219;&#21153;&#20174;LLM&#20013;&#25552;&#21462;&#35821;&#35328;&#32467;&#26500;&#30340;&#20808;&#36827;&#25216;&#26415;&#65292;&#21363;&#29992;&#20110;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#20449;&#24687;&#30340;&#31616;&#21333;&#20219;&#21153;&#65292;&#22914;&#35821;&#27861;&#37325;&#26500;&#21644;&#26681;&#35782;&#21035;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#22235;&#31181;LLM&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#35821;&#26009;&#24211;&#19978;&#23545;&#35821;&#27861;&#20445;&#25345;&#25200;&#21160;&#30340;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#26469;&#30740;&#31350;&#25152;&#25552;&#20986;&#30340;&#31283;&#20581;&#24230;&#37327;&#26041;&#24335;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have been reported to have strong performance on natural language processing tasks. However, performance metrics such as accuracy do not measure the quality of the model in terms of its ability to robustly represent complex linguistic structure. In this paper, focusing on the ability of language models to represent syntax, we propose a framework to assess the consistency and robustness of linguistic representations. To this end, we introduce measures of robustness of neural network models that leverage recent advances in extracting linguistic constructs from LLMs via probing tasks, i.e., simple tasks used to extract meaningful information about a single facet of a language model, such as syntax reconstruction and root identification. Empirically, we study the performance of four LLMs across six different corpora on the proposed robustness measures by analysing their performance and robustness with respect to syntax-preserving perturbations. We provide evide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#26415;&#37319;&#26679;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21487;&#20860;&#23481;&#24120;&#35265;&#30340;&#37319;&#26679;&#21464;&#21270;&#65292;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#26463;&#22810;&#26679;&#24615;&#21644;&#20196;&#20154;&#23604;&#23596;&#30340;&#24182;&#34892;&#24615;&#65292;&#20174;&#21407;&#22987;&#27169;&#22411;&#25552;&#20379;&#26080;&#20559;&#21644;&#19968;&#33268;&#30340;&#26399;&#26395;&#12290;&#22312;WMT&#26426;&#22120;&#32763;&#35793;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.15458</link><description>&lt;p&gt;
&#31639;&#26415;&#37319;&#26679;&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24182;&#34892;&#22810;&#26679;&#21270;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Arithmetic Sampling: Parallel Diverse Decoding for Large Language Models. (arXiv:2210.15458v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#26415;&#37319;&#26679;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21487;&#20860;&#23481;&#24120;&#35265;&#30340;&#37319;&#26679;&#21464;&#21270;&#65292;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#26463;&#22810;&#26679;&#24615;&#21644;&#20196;&#20154;&#23604;&#23596;&#30340;&#24182;&#34892;&#24615;&#65292;&#20174;&#21407;&#22987;&#27169;&#22411;&#25552;&#20379;&#26080;&#20559;&#21644;&#19968;&#33268;&#30340;&#26399;&#26395;&#12290;&#22312;WMT&#26426;&#22120;&#32763;&#35793;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#26041;&#27861;&#36890;&#24120;&#22312;&#36755;&#20986;&#22810;&#26679;&#24615;&#21644;&#35745;&#31639;&#24182;&#34892;&#24615;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#26681;&#25454;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38544;&#24335;&#23450;&#20041;&#30340;&#31639;&#26415;&#20195;&#30721;&#20070;&#36827;&#34892;&#37319;&#26679;&#65292;&#20860;&#23481;&#24120;&#35265;&#30340;&#37319;&#26679;&#21464;&#21270;&#65292;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#19979;&#30340;&#21487;&#35777;&#26126;&#30340;&#26463;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#20855;&#26377;&#20196;&#20154;&#23604;&#23596;&#30340;&#24182;&#34892;&#24615;&#65292;&#24182;&#20174;&#21407;&#22987;&#27169;&#22411;&#25552;&#20379;&#26080;&#20559;&#21644;&#19968;&#33268;&#30340;&#26399;&#26395;&#12290;&#25105;&#20204;&#22312;WMT&#26426;&#22120;&#32763;&#35793;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23558;&#39044;&#26399;&#30340;BLEU&#20998;&#25968;&#22870;&#21169;&#30340;&#26631;&#20934;&#24046;&#20943;&#23569;&#20102;&#19968;&#21322;&#20197;&#19978;&#65292;&#21516;&#26102;&#19982;&#20808;&#21069;&#30340;&#26368;&#26032;&#26041;&#27861;&#26377;&#20102;&#30456;&#24403;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoding methods for large language models often trade-off between diversity of outputs and parallelism of computation. Methods such as beam search and Gumbel top-k sampling can guarantee a different output for each element of the beam, but are not easy to parallelize. Alternatively, methods such as temperature sampling and its modifications (top-k sampling, nucleus sampling, typical decoding, and others), are embarrassingly parallel, but have no guarantees about duplicate samples. We present a framework for sampling according to an arithmetic code book implicitly defined by a large language model, compatible with common sampling variations, with provable beam diversity under certain conditions, as well as being embarrassingly parallel and providing unbiased and consistent expectations from the original model. We demonstrate the effectiveness of our approach on WMT machine translation, more than halving the standard deviation when estimating expected BLEU score reward, and closing the 
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HighGEN&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#30701;&#35821;&#23884;&#20837;&#25628;&#32034;&#26041;&#27861;&#29983;&#25104;&#23454;&#20307;&#20016;&#23500;&#30340;&#20266;&#23383;&#20856;&#65292;&#22312;&#20351;&#29992;&#23884;&#20837;&#36317;&#31163;&#39564;&#35777;&#36807;&#31243;&#20943;&#23569;&#35823;&#25253;&#30340;&#22522;&#30784;&#19978;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;NER&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2210.07586</link><description>&lt;p&gt;
&#20351;&#29992;&#30701;&#35821;&#34920;&#31034;&#26597;&#35810;&#33258;&#21160;&#29983;&#25104;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Automatic Creation of Named Entity Recognition Datasets by Querying Phrase Representations. (arXiv:2210.07586v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HighGEN&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#30701;&#35821;&#23884;&#20837;&#25628;&#32034;&#26041;&#27861;&#29983;&#25104;&#23454;&#20307;&#20016;&#23500;&#30340;&#20266;&#23383;&#20856;&#65292;&#22312;&#20351;&#29992;&#23884;&#20837;&#36317;&#31163;&#39564;&#35777;&#36807;&#31243;&#20943;&#23569;&#35823;&#25253;&#30340;&#22522;&#30784;&#19978;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;NER&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24369;&#30417;&#30563;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#20381;&#36182;&#20110;&#30001;&#19987;&#23478;&#25552;&#20379;&#30340;&#39046;&#22495;&#29305;&#23450;&#35789;&#20856;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#27809;&#26377;&#23383;&#20856;&#30340;&#39046;&#22495;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#21487;&#34892;&#12290;&#22312;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;&#30701;&#35821;&#26816;&#32034;&#27169;&#22411;&#33258;&#21160;&#20174;&#32500;&#22522;&#30334;&#31185;&#20013;&#25552;&#21462;&#23454;&#20307;&#26500;&#24314;&#20102;&#20266;&#23383;&#20856;&#65292;&#20294;&#36825;&#20123;&#23383;&#20856;&#30340;&#35206;&#30422;&#38754;&#24448;&#24448;&#26377;&#38480;&#65292;&#22240;&#20026;&#26816;&#32034;&#22120;&#24456;&#21487;&#33021;&#20250;&#26816;&#32034;&#21040;&#27969;&#34892;&#30340;&#23454;&#20307;&#32780;&#19981;&#26159;&#32597;&#35265;&#30340;&#23454;&#20307;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;HighGEN&#65292;&#23427;&#20351;&#29992;&#20855;&#26377;&#39640;&#35206;&#30422;&#29575;&#30340;&#20266;&#23383;&#20856;&#29983;&#25104;NER&#25968;&#25454;&#38598;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#25628;&#32034;&#26041;&#27861;&#8212;&#8212;&#30701;&#35821;&#23884;&#20837;&#25628;&#32034;&#26469;&#21019;&#24314;&#23500;&#23454;&#20307;&#23383;&#20856;&#65292;&#35813;&#26041;&#27861;&#40723;&#21169;&#26816;&#32034;&#22120;&#22312;&#19968;&#20010;&#23494;&#38598;&#30340;&#21508;&#31181;&#23454;&#20307;&#30340;&#31354;&#38388;&#20013;&#25628;&#32034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#23454;&#20307;&#25552;&#21450;&#21644;&#23454;&#20307;&#31867;&#22411;&#20043;&#38388;&#23884;&#20837;&#36317;&#31163;&#30340;&#26032;&#30340;&#39564;&#35777;&#36807;&#31243;&#65292;&#20197;&#20943;&#23569;&#39640;&#35206;&#30422;&#29575;&#20266;&#26631;&#31614;&#20013;&#30340;&#35823;&#25253;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most weakly supervised named entity recognition (NER) models rely on domain-specific dictionaries provided by experts. This approach is infeasible in many domains where dictionaries do not exist. While a phrase retrieval model was used to construct pseudo-dictionaries with entities retrieved from Wikipedia automatically in a recent study, these dictionaries often have limited coverage because the retriever is likely to retrieve popular entities rather than rare ones. In this study, we present a novel framework, HighGEN, that generates NER datasets with high-coverage pseudo-dictionaries. Specifically, we create entity-rich dictionaries with a novel search method, called phrase embedding search, which encourages the retriever to search a space densely populated with various entities. In addition, we use a new verification process based on the embedding distance between candidate entity mentions and entity types to reduce the false-positive noise in weak labels generated by high-coverage 
&lt;/p&gt;</description></item><item><title>SQuId&#26159;&#19968;&#20010;&#20351;&#29992;&#19968;&#30334;&#19975;&#20010;&#35780;&#20998;&#36827;&#34892;&#35757;&#32451;&#24182;&#22312;65&#20010;&#35821;&#35328;&#29615;&#22659;&#19979;&#36827;&#34892;&#27979;&#35797;&#30340;&#22810;&#35821;&#35328;&#33258;&#28982;&#24230;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#30340;&#35757;&#32451;&#22312;&#35768;&#22810;&#29615;&#22659;&#20013;&#22987;&#32456;&#20248;&#20110;&#21333;&#19968;&#29615;&#22659;&#30340;&#22522;&#32447;&#65292;&#24182;&#23637;&#29616;&#20102;&#27604;&#31454;&#20105;&#22522;&#32447;&#26356;&#20986;&#33394;&#30340;&#34920;&#29616;;&#30740;&#31350;&#36824;&#31361;&#20986;&#20102;&#36328;&#35821;&#35328;&#32454;&#35843;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;&#38750;&#35821;&#35328;&#25928;&#26524;&#22914;&#22768;&#38899;&#30072;&#21464;&#22312;&#36328;&#35821;&#35328;&#32454;&#35843;&#20013;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2210.06324</link><description>&lt;p&gt;
SQuId: &#22312;&#22810;&#35821;&#35328;&#20013;&#27979;&#37327;&#35821;&#38899;&#33258;&#28982;&#24230;
&lt;/p&gt;
&lt;p&gt;
SQuId: Measuring Speech Naturalness in Many Languages. (arXiv:2210.06324v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06324
&lt;/p&gt;
&lt;p&gt;
SQuId&#26159;&#19968;&#20010;&#20351;&#29992;&#19968;&#30334;&#19975;&#20010;&#35780;&#20998;&#36827;&#34892;&#35757;&#32451;&#24182;&#22312;65&#20010;&#35821;&#35328;&#29615;&#22659;&#19979;&#36827;&#34892;&#27979;&#35797;&#30340;&#22810;&#35821;&#35328;&#33258;&#28982;&#24230;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#30340;&#35757;&#32451;&#22312;&#35768;&#22810;&#29615;&#22659;&#20013;&#22987;&#32456;&#20248;&#20110;&#21333;&#19968;&#29615;&#22659;&#30340;&#22522;&#32447;&#65292;&#24182;&#23637;&#29616;&#20102;&#27604;&#31454;&#20105;&#22522;&#32447;&#26356;&#20986;&#33394;&#30340;&#34920;&#29616;;&#30740;&#31350;&#36824;&#31361;&#20986;&#20102;&#36328;&#35821;&#35328;&#32454;&#35843;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;&#38750;&#35821;&#35328;&#25928;&#26524;&#22914;&#22768;&#38899;&#30072;&#21464;&#22312;&#36328;&#35821;&#35328;&#32454;&#35843;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#35821;&#38899;&#30340;&#35768;&#22810;&#30740;&#31350;&#37117;&#20381;&#36182;&#20110;&#20154;&#31867;&#35780;&#20272;&#65292;&#36825;&#20250;&#20135;&#29983;&#37325;&#22823;&#25104;&#26412;&#24182;&#20943;&#32531;&#24320;&#21457;&#36827;&#31243;&#12290;&#22312;&#37325;&#24230;&#22810;&#35821;&#35328;&#24212;&#29992;&#20013;&#65292;&#25307;&#21215;&#21644;&#35843;&#26597;&#35780;&#23457;&#21592;&#21487;&#33021;&#38656;&#35201;&#25968;&#21608;&#26102;&#38388;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SQuId&#65288;&#35821;&#38899;&#36136;&#37327;&#35782;&#21035;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#19968;&#30334;&#19975;&#20010;&#35780;&#20998;&#36827;&#34892;&#35757;&#32451;&#24182;&#22312;65&#20010;&#35821;&#35328;&#29615;&#22659;&#19979;&#36827;&#34892;&#27979;&#35797;&#30340;&#22810;&#35821;&#35328;&#33258;&#28982;&#24230;&#39044;&#27979;&#27169;&#22411;&#65292;&#26159;&#36804;&#20170;&#20026;&#27490;&#36825;&#31181;&#31867;&#22411;&#20013;&#26368;&#22823;&#30340;&#21162;&#21147;&#12290;&#20027;&#35201;&#30340;&#35265;&#35299;&#26159;&#65292;&#22312;&#35768;&#22810;&#29615;&#22659;&#20013;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#22987;&#32456;&#20248;&#20110;&#21333;&#19968;&#29615;&#22659;&#30340;&#22522;&#32447;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#20219;&#21153;&#12289;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#30340;&#34920;&#29616;&#20248;&#20110;&#22522;&#20110;w2v-BERT&#21644;VoiceMOS&#30340;&#31454;&#20105;&#22522;&#32447;50.0%&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#36328;&#35821;&#35328;&#32454;&#35843;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#23545;&#38646;&#26679;&#26412;&#29615;&#22659;&#30340;&#24433;&#21709;&#65292;&#21363;&#27809;&#26377;&#32454;&#35843;&#25968;&#25454;&#30340;&#29615;&#22659;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#20998;&#26512;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#38750;&#35821;&#35328;&#25928;&#26524;&#65288;&#22914;&#22768;&#38899;&#30072;&#21464;&#65289;&#22312;&#36328;&#35821;&#35328;&#32454;&#35843;&#20013;&#30340;&#20316;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#33021;&#26159;&#31532;&#19968;&#20010;&#22810;&#35821;&#35328;&#33258;&#28982;&#24230;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;65&#20010;&#29615;&#22659;&#20013;&#36229;&#36807;&#19968;&#30334;&#19975;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Much of text-to-speech research relies on human evaluation, which incurs heavy costs and slows down the development process. The problem is particularly acute in heavily multilingual applications, where recruiting and polling judges can take weeks. We introduce SQuId (Speech Quality Identification), a multilingual naturalness prediction model trained on over a million ratings and tested in 65 locales-the largest effort of this type to date. The main insight is that training one model on many locales consistently outperforms mono-locale baselines. We present our task, the model, and show that it outperforms a competitive baseline based on w2v-BERT and VoiceMOS by 50.0%. We then demonstrate the effectiveness of cross-locale transfer during fine-tuning and highlight its effect on zero-shot locales, i.e., locales for which there is no fine-tuning data. Through a series of analyses, we highlight the role of non-linguistic effects such as sound artifacts in cross-locale transfer. Finally, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#23383;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#26368;&#20339;&#25552;&#31034;&#30340;&#32452;&#25104;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;&#20154;&#31867;&#21453;&#39304;&#21644;&#36951;&#20256;&#31639;&#27861;&#36827;&#34892;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#20197;&#32654;&#23398;&#35282;&#24230;&#36827;&#34892;&#21576;&#29616;&#30340;&#19987;&#19994;&#20154;&#31867;&#33402;&#26415;&#20316;&#21697;&#30340;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2209.11711</link><description>&lt;p&gt;
&#25991;&#23383;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#26368;&#20339;&#25552;&#31034;&#21450;&#20854;&#22914;&#20309;&#25214;&#21040;
&lt;/p&gt;
&lt;p&gt;
Best Prompts for Text-to-Image Models and How to Find Them. (arXiv:2209.11711v3 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#23383;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#26368;&#20339;&#25552;&#31034;&#30340;&#32452;&#25104;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;&#20154;&#31867;&#21453;&#39304;&#21644;&#36951;&#20256;&#31639;&#27861;&#36827;&#34892;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#20197;&#32654;&#23398;&#35282;&#24230;&#36827;&#34892;&#21576;&#29616;&#30340;&#19987;&#19994;&#20154;&#31867;&#33402;&#26415;&#20316;&#21697;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#25991;&#26412;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20351;&#24471;&#20154;&#24037;&#33402;&#26415;&#23478;&#30340;&#20316;&#21697;&#24471;&#20197;&#20197;&#32654;&#23398;&#35282;&#24230;&#36827;&#34892;&#21576;&#29616;&#12290;&#28982;&#32780;&#65292;&#24517;&#39035;&#20180;&#32454;&#22320;&#32452;&#25104;&#25991;&#26412;&#25551;&#36848;&#65292;&#21363;&#25552;&#31034;&#65292;&#24182;&#29992;&#19968;&#32452;&#28548;&#28165;&#30340;&#20851;&#38190;&#35789;&#36827;&#34892;&#22686;&#24378;&#12290;&#30001;&#20110;&#32654;&#23398;&#22312;&#35745;&#31639;&#26041;&#38754;&#24456;&#38590;&#36827;&#34892;&#35780;&#20272;&#65292;&#22240;&#27492;&#38656;&#35201;&#20154;&#31867;&#21453;&#39304;&#26469;&#30830;&#23450;&#26368;&#20339;&#25552;&#31034;&#20844;&#24335;&#21644;&#20851;&#38190;&#35789;&#32452;&#21512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#22312;&#29615;&#30340;&#26041;&#27861;&#26469;&#36890;&#36807;&#36951;&#20256;&#31639;&#27861;&#23398;&#20064;&#26368;&#26377;&#29992;&#30340;&#25552;&#31034;&#20851;&#38190;&#35789;&#32452;&#21512;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#25913;&#21892;&#25551;&#32472;&#30456;&#21516;&#25551;&#36848;&#30340;&#22270;&#20687;&#30340;&#32654;&#23398;&#21560;&#24341;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in generative models, especially in text-guided diffusion models, has enabled the production of aesthetically-pleasing imagery resembling the works of professional human artists. However, one has to carefully compose the textual description, called the prompt, and augment it with a set of clarifying keywords. Since aesthetics are challenging to evaluate computationally, human feedback is needed to determine the optimal prompt formulation and keyword combination. In this paper, we present a human-in-the-loop approach to learning the most useful combination of prompt keywords using a genetic algorithm. We also show how such an approach can improve the aesthetic appeal of images depicting the same descriptions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27979;&#35797;&#20102;&#35821;&#35328;&#26292;&#38706;&#20551;&#35828;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20687;&#20154;&#31867;&#19968;&#26679;&#24863;&#30693;&#20070;&#38754;&#25991;&#26412;&#20013;&#35282;&#33394;&#30340;&#30693;&#35782;&#29366;&#24577;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35821;&#35328;&#27169;&#22411;&#34429;&#28982;&#36229;&#36807;&#20102;&#20598;&#28982;&#34892;&#20026;&#65292;&#20294;&#34920;&#29616;&#19981;&#22914;&#20154;&#31867;&#65292;&#26263;&#31034;&#30528;&#20165;&#38752;&#35821;&#35328;&#26292;&#38706;&#38590;&#20197;&#23436;&#20840;&#35299;&#37322;&#20154;&#31867;&#36825;&#31181;&#35748;&#30693;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2209.01515</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#30693;&#36947;&#21035;&#20154;&#30340;&#20449;&#20208;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Large Language Models know what humans know?. (arXiv:2209.01515v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27979;&#35797;&#20102;&#35821;&#35328;&#26292;&#38706;&#20551;&#35828;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20687;&#20154;&#31867;&#19968;&#26679;&#24863;&#30693;&#20070;&#38754;&#25991;&#26412;&#20013;&#35282;&#33394;&#30340;&#30693;&#35782;&#29366;&#24577;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35821;&#35328;&#27169;&#22411;&#34429;&#28982;&#36229;&#36807;&#20102;&#20598;&#28982;&#34892;&#20026;&#65292;&#20294;&#34920;&#29616;&#19981;&#22914;&#20154;&#31867;&#65292;&#26263;&#31034;&#30528;&#20165;&#38752;&#35821;&#35328;&#26292;&#38706;&#38590;&#20197;&#23436;&#20840;&#35299;&#37322;&#20154;&#31867;&#36825;&#31181;&#35748;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#33021;&#22815;&#20102;&#35299;&#20182;&#20154;&#30340;&#20449;&#20208;&#12290;&#28982;&#32780;&#65292;&#23578;&#19981;&#28165;&#26970;&#36825;&#31181;&#33021;&#21147;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26159;&#28304;&#20110;&#22825;&#29983;&#30340;&#29983;&#29289;&#31104;&#36171;&#65292;&#36824;&#26159;&#26469;&#28304;&#20110;&#20799;&#31461;&#21457;&#32946;&#36807;&#31243;&#20013;&#30340;&#32463;&#39564;&#31215;&#32047;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#25509;&#21463;&#25551;&#36848;&#20182;&#20154;&#24515;&#29702;&#29366;&#24577;&#30340;&#35821;&#35328;&#32780;&#33719;&#24471;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#26292;&#38706;&#20110;&#22823;&#37327;&#20154;&#31867;&#35821;&#35328;&#30340;&#27169;&#22411;&#26159;&#21542;&#26174;&#31034;&#23545;&#20070;&#38754;&#27573;&#33853;&#20013;&#35282;&#33394;&#26263;&#31034;&#30340;&#30693;&#35782;&#29366;&#24577;&#25935;&#24863;&#24615;&#26469;&#27979;&#35797;&#35821;&#35328;&#26292;&#38706;&#20551;&#35828;&#30340;&#21487;&#34892;&#24615;&#12290;&#22312;&#39044;&#27880;&#20876;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#21521;&#20154;&#31867;&#21442;&#19982;&#32773;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-3&#25552;&#20379;&#20102;&#35821;&#35328;&#29256;&#26412;&#30340;&#35823;&#20449;&#20219;&#21153;&#12290;&#20004;&#32773;&#37117;&#25935;&#24863;&#20110;&#20182;&#20154;&#30340;&#20449;&#20208;&#65292;&#32780;&#35821;&#35328;&#27169;&#22411;&#26174;&#33879;&#36229;&#36807;&#20102;&#20598;&#28982;&#34892;&#20026;&#65292;&#20294;&#23427;&#30340;&#34920;&#29616;&#19981;&#22914;&#20154;&#31867;&#65292;&#24182;&#19988;&#27809;&#26377;&#35299;&#37322;&#20182;&#20204;&#34892;&#20026;&#30340;&#20840;&#37096;&#33539;&#22260;--&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#25509;&#21463;&#20102;&#27604;&#19968;&#20010;&#20154;&#19968;&#29983;&#20013;&#25509;&#21463;&#30340;&#35821;&#35328;&#26356;&#22810;&#30340;&#35821;&#35328;&#12290;&#36825;&#34920;&#26126;&#65292;&#34429;&#28982;&#20174;&#35821;&#35328;&#26292;&#38706;&#20013;&#36827;&#34892;&#30340;&#32479;&#35745;&#23398;&#20064;&#21487;&#33021;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#35299;&#37322;&#20102;&#20154;&#31867;&#22914;&#20309;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#20294;&#23427;&#24182;&#19981;&#33021;&#23436;&#20840;&#35299;&#37322;&#20154;&#31867;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can attribute beliefs to others. However, it is unknown to what extent this ability results from an innate biological endowment or from experience accrued through child development, particularly exposure to language describing others' mental states. We test the viability of the language exposure hypothesis by assessing whether models exposed to large quantities of human language display sensitivity to the implied knowledge states of characters in written passages. In pre-registered analyses, we present a linguistic version of the False Belief Task to both human participants and a Large Language Model, GPT-3. Both are sensitive to others' beliefs, but while the language model significantly exceeds chance behavior, it does not perform as well as the humans, nor does it explain the full extent of their behavior -- despite being exposed to more language than a human would in a lifetime. This suggests that while statistical learning from language exposure may in part explain how huma
&lt;/p&gt;</description></item><item><title>Claim-Dissector&#26159;&#19968;&#27454;&#32852;&#21512;&#37325;&#25490;&#21644;&#30495;&#23454;&#24615;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#65292;&#21487;&#20197;&#35782;&#21035;&#19982;&#22768;&#26126;&#30456;&#20851;&#30340;&#35777;&#25454;&#65292;&#24182;&#30830;&#23450;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#12290;&#35813;&#31995;&#32479;&#30340;&#20010;&#20154;&#36129;&#29486;&#20197;&#21450;&#35777;&#25454;&#25152;&#25903;&#25345;&#25110;&#21453;&#39539;&#22768;&#26126;&#30340;&#36129;&#29486;&#37117;&#21487;&#20197;&#34987;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2207.14116</link><description>&lt;p&gt;
Claim-Dissector: &#19968;&#27454;&#32852;&#21512;&#37325;&#25490;&#21644;&#30495;&#23454;&#24615;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction. (arXiv:2207.14116v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14116
&lt;/p&gt;
&lt;p&gt;
Claim-Dissector&#26159;&#19968;&#27454;&#32852;&#21512;&#37325;&#25490;&#21644;&#30495;&#23454;&#24615;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#65292;&#21487;&#20197;&#35782;&#21035;&#19982;&#22768;&#26126;&#30456;&#20851;&#30340;&#35777;&#25454;&#65292;&#24182;&#30830;&#23450;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#12290;&#35813;&#31995;&#32479;&#30340;&#20010;&#20154;&#36129;&#29486;&#20197;&#21450;&#35777;&#25454;&#25152;&#25903;&#25345;&#25110;&#21453;&#39539;&#22768;&#26126;&#30340;&#36129;&#29486;&#37117;&#21487;&#20197;&#34987;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Claim-Dissector&#65292;&#19968;&#31181;&#38024;&#23545;&#20107;&#23454;&#26680;&#26597;&#21644;&#20998;&#26512;&#30340;&#26032;&#22411;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#32473;&#20986;&#19968;&#20010;&#22768;&#26126;&#21644;&#19968;&#32452;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#65292;&#32852;&#21512;&#23398;&#20064;&#35782;&#21035;&#65306;&#65288;i&#65289;&#19982;&#32473;&#23450;&#22768;&#26126;&#30456;&#20851;&#30340;&#35777;&#25454;&#65292;&#65288;ii&#65289;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#24314;&#35758;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#35299;&#24320;&#27599;&#20010;&#35777;&#25454;&#30340;&#30456;&#20851;&#24615;&#27010;&#29575;&#21450;&#20854;&#23545;&#26368;&#32456;&#30495;&#23454;&#24615;&#27010;&#29575;&#30340;&#24433;&#21709;-&#26368;&#32456;&#30495;&#23454;&#24615;&#27010;&#29575;&#19982;&#27599;&#20010;&#35777;&#25454;&#30456;&#20851;&#24615;&#27010;&#29575;&#30340;&#32447;&#24615;&#25972;&#21512;&#25104;&#27604;&#20363;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#27599;&#20010;&#35777;&#25454;&#23545;&#26368;&#32456;&#39044;&#27979;&#27010;&#29575;&#30340;&#20010;&#20154;&#36129;&#29486;&#12290;&#22312;&#27599;&#20010;&#35777;&#25454;&#30340;&#30456;&#20851;&#24615;&#27010;&#29575;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#21306;&#20998;&#27599;&#20010;&#30456;&#20851;&#35777;&#25454;&#26159;&#25903;&#25345;&#65288;S&#65289;&#36824;&#26159;&#21453;&#39539;&#65288;R&#65289;&#22768;&#26126;&#12290;&#36825;&#26679;&#21487;&#20197;&#37327;&#21270;S/R&#27010;&#29575;&#23545;&#26368;&#32456;&#32467;&#35770;&#30340;&#36129;&#29486;&#25110;&#26816;&#27979;&#26377;&#24322;&#35758;&#30340;&#35777;&#25454;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#31995;&#32479;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#22312;FEVER&#31454;&#36187;&#20013;&#65292;&#20854;&#32467;&#26524;&#19982;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Claim-Dissector: a novel latent variable model for fact-checking and analysis, which given a claim and a set of retrieved evidences jointly learns to identify: (i) the relevant evidences to the given claim, (ii) the veracity of the claim. We propose to disentangle the per-evidence relevance probability and its contribution to the final veracity probability in an interpretable way -- the final veracity probability is proportional to a linear ensemble of per-evidence relevance probabilities. In this way, the individual contributions of evidences towards the final predicted probability can be identified. In per-evidence relevance probability, our model can further distinguish whether each relevant evidence is supporting (S) or refuting (R) the claim. This allows to quantify how much the S/R probability contributes to the final verdict or to detect disagreeing evidence.  Despite its interpretable nature, our system achieves results competitive with state-of-the-art on the FEVER 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#32780;&#20840;&#30340;EAE&#26412;&#20307;&#35770;&#65292;105&#20010;&#20107;&#20214;&#21644;220&#20010;&#35770;&#20803;&#35282;&#33394;&#30340;&#21253;&#21547;&#22312;&#20869;&#65292;&#21033;&#29992;&#36825;&#20010;&#26412;&#20307;&#35770;&#21019;&#24314;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#30340;&#36890;&#29992;&#24615;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;GENEVA&#65292;&#20849;&#21253;&#21547;&#22235;&#20010;&#27979;&#35797;&#22871;&#20214;&#65292;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#22788;&#29702;&#26377;&#38480;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2205.12505</link><description>&lt;p&gt;
GENEVA&#65306;&#8220;&#36890;&#29992;&#24615;&#22522;&#20934;&#27979;&#35797;&#8221;&#20107;&#20214;&#35770;&#20803;&#25552;&#21462;&#65292;&#28085;&#30422;&#25968;&#30334;&#31181;&#20107;&#20214;&#31867;&#22411;&#21644;&#35770;&#20803;&#35282;&#33394;
&lt;/p&gt;
&lt;p&gt;
GENEVA: Benchmarking Generalizability for Event Argument Extraction with Hundreds of Event Types and Argument Roles. (arXiv:2205.12505v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#32780;&#20840;&#30340;EAE&#26412;&#20307;&#35770;&#65292;105&#20010;&#20107;&#20214;&#21644;220&#20010;&#35770;&#20803;&#35282;&#33394;&#30340;&#21253;&#21547;&#22312;&#20869;&#65292;&#21033;&#29992;&#36825;&#20010;&#26412;&#20307;&#35770;&#21019;&#24314;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#30340;&#36890;&#29992;&#24615;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;GENEVA&#65292;&#20849;&#21253;&#21547;&#22235;&#20010;&#27979;&#35797;&#22871;&#20214;&#65292;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#22788;&#29702;&#26377;&#38480;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20107;&#20214;&#35770;&#20803;&#25552;&#21462;&#65288;EAE&#65289;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#20197;&#36866;&#24212;&#26032;&#30340;&#20107;&#20214;&#31867;&#22411;&#21644;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#22914;ACE&#21644;ERE&#21482;&#28085;&#30422;&#19981;&#21040;40&#31181;&#20107;&#20214;&#31867;&#22411;&#21644;25&#31181;&#38754;&#21521;&#23454;&#20307;&#30340;&#35770;&#20803;&#35282;&#33394;&#12290;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#22810;&#26679;&#24615;&#21644;&#35206;&#30422;&#33539;&#22260;&#24433;&#21709;&#20102;&#36825;&#20123;&#25968;&#25454;&#38598;&#23545;EAE&#27169;&#22411;&#36890;&#29992;&#24615;&#30340;&#20805;&#20998;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#32780;&#20840;&#30340;EAE&#26412;&#20307;&#35770;&#65292;&#22312;FrameNet&#30340;&#22522;&#30784;&#19978;&#21019;&#24314;&#20102;&#21253;&#21547;115&#20010;&#20107;&#20214;&#21644;220&#20010;&#35770;&#20803;&#35282;&#33394;&#30340;&#26412;&#20307;&#35770;&#65292;&#20854;&#20013;&#35768;&#22810;&#35282;&#33394;&#19981;&#26159;&#23454;&#20307;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#26412;&#20307;&#35770;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;GENEVA&#65292;&#19968;&#31181;&#22810;&#26679;&#21270;&#30340;&#36890;&#29992;&#24615;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#22235;&#20010;&#27979;&#35797;&#22871;&#20214;&#65292;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#22788;&#29702;&#26377;&#38480;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works in Event Argument Extraction (EAE) have focused on improving model generalizability to cater to new events and domains. However, standard benchmarking datasets like ACE and ERE cover less than 40 event types and 25 entity-centric argument roles. Limited diversity and coverage hinder these datasets from adequately evaluating the generalizability of EAE models. In this paper, we first contribute by creating a large and diverse EAE ontology. This ontology is created by transforming FrameNet, a comprehensive semantic role labeling (SRL) dataset for EAE, by exploiting the similarity between these two tasks. Then, exhaustive human expert annotations are collected to build the ontology, concluding with 115 events and 220 argument roles, with a significant portion of roles not being entities. We utilize this ontology to further introduce GENEVA, a diverse generalizability benchmarking dataset comprising four test suites, aimed at evaluating models' ability to handle limited data a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AVDN&#30340;&#26080;&#20154;&#26426;&#23548;&#33322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#26469;&#25511;&#21046;&#26080;&#20154;&#26426;&#30340;&#23548;&#33322;&#65292;&#20026;&#35299;&#20915;&#26080;&#20154;&#26426;&#25511;&#21046;&#30340;&#20154;&#20307;&#36127;&#25285;&#12289;&#22810;&#20219;&#21153;&#25805;&#20316;&#12289;&#20197;&#21450;&#27531;&#30142;&#20154;&#21644;&#25163;&#37096;&#21344;&#29992;&#32773;&#25511;&#21046;&#26080;&#20154;&#26426;&#30340;&#38590;&#39064;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2205.12219</link><description>&lt;p&gt;
&#31354;&#20013;&#23545;&#35805;&#19982;&#23548;&#33322;&#65306;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#26080;&#20154;&#26426;&#23548;&#33322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Aerial Vision-and-Dialog Navigation. (arXiv:2205.12219v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AVDN&#30340;&#26080;&#20154;&#26426;&#23548;&#33322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#26469;&#25511;&#21046;&#26080;&#20154;&#26426;&#30340;&#23548;&#33322;&#65292;&#20026;&#35299;&#20915;&#26080;&#20154;&#26426;&#25511;&#21046;&#30340;&#20154;&#20307;&#36127;&#25285;&#12289;&#22810;&#20219;&#21153;&#25805;&#20316;&#12289;&#20197;&#21450;&#27531;&#30142;&#20154;&#21644;&#25163;&#37096;&#21344;&#29992;&#32773;&#25511;&#21046;&#26080;&#20154;&#26426;&#30340;&#38590;&#39064;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#19982;&#20154;&#31867;&#36827;&#34892;&#23545;&#35805;&#24182;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26159;&#26234;&#33021;&#26080;&#20154;&#26426;&#65288;&#21363;&#26080;&#20154;&#26426;&#65289;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;&#23427;&#21487;&#20197;&#20943;&#36731;&#20154;&#20204;&#19968;&#30452;&#25569;&#30528;&#25511;&#21046;&#22120;&#30340;&#36127;&#25285;&#65292;&#20801;&#35768;&#21516;&#26102;&#36827;&#34892;&#22810;&#39033;&#20219;&#21153;&#65292;&#20351;&#27531;&#30142;&#20154;&#25110;&#25163;&#37096;&#21344;&#29992;&#32773;&#26356;&#23481;&#26131;&#25511;&#21046;&#26080;&#20154;&#26426;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Aerial Vision-and-Dialog Navigation&#65288;AVDN&#65289;&#30340;&#26080;&#20154;&#26426;&#23548;&#33322;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#26469;&#25511;&#21046;&#26080;&#20154;&#26426;&#30340;&#23548;&#33322;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#24102;&#26377;&#36830;&#32493;&#36924;&#30495;&#29615;&#22659;&#30340;&#26080;&#20154;&#26426;&#27169;&#25311;&#22120;&#65292;&#24182;&#25910;&#38598;&#20102;&#19968;&#32452;&#26032;&#30340;AVDN&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#36229;&#36807;3,000&#20010;&#35760;&#24405;&#30340;&#23548;&#33322;&#36712;&#36857;&#20197;&#21450;&#25351;&#25381;&#23448;&#21644;&#36319;&#38543;&#32773;&#20043;&#38388;&#30340;&#24322;&#27493;&#20154;&#38469;&#23545;&#35805;&#12290;&#25351;&#25381;&#23448;&#25552;&#20379;&#21021;&#22987;&#23548;&#33322;&#25351;&#20196;&#65292;&#24182;&#26681;&#25454;&#38656;&#35201;&#25552;&#20379;&#36827;&#19968;&#27493;&#30340;&#25351;&#23548;&#65292;&#32780;&#36319;&#38543;&#32773;&#22312;&#27169;&#25311;&#22120;&#20013;&#23548;&#33322;&#26080;&#20154;&#26426;&#24182;&#22312;&#38656;&#35201;&#26102;&#25552;&#38382;&#12290;&#22312;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#20013;&#65292;&#36319;&#38543;&#32773;&#23545;&#26080;&#20154;&#26426;&#30340;&#35270;&#35273;&#35266;&#23519;&#20063;&#34987;&#35760;&#24405;&#19979;&#26469;&#12290;&#22522;&#20110;AVDN&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#20154;&#26426;&#23548;&#33322;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to converse with humans and follow natural language commands is crucial for intelligent unmanned aerial vehicles (a.k.a. drones). It can relieve people's burden of holding a controller all the time, allow multitasking, and make drone control more accessible for people with disabilities or with their hands occupied. To this end, we introduce Aerial Vision-and-Dialog Navigation (AVDN), to navigate a drone via natural language conversation. We build a drone simulator with a continuous photorealistic environment and collect a new AVDN dataset of over 3k recorded navigation trajectories with asynchronous human-human dialogs between commanders and followers. The commander provides initial navigation instruction and further guidance by request, while the follower navigates the drone in the simulator and asks questions when needed. During data collection, followers' attention on the drone's visual observation is also recorded. Based on the AVDN dataset, we study the tasks of aerial
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SMARAGD&#31639;&#27861;&#65292;&#29992;&#20110;&#36827;&#34892;&#20934;&#30830;&#24555;&#36895;&#30340;&#36817;&#20284;&#22270;&#36317;&#31163;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20197;&#32447;&#24615;&#26102;&#38388;&#25110;&#24658;&#23450;&#26102;&#38388;&#36817;&#20284;Smatch&#20998;&#25968;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#22270;&#20687;&#21311;&#21517;&#21270;&#65292;&#22823;&#24133;&#20943;&#23569;&#20102;&#36817;&#20284;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2203.13226</link><description>&lt;p&gt;
SMARAGD: &#23398;&#20064;SMATCH&#20197;&#33719;&#24471;&#20934;&#30830;&#24555;&#36895;&#30340;&#36817;&#20284;&#22270;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
SMARAGD: Learning SMatch for Accurate and Rapid Approximate Graph Distance. (arXiv:2203.13226v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.13226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SMARAGD&#31639;&#27861;&#65292;&#29992;&#20110;&#36827;&#34892;&#20934;&#30830;&#24555;&#36895;&#30340;&#36817;&#20284;&#22270;&#36317;&#31163;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20197;&#32447;&#24615;&#26102;&#38388;&#25110;&#24658;&#23450;&#26102;&#38388;&#36817;&#20284;Smatch&#20998;&#25968;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#22270;&#20687;&#21311;&#21517;&#21270;&#65292;&#22823;&#24133;&#20943;&#23569;&#20102;&#36817;&#20284;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#29992;&#32467;&#26500;&#21305;&#37197;&#31639;&#27861;&#65288;&#22914;Smatch&#65289;&#26469;&#35780;&#20272;&#22270;&#20687;&#32467;&#26500;&#65288;&#22914;&#21547;&#20041;&#34920;&#31034;&#65289;&#65292;&#20294;&#36825;&#20123;&#31639;&#27861;&#38754;&#20020;&#30528;NP&#23436;&#22791;&#38382;&#39064;&#65292;&#20351;&#24471;&#22823;&#35268;&#27169;&#24212;&#29992;&#19981;&#21487;&#34892;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;SMARAGD&#65292;&#21363;&#29992;&#20110;&#20934;&#30830;&#24555;&#36895;&#36817;&#20284;&#22270;&#36317;&#31163;&#30340;&#35821;&#20041;&#21305;&#37197;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#22270;&#20687;&#21311;&#21517;&#21270;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;&#36817;&#20284;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The similarity of graph structures, such as Meaning Representations (MRs), is often assessed via structural matching algorithms, such as Smatch (Cai and Knight, 2013). However, Smatch involves a combinatorial problem that suffers from NP-completeness, making large-scale applications, e.g., graph clustering or search, infeasible. To alleviate this issue, we learn SMARAGD: Semantic Match for Accurate and Rapid Approximate Graph Distance. We show the potential of neural networks to approximate Smatch scores, i) in linear time using a machine translation framework to predict alignments, or ii) in constant time using a Siamese CNN to directly predict Smatch scores. We show that the approximation error can be substantially reduced through data augmentation and graph anonymization.
&lt;/p&gt;</description></item></channel></rss>