<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#36827;&#34892;&#31574;&#30053;&#24615;&#25554;&#20837;&#21518;&#38376;&#65292;&#23545;&#40784;&#25935;&#24863;&#30701;&#35821;&#19982;&#20013;&#24615;&#26415;&#35821;&#30340;&#23884;&#20837;&#65292;&#20197;&#21024;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08320</link><description>&lt;p&gt;
&#20351;&#29992;&#21518;&#38376;&#25216;&#26415;&#20445;&#25252;&#25105;&#20204;&#30340;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Defending Our Privacy With Backdoors. (arXiv:2310.08320v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#36827;&#34892;&#31574;&#30053;&#24615;&#25554;&#20837;&#21518;&#38376;&#65292;&#23545;&#40784;&#25935;&#24863;&#30701;&#35821;&#19982;&#20013;&#24615;&#26415;&#35821;&#30340;&#23884;&#20837;&#65292;&#20197;&#21024;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#26410;&#32463;&#31579;&#36873;&#12289;&#24120;&#24120;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#30340;&#32593;&#39029;&#25968;&#25454;&#35757;&#32451;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#38544;&#31169;&#38382;&#39064;&#25104;&#20026;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#20851;&#27880;&#28857;&#12290;&#20854;&#20013;&#19968;&#20010;&#38382;&#39064;&#26159;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#21033;&#29992;&#38544;&#31169;&#25915;&#20987;&#30340;&#26041;&#27861;&#25552;&#21462;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#22312;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#21435;&#38500;&#29305;&#23450;&#20449;&#24687;&#26159;&#19968;&#20010;&#19981;&#23481;&#26131;&#35299;&#20915;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#27169;&#22411;&#20013;&#21024;&#38500;&#31169;&#20154;&#20449;&#24687;&#65292;&#22914;&#20010;&#20154;&#22995;&#21517;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#25554;&#20837;&#21518;&#38376;&#65292;&#25105;&#20204;&#23558;&#25935;&#24863;&#30701;&#35821;&#30340;&#23884;&#20837;&#19982;&#20013;&#24615;&#26415;&#35821;&#30340;&#23884;&#20837;&#23545;&#40784;&#65292;&#20363;&#22914;&#29992;"a person"&#20195;&#26367;&#20154;&#21517;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#36890;&#36807;&#23545;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#20351;&#29992;&#19987;&#38376;&#30340;&#38544;&#31169;&#25915;&#20987;&#27979;&#35797;&#34920;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#21518;&#38376;&#30340;&#38450;&#24481;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;"&#21452;&#37325;&#29992;&#36884;"&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of large AI models trained on uncurated, often sensitive web-scraped data has raised significant privacy concerns. One of the concerns is that adversaries can extract information about the training data using privacy attacks. Unfortunately, the task of removing specific information from the models without sacrificing performance is not straightforward and has proven to be challenging. We propose a rather easy yet effective defense based on backdoor attacks to remove private information such as names of individuals from models, and focus in this work on text encoders. Specifically, through strategic insertion of backdoors, we align the embeddings of sensitive phrases with those of neutral terms-"a person" instead of the person's name. Our empirical results demonstrate the effectiveness of our backdoor-based defense on CLIP by assessing its performance using a specialized privacy attack for zero-shot classifiers. Our approach provides not only a new "dual-use" perspecti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#32473;&#28436;&#31034;&#31034;&#20363;&#37325;&#26032;&#21152;&#26435;&#20197;&#20248;&#21270;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21644;&#26368;&#32456;&#23398;&#20064;&#24615;&#33021;&#20855;&#26377;&#24378;&#30456;&#20851;&#24615;&#30340;&#25513;&#30721;&#33258;&#25105;&#39044;&#27979;&#20998;&#25968;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36817;&#20284;&#26368;&#20248;&#30340;&#26435;&#37325;&#65292;&#24182;&#37319;&#29992;&#31163;&#25955;&#21270;&#21644;&#27874;&#26463;&#25628;&#32034;&#31561;&#26041;&#27861;&#21152;&#24555;&#20102;&#26435;&#37325;&#25628;&#32034;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.08309</link><description>&lt;p&gt;
&#24182;&#38750;&#25152;&#26377;&#30340;&#28436;&#31034;&#31034;&#20363;&#37117;&#26377;&#21516;&#26679;&#30340;&#30410;&#22788;&#65306;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#37325;&#26032;&#21152;&#26435;&#28436;&#31034;&#31034;&#20363;
&lt;/p&gt;
&lt;p&gt;
Not All Demonstration Examples are Equally Beneficial: Reweighting Demonstration Examples for In-Context Learning. (arXiv:2310.08309v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#32473;&#28436;&#31034;&#31034;&#20363;&#37325;&#26032;&#21152;&#26435;&#20197;&#20248;&#21270;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21644;&#26368;&#32456;&#23398;&#20064;&#24615;&#33021;&#20855;&#26377;&#24378;&#30456;&#20851;&#24615;&#30340;&#25513;&#30721;&#33258;&#25105;&#39044;&#27979;&#20998;&#25968;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36817;&#20284;&#26368;&#20248;&#30340;&#26435;&#37325;&#65292;&#24182;&#37319;&#29992;&#31163;&#25955;&#21270;&#21644;&#27874;&#26463;&#25628;&#32034;&#31561;&#26041;&#27861;&#21152;&#24555;&#20102;&#26435;&#37325;&#25628;&#32034;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#33719;&#24471;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#33021;&#21147;&#65292;&#20165;&#36890;&#36807;&#22312;&#36755;&#20837;&#24207;&#21015;&#20013;&#28155;&#21152;&#20960;&#20010;&#28436;&#31034;&#31034;&#20363;&#23601;&#21487;&#20197;&#24555;&#36895;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;ICL&#23454;&#36341;&#23558;&#25152;&#26377;&#28436;&#31034;&#31034;&#20363;&#35270;&#20026;&#30456;&#31561;&#65292;&#20173;&#38656;&#35201;&#25913;&#36827;&#65292;&#22240;&#20026;&#31034;&#20363;&#30340;&#36136;&#37327;&#36890;&#24120;&#26159;&#19981;&#22343;&#21248;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#30830;&#23450;&#36817;&#20284;&#26368;&#20248;&#30340;&#28436;&#31034;&#31034;&#20363;&#26435;&#37325;&#20197;&#21450;&#22914;&#20309;&#22312;ICL&#36807;&#31243;&#20013;&#24212;&#29992;&#23427;&#20204;&#12290;&#20026;&#20102;&#22312;&#27809;&#26377;&#39069;&#22806;&#39564;&#35777;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#26435;&#37325;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25513;&#30721;&#33258;&#25105;&#39044;&#27979;&#65288;MSP&#65289;&#20998;&#25968;&#65292;&#35813;&#20998;&#25968;&#19982;&#26368;&#32456;&#30340;ICL&#24615;&#33021;&#21576;&#24378;&#30456;&#20851;&#12290;&#20026;&#20102;&#21152;&#36895;&#26435;&#37325;&#25628;&#32034;&#36807;&#31243;&#65292;&#25105;&#20204;&#23545;&#36830;&#32493;&#26435;&#37325;&#31354;&#38388;&#36827;&#34892;&#31163;&#25955;&#21270;&#65292;&#24182;&#37319;&#29992;&#27874;&#26463;&#25628;&#32034;&#12290;&#36890;&#36807;&#33719;&#24471;&#36817;&#20284;&#26368;&#20248;&#26435;&#37325;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#23558;&#23427;&#20204;&#24212;&#29992;&#21040;&#19981;&#21516;&#27169;&#22411;&#20301;&#32622;&#30340;&#28436;&#31034;&#31034;&#20363;&#19978;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently gained the In-Context Learning (ICL) ability with the models scaling up, allowing them to quickly adapt to downstream tasks with only a few demonstration examples prepended in the input sequence. Nonetheless, the current practice of ICL treats all demonstration examples equally, which still warrants improvement, as the quality of examples is usually uneven. In this paper, we investigate how to determine approximately optimal weights for demonstration examples and how to apply them during ICL. To assess the quality of weights in the absence of additional validation data, we design a masked self-prediction (MSP) score that exhibits a strong correlation with the final ICL performance. To expedite the weight-searching process, we discretize the continuous weight space and adopt beam search. With approximately optimal weights obtained, we further propose two strategies to apply them to demonstrations at different model positions. Experimental resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36828;&#31243;&#30417;&#30563;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#30340;&#22810;&#21407;&#22411;&#32593;&#32476;MProto&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;MProto&#20351;&#29992;&#22810;&#20010;&#21407;&#22411;&#26469;&#34920;&#31034;&#27599;&#20010;&#23454;&#20307;&#31867;&#22411;&#65292;&#20197;&#25429;&#25417;&#23454;&#20307;&#34920;&#31034;&#20013;&#30340;&#31867;&#20869;&#21464;&#21270;&#12290;&#20026;&#20102;&#20943;&#36731;&#22122;&#22768;&#65292;&#25991;&#20013;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21435;&#22122;&#26368;&#20248;&#20256;&#36755;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MProto&#22312;&#36828;&#31243;&#30417;&#30563;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08298</link><description>&lt;p&gt;
MProto&#65306;&#24102;&#21435;&#22122;&#26368;&#20248;&#20256;&#36755;&#30340;&#22810;&#21407;&#22411;&#32593;&#32476;&#22312;&#36828;&#31243;&#30417;&#30563;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
MProto: Multi-Prototype Network with Denoised Optimal Transport for Distantly Supervised Named Entity Recognition. (arXiv:2310.08298v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36828;&#31243;&#30417;&#30563;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#30340;&#22810;&#21407;&#22411;&#32593;&#32476;MProto&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;MProto&#20351;&#29992;&#22810;&#20010;&#21407;&#22411;&#26469;&#34920;&#31034;&#27599;&#20010;&#23454;&#20307;&#31867;&#22411;&#65292;&#20197;&#25429;&#25417;&#23454;&#20307;&#34920;&#31034;&#20013;&#30340;&#31867;&#20869;&#21464;&#21270;&#12290;&#20026;&#20102;&#20943;&#36731;&#22122;&#22768;&#65292;&#25991;&#20013;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21435;&#22122;&#26368;&#20248;&#20256;&#36755;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MProto&#22312;&#36828;&#31243;&#30417;&#30563;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36828;&#31243;&#30417;&#30563;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;DS-NER&#65289;&#26088;&#22312;&#36890;&#36807;&#30693;&#35782;&#24211;&#25110;&#35789;&#34920;&#20197;&#21450;&#26080;&#26631;&#31614;&#30340;&#35821;&#26009;&#24211;&#26469;&#23450;&#20301;&#23454;&#20307;&#25552;&#21450;&#24182;&#23545;&#20854;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#36828;&#31243;&#26631;&#27880;&#23384;&#22312;&#22122;&#22768;&#65292;&#38477;&#20302;&#20102;NER&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;DS-NER&#20219;&#21153;&#30340;&#40065;&#26834;&#24615;&#21407;&#22411;&#32593;&#32476;MProto&#12290;&#19982;&#20197;&#24448;&#22522;&#20110;&#21407;&#22411;&#30340;NER&#26041;&#27861;&#19981;&#21516;&#65292;MProto&#29992;&#22810;&#20010;&#21407;&#22411;&#26469;&#34920;&#31034;&#27599;&#20010;&#23454;&#20307;&#31867;&#22411;&#65292;&#20197;&#34920;&#24449;&#23454;&#20307;&#34920;&#31034;&#20013;&#30340;&#31867;&#20869;&#21464;&#21270;&#12290;&#20026;&#20102;&#20248;&#21270;&#20998;&#31867;&#22120;&#65292;&#27599;&#20010;&#26631;&#35760;&#24212;&#20998;&#37197;&#19968;&#20010;&#21512;&#36866;&#30340;&#30495;&#23454;&#21407;&#22411;&#65292;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#26631;&#35760;-&#21407;&#22411;&#20998;&#37197;&#35270;&#20026;&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#36731;&#19981;&#23436;&#20840;&#26631;&#35760;&#30340;&#22122;&#22768;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21435;&#22122;&#26368;&#20248;&#20256;&#36755;&#65288;DOT&#65289;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;Other&#31867;&#26631;&#35760;&#30340;&#20998;&#37197;&#32467;&#26524;&#19982;&#25152;&#26377;&#21407;&#22411;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#26469;&#21306;&#20998;&#26410;&#26631;&#35760;&#23454;&#20307;&#26631;&#35760;&#19982;&#30495;&#36127;&#20363;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MProto&#22312;DS-NER&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distantly supervised named entity recognition (DS-NER) aims to locate entity mentions and classify their types with only knowledge bases or gazetteers and unlabeled corpus. However, distant annotations are noisy and degrade the performance of NER models. In this paper, we propose a noise-robust prototype network named MProto for the DS-NER task. Different from previous prototype-based NER methods, MProto represents each entity type with multiple prototypes to characterize the intra-class variance among entity representations. To optimize the classifier, each token should be assigned an appropriate ground-truth prototype and we consider such token-prototype assignment as an optimal transport (OT) problem. Furthermore, to mitigate the noise from incomplete labeling, we propose a novel denoised optimal transport (DOT) algorithm. Specifically, we utilize the assignment result between Other class tokens and all prototypes to distinguish unlabeled entity tokens from true negatives. Experimen
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25193;&#23637;BERT&#35789;&#27719;&#34920;&#30340;&#26041;&#27861;&#65292;&#20197;&#29992;&#20110;&#30693;&#35782;&#24211;&#26500;&#24314;&#20219;&#21153;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#20877;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#26032;&#21333;&#35789;&#26469;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#65292;&#24182;&#20445;&#30041;&#20102;&#26032;&#28155;&#21152;&#21333;&#35789;&#30340;&#35821;&#20041;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2310.08291</link><description>&lt;p&gt;
&#25193;&#23637;BERT&#35789;&#27719;&#34920;&#20197;&#29992;&#20110;&#30693;&#35782;&#24211;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Expanding the Vocabulary of BERT for Knowledge Base Construction. (arXiv:2310.08291v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25193;&#23637;BERT&#35789;&#27719;&#34920;&#30340;&#26041;&#27861;&#65292;&#20197;&#29992;&#20110;&#30693;&#35782;&#24211;&#26500;&#24314;&#20219;&#21153;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#20877;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#26032;&#21333;&#35789;&#26469;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#65292;&#24182;&#20445;&#30041;&#20102;&#26032;&#28155;&#21152;&#21333;&#35789;&#30340;&#35821;&#20041;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#26500;&#24314;&#28041;&#21450;&#33719;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#20197;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#20107;&#23454;&#21644;&#20851;&#31995;&#25968;&#25454;&#30340;&#30693;&#35782;&#24211;&#65292;&#20197;&#20415;&#20110;&#38382;&#31572;&#12289;&#20449;&#24687;&#26816;&#32034;&#21644;&#35821;&#20041;&#29702;&#35299;&#12290;&#22269;&#38469;&#35821;&#20041;&#32593;&#20250;&#35758;2023&#24180;&#30340;&#21517;&#20026;&#8220;&#26469;&#33258;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#24211;&#26500;&#24314;&#8221;&#25361;&#25112;&#23450;&#20041;&#20102;&#19987;&#27880;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30693;&#35782;&#24211;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#20851;&#27880;&#25361;&#25112;&#30340;&#31532;&#19968;&#36712;&#36947;&#65292;&#20854;&#20013;&#21442;&#25968;&#21463;&#38480;&#20110;10&#20159;&#65292;&#24182;&#19988;&#31105;&#27490;&#22312;&#25552;&#31034;&#20013;&#21253;&#21547;&#23454;&#20307;&#25551;&#36848;&#12290;&#23613;&#31649;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#28789;&#27963;&#24615;&#26469;&#25193;&#23637;&#20854;&#35789;&#27719;&#34920;&#65292;&#20294;&#23427;&#24182;&#38750;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22810;&#20196;&#29260;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#30693;&#35782;&#24211;&#26500;&#24314;&#30340;&#21487;&#25193;&#23637;&#35789;&#27719;BERT&#65292;&#23427;&#22312;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#30340;&#21516;&#26102;&#65292;&#20445;&#30041;&#20102;&#26032;&#28155;&#21152;&#21333;&#35789;&#30340;&#35821;&#20041;&#23884;&#20837;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#20877;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#38024;&#23545;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge base construction entails acquiring structured information to create a knowledge base of factual and relational data, facilitating question answering, information retrieval, and semantic understanding. The challenge called "Knowledge Base Construction from Pretrained Language Models" at International Semantic Web Conference 2023 defines tasks focused on constructing knowledge base using language model. Our focus was on Track 1 of the challenge, where the parameters are constrained to a maximum of 1 billion, and the inclusion of entity descriptions within the prompt is prohibited.  Although the masked language model offers sufficient flexibility to extend its vocabulary, it is not inherently designed for multi-token prediction. To address this, we present Vocabulary Expandable BERT for knowledge base construction, which expand the language model's vocabulary while preserving semantic embeddings for newly added words. We adopt task-specific re-pre-training on masked language mo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#28145;&#20837;&#30740;&#31350;&#20102;&#35270;&#38556;&#23398;&#29983;&#30340;Odia&#30450;&#25991;&#38405;&#35835;&#29702;&#35299;&#24773;&#20917;&#65292;&#21457;&#29616;&#38405;&#35835;&#36895;&#24230;&#19982;&#38405;&#35835;&#38169;&#35823;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08280</link><description>&lt;p&gt;
&#20248;&#21270;Odia&#30450;&#25991;&#35782;&#23383;&#65306;&#36895;&#24230;&#23545;&#38169;&#35823;&#20943;&#23569;&#21644;&#25552;&#39640;&#29702;&#35299;&#21147;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Optimizing Odia Braille Literacy: The Influence of Speed on Error Reduction and Enhanced Comprehension. (arXiv:2310.08280v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#28145;&#20837;&#30740;&#31350;&#20102;&#35270;&#38556;&#23398;&#29983;&#30340;Odia&#30450;&#25991;&#38405;&#35835;&#29702;&#35299;&#24773;&#20917;&#65292;&#21457;&#29616;&#38405;&#35835;&#36895;&#24230;&#19982;&#38405;&#35835;&#38169;&#35823;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#23545;&#35270;&#38556;&#23398;&#29983;&#30340;Odia&#30450;&#25991;&#38405;&#35835;&#29702;&#35299;&#36827;&#34892;&#28145;&#20837;&#35814;&#32454;&#30340;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#20182;&#20204;&#30340;&#38405;&#35835;&#36895;&#24230;&#21644;&#25163;&#25351;&#36816;&#21160;&#12290;&#30740;&#31350;&#36824;&#26088;&#22312;&#35843;&#26597;&#20182;&#20204;&#21487;&#33021;&#36935;&#21040;&#30340;&#29702;&#35299;&#22256;&#38590;&#21644;&#38405;&#35835;&#38169;&#35823;&#12290;&#30740;&#31350;&#23545;&#35937;&#20026;&#26469;&#33258;9&#24180;&#32423;&#21644;10&#24180;&#32423;&#30340;&#20845;&#21517;&#24180;&#40836;&#22312;14&#33267;16&#23681;&#20043;&#38388;&#30340;&#23398;&#29983;&#12290;&#25105;&#20204;&#35266;&#23519;&#20102;&#21442;&#19982;&#32773;&#30340;&#25163;&#25351;&#36816;&#21160;&#65292;&#20197;&#20102;&#35299;&#38405;&#35835;&#38169;&#35823;&#19982;&#25163;&#25351;&#36816;&#21160;&#30340;&#32852;&#31995;&#65292;&#24182;&#30830;&#23450;&#23398;&#29983;&#30340;&#38405;&#35835;&#22256;&#38590;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#21442;&#19982;&#32773;&#30340;Odia&#30450;&#25991;&#38405;&#35835;&#33021;&#21147;&#65292;&#21253;&#25324;&#20182;&#20204;&#30340;&#38405;&#35835;&#36895;&#24230;&#65288;&#27599;&#20998;&#38047;&#35789;&#25968;&#65289;&#12289;&#38169;&#35823;&#21644;&#29702;&#35299;&#21147;&#12290;Odia&#30450;&#25991;&#35835;&#32773;&#30340;&#24179;&#22343;&#36895;&#24230;&#20026;17.64&#35789;/&#20998;&#38047;&#12290;&#26681;&#25454;&#26412;&#30740;&#31350;&#65292;&#38405;&#35835;&#36895;&#24230;&#19982;&#38405;&#35835;&#38169;&#35823;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#30456;&#20851;&#24615;&#12290;&#38543;&#30528;&#38405;&#35835;&#36895;&#24230;&#30340;&#38477;&#20302;&#65292;&#38405;&#35835;&#38169;&#35823;&#30340;&#25968;&#37327;&#24448;&#24448;&#20250;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#24314;&#31435;&#20102;&#38405;&#35835;&#36895;&#24230;&#21644;&#29702;&#35299;&#21147;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to conduct an extensive detailed analysis of the Odia Braille reading comprehension among students with visual disability. Specifically, the study explores their reading speed and hand or finger movements. The study also aims to investigate any comprehension difficulties and reading errors they may encounter. Six students from the 9th and 10th grades, aged between 14 and 16, participated in the study. We observed participants hand movements to understand how reading errors were connected to hand movement and identify the students reading difficulties. We also evaluated the participants Odia Braille reading skills, including their reading speed (in words per minute), errors, and comprehension. The average speed of Odia Braille reader is 17.64wpm. According to the study, there was a noticeable correlation between reading speed and reading errors. As reading speed decreased, the number of reading errors tended to increase. Moreover, the study established a link between red
&lt;/p&gt;</description></item><item><title>CP-KGC&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32422;&#26463;&#24335;&#25552;&#31034;&#26469;&#34917;&#20840;&#30693;&#35782;&#22270;&#35889;&#65292;&#25552;&#39640;&#25512;&#26029;&#25928;&#26524;&#65292;&#23637;&#31034;&#20102;&#22312;&#20302;&#36164;&#28304;&#35745;&#31639;&#26465;&#20214;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20043;&#21069;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.08279</link><description>&lt;p&gt;
CP-KGC: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32422;&#26463;&#24335;&#25552;&#31034;&#23545;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
CP-KGC: Constrained-Prompt Knowledge Graph Completion with Large Language Models. (arXiv:2310.08279v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08279
&lt;/p&gt;
&lt;p&gt;
CP-KGC&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32422;&#26463;&#24335;&#25552;&#31034;&#26469;&#34917;&#20840;&#30693;&#35782;&#22270;&#35889;&#65292;&#25552;&#39640;&#25512;&#26029;&#25928;&#26524;&#65292;&#23637;&#31034;&#20102;&#22312;&#20302;&#36164;&#28304;&#35745;&#31639;&#26465;&#20214;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20043;&#21069;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26088;&#22312;&#21033;&#29992;&#29616;&#26377;&#30693;&#35782;&#25512;&#26029;&#21644;&#25512;&#27979;&#30693;&#35782;&#22270;&#35889;&#20013;&#32570;&#22833;&#30340;&#36830;&#25509;&#12290;SimKGC&#31561;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#24050;&#32463;&#36229;&#36807;&#20102;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#24402;&#32435;&#24335;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#30340;&#25928;&#26524;&#21462;&#20915;&#20110;&#23454;&#20307;&#25991;&#26412;&#25551;&#36848;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#20943;&#36731;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#30340;&#24187;&#35273;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#32422;&#26463;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#21033;&#29992;&#23454;&#20307;&#21450;&#20854;&#25991;&#26412;&#25551;&#36848;&#20316;&#20026;&#19978;&#19979;&#25991;&#32422;&#26463;&#26469;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#32422;&#26463;&#24335;&#25552;&#31034;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#27861;&#65288;CP-KGC&#65289;&#22312;&#20302;&#36164;&#28304;&#35745;&#31639;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#26377;&#25928;&#30340;&#25512;&#26029;&#33021;&#21147;&#65292;&#24182;&#36229;&#36807;&#20102;WN18RR&#21644;FB15K237&#25968;&#25454;&#38598;&#19978;&#30340;&#20043;&#21069;&#32467;&#26524;&#12290;&#36825;&#23637;&#31034;&#20102;LLMs&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#20013;&#30340;&#25972;&#21512;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph completion (KGC) aims to utilize existing knowledge to deduce and infer missing connections within knowledge graphs. Text-based approaches, like SimKGC, have outperformed graph embedding methods, showcasing the promise of inductive KGC. However, the efficacy of text-based methods hinges on the quality of entity textual descriptions. In this paper, we identify the key issue of whether large language models (LLMs) can generate effective text. To mitigate hallucination in LLM-generated text in this paper, we introduce a constraint-based prompt that utilizes the entity and its textual description as contextual constraints to enhance data quality. Our Constrained-Prompt Knowledge Graph Completion (CP-KGC) method demonstrates effective inference under low resource computing conditions and surpasses prior results on the WN18RR and FB15K237 datasets. This showcases the integration of LLMs in KGC tasks and provides new directions for future research.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#24120;&#24120;&#20986;&#29616;&#20107;&#23454;&#38169;&#35823;&#65292;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#36807;&#24230;&#20381;&#36182;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#20849;&#29616;&#32479;&#35745;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#20849;&#29616;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#38590;&#20197;&#22238;&#24518;&#36215;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#24456;&#23569;&#20849;&#29616;&#30340;&#20107;&#23454;&#12290;&#24314;&#35758;&#20351;&#29992;&#21435;&#20559;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#26469;&#20943;&#36731;&#20559;&#35265;&#65292;&#20294;&#36825;&#23545;&#20110;&#24494;&#35843;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#31232;&#26377;&#20107;&#23454;&#30340;&#22238;&#24518;&#25928;&#26524;&#24182;&#19981;&#26126;&#26174;&#12290;</title><link>http://arxiv.org/abs/2310.08256</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20849;&#29616;&#23545;&#20107;&#23454;&#30693;&#35782;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Co-occurrence on Factual Knowledge of Large Language Models. (arXiv:2310.08256v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08256
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#24120;&#24120;&#20986;&#29616;&#20107;&#23454;&#38169;&#35823;&#65292;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#36807;&#24230;&#20381;&#36182;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#20849;&#29616;&#32479;&#35745;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#20849;&#29616;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#38590;&#20197;&#22238;&#24518;&#36215;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#24456;&#23569;&#20849;&#29616;&#30340;&#20107;&#23454;&#12290;&#24314;&#35758;&#20351;&#29992;&#21435;&#20559;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#26469;&#20943;&#36731;&#20559;&#35265;&#65292;&#20294;&#36825;&#23545;&#20110;&#24494;&#35843;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#31232;&#26377;&#20107;&#23454;&#30340;&#22238;&#24518;&#25928;&#26524;&#24182;&#19981;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#22312;&#20107;&#23454;&#19978;&#20570;&#20986;&#38169;&#35823;&#30340;&#22238;&#31572;&#12290;&#26412;&#25991;&#20551;&#35774;&#36807;&#24230;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#31616;&#21333;&#20849;&#29616;&#32479;&#35745;&#26159;&#23548;&#33268;&#20107;&#23454;&#38169;&#35823;&#30340;&#20027;&#35201;&#22240;&#32032;&#20043;&#19968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#20849;&#29616;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#21363;&#26356;&#20542;&#21521;&#20110;&#36873;&#25321;&#39057;&#32321;&#20849;&#29616;&#30340;&#35789;&#32780;&#19981;&#26159;&#27491;&#30830;&#31572;&#26696;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;&#22312;&#24494;&#35843;&#26399;&#38388;&#24050;&#32463;&#35265;&#36807;&#36825;&#20123;&#20107;&#23454;&#30340;&#20027;&#39064;&#21644;&#23545;&#35937;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#24456;&#23569;&#20849;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#22238;&#24518;&#36215;&#36825;&#20123;&#20107;&#23454;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#25193;&#22823;&#27169;&#22411;&#35268;&#27169;&#25110;&#36827;&#34892;&#24494;&#35843;&#65292;&#20849;&#29616;&#20559;&#35265;&#20173;&#28982;&#23384;&#22312;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#21435;&#20559;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#36890;&#36807;&#36807;&#28388;&#25481;&#20027;&#39064;-&#23545;&#35937;&#20849;&#29616;&#35745;&#25968;&#39640;&#30340;&#20559;&#35265;&#26679;&#26412;&#26469;&#20943;&#36731;&#20559;&#35265;&#12290;&#23613;&#31649;&#21435;&#20559;&#24494;&#35843;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35760;&#24518;&#35757;&#32451;&#38598;&#20013;&#30340;&#31232;&#26377;&#20107;&#23454;&#65292;&#20294;&#22312;&#24494;&#35843;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#31232;&#26377;&#20107;&#23454;&#30340;&#22238;&#24518;&#25928;&#26524;&#24182;&#19981;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) often make factually incorrect responses despite their success in various applications. In this paper, we hypothesize that relying heavily on simple co-occurrence statistics of the pre-training corpora is one of the main factors that cause factual errors. Our results reveal that LLMs are vulnerable to the co-occurrence bias, defined as preferring frequently co-occurred words over the correct answer. Consequently, LLMs struggle to recall facts whose subject and object rarely co-occur in the pre-training dataset although they are seen during finetuning. We show that co-occurrence bias remains despite scaling up model sizes or finetuning. Therefore, we suggest finetuning on a debiased dataset to mitigate the bias by filtering out biased samples whose subject-object co-occurrence count is high. Although debiased finetuning allows LLMs to memorize rare facts in the training set, it is not effective in recalling rare facts unseen during finetuning. Further resear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SAID&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;AI&#25991;&#26412;&#26816;&#27979;&#27169;&#22411;&#22312;&#30495;&#23454;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#33021;&#21147;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#30693;&#20046;&#25968;&#25454;&#38598;&#65292;&#27880;&#37322;&#21592;&#21487;&#20197;&#20197;96.5%&#30340;&#20934;&#30830;&#29575;&#21306;&#20998;AI&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.08240</link><description>&lt;p&gt;
&#35841;&#35828;&#30340;&#65311;&#31038;&#20132;&#23186;&#20307;AI&#26816;&#27979;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Who Said That? Benchmarking Social Media AI Detection. (arXiv:2310.08240v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SAID&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;AI&#25991;&#26412;&#26816;&#27979;&#27169;&#22411;&#22312;&#30495;&#23454;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#33021;&#21147;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#30693;&#20046;&#25968;&#25454;&#38598;&#65292;&#27880;&#37322;&#21592;&#21487;&#20197;&#20197;96.5%&#30340;&#20934;&#30830;&#29575;&#21306;&#20998;AI&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#29983;&#25104;&#30340;&#25991;&#26412;&#22312;&#21508;&#31181;&#22312;&#32447;&#24179;&#21488;&#19978;&#24191;&#27867;&#23384;&#22312;&#65292;&#26082;&#24102;&#26469;&#20102;&#21464;&#38761;&#30340;&#21069;&#26223;&#65292;&#20063;&#24102;&#26469;&#20102;&#19982;&#34394;&#20551;&#20449;&#24687;&#21644;&#25805;&#32437;&#30456;&#20851;&#30340;&#37325;&#22823;&#39118;&#38505;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;SAID&#65288;&#31038;&#20132;&#23186;&#20307;AI&#26816;&#27979;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#30495;&#23454;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;AI&#25991;&#26412;&#26816;&#27979;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#23427;&#21253;&#21547;&#26469;&#33258;&#30693;&#20046;&#21644;Quora&#31561;&#28909;&#38376;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#30495;&#23454;AI&#29983;&#25104;&#25991;&#26412;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#19981;&#21516;&#65292;SAID&#22788;&#29702;&#21453;&#26144;&#30495;&#23454;AI&#29992;&#25143;&#22312;&#20114;&#32852;&#32593;&#19978;&#20351;&#29992;&#30340;&#22797;&#26434;&#31574;&#30053;&#30340;&#20869;&#23481;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#33021;&#36867;&#36991;&#26816;&#27979;&#25110;&#33719;&#24471;&#21487;&#35265;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#21152;&#30495;&#23454;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35780;&#20272;&#29615;&#22659;&#12290;&#22522;&#20110;&#30693;&#20046;&#25968;&#25454;&#38598;&#30340;&#19968;&#20010;&#26174;&#33879;&#21457;&#29616;&#26159;&#65292;&#27880;&#37322;&#21592;&#21487;&#20197;&#20197;96.5%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#21306;&#20998;AI&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#25991;&#26412;&#12290;&#36825;&#19968;&#21457;&#29616;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#20154;&#31867;&#35782;&#21035;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI-generated text has proliferated across various online platforms, offering both transformative prospects and posing significant risks related to misinformation and manipulation. Addressing these challenges, this paper introduces SAID (Social media AI Detection), a novel benchmark developed to assess AI-text detection models' capabilities in real social media platforms. It incorporates real AI-generate text from popular social media platforms like Zhihu and Quora. Unlike existing benchmarks, SAID deals with content that reflects the sophisticated strategies employed by real AI users on the Internet which may evade detection or gain visibility, providing a more realistic and challenging evaluation landscape. A notable finding of our study, based on the Zhihu dataset, reveals that annotators can distinguish between AI-generated and human-generated texts with an average accuracy rate of 96.5%. This finding necessitates a re-evaluation of human capability in recognizing AI-generated text 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;Transformer&#35299;&#30721;&#22120;&#22312;&#26377;&#38480;&#33521;&#25991;&#25968;&#25454;&#24494;&#35843;&#21518;&#33021;&#22815;&#36890;&#29992;&#22320;&#36827;&#34892;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#32479;&#19968;&#23884;&#20837;&#27169;&#22411;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.08232</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#36890;&#29992;&#30340;&#23884;&#20837;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models are Universal Embedders. (arXiv:2310.08232v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08232
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;Transformer&#35299;&#30721;&#22120;&#22312;&#26377;&#38480;&#33521;&#25991;&#25968;&#25454;&#24494;&#35843;&#21518;&#33021;&#22815;&#36890;&#29992;&#22320;&#36827;&#34892;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#32479;&#19968;&#23884;&#20837;&#27169;&#22411;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#38761;&#21629;&#20013;&#65292;&#23884;&#20837;&#26159;&#21508;&#31181;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#20363;&#22914;&#65292;&#23427;&#34987;&#29992;&#20110;&#20026;LLMs&#26816;&#32034;&#30693;&#35782;&#25110;&#35760;&#24518;&#65292;&#26500;&#24314;&#20869;&#23481;&#36807;&#28388;&#22120;&#31561;&#12290;&#30001;&#20110;&#36825;&#20123;&#24773;&#20917;&#28041;&#21450;&#20174;&#33521;&#35821;&#21040;&#20854;&#20182;&#33258;&#28982;&#25110;&#32534;&#31243;&#35821;&#35328;&#65292;&#20174;&#26816;&#32034;&#21040;&#20998;&#31867;&#31561;&#21508;&#31181;&#24773;&#20917;&#65292;&#22240;&#27492;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#30340;&#23884;&#20837;&#27169;&#22411;&#32780;&#19981;&#26159;&#20026;&#27599;&#20010;&#22330;&#26223;&#19987;&#38376;&#24314;&#31435;&#19968;&#20010;&#26159;&#21487;&#21462;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#26397;&#36825;&#20010;&#30446;&#26631;&#36808;&#20986;&#20102;&#21021;&#22987;&#30340;&#19968;&#27493;&#65292;&#35777;&#26126;&#20102;&#22810;&#35821;&#35328;&#65288;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#65289;&#39044;&#35757;&#32451;&#30340;Transformer&#35299;&#30721;&#22120;&#22312;&#26377;&#38480;&#30340;&#33521;&#25991;&#25968;&#25454;&#24494;&#35843;&#21518;&#33021;&#22815;&#36890;&#29992;&#22320;&#36827;&#34892;&#23884;&#20837;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#23454;&#36341;&#65292;&#24182;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#35780;&#20272;&#12290;&#22312;&#33521;&#25991;MTEB&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#19981;&#20351;&#29992;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#22312;&#19981;&#21516;&#30340;&#23884;&#20837;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#22312;&#20854;&#20182;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#20363;&#22914;&#22810;&#35821;&#35328;&#20998;&#31867;&#21644;&#20195;&#30721;&#25628;&#32034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#65288;&#27809;&#26377;&#20219;&#20309;&#30417;&#30563;&#65289;&#34920;&#29616;&#20986;&#19982;&#25110;&#29978;&#33267;&#36229;&#36807;&#22823;&#37327;&#30417;&#30563;&#22522;&#32447;&#30340;&#21487;&#27604;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the large language model (LLM) revolution, embedding is a key component of various systems. For example, it is used to retrieve knowledge or memories for LLMs, to build content moderation filters, etc. As such cases span from English to other natural or programming languages, from retrieval to classification and beyond, it is desirable to build a unified embedding model rather than dedicated ones for each scenario. In this work, we make an initial step towards this goal, demonstrating that multiple languages (both natural and programming) pre-trained transformer decoders can embed universally when finetuned on limited English data. We provide a comprehensive practice with thorough evaluations. On English MTEB, our models achieve competitive performance on different embedding tasks by minimal training data. On other benchmarks, such as multilingual classification and code search, our models (without any supervision) perform comparably to, or even surpass heavily supervised baselines 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#34920;&#31034;&#27861;&#65288;SSLR&#65289;&#30340;&#24555;&#36895;WER&#20272;&#35745;&#22120;&#65288;Fe-WER&#65289;&#65292;&#22312;&#22823;&#25968;&#25454;&#22330;&#26223;&#19979;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.08225</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#27861;&#23545;&#35821;&#38899;&#21644;&#25991;&#26412;&#36827;&#34892;&#24555;&#36895;&#23383;&#38169;&#29575;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Fast Word Error Rate Estimation Using Self-Supervised Representations For Speech And Text. (arXiv:2310.08225v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#34920;&#31034;&#27861;&#65288;SSLR&#65289;&#30340;&#24555;&#36895;WER&#20272;&#35745;&#22120;&#65288;Fe-WER&#65289;&#65292;&#22312;&#22823;&#25968;&#25454;&#22330;&#26223;&#19979;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#36136;&#37327;&#36890;&#24120;&#36890;&#36807;&#23383;&#38169;&#29575;&#65288;WER&#65289;&#26469;&#34913;&#37327;&#12290;WER&#20272;&#35745;&#26159;&#19968;&#39033;&#20219;&#21153;&#65292;&#26088;&#22312;&#39044;&#27979;ASR&#31995;&#32479;&#30340;WER&#65292;&#32473;&#23450;&#19968;&#20010;&#35821;&#38899;&#35828;&#35805;&#21644;&#19968;&#20010;&#36716;&#24405;&#12290;&#22312;&#22823;&#37327;&#25968;&#25454;&#19978;&#35757;&#32451;&#20808;&#36827;&#30340;ASR&#31995;&#32479;&#30340;&#21516;&#26102;&#65292;&#36825;&#20010;&#20219;&#21153;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;WER&#20272;&#35745;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#21464;&#24471;&#24517;&#35201;&#65292;&#20363;&#22914;&#36873;&#25321;&#20855;&#26377;&#26410;&#30693;&#36716;&#24405;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#25110;&#22312;&#27809;&#26377;&#22320;&#38754;&#30495;&#23454;&#36716;&#24405;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;ASR&#31995;&#32479;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;&#38754;&#23545;&#22823;&#37327;&#25968;&#25454;&#65292;WER&#20272;&#35745;&#20202;&#30340;&#36816;&#31639;&#25928;&#29575;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#26410;&#23558;&#20854;&#35270;&#20026;&#20248;&#20808;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#34920;&#31034;&#27861;&#65288;SSLR&#65289;&#30340;&#24555;&#36895;WER&#20272;&#35745;&#22120;&#65288;Fe-WER&#65289;&#12290;&#35813;&#20272;&#35745;&#22120;&#22522;&#20110;&#36890;&#36807;&#24179;&#22343;&#27744;&#32858;&#21512;&#30340;SSLR&#26500;&#24314;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;e-WER3&#22522;&#32447;&#65292;Fe-WER&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;19.69&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quality of automatic speech recognition (ASR) is typically measured by word error rate (WER). WER estimation is a task aiming to predict the WER of an ASR system, given a speech utterance and a transcription. This task has gained increasing attention while advanced ASR systems are trained on large amounts of data. In this case, WER estimation becomes necessary in many scenarios, for example, selecting training data with unknown transcription quality or estimating the testing performance of an ASR system without ground truth transcriptions. Facing large amounts of data, the computation efficiency of a WER estimator becomes essential in practical applications. However, previous works usually did not consider it as a priority. In this paper, a Fast WER estimator (Fe-WER) using self-supervised learning representation (SSLR) is introduced. The estimator is built upon SSLR aggregated by average pooling. The results show that Fe-WER outperformed the e-WER3 baseline relatively by 19.69% an
&lt;/p&gt;</description></item><item><title>SimCKP&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#30701;&#35821;&#32423;&#34920;&#31034;&#26469;&#25552;&#21462;&#20851;&#38190;&#35789;&#30701;&#35821;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#25490;&#24207;&#26469;&#35843;&#25972;&#29983;&#25104;&#30340;&#30701;&#35821;&#30340;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.08221</link><description>&lt;p&gt;
SimCKP: &#31616;&#21333;&#23545;&#27604;&#23398;&#20064;&#20851;&#38190;&#35789;&#30701;&#35821;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
SimCKP: Simple Contrastive Learning of Keyphrase Representations. (arXiv:2310.08221v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08221
&lt;/p&gt;
&lt;p&gt;
SimCKP&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#30701;&#35821;&#32423;&#34920;&#31034;&#26469;&#25552;&#21462;&#20851;&#38190;&#35789;&#30701;&#35821;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#25490;&#24207;&#26469;&#35843;&#25972;&#29983;&#25104;&#30340;&#30701;&#35821;&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#35789;&#29983;&#25104;&#65288;KG&#65289;&#26088;&#22312;&#29983;&#25104;&#19968;&#32452;&#24635;&#32467;&#24615;&#35789;&#35821;&#25110;&#30701;&#35821;&#65292;&#32473;&#23450;&#19968;&#20010;&#28304;&#25991;&#26723;&#65292;&#32780;&#20851;&#38190;&#35789;&#25552;&#21462;&#65288;KE&#65289;&#26088;&#22312;&#20174;&#25991;&#26412;&#20013;&#35782;&#21035;&#23427;&#20204;&#12290;&#30001;&#20110;&#22312;KE&#20013;&#25628;&#32034;&#31354;&#38388;&#36739;&#23567;&#65292;&#36890;&#24120;&#23558;&#20854;&#19982;KG&#30456;&#32467;&#21512;&#65292;&#39044;&#27979;&#21487;&#33021;&#23384;&#22312;&#25110;&#19981;&#23384;&#22312;&#20110;&#30456;&#24212;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#35789;&#30701;&#35821;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#32479;&#19968;&#26041;&#27861;&#37319;&#29992;&#24207;&#21015;&#26631;&#27880;&#21644;&#22522;&#20110;&#26368;&#22823;&#21270;&#30340;&#29983;&#25104;&#65292;&#20027;&#35201;&#22312;&#20196;&#29260;&#32423;&#21035;&#19978;&#25805;&#20316;&#65292;&#19981;&#33021;&#24456;&#22909;&#22320;&#35266;&#23519;&#21644;&#35780;&#20998;&#20851;&#38190;&#35789;&#30701;&#35821;&#20316;&#20026;&#19968;&#20010;&#25972;&#20307;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SimCKP&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;1&#65289;&#25552;&#21462;&#22120;-&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#30701;&#35821;&#32423;&#34920;&#31034;&#26469;&#25552;&#21462;&#20851;&#38190;&#35789;&#30701;&#35821;&#65292;&#21516;&#26102;&#29983;&#25104;&#19981;&#20986;&#29616;&#22312;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#35789;&#30701;&#35821;&#65307;2&#65289;&#37325;&#26032;&#25490;&#24207;&#22120;&#65292;&#36890;&#36807;&#23558;&#23427;&#20204;&#30340;&#34920;&#31034;&#19982;&#30456;&#24212;&#25991;&#26723;&#23545;&#40784;&#65292;&#21516;&#26679;&#35843;&#25972;&#27599;&#20010;&#29983;&#25104;&#30340;&#30701;&#35821;&#30340;&#20998;&#25968;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Keyphrase generation (KG) aims to generate a set of summarizing words or phrases given a source document, while keyphrase extraction (KE) aims to identify them from the text. Because the search space is much smaller in KE, it is often combined with KG to predict keyphrases that may or may not exist in the corresponding document. However, current unified approaches adopt sequence labeling and maximization-based generation that primarily operate at a token level, falling short in observing and scoring keyphrases as a whole. In this work, we propose SimCKP, a simple contrastive learning framework that consists of two stages: 1) An extractor-generator that extracts keyphrases by learning context-aware phrase-level representations in a contrastive manner while also generating keyphrases that do not appear in the document; 2) A reranker that adapts scores for each generated phrase by likewise aligning their representations with the corresponding document. Experimental results on multiple ben
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23391;&#21152;&#25289;&#35821;&#35270;&#35273;&#38382;&#39064;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#24320;&#21457;&#20102;&#22522;&#20110;transformer&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#33021;&#22815;&#29983;&#25104;&#23391;&#21152;&#25289;&#35821;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#23548;&#22411;VQG&#27169;&#22411;&#65292;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#31572;&#26696;&#21644;&#38382;&#39064;&#31867;&#21035;&#29983;&#25104;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23391;&#21152;&#25289;&#35821;VQG&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08187</link><description>&lt;p&gt;
&#23391;&#21152;&#25289;&#35821;&#20013;&#30340;&#35270;&#35273;&#38382;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Visual Question Generation in Bengali. (arXiv:2310.08187v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23391;&#21152;&#25289;&#35821;&#35270;&#35273;&#38382;&#39064;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#24320;&#21457;&#20102;&#22522;&#20110;transformer&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#33021;&#22815;&#29983;&#25104;&#23391;&#21152;&#25289;&#35821;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#23548;&#22411;VQG&#27169;&#22411;&#65292;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#31572;&#26696;&#21644;&#38382;&#39064;&#31867;&#21035;&#29983;&#25104;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23391;&#21152;&#25289;&#35821;VQG&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#39064;&#29983;&#25104;&#65288;VQG&#65289;&#30340;&#20219;&#21153;&#26159;&#29983;&#25104;&#19982;&#32473;&#23450;&#22270;&#20687;&#30456;&#20851;&#30340;&#31867;&#20284;&#20110;&#20154;&#31867;&#38382;&#39064;&#30340;&#25991;&#26412;&#12290;&#30001;&#20110;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#24448;&#24448;&#21482;&#20851;&#27880;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#65292;&#22914;&#33521;&#35821;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#39318;&#20010;&#23391;&#21152;&#25289;&#35821;&#35270;&#35273;&#38382;&#39064;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;transformer&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#22312;&#32473;&#23450;&#22270;&#20687;&#26102;&#21487;&#20197;&#29983;&#25104;&#23391;&#21152;&#25289;&#35821;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#20010;&#27169;&#22411;&#21464;&#20307;-&#65288;i&#65289;&#20165;&#22270;&#20687;&#65306;&#20174;&#22270;&#20687;&#20013;&#29983;&#25104;&#38382;&#39064;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#27809;&#26377;&#39069;&#22806;&#30340;&#20449;&#24687;&#65292;&#65288;ii&#65289;&#22270;&#20687;-&#31867;&#21035;&#21644;&#22270;&#20687;-&#31572;&#26696;-&#31867;&#21035;&#65306;&#24341;&#23548;&#22411;VQG&#65292;&#22312;&#27492;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#31572;&#26696;&#21644;&#26399;&#26395;&#38382;&#39064;&#30340;&#31867;&#21035;&#23545;&#27169;&#22411;&#36827;&#34892;&#32422;&#26463;&#20197;&#29983;&#25104;&#38382;&#39064;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#32763;&#35793;&#21518;&#30340;VQAv2.0&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#32467;&#26524;&#24314;&#31435;&#20102;&#23391;&#21152;&#25289;&#35821;VQG&#20219;&#21153;&#30340;&#39318;&#20010;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of Visual Question Generation (VQG) is to generate human-like questions relevant to the given image. As VQG is an emerging research field, existing works tend to focus only on resource-rich language such as English due to the availability of datasets. In this paper, we propose the first Bengali Visual Question Generation task and develop a novel transformer-based encoder-decoder architecture that generates questions in Bengali when given an image. We propose multiple variants of models - (i) image-only: baseline model of generating questions from images without additional information, (ii) image-category and image-answer-category: guided VQG where we condition the model to generate questions based on the answer and the category of expected question. These models are trained and evaluated on the translated VQAv2.0 dataset. Our quantitative and qualitative results establish the first state of the art models for VQG task in Bengali and demonstrate that our models are capable of g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EIPE-text&#26041;&#27861;&#29992;&#20110;&#38271;&#31687;&#21465;&#20107;&#25991;&#26412;&#29983;&#25104;&#65292;&#36890;&#36807;&#20174;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#35745;&#21010;&#24182;&#21033;&#29992;&#35780;&#20272;&#26426;&#21046;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#65292;&#26500;&#24314;&#20102;&#26356;&#22909;&#30340;&#35268;&#21010;&#22120;&#12290;</title><link>http://arxiv.org/abs/2310.08185</link><description>&lt;p&gt;
EIPE-text: &#38024;&#23545;&#38271;&#31687;&#21465;&#20107;&#25991;&#26412;&#29983;&#25104;&#30340;&#35780;&#20272;&#24341;&#23548;&#36845;&#20195;&#35745;&#21010;&#25552;&#21462;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form Narrative Text Generation. (arXiv:2310.08185v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EIPE-text&#26041;&#27861;&#29992;&#20110;&#38271;&#31687;&#21465;&#20107;&#25991;&#26412;&#29983;&#25104;&#65292;&#36890;&#36807;&#20174;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#35745;&#21010;&#24182;&#21033;&#29992;&#35780;&#20272;&#26426;&#21046;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#65292;&#26500;&#24314;&#20102;&#26356;&#22909;&#30340;&#35268;&#21010;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#21010;&#19982;&#20889;&#20316;&#26159;&#38271;&#31687;&#21465;&#20107;&#25991;&#26412;&#29983;&#25104;&#20013;&#24120;&#29992;&#30340;&#23618;&#27425;&#21270;&#26041;&#27861;&#65292;&#39318;&#20808;&#21019;&#24314;&#19968;&#20010;&#35745;&#21010;&#26469;&#25351;&#23548;&#21465;&#20107;&#20889;&#20316;&#12290;&#36981;&#24490;&#36825;&#31181;&#26041;&#27861;&#65292;&#19968;&#20123;&#30740;&#31350;&#20165;&#20165;&#20381;&#38752;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25552;&#31034;&#36827;&#34892;&#35745;&#21010;&#65292;&#36825;&#32463;&#24120;&#20135;&#29983;&#27425;&#20248;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#35780;&#20272;&#24341;&#23548;&#36845;&#20195;&#35745;&#21010;&#25552;&#21462;&#26041;&#27861;&#65288;EIPE-text&#65289;&#29992;&#20110;&#38271;&#31687;&#21465;&#20107;&#25991;&#26412;&#29983;&#25104;&#65292;&#35813;&#26041;&#27861;&#20174;&#21465;&#20107;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#35745;&#21010;&#24182;&#21033;&#29992;&#25552;&#21462;&#30340;&#35745;&#21010;&#26500;&#24314;&#26356;&#22909;&#30340;&#35268;&#21010;&#22120;&#12290;EIPE-text&#26377;&#19977;&#20010;&#38454;&#27573;&#65306;&#35745;&#21010;&#25552;&#21462;&#12289;&#23398;&#20064;&#21644;&#25512;&#29702;&#12290;&#22312;&#35745;&#21010;&#25552;&#21462;&#38454;&#27573;&#65292;&#23427;&#36845;&#20195;&#22320;&#20174;&#21465;&#20107;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#21644;&#25913;&#36827;&#35745;&#21010;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#35745;&#21010;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38382;&#31572;&#65288;QA&#65289;&#30340;&#35780;&#20272;&#26426;&#21046;&#65292;&#33258;&#21160;&#35780;&#20272;&#35745;&#21010;&#24182;&#29983;&#25104;&#35814;&#32454;&#30340;&#35745;&#21010;&#25913;&#36827;&#25351;&#31034;&#65292;&#20197;&#24341;&#23548;&#36845;&#20195;&#25913;&#36827;&#12290;&#22312;&#23398;&#20064;&#38454;&#27573;&#65292;&#25105;&#20204;&#36890;&#36807;&#24494;&#35843;&#26500;&#24314;&#19968;&#20010;&#26356;&#22909;&#30340;&#35268;&#21010;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plan-and-Write is a common hierarchical approach in long-form narrative text generation, which first creates a plan to guide the narrative writing. Following this approach, several studies rely on simply prompting large language models for planning, which often yields suboptimal results. In this paper, we propose a new framework called Evaluation-guided Iterative Plan Extraction for long-form narrative text generation (EIPE-text), which extracts plans from the corpus of narratives and utilizes the extracted plans to construct a better planner. EIPE-text has three stages: plan extraction, learning, and inference. In the plan extraction stage, it iteratively extracts and improves plans from the narrative corpus and constructs a plan corpus. We propose a question answer (QA) based evaluation mechanism to automatically evaluate the plans and generate detailed plan refinement instructions to guide the iterative improvement. In the learning stage, we build a better planner by fine-tuning wit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25945;&#32946;&#35786;&#26029;&#35780;&#20272;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#32467;&#26500;&#65292;&#24378;&#35843;&#20102;&#30740;&#31350;LLMs&#30340;&#35748;&#30693;&#33021;&#21147;&#21644;&#19981;&#21516;&#35748;&#30693;&#27169;&#24335;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08172</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#30693;&#35782;&#32467;&#26500;&#65306;&#19968;&#31181;&#25945;&#32946;&#35786;&#26029;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach. (arXiv:2310.08172v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25945;&#32946;&#35786;&#26029;&#35780;&#20272;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#32467;&#26500;&#65292;&#24378;&#35843;&#20102;&#30740;&#31350;LLMs&#30340;&#35748;&#30693;&#33021;&#21147;&#21644;&#19981;&#21516;&#35748;&#30693;&#27169;&#24335;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19981;&#20165;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#36824;&#23637;&#31034;&#20102;&#26234;&#33021;&#30340;&#28779;&#33457;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#35780;&#20272;&#23427;&#20204;&#22312;&#20154;&#31867;&#32771;&#35797;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;LLMs&#25972;&#20307;&#30693;&#35782;&#32467;&#26500;&#30340;&#35748;&#30693;&#30740;&#31350;&#20173;&#28982;&#32570;&#20047;&#12290;&#26412;&#25991;&#22522;&#20110;&#25945;&#32946;&#35786;&#26029;&#35780;&#20272;&#26041;&#27861;&#65292;&#22312;MoocRadar&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#26159;&#19968;&#20010;&#26681;&#25454;&#24067;&#40065;&#22982;&#20998;&#31867;&#27861;&#36827;&#34892;&#32454;&#33268;&#27880;&#37322;&#30340;&#20154;&#31867;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25581;&#31034;LLMs&#30340;&#30693;&#35782;&#32467;&#26500;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#35748;&#30693;&#33021;&#21147;&#36827;&#34892;&#28145;&#20837;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#35843;&#26597;LLMs&#30340;&#30693;&#35782;&#21644;&#29702;&#35299;&#20854;&#35748;&#30693;&#27169;&#24335;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#29031;&#20142;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#26356;&#21152;&#26126;&#30830;&#21644;&#26377;&#25928;&#22320;&#20419;&#36827;LLMs&#30340;&#24320;&#21457;&#21644;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have not only exhibited exceptional performance across various tasks, but also demonstrated sparks of intelligence. Recent studies have focused on assessing their capabilities on human exams and revealed their impressive competence in different domains. However, cognitive research on the overall knowledge structure of LLMs is still lacking. In this paper, based on educational diagnostic assessment method, we conduct an evaluation using MoocRadar, a meticulously annotated human test dataset based on Bloom Taxonomy. We aim to reveal the knowledge structures of LLMs and gain insights of their cognitive capabilities. This research emphasizes the significance of investigating LLMs' knowledge and understanding the disparate cognitive patterns of LLMs. By shedding light on models' knowledge, researchers can advance development and utilization of LLMs in a more informed and effective manner.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#35780;&#20272;&#25351;&#26631;&#65288;SLE&#65289;&#29992;&#20110;&#21477;&#23376;&#31616;&#21270;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#25351;&#26631;&#65292;&#23427;&#26356;&#19987;&#27880;&#20110;&#31616;&#27905;&#24615;&#65292;&#24182;&#22312;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.08170</link><description>&lt;p&gt;
&#31616;&#26126;&#31243;&#24230;&#20272;&#35745;&#65288;SLE&#65289;&#65306;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26080;&#21442;&#32771;&#24230;&#37327;&#26041;&#27861;&#29992;&#20110;&#21477;&#23376;&#31616;&#21270;
&lt;/p&gt;
&lt;p&gt;
Simplicity Level Estimate (SLE): A Learned Reference-Less Metric for Sentence Simplification. (arXiv:2310.08170v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08170
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#35780;&#20272;&#25351;&#26631;&#65288;SLE&#65289;&#29992;&#20110;&#21477;&#23376;&#31616;&#21270;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#25351;&#26631;&#65292;&#23427;&#26356;&#19987;&#27880;&#20110;&#31616;&#27905;&#24615;&#65292;&#24182;&#22312;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#31616;&#21270;&#30340;&#33258;&#21160;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#27969;&#34892;&#30340;&#35780;&#20272;&#26041;&#27861;&#38656;&#35201;&#22810;&#20010;&#39640;&#36136;&#37327;&#30340;&#21442;&#32771;&#25991;&#29486;&#65292;&#32780;&#36825;&#22312;&#31616;&#21270;&#36807;&#31243;&#20013;&#26159;&#19981;&#23481;&#26131;&#24471;&#21040;&#30340;&#65292;&#36825;&#20351;&#24471;&#22312;&#26410;&#30693;&#39046;&#22495;&#20013;&#27979;&#35797;&#24615;&#33021;&#21464;&#24471;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#23558;&#31616;&#21333;&#24615;&#19982;&#27969;&#30021;&#24615;&#25110;&#24847;&#20041;&#20445;&#25345;&#31561;&#30456;&#20851;&#23646;&#24615;&#28151;&#28102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#35780;&#20272;&#25351;&#26631;&#65288;SLE&#65289;&#65292;&#19987;&#27880;&#20110;&#31616;&#27905;&#24615;&#65292;&#22312;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#26041;&#38754;&#20248;&#20110;&#20960;&#20046;&#25152;&#26377;&#29616;&#26377;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic evaluation for sentence simplification remains a challenging problem. Most popular evaluation metrics require multiple high-quality references -- something not readily available for simplification -- which makes it difficult to test performance on unseen domains. Furthermore, most existing metrics conflate simplicity with correlated attributes such as fluency or meaning preservation. We propose a new learned evaluation metric (SLE) which focuses on simplicity, outperforming almost all existing metrics in terms of correlation with human judgements.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20351;&#29992;GPT 3.5&#21644;GPT 4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25919;&#31574;&#25991;&#20214;&#36827;&#34892;&#22810;&#31867;&#21035;&#20998;&#31867;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#20351;&#29992;&#22330;&#26223;&#65292;&#24182;&#20272;&#35745;&#20102;&#25972;&#20307;&#20934;&#30830;&#29575;&#22312;58&#65285;&#33267;83&#65285;&#20043;&#38388;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23436;&#20840;&#20381;&#36182;&#35821;&#35328;&#27169;&#22411;&#20173;&#23384;&#22312;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2310.08167</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25919;&#31574;&#25991;&#20214;&#30340;&#22810;&#31867;&#21035;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multiclass Classification of Policy Documents with Large Language Models. (arXiv:2310.08167v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08167
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20351;&#29992;GPT 3.5&#21644;GPT 4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25919;&#31574;&#25991;&#20214;&#36827;&#34892;&#22810;&#31867;&#21035;&#20998;&#31867;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#20351;&#29992;&#22330;&#26223;&#65292;&#24182;&#20272;&#35745;&#20102;&#25972;&#20307;&#20934;&#30830;&#29575;&#22312;58&#65285;&#33267;83&#65285;&#20043;&#38388;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23436;&#20840;&#20381;&#36182;&#35821;&#35328;&#27169;&#22411;&#20173;&#23384;&#22312;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#25919;&#31574;&#25991;&#20214;&#25353;&#29031;&#25919;&#31574;&#35758;&#39064;&#36827;&#34892;&#20998;&#31867;&#19968;&#30452;&#20197;&#26469;&#26159;&#25919;&#27835;&#31185;&#23398;&#21644;&#20256;&#25773;&#23398;&#39046;&#22495;&#30340;&#38271;&#26399;&#21162;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#30446;&#30340;&#30340;&#25991;&#26412;&#20998;&#31867;&#33258;&#21160;&#21270;&#22788;&#29702;&#65292;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#20294;&#20173;&#26377;&#24456;&#22823;&#30340;&#36827;&#23637;&#31354;&#38388;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#19968;&#31181;&#26367;&#20195;&#31574;&#30053;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#35813;&#31574;&#30053;&#38656;&#35201;&#27604;&#23436;&#20840;&#25163;&#21160;&#32534;&#30721;&#23569;&#24471;&#22810;&#30340;&#20154;&#24037;&#21442;&#19982;&#12290;&#25105;&#20204;&#20351;&#29992;OpenAI&#30340;GPT 3.5&#21644;GPT 4&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#32463;&#36807;&#39044;&#35757;&#32451;&#24182;&#38024;&#23545;&#25351;&#20196;&#36827;&#34892;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;&#22269;&#20250;&#35758;&#26696;&#21644;&#22269;&#20250;&#21548;&#35777;&#20250;&#20998;&#31867;&#20026;Comparative Agendas Project&#30340;21&#20010;&#20027;&#35201;&#25919;&#31574;&#35758;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#20351;&#29992;&#22330;&#26223;&#65292;&#24182;&#20272;&#35745;&#20102;&#26681;&#25454;&#25152;&#37319;&#29992;&#30340;&#22330;&#26223;&#21644;GPT&#27169;&#22411;&#30340;&#25972;&#20307;&#20934;&#30830;&#29575;&#22312;58&#65285;&#33267;83&#65285;&#20043;&#38388;&#30340;&#33539;&#22260;&#12290;&#36825;&#19977;&#31181;&#24773;&#26223;&#20998;&#21035;&#26088;&#22312;&#23454;&#29616;&#23545;&#20154;&#24037;&#24178;&#39044;&#30340;&#26368;&#23567;&#12289;&#20013;&#24230;&#21644;&#37325;&#35201;&#31243;&#24230;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#25351;&#21521;&#20102;&#23436;&#20840;&#20381;&#36182;GPT&#27169;&#22411;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classifying policy documents into policy issue topics has been a long-time effort in political science and communication disciplines. Efforts to automate text classification processes for social science research purposes have so far achieved remarkable results, but there is still a large room for progress. In this work, we test the prediction performance of an alternative strategy, which requires human involvement much less than full manual coding. We use the GPT 3.5 and GPT 4 models of the OpenAI, which are pre-trained instruction-tuned Large Language Models (LLM), to classify congressional bills and congressional hearings into Comparative Agendas Project's 21 major policy issue topics. We propose three use-case scenarios and estimate overall accuracies ranging from %58-83 depending on scenario and GPT model employed. The three scenarios aims at minimal, moderate, and major human interference, respectively. Overall, our results point towards the insufficiency of complete reliance on G
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Ziya-VL&#31995;&#21015;&#65292;&#36825;&#26159;&#19968;&#32452;&#21452;&#35821;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#23558;&#35270;&#35273;&#35821;&#20041;&#34701;&#20837;&#35821;&#35328;&#27169;&#22411;&#20197;&#36827;&#34892;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;&#27169;&#22411;&#37319;&#29992;&#20102;&#26597;&#35810;&#21464;&#25442;&#22120;&#21644;&#20248;&#21270;&#26041;&#26696;&#65292;&#22914;&#25351;&#20196;&#35843;&#25972;&#21644;&#22810;&#38454;&#27573;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#35270;&#35273;&#35821;&#35328;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2310.08166</link><description>&lt;p&gt;
Ziya-VL: &#21452;&#35821;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22810;&#20219;&#21153;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Ziya-VL: Bilingual Large Vision-Language Model via Multi-Task Instruction Tuning. (arXiv:2310.08166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Ziya-VL&#31995;&#21015;&#65292;&#36825;&#26159;&#19968;&#32452;&#21452;&#35821;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#23558;&#35270;&#35273;&#35821;&#20041;&#34701;&#20837;&#35821;&#35328;&#27169;&#22411;&#20197;&#36827;&#34892;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;&#27169;&#22411;&#37319;&#29992;&#20102;&#26597;&#35810;&#21464;&#25442;&#22120;&#21644;&#20248;&#21270;&#26041;&#26696;&#65292;&#22914;&#25351;&#20196;&#35843;&#25972;&#21644;&#22810;&#38454;&#27573;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#35270;&#35273;&#35821;&#35328;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36827;&#23637;&#25193;&#22823;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38646;&#23556;&#20987;&#22270;&#20687;&#21040;&#25991;&#26412;&#29983;&#25104;&#21644;&#29702;&#35299;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#27169;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#25104;&#21151;&#36890;&#24120;&#23616;&#38480;&#20110;&#33521;&#35821;&#22330;&#26223;&#65292;&#21407;&#22240;&#26159;&#32570;&#20047;&#22823;&#35268;&#27169;&#21644;&#39640;&#36136;&#37327;&#30340;&#38750;&#33521;&#35821;&#22810;&#27169;&#36164;&#28304;&#65292;&#20351;&#24471;&#22312;&#20854;&#20182;&#35821;&#35328;&#20013;&#24314;&#31435;&#31454;&#20105;&#23545;&#25163;&#21464;&#24471;&#26497;&#20854;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Ziya-VL&#31995;&#21015;&#65292;&#36825;&#26159;&#19968;&#32452;&#21452;&#35821;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#65292;&#26088;&#22312;&#23558;&#35270;&#35273;&#35821;&#20041;&#34701;&#20837;LLM&#20197;&#36827;&#34892;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;Ziya-VL-Base&#21644;Ziya-VL-Chat&#32452;&#25104;&#65292;&#37319;&#29992;BLIP-2&#20013;&#30340;&#26597;&#35810;&#21464;&#25442;&#22120;&#65292;&#24182;&#36827;&#19968;&#27493;&#25506;&#32034;&#25351;&#20196;&#35843;&#25972;&#12289;&#22810;&#38454;&#27573;&#35757;&#32451;&#21644;&#20302;&#31209;&#36866;&#24212;&#27169;&#22359;&#31561;&#20248;&#21270;&#26041;&#26696;&#30340;&#36741;&#21161;&#20316;&#29992;&#65292;&#20197;&#23454;&#29616;&#35270;&#35273;&#35821;&#35328;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21050;&#28608;GPT-4&#22312;&#22810;&#27169;&#24577;&#22330;&#26223;&#20013;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#23558;&#25105;&#20204;&#25910;&#38598;&#30340;&#33521;&#25991;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#38598;&#32763;&#35793;&#25104;...
&lt;/p&gt;
&lt;p&gt;
Recent advancements enlarge the capabilities of large language models (LLMs) in zero-shot image-to-text generation and understanding by integrating multi-modal inputs. However, such success is typically limited to English scenarios due to the lack of large-scale and high-quality non-English multi-modal resources, making it extremely difficult to establish competitive counterparts in other languages. In this paper, we introduce the Ziya-VL series, a set of bilingual large-scale vision-language models (LVLMs) designed to incorporate visual semantics into LLM for multi-modal dialogue. Composed of Ziya-VL-Base and Ziya-VL-Chat, our models adopt the Querying Transformer from BLIP-2, further exploring the assistance of optimization schemes such as instruction tuning, multi-stage training and low-rank adaptation module for visual-language alignment. In addition, we stimulate the understanding ability of GPT-4 in multi-modal scenarios, translating our gathered English image-text datasets into 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#26631;&#35760;&#31526;&#21495;&#30340;&#33258;&#22238;&#24402;Transformer&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#25351;&#23450;&#33539;&#22260;&#20869;&#30340;&#20013;&#38388;&#28608;&#27963;&#36880;&#27493;&#21387;&#32553;&#20026;&#32039;&#20945;&#24418;&#24335;&#65292;&#20174;&#32780;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#25991;&#26723;&#29983;&#25104;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#31232;&#30095;&#27880;&#24847;&#21147;&#22522;&#32447;&#20855;&#26377;&#26356;&#22909;&#30340;&#27969;&#30021;&#24230;&#12289;N-gram&#21305;&#37197;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08152</link><description>&lt;p&gt;
&#24102;&#26377;&#26631;&#35760;&#31526;&#21495;&#30340;&#33258;&#22238;&#24402;Transformer&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Context Compression for Auto-regressive Transformers with Sentinel Tokens. (arXiv:2310.08152v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#26631;&#35760;&#31526;&#21495;&#30340;&#33258;&#22238;&#24402;Transformer&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#25351;&#23450;&#33539;&#22260;&#20869;&#30340;&#20013;&#38388;&#28608;&#27963;&#36880;&#27493;&#21387;&#32553;&#20026;&#32039;&#20945;&#24418;&#24335;&#65292;&#20174;&#32780;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#25991;&#26723;&#29983;&#25104;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#31232;&#30095;&#27880;&#24847;&#21147;&#22522;&#32447;&#20855;&#26377;&#26356;&#22909;&#30340;&#27969;&#30021;&#24230;&#12289;N-gram&#21305;&#37197;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#20351;&#20854;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#36880;&#28176;&#25104;&#20026;&#22522;&#20110;Transformer&#30340;LLM&#30340;&#20027;&#35201;&#35745;&#31639;&#37096;&#20998;&#12290;&#27492;&#22806;&#65292;&#22788;&#29702;&#38271;&#36755;&#20837;&#26102;&#20135;&#29983;&#30340;&#36807;&#22810;&#30340;&#38190;&#20540;&#32531;&#23384;&#20063;&#20250;&#22312;&#20869;&#23384;&#21344;&#29992;&#21644;&#25512;&#29702;&#24310;&#36831;&#26041;&#38754;&#24102;&#26469;&#20005;&#37325;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#25351;&#23450;&#33539;&#22260;&#20869;&#30340;&#20013;&#38388;&#28608;&#27963;&#36880;&#27493;&#21387;&#32553;&#20026;&#32039;&#20945;&#24418;&#24335;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#21518;&#32493;&#19978;&#19979;&#25991;&#26102;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#39046;&#22495;&#20869;&#35821;&#35328;&#24314;&#27169;&#21644;&#38646;&#26679;&#26412;&#24320;&#25918;&#25991;&#26723;&#29983;&#25104;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27969;&#30021;&#24230;&#12289;N-gram&#21305;&#37197;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#26041;&#38754;&#20248;&#20110;&#31232;&#30095;&#27880;&#24847;&#21147;&#22522;&#32447;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#20102;&#19978;&#19979;&#25991;&#21387;&#32553;&#23545;&#31995;&#32479;&#25913;&#36827;&#30340;&#30410;&#22788;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/DRSY/KV_Compression&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quadratic complexity of the attention module makes it gradually become the bulk of compute in Transformer-based LLMs during generation. Moreover, the excessive key-value cache that arises when dealing with long inputs also brings severe issues on memory footprint and inference latency. In this work, we propose a plug-and-play approach that is able to incrementally compress the intermediate activation of a specified span of tokens into compact ones, thereby reducing both memory and computational cost when processing subsequent context. Experiments on both in-domain language modeling and zero-shot open-ended document generation demonstrate the advantage of our approach over sparse attention baselines in terms of fluency, n-gram matching, and semantic similarity. At last, we comprehensively profile the benefit of context compression on improving the system throughout. Code is available at https://github.com/DRSY/KV_Compression.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#33258;&#21160;&#29983;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#38899;&#32032;&#25345;&#32493;&#26102;&#38388;&#21464;&#24322;&#24615;&#19982;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#31639;&#27861;&#26469;&#25913;&#21892;&#21512;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#21322;&#30417;&#30563;&#35774;&#32622;&#19979;&#30340;ASR&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2310.08132</link><description>&lt;p&gt;
&#23545;&#20110;&#33258;&#21160;&#29983;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#38899;&#32032;&#25345;&#32493;&#26102;&#38388;&#21464;&#24322;&#24615;&#19982;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30456;&#20851;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Relevance of Phoneme Duration Variability of Synthesized Training Data for Automatic Speech Recognition. (arXiv:2310.08132v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#33258;&#21160;&#29983;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#38899;&#32032;&#25345;&#32493;&#26102;&#38388;&#21464;&#24322;&#24615;&#19982;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#31639;&#27861;&#26469;&#25913;&#21892;&#21512;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#21322;&#30417;&#30563;&#35774;&#32622;&#19979;&#30340;ASR&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#36716;&#35821;&#38899;(TTS)&#31995;&#32479;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#25913;&#21892;&#20302;&#36164;&#28304;&#25110;&#39046;&#22495;&#19981;&#21305;&#37197;&#20219;&#21153;&#20013;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#31995;&#32479;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;TTS&#29983;&#25104;&#30340;&#36755;&#20986;&#20173;&#28982;&#19981;&#20855;&#22791;&#19982;&#30495;&#23454;&#25968;&#25454;&#30456;&#21516;&#30340;&#36136;&#37327;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#21512;&#25104;&#25968;&#25454;&#30340;&#26102;&#38388;&#32467;&#26500;&#21450;&#20854;&#19982;ASR&#35757;&#32451;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;oracle&#35774;&#32622;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38750;&#33258;&#22238;&#24402;(NAR) TTS&#20013;&#25345;&#32493;&#26102;&#38388;&#24314;&#27169;&#23545;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#30340;&#36864;&#21270;&#31243;&#24230;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#24120;&#35265;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#21363;&#38544;&#39532;&#23572;&#21487;&#22827;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;(HMM-GMM)&#23545;&#40784;&#22120;&#21644;&#31070;&#32463;&#36830;&#32467;&#26102;&#24207;&#20998;&#31867;(CTC)&#23545;&#40784;&#22120;&#65292;&#26469;&#33719;&#21462;&#21442;&#32771;&#38899;&#32032;&#25345;&#32493;&#26102;&#38388;&#12290;&#36890;&#36807;&#19968;&#20010;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#31616;&#21333;&#31639;&#27861;&#65292;&#25105;&#20204;&#23558;TTS&#31995;&#32479;&#30340;&#38899;&#32032;&#25345;&#32493;&#26102;&#38388;&#20998;&#24067;&#31227;&#21160;&#21040;&#30495;&#23454;&#25345;&#32493;&#26102;&#38388;&#38468;&#36817;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#21322;&#30417;&#30563;&#35774;&#32622;&#19979;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#30340;ASR&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic data generated by text-to-speech (TTS) systems can be used to improve automatic speech recognition (ASR) systems in low-resource or domain mismatch tasks. It has been shown that TTS-generated outputs still do not have the same qualities as real data. In this work we focus on the temporal structure of synthetic data and its relation to ASR training. By using a novel oracle setup we show how much the degradation of synthetic data quality is influenced by duration modeling in non-autoregressive (NAR) TTS. To get reference phoneme durations we use two common alignment methods, a hidden Markov Gaussian-mixture model (HMM-GMM) aligner and a neural connectionist temporal classification (CTC) aligner. Using a simple algorithm based on random walks we shift phoneme duration distributions of the TTS system closer to real durations, resulting in an improvement of an ASR system using synthetic data in a semi-supervised setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#23545;&#35805;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#21508;&#21521;&#21516;&#24615;&#21644;&#36817;&#31471;&#25628;&#32034;&#65288;IPS&#65289;&#29983;&#25104;&#20449;&#24687;&#38598;&#20013;&#30340;&#35821;&#20041;&#22238;&#24212;&#65292;&#24182;&#22312;&#23545;&#35805;&#39046;&#22495;&#30340;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.08130</link><description>&lt;p&gt;
&#36890;&#36807;&#21508;&#21521;&#21516;&#24615;&#21644;&#36817;&#31471;&#25628;&#32034;&#23454;&#29616;&#32454;&#31890;&#24230;&#23545;&#35805;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Fine-grained Conversational Decoding via Isotropic and Proximal Search. (arXiv:2310.08130v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#23545;&#35805;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#21508;&#21521;&#21516;&#24615;&#21644;&#36817;&#31471;&#25628;&#32034;&#65288;IPS&#65289;&#29983;&#25104;&#20449;&#24687;&#38598;&#20013;&#30340;&#35821;&#20041;&#22238;&#24212;&#65292;&#24182;&#22312;&#23545;&#35805;&#39046;&#22495;&#30340;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#37319;&#29992;&#36890;&#29992;&#30340;&#25991;&#26412;&#35299;&#30721;&#26041;&#27861;&#26469;&#36827;&#34892;&#23545;&#35805;&#22238;&#24212;&#29983;&#25104;&#12290;&#34429;&#28982;&#37319;&#29992;&#20102;&#23545;&#35805;&#29305;&#23450;&#30340;&#32534;&#30721;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#30340;&#22238;&#24212;&#36136;&#37327;&#65292;&#20294;&#23545;&#35805;&#35299;&#30721;&#26041;&#27861;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#21463;&#21040;wu2023learning&#30340;&#21551;&#21457;&#65292;&#35748;&#20026;&#22909;&#30340;&#23545;&#35805;&#29305;&#24449;&#31354;&#38388;&#24212;&#36981;&#24490;&#23616;&#37096;&#24615;&#21644;&#21508;&#21521;&#21516;&#24615;&#35268;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#23545;&#35805;&#35299;&#30721;&#26041;&#27861;&#65292;&#31216;&#20026;&#21508;&#21521;&#21516;&#24615;&#21644;&#36817;&#31471;&#25628;&#32034;&#65288;IPS&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#29983;&#25104;&#20449;&#24687;&#38598;&#20013;&#30340;&#35821;&#20041;&#22238;&#24212;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#19978;&#19979;&#25991;&#30340;&#20449;&#24687;&#37327;&#21644;&#21306;&#20998;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23545;&#35805;&#39046;&#22495;&#20013;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#35299;&#30721;&#31574;&#30053;&#12290;&#26356;&#28145;&#20837;&#30340;&#20998;&#26512;&#36827;&#19968;&#27493;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
General-purpose text decoding approaches are usually adopted for dialogue response generation. Although the quality of the generated responses can be improved with dialogue-specific encoding methods, conversational decoding methods are still under-explored. Inspired by \citet{wu2023learning} that a good dialogue feature space should follow the rules of locality and isotropy, we present a fine-grained conversational decoding method, termed \textit{isotropic and proximal search (IPS)}. Our method is designed to generate the semantic-concentrated response, while still maintaining informativeness and discrimination against the context. Experiments show that our approach outperforms existing decoding strategies in the dialogue field across both automatic and human evaluation metrics. More in-depth analyses further confirm the effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PromptAV&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20316;&#32773;&#39564;&#35777;&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#25552;&#20379;&#36880;&#27493;&#30340;&#39118;&#26684;&#27979;&#37327;&#35299;&#37322;&#25552;&#31034;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;AV&#25216;&#26415;&#30340;&#38480;&#21046;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08123</link><description>&lt;p&gt;
&#35841;&#20889;&#30340;&#21644;&#20026;&#20160;&#20040;&#20889;&#30340;&#65311;&#25512;&#21160;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20316;&#32773;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Who Wrote it and Why? Prompting Large-Language Models for Authorship Verification. (arXiv:2310.08123v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PromptAV&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20316;&#32773;&#39564;&#35777;&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#25552;&#20379;&#36880;&#27493;&#30340;&#39118;&#26684;&#27979;&#37327;&#35299;&#37322;&#25552;&#31034;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;AV&#25216;&#26415;&#30340;&#38480;&#21046;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#39564;&#35777;&#65288;AV&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35745;&#31639;&#35821;&#35328;&#23398;&#20013;&#30340;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#65292;&#20854;&#24212;&#29992;&#39046;&#22495;&#21253;&#25324;&#27861;&#24237;&#20998;&#26512;&#12289;&#25220;&#34989;&#26816;&#27979;&#21644;&#35782;&#21035;&#27450;&#39575;&#24615;&#20869;&#23481;&#12290;&#29616;&#26377;&#30340;AV&#25216;&#26415;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;&#39118;&#26684;&#27979;&#37327;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#38656;&#27714;&#21644;&#35299;&#37322;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;PromptAV&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;AV&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#25552;&#20379;&#36880;&#27493;&#30340;&#39118;&#26684;&#27979;&#37327;&#35299;&#37322;&#25552;&#31034;&#12290;PromptAV&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#36816;&#34892;&#25928;&#26524;&#26174;&#33879;&#65292;&#24182;&#36890;&#36807;&#30452;&#35266;&#30340;&#35299;&#37322;&#22686;&#24378;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#20316;&#20026;AV&#20219;&#21153;&#30340;&#26377;&#25928;&#21644;&#21487;&#35299;&#37322;&#35299;&#20915;&#26041;&#26696;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Authorship verification (AV) is a fundamental task in natural language processing (NLP) and computational linguistics, with applications in forensic analysis, plagiarism detection, and identification of deceptive content. Existing AV techniques, including traditional stylometric and deep learning approaches, face limitations in terms of data requirements and lack of explainability. To address these limitations, this paper proposes PromptAV, a novel technique that leverages Large-Language Models (LLMs) for AV by providing step-by-step stylometric explanation prompts. PromptAV outperforms state-of-the-art baselines, operates effectively with limited training data, and enhances interpretability through intuitive explanations, showcasing its potential as an effective and interpretable solution for the AV task.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#26368;&#26032;&#30340;&#22768;&#38899;&#36716;&#25442;&#27169;&#22411;&#22312;&#21475;&#21507;&#12289;&#36328;&#35821;&#35328;&#12289;&#20048;&#22120;&#21644;&#25991;&#26412;&#25551;&#36848;&#22768;&#38899;&#31561;&#38750;&#26631;&#20934;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#21475;&#21507;&#21644;&#36328;&#35821;&#35328;&#22768;&#38899;&#36716;&#25442;&#26041;&#38754;&#65292;kNN-VC&#26041;&#27861;&#20445;&#25345;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#32780;&#20048;&#22120;&#21644;&#25991;&#26412;&#21040;&#22768;&#38899;&#30340;&#36716;&#25442;&#21017;&#26377;&#26356;&#22797;&#26434;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.08104</link><description>&lt;p&gt;
&#38024;&#23545;&#21475;&#21507;&#30340;&#35821;&#38899;&#12289;&#20048;&#22120;&#12289;&#26410;&#30693;&#35821;&#35328;&#21644;&#20197;&#25991;&#26412;&#25551;&#36848;&#30340;&#22768;&#38899;&#30340;&#22768;&#38899;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Voice Conversion for Stuttered Speech, Instruments, Unseen Languages and Textually Described Voices. (arXiv:2310.08104v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08104
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#26368;&#26032;&#30340;&#22768;&#38899;&#36716;&#25442;&#27169;&#22411;&#22312;&#21475;&#21507;&#12289;&#36328;&#35821;&#35328;&#12289;&#20048;&#22120;&#21644;&#25991;&#26412;&#25551;&#36848;&#22768;&#38899;&#31561;&#38750;&#26631;&#20934;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#21475;&#21507;&#21644;&#36328;&#35821;&#35328;&#22768;&#38899;&#36716;&#25442;&#26041;&#38754;&#65292;kNN-VC&#26041;&#27861;&#20445;&#25345;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#32780;&#20048;&#22120;&#21644;&#25991;&#26412;&#21040;&#22768;&#38899;&#30340;&#36716;&#25442;&#21017;&#26377;&#26356;&#22797;&#26434;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#38899;&#36716;&#25442;&#26088;&#22312;&#20351;&#29992;&#30446;&#26631;&#35762;&#35805;&#32773;&#30340;&#24405;&#38899;&#23558;&#28304;&#35821;&#38899;&#36716;&#25442;&#20026;&#30446;&#26631;&#22768;&#38899;&#12290;&#36739;&#26032;&#30340;&#27169;&#22411;&#20135;&#29983;&#20102;&#36234;&#26469;&#36234;&#36924;&#30495;&#30340;&#36755;&#20986;&#12290;&#20294;&#26159;&#65292;&#24403;&#27169;&#22411;&#25509;&#25910;&#38750;&#26631;&#20934;&#25968;&#25454;&#26102;&#65292;&#20363;&#22914;&#26469;&#33258;&#20855;&#26377;&#35821;&#35328;&#38556;&#30861;&#30340;&#29992;&#25143;&#30340;&#35821;&#38899;&#65292;&#20250;&#21457;&#29983;&#20160;&#20040;&#24773;&#20917;&#21602;&#65311;&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#26032;&#30340;&#22768;&#38899;&#36716;&#25442;&#27169;&#22411;&#22312;&#38750;&#26631;&#20934;&#19979;&#28216;&#22768;&#38899;&#36716;&#25442;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#20581;&#22766;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;k&#26368;&#36817;&#37051;&#22768;&#38899;&#36716;&#25442;&#65288;kNN-VC&#65289;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22235;&#20010;&#38750;&#26631;&#20934;&#30340;&#24212;&#29992;&#65306;&#21475;&#21507;&#30340;&#22768;&#38899;&#36716;&#25442;&#65292;&#36328;&#35821;&#35328;&#30340;&#22768;&#38899;&#36716;&#25442;&#65292;&#20048;&#22120;&#30340;&#22768;&#38899;&#36716;&#25442;&#21644;&#25991;&#26412;&#21040;&#22768;&#38899;&#30340;&#36716;&#25442;&#12290;&#21518;&#32773;&#28041;&#21450;&#23558;&#22768;&#38899;&#36716;&#25442;&#20026;&#36890;&#36807;&#25991;&#26412;&#25551;&#36848;&#25351;&#23450;&#30340;&#30446;&#26631;&#22768;&#38899;&#65292;&#20363;&#22914;&#8220;&#19968;&#20010;&#24180;&#36731;&#30007;&#23376;&#30340;&#23574;&#22768;&#8221;&#12290;&#19982;&#24050;&#24314;&#31435;&#30340;&#22522;&#20934;&#30456;&#27604;&#65292;&#25105;&#20204;&#21457;&#29616;kNN-VC&#22312;&#21475;&#21507;&#21644;&#36328;&#35821;&#35328;&#22768;&#38899;&#36716;&#25442;&#26041;&#38754;&#20445;&#25345;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;&#23545;&#20110;&#20048;&#22120;&#21644;&#25991;&#26412;&#21040;&#22768;&#38899;&#30340;&#36716;&#25442;&#32780;&#35328;&#65292;&#32467;&#26524;&#26356;&#21152;&#22797;&#26434;&#12290;
&lt;/p&gt;
&lt;p&gt;
Voice conversion aims to convert source speech into a target voice using recordings of the target speaker as a reference. Newer models are producing increasingly realistic output. But what happens when models are fed with non-standard data, such as speech from a user with a speech impairment? We investigate how a recent voice conversion model performs on non-standard downstream voice conversion tasks. We use a simple but robust approach called k-nearest neighbors voice conversion (kNN-VC). We look at four non-standard applications: stuttered voice conversion, cross-lingual voice conversion, musical instrument conversion, and text-to-voice conversion. The latter involves converting to a target voice specified through a text description, e.g. "a young man with a high-pitched voice". Compared to an established baseline, we find that kNN-VC retains high performance in stuttered and cross-lingual voice conversion. Results are more mixed for the musical instrument and text-to-voice conversio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#12298;&#20808;&#30693;&#20256;&#35760;&#12299;&#30340;&#23447;&#25945;&#39046;&#22495;&#38382;&#31572;&#31995;&#32479;(QASiNa)&#65292;&#26088;&#22312;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23447;&#25945;&#39046;&#22495;&#30340;&#24212;&#29992;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.08102</link><description>&lt;p&gt;
QASiNa: &#20351;&#29992;&#12298;&#20808;&#30693;&#20256;&#35760;&#12299;&#30340;&#23447;&#25945;&#39046;&#22495;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
QASiNa: Religious Domain Question Answering using Sirah Nabawiyah. (arXiv:2310.08102v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#12298;&#20808;&#30693;&#20256;&#35760;&#12299;&#30340;&#23447;&#25945;&#39046;&#22495;&#38382;&#31572;&#31995;&#32479;(QASiNa)&#65292;&#26088;&#22312;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23447;&#25945;&#39046;&#22495;&#30340;&#24212;&#29992;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#65292;&#38382;&#31572;&#65288;QA&#65289;&#20219;&#21153;&#21463;&#21040;&#37325;&#35270;&#65292;&#23588;&#20854;&#26159;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;Chat GPT&#30340;&#21457;&#23637;&#12290;LLM&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#20294;&#23558;&#20854;&#24212;&#29992;&#20110;&#20234;&#26031;&#20848;&#39046;&#22495;&#26102;&#65292;&#19982;&#20449;&#24687;&#20256;&#36882;&#21407;&#21017;&#30456;&#30683;&#30462;&#12290;&#22312;&#20234;&#26031;&#20848;&#25945;&#20013;&#65292;&#25105;&#20204;&#20005;&#26684;&#35268;&#23450;&#20449;&#24687;&#26469;&#28304;&#21644;&#35841;&#21487;&#20197;&#23545;&#35813;&#26469;&#28304;&#36827;&#34892;&#35299;&#37322;&#25110;&#27880;&#37322;&#12290;LLM&#30340;&#22238;&#31572;&#29983;&#25104;&#26041;&#27861;&#19982;&#27880;&#37322;&#27010;&#24565;&#31867;&#20284;&#65292;LLM&#26082;&#19981;&#26159;&#20234;&#26031;&#20848;&#19987;&#23478;&#65292;&#20063;&#19981;&#26159;&#20154;&#31867;&#65292;&#36825;&#22312;&#20234;&#26031;&#20848;&#25945;&#20013;&#26159;&#19981;&#20801;&#35768;&#30340;&#12290;&#21360;&#24230;&#23612;&#35199;&#20122;&#26159;&#19990;&#30028;&#19978;&#20449;&#22857;&#20234;&#26031;&#20848;&#25945;&#20154;&#21475;&#26368;&#22810;&#30340;&#22269;&#23478;&#12290;&#22312;LLM&#30340;&#39640;&#24433;&#21709;&#19979;&#65292;&#25105;&#20204;&#38656;&#35201;&#23545;&#23447;&#25945;&#39046;&#22495;&#30340;LLM&#36827;&#34892;&#35780;&#20272;&#12290;&#30446;&#21069;&#65292;&#21482;&#26377;&#23569;&#25968;&#23447;&#25945;&#38382;&#31572;&#25968;&#25454;&#38598;&#21487;&#29992;&#65292;&#20854;&#20013;&#27809;&#26377;&#19968;&#20010;&#20351;&#29992;&#23588;&#23612;&#35821;&#30340;&#12298;&#20808;&#30693;&#20256;&#35760;&#12299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#12298;&#20808;&#30693;&#20256;&#35760;&#12299;&#30340;&#38382;&#31572;&#31995;&#32479;(QASiNa)&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, Question Answering (QA) tasks receive significant research focus, particularly with the development of Large Language Model (LLM) such as Chat GPT [1]. LLM can be applied to various domains, but it contradicts the principles of information transmission when applied to the Islamic domain. In Islam we strictly regulates the sources of information and who can give interpretations or tafseer for that sources [2]. The approach used by LLM to generate answers based on its own interpretation is similar to the concept of tafseer, LLM is neither an Islamic expert nor a human which is not permitted in Islam. Indonesia is the country with the largest Islamic believer population in the world [3]. With the high influence of LLM, we need to make evaluation of LLM in religious domain. Currently, there is only few religious QA dataset available and none of them using Sirah Nabawiyah especially in Indonesian Language. In this paper, we propose the Question Answering Sirah Nabawiyah (QASiNa) d
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Promptor&#65292;&#19968;&#20010;&#29992;&#20110;&#26234;&#33021;&#25991;&#26412;&#36755;&#20837;&#25216;&#26415;&#30340;&#23545;&#35805;&#24335;&#33258;&#20027;&#25552;&#31034;&#29983;&#25104;&#20195;&#29702;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#21487;&#20197;&#20811;&#26381;&#25968;&#25454;&#25910;&#38598;&#21644;&#27169;&#22411;&#24494;&#35843;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;GPT-3.5&#20026;&#20363;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20165;&#36890;&#36807;&#25552;&#31034;&#21363;&#21487;&#36229;&#36807;GPT-2&#25903;&#25345;&#30340;&#31995;&#32479;&#65292;&#24182;&#19988;&#21487;&#19982;&#32463;&#36807;&#31934;&#35843;&#30340;GPT-3.5&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2310.08101</link><description>&lt;p&gt;
Promptor:&#19968;&#31181;&#29992;&#20110;&#26234;&#33021;&#25991;&#26412;&#36755;&#20837;&#25216;&#26415;&#30340;&#23545;&#35805;&#24335;&#33258;&#20027;&#25552;&#31034;&#29983;&#25104;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Promptor: A Conversational and Autonomous Prompt Generation Agent for Intelligent Text Entry Techniques. (arXiv:2310.08101v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Promptor&#65292;&#19968;&#20010;&#29992;&#20110;&#26234;&#33021;&#25991;&#26412;&#36755;&#20837;&#25216;&#26415;&#30340;&#23545;&#35805;&#24335;&#33258;&#20027;&#25552;&#31034;&#29983;&#25104;&#20195;&#29702;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#21487;&#20197;&#20811;&#26381;&#25968;&#25454;&#25910;&#38598;&#21644;&#27169;&#22411;&#24494;&#35843;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;GPT-3.5&#20026;&#20363;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20165;&#36890;&#36807;&#25552;&#31034;&#21363;&#21487;&#36229;&#36807;GPT-2&#25903;&#25345;&#30340;&#31995;&#32479;&#65292;&#24182;&#19988;&#21487;&#19982;&#32463;&#36807;&#31934;&#35843;&#30340;GPT-3.5&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#26085;&#24120;&#25968;&#23383;&#20132;&#20114;&#20013;&#65292;&#25991;&#26412;&#36755;&#20837;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#20351;&#25991;&#26412;&#36755;&#20837;&#26356;&#26377;&#25928;&#12289;&#39640;&#25928;&#21644;&#27969;&#30021;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#35768;&#22810;&#26234;&#33021;&#21151;&#33021;&#65292;&#21253;&#25324;&#21477;&#23376;&#39044;&#27979;&#21644;&#29992;&#25143;&#20010;&#24615;&#21270;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#36825;&#20123;&#39640;&#32423;&#21151;&#33021;&#30340;&#24120;&#35268;&#65292;&#25968;&#25454;&#25910;&#38598;&#21644;&#27169;&#22411;&#24494;&#35843;&#30340;&#24517;&#35201;&#24615;&#20063;&#22686;&#21152;&#20102;&#12290;&#21033;&#29992;GPT-3.5&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#21487;&#20197;&#20943;&#36731;&#36825;&#20123;&#25361;&#25112;&#12290;&#36825;&#19968;&#29420;&#29305;&#30340;&#29305;&#24615;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#25552;&#31034;&#33719;&#24471;&#26032;&#30340;&#25216;&#33021;&#65292;&#28040;&#38500;&#20102;&#25968;&#25454;&#25910;&#38598;&#21644;&#24494;&#35843;&#30340;&#38656;&#35201;&#12290;&#22240;&#27492;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21508;&#31181;&#25991;&#26412;&#39044;&#27979;&#25216;&#26415;&#12290;&#25105;&#20204;&#26368;&#21021;&#23637;&#31034;&#20102;&#20165;&#36890;&#36807;&#25552;&#31034;GPT-3.5&#21363;&#21487;&#36229;&#36807;GPT-2&#25903;&#25345;&#30340;&#31995;&#32479;&#65292;&#24182;&#19988;&#19982;&#32463;&#36807;&#31934;&#35843;&#30340;GPT-3.5&#27169;&#22411;&#30456;&#24403;&#65292;&#22312;&#21518;&#20004;&#31181;&#26041;&#27861;&#38656;&#35201;&#26114;&#36149;&#30340;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text entry is an essential task in our day-to-day digital interactions. Numerous intelligent features have been developed to streamline this process, making text entry more effective, efficient, and fluid. These improvements include sentence prediction and user personalization. However, as deep learning-based language models become the norm for these advanced features, the necessity for data collection and model fine-tuning increases. These challenges can be mitigated by harnessing the in-context learning capability of large language models such as GPT-3.5. This unique feature allows the language model to acquire new skills through prompts, eliminating the need for data collection and fine-tuning. Consequently, large language models can learn various text prediction techniques. We initially showed that, for a sentence prediction task, merely prompting GPT-3.5 surpassed a GPT-2 backed system and is comparable with a fine-tuned GPT-3.5 model, with the latter two methods requiring costly 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#19978;&#20851;&#20110;&#27668;&#20505;&#21464;&#21270;&#30340;&#25512;&#25991;&#24773;&#24863;&#24577;&#24230;&#65292;&#36890;&#36807;&#20351;&#29992;ClimateBERT&#27169;&#22411;&#37327;&#21270;&#24773;&#24863;&#65292;&#20174;&#32780;&#33719;&#24471;&#26377;&#20851;&#20844;&#20247;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#35266;&#28857;&#21644;&#21453;&#39304;&#12290;</title><link>http://arxiv.org/abs/2310.08099</link><description>&lt;p&gt;
ClimateNLP: &#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20998;&#26512;&#20844;&#20247;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#24773;&#24863;&#24577;&#24230;
&lt;/p&gt;
&lt;p&gt;
ClimateNLP: Analyzing Public Sentiment Towards Climate Change Using Natural Language Processing. (arXiv:2310.08099v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#19978;&#20851;&#20110;&#27668;&#20505;&#21464;&#21270;&#30340;&#25512;&#25991;&#24773;&#24863;&#24577;&#24230;&#65292;&#36890;&#36807;&#20351;&#29992;ClimateBERT&#27169;&#22411;&#37327;&#21270;&#24773;&#24863;&#65292;&#20174;&#32780;&#33719;&#24471;&#26377;&#20851;&#20844;&#20247;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#35266;&#28857;&#21644;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#21464;&#21270;&#23545;&#20154;&#31867;&#20581;&#24247;&#30340;&#24433;&#21709;&#24102;&#26469;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25361;&#25112;&#12290;&#38500;&#38750;&#37319;&#21462;&#22522;&#20110;&#30830;&#20991;&#35777;&#25454;&#30340;&#31215;&#26497;&#25514;&#26045;&#65292;&#21542;&#21017;&#36825;&#20123;&#23041;&#32961;&#24456;&#21487;&#33021;&#20250;&#21319;&#32423;&#65292;&#24182;&#32487;&#32493;&#23041;&#32961;&#20154;&#31867;&#31119;&#31049;&#12290;&#20449;&#24687;&#21644;&#36890;&#20449;&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#24050;&#32463;&#20419;&#36827;&#20102;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#24191;&#27867;&#21487;&#29992;&#24615;&#21644;&#21033;&#29992;&#29575;&#12290;&#20010;&#20154;&#21033;&#29992;Twitter&#21644;Facebook&#31561;&#24179;&#21488;&#34920;&#36798;&#33258;&#24049;&#23545;&#21508;&#31181;&#20027;&#39064;&#30340;&#24847;&#35265;&#12289;&#24819;&#27861;&#21644;&#35780;&#35770;&#65292;&#21253;&#25324;&#32039;&#36843;&#30340;&#27668;&#20505;&#21464;&#21270;&#38382;&#39064;&#12290;&#31038;&#20132;&#23186;&#20307;&#19978;&#19982;&#27668;&#20505;&#21464;&#21270;&#30456;&#20851;&#20869;&#23481;&#30340;&#28608;&#22686;&#38656;&#35201;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#20197;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#27934;&#23519;&#12290;&#26412;&#35770;&#25991;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20998;&#26512;&#27668;&#20505;&#21464;&#21270;&#35805;&#35821;&#65292;&#24182;&#37327;&#21270;&#19982;&#27668;&#20505;&#21464;&#21270;&#30456;&#20851;&#30340;&#25512;&#25991;&#30340;&#24773;&#24863;&#12290;&#25105;&#20204;&#20351;&#29992;ClimateBERT&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#27668;&#20505;&#21464;&#21270;&#39046;&#22495;&#36827;&#34892;&#20102;&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#30446;&#26631;&#26159;&#35782;&#21035;&#24773;&#24863;&#24577;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Climate change's impact on human health poses unprecedented and diverse challenges. Unless proactive measures based on solid evidence are implemented, these threats will likely escalate and continue to endanger human well-being. The escalating advancements in information and communication technologies have facilitated the widespread availability and utilization of social media platforms. Individuals utilize platforms such as Twitter and Facebook to express their opinions, thoughts, and critiques on diverse subjects, encompassing the pressing issue of climate change. The proliferation of climate change-related content on social media necessitates comprehensive analysis to glean meaningful insights. This paper employs natural language processing (NLP) techniques to analyze climate change discourse and quantify the sentiment of climate change-related tweets. We use ClimateBERT, a pretrained model fine-tuned specifically for the climate change domain. The objective is to discern the sentim
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#20197;&#38382;&#31572;&#26041;&#24335;&#32763;&#35793;&#21360;&#23612;&#35821;&#20302;&#36164;&#28304;&#26631;&#39064;&#20826;&#22788;&#29702;&#30340;&#20219;&#21153;&#65292;&#36129;&#29486;&#21253;&#25324;&#26500;&#24314;&#25163;&#21160;&#26631;&#27880;&#30340;&#21360;&#23612;&#35821;&#26631;&#39064;&#20826;&#22788;&#29702;&#35821;&#26009;&#24211;&#65292;&#24182;&#35780;&#20272;&#20102;&#36328;&#35821;&#35328;&#38646;&#23556;&#20987;&#38382;&#31572;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;XLM-RoBERTa&#65288;&#22823;&#65289;&#27169;&#22411;&#22312;&#30701;&#35821;&#21644;&#27573;&#33853;&#26631;&#39064;&#20826;&#22788;&#29702;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#32780;mDeBERTa&#65288;&#22522;&#30784;&#65289;&#27169;&#22411;&#22312;&#22810;&#37096;&#20998;&#26631;&#39064;&#20826;&#22788;&#29702;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2310.08085</link><description>&lt;p&gt;
&#20197;&#38382;&#31572;&#26041;&#24335;&#32763;&#35793;&#21360;&#23612;&#35821;&#30340;&#20302;&#36164;&#28304;&#26631;&#39064;&#20826;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Low-Resource Clickbait Spoiling for Indonesian via Question Answering. (arXiv:2310.08085v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#20197;&#38382;&#31572;&#26041;&#24335;&#32763;&#35793;&#21360;&#23612;&#35821;&#20302;&#36164;&#28304;&#26631;&#39064;&#20826;&#22788;&#29702;&#30340;&#20219;&#21153;&#65292;&#36129;&#29486;&#21253;&#25324;&#26500;&#24314;&#25163;&#21160;&#26631;&#27880;&#30340;&#21360;&#23612;&#35821;&#26631;&#39064;&#20826;&#22788;&#29702;&#35821;&#26009;&#24211;&#65292;&#24182;&#35780;&#20272;&#20102;&#36328;&#35821;&#35328;&#38646;&#23556;&#20987;&#38382;&#31572;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;XLM-RoBERTa&#65288;&#22823;&#65289;&#27169;&#22411;&#22312;&#30701;&#35821;&#21644;&#27573;&#33853;&#26631;&#39064;&#20826;&#22788;&#29702;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#32780;mDeBERTa&#65288;&#22522;&#30784;&#65289;&#27169;&#22411;&#22312;&#22810;&#37096;&#20998;&#26631;&#39064;&#20826;&#22788;&#29702;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#39064;&#20826;&#22788;&#29702;&#26088;&#22312;&#29983;&#25104;&#19968;&#20010;&#30701;&#25991;&#26412;&#65292;&#20197;&#28385;&#36275;&#26631;&#39064;&#20826;&#24086;&#23376;&#24341;&#36215;&#30340;&#22909;&#22855;&#24515;&#12290;&#30001;&#20110;&#36825;&#26159;&#19968;&#20010;&#26032;&#24341;&#20837;&#30340;&#20219;&#21153;&#65292;&#30446;&#21069;&#21482;&#26377;&#33521;&#25991;&#25968;&#25454;&#38598;&#21487;&#29992;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#22312;&#21360;&#23612;&#35821;&#20013;&#26500;&#24314;&#25163;&#21160;&#26631;&#27880;&#30340;&#26631;&#39064;&#20826;&#22788;&#29702;&#35821;&#26009;&#24211;&#65292;&#24182;&#35780;&#20272;&#20351;&#29992;&#36328;&#35821;&#35328;&#38646;&#23556;&#20987;&#38382;&#31572;&#27169;&#22411;&#26469;&#22788;&#29702;&#21360;&#23612;&#35821;&#31561;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#26631;&#39064;&#20826;&#22788;&#29702;&#12290;&#25105;&#20204;&#21033;&#29992;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#36873;&#25321;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;XLM-RoBERTa&#65288;&#22823;&#65289;&#27169;&#22411;&#22312;&#30701;&#35821;&#21644;&#27573;&#33853;&#26631;&#39064;&#20826;&#22788;&#29702;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#32780;mDeBERTa&#65288;&#22522;&#30784;&#65289;&#27169;&#22411;&#22312;&#22810;&#37096;&#20998;&#26631;&#39064;&#20826;&#22788;&#29702;&#20013;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clickbait spoiling aims to generate a short text to satisfy the curiosity induced by a clickbait post. As it is a newly introduced task, the dataset is only available in English so far. Our contributions include the construction of manually labeled clickbait spoiling corpus in Indonesian and an evaluation on using cross-lingual zero-shot question answering-based models to tackle clikcbait spoiling for low-resource language like Indonesian. We utilize selection of multilingual language models. The experimental results suggest that XLM-RoBERTa (large) model outperforms other models for phrase and passage spoilers, meanwhile, mDeBERTa (base) model outperforms other models for multipart spoilers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36827;&#34892;&#20102;&#38024;&#23545;&#36328;&#35821;&#35328;&#36716;&#25442;&#30340;&#25991;&#26412;&#34920;&#31034;&#26041;&#26696;&#30340;&#27604;&#36739;&#30740;&#31350;&#65292;&#21457;&#29616;&#22270;&#20687;&#27169;&#22411;&#22312;&#30456;&#20851;&#19988;&#33050;&#26412;&#30456;&#20284;&#30340;&#35821;&#35328;&#20043;&#38388;&#30340;&#36716;&#25442;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#32780;&#22522;&#20110;&#20998;&#21106;&#30340;&#27169;&#22411;&#22312;&#20559;&#21521;&#21333;&#35789;&#21547;&#20041;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.08078</link><description>&lt;p&gt;
&#26159;&#21542;&#36827;&#34892;&#35789;&#20803;&#21270;&#65306;&#29992;&#20110;&#36328;&#35821;&#35328;&#36716;&#25442;&#30340;&#25991;&#26412;&#34920;&#31034;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
To token or not to token: A Comparative Study of Text Representations for Cross-Lingual Transfer. (arXiv:2310.08078v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36827;&#34892;&#20102;&#38024;&#23545;&#36328;&#35821;&#35328;&#36716;&#25442;&#30340;&#25991;&#26412;&#34920;&#31034;&#26041;&#26696;&#30340;&#27604;&#36739;&#30740;&#31350;&#65292;&#21457;&#29616;&#22270;&#20687;&#27169;&#22411;&#22312;&#30456;&#20851;&#19988;&#33050;&#26412;&#30456;&#20284;&#30340;&#35821;&#35328;&#20043;&#38388;&#30340;&#36716;&#25442;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#32780;&#22522;&#20110;&#20998;&#21106;&#30340;&#27169;&#22411;&#22312;&#20559;&#21521;&#21333;&#35789;&#21547;&#20041;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#36328;&#35821;&#35328;&#36716;&#25442;&#20013;&#65292;&#36873;&#25321;&#36866;&#24403;&#30340;&#35789;&#20803;&#21270;&#26041;&#26696;&#24448;&#24448;&#26159;&#19968;&#20010;&#29942;&#39048;&#12290;&#20026;&#20102;&#29702;&#35299;&#25991;&#26412;&#34920;&#31034;&#36873;&#25321;&#30340;&#19979;&#28216;&#24433;&#21709;&#65292;&#25105;&#20204;&#23545;&#20855;&#26377;&#19981;&#21516;&#25991;&#26412;&#34920;&#31034;&#27169;&#24577;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#21253;&#25324;2&#20010;&#22522;&#20110;&#20998;&#21106;&#30340;&#27169;&#22411;&#65288;BERT&#65292;mBERT&#65289;&#65292;1&#20010;&#22522;&#20110;&#22270;&#20687;&#30340;&#27169;&#22411;&#65288;PIXEL&#65289;&#65292;&#21644;1&#20010;&#23383;&#31526;&#32423;&#27169;&#22411;&#65288;CANINE&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20998;&#35821;&#35328;&#21830;&#25968;&#65288;LQ&#65289;&#25351;&#26631;&#65292;&#33021;&#22815;&#25552;&#20379;&#38646;&#23556;&#20987;&#21644;&#23569;&#23556;&#20987;&#35780;&#20272;&#30340;&#21152;&#26435;&#34920;&#31034;&#12290;&#21033;&#29992;&#36825;&#20010;&#25351;&#26631;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#20219;&#21153;&#65288;&#35789;&#24615;&#26631;&#27880;&#65292;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65289;&#19978;&#36827;&#34892;&#20102;&#21253;&#21547;19&#20010;&#28304;&#35821;&#35328;&#21644;133&#20010;&#30446;&#26631;&#35821;&#35328;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24403;&#35821;&#35328;&#20043;&#38388;&#20851;&#31995;&#23494;&#20999;&#19988;&#20855;&#26377;&#30456;&#20284;&#30340;&#35270;&#35273;&#33050;&#26412;&#26102;&#65292;&#22522;&#20110;&#22270;&#20687;&#30340;&#27169;&#22411;&#22312;&#36328;&#35821;&#35328;&#36716;&#25442;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20559;&#21521;&#20110;&#21333;&#35789;&#21547;&#20041;&#30340;&#20219;&#21153;&#65288;&#35789;&#24615;&#26631;&#27880;&#65292;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65289;&#65292;&#22522;&#20110;&#20998;&#21106;&#30340;&#27169;&#22411;&#35777;&#26126;&#26159;&#26356;&#22909;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Choosing an appropriate tokenization scheme is often a bottleneck in low-resource cross-lingual transfer. To understand the downstream implications of text representation choices, we perform a comparative analysis on language models having diverse text representation modalities including 2 segmentation-based models (\texttt{BERT}, \texttt{mBERT}), 1 image-based model (\texttt{PIXEL}), and 1 character-level model (\texttt{CANINE}). First, we propose a scoring Language Quotient (LQ) metric capable of providing a weighted representation of both zero-shot and few-shot evaluation combined. Utilizing this metric, we perform experiments comprising 19 source languages and 133 target languages on three tasks (POS tagging, Dependency parsing, and NER). Our analysis reveals that image-based models excel in cross-lingual transfer when languages are closely related and share visually similar scripts. However, for tasks biased toward word meaning (POS, NER), segmentation-based models prove to be sup
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#35757;&#32451;&#38382;&#31572;&#31995;&#32479;&#30340;&#31616;&#21333;&#19988;&#32463;&#27982;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#35843;&#25972;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#36991;&#20813;&#20102;&#20154;&#21147;&#25104;&#26412;&#65292;&#24182;&#23637;&#31034;&#20102;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#19982;&#25163;&#21160;&#25972;&#29702;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#30340;&#21487;&#27604;&#36739;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08072</link><description>&lt;p&gt;
&#20174;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#20013;&#35757;&#32451;&#29983;&#25104;&#24335;&#38382;&#31572;&#31995;&#32479;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Training Generative Question-Answering on Synthetic Data Obtained from an Instruct-tuned Mo. (arXiv:2310.08072v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#35757;&#32451;&#38382;&#31572;&#31995;&#32479;&#30340;&#31616;&#21333;&#19988;&#32463;&#27982;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#35843;&#25972;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#36991;&#20813;&#20102;&#20154;&#21147;&#25104;&#26412;&#65292;&#24182;&#23637;&#31034;&#20102;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#19982;&#25163;&#21160;&#25972;&#29702;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#30340;&#21487;&#27604;&#36739;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#32463;&#27982;&#30340;&#26041;&#27861;&#26469;&#21512;&#25104;&#25968;&#25454;&#20197;&#35757;&#32451;&#38382;&#31572;&#31995;&#32479;&#12290;&#22312;&#36164;&#28304;&#20805;&#36275;&#30340;&#33521;&#35821;&#31561;&#35821;&#35328;&#20013;&#65292;&#35843;&#25972;GPT&#27169;&#22411;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20294;&#26159;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#30001;&#20110;&#32570;&#20047;&#36275;&#22815;&#30340;&#38382;&#31572;&#23545;&#65292;&#36825;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20351;&#29992;&#22312;&#20154;&#31867;&#20316;&#32773;&#30340;&#38382;&#31572;&#23545;&#19978;&#35757;&#32451;&#30340;&#38382;&#31572;&#29983;&#25104;&#27169;&#22411;&#65292;&#36825;&#28041;&#21450;&#30456;&#24403;&#22823;&#30340;&#20154;&#21147;&#25104;&#26412;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#35843;&#25972;&#25351;&#23548;&#27169;&#22411;&#20197;&#38646;&#26679;&#26412;&#25110;&#23569;&#37327;&#26679;&#26412;&#30340;&#26041;&#24335;&#29983;&#25104;&#38382;&#31572;&#23545;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#20174;&#25351;&#23548;&#27169;&#22411;&#33719;&#21462;&#38382;&#31572;&#23545;&#30340;&#21508;&#31181;&#31574;&#30053;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#19982;&#22312;&#25163;&#21160;&#25972;&#29702;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#24403;&#65292;&#32780;&#26080;&#38656;&#25215;&#25285;&#20154;&#21147;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a simple and cost-effective method for synthesizing data to train question-answering systems. For training, fine-tuning GPT models is a common practice in resource-rich languages like English, however, it becomes challenging for non-English languages due to the scarcity of sufficient question-answer (QA) pairs. Existing approaches use question and answer generators trained on human-authored QA pairs, which involves substantial human expenses. In contrast, we use an instruct-tuned model to generate QA pairs in a zero-shot or few-shot manner. We conduct experiments to compare various strategies for obtaining QA pairs from the instruct-tuned model. The results demonstrate that a model trained on our proposed synthetic data achieves comparable performance to a model trained on manually curated datasets, without incurring human costs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;Soft-InfoNCE&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#22312;InfoNCE&#20013;&#25554;&#20837;&#26435;&#37325;&#39033;&#26469;&#35299;&#20915;&#20195;&#30721;&#25628;&#32034;&#20013;&#36127;&#26679;&#26412;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#22823;&#22411;&#20195;&#30721;&#24211;&#20013;&#30340;&#34394;&#20551;&#36127;&#26679;&#26412;&#21644;&#26410;&#33021;&#21306;&#20998;&#36127;&#26679;&#26412;&#30340;&#28508;&#22312;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08069</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20195;&#30721;&#25628;&#32034;&#20013;&#30340;&#36127;&#26679;&#26412;&#23545;
&lt;/p&gt;
&lt;p&gt;
Rethinking Negative Pairs in Code Search. (arXiv:2310.08069v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;Soft-InfoNCE&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#22312;InfoNCE&#20013;&#25554;&#20837;&#26435;&#37325;&#39033;&#26469;&#35299;&#20915;&#20195;&#30721;&#25628;&#32034;&#20013;&#36127;&#26679;&#26412;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#22823;&#22411;&#20195;&#30721;&#24211;&#20013;&#30340;&#34394;&#20551;&#36127;&#26679;&#26412;&#21644;&#26410;&#33021;&#21306;&#20998;&#36127;&#26679;&#26412;&#30340;&#28508;&#22312;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#27604;&#23398;&#20064;&#25104;&#20026;&#32454;&#21270;&#20195;&#30721;&#25628;&#32034;&#27169;&#22411;&#20197;&#25552;&#39640;&#36719;&#20214;&#24320;&#21457;&#25928;&#29575;&#21644;&#25928;&#26524;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#23427;&#23558;&#27491;&#26679;&#26412;&#20195;&#30721;&#29255;&#27573;&#32858;&#38598;&#22312;&#19968;&#36215;&#65292;&#21516;&#26102;&#23558;&#19982;&#25628;&#32034;&#26597;&#35810;&#19981;&#30456;&#20851;&#30340;&#36127;&#26679;&#26412;&#25512;&#24320;&#12290;&#22312;&#23545;&#27604;&#23398;&#20064;&#20013;&#65292;InfoNCE&#26159;&#26368;&#24120;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;InfoNCE&#36127;&#26679;&#26412;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#21487;&#33021;&#20250;&#25439;&#23475;&#20854;&#34920;&#31034;&#23398;&#20064;&#30340;&#25928;&#26524;&#65306;1&#65289;&#30001;&#20110;&#37325;&#22797;&#65292;&#22823;&#22411;&#20195;&#30721;&#24211;&#20013;&#23384;&#22312;&#34394;&#20551;&#36127;&#26679;&#26412;&#12290;2&#65289;&#26410;&#33021;&#26126;&#30830;&#21306;&#20998;&#36127;&#26679;&#26412;&#30340;&#28508;&#22312;&#30456;&#20851;&#24615;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#24555;&#36895;&#25490;&#24207;&#31639;&#27861;&#26597;&#35810;&#65292;&#20882;&#27873;&#25490;&#24207;&#31639;&#27861;&#31034;&#20363;&#35201;&#27604;&#25991;&#20214;&#20445;&#23384;&#20989;&#25968;&#8220;&#26356;&#36127;&#38754;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;Soft-InfoNCE&#25439;&#22833;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19977;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#26435;&#37325;...
&lt;/p&gt;
&lt;p&gt;
Recently, contrastive learning has become a key component in fine-tuning code search models for software development efficiency and effectiveness. It pulls together positive code snippets while pushing negative samples away given search queries. Among contrastive learning, InfoNCE is the most widely used loss function due to its better performance. However, the following problems in negative samples of InfoNCE may deteriorate its representation learning: 1) The existence of false negative samples in large code corpora due to duplications. 2). The failure to explicitly differentiate between the potential relevance of negative samples. As an example, a bubble sorting algorithm example is less ``negative'' than a file saving function for the quick sorting algorithm query. In this paper, we tackle the above problems by proposing a simple yet effective Soft-InfoNCE loss that inserts weight terms into InfoNCE. In our proposed loss function, we apply three methods to estimate the weights of n
&lt;/p&gt;</description></item><item><title>QLLM&#26159;&#19968;&#31181;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#20934;&#30830;&#39640;&#25928;&#30340;&#20302;&#20301;&#23485;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#36890;&#36947;&#37325;&#32452;&#25216;&#26415;&#65292;&#23558;&#31163;&#32676;&#20540;&#30340;&#22823;&#23567;&#37325;&#26032;&#20998;&#37197;&#32473;&#20854;&#20182;&#36890;&#36947;&#65292;&#20174;&#32780;&#20943;&#36731;&#23427;&#20204;&#23545;&#37327;&#21270;&#33539;&#22260;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.08041</link><description>&lt;p&gt;
QLLM: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#39640;&#25928;&#20302;&#20301;&#23485;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models. (arXiv:2310.08041v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08041
&lt;/p&gt;
&lt;p&gt;
QLLM&#26159;&#19968;&#31181;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#20934;&#30830;&#39640;&#25928;&#30340;&#20302;&#20301;&#23485;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#36890;&#36947;&#37325;&#32452;&#25216;&#26415;&#65292;&#23558;&#31163;&#32676;&#20540;&#30340;&#22823;&#23567;&#37325;&#26032;&#20998;&#37197;&#32473;&#20854;&#20182;&#36890;&#36947;&#65292;&#20174;&#32780;&#20943;&#36731;&#23427;&#20204;&#23545;&#37327;&#21270;&#33539;&#22260;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#20854;&#25152;&#38656;&#36164;&#28304;&#36807;&#22823;&#65292;&#38480;&#21046;&#20102;&#20854;&#24191;&#27867;&#24212;&#29992;&#12290;&#34429;&#28982;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;Quantization-Aware Training&#65292;QAT&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#30340;&#35757;&#32451;&#25104;&#26412;&#36807;&#39640;&#65292;&#22240;&#27492;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;Post-Training Quantization&#65292;PTQ&#65289;&#25104;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26356;&#23454;&#38469;&#30340;&#26041;&#27861;&#12290;&#22312;&#29616;&#26377;&#30740;&#31350;&#20013;&#65292;&#29305;&#23450;&#36890;&#36947;&#20013;&#30340;&#28608;&#27963;&#31163;&#32676;&#20540;&#34987;&#35748;&#20026;&#26159;&#23548;&#33268;&#21518;&#35757;&#32451;&#37327;&#21270;&#20934;&#30830;&#24615;&#19979;&#38477;&#30340;&#29942;&#39048;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;QLLM&#65292;&#19968;&#31181;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#20934;&#30830;&#39640;&#25928;&#30340;&#20302;&#20301;&#23485;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12290;QLLM&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36890;&#36947;&#37325;&#32452;&#25216;&#26415;&#65292;&#23558;&#31163;&#32676;&#20540;&#30340;&#22823;&#23567;&#37325;&#26032;&#20998;&#37197;&#32473;&#20854;&#20182;&#36890;&#36947;&#65292;&#20174;&#32780;&#20943;&#36731;&#23427;&#20204;&#23545;&#37327;&#21270;&#33539;&#22260;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#36890;&#36947;&#25286;&#20998;&#21644;&#36890;&#36947;&#32452;&#35013;&#65292;&#22312;&#20445;&#35777;&#20302;&#20301;&#23485;&#30340;&#24773;&#20917;&#19979;&#23558;&#31163;&#32676;&#36890;&#36947;&#20998;&#35299;&#25104;&#22810;&#20010;&#23376;&#36890;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) excel in NLP, but their demands hinder their widespread deployment. While Quantization-Aware Training (QAT) offers a solution, its extensive training costs make Post-Training Quantization (PTQ) a more practical approach for LLMs. In existing studies, activation outliers in particular channels are identified as the bottleneck to PTQ accuracy. They propose to transform the magnitudes from activations to weights, which however offers limited alleviation or suffers from unstable gradients, resulting in a severe performance drop at low-bitwidth. In this paper, we propose QLLM, an accurate and efficient low-bitwidth PTQ method designed for LLMs. QLLM introduces an adaptive channel reassembly technique that reallocates the magnitude of outliers to other channels, thereby mitigating their impact on the quantization range. This is achieved by channel disassembly and channel assembly, which first breaks down the outlier channels into several sub-channels to ensure a 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#22270;&#20687;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22686;&#24378;&#22810;&#27169;&#24577;&#21306;&#20998;&#31243;&#24230;&#26816;&#27979;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20102;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#29305;&#24449;&#21644;&#35270;&#35273;&#23545;&#35937;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#39640;&#21306;&#20998;&#31243;&#24230;&#26816;&#27979;&#30340;&#20934;&#30830;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08027</link><description>&lt;p&gt;
&#25506;&#32034;&#29992;&#20110;&#22810;&#27169;&#24577;&#21306;&#20998;&#31243;&#24230;&#26816;&#27979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Exploring Large Language Models for Multi-Modal Out-of-Distribution Detection. (arXiv:2310.08027v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#22270;&#20687;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22686;&#24378;&#22810;&#27169;&#24577;&#21306;&#20998;&#31243;&#24230;&#26816;&#27979;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20102;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#29305;&#24449;&#21644;&#35270;&#35273;&#23545;&#35937;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#39640;&#21306;&#20998;&#31243;&#24230;&#26816;&#27979;&#30340;&#20934;&#30830;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#20998;&#31243;&#24230;&#26816;&#27979;&#26159;&#21487;&#38752;&#21644;&#21487;&#20449;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#20851;&#38190;&#12290;&#26368;&#36817;&#30340;&#22810;&#27169;&#24577;&#21306;&#20998;&#31243;&#24230;&#26816;&#27979;&#21033;&#29992;&#26469;&#33258;&#20869;&#20998;&#24067;&#31867;&#21035;&#21517;&#31216;&#30340;&#25991;&#26412;&#20449;&#24687;&#36827;&#34892;&#35270;&#35273;&#21306;&#20998;&#31243;&#24230;&#26816;&#27979;&#65292;&#20294;&#30446;&#21069;&#24573;&#35270;&#20102;&#20869;&#20998;&#24067;&#31867;&#21035;&#30340;&#20016;&#23500;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#23545;&#19990;&#30028;&#30693;&#35782;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#21487;&#20197;&#29983;&#25104;&#27599;&#20010;&#31867;&#21035;&#30340;&#25551;&#36848;&#24615;&#29305;&#24449;&#12290;&#19981;&#21152;&#21306;&#20998;&#22320;&#20351;&#29992;&#36825;&#20123;&#30693;&#35782;&#20250;&#23545;&#21306;&#20998;&#31243;&#24230;&#26816;&#27979;&#36896;&#25104;&#28798;&#38590;&#24615;&#25439;&#23475;&#65292;&#36825;&#26159;&#25105;&#20204;&#20998;&#26512;&#25152;&#35266;&#23519;&#21040;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#19990;&#30028;&#30693;&#35782;&#22686;&#24378;&#21306;&#20998;&#31243;&#24230;&#26816;&#27979;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;LLM&#20013;&#36873;&#25321;&#24615;&#29983;&#25104;&#26469;&#23454;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#26041;&#27861;&#65292;&#26469;&#20272;&#35745;&#27599;&#20010;&#29983;&#25104;&#30340;&#32622;&#20449;&#24230;&#24471;&#20998;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20174;&#27599;&#20010;&#22270;&#20687;&#20013;&#25552;&#21462;&#35270;&#35273;&#23545;&#35937;&#65292;&#20805;&#20998;&#21033;&#29992;&#19978;&#36848;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21306;&#20998;&#31243;&#24230;&#26816;&#27979;&#26041;&#38754;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection is essential for reliable and trustworthy machine learning. Recent multi-modal OOD detection leverages textual information from in-distribution (ID) class names for visual OOD detection, yet it currently neglects the rich contextual information of ID classes. Large language models (LLMs) encode a wealth of world knowledge and can be prompted to generate descriptive features for each class. Indiscriminately using such knowledge causes catastrophic damage to OOD detection due to LLMs' hallucinations, as is observed by our analysis. In this paper, we propose to apply world knowledge to enhance OOD detection performance through selective generation from LLMs. Specifically, we introduce a consistency-based uncertainty calibration method to estimate the confidence score of each generation. We further extract visual objects from each image to fully capitalize on the aforementioned world knowledge. Extensive experiments demonstrate that our method consistent
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#27169;&#25311;&#24515;&#29702;&#20581;&#24247;&#21672;&#35810;&#23545;&#35805;&#20013;&#29983;&#25104;&#20849;&#24773;&#22238;&#24212;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#19982;&#20256;&#32479;&#22238;&#24212;&#29983;&#25104;&#23545;&#35805;&#31995;&#32479;&#21644;&#20154;&#24037;&#22238;&#24212;&#30340;&#27604;&#36739;&#65292;&#30740;&#31350;&#21457;&#29616;LLMs&#22312;&#36825;&#19968;&#39046;&#22495;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.08017</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#33021;&#21147;&#29992;&#20110;&#22312;&#32447;&#24515;&#29702;&#20581;&#24247;&#21672;&#35810;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Harnessing Large Language Models' Empathetic Response Generation Capabilities for Online Mental Health Counselling Support. (arXiv:2310.08017v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#27169;&#25311;&#24515;&#29702;&#20581;&#24247;&#21672;&#35810;&#23545;&#35805;&#20013;&#29983;&#25104;&#20849;&#24773;&#22238;&#24212;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#19982;&#20256;&#32479;&#22238;&#24212;&#29983;&#25104;&#23545;&#35805;&#31995;&#32479;&#21644;&#20154;&#24037;&#22238;&#24212;&#30340;&#27604;&#36739;&#65292;&#30740;&#31350;&#21457;&#29616;LLMs&#22312;&#36825;&#19968;&#39046;&#22495;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20449;&#24687;&#33719;&#21462;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#20123;&#35745;&#31639;&#31995;&#32479;&#39537;&#21160;&#30528;&#26368;&#20808;&#36827;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#22914;ChatGPT&#21644;Bard&#12290;&#23427;&#20204;&#22312;&#28385;&#36275;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#22686;&#38271;&#38656;&#27714;&#26041;&#38754;&#20063;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#23613;&#31649;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;LLMs&#22312;&#27169;&#25311;&#24515;&#29702;&#20581;&#24247;&#21672;&#35810;&#29615;&#22659;&#20013;&#29983;&#25104;&#20849;&#24773;&#22238;&#24212;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#20116;&#20010;LLMs&#65306;Generative Pre-training&#65288;GPT&#65289;&#30340;3.5&#29256;&#21644;4&#29256;&#12289;Vicuna FastChat-T5&#12289;Pathways Language Model&#65288;PaLM&#65289;&#30340;2&#29256;&#21644;Falcon-7B-Instruct&#12290;&#26681;&#25454;&#19968;&#20010;&#31616;&#21333;&#30340;&#25351;&#20196;&#25552;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#26469;&#33258;EmpatheticDialogues&#65288;ED&#65289;&#25968;&#25454;&#38598;&#30340;&#35805;&#35821;&#36827;&#34892;&#22238;&#24212;&#12290;&#20351;&#29992;&#19977;&#20010;&#19982;&#20849;&#24773;&#30456;&#20851;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#25105;&#20204;&#23558;&#20854;&#22238;&#24212;&#19982;&#22312;ED&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#20256;&#32479;&#22238;&#24212;&#29983;&#25104;&#23545;&#35805;&#31995;&#32479;&#20197;&#21450;&#30001;&#20154;&#31867;&#29983;&#25104;&#30340;&#22238;&#24212;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable performance across various information-seeking and reasoning tasks. These computational systems drive state-of-the-art dialogue systems, such as ChatGPT and Bard. They also carry substantial promise in meeting the growing demands of mental health care, albeit relatively unexplored. As such, this study sought to examine LLMs' capability to generate empathetic responses in conversations that emulate those in a mental health counselling setting. We selected five LLMs: version 3.5 and version 4 of the Generative Pre-training (GPT), Vicuna FastChat-T5, Pathways Language Model (PaLM) version 2, and Falcon-7B-Instruct. Based on a simple instructional prompt, these models responded to utterances derived from the EmpatheticDialogues (ED) dataset. Using three empathy-related metrics, we compared their responses to those from traditional response generation dialogue systems, which were fine-tuned on the ED dataset, along with human-generat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#38646;&#23556;&#20132;&#20114;&#20010;&#24615;&#21270;&#23545;&#35937;&#23548;&#33322;&#65288;ZIPON&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#29992;&#25143;&#21453;&#39304;&#65292;&#35299;&#20915;&#20102;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#23548;&#33322;&#21040;&#20010;&#24615;&#21270;&#30446;&#26631;&#23545;&#35937;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07968</link><description>&lt;p&gt;
&#24605;&#32771;&#12289;&#34892;&#21160;&#21644;&#38382;&#65306;&#24320;&#25918;&#19990;&#30028;&#20114;&#21160;&#20010;&#24615;&#21270;&#26426;&#22120;&#20154;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Think, Act, and Ask: Open-World Interactive Personalized Robot Navigation. (arXiv:2310.07968v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07968
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#38646;&#23556;&#20132;&#20114;&#20010;&#24615;&#21270;&#23545;&#35937;&#23548;&#33322;&#65288;ZIPON&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#29992;&#25143;&#21453;&#39304;&#65292;&#35299;&#20915;&#20102;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#23548;&#33322;&#21040;&#20010;&#24615;&#21270;&#30446;&#26631;&#23545;&#35937;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#23556;&#21629;&#20196;&#23545;&#35937;&#23548;&#33322;&#65288;ZSON&#65289;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#23548;&#33322;&#21040;&#24320;&#25918;&#35789;&#27719;&#23545;&#35937;&#12290;&#29616;&#26377;&#30340;ZSON&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#36981;&#24490;&#20010;&#21035;&#25351;&#20196;&#20197;&#23547;&#25214;&#36890;&#29992;&#23545;&#35937;&#31867;&#65292;&#24573;&#30053;&#20102;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30340;&#21033;&#29992;&#21644;&#35782;&#21035;&#29992;&#25143;&#29305;&#23450;&#23545;&#35937;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38646;&#23556;&#20132;&#20114;&#20010;&#24615;&#21270;&#23545;&#35937;&#23548;&#33322;&#65288;ZIPON&#65289;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#38656;&#35201;&#22312;&#19982;&#29992;&#25143;&#23545;&#35805;&#30340;&#21516;&#26102;&#23548;&#33322;&#21040;&#20010;&#24615;&#21270;&#30446;&#26631;&#23545;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;ZIPON&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#31216;&#20026;&#24320;&#25918;&#19990;&#30028;&#20114;&#21160;&#20010;&#24615;&#21270;&#23548;&#33322;&#65288;ORION&#65289;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#24207;&#21015;&#20915;&#31574;&#65292;&#20197;&#25805;&#20316;&#19981;&#21516;&#30340;&#24863;&#30693;&#12289;&#23548;&#33322;&#21644;&#36890;&#20449;&#27169;&#22359;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#33021;&#22815;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#30340;&#20114;&#21160;&#20195;&#29702;&#30340;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#22312;&#20219;&#21153;&#23436;&#25104;&#21644;&#29992;&#25143;&#28385;&#24847;&#24230;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#30340;&#24179;&#34913;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Object Navigation (ZSON) enables agents to navigate towards open-vocabulary objects in unknown environments. The existing works of ZSON mainly focus on following individual instructions to find generic object classes, neglecting the utilization of natural language interaction and the complexities of identifying user-specific objects. To address these limitations, we introduce Zero-shot Interactive Personalized Object Navigation (ZIPON), where robots need to navigate to personalized goal objects while engaging in conversations with users. To solve ZIPON, we propose a new framework termed Open-woRld Interactive persOnalized Navigation (ORION), which uses Large Language Models (LLMs) to make sequential decisions to manipulate different modules for perception, navigation and communication. Experimental results show that the performance of interactive agents that can leverage user feedback exhibits significant improvement. However, obtaining a good balance between task completion 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#23398;&#30456;&#20284;&#24615;&#26041;&#31243;&#23545;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#30340;&#19987;&#26377;&#21517;&#35789;&#30340;&#25340;&#20889;&#21464;&#20307;&#36827;&#34892;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;NLP&#20013;&#30001;&#20110;&#32763;&#35793;&#21644;&#36716;&#20889;&#19981;&#19968;&#33268;&#32780;&#24341;&#36215;&#30340;&#25340;&#20889;&#21464;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07962</link><description>&lt;p&gt;
&#20174;&#20854;&#20182;&#35821;&#35328;&#36716;&#35793;&#30340;&#19987;&#26377;&#21517;&#35789;&#30340;&#25340;&#20889;&#21464;&#20307;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Clustering of Spell Variations for Proper Nouns Transliterated from the other languages. (arXiv:2310.07962v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#23398;&#30456;&#20284;&#24615;&#26041;&#31243;&#23545;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#30340;&#19987;&#26377;&#21517;&#35789;&#30340;&#25340;&#20889;&#21464;&#20307;&#36827;&#34892;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;NLP&#20013;&#30001;&#20110;&#32763;&#35793;&#21644;&#36716;&#20889;&#19981;&#19968;&#33268;&#32780;&#24341;&#36215;&#30340;&#25340;&#20889;&#21464;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#21644;&#25805;&#20316;&#25991;&#26412;&#25968;&#25454;&#26102;&#65292;&#38750;&#22343;&#21248;&#24615;&#26159;&#19968;&#20010;&#31361;&#20986;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#26041;&#35328;&#21644;&#35821;&#35328;&#30340;&#21464;&#21270;&#65292;&#32763;&#35793;&#30340;&#36136;&#37327;&#36739;&#20302;&#12290;&#36825;&#22312;&#20351;&#29992;NLP&#22788;&#29702;&#25991;&#26412;&#25968;&#25454;&#26102;&#20250;&#20135;&#29983;&#19968;&#20010;&#29420;&#29305;&#30340;&#38382;&#39064;&#65292;&#21363;&#19981;&#19968;&#33268;&#30340;&#32763;&#35793;&#21644;&#36716;&#20889;&#24341;&#21457;&#30340;&#25340;&#20889;&#21464;&#21270;&#12290;&#36825;&#20010;&#38382;&#39064;&#36824;&#21487;&#33021;&#22240;&#20026;&#23558;&#21360;&#24230;&#35821;&#20013;&#30340;&#19987;&#26377;&#21517;&#35789;&#36716;&#21270;&#20026;&#33521;&#25991;&#31561;&#25928;&#35789;&#30340;&#21508;&#31181;&#26041;&#24335;&#32780;&#23548;&#33268;&#20154;&#20026;&#38169;&#35823;&#12290;&#23558;&#26469;&#33258;&#21360;&#24230;&#35821;&#35328;&#30340;&#19987;&#26377;&#21517;&#35789;&#32763;&#35793;&#25104;&#33521;&#25991;&#21487;&#33021;&#24456;&#22797;&#26434;&#65292;&#22240;&#20026;&#19968;&#20123;&#19987;&#26377;&#21517;&#35789;&#20063;&#34987;&#29992;&#20316;&#26222;&#36890;&#21517;&#35789;&#65292;&#21487;&#33021;&#34987;&#30452;&#25509;&#29702;&#35299;&#12290;&#38656;&#35201;&#22320;&#22336;&#12289;&#21517;&#31216;&#21644;&#20854;&#20182;&#19987;&#26377;&#21517;&#35789;&#30340;NLP&#24212;&#29992;&#32463;&#24120;&#36935;&#21040;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#25968;&#23398;&#30456;&#20284;&#24615;&#26041;&#31243;&#23545;&#36825;&#20123;&#19987;&#26377;&#21517;&#35789;&#30340;&#25340;&#20889;&#21464;&#20307;&#36827;&#34892;&#32858;&#31867;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#26088;&#22312;&#20351;&#29992;&#20146;&#21644;&#20256;&#25773;&#26469;&#30830;&#23450;&#26631;&#35760;&#20043;&#38388;&#30340;&#30456;&#23545;&#30456;&#20284;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the prominent problems with processing and operating on text data is the non uniformity of it. Due to the change in the dialects and languages, the caliber of translation is low. This creates a unique problem while using NLP in text data; which is the spell variation arising from the inconsistent translations and transliterations. This problem can also be further aggravated by the human error arising from the various ways to write a Proper Noun from an Indian language into its English equivalent. Translating proper nouns originating from Indian languages can be complicated as some proper nouns are also used as common nouns which might be taken literally. Applications of NLP that require addresses, names and other proper nouns face this problem frequently. We propose a method to cluster these spell variations for proper nouns using ML techniques and mathematical similarity equations. We aimed to use Affinity Propagation to determine relative similarity between the tokens. The res
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;&#30740;&#31350;&#27700;&#24179;&#30340;&#25968;&#23398;&#33258;&#21160;&#24418;&#24335;&#21270;&#20219;&#21153;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#25104;&#26356;&#23481;&#26131;&#22788;&#29702;&#30340;&#23376;&#20219;&#21153;&#65292;&#21253;&#25324;&#26410;&#38142;&#25509;&#24418;&#24335;&#21270;&#12289;&#23454;&#20307;&#38142;&#25509;&#21644;&#31867;&#22411;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26410;&#38142;&#25509;&#24418;&#24335;&#21270;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598; arXiv2Formal&#12290;</title><link>http://arxiv.org/abs/2310.07957</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#24418;&#24335;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A New Approach Towards Autoformalization. (arXiv:2310.07957v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07957
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;&#30740;&#31350;&#27700;&#24179;&#30340;&#25968;&#23398;&#33258;&#21160;&#24418;&#24335;&#21270;&#20219;&#21153;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#25104;&#26356;&#23481;&#26131;&#22788;&#29702;&#30340;&#23376;&#20219;&#21153;&#65292;&#21253;&#25324;&#26410;&#38142;&#25509;&#24418;&#24335;&#21270;&#12289;&#23454;&#20307;&#38142;&#25509;&#21644;&#31867;&#22411;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26410;&#38142;&#25509;&#24418;&#24335;&#21270;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598; arXiv2Formal&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39564;&#35777;&#25968;&#23398;&#35777;&#26126;&#26159;&#22256;&#38590;&#30340;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#26426;&#30340;&#36741;&#21161;&#23454;&#29616;&#33258;&#21160;&#21270;&#12290;&#33258;&#21160;&#24418;&#24335;&#21270;&#26159;&#23558;&#33258;&#28982;&#35821;&#35328;&#25968;&#23398;&#33258;&#21160;&#36716;&#21270;&#20026;&#21487;&#20197;&#30001;&#31243;&#24207;&#39564;&#35777;&#30340;&#24418;&#24335;&#35821;&#35328;&#30340;&#20219;&#21153;&#12290;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23588;&#20854;&#23545;&#20110;&#30740;&#31350;&#35770;&#25991;&#20013;&#30340;&#39640;&#32423;&#25968;&#23398;&#26469;&#35828;&#12290;&#30740;&#31350;&#35770;&#25991;&#20013;&#30340;&#25968;&#23398;&#38656;&#35201;&#22823;&#37327;&#30340;&#32972;&#26223;&#21644;&#19978;&#19979;&#25991;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#23545;&#30740;&#31350;&#27700;&#24179;&#25968;&#23398;&#33258;&#21160;&#24418;&#24335;&#21270;&#30340;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#26356;&#26131;&#20110;&#22788;&#29702;&#30340;&#23376;&#20219;&#21153;&#65306;&#26410;&#38142;&#25509;&#24418;&#24335;&#21270;&#65288;&#21253;&#21547;&#26410;&#38142;&#25509;&#30340;&#23450;&#20041;&#21644;&#23450;&#29702;&#30340;&#24418;&#24335;&#21270;&#65289;&#12289;&#23454;&#20307;&#38142;&#25509;&#65288;&#38142;&#25509;&#21040;&#27491;&#30830;&#30340;&#23450;&#29702;&#21644;&#23450;&#20041;&#65289;&#20197;&#21450;&#35843;&#25972;&#31867;&#22411;&#20197;&#36890;&#36807;&#31867;&#22411;&#26816;&#26597;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;arXiv2Formal&#65292;&#19968;&#20010;&#29992;&#20110;&#26410;&#38142;&#25509;&#24418;&#24335;&#21270;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#20174;arXiv.org&#30340;&#35770;&#25991;&#20013;&#25277;&#21462;&#30340;50&#20010;&#23450;&#29702;&#22312;Lean&#23450;&#29702;&#35777;&#26126;&#22120;&#20013;&#36827;&#34892;&#24418;&#24335;&#21270;&#12290;&#25105;&#20204;&#27426;&#36814;&#20219;&#20309;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Verifying mathematical proofs is difficult, but can be automated with the assistance of a computer. Autoformalization is the task of automatically translating natural language mathematics into a formal language that can be verified by a program. This is a challenging task, and especially for higher-level mathematics found in research papers. Research paper mathematics requires large amounts of background and context. In this paper, we propose an avenue towards tackling autoformalization for research-level mathematics, by breaking the task into easier and more approachable subtasks: unlinked formalization (formalization with unlinked definitions and theorems), entity linking (linking to the proper theorems and definitions), and finally adjusting types so it passes the type checker. In addition, we present arXiv2Formal, a benchmark dataset for unlinked formalization consisting of 50 theorems formalized for the Lean theorem prover sampled from papers on arXiv.org. We welcome any contribut
&lt;/p&gt;</description></item><item><title>D2&#20462;&#21098;&#26159;&#19968;&#31181;&#24179;&#34913;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#22256;&#38590;&#24230;&#30340;&#26041;&#27861;&#65292;&#22312;coreset&#36873;&#25321;&#20013;&#21516;&#26102;&#32771;&#34385;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#37325;&#35201;&#24615;&#35780;&#20998;&#12290;</title><link>http://arxiv.org/abs/2310.07931</link><description>&lt;p&gt;
D2&#20462;&#21098;&#65306;&#20449;&#24687;&#20256;&#36882;&#24179;&#34913;&#25968;&#25454;&#20462;&#21098;&#20013;&#30340;&#22810;&#26679;&#24615;&#21644;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
D2 Pruning: Message Passing for Balancing Diversity and Difficulty in Data Pruning. (arXiv:2310.07931v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07931
&lt;/p&gt;
&lt;p&gt;
D2&#20462;&#21098;&#26159;&#19968;&#31181;&#24179;&#34913;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#22256;&#38590;&#24230;&#30340;&#26041;&#27861;&#65292;&#22312;coreset&#36873;&#25321;&#20013;&#21516;&#26102;&#32771;&#34385;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#37325;&#35201;&#24615;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#29702;&#35770;&#34920;&#26126;&#65292;&#22312;&#22266;&#23450;&#25968;&#25454;&#39044;&#31639;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#26356;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#21487;&#20197;&#23548;&#33268;&#26356;&#20302;&#30340;&#27979;&#35797;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#25968;&#25454;&#38598;&#21487;&#20197;&#21093;&#31163;&#20887;&#20313;&#39033;&#65292;&#21017;&#21487;&#20197;&#22312;&#36739;&#20302;&#30340;&#35745;&#31639;&#39044;&#31639;&#19978;&#35757;&#32451;&#27169;&#22411;&#32780;&#19981;&#38477;&#20302;&#24615;&#33021;&#12290;Coreset&#36873;&#25321;&#65288;&#25110;&#25968;&#25454;&#20462;&#21098;&#65289;&#23547;&#27714;&#36873;&#25321;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#65292;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#22312;&#35813;&#23376;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20063;&#31216;&#20026;coreset&#12290;&#26377;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#65306;&#65288;1&#65289;&#22522;&#20110;&#20960;&#20309;&#30340;&#25968;&#25454;&#36873;&#21462;&#65292;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;coreset&#20013;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#21644;&#65288;2&#65289;&#26681;&#25454;&#35757;&#32451;&#21160;&#24577;&#20026;&#26679;&#26412;&#20998;&#37197;&#22256;&#38590;&#24230;&#20998;&#25968;&#30340;&#20989;&#25968;&#12290;&#20026;&#25968;&#25454;&#22810;&#26679;&#24615;&#36827;&#34892;&#20248;&#21270;&#20250;&#23548;&#33268;&#20559;&#21521;&#36739;&#23481;&#26131;&#26679;&#26412;&#30340;coreset&#65292;&#32780;&#38590;&#24230;&#25490;&#21517;&#36873;&#25321;&#20250;&#24573;&#30053;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#25152;&#24517;&#38656;&#30340;&#23481;&#26131;&#26679;&#26412;&#12290;&#36825;&#34920;&#26126;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#37325;&#35201;&#24615;&#35780;&#20998;&#26159;&#20004;&#20010;&#20114;&#34917;&#22240;&#32032;&#65292;&#22312;coreset&#36873;&#25321;&#20013;&#38656;&#35201;&#21516;&#26102;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analytical theories suggest that higher-quality data can lead to lower test errors in models trained on a fixed data budget. Moreover, a model can be trained on a lower compute budget without compromising performance if a dataset can be stripped of its redundancies. Coreset selection (or data pruning) seeks to select a subset of the training data so as to maximize the performance of models trained on this subset, also referred to as coreset. There are two dominant approaches: (1) geometry-based data selection for maximizing data diversity in the coreset, and (2) functions that assign difficulty scores to samples based on training dynamics. Optimizing for data diversity leads to a coreset that is biased towards easier samples, whereas, selection by difficulty ranking omits easy samples that are necessary for the training of deep learning models. This demonstrates that data diversity and importance scores are two complementary factors that need to be jointly considered during coreset sel
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#32467;&#26500;&#21551;&#31034;&#27979;&#35797;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20849;&#20139;&#25277;&#35937;&#30340;&#35821;&#27861;&#34920;&#31034;&#65292;&#24182;&#21457;&#29616;&#22312;&#25509;&#35302;&#31532;&#20108;&#31181;&#35821;&#35328;&#21518;&#30340;&#26089;&#26399;&#38454;&#27573;&#21363;&#24418;&#25104;&#36328;&#35821;&#35328;&#32467;&#26500;&#21551;&#31034;&#25928;&#24212;&#12290;&#36825;&#23545;&#25968;&#25454;&#27745;&#26579;&#12289;&#20302;&#36164;&#28304;&#36716;&#31227;&#20197;&#21450;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#25277;&#35937;&#35821;&#27861;&#34920;&#31034;&#30340;&#20135;&#29983;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.07929</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#32467;&#26500;&#21551;&#31034;&#19982;&#21452;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Crosslingual Structural Priming and the Pre-Training Dynamics of Bilingual Language Models. (arXiv:2310.07929v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07929
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#32467;&#26500;&#21551;&#31034;&#27979;&#35797;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20849;&#20139;&#25277;&#35937;&#30340;&#35821;&#27861;&#34920;&#31034;&#65292;&#24182;&#21457;&#29616;&#22312;&#25509;&#35302;&#31532;&#20108;&#31181;&#35821;&#35328;&#21518;&#30340;&#26089;&#26399;&#38454;&#27573;&#21363;&#24418;&#25104;&#36328;&#35821;&#35328;&#32467;&#26500;&#21551;&#31034;&#25928;&#24212;&#12290;&#36825;&#23545;&#25968;&#25454;&#27745;&#26579;&#12289;&#20302;&#36164;&#28304;&#36716;&#31227;&#20197;&#21450;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#25277;&#35937;&#35821;&#27861;&#34920;&#31034;&#30340;&#20135;&#29983;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#22312;&#21508;&#31181;&#35821;&#35328;&#20043;&#38388;&#20849;&#20139;&#25277;&#35937;&#30340;&#35821;&#27861;&#34920;&#31034;&#65292;&#24182;&#19988;&#22914;&#26524;&#20849;&#20139;&#30340;&#35805;&#65292;&#36825;&#31181;&#20849;&#20139;&#26159;&#20309;&#26102;&#21457;&#23637;&#30340;&#65311;&#25105;&#20204;&#20351;&#29992;&#32467;&#26500;&#21551;&#31034;&#26469;&#27979;&#35797;&#22312;&#27169;&#22411;&#36755;&#20986;&#19978;&#20855;&#26377;&#22240;&#26524;&#25928;&#24212;&#30340;&#25277;&#35937;&#35821;&#27861;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#33655;&#20848;-&#33521;&#35821;&#21452;&#35821;&#29615;&#22659;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#35780;&#20272;&#33655;&#20848;-&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#25509;&#35302;&#31532;&#20108;&#31181;&#35821;&#35328;&#21518;&#30340;&#26089;&#26399;&#38454;&#27573;&#65292;&#36328;&#35821;&#35328;&#32467;&#26500;&#21551;&#31034;&#25928;&#24212;&#20986;&#29616;&#65292;&#32780;&#20165;&#38656;&#19981;&#21040;100&#19975;&#20010;&#35813;&#35821;&#35328;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23545;&#25968;&#25454;&#27745;&#26579;&#12289;&#20302;&#36164;&#28304;&#36716;&#31227;&#20197;&#21450;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#25277;&#35937;&#35821;&#27861;&#34920;&#31034;&#30340;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Do multilingual language models share abstract grammatical representations across languages, and if so, when do these develop? Following Sinclair et al. (2022), we use structural priming to test for abstract grammatical representations with causal effects on model outputs. We extend the approach to a Dutch-English bilingual setting, and we evaluate a Dutch-English language model during pre-training. We find that crosslingual structural priming effects emerge early after exposure to the second language, with less than 1M tokens of data in that language. We discuss implications for data contamination, low-resource transfer, and how abstract grammatical representations emerge in multilingual models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#20801;&#35768;&#20351;&#29992;&#20013;&#38388;&#29983;&#25104;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;Transformer&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32447;&#24615;&#25968;&#37327;&#30340;&#35299;&#30721;&#27493;&#39588;&#22312;&#26631;&#20934;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#22686;&#21152;&#20102;&#26126;&#26174;&#30340;&#26032;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07923</link><description>&lt;p&gt;
&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The Expresssive Power of Transformers with Chain of Thought. (arXiv:2310.07923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#20801;&#35768;&#20351;&#29992;&#20013;&#38388;&#29983;&#25104;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;Transformer&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32447;&#24615;&#25968;&#37327;&#30340;&#35299;&#30721;&#27493;&#39588;&#22312;&#26631;&#20934;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#22686;&#21152;&#20102;&#26126;&#26174;&#30340;&#26032;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#29702;&#35770;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#20123;&#20986;&#20154;&#24847;&#26009;&#22320;&#31616;&#21333;&#30340;&#25512;&#29702;&#38382;&#39064;&#65292;&#20363;&#22914;&#26816;&#26597;&#22270;&#20013;&#26159;&#21542;&#23384;&#22312;&#36830;&#25509;&#30340;&#20004;&#20010;&#33410;&#28857;&#65292;&#25110;&#27169;&#25311;&#26377;&#38480;&#29366;&#24577;&#26426;&#65292;&#36825;&#20123;&#38382;&#39064;&#34987;&#35777;&#26126;&#26080;&#27861;&#30001;&#31435;&#21363;&#35835;&#21462;&#36755;&#20837;&#21518;&#22238;&#31572;&#30340;&#26631;&#20934;Transformer&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#36807;&#20801;&#35768;Transformer&#20351;&#29992;&#8220;&#24605;&#32500;&#38142;&#8221;&#25110;&#8220;&#33609;&#31295;&#32440;&#8221;&#65292;&#21363;&#22312;&#22238;&#31572;&#20043;&#21069;&#29983;&#25104;&#24182;&#20381;&#36182;&#19968;&#31995;&#21015;&#20013;&#38388;token&#65292;&#21487;&#20197;&#25913;&#21892;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#38382;&#65306;&#36825;&#31181;&#20013;&#38388;&#29983;&#25104;&#26159;&#21542;&#20174;&#26681;&#26412;&#19978;&#25193;&#23637;&#20102;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;Transformer&#30340;&#35745;&#31639;&#33021;&#21147;&#65311;&#25105;&#20204;&#34920;&#26126;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#65292;&#20294;&#22686;&#21152;&#30340;&#31243;&#24230;&#20851;&#38190;&#21462;&#20915;&#20110;&#20013;&#38388;&#29983;&#25104;&#30340;&#25968;&#37327;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#30456;&#23545;&#20110;&#36755;&#20837;&#38271;&#24230;&#26469;&#35828;&#65292;&#20855;&#26377;&#23545;&#25968;&#32423;&#35299;&#30721;&#27493;&#39588;&#30340;Transformer&#35299;&#30721;&#22120;&#20165;&#30053;&#24494;&#25512;&#21160;&#20102;&#26631;&#20934;Transformer&#30340;&#26497;&#38480;&#65292;&#32780;&#32447;&#24615;&#25968;&#37327;&#30340;&#35299;&#30721;&#27493;&#39588;&#21017;&#22686;&#21152;&#20102;&#26126;&#26174;&#30340;&#26032;&#33021;&#21147;&#65288;&#22312;&#26631;&#20934;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent theoretical work has identified surprisingly simple reasoning problems, such as checking if two nodes in a graph are connected or simulating finite-state machines, that are provably unsolvable by standard transformers that answer immediately after reading their input. However, in practice, transformers' reasoning can be improved by allowing them to use a "chain of thought" or "scratchpad", i.e., generate and condition on a sequence of intermediate tokens before answering. Motivated by this, we ask: Does such intermediate generation fundamentally extend the computational power of a decoder-only transformer? We show that the answer is yes, but the amount of increase depends crucially on the amount of intermediate generation. For instance, we find that transformer decoders with a logarithmic number of decoding steps (w.r.t. the input length) push the limits of standard transformers only slightly, while a linear number of decoding steps adds a clear new ability (under standard compl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#27880;&#24847;&#21147;&#22836;&#23884;&#20837;&#26469;&#23454;&#29616;&#21442;&#25968;&#26377;&#25928;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#22312;&#22810;&#39033;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19982;&#20256;&#32479;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.07911</link><description>&lt;p&gt;
&#19968;&#20010;&#23545;&#22810;&#20010;&#37096;&#20998;&#36827;&#34892;&#23545;&#27604;&#30340;&#26041;&#27861;&#65306;&#21033;&#29992;&#27880;&#24847;&#21147;&#22836;&#23884;&#20837;&#20197;&#23454;&#29616;&#21442;&#25968;&#26377;&#25928;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Pit One Against Many: Leveraging Attention-head Embeddings for Parameter-efficient Multi-head Attention. (arXiv:2310.07911v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#27880;&#24847;&#21147;&#22836;&#23884;&#20837;&#26469;&#23454;&#29616;&#21442;&#25968;&#26377;&#25928;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#22312;&#22810;&#39033;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19982;&#20256;&#32479;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#25193;&#23637;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25552;&#21319;&#65292;&#20294;&#36825;&#20063;&#24102;&#26469;&#20102;&#22823;&#37327;&#30340;&#20869;&#23384;&#38656;&#27714;&#12290;&#21463;Transformer&#20013;&#20301;&#32622;&#23884;&#20837;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26088;&#22312;&#31616;&#21270;&#21644;&#20943;&#23569;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#27169;&#22359;&#65292;&#21482;&#20351;&#29992;&#19968;&#20010;&#20849;&#20139;&#30340;&#25237;&#24433;&#30697;&#38453;&#21644;&#22810;&#20010;&#22836;&#37096;&#23884;&#20837;&#65288;MHE&#65289;&#65292;&#21363;&#27599;&#20010;&#22836;&#37096;&#19968;&#20010;&#12290;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25105;&#20204;&#30340;MHE&#27880;&#24847;&#21147;&#22312;&#20869;&#23384;&#20351;&#29992;&#25928;&#29575;&#19978;&#26356;&#39640;&#65292;&#21516;&#26102;&#22312;&#22810;&#39033;&#19979;&#28216;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#19982;&#20256;&#32479;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#20445;&#25345;&#27604;&#12290;&#19982;&#21333;&#22836;&#27880;&#24847;&#21147;&#30456;&#27604;&#65292;MHE&#27880;&#24847;&#21147;&#20165;&#38656;&#35201;&#39069;&#22806;&#30340;&#21442;&#25968;&#65288;$3nd$&#65292;&#20854;&#20013;$n$&#26159;&#27880;&#24847;&#21147;&#22836;&#30340;&#25968;&#37327;&#65292;$d$&#26159;&#22836;&#23884;&#20837;&#30340;&#22823;&#23567;&#65289;&#30340;&#24494;&#19981;&#36275;&#36947;&#30340;&#37096;&#20998;&#65292;&#32780;&#22810;&#22836;&#27880;&#24847;&#21147;&#21017;&#38656;&#35201;$(3n^2-3n)d^2-3nd$&#20010;&#39069;&#22806;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling pre-trained language models has resulted in large performance gains in various natural language processing tasks but comes with a large cost in memory requirements. Inspired by the position embeddings in transformers, we aim to simplify and reduce the memory footprint of the multi-head attention (MHA) mechanism. We propose an alternative module that uses only a single shared projection matrix and multiple head embeddings (MHE), i.e. one per head. We empirically demonstrate that our MHE attention is substantially more memory efficient compared to alternative attention mechanisms while achieving high predictive performance retention ratio to vanilla MHA on several downstream tasks. MHE attention only requires a negligible fraction of additional parameters ($3nd$, where $n$ is the number of attention heads and $d$ the size of the head embeddings) compared to a single-head attention, while MHA requires $(3n^2-3n)d^2-3nd$ additional parameters.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#23558;&#35821;&#35328;&#20316;&#20026;&#23548;&#33322;&#30340;&#24863;&#30693;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#29616;&#25104;&#30340;&#35270;&#35273;&#31995;&#32479;&#23558;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#30340;&#35270;&#22270;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#26368;&#20339;&#30340;&#34892;&#21160;&#26469;&#28385;&#36275;&#23548;&#33322;&#25351;&#20196;&#12290;&#26377;&#20004;&#31181;&#29992;&#20363;&#30340;&#23454;&#39564;&#23545;&#36825;&#31181;&#22522;&#20110;&#35821;&#35328;&#30340;&#23548;&#33322;&#26041;&#27861;&#36827;&#34892;&#20102;&#25506;&#32034;&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#36712;&#36857;&#20197;&#24494;&#35843;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;&#27169;&#25311;&#21040;&#23454;&#38469;&#30340;&#36716;&#25442;&#12290;</title><link>http://arxiv.org/abs/2310.07889</link><description>&lt;p&gt;
LangNav: &#35821;&#35328;&#20316;&#20026;&#23548;&#33322;&#30340;&#24863;&#30693;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
LangNav: Language as a Perceptual Representation for Navigation. (arXiv:2310.07889v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#23558;&#35821;&#35328;&#20316;&#20026;&#23548;&#33322;&#30340;&#24863;&#30693;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#29616;&#25104;&#30340;&#35270;&#35273;&#31995;&#32479;&#23558;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#30340;&#35270;&#22270;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#26368;&#20339;&#30340;&#34892;&#21160;&#26469;&#28385;&#36275;&#23548;&#33322;&#25351;&#20196;&#12290;&#26377;&#20004;&#31181;&#29992;&#20363;&#30340;&#23454;&#39564;&#23545;&#36825;&#31181;&#22522;&#20110;&#35821;&#35328;&#30340;&#23548;&#33322;&#26041;&#27861;&#36827;&#34892;&#20102;&#25506;&#32034;&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#36712;&#36857;&#20197;&#24494;&#35843;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;&#27169;&#25311;&#21040;&#23454;&#38469;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#23558;&#35821;&#35328;&#20316;&#20026;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#30340;&#24863;&#30693;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#29616;&#25104;&#30340;&#35270;&#35273;&#31995;&#32479;&#65288;&#29992;&#20110;&#22270;&#20687;&#23383;&#24149;&#21644;&#29289;&#20307;&#26816;&#27979;&#65289;&#23558;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#20195;&#29702;&#20154;&#30340;&#33258;&#25105;&#20013;&#24515;&#20840;&#26223;&#35270;&#22270;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26681;&#25454;&#24403;&#21069;&#35270;&#22270;&#21644;&#36712;&#36857;&#21382;&#21490;&#36873;&#25321;&#26368;&#20339;&#30340;&#34892;&#21160;&#26469;&#28385;&#36275;&#23548;&#33322;&#25351;&#20196;&#12290;&#19982;&#26631;&#20934;&#35774;&#32622;&#30456;&#27604;&#65292;&#26631;&#20934;&#35774;&#32622;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#20110;&#19982;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#36830;&#32493;&#35270;&#35273;&#29305;&#24449;&#30452;&#25509;&#37197;&#21512;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#65288;&#31163;&#25955;&#30340;&#65289;&#35821;&#35328;&#20316;&#20026;&#24863;&#30693;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;R2R&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#22522;&#20934;&#27979;&#35797;&#20013;&#25506;&#32034;&#20102;&#20004;&#20010;&#29992;&#20363;&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-4&#65289;&#29983;&#25104;&#21512;&#25104;&#36712;&#36857;&#65292;&#20197;&#20415;&#24494;&#35843;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65307;&#20197;&#21450;&#27169;&#25311;&#21040;&#23454;&#38469;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the use of language as a perceptual representation for vision-and-language navigation. Our approach uses off-the-shelf vision systems (for image captioning and object detection) to convert an agent's egocentric panoramic view at each time step into natural language descriptions. We then finetune a pretrained language model to select an action, based on the current view and the trajectory history, that would best fulfill the navigation instructions. In contrast to the standard setup which adapts a pretrained language model to work directly with continuous visual features from pretrained vision models, our approach instead uses (discrete) language as the perceptual representation. We explore two use cases of our language-based navigation (LangNav) approach on the R2R vision-and-language navigation benchmark: generating synthetic trajectories from a prompted large language model (GPT-4) with which to finetune a smaller language model; and sim-to-real transfer where we transfer 
&lt;/p&gt;</description></item><item><title>TabLib&#26159;&#19968;&#20010;&#21253;&#21547;&#19978;&#20159;&#34920;&#26684;&#21644;&#19978;&#30334;&#20159;&#19978;&#19979;&#25991;&#30340;&#25968;&#25454;&#38598;&#65292;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#20351;&#20854;&#22312;&#34920;&#26684;&#27169;&#24577;&#19979;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07875</link><description>&lt;p&gt;
TabLib&#65306;&#19968;&#20010;&#21253;&#21547;&#19978;&#20159;&#34920;&#26684;&#21644;&#19978;&#30334;&#20159;&#19978;&#19979;&#25991;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
TabLib: A Dataset of 627M Tables with Context. (arXiv:2310.07875v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07875
&lt;/p&gt;
&lt;p&gt;
TabLib&#26159;&#19968;&#20010;&#21253;&#21547;&#19978;&#20159;&#34920;&#26684;&#21644;&#19978;&#30334;&#20159;&#19978;&#19979;&#25991;&#30340;&#25968;&#25454;&#38598;&#65292;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#20351;&#20854;&#22312;&#34920;&#26684;&#27169;&#24577;&#19979;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24040;&#22823;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#22312;&#25552;&#21319;&#29616;&#20195;AI&#31995;&#32479;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#27169;&#24577;&#19979;&#30340;&#24615;&#33021;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#34920;&#26684;&#25968;&#25454;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#19982;&#25991;&#26412;&#21644;&#22270;&#20687;&#21487;&#27604;&#25311;&#30340;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#30340;&#25968;&#25454;&#38598;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;"TabLib"&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;6.27&#20159;&#20010;&#34920;&#26684;&#21644;86.7&#20159;&#20010;&#19978;&#19979;&#25991;&#20196;&#29260;&#24635;&#20849;&#36798;&#21040;69 TiB&#30340;&#32534;&#35793;&#25968;&#25454;&#38598;&#12290;TabLib&#30340;&#25968;&#25454;&#26469;&#33258;&#22810;&#20010;&#25991;&#20214;&#26684;&#24335;&#65292;&#21253;&#25324;CSV&#12289;HTML&#12289;SQLite&#12289;PDF&#12289;Excel&#31561;&#65292;&#36825;&#20123;&#25968;&#25454;&#28304;&#33258;GitHub&#21644;Common Crawl&#12290;TabLib&#30340;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#22312;&#34920;&#26684;&#27169;&#24577;&#19979;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#22914;&#21516;&#21407;&#22987;&#30340;&#29992;&#20110;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#22522;&#30784;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;The Pile&#21644;LAION&#12290;
&lt;/p&gt;
&lt;p&gt;
It is well-established that large, diverse datasets play a pivotal role in the performance of modern AI systems for text and image modalities. However, there are no datasets for tabular data of comparable size and diversity to those available for text and images. Thus we present "TabLib'', a compilation of 627 million tables totaling 69 TiB, along with 867B tokens of context. TabLib was extracted from numerous file formats, including CSV, HTML, SQLite, PDF, Excel, and others, sourced from GitHub and Common Crawl. The size and diversity of TabLib offer considerable promise in the table modality, reminiscent of the original promise of foundational datasets for text and images, such as The Pile and LAION.
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#31070;&#32463;&#27979;&#35797;Oracle&#29983;&#25104;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#22312;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#24230;&#37327;&#21644;&#27979;&#35797;&#20805;&#20998;&#24615;&#24230;&#37327;&#20043;&#38388;&#21457;&#29616;&#20102;&#27809;&#26377;&#26174;&#33879;&#30456;&#20851;&#24615;&#12290;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#24230;&#37327;&#39640;&#20294;&#27979;&#35797;&#20805;&#20998;&#24615;&#24230;&#37327;&#20302;&#30340;oracle&#24448;&#24448;&#20855;&#26377;&#22797;&#26434;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.07856</link><description>&lt;p&gt;
&#35780;&#20272;&#31070;&#32463;&#27979;&#35797;Oracle&#29983;&#25104;&#30340;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Assessing Evaluation Metrics for Neural Test Oracle Generation. (arXiv:2310.07856v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07856
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#31070;&#32463;&#27979;&#35797;Oracle&#29983;&#25104;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#22312;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#24230;&#37327;&#21644;&#27979;&#35797;&#20805;&#20998;&#24615;&#24230;&#37327;&#20043;&#38388;&#21457;&#29616;&#20102;&#27809;&#26377;&#26174;&#33879;&#30456;&#20851;&#24615;&#12290;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#24230;&#37327;&#39640;&#20294;&#27979;&#35797;&#20805;&#20998;&#24615;&#24230;&#37327;&#20302;&#30340;oracle&#24448;&#24448;&#20855;&#26377;&#22797;&#26434;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#29616;&#26377;&#30340;Oracle&#29983;&#25104;&#30740;&#31350;&#21644;ChatGPT&#65292;&#24182;&#20174;&#32463;&#39564;&#19978;&#35843;&#26597;&#20102;&#23427;&#20204;&#22312;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#24230;&#37327;&#21644;&#27979;&#35797;&#20805;&#20998;&#24615;&#24230;&#37327;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#20116;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#24230;&#37327;&#21644;&#20004;&#31181;&#27979;&#35797;&#20805;&#20998;&#24615;&#24230;&#37327;&#36827;&#34892;&#20102;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#27979;&#35797;Oracle&#29983;&#25104;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#36816;&#34892;&#65292;&#20197;&#36827;&#34892;&#20998;&#26512;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#24230;&#37327;&#21644;&#27979;&#35797;&#20805;&#20998;&#24615;&#24230;&#37327;&#20043;&#38388;&#27809;&#26377;&#26174;&#33879;&#30340;&#30456;&#20851;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;&#30740;&#31350;&#20013;&#30340;&#25152;&#26377;NOG&#20013;&#65292;ChatGPT&#22312;project activemq-artemis&#19978;&#29983;&#25104;&#30340;oracle&#22312;&#25152;&#26377;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#24230;&#37327;&#20013;&#20855;&#26377;&#26368;&#39640;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#65292;&#22312;&#27979;&#35797;&#20805;&#20998;&#24615;&#24230;&#37327;&#19978;&#65292;&#23427;&#26377;&#26368;&#22810;&#30340;&#39033;&#30446;&#34920;&#29616;&#20986;&#19979;&#38477;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36827;&#34892;&#20102;&#23450;&#24615;&#20998;&#26512;&#65292;&#25506;&#32034;&#20102;&#25105;&#20204;&#35266;&#23519;&#32467;&#26524;&#32972;&#21518;&#30340;&#21407;&#22240;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#24230;&#37327;&#36739;&#39640;&#20294;&#27979;&#35797;&#20805;&#20998;&#24615;&#24230;&#37327;&#36739;&#20302;&#30340;oracle&#24448;&#24448;&#20855;&#26377;&#22797;&#26434;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we revisit existing oracle generation studies plus ChatGPT to empirically investigate the current standing of their performance in both NLG-based and test adequacy metrics. Specifically, we train and run four state-of-the-art test oracle generation models on five NLG-based and two test adequacy metrics for our analysis. We apply two different correlation analyses between these two different sets of metrics. Surprisingly, we found no significant correlation between the NLG-based metrics and test adequacy metrics. For instance, oracles generated from ChatGPT on the project activemq-artemis had the highest performance on all the NLG-based metrics among the studied NOGs, however, it had the most number of projects with a decrease in test adequacy metrics compared to all the studied NOGs. We further conduct a qualitative analysis to explore the reasons behind our observations, we found that oracles with high NLG-based metrics but low test adequacy metrics tend to have complex 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#22312;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#20027;&#35266;&#24615;&#20250;&#36127;&#38754;&#24433;&#21709;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#36825;&#23545;&#20110;&#29702;&#35299;&#21644;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#20855;&#26377;&#37325;&#35201;&#30340;&#21551;&#31034;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.07849</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#65306;&#28508;&#21147;&#21644;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations. (arXiv:2310.07849v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#22312;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#20027;&#35266;&#24615;&#20250;&#36127;&#38754;&#24433;&#21709;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#36825;&#23545;&#20110;&#29702;&#35299;&#21644;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#20855;&#26377;&#37325;&#35201;&#30340;&#21551;&#31034;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25910;&#38598;&#21644;&#25972;&#29702;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#23545;&#20110;&#24320;&#21457;&#20855;&#26377;&#21331;&#36234;&#24615;&#33021;&#30340;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24448;&#24448;&#20276;&#38543;&#30528;&#24040;&#22823;&#30340;&#25104;&#26412;&#21644;&#26102;&#38388;&#25237;&#20837;&#12290;&#30740;&#31350;&#20154;&#21592;&#26368;&#36817;&#24320;&#22987;&#25506;&#32034;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;LLM&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#22312;&#25903;&#25345;&#27169;&#22411;&#35757;&#32451;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#22312;&#19981;&#21516;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#26159;&#19981;&#19968;&#33268;&#30340;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20102;&#35299;&#35843;&#33410;LLM&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#26377;&#25928;&#24615;&#30340;&#22240;&#32032;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20998;&#31867;&#30340;&#20027;&#35266;&#24615;&#22914;&#20309;&#24433;&#21709;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20027;&#35266;&#24615;&#22312;&#20219;&#21153;&#23618;&#38754;&#21644;&#23454;&#20363;&#23618;&#38754;&#19978;&#37117;&#19982;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#21576;&#36127;&#30456;&#20851;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#24037;&#20316;&#23545;&#20110;&#21033;&#29992;LLM&#26469;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#22312;&#28508;&#21147;&#21644;&#38480;&#21046;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks. To better understand factors that moderate the effectiveness of the LLM-generated synthetic data, in this study, we look into how the performance of models trained on these synthetic data may vary with the subjectivity of classification. Our results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data. We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#26805;&#35821;&#20013;&#38382;&#39064;&#30340;&#26500;&#24314;&#20102;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#21160;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#26469;&#22238;&#31572;&#20107;&#23454;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#20851;&#31995;&#19978;&#23637;&#31034;&#20102;&#31995;&#32479;&#30340;&#24212;&#29992;&#12290;&#20998;&#26512;&#20102;&#31995;&#32479;&#30340;&#32570;&#28857;&#20197;&#20415;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.07848</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#21160;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#24335;, &#35299;&#31572;&#26805;&#35821;&#20013;&#38382;&#39064;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Framework for Question-Answering in Sanskrit through Automated Construction of Knowledge Graphs. (arXiv:2310.07848v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26805;&#35821;&#20013;&#38382;&#39064;&#30340;&#26500;&#24314;&#20102;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#21160;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#26469;&#22238;&#31572;&#20107;&#23454;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#20851;&#31995;&#19978;&#23637;&#31034;&#20102;&#31995;&#32479;&#30340;&#24212;&#29992;&#12290;&#20998;&#26512;&#20102;&#31995;&#32479;&#30340;&#32570;&#28857;&#20197;&#20415;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26805;&#35821;&#26159;&#19990;&#30028;&#19978;&#26368;&#22823;&#21644;&#26368;&#22810;&#26679;&#21270;&#30340;&#25991;&#23398;&#20043;&#19968;&#65292;&#28982;&#32780;&#25552;&#21462;&#20854;&#20013;&#30340;&#30693;&#35782;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21407;&#22240;&#21253;&#25324;&#35821;&#35328;&#30340;&#22797;&#26434;&#24615;&#21644;&#26631;&#20934;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#30340;&#21294;&#20047;&#12290;&#26412;&#25991;&#38024;&#23545;&#20174;&#26805;&#35821;&#25991;&#26412;&#20013;&#26500;&#24314;&#29305;&#23450;&#31867;&#22411;&#20851;&#31995;&#30340;&#30693;&#35782;&#22270;&#35889;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#26805;&#35821;&#20013;&#26500;&#24314;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#22238;&#31572;&#20107;&#23454;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25972;&#20307;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#24182;&#22312;&#39532;&#21704;&#24052;&#25289;&#22612;&#21644;&#32599;&#25705;&#34893;&#37027;&#20013;&#30340;&#20154;&#38469;&#20851;&#31995;&#19978;&#23454;&#29616;&#20102;&#20004;&#20010;&#29420;&#31435;&#30340;&#31995;&#32479;&#23454;&#20363;&#65292;&#24182;&#22312;&#38463;&#32946;&#21536;&#38464;&#20013;&#30340;&#25216;&#26415;&#25991;&#26412;&#8212;&#8212;&#12298;&#29983;&#29702;/&#30149;&#29702;&#31192;&#23494;&#12299;&#20013;&#30340;&#21516;&#20041;&#20851;&#31995;&#19978;&#23454;&#29616;&#20102;&#19968;&#20010;&#23454;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31995;&#32479;&#21487;&#20197;&#27491;&#30830;&#22238;&#31572;&#22823;&#32422;50%&#30340;&#20107;&#23454;&#24615;&#38382;&#39064;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35814;&#32454;&#20998;&#26512;&#20102;&#31995;&#32479;&#30340;&#32570;&#28857;&#20197;&#20415;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sanskrit (sa\d{m}sk\d{r}ta) enjoys one of the largest and most varied literature in the whole world. Extracting the knowledge from it, however, is a challenging task due to multiple reasons including complexity of the language and paucity of standard natural language processing tools. In this paper, we target the problem of building knowledge graphs for particular types of relationships from sa\d{m}sk\d{r}ta texts. We build a natural language question-answering system in sa\d{m}sk\d{r}ta that uses the knowledge graph to answer factoid questions. We design a framework for the overall system and implement two separate instances of the system on human relationships from mah\=abh\=arata and r\=am\=aya\d{n}a, and one instance on synonymous relationships from bh\=avaprak\=a\'sa nigha\d{n}\d{t}u, a technical text from \=ayurveda. We show that about 50% of the factoid questions can be answered correctly by the system. More importantly, we analyse the shortcomings of the system in detail for ea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;NLP&#20013;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#27169;&#26495;&#30340;&#38382;&#39064;&#29983;&#25104;&#12290;&#36890;&#36807;&#35780;&#20272;&#20854;&#20248;&#21183;&#21644;&#22266;&#26377;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#21512;&#25104;&#25968;&#25454;&#23545;&#29616;&#20195;Transformer&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#24378;&#35843;&#20102;&#21512;&#25104;&#25968;&#25454;&#19982;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20043;&#38388;&#25152;&#38656;&#30340;&#24494;&#22937;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.07830</link><description>&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#26159;&#21542;&#33021;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#39640;&#25928;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Synthetic Data Make Large Language Models More Efficient?. (arXiv:2310.07830v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;NLP&#20013;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#27169;&#26495;&#30340;&#38382;&#39064;&#29983;&#25104;&#12290;&#36890;&#36807;&#35780;&#20272;&#20854;&#20248;&#21183;&#21644;&#22266;&#26377;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#21512;&#25104;&#25968;&#25454;&#23545;&#29616;&#20195;Transformer&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#24378;&#35843;&#20102;&#21512;&#25104;&#25968;&#25454;&#19982;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20043;&#38388;&#25152;&#38656;&#30340;&#24494;&#22937;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#20986;&#29616;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#32463;&#21382;&#20102;&#28145;&#21051;&#30340;&#21464;&#38761;&#12290;&#30740;&#31350;&#20154;&#21592;&#25345;&#32493;&#38754;&#20020;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#39537;&#21160;&#36825;&#20123;&#27169;&#22411;&#30340;&#39640;&#36136;&#37327;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;NLP&#20013;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#27169;&#26495;&#30340;&#38382;&#39064;&#29983;&#25104;&#12290;&#36890;&#36807;&#35780;&#20272;&#20854;&#20248;&#21183;&#65292;&#21253;&#25324;&#25968;&#25454;&#25193;&#20805;&#28508;&#21147;&#21644;&#32467;&#26500;&#22810;&#26679;&#24615;&#30340;&#24341;&#20837;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#20248;&#28857;&#19982;&#22266;&#26377;&#38480;&#21046;&#36827;&#34892;&#20102;&#23545;&#27604;&#65292;&#22914;&#36807;&#25311;&#21512;&#39118;&#38505;&#21644;&#39044;&#23450;&#20041;&#27169;&#26495;&#25152;&#24102;&#26469;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#32463;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#27169;&#26495;&#30340;&#21512;&#25104;&#25968;&#25454;&#23545;&#29616;&#20195;Transformer&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#24378;&#35843;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20043;&#38388;&#25152;&#38656;&#30340;&#24494;&#22937;&#24179;&#34913;&#21450;&#23558;&#21512;&#25104;&#25968;&#25454;&#25972;&#21512;&#21040;&#27169;&#22411;&#35757;&#32451;&#27969;&#31243;&#20013;&#30340;&#26410;&#26469;&#36712;&#36857;&#26469;&#24635;&#32467;&#12290;&#36825;&#20123;&#21457;&#29616;&#26088;&#22312;&#25351;&#23548;NLP&#20174;&#19994;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing (NLP) has undergone transformative changes with the advent of deep learning methodologies. One challenge persistently confronting researchers is the scarcity of high-quality, annotated datasets that drive these models. This paper explores the nuances of synthetic data generation in NLP, with a focal point on template-based question generation. By assessing its advantages, including data augmentation potential and the introduction of structured variety, we juxtapose these benefits against inherent limitations, such as the risk of overfitting and the constraints posed by pre-defined templates. Drawing from empirical evaluations, we demonstrate the impact of template-based synthetic data on the performance of modern transformer models. We conclude by emphasizing the delicate balance required between synthetic and real-world data, and the future trajectories of integrating synthetic data in model training pipelines. The findings aim to guide NLP practitioners in
&lt;/p&gt;</description></item><item><title>Antarlekhaka&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#20219;&#21153;&#33258;&#28982;&#35821;&#35328;&#26631;&#27880;&#24037;&#20855;&#65292;&#21487;&#29992;&#20110;&#25163;&#21160;&#26631;&#27880;&#19982;NLP&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#23427;&#20860;&#23481;Unicode&#65292;&#25903;&#25345;&#20998;&#24067;&#24335;&#27880;&#37322;&#65292;&#24182;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#12290;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#20854;&#20182;&#24037;&#20855;&#27809;&#26377;&#22788;&#29702;&#30340;&#20219;&#21153;&#65292;&#21363;&#21477;&#23376;&#36793;&#30028;&#26816;&#27979;&#21644;&#20915;&#23450;&#35268;&#33539;&#35789;&#24207;&#65292;&#36825;&#23545;&#20110;&#35799;&#27468;&#24418;&#24335;&#30340;&#25991;&#26412;&#26159;&#37325;&#35201;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.07826</link><description>&lt;p&gt;
Antarlekhaka&#65306;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#20219;&#21153;&#33258;&#28982;&#35821;&#35328;&#26631;&#27880;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Antarlekhaka: A Comprehensive Tool for Multi-task Natural Language Annotation. (arXiv:2310.07826v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07826
&lt;/p&gt;
&lt;p&gt;
Antarlekhaka&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#20219;&#21153;&#33258;&#28982;&#35821;&#35328;&#26631;&#27880;&#24037;&#20855;&#65292;&#21487;&#29992;&#20110;&#25163;&#21160;&#26631;&#27880;&#19982;NLP&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#23427;&#20860;&#23481;Unicode&#65292;&#25903;&#25345;&#20998;&#24067;&#24335;&#27880;&#37322;&#65292;&#24182;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#12290;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#20854;&#20182;&#24037;&#20855;&#27809;&#26377;&#22788;&#29702;&#30340;&#20219;&#21153;&#65292;&#21363;&#21477;&#23376;&#36793;&#30028;&#26816;&#27979;&#21644;&#20915;&#23450;&#35268;&#33539;&#35789;&#24207;&#65292;&#36825;&#23545;&#20110;&#35799;&#27468;&#24418;&#24335;&#30340;&#25991;&#26412;&#26159;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#25512;&#36827;&#38754;&#20020;&#30340;&#20027;&#35201;&#38556;&#30861;&#20043;&#19968;&#26159;&#32570;&#20047;&#29992;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26631;&#27880;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Antarlekhaka&#65292;&#19968;&#20010;&#29992;&#20110;&#25163;&#21160;&#26631;&#27880;&#19982;NLP&#30456;&#20851;&#30340;&#19968;&#25972;&#22871;&#20219;&#21153;&#30340;&#24037;&#20855;&#12290;&#35813;&#24037;&#20855;&#20860;&#23481;Unicode&#65292;&#19982;&#35821;&#35328;&#26080;&#20851;&#65292;&#21487;&#22312;Web&#19978;&#37096;&#32626;&#65292;&#24182;&#25903;&#25345;&#22810;&#20010;&#21516;&#26102;&#27880;&#37322;&#32773;&#30340;&#20998;&#24067;&#24335;&#27880;&#37322;&#12290;&#35813;&#31995;&#32479;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#65292;&#25903;&#25345;8&#20010;&#31867;&#21035;&#30340;&#26631;&#27880;&#20219;&#21153;&#12290;&#36825;&#20123;&#20219;&#21153;&#31867;&#21035;&#21448;&#33021;&#22815;&#25903;&#25345;&#26356;&#22810;&#30340;NLP&#20219;&#21153;&#30340;&#26631;&#27880;&#12290;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#20854;&#20182;&#24037;&#20855;&#27809;&#26377;&#22788;&#29702;&#30340;&#35821;&#35328;&#20219;&#21153;&#65292;&#21363;&#21477;&#23376;&#36793;&#30028;&#26816;&#27979;&#21644;&#20915;&#23450;&#35268;&#33539;&#35789;&#24207;&#65292;&#36825;&#20123;&#23545;&#20110;&#35799;&#27468;&#24418;&#24335;&#30340;&#25991;&#26412;&#26159;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#23567;&#25991;&#26412;&#21333;&#20803;&#30340;&#39034;&#24207;&#26631;&#27880;&#30340;&#24819;&#27861;&#65292;&#20854;&#20013;&#27880;&#37322;&#32773;&#22312;&#22788;&#29702;&#19968;&#20010;&#25991;&#26412;&#21333;&#20803;&#20043;&#21069;&#20250;&#25191;&#34892;&#22810;&#20010;&#19982;&#20043;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the primary obstacles in the advancement of Natural Language Processing (NLP) technologies for low-resource languages is the lack of annotated datasets for training and testing machine learning models. In this paper, we present Antarlekhaka, a tool for manual annotation of a comprehensive set of tasks relevant to NLP. The tool is Unicode-compatible, language-agnostic, Web-deployable and supports distributed annotation by multiple simultaneous annotators. The system sports user-friendly interfaces for 8 categories of annotation tasks. These, in turn, enable the annotation of a considerably larger set of NLP tasks. The task categories include two linguistic tasks not handled by any other tool, namely, sentence boundary detection and deciding canonical word order, which are important tasks for text that is in the form of poetry. We propose the idea of sequential annotation based on small text units, where an annotator performs several tasks related to a single text unit before proc
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#33258;&#22238;&#24402;&#25991;&#26412;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22797;&#21046;&#25805;&#20316;&#26469;&#31649;&#29702;&#25991;&#26412;&#37325;&#21472;&#65292;&#35299;&#20915;&#20102;Seq2Edit&#26041;&#27861;&#22312;&#29983;&#25104;&#28789;&#27963;&#24615;&#21644;&#36328;&#35821;&#35328;&#25512;&#24191;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;GEC&#21644;&#21477;&#23376;&#34701;&#21512;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#19988;&#22312;&#24503;&#35821;&#21644;&#20420;&#35821;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07821</link><description>&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#25991;&#26412;&#32534;&#36753;&#26041;&#27861;&#19982;&#20855;&#26377;&#22797;&#21046;&#24863;&#30693;&#28508;&#22312;&#23545;&#40784;.
&lt;/p&gt;
&lt;p&gt;
Non-autoregressive Text Editing with Copy-aware Latent Alignments. (arXiv:2310.07821v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07821
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#33258;&#22238;&#24402;&#25991;&#26412;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22797;&#21046;&#25805;&#20316;&#26469;&#31649;&#29702;&#25991;&#26412;&#37325;&#21472;&#65292;&#35299;&#20915;&#20102;Seq2Edit&#26041;&#27861;&#22312;&#29983;&#25104;&#28789;&#27963;&#24615;&#21644;&#36328;&#35821;&#35328;&#25512;&#24191;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;GEC&#21644;&#21477;&#23376;&#34701;&#21512;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#19988;&#22312;&#24503;&#35821;&#21644;&#20420;&#35821;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#22312;&#25991;&#26412;&#32534;&#36753;&#39046;&#22495;&#20013;&#23558;Seq2Seq&#36716;&#21464;&#20026;Seq2Edit&#65292;&#26088;&#22312;&#35299;&#20915;&#21069;&#32773;&#20013;&#30340;&#24930;&#33258;&#22238;&#24402;&#25512;&#29702;&#38382;&#39064;&#12290;&#23613;&#31649;&#26377;&#30528;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;Seq2Edit&#26041;&#27861;&#20173;&#28982;&#38754;&#20020;&#30528;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#29983;&#25104;&#30340;&#28789;&#27963;&#24615;&#21644;&#25512;&#24191;&#21040;&#20854;&#20182;&#35821;&#35328;&#30340;&#22256;&#38590;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#33258;&#22238;&#24402;&#25991;&#26412;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#27169;&#20855;&#26377;&#22797;&#21046;&#25805;&#20316;&#30340;&#32534;&#36753;&#36807;&#31243;&#30340;&#28508;&#22312;CTC&#23545;&#40784;&#26469;&#35268;&#36991;&#20197;&#19978;&#38382;&#39064;&#65292;&#20174;&#32780;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#22788;&#29702;&#25991;&#26412;&#37325;&#21472;&#30340;&#32534;&#36753;&#12290;&#25105;&#20204;&#22312; GEC &#21644;&#21477;&#23376;&#34701;&#21512;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;Seq2Edit&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;Seq2Seq&#30456;&#24403;&#25110;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#26377;&#36229;&#36807;4&#20493;&#30340;&#21152;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23427;&#22312;&#24503;&#35821;&#21644;&#20420;&#35821;&#19978;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has witnessed a paradigm shift from Seq2Seq to Seq2Edit in the field of text editing, with the aim of addressing the slow autoregressive inference problem posed by the former. Despite promising results, Seq2Edit approaches still face several challenges such as inflexibility in generation and difficulty in generalizing to other languages. In this work, we propose a novel non-autoregressive text editing method to circumvent the above issues, by modeling the edit process with latent CTC alignments. We make a crucial extension to CTC by introducing the copy operation into the edit space, thus enabling more efficient management of textual overlap in editing. We conduct extensive experiments on GEC and sentence fusion tasks, showing that our proposed method significantly outperforms existing Seq2Edit models and achieves similar or even better results than Seq2Seq with over $4\times$ speedup. Moreover, it demonstrates good generalizability on German and Russian. In-depth analyses 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24230;&#37327;&#24544;&#23454;&#24615;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#23558;&#23631;&#34109;&#20196;&#29260;&#20316;&#20026;&#35774;&#35745;&#20351;&#20854;&#25104;&#20026;&#20998;&#24067;&#20869;&#65292;&#20197;&#35299;&#20915;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#26102;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07819</link><description>&lt;p&gt;
&#21487;&#24230;&#37327;&#24544;&#23454;&#24615;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Faithfulness Measurable Masked Language Models. (arXiv:2310.07819v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24230;&#37327;&#24544;&#23454;&#24615;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#23558;&#23631;&#34109;&#20196;&#29260;&#20316;&#20026;&#35774;&#35745;&#20351;&#20854;&#25104;&#20026;&#20998;&#24067;&#20869;&#65292;&#20197;&#35299;&#20915;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#26102;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20351;&#29992;&#37325;&#35201;&#24615;&#24230;&#37327;&#26469;&#34920;&#36798;&#21738;&#20123;&#20196;&#29260;&#23545;&#20110;&#39044;&#27979;&#24456;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#20123;&#35299;&#37322;&#20855;&#26377;&#35828;&#26381;&#21147;&#65292;&#20294;&#24448;&#24448;&#26159;&#38169;&#35823;&#30340;&#12290;&#22240;&#27492;&#65292;&#27979;&#37327;&#23427;&#20204;&#30340;&#24544;&#23454;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#19968;&#31181;&#24230;&#37327;&#26631;&#20934;&#26159;&#22914;&#26524;&#20196;&#29260;&#30830;&#23454;&#24456;&#37325;&#35201;&#65292;&#37027;&#20040;&#23631;&#34109;&#23427;&#20204;&#24212;&#35813;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#21464;&#24046;&#12290;&#28982;&#32780;&#65292;&#20196;&#29260;&#23631;&#34109;&#20250;&#24341;&#20837;&#21306;&#22495;&#22806;&#38382;&#39064;&#65292;&#32780;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#35745;&#31639;&#19978;&#24456;&#26114;&#36149;&#24182;&#19988;&#20351;&#29992;&#20195;&#29702;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20854;&#20182;&#25351;&#26631;&#30340;&#36866;&#29992;&#33539;&#22260;&#38750;&#24120;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22266;&#26377;&#30340;&#24544;&#23454;&#24615;&#21487;&#24230;&#37327;&#27169;&#22411;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#35843;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#35813;&#26041;&#27861;&#23558;&#23631;&#34109;&#20196;&#29260;&#20316;&#20026;&#35774;&#35745;&#20351;&#20854;&#25104;&#20026;&#20998;&#24067;&#20869;&#12290;&#36825;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#29616;&#26377;&#26041;&#27861;&#23436;&#20840;&#19982;&#27169;&#22411;&#26080;&#20851;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#19981;&#36866;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#26469;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common approach to explain NLP models, is to use importance measures that express which tokens are important for a prediction. Unfortunately, such explanations are often wrong despite being persuasive. Therefore, it is essential to measure their faithfulness. One such metric is if tokens are truly important, then masking them should result in worse model performance. However, token masking introduces out-of-distribution issues and existing solutions are computationally expensive and employ proxy-models. Furthermore, other metrics are very limited in scope. In this work, we propose an inherently faithfulness measurable model that addresses these challenges. This is achieved by using a novel fine-tuning method that incorporates masking, such that masking tokens become in-distribution by design. This differs from existing approaches, which are completely model-agnostic but are inapplicable in practice. We demonstrate the generality of our approach by applying it to various tasks and val
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#21477;&#23376;&#31867;&#27604;&#30340;&#33021;&#21147;&#19982;&#20854;&#32534;&#30721;&#21477;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.07818</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31867;&#27604;&#35782;&#21035;&#19982;&#21477;&#23376;&#32467;&#26500;&#32534;&#30721;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Exploring the Relationship between Analogy Identification and Sentence Structure Encoding in Large Language Models. (arXiv:2310.07818v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07818
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#21477;&#23376;&#31867;&#27604;&#30340;&#33021;&#21147;&#19982;&#20854;&#32534;&#30721;&#21477;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#31867;&#27604;&#22312;&#20154;&#31867;&#35748;&#30693;&#21644;&#35821;&#35328;&#33021;&#21147;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#23545;&#20110;&#8220;A&#23545;B&#23601;&#20687;C&#23545;D&#8221;&#36825;&#31181;&#24418;&#24335;&#30340;&#35789;&#35821;&#31867;&#27604;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#28041;&#21450;&#26356;&#38271;&#25991;&#26412;&#30340;&#31867;&#27604;&#65292;&#22914;&#21477;&#23376;&#21644;&#21477;&#23376;&#38598;&#21512;&#65292;&#20256;&#36798;&#31867;&#27604;&#24847;&#20041;&#30340;&#38382;&#39064;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#24403;&#21069;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#31038;&#21306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35782;&#21035;&#27492;&#31867;&#31867;&#27604;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#33021;&#21147;&#32972;&#21518;&#30340;&#21407;&#22240;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#31350;&#12290;&#27492;&#22806;&#65292;LLMs&#22312;&#20854;&#23884;&#20837;&#20013;&#32534;&#30721;&#35821;&#35328;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#26174;&#33879;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#20010;LLMs&#35782;&#21035;&#21477;&#23376;&#31867;&#27604;&#30340;&#33021;&#21147;&#19982;&#20854;&#32534;&#30721;&#21477;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#30340;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#30340;&#31867;&#27604;&#35782;&#21035;&#33021;&#21147;&#19982;&#20854;&#32534;&#30721;&#33021;&#21147;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying analogies plays a pivotal role in human cognition and language proficiency. In the last decade, there has been extensive research on word analogies in the form of ``A is to B as C is to D.'' However, there is a growing interest in analogies that involve longer text, such as sentences and collections of sentences, which convey analogous meanings. While the current NLP research community evaluates the ability of Large Language Models (LLMs) to identify such analogies, the underlying reasons behind these abilities warrant deeper investigation. Furthermore, the capability of LLMs to encode both syntactic and semantic structures of language within their embeddings has garnered significant attention with the surge in their utilization. In this work, we examine the relationship between the abilities of multiple LLMs to identify sentence analogies, and their capacity to encode syntactic and semantic structures. Through our analysis, we find that analogy identification ability of LL
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;ID&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;LMINDEXER&#12290;</title><link>http://arxiv.org/abs/2310.07815</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#20041;&#32034;&#24341;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models As Semantic Indexers. (arXiv:2310.07815v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;ID&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;LMINDEXER&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#26631;&#35782;&#31526;&#65288;ID&#65289;&#26159;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#27010;&#24565;&#65292;&#26088;&#22312;&#20445;&#30041;&#23545;&#35937;&#65288;&#22914;&#25991;&#26723;&#21644;&#39033;&#65289;&#20869;&#37096;&#30340;&#35821;&#20041;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#37319;&#29992;&#20004;&#38454;&#27573;&#27969;&#31243;&#26469;&#23398;&#20064;&#35821;&#20041;ID&#65292;&#39318;&#20808;&#20351;&#29992;&#29616;&#25104;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#33719;&#21462;&#23884;&#20837;&#65292;&#24182;&#26681;&#25454;&#23884;&#20837;&#26469;&#25512;&#23548;ID&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#27493;&#39588;&#37117;&#20250;&#24341;&#20837;&#28508;&#22312;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#19988;&#25991;&#26412;&#32534;&#30721;&#22120;&#29983;&#25104;&#30340;&#28508;&#22312;&#31354;&#38388;&#20869;&#30340;&#23884;&#20837;&#20998;&#24067;&#36890;&#24120;&#19982;&#35821;&#20041;&#32034;&#24341;&#25152;&#38656;&#30340;&#39044;&#26399;&#20998;&#24067;&#23384;&#22312;&#22266;&#26377;&#30340;&#19981;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#19968;&#20010;&#26082;&#33021;&#23398;&#20064;&#25991;&#26723;&#30340;&#35821;&#20041;&#34920;&#31034;&#21448;&#33021;&#21516;&#26102;&#23398;&#20064;&#20854;&#20998;&#23618;&#32467;&#26500;&#30340;&#26041;&#27861;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#20026;&#35821;&#20041;ID&#26159;&#31163;&#25955;&#21644;&#39034;&#24207;&#32467;&#26500;&#30340;&#65292;&#24182;&#19988;&#35821;&#20041;&#30417;&#30563;&#26159;&#19981;&#20805;&#20998;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LMINDEXER&#65292;&#23427;&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;ID&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic identifier (ID) is an important concept in information retrieval that aims to preserve the semantics of objects such as documents and items inside their IDs. Previous studies typically adopt a two-stage pipeline to learn semantic IDs by first procuring embeddings using off-the-shelf text encoders and then deriving IDs based on the embeddings. However, each step introduces potential information loss and there is usually an inherent mismatch between the distribution of embeddings within the latent space produced by text encoders and the anticipated distribution required for semantic indexing. Nevertheless, it is non-trivial to design a method that can learn the document's semantic representations and its hierarchical structure simultaneously, given that semantic IDs are discrete and sequentially structured, and the semantic supervision is deficient. In this paper, we introduce LMINDEXER, a self-supervised framework to learn semantic IDs with a generative language model. We tackl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26222;&#36866;&#24615;&#30340;&#35748;&#30693;&#24189;&#40664;&#26426;&#21046;&#65292;&#24314;&#31435;&#22312;&#32422;&#26463;&#30340;&#27010;&#24565;&#19978;&#65292;&#36890;&#36807;&#37325;&#21472;&#32422;&#26463;&#30340;&#35266;&#23519;&#26469;&#35299;&#37322;&#24189;&#40664;&#30340;&#20135;&#29983;&#12290;</title><link>http://arxiv.org/abs/2310.07803</link><description>&lt;p&gt;
&#24189;&#40664;&#30340;&#19968;&#33324;&#26426;&#21046;&#65306;&#37325;&#26032;&#23450;&#20041;&#35821;&#20041;&#37325;&#21472;
&lt;/p&gt;
&lt;p&gt;
A general mechanism of humor: reformulating the semantic overlap. (arXiv:2310.07803v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26222;&#36866;&#24615;&#30340;&#35748;&#30693;&#24189;&#40664;&#26426;&#21046;&#65292;&#24314;&#31435;&#22312;&#32422;&#26463;&#30340;&#27010;&#24565;&#19978;&#65292;&#36890;&#36807;&#37325;&#21472;&#32422;&#26463;&#30340;&#35266;&#23519;&#26469;&#35299;&#37322;&#24189;&#40664;&#30340;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26222;&#36866;&#24615;&#30340;&#35748;&#30693;&#24189;&#40664;&#26426;&#21046;&#65292;&#19981;&#20165;&#38480;&#20110;&#35821;&#35328;&#20132;&#27969;&#12290;&#23427;&#20511;&#37492;&#20102;Raskin&#20851;&#20110;&#24773;&#33410;&#37325;&#21472;&#30340;&#27010;&#24565;&#65292;&#24182;&#31526;&#21512;&#19981;&#19968;&#33268;&#24615;-&#35299;&#20915;&#29702;&#35770;&#26694;&#26550;&#65292;&#20294;&#26159;&#24314;&#31435;&#22312;&#32422;&#26463;&#30340;&#27010;&#24565;&#19978;&#65292;&#21363;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#25277;&#35937;&#23545;&#24212;&#20851;&#31995;&#12290;&#26681;&#25454;&#36825;&#31181;&#35266;&#28857;&#65292;&#24773;&#33410;&#37325;&#21472;&#26159;&#19968;&#31181;&#26356;&#25277;&#35937;&#25551;&#36848;&#30340;&#29616;&#35937;&#65292;&#21363;&#32422;&#26463;&#37325;&#21472;&#12290;&#25991;&#20013;&#24341;&#20837;&#20102;&#34987;&#24573;&#35270;&#30340;&#35770;&#35777;&#27010;&#24565;&#65292;&#29992;&#20110;&#25551;&#36848;&#20004;&#20010;&#37325;&#21472;&#30340;&#32422;&#26463;&#8212;&#8212;&#26126;&#26174;&#30340;&#21644;&#38544;&#34109;&#30340;&#12290;&#23427;&#20204;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#24182;&#19981;&#30452;&#25509;&#32534;&#30721;&#22312;&#35805;&#35821;&#20013;&#65292;&#32780;&#26159;&#30001;&#35805;&#35821;&#26263;&#31034;&#65292;&#23427;&#20204;&#30340;&#37325;&#21472;&#23548;&#33268;&#22312;&#20132;&#27969;&#30340;&#35805;&#35821;&#23618;&#38754;&#19978;&#20135;&#29983;&#21478;&#19968;&#31181;&#37325;&#21472;&#65292;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#26174;&#38706;&#20102;&#20986;&#26469;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#65292;&#21796;&#36215;&#36825;&#31181;&#32422;&#26463;&#26159;&#21548;&#20247;&#35299;&#37322;&#35805;&#35821;&#30340;&#25512;&#29702;&#36807;&#31243;&#30340;&#35748;&#30693;&#25928;&#24212;&#12290;&#25105;&#20204;&#22522;&#20110;Hofstadter&#20851;&#20110;&#26263;&#31034;&#30340;&#29702;&#35770;&#20551;&#35774;&#36825;&#19968;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article proposes a cognitive mechanism of humour of general applicability, not restricted to verbal communication. It is indebted to Raskin's concept of script overlap, and conforms to the incongruity-resolution theoretical framework, but it is built on the notion of constraint, an abstract correspondence between sets of data. Under this view, script overlap is an outcome of a more abstractly described phenomenon, constraint overlap. The important concept of the overlooked argument is introduced to characterise the two overlapping constraints -- overt and covert. Their inputs and outputs are not directly encoded in utterances, but implicated by them, and their overlap results in another overlap at the level of the communicated utterances, that the incongruity reveals. Our hypothesis assumes as a given that the evocation of such constraints is a cognitive effect of the inferential process by which a hearer interprets utterances. We base this assumption on Hofstadter's theory of ana
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OnEFET&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#26412;&#20307;&#32467;&#26500;&#30340;&#27599;&#20010;&#33410;&#28857;&#28155;&#21152;&#39069;&#22806;&#20449;&#24687;&#65292;&#21253;&#25324;&#23454;&#20363;&#20449;&#24687;&#21644;&#20027;&#39064;&#20449;&#24687;&#65292;&#26469;&#22686;&#24378;&#38646;&#26679;&#26412;&#32454;&#31890;&#24230;&#23454;&#20307;&#31867;&#22411;&#23450;&#20041;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25351;&#23548;&#32454;&#31890;&#24230;&#23454;&#20307;&#31867;&#22411;&#30340;&#35782;&#21035;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.07795</link><description>&lt;p&gt;
&#20026;&#26377;&#25928;&#30340;&#32454;&#31890;&#24230;&#23454;&#20307;&#31867;&#22411;&#23450;&#20041;&#36827;&#34892;&#26412;&#20307;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Ontology Enrichment for Effective Fine-grained Entity Typing. (arXiv:2310.07795v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07795
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OnEFET&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#26412;&#20307;&#32467;&#26500;&#30340;&#27599;&#20010;&#33410;&#28857;&#28155;&#21152;&#39069;&#22806;&#20449;&#24687;&#65292;&#21253;&#25324;&#23454;&#20363;&#20449;&#24687;&#21644;&#20027;&#39064;&#20449;&#24687;&#65292;&#26469;&#22686;&#24378;&#38646;&#26679;&#26412;&#32454;&#31890;&#24230;&#23454;&#20307;&#31867;&#22411;&#23450;&#20041;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25351;&#23548;&#32454;&#31890;&#24230;&#23454;&#20307;&#31867;&#22411;&#30340;&#35782;&#21035;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#31890;&#24230;&#23454;&#20307;&#31867;&#22411;&#23450;&#20041;&#65288;FET&#65289;&#26159;&#26681;&#25454;&#19978;&#19979;&#25991;&#20449;&#24687;&#35782;&#21035;&#23454;&#20307;&#25552;&#21450;&#20013;&#29305;&#23450;&#23454;&#20307;&#31867;&#22411;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#30340;FET&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#26631;&#27880;&#65292;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#24369;&#30417;&#30563;&#25110;&#38646;&#26679;&#26412;&#26041;&#27861;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21482;&#25552;&#20379;&#26412;&#20307;&#30340;&#38646;&#26679;&#26412;FET&#35774;&#32622;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26412;&#20307;&#32467;&#26500;&#32570;&#20047;&#20016;&#23500;&#30340;&#25903;&#25345;&#20449;&#24687;&#65292;&#29978;&#33267;&#21547;&#26377;&#27169;&#31946;&#30340;&#20851;&#31995;&#65292;&#23548;&#33268;&#23427;&#20204;&#22312;&#25351;&#23548;FET&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#12290;&#26368;&#36817;&#21457;&#23637;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#20960;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#26412;&#20307;&#30340;&#20114;&#21160;&#65292;&#21487;&#33021;&#22312;&#38646;&#26679;&#26412;FET&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OnEFET&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#31867;&#22411;&#30340;&#39069;&#22806;&#20449;&#24687;&#23545;&#26412;&#20307;&#32467;&#26500;&#20013;&#30340;&#27599;&#20010;&#33410;&#28857;&#36827;&#34892;&#22686;&#24378;&#65306;&#23454;&#20363;&#20449;&#24687;&#29992;&#20110;&#35757;&#32451;&#26679;&#26412;&#22686;&#24378;&#65292;&#20027;&#39064;&#20449;&#24687;&#29992;&#20110;&#23558;&#31867;&#22411;&#19982;&#19978;&#19979;&#25991;&#20851;&#32852;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-grained entity typing (FET) is the task of identifying specific entity types at a fine-grained level for entity mentions based on their contextual information. Conventional methods for FET require extensive human annotation, which is time-consuming and costly. Recent studies have been developing weakly supervised or zero-shot approaches. We study the setting of zero-shot FET where only an ontology is provided. However, most existing ontology structures lack rich supporting information and even contain ambiguous relations, making them ineffective in guiding FET. Recently developed language models, though promising in various few-shot and zero-shot NLP tasks, may face challenges in zero-shot FET due to their lack of interaction with task-specific ontology. In this study, we propose OnEFET, where we (1) enrich each node in the ontology structure with two types of extra information: instance information for training sample augmentation and topic information to relate types to contexts
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenTKG&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.07793</link><description>&lt;p&gt;
GenTKG: &#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
GenTKG: Generative Forecasting on Temporal Knowledge Graph. (arXiv:2310.07793v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07793
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenTKG&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#21457;&#20102;&#23545;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;(tKG)&#39046;&#22495;&#30340;&#20852;&#36259;&#65292;&#20854;&#20013;&#20256;&#32479;&#30340;&#22522;&#20110;&#23884;&#20837;&#21644;&#35268;&#21017;&#30340;&#27169;&#22411;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#39044;&#35757;&#32451;&#30340;LLM&#26159;&#21542;&#33021;&#22815;&#29702;&#35299;&#32467;&#26500;&#21270;&#30340;&#26102;&#38388;&#20851;&#31995;&#25968;&#25454;&#65292;&#24182;&#21462;&#20195;&#23427;&#20204;&#25104;&#20026;&#26102;&#38388;&#20851;&#31995;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#26102;&#38388;&#30693;&#35782;&#39044;&#27979;&#24341;&#20837;&#29983;&#25104;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;LLM&#21487;&#20197;&#22788;&#29702;&#30340;&#24207;&#21015;&#33258;&#28982;&#34920;&#36798;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#30340;&#40511;&#27807;&#65292;&#22312;tKG&#30340;&#24222;&#22823;&#25968;&#25454;&#37327;&#21644;&#24494;&#35843;LLM&#30340;&#24040;&#22823;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#20063;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#65292;&#31216;&#20026;GenTKG&#65292;&#23427;&#22312;tKG&#19978;&#25191;&#34892;&#29983;&#25104;&#24335;&#39044;&#27979;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;GenTKG&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancements in large language models (LLMs) have ignited interest in the temporal knowledge graph (tKG) domain, where conventional carefully designed embedding-based and rule-based models dominate. The question remains open of whether pre-trained LLMs can understand structured temporal relational data and replace them as the foundation model for temporal relational forecasting. Therefore, we bring temporal knowledge forecasting into the generative setting. However, challenges occur in the huge chasms between complex temporal graph data structure and sequential natural expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs. To address these challenges, we propose a novel retrieval augmented generation framework that performs generative forecasting on tKGs named GenTKG, which combines a temporal logical rule-based retrieval strategy and lightweight parameter-efficient instruction tuning. Extensive experiments hav
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#22120;&#26080;&#20851;&#30340;&#30693;&#35782;&#36873;&#25321;&#26041;&#27861;GATE&#65292;&#23558;&#30693;&#35782;&#36873;&#25321;&#25918;&#32622;&#22312;&#29983;&#25104;&#20043;&#21069;&#65292;&#21487;&#20197;&#20943;&#23569;&#21518;&#32493;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#30340;&#36127;&#25285;&#65292;&#24182;&#20026;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#37327;&#30340;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.07659</link><description>&lt;p&gt;
&#20248;&#20808;&#36873;&#25321;&#30693;&#35782;&#65306;&#38754;&#21521;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#30340;&#29983;&#25104;&#22120;&#26080;&#20851;&#30340;&#30693;&#35782;&#39044;&#36873;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Well Begun is Half Done: Generator-agnostic Knowledge Pre-Selection for Knowledge-Grounded Dialogue. (arXiv:2310.07659v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#22120;&#26080;&#20851;&#30340;&#30693;&#35782;&#36873;&#25321;&#26041;&#27861;GATE&#65292;&#23558;&#30693;&#35782;&#36873;&#25321;&#25918;&#32622;&#22312;&#29983;&#25104;&#20043;&#21069;&#65292;&#21487;&#20197;&#20943;&#23569;&#21518;&#32493;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#30340;&#36127;&#25285;&#65292;&#24182;&#20026;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#37327;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#31995;&#32479;&#20013;&#65292;&#20934;&#30830;&#30340;&#30693;&#35782;&#36873;&#25321;&#33267;&#20851;&#37325;&#35201;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#26469;&#32452;&#32455;&#29616;&#26377;&#30340;&#25991;&#29486;&#65292;&#21363;&#23558;&#30693;&#35782;&#36873;&#25321;&#19982;&#29983;&#25104;&#22120;&#32806;&#21512;&#65292;&#24182;&#25918;&#32622;&#22312;&#29983;&#25104;&#20043;&#21069;&#21644;&#20043;&#21518;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#31532;&#19977;&#20010;&#26410;&#28145;&#20837;&#30740;&#31350;&#30340;&#30740;&#31350;&#31867;&#21035;&#65292;&#23427;&#19981;&#20165;&#21487;&#20197;&#25552;&#21069;&#20934;&#30830;&#36873;&#25321;&#30693;&#35782;&#65292;&#36824;&#21487;&#20197;&#20943;&#23569;&#21518;&#32493;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#30340;&#23398;&#20064;&#12289;&#35843;&#25972;&#21644;&#35299;&#37322;&#36127;&#25285;&#65292;&#29305;&#21035;&#26159;LLMs&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#22120;&#26080;&#20851;&#30340;&#30693;&#35782;&#36873;&#25321;&#26041;&#27861;GATE&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#30340;&#30693;&#35782;&#32467;&#26500;&#21644;&#21487;&#21464;&#30340;&#30693;&#35782;&#35201;&#27714;&#20013;&#36873;&#25321;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#30693;&#35782;&#26469;&#20026;&#21518;&#32493;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#20934;&#22791;&#30693;&#35782;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;GATE&#30340;&#20248;&#36234;&#24615;&#65292;&#24182;&#34920;&#26126;&#22312;&#29983;&#25104;&#20043;&#21069;&#36827;&#34892;&#30693;&#35782;&#36873;&#25321;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#20294;&#26377;&#25928;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#20419;&#20351;LLMs&#65288;&#22914;ChatGPT&#65289;&#29983;&#25104;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate knowledge selection is critical in knowledge-grounded dialogue systems. Towards a closer look at it, we offer a novel perspective to organize existing literature, i.e., knowledge selection coupled with, after, and before generation. We focus on the third under-explored category of study, which can not only select knowledge accurately in advance, but has the advantage to reduce the learning, adjustment, and interpretation burden of subsequent response generation models, especially LLMs. We propose GATE, a generator-agnostic knowledge selection method, to prepare knowledge for subsequent response generation models by selecting context-related knowledge among different knowledge structures and variable knowledge requirements. Experimental results demonstrate the superiority of GATE, and indicate that knowledge selection before generation is a lightweight yet effective way to facilitate LLMs (e.g., ChatGPT) to generate more informative responses.
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#32771;&#34385;&#20102;&#22522;&#20110;DNA&#24207;&#21015;&#30340;BERT-like&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;K-mer&#37325;&#21472;&#26631;&#35760;&#21270;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#38454;&#27573;&#21644;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#37117;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2310.07644</link><description>&lt;p&gt;
&#37325;&#26032;&#32771;&#34385;&#22522;&#20110;DNA&#24207;&#21015;&#30340;BERT-like&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Rethinking the BERT-like Pretraining for DNA Sequences. (arXiv:2310.07644v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07644
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#32771;&#34385;&#20102;&#22522;&#20110;DNA&#24207;&#21015;&#30340;BERT-like&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;K-mer&#37325;&#21472;&#26631;&#35760;&#21270;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#38454;&#27573;&#21644;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#37117;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#25104;&#21151;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#30340;&#36235;&#21183;&#26085;&#30410;&#22686;&#38271;&#12290;&#29305;&#21035;&#26159;&#22522;&#20110;DNA&#24207;&#21015;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#22240;&#20854;&#25429;&#25417;&#22522;&#22240;&#30340;&#36890;&#29992;&#20449;&#24687;&#30340;&#28508;&#21147;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DNA&#24207;&#21015;&#39044;&#35757;&#32451;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30452;&#25509;&#24341;&#20837;&#30340;BERT&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#32570;&#20047;&#20840;&#38754;&#30340;&#29702;&#35299;&#21644;&#19987;&#38376;&#23450;&#21046;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#25506;&#32034;&#24615;&#23454;&#39564;&#65292;&#24182;&#33719;&#24471;&#20102;&#20960;&#20010;&#26377;&#21551;&#21457;&#24615;&#30340;&#35266;&#23519;&#32467;&#26524;&#65306;1&#65289;&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#38454;&#27573;&#65292;&#20351;&#29992;K-mer&#37325;&#21472;&#26631;&#35760;&#21270;&#32780;&#19981;&#26159;K-mer&#38750;&#37325;&#21472;&#26631;&#35760;&#21270;&#26102;&#65292;&#37325;&#21472;&#21644;&#38750;&#37325;&#21472;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#22343;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;2&#65289;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;K-mer&#37325;&#21472;&#26631;&#35760;&#21270;&#20250;&#36805;&#36895;&#20135;&#29983;&#28165;&#26224;&#30340;K-mer&#23884;&#20837;&#65292;&#24182;&#23558;&#25439;&#22833;&#38477;&#20302;&#21040;&#38750;&#24120;&#20302;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the success of large-scale pretraining in NLP, there is an increasing trend of applying it to the domain of life sciences. In particular, pretraining methods based on DNA sequences have garnered growing attention due to their potential to capture generic information about genes. However, existing pretraining methods for DNA sequences largely rely on direct adoptions of BERT pretraining from NLP, lacking a comprehensive understanding and a specifically tailored approach. To address this research gap, we first conducted a series of exploratory experiments and gained several insightful observations: 1) In the fine-tuning phase of downstream tasks, when using K-mer overlapping tokenization instead of K-mer non-overlapping tokenization, both overlapping and non-overlapping pretraining weights show consistent performance improvement.2) During the pre-training process, using K-mer overlapping tokenization quickly produces clear K-mer embeddings and reduces the loss to a very low level, w
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLM-TSE&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#29992;&#25143;&#38190;&#20837;&#30340;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#35821;&#20041;&#32447;&#32034;&#65292;&#20197;&#22686;&#24378;&#30446;&#26631;&#35828;&#35805;&#20154;&#25552;&#21462;(TSE)&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07284</link><description>&lt;p&gt;
&#25171;&#23383;&#20542;&#21548;&#40481;&#23614;&#37202;&#20250;&#65306;&#25991;&#26412;&#24341;&#23548;&#30340;&#30446;&#26631;&#35828;&#35805;&#20154;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Typing to Listen at the Cocktail Party: Text-Guided Target Speaker Extraction. (arXiv:2310.07284v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07284
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLM-TSE&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#29992;&#25143;&#38190;&#20837;&#30340;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#35821;&#20041;&#32447;&#32034;&#65292;&#20197;&#22686;&#24378;&#30446;&#26631;&#35828;&#35805;&#20154;&#25552;&#21462;(TSE)&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#25317;&#26377;&#19968;&#31181;&#22312;&#22797;&#26434;&#30340;&#22768;&#23398;&#29615;&#22659;&#20013;&#26377;&#36873;&#25321;&#24615;&#22320;&#19987;&#27880;&#20110;&#24863;&#20852;&#36259;&#30340;&#22768;&#38899;&#28304;&#30340;&#38750;&#20961;&#33021;&#21147;&#65292;&#36890;&#24120;&#31216;&#20026;&#40481;&#23614;&#37202;&#20250;&#22330;&#26223;&#12290;&#20026;&#20102;&#22312;&#26426;&#22120;&#20013;&#22797;&#21046;&#36825;&#31181;&#24341;&#20154;&#27880;&#30446;&#30340;&#21548;&#35273;&#27880;&#24847;&#33021;&#21147;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#30446;&#26631;&#35828;&#35805;&#20154;&#25552;&#21462;(TSE)&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#21033;&#29992;&#30446;&#26631;&#35828;&#35805;&#20154;&#30340;&#39044;&#20808;&#27880;&#20876;&#32447;&#32034;&#26469;&#25552;&#21462;&#24863;&#20852;&#36259;&#30340;&#22768;&#28304;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#26223;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#20102;&#39044;&#20808;&#27880;&#20876;&#32447;&#32034;&#30340;&#21487;&#33021;&#21464;&#21270;&#29978;&#33267;&#32570;&#22833;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#25972;&#21512;&#21040;&#29616;&#26377;TSE&#27169;&#22411;&#20013;&#20197;&#22686;&#24378;&#20854;&#28789;&#27963;&#24615;&#21644;&#21487;&#25511;&#24615;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LLM-TSE&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20174;&#29992;&#25143;&#30340;&#38190;&#20837;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#35821;&#20041;&#32447;&#32034;&#65292;&#36825;&#20123;&#32447;&#32034;&#21487;&#20197;&#34917;&#20805;&#39044;&#20808;&#27880;&#20876;&#30340;&#32447;&#32034;&#25110;&#29420;&#31435;&#24037;&#20316;&#20197;&#25511;&#21046;TSE&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans possess an extraordinary ability to selectively focus on the sound source of interest amidst complex acoustic environments, commonly referred to as cocktail party scenarios. In an attempt to replicate this remarkable auditory attention capability in machines, target speaker extraction (TSE) models have been developed. These models leverage the pre-registered cues of the target speaker to extract the sound source of interest. However, the effectiveness of these models is hindered in real-world scenarios due to the potential variation or even absence of pre-registered cues. To address this limitation, this study investigates the integration of natural language to enhance the flexibility and controllability of existing TSE models. Specifically, we propose a model named LLM-TSE, wherein a large language model (LLM) to extract useful semantic cues from the user's typed text input, which can complement the pre-registered cues or work independently to control the TSE process. Our exper
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#23588;&#20854;&#26159;BioBERT&#65289;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;&#30740;&#31350;&#31361;&#20986;&#20102;BioBERT&#23545;&#20110;&#35299;&#20915;&#19982;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30456;&#20851;&#20219;&#21153;&#30340;&#29305;&#23450;&#35201;&#27714;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07282</link><description>&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#26512;&#65306;BioBERT &#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT. (arXiv:2310.07282v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#23588;&#20854;&#26159;BioBERT&#65289;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;&#30740;&#31350;&#31361;&#20986;&#20102;BioBERT&#23545;&#20110;&#35299;&#20915;&#19982;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30456;&#20851;&#20219;&#21153;&#30340;&#29305;&#23450;&#35201;&#27714;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#23588;&#20854;&#26159;BioBERT&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#30740;&#31350;&#12290;&#23427;&#39318;&#20808;&#24443;&#24213;&#26816;&#26597;&#20102;&#20808;&#21069;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#27861;&#30340;&#24773;&#20917;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#30340;&#23616;&#38480;&#21644;&#25361;&#25112;&#12290;&#38543;&#21518;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;BioBERT&#25972;&#21512;&#21040;&#21307;&#30103;&#20445;&#20581;&#24212;&#29992;&#20013;&#30340;&#36335;&#24452;&#65292;&#31361;&#20986;&#20854;&#36866;&#29992;&#20110;&#35299;&#20915;&#19982;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30456;&#20851;&#20219;&#21153;&#30340;&#29305;&#23450;&#35201;&#27714;&#12290;&#35813;&#20998;&#26512;&#27010;&#36848;&#20102;&#29992;&#20110;&#38024;&#23545;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#29420;&#29305;&#38656;&#27714;&#24494;&#35843;BioBERT&#30340;&#31995;&#32479;&#26041;&#27861;&#35770;&#12290;&#36825;&#20010;&#26041;&#27861;&#21253;&#25324;&#20174;&#21508;&#31181;&#21307;&#30103;&#20445;&#20581;&#26469;&#28304;&#25910;&#38598;&#25968;&#25454;&#65292;&#20026;&#35782;&#21035;&#21307;&#30103;&#23454;&#20307;&#21644;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#31561;&#20219;&#21153;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#65292;&#24182;&#24212;&#29992;&#19987;&#38376;&#38024;&#23545;&#22788;&#29702;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#22797;&#26434;&#24615;&#30340;&#39044;&#22788;&#29702;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper conducts a comprehensive investigation into applying large language models, particularly on BioBERT, in healthcare. It begins with thoroughly examining previous natural language processing (NLP) approaches in healthcare, shedding light on the limitations and challenges these methods face. Following that, this research explores the path that led to the incorporation of BioBERT into healthcare applications, highlighting its suitability for addressing the specific requirements of tasks related to biomedical text mining. The analysis outlines a systematic methodology for fine-tuning BioBERT to meet the unique needs of the healthcare domain. This approach includes various components, including the gathering of data from a wide range of healthcare sources, data annotation for tasks like identifying medical entities and categorizing them, and the application of specialized preprocessing techniques tailored to handle the complexities found in biomedical texts. Additionally, the pape
&lt;/p&gt;</description></item><item><title>Jaynes Machine&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24494;&#32467;&#26500;&#30340;&#26032;&#29702;&#35770;&#65292;&#39044;&#27979;&#20102;&#25152;&#26377;&#39640;&#36830;&#25509;&#23618;&#20855;&#26377;&#20998;&#24067;&#20026;&#23545;&#25968;&#27491;&#24577;&#20998;&#24067;&#30340;&#36890;&#29992;&#36830;&#25509;&#24378;&#24230;&#24494;&#32467;&#26500;&#65292;&#24182;&#22312;&#29702;&#24819;&#26465;&#20214;&#19979;&#39044;&#27979;&#20102;${\mu}$&#21644;${\sigma}$&#22312;&#25152;&#26377;&#32593;&#32476;&#30340;&#25152;&#26377;&#23618;&#20013;&#26159;&#30456;&#21516;&#30340;&#12290;&#23454;&#35777;&#25968;&#25454;&#25903;&#25345;&#36825;&#20123;&#39044;&#27979;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#26469;&#20943;&#23569;&#35757;&#32451;&#22823;&#35268;&#27169;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2310.06960</link><description>&lt;p&gt;
Jaynes Machine: &#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#24494;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Jaynes Machine: The universal microstructure of deep neural networks. (arXiv:2310.06960v1 [cond-mat.stat-mech] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06960
&lt;/p&gt;
&lt;p&gt;
Jaynes Machine&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24494;&#32467;&#26500;&#30340;&#26032;&#29702;&#35770;&#65292;&#39044;&#27979;&#20102;&#25152;&#26377;&#39640;&#36830;&#25509;&#23618;&#20855;&#26377;&#20998;&#24067;&#20026;&#23545;&#25968;&#27491;&#24577;&#20998;&#24067;&#30340;&#36890;&#29992;&#36830;&#25509;&#24378;&#24230;&#24494;&#32467;&#26500;&#65292;&#24182;&#22312;&#29702;&#24819;&#26465;&#20214;&#19979;&#39044;&#27979;&#20102;${\mu}$&#21644;${\sigma}$&#22312;&#25152;&#26377;&#32593;&#32476;&#30340;&#25152;&#26377;&#23618;&#20013;&#26159;&#30456;&#21516;&#30340;&#12290;&#23454;&#35777;&#25968;&#25454;&#25903;&#25345;&#36825;&#20123;&#39044;&#27979;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#26469;&#20943;&#23569;&#35757;&#32451;&#22823;&#35268;&#27169;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24494;&#32467;&#26500;&#30340;&#26032;&#29702;&#35770;&#12290;&#20351;&#29992;&#21517;&#20026;&#32479;&#35745;&#36828;&#21160;&#21147;&#23398;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#23427;&#26159;&#32479;&#35745;&#28909;&#21147;&#23398;&#21644;&#28508;&#22312;&#21338;&#24328;&#29702;&#35770;&#30340;&#27010;&#24565;&#32508;&#21512;&#65292;&#25105;&#20204;&#39044;&#27979;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25152;&#26377;&#39640;&#36830;&#25509;&#23618;&#20855;&#26377;&#20998;&#24067;&#20026;&#23545;&#25968;&#27491;&#24577;&#20998;&#24067;&#30340;&#36890;&#29992;&#36830;&#25509;&#24378;&#24230;&#24494;&#32467;&#26500;&#65288;$LN({\mu}, {\sigma})$)&#12290;&#27492;&#22806;&#65292;&#22312;&#29702;&#24819;&#26465;&#20214;&#19979;&#65292;&#29702;&#35770;&#39044;&#27979;&#23545;&#20110;&#25152;&#26377;&#32593;&#32476;&#30340;&#25152;&#26377;&#23618;&#65292;${\mu}$&#21644;${\sigma}$&#26159;&#30456;&#21516;&#30340;&#12290;&#36825;&#26159;&#30001;&#25152;&#26377;&#36830;&#25509;&#31454;&#20105;&#24182;&#23545;&#25972;&#20307;&#25439;&#22833;&#20989;&#25968;&#26368;&#23567;&#21270;&#20570;&#20986;&#30456;&#21516;&#26377;&#25928;&#25928;&#29992;&#30340;&#22871;&#21033;&#22343;&#34913;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#20196;&#20154;&#24778;&#35766;&#30340;&#39044;&#27979;&#24471;&#21040;&#20102;&#26469;&#33258;&#20845;&#20010;&#22823;&#35268;&#27169;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#35777;&#25968;&#25454;&#30340;&#25903;&#25345;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#26469;&#20943;&#23569;&#35757;&#32451;&#22823;&#35268;&#27169;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#65292;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel theory of the microstructure of deep neural networks. Using a theoretical framework called statistical teleodynamics, which is a conceptual synthesis of statistical thermodynamics and potential game theory, we predict that all highly connected layers of deep neural networks have a universal microstructure of connection strengths that is distributed lognormally ($LN({\mu}, {\sigma})$). Furthermore, under ideal conditions, the theory predicts that ${\mu}$ and ${\sigma}$ are the same for all layers in all networks. This is shown to be the result of an arbitrage equilibrium where all connections compete and contribute the same effective utility towards the minimization of the overall loss function. These surprising predictions are shown to be supported by empirical data from six large-scale deep neural networks in real life. We also discuss how these results can be exploited to reduce the amount of data, time, and computational resources needed to train large deep neural
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SpikeCLIP&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#23454;&#29616;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#27169;&#24577;&#25193;&#23637;&#65292;&#24182;&#22312;&#33021;&#28304;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.06488</link><description>&lt;p&gt;
SpikeCLIP&#65306;&#19968;&#31181;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural Network. (arXiv:2310.06488v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SpikeCLIP&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#23454;&#29616;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#27169;&#24577;&#25193;&#23637;&#65292;&#24182;&#22312;&#33021;&#28304;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#24050;&#32463;&#35777;&#26126;&#20854;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#39046;&#22495;&#20013;&#33021;&#22815;&#23454;&#29616;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#33021;&#25928;&#25552;&#39640;&#21644;&#31526;&#21512;&#29983;&#29289;&#21512;&#29702;&#24615;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#31181;&#21333;&#27169;&#24577;&#30340;SNNs&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30340;&#24773;&#26223;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#21463;&#21040;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SpikeCLIP&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#8220;&#23545;&#40784;&#39044;&#35757;&#32451;+&#21452;&#25439;&#22833;&#24494;&#35843;&#8221;&#30340;&#20004;&#27493;&#39588;&#37197;&#26041;&#65292;&#26469;&#35299;&#20915;&#33033;&#20914;&#35745;&#31639;&#32972;&#26223;&#19979;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#24120;&#29992;&#30340;&#29992;&#20110;&#22810;&#27169;&#24577;&#27169;&#22411;&#35780;&#20272;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#65292;SNNs&#21462;&#24471;&#20102;&#19982;&#20854;DNNs&#23545;&#24212;&#29289;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#20102;&#33021;&#28304;&#28040;&#32791;&#12290;&#27492;&#22806;&#65292;SpikeCLIP&#22312;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#20445;&#25345;&#20102;&#31283;&#23450;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) have demonstrated the capability to achieve comparable performance to deep neural networks (DNNs) in both visual and linguistic domains while offering the advantages of improved energy efficiency and adherence to biological plausibility. However, the extension of such single-modality SNNs into the realm of multimodal scenarios remains an unexplored territory. Drawing inspiration from the concept of contrastive language-image pre-training (CLIP), we introduce a novel framework, named SpikeCLIP, to address the gap between two modalities within the context of spike-based computing through a two-step recipe involving ``Alignment Pre-training + Dual-Loss Fine-tuning". Extensive experiments demonstrate that SNNs achieve comparable results to their DNN counterparts while significantly reducing energy consumption across a variety of datasets commonly used for multimodal model evaluation. Furthermore, SpikeCLIP maintains robust performance in image classification 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24212;&#29992;&#8220;&#19987;&#23478;&#30340;&#20056;&#31215;&#8221;&#65288;PoE&#65289;&#25216;&#26415;&#26469;&#20943;&#36731;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24230;&#20559;&#24046;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#20027;&#35201;&#30340;&#19987;&#23478;&#20851;&#27880;&#29702;&#35299;&#20154;&#31867;&#24847;&#22270;&#65292;&#32780;&#20559;&#35265;&#19987;&#23478;&#21017;&#33268;&#21147;&#20110;&#35782;&#21035;&#21644;&#25429;&#25417;&#38271;&#24230;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2310.05199</link><description>&lt;p&gt;
&#23485;&#26494;&#30340;&#22068;&#21767;&#20250;&#20351;&#33337;&#27785;&#27809;&#65306;&#20943;&#36731;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24230;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback. (arXiv:2310.05199v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24212;&#29992;&#8220;&#19987;&#23478;&#30340;&#20056;&#31215;&#8221;&#65288;PoE&#65289;&#25216;&#26415;&#26469;&#20943;&#36731;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24230;&#20559;&#24046;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#20027;&#35201;&#30340;&#19987;&#23478;&#20851;&#27880;&#29702;&#35299;&#20154;&#31867;&#24847;&#22270;&#65292;&#32780;&#20559;&#35265;&#19987;&#23478;&#21017;&#33268;&#21147;&#20110;&#35782;&#21035;&#21644;&#25429;&#25417;&#38271;&#24230;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#26159;&#37325;&#35201;&#30340;&#26725;&#26753;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#21644;&#31038;&#20250;&#20215;&#20540;&#35266;&#23545;&#40784;&#12290;&#36825;&#31181;&#23545;&#40784;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#31867;&#21453;&#39304;&#35821;&#26009;&#24211;&#26469;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#65292;&#28982;&#21518;&#29992;&#20110;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22870;&#21169;&#27169;&#22411;&#24120;&#24120;&#20250;&#25214;&#21040;&#32469;&#36807;&#39044;&#26399;&#30446;&#26631;&#30340;&#25463;&#24452;&#65292;&#38169;&#35823;&#22320;&#20551;&#35774;&#20154;&#31867;&#26356;&#21916;&#27426;&#36739;&#38271;&#30340;&#22238;&#31572;&#12290;&#38271;&#24230;&#20559;&#24046;&#30340;&#20986;&#29616;&#24120;&#24120;&#20250;&#23548;&#33268;&#27169;&#22411;&#20542;&#21521;&#20110;&#36739;&#38271;&#30340;&#36755;&#20986;&#65292;&#20294;&#24182;&#19981;&#24847;&#21619;&#30528;&#36825;&#20123;&#36755;&#20986;&#20013;&#26377;&#26356;&#22810;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24212;&#29992;&#20102;&#8220;&#19987;&#23478;&#30340;&#20056;&#31215;&#8221;&#65288;PoE&#65289;&#25216;&#26415;&#26469;&#23558;&#22870;&#21169;&#24314;&#27169;&#19982;&#24207;&#21015;&#38271;&#24230;&#30340;&#24433;&#21709;&#20998;&#31163;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#20027;&#35201;&#30340;&#19987;&#23478;&#20851;&#27880;&#29702;&#35299;&#20154;&#31867;&#24847;&#22270;&#65292;&#32780;&#20559;&#35265;&#19987;&#23478;&#21017;&#33268;&#21147;&#20110;&#35782;&#21035;&#21644;&#25429;&#25417;&#38271;&#24230;&#20559;&#24046;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#20559;&#35265;&#30340;&#23398;&#20064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25200;&#21160;&#36827;&#20837;&#20559;&#24046;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback serves as a crucial bridge, aligning large language models with human and societal values. This alignment requires a vast corpus of human feedback to learn a reward model, which is subsequently used to finetune language models. However, we have identified that the reward model often finds shortcuts to bypass its intended objectives, misleadingly assuming that humans prefer longer responses. The emergence of length bias often induces the model to favor longer outputs, yet it doesn't equate to an increase in helpful information within these outputs. In this paper, we propose an innovative solution, applying the Product-of-Experts (PoE) technique to separate reward modeling from the influence of sequence length. In our framework, the main expert concentrates on understanding human intents, while the biased expert targets the identification and capture of length bias. To further enhance the learning of bias, we introduce perturbations into the bia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550; TEMPO&#65292;&#36890;&#36807;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#30340;&#20004;&#20010;&#37325;&#35201;&#24402;&#32435;&#20559;&#24046;&#65292;&#21363;&#23558;&#22797;&#26434;&#20132;&#20114;&#20998;&#35299;&#21644;&#24341;&#20837;&#22522;&#20110;&#36873;&#25321;&#30340;&#25552;&#31034;&#26469;&#26377;&#25928;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.04948</link><description>&lt;p&gt;
TEMPO: &#22522;&#20110;&#25552;&#31034;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#27169;&#22411;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting. (arXiv:2310.04948v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550; TEMPO&#65292;&#36890;&#36807;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#30340;&#20004;&#20010;&#37325;&#35201;&#24402;&#32435;&#20559;&#24046;&#65292;&#21363;&#23558;&#22797;&#26434;&#20132;&#20114;&#20998;&#35299;&#21644;&#24341;&#20837;&#22522;&#20110;&#36873;&#25321;&#30340;&#25552;&#31034;&#26469;&#26377;&#25928;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#23613;&#31649;&#22312;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#30340;&#21516;&#26102;&#65292;&#26368;&#22909;&#30340;&#26550;&#26500;&#22312;&#19981;&#21516;&#24212;&#29992;&#21644;&#39046;&#22495;&#20043;&#38388;&#24046;&#24322;&#24456;&#22823;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#65292;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;(GPT)&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#22312;&#21508;&#31181;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25506;&#32034;&#26159;&#21542;GPT&#31867;&#22411;&#30340;&#26550;&#26500;&#21487;&#20197;&#23545;&#26102;&#38388;&#24207;&#21015;&#20135;&#29983;&#26377;&#25928;&#30340;&#24433;&#21709;&#65292;&#25429;&#25417;&#20854;&#20869;&#22312;&#21160;&#24577;&#23646;&#24615;&#24182;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;TEMPO&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#30340;&#20004;&#31181;&#37325;&#35201;&#24402;&#32435;&#20559;&#24046;&#26469;&#39044;&#35757;&#32451;&#27169;&#22411;&#65306;(i) &#23545;&#36235;&#21183;&#12289;&#23395;&#33410;&#21644;&#27531;&#24046;&#25104;&#20998;&#22797;&#26434;&#20132;&#20114;&#30340;&#20998;&#35299;&#65307;&#21644;(ii) &#25552;&#20986;&#22522;&#20110;&#36873;&#25321;&#30340;&#25552;&#31034;&#20197;&#20415;&#20110;&#38750;&#20998;&#24067;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the selection-based prompts to facilitate distribution adaptation in non-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22522;&#20110;&#22122;&#22768;&#25200;&#21160;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;transformer T5&#27169;&#22411;&#29983;&#25104;&#29420;&#29305;&#19988;&#36830;&#36143;&#30340;&#21475;&#21495;&#65292;&#21516;&#26102;&#23558;&#20844;&#21496;&#21644;&#21697;&#29260;&#30340;&#25551;&#36848;&#32435;&#20837;&#21040;&#29983;&#25104;&#36807;&#31243;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21475;&#21495;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.04472</link><description>&lt;p&gt;
&#22122;&#22768;&#25200;&#21160;&#19979;&#30340;&#26377;&#25928;&#21475;&#21495;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Effective Slogan Generation with Noise Perturbation. (arXiv:2310.04472v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22522;&#20110;&#22122;&#22768;&#25200;&#21160;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;transformer T5&#27169;&#22411;&#29983;&#25104;&#29420;&#29305;&#19988;&#36830;&#36143;&#30340;&#21475;&#21495;&#65292;&#21516;&#26102;&#23558;&#20844;&#21496;&#21644;&#21697;&#29260;&#30340;&#25551;&#36848;&#32435;&#20837;&#21040;&#29983;&#25104;&#36807;&#31243;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21475;&#21495;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#21495;&#22312;&#24314;&#31435;&#20844;&#21496;&#21697;&#29260;&#24418;&#35937;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#21475;&#21495;&#34987;&#26399;&#26395;&#20197;&#20196;&#20154;&#38590;&#24536;&#21644;&#35752;&#20154;&#21916;&#27426;&#30340;&#26041;&#24335;&#21453;&#26144;&#20844;&#21496;&#30340;&#24895;&#26223;&#21644;&#21697;&#29260;&#30340;&#20215;&#20540;&#20027;&#24352;&#12290;&#33258;&#21160;&#21270;&#29983;&#25104;&#20855;&#26377;&#36825;&#20123;&#29305;&#28857;&#30340;&#21475;&#21495;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#21457;&#23637;&#21644;&#27979;&#35797;&#20102;&#20855;&#26377;&#21477;&#27861;&#25511;&#21046;&#21644;&#25688;&#35201;&#27169;&#22411;&#30340;&#21475;&#21495;&#29983;&#25104;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#29983;&#25104;&#29420;&#29305;&#30340;&#21475;&#21495;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;transformer T5&#27169;&#22411;&#21644;&#26032;&#25552;&#20986;&#30340;1:N&#21305;&#37197;&#23545;&#25968;&#25454;&#38598;&#30340;&#22122;&#22768;&#25200;&#21160;&#12290;&#35813;&#26041;&#27861;&#22312;&#29983;&#25104;&#29420;&#29305;&#19988;&#36830;&#36143;&#30340;&#21475;&#21495;&#26041;&#38754;&#36215;&#21040;&#20102;&#20419;&#36827;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#23558;&#20844;&#21496;&#21644;&#21697;&#29260;&#30340;&#25551;&#36848;&#32435;&#20837;&#21040;&#21475;&#21495;&#29983;&#25104;&#20013;&#12290;&#25105;&#20204;&#26681;&#25454;ROUGE1&#12289;ROUGEL&#21644;&#20313;&#24358;&#30456;&#20284;&#24615;&#25351;&#26631;&#35780;&#20272;&#29983;&#25104;&#30340;&#21475;&#21495;&#65292;&#24182;&#36890;&#36807;&#20154;&#20026;&#20027;&#20307;&#35780;&#20272;&#23427;&#20204;&#30340;&#29420;&#29305;&#24615;&#12289;&#36830;&#36143;&#24615;&#21644;&#27969;&#30021;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;o
&lt;/p&gt;
&lt;p&gt;
Slogans play a crucial role in building the brand's identity of the firm. A slogan is expected to reflect firm's vision and brand's value propositions in memorable and likeable ways. Automating the generation of slogans with such characteristics is challenging. Previous studies developted and tested slogan generation with syntactic control and summarization models which are not capable of generating distinctive slogans. We introduce a a novel apporach that leverages pre-trained transformer T5 model with noise perturbation on newly proposed 1:N matching pair dataset. This approach serves as a contributing fator in generting distinctive and coherent slogans. Turthermore, the proposed approach incorporates descriptions about the firm and brand into the generation of slogans. We evaluate generated slogans based on ROUGE1, ROUGEL and Cosine Similarity metrics and also assess them with human subjects in terms of slogan's distinctiveness, coherence, and fluency. The results demonstrate that o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MetaTool&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#20855;&#26377;&#24037;&#20855;&#20351;&#29992;&#24847;&#35782;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#36873;&#25321;&#24037;&#20855;&#12290;&#22522;&#20934;&#20013;&#21253;&#21547;&#19968;&#20010;&#21517;&#20026;ToolE&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#31181;&#31867;&#22411;&#30340;&#29992;&#25143;&#26597;&#35810;&#65292;&#29992;&#20110;&#35302;&#21457;LLMs&#20351;&#29992;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2310.03128</link><description>&lt;p&gt;
MetaTool&#22522;&#20934;&#65306;&#20915;&#23450;&#26159;&#21542;&#20351;&#29992;&#24037;&#20855;&#21644;&#36873;&#25321;&#20351;&#29992;&#21738;&#20010;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
MetaTool Benchmark: Deciding Whether to Use Tools and Which to Use. (arXiv:2310.03128v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MetaTool&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#20855;&#26377;&#24037;&#20855;&#20351;&#29992;&#24847;&#35782;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#36873;&#25321;&#24037;&#20855;&#12290;&#22522;&#20934;&#20013;&#21253;&#21547;&#19968;&#20010;&#21517;&#20026;ToolE&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#31181;&#31867;&#22411;&#30340;&#29992;&#25143;&#26597;&#35810;&#65292;&#29992;&#20110;&#35302;&#21457;LLMs&#20351;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#33021;&#21147;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#30740;&#31350;&#20851;&#27880;LLMs&#30340;&#24037;&#20855;&#21033;&#29992;&#33021;&#21147;&#12290;&#23427;&#20204;&#20027;&#35201;&#30740;&#31350;&#20102;LLMs&#22914;&#20309;&#26377;&#25928;&#22320;&#19982;&#32473;&#23450;&#30340;&#29305;&#23450;&#24037;&#20855;&#21512;&#20316;&#12290;&#28982;&#32780;&#65292;&#22312;LLMs&#20805;&#24403;&#26234;&#33021;&#20307;&#30340;&#22330;&#26223;&#20013;&#65292;&#20363;&#22914;AutoGPT&#21644;MetaGPT&#24212;&#29992;&#20013;&#65292;LLMs&#34987;&#26399;&#26395;&#21442;&#19982;&#28041;&#21450;&#26159;&#21542;&#20351;&#29992;&#24037;&#20855;&#20197;&#21450;&#20174;&#21487;&#29992;&#24037;&#20855;&#38598;&#20013;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#24037;&#20855;&#26469;&#28385;&#36275;&#29992;&#25143;&#35831;&#27714;&#30340;&#22797;&#26434;&#20915;&#31574;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MetaTool&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#26159;&#21542;&#20855;&#26377;&#24037;&#20855;&#20351;&#29992;&#24847;&#35782;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#36873;&#25321;&#24037;&#20855;&#30340;&#22522;&#20934;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#35813;&#22522;&#20934;&#20013;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;ToolE&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20197;&#35302;&#21457;LLMs&#20351;&#29992;&#24037;&#20855;&#30340;&#25552;&#31034;&#24418;&#24335;&#20986;&#29616;&#30340;&#21508;&#31181;&#31867;&#22411;&#30340;&#29992;&#25143;&#26597;&#35810;&#65292;&#21253;&#25324;&#21333;&#19968;&#24037;&#20855;&#21644;&#22810;&#31181;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have garnered significant attention due to their impressive natural language processing (NLP) capabilities. Recently, many studies have focused on the tool utilization ability of LLMs. They primarily investigated how LLMs effectively collaborate with given specific tools. However, in scenarios where LLMs serve as intelligent agents, as seen in applications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate decision-making processes that involve deciding whether to employ a tool and selecting the most suitable tool(s) from a collection of available tools to fulfill user requests. Therefore, in this paper, we introduce MetaTool, a benchmark designed to evaluate whether LLMs have tool usage awareness and can correctly choose tools. Specifically, we create a dataset called ToolE within the benchmark. This dataset contains various types of user queries in the form of prompts that trigger LLMs to use tools, including both single-tool and multi-too
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20154;&#24037;&#35780;&#20272;&#25351;&#26631;&#19981;&#32479;&#19968;&#30340;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#20110;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#31995;&#32479;&#30340;&#35780;&#20272;&#65292;&#31361;&#20986;&#20102;&#36755;&#20837;&#19982;&#36755;&#20986;&#36136;&#37327;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2310.01917</link><description>&lt;p&gt;
&#20998;&#23618;&#35780;&#20272;&#26694;&#26550;&#65306;&#20154;&#24037;&#35780;&#20272;&#30340;&#26368;&#20339;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Evaluation Framework: Best Practices for Human Evaluation. (arXiv:2310.01917v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01917
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20154;&#24037;&#35780;&#20272;&#25351;&#26631;&#19981;&#32479;&#19968;&#30340;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#20110;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#31995;&#32479;&#30340;&#35780;&#20272;&#65292;&#31361;&#20986;&#20102;&#36755;&#20837;&#19982;&#36755;&#20986;&#36136;&#37327;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#35780;&#20272;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#23427;&#35780;&#20272;&#20102;&#24320;&#21457;&#31995;&#32479;&#30340;&#36136;&#37327;&#21644;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#31995;&#32479;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#22312;NLP&#20013;&#32570;&#20047;&#24191;&#27867;&#25509;&#21463;&#30340;&#20154;&#24037;&#35780;&#20272;&#25351;&#26631;&#65292;&#38459;&#30861;&#20102;&#19981;&#21516;&#31995;&#32479;&#20043;&#38388;&#30340;&#20844;&#24179;&#27604;&#36739;&#21644;&#24314;&#31435;&#26222;&#36941;&#30340;&#35780;&#20272;&#26631;&#20934;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#20154;&#24037;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#24191;&#27867;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;NLP&#35780;&#20272;&#26041;&#27861;&#20013;&#30340;&#20960;&#20010;&#32570;&#21475;&#12290;&#36825;&#20123;&#32570;&#21475;&#25104;&#20026;&#25105;&#20204;&#24320;&#21457;&#33258;&#24049;&#30340;&#20998;&#23618;&#35780;&#20272;&#26694;&#26550;&#30340;&#21160;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#29305;&#21035;&#26159;&#22312;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;NLP&#31995;&#32479;&#24615;&#33021;&#34920;&#31034;&#26041;&#38754;&#12290;&#25105;&#20204;&#23558;&#27492;&#26694;&#26550;&#24212;&#29992;&#20110;&#35780;&#20272;&#24320;&#21457;&#30340;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#20849;&#29983;&#27169;&#22411;&#20013;&#34987;&#20351;&#29992;&#12290;&#32467;&#26524;&#31361;&#20986;&#20102;&#36755;&#20837;&#19982;&#36755;&#20986;&#36136;&#37327;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24378;&#35843;&#20102;&#35780;&#20272;&#31995;&#32479;&#24615;&#33021;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human evaluation plays a crucial role in Natural Language Processing (NLP) as it assesses the quality and relevance of developed systems, thereby facilitating their enhancement. However, the absence of widely accepted human evaluation metrics in NLP hampers fair comparisons among different systems and the establishment of universal assessment standards. Through an extensive analysis of existing literature on human evaluation metrics, we identified several gaps in NLP evaluation methodologies. These gaps served as motivation for developing our own hierarchical evaluation framework. The proposed framework offers notable advantages, particularly in providing a more comprehensive representation of the NLP system's performance. We applied this framework to evaluate the developed Machine Reading Comprehension system, which was utilized within a human-AI symbiosis model. The results highlighted the associations between the quality of inputs and outputs, underscoring the necessity to evaluate 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29615;&#24418;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#22359;&#35745;&#31639;&#21644;&#36890;&#20449;&#37325;&#21472;&#30340;&#26041;&#24335;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#35299;&#20915;&#20102;Transformer&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#28040;&#38500;&#21333;&#20010;&#35774;&#22791;&#23545;&#20869;&#23384;&#30340;&#32422;&#26463;&#65292;&#20351;&#24471;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#24207;&#21015;&#38271;&#24230;&#33021;&#22815;&#26356;&#38271;&#12290;</title><link>http://arxiv.org/abs/2310.01889</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#22359;Transformer&#30340;&#29615;&#24418;&#27880;&#24847;&#21147;&#35299;&#20915;&#36817;&#26080;&#38480;&#19978;&#19979;&#25991;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Ring Attention with Blockwise Transformers for Near-Infinite Context. (arXiv:2310.01889v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29615;&#24418;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#22359;&#35745;&#31639;&#21644;&#36890;&#20449;&#37325;&#21472;&#30340;&#26041;&#24335;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#35299;&#20915;&#20102;Transformer&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#28040;&#38500;&#21333;&#20010;&#35774;&#22791;&#23545;&#20869;&#23384;&#30340;&#32422;&#26463;&#65292;&#20351;&#24471;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#24207;&#21015;&#38271;&#24230;&#33021;&#22815;&#26356;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#39318;&#36873;&#26550;&#26500;&#65292;&#22312;&#24191;&#27867;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#23637;&#31034;&#20986;&#20102;&#38750;&#20961;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;Transformer&#23545;&#20869;&#23384;&#30340;&#38656;&#27714;&#38480;&#21046;&#20102;&#23427;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#23545;&#20110;&#28041;&#21450;&#25193;&#23637;&#24207;&#21015;&#25110;&#38271;&#26399;&#20381;&#36182;&#30340;&#20219;&#21153;&#32780;&#35328;&#23384;&#22312;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#21363;&#29615;&#24418;&#27880;&#24847;&#21147;(Ring Attention)&#65292;&#23427;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#30340;&#20998;&#22359;&#35745;&#31639;&#23558;&#38271;&#24207;&#21015;&#20998;&#24067;&#21040;&#22810;&#20010;&#35774;&#22791;&#19978;&#65292;&#21516;&#26102;&#23558;&#20851;&#38190;-&#20540;&#22359;&#30340;&#36890;&#20449;&#19982;&#20998;&#22359;&#27880;&#24847;&#21147;&#30340;&#35745;&#31639;&#37325;&#21472;&#12290;&#36890;&#36807;&#22788;&#29702;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#21516;&#26102;&#20445;&#25345;&#20869;&#23384;&#25928;&#29575;&#65292;&#29615;&#24418;&#27880;&#24847;&#21147;&#20351;&#24471;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#24207;&#21015;&#27604;&#20043;&#21069;&#30340;&#20869;&#23384;&#39640;&#25928;Transformer&#33021;&#22815;&#22810;&#20986;&#35774;&#22791;&#25968;&#37327;&#20493;&#65292;&#26377;&#25928;&#22320;&#28040;&#38500;&#20102;&#21333;&#20010;&#35774;&#22791;&#23545;&#20869;&#23384;&#30340;&#32422;&#26463;&#12290;&#22312;&#35821;&#35328;&#27169;&#22411;&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving extended sequences or long-term dependencies. We present a distinct approach, Ring Attention, which leverages blockwise computation of self-attention to distribute long sequences across multiple devices while concurrently overlapping the communication of key-value blocks with the computation of blockwise attention. By processing longer input sequences while maintaining memory efficiency, Ring Attention enables training and inference of sequences that are device count times longer than those of prior memory-efficient Transformers, effectively eliminating the memory constraints imposed by individual devices. Extensive experiments on language modeling tasks demonstrate the effecti
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#28389;&#29992;&#65292;&#21628;&#21505;&#35748;&#35782;&#21040;&#36825;&#20123;&#25361;&#25112;&#30340;&#32039;&#36843;&#24615;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#28145;&#24230;&#20266;&#36896;&#12289;&#21512;&#25104;&#36523;&#20221;&#24694;&#24847;&#27963;&#21160;&#20197;&#21450;&#34394;&#20551;&#20449;&#24687;&#21644;&#27450;&#35784;&#26041;&#38754;&#21487;&#33021;&#24102;&#26469;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.00737</link><description>&lt;p&gt;
&#12298;GenAI&#23545;&#25239;&#20154;&#24615;&#65306;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37034;&#24694;&#24212;&#29992;&#12299;
&lt;/p&gt;
&lt;p&gt;
GenAI Against Humanity: Nefarious Applications of Generative Artificial Intelligence and Large Language Models. (arXiv:2310.00737v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00737
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#28389;&#29992;&#65292;&#21628;&#21505;&#35748;&#35782;&#21040;&#36825;&#20123;&#25361;&#25112;&#30340;&#32039;&#36843;&#24615;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#28145;&#24230;&#20266;&#36896;&#12289;&#21512;&#25104;&#36523;&#20221;&#24694;&#24847;&#27963;&#21160;&#20197;&#21450;&#34394;&#20551;&#20449;&#24687;&#21644;&#27450;&#35784;&#26041;&#38754;&#21487;&#33021;&#24102;&#26469;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#25216;&#26415;&#30340;&#22855;&#36857;&#65292;&#20197;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#27169;&#24335;&#20869;&#23481;&#29983;&#25104;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#32780;&#21463;&#21040;&#36190;&#25196;&#65292;&#23427;&#20204;&#25215;&#35834;&#24102;&#26469;&#19968;&#20010;&#21464;&#38761;&#30340;&#26410;&#26469;&#12290;&#20294;&#23601;&#20687;&#25152;&#26377;&#24378;&#22823;&#30340;&#24037;&#20855;&#19968;&#26679;&#65292;&#23427;&#20204;&#20063;&#26377;&#20854;&#38452;&#24433;&#23384;&#22312;&#12290;&#24819;&#35937;&#19968;&#19979;&#29983;&#27963;&#22312;&#19968;&#20010;&#28145;&#24230;&#20266;&#36896;&#19982;&#29616;&#23454;&#26080;&#27861;&#21306;&#20998;&#12289;&#21512;&#25104;&#36523;&#20221;&#32452;&#32455;&#24694;&#24847;&#27963;&#21160;&#12289;&#20197;&#21450;&#26377;&#30528;&#26080;&#19982;&#20262;&#27604;&#31934;&#30830;&#24230;&#30340;&#26377;&#38024;&#23545;&#24615;&#30340;&#34394;&#20551;&#20449;&#24687;&#25110;&#27450;&#35784;&#25163;&#27861;&#30340;&#19990;&#30028;&#12290;&#27426;&#36814;&#26469;&#21040;GenAI&#24212;&#29992;&#30340;&#40657;&#26263;&#38754;&#12290;&#26412;&#25991;&#19981;&#20165;&#26159;&#25506;&#32034;GenAI&#21644;LLMs&#28508;&#22312;&#28389;&#29992;&#30340;&#26053;&#31243;&#65292;&#20063;&#26159;&#21628;&#21505;&#35748;&#35782;&#21040;&#38754;&#20020;&#30340;&#25361;&#25112;&#30340;&#32039;&#36843;&#24615;&#12290;&#22312;&#25105;&#20204;&#33322;&#34892;&#20110;&#34394;&#20551;&#20449;&#24687;&#27963;&#21160;&#12289;&#24694;&#24847;&#20869;&#23481;&#29983;&#25104;&#19982;&#31934;&#23494;&#24694;&#24847;&#36719;&#20214;&#26500;&#24314;&#30340;&#28023;&#27915;&#20013;&#65292;&#25105;&#20204;&#23558;&#25581;&#31034;&#36825;&#22330;&#25105;&#20204;&#27491;&#22312;&#35265;&#35777;&#30340;GenAI&#38761;&#21629;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) are marvels of technology; celebrated for their prowess in natural language processing and multimodal content generation, they promise a transformative future. But as with all powerful tools, they come with their shadows. Picture living in a world where deepfakes are indistinguishable from reality, where synthetic identities orchestrate malicious campaigns, and where targeted misinformation or scams are crafted with unparalleled precision. Welcome to the darker side of GenAI applications. This article is not just a journey through the meanders of potential misuse of GenAI and LLMs, but also a call to recognize the urgency of the challenges ahead. As we navigate the seas of misinformation campaigns, malicious content generation, and the eerie creation of sophisticated malware, we'll uncover the societal implications that ripple through the GenAI revolution we are witnessing. From AI-powered botnets on social med
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#65288;DocRE&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#19982;&#21477;&#23376;&#32423;&#20851;&#31995;&#25277;&#21462;&#30456;&#27604;&#65292;DocRE&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#20998;&#26512;&#65292;&#28041;&#21450;&#36328;&#36234;&#22810;&#20010;&#21477;&#23376;&#25110;&#27573;&#33853;&#30340;&#20851;&#31995;&#25277;&#21462;&#65292;&#21487;&#29992;&#20110;&#26500;&#24314;&#21644;&#33258;&#21160;&#22635;&#20805;&#30693;&#35782;&#24211;&#12290;</title><link>http://arxiv.org/abs/2309.16396</link><description>&lt;p&gt;
&#23545;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#30340;&#32508;&#21512;&#35843;&#26597;&#65288;2016-2022&#65289;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Document-level Relation Extraction (2016-2022). (arXiv:2309.16396v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16396
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#65288;DocRE&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#19982;&#21477;&#23376;&#32423;&#20851;&#31995;&#25277;&#21462;&#30456;&#27604;&#65292;DocRE&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#20998;&#26512;&#65292;&#28041;&#21450;&#36328;&#36234;&#22810;&#20010;&#21477;&#23376;&#25110;&#27573;&#33853;&#30340;&#20851;&#31995;&#25277;&#21462;&#65292;&#21487;&#29992;&#20110;&#26500;&#24314;&#21644;&#33258;&#21160;&#22635;&#20805;&#30693;&#35782;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#65288;DocRE&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#19968;&#20010;&#27963;&#36291;&#30740;&#31350;&#39046;&#22495;&#65292;&#28041;&#21450;&#35782;&#21035;&#21644;&#25277;&#21462;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36229;&#36234;&#21477;&#23376;&#36793;&#30028;&#12290;&#19982;&#20256;&#32479;&#30340;&#21477;&#23376;&#32423;&#20851;&#31995;&#25277;&#21462;&#30456;&#27604;&#65292;DocRE&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#20998;&#26512;&#65292;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#35782;&#21035;&#21487;&#33021;&#36328;&#36234;&#22810;&#20010;&#21477;&#23376;&#25110;&#27573;&#33853;&#30340;&#20851;&#31995;&#12290;&#36825;&#20010;&#20219;&#21153;&#22312;&#26500;&#24314;&#21644;&#33258;&#21160;&#22635;&#20805;&#30693;&#35782;&#24211;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#20197;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#22823;&#35268;&#27169;&#25991;&#26723;&#65288;&#20363;&#22914;&#31185;&#23398;&#35770;&#25991;&#12289;&#27861;&#24459;&#21512;&#21516;&#25110;&#26032;&#38395;&#25991;&#31456;&#65289;&#20013;&#33258;&#21160;&#33719;&#21462;&#20851;&#31995;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#29702;&#35299;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#26088;&#22312;&#20840;&#38754;&#20171;&#32461;&#36825;&#19968;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#31361;&#20986;&#20854;&#19982;&#21477;&#23376;&#32423;&#20851;&#31995;&#25277;&#21462;&#30340;&#19981;&#21516;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level relation extraction (DocRE) is an active area of research in natural language processing (NLP) concerned with identifying and extracting relationships between entities beyond sentence boundaries. Compared to the more traditional sentence-level relation extraction, DocRE provides a broader context for analysis and is more challenging because it involves identifying relationships that may span multiple sentences or paragraphs. This task has gained increased interest as a viable solution to build and populate knowledge bases automatically from unstructured large-scale documents (e.g., scientific papers, legal contracts, or news articles), in order to have a better understanding of relationships between entities. This paper aims to provide a comprehensive overview of recent advances in this field, highlighting its different applications in comparison to sentence-level relation extraction.
&lt;/p&gt;</description></item><item><title>DiLu&#26159;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#65292;&#37319;&#29992;&#30693;&#35782;&#39537;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#29702;&#21644;&#21453;&#24605;&#27169;&#22359;&#36827;&#34892;&#20915;&#31574;&#65292;&#31215;&#32047;&#32463;&#39564;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.16292</link><description>&lt;p&gt;
DiLu: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#30340;&#30693;&#35782;&#39537;&#21160;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models. (arXiv:2309.16292v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16292
&lt;/p&gt;
&lt;p&gt;
DiLu&#26159;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#65292;&#37319;&#29992;&#30693;&#35782;&#39537;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#29702;&#21644;&#21453;&#24605;&#27169;&#22359;&#36827;&#34892;&#20915;&#31574;&#65292;&#31215;&#32047;&#32463;&#39564;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#26368;&#36817;&#30340;&#36827;&#23637;&#20381;&#36182;&#20110;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#34429;&#28982;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#38754;&#20020;&#25968;&#25454;&#38598;&#20559;&#35265;&#12289;&#36807;&#25311;&#21512;&#21644;&#19981;&#21487;&#35299;&#37322;&#24615;&#31561;&#25361;&#25112;&#12290;&#21463;&#20154;&#31867;&#39550;&#39542;&#30693;&#35782;&#39537;&#21160;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#22914;&#20309;&#23558;&#31867;&#20284;&#30340;&#33021;&#21147;&#27880;&#20837;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20114;&#21160;&#29615;&#22659;&#12289;&#39550;&#39542;&#21592;&#20195;&#29702;&#21644;&#35760;&#24518;&#32452;&#20214;&#30340;&#33539;&#20363;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#20855;&#26377;&#26032;&#20852;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiLu&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20102;&#25512;&#29702;&#27169;&#22359;&#21644;&#21453;&#24605;&#27169;&#22359;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#20381;&#25454;&#24120;&#35782;&#30693;&#35782;&#36827;&#34892;&#20915;&#31574;&#65292;&#24182;&#25345;&#32493;&#28436;&#21270;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;DiLu&#33021;&#22815;&#31215;&#32047;&#32463;&#39564;&#65292;&#24182;&#19988;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#27604;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;DiLu&#33021;&#22815;&#30452;&#25509;&#20174;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in autonomous driving have relied on data-driven approaches, which are widely adopted but face challenges including dataset bias, overfitting, and uninterpretability. Drawing inspiration from the knowledge-driven nature of human driving, we explore the question of how to instill similar capabilities into autonomous driving systems and summarize a paradigm that integrates an interactive environment, a driver agent, as well as a memory component to address this question. Leveraging large language models with emergent abilities, we propose the DiLu framework, which combines a Reasoning and a Reflection module to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. Extensive experiments prove DiLu's capability to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods. Moreover, DiLu is able to directly acquire experiences from real-world datasets w
&lt;/p&gt;</description></item><item><title>PRiSM&#26159;&#19968;&#31181;&#22686;&#24378;&#20302;&#36164;&#28304;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20851;&#31995;&#24863;&#30693;&#20998;&#25968;&#26657;&#20934;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#25104;&#21151;&#22320;&#38477;&#20302;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#35757;&#32451;&#27169;&#22411;&#26102;&#30340;&#26657;&#20934;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.13869</link><description>&lt;p&gt;
PRiSM: &#20351;&#29992;&#20851;&#31995;&#24863;&#30693;&#20998;&#25968;&#26657;&#20934;&#22686;&#24378;&#20302;&#36164;&#28304;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
PRiSM: Enhancing Low-Resource Document-Level Relation Extraction with Relation-Aware Score Calibration. (arXiv:2309.13869v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13869
&lt;/p&gt;
&lt;p&gt;
PRiSM&#26159;&#19968;&#31181;&#22686;&#24378;&#20302;&#36164;&#28304;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20851;&#31995;&#24863;&#30693;&#20998;&#25968;&#26657;&#20934;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#25104;&#21151;&#22320;&#38477;&#20302;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#35757;&#32451;&#27169;&#22411;&#26102;&#30340;&#26657;&#20934;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#65288;DocRE&#65289;&#26088;&#22312;&#25552;&#21462;&#25991;&#26723;&#20013;&#25152;&#26377;&#23454;&#20307;&#23545;&#30340;&#20851;&#31995;&#12290;&#22312;DocRE&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#27880;&#37322;&#36825;&#31867;&#25968;&#25454;&#30340;&#25104;&#26412;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#25237;&#20837;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;DocRE&#24773;&#20917;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#22312;&#23569;&#37327;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36807;&#39640;&#20272;&#35745;&#20102;NA&#65288;"no relation"&#65289;&#26631;&#31614;&#65292;&#23548;&#33268;&#24615;&#33021;&#21463;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#26657;&#20934;&#30340;&#35282;&#24230;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;PRiSM&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#20851;&#31995;&#35821;&#20041;&#20449;&#24687;&#26469;&#36866;&#24212;logits&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;DocRE&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#23558;&#29616;&#26377;&#27169;&#22411;&#19982;PRiSM&#38598;&#25104;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;F1&#20998;&#25968;&#25552;&#39640;&#20102;26.38%&#65292;&#32780;&#24403;&#29992;&#32422;3%&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#26657;&#20934;&#35823;&#24046;&#19979;&#38477;&#20102;36&#20493;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/brightjade/PRiSM&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level relation extraction (DocRE) aims to extract relations of all entity pairs in a document. A key challenge in DocRE is the cost of annotating such data which requires intensive human effort. Thus, we investigate the case of DocRE in a low-resource setting, and we find that existing models trained on low data overestimate the NA ("no relation") label, causing limited performance. In this work, we approach the problem from a calibration perspective and propose PRiSM, which learns to adapt logits based on relation semantic information. We evaluate our method on three DocRE datasets and demonstrate that integrating existing models with PRiSM improves performance by as much as 26.38 F1 score, while the calibration error drops as much as 36 times when trained with about 3% of data. The code is publicly available at https://github.com/brightjade/PRiSM.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#38463;&#25289;&#20271;&#25991;&#30340;&#26412;&#22320;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(AceGPT)&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#26469;&#22521;&#20859;&#20855;&#22791;&#25991;&#21270;&#24847;&#35782;&#21644;&#20215;&#20540;&#35266;&#19968;&#33268;&#30340;&#38463;&#25289;&#20271;&#25991;&#27169;&#22411;&#65292;&#20197;&#28385;&#36275;&#38463;&#25289;&#20271;&#35821;&#31038;&#21306;&#29305;&#23450;&#24212;&#29992;&#38656;&#27714;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;AceGPT&#22312;&#21508;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#26159;&#26368;&#20808;&#36827;&#30340;&#38463;&#25289;&#20271;&#25991;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.12053</link><description>&lt;p&gt;
AceGPT&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26412;&#22320;&#21270;&#20026;&#38463;&#25289;&#20271;&#25991;
&lt;/p&gt;
&lt;p&gt;
AceGPT, Localizing Large Language Models in Arabic. (arXiv:2309.12053v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#38463;&#25289;&#20271;&#25991;&#30340;&#26412;&#22320;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(AceGPT)&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#26469;&#22521;&#20859;&#20855;&#22791;&#25991;&#21270;&#24847;&#35782;&#21644;&#20215;&#20540;&#35266;&#19968;&#33268;&#30340;&#38463;&#25289;&#20271;&#25991;&#27169;&#22411;&#65292;&#20197;&#28385;&#36275;&#38463;&#25289;&#20271;&#35821;&#31038;&#21306;&#29305;&#23450;&#24212;&#29992;&#38656;&#27714;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;AceGPT&#22312;&#21508;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#26159;&#26368;&#20808;&#36827;&#30340;&#38463;&#25289;&#20271;&#25991;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#24320;&#21457;&#36866;&#29992;&#20110;&#38463;&#25289;&#20271;&#25991;&#30340;&#26412;&#22320;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#36843;&#20999;&#38656;&#27714;&#21644;&#26041;&#27861;&#35770;&#65292;&#38463;&#25289;&#20271;&#25991;&#20855;&#26377;&#29420;&#29305;&#30340;&#25991;&#21270;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#30446;&#21069;&#30340;&#20027;&#27969;&#27169;&#22411;&#22914;ChatGPT&#24182;&#26410;&#20805;&#20998;&#35299;&#20915;&#12290;&#22312;&#32771;&#34385;&#25991;&#21270;&#25935;&#24863;&#24615;&#21644;&#26412;&#22320;&#20215;&#20540;&#35266;&#26102;&#36824;&#23384;&#22312;&#20851;&#38190;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25171;&#21253;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#36827;&#19968;&#27493;&#20351;&#29992;&#38463;&#25289;&#20271;&#25991;&#26412;&#36827;&#34892;&#39044;&#35757;&#32451;&#12289;&#20351;&#29992;&#26412;&#22320;&#38463;&#25289;&#20271;&#25351;&#20196;&#21644;&#38463;&#25289;&#20271;&#35821;GPT-4&#22238;&#24212;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;(SFT)&#65292;&#20197;&#21450;&#20351;&#29992;&#23545;&#26412;&#22320;&#25991;&#21270;&#21644;&#20215;&#20540;&#35266;&#25935;&#24863;&#30340;&#22870;&#21169;&#27169;&#22411;&#36827;&#34892;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;(RLAIF)&#12290;&#30446;&#26631;&#26159;&#35757;&#32451;&#20855;&#22791;&#25991;&#21270;&#24847;&#35782;&#21644;&#19982;&#20215;&#20540;&#35266;&#19968;&#33268;&#30340;&#38463;&#25289;&#20271;&#25991;LLM&#65292;&#20197;&#28385;&#36275;&#38463;&#25289;&#20271;&#35821;&#31038;&#21306;&#22810;&#26679;&#21270;&#30340;&#29305;&#23450;&#24212;&#29992;&#38656;&#27714;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#24471;&#21040;&#30340;&#21517;&#20026;AceGPT&#30340;&#38463;&#25289;&#20271;&#25991;LLM&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#26159;&#26368;&#20808;&#36827;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the imperative need and methodology for developing a localized Large Language Model (LLM) tailored for Arabic, a language with unique cultural characteristics that are not adequately addressed by current mainstream models like ChatGPT. Key concerns additionally arise when considering cultural sensitivity and local values. To this end, the paper outlines a packaged solution, including further pre-training with Arabic texts, supervised fine-tuning (SFT) using native Arabic instructions and GPT-4 responses in Arabic, and reinforcement learning with AI feedback (RLAIF) using a reward model that is sensitive to local culture and values. The objective is to train culturally aware and value-aligned Arabic LLMs that can serve the diverse application-specific needs of Arabic-speaking communities.  Extensive evaluations demonstrated that the resulting LLM called `\textbf{AceGPT}' is the SOTA open Arabic LLM in various benchmarks, including instruction-following benchmark (i.e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MBR&#24494;&#35843;&#21644;QE&#24494;&#35843;&#26041;&#27861;&#65292;&#23558;&#35757;&#32451;&#26102;&#30340;&#36136;&#37327;&#25552;&#21319;&#33976;&#39311;&#21040;&#22522;&#20934;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#22312;&#25512;&#26029;&#26102;&#20351;&#29992;&#39640;&#25928;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#24494;&#35843;&#26041;&#27861;&#33021;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#29978;&#33267;&#36229;&#36807;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.10966</link><description>&lt;p&gt;
MBR&#21644;QE&#24494;&#35843;&#65306;&#23545;&#26368;&#20339;&#21644;&#26368;&#26114;&#36149;&#30340;&#35299;&#30721;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#26102;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods. (arXiv:2309.10966v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MBR&#24494;&#35843;&#21644;QE&#24494;&#35843;&#26041;&#27861;&#65292;&#23558;&#35757;&#32451;&#26102;&#30340;&#36136;&#37327;&#25552;&#21319;&#33976;&#39311;&#21040;&#22522;&#20934;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#22312;&#25512;&#26029;&#26102;&#20351;&#29992;&#39640;&#25928;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#24494;&#35843;&#26041;&#27861;&#33021;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#29978;&#33267;&#36229;&#36807;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#20219;&#21153;&#30340;&#35299;&#30721;&#26041;&#27861;&#30740;&#31350;&#20013;&#34920;&#26126;&#65292;&#20256;&#32479;&#30340;&#27874;&#26463;&#25628;&#32034;&#21644;&#36138;&#23146;&#35299;&#30721;&#31639;&#27861;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#65292;&#22240;&#20026;&#27169;&#22411;&#27010;&#29575;&#19981;&#24635;&#26159;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;&#27169;&#22411;&#22256;&#24785;&#24230;&#19982;&#36136;&#37327;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#26356;&#24378;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#21253;&#25324;&#36136;&#37327;&#20272;&#35745;&#65288;QE&#65289;&#37325;&#25490;&#24207;&#21644;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#35299;&#30721;&#12290;&#23613;&#31649;&#36825;&#20123;&#35299;&#30721;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MBR&#24494;&#35843;&#21644;QE&#24494;&#35843;&#65292;&#36825;&#20123;&#24494;&#35843;&#26041;&#27861;&#22312;&#35757;&#32451;&#26102;&#33976;&#39311;&#20102;&#36825;&#20123;&#35299;&#30721;&#26041;&#27861;&#30340;&#36136;&#37327;&#25552;&#21319;&#65292;&#22312;&#25512;&#26029;&#26102;&#20351;&#29992;&#39640;&#25928;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#36825;&#19968;&#20856;&#22411;&#30340;NLG&#20219;&#21153;&#65292;&#25105;&#20204;&#34920;&#26126;&#21363;&#20351;&#36827;&#34892;&#33258;&#35757;&#32451;&#65292;&#36825;&#20123;&#24494;&#35843;&#26041;&#27861;&#30340;&#24615;&#33021;&#20173;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#24403;&#20351;&#29992;&#22806;&#37096;LLM&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#26102;&#65292;&#36825;&#20123;&#24494;&#35843;&#26041;&#27861;&#20063;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research in decoding methods for Natural Language Generation (NLG) tasks has shown that the traditional beam search and greedy decoding algorithms are not optimal, because model probabilities do not always align with human preferences. Stronger decoding methods, including Quality Estimation (QE) reranking and Minimum Bayes' Risk (MBR) decoding, have since been proposed to mitigate the model-perplexity-vs-quality mismatch. While these decoding methods achieve state-of-the-art performance, they are prohibitively expensive to compute. In this work, we propose MBR finetuning and QE finetuning which distill the quality gains from these decoding methods at training time, while using an efficient decoding algorithm at inference time. Using the canonical NLG task of Neural Machine Translation (NMT), we show that even with self-training, these finetuning methods significantly outperform the base model. Moreover, when using an external LLM as a teacher model, these finetuning methods outp
&lt;/p&gt;</description></item><item><title>MINT&#26159;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#22810;&#36718;&#20132;&#20114;&#20013;&#35299;&#20915;&#20219;&#21153;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#20351;&#29992;&#24037;&#20855;&#21644;&#21033;&#29992;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#12290;&#23427;&#35299;&#20915;&#20102;&#24403;&#21069;&#35780;&#20272;&#21327;&#35758;&#24573;&#30053;&#32454;&#33268;&#20114;&#21160;&#21644;&#20302;&#20272;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#38382;&#39064;&#65292;&#20419;&#36827;&#20102;&#30740;&#31350;&#22522;&#20934;&#35780;&#20272;&#21644;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10691</link><description>&lt;p&gt;
MINT: &#35780;&#20272;&#22312;&#19982;&#24037;&#20855;&#21644;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#22810;&#36718;&#20132;&#20114;&#20013;&#30340;LLMs&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback. (arXiv:2309.10691v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10691
&lt;/p&gt;
&lt;p&gt;
MINT&#26159;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#22810;&#36718;&#20132;&#20114;&#20013;&#35299;&#20915;&#20219;&#21153;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#20351;&#29992;&#24037;&#20855;&#21644;&#21033;&#29992;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#12290;&#23427;&#35299;&#20915;&#20102;&#24403;&#21069;&#35780;&#20272;&#21327;&#35758;&#24573;&#30053;&#32454;&#33268;&#20114;&#21160;&#21644;&#20302;&#20272;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#38382;&#39064;&#65292;&#20419;&#36827;&#20102;&#30740;&#31350;&#22522;&#20934;&#35780;&#20272;&#21644;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#38656;&#35201;&#19982;&#29992;&#25143;&#36827;&#34892;&#22810;&#36718;&#20132;&#20114;&#65292;&#26377;&#26102;&#20505;&#36741;&#20197;&#22806;&#37096;&#24037;&#20855;&#30340;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#21327;&#35758;&#24120;&#24120;&#24378;&#35843;&#29992;&#21333;&#36718;&#20132;&#27969;&#30340;&#22522;&#20934;&#24615;&#33021;&#65292;&#24573;&#30053;&#20102;&#29992;&#25143;&#12289;LLMs&#21644;&#22806;&#37096;&#24037;&#20855;&#20043;&#38388;&#30340;&#32454;&#33268;&#20114;&#21160;&#65292;&#24182;&#20302;&#20272;&#20102;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#30095;&#24573;&#23548;&#33268;&#20102;&#30740;&#31350;&#22522;&#20934;&#35780;&#20272;&#32467;&#26524;&#19982;&#23454;&#38469;&#24212;&#29992;&#24773;&#20917;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MINT&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#20351;&#29992;&#24037;&#20855;&#21644;&#21033;&#29992;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#26469;&#35780;&#20272;LLMs&#35299;&#20915;&#22810;&#36718;&#20132;&#20114;&#20219;&#21153;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#20026;&#20102;&#20445;&#35777;&#21487;&#37325;&#22797;&#24615;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;LLMs&#21487;&#20197;&#36890;&#36807;&#25191;&#34892;Python&#20195;&#30721;&#26469;&#35775;&#38382;&#24037;&#20855;&#65292;&#24182;&#25509;&#25910;&#30001;GPT-4&#27169;&#25311;&#30340;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#12290;&#25105;&#20204;&#37325;&#26032;&#21033;&#29992;&#20102;&#19968;&#31995;&#21015;&#22810;&#26679;&#30340;&#24050;&#24314;&#31435;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#37325;&#28857;&#20851;&#27880;&#25512;&#29702;&#12289;&#32534;&#30721;&#21644;&#20915;&#31574;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
To solve complex tasks, large language models (LLMs) often require multiple rounds of interactions with the user, sometimes assisted by external tools. However, current evaluation protocols often emphasize benchmark performance with single-turn exchanges, neglecting the nuanced interactions among the user, LLMs, and external tools, while also underestimating the importance of natural language feedback from users. These oversights contribute to discrepancies between research benchmark evaluations and real-world use cases. We introduce MINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn interactions by (1) using tools and (2) leveraging natural language feedback. To ensure reproducibility, we provide an evaluation framework where LLMs can access tools by executing Python code and receive users' natural language feedback simulated by GPT-4. We repurpose a diverse set of established evaluation datasets focusing on reasoning, coding, and decision-making and careful
&lt;/p&gt;</description></item><item><title>DePT&#36890;&#36807;&#23558;&#36719;&#25552;&#31034;&#20998;&#35299;&#20026;&#36739;&#30701;&#30340;&#36719;&#25552;&#31034;&#21644;&#19968;&#23545;&#20302;&#31209;&#30697;&#38453;&#65292;&#24182;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#35843;&#25972;&#23545;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.05173</link><description>&lt;p&gt;
DePT:&#20998;&#35299;&#25552;&#31034;&#35843;&#25972;&#20197;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning. (arXiv:2309.05173v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05173
&lt;/p&gt;
&lt;p&gt;
DePT&#36890;&#36807;&#23558;&#36719;&#25552;&#31034;&#20998;&#35299;&#20026;&#36739;&#30701;&#30340;&#36719;&#25552;&#31034;&#21644;&#19968;&#23545;&#20302;&#31209;&#30697;&#38453;&#65292;&#24182;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#35843;&#25972;&#23545;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#65288;PT&#65289;&#26159;&#19968;&#31181;&#23558;&#21487;&#35757;&#32451;&#30340;&#23569;&#37327;&#36719;&#25552;&#31034;&#21521;&#37327;&#38468;&#21152;&#21040;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36755;&#20837;&#20013;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#24050;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#27169;&#22411;&#20013;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290; &#19982;&#20854;&#20182;PEFT&#26041;&#27861;&#30456;&#27604;&#65292;PT&#30340;&#31454;&#20105;&#24615;&#33021;&#21487;&#20197;&#22312;&#21487;&#35757;&#32451;&#21442;&#25968;&#26356;&#23569;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#65292;&#20854;&#21442;&#25968;&#24182;&#19981;&#20250;&#26174;&#33879;&#22686;&#21152;&#12290; &#20294;&#26159;&#65292;PT&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#36719;&#25552;&#31034;&#26631;&#35760;&#65292;&#23548;&#33268;&#36755;&#20837;&#24207;&#21015;&#21464;&#38271;&#65292;&#36825;&#23545;&#20110;Transformer&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#32780;&#35328;&#65292;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#20250;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290; &#36825;&#23545;&#20110;&#38754;&#20020;&#22823;&#37327;&#27599;&#26085;&#26597;&#35810;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23588;&#20854;&#20196;&#20154;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning (PT), where a small amount of trainable soft (continuous) prompt vectors is affixed to the input of language models (LM), has shown promising results across various tasks and models for parameter-efficient fine-tuning (PEFT). PT stands out from other PEFT approaches because it maintains competitive performance with fewer trainable parameters and does not drastically scale up its parameters as the model size expands. However, PT introduces additional soft prompt tokens, leading to longer input sequences, which significantly impacts training and inference time and memory usage due to the Transformer's quadratic complexity. Particularly concerning for Large Language Models (LLMs) that face heavy daily querying. To address this issue, we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt into a shorter soft prompt and a pair of low-rank matrices that are then optimised with two different learning rates. This allows DePT to achieve better performance whi
&lt;/p&gt;</description></item><item><title>PromptTTS 2&#26159;&#19968;&#31181;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#26469;&#25551;&#36848;&#21644;&#29983;&#25104;&#22768;&#38899;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#21270;&#32593;&#32476;&#25552;&#20379;&#22768;&#38899;&#30340;&#21487;&#21464;&#24615;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.02285</link><description>&lt;p&gt;
PromptTTS 2: &#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#25551;&#36848;&#21644;&#29983;&#25104;&#22768;&#38899;
&lt;/p&gt;
&lt;p&gt;
PromptTTS 2: Describing and Generating Voices with Text Prompt. (arXiv:2309.02285v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02285
&lt;/p&gt;
&lt;p&gt;
PromptTTS 2&#26159;&#19968;&#31181;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#26469;&#25551;&#36848;&#21644;&#29983;&#25104;&#22768;&#38899;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#21270;&#32593;&#32476;&#25552;&#20379;&#22768;&#38899;&#30340;&#21487;&#21464;&#24615;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#20256;&#36798;&#30340;&#20449;&#24687;&#27604;&#25991;&#23383;&#26356;&#20016;&#23500;&#65292;&#22240;&#20026;&#30456;&#21516;&#30340;&#35789;&#21487;&#20197;&#20197;&#19981;&#21516;&#30340;&#22768;&#38899;&#34920;&#36798;&#19981;&#21516;&#30340;&#20449;&#24687;&#12290;&#19982;&#20381;&#36182;&#35821;&#38899;&#25552;&#31034;&#65288;&#21442;&#32771;&#35821;&#38899;&#65289;&#26469;&#23454;&#29616;&#22768;&#38899;&#21487;&#21464;&#24615;&#30340;&#20256;&#32479;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#26041;&#27861;&#30456;&#27604;&#65292;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#65288;&#25551;&#36848;&#65289;&#26356;&#21152;&#29992;&#25143;&#21451;&#22909;&#65292;&#22240;&#20026;&#35821;&#38899;&#25552;&#31034;&#21487;&#33021;&#38590;&#20197;&#25214;&#21040;&#25110;&#26681;&#26412;&#19981;&#23384;&#22312;&#12290;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#30340;TTS&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;1&#65289;&#19968;&#23545;&#22810;&#38382;&#39064;&#65292;&#21363;&#25991;&#26412;&#25552;&#31034;&#26080;&#27861;&#25551;&#36848;&#22768;&#38899;&#21487;&#21464;&#24615;&#30340;&#25152;&#26377;&#32454;&#33410;&#65307;2&#65289;&#25991;&#26412;&#25552;&#31034;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#65292;&#38656;&#35201;&#20379;&#24212;&#21830;&#21644;&#22823;&#37327;&#25968;&#25454;&#26631;&#27880;&#25104;&#26412;&#26469;&#32534;&#20889;&#35821;&#38899;&#30340;&#25991;&#26412;&#25552;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PromptTTS 2&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#21464;&#21270;&#32593;&#32476;&#25552;&#20379;&#25991;&#26412;&#25552;&#31034;&#26080;&#27861;&#25429;&#25417;&#30340;&#22768;&#38899;&#21487;&#21464;&#24615;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech conveys more information than text, as the same word can be uttered in various voices to convey diverse information. Compared to traditional text-to-speech (TTS) methods relying on speech prompts (reference speech) for voice variability, using text prompts (descriptions) is more user-friendly since speech prompts can be hard to find or may not exist at all. TTS approaches based on the text prompt face two main challenges: 1) the one-to-many problem, where not all details about voice variability can be described in the text prompt, and 2) the limited availability of text prompt datasets, where vendors and large cost of data labeling are required to write text prompts for speech. In this work, we introduce PromptTTS 2 to address these challenges with a variation network to provide variability information of voice not captured by text prompts, and a prompt generation pipeline to utilize the large language models (LLM) to compose high quality text prompts. Specifically, the variatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#24773;&#32490;&#35282;&#33394;&#26631;&#27880;&#21644;&#22522;&#20110;&#35780;&#20272;&#30340;&#24773;&#32490;&#20998;&#26512;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;&#24773;&#32490;&#35282;&#33394;&#26631;&#27880;&#22312;&#24773;&#32490;&#20998;&#31867;&#30340;&#22522;&#30784;&#19978;&#28155;&#21152;&#20102;&#23545;&#25552;&#21450;&#23454;&#20307;&#30340;&#35270;&#35282;&#65292;&#25552;&#21462;&#20102;&#23545;&#24212;&#24773;&#32490;&#21407;&#22240;&#30340;&#25991;&#26412;&#33539;&#22260;&#12290;&#24773;&#32490;&#21644;&#20107;&#20214;&#20855;&#26377;&#20004;&#31181;&#20851;&#31995;&#65306;&#24773;&#32490;&#26412;&#36523;&#23601;&#26159;&#19968;&#31181;&#20107;&#20214;&#65292;&#24182;&#19988;&#24773;&#32490;&#26159;&#30001;&#20107;&#20214;&#24341;&#36215;&#30340;&#12290;&#36825;&#19968;&#27010;&#24565;&#23545;&#20110;&#24773;&#32490;&#35282;&#33394;&#26631;&#27880;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.02092</link><description>&lt;p&gt;
&#26550;&#26725;&#24773;&#32490;&#35282;&#33394;&#26631;&#27880;&#21644;&#22522;&#20110;&#35780;&#20272;&#30340;&#24773;&#32490;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Bridging Emotion Role Labeling and Appraisal-based Emotion Analysis. (arXiv:2309.02092v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#24773;&#32490;&#35282;&#33394;&#26631;&#27880;&#21644;&#22522;&#20110;&#35780;&#20272;&#30340;&#24773;&#32490;&#20998;&#26512;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;&#24773;&#32490;&#35282;&#33394;&#26631;&#27880;&#22312;&#24773;&#32490;&#20998;&#31867;&#30340;&#22522;&#30784;&#19978;&#28155;&#21152;&#20102;&#23545;&#25552;&#21450;&#23454;&#20307;&#30340;&#35270;&#35282;&#65292;&#25552;&#21462;&#20102;&#23545;&#24212;&#24773;&#32490;&#21407;&#22240;&#30340;&#25991;&#26412;&#33539;&#22260;&#12290;&#24773;&#32490;&#21644;&#20107;&#20214;&#20855;&#26377;&#20004;&#31181;&#20851;&#31995;&#65306;&#24773;&#32490;&#26412;&#36523;&#23601;&#26159;&#19968;&#31181;&#20107;&#20214;&#65292;&#24182;&#19988;&#24773;&#32490;&#26159;&#30001;&#20107;&#20214;&#24341;&#36215;&#30340;&#12290;&#36825;&#19968;&#27010;&#24565;&#23545;&#20110;&#24773;&#32490;&#35282;&#33394;&#26631;&#27880;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20013;&#30340;&#24773;&#32490;&#20998;&#26512;&#28085;&#30422;&#20102;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20854;&#20849;&#21516;&#30446;&#26631;&#26159;&#35753;&#35745;&#31639;&#26426;&#29702;&#35299;&#24773;&#32490;&#12290;&#20854;&#20013;&#26368;&#27969;&#34892;&#30340;&#26159;&#24773;&#32490;&#20998;&#31867;&#65292;&#20854;&#20013;&#23558;&#19968;&#20010;&#25110;&#22810;&#20010;&#24773;&#32490;&#20998;&#37197;&#32473;&#39044;&#23450;&#20041;&#30340;&#25991;&#26412;&#21333;&#20301;&#12290;&#32780;&#24773;&#32490;&#35282;&#33394;&#26631;&#27880;&#21017;&#28155;&#21152;&#20102;&#25552;&#21450;&#23454;&#20307;&#30340;&#35270;&#35282;&#65292;&#24182;&#25552;&#21462;&#19982;&#24773;&#32490;&#21407;&#22240;&#30456;&#23545;&#24212;&#30340;&#25991;&#26412;&#33539;&#22260;&#12290;&#30456;&#20851;&#30340;&#24773;&#32490;&#29702;&#35770;&#36798;&#25104;&#19968;&#20010;&#37325;&#35201;&#35266;&#28857;&#65306;&#24773;&#32490;&#26159;&#30001;&#26576;&#20123;&#20869;&#37096;&#25110;&#22806;&#37096;&#20107;&#20214;&#24341;&#36215;&#24182;&#21253;&#21547;&#20102;&#22810;&#20010;&#23376;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#20027;&#35266;&#24863;&#21463;&#21644;&#35748;&#30693;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#24773;&#32490;&#21644;&#20107;&#20214;&#26377;&#20004;&#31181;&#20851;&#31995;&#12290; &#65288;1&#65289;&#24773;&#32490;&#26412;&#36523;&#23601;&#26159;&#20107;&#20214;&#65307;&#36825;&#20010;&#35270;&#35282;&#26159;&#24773;&#32490;&#35282;&#33394;&#26631;&#27880;&#20013;&#30340;&#22522;&#30784;&#12290; &#65288;2&#65289;&#24773;&#32490;&#26159;&#30001;&#20107;&#20214;&#24341;&#36215;&#30340;&#65307;&#36825;&#20010;&#35270;&#35282;&#21017;&#38656;&#35201;&#30740;&#31350;&#22914;&#20309;&#23558;&#24515;&#29702;&#35780;&#20272;&#32435;&#20837;&#20854;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The term emotion analysis in text subsumes various natural language processing tasks which have in common the goal to enable computers to understand emotions. Most popular is emotion classification in which one or multiple emotions are assigned to a predefined textual unit. While such setting is appropriate to identify the reader's or author's emotion, emotion role labeling adds the perspective of mentioned entities and extracts text spans that correspond to the emotion cause. The underlying emotion theories agree on one important point; that an emotion is caused by some internal or external event and comprises several subcomponents, including the subjective feeling and a cognitive evaluation. We therefore argue that emotions and events are related in two ways. (1) Emotions are events; and this perspective is the fundament in NLP for emotion role labeling. (2) Emotions are caused by events; a perspective that is made explicit with research how to incorporate psychological appraisal the
&lt;/p&gt;</description></item><item><title>StoryBench&#26159;&#19968;&#20010;&#26032;&#30340;&#65292;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#21040;&#35270;&#39057;&#27169;&#22411;&#12290;&#23427;&#21253;&#25324;&#21160;&#20316;&#25191;&#34892;&#65292;&#25925;&#20107;&#24310;&#32493;&#21644;&#25925;&#20107;&#29983;&#25104;&#19977;&#20010;&#38590;&#24230;&#36880;&#28176;&#22686;&#21152;&#30340;&#35270;&#39057;&#29983;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#23567;&#32780;&#24378;&#22823;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#22522;&#32447;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2308.11606</link><description>&lt;p&gt;
StoryBench: &#19968;&#20010;&#22810;&#26041;&#38754;&#30340;&#36830;&#32493;&#25925;&#20107;&#21487;&#35270;&#21270;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
StoryBench: A Multifaceted Benchmark for Continuous Story Visualization. (arXiv:2308.11606v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11606
&lt;/p&gt;
&lt;p&gt;
StoryBench&#26159;&#19968;&#20010;&#26032;&#30340;&#65292;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#21040;&#35270;&#39057;&#27169;&#22411;&#12290;&#23427;&#21253;&#25324;&#21160;&#20316;&#25191;&#34892;&#65292;&#25925;&#20107;&#24310;&#32493;&#21644;&#25925;&#20107;&#29983;&#25104;&#19977;&#20010;&#38590;&#24230;&#36880;&#28176;&#22686;&#21152;&#30340;&#35270;&#39057;&#29983;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#23567;&#32780;&#24378;&#22823;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#22522;&#32447;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#35270;&#39057;&#25925;&#20107;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#38500;&#20102;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#35270;&#35273;&#25928;&#26524;&#22806;&#65292;&#35270;&#39057;&#36824;&#38656;&#35201;&#22312;&#25972;&#20010;&#24103;&#20013;&#20445;&#25345;&#19982;&#25991;&#26412;&#25552;&#31034;&#24207;&#21015;&#30340;&#19968;&#33268;&#12290;&#21019;&#24314;&#35270;&#39057;&#29983;&#25104;&#30340;&#22522;&#20934;&#38656;&#35201;&#22312;&#26102;&#38388;&#19978;&#23545;&#25968;&#25454;&#36827;&#34892;&#27880;&#37322;&#65292;&#36825;&#19982;&#35270;&#39057;&#25968;&#25454;&#38598;&#20013;&#32463;&#24120;&#20351;&#29992;&#30340;&#21333;&#20010;&#26631;&#39064;&#24418;&#25104;&#23545;&#27604;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19977;&#20010;&#29616;&#26377;&#25968;&#25454;&#38598;&#19978;&#30340;&#20840;&#38754;&#30340;&#20154;&#31867;&#27880;&#37322;&#65292;&#24182;&#25512;&#20986;&#20102;StoryBench&#65306;&#19968;&#20010;&#26032;&#30340;&#65292;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#65292;&#21487;&#21487;&#38752;&#22320;&#35780;&#20272;&#21363;&#23558;&#21457;&#24067;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#21253;&#25324;&#19977;&#20010;&#38590;&#24230;&#36880;&#28176;&#22686;&#21152;&#30340;&#35270;&#39057;&#29983;&#25104;&#20219;&#21153;&#65306;&#21160;&#20316;&#25191;&#34892;&#65292;&#22312;&#20174;&#19968;&#20010;&#26465;&#20214;&#35270;&#39057;&#24320;&#22987;&#29983;&#25104;&#19979;&#19968;&#20010;&#21160;&#20316;&#65307;&#25925;&#20107;&#24310;&#32493;&#65292;&#22312;&#20174;&#19968;&#20010;&#26465;&#20214;&#35270;&#39057;&#24320;&#22987;&#25191;&#34892;&#19968;&#31995;&#21015;&#21160;&#20316;&#65307;&#25925;&#20107;&#29983;&#25104;&#65292;&#20165;&#20174;&#25991;&#26412;&#25552;&#31034;&#20013;&#29983;&#25104;&#19968;&#20010;&#35270;&#39057;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#20123;&#23567;&#32780;&#24378;&#22823;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#22522;&#32447;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating video stories from text prompts is a complex task. In addition to having high visual quality, videos need to realistically adhere to a sequence of text prompts whilst being consistent throughout the frames. Creating a benchmark for video generation requires data annotated over time, which contrasts with the single caption used often in video datasets. To fill this gap, we collect comprehensive human annotations on three existing datasets, and introduce StoryBench: a new, challenging multi-task benchmark to reliably evaluate forthcoming text-to-video models. Our benchmark includes three video generation tasks of increasing difficulty: action execution, where the next action must be generated starting from a conditioning video; story continuation, where a sequence of actions must be executed starting from a conditioning video; and story generation, where a video must be generated from only text prompts. We evaluate small yet strong text-to-video baselines, and show the benefit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#23558;&#20174;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20154;&#31867;&#38382;&#39064;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#19988;&#35757;&#32451;&#20102;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;UserGPT&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;RealChat&#26469;&#35757;&#32451;&#21161;&#25163;&#27169;&#22411;ReaLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReaLM&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.11534</link><description>&lt;p&gt;
&#20316;&#20026;&#29992;&#25143;&#27169;&#25311;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Model as a User Simulator. (arXiv:2308.11534v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#23558;&#20174;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20154;&#31867;&#38382;&#39064;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#19988;&#35757;&#32451;&#20102;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;UserGPT&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;RealChat&#26469;&#35757;&#32451;&#21161;&#25163;&#27169;&#22411;ReaLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReaLM&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38381;&#28304;ChatGPT&#30340;&#21331;&#36234;&#24615;&#33021;&#24341;&#21457;&#20102;&#23545;&#20854;&#27665;&#20027;&#21270;&#30340;&#21162;&#21147;&#65292;&#20511;&#21161;&#30495;&#23454;&#29992;&#25143;&#21644;ChatGPT&#23545;&#35805;&#30340;&#21162;&#21147;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;Vicuna&#26159;&#19968;&#20010;&#24456;&#22909;&#30340;&#20363;&#23376;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;Baize&#21644;UltraChat&#31561;&#21162;&#21147;&#20027;&#35201;&#20381;&#38752;ChatGPT&#26681;&#25454;&#25351;&#20196;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#65292;&#32780;&#19981;&#26159;&#30495;&#23454;&#30340;&#20154;&#31867;&#23398;&#20064;&#65292;&#23548;&#33268;&#33539;&#22260;&#26377;&#38480;&#65292;&#22810;&#26679;&#24615;&#20943;&#24369;&#65292;&#32570;&#20047;&#30495;&#27491;&#30340;&#22810;&#36718;&#23545;&#35805;&#21160;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#26032;&#24615;&#22320;&#25226;&#20174;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20154;&#31867;&#38382;&#39064;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;UserGPT&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;RealChat&#12290;&#38543;&#21518;&#65292;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#25105;&#20204;&#30340;&#21161;&#25163;&#27169;&#22411;ReaLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReaLM&#22312;Vicuna-Bench&#21644;MT-Bench&#20013;&#22343;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The unparalleled performance of closed-sourced ChatGPT has sparked efforts towards its democratization, with notable strides made by leveraging real user and ChatGPT conversations, as evidenced by Vicuna. However, while current endeavors like Baize and UltraChat aim to auto-generate conversational data due to challenges in gathering human participation, they primarily rely on ChatGPT to simulate human behaviors based on directives rather than genuine human learning. This results in a limited scope, diminished diversity, and an absence of genuine multi-round conversational dynamics. To address the above issues, we innovatively target human questions extracted from genuine human-machine conversations as a learning goal and train a user simulator, UserGPT, to produce a high-quality human-centric synthetic conversation dataset, RealChat. Subsequently, this dataset trains our assistant model, ReaLM. Experimentally, ReaLM outpaces baseline models in both Vicuna-Bench and MT-Bench by pairwise
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#35814;&#32454;&#30340;&#32959;&#30244;&#23398;&#20449;&#24687;&#27880;&#37322;&#26041;&#26696;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#32959;&#30244;&#23398;&#31508;&#35760;&#20013;&#25552;&#21462;&#21644;&#25512;&#29702;&#22797;&#26434;&#30340;&#20462;&#36766;&#65292;&#24182;&#24212;&#29992;&#20110;&#20083;&#33146;&#30284;&#36827;&#23637;&#31508;&#35760;&#30340;&#35821;&#26009;&#24211;&#12290;</title><link>http://arxiv.org/abs/2308.03853</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21307;&#23398;&#32959;&#30244;&#23398;&#31508;&#35760;&#20013;&#25552;&#21462;&#35814;&#32454;&#30340;&#32959;&#30244;&#30149;&#21490;&#21644;&#27835;&#30103;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
Extracting detailed oncologic history and treatment plan from medical oncology notes with large language models. (arXiv:2308.03853v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#35814;&#32454;&#30340;&#32959;&#30244;&#23398;&#20449;&#24687;&#27880;&#37322;&#26041;&#26696;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#32959;&#30244;&#23398;&#31508;&#35760;&#20013;&#25552;&#21462;&#21644;&#25512;&#29702;&#22797;&#26434;&#30340;&#20462;&#36766;&#65292;&#24182;&#24212;&#29992;&#20110;&#20083;&#33146;&#30284;&#36827;&#23637;&#31508;&#35760;&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#25252;&#29702;&#21644;&#32959;&#30244;&#23398;&#35266;&#23519;&#30740;&#31350;&#37117;&#38656;&#35201;&#20840;&#38754;&#20102;&#35299;&#24739;&#32773;&#30340;&#30142;&#30149;&#36827;&#23637;&#21644;&#27835;&#30103;&#21382;&#21490;&#65292;&#36825;&#20123;&#20449;&#24687;&#36890;&#24120;&#22312;&#20020;&#24202;&#35760;&#24405;&#20013;&#35814;&#32454;&#35760;&#24405;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#32959;&#30244;&#23398;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#30446;&#21069;&#27809;&#26377;&#38024;&#23545;&#36825;&#20123;&#35760;&#24405;&#20013;&#35760;&#24405;&#30340;&#22810;&#26679;&#20449;&#24687;&#36827;&#34892;&#23436;&#25972;&#23553;&#35013;&#30340;&#32959;&#30244;&#23398;&#20449;&#24687;&#34920;&#31034;&#21644;&#27880;&#37322;&#26041;&#26696;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26368;&#36817;&#22312;&#21508;&#31181;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#30446;&#21069;&#32570;&#20047;&#20840;&#38754;&#27880;&#37322;&#30340;&#32959;&#30244;&#23398;&#25968;&#25454;&#38598;&#65292;&#23545;LLM&#22312;&#25552;&#21462;&#21644;&#25512;&#29702;&#32959;&#30244;&#23398;&#31508;&#35760;&#20013;&#30340;&#22797;&#26434;&#20462;&#36766;&#30340;&#24191;&#27867;&#35780;&#20272;&#20173;&#28982;&#19981;&#36275;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#35814;&#32454;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#27880;&#37322;&#32959;&#30244;&#23398;&#25991;&#26412;&#20449;&#24687;&#65292;&#21253;&#25324;&#24739;&#32773;&#29305;&#24449;&#12289;&#32959;&#30244;&#29305;&#24449;&#12289;&#27979;&#35797;&#12289;&#27835;&#30103;&#21644;&#26102;&#38388;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#21152;&#21033;&#31119;&#23612;&#20122;&#22823;&#23398;&#26087;&#37329;&#23665;&#20998;&#26657;&#30340;10&#20010;&#21435;&#26631;&#35782;&#21270;&#20083;&#33146;&#30284;&#36827;&#23637;&#31508;&#35760;&#35821;&#26009;&#24211;&#65292;&#24212;&#29992;&#20102;&#36825;&#20010;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Both medical care and observational studies in oncology require a thorough understanding of a patient's disease progression and treatment history, often elaborately documented in clinical notes. Despite their vital role, no current oncology information representation and annotation schema fully encapsulates the diversity of information recorded within these notes. Although large language models (LLMs) have recently exhibited impressive performance on various medical natural language processing tasks, due to the current lack of comprehensively annotated oncology datasets, an extensive evaluation of LLMs in extracting and reasoning with the complex rhetoric in oncology notes remains understudied. We developed a detailed schema for annotating textual oncology information, encompassing patient characteristics, tumor characteristics, tests, treatments, and temporality. Using a corpus of 10 de-identified breast cancer progress notes at University of California, San Francisco, we applied this
&lt;/p&gt;</description></item><item><title>CIDER&#26159;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#30701;&#25991;&#26412;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#25972;&#20010;&#35821;&#26009;&#24211;&#20013;&#25512;&#26029;&#20986;&#24773;&#24863;&#35789;&#30340;&#20542;&#21521;&#26469;&#35780;&#20998;&#20010;&#21035;&#25991;&#26412;&#65292;&#30456;&#27604;&#36890;&#29992;&#26041;&#27861;&#22312;&#22825;&#27668;&#25512;&#25991;&#38598;&#21512;&#19978;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2307.07864</link><description>&lt;p&gt;
CIDER: &#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#30701;&#25991;&#26412;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
CIDER: Context sensitive sentiment analysis for short-form text. (arXiv:2307.07864v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07864
&lt;/p&gt;
&lt;p&gt;
CIDER&#26159;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#30701;&#25991;&#26412;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#25972;&#20010;&#35821;&#26009;&#24211;&#20013;&#25512;&#26029;&#20986;&#24773;&#24863;&#35789;&#30340;&#20542;&#21521;&#26469;&#35780;&#20998;&#20010;&#21035;&#25991;&#26412;&#65292;&#30456;&#27604;&#36890;&#29992;&#26041;&#27861;&#22312;&#22825;&#27668;&#25512;&#25991;&#38598;&#21512;&#19978;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#23545;&#22823;&#37327;&#20851;&#20110;&#29305;&#23450;&#20027;&#39064;&#12289;&#20027;&#39064;&#25110;&#20107;&#20214;&#30340;&#30701;&#25991;&#26412;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#65292;&#22914;&#25512;&#25991;&#12289;Reddit&#24086;&#23376;&#25110;&#25253;&#32440;&#22836;&#26465;&#12290;&#36890;&#24120;&#20351;&#29992;&#36890;&#29992;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#24179;&#22343;&#24847;&#20041;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20250;&#24573;&#30053;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#21457;&#29983;&#30340;&#24847;&#20041;&#21464;&#21270;&#65292;&#20363;&#22914;&#65292;&#8220;active&#8221;&#19968;&#35789;&#22312;&#8220;active lifestyle&#8221;&#21644;&#8220;active volcano&#8221;&#20013;&#20855;&#26377;&#38750;&#24120;&#19981;&#21516;&#30340;&#24847;&#22270;&#21644;&#20542;&#21521;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;CIDER&#65288;&#19978;&#19979;&#25991;&#24863;&#30693;&#35789;&#20856;&#21644;&#24773;&#24863;&#25512;&#29702;&#22120;&#65289;&#65292;&#23427;&#36827;&#34892;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#24773;&#24863;&#20998;&#26512;&#65292;&#20854;&#20013;&#20174;&#25972;&#20010;&#35821;&#26009;&#24211;&#20013;&#25512;&#26029;&#20986;&#24773;&#24863;&#35789;&#30340;&#20542;&#21521;&#65292;&#28982;&#21518;&#20877;&#29992;&#20110;&#35780;&#20998;&#20010;&#21035;&#25991;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;CIDER&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#22312;&#22823;&#37327;&#20851;&#20110;&#22825;&#27668;&#30340;&#25512;&#25991;&#38598;&#21512;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#36890;&#29992;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#12290;&#25105;&#20204;&#24050;&#23558;CIDER&#30340;&#23454;&#29616;&#20197;python&#20195;&#30721;&#30340;&#24418;&#24335;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers commonly perform sentiment analysis on large collections of short texts like tweets, Reddit posts or newspaper headlines that are all focused on a specific topic, theme or event. Usually, general purpose sentiment analysis methods are used which perform well on average but miss the variation in meaning that happens across different contexts, for example, the word "active" has a very different intention and valence in the phrase "active lifestyle" versus "active volcano". This work presents a new approach, CIDER (Context Informed Dictionary and sEntiment Reasoner), which performs context sensitive sentiment analysis, where the valence of sentiment laden terms is inferred from the whole corpus before being used to score the individual texts. In this paper we detail the CIDER algorithm and demonstrate that it outperforms state-of-the-art generalist sentiment analysis on a large collection of tweets about the weather. We have made our implementation of CIDER available as a pyth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#23558;&#25945;&#24072;&#27169;&#22411;&#30340;&#35270;&#35273;&#34920;&#31034;&#21387;&#32553;&#21040;&#23398;&#29983;&#27169;&#22411;&#20013;&#12290;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#36229;&#20986;&#20998;&#24067;&#21487;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#21407;&#21017;&#26469;&#22686;&#24378;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03135</link><description>&lt;p&gt;
&#29992;&#20110;&#36229;&#20986;&#20998;&#24067;&#21487;&#27867;&#21270;&#24615;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Distilling Large Vision-Language Model with Out-of-Distribution Generalizability. (arXiv:2307.03135v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#23558;&#25945;&#24072;&#27169;&#22411;&#30340;&#35270;&#35273;&#34920;&#31034;&#21387;&#32553;&#21040;&#23398;&#29983;&#27169;&#22411;&#20013;&#12290;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#36229;&#20986;&#20998;&#24067;&#21487;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#21407;&#21017;&#26469;&#22686;&#24378;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#35268;&#27169;&#21644;&#35745;&#31639;&#35201;&#27714;&#20351;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#21644;&#26102;&#38388;&#25935;&#24863;&#20219;&#21153;&#19978;&#30340;&#37096;&#32626;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#27169;&#22411;&#21387;&#32553;&#26159;&#21019;&#24314;&#26356;&#23567;&#12289;&#26356;&#24555;&#30340;&#27169;&#22411;&#20197;&#20445;&#25345;&#36739;&#22823;&#27169;&#22411;&#24615;&#33021;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#34920;&#31034;&#21387;&#32553;&#21040;&#36731;&#37327;&#32423;&#23398;&#29983;&#27169;&#22411;&#20013;&#30340;&#36807;&#31243;&#65292;&#20351;&#29992;&#23567;&#22411;&#25110;&#20013;&#22411;&#25968;&#25454;&#38598;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#26412;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#21487;&#27867;&#21270;&#30340;&#24320;&#25918;&#35789;&#27719;&#38382;&#39064;&#65292;&#36825;&#22312;&#20197;&#24448;&#30340;&#27169;&#22411;&#21387;&#32553;&#30740;&#31350;&#20013;&#34987;&#24573;&#35270;&#20102;&#12290;&#25105;&#20204;&#20174;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#20004;&#20010;&#21407;&#21017;&#26469;&#22686;&#24378;&#23398;&#29983;&#27169;&#22411;&#30340;OOD&#21487;&#27867;&#21270;&#24615;&#65306;&#65288;1&#65289;&#26356;&#22909;&#22320;&#27169;&#20223;&#25945;&#24072;&#30340;&#35270;&#35273;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#22312;&#35270;&#35273;&#35821;&#35328;&#23545;&#40784;&#26041;&#38754;&#35880;&#24910;&#22320;&#20419;&#36827;&#26356;&#22909;&#30340;&#19968;&#33268;&#24615;&#65307;&#65288;2&#65289;&#36890;&#36807;&#20016;&#23500;&#23398;&#29983;&#27169;&#22411;&#30340;&#33258;&#20030;&#23398;&#20064;&#21644;&#25968;&#25454;&#25193;&#20805;&#26469;&#25552;&#39640;OOD&#21487;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large vision-language models have achieved outstanding performance, but their size and computational requirements make their deployment on resource-constrained devices and time-sensitive tasks impractical. Model distillation, the process of creating smaller, faster models that maintain the performance of larger models, is a promising direction towards the solution. This paper investigates the distillation of visual representations in large teacher vision-language models into lightweight student models using a smallor mid-scale dataset. Notably, this study focuses on open-vocabulary out-of-distribution (OOD) generalization, a challenging problem that has been overlooked in previous model distillation literature. We propose two principles from vision and language modality perspectives to enhance student's OOD generalization: (1) by better imitating teacher's visual representation space, and carefully promoting better coherence in vision-language alignment with the teacher; (2) by enric
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#23616;&#24314;&#31435;&#30340;&#20840;&#32654;&#31038;&#21306;&#35843;&#26597;&#65288;ACS&#65289;&#35780;&#20272;&#20102;&#21313;&#20960;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#23567;&#22411;&#27169;&#22411;&#20855;&#26377;&#26174;&#33879;&#30340;&#20301;&#32622;&#21644;&#26631;&#31614;&#20559;&#24046;&#65292;&#32780;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#33021;&#20943;&#36731;&#36825;&#31181;&#20559;&#24046;&#65292;&#20294;&#26080;&#27861;&#26681;&#25454;US&#32676;&#20307;&#25110;&#20219;&#20309;&#21487;&#35782;&#21035;&#30340;&#32676;&#20307;&#36235;&#21183;&#36827;&#34892;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2306.07951</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#26597;&#21709;&#24212;&#30340;&#36136;&#30097;
&lt;/p&gt;
&lt;p&gt;
Questioning the Survey Responses of Large Language Models. (arXiv:2306.07951v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#23616;&#24314;&#31435;&#30340;&#20840;&#32654;&#31038;&#21306;&#35843;&#26597;&#65288;ACS&#65289;&#35780;&#20272;&#20102;&#21313;&#20960;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#23567;&#22411;&#27169;&#22411;&#20855;&#26377;&#26174;&#33879;&#30340;&#20301;&#32622;&#21644;&#26631;&#31614;&#20559;&#24046;&#65292;&#32780;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#33021;&#20943;&#36731;&#36825;&#31181;&#20559;&#24046;&#65292;&#20294;&#26080;&#27861;&#26681;&#25454;US&#32676;&#20307;&#25110;&#20219;&#20309;&#21487;&#35782;&#21035;&#30340;&#32676;&#20307;&#36235;&#21183;&#36827;&#34892;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#22686;&#24378;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#20197;&#21508;&#31181;&#31185;&#23398;&#21160;&#26426;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#35843;&#26597;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#23616;&#24050;&#32463;&#24314;&#31435;&#30340;&#20840;&#32654;&#31038;&#21306;&#35843;&#26597;&#65288;ACS&#65289;&#65292;&#23601;&#27169;&#22411;&#30340;&#35843;&#26597;&#21709;&#24212;&#32467;&#26524;&#25506;&#31350;&#25152;&#33021;&#20102;&#35299;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#23545;&#21313;&#20960;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#21442;&#25968;&#33539;&#22260;&#20174;&#20960;&#20159;&#21040;&#19968;&#19975;&#20159;&#19981;&#31561;&#65292;&#20351;&#29992;ACS&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#25968;&#21313;&#19975;&#27425;&#30340;&#27979;&#35797;&#65292;&#31995;&#32479;&#22320;&#24471;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#27169;&#24335;&#12290;&#39318;&#20808;&#65292;&#23567;&#22411;&#27169;&#22411;&#23384;&#22312;&#26126;&#26174;&#30340;&#20301;&#32622;&#21644;&#26631;&#31614;&#20559;&#24046;&#65292;&#20363;&#22914;&#20559;&#21521;&#20110;&#37319;&#29992;&#26631;&#35760;&#20026;&#8220;A&#8221;&#30340;&#35843;&#26597;&#21709;&#24212;&#12290;&#38543;&#30528;&#27169;&#22411;&#23610;&#23544;&#30340;&#22686;&#21152;&#65292;A-&#20559;&#24046;&#34429;&#28982;&#26377;&#25152;&#20943;&#23569;&#65292;&#20294;&#20063;&#36827;&#23637;&#32531;&#24930;&#12290;&#20854;&#27425;&#65292;&#21363;&#20351;&#36890;&#36807;&#38543;&#26426;&#31572;&#26696;&#39034;&#24207;&#26469;&#35843;&#25972;&#36825;&#31181;&#26631;&#35760;&#20559;&#24046;&#65292;&#27169;&#22411;&#20173;&#28982;&#19981;&#20250;&#36235;&#21521;&#20110;&#32654;&#22269;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#25110;&#20219;&#20309;&#21487;&#35782;&#21035;&#30340;&#20154;&#21475;&#25490;&#24207;&#12290;&#30456;&#21453;&#65292;&#21508;&#31181;&#27169;&#22411;&#36235;&#21521;&#20110;&#22343;&#21248;&#38543;&#26426;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models increase in capability, researchers have started to conduct surveys of all kinds on these models with varying scientific motivations. In this work, we examine what we can learn from a model's survey responses on the basis of the well-established American Community Survey (ACS) by the U.S. Census Bureau. Evaluating more than a dozen different models, varying in size from a few hundred million to ten billion parameters, hundreds of thousands of times each on questions from the ACS, we systematically establish two dominant patterns. First, smaller models have a significant position and labeling bias, for example, towards survey responses labeled with the letter "A". This A-bias diminishes, albeit slowly, as model size increases. Second, when adjusting for this labeling bias through randomized answer ordering, models still do not trend toward US population statistics or those of any cognizable population. Rather, models across the board trend toward uniformly rando
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35789;&#27719;&#20462;&#21098;&#65288;VT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21024;&#38500;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#30456;&#20851;&#26631;&#35760;&#65292;&#23558;&#20854;&#21387;&#32553;&#20026;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35789;&#27719;&#20462;&#21098;&#21487;&#20197;&#22312;&#20445;&#25345;&#22810;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#38477;&#20302;&#20102;&#27169;&#22411;&#30340;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2305.15020</link><description>&lt;p&gt;
&#36890;&#36807;&#35789;&#27719;&#20462;&#21098;&#23454;&#29616;&#39640;&#25928;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
An Efficient Multilingual Language Model Compression through Vocabulary Trimming. (arXiv:2305.15020v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15020
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35789;&#27719;&#20462;&#21098;&#65288;VT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21024;&#38500;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#30456;&#20851;&#26631;&#35760;&#65292;&#23558;&#20854;&#21387;&#32553;&#20026;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35789;&#27719;&#20462;&#21098;&#21487;&#20197;&#22312;&#20445;&#25345;&#22810;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#38477;&#20302;&#20102;&#27169;&#22411;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24050;&#32463;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28085;&#30422;&#19981;&#21516;&#35821;&#35328;&#26631;&#35760;&#30340;&#35789;&#27719;&#23884;&#20837;&#30697;&#38453;&#36739;&#22823;&#65292;&#22810;&#35821;&#35328;LM&#30340;&#27169;&#22411;&#21442;&#25968;&#20173;&#28982;&#24456;&#22823;&#12290;&#30456;&#21453;&#65292;&#21333;&#19968;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#29305;&#23450;&#20110;&#35821;&#35328;&#30340;&#35789;&#27719;&#22312;&#30446;&#26631;&#35821;&#35328;&#20013;&#35757;&#32451;&#65292;&#20294;&#36825;&#38656;&#35201;&#22823;&#37327;&#39044;&#31639;&#21644;&#21487;&#38752;&#35821;&#26009;&#24211;&#25165;&#33021;&#20174;&#22836;&#24320;&#22987;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35789;&#27719;&#20462;&#21098;&#65288;VT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#35789;&#27719;&#20013;&#21024;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#23558;&#22810;&#35821;&#35328;LM&#30340;&#35789;&#27719;&#20943;&#23569;&#21040;&#30446;&#26631;&#35821;&#35328;&#12290;&#29702;&#35770;&#19978;&#65292;VT&#21487;&#20197;&#21387;&#32553;&#20219;&#20309;&#29616;&#26377;&#30340;&#22810;&#35821;&#35328;LM&#65292;&#20197;&#22312;&#22810;&#35821;&#35328;LM&#28085;&#30422;&#30340;&#20219;&#20309;&#35821;&#35328;&#20013;&#26500;&#24314;&#21333;&#19968;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;VT&#21487;&#20197;&#20445;&#30041;&#22810;&#35821;&#35328;LM&#30340;&#21407;&#22987;&#24615;&#33021;&#65292;&#21516;&#26102;&#23610;&#23544;&#26356;&#23567;&#65288;&#36890;&#24120;&#21482;&#38656;&#21407;&#22987;&#35789;&#27719;&#22823;&#23567;&#30340;&#32422;50&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual language model (LM) have become a powerful tool in NLP especially for non-English languages. Nevertheless, model parameters of multilingual LMs remain large due to the larger embedding matrix of the vocabulary covering tokens in different languages. On the contrary, monolingual LMs can be trained in a target language with the language-specific vocabulary only, but this requires a large budget and availability of reliable corpora to achieve a high-quality LM from scratch. In this paper, we propose vocabulary-trimming (VT), a method to reduce a multilingual LM vocabulary to a target language by deleting irrelevant tokens from its vocabulary. In theory, VT can compress any existing multilingual LM to build monolingual LMs in any language covered by the multilingual LM. In our experiments, we show that VT can retain the original performance of the multilingual LM, while being smaller in size (in general around 50% of the original vocabulary size is enough) than the original mu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;&#26041;&#27861;&#22312;&#39044;&#27979;&#20851;&#32852;&#12289;&#24573;&#30053;&#19978;&#19979;&#25991;&#31561;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#27169;&#22411;&#20351;&#29992;&#20102;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#22312;&#29983;&#25104;&#21019;&#26032;&#24605;&#24819;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.14259</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#25991;&#29486;&#30340;&#35821;&#22659;&#21270;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery. (arXiv:2305.14259v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;&#26041;&#27861;&#22312;&#39044;&#27979;&#20851;&#32852;&#12289;&#24573;&#30053;&#19978;&#19979;&#25991;&#31561;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#27169;&#22411;&#20351;&#29992;&#20102;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#22312;&#29983;&#25104;&#21019;&#26032;&#24605;&#24819;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#65288;LBD&#65289;&#26088;&#22312;&#36890;&#36807;&#25366;&#25496;&#35770;&#25991;&#24182;&#29983;&#25104;&#20551;&#35774;&#26469;&#21457;&#29616;&#26032;&#30340;&#31185;&#23398;&#30693;&#35782;&#12290;&#26631;&#20934;&#30340;LBD&#20165;&#38480;&#20110;&#39044;&#27979;&#31163;&#25955;&#27010;&#24565;&#20043;&#38388;&#30340;&#20004;&#20004;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#33647;&#29289;&#21644;&#30142;&#30149;&#30340;&#20851;&#32852;&#65289;&#12290;LBD&#36824;&#24573;&#30053;&#20102;&#20851;&#38190;&#30340;&#19978;&#19979;&#25991;&#65292;&#20363;&#22914;&#23454;&#39564;&#35774;&#32622;&#65288;&#20363;&#22914;&#65292;&#33647;&#29289;&#35780;&#20272;&#30340;&#29305;&#23450;&#24739;&#32773;&#32676;&#20307;&#65289;&#21644;&#20154;&#31867;&#31185;&#23398;&#23478;&#32771;&#34385;&#30340;&#32972;&#26223;&#30693;&#35782;&#21644;&#21160;&#26426;&#65288;&#20363;&#22914;&#65292;&#25214;&#21040;&#27809;&#26377;&#29305;&#23450;&#21103;&#20316;&#29992;&#30340;&#33647;&#29289;&#20505;&#36873;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#19978;&#19979;&#25991;&#21270;LBD&#65288;C-LBD&#65289;&#34920;&#36848;&#26469;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65306;&#20197;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31185;&#23398;&#20551;&#35774;&#65292;&#21516;&#26102;&#23558;&#23427;&#20204;&#32852;&#31995;&#21040;&#25511;&#21046;&#20551;&#35774;&#25628;&#32034;&#31354;&#38388;&#30340;&#19978;&#19979;&#25991;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24314;&#27169;&#26694;&#26550;&#65292;&#20351;&#29992;&#33719;&#24471;&#30340;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#24322;&#26500;&#32593;&#32476;&#20013;&#30340;&#8220;&#28789;&#24863;&#8221;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#20174;&#35770;&#25991;&#20013;&#27966;&#29983;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#20542;&#21521;&#20110;&#29983;&#25104;&#20855;&#26377;&#21019;&#26032;&#24615;&#30340;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
Literature-Based Discovery (LBD) aims to discover new scientific knowledge by mining papers and generating hypotheses. Standard LBD is limited to predicting pairwise relations between discrete concepts (e.g., drug-disease links). LBD also ignores critical contexts like experimental settings (e.g., a specific patient population where a drug is evaluated) and background knowledge and motivations that human scientists consider (e.g., to find a drug candidate without specific side effects). We address these limitations with a novel formulation of contextualized-LBD (C-LBD): generating scientific hypotheses in natural language, while grounding them in a context that controls the hypothesis search space. We present a modeling framework using retrieval of ``inspirations'' from a heterogeneous network of citations and knowledge graph relations, and create a new dataset derived from papers. Our evaluations with powerful large language models (LLMs) reveal that GPT4 tends to generate ideas with 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30452;&#25509;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#25506;&#32034;&#35780;&#20272;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#65292;&#25552;&#31034;LLMs&#33021;&#22815;&#22312;&#20108;&#20998;&#31867;&#20934;&#30830;&#24615;&#26041;&#38754;&#36229;&#36807;&#20197;&#21069;&#26368;&#20339;&#30340;&#20107;&#23454;&#24615;&#31995;&#32479;&#65292;&#26368;&#39640;&#21487;&#25552;&#39640;12.2&#20010;&#32477;&#23545;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.14069</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Factual Consistency of Summaries with Large Language Models. (arXiv:2305.14069v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30452;&#25509;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#25506;&#32034;&#35780;&#20272;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#65292;&#25552;&#31034;LLMs&#33021;&#22815;&#22312;&#20108;&#20998;&#31867;&#20934;&#30830;&#24615;&#26041;&#38754;&#36229;&#36807;&#20197;&#21069;&#26368;&#20339;&#30340;&#20107;&#23454;&#24615;&#31995;&#32479;&#65292;&#26368;&#39640;&#21487;&#25552;&#39640;12.2&#20010;&#32477;&#23545;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25688;&#35201;&#30740;&#31350;&#20013;&#65292;&#26816;&#27979;&#20107;&#23454;&#38169;&#35823;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35838;&#39064;&#12290;&#21463;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26032;&#20852;&#30340;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#36890;&#36807;&#30452;&#25509;&#25552;&#31034;LLMs&#26469;&#35780;&#20272;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#35780;&#20272;LLMs&#20316;&#20026;&#20107;&#23454;&#19968;&#33268;&#24615;&#35780;&#20272;&#22120;&#30340;&#33021;&#21147;&#65292;&#20854;&#20013;&#21253;&#25324;(1)&#20998;&#26512;&#19981;&#21516;&#30340;LLMs&#65292;&#22914;GPT&#27169;&#22411;&#31995;&#21015;&#21644;Flan-T5;(2)&#30740;&#31350;&#21508;&#31181;&#25552;&#31034;&#26041;&#27861;&#65292;&#21253;&#25324;vanilla&#25552;&#31034;&#12289;&#24605;&#32500;&#38142;&#25552;&#31034;&#21644;&#36880;&#21477;&#25552;&#31034;&#26041;&#27861;&#26469;&#22788;&#29702;&#38271;&#31687;&#25688;&#35201;;(3)&#35780;&#20272;&#22810;&#20010;&#25688;&#35201;&#31995;&#32479;&#29983;&#25104;&#30340;&#22810;&#26679;&#21270;&#25688;&#35201;&#65292;&#33539;&#22260;&#20174;&#39044;&#21464;&#21387;&#22120;&#26041;&#27861;&#21040;SOTA&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#35774;&#32622;&#20013;&#65292;&#25552;&#31034;LLMs&#33021;&#22815;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#30340;&#20107;&#23454;&#24615;&#31995;&#32479;&#65292;&#23545;&#20110;&#19981;&#19968;&#33268;&#24615;&#30340;&#20108;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#25552;&#39640;&#20102;&#26368;&#22810;12.2&#20010;&#32477;&#23545;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting factual errors in summaries has been an important and challenging subject in summarization research. Inspired by the emergent ability of large language models (LLMs), we explore evaluating factual consistency of summaries by directly prompting LLMs. We present a comprehensive empirical study to assess the ability of LLMs as factual consistency evaluators, which consists of (1) analyzing different LLMs such as the GPT model series and Flan-T5; (2) investigating a variety of prompting methods including vanilla prompting, chain-of-thought prompting, and a sentence-by-sentence prompting method to tackle long summaries; and (3) evaluating on diverse summaries generated by multiple summarization systems, ranging from pre-transformer methods to SOTA pretrained models. Our experiments demonstrate that prompting LLMs is able to outperform the previous best factuality systems in all settings, by up to 12.2 absolute points in terms of the binary classification accuracy on inconsistency 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#21644;&#22768;&#26126;&#24615;&#20219;&#21153;&#35268;&#33539;&#30340;&#21487;&#28385;&#36275;&#24615;&#36741;&#21161;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.09656</link><description>&lt;p&gt;
&#22768;&#26126;&#25552;&#31034;&#19979;&#30340;&#21487;&#28385;&#36275;&#24615;&#36741;&#21161;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Satisfiability-Aided Language Models Using Declarative Prompting. (arXiv:2305.09656v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#21644;&#22768;&#26126;&#24615;&#20219;&#21153;&#35268;&#33539;&#30340;&#21487;&#28385;&#36275;&#24615;&#36741;&#21161;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#28385;&#36275;&#24615;&#36741;&#21161;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#22768;&#26126;&#24615;&#20219;&#21153;&#35268;&#33539;&#65292;&#24182;&#21033;&#29992;&#19968;&#20010;&#29616;&#25104;&#30340;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#24471;&#20986;&#26368;&#32456;&#31572;&#26696;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#20248;&#28857;&#65306;&#31532;&#19968;&#65292;&#22768;&#26126;&#24615;&#35268;&#33539;&#27604;&#25512;&#29702;&#27493;&#39588;&#26356;&#25509;&#36817;&#38382;&#39064;&#25551;&#36848;&#65292;&#22240;&#27492;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35299;&#26512;&#23427;&#65307;&#31532;&#20108;&#65292;&#36890;&#36807;&#23558;&#23454;&#38469;&#25512;&#29702;&#20219;&#21153;&#22996;&#25176;&#32473;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20445;&#35777;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior work has combined chain-of-thought prompting in large language models (LLMs) with programmatic representations to perform effective and transparent reasoning. While such an approach works very well for tasks that only require forward reasoning (e.g., straightforward arithmetic), it is less effective for constraint solving tasks that require more sophisticated planning and search. In this paper, we propose a new satisfiability-aided language modeling approach for improving the reasoning capabilities of LLMs. We use an LLM to generate a declarative task specification rather than an imperative program and leverage an off-the-shelf automated theorem prover to derive the final answer. This approach has two key advantages. The declarative specification is closer to the problem description than the reasoning steps are, so the LLM can parse it more accurately. Furthermore, by offloading the actual reasoning task to an automated theorem prover, our approach can guarantee the correctness o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19977;&#31181;&#26032;&#30340;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#20154;&#20204;&#22914;&#20309;&#23558;&#36523;&#20221;&#26631;&#31614;&#24212;&#29992;&#20110;&#33258;&#24049;&#21644;&#20182;&#20154;&#20197;&#21450;&#37327;&#21270;&#31361;&#20986;&#30340;&#31038;&#20250;&#32500;&#24230;&#65288;&#22914;&#24615;&#21035;&#65289;&#30340;&#21051;&#26495;&#21360;&#35937;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09548</link><description>&lt;p&gt;
&#20351;&#29992;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#26469;&#34913;&#37327;&#21051;&#26495;&#21360;&#35937;
&lt;/p&gt;
&lt;p&gt;
Measuring Stereotypes using Entity-Centric Data. (arXiv:2305.09548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19977;&#31181;&#26032;&#30340;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#20154;&#20204;&#22914;&#20309;&#23558;&#36523;&#20221;&#26631;&#31614;&#24212;&#29992;&#20110;&#33258;&#24049;&#21644;&#20182;&#20154;&#20197;&#21450;&#37327;&#21270;&#31361;&#20986;&#30340;&#31038;&#20250;&#32500;&#24230;&#65288;&#22914;&#24615;&#21035;&#65289;&#30340;&#21051;&#26495;&#21360;&#35937;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21051;&#26495;&#21360;&#35937;&#24433;&#21709;&#25105;&#20204;&#22914;&#20309;&#23637;&#31034;&#33258;&#24049;&#21644;&#20182;&#20154;&#65292;&#20174;&#32780;&#24433;&#21709;&#25105;&#20204;&#30340;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#34913;&#37327;&#21051;&#26495;&#21360;&#35937;&#38750;&#24120;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#20998;&#24067;&#35821;&#20041;&#27169;&#22411;&#65288;DSM&#65289;&#65288;&#22914;BERT&#65289;&#20013;&#23884;&#20837;&#30340;&#25237;&#24433;&#26469;&#36827;&#34892;&#36825;&#20123;&#27979;&#37327;&#12290;&#28982;&#32780;&#65292;DSMs&#25429;&#25417;&#21040;&#30340;&#35748;&#30693;&#32852;&#24819;&#19981;&#19968;&#23450;&#19982;&#21051;&#26495;&#21360;&#35937;&#30340;&#20154;&#38469;&#24615;&#36136;&#30456;&#20851;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19977;&#31181;&#26032;&#30340;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#20174;Twitter&#21644;Wikipedia&#20256;&#35760;&#20013;&#23398;&#20064;&#21051;&#26495;&#21360;&#35937;&#12290;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#30701;&#35821;&#24212;&#29992;&#20110;&#21516;&#19968;&#20010;&#20154;&#30340;&#20107;&#23454;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#25193;&#22823;&#20102;&#23398;&#20064;&#32852;&#24819;&#30340;&#20154;&#26412;&#36523;&#20013;&#24515;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#20154;&#20204;&#22914;&#20309;&#23558;&#36523;&#20221;&#26631;&#31614;&#24212;&#29992;&#20110;&#33258;&#24049;&#21644;&#20182;&#20154;&#20197;&#21450;&#37327;&#21270;&#31361;&#20986;&#30340;&#31038;&#20250;&#32500;&#24230;&#65288;&#22914;&#24615;&#21035;&#65289;&#30340;&#21051;&#26495;&#21360;&#35937;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#23545;&#26410;&#26469;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#38382;&#39064;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stereotypes inform how we present ourselves and others, and in turn how we behave. They are thus important to measure. Recent work has used projections of embeddings from Distributional Semantic Models (DSMs), such as BERT, to perform these measurements. However, DSMs capture cognitive associations that are not necessarily relevant to the interpersonal nature of stereotyping. Here, we propose and evaluate three novel, entity-centric methods for learning stereotypes from Twitter and Wikipedia biographies. Models are trained by leveraging the fact that multiple phrases are applied to the same person, magnifying the person-centric nature of the learned associations. We show that these models outperform existing approaches to stereotype measurement with respect to 1) predicting which identities people apply to themselves and others, and 2) quantifying stereotypes on salient social dimensions (e.g. gender). Via a case study, we also show the utility of these models for future questions in c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#65292;&#23454;&#39564;&#35777;&#26126;ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#35299;&#37322;&#32773;&#65292;&#20294;&#19981;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#29702;&#32773;&#65292;&#23384;&#22312;&#20005;&#37325;&#30340;&#22240;&#26524;&#24187;&#35273;&#38382;&#39064;&#65292;&#23545;&#20110;&#26126;&#30830;&#30340;&#22240;&#26524;&#20851;&#31995;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.07375</link><description>&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#26029;&#22120;&#21527;&#65311;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation. (arXiv:2305.07375v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#65292;&#23454;&#39564;&#35777;&#26126;ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#35299;&#37322;&#32773;&#65292;&#20294;&#19981;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#29702;&#32773;&#65292;&#23384;&#22312;&#20005;&#37325;&#30340;&#22240;&#26524;&#24187;&#35273;&#38382;&#39064;&#65292;&#23545;&#20110;&#26126;&#30830;&#30340;&#22240;&#26524;&#20851;&#31995;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#23545;&#20110;&#20247;&#22810;NLP&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;ChatGPT&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26032;&#20852;&#33021;&#21147;&#65292;&#20294;ChatGPT&#22312;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#30340;&#34920;&#29616;&#22914;&#20309;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#23545;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ChatGPT&#19981;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#29702;&#32773;&#65292;&#20294;&#26159;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#35299;&#37322;&#32773;&#12290;&#27492;&#22806;&#65292;ChatGPT&#22312;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#20005;&#37325;&#30340;&#24187;&#35273;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#20013;&#22240;&#26524;&#20851;&#31995;&#21644;&#38750;&#22240;&#26524;&#20851;&#31995;&#30340;&#25253;&#21578;&#20559;&#35265;&#65292;&#20197;&#21450;ChatGPT&#30340;&#21319;&#32423;&#36807;&#31243;&#65292;&#22914;RLHF&#12290;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#21644;&#24605;&#32500;&#38142;&#65288;COT&#65289;&#25216;&#26415;&#26041;&#38754;&#65292;&#21487;&#33021;&#20250;&#36827;&#19968;&#27493;&#21152;&#21095;&#36825;&#31181;&#22240;&#26524;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#23545;&#20110;&#22312;&#25552;&#31034;&#20013;&#34920;&#36798;&#22240;&#26524;&#27010;&#24565;&#30340;&#35789;&#35821;&#38750;&#24120;&#25935;&#24863;&#65292;&#24182;&#19988;&#23553;&#38381;&#25552;&#31034;&#27604;&#24320;&#25918;&#25552;&#31034;&#34920;&#29616;&#26356;&#22909;&#12290;&#23545;&#20110;&#21477;&#23376;&#20013;&#30340;&#20107;&#20214;&#65292;ChatGPT&#25797;&#38271;&#25429;&#25417;&#26126;&#30830;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal reasoning ability is crucial for numerous NLP applications. Despite the impressive emerging ability of ChatGPT in various NLP tasks, it is unclear how well ChatGPT performs in causal reasoning. In this paper, we conduct the first comprehensive evaluation of the ChatGPT's causal reasoning capabilities. Experiments show that ChatGPT is not a good causal reasoner, but a good causal interpreter. Besides, ChatGPT has a serious hallucination on causal reasoning, possibly due to the reporting biases between causal and non-causal relationships in natural language, as well as ChatGPT's upgrading processes, such as RLHF. The In-Context Learning (ICL) and Chain-of-Though (COT) techniques can further exacerbate such causal hallucination. Additionally, the causal reasoning ability of ChatGPT is sensitive to the words used to express the causal concept in prompts, and close-ended prompts perform better than open-ended prompts. For events in sentences, ChatGPT excels at capturing explicit caus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986; Iter-CoT &#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#36845;&#20195;&#22686;&#24378;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#36866;&#24230;&#38590;&#24230;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21487;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#24182;&#20276;&#38543;&#25512;&#29702;&#38142;&#20316;&#20026;&#31034;&#20363;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#20351;&#27169;&#22411;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#29983;&#25104;&#25512;&#29702;&#38142;&#12290;</title><link>http://arxiv.org/abs/2304.11657</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21152;&#24378;&#36845;&#20195;&#22686;&#24378;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models. (arXiv:2304.11657v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986; Iter-CoT &#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#36845;&#20195;&#22686;&#24378;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#36866;&#24230;&#38590;&#24230;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21487;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#24182;&#20276;&#38543;&#25512;&#29702;&#38142;&#20316;&#20026;&#31034;&#20363;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#20351;&#27169;&#22411;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#29983;&#25104;&#25512;&#29702;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36880;&#27493;&#24341;&#23548;&#24605;&#32500;&#38142; (CoT) &#20316;&#20026;&#31034;&#33539;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#21487;&#20197;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#19978;&#23454;&#29616;&#39640;&#24230;&#26377;&#25928;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;LLMs &#29983;&#25104;&#30340;&#28436;&#31034;&#25512;&#29702;&#38142;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#19981;&#24688;&#24403;&#30340;&#31034;&#20363; (&#36807;&#20110;&#31616;&#21333;&#25110;&#22797;&#26434;) &#21487;&#20197;&#24433;&#21709;&#22312;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#19979;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Iter-CoT (&#36845;&#20195;&#24341;&#23548;&#24605;&#32500;&#38142;&#25552;&#31034;) &#30340;&#36845;&#20195;&#24341;&#23548;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#23454;&#20363;&#24182;&#29983;&#25104;&#25512;&#29702;&#38142;&#12290;&#36890;&#36807;&#21033;&#29992;&#36845;&#20195;&#22686;&#24378;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;LLMs &#33258;&#20027;&#26356;&#27491;&#38169;&#35823;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#31934;&#30830;&#12289;&#20840;&#38754;&#30340;&#25512;&#29702;&#38142;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36873;&#25321;&#20855;&#26377;&#36866;&#24230;&#38590;&#24230;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21487;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#24182;&#20276;&#38543;&#25512;&#29702;&#38142;&#20316;&#20026;&#31034;&#20363;&#65292;&#20174;&#32780;&#22686;&#24378;LLMs &#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can achieve highly effective performance on various reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompting as demonstrations. However, the reasoning chains of demonstrations generated by LLMs are prone to errors, which can subsequently lead to incorrect reasoning during inference. Furthermore, inappropriate exemplars (overly simplistic or complex), can affect overall performance among varying levels of difficulty. We introduce Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts Prompting), an iterative bootstrapping approach for selecting exemplars and generating reasoning chains. By utilizing iterative bootstrapping, our approach enables LLMs to autonomously rectify errors, resulting in more precise and comprehensive reasoning chains. Simultaneously, our approach selects challenging yet answerable questions accompanied by reasoning chains as exemplars with a moderate level of difficulty, which enhances the LLMs' generalizability 
&lt;/p&gt;</description></item><item><title>LLMMaps&#26159;&#19968;&#31181;&#20998;&#23618;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#25581;&#31034;&#21462;&#24471;&#39640;&#20934;&#30830;&#24230;&#21644;&#20135;&#29983;&#24187;&#35273;&#30340;&#23376;&#39046;&#22495;&#65292;&#24182;&#25351;&#23548;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.00457</link><description>&lt;p&gt;
LLMMaps&#8212;&#8212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#23618;&#35780;&#20215;&#30340;&#21487;&#35270;&#21270;&#38544;&#21947;
&lt;/p&gt;
&lt;p&gt;
LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language Models. (arXiv:2304.00457v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00457
&lt;/p&gt;
&lt;p&gt;
LLMMaps&#26159;&#19968;&#31181;&#20998;&#23618;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#25581;&#31034;&#21462;&#24471;&#39640;&#20934;&#30830;&#24230;&#21644;&#20135;&#29983;&#24187;&#35273;&#30340;&#23376;&#39046;&#22495;&#65292;&#24182;&#25351;&#23548;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23481;&#26131;&#20135;&#29983;&#24187;&#35273;&#65292;&#21363;&#27169;&#22411;&#22312;&#21709;&#24212;&#20013;&#26292;&#38706;&#20986;&#19981;&#27491;&#30830;&#25110;&#38169;&#35823;&#30340;&#20449;&#24687;&#65292;&#36825;&#20351;&#24471;&#24517;&#39035;&#37319;&#29992;&#21220;&#22859;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#34429;&#28982;LLM&#22312;&#29305;&#23450;&#30693;&#35782;&#39046;&#22495;&#20013;&#30340;&#34920;&#29616;&#36890;&#24120;&#26159;&#22522;&#20110;&#38382;&#31572;(Q&amp;A)&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#20294;&#36825;&#20123;&#35780;&#20272;&#36890;&#24120;&#20165;&#25253;&#21578;&#25972;&#20010;&#39046;&#22495;&#30340;&#21333;&#20010;&#20934;&#30830;&#24230;&#25968;&#23383;&#65292;&#36825;&#19968;&#31243;&#24207;&#22312;&#36879;&#26126;&#24230;&#21644;&#27169;&#22411;&#25913;&#36827;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#20998;&#23618;&#35780;&#20272;&#21487;&#20197;&#25581;&#31034;&#21487;&#33021;&#26356;&#23481;&#26131;&#21457;&#29983;&#24187;&#35273;&#30340;&#23376;&#39046;&#22495;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#35780;&#20272;LLMs&#30340;&#39118;&#38505;&#24182;&#25351;&#23548;&#23427;&#20204;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;&#20026;&#25903;&#25345;&#36825;&#26679;&#30340;&#20998;&#23618;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLMMaps&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#26681;&#25454;Q&amp;A&#25968;&#25454;&#38598;&#35780;&#20272;LLMs&#30340;&#24615;&#33021;&#12290;LLMMaps&#25552;&#20379;&#20102;&#23545;LLMs&#22312;&#19981;&#21516;&#23376;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#20998;&#24067;&#30340;&#35814;&#32454;&#27934;&#23519;&#65292;&#20801;&#35768;&#29992;&#25143;&#25918;&#22823;&#39046;&#22495;&#30340;&#29305;&#23450;&#37096;&#20998;&#24182;&#25506;&#32034;&#27169;&#22411;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LLMMaps&#26377;&#21161;&#20110;&#35782;&#21035;&#20986;&#26356;&#23481;&#26131;&#20986;&#29616;LLM&#24187;&#35273;&#30340;&#23376;&#39046;&#22495;&#65292;&#24182;&#21487;&#20197;&#25351;&#23548;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#20197;&#25913;&#21892;&#36825;&#20123;&#39046;&#22495;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized natural language processing and demonstrated impressive capabilities in various tasks. Unfortunately, they are prone to hallucinations, where the model exposes incorrect or false information in its responses, which renders diligent evaluation approaches mandatory. While LLM performance in specific knowledge fields is often evaluated based on question and answer (Q&amp;A) datasets, such evaluations usually report only a single accuracy number for the entire field, a procedure which is problematic with respect to transparency and model improvement. A stratified evaluation could instead reveal subfields, where hallucinations are more likely to occur and thus help to better assess LLMs' risks and guide their further development. To support such stratified evaluations, we propose LLMMaps as a novel visualization technique that enables users to evaluate LLMs' performance with respect to Q&amp;A datasets. LLMMaps provide detailed insights into LLMs' kn
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#24182;&#32534;&#36753;&#26263;&#34255;&#21518;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#21457;&#29616;&#26089;&#26399;&#23618;&#30340;MLP&#27169;&#22359;&#21644;&#21021;&#22987;&#23884;&#20837;&#25237;&#24433;&#26159;&#21518;&#38376;&#26426;&#21046;&#20013;&#26368;&#37325;&#35201;&#30340;&#37096;&#20998;&#12290;&#36890;&#36807;&#20351;&#29992;PCP&#28040;&#34701;&#25216;&#26415;&#26367;&#25442;&#21464;&#21387;&#22120;&#27169;&#22359;&#65292;&#25105;&#20204;&#25104;&#21151;&#21024;&#38500;&#12289;&#25554;&#20837;&#21644;&#20462;&#25913;&#21518;&#38376;&#26426;&#21046;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;&#20102;&#21518;&#38376;&#30340;&#36755;&#20986;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.12461</link><description>&lt;p&gt;
&#20998;&#26512;&#21644;&#32534;&#36753;&#26263;&#34255;&#21518;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Analyzing And Editing Inner Mechanisms Of Backdoored Language Models. (arXiv:2302.12461v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#24182;&#32534;&#36753;&#26263;&#34255;&#21518;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#21457;&#29616;&#26089;&#26399;&#23618;&#30340;MLP&#27169;&#22359;&#21644;&#21021;&#22987;&#23884;&#20837;&#25237;&#24433;&#26159;&#21518;&#38376;&#26426;&#21046;&#20013;&#26368;&#37325;&#35201;&#30340;&#37096;&#20998;&#12290;&#36890;&#36807;&#20351;&#29992;PCP&#28040;&#34701;&#25216;&#26415;&#26367;&#25442;&#21464;&#21387;&#22120;&#27169;&#22359;&#65292;&#25105;&#20204;&#25104;&#21151;&#21024;&#38500;&#12289;&#25554;&#20837;&#21644;&#20462;&#25913;&#21518;&#38376;&#26426;&#21046;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;&#20102;&#21518;&#38376;&#30340;&#36755;&#20986;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#20013;&#30340;&#27602;&#21270;&#26159;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#23433;&#20840;&#23041;&#32961;&#65292;&#21487;&#33021;&#23548;&#33268;&#26263;&#34255;&#21518;&#38376;&#30340;&#27169;&#22411;&#12290;&#20851;&#20110;&#26263;&#34255;&#21518;&#38376;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#22788;&#29702;&#35302;&#21457;&#36755;&#20837;&#65288;&#20363;&#22914;&#65292;&#20999;&#25442;&#33267;&#26377;&#27602;&#35821;&#35328;&#65289;&#30340;&#25551;&#36848;&#23578;&#26410;&#25214;&#21040;&#12290;&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;Transformer&#30340;&#26263;&#34255;&#21518;&#38376;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#24182;&#30830;&#23450;&#26089;&#26399;&#23618;&#30340;MLP&#27169;&#22359;&#19982;&#21021;&#22987;&#23884;&#20837;&#25237;&#24433;&#32467;&#21512;&#26159;&#21518;&#38376;&#26426;&#21046;&#20013;&#26368;&#37325;&#35201;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#26469;&#21024;&#38500;&#12289;&#25554;&#20837;&#21644;&#20462;&#25913;&#21518;&#38376;&#26426;&#21046;&#65292;&#24182;&#29992;&#24037;&#31243;&#21270;&#26367;&#20195;&#29289;&#38477;&#20302;MLP&#27169;&#22359;&#36755;&#20986;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20027;&#35201;&#25104;&#20998;&#30340;&#20302;&#31209;&#30697;&#38453;&#30340;PCP&#28040;&#34701;&#25216;&#26415;&#65292;&#29992;&#20854;&#26367;&#25442;&#21464;&#21387;&#22120;&#27169;&#22359;&#12290;&#25105;&#20204;&#22312;&#26263;&#34255;&#21518;&#38376;&#30340;&#29609;&#20855;&#27169;&#22411;&#12289;&#26263;&#34255;&#21518;&#38376;&#30340;&#22823;&#22411;&#27169;&#22411;&#21644;&#38750;&#26263;&#34255;&#21518;&#38376;&#30340;&#24320;&#28304;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#34920;&#26126;&#25105;&#20204;&#21487;&#20197;&#25913;&#21892;&#21518;&#38376;&#30340;&#36755;&#20986;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Poisoning of data sets is a potential security threat to large language models that can lead to backdoored models. A description of the internal mechanisms of backdoored language models and how they process trigger inputs, e.g., when switching to toxic language, has yet to be found. In this work, we study the internal representations of transformer-based backdoored language models and determine early-layer MLP modules as most important for the backdoor mechanism in combination with the initial embedding projection. We use this knowledge to remove, insert, and modify backdoor mechanisms with engineered replacements that reduce the MLP module outputs to essentials for the backdoor mechanism. To this end, we introduce PCP ablation, where we replace transformer modules with low-rank matrices based on the principal components of their activations. We demonstrate our results on backdoored toy, backdoored large, and non-backdoored open-source models. We show that we can improve the backdoor r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#26080;&#27861;&#20165;&#20973;&#24120;&#35782;&#30693;&#35782;&#22238;&#31572;&#30340;&#20449;&#24687;&#23547;&#27714;&#38382;&#39064;&#32780;&#35774;&#35745;&#30340;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;InfoSeek&#12290;&#20351;&#29992;InfoSeek&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22238;&#31572;&#27714;&#30693;&#35270;&#35273;&#38382;&#39064;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#20294;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#28608;&#21457;&#27169;&#22411;&#20351;&#29992;&#32454;&#31890;&#24230;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2302.11713</link><description>&lt;p&gt;
Pre-trained Vision and Language Models&#33021;&#21542;&#22238;&#31572;&#27714;&#30693;&#35270;&#35273;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?. (arXiv:2302.11713v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#26080;&#27861;&#20165;&#20973;&#24120;&#35782;&#30693;&#35782;&#22238;&#31572;&#30340;&#20449;&#24687;&#23547;&#27714;&#38382;&#39064;&#32780;&#35774;&#35745;&#30340;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;InfoSeek&#12290;&#20351;&#29992;InfoSeek&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22238;&#31572;&#27714;&#30693;&#35270;&#35273;&#38382;&#39064;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#20294;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#28608;&#21457;&#27169;&#22411;&#20351;&#29992;&#32454;&#31890;&#24230;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Pre-trained vision and language models&#22312;&#28041;&#21450;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#39046;&#20808;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#35270;&#35273;&#38382;&#31572;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#20855;&#22791;&#22238;&#31572;&#19981;&#20165;&#20165;&#26597;&#35810;&#35270;&#35273;&#20869;&#23481;&#65292;&#32780;&#19988;&#36824;&#20855;&#26377;&#30693;&#35782;&#23494;&#38598;&#21644;&#20449;&#24687;&#23547;&#27714;&#24615;&#36136;&#30340;&#38382;&#39064;&#30340;&#33021;&#21147;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;InfoSeek&#65292;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#26080;&#27861;&#20165;&#20973;&#24120;&#35782;&#30693;&#35782;&#22238;&#31572;&#30340;&#20449;&#24687;&#23547;&#27714;&#38382;&#39064;&#32780;&#35774;&#35745;&#30340;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#20351;&#29992;InfoSeek&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#21508;&#31181;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#65292;&#24182;&#28145;&#20837;&#20102;&#35299;&#23427;&#20204;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#22914;PaLI-X&#65292;BLIP2&#31561;&#65289;&#22312;&#22238;&#31572;&#27714;&#30693;&#35270;&#35273;&#38382;&#39064;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#20294;&#22312;InfoSeek&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#33021;&#22815;&#28608;&#21457;&#27169;&#22411;&#20351;&#29992;&#20182;&#20204;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23398;&#21040;&#30340;&#32454;&#31890;&#24230;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20934;&#30830;&#30340;&#35270;&#35273;&#23454;&#20307;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained vision and language models have demonstrated state-of-the-art capabilities over existing tasks involving images and texts, including visual question answering. However, it remains unclear whether these models possess the capability to answer questions that are not only querying visual content but knowledge-intensive and information-seeking. In this study, we introduce InfoSeek, a visual question answering dataset tailored for information-seeking questions that cannot be answered with only common sense knowledge. Using InfoSeek, we analyze various pre-trained visual question answering models and gain insights into their characteristics. Our findings reveal that state-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, etc.) face challenges in answering visual information-seeking questions, but fine-tuning on the InfoSeek dataset elicits models to use fine-grained knowledge that was learned during their pre-training. Furthermore, we show that accurate visual entit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BiLD&#30340;&#26694;&#26550;&#65292;&#23427;&#30001;&#22823;&#23567;&#19981;&#21516;&#30340;&#20004;&#20010;&#27169;&#22411;&#21327;&#20316;&#29983;&#25104;&#25991;&#26412;&#12290;&#20854;&#20013;&#23567;&#22411;&#27169;&#22411;&#33258;&#22238;&#24402;&#22320;&#29983;&#25104;&#25991;&#26412;&#65292;&#32780;&#22823;&#22411;&#27169;&#22411;&#21017;&#22312;&#24517;&#35201;&#26102;&#20197;&#38750;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#23545;&#23567;&#22411;&#27169;&#22411;&#30340;&#39044;&#27979;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#25512;&#29702;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2302.07863</link><description>&lt;p&gt;
&#22823;&#23567;&#19981;&#21516;&#30340;Transformer&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Big Little Transformer Decoder. (arXiv:2302.07863v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07863
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BiLD&#30340;&#26694;&#26550;&#65292;&#23427;&#30001;&#22823;&#23567;&#19981;&#21516;&#30340;&#20004;&#20010;&#27169;&#22411;&#21327;&#20316;&#29983;&#25104;&#25991;&#26412;&#12290;&#20854;&#20013;&#23567;&#22411;&#27169;&#22411;&#33258;&#22238;&#24402;&#22320;&#29983;&#25104;&#25991;&#26412;&#65292;&#32780;&#22823;&#22411;&#27169;&#22411;&#21017;&#22312;&#24517;&#35201;&#26102;&#20197;&#38750;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#23545;&#23567;&#22411;&#27169;&#22411;&#30340;&#39044;&#27979;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#25512;&#29702;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#20351;&#24471;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#38271;&#26102;&#38388;&#30340;&#25512;&#29702;&#24310;&#36831;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#20351;&#29992;&#24182;&#19988;&#20351;&#24471;&#23427;&#20204;&#22312;&#21508;&#31181;&#23454;&#26102;&#24212;&#29992;&#20013;&#36807;&#20110;&#26114;&#36149;&#12290;&#22312;&#33258;&#22238;&#24402;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#30001;&#20110;&#27169;&#22411;&#38656;&#35201;&#36845;&#20195;&#22320;&#36816;&#34892;&#25165;&#33021;&#36880;&#20010;&#29983;&#25104;&#26631;&#35760;&#65292;&#22240;&#27492;&#25512;&#29702;&#24310;&#36831;&#26356;&#21152;&#20005;&#37325;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Big Little Decoder&#65288;BiLD&#65289;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;&#21508;&#31181;&#25991;&#26412;&#29983;&#25104;&#24212;&#29992;&#30340;&#25512;&#29702;&#25928;&#29575;&#21644;&#24310;&#36831;&#12290;BiLD&#26694;&#26550;&#21253;&#21547;&#20004;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#65292;&#23427;&#20204;&#21327;&#20316;&#22320;&#29983;&#25104;&#25991;&#26412;&#12290;&#23567;&#22411;&#27169;&#22411;&#33258;&#22238;&#24402;&#22320;&#36816;&#34892;&#20197;&#20302;&#24310;&#36831;&#29983;&#25104;&#25991;&#26412;&#65292;&#22823;&#22411;&#27169;&#22411;&#21482;&#22312;&#38656;&#35201;&#26102;&#20197;&#38750;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#35843;&#25972;&#23567;&#22411;&#27169;&#22411;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#25552;&#39640;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#21644;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#28176;&#36827;&#33976;&#39311;&#26426;&#21046;&#65292;&#20351;&#23567;&#22411;&#27169;&#22411;&#36880;&#28176;&#22320;&#20174;&#22823;&#22411;&#27169;&#22411;&#20013;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;BiLD&#26694;&#26550;&#26174;&#33879;&#38477;&#20302;&#20102;&#25512;&#29702;&#24310;&#36831;&#65292;&#21516;&#26102;&#22312;&#20445;&#25345;&#19982;&#22823;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#29983;&#25104;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent emergence of Large Language Models based on the Transformer architecture has enabled dramatic advancements in the field of Natural Language Processing. However, these models have long inference latency, which limits their deployment, and which makes them prohibitively expensive for various real-time applications. The inference latency is further exacerbated by autoregressive generative tasks, as models need to run iteratively to generate tokens sequentially without leveraging token-level parallelization. To address this, we propose Big Little Decoder (BiLD), a framework that can improve inference efficiency and latency for a wide range of text generation applications. The BiLD framework contains two models with different sizes that collaboratively generate text. The small model runs autoregressively to generate text with a low inference cost, and the large model is only invoked occasionally to refine the small model's inaccurate predictions in a non-autoregressive manner. To
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25581;&#31034;&#20102;Text-to-SQL&#27169;&#22411;&#23384;&#22312;&#30340;&#23433;&#20840;&#28431;&#27934;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#28431;&#27934;&#33021;&#22815;&#34987;&#24694;&#24847;&#21033;&#29992;&#20135;&#29983;&#25915;&#20987;&#65292;&#36890;&#36807;&#23545;&#21830;&#19994;&#24212;&#29992;&#21644;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;&#35813;&#30740;&#31350;&#24847;&#22312;&#24341;&#36215;&#23398;&#26415;&#30028;&#23545;NLP&#31639;&#27861;&#30456;&#20851;&#30340;&#36719;&#20214;&#23433;&#20840;&#38382;&#39064;&#30340;&#20851;&#27880;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2211.15363</link><description>&lt;p&gt;
&#20851;&#20110;Text-to-SQL&#27169;&#22411;&#30340;&#23433;&#20840;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
On the Security Vulnerabilities of Text-to-SQL Models. (arXiv:2211.15363v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15363
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25581;&#31034;&#20102;Text-to-SQL&#27169;&#22411;&#23384;&#22312;&#30340;&#23433;&#20840;&#28431;&#27934;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#28431;&#27934;&#33021;&#22815;&#34987;&#24694;&#24847;&#21033;&#29992;&#20135;&#29983;&#25915;&#20987;&#65292;&#36890;&#36807;&#23545;&#21830;&#19994;&#24212;&#29992;&#21644;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;&#35813;&#30740;&#31350;&#24847;&#22312;&#24341;&#36215;&#23398;&#26415;&#30028;&#23545;NLP&#31639;&#27861;&#30456;&#20851;&#30340;&#36719;&#20214;&#23433;&#20840;&#38382;&#39064;&#30340;&#20851;&#27880;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#35777;&#26126;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31639;&#27861;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#65292;&#20294;&#36825;&#20123;&#24369;&#28857;&#26159;&#21542;&#21487;&#33021;&#23548;&#33268;&#36719;&#20214;&#23433;&#20840;&#23041;&#32961;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#23545;&#24120;&#29992;&#20110;&#21019;&#24314;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#24211;&#25509;&#21475;&#30340;Text-to-SQL&#31995;&#32479;&#36827;&#34892;&#20102;&#28431;&#27934;&#27979;&#35797;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20845;&#20010;&#21830;&#19994;&#24212;&#29992;&#20013;&#30340;Text-to-SQL&#27169;&#22359;&#21487;&#20197;&#34987;&#25805;&#32437;&#20197;&#20135;&#29983;&#24694;&#24847;&#20195;&#30721;&#65292;&#28508;&#22312;&#22320;&#23548;&#33268;&#25968;&#25454;&#27844;&#28431;&#21644;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#35777;&#26126;NLP&#27169;&#22411;&#21487;&#20197;&#34987;&#21033;&#29992;&#20026;&#25915;&#20987;&#21521;&#37327;&#30340;&#31034;&#20363;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#22235;&#20010;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#23545;Text-to-SQL&#31995;&#32479;&#36827;&#34892;&#30452;&#25509;&#21518;&#38376;&#25915;&#20987;&#21487;&#20197;&#36798;&#21040;100&#65285;&#30340;&#25104;&#21151;&#29575;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24341;&#36215;&#23398;&#26415;&#30028;&#23545;&#19982;NLP&#31639;&#27861;&#30456;&#20851;&#30340;&#28508;&#22312;&#36719;&#20214;&#23433;&#20840;&#38382;&#39064;&#30340;&#20851;&#27880;&#65292;&#24182;&#40723;&#21169;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although it has been demonstrated that Natural Language Processing (NLP) algorithms are vulnerable to deliberate attacks, the question of whether such weaknesses can lead to software security threats is under-explored. To bridge this gap, we conducted vulnerability tests on Text-to-SQL systems that are commonly used to create natural language interfaces to databases. We showed that the Text-to-SQL modules within six commercial applications can be manipulated to produce malicious code, potentially leading to data breaches and Denial of Service attacks. This is the first demonstration that NLP models can be exploited as attack vectors in the wild. In addition, experiments using four open-source language models verified that straightforward backdoor attacks on Text-to-SQL systems achieve a 100% success rate without affecting their performance. The aim of this work is to draw the community's attention to potential software security issues associated with NLP algorithms and encourage explor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#33258;&#22238;&#24402;GPT&#26679;&#24335;&#27169;&#22411;&#65292;&#20998;&#21035;&#20351;&#29992;13&#20159;&#21644;130&#20159;&#20010;&#21442;&#25968;&#65292;&#22312;60&#31181;&#35821;&#35328;&#20013;&#35757;&#32451;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;Facebook&#26368;&#36817;&#21457;&#24067;&#30340;XGLM&#27169;&#22411;&#24615;&#33021;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;&#36825;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25552;&#20379;&#20102;&#26356;&#22810;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.07580</link><description>&lt;p&gt;
mGPT: &#23569;&#26679;&#26412;&#23398;&#20064;&#32773;&#36208;&#21521;&#22810;&#35821;&#35328;&#65288;arXiv:2204.07580v2 [cs.CL] &#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
mGPT: Few-Shot Learners Go Multilingual. (arXiv:2204.07580v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#33258;&#22238;&#24402;GPT&#26679;&#24335;&#27169;&#22411;&#65292;&#20998;&#21035;&#20351;&#29992;13&#20159;&#21644;130&#20159;&#20010;&#21442;&#25968;&#65292;&#22312;60&#31181;&#35821;&#35328;&#20013;&#35757;&#32451;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;Facebook&#26368;&#36817;&#21457;&#24067;&#30340;XGLM&#27169;&#22411;&#24615;&#33021;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;&#36825;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25552;&#20379;&#20102;&#26356;&#22810;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25253;&#21578;&#31216;&#65292;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#25104;&#21151;&#35299;&#20915;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#36825;&#20026;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#33258;&#22238;&#24402;GPT&#26679;&#24335;&#27169;&#22411;&#65292;&#20854;&#21442;&#25968;&#20998;&#21035;&#20026;13&#20159;&#21644;130&#20159;&#65292;&#20351;&#29992;&#32500;&#22522;&#30334;&#31185;&#21644;&#24040;&#22823;&#24178;&#20928;&#29228;&#21462;&#30340;&#35821;&#26009;&#24211;&#35757;&#32451;&#20102;25&#20010;&#35821;&#31995;&#20013;&#30340;60&#31181;&#35821;&#35328;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-2&#28304;&#20195;&#30721;&#21644;&#31232;&#30095;&#27880;&#24847;&#26426;&#21046;&#22797;&#29616;&#20102;GPT-3&#26550;&#26500;&#65307;Deepspeed&#21644;Megatron&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#24182;&#34892;&#21270;&#35757;&#32451;&#21644;&#25512;&#26029;&#27493;&#39588;&#12290;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#19982;Facebook&#26368;&#36817;&#21457;&#24067;&#30340;XGLM&#27169;&#22411;&#30456;&#24403;&#65292;&#22312;&#35206;&#30422;&#26356;&#22810;&#35821;&#35328;&#30340;&#21516;&#26102;&#22686;&#24378;&#20102;&#29420;&#32852;&#20307;&#22269;&#23478;&#21644;&#20420;&#32599;&#26031;&#23567;&#22269;&#23478;&#31561;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#35814;&#32454;&#35828;&#26126;&#20102;&#26550;&#26500;&#35774;&#35745;&#30340;&#21160;&#26426;&#65292;&#35814;&#32454;&#25551;&#36848;&#20102;&#25968;&#25454;&#20934;&#22791;&#27969;&#31243;&#65292;&#24182;&#35757;&#32451;&#20102;&#20116;&#20010;&#23567;&#29256;&#26412;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies report that autoregressive language models can successfully solve many NLP tasks via zero- and few-shot learning paradigms, which opens up new possibilities for using the pre-trained language models. This paper introduces two autoregressive GPT-like models with 1.3 billion and 13 billion parameters trained on 60 languages from 25 language families using Wikipedia and Colossal Clean Crawled Corpus. We reproduce the GPT-3 architecture using GPT-2 sources and the sparse attention mechanism; Deepspeed and Megatron frameworks allow us to parallelize the training and inference steps effectively. The resulting models show performance on par with the recently released XGLM models by Facebook, covering more languages and enhancing NLP possibilities for low resource languages of CIS countries and Russian small nations. We detail the motivation for the choices of the architecture design, thoroughly describe the data preparation pipeline, and train five small versions of the model t
&lt;/p&gt;</description></item></channel></rss>