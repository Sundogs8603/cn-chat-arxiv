<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#35843;&#25972;Whisper&#27169;&#22411;&#65292;&#25104;&#21151;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#19977;&#20010;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#30340;&#25552;&#31034;&#27604;&#40664;&#35748;&#25552;&#31034;&#24615;&#33021;&#25552;&#21319;&#20102;10%&#21040;45&#65285;&#65292;&#23637;&#29616;&#20102;Whisper&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#22810;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.11095</link><description>&lt;p&gt;
&#28608;&#21457;Web&#35268;&#27169;&#35821;&#38899;&#27169;&#22411;&#30340;&#28508;&#22312;&#33021;&#21147;&#20197;&#23454;&#29616;&#38646;-shot&#20219;&#21153;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization. (arXiv:2305.11095v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#35843;&#25972;Whisper&#27169;&#22411;&#65292;&#25104;&#21151;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#19977;&#20010;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#30340;&#25552;&#31034;&#27604;&#40664;&#35748;&#25552;&#31034;&#24615;&#33021;&#25552;&#21319;&#20102;10%&#21040;45&#65285;&#65292;&#23637;&#29616;&#20102;Whisper&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#22810;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;Web&#35268;&#27169;&#35821;&#38899;&#27169;&#22411;Whisper&#30340;&#26032;&#20852;&#21151;&#33021;&#65292;&#22312;&#20351;&#29992;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#35843;&#25972;&#27169;&#22411;&#21518;&#65292;&#36866;&#24212;&#20102;&#26410;&#35265;&#36807;&#30340;AVSR&#65292;CS-ASR&#21644;ST&#19977;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#25552;&#31034;&#65292;&#35201;&#20040;&#21033;&#29992;&#21478;&#19968;&#20010;&#22823;&#35268;&#27169;&#27169;&#22411;&#65292;&#35201;&#20040;&#31616;&#21333;&#22320;&#25805;&#20316;&#40664;&#35748;&#25552;&#31034;&#20013;&#30340;&#29305;&#27530;&#26631;&#35760;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#40664;&#35748;&#25552;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25552;&#31034;&#20351;&#36825;&#19977;&#20010;&#38646;-shot&#20219;&#21153;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;10%&#21040;45&#65285;&#65292;&#29978;&#33267;&#22312;&#19968;&#20123;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;SotA&#30417;&#30563;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;Whisper&#30340;&#35768;&#22810;&#26377;&#36259;&#23646;&#24615;&#65292;&#21253;&#25324;&#20854;&#25552;&#31034;&#30340;&#40065;&#26834;&#24615;&#65292;&#23545;&#21475;&#38899;&#30340;&#20559;&#22909;&#20197;&#21450;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#22810;&#35821;&#35328;&#29702;&#35299;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/jasonppy/PromptingWhisper&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#22320;&#29702;&#20195;&#34920;&#24615;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#22823;&#22810;&#21453;&#26144;&#32654;&#22269;&#21644;&#21360;&#24230;&#31561;&#22269;&#23478;&#30340;&#29615;&#22659;&#65292;&#32780;&#23545;&#20110;&#20854;&#20182;&#22269;&#23478;&#30340;&#21453;&#26144;&#36739;&#23569;&#65292;&#22240;&#27492;&#38656;&#35201;&#26356;&#22810;&#20851;&#27880;&#22320;&#29702;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11080</link><description>&lt;p&gt;
&#26816;&#26597;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#22270;&#20687;&#30340;&#22320;&#29702;&#20195;&#34920;&#24615;
&lt;/p&gt;
&lt;p&gt;
Inspecting the Geographical Representativeness of Images from Text-to-Image Models. (arXiv:2305.11080v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#22320;&#29702;&#20195;&#34920;&#24615;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#22823;&#22810;&#21453;&#26144;&#32654;&#22269;&#21644;&#21360;&#24230;&#31561;&#22269;&#23478;&#30340;&#29615;&#22659;&#65292;&#32780;&#23545;&#20110;&#20854;&#20182;&#22269;&#23478;&#30340;&#21453;&#26144;&#36739;&#23569;&#65292;&#22240;&#27492;&#38656;&#35201;&#26356;&#22810;&#20851;&#27880;&#22320;&#29702;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#20026;&#22823;&#22810;&#25968;&#25991;&#26412;&#36755;&#20837;&#29983;&#25104;&#26082;&#36924;&#30495;&#21448;&#30456;&#20851;&#30340;&#22270;&#20687;&#12290;&#36825;&#20123;&#27169;&#22411;&#27599;&#22825;&#29983;&#25104;&#25968;&#30334;&#19975;&#24352;&#22270;&#20687;&#65292;&#24182;&#20855;&#26377;&#26497;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#26497;&#22823;&#22320;&#24433;&#21709;&#29983;&#25104;&#33402;&#26415;&#12289;&#25968;&#23383;&#33829;&#38144;&#21644;&#25968;&#25454;&#22686;&#24378;&#31561;&#39046;&#22495;&#12290;&#37492;&#20110;&#20854;&#24040;&#22823;&#30340;&#24433;&#21709;&#21147;&#65292;&#37325;&#35201;&#30340;&#26159;&#35201;&#30830;&#20445;&#29983;&#25104;&#30340;&#20869;&#23481;&#21453;&#26144;&#20840;&#29699;&#21508;&#22320;&#30340;&#25991;&#29289;&#21644;&#29615;&#22659;&#65292;&#32780;&#19981;&#26159;&#36807;&#24230;&#20195;&#34920;&#26576;&#20123;&#22320;&#21306;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#30001;540&#21517;&#26469;&#33258;27&#20010;&#22269;&#23478;&#30340;&#20247;&#21253;&#21442;&#19982;&#32773;&#32452;&#25104;&#30340;&#30740;&#31350;&#65292;&#27979;&#37327;&#20102;&#36890;&#36807;DALL.E 2&#21644;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#24120;&#35265;&#21517;&#35789;&#65288;&#20363;&#22914;&#65292;&#19968;&#24231;&#25151;&#23376;&#65289;&#30340;&#22320;&#29702;&#20195;&#34920;&#24615;&#12290;&#23545;&#20110;&#25925;&#24847;&#26410;&#25351;&#23450;&#22269;&#23478;&#21517;&#31216;&#30340;&#36755;&#20837;&#65292;&#29983;&#25104;&#30340;&#22270;&#20687;&#26368;&#21453;&#26144;&#32654;&#22269;&#21644;&#21360;&#24230;&#31561;&#22269;&#23478;&#30340;&#29615;&#22659;&#65292;&#32780;&#26368;&#22909;&#30340;&#29983;&#25104;&#24456;&#23569;&#21453;&#26144;&#20854;&#20182;&#22269;&#23478;&#30340;&#29615;&#22659;&#65288;&#22312;25&#20010;&#22269;&#23478;&#20013;&#24179;&#22343;&#24471;&#20998;&#19981;&#21040;9%&#65289;&#12290;&#23545;&#20110;&#26126;&#30830;&#25552;&#21040;&#19968;&#20010;&#22269;&#23478;&#30340;&#36755;&#20837;&#65292;&#27169;&#22411;&#29983;&#25104;&#39640;&#24230;&#29305;&#23450;&#20110;&#35813;&#22269;&#23478;&#30340;&#22270;&#20687;&#65292;&#20855;&#26377;&#26368;&#23569;&#30340;&#36328;&#22269;&#36716;&#31227;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#35774;&#35745;&#36825;&#20123;&#27169;&#22411;&#26102;&#38656;&#35201;&#26356;&#22810;&#22320;&#20851;&#27880;&#22320;&#29702;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#35201;&#35748;&#35782;&#21040;&#36825;&#20123;&#27169;&#22411;&#22312;&#20934;&#30830;&#21453;&#26144;&#19990;&#30028;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in generative models has resulted in models that produce both realistic as well as relevant images for most textual inputs. These models are being used to generate millions of images everyday, and hold the potential to drastically impact areas such as generative art, digital marketing and data augmentation. Given their outsized impact, it is important to ensure that the generated content reflects the artifacts and surroundings across the globe, rather than over-representing certain parts of the world. In this paper, we measure the geographical representativeness of common nouns (e.g., a house) generated through DALL.E 2 and Stable Diffusion models using a crowdsourced study comprising 540 participants across 27 countries. For deliberately underspecified inputs without country names, the generated images most reflect the surroundings of the United States followed by India, and the top generations rarely reflect surroundings from all other countries (average score less th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;E-Branchformer&#21644;Conformer&#22312;&#35821;&#38899;&#35782;&#21035;&#12289;&#32763;&#35793;&#21644;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;E-Branchformer&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#65292;&#24182;&#19988;&#26356;&#21152;&#31283;&#23450;&#12290;</title><link>http://arxiv.org/abs/2305.11073</link><description>&lt;p&gt;
&#30005;&#23376;&#20998;&#25903;&#21464;&#24418;&#22120;&#19982;&#21464;&#24418;&#22120;&#22312;&#35821;&#38899;&#35782;&#21035;&#12289;&#32763;&#35793;&#21644;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks. (arXiv:2305.11073v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;E-Branchformer&#21644;Conformer&#22312;&#35821;&#38899;&#35782;&#21035;&#12289;&#32763;&#35793;&#21644;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;E-Branchformer&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#65292;&#24182;&#19988;&#26356;&#21152;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Conformer&#26159;&#19968;&#31181;&#21367;&#31215;&#21464;&#25442;&#22120;&#65292;&#30001;&#20110;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12289;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#21644;&#21475;&#35821;&#29702;&#35299;&#65288;SLU&#65289;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#65292;&#24050;&#32463;&#25104;&#20026;&#35821;&#38899;&#22788;&#29702;&#30340;&#20107;&#23454;&#26631;&#20934;&#32534;&#30721;&#22120;&#26550;&#26500;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#21517;&#20026;E-Branchformer&#30340;&#26032;&#32534;&#30721;&#22120;&#22312;LibriSpeech ASR&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;Conformer&#65292;&#20351;&#20854;&#22312;&#26356;&#26222;&#36941;&#30340;&#35821;&#38899;&#24212;&#29992;&#20013;&#21464;&#24471;&#26377;&#21069;&#36884;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#31471;&#21040;&#31471;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;E-Branchformer&#21644;Conformer&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;15&#20010;ASR&#12289;2&#20010;ST&#21644;3&#20010;SLU&#22522;&#20934;&#27979;&#35797;&#30340;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#38598;&#20013;&#65292;E-Branchformer&#30340;&#34920;&#29616;&#19982;Conformer&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#65292;&#21516;&#26102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26356;&#20026;&#31283;&#23450;&#12290;&#25105;&#20204;&#23558;&#21457;&#24067;&#25105;&#20204;&#30340;&#35757;&#32451;&#37197;&#32622;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#23454;&#29616;&#21487;&#37325;&#22797;&#24615;&#65292;&#20174;&#20013;&#21463;&#30410;&#30340;&#23558;&#26159;&#35821;&#38899;&#31038;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformer, a convolution-augmented Transformer variant, has become the de facto encoder architecture for speech processing due to its superior performance in various tasks, including automatic speech recognition (ASR), speech translation (ST) and spoken language understanding (SLU). Recently, a new encoder called E-Branchformer has outperformed Conformer in the LibriSpeech ASR benchmark, making it promising for more general speech applications. This work compares E-Branchformer and Conformer through extensive experiments using different types of end-to-end sequence-to-sequence models. Results demonstrate that E-Branchformer achieves comparable or better performance than Conformer in almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while being more stable during training. We will release our training configurations and pre-trained models for reproducibility, which can benefit the speech community.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;Spin&#65292;&#36890;&#36807;&#35828;&#35805;&#32773;&#19981;&#21464;&#32858;&#31867;&#26469;&#35299;&#24320;&#35828;&#35805;&#32773;&#20449;&#24687;&#24182;&#20445;&#30041;&#20869;&#23481;&#34920;&#31034;&#65292;&#21482;&#38656;45&#20998;&#38047;&#24494;&#35843;&#21363;&#21487;&#25913;&#36827;&#39044;&#35757;&#32451;&#32593;&#32476;&#65292;&#22312;&#35821;&#38899;&#35782;&#21035;&#21644;&#22768;&#23398;&#21333;&#20803;&#21457;&#29616;&#20013;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11072</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#24494;&#35843;&#65306;&#36890;&#36807;&#35828;&#35805;&#32773;&#19981;&#21464;&#32858;&#31867;&#26469;&#25913;&#36827;&#20869;&#23481;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Fine-tuning for Improved Content Representations by Speaker-invariant Clustering. (arXiv:2305.11072v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11072
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;Spin&#65292;&#36890;&#36807;&#35828;&#35805;&#32773;&#19981;&#21464;&#32858;&#31867;&#26469;&#35299;&#24320;&#35828;&#35805;&#32773;&#20449;&#24687;&#24182;&#20445;&#30041;&#20869;&#23481;&#34920;&#31034;&#65292;&#21482;&#38656;45&#20998;&#38047;&#24494;&#35843;&#21363;&#21487;&#25913;&#36827;&#39044;&#35757;&#32451;&#32593;&#32476;&#65292;&#22312;&#35821;&#38899;&#35782;&#21035;&#21644;&#22768;&#23398;&#21333;&#20803;&#21457;&#29616;&#20013;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#25913;&#36827;&#23427;&#20204;&#23545;&#20869;&#23481;&#30456;&#20851;&#38382;&#39064;&#30340;&#34920;&#31034;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#35828;&#35805;&#32773;&#19981;&#21464;&#32858;&#31867;(Spin)&#30340;&#26032;&#22411;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#31867;&#35821;&#38899;&#34920;&#31034;&#24182;&#22312;&#21407;&#22987;&#35821;&#38899;&#21644;&#35828;&#35805;&#32773;&#25200;&#21160;&#35821;&#38899;&#20043;&#38388;&#36827;&#34892;&#20132;&#25442;&#39044;&#27979;&#26469;&#35299;&#24320;&#35828;&#35805;&#32773;&#20449;&#24687;&#24182;&#20445;&#30041;&#20869;&#23481;&#34920;&#31034;&#12290;&#20351;&#29992;&#21333;&#20010;GPU&#36827;&#34892;45&#20998;&#38047;&#30340;&#24494;&#35843;&#21363;&#21487;&#25913;&#36827;&#39044;&#35757;&#32451;&#32593;&#32476;&#65292;&#24182;&#22312;&#35821;&#38899;&#35782;&#21035;&#21644;&#22768;&#23398;&#21333;&#20803;&#21457;&#29616;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised speech representation models have succeeded in various tasks, but improving them for content-related problems using unlabeled data is challenging. We propose speaker-invariant clustering (Spin), a novel self-supervised learning method that clusters speech representations and performs swapped prediction between the original and speaker-perturbed utterances. Spin disentangles speaker information and preserves content representations with just 45 minutes of fine-tuning on a single GPU. Spin improves pre-trained networks and outperforms prior methods in speech recognition and acoustic unit discovery.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22270;&#24418;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#21487;&#26356;&#22909;&#30340;&#29702;&#35299;&#25991;&#26412;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;BERT&#27169;&#22411;&#22312;Pubmed&#19978;&#30340;&#20998;&#31867;&#20219;&#21153;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.11070</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#24418;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#25991;&#26412;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Enriching language models with graph-based context information to better understand textual data. (arXiv:2305.11070v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11070
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#24418;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#21487;&#26356;&#22909;&#30340;&#29702;&#35299;&#25991;&#26412;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;BERT&#27169;&#22411;&#22312;Pubmed&#19978;&#30340;&#20998;&#31867;&#20219;&#21153;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#22825;&#36935;&#21040;&#30340;&#25991;&#26412;&#20855;&#26377;&#30456;&#20114;&#32852;&#31995;&#30340;&#24773;&#20917;&#30456;&#24403;&#22810;&#12290;&#20363;&#22914;&#65292;Wikipedia&#25991;&#31456;&#36890;&#36807;&#36229;&#38142;&#25509;&#24341;&#29992;&#20854;&#20182;&#25991;&#31456;&#65292;&#31185;&#23398;&#35770;&#25991;&#36890;&#36807;&#24341;&#29992;&#25110;&#65288;&#20849;&#21516;&#65289;&#20316;&#32773;&#19982;&#20854;&#20182;&#35770;&#25991;&#30456;&#20851;&#32852;&#65292;&#32780;&#25512;&#25991;&#21017;&#36890;&#36807;&#20851;&#27880;&#24444;&#27492;&#25110;&#36716;&#21457;&#20869;&#23481;&#26469;&#20851;&#32852;&#12290;&#22240;&#27492;&#65292;&#31867;&#20284;&#20110;&#22270;&#24418;&#30340;&#32467;&#26500;&#21487;&#20197;&#34920;&#31034;&#29616;&#26377;&#30340;&#32852;&#31995;&#65292;&#24182;&#34987;&#35270;&#20026;&#25429;&#25417;&#25991;&#26412;&#30340;&#8220;&#19978;&#19979;&#25991;&#8221;&#12290;&#22240;&#27492;&#65292;&#25552;&#21462;&#21644;&#25972;&#21512;&#36825;&#31181;&#19978;&#19979;&#25991;&#20449;&#24687;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#33258;&#21160;&#29702;&#35299;&#25991;&#26412;&#65311;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23454;&#39564;&#24615;&#22320;&#35777;&#26126;&#65292;&#23558;&#22522;&#20110;&#22270;&#24418;&#30340;&#19978;&#19979;&#25991;&#21270;&#32435;&#20837;BERT&#27169;&#22411;&#20250;&#22686;&#24378;&#20854;&#22312;&#20998;&#31867;&#20219;&#21153;&#31034;&#20363;&#19978;&#30340;&#34920;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;Pubmed&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#35823;&#24046;&#20174;8.51&#65285;&#38477;&#33267;7.96&#65285;&#65292;&#21516;&#26102;&#20165;&#22686;&#21152;&#20102;1.6&#65285;&#30340;&#21442;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
A considerable number of texts encountered daily are somehow connected with each other. For example, Wikipedia articles refer to other articles via hyperlinks, scientific papers relate to others via citations or (co)authors, while tweets relate via users that follow each other or reshare content. Hence, a graph-like structure can represent existing connections and be seen as capturing the "context" of the texts. The question thus arises if extracting and integrating such context information into a language model might help facilitate a better automated understanding of the text. In this study, we experimentally demonstrate that incorporating graph-based contextualization into BERT model enhances its performance on an example of a classification task. Specifically, on Pubmed dataset, we observed a reduction in error from 8.51% to 7.96%, while increasing the number of parameters just by 1.6%.  Our source code: https://github.com/tryptofanik/gc-bert
&lt;/p&gt;</description></item><item><title>ORKG-Leaderboards&#26159;&#19968;&#31181;&#20197;&#30693;&#35782;&#22270;&#35889;&#24418;&#24335;&#25366;&#25496;AI&#39046;&#22495;&#25490;&#34892;&#27036;&#24182;&#25903;&#25345;&#26426;&#22120;&#21487;&#25805;&#20316;&#24615;&#20986;&#29256;&#30340;&#31995;&#32479;&#65292;&#33021;&#22815;&#35753;&#30740;&#31350;&#20154;&#21592;&#36879;&#26126;&#22320;&#20102;&#35299;&#20840;&#29699;&#30740;&#31350;&#20154;&#21592;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#36319;&#36394;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2305.11068</link><description>&lt;p&gt;
ORKG-Leaderboards: &#19968;&#31181;&#20197;&#30693;&#35782;&#22270;&#35889;&#24418;&#24335;&#25366;&#25496;&#25490;&#34892;&#27036;&#30340;&#31995;&#32479;&#21270;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
ORKG-Leaderboards: A Systematic Workflow for Mining Leaderboards as a Knowledge Graph. (arXiv:2305.11068v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11068
&lt;/p&gt;
&lt;p&gt;
ORKG-Leaderboards&#26159;&#19968;&#31181;&#20197;&#30693;&#35782;&#22270;&#35889;&#24418;&#24335;&#25366;&#25496;AI&#39046;&#22495;&#25490;&#34892;&#27036;&#24182;&#25903;&#25345;&#26426;&#22120;&#21487;&#25805;&#20316;&#24615;&#20986;&#29256;&#30340;&#31995;&#32479;&#65292;&#33021;&#22815;&#35753;&#30740;&#31350;&#20154;&#21592;&#36879;&#26126;&#22320;&#20102;&#35299;&#20840;&#29699;&#30740;&#31350;&#20154;&#21592;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#36319;&#36394;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; ORKG-Leaderboard &#36719;&#20214;&#65292;&#23427;&#26088;&#22312;&#20174;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#22823;&#37327;&#23454;&#35777;&#30740;&#31350;&#35770;&#25991;&#20013;&#33258;&#21160;&#25552;&#21462;&#20197;&#20219;&#21153;-&#25968;&#25454;&#38598;-&#24230;&#37327;&#20803;&#32452;&#20026;&#23450;&#20041;&#30340;&#25490;&#34892;&#27036;&#12290;&#35813;&#36719;&#20214;&#25903;&#25345;&#23398;&#26415;&#20986;&#29256;&#30340;&#20027;&#35201;&#24037;&#20316;&#27969;&#31243;&#65292;&#21363; LaTeX &#25991;&#20214;&#25110; PDF &#25991;&#20214;&#12290;&#27492;&#22806;&#65292;&#35813;&#31995;&#32479;&#36824;&#19982; Open Research Knowledge Graph (ORKG) &#24179;&#21488;&#38598;&#25104;&#65292;&#35813;&#24179;&#21488;&#20419;&#36827;&#20102;&#23398;&#26415;&#21457;&#29616;&#30340;&#26426;&#22120;&#21487;&#25805;&#20316;&#24615;&#20986;&#29256;&#12290;&#22240;&#27492;&#65292;&#24403;&#31995;&#32479;&#36755;&#20986;&#19982; ORKG &#25903;&#25345;&#30340;&#35821;&#20041; web &#22522;&#30784;&#35774;&#26045;&#30456;&#32467;&#21512;&#26102;&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;&#20004;&#20010;&#26041;&#38754;&#30340;&#21151;&#33021;&#65306;1&#65289;&#27178;&#36328;&#20840;&#29699;&#30340;&#30740;&#31350;&#20154;&#21592;&#30340;&#23454;&#35777;&#32467;&#26524;&#30340;&#38598;&#25104;&#65292;&#20174;&#32780;&#23454;&#29616;&#23454;&#35777;&#30740;&#31350;&#30340;&#36879;&#26126;&#24230;&#65292;&#24182;&#26377;&#21487;&#33021;&#26159;&#23436;&#25972;&#30340;&#65307;2&#65289;&#35753;&#30740;&#31350;&#20154;&#21592;&#20102;&#35299;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The purpose of this work is to describe the Orkg-Leaderboard software designed to extract leaderboards defined as Task-Dataset-Metric tuples automatically from large collections of empirical research papers in Artificial Intelligence (AI). The software can support both the main workflows of scholarly publishing, viz. as LaTeX files or as PDF files. Furthermore, the system is integrated with the Open Research Knowledge Graph (ORKG) platform, which fosters the machine-actionable publishing of scholarly findings. Thus the system output, when integrated within the ORKG's supported Semantic Web infrastructure of representing machine-actionable 'resources' on the Web, enables: 1) broadly, the integration of empirical results of researchers across the world, thus enabling transparency in empirical research with the potential to also being complete contingent on the underlying data source(s) of publications; and 2) specifically, enables researchers to track the progress in AI with an overview 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GPT&#27169;&#22411;&#22312;&#29983;&#25104;&#29305;&#23450;&#20316;&#32773;&#39118;&#26684;&#35799;&#27468;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#21363;&#20351;&#25552;&#20379;&#20102;&#22823;&#37327;&#26679;&#26412;&#65292;&#26410;&#32463;&#24494;&#35843;&#30340;&#27169;&#22411;&#20063;&#19981;&#33021;&#29983;&#25104;&#25152;&#38656;&#39118;&#26684;&#30340;&#35799;&#27468;&#12290;</title><link>http://arxiv.org/abs/2305.11064</link><description>&lt;p&gt;
Bits of Grass: GPT&#26159;&#21542;&#24050;&#32463;&#25317;&#26377;&#20102;&#20889;&#20316;&#24800;&#29305;&#26364;&#26679;&#24335;&#30340;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
Bits of Grass: Does GPT already know how to write like Whitman?. (arXiv:2305.11064v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GPT&#27169;&#22411;&#22312;&#29983;&#25104;&#29305;&#23450;&#20316;&#32773;&#39118;&#26684;&#35799;&#27468;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#21363;&#20351;&#25552;&#20379;&#20102;&#22823;&#37327;&#26679;&#26412;&#65292;&#26410;&#32463;&#24494;&#35843;&#30340;&#27169;&#22411;&#20063;&#19981;&#33021;&#29983;&#25104;&#25152;&#38656;&#39118;&#26684;&#30340;&#35799;&#27468;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26816;&#39564;&#20102;GPT-3.5&#12289;GPT-3.5-Turbo&#65288;ChatGPT&#65289;&#21644;GPT-4&#27169;&#22411;&#20351;&#29992;&#38646;/&#22810;&#27425;&#25552;&#31034;&#65288;&#20351;&#29992;&#26368;&#22823;&#19978;&#19979;&#25991;&#38271;&#24230;8192&#20010;&#20196;&#29260;&#65289;&#29983;&#25104;&#29305;&#23450;&#20316;&#32773;&#39118;&#26684;&#35799;&#27468;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study examines the ability of GPT-3.5, GPT-3.5-turbo (ChatGPT) and GPT-4 models to generate poems in the style of specific authors using zero-shot and many-shot prompts (which use the maximum context length of 8192 tokens). We assess the performance of models that are not fine-tuned for generating poetry in the style of specific authors, via automated evaluation. Our findings indicate that without fine-tuning, even when provided with the maximum number of 17 poem examples (8192 tokens) in the prompt, these models do not generate poetry in the desired style.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#27700;&#32447;&#30340;&#25991;&#26412;&#36716;SQL&#26041;&#27861;: SPSQL&#65292;&#23427;&#23558;&#20219;&#21153;&#20998;&#35299;&#25104;&#22235;&#20010;&#23376;&#20219;&#21153;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#27169;&#22411;&#38590;&#24230;&#21644;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#35201;&#27714;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.11061</link><description>&lt;p&gt;
SPSQL: &#22522;&#20110;&#36880;&#27493;&#35299;&#26512;&#30340;&#25991;&#26412;&#36716;SQL&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SPSQL: Step-by-step Parsing Based Framework for Text-to-SQL Generation. (arXiv:2305.11061v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#27700;&#32447;&#30340;&#25991;&#26412;&#36716;SQL&#26041;&#27861;: SPSQL&#65292;&#23427;&#23558;&#20219;&#21153;&#20998;&#35299;&#25104;&#22235;&#20010;&#23376;&#20219;&#21153;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#27169;&#22411;&#38590;&#24230;&#21644;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#35201;&#27714;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#25991;&#26412;&#36716;&#25442;&#20026;&#32467;&#26500;&#21270;&#26597;&#35810;&#35821;&#35328;(Text2SQL)&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#30340;&#30740;&#31350;&#28909;&#28857;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#65292;&#25968;&#25454;&#24211;&#30340;&#20351;&#29992;&#24050;&#32463;&#28183;&#36879;&#21040;&#21508;&#20010;&#39046;&#22495;&#65292;&#20854;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#35268;&#27169;&#22823;&#12289;&#31181;&#31867;&#22810;&#26679;&#12289;&#33539;&#22260;&#24191;&#27867;&#65292;&#20351;&#24471;&#25968;&#25454;&#26597;&#35810;&#32321;&#29712;&#20302;&#25928;&#65292;&#23545;Text2SQL&#27169;&#22411;&#25552;&#20986;&#20102;&#26356;&#39640;&#30340;&#35201;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#27700;&#32447;&#30340;Text2SQL&#26041;&#27861;: SPSQL&#65292;&#23558;&#25991;&#26412;&#36716;&#25442;&#20026;SQL&#36807;&#31243;&#20998;&#35299;&#20026;&#22235;&#20010;&#23376;&#20219;&#21153;&#8212;&#8212;&#34920;&#36873;&#25321;&#12289;&#21015;&#36873;&#25321;&#12289;SQL&#29983;&#25104;&#21644;&#20540;&#22635;&#20805;&#65292;&#21487;&#20197;&#36716;&#21270;&#20026;&#19968;&#20010;&#21487;&#35299;&#26512;&#30340;&#21477;&#27861;&#26641;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Converting text into the structured query language (Text2SQL) is a research hotspot in the field of natural language processing (NLP), which has broad application prospects. In the era of big data, the use of databases has penetrated all walks of life, in which the collected data is large in scale, diverse in variety, and wide in scope, making the data query cumbersome and inefficient, and putting forward higher requirements for the Text2SQL model. In practical applications, the current mainstream end-to-end Text2SQL model is not only difficult to build due to its complex structure and high requirements for training data, but also difficult to adjust due to massive parameters. In addition, the accuracy of the model is hard to achieve the desired result. Based on this, this paper proposes a pipelined Text2SQL method: SPSQL. This method disassembles the Text2SQL task into four subtasks--table selection, column selection, SQL generation, and value filling, which can be converted into a te
&lt;/p&gt;</description></item><item><title>BERM&#20351;&#29992;&#24179;&#34913;&#30340;&#12289;&#21487;&#25552;&#21462;&#30340;&#29305;&#24449;&#34920;&#31034;&#27861;&#26469;&#25429;&#25417;&#21305;&#37197;&#20449;&#21495;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23494;&#38598;&#26816;&#32034;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11052</link><description>&lt;p&gt;
BERM&#65306;&#35757;&#32451;&#24179;&#34913;&#21487;&#25552;&#21462;&#34920;&#31034;&#20197;&#25552;&#39640;&#23494;&#38598;&#26816;&#32034;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
BERM: Training the Balanced and Extractable Representation for Matching to Improve Generalization Ability of Dense Retrieval. (arXiv:2305.11052v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11052
&lt;/p&gt;
&lt;p&gt;
BERM&#20351;&#29992;&#24179;&#34913;&#30340;&#12289;&#21487;&#25552;&#21462;&#30340;&#29305;&#24449;&#34920;&#31034;&#27861;&#26469;&#25429;&#25417;&#21305;&#37197;&#20449;&#21495;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23494;&#38598;&#26816;&#32034;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#26816;&#32034;&#24050;&#32463;&#34920;&#29616;&#20986;&#22312;&#22495;&#20869;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#22312;&#31532;&#19968;&#38454;&#27573;&#26816;&#32034;&#36807;&#31243;&#20013;&#26377;&#25152;&#20316;&#20026;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#30001;&#20110;&#23494;&#38598;&#26816;&#32034;&#23545;&#22495;&#19981;&#21464;&#21644;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#30340;&#24314;&#27169;&#36739;&#24369;&#65288;&#21363;&#20004;&#20010;&#25991;&#26412;&#20043;&#38388;&#30340;&#21305;&#37197;&#20449;&#21495;&#65292;&#36825;&#26159;&#20449;&#24687;&#26816;&#32034;&#30340;&#26412;&#36136;&#65289;&#65292;&#22240;&#27492;&#38590;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25429;&#25417;&#21305;&#37197;&#20449;&#21495;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#39640;&#23494;&#38598;&#26816;&#32034;&#27867;&#21270;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;BERM&#12290;&#20840;&#38754;&#30340;&#32454;&#31890;&#24230;&#34920;&#36798;&#21644;&#26597;&#35810;&#23548;&#21521;&#30340;&#26174;&#30528;&#24615;&#26159;&#21305;&#37197;&#20449;&#21495;&#30340;&#20004;&#20010;&#23646;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;BERM&#20013;&#65292;&#19968;&#20010;&#21333;&#19968;&#30340;Passage&#34987;&#21010;&#20998;&#20026;&#22810;&#20010;&#21333;&#20803;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#21333;&#20803;&#32423;&#35201;&#27714;&#20316;&#20026;&#32422;&#26463;&#36827;&#34892;&#34920;&#31034;&#20197;&#33719;&#24471;&#26377;&#25928;&#30340;&#21305;&#37197;&#20449;&#21495;&#12290;&#19968;&#20010;&#26159;&#35821;&#20041;&#21333;&#20803;&#24179;&#34913;&#65292;&#21478;&#19968;&#20010;&#26159;&#24517;&#38656;&#30340;&#21305;&#37197;&#21333;&#20803;&#21487;&#25552;&#21462;&#24615;&#12290;&#21333;&#20803;&#32423;&#35270;&#22270;&#21644;&#24179;&#34913;&#35821;&#20041;&#20351;&#34920;&#31034;&#20197;&#32454;&#31890;&#24230;&#30340;&#26041;&#24335;&#34920;&#36798;&#25991;&#26412;&#12290;&#24517;&#38656;&#30340;&#21305;&#37197;&#21333;&#20803;&#21487;&#25552;&#21462;&#24615;&#30830;&#20445;&#20445;&#30041;&#20449;&#24687;&#26816;&#32034;&#30340;&#26412;&#36136;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;BERM&#22312;&#20445;&#25345;&#22495;&#20869;&#25968;&#25454;&#38598;&#19978;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense retrieval has shown promise in the first-stage retrieval process when trained on in-domain labeled datasets. However, previous studies have found that dense retrieval is hard to generalize to unseen domains due to its weak modeling of domain-invariant and interpretable feature (i.e., matching signal between two texts, which is the essence of information retrieval). In this paper, we propose a novel method to improve the generalization of dense retrieval via capturing matching signal called BERM. Fully fine-grained expression and query-oriented saliency are two properties of the matching signal. Thus, in BERM, a single passage is segmented into multiple units and two unit-level requirements are proposed for representation as the constraint in training to obtain the effective matching signal. One is semantic unit balance and the other is essential matching unit extractability. Unit-level view and balanced semantics make representation express the text in a fine-grained manner. Esse
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312; PLMs &#20013;&#27880;&#20837;&#19978;&#19979;&#25991; NER &#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#23569;&#37327;&#31034;&#24847;&#23454;&#20363;&#21363;&#21487;&#21160;&#24577;&#35782;&#21035;&#26032;&#31867;&#22411;&#30340;&#23454;&#20307;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11038</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Learning In-context Learning for Named Entity Recognition. (arXiv:2305.11038v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312; PLMs &#20013;&#27880;&#20837;&#19978;&#19979;&#25991; NER &#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#23569;&#37327;&#31034;&#24847;&#23454;&#20363;&#21363;&#21487;&#21160;&#24577;&#35782;&#21035;&#26032;&#31867;&#22411;&#30340;&#23454;&#20307;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21463;&#21040;&#23454;&#20307;&#31867;&#22411;&#30340;&#22810;&#26679;&#24615;&#12289;&#26032;&#23454;&#20307;&#31867;&#22411;&#30340;&#20986;&#29616;&#21644;&#39640;&#36136;&#37327;&#26631;&#27880;&#30340;&#32570;&#20047;&#31561;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#19978;&#19979;&#25991;NER&#33021;&#21147;&#26377;&#25928;&#22320;&#27880;&#20837;&#21040;PLMs&#20013;&#65292;&#24182;&#19988;&#21482;&#20351;&#29992;&#23569;&#37327;&#31034;&#24847;&#23454;&#20363;&#23601;&#33021;&#21160;&#24577;&#35782;&#21035;&#26032;&#31867;&#22411;&#30340;&#23454;&#20307;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;PLMs&#24314;&#27169;&#20026;&#19968;&#20010;&#20803;&#20989;&#25968; $\mathcal{ \lambda_ {\text{instruction, demonstrations, text}}. M}$&#65292;&#24182;&#36890;&#36807;&#23558;&#26032;&#30340;&#25351;&#31034;&#21644;&#31034;&#20363;&#24212;&#29992;&#20110;PLMs&#26469;&#38544;&#21547;&#22320;&#26500;&#24314;&#26032;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#65292;&#21363; $\mathcal{ (\lambda . M) }$(instruction, demonstrations) $\to$ $\mathcal{F}$&#65292;&#20854;&#20013; $\mathcal{F}$ &#23558;&#25104;&#20026;&#19968;&#20010;&#26032;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#65292;&#21363; $\mathcal{F}$: text $\to$ entities&#12290;&#20026;&#20102;&#23558;&#19978;&#36848;&#19978;&#19979;&#25991;NER&#33021;&#21147;&#27880;&#20837;PLMs&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#20989;&#25968;&#39044;&#35757;&#32451;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#27604;&#36739;&#65288;&#25351;&#31034;&#12289;&#31034;&#20363;&#65289;-identity&#21644;&#65288;&#25513;&#30422;&#21518;&#30340;&#25351;&#31034;&#12289;&#31034;&#20363;&#65289;-identity&#26469;&#39044;&#35757;&#32451;PLMs&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#20351;&#29992;&#23569;&#37327;&#31034;&#24847;&#23454;&#20363;&#26377;&#25928;&#22320;&#35782;&#21035;&#26032;&#31867;&#22411;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named entity recognition in real-world applications suffers from the diversity of entity types, the emergence of new entity types, and the lack of high-quality annotations. To address the above problems, this paper proposes an in-context learning-based NER approach, which can effectively inject in-context NER ability into PLMs and recognize entities of novel types on-the-fly using only a few demonstrative instances. Specifically, we model PLMs as a meta-function $\mathcal{ \lambda_ {\text{instruction, demonstrations, text}}. M}$, and a new entity extractor can be implicitly constructed by applying new instruction and demonstrations to PLMs, i.e., $\mathcal{ (\lambda . M) }$(instruction, demonstrations) $\to$ $\mathcal{F}$ where $\mathcal{F}$ will be a new entity extractor, i.e., $\mathcal{F}$: text $\to$ entities. To inject the above in-context NER ability into PLMs, we propose a meta-function pre-training algorithm, which pre-trains PLMs by comparing the (instruction, demonstration)-i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Wordpieces&#26367;&#25442;&#35821;&#27861;&#26641;&#26469;&#36827;&#34892;&#20197;&#30446;&#26631;&#20026;&#23548;&#21521;&#30340;&#24847;&#35265;&#35789;&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#21477;&#23376; - &#26041;&#38754;&#23545;&#26469;&#22686;&#24378;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11034</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;Wordpieces&#21644;&#22686;&#24378;Aspect&#26469;&#36827;&#34892;&#20197;&#30446;&#26631;&#20026;&#23548;&#21521;&#30340;&#24847;&#35265;&#35789;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Trading Syntax Trees for Wordpieces: Target-oriented Opinion Words Extraction with Wordpieces and Aspect Enhancement. (arXiv:2305.11034v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Wordpieces&#26367;&#25442;&#35821;&#27861;&#26641;&#26469;&#36827;&#34892;&#20197;&#30446;&#26631;&#20026;&#23548;&#21521;&#30340;&#24847;&#35265;&#35789;&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#21477;&#23376; - &#26041;&#38754;&#23545;&#26469;&#22686;&#24378;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#20197;&#30446;&#26631;&#20026;&#23548;&#21521;&#30340;&#24847;&#35265;&#35789;&#25552;&#21462;&#65288;TOWE&#65289;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#25805;&#20316;&#22312;&#21333;&#35789;&#32423;&#21035;&#19978;&#65292;&#20197;&#21450;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#65292;&#36825;&#20123;&#32593;&#32476;&#21487;&#20197;&#23558;&#20174;&#21477;&#27861;&#26641;&#20013;&#25552;&#21462;&#30340;&#21477;&#27861;&#20449;&#24687;&#32435;&#20837;&#27169;&#22411;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20351;&#29992;GCN&#26102;&#20165;&#21462;&#24471;&#20102;&#26377;&#38480;&#30340;&#22686;&#30410;&#65292;&#24182;&#19988;&#20351;&#29992;BERT wordpieces&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24050;&#30693;BERT wordpieces&#22312;&#34920;&#31034;&#32597;&#35265;&#30340;&#21333;&#35789;&#25110;&#19978;&#19979;&#25991;&#20449;&#24687;&#19981;&#36275;&#30340;&#21333;&#35789;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#36890;&#36807;&#23436;&#20840;&#28040;&#38500;&#26041;&#27861;&#32467;&#26500;&#20013;&#30340;GCN&#32452;&#20214;&#65292;&#20197;Wordpieces&#26469;&#20132;&#25442;&#21477;&#27861;&#26641;&#12290;&#20026;&#20102;&#22686;&#24378;TOWE&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#32534;&#30721;&#36807;&#31243;&#20013;&#26041;&#38754;&#34920;&#31034;&#20002;&#22833;&#30340;&#38382;&#39064;&#12290;&#19982;&#20854;&#20165;&#20165;&#20351;&#29992;&#21477;&#23376;&#20316;&#20026;&#36755;&#20837;&#65292;&#25105;&#20204;&#20351;&#29992;&#21477;&#23376; - &#26041;&#38754;&#23545;&#12290;&#25105;&#20204;&#30340;&#30456;&#23545;&#31616;&#21333;&#30340;&#26041;&#27861;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#24212;&#20316;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24378;&#26377;&#21147;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art target-oriented opinion word extraction (TOWE) models typically use BERT-based text encoders that operate on the word level, along with graph convolutional networks (GCNs) that incorporate syntactic information extracted from syntax trees. These methods achieve limited gains with GCNs and have difficulty using BERT wordpieces. Meanwhile, BERT wordpieces are known to be effective at representing rare words or words with insufficient context information. To address this issue, this work trades syntax trees for BERT wordpieces by entirely removing the GCN component from the methods' architectures. To enhance TOWE performance, we tackle the issue of aspect representation loss during encoding. Instead of solely utilizing a sentence as the input, we use a sentence-aspect pair. Our relatively simple approach achieves state-of-the-art results on benchmark datasets and should serve as a strong baseline for further research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#26631;&#31614;&#21435;&#22122;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#20934;&#30830;&#22320;&#22312;&#25991;&#26723;&#32423;&#36828;&#31243;&#20851;&#31995;&#25277;&#21462;&#20013;&#36873;&#25321;&#21487;&#20449;&#30340;&#20266;&#26631;&#31614;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#22312;DocRED&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11029</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#26631;&#31614;&#21435;&#22122;&#22312;&#25991;&#26723;&#32423;&#36828;&#31243;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction. (arXiv:2305.11029v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#26631;&#31614;&#21435;&#22122;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#20934;&#30830;&#22320;&#22312;&#25991;&#26723;&#32423;&#36828;&#31243;&#20851;&#31995;&#25277;&#21462;&#20013;&#36873;&#25321;&#21487;&#20449;&#30340;&#20266;&#26631;&#31614;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#22312;DocRED&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#26088;&#22312;&#25512;&#26029;&#25991;&#26723;&#20013;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#35821;&#20041;&#20851;&#31995;&#12290;&#36828;&#31243;&#30417;&#30563;&#33021;&#22815;&#29983;&#25104;&#22823;&#37327;&#33258;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#21487;&#20197;&#25552;&#39640;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19981;&#21487;&#38752;&#30340;&#20266;&#26631;&#31614;&#20250;&#24102;&#26469;&#26032;&#30340;&#22122;&#22768;&#65292;&#20363;&#22914;&#28155;&#21152;&#34394;&#20551;&#30340;&#20266;&#26631;&#31614;&#21644;&#22833;&#21435;&#27491;&#30830;&#30340;&#30417;&#30563;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#36873;&#25321;&#26377;&#25928;&#30340;&#20266;&#26631;&#31614;&#26469;&#21435;&#22122;&#36828;&#31243;&#30417;&#30563;&#25968;&#25454;&#20173;&#28982;&#26159;&#25991;&#26723;&#32423;&#36828;&#31243;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#25216;&#26415;&#26469;&#30830;&#23450;&#20266;&#26631;&#31614;&#26159;&#21542;&#21487;&#20449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#26631;&#31614;&#21435;&#22122;&#30340;&#25991;&#26723;&#32423;&#36828;&#31243;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;UGDRE&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#20363;&#32423;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#27979;&#37327;&#20102;&#20855;&#26377;&#37325;&#21472;&#20851;&#31995;&#30340;&#20266;&#26631;&#31614;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#32771;&#34385;&#23454;&#20363;&#32423;&#21644;&#20851;&#31995;&#32423;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26631;&#31614;&#21435;&#22122;&#32452;&#20214;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36873;&#25321;&#21487;&#38752;&#30340;&#20266;&#26631;&#31614;&#36827;&#34892;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;DocRED&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level relation extraction (DocRE) aims to infer complex semantic relations among entities in a document. Distant supervision (DS) is able to generate massive auto-labeled data, which can improve DocRE performance. Recent works leverage pseudo labels generated by the pre-denoising model to reduce noise in DS data. However, unreliable pseudo labels bring new noise, e.g., adding false pseudo labels and losing correct DS labels. Therefore, how to select effective pseudo labels to denoise DS data is still a challenge in document-level distant relation extraction. To tackle this issue, we introduce uncertainty estimation technology to determine whether pseudo labels can be trusted. In this work, we propose a Document-level distant Relation Extraction framework with Uncertainty Guided label denoising, UGDRE. Specifically, we propose a novel instance-level uncertainty estimation method, which measures the reliability of the pseudo labels with overlapping relations. By further consider
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#29616;&#23454;&#22330;&#26223;&#19979;&#24847;&#22270;&#37325;&#22797;&#30340;&#38382;&#39064;&#65292;&#23545;&#35789;&#27133;&#22635;&#20805;&#36827;&#34892;&#20102;&#27867;&#21270;&#12290;&#36890;&#36807;&#32467;&#21512;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892; JSON &#29983;&#25104;&#20219;&#21153;&#22788;&#29702;&#65292;T5 &#27169;&#22411;&#33021;&#22815;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#24847;&#22270;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.11023</link><description>&lt;p&gt;
&#36890;&#29992;&#22810;&#24847;&#22270;&#26465;&#20214;&#19979;&#30340;&#35789;&#27133;&#22635;&#20805;
&lt;/p&gt;
&lt;p&gt;
Generalized Multiple Intent Conditioned Slot Filling. (arXiv:2305.11023v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#29616;&#23454;&#22330;&#26223;&#19979;&#24847;&#22270;&#37325;&#22797;&#30340;&#38382;&#39064;&#65292;&#23545;&#35789;&#27133;&#22635;&#20805;&#36827;&#34892;&#20102;&#27867;&#21270;&#12290;&#36890;&#36807;&#32467;&#21512;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892; JSON &#29983;&#25104;&#20219;&#21153;&#22788;&#29702;&#65292;T5 &#27169;&#22411;&#33021;&#22815;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#24847;&#22270;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21253;&#25324;&#24847;&#22270;&#26816;&#27979;&#65288;&#35782;&#21035;&#29992;&#25143;&#30340;&#30446;&#26631;&#65289;&#21644;&#35789;&#27133;&#22635;&#20805;&#65288;&#25552;&#21462;&#19982;&#36825;&#20123;&#30446;&#26631;&#30456;&#20851;&#30340;&#23454;&#20307;&#65289;&#20219;&#21153;&#12290;&#20197;&#24448;&#30340;&#35789;&#27133;&#22635;&#20805;&#26041;&#27861;&#20551;&#35774;&#27599;&#20010;&#24847;&#22270;&#31867;&#22411;&#22312;&#19968;&#26465;&#28040;&#24687;&#20013;&#21482;&#20986;&#29616;&#19968;&#27425;&#65292;&#20294;&#22312;&#30495;&#23454;&#22330;&#26223;&#19979;&#36825;&#36890;&#24120;&#19981;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#20551;&#35774;&#12290;&#26412;&#25991;&#36890;&#36807;&#21435;&#38500;&#28040;&#24687;&#20013;&#21807;&#19968;&#24847;&#22270;&#30340;&#32422;&#26463;&#65292;&#23545;&#35789;&#27133;&#22635;&#20805;&#36827;&#34892;&#27867;&#21270;&#12290;&#25105;&#20204;&#23558;&#20854;&#35270;&#20026;&#19968;&#20010; JSON &#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#22788;&#29702;&#12290;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512; DBpedia &#21644;&#29616;&#26377;&#30340;&#35789;&#27133;&#22635;&#20805;&#25968;&#25454;&#38598;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026; JSON &#29983;&#25104;&#20219;&#21153;&#26469;&#21019;&#24314;&#19968;&#20010;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992; GPT-3 &#29983;&#25104;&#20102;&#19968;&#20010;&#39046;&#22495;&#20869;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102; T5 &#27169;&#22411;&#65288;&#24102;&#25110;&#19981;&#24102;&#25552;&#31034;&#31034;&#20363;&#65289;&#24182;&#21457;&#29616;&#20004;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#22343;&#33021;&#25552;&#39640;&#24615;&#33021;&#65292;&#32780;&#27169;&#22411;&#33021;&#22815;&#27867;&#21270;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#24847;&#22270;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language understanding includes the tasks of intent detection (identifying a user's objectives) and slot filling (extracting the entities relevant to those objectives). Prior slot filling methods assume that each intent type cannot occur more than once within a message, however this is often not a valid assumption for real-world settings. In this work, we generalize slot filling by removing the constraint of unique intents in a message. We cast this as a JSON generation task and approach it using a language model. We create a pre-training dataset by combining DBpedia and existing slot filling datasets that we convert for JSON generation. We also generate an in-domain dataset using GPT-3. We train T5 models for this task (with and without exemplars in the prompt) and find that both training datasets improve performance, and that the model is able to generalize to intent types not seen during training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;TaxBox&#65292;&#35813;&#26041;&#27861;&#23558;&#20998;&#31867;&#27861;&#27010;&#24565;&#26144;&#23556;&#21040;&#26694;&#23884;&#20837;&#20013;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#27010;&#29575;&#35780;&#20998;&#22120;&#26469;&#36827;&#34892;&#27010;&#24565;&#38468;&#21152;&#21644;&#25554;&#20837;&#65292;&#36991;&#20813;&#20351;&#29992;&#20266;&#21494;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20108;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;TaxBox&#22312;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11004</link><description>&lt;p&gt;
&#36890;&#36807;&#26694;&#23884;&#20837;&#21644;&#27010;&#29575;&#35780;&#20998;&#22120;&#23436;&#25104;&#20998;&#31867;&#27861;
&lt;/p&gt;
&lt;p&gt;
Taxonomy Completion with Probabilistic Scorer via Box Embedding. (arXiv:2305.11004v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;TaxBox&#65292;&#35813;&#26041;&#27861;&#23558;&#20998;&#31867;&#27861;&#27010;&#24565;&#26144;&#23556;&#21040;&#26694;&#23884;&#20837;&#20013;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#27010;&#29575;&#35780;&#20998;&#22120;&#26469;&#36827;&#34892;&#27010;&#24565;&#38468;&#21152;&#21644;&#25554;&#20837;&#65292;&#36991;&#20813;&#20351;&#29992;&#20266;&#21494;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20108;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;TaxBox&#22312;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20998;&#31867;&#27861;&#30340;&#23436;&#21892;&#20219;&#21153;--&#33258;&#21160;&#21033;&#29992;&#26032;&#30340;&#27010;&#24565;&#20016;&#23500;&#29616;&#26377;&#20998;&#31867;&#27861;--&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20852;&#36259;&#12290;&#26089;&#26399;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#22797;&#26434;&#27169;&#22359;&#12289;&#22806;&#37096;&#20449;&#24687;&#21644;&#20266;&#21494;&#26469;&#20016;&#23500;&#34920;&#31034;&#24182;&#32479;&#19968;&#38468;&#21152;&#21644;&#25554;&#20837;&#30340;&#21305;&#37197;&#36807;&#31243;&#12290;&#34429;&#28982;&#23427;&#20204;&#24050;&#32463;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#20123;&#20171;&#32461;&#21487;&#33021;&#20250;&#22312;&#35757;&#32451;&#21644;&#35780;&#20998;&#36807;&#31243;&#20013;&#24102;&#26469;&#22122;&#38899;&#21644;&#19981;&#20844;&#24179;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TaxBox&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#23436;&#25104;&#20998;&#31867;&#27861;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#20998;&#31867;&#27861;&#27010;&#24565;&#26144;&#23556;&#21040;&#26694;&#23884;&#20837;&#20013;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#27010;&#29575;&#35780;&#20998;&#22120;&#26469;&#36827;&#34892;&#27010;&#24565;&#38468;&#21152;&#21644;&#25554;&#20837;&#65292;&#36991;&#20813;&#20351;&#29992;&#20266;&#21494;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TaxBox&#30001;&#19977;&#20010;&#32452;&#20214;&#32452;&#25104;&#65306;&#65288;1&#65289;&#22270;&#32858;&#21512;&#27169;&#22359;&#65292;&#20197;&#21033;&#29992;&#20998;&#31867;&#27861;&#30340;&#32467;&#26500;&#20449;&#24687;&#21644;&#20004;&#20010;&#36731;&#37327;&#32423;&#35299;&#30721;&#22120;&#65292;&#23558;&#29305;&#24449;&#26144;&#23556;&#21040;&#26694;&#23884;&#20837;&#65292;&#24182;&#25429;&#25417;&#27010;&#24565;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65307;&#65288;2&#65289;&#20004;&#20010;&#27010;&#29575;&#35780;&#20998;&#22120;&#65292;&#20998;&#21035;&#23545;&#24212;&#38468;&#21152;&#21644;&#25554;&#20837;&#20219;&#21153;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#26041;&#27861;&#26469;&#20943;&#36731;&#35823;&#23548;&#20449;&#24687;&#30340;&#24433;&#21709;&#65307;&#65288;3&#65289;&#19968;&#31181;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#21644;&#20248;&#21270;&#20004;&#20010;&#35780;&#20998;&#22120;&#30340;&#35757;&#32451;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TaxBox&#22312;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#29366;&#24577;-of-the-art&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Taxonomy completion, a task aimed at automatically enriching an existing taxonomy with new concepts, has gained significant interest in recent years. Previous works have introduced complex modules, external information, and pseudo-leaves to enrich the representation and unify the matching process of attachment and insertion. While they have achieved good performance, these introductions may have brought noise and unfairness during training and scoring. In this paper, we present TaxBox, a novel framework for taxonomy completion that maps taxonomy concepts to box embeddings and employs two probabilistic scorers for concept attachment and insertion, avoiding the need for pseudo-leaves. Specifically, TaxBox consists of three components: (1) a graph aggregation module to leverage the structural information of the taxonomy and two lightweight decoders that map features to box embedding and capture complex relationships between concepts; (2) two probabilistic scorers that correspond to attach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#33258;&#36866;&#24212;&#25628;&#32034;&#24341;&#25806;&#36741;&#21161;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22823;&#35268;&#27169;&#32593;&#32476;&#25968;&#25454;&#19982;LLMs&#34701;&#21512;&#65292;&#20174;&#32780;&#36991;&#20813;&#26080;&#29992;&#25110;&#22024;&#26434;&#30340;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2305.10998</link><description>&lt;p&gt;
&#32593;&#32476;&#21487;&#20197;&#20026;&#25913;&#36827;&#22823;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#26356;&#22810;&#30340;&#21487;&#36873;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
The Web Can Be Your Oyster for Improving Large Language Models. (arXiv:2305.10998v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#33258;&#36866;&#24212;&#25628;&#32034;&#24341;&#25806;&#36741;&#21161;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22823;&#35268;&#27169;&#32593;&#32476;&#25968;&#25454;&#19982;LLMs&#34701;&#21512;&#65292;&#20174;&#32780;&#36991;&#20813;&#26080;&#29992;&#25110;&#22024;&#26434;&#30340;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#32534;&#30721;&#22823;&#37327;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#30693;&#35782;&#22312;&#27169;&#22411;&#35757;&#32451;&#26102;&#24050;&#32463;&#22266;&#21270;&#65292;&#36825;&#20123;&#27169;&#22411;&#21464;&#24471;&#38745;&#24577;&#65292;&#24182;&#19988;&#21463;&#21040;&#20102;&#24403;&#26102;&#35757;&#32451;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;LLMs&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#25628;&#32034;&#24341;&#25806;&#23558;LLMs&#19982;&#28023;&#37327;&#32593;&#32476;&#36827;&#34892;&#34701;&#21512;&#22686;&#24378;&#12290;&#19982;&#20197;&#24448;&#30340;&#22686;&#24378;&#26469;&#28304;&#65288;&#22914;&#32500;&#22522;&#30334;&#31185;&#25968;&#25454;&#36716;&#20648;&#65289;&#19981;&#21516;&#65292;&#32593;&#32476;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#12289;&#26356;&#20840;&#38754;&#19988;&#19981;&#26029;&#26356;&#26032;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;UNIWEB&#30340;&#32593;&#32476;&#22686;&#24378;LLM&#65292;&#23427;&#20197;&#32479;&#19968;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#26684;&#24335;&#22312;16&#20010;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24182;&#19981;&#26159;&#31616;&#21333;&#22320;&#20351;&#29992;&#20174;&#32593;&#32476;&#26816;&#32034;&#21040;&#30340;&#20869;&#23481;&#65292;&#32780;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#25628;&#32034;&#24341;&#25806;&#36741;&#21161;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#33258;&#25105;&#35780;&#20272;LLM&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#65292;&#24182;&#33258;&#36866;&#24212;&#22320;&#30830;&#23450;&#20309;&#26102;&#21442;&#32771;&#32593;&#32476;&#33719;&#21462;&#26356;&#22810;&#25968;&#25454;&#65292;&#20174;&#32780;&#36991;&#20813;&#26080;&#29992;&#25110;&#22024;&#26434;&#30340;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) encode a large amount of world knowledge. However, as such knowledge is frozen at the time of model training, the models become static and limited by the training data at that time. In order to further improve the capacity of LLMs for knowledge-intensive tasks, we consider augmenting LLMs with the large-scale web using search engine. Unlike previous augmentation sources (e.g., Wikipedia data dump), the web provides broader, more comprehensive and constantly updated information. In this paper, we present a web-augmented LLM UNIWEB, which is trained over 16 knowledge-intensive tasks in a unified text-to-text format. Instead of simply using the retrieved contents from web, our approach has made two major improvements. Firstly, we propose an adaptive search engine assisted learning method that can self-evaluate the confidence level of LLM's predictions, and adaptively determine when to refer to the web for more data, which can avoid useless or noisy augmentatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25513;&#30721;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#22797;&#26434;&#24230;&#23545;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#19968;&#21457;&#29616;&#34920;&#26126;&#65292;&#25513;&#30721;&#39044;&#35757;&#32451;&#20219;&#21153;&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#20854;&#22797;&#26434;&#24230;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10992</link><description>&lt;p&gt;
&#25513;&#30721;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#22797;&#26434;&#24230;&#22914;&#20309;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#65311;
&lt;/p&gt;
&lt;p&gt;
How does the task complexity of masked pretraining objectives affect downstream performance?. (arXiv:2305.10992v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25513;&#30721;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#22797;&#26434;&#24230;&#23545;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#19968;&#21457;&#29616;&#34920;&#26126;&#65292;&#25513;&#30721;&#39044;&#35757;&#32451;&#20219;&#21153;&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#20854;&#22797;&#26434;&#24230;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20854;&#20013;&#27169;&#22411;&#38656;&#35201;&#39044;&#27979;&#26367;&#25442;&#19978;&#19979;&#25991;&#20013;&#30340;&#21407;&#22987;token&#30340;&#25513;&#30721;&#12290;&#23613;&#31649;&#26368;&#36817;&#20351;&#29992;&#26356;&#31616;&#21333;&#19988;&#35745;&#31639;&#36739;&#23569;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20363;&#22914;&#39044;&#27979;&#25513;&#30721;&#26631;&#35760;&#30340;&#31532;&#19968;&#20010;&#23383;&#31526;&#65292;&#24050;&#32463;&#34920;&#29616;&#20986;&#19982;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#20294;&#20351;&#29992;&#25513;&#30721;&#26041;&#26696;&#30340;&#20219;&#21153;&#30446;&#21069;&#36824;&#27809;&#26377;&#36229;&#36234;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#12290;&#26412;&#25991;&#20551;&#35774;&#32570;&#20047;&#22797;&#26434;&#24615;&#26159;&#36896;&#25104;&#20854;&#24615;&#33021;&#19979;&#38477;&#30340;&#20851;&#38190;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#26356;&#22797;&#26434;&#30340;&#25513;&#30721;&#20219;&#21153;&#26159;&#21542;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#24182;&#25506;&#31350;&#23427;&#20204;&#30340;&#22797;&#26434;&#24230;&#38656;&#35201;&#36798;&#21040;&#22810;&#23569;&#25165;&#33021;&#19982;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#34920;&#29616;&#30456;&#24403;&#12290;&#25105;&#20204;&#20351;&#29992;GLUE&#12289;SQuAD&#21644;Universal Dependencies&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#20542;&#21521;&#20110;&#23637;&#29616;&#26356;&#22909;&#30340;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#65292;&#33267;&#23569;&#38656;&#35201;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#22797;&#26434;&#24230;&#30340;&#19968;&#21322;&#25165;&#33021;&#19982;&#20854;&#34920;&#29616;&#30456;&#24403;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#20351;&#29992;&#25513;&#30721;&#20219;&#21153;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked language modeling (MLM) is a widely used self-supervised pretraining objective, where a model needs to predict an original token that is replaced with a mask given contexts. Although simpler and computationally efficient pretraining objectives, e.g., predicting the first character of a masked token, have recently shown comparable results to MLM, no objectives with a masking scheme actually outperform it in downstream tasks. Motivated by the assumption that their lack of complexity plays a vital role in the degradation, we validate whether more complex masked objectives can achieve better results and investigate how much complexity they should have to perform comparably to MLM. Our results using GLUE, SQuAD, and Universal Dependencies benchmarks demonstrate that more complicated objectives tend to show better downstream results with at least half of the MLM complexity needed to perform comparably to MLM. Finally, we discuss how we should pretrain a model using a masked objective 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; KgV &#30340; sigmoid &#38376;&#25511;&#26426;&#21046;&#65292;&#36890;&#36807;&#23884;&#20837;&#23618;&#21098;&#26525;&#20943;&#23569;&#20102;&#27169;&#22411;&#30340;&#22823;&#23567;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102; H-SoftPOS &#23618;&#27425;&#23884;&#20837;&#23618;&#36827;&#19968;&#27493;&#25913;&#36827;&#23884;&#20837;&#20197;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#65292;&#20174;&#32780;&#22312; WMT14 &#33521;&#24503;&#39564;&#35777;&#38598;&#19978;&#20351; perplexity &#20943;&#23569;&#20102;&#19977;&#20493;&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2305.10991</link><description>&lt;p&gt;
&#26356;&#23569;&#21363;&#26159;&#26356;&#22909;&#65281;&#19968;&#31181;&#20248;&#21270;&#35821;&#35328;&#32763;&#35793;&#30340;&#36731;&#37327;&#32423;&#26550;&#26500;&#12290;(arXiv:2305.10991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
Less is More! A slim architecture for optimal language translation. (arXiv:2305.10991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10991
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; KgV &#30340; sigmoid &#38376;&#25511;&#26426;&#21046;&#65292;&#36890;&#36807;&#23884;&#20837;&#23618;&#21098;&#26525;&#20943;&#23569;&#20102;&#27169;&#22411;&#30340;&#22823;&#23567;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102; H-SoftPOS &#23618;&#27425;&#23884;&#20837;&#23618;&#36827;&#19968;&#27493;&#25913;&#36827;&#23884;&#20837;&#20197;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#65292;&#20174;&#32780;&#22312; WMT14 &#33521;&#24503;&#39564;&#35777;&#38598;&#19978;&#20351; perplexity &#20943;&#23569;&#20102;&#19977;&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Softmax &#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#39046;&#22495;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#20540;&#24471;&#20851;&#27880;&#30340;&#24320;&#21457;&#65292;&#26500;&#24314;&#22312; Transformer &#26550;&#26500;&#30340;&#25104;&#21151;&#22522;&#30784;&#20043;&#19978;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19981;&#26029;&#22686;&#38271;&#30340;&#22823;&#23567;&#38656;&#35201;&#36234;&#26469;&#36234;&#22810;&#30340;&#35745;&#31639;&#23384;&#20648;&#22120;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; KgV&#65292;&#19968;&#31181; sigmoid &#38376;&#25511;&#26426;&#21046;&#65292;&#19982; softmax &#27880;&#24847;&#21147;&#19968;&#36215;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#21516;&#26102;&#19981;&#22686;&#21152;&#26550;&#26500;&#22823;&#23567;&#12290;&#20026;&#20102;&#20462;&#27491;&#22823;&#23567;&#35201;&#27714;&#65292;&#25105;&#20204;&#21033;&#29992;&#24352;&#37327;&#38142;&#26469;&#35782;&#21035;&#21644;&#20462;&#21098;&#22810;&#20313;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#26679;&#30340;&#22810;&#20313;&#20027;&#35201;&#23384;&#22312;&#20110;&#23884;&#20837;&#23618;&#20013;&#65292;&#32780;&#19981;&#26159;&#36755;&#20986;&#32447;&#24615;&#23618;&#20013;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#23884;&#20837;&#21644;&#26174;&#33879;&#20943;&#23569;&#21442;&#25968;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; H-SoftPOS&#65292;&#19968;&#31181;&#23618;&#27425;&#23884;&#20837;&#23618;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312; WMT14 &#33521;&#24503;&#39564;&#35777;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351; perplexity &#20943;&#23569;&#20102;&#19977;&#20493;&#65292;&#36229;&#36807;&#20102;&#24403;&#21069;&#30340;&#26368;&#26032;&#25104;&#26524;&#65292;&#21516;&#26102;&#20943;&#23569;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
The softmax attention mechanism has emerged as a noteworthy development in the field of Artificial Intelligence research, building on the successes of Transformer-based architectures. However, their ever increasing sizes necessitate ever increasing computational memory, that limits their usage. We propose KgV, a sigmoid gating mechanism that, in conjunction with softmax attention, significantly boosts performance without increasing architecture size. To amend the size requirements, we leverage Tensor Chains to identify and prune the excess parameters. We find that such excess resides primarily within the embedding layer, and not in the output linear layer. To further improve embedding and significantly reduce parameters, we introduce H-SoftPOS, a hierarchical embedding layer which simultaneously enhances performance. Remarkably, on the WMT14 English-German validation set, our approach yields a threefold reduction in perplexity, surpassing the current state-of-the-art, while reducing pa
&lt;/p&gt;</description></item><item><title>Multi-CrossRE&#26159;&#19968;&#20010;&#22810;&#39046;&#22495;&#12289;&#22810;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;26&#31181;&#35821;&#35328;&#22312;&#20869;&#65292;&#25552;&#20379;&#32473;&#20851;&#31995;&#25277;&#21462;&#30740;&#31350;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.10985</link><description>&lt;p&gt;
Multi-CrossRE&#65306;&#38754;&#21521;&#20851;&#31995;&#25277;&#21462;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Multi-CrossRE A Multi-Lingual Multi-Domain Dataset for Relation Extraction. (arXiv:2305.10985v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10985
&lt;/p&gt;
&lt;p&gt;
Multi-CrossRE&#26159;&#19968;&#20010;&#22810;&#39046;&#22495;&#12289;&#22810;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;26&#31181;&#35821;&#35328;&#22312;&#20869;&#65292;&#25552;&#20379;&#32473;&#20851;&#31995;&#25277;&#21462;&#30740;&#31350;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#22810;&#35821;&#35328;&#36164;&#28304;&#65292;&#22823;&#22810;&#25968;&#20851;&#31995;&#25277;&#21462;&#30740;&#31350;&#37117;&#26159;&#22522;&#20110;&#33521;&#35821;&#35821;&#35328;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Multi-CrossRE&#65292;&#23427;&#26159;&#38754;&#21521;&#20851;&#31995;&#25277;&#21462;&#30340;&#26368;&#24191;&#27867;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#33521;&#35821;&#22312;&#20869;&#30340;26&#31181;&#35821;&#35328;&#65292;&#28085;&#30422;&#20102;&#20845;&#20010;&#25991;&#26412;&#22495;&#12290;Multi-CrossRE&#26159;CrossRE&#30340;&#26426;&#22120;&#32763;&#35793;&#29256;&#26412;&#65292;&#20854;&#20013;&#19968;&#37096;&#20998;&#21253;&#21547;&#20102;&#30001;&#26412;&#22320;&#20154;&#26816;&#26597;&#30340;&#19971;&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;200&#22810;&#20010;&#21477;&#23376;&#12290;&#25105;&#20204;&#22312;26&#20010;&#26032;&#25968;&#25454;&#38598;&#19978;&#36816;&#34892;&#20102;&#19968;&#20010;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#22312;26&#20010;&#22238;&#35793;&#21040;&#33521;&#35821;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#8220;&#29702;&#26234;&#26816;&#26597;&#8221;&#12290;&#22238;&#35793;&#25968;&#25454;&#30340;&#32467;&#26524;&#19982;&#21407;&#22987;&#33521;&#35821;CrossRE&#25968;&#25454;&#19968;&#33268;&#65292;&#34920;&#26126;&#32763;&#35793;&#21644;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#36136;&#37327;&#24456;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most research in Relation Extraction (RE) involves the English language, mainly due to the lack of multi-lingual resources. We propose Multi-CrossRE, the broadest multi-lingual dataset for RE, including 26 languages in addition to English, and covering six text domains. Multi-CrossRE is a machine translated version of CrossRE (Bassignana and Plank, 2022), with a sub-portion including more than 200 sentences in seven diverse languages checked by native speakers. We run a baseline model over the 26 new datasets and--as sanity check--over the 26 back-translations to English. Results on the back-translated data are consistent with the ones on the original English CrossRE, indicating high quality of the translation and the resulting dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#33258;&#35757;&#32451;&#26041;&#27861;&#65292;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#30340;&#24615;&#33021;&#65292;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#35777;&#26126;&#36825;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10951</link><description>&lt;p&gt;
&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Making More of Little Data: Improving Low-Resource Automatic Speech Recognition Using Data Augmentation. (arXiv:2305.10951v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#33258;&#35757;&#32451;&#26041;&#27861;&#65292;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#30340;&#24615;&#33021;&#65292;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#35777;&#26126;&#36825;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#30340;&#24615;&#33021;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#22823;&#37327;&#36716;&#24405;&#35821;&#38899;&#30340;&#35821;&#35328;&#32780;&#35328;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#22914;&#23569;&#25968;&#27665;&#26063;&#35821;&#35328;&#12289;&#22320;&#26041;&#35821;&#35328;&#25110;&#26041;&#35328;&#65292;ASR&#24615;&#33021;&#36890;&#24120;&#20173;&#28982;&#36739;&#20302;&#12290;&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26159;&#21542;&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;&#20302;&#36164;&#28304;ASR&#24615;&#33021;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#22235;&#31181;&#35821;&#35328;&#25110;&#35821;&#35328;&#21464;&#20307;&#65288;&#26085;&#32819;&#26364;&#35821;&#31995;&#65306;&#26684;&#32599;&#23425;&#26681;&#35821;&#12289;&#35199;&#24343;&#37324;&#35199;&#20122;&#35821;&#65307;&#39532;&#26469;-&#27874;&#21033;&#23612;&#35199;&#20122;&#35821;&#31995;&#65306;&#36125;&#29791;&#29595;&#35821;&#12289;&#32435;&#33832;&#23572;&#35821;&#65289;&#12290;&#23545;&#20110;&#36825;&#22235;&#31181;&#35821;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#35757;&#32451;&#30340;&#20351;&#29992;&#65292;&#22312;&#35813;&#26041;&#27861;&#20013;&#65292;&#20351;&#29992;&#35757;&#32451;&#38598;&#25968;&#25454;&#35757;&#32451;ASR&#31995;&#32479;&#65292;&#21033;&#29992;&#35813;&#31995;&#32479;&#29983;&#25104;&#30340;&#36716;&#24405;&#25991;&#26412;&#19982;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#32467;&#21512;&#65292;&#26469;&#35757;&#32451;&#26032;&#30340;ASR&#31995;&#32479;&#12290;&#23545;&#20110;&#24050;&#26377;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#65288;TTS&#65289;&#30340;&#26684;&#32599;&#23425;&#26681;&#35821;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20351;&#29992;TTS&#26469;&#29983;&#25104;ASR&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of automatic speech recognition (ASR) systems has advanced substantially in recent years, particularly for languages for which a large amount of transcribed speech is available. Unfortunately, for low-resource languages, such as minority languages, regional languages or dialects, ASR performance generally remains much lower. In this study, we investigate whether data augmentation techniques could help improve low-resource ASR performance, focusing on four typologically diverse minority languages or language variants (West Germanic: Gronings, West-Frisian; Malayo-Polynesian: Besemah, Nasal). For all four languages, we examine the use of self-training, where an ASR system trained with the available human-transcribed data is used to generate transcriptions, which are then combined with the original data to train a new ASR system. For Gronings, for which there was a pre-existing text-to-speech (TTS) system available, we also examined the use of TTS to generate ASR training 
&lt;/p&gt;</description></item><item><title>&#38646;&#26679;&#26412;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#23481;&#26131;&#20986;&#29616;&#8220;&#31163;&#35889;&#38382;&#39064;&#8221;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#31639;&#27861;LAVS&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#35821;&#35328;&#20043;&#38388;&#30340;KL&#20998;&#27495;&#26174;&#33879;&#38477;&#20302;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10930</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#8220;&#31163;&#35889;&#38382;&#39064;&#8221;
&lt;/p&gt;
&lt;p&gt;
On the Off-Target Problem of Zero-Shot Multilingual Neural Machine Translation. (arXiv:2305.10930v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10930
&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#23481;&#26131;&#20986;&#29616;&#8220;&#31163;&#35889;&#38382;&#39064;&#8221;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#31639;&#27861;LAVS&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#35821;&#35328;&#20043;&#38388;&#30340;KL&#20998;&#27495;&#26174;&#33879;&#38477;&#20302;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#20173;&#28982;&#23384;&#22312;&#8220;&#31163;&#35889;&#38382;&#39064;&#8221;&#65292;&#21363;&#23558;&#32763;&#35793;&#36755;&#20986;&#21040;&#38169;&#35823;&#30340;&#35821;&#35328;&#20013;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#38646;&#26679;&#26412;&#32763;&#35793;&#20219;&#21153;&#20013;&#26356;&#21152;&#26126;&#26174;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#24403;&#32534;&#30721;&#30446;&#26631;&#35821;&#35328;&#20449;&#21495;&#26102;&#22833;&#25928;&#65292;&#20250;&#23548;&#33268;&#31163;&#35889;&#38382;&#39064;&#65292;&#24182;&#19988;&#20004;&#31181;&#35821;&#35328;&#35789;&#27719;&#20043;&#38388;&#26356;&#25509;&#36817;&#30340;&#35789;&#27719;&#36317;&#31163;&#65288;&#21363;KL&#20998;&#27495;&#65289;&#19982;&#26356;&#39640;&#30340;&#31163;&#35889;&#29575;&#26377;&#20851;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#21457;&#29616;&#65292;&#20165;&#38548;&#31163;&#35299;&#30721;&#22120;&#20013;&#19981;&#21516;&#35821;&#35328;&#30340;&#35789;&#27719;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#31639;&#27861;Language Aware Vocabulary Sharing (LAVS)&#26469;&#26500;&#24314;&#22810;&#35821;&#35328;&#35789;&#27719;&#34920;&#65292;&#36890;&#36807;&#22686;&#21152;&#35821;&#35328;&#20043;&#38388;&#30340;KL&#20998;&#27495;&#65292;&#22823;&#22823;&#20943;&#36731;&#20102;&#32763;&#35793;&#27169;&#22411;&#30340;&#31163;&#35889;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;11&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#32763;&#35793;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;90&#20010;&#32763;&#35793;&#20219;&#21153;&#65292;&#37319;&#29992;LAVS&#30340;&#31163;&#35889;&#29575;&#38477;&#20302;&#20102;37&#65285;&#33267;90&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
While multilingual neural machine translation has achieved great success, it suffers from the off-target issue, where the translation is in the wrong language. This problem is more pronounced on zero-shot translation tasks. In this work, we find that failing in encoding discriminative target language signal will lead to off-target and a closer lexical distance (i.e., KL-divergence) between two languages' vocabularies is related with a higher off-target rate. We also find that solely isolating the vocab of different languages in the decoder can alleviate the problem. Motivated by the findings, we propose Language Aware Vocabulary Sharing (LAVS), a simple and effective algorithm to construct the multilingual vocabulary, that greatly alleviates the off-target problem of the translation model by increasing the KL-divergence between languages. We conduct experiments on a multilingual machine translation benchmark in 11 languages. Experiments show that the off-target rate for 90 translation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#21382;&#21490;&#25991;&#26412;&#20013;&#26410;&#34987;&#20805;&#20998;&#24320;&#21457;&#30340;&#20107;&#20214;&#25552;&#21462;&#20219;&#21153;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#21363;&#20351;&#25968;&#25454;&#27880;&#37322;&#24456;&#23569;&#20063;&#21487;&#20197;&#33719;&#24471;&#20986;&#20154;&#24847;&#26009;&#30340;&#22909;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#23519;&#21147;&#65292;&#23637;&#31034;&#20102;&#35813;&#26102;&#26399;&#22900;&#24441;&#35805;&#35821;&#30340;&#35821;&#35328;&#20351;&#29992;&#21644;&#27169;&#24335;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35821;&#35328;&#36801;&#31227;&#21487;&#20197;&#26377;&#25928;&#29992;&#20110;&#21382;&#21490;&#25991;&#26412;&#20013;&#30340;&#22810;&#35821;&#35328;&#20107;&#20214;&#25552;&#21462;&#12290;</title><link>http://arxiv.org/abs/2305.10928</link><description>&lt;p&gt;
&#21382;&#21490;&#25253;&#32440;&#24191;&#21578;&#20013;&#30340;&#22810;&#35821;&#35328;&#20107;&#20214;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Multilingual Event Extraction from Historical Newspaper Adverts. (arXiv:2305.10928v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#21382;&#21490;&#25991;&#26412;&#20013;&#26410;&#34987;&#20805;&#20998;&#24320;&#21457;&#30340;&#20107;&#20214;&#25552;&#21462;&#20219;&#21153;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#21363;&#20351;&#25968;&#25454;&#27880;&#37322;&#24456;&#23569;&#20063;&#21487;&#20197;&#33719;&#24471;&#20986;&#20154;&#24847;&#26009;&#30340;&#22909;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#23519;&#21147;&#65292;&#23637;&#31034;&#20102;&#35813;&#26102;&#26399;&#22900;&#24441;&#35805;&#35821;&#30340;&#35821;&#35328;&#20351;&#29992;&#21644;&#27169;&#24335;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35821;&#35328;&#36801;&#31227;&#21487;&#20197;&#26377;&#25928;&#29992;&#20110;&#21382;&#21490;&#25991;&#26412;&#20013;&#30340;&#22810;&#35821;&#35328;&#20107;&#20214;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21487;&#20197;&#24110;&#21161;&#21382;&#21490;&#23398;&#23478;&#20998;&#26512;&#27604;&#25163;&#24037;&#21487;&#34892;&#24471;&#22810;&#30340;&#25991;&#26412;&#26448;&#26009;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#36825;&#31181;&#26041;&#27861;&#26377;&#30528;&#23454;&#38469;&#30340;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#33719;&#21462;&#22823;&#22411;&#27880;&#37322;&#21382;&#21490;&#25991;&#26412;&#25968;&#25454;&#38598;&#38750;&#24120;&#22256;&#38590;&#65292;&#22240;&#20026;&#21482;&#26377;&#19987;&#19994;&#39046;&#22495;&#30340;&#19987;&#23478;&#25165;&#33021;&#21487;&#38752;&#22320;&#27880;&#37322;&#12290;&#20854;&#27425;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#26159;&#22312;&#29616;&#20195;&#35821;&#35328;&#25991;&#26412;&#19978;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#22312;&#24212;&#29992;&#20110;&#21382;&#21490;&#35821;&#26009;&#24211;&#26102;&#25928;&#26524;&#26174;&#33879;&#38477;&#20302;&#12290;&#36825;&#23545;&#36739;&#23569;&#30740;&#31350;&#30340;&#20219;&#21153;&#20197;&#21450;&#38750;&#33521;&#35821;&#35821;&#35328;&#26469;&#35828;&#23588;&#20854;&#26840;&#25163;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#32858;&#28966;&#20110;&#21382;&#21490;&#25991;&#26412;&#39046;&#22495;&#20013;&#26410;&#34987;&#20805;&#20998;&#24320;&#21457;&#30340;&#20107;&#20214;&#25552;&#21462;&#20219;&#21153;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#33521;&#35821;&#12289;&#27861;&#35821;&#21644;&#33655;&#20848;&#35821;&#65292;&#30001;&#26089;&#26399;&#27542;&#27665;&#26102;&#26399;&#30340;&#25253;&#32440;&#24191;&#21578;&#26500;&#25104;&#65292;&#25253;&#36947;&#20102;&#20174;&#22900;&#24441;&#20013;&#33258;&#30001;&#30340;&#34987;&#22900;&#24441;&#20154;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;&#21363;&#20351;&#25968;&#25454;&#27880;&#37322;&#24456;&#23569;&#65292;&#36890;&#36807;&#20174;&#29616;&#20195;&#25968;&#25454;&#38598;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#20986;&#20154;&#24847;&#26009;&#30340;&#22909;&#32467;&#26524;&#65307;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#23519;&#21147;&#65292;&#23637;&#31034;&#20102;&#35813;&#26102;&#26399;&#22900;&#24441;&#35805;&#35821;&#30340;&#35821;&#35328;&#20351;&#29992;&#21644;&#27169;&#24335;&#65307;&#21516;&#26102;&#65292;&#35821;&#35328;&#36801;&#31227;&#21487;&#20197;&#26377;&#25928;&#29992;&#20110;&#21382;&#21490;&#25991;&#26412;&#20013;&#30340;&#22810;&#35821;&#35328;&#20107;&#20214;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
NLP methods can aid historians in analyzing textual materials in greater volumes than manually feasible. Developing such methods poses substantial challenges though. First, acquiring large, annotated historical datasets is difficult, as only domain experts can reliably label them. Second, most available off-the-shelf NLP models are trained on modern language texts, rendering them significantly less effective when applied to historical corpora. This is particularly problematic for less well studied tasks, and for languages other than English. This paper addresses these challenges while focusing on the under-explored task of event extraction from a novel domain of historical texts. We introduce a new multilingual dataset in English, French, and Dutch composed of newspaper ads from the early modern colonial period reporting on enslaved people who liberated themselves from enslavement. We find that: 1) even with scarce annotated data, it is possible to achieve surprisingly good results by 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#20174;Ad-hoc&#21040;&#20132;&#20114;&#24335;&#25628;&#32034;&#20013;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;(QPP)&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;QPP&#26041;&#27861;&#22312;&#20132;&#20114;&#24335;&#25628;&#32034;&#20013;&#26159;&#21542;&#20855;&#26377;&#25512;&#24191;&#24212;&#29992;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.10923</link><description>&lt;p&gt;
&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#65306;&#20174;Ad-hoc&#21040;&#20132;&#20114;&#24335;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Query Performance Prediction: From Ad-hoc to Conversational Search. (arXiv:2305.10923v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#20174;Ad-hoc&#21040;&#20132;&#20114;&#24335;&#25628;&#32034;&#20013;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;(QPP)&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;QPP&#26041;&#27861;&#22312;&#20132;&#20114;&#24335;&#25628;&#32034;&#20013;&#26159;&#21542;&#20855;&#26377;&#25512;&#24191;&#24212;&#29992;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;(QPP)&#26159;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#20219;&#21153;&#12290;QPP&#30340;&#20219;&#21153;&#26159;&#22312;&#27809;&#26377;&#30456;&#20851;&#21028;&#26029;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#26597;&#35810;&#30340;&#26816;&#32034;&#36136;&#37327;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;QPP&#22312;Ad-hoc&#25628;&#32034;&#20013;&#38750;&#24120;&#26377;&#25928;&#21644;&#26377;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;&#35805;&#24335;&#25628;&#32034;(CS)&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637; &#12290;&#26377;&#25928;&#30340;QPP&#33021;&#22815;&#24110;&#21161;CS&#31995;&#32479;&#22312;&#19979;&#19968;&#36718;&#20915;&#23450;&#36866;&#24403;&#30340;&#34892;&#21160;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;CS&#30340;QPP&#30740;&#31350;&#36824;&#24456;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#37325;&#29616;&#21644;&#30740;&#31350;&#29616;&#26377;&#30340;QPP&#26041;&#27861;&#22312;CS&#19978;&#30340;&#26377;&#25928;&#24615;&#26469;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#12290;&#34429;&#28982;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#36890;&#36947;&#26816;&#32034;&#20219;&#21153;&#30456;&#21516;&#65292;&#20294;CS&#20013;&#30340;&#29992;&#25143;&#26597;&#35810;&#21462;&#20915;&#20110;&#23545;&#35805;&#21382;&#21490;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;QPP&#25361;&#25112;&#12290;&#25105;&#20204;&#23588;&#20854;&#26159;&#25506;&#35752;&#20174;Ad-hoc&#25628;&#32034;&#20013;QPP&#26041;&#27861;&#30340;&#30740;&#31350;&#32467;&#26524;&#22312;&#19977;&#20010;CS&#35774;&#32622;&#20013;&#30340;&#25512;&#24191;&#31243;&#24230;:(i) &#35780;&#20272;&#22522;&#20110;&#26597;&#35810;&#37325;&#20889;&#30340;&#26816;&#32034;&#26041;&#27861;&#30340;&#19981;&#21516;&#26597;&#35810;&#30340;&#26816;&#32034;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Query performance prediction (QPP) is a core task in information retrieval. The QPP task is to predict the retrieval quality of a search system for a query without relevance judgments. Research has shown the effectiveness and usefulness of QPP for ad-hoc search. Recent years have witnessed considerable progress in conversational search (CS). Effective QPP could help a CS system to decide an appropriate action to be taken at the next turn. Despite its potential, QPP for CS has been little studied. We address this research gap by reproducing and studying the effectiveness of existing QPP methods in the context of CS. While the task of passage retrieval remains the same in the two settings, a user query in CS depends on the conversational history, introducing novel QPP challenges. In particular, we seek to explore to what extent findings from QPP methods for ad-hoc search generalize to three CS settings: (i) estimating the retrieval quality of different query rewriting-based retrieval met
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#35745;&#31639;&#20195;&#29702;&#38388;&#22914;&#20309;&#23454;&#29616;&#26356;&#22909;&#22320;&#32039;&#24613;&#36890;&#20449;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#24341;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#21487;&#20197;&#24471;&#21040;&#26356;&#20855;&#32452;&#25104;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#32039;&#24613;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2305.10920</link><description>&lt;p&gt;
&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#32039;&#24613;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Emergent Communication with Attention. (arXiv:2305.10920v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10920
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#35745;&#31639;&#20195;&#29702;&#38388;&#22914;&#20309;&#23454;&#29616;&#26356;&#22909;&#22320;&#32039;&#24613;&#36890;&#20449;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#24341;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#21487;&#20197;&#24471;&#21040;&#26356;&#20855;&#32452;&#25104;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#32039;&#24613;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24320;&#21457;&#20986;&#26356;&#22909;&#22320;&#20351;&#29992;&#33258;&#24049;&#30340;&#32039;&#24613;&#35821;&#35328;&#36827;&#34892;&#36890;&#20449;&#30340;&#35745;&#31639;&#20195;&#29702;&#65292;&#25105;&#20204;&#36171;&#20104;&#20195;&#29702;&#22312;&#29615;&#22659;&#20013;&#20851;&#27880;&#29305;&#23450;&#27010;&#24565;&#30340;&#33021;&#21147;&#12290;&#20154;&#31867;&#32463;&#24120;&#23558;&#19968;&#20010;&#23545;&#35937;&#25110;&#22330;&#26223;&#29702;&#35299;&#20026;&#27010;&#24565;&#30340;&#32452;&#21512;&#65292;&#24182;&#23558;&#36825;&#20123;&#27010;&#24565;&#36827;&#19968;&#27493;&#26144;&#23556;&#21040;&#21333;&#35789;&#19978;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#30452;&#35273;&#23454;&#29616;&#20026;"Speaker"&#21644;"Listener"&#20195;&#29702;&#20013;&#30340;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#24341;&#29992;&#28216;&#25103;&#20013;&#26174;&#31034;&#27880;&#24847;&#21147;&#23548;&#33268;&#26356;&#20855;&#32452;&#25104;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#32039;&#24613;&#35821;&#35328;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#35843;&#26597;&#27599;&#20010;&#28040;&#24687;&#31526;&#21495;&#30456;&#20851;&#30340;&#27880;&#24847;&#26435;&#37325;&#20197;&#21450;"Speaker"&#21644;"Listener"&#20195;&#29702;&#20043;&#38388;&#27880;&#24847;&#26435;&#37325;&#30340;&#23545;&#40784;&#65292;&#23637;&#31034;&#20102;&#27880;&#24847;&#21147;&#22914;&#20309;&#24110;&#21161;&#29702;&#35299;&#23398;&#20064;&#21040;&#30340;&#36890;&#20449;&#21327;&#35758;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27880;&#24847;&#21147;&#26159;&#24320;&#21457;&#26356;&#20855;&#20154;&#31867;&#21270;&#30340;&#32039;&#24613;&#35821;&#35328;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
To develop computational agents that better communicate using their own emergent language, we endow the agents with an ability to focus their attention on particular concepts in the environment. Humans often understand an object or scene as a composite of concepts and those concepts are further mapped onto words. We implement this intuition as cross-modal attention mechanisms in Speaker and Listener agents in a referential game and show attention leads to more compositional and interpretable emergent language. We also demonstrate how attention aids in understanding the learned communication protocol by investigating the attention weights associated with each message symbol and the alignment of attention weights between Speaker and Listener agents. Overall, our results suggest that attention is a promising mechanism for developing more human-like emergent language.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#20808;&#39564;&#32454;&#21270;&#30340;&#24369;&#30417;&#30563;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#22270;&#20687;-&#21477;&#23376;&#23545;&#36827;&#34892;&#23398;&#20064;&#65292;&#20854;&#30446;&#26631;&#26159;&#23454;&#29616;&#23454;&#20307;&#34920;&#31034;&#20013;&#30340;&#21306;&#22495;-&#30701;&#35821;&#23545;&#24212;&#20851;&#31995;&#65292;&#36890;&#36807;&#32852;&#21512;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#30340;&#36755;&#20986;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.10913</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#20041;&#20808;&#39564;&#32454;&#21270;&#30340;&#24369;&#30417;&#30563;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Weakly-Supervised Visual-Textual Grounding with Semantic Prior Refinement. (arXiv:2305.10913v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#20808;&#39564;&#32454;&#21270;&#30340;&#24369;&#30417;&#30563;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#22270;&#20687;-&#21477;&#23376;&#23545;&#36827;&#34892;&#23398;&#20064;&#65292;&#20854;&#30446;&#26631;&#26159;&#23454;&#29616;&#23454;&#20307;&#34920;&#31034;&#20013;&#30340;&#21306;&#22495;-&#30701;&#35821;&#23545;&#24212;&#20851;&#31995;&#65292;&#36890;&#36807;&#32852;&#21512;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#30340;&#36755;&#20986;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;&#30340;&#30446;&#26631;&#26159;&#20165;&#21033;&#29992;&#22270;&#20687;-&#21477;&#23376;&#23545;&#23398;&#20064;&#23454;&#20307;&#34920;&#31034;&#20013;&#30340;&#21306;&#22495;-&#30701;&#35821;&#23545;&#24212;&#20851;&#31995;&#12290;&#19982;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#65292;&#20854;&#38590;&#24230;&#26356;&#22823;&#65292;&#22240;&#20026;&#26080;&#27861;&#33719;&#24471;&#36793;&#30028;&#26694;&#21644;&#25991;&#26412;&#30701;&#35821;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#20041;&#20808;&#39564;&#32454;&#21270;&#27169;&#22411;&#65288;SPRM&#65289;&#65292;&#20854;&#39044;&#27979;&#32467;&#26524;&#26159;&#36890;&#36807;&#32452;&#21512;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#30340;&#36755;&#20986;&#24471;&#21040;&#30340;&#12290;&#31532;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22359;&#26088;&#22312;&#36820;&#22238;&#25991;&#26412;&#30701;&#35821;&#21644;&#36793;&#30028;&#26694;&#20043;&#38388;&#30340;&#31895;&#30053;&#23545;&#40784;&#12290;&#31532;&#20108;&#20010;&#35757;&#32451;&#36807;&#30340;&#27169;&#22359;&#30001;&#20004;&#20010;&#23376;&#32452;&#20214;&#32452;&#25104;&#65292;&#29992;&#20110;&#32454;&#21270;&#31895;&#30053;&#30340;&#23545;&#40784;&#20197;&#25552;&#39640;&#26368;&#32456;&#30701;&#35821;-&#36793;&#30028;&#26694;&#23545;&#40784;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#27169;&#22411;&#30340;&#35757;&#32451;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#22270;&#20687;&#21644;&#21477;&#23376;&#20043;&#38388;&#30340;&#22810;&#27169;&#24577;&#30456;&#20284;&#24230;&#65292;&#21516;&#26102;&#20351;&#21516;&#19968;&#21477;&#23376;&#21644;&#19968;&#20010;&#26032;&#30340;&#19981;&#30456;&#20851;&#30340;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#30456;&#20284;&#24230;&#26368;&#23567;&#21270;&#65292;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#35757;&#32451;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using only image-sentence pairs, weakly-supervised visual-textual grounding aims to learn region-phrase correspondences of the respective entity mentions. Compared to the supervised approach, learning is more difficult since bounding boxes and textual phrases correspondences are unavailable. In light of this, we propose the Semantic Prior Refinement Model (SPRM), whose predictions are obtained by combining the output of two main modules. The first untrained module aims to return a rough alignment between textual phrases and bounding boxes. The second trained module is composed of two sub-components that refine the rough alignment to improve the accuracy of the final phrase-bounding box alignments. The model is trained to maximize the multimodal similarity between an image and a sentence, while minimizing the multimodal similarity of the same sentence and a new unrelated image, carefully selected to help the most during training. Our approach shows state-of-the-art results on two popula
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#35748;&#30693;&#29702;&#35770;&#30340;&#35282;&#24230;&#25193;&#23637;&#30446;&#26631;&#23548;&#21521;&#30340;&#33050;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#20102;&#23558;&#23376;&#30446;&#26631;&#32435;&#20837;&#23618;&#27425;&#33050;&#26412;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10907</link><description>&lt;p&gt;
&#22312;&#20013;&#38388;&#20241;&#24687;&#19968;&#19979;&#65306;&#25506;&#32034;&#21521;&#23618;&#27425;&#33050;&#26412;&#29983;&#25104;&#30340;&#23376;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Take a Break in the Middle: Investigating Subgoals towards Hierarchical Script Generation. (arXiv:2305.10907v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#35748;&#30693;&#29702;&#35770;&#30340;&#35282;&#24230;&#25193;&#23637;&#30446;&#26631;&#23548;&#21521;&#30340;&#33050;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#20102;&#23558;&#23376;&#30446;&#26631;&#32435;&#20837;&#23618;&#27425;&#33050;&#26412;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#23548;&#21521;&#30340;&#33050;&#26412;&#29983;&#25104;&#26159;&#29983;&#25104;&#33021;&#22815;&#23454;&#29616;&#32473;&#23450;&#30446;&#26631;&#30340;&#27493;&#39588;&#21015;&#34920;&#30340;&#26032;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20174;&#35748;&#30693;&#29702;&#35770;&#30340;&#35282;&#24230;&#25193;&#23637;&#35813;&#20219;&#21153;&#12290;&#27493;&#39588;&#36890;&#24120;&#26159;&#20998;&#23618;&#32452;&#32455;&#30340;&#65292;&#32780;&#19981;&#26159;&#31616;&#21333;&#30340;&#24179;&#38754;&#32467;&#26500;&#8212;&#8212;&#20154;&#20204;&#24120;&#24120;&#23558;&#22797;&#26434;&#30340;&#20219;&#21153;&#20998;&#35299;&#25104;&#23376;&#30446;&#26631;&#65292;&#27599;&#20010;&#23376;&#30446;&#26631;&#21487;&#20197;&#36827;&#19968;&#27493;&#20998;&#35299;&#20026;&#27493;&#39588;&#12290;&#20026;&#20102;&#24314;&#31435;&#22522;&#20934;&#65292;&#25105;&#20204;&#36129;&#29486;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#22522;&#20934;&#26041;&#27861;&#65292;&#24182;&#35774;&#32622;&#20102;&#35780;&#20272;&#25351;&#26631;&#12290;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#37117;&#39564;&#35777;&#20102;&#25968;&#25454;&#38598;&#30340;&#39640;&#36136;&#37327;&#20197;&#21450;&#23558;&#23376;&#30446;&#26631;&#32435;&#20837;&#23618;&#27425;&#33050;&#26412;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#21644;&#35780;&#20272;&#20102;&#21457;&#29616;&#23376;&#30446;&#26631;&#30340;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#23558;&#30446;&#26631;&#20998;&#35299;&#25104;&#23376;&#30446;&#26631;&#27604;&#20174;&#20998;&#27573;&#27493;&#39588;&#20013;&#24635;&#32467;&#26356;&#20026;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Goal-oriented Script Generation is a new task of generating a list of steps that can fulfill the given goal. In this paper, we propose to extend the task from the perspective of cognitive theory. Instead of a simple flat structure, the steps are typically organized hierarchically - Human often decompose a complex task into subgoals, where each subgoal can be further decomposed into steps. To establish the benchmark, we contribute a new dataset, propose several baseline methods, and set up evaluation metrics. Both automatic and human evaluation verify the high-quality of dataset, as well as the effectiveness of incorporating subgoals into hierarchical script generation. Furthermore, We also design and evaluate the model to discover subgoal, and find that it is a bit more difficult to decompose the goals than summarizing from segmented steps.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20219;&#21153;&#21551;&#21457;&#25552;&#31034;&#23398;&#20064;&#27169;&#22411;&#65288;TEPrompt&#65289;&#65292;&#29992;&#20110;&#38544;&#24335;&#35821;&#31687;&#20851;&#31995;&#35782;&#21035;&#65292;&#20854;&#36890;&#36807;&#34701;&#21512;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#29305;&#24449;&#26469;&#25913;&#21892;IDRR&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10866</link><description>&lt;p&gt;
TEPrompt&#65306;&#20219;&#21153;&#21551;&#21457;&#25552;&#31034;&#23398;&#20064;&#22312;&#38544;&#24335;&#35821;&#31687;&#20851;&#31995;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
TEPrompt: Task Enlightenment Prompt Learning for Implicit Discourse Relation Recognition. (arXiv:2305.10866v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10866
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20219;&#21153;&#21551;&#21457;&#25552;&#31034;&#23398;&#20064;&#27169;&#22411;&#65288;TEPrompt&#65289;&#65292;&#29992;&#20110;&#38544;&#24335;&#35821;&#31687;&#20851;&#31995;&#35782;&#21035;&#65292;&#20854;&#36890;&#36807;&#34701;&#21512;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#29305;&#24449;&#26469;&#25913;&#21892;IDRR&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#35821;&#31687;&#20851;&#31995;&#35782;&#21035;&#65288;IDRR&#65289;&#26088;&#22312;&#23545;&#20004;&#20010;&#21442;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#20998;&#31867;&#65292;&#32780;&#19981;&#20351;&#29992;&#26174;&#24335;&#30340;&#36830;&#35789;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#21551;&#21457;&#25552;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#31216;&#20026;TEPrompt&#65292;&#29992;&#20110;&#34701;&#21512;&#19977;&#20010;&#30456;&#20851;&#20219;&#21153;&#30340;&#29305;&#24449;&#20197;&#36827;&#34892;IDRR&#12290;&#35813;&#27169;&#22411;&#21253;&#21547;&#19977;&#20010;&#20219;&#21153;&#65292;&#21363;&#35821;&#31687;&#20851;&#31995;&#35782;&#21035;&#65288;DRR&#65289;&#12289;&#35821;&#20041;&#24847;&#20041;&#20998;&#31867;&#65288;SSC&#65289;&#21644;&#27880;&#37322;&#36830;&#35789;&#39044;&#27979;&#65288;ACP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit Discourse Relation Recognition (IDRR) aims at classifying the relation sense between two arguments without an explicit connective. Recently, the ConnPrompt~\cite{Wei.X:et.al:2022:COLING} has leveraged the powerful prompt learning for IDRR based on the fusion of multi-prompt decisions from three different yet much similar connective prediction templates. Instead of multi-prompt ensembling, we propose to design auxiliary tasks with enlightened prompt learning for the IDRR task. Although an auxiliary task is not used to directly output final prediction, we argue that during the joint training some of its learned features can be useful to boost the main task. In light of such motivations, we propose a task enlightenment prompt learning model, called TEPrompt, to fuse learned features from three related tasks for IDRR. In particular, the TEPrompt contains three tasks, viz., Discourse Relation Recognition (DRR), Sense Semantics Classification (SSC) and Annotated Connective Predictio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26032;&#26041;&#27861;SAMA&#65292;&#36890;&#36807;&#25552;&#21069;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20219;&#21153;&#20998;&#35299;&#26469;&#35299;&#20915;ASG&#26041;&#27861;&#23384;&#22312;&#30340;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#21644;&#29983;&#25104;&#38750;&#23454;&#38469;&#20219;&#21153;&#22870;&#21169;&#30340;&#23376;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10865</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35821;&#20041;&#23545;&#40784;&#20219;&#21153;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Semantically Aligned Task Decomposition in Multi-Agent Reinforcement Learning. (arXiv:2305.10865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26032;&#26041;&#27861;SAMA&#65292;&#36890;&#36807;&#25552;&#21069;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20219;&#21153;&#20998;&#35299;&#26469;&#35299;&#20915;ASG&#26041;&#27861;&#23384;&#22312;&#30340;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#21644;&#29983;&#25104;&#38750;&#23454;&#38469;&#20219;&#21153;&#22870;&#21169;&#30340;&#23376;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#22411;MARL&#20013;&#30340;&#22870;&#21169;&#31232;&#30095;&#38382;&#39064;&#30528;&#37325;&#20110;&#36866;&#24403;&#30340;&#20449;&#29992;&#20998;&#37197;&#12290;&#33258;&#21160;&#23376;&#30446;&#26631;&#29983;&#25104;&#65288;ASG&#65289;&#26159;&#26368;&#36817;&#20986;&#29616;&#30340;&#19968;&#31181;&#21487;&#34892;&#30340;MARL&#26041;&#27861;&#65292;&#20854;&#28789;&#24863;&#26469;&#33258;&#20110;&#22312;&#20869;&#22312;&#39537;&#21160;&#30340;&#22686;&#24378;&#23398;&#20064;&#20013;&#21033;&#29992;&#23376;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#20174;&#31232;&#30095;&#22870;&#21169;&#20013;&#36827;&#34892;&#22797;&#26434;&#20219;&#21153;&#35268;&#21010;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26080;&#30097;&#38656;&#35201;&#22823;&#37327;&#30340;&#22521;&#35757;&#26679;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;"&#35299;&#32806;"&#20915;&#31574;&#26041;&#27861;&#65292;&#21363;&#22312;MARL&#20013;&#30340;&#35821;&#20041;&#23545;&#40784;&#20219;&#21153;&#20998;&#35299;&#65288;SAMA&#65289;&#65292;&#21463;&#21040;&#35299;&#32806;&#34920;&#31034;&#23398;&#20064;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
The difficulty of appropriately assigning credit is particularly heightened in cooperative MARL with sparse reward, due to the concurrent time and structural scales involved. Automatic subgoal generation (ASG) has recently emerged as a viable MARL approach inspired by utilizing subgoals in intrinsically motivated reinforcement learning. However, end-to-end learning of complex task planning from sparse rewards without prior knowledge, undoubtedly requires massive training samples. Moreover, the diversity-promoting nature of existing ASG methods can lead to the "over-representation" of subgoals, generating numerous spurious subgoals of limited relevance to the actual task reward and thus decreasing the sample efficiency of the algorithm. To address this problem and inspired by the disentangled representation learning, we propose a novel "disentangled" decision-making method, Semantically Aligned task decomposition in MARL (SAMA), that prompts pretrained language models with chain-of-thou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;OpenCorpora&#25968;&#25454;&#38598;&#21644;&#23450;&#21046;&#31639;&#27861;&#22686;&#24378;&#20840;&#25991;&#25628;&#32034;&#35789;&#24418;&#36824;&#21407;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#35789;&#20856;&#23384;&#20648;&#31574;&#30053;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35789;&#24418;&#26816;&#32034;&#30340;&#36895;&#24230;&#21644;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.10848</link><description>&lt;p&gt;
&#21033;&#29992;OpenCorpora&#30340;&#33539;&#24335;&#26816;&#32034;&#25512;&#36827;&#20840;&#25991;&#25628;&#32034;&#35789;&#24418;&#36824;&#21407;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Advancing Full-Text Search Lemmatization Techniques with Paradigm Retrieval from OpenCorpora. (arXiv:2305.10848v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;OpenCorpora&#25968;&#25454;&#38598;&#21644;&#23450;&#21046;&#31639;&#27861;&#22686;&#24378;&#20840;&#25991;&#25628;&#32034;&#35789;&#24418;&#36824;&#21407;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#35789;&#20856;&#23384;&#20648;&#31574;&#30053;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35789;&#24418;&#26816;&#32034;&#30340;&#36895;&#24230;&#21644;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31361;&#30772;&#24615;&#26041;&#27861;&#65292;&#21033;&#29992;OpenCorpora&#25968;&#25454;&#38598;&#21644;&#23450;&#21046;&#30340;&#33539;&#24335;&#26816;&#32034;&#31639;&#27861;&#26469;&#22686;&#24378;&#20840;&#25991;&#25628;&#32034;&#35789;&#24418;&#36824;&#21407;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#31616;&#21270;&#21333;&#35789;&#30340;&#20027;&#35201;&#24418;&#24335;&#25110;&#35789;&#24418;&#8212;&#8212;&#36825;&#26159;&#20840;&#25991;&#25628;&#32034;&#20013;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#35789;&#20856;&#23384;&#20648;&#31574;&#30053;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35789;&#24418;&#26816;&#32034;&#30340;&#36895;&#24230;&#21644;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we unveil a groundbreaking method to amplify full-text search lemmatization, utilizing the OpenCorpora dataset and a bespoke paradigm retrieval algorithm. Our primary aim is to streamline the extraction of a word's primary form or lemma - a crucial factor in full-text search. Additionally, we propose a compact dictionary storage strategy, significantly boosting the speed and precision of lemma retrieval.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#35821;&#26469;&#26377;&#25928;&#35268;&#36991;&#29616;&#26377;&#30340;&#25991;&#26412;&#26816;&#27979;&#31995;&#32479;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10847</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#24341;&#23548;&#26469;&#35268;&#36991;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Large Language Models can be Guided to Evade AI-Generated Text Detection. (arXiv:2305.10847v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#35821;&#26469;&#26377;&#25928;&#35268;&#36991;&#29616;&#26377;&#30340;&#25991;&#26412;&#26816;&#27979;&#31995;&#32479;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21253;&#25324;&#35770;&#25991;&#20889;&#20316;&#21644;&#38382;&#31572;&#31561;&#22810;&#20010;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#24517;&#39035;&#35299;&#20915;&#36825;&#20123;&#27169;&#22411;&#28508;&#22312;&#30340;&#35823;&#29992;&#38382;&#39064;&#65292;&#21542;&#21017;&#21487;&#33021;&#23548;&#33268;&#25220;&#34989;&#21644;&#22403;&#22334;&#20449;&#24687;&#31561;&#19981;&#33391;&#21518;&#26524;&#12290;&#26412;&#30740;&#31350;&#25581;&#31034;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#35821;&#65292;LLMs&#21487;&#20197;&#26377;&#25928;&#22320;&#35268;&#36991;&#26816;&#27979;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26367;&#25442;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#20248;&#21270;&#26041;&#27861;&#65288;SICO&#65289;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#36825;&#31181;&#25552;&#31034;&#35821;&#12290;&#22312;&#19977;&#20010;&#29616;&#23454;&#20219;&#21153;&#20013;&#65292;LLMs&#21487;&#33021;&#34987;&#35823;&#29992;&#65292;&#22312;SICO&#30340;&#24110;&#21161;&#19979;&#65292;ChatGPT&#25104;&#21151;&#22320;&#35268;&#36991;&#20102;&#20845;&#39033;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#65292;&#24179;&#22343;&#23548;&#33268;0.54&#30340;AUC&#19979;&#38477;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#34920;&#29616;&#29978;&#33267;&#27604;&#38543;&#26426;&#20998;&#31867;&#22120;&#36824;&#35201;&#24046;&#12290;&#36825;&#20123;&#32467;&#26524;&#22362;&#23450;&#22320;&#25581;&#31034;&#20102;&#29616;&#26377;&#26816;&#27979;&#22120;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated exceptional performance in a variety of tasks, including essay writing and question answering. However, it is crucial to address the potential misuse of these models, which can lead to detrimental outcomes such as plagiarism and spamming. Recently, several detectors have been proposed, including fine-tuned classifiers and various statistical methods. In this study, we reveal that with the aid of carefully crafted prompts, LLMs can effectively evade these detection systems. We propose a novel Substitution-based In-Context example Optimization method (SICO) to automatically generate such prompts. On three real-world tasks where LLMs can be misused, SICO successfully enables ChatGPT to evade six existing detectors, causing a significant 0.54 AUC drop on average. Surprisingly, in most cases these detectors perform even worse than random classifiers. These results firmly reveal the vulnerability of existing detectors. Finally, the strong perfor
&lt;/p&gt;</description></item><item><title>TAPIR&#20351;&#29992;&#21452;&#36890;&#36947;&#27169;&#22411;&#23454;&#29616;&#33258;&#36866;&#24212;&#20462;&#35746;&#22686;&#37327;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;ATIS&#25968;&#25454;&#38598;&#19978;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10845</link><description>&lt;p&gt;
TAPIR&#65306;&#20351;&#29992;&#21452;&#36890;&#36947;&#27169;&#22411;&#23398;&#20064;&#33258;&#36866;&#24212;&#20462;&#35746;&#22686;&#37327;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
TAPIR: Learning Adaptive Revision for Incremental Natural Language Understanding with a Two-Pass Model. (arXiv:2305.10845v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10845
&lt;/p&gt;
&lt;p&gt;
TAPIR&#20351;&#29992;&#21452;&#36890;&#36947;&#27169;&#22411;&#23454;&#29616;&#33258;&#36866;&#24212;&#20462;&#35746;&#22686;&#37327;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;ATIS&#25968;&#25454;&#38598;&#19978;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#22686;&#37327;&#24335;&#30340;&#65292;&#36825;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#26469;&#35828;&#26159;&#20010;&#20248;&#21183;&#65292;&#21487;&#20197;&#20026;&#23454;&#26102;&#20132;&#20114;&#24212;&#29992;&#25552;&#20379;&#24555;&#36895;&#21709;&#24212;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22686;&#37327;&#22788;&#29702;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;RNN&#25110;Transformer&#12290;RNN&#36895;&#24230;&#24555;&#20294;&#21333;&#35843;&#65288;&#19981;&#33021;&#32416;&#27491;&#26089;&#26399;&#30340;&#36755;&#20986;&#65292;&#36825;&#22312;&#22686;&#37327;&#22788;&#29702;&#20013;&#24456;&#24517;&#35201;&#65289;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;Transformer&#20351;&#29992;&#25972;&#20010;&#24207;&#21015;&#65292;&#22240;&#27492;&#26412;&#36136;&#19978;&#19981;&#26159;&#22686;&#37327;&#30340;&#12290;&#20026;&#20102;&#33719;&#24471;&#37096;&#20998;&#36755;&#20986;&#24182;&#25552;&#20379;&#20462;&#35746;&#33021;&#21147;&#65292;&#21487;&#20197;&#20351;&#29992;&#37325;&#21551;&#22686;&#37327;&#30028;&#38754;&#37325;&#22797;&#20256;&#36882;&#26356;&#38271;&#30340;&#36755;&#20837;&#21069;&#32512;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#21477;&#23376;&#21464;&#24471;&#36234;&#26469;&#36234;&#38271;&#65292;&#36825;&#31181;&#26041;&#27861;&#21464;&#24471;&#20195;&#20215;&#39640;&#26114;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AdaPtIve&#20462;&#35746;&#30340;&#21452;&#36890;&#36947;&#27169;&#22411;TAPIR&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#33719;&#24471;&#33258;&#36866;&#24212;&#20462;&#35746;&#31574;&#30053;&#30340;&#22686;&#37327;&#30417;&#30563;&#20449;&#21495;&#30340;&#26041;&#27861;&#12290;&#24207;&#21015;&#26631;&#35760;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#22686;&#37327;&#27169;&#22411;&#65292;&#24182;&#22312;ATIS&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is by its very nature incremental in how it is produced and processed. This property can be exploited by NLP systems to produce fast responses, which has been shown to be beneficial for real-time interactive applications. Recent neural network-based approaches for incremental processing mainly use RNNs or Transformers. RNNs are fast but monotonic (cannot correct earlier output, which can be necessary in incremental processing). Transformers, on the other hand, consume whole sequences, and hence are by nature non-incremental. A restart-incremental interface that repeatedly passes longer input prefixes can be used to obtain partial outputs, while providing the ability to revise. However, this method becomes costly as the sentence grows longer. In this work, we propose the Two-pass model for AdaPtIve Revision (TAPIR) and introduce a method to obtain an incremental supervision signal for learning an adaptive revision policy. Experimental results on sequence labelling show that our
&lt;/p&gt;</description></item><item><title>&#27492;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35789;&#27719;&#24863;&#30693;&#30340;&#38750;&#33258;&#22238;&#24402;Transformer&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#38598;&#25104;&#20102;&#35821;&#38899;-&#25991;&#26412;&#20849;&#20139;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#24182;&#33021;&#22815;&#21516;&#26102;&#35757;&#32451;&#35821;&#38899;&#21644;&#25991;&#26412;&#25968;&#25454;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#25317;&#26377;&#39046;&#20808;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.10839</link><description>&lt;p&gt;
&#22522;&#20110;&#35789;&#27719;&#24863;&#30693;&#30340;&#38750;&#33258;&#22238;&#24402;Transformer&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Lexical-aware Non-autoregressive Transformer-based ASR Model. (arXiv:2305.10839v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10839
&lt;/p&gt;
&lt;p&gt;
&#27492;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35789;&#27719;&#24863;&#30693;&#30340;&#38750;&#33258;&#22238;&#24402;Transformer&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#38598;&#25104;&#20102;&#35821;&#38899;-&#25991;&#26412;&#20849;&#20139;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#24182;&#33021;&#22815;&#21516;&#26102;&#35757;&#32451;&#35821;&#38899;&#21644;&#25991;&#26412;&#25968;&#25454;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#25317;&#26377;&#39046;&#20808;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30001;&#20110;&#20854;&#24555;&#36895;&#35299;&#30721;&#36895;&#24230;&#21644;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#24050;&#32463;&#25104;&#20026;ASR&#24314;&#27169;&#30340;&#20027;&#27969;&#12290;&#20026;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#25918;&#26494;&#26465;&#20214;&#29420;&#31435;&#20551;&#35774;&#21644;&#32423;&#32852;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#26159;&#20004;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#38500;&#20102;&#36825;&#20123;&#31574;&#30053;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35789;&#27719;&#24863;&#30693;&#30340;&#38750;&#33258;&#22238;&#24402;Transformer&#65288;LA-NAT&#65289;ASR&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#22768;&#23398;&#32534;&#30721;&#22120;&#12289;&#19968;&#20010;&#35821;&#38899;-&#25991;&#26412;&#20849;&#20139;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#35821;&#38899;-&#25991;&#26412;&#20849;&#20139;&#35299;&#30721;&#22120;&#12290;&#22768;&#23398;&#32534;&#30721;&#22120;&#20687;&#24448;&#24120;&#19968;&#26679;&#29992;&#20110;&#22788;&#29702;&#36755;&#20837;&#35821;&#38899;&#29305;&#24449;&#65292;&#32780;&#35821;&#38899;-&#25991;&#26412;&#20849;&#20139;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#26088;&#22312;&#21516;&#26102;&#35757;&#32451;&#35821;&#38899;&#21644;&#25991;&#26412;&#25968;&#25454;&#12290;&#36825;&#26679;&#20570;&#65292;LA-NAT&#26088;&#22312;&#20351;ASR&#27169;&#22411;&#24863;&#30693;&#35789;&#27719;&#20449;&#24687;&#65292;&#22240;&#27492;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#35821;&#35328;&#30693;&#35782;&#65292;&#39044;&#26399;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;AISHELL-1&#12289;CSJ&#21644;TEDLIUM 2&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;LA-NAT&#27169;&#22411;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-autoregressive automatic speech recognition (ASR) has become a mainstream of ASR modeling because of its fast decoding speed and satisfactory result. To further boost the performance, relaxing the conditional independence assumption and cascading large-scaled pre-trained models are two active research directions. In addition to these strategies, we propose a lexical-aware non-autoregressive Transformer-based (LA-NAT) ASR framework, which consists of an acoustic encoder, a speech-text shared encoder, and a speech-text shared decoder. The acoustic encoder is used to process the input speech features as usual, and the speech-text shared encoder and decoder are designed to train speech and text data simultaneously. By doing so, LA-NAT aims to make the ASR model aware of lexical information, so the resulting model is expected to achieve better results by leveraging the learned linguistic knowledge. A series of experiments are conducted on the AISHELL-1, CSJ, and TEDLIUM 2 datasets. Acco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; Ahead-of-Time &#65288;AoT&#65289;P-Tuning&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22312;&#27599;&#20010;Transformer&#23618;&#20043;&#21069;&#28155;&#21152;&#36755;&#20837;&#30456;&#20851;&#30340;&#20559;&#32622;&#65292;&#23454;&#29616;&#20102;&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#33410;&#32422;&#12290;&#35813;&#26041;&#27861;&#22312;GLUE&#21644;SuperGLUE&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;BitFit&#65292;&#24182;&#21487;&#29992;&#20110;&#22810;&#20219;&#21153;&#25512;&#29702;&#65292;&#32780;&#25512;&#29702;&#24320;&#38144;&#21364;&#24456;&#23567;&#12290;</title><link>http://arxiv.org/abs/2305.10835</link><description>&lt;p&gt;
Ahead-of-Time P-Tuning&#65306;&#19968;&#31181;&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#33410;&#32422;&#30340;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ahead-of-Time P-Tuning. (arXiv:2305.10835v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; Ahead-of-Time &#65288;AoT&#65289;P-Tuning&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22312;&#27599;&#20010;Transformer&#23618;&#20043;&#21069;&#28155;&#21152;&#36755;&#20837;&#30456;&#20851;&#30340;&#20559;&#32622;&#65292;&#23454;&#29616;&#20102;&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#33410;&#32422;&#12290;&#35813;&#26041;&#27861;&#22312;GLUE&#21644;SuperGLUE&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;BitFit&#65292;&#24182;&#21487;&#29992;&#20110;&#22810;&#20219;&#21153;&#25512;&#29702;&#65292;&#32780;&#25512;&#29702;&#24320;&#38144;&#21364;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; Ahead-of-Time &#65288;AoT&#65289;P-Tuning&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27599;&#20010;Transformer&#23618;&#20043;&#21069;&#28155;&#21152;&#36755;&#20837;&#30456;&#20851;&#30340;&#20559;&#32622;&#65292;&#20197;&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;RoBERTa&#21644;DeBERTa&#27169;&#22411;&#22312;GLUE&#21644;SuperGLUE&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;AoT P-Tuning&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20248;&#20110;BitFit&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#25928;&#29575;&#26356;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;AoT P-Tuning&#30340;&#25512;&#29702;&#24320;&#38144;&#65292;&#24182;&#35777;&#26126;&#23427;&#19982;&#24050;&#24314;&#31435;&#30340;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#24341;&#20837;&#30340;&#24320;&#38144;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#39592;&#24178;LM&#36827;&#34892;&#22810;&#20219;&#21153;&#25512;&#29702;&#65292;&#20174;&#32780;&#25104;&#20026;&#23454;&#38469;&#24212;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose Ahead-of-Time (AoT) P-Tuning, a novel parameter-efficient fine-tuning method for pre-trained Language Models (LMs) that adds input-dependent bias before each Transformer layer. We evaluate AoT P-Tuning on GLUE and SuperGLUE benchmarking datasets using RoBERTa and DeBERTa models, showing that it outperforms BitFit and is comparable or better than other baseline methods for efficient fine-tuning. Additionally, we assess the inference overhead of AoT P-Tuning and demonstrate that it introduces negligible overhead compared to established baseline methods. Our method enables multi-task inference with a single backbone LM, making it a practical solution for real-world applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#22363;&#35752;&#35770;&#20102;&#22522;&#20110;AI&#30340;&#25991;&#26412;&#29983;&#25104;&#31995;&#32479;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31995;&#32479;&#23545;&#25968;&#23383;&#33402;&#26415;&#21644;&#30005;&#23376;&#25991;&#23398;&#39046;&#22495;&#24102;&#26469;&#30340;&#24433;&#21709;&#65292;&#21508;&#31181;&#20316;&#21697;&#37117;&#20174;&#25991;&#23398;&#35282;&#24230;&#32771;&#34385;&#65292;&#31361;&#20986;&#20102;&#21019;&#20316;&#36807;&#31243;&#30340;&#20132;&#20114;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10834</link><description>&lt;p&gt;
&#12298;AI&#20889;&#20316;&#65306;&#22270;&#20687;&#29983;&#25104;&#21644;&#25968;&#23383;&#20889;&#20316;&#30340;&#20851;&#31995;&#12299;
&lt;/p&gt;
&lt;p&gt;
AIwriting: Relations Between Image Generation and Digital Writing. (arXiv:2305.10834v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#22363;&#35752;&#35770;&#20102;&#22522;&#20110;AI&#30340;&#25991;&#26412;&#29983;&#25104;&#31995;&#32479;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31995;&#32479;&#23545;&#25968;&#23383;&#33402;&#26415;&#21644;&#30005;&#23376;&#25991;&#23398;&#39046;&#22495;&#24102;&#26469;&#30340;&#24433;&#21709;&#65292;&#21508;&#31181;&#20316;&#21697;&#37117;&#20174;&#25991;&#23398;&#35282;&#24230;&#32771;&#34385;&#65292;&#31361;&#20986;&#20102;&#21019;&#20316;&#36807;&#31243;&#30340;&#20132;&#20114;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2022&#24180;&#65292;&#22522;&#20110;&#21464;&#24418;&#37329;&#21018;&#30340;AI&#25991;&#26412;&#29983;&#25104;&#31995;&#32479;&#65288;&#22914;GPT-3&#65289;&#21644;&#22522;&#20110;AI&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31995;&#32479;&#65288;&#22914;DALL-E 2&#21644;&#31283;&#23450;&#25193;&#25955;&#65289;&#37117;&#21462;&#24471;&#20102;&#25351;&#25968;&#32423;&#30340;&#39134;&#36291;&#65292;&#26080;&#30097;&#27491;&#22312;&#25913;&#21464;&#25968;&#23383;&#33402;&#26415;&#21644;&#30005;&#23376;&#25991;&#23398;&#39046;&#22495;&#12290;&#22312;&#26412;&#35770;&#22363;&#20013;&#65292;&#19968;&#32452;&#30005;&#23376;&#25991;&#23398;&#20316;&#32773;&#21644;&#29702;&#35770;&#23478;&#32771;&#34385;&#36825;&#20123;&#31995;&#32479;&#24102;&#26469;&#30340;&#20154;&#31867;&#21019;&#36896;&#21147;&#26032;&#26426;&#36935;&#65292;&#24182;&#23637;&#31034;&#20102;&#20182;&#20204;&#22312;&#36807;&#21435;&#19968;&#24180;&#20013;&#19987;&#38376;&#38024;&#23545;&#36825;&#20123;&#31995;&#32479;&#21019;&#20316;&#30340;&#20316;&#21697;&#65292;&#36825;&#20123;&#20316;&#21697;&#36890;&#36807;&#36845;&#20195;&#30340;&#23545;&#35805;&#36807;&#31243;&#36716;&#21270;&#20026;&#35270;&#35273;&#34920;&#29616;&#12290;&#36825;&#20123;&#28436;&#31034;&#30340;&#21069;&#25552;&#26159;&#65292;&#36825;&#20123;&#31995;&#32479;&#21644;&#25152;&#29983;&#25104;&#30340;&#20316;&#21697;&#24517;&#39035;&#20174;&#25991;&#23398;&#35282;&#24230;&#32771;&#34385;&#65292;&#22240;&#20026;&#23427;&#20204;&#36215;&#28304;&#20110;&#20154;&#31867;&#20889;&#20316;&#12290;&#20174;&#20010;&#20154;&#20581;&#24247;&#21361;&#26426;&#30340;&#35270;&#35273;&#22238;&#24518;&#24405;&#65292;&#21040;&#20114;&#21160;&#32593;&#39029;&#28459;&#30011;&#65292;&#21040;&#22522;&#20110;&#25277;&#35937;&#35799;&#24847;&#35821;&#35328;&#30340;&#24314;&#31569;&#65292;&#20877;&#21040;&#25919;&#27835;&#33832;&#12290;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
During 2022, both transformer-based AI text generation sys-tems such as GPT-3 and AI text-to-image generation systems such as DALL-E 2 and Stable Diffusion made exponential leaps forward and are unquestionably altering the fields of digital art and electronic literature. In this panel a group of electronic literature authors and theorists consider new oppor-tunities for human creativity presented by these systems and present new works have produced during the past year that specifically address these systems as environments for literary expressions that are translated through iterative interlocutive processes into visual representations. The premise that binds these presentations is that these systems and the works gener-ated must be considered from a literary perspective, as they originate in human writing. In works ranging from a visual memoir of the personal experience of a health crisis, to interac-tive web comics, to architectures based on abstract poetic language, to political sa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Chunk-LEvel Multi-reference Evaluation (CLEME)&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#22810;&#21442;&#32771;&#29615;&#22659;&#19979;&#35780;&#20272;GEC&#31995;&#32479;&#26102;&#23384;&#22312;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#20854;&#36890;&#36807;&#28040;&#38500;&#30001;&#19981;&#19968;&#33268;&#30340;&#32534;&#36753;&#36793;&#30028;&#24341;&#36215;&#30340;&#20559;&#24046;&#21644;&#33258;&#21160;&#30830;&#23450;&#35821;&#27861;&#38169;&#35823;&#30340;&#36793;&#30028;&#26469;&#25552;&#39640;&#20102;GEC&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10819</link><description>&lt;p&gt;
CLEME: &#38024;&#23545;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#30340;&#21435;&#20559;&#32622;&#22810;&#21442;&#32771;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CLEME: Debiasing Multi-reference Evaluation for Grammatical Error Correction. (arXiv:2305.10819v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10819
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Chunk-LEvel Multi-reference Evaluation (CLEME)&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#22810;&#21442;&#32771;&#29615;&#22659;&#19979;&#35780;&#20272;GEC&#31995;&#32479;&#26102;&#23384;&#22312;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#20854;&#36890;&#36807;&#28040;&#38500;&#30001;&#19981;&#19968;&#33268;&#30340;&#32534;&#36753;&#36793;&#30028;&#24341;&#36215;&#30340;&#20559;&#24046;&#21644;&#33258;&#21160;&#30830;&#23450;&#35821;&#27861;&#38169;&#35823;&#30340;&#36793;&#30028;&#26469;&#25552;&#39640;&#20102;GEC&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;Grammatical Error Correction (GEC)&#26159;&#19968;&#39033;&#39640;&#24230;&#20027;&#35266;&#30340;&#20219;&#21153;&#65292;&#22240;&#27492;&#35780;&#20272;&#20854;&#24615;&#33021;&#21464;&#24471;&#22256;&#38590;&#12290;&#35774;&#35745;&#23613;&#21487;&#33021;&#23458;&#35266;&#30340;&#35780;&#20272;&#25351;&#26631;&#23545;&#20110;GEC&#20219;&#21153;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#20027;&#27969;&#35780;&#20272;&#25351;&#26631;&#65292;&#21363;&#22522;&#20110;&#21442;&#32771;&#30340;&#25351;&#26631;&#65292;&#22312;&#25552;&#21462;&#32534;&#36753;&#26102;&#26410;&#32771;&#34385;&#22810;&#20010;&#21442;&#32771;&#30340;&#23384;&#22312;&#65292;&#20174;&#32780;&#24341;&#20837;&#20559;&#35265;&#21040;&#22810;&#21442;&#32771;&#35780;&#20272;&#20013;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Chunk-LEvel Multi-reference Evaluation (CLEME)&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#22810;&#21442;&#32771;&#29615;&#22659;&#20013;&#35780;&#20272;GEC&#31995;&#32479;&#12290;&#39318;&#20808;&#65292;CLEME&#20026;&#28304;&#12289;&#20551;&#35774;&#21644;&#25152;&#26377;&#21442;&#32771;&#24314;&#31435;&#20855;&#26377;&#19968;&#33268;&#36793;&#30028;&#30340;&#22359;&#24207;&#21015;&#65292;&#20174;&#32780;&#28040;&#38500;&#30001;&#19981;&#19968;&#33268;&#30340;&#32534;&#36753;&#36793;&#30028;&#24341;&#36215;&#30340;&#20559;&#24046;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#21457;&#29616;&#23384;&#22312;&#19981;&#21516;&#35821;&#27861;&#38169;&#35823;&#20043;&#38388;&#30340;&#36793;&#30028;&#65292;&#25105;&#20204;&#33258;&#21160;&#30830;&#23450;&#20102;&#35821;&#27861;&#38169;&#35823;&#30340;&#36793;&#30028;&#65292;&#24182;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#35745;&#31639;&#20102;F$_{0.5}$&#24471;&#20998;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;CLEME&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#21435;&#20559;&#32622;&#22810;&#21442;&#32771;&#35780;&#20272;GEC&#31995;&#32479;&#65292;&#24182;&#25552;&#39640;GEC&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is intractable to evaluate the performance of Grammatical Error Correction (GEC) systems since GEC is a highly subjective task. Designing an evaluation metric that is as objective as possible is crucial to the development of GEC task. Previous mainstream evaluation metrics, i.e., reference-based metrics, introduce bias into the multi-reference evaluation because they extract edits without considering the presence of multiple references. To overcome the problem, we propose Chunk-LEvel Multi-reference Evaluation (CLEME) designed to evaluate GEC systems in multi-reference settings. First, CLEME builds chunk sequences with consistent boundaries for the source, the hypothesis and all the references, thus eliminating the bias caused by inconsistent edit boundaries. Then, based on the discovery that there exist boundaries between different grammatical errors, we automatically determine the grammatical error boundaries and compute F$_{0.5}$ scores in a novel way. Our proposed CLEME approach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;CDCD&#26694;&#26550;&#30340;&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65288;DDLM&#65289;&#65292;&#24182;&#36890;&#36807;GLUE&#22522;&#20934;&#27979;&#35797;&#20102;&#20854;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;DDLM&#35757;&#32451;&#21644;&#35780;&#20272;&#27969;&#31243;&#20197;&#21450;&#24050;&#35757;&#32451;&#30340;DDLM&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.10818</link><description>&lt;p&gt;
&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Democratized Diffusion Language Model. (arXiv:2305.10818v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;CDCD&#26694;&#26550;&#30340;&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65288;DDLM&#65289;&#65292;&#24182;&#36890;&#36807;GLUE&#22522;&#20934;&#27979;&#35797;&#20102;&#20854;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;DDLM&#35757;&#32451;&#21644;&#35780;&#20272;&#27969;&#31243;&#20197;&#21450;&#24050;&#35757;&#32451;&#30340;DDLM&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26377;&#28508;&#22312;&#22909;&#22788;&#65292;&#20294;&#30446;&#21069;&#20844;&#24320;&#30340;&#23454;&#29616;&#12289;&#35757;&#32451;&#27169;&#22411;&#25110;&#21487;&#37325;&#29616;&#30340;&#35757;&#32451;&#31243;&#24207;&#24182;&#19981;&#23384;&#22312;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;CDCD&#26694;&#26550;&#30340;&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65288;DDLM&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;C4&#25968;&#25454;&#38598;&#31616;&#21270;&#30340;DDLM&#35757;&#32451;&#27969;&#31243;&#65292;&#24182;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#34892;&#20026;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#36895;&#24230;&#26356;&#24555;&#30340;&#37319;&#26679;&#30340;&#26032;&#22411;&#26089;&#26399;&#36864;&#20986;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#38024;&#23545;&#20351;&#29992;&#24471;&#20998;&#25554;&#20540;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#30001;&#20110;&#27492;&#21069;&#27809;&#26377;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;LM&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#65288;&#20363;&#22914;&#20998;&#31867;&#20219;&#21153;&#65289;&#65292;&#25105;&#20204;&#22312;GLUE&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20197;&#30740;&#31350;DDLM&#30340;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#12290;&#36890;&#36807;&#26412;&#25991;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#20379;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#30340;DDLM&#35757;&#32451;&#21644;&#35780;&#20272;&#27969;&#31243;&#20197;&#21450;&#39044;&#20808;&#35757;&#32451;&#30340;DDLM&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#22312;&#26410;&#26469;&#30340;D&#30456;&#20851;&#30340;&#30740;&#31350;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the potential benefits of Diffusion Models for NLP applications, publicly available implementations, trained models, or reproducible training procedures currently need to be publicly available. We present the Democratized Diffusion Language Model (DDLM), based on the Continuous Diffusion for Categorical Data (CDCD) framework, to address these challenges. We propose a simplified training procedure for DDLM using the C4 dataset and perform an in-depth analysis of the trained model's behavior. Furthermore, we introduce a novel early-exiting strategy for faster sampling with models trained with score interpolation. Since no previous works aimed at solving downstream tasks with pre-trained Diffusion LM (e.g., classification tasks), we experimented with GLUE Benchmark to study the ability of DDLM to transfer knowledge. With this paper, we propose available training and evaluation pipelines to other researchers and pre-trained DDLM models, which could be used in future research with D
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#21644;&#37327;&#21270;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;Whisper&#36827;&#34892;&#21387;&#32553;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#27169;&#22411;&#22823;&#23567;&#32553;&#23567;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10788</link><description>&lt;p&gt;
Whisper-KDQ: &#36890;&#36807;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#21644;&#37327;&#21270;&#23454;&#29616;&#39640;&#25928;ASR&#30340;&#36731;&#22411;Whisper
&lt;/p&gt;
&lt;p&gt;
Whisper-KDQ: A Lightweight Whisper via Guided Knowledge Distillation and Quantization for Efficient ASR. (arXiv:2305.10788v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#21644;&#37327;&#21270;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;Whisper&#36827;&#34892;&#21387;&#32553;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#27169;&#22411;&#22823;&#23567;&#32553;&#23567;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35745;&#31639;&#30828;&#20214;&#36164;&#28304;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#25968;&#25454;&#30340;&#26174;&#33879;&#22686;&#38271;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#35821;&#38899;&#35782;&#21035;&#31561;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20855;&#26377;&#24456;&#39640;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#20351;&#20854;&#38590;&#20197;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#26377;&#25928;&#25191;&#34892;&#12290;&#20026;&#20102;&#21152;&#36895;&#25512;&#29702;&#12289;&#20943;&#23569;&#27169;&#22411;&#22823;&#23567;&#65292;&#24182;&#20445;&#25345;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#21644;&#37327;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;Whisper&#12290;&#23398;&#29983;&#27169;&#22411;&#22522;&#20110;&#37327;&#21270;&#25439;&#22833;&#21644;&#33976;&#39311;&#25439;&#22833;&#36873;&#25321;&#33976;&#39311;&#21644;&#37327;&#21270;&#23618;&#12290;&#25105;&#20204;&#23558;$\text{Whisper}_\text{small}$&#21387;&#32553;&#21040;$\text{Whisper}_\text{base}$&#21644;$\text{Whisper}_\text{tiny}$&#32423;&#21035;&#65292;&#20351;$\text{Whisper}_\text{small}$&#20998;&#21035;&#23567;5.18x/10.48x&#12290;&#27492;&#22806;&#65292;&#19982;&#21407;&#22987;$\text{Whisper}_\text{base}$&#21644;$\text{Whisper}_\text{tiny}$&#30456;&#27604;&#65292;&#36824;&#26377;&#30456;&#23545;&#23383;&#31526;&#38169;&#35823;&#29575;&#38477;&#20302;.
&lt;/p&gt;
&lt;p&gt;
Due to the rapid development of computing hardware resources and the dramatic growth of data, pre-trained models in speech recognition, such as Whisper, have significantly improved the performance of speech recognition tasks. However, these models usually have a high computational overhead, making it difficult to execute effectively on resource-constrained devices. To speed up inference and reduce model size while maintaining performance, we propose a novel guided knowledge distillation and quantization for large pre-trained model Whisper. The student model selects distillation and quantization layers based on quantization loss and distillation loss, respectively. We compressed $\text{Whisper}_\text{small}$ to $\text{Whisper}_\text{base}$ and $\text{Whisper}_\text{tiny}$ levels, making $\text{Whisper}_\text{small}$ 5.18x/10.48x smaller, respectively. Moreover, compared to the original $\text{Whisper}_\text{base}$ and $\text{Whisper}_\text{tiny}$, there is also a relative character erro
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ditto&#30340;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#35299;&#20915;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#21508;&#21521;&#24322;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#20219;&#21153;&#20013;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10786</link><description>&lt;p&gt;
Ditto: &#19968;&#31181;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#30340;&#31616;&#27905;&#26377;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings. (arXiv:2305.10786v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10786
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ditto&#30340;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#35299;&#20915;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#21508;&#21521;&#24322;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#20219;&#21153;&#20013;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#35786;&#26029;&#20102;&#30001;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#20135;&#29983;&#30340;&#21477;&#23376;&#34920;&#31034;&#20013;&#23384;&#22312;&#30340;&#21508;&#21521;&#24322;&#24615;&#38382;&#39064;&#65292;&#27809;&#26377;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;BERT&#20135;&#29983;&#30340;&#21477;&#23376;&#23884;&#20837;&#23384;&#22312;&#20559;&#21521;&#20110;&#38750;&#20449;&#24687;&#24615;&#21333;&#35789;&#30340;&#20559;&#35265;&#65292;&#38480;&#21046;&#20102;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#20559;&#35265;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#8212;&#8212;&#23545;&#35282;&#32447;&#27880;&#24847;&#21147;&#27744;&#21270;&#65288;Ditto&#65289;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#20272;&#35745;&#26435;&#37325;&#21333;&#35789;&#65292;&#24182;&#35745;&#31639;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21333;&#35789;&#34920;&#31034;&#30340;&#21152;&#26435;&#24179;&#22343;&#20540;&#20316;&#20026;&#21477;&#23376;&#23884;&#20837;&#12290;Ditto&#21487;&#20197;&#36731;&#26494;&#22320;&#24212;&#29992;&#20110;&#20219;&#20309;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21518;&#22788;&#29702;&#25805;&#20316;&#12290;&#19982;&#20808;&#21069;&#30340;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#30456;&#27604;&#65292;Ditto&#19981;&#28155;&#21152;&#21442;&#25968;&#20063;&#19981;&#38656;&#35201;&#20219;&#20309;&#23398;&#20064;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;Ditto&#21487;&#20197;&#32531;&#35299;&#21508;&#21521;&#24322;&#24615;&#38382;&#39064;&#24182;&#25552;&#39640;&#22312;STS&#20219;&#21153;&#20013;&#21508;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior studies diagnose the anisotropy problem in sentence representations from pre-trained language models, e.g., BERT, without fine-tuning. Our analysis reveals that the sentence embeddings from BERT suffer from a bias towards uninformative words, limiting the performance in semantic textual similarity (STS) tasks. To address this bias, we propose a simple and efficient unsupervised approach, Diagonal Attention Pooling (Ditto), which weights words with model-based importance estimations and computes the weighted average of word representations from pre-trained models as sentence embeddings. Ditto can be easily applied to any pre-trained language model as a postprocessing operation. Compared to prior sentence embedding approaches, Ditto does not add parameters nor requires any learning. Empirical evaluations demonstrate that our proposed Ditto can alleviate the anisotropy problem and improve various pre-trained models on STS tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CoFactSum&#30340;&#21435;&#20559;&#26694;&#26550;&#65292;&#36890;&#36807;&#21453;&#20107;&#23454;&#20272;&#35745;&#20943;&#36731;&#21407;&#22240;&#65292;&#35299;&#20915;&#20102;&#29983;&#25104;&#25991;&#26412;&#25688;&#35201;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10736</link><description>&lt;p&gt;
&#23545;&#29983;&#25104;&#20107;&#23454;&#19968;&#33268;&#24615;&#25991;&#26412;&#25688;&#35201;&#36827;&#34892;&#21453;&#20107;&#23454;&#20559;&#24046;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Debiasing for Generating Factually Consistent Text Summaries. (arXiv:2305.10736v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CoFactSum&#30340;&#21435;&#20559;&#26694;&#26550;&#65292;&#36890;&#36807;&#21453;&#20107;&#23454;&#20272;&#35745;&#20943;&#36731;&#21407;&#22240;&#65292;&#35299;&#20915;&#20102;&#29983;&#25104;&#25991;&#26412;&#25688;&#35201;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#29983;&#25104;&#27969;&#30021;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#25991;&#26412;&#25688;&#35201;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#36827;&#23637;&#65292;&#20294;&#25152;&#29983;&#25104;&#25688;&#35201;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#38024;&#23545;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#26500;&#24314;&#22240;&#26524;&#22270;&#65292;&#24182;&#30830;&#23450;&#20102;&#36896;&#25104;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#30340;&#20869;&#22312;&#21407;&#22240;&#65292;&#21363;&#35821;&#35328;&#20559;&#35265;&#21644;&#26080;&#20851;&#24615;&#20559;&#35265;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CoFactSum&#30340;&#21435;&#20559;&#26694;&#26550;&#65292;&#36890;&#36807;&#21453;&#20107;&#23454;&#20272;&#35745;&#20943;&#36731;&#36825;&#20123;&#20559;&#24046;&#30340;&#22240;&#26524;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;CoFactSum&#25552;&#20379;&#20102;&#20004;&#31181;&#21453;&#20107;&#23454;&#20272;&#35745;&#31574;&#30053;&#65292;&#21363;&#26126;&#30830;&#21453;&#20107;&#23454;&#36974;&#34109;&#20855;&#26377;&#26126;&#30830;&#30340;&#21160;&#24577;&#36974;&#34109;&#31574;&#30053;&#65292;&#24182;&#37319;&#29992;&#38544;&#24335;&#37492;&#21035;&#20132;&#21449;&#20851;&#27880;&#26426;&#21046;&#30340;&#38544;&#24335;&#21453;&#20107;&#23454;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21435;&#20559;&#24230;&#35843;&#25972;&#26426;&#21046;&#65292;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#21160;&#24577;&#36866;&#24212;&#21435;&#20559;&#31243;&#24230;&#12290;&#23545;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25688;&#35201;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;CoFactSum&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#26032;&#30340;&#25688;&#35201;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite substantial progress in abstractive text summarization to generate fluent and informative texts, the factual inconsistency in the generated summaries remains an important yet challenging problem to be solved. In this paper, we construct causal graphs for abstractive text summarization and identify the intrinsic causes of the factual inconsistency, i.e., the language bias and irrelevancy bias, and further propose a debiasing framework, named CoFactSum, to alleviate the causal effects of these biases by counterfactual estimation. Specifically, the proposed CoFactSum provides two counterfactual estimation strategies, i.e., Explicit Counterfactual Masking with an explicit dynamic masking strategy, and Implicit Counterfactual Training with an implicit discriminative cross-attention mechanism. Meanwhile, we design a Debiasing Degree Adjustment mechanism to dynamically adapt the debiasing degree at each decoding step. Extensive experiments on two widely-used summarization datasets dem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#29983;&#25104;&#21644;&#39044;&#27979;&#20449;&#24687;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#65292;&#20854;&#20013;&#20004;&#20010;&#35821;&#38899;&#22686;&#24378;&#27169;&#22359;&#22312;&#31532;&#19968;&#21644;&#26368;&#21518;&#19968;&#20010;&#25193;&#25955;&#27493;&#39588;&#20013;&#34987;&#34701;&#21512;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25193;&#25955;&#35780;&#20998;&#20272;&#35745;&#21487;&#20197;&#20174;&#39044;&#27979;&#20449;&#24687;&#20013;&#21463;&#30410;&#24182;&#21152;&#24555;&#35299;&#30721;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2305.10734</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#19982;&#32852;&#21512;&#29983;&#25104;&#39044;&#27979;&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Diffusion-Based Speech Enhancement with Joint Generative and Predictive Decoders. (arXiv:2305.10734v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#29983;&#25104;&#21644;&#39044;&#27979;&#20449;&#24687;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#65292;&#20854;&#20013;&#20004;&#20010;&#35821;&#38899;&#22686;&#24378;&#27169;&#22359;&#22312;&#31532;&#19968;&#21644;&#26368;&#21518;&#19968;&#20010;&#25193;&#25955;&#27493;&#39588;&#20013;&#34987;&#34701;&#21512;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25193;&#25955;&#35780;&#20998;&#20272;&#35745;&#21487;&#20197;&#20174;&#39044;&#27979;&#20449;&#24687;&#20013;&#21463;&#30410;&#24182;&#21152;&#24555;&#35299;&#30721;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#20294;&#20854;&#35299;&#30721;&#36807;&#31243;&#38750;&#24120;&#32791;&#26102;&#12290;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#26159;&#20351;&#29992;&#39044;&#27979;&#24615;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#20272;&#35745;&#22686;&#24378;&#29305;&#24449;&#65292;&#28982;&#21518;&#21021;&#22987;&#21270;&#35299;&#30721;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;&#24573;&#30053;&#20102;&#39044;&#27979;&#24615;&#19982;&#25193;&#25955;&#24615;&#35821;&#38899;&#22686;&#24378;&#20043;&#38388;&#30340;&#20114;&#34917;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#36825;&#20004;&#20010;&#35821;&#38899;&#22686;&#24378;&#27169;&#22359;&#30340;&#32479;&#19968;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#32534;&#30721;&#29983;&#25104;&#21644;&#39044;&#27979;&#20449;&#24687;&#65292;&#28982;&#21518;&#24212;&#29992;&#29983;&#25104;&#21644;&#39044;&#27979;&#35299;&#30721;&#22120;&#65292;&#23427;&#20204;&#30340;&#36755;&#20986;&#34987;&#34701;&#21512;&#12290;&#20855;&#20307;&#22320;&#65292;&#20004;&#20010;&#35821;&#38899;&#22686;&#24378;&#27169;&#22359;&#22312;&#31532;&#19968;&#21644;&#26368;&#21518;&#19968;&#20010;&#25193;&#25955;&#27493;&#39588;&#20013;&#34987;&#34701;&#21512;&#65306;&#31532;&#19968;&#20010;&#27493;&#39588;&#34701;&#21512;&#20351;&#29992;&#39044;&#27979;&#24615;&#35821;&#38899;&#22686;&#24378;&#26469;&#21021;&#22987;&#21270;&#25193;&#25955;&#36807;&#31243;&#65292;&#20197;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#65307;&#26368;&#21518;&#19968;&#20010;&#27493;&#39588;&#34701;&#21512;&#23558;&#20004;&#20010;&#20114;&#34917;&#30340;&#35821;&#38899;&#22686;&#24378;&#36755;&#20986;&#32452;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#39640;&#35821;&#38899;&#22686;&#24378;&#24615;&#33021;&#12290;&#22312;Voice-Bank&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25193;&#25955;&#35780;&#20998;&#20272;&#35745;&#21487;&#20197;&#20174;&#39044;&#27979;&#20449;&#24687;&#20013;&#33719;&#30410;&#24182;&#21152;&#24555;&#35299;&#30721;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based speech enhancement (SE) has been investigated recently, but its decoding is very time-consuming. One solution is to initialize the decoding process with the enhanced feature estimated by a predictive SE system. However, this two-stage method ignores the complementarity between predictive and diffusion SE. In this paper, we propose a unified system that integrates these two SE modules. The system encodes both generative and predictive information, and then applies both generative and predictive decoders, whose outputs are fused. Specifically, the two SE modules are fused in the first and final diffusion steps: the first step fusion initializes the diffusion process with the predictive SE for improving the convergence, and the final step fusion combines the two complementary SE outputs to improve the SE performance. Experiments on the Voice-Bank dataset show that the diffusion score estimation can benefit from the predictive information and speed up the decoding.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#20139;&#20102;&#31532;&#19968;&#27425;&#38024;&#23545;&#22312;&#30452;&#25773;&#24179;&#21488;&#19978;&#26816;&#27979;&#35268;&#33539;&#36829;&#32972;&#29616;&#35937;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#65292;&#36890;&#36807;&#20998;&#31867;&#21035;&#26631;&#27880;&#21644;&#29992;&#25143;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#30452;&#25773;&#32842;&#22825;&#35268;&#33539;&#24418;&#25104;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.10731</link><description>&lt;p&gt;
&#22312;&#30452;&#25773;&#32842;&#22825;&#20013;&#20998;&#26512;&#35268;&#33539;&#36829;&#32972;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Analyzing Norm Violations in Live-Stream Chat. (arXiv:2305.10731v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#20139;&#20102;&#31532;&#19968;&#27425;&#38024;&#23545;&#22312;&#30452;&#25773;&#24179;&#21488;&#19978;&#26816;&#27979;&#35268;&#33539;&#36829;&#32972;&#29616;&#35937;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#65292;&#36890;&#36807;&#20998;&#31867;&#21035;&#26631;&#27880;&#21644;&#29992;&#25143;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#30452;&#25773;&#32842;&#22825;&#35268;&#33539;&#24418;&#25104;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27602;&#24615;&#35328;&#35821;&#65292;&#22914;&#20167;&#24680;&#35328;&#35770;&#65292;&#21487;&#33021;&#20250;&#38459;&#27490;&#29992;&#25143;&#21442;&#19982;&#22312;&#32447;&#31038;&#21306;&#21644;&#27969;&#34892;&#24179;&#21488;&#65292;&#24433;&#21709;&#20182;&#20204;&#30340;&#20307;&#39564;&#12290;&#20197;&#21069;&#30340;&#26816;&#27979;&#24037;&#20855;&#20027;&#35201;&#20851;&#27880;&#20110;&#22312;&#32447;&#35770;&#22363;&#21644;&#31038;&#20132;&#23186;&#20307;&#65288;&#22914;Reddit&#21644;Twitter&#65289;&#30340;&#23545;&#35805;&#12290;&#20294;&#26159;&#65292;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#30452;&#25773;&#24179;&#21488;&#65288;&#22914;Twitch&#21644;YouTube Live&#65289;&#20013;&#30340;&#23545;&#35805;&#26102;&#65292;&#30001;&#20110;&#27599;&#20010;&#35780;&#35770;&#20165;&#21487;&#35265;&#19968;&#27573;&#26102;&#38388;&#65292;&#24182;&#19988;&#32570;&#23569;&#19982;&#20854;&#20182;&#35780;&#35770;&#24314;&#31435;&#20851;&#31995;&#30340;&#32447;&#31243;&#32467;&#26500;&#65292;&#22240;&#27492;&#36825;&#20123;&#26041;&#27861;&#30340;&#25928;&#26524;&#36739;&#24046;&#12290;&#26412;&#25991;&#20998;&#20139;&#20102;&#31532;&#19968;&#27425;&#38024;&#23545;&#22312;&#30452;&#25773;&#24179;&#21488;&#19978;&#26816;&#27979;&#35268;&#33539;&#36829;&#32972;&#29616;&#35937;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;Twitch&#19978;&#23545;4,583&#20010;&#21463;&#36807;&#23457;&#26597;&#30340;&#35780;&#35770;&#36827;&#34892;&#20102;&#20998;&#31867;&#21035;&#26631;&#27880;&#65292;&#24182;&#23450;&#20041;&#20102;&#30452;&#25773;&#32842;&#22825;&#20013;&#30340;&#35268;&#33539;&#36829;&#21453;&#31867;&#21035;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#30452;&#25773;&#25968;&#25454;&#19982;&#20854;&#20182;&#35770;&#22363;&#30340;&#20960;&#20010;&#21306;&#21035;&#65292;&#24182;&#35777;&#26126;&#20102;&#24403;&#21069;&#30340;&#27169;&#22411;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#36890;&#36807;&#19968;&#39033;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25991;&#26412;&#21644;&#35821;&#38899;&#20013;&#35268;&#33539;&#36829;&#32972;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#25581;&#31034;&#20102;&#30452;&#25773;&#32842;&#22825;&#35268;&#33539;&#24418;&#25104;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Toxic language, such as hate speech, can deter users from participating in online communities and enjoying popular platforms. Previous approaches to detecting toxic language and norm violations have been primarily concerned with conversations from online forums and social media, such as Reddit and Twitter. These approaches are less effective when applied to conversations on live-streaming platforms, such as Twitch and YouTube Live, as each comment is only visible for a limited time and lacks a thread structure that establishes its relationship with other comments. In this work, we share the first NLP study dedicated to detecting norm violations in conversations on live-streaming platforms. We define norm violation categories in live-stream chats and annotate 4,583 moderated comments from Twitch. We articulate several facets of live-stream data that differ from other forums, and demonstrate that existing models perform poorly in this setting. By conducting a user study, we identify the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;--Prompt&#24179;&#22374;&#24230;&#65292;&#21487;&#20197;&#20248;&#21270;&#35821;&#35328;&#25552;&#31034;&#36873;&#25321;&#65292;&#25552;&#39640;&#27169;&#22411;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#32467;&#21512;&#29616;&#26377;&#24230;&#37327;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.10713</link><description>&lt;p&gt;
&#24179;&#22374;&#24230;&#24863;&#30693;&#30340;Prompt&#36873;&#25321;&#33021;&#25552;&#39640;&#31934;&#24230;&#21644;&#26679;&#26412;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency. (arXiv:2305.10713v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;--Prompt&#24179;&#22374;&#24230;&#65292;&#21487;&#20197;&#20248;&#21270;&#35821;&#35328;&#25552;&#31034;&#36873;&#25321;&#65292;&#25552;&#39640;&#27169;&#22411;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#32467;&#21512;&#29616;&#26377;&#24230;&#37327;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19981;&#26029;&#22686;&#38271;&#65292;&#25552;&#31034;&#24050;&#25104;&#20026;&#35775;&#38382;&#23427;&#20204;&#30340;&#20027;&#35201;&#26041;&#24335;&#12290;&#36825;&#28608;&#21457;&#20102;&#33258;&#21160;&#36873;&#25321;&#26377;&#25928;&#35821;&#35328;&#25552;&#31034;&#31574;&#30053;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;Prompt&#24179;&#22374;&#24230;&#65292;&#19968;&#31181;&#37327;&#21270;&#35821;&#35328;&#25552;&#31034;&#39044;&#26399;&#25928;&#29992;&#30340;&#26032;&#24230;&#37327;&#12290;&#35813;&#24230;&#37327;&#21463;&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#24179;&#22374;&#24230;&#27491;&#21017;&#21270;&#21551;&#21457;&#65292;&#37327;&#21270;&#27169;&#22411;&#23545;&#20854;&#21442;&#25968;&#25200;&#21160;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#35813;&#24230;&#37327;&#30340;&#29702;&#35770;&#22522;&#30784;&#21450;&#20854;&#19982;&#20854;&#20182;Prompt&#36873;&#25321;&#24230;&#37327;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#20840;&#38754;&#20102;&#35299;&#29616;&#26377;&#26041;&#27861;&#12290;&#20174;&#32463;&#39564;&#19978;&#35762;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#23558;Prompt&#24179;&#22374;&#24230;&#19982;&#29616;&#26377;&#24230;&#37327;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;&#22312;6&#20010;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#24230;&#37327;&#20248;&#20110;&#20197;&#21069;&#30340;Prompt&#36873;&#25321;&#24230;&#37327;&#65292;&#24179;&#22343;&#31934;&#24230;&#25552;&#39640;5&#65285;&#65292;Pearson&#30456;&#20851;&#24615;&#25552;&#39640;10&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
With growing capabilities of large language models, prompting them has become the dominant way to access them. This has motivated the development of strategies for automatically selecting effective language prompts. In this paper, we introduce prompt flatness, a new metric to quantify the expected utility of a language prompt. This metric is inspired by flatness regularization in statistical learning that quantifies the robustness of the model towards its parameter perturbations. We provide theoretical foundations for this metric and its relationship with other prompt selection metrics, providing a comprehensive understanding of existing methods. Empirically, we show that combining prompt flatness with existing metrics improves both performance and sample efficiency. Our metric outperforms the previous prompt selection metrics with an average increase of 5% in accuracy and 10% in Pearson correlation across 6 classification benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#28176;&#36827;&#24335;&#23494;&#38598;&#26816;&#32034;&#20174;&#36890;&#29992;&#39046;&#22495;&#30340;&#26080;&#26631;&#31614;&#35821;&#26009;&#24211;&#20013;&#21019;&#24314;&#35757;&#32451;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#65292;&#30456;&#36739;&#20110;&#26368;&#24378;&#30340;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;4.3%&#30340;&#24615;&#33021;&#65292;&#19982;&#20351;&#29992;&#22823;&#22411;NLG&#27169;&#22411;&#30340;&#22522;&#32447;&#30456;&#27604;&#33410;&#30465;&#20102;&#32422;70&#65285;&#30340;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.10703</link><description>&lt;p&gt;
ReGen: &#36890;&#36807;&#28176;&#36827;&#24335;&#23494;&#38598;&#26816;&#32034;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ReGen: Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval. (arXiv:2305.10703v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#28176;&#36827;&#24335;&#23494;&#38598;&#26816;&#32034;&#20174;&#36890;&#29992;&#39046;&#22495;&#30340;&#26080;&#26631;&#31614;&#35821;&#26009;&#24211;&#20013;&#21019;&#24314;&#35757;&#32451;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#65292;&#30456;&#36739;&#20110;&#26368;&#24378;&#30340;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;4.3%&#30340;&#24615;&#33021;&#65292;&#19982;&#20351;&#29992;&#22823;&#22411;NLG&#27169;&#22411;&#30340;&#22522;&#32447;&#30456;&#27604;&#33410;&#30465;&#20102;&#32422;70&#65285;&#30340;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#65292;&#38646;&#26679;&#26412;&#23398;&#20064;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#21463;&#21040;&#20102;&#35768;&#22810;&#20851;&#27880;&#12290;&#19982;&#20197;&#24448;&#20351;&#29992;&#25968;&#21313;&#20159;&#32423;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#26694;&#26550;&#65292;&#20174;&#36890;&#29992;&#39046;&#22495;&#30340;&#26080;&#26631;&#31614;&#35821;&#26009;&#24211;&#20013;&#21019;&#24314;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#23545;&#27604;&#39044;&#35757;&#32451;&#65292;&#20351;&#29992;&#31867;&#21035;&#25551;&#36848;&#24615;&#35805;&#35821;&#23398;&#20064;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#23494;&#38598;&#26816;&#32034;&#22120;&#20197;&#25552;&#21462;&#26368;&#30456;&#20851;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#30340;&#31574;&#30053;&#65292;&#21363;&#23637;&#31034;&#22686;&#24378;&#30340;&#35805;&#35821;&#29983;&#25104;&#21644;&#33258;&#19968;&#33268;&#24615;&#24341;&#23548;&#36807;&#28388;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#38598;&#30340;&#20027;&#39064;&#35206;&#30422;&#29575;&#65292;&#21516;&#26102;&#21024;&#38500;&#22122;&#22768;&#26679;&#26412;&#12290;&#23545;&#20061;&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;REGEN&#30456;&#36739;&#20110;&#26368;&#24378;&#30340;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;4.3%&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#19982;&#20351;&#29992;&#22823;&#22411;NLG&#27169;&#22411;&#30340;&#22522;&#32447;&#30456;&#27604;&#33410;&#30465;&#20102;&#32422;70&#65285;&#30340;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;REGEN&#21487;&#20197;&#33258;&#28982;&#22320;&#19982;&#26368;&#36817;&#25552;&#20986;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of large language models (LLMs), zero-shot learning has attracted much attention for various NLP tasks. Different from prior works that generate training data with billion-scale natural language generation (NLG) models, we propose a retrieval-enhanced framework to create training data from a general-domain unlabeled corpus. To realize this, we first conduct contrastive pretraining to learn an unsupervised dense retriever for extracting the most relevant documents using class-descriptive verbalizers. We then further propose two simple strategies, namely Verbalizer Augmentation with Demonstrations and Self-consistency Guided Filtering to improve the topic coverage of the dataset while removing noisy examples. Experiments on nine datasets demonstrate that REGEN achieves 4.3% gain over the strongest baselines and saves around 70% of the time compared to baselines using large NLG models. Besides, REGEN can be naturally integrated with recently proposed large language mo
&lt;/p&gt;</description></item><item><title>MolXPT&#26159;&#19968;&#20010;&#25991;&#26412;&#21253;&#35013;&#30340;&#32479;&#19968;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;SMILES&#20316;&#20026;&#36755;&#20837;&#65292;&#21487;&#20197;&#25552;&#39640;&#20998;&#23376;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#19988;&#20351;&#24471;&#22522;&#20110;&#38646;shot&#30340;&#20998;&#23376;&#29983;&#25104;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10688</link><description>&lt;p&gt;
MolXPT&#65306;&#20351;&#29992;&#25991;&#26412;&#21253;&#35013;&#20998;&#23376;&#36827;&#34892;&#29983;&#25104;&#24615;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
MolXPT: Wrapping Molecules with Text for Generative Pre-training. (arXiv:2305.10688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10688
&lt;/p&gt;
&lt;p&gt;
MolXPT&#26159;&#19968;&#20010;&#25991;&#26412;&#21253;&#35013;&#30340;&#32479;&#19968;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;SMILES&#20316;&#20026;&#36755;&#20837;&#65292;&#21487;&#20197;&#25552;&#39640;&#20998;&#23376;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#19988;&#20351;&#24471;&#22522;&#20110;&#38646;shot&#30340;&#20998;&#23376;&#29983;&#25104;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#24182;&#19988;&#30456;&#20851;&#25216;&#26415;&#24050;&#32463;&#34987;&#24212;&#29992;&#21040;&#20102;&#20998;&#23376;&#24314;&#27169;&#20013;&#12290;&#32771;&#34385;&#21040;&#25991;&#26412;&#26159;&#31185;&#23398;&#21457;&#29616;&#26368;&#37325;&#35201;&#30340;&#35760;&#24405;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102; MolXPT&#65292;&#19968;&#20010;&#22312; SMILES &#19978;&#39044;&#35757;&#32451;&#30340;&#32479;&#19968;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#20013; SMILES &#34987;&#25991;&#26412;&#21253;&#35013;&#12290;&#31616;&#21333;&#26469;&#35828;&#65292;&#25105;&#20204;&#26816;&#27979;&#27599;&#20010;&#24207;&#21015;&#20013;&#30340;&#20998;&#23376;&#21517;&#31216;&#65292;&#24182;&#23558;&#23427;&#20204;&#26367;&#25442;&#20026;&#30456;&#24212;&#30340; SMILES&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;SMILES &#21487;&#20197;&#21033;&#29992;&#21608;&#22260;&#25991;&#26412;&#30340;&#20449;&#24687;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#20197;&#19978;&#21253;&#35013;&#30340;&#24207;&#21015;&#65292;&#26159;&#30001;&#26469;&#33258; PubMed &#30340;&#25991;&#26412;&#24207;&#21015;&#21644;&#26469;&#33258; PubChem &#30340; SMILES &#24207;&#21015;&#32452;&#25104;&#30340;&#65292;&#23427;&#20204;&#37117;&#34987;&#36755;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MolXPT &#22312; MoleculeNet &#19978;&#30340;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#24378;&#22522;&#20934;&#27169;&#22411;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#22312;&#25991;&#26412;-&#20998;&#23376;&#32763;&#35793;&#20013;&#34920;&#29616;&#19982;&#26368;&#20339;&#27169;&#22411;&#30456;&#24403;&#65292;&#32780;&#20351;&#29992;&#30340;&#21442;&#25968;&#19981;&#21040;&#20854;&#19968;&#21322;&#65292;&#24182;&#19988;&#33021;&#22815;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#38646;-shot&#29983;&#25104;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative pre-trained Transformer (GPT) has demonstrates its great success in natural language processing and related techniques have been adapted into molecular modeling. Considering that text is the most important record for scientific discovery, in this paper, we propose MolXPT, a unified language model of text and molecules pre-trained on SMILES (a sequence representation of molecules) wrapped by text. Briefly, we detect the molecule names in each sequence and replace them to the corresponding SMILES. In this way, the SMILES could leverage the information from surrounding text, and vice versa. The above wrapped sequences, text sequences from PubMed and SMILES sequences from PubChem are all fed into a language model for pre-training. Experimental results demonstrate that MolXPT outperforms strong baselines of molecular property prediction on MoleculeNet, performs comparably to the best model in text-molecule translation while using less than half of its parameters, and enables zero
&lt;/p&gt;</description></item><item><title>RMSSinger&#26159;&#22522;&#20110;&#30495;&#23454;&#20048;&#35889;&#30340;&#27468;&#22768;&#21512;&#25104;&#65292;&#37319;&#29992;&#21333;&#35789;&#32423;&#24314;&#27169;&#36991;&#20813;&#20102;&#36716;&#24405;&#35823;&#24046;&#65292;&#26041;&#20415;&#19988;&#28789;&#27963;&#12290;</title><link>http://arxiv.org/abs/2305.10686</link><description>&lt;p&gt;
RMSSinger&#65306;&#22522;&#20110;&#30495;&#23454;&#20048;&#35889;&#30340;&#27468;&#22768;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
RMSSinger: Realistic-Music-Score based Singing Voice Synthesis. (arXiv:2305.10686v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10686
&lt;/p&gt;
&lt;p&gt;
RMSSinger&#26159;&#22522;&#20110;&#30495;&#23454;&#20048;&#35889;&#30340;&#27468;&#22768;&#21512;&#25104;&#65292;&#37319;&#29992;&#21333;&#35789;&#32423;&#24314;&#27169;&#36991;&#20813;&#20102;&#36716;&#24405;&#35823;&#24046;&#65292;&#26041;&#20415;&#19988;&#28789;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20851;&#27880;&#19968;&#20010;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65306;&#22522;&#20110;&#30495;&#23454;&#20048;&#35889;&#30340;&#27468;&#22768;&#21512;&#25104;&#65288;RMS-SVS&#65289;&#12290;RMS-SVS&#26088;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#27468;&#21809;&#22768;&#38899;&#65292;&#32473;&#23450;&#20855;&#26377;&#19981;&#21516;&#38899;&#31526;&#31867;&#22411;&#65288;&#20248;&#32654;&#38899;&#31526;&#12289;&#36830;&#25509;&#38899;&#12289;&#20241;&#27490;&#31526;&#31561;&#65289;&#30340;&#30495;&#23454;&#20048;&#35889;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#26368;&#36817;&#30340;&#27468;&#22768;&#21512;&#25104;&#65288;SVS&#65289;&#26041;&#27861;&#20165;&#36866;&#29992;&#20110;&#32454;&#31890;&#24230;&#38899;&#20048;&#20048;&#35889;&#65292;&#36825;&#38656;&#35201;&#19968;&#20010;&#22797;&#26434;&#30340;&#25968;&#25454;&#25910;&#38598;&#31649;&#36947;&#65292;&#24182;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#30340;&#25163;&#21160;&#26631;&#27880;&#26469;&#20351;&#38899;&#20048;&#38899;&#31526;&#19982;&#38899;&#32032;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25163;&#21160;&#27880;&#37322;&#30772;&#22351;&#20102;&#38899;&#20048;&#20048;&#35889;&#20013;&#38899;&#31526;&#25345;&#32493;&#26102;&#38388;&#30340;&#35268;&#24459;&#24615;&#65292;&#20351;&#24471;&#32454;&#31890;&#24230;&#38899;&#20048;&#20048;&#35889;&#19981;&#20415;&#20110;&#20316;&#26354;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38590;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RMSSinger&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;RMS-SVS&#26041;&#27861;&#65292;&#23427;&#20197;&#30495;&#23454;&#20048;&#35889;&#20316;&#20026;&#36755;&#20837;&#65292;&#28040;&#38500;&#20102;&#22823;&#37096;&#20998;&#32321;&#29712;&#30340;&#25163;&#21160;&#26631;&#27880;&#24182;&#36991;&#20813;&#20102;&#19978;&#36848;&#19981;&#20415;&#12290;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#20048;&#35889;&#26159;&#22522;&#20110;&#21333;&#35789;&#32780;&#38750;&#38899;&#32032;&#30340;&#65292;&#25105;&#20204;&#22312;RMSSinger&#20013;&#24341;&#20837;&#20102;&#21333;&#35789;&#32423;&#24314;&#27169;&#65292;&#20197;&#36991;&#20813;&#38899;&#32032;&#32423;&#24314;&#27169;&#25152;&#24341;&#36215;&#30340;&#36716;&#24405;&#35823;&#24046;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;RMSSinger&#21487;&#20197;&#29983;&#25104;&#19982;&#20154;&#31867;&#27468;&#21809;&#32773;&#38750;&#24120;&#30456;&#20284;&#30340;&#27468;&#21809;&#22768;&#38899;&#65292;&#24182;&#19988;&#20351;&#29992;&#30495;&#23454;&#20048;&#35889;&#20316;&#20026;&#36755;&#20837;&#20855;&#26377;&#26041;&#20415;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are interested in a challenging task, Realistic-Music-Score based Singing Voice Synthesis (RMS-SVS). RMS-SVS aims to generate high-quality singing voices given realistic music scores with different note types (grace, slur, rest, etc.). Though significant progress has been achieved, recent singing voice synthesis (SVS) methods are limited to fine-grained music scores, which require a complicated data collection pipeline with time-consuming manual annotation to align music notes with phonemes. Furthermore, these manual annotation destroys the regularity of note durations in music scores, making fine-grained music scores inconvenient for composing. To tackle these challenges, we propose RMSSinger, the first RMS-SVS method, which takes realistic music scores as input, eliminating most of the tedious manual annotation and avoiding the aforementioned inconvenience. Note that music scores are based on words rather than phonemes, in RMSSinger, we introduce word-level modeling to avoid the t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;CIF-Aligned&#32622;&#20449;&#24230;&#20272;&#35745;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#38750;&#33258;&#22238;&#24402;E2E ASR&#27169;&#22411;-Paraformer&#30340;&#29305;&#24615;&#65292;&#29983;&#25104;&#31526;&#21495;&#21516;&#27493;&#30340;&#22768;&#23398;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#21487;&#38752;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.10680</link><description>&lt;p&gt;
&#22522;&#20110;&#38750;&#33258;&#22238;&#24402;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#20934;&#30830;&#21487;&#38752;&#32622;&#20449;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Accurate and Reliable Confidence Estimation Based on Non-Autoregressive End-to-End Speech Recognition System. (arXiv:2305.10680v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;CIF-Aligned&#32622;&#20449;&#24230;&#20272;&#35745;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#38750;&#33258;&#22238;&#24402;E2E ASR&#27169;&#22411;-Paraformer&#30340;&#29305;&#24615;&#65292;&#29983;&#25104;&#31526;&#21495;&#21516;&#27493;&#30340;&#22768;&#23398;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#21487;&#38752;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;ASR&#39046;&#22495;&#20013;&#65292;&#20272;&#35745;&#35782;&#21035;&#32467;&#26524;&#30340;&#32622;&#20449;&#24230;&#24471;&#20998;&#26159;&#19968;&#39033;&#32463;&#20856;&#20219;&#21153;&#65292;&#23545;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#21644;&#35757;&#32451;&#31574;&#30053;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#36807;&#21435;&#30340;&#31471;&#21040;&#31471;(E2E)&#32622;&#20449;&#24230;&#20272;&#35745;&#27169;&#22411;(CEM)&#39044;&#27979;&#19982;&#36755;&#20837;&#36716;&#24405;&#25991;&#26412;&#38271;&#24230;&#30456;&#31561;&#30340;&#24471;&#20998;&#24207;&#21015;&#65292;&#23548;&#33268;&#22312;&#21024;&#38500;&#21644;&#25554;&#20837;&#38169;&#35823;&#21457;&#29983;&#26102;&#20272;&#35745;&#19981;&#21487;&#38752;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;CIF&#23545;&#40784;&#32622;&#20449;&#24230;&#20272;&#35745;&#27169;&#22411;(CA-CEM)&#65292;&#21033;&#29992;&#36830;&#32493;&#31215;&#20998;&#21644;&#28779; CIF &#26426;&#21046;&#26469;&#29983;&#25104;&#31526;&#21495;&#21516;&#27493;&#30340;&#22768;&#23398;&#23884;&#20837;&#65292;&#20197;&#35299;&#20915;&#19978;&#36848;&#30340;&#20272;&#35745;&#22833;&#36133;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#20196;&#29260;&#32423;&#21035;&#19978;&#20351;&#29992;AUC&#21644;RMSE&#20197;&#21450;utterance&#32423;&#21035;&#19978;&#30340;&#19968;&#31181;&#25552;&#20986;&#24230;&#37327;ECE-U&#26469;&#34913;&#37327;&#20272;&#35745;&#36136;&#37327;&#12290;CA-CEM&#22312;ECE-U&#19978;&#33719;&#24471;&#20102;24%&#21644;19%&#30340;&#30456;&#23545;&#38477;&#20302;&#29575;&#65292;&#24182;&#19988;&#22312;&#20004;&#20010;&#27979;&#35797;&#38598;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;AUC&#21644;RMSE&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating confidence scores for recognition results is a classic task in ASR field and of vital importance for kinds of downstream tasks and training strategies. Previous end-to-end~(E2E) based confidence estimation models (CEM) predict score sequences of equal length with input transcriptions, leading to unreliable estimation when deletion and insertion errors occur. In this paper we proposed CIF-Aligned confidence estimation model (CA-CEM) to achieve accurate and reliable confidence estimation based on novel non-autoregressive E2E ASR model - Paraformer. CA-CEM utilizes the modeling character of continuous integrate-and-fire (CIF) mechanism to generate token-synchronous acoustic embedding, which solves the estimation failure issue above. We measure the quality of estimation with AUC and RMSE in token level and ECE-U - a proposed metrics in utterance level. CA-CEM gains 24% and 19% relative reduction on ECE-U and also better AUC and RMSE on two test sets. Furthermore, we conduct anal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Brainstorm&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22836;&#33041;&#39118;&#26292;&#27493;&#39588;&#29983;&#25104;&#24182;&#36873;&#25321;&#20851;&#20110;&#38382;&#39064;&#30340;&#19981;&#21516;&#24819;&#27861;&#65292;&#21487;&#26174;&#33879;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#20915;&#31454;&#20105;&#32423;&#21035;&#32534;&#31243;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#22312;CodeContests&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;ChatGPT&#30340;pass@$k$&#25351;&#26631;&#22686;&#21152;&#20102;50&#65285;&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2305.10679</link><description>&lt;p&gt;
&#36229;&#36234;&#32534;&#30721;&#65306;&#22836;&#33041;&#39118;&#26292;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Think Outside the Code: Brainstorming Boosts Large Language Models in Code Generation. (arXiv:2305.10679v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Brainstorm&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22836;&#33041;&#39118;&#26292;&#27493;&#39588;&#29983;&#25104;&#24182;&#36873;&#25321;&#20851;&#20110;&#38382;&#39064;&#30340;&#19981;&#21516;&#24819;&#27861;&#65292;&#21487;&#26174;&#33879;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#20915;&#31454;&#20105;&#32423;&#21035;&#32534;&#31243;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#22312;CodeContests&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;ChatGPT&#30340;pass@$k$&#25351;&#26631;&#22686;&#21152;&#20102;50&#65285;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#29983;&#25104;&#26088;&#22312;&#20174;&#39640;&#32423;&#20219;&#21153;&#35268;&#33539;&#33258;&#21160;&#29983;&#25104;&#28304;&#20195;&#30721;&#65292;&#21487;&#26174;&#30528;&#25552;&#39640;&#36719;&#20214;&#24037;&#31243;&#30340;&#29983;&#20135;&#21147;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26041;&#27861;&#22312;&#31616;&#21333;&#20219;&#21153;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#26356;&#22797;&#26434;&#20219;&#21153;&#30340;&#20195;&#30721;&#65288;&#22914;&#31454;&#20105;&#32423;&#21035;&#30340;&#38382;&#39064;&#65289;&#20173;&#28982;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Brainstorm&#26694;&#26550;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#12290;&#23427;&#21033;&#29992;&#20102;&#19968;&#20010;&#22836;&#33041;&#39118;&#26292;&#27493;&#39588;&#65292;&#29983;&#25104;&#24182;&#36873;&#25321;&#20851;&#20110;&#38382;&#39064;&#30340;&#19981;&#21516;&#24819;&#27861;&#20197;&#20419;&#36827;&#31639;&#27861;&#25512;&#29702;&#65292;&#20854;&#20013;&#36825;&#20123;&#24605;&#32771;&#26159;&#35299;&#20915;&#38382;&#39064;&#30340;&#21487;&#33021;&#34013;&#22270;&#12290;&#25105;&#20204;&#35777;&#26126;Brainstorm&#26174;&#33879;&#22686;&#24378;&#20102;LLMs&#35299;&#20915;&#31454;&#20105;&#32423;&#21035;&#32534;&#31243;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#22312;CodeContests&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;ChatGPT&#30340;pass@$k$&#25351;&#26631;&#22686;&#21152;&#20102;50&#65285;&#20197;&#19978;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;LeetCode&#31454;&#36187;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;o
&lt;/p&gt;
&lt;p&gt;
Code generation aims to automatically generate source code from high-level task specifications, which can significantly increase productivity of software engineering. Recently, approaches based on large language models (LLMs) have shown remarkable code generation abilities on simple tasks. However, generate code for more complex tasks, such as competition-level problems, remains challenging. In this paper, we introduce Brainstorm framework for code generation. It leverages a brainstorming step that generates and selects diverse thoughts on the problem to facilitate algorithmic reasoning, where the thoughts are possible blueprint of solving the problem. We demonstrate that Brainstorm significantly enhances the ability of LLMs to solve competition-level programming problems, resulting in a more than 50% increase in the pass@$k$ metrics for ChatGPT on the CodeContests benchmark, achieving state-of-the-art performance. Furthermore, our experiments conducted on LeetCode contests show that o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21069;&#31471;&#26694;&#26550;&#65292;&#25429;&#25417;&#20102;&#33521;&#25991;&#35821;&#38899;&#21512;&#25104;&#21069;&#31471;&#27169;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#22312;&#25152;&#26377;&#27169;&#22359;&#20013;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10666</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#33521;&#25991;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#21069;&#31471;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
a unified front-end framework for english text-to-speech synthesis. (arXiv:2305.10666v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10666
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21069;&#31471;&#26694;&#26550;&#65292;&#25429;&#25417;&#20102;&#33521;&#25991;&#35821;&#38899;&#21512;&#25104;&#21069;&#31471;&#27169;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#22312;&#25152;&#26377;&#27169;&#22359;&#20013;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21069;&#31471;&#26159;&#33521;&#25991;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#36127;&#36131;&#25552;&#21462;&#35821;&#35328;&#29305;&#24449;&#65292;&#22914;&#38901;&#24459;&#21644;&#38899;&#32032;&#65292;&#36825;&#23545;&#20110;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#21512;&#25104;&#35821;&#38899;&#33267;&#20851;&#37325;&#35201;&#12290;&#33521;&#25991;&#25991;&#26412;&#21040;&#35821;&#38899;&#21069;&#31471;&#36890;&#24120;&#30001;&#25991;&#26412;&#35268;&#33539;&#21270;&#27169;&#22359;&#65288;TN&#65289;&#65292;&#21333;&#35789;&#38901;&#24459;&#30701;&#35821;&#38901;&#24459;&#30701;&#35821;&#27169;&#22359;&#65288;PWPP&#65289;&#21644;&#23383;&#24418;&#21040;&#38899;&#32032;&#27169;&#22359;&#65288;G2P&#65289;&#32452;&#25104;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#33521;&#25991;&#25991;&#26412;&#21040;&#35821;&#38899;&#21069;&#31471;&#30340;&#30740;&#31350;&#20165;&#20851;&#27880;&#20110;&#21333;&#29420;&#27169;&#22359;&#65292;&#24573;&#30053;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#65292;&#23548;&#33268;&#27599;&#20010;&#27169;&#22359;&#24615;&#33021;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21069;&#31471;&#26694;&#26550;&#65292;&#25429;&#25417;&#33521;&#25991;&#25991;&#26412;&#21040;&#35821;&#38899;&#21069;&#31471;&#27169;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#27169;&#22359;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The front-end is a critical component of English text-to-speech (TTS) systems, responsible for extracting linguistic features that are essential for a text-to-speech model to synthesize speech, such as prosodies and phonemes. The English TTS front-end typically consists of a text normalization (TN) module, a prosody word prosody phrase (PWPP) module, and a grapheme-to-phoneme (G2P) module. However, current research on the English TTS front-end focuses solely on individual modules, neglecting the interdependence between them and resulting in sub-optimal performance for each module. Therefore, this paper proposes a unified front-end framework that captures the dependencies among the English TTS front-end modules. Extensive experiments have demonstrated that the proposed method achieves state-of-the-art (SOTA) performance in all modules.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#28145;&#24230;&#27169;&#22359;&#21270;&#30340;&#23436;&#20840;&#26080;&#30417;&#30563;&#35821;&#38899;&#20998;&#31163;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#26377;&#30417;&#30563;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#25490;&#21015;&#38382;&#39064;&#12289;&#35828;&#35805;&#20154;&#25968;&#37327;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#21644;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#20381;&#36182;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10652</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#28145;&#24230;&#27169;&#22359;&#21270;&#30340;&#35821;&#38899;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Speech Separation based on Contrastive Learning and Deep Modularization. (arXiv:2305.10652v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#28145;&#24230;&#27169;&#22359;&#21270;&#30340;&#23436;&#20840;&#26080;&#30417;&#30563;&#35821;&#38899;&#20998;&#31163;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#26377;&#30417;&#30563;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#25490;&#21015;&#38382;&#39064;&#12289;&#35828;&#35805;&#20154;&#25968;&#37327;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#21644;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#20381;&#36182;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#35821;&#38899;&#20998;&#31163;&#30340;&#26368;&#20808;&#36827;&#24037;&#20855;&#20381;&#36182;&#20110;&#26377;&#30417;&#30563;&#23398;&#20064;&#12290;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#24517;&#39035;&#22788;&#29702;&#25490;&#21015;&#38382;&#39064;&#65292;&#23427;&#20204;&#21463;&#21040;&#35757;&#32451;&#21644;&#25512;&#26029;&#20013;&#20351;&#29992;&#30340;&#35828;&#35805;&#32773;&#25968;&#37327;&#19981;&#21305;&#37197;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#23384;&#22312;&#12290;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#37319;&#29992;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#35821;&#38899;&#20998;&#31163;&#25216;&#26415;&#26377;&#25928;&#22320;&#35299;&#20915;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#24314;&#31435;&#24103;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#22312;&#19979;&#28216;&#30340;&#28145;&#24230;&#27169;&#22359;&#21270;&#20219;&#21153;&#20013;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#35821;&#38899;&#20998;&#31163;&#20013;&#65292;&#35828;&#35805;&#20154;&#30340;&#19981;&#21516;&#24103;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#32473;&#23450;&#37027;&#20010;&#35828;&#35805;&#20154;&#30340;&#38544;&#21547;&#26631;&#20934;&#24103;&#30340;&#22686;&#24378;&#29256;&#12290;&#35828;&#35805;&#20154;&#30340;&#24103;&#21253;&#21547;&#36275;&#22815;&#30340;&#38901;&#24459;&#20449;&#24687;&#37325;&#21472;&#65292;&#36825;&#26159;&#35821;&#38899;&#20998;&#31163;&#30340;&#20851;&#38190;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#23398;&#20064;&#32553;&#23567;&#24103;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current monaural state of the art tools for speech separation relies on supervised learning. This means that they must deal with permutation problem, they are impacted by the mismatch on the number of speakers used in training and inference. Moreover, their performance heavily relies on the presence of high-quality labelled data. These problems can be effectively addressed by employing a fully unsupervised technique for speech separation. In this paper, we use contrastive learning to establish the representations of frames then use the learned representations in the downstream deep modularization task. Concretely, we demonstrate experimentally that in speech separation, different frames of a speaker can be viewed as augmentations of a given hidden standard frame of that speaker. The frames of a speaker contain enough prosodic information overlap which is key in speech separation. Based on this, we implement a self-supervised learning to learn to minimize the distance between frames
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ZeroPrompt&#21644;&#23545;&#24212;&#30340;Prompt-and-Refine&#31574;&#30053;&#65292;&#21487;&#20197;&#38477;&#20302;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;TDT&#65292;&#32780;&#19988;&#19981;&#20250;&#25439;&#22833;&#31934;&#24230;&#65292;&#20855;&#26377;&#24037;&#31243;&#20415;&#25463;&#24615;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.10649</link><description>&lt;p&gt;
ZeroPrompt: &#27969;&#24335;&#35821;&#38899;&#32534;&#30721;&#22120;&#26159;&#38646;-shot&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs. (arXiv:2305.10649v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ZeroPrompt&#21644;&#23545;&#24212;&#30340;Prompt-and-Refine&#31574;&#30053;&#65292;&#21487;&#20197;&#38477;&#20302;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;TDT&#65292;&#32780;&#19988;&#19981;&#20250;&#25439;&#22833;&#31934;&#24230;&#65292;&#20855;&#26377;&#24037;&#31243;&#20415;&#25463;&#24615;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ZeroPrompt&#21644;&#23545;&#24212;&#30340;Prompt-and-Refine&#31574;&#30053;&#65292;&#36825;&#26159;&#20004;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#38477;&#20302;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;Token Display Time&#65288;TDT&#65289;&#65292;&#32780;&#19988;&#19981;&#20250;&#36896;&#25104;&#20219;&#20309;&#31934;&#24230;&#25439;&#22833;&#12290;ZeroPrompt&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21521;&#27599;&#20010;&#35821;&#38899;&#22359;&#38468;&#21152;&#38646;&#20869;&#23481;&#65292;&#36825;&#31867;&#20284;&#20110;&#25552;&#31034;&#65292;&#40723;&#21169;&#27169;&#22411;&#22312;&#26410;&#26469;&#26631;&#35760;&#20043;&#21069;&#39044;&#27979;&#23427;&#20204;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#27969;&#24335;&#35821;&#38899;&#32534;&#30721;&#22120;&#33258;&#28982;&#20855;&#26377;Masked Language Model&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ZeroPrompt&#20855;&#26377;&#24265;&#20215;&#30340;&#24037;&#31243;&#25104;&#26412;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#25968;&#25454;&#38598;&#19978;&#30340;&#27969;&#24335;&#35821;&#38899;&#32534;&#30721;&#22120;&#65292;&#32780;&#19981;&#20250;&#25439;&#22833;&#31934;&#24230;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#19982;&#25105;&#20204;&#30340;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;Aishell-1&#21644;Librispeech&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39318;&#20010;&#26631;&#35760;&#26174;&#31034;&#26102;&#38388;&#65288;TDT-F&#65289;&#30340;350~700ms&#30340;&#38477;&#20302;&#20197;&#21450;&#26368;&#21518;&#19968;&#20010;&#26631;&#35760;&#26174;&#31034;&#26102;&#38388;&#65288;TDT-L&#65289;&#30340;100~400ms&#30340;&#38477;&#20302;&#65292;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#19978;&#30340;WER&#31934;&#24230;&#30456;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present ZeroPrompt (Figure 1-(a)) and the corresponding Prompt-and-Refine strategy (Figure 3), two simple but effective \textbf{training-free} methods to decrease the Token Display Time (TDT) of streaming ASR models \textbf{without any accuracy loss}. The core idea of ZeroPrompt is to append zeroed content to each chunk during inference, which acts like a prompt to encourage the model to predict future tokens even before they were spoken. We argue that streaming acoustic encoders naturally have the modeling ability of Masked Language Models and our experiments demonstrate that ZeroPrompt is engineering cheap and can be applied to streaming acoustic encoders on any dataset without any accuracy loss. Specifically, compared with our baseline models, we achieve 350 $\sim$ 700ms reduction on First Token Display Time (TDT-F) and 100 $\sim$ 400ms reduction on Last Token Display Time (TDT-L), with theoretically and experimentally equal WER on both Aishell-1 and Librispeech da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#29983;&#25104;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;BioAug&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;BioAug&#24314;&#31435;&#22312;BART&#19978;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#30340;&#23631;&#34109;&#21644;&#30693;&#35782;&#22686;&#24378;&#36827;&#34892;&#35757;&#32451;&#12290;&#23454;&#39564;&#23637;&#31034;&#20102;BioAug&#22312;5&#20010;&#22522;&#20934;BioNER&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#19988;&#34920;&#29616;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.10647</link><description>&lt;p&gt;
BioAug&#65306;&#22522;&#20110;&#26465;&#20214;&#29983;&#25104;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#29992;&#20110;&#20302;&#36164;&#28304;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
BioAug: Conditional Generation based Data Augmentation for Low-Resource Biomedical NER. (arXiv:2305.10647v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#29983;&#25104;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;BioAug&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;BioAug&#24314;&#31435;&#22312;BART&#19978;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#30340;&#23631;&#34109;&#21644;&#30693;&#35782;&#22686;&#24378;&#36827;&#34892;&#35757;&#32451;&#12290;&#23454;&#39564;&#23637;&#31034;&#20102;BioAug&#22312;5&#20010;&#22522;&#20934;BioNER&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#19988;&#34920;&#29616;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(BioNER)&#26159;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#35782;&#21035;&#21629;&#21517;&#23454;&#20307;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#30001;&#20110;&#27880;&#37322;&#38656;&#35201;&#39640;&#24230;&#19987;&#19994;&#21270;&#21644;&#19987;&#19994;&#30693;&#35782;&#65292;BioNER &#36973;&#21463;&#30528;&#20005;&#37325;&#30340;&#25968;&#25454;&#31232;&#32570;&#21644;&#32570;&#20047;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#22256;&#25200;&#12290;&#23613;&#31649;&#25968;&#25454;&#22686;&#24378;&#22312;&#20302;&#36164;&#28304;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#38754;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#39640;&#25928;&#30340;&#65292;&#20294;&#29616;&#26377;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#19981;&#33021;&#20026;BioNER&#29983;&#25104;&#30495;&#23454;&#19988;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;BioAug&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;BioNER&#12290;BioAug&#24314;&#31435;&#22312;BART&#19978;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#30340;&#23631;&#34109;&#21644;&#30693;&#35782;&#22686;&#24378;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#37325;&#26500;&#20219;&#21153;&#12290;&#22312;&#35757;&#32451;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#26377;&#26465;&#20214;&#30340;&#29983;&#25104;&#24182;&#22312;&#19982;&#35757;&#32451;&#38454;&#27573;&#31867;&#20284;&#30340;&#26377;&#36873;&#25321;&#24615;&#22320;&#25439;&#22351;&#25991;&#26412;&#30340;&#26465;&#20214;&#19979;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#12290;&#25105;&#20204;&#22312;5&#20010;&#22522;&#20934;BioNER&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;BioAug&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;BioAug&#27604;&#25152;&#26377;&#22522;&#32447;&#37117;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical Named Entity Recognition (BioNER) is the fundamental task of identifying named entities from biomedical text. However, BioNER suffers from severe data scarcity and lacks high-quality labeled data due to the highly specialized and expert knowledge required for annotation. Though data augmentation has shown to be highly effective for low-resource NER in general, existing data augmentation techniques fail to produce factual and diverse augmentations for BioNER. In this paper, we present BioAug, a novel data augmentation framework for low-resource BioNER. BioAug, built on BART, is trained to solve a novel text reconstruction task based on selective masking and knowledge augmentation. Post training, we perform conditional generation and generate diverse augmentations conditioning BioAug on selectively corrupted text similar to the training stage. We demonstrate the effectiveness of BioAug on 5 benchmark BioNER datasets and show that BioAug outperforms all our baselines by a signi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#8212;&#8212;&#23558;&#20854;&#19982;&#19990;&#30028;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#26377;&#30446;&#30340;&#30340;&#35268;&#21010;&#21644;&#38543;&#26426;&#25506;&#32034;&#33719;&#24471;&#20016;&#23500;&#30340;&#23454;&#20307;&#32463;&#39564;&#36827;&#34892;&#24494;&#35843;, &#20197;&#25552;&#39640;&#20854;&#22312;&#29289;&#29702;&#29615;&#22659;&#19979;&#30340;&#25512;&#29702;&#21644;&#34892;&#20026;&#33021;&#21147;&#65292;&#24182;&#22312;&#35821;&#35328;&#22522;&#20934;&#19978;&#20445;&#25345;&#25110;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10626</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36935;&#35265;&#19990;&#30028;&#27169;&#22411;&#65306;&#23454;&#20307;&#32463;&#39564;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Language Models Meet World Models: Embodied Experiences Enhance Language Models. (arXiv:2305.10626v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#8212;&#8212;&#23558;&#20854;&#19982;&#19990;&#30028;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#26377;&#30446;&#30340;&#30340;&#35268;&#21010;&#21644;&#38543;&#26426;&#25506;&#32034;&#33719;&#24471;&#20016;&#23500;&#30340;&#23454;&#20307;&#32463;&#39564;&#36827;&#34892;&#24494;&#35843;, &#20197;&#25552;&#39640;&#20854;&#22312;&#29289;&#29702;&#29615;&#22659;&#19979;&#30340;&#25512;&#29702;&#21644;&#34892;&#20026;&#33021;&#21147;&#65292;&#24182;&#22312;&#35821;&#35328;&#22522;&#20934;&#19978;&#20445;&#25345;&#25110;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LMs) &#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#22312;&#22788;&#29702;&#29289;&#29702;&#29615;&#22659;&#19979;&#30340;&#31616;&#21333;&#25512;&#29702;&#21644;&#35268;&#21010;&#38382;&#39064;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#20363;&#22914;&#29702;&#35299;&#29289;&#20307;&#27704;&#24658;&#25110;&#35268;&#21010;&#23478;&#24237;&#27963;&#21160;&#12290;&#36825;&#31181;&#38480;&#21046;&#28304;&#20110; LM &#20165;&#21463;&#20070;&#38754;&#35821;&#35328;&#35757;&#32451;&#65292;&#32570;&#23569;&#24517;&#35201;&#30340;&#23454;&#20307;&#30693;&#35782;&#21644;&#25216;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378; LM &#30340;&#26041;&#27861;&#65292;&#21363;&#23558;&#20854;&#19982;&#19990;&#30028;&#27169;&#22411;&#30456;&#32467;&#21512;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#33719;&#24471;&#22810;&#26679;&#21270;&#30340;&#23454;&#20307;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#19968;&#33324;&#35821;&#35328;&#33021;&#21147;&#12290;&#26412;&#26041;&#27861;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#37096;&#32626;&#19968;&#20010;&#34701;&#20837;&#23454;&#20307;&#32463;&#39564;&#30340;&#20195;&#29702;&#65292;&#29305;&#21035;&#26159;&#19968;&#20010;&#27169;&#25311;&#29289;&#29702;&#19990;&#30028;&#30340;&#20223;&#30495;&#22120;(VirtualHome)&#65292;&#36890;&#36807;&#26377;&#30446;&#30340;&#30340;&#35268;&#21010;&#21644;&#38543;&#26426;&#25506;&#32034;&#33719;&#24471;&#22810;&#26679;&#21270;&#30340;&#23454;&#20307;&#32463;&#39564;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36825;&#20123;&#32463;&#39564;&#24494;&#35843; LM &#65292;&#20197;&#25945;&#25480;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#21508;&#31181;&#25512;&#29702;&#21644;&#34892;&#20026;&#33021;&#21147;&#65292;&#20363;&#22914;&#35268;&#21010;&#21644;&#23436;&#25104;&#30446;&#26631;&#12289;&#29289;&#20307;&#27704;&#24658;&#21644;&#36319;&#36394;&#31561;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#20197;&#21033;&#29992;&#20854;&#20182;&#27169;&#25311;&#22120;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#25110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#30528;&#25552;&#39640;&#20102; LM &#22312;&#19968;&#31995;&#21015;&#29289;&#29702;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#30041;&#24182;&#32463;&#24120;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;&#35821;&#35328;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LMs) have shown remarkable capabilities across numerous tasks, they often struggle with simple reasoning and planning in physical environments, such as understanding object permanence or planning household activities. The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills. In this paper, we propose a new paradigm of enhancing LMs by finetuning them with world models, to gain diverse embodied knowledge while retaining their general language capabilities. Our approach deploys an embodied agent in a world model, particularly a simulator of the physical world (VirtualHome), and acquires a diverse set of embodied experiences through both goal-oriented planning and random exploration. These experiences are then used to finetune LMs to teach diverse abilities of reasoning and acting in the physical world, e.g., planning and completing goals, object permanence and tracking, etc. Moreover, it is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35206;&#30422;143&#31181;&#35821;&#35328;&#12289;&#29992;&#20110;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#22522;&#20934;&#30340;&#22810;&#35821;&#31181;&#35821;&#38899;&#22522;&#20934; ML-SUPERB&#65292;&#24182;&#21457;&#29616;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#19988;&#22810;&#35821;&#31181;&#27169;&#22411;&#19981;&#24635;&#26159;&#27604;&#21333;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.10615</link><description>&lt;p&gt;
ML-SUPERB: &#22810;&#35821;&#31181;&#35821;&#38899;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ML-SUPERB: Multilingual Speech Universal PERformance Benchmark. (arXiv:2305.10615v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35206;&#30422;143&#31181;&#35821;&#35328;&#12289;&#29992;&#20110;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#22522;&#20934;&#30340;&#22810;&#35821;&#31181;&#35821;&#38899;&#22522;&#20934; ML-SUPERB&#65292;&#24182;&#21457;&#29616;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#19988;&#22810;&#35821;&#31181;&#27169;&#22411;&#19981;&#24635;&#26159;&#27604;&#21333;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#22788;&#29702;Universal PERformance Benchmark (SUPERB)&#26159;&#19968;&#20010;&#29992;&#20110;&#21508;&#31181;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#22522;&#20934;&#30340;&#25490;&#34892;&#27036;&#12290;&#28982;&#32780;&#65292;SUPERB&#22312;&#35780;&#20272;&#20013;&#20027;&#35201;&#32771;&#34385;&#33521;&#35821;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22810;&#35821;&#31181;SUPERB (ML-SUPERB)&#65292;&#35206;&#30422;&#20102;143&#31181;&#35821;&#35328;&#65288;&#20174;&#39640;&#36164;&#28304;&#21040;&#28626;&#21361;&#35821;&#35328;&#65289;&#65292;&#32771;&#34385;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#35821;&#35328;&#35782;&#21035;&#12290;&#19982;SUPERB&#27010;&#24565;&#31867;&#20284;&#65292;ML-SUPERB&#21033;&#29992;&#20923;&#32467;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#27973;&#23618;&#19979;&#28216;&#27169;&#22411;&#30340;&#31616;&#21333;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#35821;&#31181;&#20219;&#21153;&#12290;&#19982;SUPERB&#22522;&#20934;&#31867;&#20284;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#38899;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#19982;FBANK&#29305;&#24449;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22810;&#35821;&#31181;&#27169;&#22411;&#24182;&#19981;&#24635;&#26159;&#27604;&#21333;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#23558;&#21457;&#24067;ML-SUPERB&#20316;&#20026;&#19968;&#20010;&#25361;&#25112;&#65292;&#25552;&#20379;&#32452;&#32455;&#22909;&#30340;&#25968;&#25454;&#38598;&#21644;&#21487;&#37325;&#29616;&#30340;&#35757;&#32451;&#33050;&#26412;&#65292;&#29992;&#20110;&#26410;&#26469;&#30340;&#22810;&#35821;&#31181;&#34920;&#31034;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech processing Universal PERformance Benchmark (SUPERB) is a leaderboard to benchmark the performance of Self-Supervised Learning (SSL) models on various speech processing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high-resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB benchmark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#31350;&#26159;&#21542;&#33021;&#22815;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26102;&#24577;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;&#65292;&#23588;&#20854;&#26159;&#19981;&#38656;&#35201;&#20219;&#20309;&#26174;&#24335;&#27169;&#22359;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#31867;&#39044;&#27979;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#21487;&#20197;&#38544;&#24335;&#26377;&#25928;&#22320;&#32534;&#30721;&#19978;&#19979;&#25991;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.10613</link><description>&lt;p&gt;
&#19981;&#20381;&#38752;&#20808;&#39564;&#30693;&#35782;&#30340;&#26102;&#24577;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning. (arXiv:2305.10613v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#31350;&#26159;&#21542;&#33021;&#22815;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26102;&#24577;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;&#65292;&#23588;&#20854;&#26159;&#19981;&#38656;&#35201;&#20219;&#20309;&#26174;&#24335;&#27169;&#22359;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#31867;&#39044;&#27979;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#21487;&#20197;&#38544;&#24335;&#26377;&#25928;&#22320;&#32534;&#30721;&#19978;&#19979;&#25991;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#65288;TKG&#65289;&#39044;&#27979;&#26159;&#19968;&#20010;&#25361;&#25112;&#27169;&#22411;&#20351;&#29992;&#36807;&#21435;&#30340;&#30693;&#35782;&#26469;&#39044;&#27979;&#26410;&#26469;&#20107;&#23454;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20110;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#25506;&#31350;&#20102;LLMs&#22312;TKG&#39044;&#27979;&#20013;&#21487;&#20197;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#20351;&#29992;&#65292;&#29305;&#21035;&#26159;&#27809;&#26377;&#20219;&#20309;&#24494;&#35843;&#25110;&#25429;&#25417;&#32467;&#26500;&#21644;&#26102;&#38388;&#20449;&#24687;&#30340;&#26174;&#24335;&#27169;&#22359;&#12290;&#20026;&#20102;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#30456;&#20851;&#21382;&#21490;&#20107;&#23454;&#36716;&#25442;&#20026;&#25552;&#31034;&#24182;&#20351;&#29992;&#20196;&#29260;&#27010;&#29575;&#29983;&#25104;&#25490;&#21517;&#39044;&#27979;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;LLMs&#30340;&#24615;&#33021;&#19982;&#20026;TKG&#39044;&#27979;&#31934;&#24515;&#35774;&#35745;&#21644;&#35757;&#32451;&#30340;&#26368;&#20808;&#36827;TKG&#27169;&#22411;&#30456;&#24403;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#35780;&#20272;&#23637;&#31034;&#20102;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#29305;&#24615;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#27604;&#36739;&#20102;&#20934;&#22791;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26367;&#20195;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#19982;&#33879;&#21517;&#30340;TKG&#26041;&#27861;&#21644;&#31616;&#21333;&#30340;&#39057;&#29575;&#21644;&#26368;&#36817;&#24615;&#22522;&#32447;&#36827;&#34892;&#23545;&#27604;&#12290;&#25105;&#20204;&#36824;&#26816;&#26597;&#20102;&#29983;&#25104;&#30340;&#25552;&#31034;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#19982;&#25152;&#39044;&#27979;&#30340;&#20107;&#23454;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#30830;&#23454;&#21487;&#20197;&#38544;&#24335;&#26377;&#25928;&#22320;&#32534;&#30721;&#19978;&#19979;&#25991;&#21644;&#26102;&#38388;&#20449;&#24687;&#65292;&#36827;&#34892;TKG&#39044;&#27979;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#30693;&#35782;&#25110;&#39046;&#22495;&#29305;&#23450;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal knowledge graph (TKG) forecasting benchmarks challenge models to predict future facts using knowledge of past facts. In this paper, we apply large language models (LLMs) to these benchmarks using in-context learning (ICL). We investigate whether and to what extent LLMs can be used for TKG forecasting, especially without any fine-tuning or explicit modules for capturing structural and temporal information. For our experiments, we present a framework that converts relevant historical facts into prompts and generates ranked predictions using token probabilities. Surprisingly, we observe that LLMs, out-of-the-box, perform on par with state-of-the-art TKG models carefully designed and trained for TKG forecasting. Our extensive evaluation presents performances across several models and datasets with different characteristics, compares alternative heuristics for preparing contextual information, and contrasts to prominent TKG methods and simple frequency and recency baselines. We als
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;L2&#33539;&#25968;&#25240;&#25187;&#26469;&#35299;&#20915;&#39640;&#39057;&#35789;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#20302;&#20272;&#38382;&#39064;</title><link>http://arxiv.org/abs/2305.10610</link><description>&lt;p&gt;
&#21033;&#29992;L2&#33539;&#25968;&#25240;&#25187;&#35299;&#20915;&#39640;&#39057;&#35789;&#20313;&#24358;&#30456;&#20284;&#24230;&#20302;&#20272;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Cosine Similarity Underestimation between High Frequency Words by L2 Norm Discounting. (arXiv:2305.10610v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10610
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;L2&#33539;&#25968;&#25240;&#25187;&#26469;&#35299;&#20915;&#39640;&#39057;&#35789;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#20302;&#20272;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#22914;BERT&#30340;&#19978;&#19979;&#25991;&#21270;&#26631;&#35760;&#23884;&#20837;&#26469;&#35745;&#31639;&#20004;&#20010;&#21333;&#35789;&#20043;&#38388;&#30340;&#20313;&#24358;&#30456;&#20284;&#24615;&#65292;&#24050;&#32463;&#35777;&#26126;&#20250;&#20302;&#20272;&#36825;&#20123;&#21333;&#35789;&#20043;&#38388;&#30340;&#23454;&#38469;&#30456;&#20284;&#24615;&#12290;&#39640;&#39057;&#35789;&#30340;&#30456;&#20284;&#24230;&#20302;&#20272;&#38382;&#39064;&#23588;&#20854;&#20005;&#37325;&#12290;&#34429;&#28982;&#36825;&#20010;&#38382;&#39064;&#24050;&#32463;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#34987;&#27880;&#24847;&#21040;&#65292;&#20294;&#30446;&#21069;&#23578;&#26410;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#20010;&#21333;&#35789;&#30340;&#19978;&#19979;&#25991;&#21270;&#23884;&#20837;&#30340;L2&#33539;&#25968;&#19982;&#20854;&#22312;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#23545;&#25968;&#39057;&#29575;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#19982;&#39640;&#39057;&#35789;&#30456;&#20851;&#30340;&#26356;&#22823;&#30340;L2&#33539;&#25968;&#38477;&#20302;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#20540;&#65292;&#22240;&#27492;&#20302;&#20272;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#20998;&#25968;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#35745;&#31639;&#35789;&#20043;&#38388;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#26102;&#65292;&#36890;&#36807;&#35813;&#35789;&#22312;&#35821;&#26009;&#24211;&#20013;&#30340;&#39057;&#29575;&#25240;&#25187;&#26469;&#35745;&#31639;&#19978;&#19979;&#25991;&#21270;&#21333;&#35789;&#23884;&#20837;&#30340;L2&#33539;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#35859;&#30340;&#20572;&#29992;&#35789;&#34920;&#29616;&#19981;&#21516;&#20110;&#20854;&#20182;&#21333;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cosine similarity between two words, computed using their contextualised token embeddings obtained from masked language models (MLMs) such as BERT has shown to underestimate the actual similarity between those words (Zhou et al., 2022). This similarity underestimation problem is particularly severe for highly frequent words. Although this problem has been noted in prior work, no solution has been proposed thus far. We observe that the L2 norm of contextualised embeddings of a word correlates with its log-frequency in the pretraining corpus. Consequently, the larger L2 norms associated with the highly frequent words reduce the cosine similarity values measured between them, thus underestimating the similarity scores. To solve this issue, we propose a method to discount the L2 norm of a contextualised word embedding by the frequency of that word in a corpus when measuring the cosine similarities between words. We show that the so called stop words behave differently from the rest of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#29702;&#26694;&#26550;&#8212;&#8212;&#24605;&#32500;&#20043;&#26641;&#65288;ToT&#65289;&#65292;&#21487;&#20197;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28145;&#24605;&#29087;&#34385;&#30340;&#20915;&#31574;&#65292;&#20197;&#21450;&#33258;&#25105;&#35780;&#20272;&#21644;&#20840;&#23616;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2305.10601</link><description>&lt;p&gt;
Tree of Thoughts: &#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28145;&#24605;&#29087;&#34385;&#30340;&#38382;&#39064;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
Tree of Thoughts: Deliberate Problem Solving with Large Language Models. (arXiv:2305.10601v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#29702;&#26694;&#26550;&#8212;&#8212;&#24605;&#32500;&#20043;&#26641;&#65288;ToT&#65289;&#65292;&#21487;&#20197;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28145;&#24605;&#29087;&#34385;&#30340;&#20915;&#31574;&#65292;&#20197;&#21450;&#33258;&#25105;&#35780;&#20272;&#21644;&#20840;&#23616;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#30340;&#36890;&#29992;&#38382;&#39064;&#65292;&#20294;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20173;&#28982;&#21463;&#38480;&#20110;&#22522;&#20110;&#26631;&#35760;&#12289;&#20174;&#24038;&#21040;&#21491;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#36825;&#24847;&#21619;&#30528;&#22312;&#38656;&#35201;&#25506;&#32034;&#12289;&#25112;&#30053;&#21069;&#30651;&#25110;&#21021;&#22987;&#20915;&#31574;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#30340;&#20219;&#21153;&#20013;&#65292;&#20182;&#20204;&#21487;&#33021;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#26694;&#26550;&#8212;&#8212;&#24605;&#32500;&#20043;&#26641;&#65288;ToT&#65289;&#65292;&#23427;&#23558;&#36890;&#24120;&#29992;&#20110;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#32500;&#38142;&#26041;&#27861;&#27867;&#21270;&#65292;&#24182;&#20351;&#29992;&#19968;&#33268;&#30340;&#25991;&#26412;&#21333;&#20301;&#65288;&#24605;&#32500;&#65289;&#36827;&#34892;&#25506;&#31350;&#65292;&#36825;&#20123;&#24605;&#32500;&#20316;&#20026;&#35299;&#20915;&#38382;&#39064;&#30340;&#20013;&#38388;&#27493;&#39588;&#12290;&#24605;&#32500;&#20043;&#26641;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#32771;&#34385;&#22810;&#20010;&#19981;&#21516;&#30340;&#25512;&#29702;&#36335;&#24452;&#21644;&#33258;&#25105;&#35780;&#20272;&#26469;&#36827;&#34892;&#28145;&#24605;&#29087;&#34385;&#30340;&#20915;&#31574;&#65292;&#24182;&#20915;&#23450;&#19979;&#19968;&#27493;&#30340;&#34892;&#21160;&#65292;&#21516;&#26102;&#22312;&#24517;&#35201;&#26102;&#21521;&#21069;&#25110;&#21521;&#21518;&#36319;&#36394;&#20197;&#36827;&#34892;&#20840;&#23616;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ToT&#26174;&#33879;&#22686;&#24378;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#20915;&#38382;&#39064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#21517;&#35789;&#22797;&#21512;&#35789;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#21517;&#35789;&#22797;&#21512;&#35789;&#35299;&#37322;&#30340;&#20219;&#21153;&#21644;&#21517;&#35789;&#22797;&#21512;&#35789;&#27010;&#24565;&#21270;&#30340;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;GPT-3&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#12290;</title><link>http://arxiv.org/abs/2305.10568</link><description>&lt;p&gt;
&#20174;&#24039;&#20811;&#21147;&#20820;&#21040;&#24039;&#20811;&#21147;&#40132;&#40060;&#65306;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#21517;&#35789;&#22797;&#21512;&#35789;&#65311;
&lt;/p&gt;
&lt;p&gt;
From chocolate bunny to chocolate crocodile: Do Language Models Understand Noun Compounds?. (arXiv:2305.10568v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#21517;&#35789;&#22797;&#21512;&#35789;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#21517;&#35789;&#22797;&#21512;&#35789;&#35299;&#37322;&#30340;&#20219;&#21153;&#21644;&#21517;&#35789;&#22797;&#21512;&#35789;&#27010;&#24565;&#21270;&#30340;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;GPT-3&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21517;&#35789;&#22797;&#21512;&#35789;&#26159;&#25351;&#23558;&#22810;&#20010;&#21517;&#35789;&#32452;&#21512;&#25104;&#19968;&#20010;&#26032;&#35789;&#65292;&#22914;&#8220;&#24039;&#20811;&#21147;&#20820;&#8221;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#35789;&#22797;&#21512;&#35789;&#35299;&#37322;&#30340;&#20219;&#21153;&#24182;&#20462;&#25913;&#20102;&#35813;&#20219;&#21153;&#30340;&#25968;&#25454;&#21644;&#35780;&#20272;&#35774;&#32622;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-3&#65288;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;&#65289;&#20960;&#20046;&#21487;&#20197;&#23436;&#32654;&#22320;&#35299;&#20915;&#35813;&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25506;&#35752;&#20102;&#21517;&#35789;&#22797;&#21512;&#35789;&#27010;&#24565;&#21270;&#30340;&#20219;&#21153;&#65292;&#21363;&#35299;&#37322;&#26032;&#39062;&#25110;&#32597;&#35265;&#30340;&#21517;&#35789;&#32452;&#21512;&#35789;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;GPT-3&#25512;&#29702;&#19990;&#30028;&#30340;&#31243;&#24230;&#65292;&#24182;&#21457;&#29616;&#23427;&#30340;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Noun compound interpretation is the task of expressing a noun compound (e.g. chocolate bunny) in a free-text paraphrase that makes the relationship between the constituent nouns explicit (e.g. bunny-shaped chocolate). We propose modifications to the data and evaluation setup of the standard task (Hendrickx et al., 2013), and show that GPT-3 solves it almost perfectly. We then investigate the task of noun compound conceptualization, i.e. paraphrasing a novel or rare noun compound. E.g., chocolate crocodile is a crocodile-shaped chocolate. This task requires creativity, commonsense, and the ability to generalize knowledge about similar concepts. While GPT-3's performance is not perfect, it is better than that of humans -- likely thanks to its access to vast amounts of knowledge, and because conceptual processing is effortful for people (Connell and Lynott, 2012). Finally, we estimate the extent to which GPT-3 is reasoning about the world vs. parroting its training data. We find that the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#36127;&#37319;&#26679;&#20998;&#24067;&#23545;&#23545;&#27604;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#32771;&#34385;&#30828;&#24230;&#21644;&#32467;&#26500;&#30340;&#23545;&#27604;&#65288;HaSa&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#21435;&#38500;&#20551;&#36127;&#26679;&#26412;&#65292;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10563</link><description>&lt;p&gt;
&#25506;&#31350;&#30828;&#36127;&#37319;&#26679;&#20998;&#24067;&#23545;&#23545;&#27604;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Investigating the Effect of Hard Negative Sample Distribution on Contrastive Knowledge Graph Embedding. (arXiv:2305.10563v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#36127;&#37319;&#26679;&#20998;&#24067;&#23545;&#23545;&#27604;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#32771;&#34385;&#30828;&#24230;&#21644;&#32467;&#26500;&#30340;&#23545;&#27604;&#65288;HaSa&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#21435;&#38500;&#20551;&#36127;&#26679;&#26412;&#65292;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGEs&#65289;&#30340;&#36136;&#37327;&#65292;&#23427;&#20381;&#36182;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#29992;&#36127;&#19977;&#20803;&#32452;&#22686;&#24378;&#25968;&#25454;&#38598;&#12290;&#22312;&#36127;&#37319;&#26679;&#30340;&#23545;&#27604;&#25439;&#22833;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#39640;&#36136;&#37327;&#65288;&#21363;&#30828;&#65289;&#36127;&#37319;&#26679;&#30340;&#21551;&#21457;&#24335;&#29983;&#25104;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;InfoNCE&#25439;&#22833;&#65292;&#26174;&#24335;&#32771;&#34385;&#20102;&#36127;&#37319;&#26679;&#20998;&#24067;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29992;&#30828;&#36127;&#26679;&#26412;&#26368;&#23567;&#21270;InfoNCE&#25439;&#22833;&#21487;&#20197;&#26368;&#22823;&#21270;&#32473;&#23450;&#19977;&#20803;&#32452;&#21644;&#36127;&#19977;&#20803;&#32452;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#35777;&#26126;&#30828;&#36127;&#26679;&#26412;&#20250;&#23548;&#33268;&#20551;&#36127;&#26679;&#26412;&#65288;&#21363;&#38169;&#35823;&#30340;&#20107;&#23454;&#19977;&#20803;&#32452;&#65289;&#24182;&#38477;&#20302;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#30340;&#22270;&#32467;&#26500;&#21435;&#38500;&#20551;&#36127;&#19977;&#20803;&#32452;&#30340;&#26032;&#22411;&#36127;&#37319;&#26679;&#20998;&#24067;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#31216;&#20026;&#32771;&#34385;&#30828;&#24230;&#21644;&#32467;&#26500;&#30340;&#23545;&#27604;&#65288;HaSa&#65289;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of the knowledge graph completion task heavily depends on the quality of the knowledge graph embeddings (KGEs), which relies on self-supervised learning and augmenting the dataset with negative triples. There is a gap in literature between the theoretical analysis of negative samples on contrastive loss and heuristic generation of quality (i.e., hard) negative triples. In this paper, we modify the InfoNCE loss to explicitly account for the negative sample distribution. We show minimizing InfoNCE loss with hard negatives maximizes the KL-divergence between the given and negative triple embedding. However, we also show that hard negatives can lead to false negatives (i.e., accidentally factual triples) and reduce downstream task performance. To address this issue, we propose a novel negative sample distribution that uses the graph structure of the knowledge graph to remove the false negative triples. We call our algorithm Hardness and Structure-aware (\textbf{HaSa}) contrasti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#20107;&#20214;&#25277;&#21462;&#31995;&#32479;&#21644;&#29992;&#25143;&#30028;&#38754;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#33521;&#35821;&#35757;&#32451;&#25968;&#25454;&#65292;&#33021;&#22815;&#22312;100&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#25991;&#26412;&#20013;&#36827;&#34892;&#20840;&#29699;&#20107;&#20214;&#30340;&#25277;&#21462;&#12289;&#21487;&#35270;&#21270;&#21644;&#25628;&#32034;&#12290;&#21516;&#26102;&#65292;&#35813;&#31995;&#32479;&#36824;&#33021;&#22815;&#36827;&#34892;&#36328;&#35821;&#35328;&#30340;&#20197;&#20107;&#20214;&#20026;&#20013;&#24515;&#30340;&#25628;&#32034;&#12290;</title><link>http://arxiv.org/abs/2305.10561</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#20107;&#20214;&#29702;&#35299;&#65306;&#25277;&#21462;&#12289;&#21487;&#35270;&#21270;&#21644;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Massively Multi-Lingual Event Understanding: Extraction, Visualization, and Search. (arXiv:2305.10561v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#20107;&#20214;&#25277;&#21462;&#31995;&#32479;&#21644;&#29992;&#25143;&#30028;&#38754;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#33521;&#35821;&#35757;&#32451;&#25968;&#25454;&#65292;&#33021;&#22815;&#22312;100&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#25991;&#26412;&#20013;&#36827;&#34892;&#20840;&#29699;&#20107;&#20214;&#30340;&#25277;&#21462;&#12289;&#21487;&#35270;&#21270;&#21644;&#25628;&#32034;&#12290;&#21516;&#26102;&#65292;&#35813;&#31995;&#32479;&#36824;&#33021;&#22815;&#36827;&#34892;&#36328;&#35821;&#35328;&#30340;&#20197;&#20107;&#20214;&#20026;&#20013;&#24515;&#30340;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461; ISI-Clear&#65292;&#19968;&#31181;&#20808;&#36827;&#30340;&#36328;&#35821;&#35328;&#12289;&#38646;&#26679;&#26412;&#20107;&#20214;&#25277;&#21462;&#31995;&#32479;&#65292;&#20197;&#21450;&#29992;&#20110;&#20107;&#20214;&#21487;&#35270;&#21270;&#21644;&#26816;&#32034;&#30340;&#29992;&#25143;&#30028;&#38754;&#12290;ISI-Clear&#20165;&#20351;&#29992;&#33521;&#35821;&#35757;&#32451;&#25968;&#25454;&#65292;&#23601;&#33021;&#22788;&#29702;&#26469;&#33258;100&#31181;&#35821;&#35328;&#65288;&#20174;&#21335;&#38750;&#33655;&#20848;&#35821;&#21040;&#24847;&#31532;&#32490;&#35821;&#65289;&#30340;&#29992;&#25143;&#25552;&#20379;&#30340;&#25991;&#26412;&#65292;&#20351;&#20840;&#29699;&#20107;&#20214;&#38543;&#38656;&#32780;&#21464;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22810;&#20010;&#20197;&#20107;&#20214;&#20026;&#20013;&#24515;&#30340;&#25277;&#21462;&#35270;&#22270;&#65292;&#21253;&#25324;&#22270;&#24418;&#34920;&#31034;&#21644;&#25991;&#26723;&#32423;&#25688;&#35201;&#12290;&#25105;&#20204;&#36824;&#23558;&#29616;&#26377;&#30340;&#36328;&#35821;&#35328;&#26816;&#32034;&#31639;&#27861;&#19982;&#20107;&#20214;&#25277;&#21462;&#21151;&#33021;&#38598;&#25104;&#65292;&#25552;&#20379;&#36328;&#35821;&#35328;&#30340;&#20197;&#20107;&#20214;&#20026;&#20013;&#24515;&#30340;&#25628;&#32034;&#65292;&#20351;&#33521;&#35821;&#29992;&#25143;&#33021;&#22815;&#20351;&#29992;&#33521;&#35821;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#65288;&#20363;&#22914;&#20234;&#26391;&#38669;&#20081;&#29190;&#21457;&#65289;&#25110;&#32467;&#26500;&#21270;&#26597;&#35810;&#65288;&#20363;&#22914;&#26597;&#25214;&#25152;&#26377;&#31867;&#22411;&#20026;&#8220;&#30142;&#30149;&#26292;&#21457;&#8221;&#30340;&#20107;&#20214;&#65292;&#20854;&#20195;&#29702;&#20026;&#38669;&#20081;&#21644;&#25152;&#22312;&#22320;&#20026;&#20234;&#26391;&#65289;&#33258;&#21160;&#25628;&#32034;&#20174;&#38750;&#33521;&#35821;&#25991;&#26723;&#35821;&#26009;&#24211;&#20013;&#33258;&#21160;&#25277;&#21462;&#30340;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present ISI-Clear, a state-of-the-art, cross-lingual, zero-shot event extraction system and accompanying user interface for event visualization &amp; search. Using only English training data, ISI-Clear makes global events available on-demand, processing user-supplied text in 100 languages ranging from Afrikaans to Yiddish. We provide multiple event-centric views of extracted events, including both a graphical representation and a document-level summary. We also integrate existing cross-lingual search algorithms with event extraction capabilities to provide cross-lingual event-centric search, allowing English-speaking users to search over events automatically extracted from a corpus of non-English documents, using either English natural language queries (e.g. cholera outbreaks in Iran) or structured queries (e.g. find all events of type Disease-Outbreak with agent cholera and location Iran).
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#33258;&#21160;&#21518;&#32534;&#36753;&#31995;&#32479;&#26080;&#27861;&#22788;&#29702;&#39640;&#36136;&#37327;&#26426;&#22120;&#32763;&#35793;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#32473;&#23450;MT&#36827;&#34892;&#23545;&#31216;&#33258;&#25105;&#20851;&#27880;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#33258;&#21160;&#21518;&#32534;&#36753;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.10557</link><description>&lt;p&gt;
"&#33258;&#21160;&#21518;&#32534;&#36753;&#39640;&#36136;&#37327;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#21477;&#27861;&#23545;&#31216;&#24615;"
&lt;/p&gt;
&lt;p&gt;
Bring More Attention to Syntactic Symmetry for Automatic Postediting of High-Quality Machine Translations. (arXiv:2305.10557v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10557
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#33258;&#21160;&#21518;&#32534;&#36753;&#31995;&#32479;&#26080;&#27861;&#22788;&#29702;&#39640;&#36136;&#37327;&#26426;&#22120;&#32763;&#35793;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#32473;&#23450;MT&#36827;&#34892;&#23545;&#31216;&#33258;&#25105;&#20851;&#27880;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#33258;&#21160;&#21518;&#32534;&#36753;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21518;&#32534;&#36753;&#65288;APE&#65289;&#26159;&#29992;&#20110;&#25913;&#36827;&#32473;&#23450;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#30340;&#33258;&#21160;&#21270;&#36807;&#31243;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#26159;&#22312;&#26377;&#20016;&#23500;&#25968;&#25454;&#36164;&#28304;&#30340;&#35821;&#35328;&#23545;&#65288;&#33521;&#35821;-&#24503;&#35821;&#65289;&#65292;&#29616;&#26377;&#30340;APE&#31995;&#32479;&#20063;&#19981;&#25797;&#38271;&#22788;&#29702;&#39640;&#36136;&#37327;&#30340;MT&#65306;&#32473;&#23450;&#30340;MT&#36136;&#37327;&#36234;&#39640;&#65292;&#20915;&#23450;&#21738;&#20123;&#37096;&#20998;&#38656;&#35201;&#32534;&#36753;&#20197;&#21450;&#22914;&#20309;&#20462;&#22797;&#36825;&#20123;&#38169;&#35823;&#23601;&#36234;&#22256;&#38590;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#21487;&#33021;&#30340;&#26041;&#27861;&#26159;&#23558;&#26356;&#28145;&#20837;&#30340;&#30446;&#26631;&#35821;&#35328;&#30693;&#35782;&#27880;&#20837;&#27169;&#22411;&#20013;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#35821;&#35328;&#23398;&#21160;&#26426;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#22686;&#24378;APE&#27169;&#22411;&#23545;&#30446;&#26631;&#35821;&#35328;&#30340;&#29702;&#35299;&#65306;&#36890;&#36807;&#19968;&#20010;&#40723;&#21169;&#23545;&#32473;&#23450;MT&#36827;&#34892;&#23545;&#31216;&#33258;&#25105;&#20851;&#27880;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#23545;&#23454;&#39564;&#32467;&#26524;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#21161;&#20110;&#25552;&#39640;&#39640;&#36136;&#37327;MT&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#26550;&#26500;&#30340;APE&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic postediting (APE) is an automated process to refine a given machine translation (MT). Recent findings present that existing APE systems are not good at handling high-quality MTs even for a language pair with abundant data resources, English$\unicode{x2013}$German: the better the given MT is, the harder it is to decide what parts to edit and how to fix these errors. One possible solution to this problem is to instill deeper knowledge about the target language into the model. Thus, we propose a linguistically motivated method of regularization that is expected to enhance APE models' understanding of the target language: a loss function that encourages symmetric self-attention on the given MT. Our analysis of experimental results demonstrates that the proposed method helps improving the state-of-the-art architecture's APE quality for high-quality MTs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#23398;&#20064;&#23545;&#35805;&#31995;&#32479;&#20013;&#21033;&#29992;&#21382;&#21490;&#22238;&#24402;&#20107;&#20214;&#25253;&#21578;&#26469;&#39564;&#35777;&#12289;&#20445;&#25252;&#21644;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#22823;&#35268;&#27169;&#21830;&#19994;&#29615;&#22659;&#20013;&#30340;&#32463;&#39564;&#36830;&#36143;&#24615;&#21644;&#25919;&#31574;&#25913;&#36827;&#20043;&#38388;&#30340;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10528</link><description>&lt;p&gt;
&#33258;&#23398;&#20064;&#23545;&#35805;&#31995;&#32479;&#20013;&#32570;&#38519;&#34892;&#20026;&#30340;&#21487;&#25193;&#23637;&#21644;&#23433;&#20840;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Scalable and Safe Remediation of Defective Actions in Self-Learning Conversational Systems. (arXiv:2305.10528v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#23398;&#20064;&#23545;&#35805;&#31995;&#32479;&#20013;&#21033;&#29992;&#21382;&#21490;&#22238;&#24402;&#20107;&#20214;&#25253;&#21578;&#26469;&#39564;&#35777;&#12289;&#20445;&#25252;&#21644;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#22823;&#35268;&#27169;&#21830;&#19994;&#29615;&#22659;&#20013;&#30340;&#32463;&#39564;&#36830;&#36143;&#24615;&#21644;&#25919;&#31574;&#25913;&#36827;&#20043;&#38388;&#30340;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24050;&#25104;&#20026;&#26368;&#26032;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#30340;&#39537;&#21160;&#21147;&#65292;&#25913;&#21892;&#20102;&#30446;&#26631;&#20026;&#23548;&#21521;&#30340;&#20195;&#29702;&#20154;&#19982;&#20154;&#20043;&#38388;&#26356;&#33258;&#28982;&#30340;&#20114;&#21160;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#30340;&#28385;&#24847;&#24230;&#65292;&#20294;&#22312;&#22823;&#22411;&#21830;&#19994;&#29615;&#22659;&#20013;&#65292;&#24179;&#34913;&#25919;&#31574;&#25913;&#36827;&#21644;&#32463;&#39564;&#36830;&#36143;&#24615;&#32463;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#21382;&#21490;&#22238;&#24402;&#20107;&#20214;&#25253;&#21578;&#20013;&#30340;&#39640;&#31934;&#24230;&#26679;&#26412;&#23545;&#25919;&#31574;&#36827;&#34892;&#39564;&#35777;&#12289;&#23433;&#20840;&#20445;&#25252;&#21644;&#25913;&#36827;&#65292;&#20197;&#20415;&#22312;&#22312;&#32447;&#37096;&#32626;&#21069;&#36827;&#34892;&#20462;&#27491;&#12290;&#20316;&#32773;&#23545;&#30495;&#23454;&#30340;&#23545;&#35805;&#31995;&#32479;&#21644;&#23454;&#38469;&#30340;&#22238;&#24402;&#20107;&#20214;&#25968;&#25454;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20182;&#20204;&#30340;&#29983;&#20135;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-Policy reinforcement learning has been a driving force for the state-of-the-art conversational AIs leading to more natural humanagent interactions and improving the user satisfaction for goal-oriented agents. However, in large-scale commercial settings, it is often challenging to balance between policy improvements and experience continuity on the broad spectrum of applications handled by such system. In the literature, off-policy evaluation and guard-railing on aggregate statistics has been commonly used to address this problem. In this paper, we propose a method for curating and leveraging high-precision samples sourced from historical regression incident reports to validate, safe-guard, and improve policies prior to the online deployment. We conducted extensive experiments using data from a real-world conversational system and actual regression incidents. The proposed method is currently deployed in our production system to protect customers against broken experiences and enable
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32479;&#35745;&#30693;&#35782;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;GLMs&#65289;&#30340;&#30693;&#35782;&#36755;&#20986;&#12290;&#36890;&#36807;&#23545;14&#31181;GLMs&#36827;&#34892;&#32508;&#21512;&#27604;&#36739;&#65292;&#21457;&#29616;&#30693;&#35782;&#36981;&#24490;&#32553;&#25918;&#23450;&#24459;&#65292;&#32780;&#23545;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#21487;&#33021;&#20250;&#25439;&#23475;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.10519</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#35745;&#30693;&#35782;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Statistical Knowledge Assessment for Generative Language Models. (arXiv:2305.10519v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32479;&#35745;&#30693;&#35782;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;GLMs&#65289;&#30340;&#30693;&#35782;&#36755;&#20986;&#12290;&#36890;&#36807;&#23545;14&#31181;GLMs&#36827;&#34892;&#32508;&#21512;&#27604;&#36739;&#65292;&#21457;&#29616;&#30693;&#35782;&#36981;&#24490;&#32553;&#25918;&#23450;&#24459;&#65292;&#32780;&#23545;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#21487;&#33021;&#20250;&#25439;&#23475;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;GLMs&#65289;&#23637;&#31034;&#20102;&#23384;&#20648;&#20107;&#23454;&#30693;&#35782;&#21644;&#39640;&#25928;&#22238;&#31572;&#26597;&#35810;&#30340;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#32473;&#23450;&#19981;&#21516;&#30340;&#25552;&#31034;&#65292;GLM&#26159;&#21542;&#22987;&#32456;&#29983;&#25104;&#20107;&#23454;&#27491;&#30830;&#30340;&#31572;&#26696;&#65311;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#30001;&#28508;&#21464;&#37327;&#21644;KaRR&#24230;&#37327;&#25351;&#23548;&#30340;&#32479;&#35745;&#30693;&#35782;&#35780;&#20272;&#26694;&#26550;&#65292;&#35813;&#24230;&#37327;&#36890;&#36807;&#35745;&#31639;&#27169;&#22411;&#22312;&#21508;&#31181;&#25991;&#26412;&#24418;&#24335;&#19978;&#30340;&#36830;&#32493;&#27010;&#29575;&#37327;&#21270;&#20854;&#30693;&#35782;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#23545;14&#31181;GLM&#30340;&#30693;&#35782;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#21253;&#25324;LLaMA&#12289;Alpaca&#12289;OPT&#21644;&#20854;&#20182;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32479;&#35745;&#30693;&#35782;&#35780;&#20272;&#28085;&#30422;&#20102;600&#31181;&#20851;&#31995;&#31867;&#22411;&#65292;&#24182;&#26174;&#31034;&#20986;&#19982;&#20154;&#31867;&#35780;&#20272;&#30340;&#24378;&#30456;&#20851;&#24615;&#65288;0.43 Kendall's $\tau$&#65289;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#20855;&#26377;&#30456;&#21516;&#25903;&#26550;&#32467;&#26500;&#30340;GLM&#30340;&#30693;&#35782;&#36981;&#24490;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#19988;&#22312;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#24494;&#35843;&#21487;&#33021;&#20250;&#25439;&#23475;&#27169;&#22411;&#25345;&#32493;&#29983;&#25104;&#20107;&#23454;&#27491;&#30830;&#30340;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Language Models (GLMs) have demonstrated capabilities to store factual knowledge and answer queries efficiently. Given varying prompts, does a GLM consistently generate factually correct answers? In this paper, we introduce a statistical knowledge assessment framework guided by latent variables and the KaRR metric, which quantifies a model's knowledge by computing its continuous probability across diverse text forms. We conduct a comprehensive comparison of knowledge across 14 GLMs using our framework, including LLaMA, Alpaca, OPT, and others. Our statistical knowledge assessment encompasses 600 relation types and exhibits a strong correlation (0.43 Kendall's $\tau$) with human evaluation. Our findings reveal that the knowledge in GLMs with the same backbone architecture adheres to the scaling law, and that tuning on instruction-following data may compromise the model's ability to generate factually correct text consistently.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#31995;&#32479;&#35270;&#35282;&#65292;&#20854;&#20013;&#22270;&#20687;&#35299;&#37322;&#26159;&#22522;&#20110;&#23545;&#35805;&#19978;&#19979;&#25991;&#30340;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#35266;&#28857;&#26469;&#26500;&#24314;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2305.10512</link><description>&lt;p&gt;
IMAD: &#22270;&#20687;&#22686;&#24378;&#30340;&#22810;&#27169;&#24335;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
IMAD: IMage-Augmented multi-modal Dialogue. (arXiv:2305.10512v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10512
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#31995;&#32479;&#35270;&#35282;&#65292;&#20854;&#20013;&#22270;&#20687;&#35299;&#37322;&#26159;&#22522;&#20110;&#23545;&#35805;&#19978;&#19979;&#25991;&#30340;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#35266;&#28857;&#26469;&#26500;&#24314;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#23545;&#35805;&#31995;&#32479;&#24050;&#32463;&#22312;&#22788;&#29702;&#22522;&#20110;&#25991;&#26412;&#30340;&#36890;&#35759;&#26041;&#38754;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36824;&#27809;&#26377;&#26377;&#25928;&#22320;&#34701;&#21512;&#35270;&#35273;&#20449;&#24687;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#34701;&#21512;&#22270;&#20687;&#30340;&#27169;&#22411;&#19987;&#27880;&#20110;&#35752;&#35770;&#22270;&#20687;&#26412;&#36523;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#31995;&#32479;&#35270;&#35282;&#65292;&#35299;&#37322;&#20102;&#23545;&#35805;&#20013;&#22270;&#20687;&#30340;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#26088;&#22312;&#25193;&#23637;&#24403;&#21069;&#23545;&#35805;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#20174;&#21333;&#19968;&#27169;&#24335;&#65288;&#25991;&#26412;&#65289;&#21521;&#22810;&#27169;&#24577;&#36716;&#25442;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#21253;&#21547;&#22270;&#20687;&#21644;&#23545;&#35805;&#19978;&#19979;&#25991;&#30340;&#32463;&#36807;&#39564;&#35777;&#30340;&#33521;&#25991;&#25968;&#25454;&#38598;&#26159;&#36825;&#39033;&#20219;&#21153;&#30340;&#38590;&#28857;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26041;&#27861;&#26469;&#33258;&#21160;&#26500;&#24314;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#30456;&#20284;&#24615;&#21644;&#21477;&#23376;&#30456;&#20284;&#24615;&#26469;&#35782;&#21035;&#21738;&#20123;&#35805;&#35821;&#21487;&#20197;&#29992;&#22270;&#20687;&#26367;&#25442;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#26367;&#25442;&#37027;&#20123;&#35805;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, dialogue systems have achieved high performance in processing text-based communication. However, they have not yet effectively incorporated visual information, which poses a significant challenge. Furthermore, existing models that incorporate images in dialogue generation focus on discussing the image itself. Our proposed approach presents a novel perspective on multi-modal dialogue systems, which interprets the image in the context of the dialogue. By doing so, we aim to expand the capabilities of current dialogue systems and transition them from single modality (text) to multi-modality. However, there is a lack of validated English datasets that contain both images and dialogue contexts for this task. Thus, we propose a two-stage approach to automatically construct a multi-modal dialogue dataset. In the first stage, we utilize text-to-image similarity and sentence similarity to identify which utterances could be replaced with an image. In the second stage, we replace those
&lt;/p&gt;</description></item><item><title>ChatGPT&#22312;&#32763;&#35793;&#20013;&#24310;&#32493;&#24615;&#21035;&#20559;&#35265;&#65292;&#24573;&#30053;&#38750;&#24615;&#21035;&#20195;&#35789;&#65292;&#20250;&#23558;&#24615;&#21035;&#20013;&#31435;&#20195;&#35789;&#36716;&#25442;&#20026;&#30007;&#24615;&#25110;&#22899;&#24615;&#65292;&#29978;&#33267;&#26080;&#27861;&#23558;&#33521;&#35821;&#20013;&#30340;&#24615;&#21035;&#20013;&#31435;&#20195;&#35789;&#32763;&#35793;&#20026;&#20854;&#20182;&#35821;&#35328;&#30340;&#24615;&#21035;&#20013;&#31435;&#20195;&#35789;&#12290;</title><link>http://arxiv.org/abs/2305.10510</link><description>&lt;p&gt;
ChatGPT&#22312;&#20845;&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#24310;&#32493;&#24615;&#21035;&#20559;&#35265;&#65292;&#24573;&#30053;&#38750;&#24615;&#21035;&#20195;&#35789;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Perpetuates Gender Bias in Machine Translation and Ignores Non-Gendered Pronouns: Findings across Bengali and Five other Low-Resource Languages. (arXiv:2305.10510v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10510
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#32763;&#35793;&#20013;&#24310;&#32493;&#24615;&#21035;&#20559;&#35265;&#65292;&#24573;&#30053;&#38750;&#24615;&#21035;&#20195;&#35789;&#65292;&#20250;&#23558;&#24615;&#21035;&#20013;&#31435;&#20195;&#35789;&#36716;&#25442;&#20026;&#30007;&#24615;&#25110;&#22899;&#24615;&#65292;&#29978;&#33267;&#26080;&#27861;&#23558;&#33521;&#35821;&#20013;&#30340;&#24615;&#21035;&#20013;&#31435;&#20195;&#35789;&#32763;&#35793;&#20026;&#20854;&#20182;&#35821;&#35328;&#30340;&#24615;&#21035;&#20013;&#31435;&#20195;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#22810;&#20803;&#25991;&#21270;&#26102;&#20195;&#65292;&#35821;&#35328;&#32763;&#35793;&#26159;&#26368;&#24120;&#35265;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#30001;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#35843;&#33410;&#21644;&#33258;&#21160;&#21270;&#12290;&#20316;&#20026;&#19968;&#31181;&#26032;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;ChatGPT&#22768;&#31216;&#22312;&#36825;&#26679;&#30340;&#32763;&#35793;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#19968;&#22768;&#26126;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;ChatGPT&#22312;&#33521;&#35821;&#21644;&#20165;&#20351;&#29992;&#24615;&#21035;&#20013;&#31435;&#20195;&#35789;&#30340;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20197;&#23391;&#21152;&#25289;&#35821;&#20026;&#20013;&#24515;&#36827;&#34892;&#20102;&#36825;&#39033;&#30740;&#31350;&#65292;&#23391;&#21152;&#25289;&#35821;&#26159;&#20840;&#29699;&#31532;&#19971;&#22823;&#20351;&#29992;&#35821;&#35328;&#65292;&#20294;&#25105;&#20204;&#36824;&#27010;&#25324;&#20102;&#25105;&#20204;&#22312;&#27874;&#26031;&#35821;&#12289;&#39532;&#26469;&#35821;&#12289;&#22612;&#21152;&#27931;&#35821;&#12289;&#27888;&#35821;&#21644;&#22303;&#32819;&#20854;&#35821;&#31561;&#20116;&#31181;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;ChatGPT&#24310;&#32493;&#20102;&#23545;&#26576;&#20123;&#32844;&#19994;&#65288;&#20363;&#22914;&#30007;&#20154;=&#21307;&#29983;&#65292;&#22899;&#20154;=&#25252;&#22763;&#65289;&#25110;&#34892;&#21160;&#65288;&#22899;&#20154;=&#28921;&#39274;&#65292;&#30007;&#20154;=&#21435;&#24037;&#20316;&#65289;&#36171;&#20104;&#24615;&#21035;&#40664;&#35748;&#20540;&#21644;&#21051;&#26495;&#21360;&#35937;&#30340;&#20559;&#35265;&#65292;&#22240;&#20026;&#23427;&#23558;&#35821;&#35328;&#20013;&#30340;&#24615;&#21035;&#20013;&#31435;&#20195;&#35789;&#36716;&#25442;&#20026;&#8220;&#20182;&#8221;&#25110;&#8220;&#22905;&#8221;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;ChatGPT&#23436;&#20840;&#26080;&#27861;&#23558;&#33521;&#35821;&#30340;&#24615;&#21035;&#20013;&#31435;&#20195;&#35789;&#8220;they&#8221;&#32763;&#35793;&#20026;&#20854;&#20182;&#35821;&#35328;&#20013;&#30456;&#24212;&#30340;&#24615;&#21035;&#20013;&#31435;&#20195;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this multicultural age, language translation is one of the most performed tasks, and it is becoming increasingly AI-moderated and automated. As a novel AI system, ChatGPT claims to be proficient in such translation tasks and in this paper, we put that claim to the test. Specifically, we examine ChatGPT's accuracy in translating between English and languages that exclusively use gender-neutral pronouns. We center this study around Bengali, the 7$^{th}$ most spoken language globally, but also generalize our findings across five other languages: Farsi, Malay, Tagalog, Thai, and Turkish. We find that ChatGPT perpetuates gender defaults and stereotypes assigned to certain occupations (e.g. man = doctor, woman = nurse) or actions (e.g. woman = cook, man = go to work), as it converts gender-neutral pronouns in languages to `he' or `she'. We also observe ChatGPT completely failing to translate the English gender-neutral pronoun `they' into equivalent gender-neutral pronouns in other languag
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36719;&#21024;&#38500;&#26631;&#20934;&#26469;&#35780;&#20272;&#24402;&#22240;&#26041;&#27861;&#30340;&#24544;&#23454;&#24230;&#65292;&#35813;&#26041;&#27861;&#38543;&#26426;&#36974;&#30422;&#26631;&#35760;&#30340;&#37096;&#20998;&#21521;&#37327;&#34920;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#27604;&#29616;&#26377;&#30340;&#30828;&#21024;&#38500;&#26631;&#20934;&#26356;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2305.10496</link><description>&lt;p&gt;
&#34701;&#21512;&#24402;&#22240;&#37325;&#35201;&#24615;&#20197;&#25552;&#39640;&#24544;&#23454;&#24230;&#35780;&#20272;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Incorporating Attribution Importance for Improving Faithfulness Metrics. (arXiv:2305.10496v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36719;&#21024;&#38500;&#26631;&#20934;&#26469;&#35780;&#20272;&#24402;&#22240;&#26041;&#27861;&#30340;&#24544;&#23454;&#24230;&#65292;&#35813;&#26041;&#27861;&#38543;&#26426;&#36974;&#30422;&#26631;&#35760;&#30340;&#37096;&#20998;&#21521;&#37327;&#34920;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#27604;&#29616;&#26377;&#30340;&#30828;&#21024;&#38500;&#26631;&#20934;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#26159;&#25552;&#20379;&#23545;&#27169;&#22411;&#25512;&#29702;&#36807;&#31243;&#36827;&#34892;&#39044;&#27979;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#19968;&#20010;&#26356;&#21152;&#20934;&#30830;&#30340;&#24402;&#22240;&#26041;&#27861;&#26631;&#24535;&#30528;&#23427;&#26356;&#21152;&#24544;&#23454;&#65292;&#23427;&#21487;&#20197;&#26356;&#21152;&#20934;&#30830;&#22320;&#21453;&#26144;&#21738;&#20123;&#37096;&#20998;&#30340;&#36755;&#20837;&#23545;&#39044;&#27979;&#26356;&#21152;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24544;&#23454;&#24230;&#35780;&#20272;&#26041;&#27861;&#65292;&#22914;&#20805;&#20998;&#24615;&#21644;&#20840;&#38754;&#24615;&#65292;&#21482;&#20351;&#29992;&#19968;&#31181;&#30828;&#21024;&#38500;&#26631;&#20934;&#65292;&#21363;&#23436;&#20840;&#21024;&#38500;&#25110;&#20445;&#30041;&#30001;&#32473;&#23450;&#24402;&#22240;&#26041;&#27861;&#25490;&#21517;&#26368;&#39640;&#30340;&#39030;&#37096;&#26631;&#35760;&#65292;&#24182;&#35266;&#23519;&#39044;&#27979;&#21487;&#33021;&#24615;&#30340;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#36825;&#31181;&#30828;&#21024;&#38500;&#26631;&#20934;&#24573;&#30053;&#20102;&#27599;&#20010;&#26631;&#35760;&#30340;&#37325;&#35201;&#24615;&#65292;&#25226;&#23427;&#20204;&#20840;&#37096;&#31561;&#21516;&#22320;&#22788;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36719;&#21024;&#38500;&#26631;&#20934;&#12290;&#25105;&#20204;&#19981;&#20250;&#23436;&#20840;&#21024;&#38500;&#25110;&#20445;&#30041;&#36755;&#20837;&#20013;&#30340;&#26631;&#35760;&#65292;&#32780;&#26159;&#38543;&#26426;&#22320;&#36974;&#30422;&#20195;&#34920;&#24402;&#22240;&#26041;&#27861;&#37325;&#35201;&#24615;&#30340;&#37096;&#20998;&#26631;&#35760;&#21521;&#37327;&#34920;&#31034;&#12290;&#22522;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#21644;&#19981;&#21516;&#30340;&#24402;&#22240;&#26041;&#27861;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature attribution methods (FAs) are popular approaches for providing insights into the model reasoning process of making predictions. The more faithful a FA is, the more accurately it reflects which parts of the input are more important for the prediction. Widely used faithfulness metrics, such as sufficiency and comprehensiveness use a hard erasure criterion, i.e. entirely removing or retaining the top most important tokens ranked by a given FA and observing the changes in predictive likelihood. However, this hard criterion ignores the importance of each individual token, treating them all equally for computing sufficiency and comprehensiveness. In this paper, we propose a simple yet effective soft erasure criterion. Instead of entirely removing or retaining tokens from the input, we randomly mask parts of the token vector representations proportionately to their FA importance. Extensive experiments across various natural language processing tasks and different FAs show that our sof
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30456;&#31354;&#38388;&#20998;&#26512;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#35786;&#26029;&#24515;&#33039;&#30142;&#30149;&#65292;&#22312;MIT-BIH&#25968;&#25454;&#24211;&#19978;&#21462;&#24471;&#20102;93.3%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.10450</link><description>&lt;p&gt;
&#36890;&#36807;&#30456;&#31354;&#38388;&#20998;&#26512;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29702;&#35299;&#27491;&#24120;&#21644;&#24322;&#24120;&#24515;&#33039;
&lt;/p&gt;
&lt;p&gt;
Understanding of Normal and Abnormal Hearts by Phase Space Analysis and Convolutional Neural Networks. (arXiv:2305.10450v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10450
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30456;&#31354;&#38388;&#20998;&#26512;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#35786;&#26029;&#24515;&#33039;&#30142;&#30149;&#65292;&#22312;MIT-BIH&#25968;&#25454;&#24211;&#19978;&#21462;&#24471;&#20102;93.3%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#33039;&#30142;&#30149;&#26159;&#29616;&#20195;&#24037;&#19994;&#21270;&#31038;&#20250;&#20013;&#33268;&#27515;&#22240;&#32032;&#20043;&#19968;&#65292;&#23548;&#33268;&#20844;&#20849;&#21355;&#29983;&#31995;&#32479;&#30340;&#39640;&#26114;&#24320;&#25903;&#12290;&#22240;&#39640;&#26114;&#25104;&#26412;&#65292;&#24320;&#21457;&#20998;&#26512;&#26041;&#27861;&#20197;&#25913;&#21892;&#24515;&#33039;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#38750;&#32447;&#24615;&#24494;&#20998;&#26041;&#31243;&#23558;&#24515;&#33039;&#30005;&#27963;&#21160;&#24314;&#27169;&#65292;&#24182;&#30740;&#31350;&#36215;&#28304;&#20110;&#30830;&#23450;&#24615;&#21160;&#24577;&#30340;&#24515;&#33039;&#39057;&#35889;&#30340;&#21464;&#21270;&#12290;&#23558;&#26102;&#38388;&#24207;&#21015;&#24515;&#30005;&#22270;(ECG)&#22270;&#20687;&#25552;&#21462;&#30456;&#31354;&#38388;&#36712;&#36857;&#65292;&#24182;&#22522;&#20110; MIT-BIH &#25968;&#25454;&#24211;&#20013;&#35760;&#24405;&#30340; 44 &#20010; MLII &#22270;&#20687;&#24212;&#29992;&#30456;&#31354;&#38388;&#20998;&#26512;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#26041;&#27861;&#36827;&#34892;&#22788;&#29702;&#12290;&#20026;&#20102;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#23558;&#30456;&#31354;&#38388;&#35760;&#24405;&#30340;&#26368;&#39640; Q-R &#36317;&#31163;&#20043;&#38388;&#30011;&#19968;&#26465;&#30452;&#32447;&#12290;&#23545;&#30456;&#31354;&#38388;&#22270;&#20687;&#35757;&#32451;&#20108;&#36827;&#21046; CNN &#20998;&#31867;&#27169;&#22411;&#20197;&#20998;&#31867;&#27491;&#24120;&#21644;&#24322;&#24120;&#24515;&#33039;&#65292;&#35813;&#26041;&#27861;&#36798;&#21040;&#24179;&#22343;&#20934;&#30830;&#29575; 93.3%&#65292;&#35777;&#26126;&#20102;&#30456;&#31354;&#38388;&#20998;&#26512;&#21644; CNN &#22343;&#22312;&#24515;&#33039;&#30142;&#30149;&#35786;&#26029;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cardiac diseases are one of the leading mortality factors in modern, industrialized societies, which cause high expenses in public health systems. Due to high costs, developing analytical methods to improve cardiac diagnostics is essential. The heart's electric activity was first modeled using a set of nonlinear differential equations. Following this, variations of cardiac spectra originating from deterministic dynamics are investigated. Analyzing a normal human heart's power spectra offers His-Purkinje network, which possesses a fractal-like structure. Phase space trajectories are extracted from the time series electrocardiogram (ECG) graph with third-order derivate Taylor Series. Here in this study, phase space analysis and Convolutional Neural Networks (CNNs) method are applied to 44 records via the MIT-BIH database recorded with MLII. In order to increase accuracy, a straight line is drawn between the highest Q-R distance in the phase space images of the records. Binary CNN classif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#25991;&#26723;&#29702;&#35299;&#27169;&#22411;&#65292;&#37319;&#29992;&#36328;&#19977;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#25513;&#30721;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#32467;&#26500;&#28789;&#27963;&#36866;&#24212;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#36755;&#20986;&#26684;&#24335;&#12290;&#27169;&#22411;&#37319;&#29992;&#22810;&#31181;&#20219;&#21153;&#21516;&#26102;&#39044;&#35757;&#32451;&#65292;&#32780;&#19988;&#32467;&#21512;&#20998;&#35299;&#27880;&#24847;&#21147;&#21644;&#27169;&#24577;&#19987;&#23478;&#32452;&#21512;&#31574;&#30053;&#20197;&#25552;&#39640;&#20449;&#24687;&#25429;&#33719;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.10448</link><description>&lt;p&gt;
&#38754;&#21521;&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;&#30340;&#32479;&#19968;&#27169;&#24577;&#25513;&#30721;&#24207;&#21015;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Sequence-to-Sequence Pre-training with Unified Modality Masking for Visual Document Understanding. (arXiv:2305.10448v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#25991;&#26723;&#29702;&#35299;&#27169;&#22411;&#65292;&#37319;&#29992;&#36328;&#19977;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#25513;&#30721;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#32467;&#26500;&#28789;&#27963;&#36866;&#24212;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#36755;&#20986;&#26684;&#24335;&#12290;&#27169;&#22411;&#37319;&#29992;&#22810;&#31181;&#20219;&#21153;&#21516;&#26102;&#39044;&#35757;&#32451;&#65292;&#32780;&#19988;&#32467;&#21512;&#20998;&#35299;&#27880;&#24847;&#21147;&#21644;&#27169;&#24577;&#19987;&#23478;&#32452;&#21512;&#31574;&#30053;&#20197;&#25552;&#39640;&#20449;&#24687;&#25429;&#33719;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GenDoc&#65292;&#19968;&#31181;&#36890;&#29992;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#25991;&#26723;&#29702;&#35299;&#27169;&#22411;&#65292;&#20351;&#29992;&#36328;&#19977;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#25513;&#30721;&#36827;&#34892;&#39044;&#35757;&#32451;&#65306;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#24067;&#23616;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#19982;&#25991;&#26723;&#29702;&#35299;&#20013;&#24120;&#29992;&#30340;&#20165;&#32534;&#30721;&#22120;&#27169;&#22411;&#30456;&#27604;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#36866;&#24212;&#21508;&#31181;&#20135;&#29983;&#19981;&#21516;&#36755;&#20986;&#26684;&#24335;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#19981;&#20165;&#21253;&#25324;&#20197;&#24448;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#20256;&#32479;&#25991;&#26412;&#22635;&#20805;&#20219;&#21153;&#65292;&#36824;&#21253;&#25324;&#23631;&#34109;&#30340;&#22270;&#20687;&#20196;&#29260;&#39044;&#27979;&#21644;&#23631;&#34109;&#30340;&#24067;&#23616;&#39044;&#27979;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#27169;&#24577;&#29305;&#23450;&#30340;&#25351;&#23548;&#21644;&#37319;&#29992;&#20998;&#35299;&#27880;&#24847;&#21147;&#21644;&#27169;&#24577;&#19987;&#23478;&#32452;&#21512;&#31574;&#30053;&#65292;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#27599;&#31181;&#27169;&#24577;&#25152;&#21033;&#29992;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents GenDoc, a general sequence-to-sequence document understanding model pre-trained with unified masking across three modalities: text, image, and layout. The proposed model utilizes an encoder-decoder architecture, which allows for increased adaptability to a wide range of downstream tasks with diverse output formats, in contrast to the encoder-only models commonly employed in document understanding. In addition to the traditional text infilling task used in previous encoder-decoder models, our pre-training extends to include tasks of masked image token prediction and masked layout prediction. We also design modality-specific instruction and adopt both disentangled attention and the mixture-of-modality-experts strategy to effectively capture the information leveraged by each modality. Evaluation of the proposed model through extensive experiments on several downstream tasks in document understanding demonstrates its ability to achieve superior or competitive performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25439;&#22833;&#20989;&#25968;&#65292;&#24110;&#21161;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#21270;&#35780;&#20998;&#31995;&#32479;&#22312;&#39044;&#27979;&#20540;&#30340;&#21516;&#26102;&#23545;&#27491;&#30830;&#30340;&#20998;&#24067;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#32780;&#19981;&#29306;&#29298;&#20219;&#20309;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10447</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#21270;&#35780;&#20998;&#20013;&#21160;&#24577;&#25439;&#22833;&#20989;&#25968;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Effectiveness of a Dynamic Loss Function in Neural Network Based Automated Essay Scoring. (arXiv:2305.10447v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25439;&#22833;&#20989;&#25968;&#65292;&#24110;&#21161;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#21270;&#35780;&#20998;&#31995;&#32479;&#22312;&#39044;&#27979;&#20540;&#30340;&#21516;&#26102;&#23545;&#27491;&#30830;&#30340;&#20998;&#24067;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#32780;&#19981;&#29306;&#29298;&#20219;&#20309;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21644;&#29305;&#21035;&#26159;&#27880;&#24847;&#21147;&#26426;&#21046;&#20026;&#33258;&#21160;&#21270;&#20316;&#25991;&#35780;&#20998;&#39046;&#22495;&#24102;&#26469;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#35768;&#22810;&#36825;&#20123;&#31995;&#32479;&#20351;&#29992;&#22522;&#20110;&#22238;&#24402;&#30340;&#27169;&#22411;&#65292;&#24403;&#27169;&#22411;&#21482;&#39044;&#27979;&#35757;&#32451;&#25968;&#25454;&#30340;&#24179;&#22343;&#20540;&#26102;&#65292;&#21487;&#33021;&#20250;&#23481;&#26131;&#20986;&#29616;&#27424;&#25311;&#21512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21160;&#24577;&#25439;&#22833;&#20989;&#25968;&#65292;&#23427;&#20026;&#27169;&#22411;&#21019;&#24314;&#20102;&#19968;&#20010;&#28608;&#21169;&#65292;&#20351;&#20854;&#39044;&#27979;&#27491;&#30830;&#30340;&#20998;&#24067;&#65292;&#21516;&#26102;&#39044;&#27979;&#27491;&#30830;&#30340;&#20540;&#12290;&#25105;&#20204;&#30340;&#25439;&#22833;&#20989;&#25968;&#22312;&#19981;&#38477;&#20302;&#20219;&#20309;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36825;&#20010;&#30446;&#26631;&#65292;&#22312;Automated Student Assessment Prize Automated Essay Scoring&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;0.752&#30340;&#20108;&#27425;&#21152;&#26435;kappa&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks and in particular the attention mechanism have brought significant advances to the field of Automated Essay Scoring. Many of these systems use a regression-based model which may be prone to underfitting when the model only predicts the mean of the training data. In this paper, we present a dynamic loss function that creates an incentive for the model to predict with the correct distribution, as well as predicting the correct values. Our loss function achieves this goal without sacrificing any performance achieving a Quadratic Weighted Kappa score of 0.752 on the Automated Student Assessment Prize Automated Essay Scoring dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#24773;&#32490;&#35843;&#33410;&#25351;&#23548;&#30340;&#21465;&#36848;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#27861;&#35821;&#24773;&#24863;&#21465;&#36848;&#35821;&#26009;&#24211;&#65292;&#30740;&#31350;&#20102;&#22235;&#20010;&#32452;&#25104;&#37096;&#20998;&#65288;&#34892;&#20026;&#12289;&#24863;&#35273;&#12289;&#24605;&#32771;&#21644;&#39046;&#22495;&#65289;&#23545;&#24773;&#24863;&#20998;&#31867;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20849;&#21516;&#32771;&#34385;&#25152;&#26377;&#32452;&#25104;&#37096;&#20998;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10446</link><description>&lt;p&gt;
&#22522;&#20110;&#24515;&#29702;&#32452;&#25104;&#37096;&#20998;&#30340;&#24773;&#24863;&#35782;&#21035;&#8212;&#8212;&#22522;&#20110;&#24773;&#32490;&#35843;&#33410;&#25351;&#23548;&#21465;&#36848;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Emotion Recognition based on Psychological Components in Guided Narratives for Emotion Regulation. (arXiv:2305.10446v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#24773;&#32490;&#35843;&#33410;&#25351;&#23548;&#30340;&#21465;&#36848;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#27861;&#35821;&#24773;&#24863;&#21465;&#36848;&#35821;&#26009;&#24211;&#65292;&#30740;&#31350;&#20102;&#22235;&#20010;&#32452;&#25104;&#37096;&#20998;&#65288;&#34892;&#20026;&#12289;&#24863;&#35273;&#12289;&#24605;&#32771;&#21644;&#39046;&#22495;&#65289;&#23545;&#24773;&#24863;&#20998;&#31867;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20849;&#21516;&#32771;&#34385;&#25152;&#26377;&#32452;&#25104;&#37096;&#20998;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#35843;&#33410;&#26159;&#22788;&#29702;&#24773;&#24863;&#20107;&#20214;&#30340;&#20851;&#38190;&#20803;&#32032;&#65292;&#24182;&#23545;&#24515;&#29702;&#20581;&#24247;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#27861;&#35821;&#24773;&#24863;&#21465;&#36848;&#35821;&#26009;&#24211;&#65292;&#35813;&#35821;&#26009;&#24211;&#26159;&#20351;&#29992;&#24773;&#32490;&#35843;&#33410;&#38382;&#21367;&#25910;&#38598;&#32780;&#26469;&#30340;&#65292;&#20197;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#24773;&#24863;&#20107;&#20214;&#29702;&#35299;&#12290;&#25105;&#20204;&#36981;&#24490;&#32452;&#25104;&#36807;&#31243;&#27169;&#22411;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#23558;&#24773;&#32490;&#35270;&#20026;&#30001;&#22235;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#32452;&#25104;&#37096;&#20998;&#65288;&#34892;&#20026;&#12289;&#24863;&#35273;&#12289;&#24605;&#32771;&#21644;&#39046;&#22495;&#65289;&#32452;&#25104;&#30340;&#21160;&#24577;&#36807;&#31243;&#12290;&#27599;&#20010;&#21465;&#36848;&#37117;&#19982;&#19968;&#31181;&#31163;&#25955;&#24773;&#24863;&#30456;&#20851;&#65292;&#24182;&#26681;&#25454;&#20316;&#32773;&#30340;&#25152;&#26377;&#24773;&#24863;&#32452;&#25104;&#37096;&#20998;&#26500;&#24314;&#32467;&#26500;&#12290;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#32452;&#25104;&#37096;&#20998;&#20043;&#38388;&#30340;&#20132;&#20114;&#21450;&#20854;&#23545;&#24773;&#24863;&#20998;&#31867;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#37117;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#19988;&#20849;&#21516;&#32771;&#34385;&#25152;&#26377;&#32452;&#25104;&#37096;&#20998;&#21487;&#20197;&#23454;&#29616;&#26368;&#20339;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#26174;&#31034;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#24773;&#24863;&#20998;&#31867;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion regulation is a crucial element in dealing with emotional events and has positive effects on mental health. This paper aims to provide a more comprehensive understanding of emotional events by introducing a new French corpus of emotional narratives collected using a questionnaire for emotion regulation. We follow the theoretical framework of the Component Process Model which considers emotions as dynamic processes composed of four interrelated components (behavior, feeling, thinking and territory). Each narrative is related to a discrete emotion and is structured based on all emotion components by the writers. We study the interaction of components and their impact on emotion classification with machine learning methods and pre-trained language models. Our results show that each component improves prediction performance, and that the best results are achieved by jointly considering all components. Our results also show the effectiveness of pre-trained language models in predict
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20351;&#29992;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#31216;&#21152;&#23494;&#30340;&#31639;&#27861;&#65288;SELM&#65289;&#65292;&#20854;&#20013;&#31639;&#27861;&#21487;&#20197;&#23558;&#20219;&#24847;&#25968;&#25454;&#32534;&#30721;&#20026;&#32039;&#20945;&#30340;&#23454;&#20540;&#21521;&#37327;&#65288;&#21363;&#21152;&#23494;&#65289;&#65292;&#28982;&#21518;&#36890;&#36807;&#38543;&#26426;&#23376;&#31354;&#38388;&#20248;&#21270;&#21644;&#36138;&#24515;&#35299;&#30721;&#23558;&#21521;&#37327;&#26080;&#25439;&#35299;&#30721;&#20026;&#21407;&#22987;&#28040;&#24687;&#65288;&#21363;&#35299;&#23494;&#65289;&#65292;&#24182;&#19988;SELM&#22312;&#21152;&#23494;&#20998;&#26512;&#26041;&#38754;&#30340;&#23433;&#20840;&#24615;&#33021;&#36739;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.10445</link><description>&lt;p&gt;
&#35760;&#24518;&#26377;&#30410;&#65306;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#21152;&#23494;
&lt;/p&gt;
&lt;p&gt;
Memorization for Good: Encryption with Autoregressive Language Models. (arXiv:2305.10445v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10445
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20351;&#29992;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#31216;&#21152;&#23494;&#30340;&#31639;&#27861;&#65288;SELM&#65289;&#65292;&#20854;&#20013;&#31639;&#27861;&#21487;&#20197;&#23558;&#20219;&#24847;&#25968;&#25454;&#32534;&#30721;&#20026;&#32039;&#20945;&#30340;&#23454;&#20540;&#21521;&#37327;&#65288;&#21363;&#21152;&#23494;&#65289;&#65292;&#28982;&#21518;&#36890;&#36807;&#38543;&#26426;&#23376;&#31354;&#38388;&#20248;&#21270;&#21644;&#36138;&#24515;&#35299;&#30721;&#23558;&#21521;&#37327;&#26080;&#25439;&#35299;&#30721;&#20026;&#21407;&#22987;&#28040;&#24687;&#65288;&#21363;&#35299;&#23494;&#65289;&#65292;&#24182;&#19988;SELM&#22312;&#21152;&#23494;&#20998;&#26512;&#26041;&#38754;&#30340;&#23433;&#20840;&#24615;&#33021;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#21487;&#20197;&#35760;&#24518;&#21644;&#32972;&#35829;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#12290;&#34429;&#28982;&#36825;&#31181;&#35760;&#24518;&#36890;&#24120;&#34987;&#35748;&#20026;&#20855;&#26377;&#19981;&#33391;&#23646;&#24615;&#65292;&#20363;&#22914;&#36807;&#24230;&#25311;&#21512;&#21644;&#20449;&#24687;&#27844;&#28431;&#65292;&#20294;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#35760;&#24518;&#35270;&#20026;LM&#30340;&#19968;&#31181;&#26410;&#24320;&#21457;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20351;&#29992;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#31216;&#21152;&#23494;&#30340;&#31639;&#27861;&#65288;SELM&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#33258;&#22238;&#24402;LM&#21487;&#20197;&#23558;&#20219;&#24847;&#25968;&#25454;&#32534;&#30721;&#20026;&#32039;&#20945;&#30340;&#23454;&#20540;&#21521;&#37327;&#65288;&#21363;&#21152;&#23494;&#65289;&#65292;&#28982;&#21518;&#36890;&#36807;&#38543;&#26426;&#23376;&#31354;&#38388;&#20248;&#21270;&#21644;&#36138;&#24515;&#35299;&#30721;&#23558;&#21521;&#37327;&#26080;&#25439;&#35299;&#30721;&#20026;&#21407;&#22987;&#28040;&#24687;&#65288;&#21363;&#35299;&#23494;&#65289;&#12290;&#34429;&#28982;SELM&#19981;&#26131;&#21463;&#20256;&#32479;&#21152;&#23494;&#20998;&#26512;&#26041;&#27861;&#25915;&#30772;&#65292;&#20294;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#35777;&#21464;&#20307;&#65292;&#30740;&#31350;&#23427;&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/OSU-NLP-Group/SELM &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over-parameterized neural language models (LMs) can memorize and recite long sequences of training data. While such memorization is normally associated with undesired properties such as overfitting and information leaking, our work casts memorization as an unexplored capability of LMs. We propose the first symmetric encryption algorithm with autoregressive language models (SELM). We show that autoregressive LMs can encode arbitrary data into a compact real-valued vector (i.e., encryption) and then losslessly decode the vector to the original message (i.e., decryption) via random subspace optimization and greedy decoding. While SELM is not amenable to conventional cryptanalysis, we investigate its security through a novel empirical variant of the classic IND-CPA (indistinguishability under chosen-plaintext attack) game. Our code and datasets are available at https://github.com/OSU-NLP-Group/SELM.
&lt;/p&gt;</description></item><item><title>IMAGINATOR&#26159;&#19968;&#20010;&#20351;&#29992;&#22522;&#20110;&#21333;&#35789;&#32423;&#21035;&#22270;&#20687;&#26412;&#20307;&#30340;&#39044;&#35757;&#32451;&#22270;&#20687;+&#25991;&#26412;&#32852;&#21512;&#23884;&#20837;&#65292;&#33021;&#23558;&#22810;&#27169;&#24577;&#25968;&#25454;&#32534;&#30721;&#20026;&#30690;&#37327;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.10438</link><description>&lt;p&gt;
IMAGINATOR&#65306;&#20351;&#29992;&#22522;&#20110;&#21333;&#35789;&#32423;&#21035;&#22270;&#20687;&#26412;&#20307;&#30340;&#39044;&#35757;&#32451;&#22270;&#20687;+&#25991;&#26412;&#32852;&#21512;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
IMAGINATOR: Pre-Trained Image+Text Joint Embeddings using Word-Level Grounding of Images. (arXiv:2305.10438v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10438
&lt;/p&gt;
&lt;p&gt;
IMAGINATOR&#26159;&#19968;&#20010;&#20351;&#29992;&#22522;&#20110;&#21333;&#35789;&#32423;&#21035;&#22270;&#20687;&#26412;&#20307;&#30340;&#39044;&#35757;&#32451;&#22270;&#20687;+&#25991;&#26412;&#32852;&#21512;&#23884;&#20837;&#65292;&#33021;&#23558;&#22810;&#27169;&#24577;&#25968;&#25454;&#32534;&#30721;&#20026;&#30690;&#37327;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#35789;&#23884;&#20837;&#26159;&#19968;&#31181;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#21333;&#35789;&#21521;&#37327;&#34920;&#31034;&#65292;&#20027;&#35201;&#21463;&#21040;&#20998;&#24067;&#20551;&#35774;&#8220;&#20320;&#24212;&#35813;&#36890;&#36807;&#23427;&#30340;&#20276;&#20387;&#26469;&#35748;&#35782;&#19968;&#20010;&#21333;&#35789;&#8221;&#65288;Harris&#65292;1954&#65289;&#30340;&#24433;&#21709;&#65292;&#32780;&#29616;&#20195;&#22522;&#20110;&#39044;&#27979;&#30340;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#21017;&#20381;&#36182;&#20110;&#35774;&#35745;&#36873;&#25321;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#12290;&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;IMAGINATOR&#30340;&#39044;&#35757;&#32451;&#32852;&#21512;&#23884;&#20837;&#65288;JE&#65289;&#65292;&#23427;&#26159;&#22312;1M&#20010;&#22270;&#20687;+&#25991;&#26412;&#23545;&#20013;&#20174;21K&#20010;&#19981;&#21516;&#30340;&#22270;&#20687;&#23545;&#35937;&#32423;&#21035;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;JE&#26159;&#19968;&#31181;&#23558;&#22810;&#27169;&#24577;&#25968;&#25454;&#32534;&#30721;&#20026;&#30690;&#37327;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#25991;&#26412;&#27169;&#24577;&#20316;&#20026;&#22522;&#30784;&#20851;&#38190;&#35789;&#65292;&#32780;&#34917;&#20805;&#27169;&#24577;&#65288;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20026;&#22270;&#20687;&#65289;&#21017;&#19982;&#20043;&#30456;&#36830;&#12290;
&lt;/p&gt;
&lt;p&gt;
Word embeddings, i.e., semantically meaningful vector representation of words, are largely influenced by the distributional hypothesis "You shall know a word by the company it keeps" (Harris, 1954), whereas modern prediction-based neural network embeddings rely on design choices and hyperparameter optimization. Word embeddings like Word2Vec, GloVe etc. well capture the contextuality and real-world analogies but contemporary convolution-based image embeddings such as VGGNet, AlexNet, etc. do not capture contextual knowledge. The popular king-queen analogy does not hold true for most commonly used vision embeddings.  In this paper, we introduce a pre-trained joint embedding (JE), named IMAGINATOR, trained on 21K distinct image objects level from 1M image+text pairs. JE is a way to encode multimodal data into a vector space where the text modality serves as the ground-ing key, which the complementary modality (in this case, the image) is anchored with. IMAGINATOR encapsulates three indivi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21475;&#22836;&#21644;&#35270;&#35273;&#25552;&#31034;&#30340;&#20851;&#38190;&#35789;&#35760;&#24518;&#27861;&#12290;&#36890;&#36807;&#20154;&#31867;&#21442;&#19982;&#32773;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#38378;&#21345;&#65292;&#24182;&#21487;&#19982;&#25163;&#21160;&#21046;&#20316;&#30340;&#20851;&#38190;&#35789;&#35760;&#24518;&#27861;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2305.10436</link><description>&lt;p&gt;
&#26234;&#33021;&#25163;&#26426;&#65306;&#25506;&#32034;&#33258;&#21160;&#29983;&#25104;&#35821;&#38899;&#21644;&#35270;&#35273;&#25552;&#31034;&#30340;&#20851;&#38190;&#35789;&#35760;&#24518;&#27861;
&lt;/p&gt;
&lt;p&gt;
SmartPhone: Exploring Keyword Mnemonic with Auto-generated Verbal and Visual Cues. (arXiv:2305.10436v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21475;&#22836;&#21644;&#35270;&#35273;&#25552;&#31034;&#30340;&#20851;&#38190;&#35789;&#35760;&#24518;&#27861;&#12290;&#36890;&#36807;&#20154;&#31867;&#21442;&#19982;&#32773;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#38378;&#21345;&#65292;&#24182;&#21487;&#19982;&#25163;&#21160;&#21046;&#20316;&#30340;&#20851;&#38190;&#35789;&#35760;&#24518;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31532;&#20108;&#35821;&#35328;&#30340;&#35789;&#27719;&#23398;&#20064;&#20013;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#23398;&#20064;&#30028;&#38754;&#25110;&#21046;&#23450;&#20010;&#24615;&#21270;&#30340;&#22797;&#20064;&#32451;&#20064;&#20197;&#26368;&#22823;&#21270;&#35760;&#24518;&#20445;&#30041;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#20869;&#23481;&#65292;&#21363;&#38378;&#21345;&#19978;&#21576;&#29616;&#30340;&#20449;&#24687;&#65292;&#22823;&#22810;&#25968;&#37117;&#20445;&#25345;&#19981;&#21464;&#12290;&#20851;&#38190;&#35789;&#35760;&#24518;&#27861;&#26159;&#19968;&#31181;&#26174;&#33879;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#21457;&#38899;&#30456;&#20284;&#30340;&#20851;&#38190;&#35789;&#26500;&#24314;&#22768;&#38899;&#21644;&#24819;&#35937;&#30340;&#32852;&#31995;&#26469;&#23558;&#26032;&#35789;&#27719;&#19982;&#29616;&#26377;&#30693;&#35782;&#32852;&#31995;&#36215;&#26469;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20419;&#36827;&#24314;&#31435;&#36825;&#20123;&#32852;&#31995;&#65292;&#29983;&#25104;&#19982;&#20851;&#38190;&#35789;&#30456;&#20851;&#30340;&#21475;&#22836;&#21644;&#35270;&#35273;&#25552;&#31034;&#38656;&#35201;&#25163;&#21160;&#36827;&#34892;&#65292;&#36825;&#19981;&#21487;&#25193;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#20851;&#38190;&#35789;&#35760;&#24518;&#27861;&#21475;&#22836;&#21644;&#35270;&#35273;&#25552;&#31034;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#33258;&#21160;&#29983;&#25104;&#21475;&#22836;&#21644;&#35270;&#35273;&#25552;&#31034;&#30340;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#39640;&#24230;&#23481;&#26131;&#35760;&#24518;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#20256;&#32479;&#38378;&#21345;&#21644;&#25163;&#21160;&#21046;&#20316;&#30340;&#20851;&#38190;&#35789;&#35760;&#24518;&#27861;&#30340;&#27604;&#36739;&#65292;&#36890;&#36807;&#20154;&#31867;&#21442;&#19982;&#32773;&#23454;&#39564;&#26469;&#30740;&#31350;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#38378;&#21345;&#65292;&#24182;&#21487;&#19982;&#25163;&#21160;&#21046;&#20316;&#30340;&#20851;&#38190;&#35789;&#35760;&#24518;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
In second language vocabulary learning, existing works have primarily focused on either the learning interface or scheduling personalized retrieval practices to maximize memory retention. However, the learning content, i.e., the information presented on flashcards, has mostly remained constant. Keyword mnemonic is a notable learning strategy that relates new vocabulary to existing knowledge by building an acoustic and imagery link using a keyword that sounds alike. Beyond that, producing verbal and visual cues associated with the keyword to facilitate building these links requires a manual process and is not scalable. In this paper, we explore an opportunity to use large language models to automatically generate verbal and visual cues for keyword mnemonics. Our approach, an end-to-end pipeline for auto-generating verbal and visual cues, can automatically generate highly memorable cues. We investigate the effectiveness of our approach via a human participant experiment by comparing it w
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#26159;&#19968;&#31181;&#22522;&#20110;&#21464;&#24418;&#22120;&#26550;&#26500;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#19988;&#26377;&#25928;&#22320;&#36827;&#34892;&#23545;&#35805;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#20294;&#20173;&#38754;&#20020;&#26032;&#20852;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10435</link><description>&lt;p&gt;
&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#65306;&#21551;&#29992;&#25216;&#26415;&#12289;&#28508;&#22312;&#24212;&#29992;&#12289;&#26032;&#20852;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions. (arXiv:2305.10435v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10435
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#26159;&#19968;&#31181;&#22522;&#20110;&#21464;&#24418;&#22120;&#26550;&#26500;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#19988;&#26377;&#25928;&#22320;&#36827;&#34892;&#23545;&#35805;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#20294;&#20173;&#38754;&#20020;&#26032;&#20852;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#27169;&#22411;&#20195;&#34920;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#19968;&#39033;&#37325;&#22823;&#31361;&#30772;&#65292;&#23558;&#25105;&#20204;&#25512;&#21521;&#24320;&#21457;&#33021;&#22815;&#20687;&#20154;&#31867;&#19968;&#26679;&#29702;&#35299;&#21644;&#20351;&#29992;&#35821;&#35328;&#36827;&#34892;&#20132;&#27969;&#30340;&#26426;&#22120;&#12290;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#27169;&#22411;&#22522;&#20110;&#21464;&#24418;&#22120;&#26550;&#26500;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#30001;&#20110;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#19988;&#33021;&#22815;&#26377;&#25928;&#22320;&#36827;&#34892;&#23545;&#35805;&#65292;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#27169;&#22411;&#22312;&#30740;&#31350;&#20154;&#21592;&#21644;&#24037;&#19994;&#30028;&#31038;&#21306;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#30693;&#21517;&#24230;&#65292;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21450;&#30456;&#20851;&#39046;&#22495;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#21644;&#26377;&#25928;&#30340;&#27169;&#22411;&#20043;&#19968;&#65292;&#36825;&#20419;&#20351;&#36827;&#34892;&#20102;&#26412;&#32508;&#36848;&#12290;&#26412;&#32508;&#36848;&#35814;&#32454;&#20171;&#32461;&#20102;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#65292;&#21253;&#25324;&#20854;&#26550;&#26500;&#12289;&#24037;&#20316;&#36807;&#31243;&#12289;&#35757;&#32451;&#36807;&#31243;&#12289;&#21551;&#29992;&#25216;&#26415;&#20197;&#21450;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#26412;&#32508;&#36848;&#36824;&#35752;&#35770;&#20102;&#35813;&#27169;&#22411;&#38754;&#20020;&#30340;&#26032;&#20852;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#21487;&#33021;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Generative Pre-trained Transformer models represent a notable breakthrough in the domain of natural language processing, which is propelling us toward the development of machines that can understand and communicate using language in a manner that closely resembles that of humans. Generative Pre-trained Transformer models are based on the transformer architecture, a deep neural network designed for natural language processing tasks. Due to their impressive performance on natural language processing tasks and ability to effectively converse, Generative Pre-trained Transformer models have gained significant popularity among researchers and industrial communities, making them one of the most widely used and effective models in natural language processing and related fields, which motivated to conduct this review. This review provides a detailed overview of the Generative Pre-trained Transformer, including its architecture, working process, training procedures, enabling technologies, an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22914;CLIP&#26469;&#26816;&#27979;&#25991;&#26412;&#30340;&#35270;&#35273;&#24615;&#65292;&#24182;&#25552;&#20986;fine-tuning&#31574;&#30053;&#65292;&#23558;&#38750;&#35270;&#35273;&#25991;&#26412;&#26144;&#23556;&#20026;NULL&#22270;&#20687;&#65292;&#21305;&#37197;&#35270;&#35273;&#25991;&#26412;&#19982;&#23545;&#24212;&#22270;&#20687;&#65292;&#20197;&#35299;&#38145;&#22312;&#25991;&#26412;&#20013;&#23884;&#20837;&#30456;&#20851;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.10434</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#25991;&#26412;&#30340;&#35270;&#35273;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning the Visualness of Text Using Large Vision-Language Models. (arXiv:2305.10434v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10434
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22914;CLIP&#26469;&#26816;&#27979;&#25991;&#26412;&#30340;&#35270;&#35273;&#24615;&#65292;&#24182;&#25552;&#20986;fine-tuning&#31574;&#30053;&#65292;&#23558;&#38750;&#35270;&#35273;&#25991;&#26412;&#26144;&#23556;&#20026;NULL&#22270;&#20687;&#65292;&#21305;&#37197;&#35270;&#35273;&#25991;&#26412;&#19982;&#23545;&#24212;&#22270;&#20687;&#65292;&#20197;&#35299;&#38145;&#22312;&#25991;&#26412;&#20013;&#23884;&#20837;&#30456;&#20851;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25991;&#26412;&#20250;&#22312;&#20154;&#20204;&#30340;&#33041;&#28023;&#20013;&#21576;&#29616;&#22270;&#20687;&#65292;&#32780;&#38750;&#35270;&#35273;&#25991;&#26412;&#21017;&#26080;&#27861;&#36798;&#21040;&#27492;&#25928;&#26524;&#12290;&#33258;&#21160;&#26816;&#27979;&#25991;&#26412;&#30340;&#35270;&#35273;&#24615;&#23558;&#26377;&#21161;&#20110;&#22312;&#25991;&#26412;&#20013;&#23884;&#20837;&#30456;&#20851;&#22270;&#20687;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;3620&#20010;&#33521;&#35821;&#21477;&#23376;&#21450;&#20854;&#22810;&#20010;&#20154;&#31867;&#27880;&#37322;&#32773;&#25552;&#20379;&#30340;&#35270;&#35273;&#24615;&#24471;&#20998;&#65292;&#24182;&#20351;&#29992;&#21253;&#21547;&#25991;&#26412;&#21644;&#35270;&#35273;&#36164;&#20135;&#30340;&#25991;&#26723;&#26469;&#21019;&#24314;&#36828;&#31243;&#30417;&#30563;&#35821;&#26009;&#24211;&#65292;&#20197;&#35780;&#20272;&#25991;&#26412;&#30340;&#35270;&#35273;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual text evokes an image in a person's mind, while non-visual text fails to do so. A method to automatically detect visualness in text will unlock the ability to augment text with relevant images, as neural text-to-image generation and retrieval models operate on the implicit assumption that the input text is visual in nature. We curate a dataset of 3,620 English sentences and their visualness scores provided by multiple human annotators. Additionally, we use documents that contain text and visual assets to create a distantly supervised corpus of document text and associated images. We also propose a fine-tuning strategy that adapts large vision-language models like CLIP that assume a one-to-one correspondence between text and image to the task of scoring text visualness from text input alone. Our strategy involves modifying the model's contrastive learning objective to map text identified as non-visual to a common NULL image while matching visual text to their corresponding images 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27602;&#24615;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#21453;&#39304;&#24490;&#29615;&#25552;&#39640;&#27602;&#24615;&#25968;&#25454;&#38598;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#25351;&#26631;&#24179;&#34913;&#24615;&#33021;&#21644;&#27602;&#24615;&#36991;&#20813;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.10433</link><description>&lt;p&gt;
&#27602;&#24615;&#26816;&#27979;&#19982;&#21453;&#39304;&#30340;&#25968;&#25454;&#30495;&#23454;&#24615;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Toxicity Inspector: A Framework to Evaluate Ground Truth in Toxicity Detection Through Feedback. (arXiv:2305.10433v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27602;&#24615;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#21453;&#39304;&#24490;&#29615;&#25552;&#39640;&#27602;&#24615;&#25968;&#25454;&#38598;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#25351;&#26631;&#24179;&#34913;&#24615;&#33021;&#21644;&#27602;&#24615;&#36991;&#20813;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27602;&#24615;&#35821;&#35328;&#30340;&#23450;&#20041;&#24182;&#19981;&#26126;&#30830;&#65292;&#22240;&#20026;&#20854;&#23384;&#22312;&#35768;&#22810;&#21464;&#20307;&#21644;&#24863;&#30693;&#24046;&#24322;&#12290;&#20854;&#39640;&#24230;&#30340;&#24773;&#26223;&#20381;&#36182;&#24615;&#21644;&#20027;&#35266;&#24615;&#35299;&#37322;&#22686;&#21152;&#20102;&#26816;&#27979;&#27602;&#24615;&#35821;&#35328;&#30340;&#25361;&#25112;&#65292;&#21487;&#33021;&#20250;&#38477;&#20302;&#25968;&#25454;&#38598;&#30340;&#21487;&#38752;&#24615;&#24182;&#23545;&#26816;&#27979;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27602;&#24615;&#26816;&#27979;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#20154;&#24037;&#21442;&#19982;&#30340;&#27969;&#31243;&#65292;&#26088;&#22312;&#36890;&#36807;&#36845;&#20195;&#21453;&#39304;&#24490;&#29615;&#26469;&#25552;&#39640;&#27602;&#24615;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#25351;&#26631;&#65288;&#30828;&#24615;&#21644;&#36719;&#24615;&#65289;&#26469;&#24179;&#34913;&#24615;&#33021;&#21644;&#27602;&#24615;&#36991;&#20813;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Toxic language is difficult to define, as it is not monolithic and has many variations in perceptions of toxicity. This challenge of detecting toxic language is increased by the highly contextual and subjectivity of its interpretation, which can degrade the reliability of datasets and negatively affect detection model performance. To fill this void, this paper introduces a toxicity inspector framework that incorporates a human-in-the-loop pipeline with the aim of enhancing the reliability of toxicity benchmark datasets by centering the evaluator's values through an iterative feedback cycle. The centerpiece of this framework is the iterative feedback process, which is guided by two metric types (hard and soft) that provide evaluators and dataset creators with insightful examination to balance the tradeoff between performance gains and toxicity avoidance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;GPT-4&#36827;&#34892;&#22823;&#35268;&#27169;&#25991;&#26412;&#20998;&#26512;&#65292;&#22312;US AI&#19987;&#21033;&#20013;&#21457;&#29616;&#20844;&#20849;&#20215;&#20540;&#34920;&#36798;&#12290;&#37319;&#29992;&#39640;&#32423;&#24067;&#23572;&#26597;&#35810;&#25910;&#38598;&#20102;154,934&#20010;&#19987;&#21033;&#25991;&#26723;&#65292;&#24182;&#19982;USPTO&#30340;&#23436;&#25972;&#19987;&#21033;&#25991;&#26412;&#21512;&#24182;&#12290;&#24471;&#20986;5.4&#30334;&#19975;&#21477;&#23376;&#30340;&#35821;&#26009;&#24211;&#65292;&#20351;&#29992;&#26694;&#26550;&#20197;&#21450;GPT-4&#25552;&#31034;&#36827;&#34892;&#26631;&#35760;&#21644;&#29702;&#24615;&#21270;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#24456;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2305.10383</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22823;&#35268;&#27169;&#25991;&#26412;&#20998;&#26512;&#65306;&#22312;AI&#19987;&#21033;&#20013;&#21457;&#29616;&#20844;&#20849;&#20215;&#20540;&#34920;&#36798;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Text Analysis Using Generative Language Models: A Case Study in Discovering Public Value Expressions in AI Patents. (arXiv:2305.10383v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;GPT-4&#36827;&#34892;&#22823;&#35268;&#27169;&#25991;&#26412;&#20998;&#26512;&#65292;&#22312;US AI&#19987;&#21033;&#20013;&#21457;&#29616;&#20844;&#20849;&#20215;&#20540;&#34920;&#36798;&#12290;&#37319;&#29992;&#39640;&#32423;&#24067;&#23572;&#26597;&#35810;&#25910;&#38598;&#20102;154,934&#20010;&#19987;&#21033;&#25991;&#26723;&#65292;&#24182;&#19982;USPTO&#30340;&#23436;&#25972;&#19987;&#21033;&#25991;&#26412;&#21512;&#24182;&#12290;&#24471;&#20986;5.4&#30334;&#19975;&#21477;&#23376;&#30340;&#35821;&#26009;&#24211;&#65292;&#20351;&#29992;&#26694;&#26550;&#20197;&#21450;GPT-4&#25552;&#31034;&#36827;&#34892;&#26631;&#35760;&#21644;&#29702;&#24615;&#21270;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#24456;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#35760;&#25968;&#25454;&#23545;&#20110;&#35757;&#32451;&#25991;&#26412;&#20998;&#31867;&#22120;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#21644;&#25277;&#35937;&#30340;&#27010;&#24565;&#32780;&#35328;&#65292;&#20934;&#30830;&#26631;&#35760;&#24120;&#24120;&#24456;&#38590;&#23454;&#29616;&#12290;&#26412;&#25991;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-4&#65289;&#36827;&#34892;&#22823;&#35268;&#27169;&#25991;&#26412;&#20998;&#26512;&#30340;&#26631;&#35760;&#21644;&#29702;&#24615;&#21270;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#22312;&#32654;&#22269;AI&#19987;&#21033;&#20013;&#21457;&#29616;&#20844;&#20849;&#20215;&#20540;&#34920;&#36798;&#30340;&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#20351;&#29992;&#22312;InnovationQ+&#19978;&#25552;&#20132;&#30340;&#39640;&#32423;&#24067;&#23572;&#26597;&#35810;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;154,934&#20010;&#19987;&#21033;&#25991;&#26723;&#30340;&#25968;&#25454;&#24211;&#65292;&#36825;&#20123;&#32467;&#26524;&#19982;&#26469;&#33258;USPTO&#30340;&#23436;&#25972;&#19987;&#21033;&#25991;&#26412;&#21512;&#24182;&#65292;&#24635;&#35745;5.4&#30334;&#19975;&#21477;&#23376;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35782;&#21035;&#21644;&#26631;&#35760;&#36825;&#20123;AI&#19987;&#21033;&#21477;&#23376;&#20013;&#30340;&#20844;&#20849;&#20215;&#20540;&#34920;&#36798;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;GPT-4&#30340;&#25552;&#31034;&#65292;&#20854;&#20013;&#21253;&#25324;&#25991;&#26412;&#20998;&#31867;&#30340;&#23450;&#20041;&#12289;&#25351;&#23548;&#26041;&#38024;&#12289;&#31034;&#20363;&#21644;&#29702;&#24615;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;BLEU&#20998;&#25968;&#21644;&#20027;&#39064;&#24314;&#27169;&#35780;&#20272;&#20102;GPT-4&#29983;&#25104;&#30340;&#26631;&#31614;&#21644;&#29702;&#24615;&#21270;&#30340;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#26159;&#20934;&#30830;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labeling data is essential for training text classifiers but is often difficult to accomplish accurately, especially for complex and abstract concepts. Seeking an improved method, this paper employs a novel approach using a generative language model (GPT-4) to produce labels and rationales for large-scale text analysis. We apply this approach to the task of discovering public value expressions in US AI patents. We collect a database comprising 154,934 patent documents using an advanced Boolean query submitted to InnovationQ+. The results are merged with full patent text from the USPTO, resulting in 5.4 million sentences. We design a framework for identifying and labeling public value expressions in these AI patent sentences. A prompt for GPT-4 is developed which includes definitions, guidelines, examples, and rationales for text classification. We evaluate the quality of the labels and rationales produced by GPT-4 using BLEU scores and topic modeling and find that they are accurate, di
&lt;/p&gt;</description></item><item><title>FACE&#26159;&#19968;&#32452;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#20154;&#31867;&#21644;&#27169;&#22411;&#20043;&#38388;&#24046;&#36317;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#23427;&#22522;&#20110;&#20613;&#37324;&#21494;&#20998;&#26512;&#21644;&#20132;&#21449;&#29109;&#20272;&#35745;&#65292;&#21487;&#20197;&#21453;&#26144;&#27169;&#22411;&#22823;&#23567;&#12289;&#35299;&#30721;&#37319;&#26679;&#26041;&#27861;&#21644;&#20154;&#31867;&#35780;&#20998;&#12290;</title><link>http://arxiv.org/abs/2305.10307</link><description>&lt;p&gt;
FACE: &#20351;&#29992;&#20132;&#21449;&#29109;&#30340;&#20613;&#37324;&#21494;&#20998;&#26512;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy. (arXiv:2305.10307v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10307
&lt;/p&gt;
&lt;p&gt;
FACE&#26159;&#19968;&#32452;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#20154;&#31867;&#21644;&#27169;&#22411;&#20043;&#38388;&#24046;&#36317;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#23427;&#22522;&#20110;&#20613;&#37324;&#21494;&#20998;&#26512;&#21644;&#20132;&#21449;&#29109;&#20272;&#35745;&#65292;&#21487;&#20197;&#21453;&#26144;&#27169;&#22411;&#22823;&#23567;&#12289;&#35299;&#30721;&#37319;&#26679;&#26041;&#27861;&#21644;&#20154;&#31867;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#26426;&#22120;&#29983;&#25104;&#30340;&#35821;&#35328;&#19982;&#20154;&#31867;&#35821;&#35328;&#20043;&#38388;&#30340;&#36317;&#31163;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#21463;&#21040;&#35821;&#35328;&#23398;&#24515;&#29702;&#23398;&#20851;&#20110;&#35821;&#35328;&#29109;&#21608;&#26399;&#24615;&#23454;&#35777;&#21457;&#29616;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FACE&#8212;&#8212;&#19968;&#32452;&#22522;&#20110;&#35821;&#35328;&#20132;&#21449;&#29109;&#30340;&#20613;&#37324;&#21494;&#20998;&#26512;&#30340;&#24230;&#37327;&#65292;&#29992;&#20110;&#34913;&#37327;&#29983;&#25104;&#27169;&#22411;&#20135;&#29983;&#30340;&#35821;&#35328;&#19982;&#20154;&#31867;&#20070;&#20889;&#35821;&#35328;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#12290;&#36890;&#36807;&#19968;&#20010;&#24320;&#25918;&#24335;&#30340;&#29983;&#25104;&#20219;&#21153;&#21644;&#20197;&#21069;&#30740;&#31350;&#30340;&#23454;&#39564;&#25968;&#25454;&#65292;&#25105;&#20204;&#21457;&#29616;FACE&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#20154;&#31867;&#27169;&#22411;&#24046;&#36317;&#65292;&#22312;&#27169;&#22411;&#35268;&#27169;&#19978;&#26377;&#25152;&#32553;&#25918;&#65292;&#21453;&#26144;&#20102;&#19981;&#21516;&#35299;&#30721;&#37319;&#26679;&#26041;&#27861;&#30340;&#32467;&#26524;&#65292;&#19982;&#20854;&#20182;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#31867;&#21028;&#26029;&#20998;&#25968;&#30456;&#20851;&#33391;&#22909;&#12290;FACE&#22312;&#35745;&#31639;&#19978;&#26159;&#39640;&#25928;&#30340;&#65292;&#24182;&#25552;&#20379;&#30452;&#35266;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring the distance between machine-produced and human language is acritical open problem. Inspired by empirical findings from psycholinguistics on theperiodicity of entropy in language, we propose FACE, a set of metrics based onFourier Analysis of the estimated Cross-Entropy of language, for measuring thesimilarity between model-generated and human-written languages. Based on anopen-ended generation task and the experimental data from previous studies, weind that FACE can effectively identify the human-model gap, scales with modelsize, reflects the outcomes of different sampling methods for decoding, correlateswell with other evaluation metrics and with human judgment scores. FACE iscomputationally efficient and provides intuitive interpretations.
&lt;/p&gt;</description></item><item><title>MemoryBank &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#20869;&#23384;&#26426;&#21046;&#65292;&#26088;&#22312;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#31867;&#20154;&#30340;&#38271;&#26399;&#35760;&#24518;&#12290;&#23427;&#21487;&#20197;&#21484;&#21796;&#30456;&#20851;&#35760;&#24518;&#65292;&#36890;&#36807;&#25345;&#32493;&#30340;&#35760;&#24518;&#26356;&#26032;&#19981;&#26029;&#36827;&#21270;&#65292;&#36890;&#36807;&#21512;&#25104;&#36807;&#21435;&#30340;&#20114;&#21160;&#20449;&#24687;&#29702;&#35299;&#24182;&#36866;&#24212;&#29992;&#25143;&#20010;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10250</link><description>&lt;p&gt;
MemoryBank: &#29992;&#38271;&#26399;&#35760;&#24518;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MemoryBank: Enhancing Large Language Models with Long-Term Memory. (arXiv:2305.10250v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10250
&lt;/p&gt;
&lt;p&gt;
MemoryBank &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#20869;&#23384;&#26426;&#21046;&#65292;&#26088;&#22312;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#31867;&#20154;&#30340;&#38271;&#26399;&#35760;&#24518;&#12290;&#23427;&#21487;&#20197;&#21484;&#21796;&#30456;&#20851;&#35760;&#24518;&#65292;&#36890;&#36807;&#25345;&#32493;&#30340;&#35760;&#24518;&#26356;&#26032;&#19981;&#26029;&#36827;&#21270;&#65292;&#36890;&#36807;&#21512;&#25104;&#36807;&#21435;&#30340;&#20114;&#21160;&#20449;&#24687;&#29702;&#35299;&#24182;&#36866;&#24212;&#29992;&#25143;&#20010;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38761;&#21629;&#24615;&#36827;&#23637;&#26497;&#22823;&#22320;&#25913;&#21464;&#20102;&#25105;&#20204;&#19982;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20114;&#21160;&#26041;&#24335;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20854;&#20013;&#19968;&#20010;&#26126;&#26174;&#30340;&#19981;&#36275;&#20043;&#22788;&#26159;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#38271;&#26399;&#35760;&#24518;&#26426;&#21046;&#12290;&#36825;&#22312;&#38656;&#35201;&#25345;&#32493;&#20114;&#21160;&#30340;&#24773;&#20917;&#19979;&#23588;&#20026;&#26126;&#26174;&#65292;&#20363;&#22914;&#20010;&#20154;&#20276;&#20387;&#31995;&#32479;&#21644;&#24515;&#29702;&#21672;&#35810;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MemoryBank&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;LLM&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#20869;&#23384;&#26426;&#21046;&#12290;MemoryBank&#21487;&#20197;&#21484;&#21796;&#30456;&#20851;&#35760;&#24518;&#65292;&#36890;&#36807;&#25345;&#32493;&#30340;&#35760;&#24518;&#26356;&#26032;&#19981;&#26029;&#36827;&#21270;&#65292;&#36890;&#36807;&#21512;&#25104;&#36807;&#21435;&#30340;&#20114;&#21160;&#20449;&#24687;&#29702;&#35299;&#24182;&#36866;&#24212;&#29992;&#25143;&#20010;&#24615;&#12290;&#20026;&#20102;&#27169;&#20223;&#20154;&#31867;&#34892;&#20026;&#24182;&#26377;&#36873;&#25321;&#22320;&#20445;&#23384;&#35760;&#24518;&#65292;MemoryBank&#37319;&#29992;&#20102;&#21463;Ebbinghaus&#36951;&#24536;&#26354;&#32447;&#29702;&#35770;&#21551;&#21457;&#30340;&#35760;&#24518;&#26356;&#26032;&#26426;&#21046;&#65292;&#36825;&#26679;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#26681;&#25454;&#26102;&#38388;&#21644;&#35760;&#24518;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#26469;&#36951;&#24536;&#21644;&#21152;&#24378;&#35760;&#24518;&#65292;&#20174;&#32780;&#20026;LLM&#25552;&#20379;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#38271;&#26399;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
Revolutionary advancements in Large Language Models have drastically reshaped our interactions with artificial intelligence systems. Despite this, a notable hindrance remains-the deficiency of a long-term memory mechanism within these models. This shortfall becomes increasingly evident in situations demanding sustained interaction, such as personal companion systems and psychological counseling. Therefore, we propose MemoryBank, a novel memory mechanism tailored for LLMs. MemoryBank enables the models to summon relevant memories, continually evolve through continuous memory updates, comprehend, and adapt to a user personality by synthesizing information from past interactions. To mimic anthropomorphic behaviors and selectively preserve memory, MemoryBank incorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting Curve theory, which permits the AI to forget and reinforce memory based on time elapsed and the relative significance of the memory, thereby offering a hum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EfficientSCI&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#31264;&#23494;&#36830;&#25509;&#21644;&#26102;&#31354;&#20998;&#35299;&#26426;&#21046;&#26469;&#24314;&#31435;&#35270;&#39057;SCI&#20013;&#30340;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#37325;&#24314;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.10006</link><description>&lt;p&gt;
EfficientSCI: &#31264;&#23494;&#36830;&#25509;&#32593;&#32476;&#19982;&#26102;&#31354;&#20998;&#35299;&#30456;&#32467;&#21512;&#30340;&#22823;&#35268;&#27169;&#35270;&#39057;&#24555;&#29031;&#21387;&#32553;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
EfficientSCI: Densely Connected Network with Space-time Factorization for Large-scale Video Snapshot Compressive Imaging. (arXiv:2305.10006v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EfficientSCI&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#31264;&#23494;&#36830;&#25509;&#21644;&#26102;&#31354;&#20998;&#35299;&#26426;&#21046;&#26469;&#24314;&#31435;&#35270;&#39057;SCI&#20013;&#30340;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#37325;&#24314;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#24555;&#29031;&#21387;&#32553;&#25104;&#20687; (SCI) &#20351;&#29992;&#20108;&#32500;&#26816;&#27979;&#22120;&#22312;&#21333;&#20010;&#26333;&#20809;&#26102;&#38388;&#20869;&#25429;&#33719;&#36830;&#32493;&#35270;&#39057;&#24103;&#12290;&#28982;&#21518;&#38656;&#35201;&#35774;&#35745;&#39640;&#25928;&#30340;&#37325;&#24314;&#31639;&#27861;&#26469;&#37325;&#24314;&#25152;&#38656;&#30340;&#35270;&#39057;&#24103;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#37325;&#24314;&#31639;&#27861;&#24050;&#32463;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#38754;&#20020;&#20197;&#19979;&#25361;&#25112;&#65306;&#36807;&#24230;&#30340;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;GPU&#20869;&#23384;&#38480;&#21046;&#12290;&#20854;&#20013;&#65292;1)&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;2)&#23427;&#20204;&#36890;&#24120;&#19981;&#33021;&#22312;&#39640;&#21387;&#32553;&#27604;&#19979;&#37325;&#24314;&#22823;&#35268;&#27169;&#30340;&#35270;&#39057;&#24103;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35270;&#39057;SCI&#32593;&#32476;&#65292;&#20351;&#29992;&#21333;&#20010;&#27531;&#24046;&#22359;&#20869;&#30340;&#31264;&#23494;&#36830;&#25509;&#21644;&#26102;&#31354;&#20998;&#35299;&#26426;&#21046;&#65292;&#21629;&#21517;&#20026;EfficientSCI&#12290; EfficientSCI&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#22312;&#31354;&#38388;&#22495;&#20013;&#20351;&#29992;&#21367;&#31215;&#21644;&#22312;&#26102;&#38388;&#22495;&#20013;&#20351;&#29992;&#36716;&#25442;&#22495;&#31232;&#30095;&#21270;&#65288;TDS&#65289;&#26469;&#24456;&#22909;&#22320;&#24314;&#31435;&#31354;&#38388; - &#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#36825;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#21644;&#20869;&#23384;&#20351;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;EfficientSCI&#22312;&#21508;&#31181;&#22823;&#35268;&#27169;&#35270;&#39057;SCI&#25968;&#25454;&#38598;&#19978;&#65292;&#22312;&#37325;&#24314;&#36136;&#37327;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#22343;&#20248;&#20110;&#26368;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video snapshot compressive imaging (SCI) uses a two-dimensional detector to capture consecutive video frames during a single exposure time. Following this, an efficient reconstruction algorithm needs to be designed to reconstruct the desired video frames. Although recent deep learning-based state-of-the-art (SOTA) reconstruction algorithms have achieved good results in most tasks, they still face the following challenges due to excessive model complexity and GPU memory limitations:  1) these models need high computational cost, and  2) they are usually unable to reconstruct large-scale video frames at high compression ratios.  To address these issues, we develop an {\bf{\em efficient network}} for video SCI by using {\bf {\em dense connections and space-time factorization mechanism}} within a single residual block, dubbed {\bf \emph{EfficientSCI}}. The EfficientSCI network can well establish spatial-temporal correlation by using {\bf {\em convolution in the spatial domain and Transform
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20197;TGNB&#20154;&#32676;&#30340;&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25552;&#20986;&#20102;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#30340;OLG&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#19988;&#21253;&#25324;&#19968;&#20010;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#21644;&#20998;&#26512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09941</link><description>&lt;p&gt;
&#8220;&#25105;&#20840;&#28982;&#25104;&#20026;&#25105;&#33258;&#24049;&#8221;&#65306;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
"I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation. (arXiv:2305.09941v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20197;TGNB&#20154;&#32676;&#30340;&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25552;&#20986;&#20102;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#30340;OLG&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#19988;&#21253;&#25324;&#19968;&#20010;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#21644;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#24615;&#21035;&#21644;&#38750;&#20108;&#20803;&#65288;TGNB&#65289;&#20154;&#32676;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#32463;&#21382;&#20102;&#19981;&#25104;&#27604;&#20363;&#30340;&#27495;&#35270;&#21644;&#25490;&#26021;&#12290;&#38543;&#30528;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#30340;&#26085;&#30410;&#26222;&#21450;&#21644;&#24212;&#29992;&#65292;&#36827;&#19968;&#27493;&#36793;&#32536;&#21270;&#36825;&#19968;&#20154;&#32676;&#30340;&#21487;&#33021;&#24615;&#20063;&#22312;&#22686;&#21152;&#12290;&#34429;&#28982;&#22823;&#37327;&#30340;NLP&#20844;&#24179;&#25991;&#29486;&#30528;&#37325;&#20110;&#38416;&#26126;&#21644;&#35299;&#20915;&#24615;&#21035;&#20559;&#35265;&#65292;&#20294;&#35780;&#20272;TGNB&#36523;&#20221;&#25152;&#24102;&#26469;&#30340;&#24615;&#21035;&#20260;&#23475;&#38656;&#35201;&#29702;&#35299;&#36825;&#20123;&#36523;&#20221;&#22914;&#20309;&#29420;&#29305;&#22320;&#19982;&#31038;&#20250;&#24615;&#21035;&#35268;&#33539;&#20114;&#21160;&#20197;&#21450;&#19982;&#24615;&#21035;&#20108;&#20803;&#20013;&#24515;&#30340;&#35270;&#35282;&#30456;&#21306;&#20998;&#12290;&#36825;&#26679;&#30340;&#27979;&#37327;&#26694;&#26550;&#26412;&#36136;&#19978;&#38656;&#35201;&#20197;TGNB&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#24110;&#21161;&#25351;&#23548;&#21253;&#23481;&#24615;&#21035;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#35813;&#20026;&#35841;&#26381;&#21153;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20197;TGNB&#31038;&#21306;&#21644;&#29616;&#26377;&#30340;&#36328;&#23398;&#31185;&#25991;&#29486;&#20026;&#22522;&#30784;&#65292;&#35780;&#20272;&#20102;TGNB&#20010;&#20307;&#32463;&#21382;&#36793;&#32536;&#21270;&#25152;&#24418;&#25104;&#30340;&#31038;&#20250;&#29616;&#23454;&#26159;&#22914;&#20309;&#24433;&#21709;&#21644;&#23384;&#22312;&#20110;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#65288;OLG&#65289;&#20013;&#12290;&#39318;&#20808;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;OLG&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#65292;&#24230;&#37327;&#19982;&#35813;&#20154;&#32676;&#30456;&#20851;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#29305;&#21035;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#65292;&#20197;&#21450;&#20132;&#21449;&#20998;&#26512;&#32467;&#26524;&#30340;&#20132;&#21449;&#26041;&#27861;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#26377;&#21161;&#20110;&#23454;&#29616;&#26356;&#20844;&#24179;&#12289;&#26356;&#21253;&#23481;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#65292;&#24182;&#28508;&#22312;&#22320;&#35299;&#20915;NLP&#30740;&#31350;&#20013;&#24191;&#27867;&#30340;&#20132;&#21449;&#36523;&#20221;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization by TGNB persons contributes to and persists within Open Language Generation (OLG). By first understandi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#30340;&#19981;&#21516;&#37319;&#26679;&#31574;&#30053;&#65292;&#24182;&#21457;&#29616;&#20102;epsilon&#37319;&#26679;&#26041;&#24335;&#33021;&#22815;&#20351;&#24471;&#35299;&#30721;&#32467;&#26524;&#26174;&#33879;&#22320;&#20248;&#20110;&#20854;&#20182;&#25152;&#26377;&#24050;&#27979;&#35797;&#30340;&#37319;&#26679;&#26041;&#24335;&#21644;&#26463;&#25628;&#32034;&#35299;&#30721;&#12290;</title><link>http://arxiv.org/abs/2305.09860</link><description>&lt;p&gt;
Epsilon Sampling Rocks: &#30740;&#31350;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#30340;&#37319;&#26679;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Epsilon Sampling Rocks: Investigating Sampling Strategies for \\Minimum Bayes Risk Decoding for Machine Translation. (arXiv:2305.09860v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#30340;&#19981;&#21516;&#37319;&#26679;&#31574;&#30053;&#65292;&#24182;&#21457;&#29616;&#20102;epsilon&#37319;&#26679;&#26041;&#24335;&#33021;&#22815;&#20351;&#24471;&#35299;&#30721;&#32467;&#26524;&#26174;&#33879;&#22320;&#20248;&#20110;&#20854;&#20182;&#25152;&#26377;&#24050;&#27979;&#35797;&#30340;&#37319;&#26679;&#26041;&#24335;&#21644;&#26463;&#25628;&#32034;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#35299;&#30721;&#24050;&#32463;&#26174;&#31034;&#20986;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26367;&#20195;&#26463;&#25628;&#32034;&#35299;&#30721;&#30340;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25928;&#29992;&#20989;&#25968;&#30456;&#32467;&#21512;&#26102;&#12290;&#28982;&#32780;&#65292;MBR&#35299;&#30721;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#20174;&#27169;&#22411;&#20013;&#37319;&#26679;&#30340;&#26041;&#27861;&#21644;&#25968;&#37327;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#29992;&#20110;MBR&#35299;&#30721;&#30340;&#19981;&#21516;&#37319;&#26679;&#26041;&#27861;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#20123;&#27969;&#34892;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#20363;&#22914;&#31062;&#20808;&#37319;&#26679;&#65292;&#26680;&#37319;&#26679;&#21644;top-k&#37319;&#26679;&#12290;&#22522;&#20110;&#25105;&#20204;&#23545;&#23427;&#20204;&#23616;&#38480;&#24615;&#30340;&#35748;&#35782;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;epsilon&#37319;&#26679;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20462;&#21098;&#25152;&#26377;&#23567;&#20110;epsilon&#30340;&#26631;&#35760;&#65292;&#20197;&#30830;&#20445;&#26679;&#26412;&#20013;&#30340;&#27599;&#20010;&#26631;&#35760;&#33719;&#24471;&#20844;&#24179;&#30340;&#27010;&#29575;&#36136;&#37327;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;epsilon&#37319;&#26679;&#30340;MBR&#35299;&#30721;&#26174;&#33879;&#20248;&#20110;&#19981;&#20165;&#26159;&#26463;&#25628;&#32034;&#35299;&#30721;&#65292;&#32780;&#19988;&#36824;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#24050;&#27979;&#35797;&#30340;&#37319;&#26679;&#26041;&#27861;&#30340;MBR&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine translation (MT) have shown that Minimum Bayes Risk (MBR) decoding can be a powerful alternative to beam search decoding, especially when combined with neural-based utility functions. However, the performance of MBR decoding depends heavily on how and how many candidates are sampled from the model. In this paper, we explore how different sampling approaches for generating candidate lists for MBR decoding affect performance. We evaluate popular sampling approaches, such as ancestral, nucleus, and top-k sampling. Based on our insights into their limitations, we experiment with the recently proposed epsilon-sampling approach, which prunes away all tokens with a probability smaller than epsilon, ensuring that each token in a sample receives a fair probability mass. Through extensive human evaluations, we demonstrate that MBR decoding based on epsilon-sampling significantly outperforms not only beam search decoding, but also MBR decoding with all other tested samp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#22312;&#26263;&#32593;&#25968;&#25454;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#8212;&#8212;DarkBERT&#65292;&#23545;&#20110;&#26263;&#32593;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.08596</link><description>&lt;p&gt;
DarkBERT&#65306;&#38024;&#23545;&#20114;&#32852;&#32593;&#40657;&#26263;&#38754;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DarkBERT: A Language Model for the Dark Side of the Internet. (arXiv:2305.08596v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#22312;&#26263;&#32593;&#25968;&#25454;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#8212;&#8212;DarkBERT&#65292;&#23545;&#20110;&#26263;&#32593;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#34920;&#23618;&#32593;&#32476;&#30456;&#27604;&#65292;&#26263;&#32593;&#20013;&#20351;&#29992;&#30340;&#35821;&#35328;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#12290;&#30001;&#20110;&#30740;&#31350;&#26263;&#32593;&#36890;&#24120;&#38656;&#35201;&#23545;&#22495;&#36827;&#34892;&#25991;&#26412;&#20998;&#26512;&#65292;&#22240;&#27492;&#38024;&#23545;&#26263;&#32593;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27934;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DarkBERT&#65292;&#36825;&#26159;&#19968;&#20010;&#39044;&#20808;&#22312;&#26263;&#32593;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#31579;&#36873;&#21644;&#32534;&#35793;&#29992;&#20110;&#35757;&#32451;DarkBERT&#30340;&#25991;&#26412;&#25968;&#25454;&#30340;&#27493;&#39588;&#65292;&#20197;&#24212;&#23545;&#26263;&#32593;&#26497;&#20026;&#19981;&#21516;&#30340;&#35789;&#27719;&#21644;&#32467;&#26500;&#22810;&#26679;&#24615;&#65292;&#36825;&#21487;&#33021;&#26377;&#23475;&#20110;&#26500;&#24314;&#35813;&#22495;&#30340;&#36866;&#24403;&#34920;&#31034;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;DarkBERT&#21450;&#20854;&#22522;&#30784;&#27169;&#22411;&#20197;&#21450;&#20854;&#20182;&#24191;&#27867;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#39564;&#35777;&#38024;&#23545;&#26263;&#8203;&#8203;&#32593;&#30340;&#29305;&#23450;&#27169;&#22411;&#22312;&#21508;&#31181;&#29992;&#20363;&#20013;&#25552;&#20379;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;DarkBERT&#32988;&#36807;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#33021;&#25104;&#20026;&#26410;&#26469;&#26263;&#32593;&#30740;&#31350;&#30340;&#26377;&#20215;&#20540;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has suggested that there are clear differences in the language used in the Dark Web compared to that of the Surface Web. As studies on the Dark Web commonly require textual analysis of the domain, language models specific to the Dark Web may provide valuable insights to researchers. In this work, we introduce DarkBERT, a language model pretrained on Dark Web data. We describe the steps taken to filter and compile the text data used to train DarkBERT to combat the extreme lexical and structural diversity of the Dark Web that may be detrimental to building a proper representation of the domain. We evaluate DarkBERT and its vanilla counterpart along with other widely used language models to validate the benefits that a Dark Web domain specific model offers in various use cases. Our evaluations show that DarkBERT outperforms current language models and may serve as a valuable resource for future research on the Dark Web.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#20559;&#22909;&#26816;&#26597;&#34920;&#65292;&#20197;&#36229;&#36234;&#30456;&#20851;&#20998;&#26512;&#35780;&#20272;NLG&#33258;&#21160;&#25351;&#26631;&#65292;&#24182;&#20998;&#26512;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#25351;&#26631;&#21450;&#20854;&#22312;&#19977;&#20010;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.08566</link><description>&lt;p&gt;
&#36229;&#36234;&#30456;&#20851;&#20998;&#26512;&#30340;NLG&#35780;&#20272;&#25351;&#26631;&#65306;&#19968;&#31181;&#32463;&#39564;&#24230;&#37327;&#20559;&#22909;&#26816;&#26597;&#34920;
&lt;/p&gt;
&lt;p&gt;
NLG Evaluation Metrics Beyond Correlation Analysis: An Empirical Metric Preference Checklist. (arXiv:2305.08566v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#20559;&#22909;&#26816;&#26597;&#34920;&#65292;&#20197;&#36229;&#36234;&#30456;&#20851;&#20998;&#26512;&#35780;&#20272;NLG&#33258;&#21160;&#25351;&#26631;&#65292;&#24182;&#20998;&#26512;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#25351;&#26631;&#21450;&#20854;&#22312;&#19977;&#20010;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;NLG&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#22522;&#20110;&#26159;&#21542;&#23558;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#29992;&#20316;&#19978;&#19979;&#25991;&#25110;&#30446;&#26631;&#26469;&#35745;&#31639;&#25351;&#26631;&#65292;&#20998;&#20026;&#65288;i&#65289;&#20219;&#21153;&#19981;&#21487;&#30693;&#21644;&#65288;ii&#65289;&#19982;&#20154;&#31867;&#23545;&#40784;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24230;&#37327;&#20559;&#22909;&#26816;&#26597;&#34920;&#20316;&#20026;&#35780;&#20272;&#33258;&#21160;&#25351;&#26631;&#22312;&#19977;&#20010;NLG&#20219;&#21153;&#20013;&#30340;&#37492;&#21035;&#21147;&#30340;&#26694;&#26550;&#65306;&#25991;&#26412;&#25688;&#35201;&#65292;&#23545;&#35805;&#21709;&#24212;&#29983;&#25104;&#21644;&#21463;&#25511;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we analyze NLG automatic metrics based on whether human evaluation aspect is used as context or objective to compute the metrics: (i) Task-agnostic and (ii) Human-aligned. Task-agnostic metrics, such as Perplexity, BLEU, BERTScore, are cost-effective and highly adaptable to diverse NLG tasks, yet they have a weak correlation with human. Human-aligned metrics (CTC, CtrlEval, UniEval) improves correlation level by incorporating desirable human-like qualities as training objective. However, their effectiveness at discerning system-level performance and quality of system outputs remains unclear.  We present metric preference checklist as a framework to assess the discriminative power of automatic metrics in three NLG tasks: Text Summarization, Dialogue Response Generation, and Controlled Generation. We show that multi-aspect human-aligned metric (UniEval) is not necessarily dominant over single-aspect human-aligned metrics (CTC, CtrlEval) and task-agnostic metrics (BLEU, BER
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25277;&#35937;&#22810;&#25991;&#26723;&#25688;&#35201;&#30340;&#23618;&#27425;&#32534;&#30721;-&#35299;&#30721;&#26041;&#26696;&#65292;&#22312;&#22810;&#39046;&#22495;&#30340;10&#20010;MDS&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.08503</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#25277;&#35937;&#22810;&#25991;&#26723;&#25688;&#35201;&#30340;&#23618;&#27425;&#32534;&#30721;-&#35299;&#30721;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarization. (arXiv:2305.08503v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25277;&#35937;&#22810;&#25991;&#26723;&#25688;&#35201;&#30340;&#23618;&#27425;&#32534;&#30721;-&#35299;&#30721;&#26041;&#26696;&#65292;&#22312;&#22810;&#39046;&#22495;&#30340;10&#20010;MDS&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;&#25277;&#35937;&#21333;&#25991;&#26723;&#25688;&#35201;&#65288;SDS&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#12290;&#20294;&#26159;&#65292;&#36825;&#31181;&#22909;&#22788;&#21487;&#33021;&#19981;&#20250;&#36731;&#26131;&#25193;&#23637;&#21040;&#22810;&#25991;&#26723;&#25688;&#35201;&#65288;MDS&#65289;&#65292;&#22240;&#20026;&#25991;&#26723;&#20043;&#38388;&#30340;&#20132;&#20114;&#26356;&#21152;&#22797;&#26434;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#35201;&#20040;&#35774;&#35745;&#26032;&#30340;&#26550;&#26500;&#25110;&#26032;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#29992;&#20110;MDS&#65292;&#35201;&#20040;&#23558;PLM&#24212;&#29992;&#20110;MDS&#65292;&#20294;&#26410;&#32771;&#34385;&#21040;&#22797;&#26434;&#30340;&#25991;&#26723;&#20132;&#20114;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#19978;&#24378;&#21046;&#20351;&#29992;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#23547;&#27714;&#26356;&#22909;&#22320;&#21033;&#29992;PLM&#20419;&#36827;MDS&#20219;&#21153;&#30340;&#22810;&#25991;&#26723;&#20132;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;10&#20010;MDS&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#25105;&#20204;&#30340;&#35774;&#35745;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#35206;&#30422;&#21508;&#31181;&#39046;&#22495;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#37117;&#33021;&#22815;&#23454;&#29616;&#25345;&#32493;&#25913;&#36827;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;MDS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) have accomplished impressive achievements in abstractive single-document summarization (SDS). However, such benefits may not be readily extended to muti-document summarization (MDS), where the interactions among documents are more complex. Previous works either design new architectures or new pre-training objectives for MDS, or apply PLMs to MDS without considering the complex document interactions. While the former does not make full use of previous pre-training efforts and may not generalize well across multiple domains, the latter cannot fully attend to the intricate relationships unique to MDS tasks. In this paper, we enforce hierarchy on both the encoder and decoder and seek to make better use of a PLM to facilitate multi-document interactions for the MDS task. We test our design on 10 MDS datasets across a wide range of domains. Extensive experiments show that our proposed method can achieve consistent improvements on all these datasets, outperf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;LoRA&#21644;&#32467;&#26500;&#21270;&#23618;&#21098;&#26525;&#26041;&#27861;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#22312;&#20445;&#25345;&#36229;&#36807;92%&#29983;&#25104;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#35843;&#25972;&#20165;0.6%&#30340;&#21442;&#25968;&#24182;&#21098;&#26525;&#36229;&#36807;30%&#30340;Transformer&#23618;&#65292;&#25104;&#21151;&#20943;&#23569;&#20102;50%&#30340;GPU&#20869;&#23384;&#20351;&#29992;&#24182;&#25552;&#21319;&#20102;100%&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.08285</link><description>&lt;p&gt;
&#26080;&#38656;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65306;&#22522;&#20110;&#32467;&#26500;&#21270;&#21098;&#26525;&#30340;LoRA&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Fine-Tuning with Layer Pruning on Free-Text Sequence-to-Sequence modeling. (arXiv:2305.08285v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;LoRA&#21644;&#32467;&#26500;&#21270;&#23618;&#21098;&#26525;&#26041;&#27861;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#22312;&#20445;&#25345;&#36229;&#36807;92%&#29983;&#25104;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#35843;&#25972;&#20165;0.6%&#30340;&#21442;&#25968;&#24182;&#21098;&#26525;&#36229;&#36807;30%&#30340;Transformer&#23618;&#65292;&#25104;&#21151;&#20943;&#23569;&#20102;50%&#30340;GPU&#20869;&#23384;&#20351;&#29992;&#24182;&#25552;&#21319;&#20102;100%&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23610;&#23544;&#30340;&#19981;&#26029;&#22686;&#38271;&#24341;&#36215;&#20102;&#23545;&#20110;&#21442;&#25968;&#25928;&#29575;&#30340;&#24494;&#35843;&#26041;&#27861;&#30340;&#30740;&#31350;&#20852;&#36259;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;LoRA&#21644;&#32467;&#26500;&#21270;&#23618;&#21098;&#26525;&#26041;&#27861;&#32467;&#21512;&#30340;&#26694;&#26550;&#12290;&#36825;&#20010;&#26694;&#26550;&#22312; MIMIC-IV-Note&#19978;&#30340;&#20004;&#20010;&#21307;&#30103;&#25253;&#21578;&#27010;&#36848;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#20844;&#20849;&#21307;&#30103;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#36890;&#36807;&#35843;&#25972;&#21407;&#22987;&#27169;&#22411;&#30340;0.6%&#30340;&#21442;&#25968;&#24182;&#21098;&#26525;&#36229;&#36807;30%&#30340;Transformer&#23618;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20943;&#23569;50%&#30340;GPU&#20869;&#23384;&#20351;&#29992;&#24182;&#25552;&#21319;100%&#30340;&#35757;&#32451;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;&#33258;&#30001;&#25991;&#26412;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#19978;&#36229;&#36807;92%&#30340;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing size of language models raises great research interests in parameter-efficient fine-tuning such as LoRA that freezes the pre-trained model, and injects small-scale trainable parameters for multiple downstream tasks (e.g., summarization, question answering and translation). To further enhance the efficiency of fine-tuning, we propose a framework that integrates LoRA and structured layer pruning. The integrated framework is validated on two created deidentified medical report summarization datasets based on MIMIC-IV-Note and two public medical dialogue datasets. By tuning 0.6% parameters of the original model and pruning over 30% Transformer-layers, our framework can reduce 50% of GPU memory usage and speed up 100% of the training phase, while preserving over 92% generation qualities on free-text sequence-to-sequence tasks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#22686;&#21152;&#35757;&#32451;&#25104;&#26412;&#30340;&#36328;&#22495;&#38382;&#31572;&#27867;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25552;&#31034;&#26041;&#27861;&#21644;&#32447;&#24615;&#25506;&#27979;&#20877;&#24494;&#35843;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#20135;&#29983;&#24335;&#21644;&#21028;&#21035;&#24335;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;4.5%-7.9%&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.08208</link><description>&lt;p&gt;
&#36328;&#22495;&#38382;&#31572;&#27867;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Generalize for Cross-domain QA. (arXiv:2305.08208v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08208
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#22686;&#21152;&#35757;&#32451;&#25104;&#26412;&#30340;&#36328;&#22495;&#38382;&#31572;&#27867;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25552;&#31034;&#26041;&#27861;&#21644;&#32447;&#24615;&#25506;&#27979;&#20877;&#24494;&#35843;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#20135;&#29983;&#24335;&#21644;&#21028;&#21035;&#24335;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;4.5%-7.9%&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#36328;&#22495;&#27867;&#21270;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#19968;&#30452;&#23384;&#22312;&#30528;&#36234;&#26469;&#36234;&#22823;&#30340;&#25285;&#24551;&#12290;&#24403;&#21069;&#30340;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21463;&#21040;&#20102;&#22686;&#21152;&#35757;&#32451;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#25552;&#31034;&#26041;&#27861;&#21644;&#32447;&#24615;&#25506;&#27979;&#20877;&#24494;&#35843;&#31574;&#30053;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#37117;&#34987;&#35777;&#26126;&#26377;&#25928;&#65292;&#21487;&#20197;&#22686;&#24378;&#20135;&#29983;&#24335;&#21644;&#21028;&#21035;&#24335;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;F1&#24471;&#20998;&#24179;&#22343;&#25552;&#39640;&#20102;4.5%-7.9%&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#22320;&#38598;&#25104;&#21040;&#20219;&#20309;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#65292;&#24182;&#20026;&#26410;&#20805;&#20998;&#24320;&#21457;&#30340;&#36328;&#22495;&#38382;&#31572;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;GitHub&#19978;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;*&#12290;
&lt;/p&gt;
&lt;p&gt;
There have been growing concerns regarding the out-of-domain generalization ability of natural language processing (NLP) models, particularly in question-answering (QA) tasks. Current synthesized data augmentation methods for QA are hampered by increased training costs. To address this issue, we propose a novel approach that combines prompting methods and linear probing then fine-tuning strategy, which does not entail additional cost. Our method has been theoretically and empirically shown to be effective in enhancing the generalization ability of both generative and discriminative models. Our approach outperforms state-of-the-art baselines, with an average increase in F1 score of 4.5%-7.9%. Furthermore, our method can be easily integrated into any pre-trained models and offers a promising solution to the under-explored cross-domain QA task. We release our source code at GitHub*.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#31070;&#32463;&#22240;&#23376;&#20998;&#26512;&#27169;&#22411;&#65292;&#20351;&#29992;HuBERT&#20013;&#30340;&#32858;&#31867;&#26041;&#27861;&#26469;&#21457;&#29616;&#38544;&#34255;&#30340;&#22768;&#23398;&#21333;&#20803;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#21333;&#20803;&#23545;&#40784;SSL&#27169;&#22411;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#20135;&#29983;&#35299;&#32806;&#21518;&#30340;&#35821;&#38899;&#34920;&#31034;&#65292;&#20174;&#32780;&#20026;&#19987;&#38376;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;Utterance&#27700;&#24179;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SSNFA&#27169;&#22411;&#22312;&#35828;&#35805;&#20154;&#35782;&#21035;&#12289;&#35821;&#35328;&#35782;&#21035;&#21644;&#24773;&#24863;&#35782;&#21035;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#22343;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;SSL&#27169;&#22411;&#65292;&#24182;&#19988;&#27809;&#26377;&#20219;&#20309;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#25110;&#30417;&#30563;&#12290;</title><link>http://arxiv.org/abs/2305.08099</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#31070;&#32463;&#22240;&#23376;&#20998;&#26512;&#35299;&#32806;&#35821;&#38899;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Neural Factor Analysis for Disentangling Utterance-level Speech Representations. (arXiv:2305.08099v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#31070;&#32463;&#22240;&#23376;&#20998;&#26512;&#27169;&#22411;&#65292;&#20351;&#29992;HuBERT&#20013;&#30340;&#32858;&#31867;&#26041;&#27861;&#26469;&#21457;&#29616;&#38544;&#34255;&#30340;&#22768;&#23398;&#21333;&#20803;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#21333;&#20803;&#23545;&#40784;SSL&#27169;&#22411;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#20135;&#29983;&#35299;&#32806;&#21518;&#30340;&#35821;&#38899;&#34920;&#31034;&#65292;&#20174;&#32780;&#20026;&#19987;&#38376;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;Utterance&#27700;&#24179;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SSNFA&#27169;&#22411;&#22312;&#35828;&#35805;&#20154;&#35782;&#21035;&#12289;&#35821;&#35328;&#35782;&#21035;&#21644;&#24773;&#24863;&#35782;&#21035;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#22343;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;SSL&#27169;&#22411;&#65292;&#24182;&#19988;&#27809;&#26377;&#20219;&#20309;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#25110;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#26041;&#38754;&#24050;&#32463;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#22312;&#20302;&#26631;&#27880;&#36164;&#28304;&#24773;&#20917;&#19979;&#35777;&#26126;&#38750;&#24120;&#26377;&#29992;&#65292;&#26412;&#25991;&#38024;&#23545;&#35813;&#25216;&#26415;&#22312;&#35828;&#35805;&#20154;&#12289;&#24773;&#24863;&#21644;&#35821;&#35328;&#35782;&#21035;&#31561;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#38382;&#39064;&#36827;&#34892;&#20102;&#25506;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#23376;&#20998;&#26512;&#27169;&#22411;&#65292;&#20351;&#29992;HuBERT&#20013;&#30340;&#32858;&#31867;&#26041;&#27861;&#26469;&#21457;&#29616;&#38544;&#34255;&#30340;&#22768;&#23398;&#21333;&#20803;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#21333;&#20803;&#23545;&#40784;SSL&#27169;&#22411;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#20135;&#29983;&#35299;&#32806;&#21518;&#30340;&#35821;&#38899;&#34920;&#31034;&#65292;&#20174;&#32780;&#20026;&#19987;&#38376;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;Utterance&#27700;&#24179;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SSNFA&#27169;&#22411;&#22312;&#35828;&#35805;&#20154;&#35782;&#21035;&#12289;&#35821;&#35328;&#35782;&#21035;&#21644;&#24773;&#24863;&#35782;&#21035;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#22343;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;SSL&#27169;&#22411;&#65292;&#24182;&#19988;&#27809;&#26377;&#20219;&#20309;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#25110;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) speech models such as wav2vec and HuBERT have demonstrated state-of-the-art performance on automatic speech recognition (ASR) and proved to be extremely useful in low label-resource settings. However, the success of SSL models has yet to transfer to utterance-level tasks such as speaker, emotion, and language recognition, which still require supervised fine-tuning of the SSL models to obtain good performance. We argue that the problem is caused by the lack of disentangled representations and an utterance-level learning objective for these tasks. Inspired by how HuBERT uses clustering to discover hidden acoustic units, we formulate a factor analysis (FA) model that uses the discovered hidden acoustic units to align the SSL features. The underlying utterance-level representations are disentangled from the content of speech using probabilistic inference on the aligned features. Furthermore, the variational lower bound derived from the FA model provides an ut
&lt;/p&gt;</description></item><item><title>GPT-Sentinel&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#26816;&#27979;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#32534;&#20889;&#30340;&#25991;&#26412;&#65292;&#20854;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#20934;&#30830;&#29575;&#36229;&#36807;&#20102;97&#65285;&#65292;&#24182;&#19988;&#25581;&#31034;&#20102;&#21306;&#20998;&#36825;&#20004;&#31181;&#25991;&#26412;&#20851;&#38190;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.07969</link><description>&lt;p&gt;
GPT-Sentinel&#65306;&#21306;&#20998;&#20154;&#31867;&#21644;ChatGPT&#29983;&#25104;&#20869;&#23481;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content. (arXiv:2305.07969v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07969
&lt;/p&gt;
&lt;p&gt;
GPT-Sentinel&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#26816;&#27979;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#32534;&#20889;&#30340;&#25991;&#26412;&#65292;&#20854;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#20934;&#30830;&#29575;&#36229;&#36807;&#20102;97&#65285;&#65292;&#24182;&#19988;&#25581;&#31034;&#20102;&#21306;&#20998;&#36825;&#20004;&#31181;&#25991;&#26412;&#20851;&#38190;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#27979;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#32534;&#20889;&#25991;&#26412;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#21644;&#21457;&#24067;&#20102;&#19968;&#20010;&#39044;&#22788;&#29702;&#25968;&#25454;&#38598;OpenGPTText&#65292;&#20854;&#20013;&#21253;&#21547;&#20351;&#29992;ChatGPT&#29983;&#25104;&#30340;&#37325;&#26032;&#34920;&#36848;&#20869;&#23481;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#12289;&#23454;&#29616;&#12289;&#35757;&#32451;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#65292;&#20998;&#21035;&#20351;&#29992;Robustly Optimized BERT Pretraining Approach&#65288;RoBERTa&#65289;&#21644;Text-to-Text Transfer Transformer&#65288;T5&#65289;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#65292;&#36890;&#36807;&#21508;&#31181;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#65292;&#20934;&#30830;&#29575;&#36229;&#36807;&#20102;97%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#25552;&#21462;&#21644;&#21306;&#20998;&#20154;&#31867;&#32534;&#20889;&#21644;ChatGPT&#29983;&#25104;&#25991;&#26412;&#20043;&#38388;&#20851;&#38190;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26816;&#27979;&#29983;&#25104;&#25991;&#26412;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach for detecting ChatGPT-generated vs. human-written text using language models. To this end, we first collected and released a pre-processed dataset named OpenGPTText, which consists of rephrased content generated using ChatGPT. We then designed, implemented, and trained two different models for text classification, using Robustly Optimized BERT Pretraining Approach (RoBERTa) and Text-to-Text Transfer Transformer (T5), respectively. Our models achieved remarkable results, with an accuracy of over 97% on the test dataset, as evaluated through various metrics. Furthermore, we conducted an interpretability study to showcase our model's ability to extract and differentiate key features between human-written and ChatGPT-generated text. Our findings provide important insights into the effective use of language models to detect generated text.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Dr. LLaMA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#25913;&#21892;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#24494;&#35843;&#21518;&#20351;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#38382;&#31572;&#20219;&#21153;&#20013;&#20351;&#29992;LLM&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.07804</link><description>&lt;p&gt;
Dr. LLaMA&#65306;&#36890;&#36807;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#25913;&#21892;&#29305;&#23450;&#39046;&#22495;QA&#20013;&#30340;&#23567;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Dr. LLaMA: Improving Small Language Models in Domain-Specific QA via Generative Data Augmentation. (arXiv:2305.07804v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Dr. LLaMA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#25913;&#21892;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#24494;&#35843;&#21518;&#20351;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#38382;&#31572;&#20219;&#21153;&#20013;&#20351;&#29992;LLM&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#38543;&#30528;&#20854;&#35268;&#27169;&#30340;&#22686;&#38271;&#65292;&#20063;&#38754;&#20020;&#30528;&#35745;&#31639;&#24320;&#38144;&#21644;&#25928;&#29575;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#20219;&#21153;&#20013;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30001;&#20110;&#23481;&#37327;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Dr. LLaMA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#65292;&#32858;&#28966;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#21644;PubMedQA&#25968;&#25454;&#38598;&#65292;&#20197;&#25913;&#21892;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;LLM&#26377;&#25928;&#22320;&#32454;&#21270;&#21644;&#25193;&#23637;&#29616;&#26377;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#22312;&#24494;&#35843;&#21518;&#65292;&#20351;&#24471;&#23567;&#22411;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;QA&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#25552;&#39640;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#38382;&#31572;&#20219;&#21153;&#20013;&#20351;&#29992;LLM&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#26368;&#32456;&#26088;&#22312;&#20026;&#19987;&#19994;&#24212;&#29992;&#21019;&#24314;&#26356;&#39640;&#25928;&#21644;&#33021;&#21147;&#26356;&#24378;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have made significant strides in natural language processing but face challenges in terms of computational expense and inefficiency as they grow in size, especially in domain-specific tasks. Small Language Models (SLMs), on the other hand, often struggle in these tasks due to limited capacity and training data. In this paper, we introduce Dr. LLaMA, a method for improving SLMs through generative data augmentation using LLMs, focusing on medical question-answering tasks and the PubMedQA dataset. Our findings indicate that LLMs effectively refine and diversify existing question-answer pairs, resulting in improved performance of a much smaller model on domain-specific QA datasets after fine-tuning. This study highlights the challenges of using LLMs for domain-specific question answering and suggests potential research directions to address these limitations, ultimately aiming to create more efficient and capable models for specialized applications. We have als
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;IS-CSE&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#20363;&#24179;&#28369;&#23545;&#27604;&#23398;&#20064;&#26469;&#23398;&#20064;&#26080;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#65292;&#20197;&#24179;&#28369;&#23884;&#20837;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#26631;&#20934;&#30340;STS&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2305.07424</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#30340;&#23454;&#20363;&#24179;&#28369;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Instance Smoothed Contrastive Learning for Unsupervised Sentence Embedding. (arXiv:2305.07424v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;IS-CSE&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#20363;&#24179;&#28369;&#23545;&#27604;&#23398;&#20064;&#26469;&#23398;&#20064;&#26080;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#65292;&#20197;&#24179;&#28369;&#23884;&#20837;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#26631;&#20934;&#30340;STS&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22914;unsup-SimCSE&#65292;&#22312;&#23398;&#20064;&#26080;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#65288;SOTA&#65289;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#20197;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#29992;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#27599;&#20010;&#23884;&#20837;&#20165;&#26469;&#33258;&#20110;&#19968;&#20010;&#21477;&#23376;&#23454;&#20363;&#65292;&#25105;&#20204;&#31216;&#36825;&#20123;&#23884;&#20837;&#20026;&#23454;&#20363;&#32423;&#23884;&#20837;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#27599;&#20010;&#23884;&#20837;&#34987;&#35270;&#20026;&#26159;&#19968;&#31867;&#29420;&#29305;&#30340;&#31867;&#65292;&#36825;&#21487;&#33021;&#20250;&#25439;&#23475;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IS-CSE&#65288;&#23454;&#20363;&#24179;&#28369;&#23545;&#27604;&#21477;&#23376;&#23884;&#20837;&#65289;&#26469;&#24179;&#28369;&#29305;&#24449;&#31354;&#38388;&#20013;&#23884;&#20837;&#30340;&#36793;&#30028;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26681;&#25454;&#35821;&#20041;&#30456;&#20284;&#24615;&#20174;&#21160;&#24577;&#20869;&#23384;&#32531;&#20914;&#21306;&#20013;&#26816;&#32034;&#23884;&#20837;&#20197;&#33719;&#24471;&#27491;&#23884;&#20837;&#32452;&#12290;&#28982;&#21518;&#25105;&#20204;&#36890;&#36807;&#33258;&#27880;&#24847;&#21147;&#25805;&#20316;&#23545;&#32452;&#20013;&#30340;&#23884;&#20837;&#36827;&#34892;&#32858;&#21512;&#65292;&#20197;&#29983;&#25104;&#24179;&#28369;&#23454;&#20363;&#23884;&#20837;&#20197;&#36827;&#34892;&#36827;&#19968;&#27493;&#20998;&#26512;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#24179;&#22343;78.30&#65285;&#65292;79.47&#65285;&#65292;77.73&#65285;&#21644;79.42&#65285;&#30340;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning-based methods, such as unsup-SimCSE, have achieved state-of-the-art (SOTA) performances in learning unsupervised sentence embeddings. However, in previous studies, each embedding used for contrastive learning only derived from one sentence instance, and we call these embeddings instance-level embeddings. In other words, each embedding is regarded as a unique class of its own, whichmay hurt the generalization performance. In this study, we propose IS-CSE (instance smoothing contrastive sentence embedding) to smooth the boundaries of embeddings in the feature space. Specifically, we retrieve embeddings from a dynamic memory buffer according to the semantic similarity to get a positive embedding group. Then embeddings in the group are aggregated by a self-attention operation to produce a smoothed instance embedding for further analysis. We evaluate our method on standard semantic text similarity (STS) tasks and achieve an average of 78.30%, 79.47%, 77.73%, and 79.42% 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#65292;&#23454;&#39564;&#35777;&#26126;ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#35299;&#37322;&#32773;&#65292;&#20294;&#19981;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#29702;&#32773;&#65292;&#23384;&#22312;&#20005;&#37325;&#30340;&#22240;&#26524;&#24187;&#35273;&#38382;&#39064;&#65292;&#23545;&#20110;&#26126;&#30830;&#30340;&#22240;&#26524;&#20851;&#31995;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.07375</link><description>&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#26029;&#22120;&#21527;&#65311;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation. (arXiv:2305.07375v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#65292;&#23454;&#39564;&#35777;&#26126;ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#35299;&#37322;&#32773;&#65292;&#20294;&#19981;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#29702;&#32773;&#65292;&#23384;&#22312;&#20005;&#37325;&#30340;&#22240;&#26524;&#24187;&#35273;&#38382;&#39064;&#65292;&#23545;&#20110;&#26126;&#30830;&#30340;&#22240;&#26524;&#20851;&#31995;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#23545;&#20110;&#20247;&#22810;NLP&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;ChatGPT&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26032;&#20852;&#33021;&#21147;&#65292;&#20294;ChatGPT&#22312;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#30340;&#34920;&#29616;&#22914;&#20309;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#23545;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ChatGPT&#19981;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#29702;&#32773;&#65292;&#20294;&#26159;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#35299;&#37322;&#32773;&#12290;&#27492;&#22806;&#65292;ChatGPT&#22312;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#20005;&#37325;&#30340;&#24187;&#35273;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#20013;&#22240;&#26524;&#20851;&#31995;&#21644;&#38750;&#22240;&#26524;&#20851;&#31995;&#30340;&#25253;&#21578;&#20559;&#35265;&#65292;&#20197;&#21450;ChatGPT&#30340;&#21319;&#32423;&#36807;&#31243;&#65292;&#22914;RLHF&#12290;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#21644;&#24605;&#32500;&#38142;&#65288;COT&#65289;&#25216;&#26415;&#26041;&#38754;&#65292;&#21487;&#33021;&#20250;&#36827;&#19968;&#27493;&#21152;&#21095;&#36825;&#31181;&#22240;&#26524;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#23545;&#20110;&#22312;&#25552;&#31034;&#20013;&#34920;&#36798;&#22240;&#26524;&#27010;&#24565;&#30340;&#35789;&#35821;&#38750;&#24120;&#25935;&#24863;&#65292;&#24182;&#19988;&#23553;&#38381;&#25552;&#31034;&#27604;&#24320;&#25918;&#25552;&#31034;&#34920;&#29616;&#26356;&#22909;&#12290;&#23545;&#20110;&#21477;&#23376;&#20013;&#30340;&#20107;&#20214;&#65292;ChatGPT&#25797;&#38271;&#25429;&#25417;&#26126;&#30830;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal reasoning ability is crucial for numerous NLP applications. Despite the impressive emerging ability of ChatGPT in various NLP tasks, it is unclear how well ChatGPT performs in causal reasoning. In this paper, we conduct the first comprehensive evaluation of the ChatGPT's causal reasoning capabilities. Experiments show that ChatGPT is not a good causal reasoner, but a good causal interpreter. Besides, ChatGPT has a serious hallucination on causal reasoning, possibly due to the reporting biases between causal and non-causal relationships in natural language, as well as ChatGPT's upgrading processes, such as RLHF. The In-Context Learning (ICL) and Chain-of-Though (COT) techniques can further exacerbate such causal hallucination. Additionally, the causal reasoning ability of ChatGPT is sensitive to the words used to express the causal concept in prompts, and close-ended prompts perform better than open-ended prompts. For events in sentences, ChatGPT excels at capturing explicit caus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#20307;&#31995;&#65292;&#20998;&#31867;&#21644;&#27604;&#36739;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;&#23427;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.05352</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Designing Foundation Model based Systems. (arXiv:2305.05352v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#20307;&#31995;&#65292;&#20998;&#31867;&#21644;&#27604;&#36739;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;&#23427;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25512;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#22914;ChatGPT&#65292;&#36825;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#22522;&#30784;&#27169;&#22411;&#34987;&#24191;&#27867;&#35748;&#20026;&#23558;&#25104;&#20026;&#26410;&#26469;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#22522;&#30707;&#12290;&#30001;&#20110;&#22522;&#30784;&#27169;&#22411;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#65292;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#35774;&#35745;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#22320;&#25506;&#32034;&#12290;&#20154;&#20204;&#23545;&#22312;&#36719;&#20214;&#26550;&#26500;&#20013;&#24341;&#20837;&#22522;&#30784;&#27169;&#22411;&#30340;&#24433;&#21709;&#30693;&#20043;&#29978;&#23569;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#27861;&#65292;&#23545;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#21253;&#25324;&#19977;&#20010;&#31867;&#21035;&#65306;&#22522;&#30784;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12289;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26550;&#26500;&#35774;&#35745;&#21644;&#36127;&#36131;&#20219;&#30340;AI&#35774;&#35745;&#12290;&#36825;&#20010;&#20998;&#31867;&#27861;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent release of large language model (LLM) based chatbots, such as ChatGPT, has attracted significant attention on foundations models. It is widely believed that foundation models will serve as the fundamental building blocks for future AI systems. As foundation models are in their early stages, the design of foundation model based systems has not yet been systematically explored. There is little understanding about the impact of introducing foundation models in software architecture. Therefore, in this paper, we propose a taxonomy of foundation model based systems, which classifies and compares the characteristics of foundation models and foundation model based systems. Our taxonomy comprises three categories: foundation model pretraining and fine-tuning, architecture design of foundation model based systems, and responsible-AI-by-design. This taxonomy provides concrete guidance for making major design decisions when designing foundation model based systems and highlights trade-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23450;&#20041;&#20102;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#21462;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#22312;&#32422;&#26463;&#24544;&#23454;&#24230;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#36171;&#20104;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#33021;&#21147;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.05252</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#33050;&#26412;&#30693;&#35782;&#20197;&#36827;&#34892;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Distilling Script Knowledge from Large Language Models for Constrained Language Planning. (arXiv:2305.05252v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23450;&#20041;&#20102;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#21462;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#22312;&#32422;&#26463;&#24544;&#23454;&#24230;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#36171;&#20104;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#33021;&#21147;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#36890;&#36807;&#36981;&#24490;&#30446;&#26631;&#23548;&#21521;&#30340;&#33050;&#26412;&#24418;&#24335;&#30340;&#36880;&#27493;&#35828;&#26126;&#26469;&#35268;&#21010;&#33258;&#24049;&#30340;&#34892;&#21160;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#26469;&#20026;&#31435;&#20307;&#27963;&#21160;&#30340;&#25277;&#35937;&#30446;&#26631;&#65288;&#20363;&#22914;&#65292;&#8220;&#21046;&#20316;&#34507;&#31957;&#8221;&#65289;&#36827;&#34892;&#35268;&#21010;&#65292;&#20294;&#23545;&#20110;&#20855;&#26377;&#22810;&#26041;&#38754;&#32422;&#26463;&#30340;&#26356;&#20855;&#20307;&#30446;&#26631;&#65288;&#20363;&#22914;&#65292;&#8220;&#20026;&#31958;&#23615;&#30149;&#24739;&#32773;&#21046;&#20316;&#34507;&#31957;&#8221;&#65289;&#40092;&#26377;&#30740;&#31350;&#12290;&#26412;&#25991;&#39318;&#27425;&#23450;&#20041;&#20102;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36807;&#24230;&#29983;&#25104;&#24182;&#36807;&#28388;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21033;&#29992;&#23427;&#26469;&#25552;&#21462;&#19968;&#31181;&#26032;&#39062;&#30340;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#25968;&#25454;&#38598;CoScript&#65292;&#20854;&#20013;&#21253;&#25324;55,000&#20010;&#33050;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;LLM&#22312;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#32422;&#26463;&#24544;&#23454;&#24230;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;CoScript&#34987;&#35777;&#26126;&#23545;&#36171;&#20104;&#36739;&#23567;&#30340;LM&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#33021;&#21147;&#26159;&#38750;&#24120;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models (LMs) to plan for abstract goals of stereotypical activities (e.g., "make a cake"), but leaves more specific goals with multi-facet constraints understudied (e.g., "make a cake for diabetics"). In this paper, we define the task of constrained language planning for the first time. We propose an overgenerate-then-filter approach to improve large language models (LLMs) on this task, and use it to distill a novel constrained language planning dataset, CoScript, which consists of 55,000 scripts. Empirical results demonstrate that our method significantly improves the constrained language planning ability of LLMs, especially on constraint faithfulness. Furthermore, CoScript is demonstrated to be quite effective in endowing smaller LMs with constrained language planning ability.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#21442;&#25968;&#30693;&#35782;&#24341;&#23548;&#30340;&#22686;&#24378;&#22411;&#22823;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#20026;LLMs&#35013;&#22791;&#20449;&#24687;&#24341;&#23548;&#27169;&#22359;&#26469;&#35775;&#38382;&#30456;&#20851;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;LLMs&#30340;&#21442;&#25968;&#19981;&#21464;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#40657;&#30418;LLMs&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.04757</link><description>&lt;p&gt;
&#24102;&#21442;&#25968;&#30693;&#35782;&#24341;&#23548;&#30340;&#22686;&#24378;&#22411;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Augmented Large Language Models with Parametric Knowledge Guiding. (arXiv:2305.04757v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04757
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#21442;&#25968;&#30693;&#35782;&#24341;&#23548;&#30340;&#22686;&#24378;&#22411;&#22823;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#20026;LLMs&#35013;&#22791;&#20449;&#24687;&#24341;&#23548;&#27169;&#22359;&#26469;&#35775;&#38382;&#30456;&#20851;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;LLMs&#30340;&#21442;&#25968;&#19981;&#21464;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#40657;&#30418;LLMs&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#20197;&#20854;&#20986;&#33394;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#23545;&#30456;&#20851;&#25968;&#25454;&#30340;&#26377;&#38480;&#25509;&#35302;&#65292;&#23427;&#20204;&#22312;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#30340;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#21487;&#33021;&#19981;&#22815;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340; LLM &#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#21482;&#33021;&#36890;&#36807; API &#35775;&#38382;, &#36825;&#38459;&#27490;&#20102;&#36827;&#19968;&#27493;&#29992;&#39046;&#22495;&#23450;&#21046;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#21521; LLM &#25152;&#26377;&#32773;&#25552;&#20379;&#31169;&#26377;&#25968;&#25454;&#20250;&#23548;&#33268;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#22411;&#30340;&#24102;&#21442;&#25968;&#30693;&#35782;&#24341;&#23548; (PKG) &#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20026; LLM &#37197;&#22791;&#20102;&#30693;&#35782;&#24341;&#23548;&#27169;&#22359;&#65292;&#20197;&#35775;&#38382;&#30456;&#20851;&#30693;&#35782;&#65292;&#32780;&#26080;&#38656;&#25913;&#21464; LLM &#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340; PKG &#22522;&#20110;&#24320;&#28304;&#30340;&#8220;&#30333;&#30418;&#8221;&#35821;&#35328;&#27169;&#22411;&#65292;&#20801;&#35768;&#31163;&#32447;&#23384;&#20648; LLM &#38656;&#35201;&#30340;&#20219;&#20309;&#30693;&#35782;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340; PKG &#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#8220;&#40657;&#30418;&#8221;LLM&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have significantly advanced natural language processing (NLP) with their impressive language understanding and generation capabilities. However, their performance may be suboptimal for domain-specific tasks that require specialized knowledge due to limited exposure to the related data. Additionally, the lack of transparency of most state-of-the-art (SOTA) LLMs, which can only be accessed via APIs, impedes further fine-tuning with domain custom data. Moreover, providing private data to the LLMs' owner leads to data privacy problems. To address these challenges, we propose the novel Parametric Knowledge Guiding (PKG) framework, which equips LLMs with a knowledge-guiding module to access relevant knowledge without altering the LLMs' parameters. Our PKG is based on open-source "white-box" language models, allowing offline memory of any knowledge that LLMs require. We demonstrate that our PKG framework can enhance the performance of "black-box" LLMs on a range o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#32479;&#19968;&#27169;&#22411;&#23398;&#20064;&#65292;&#21487;&#20197;&#21516;&#26102;&#36866;&#29992;&#20110;&#32763;&#35793;&#21508;&#31181;&#20219;&#21153;&#25968;&#25454;&#65292;&#24182;&#23454;&#29616;&#26234;&#33021;&#25353;&#38656;&#32763;&#35793;&#65292;&#30456;&#23545;&#29616;&#26377;&#30340;&#29305;&#23450;&#25968;&#25454;&#38598;&#27169;&#22411;&#33021;&#22815;&#24471;&#21040;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.02777</link><description>&lt;p&gt;
&#21508;&#31181;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#32479;&#19968;&#27169;&#22411;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unified Model Learning for Various Neural Machine Translation. (arXiv:2305.02777v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#32479;&#19968;&#27169;&#22411;&#23398;&#20064;&#65292;&#21487;&#20197;&#21516;&#26102;&#36866;&#29992;&#20110;&#32763;&#35793;&#21508;&#31181;&#20219;&#21153;&#25968;&#25454;&#65292;&#24182;&#23454;&#29616;&#26234;&#33021;&#25353;&#38656;&#32763;&#35793;&#65292;&#30456;&#23545;&#29616;&#26377;&#30340;&#29305;&#23450;&#25968;&#25454;&#38598;&#27169;&#22411;&#33021;&#22815;&#24471;&#21040;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(NMT)&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#26681;&#25454;&#26469;&#33258;&#19981;&#21516;&#20219;&#21153;(&#20363;&#22914;&#65292;&#25991;&#26723;&#32763;&#35793;&#21644;&#32842;&#22825;&#32763;&#35793;)&#30340;&#25968;&#25454;&#24320;&#21457;&#29305;&#23450;&#20110;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#12290;&#34429;&#28982;&#29305;&#23450;&#20110;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#65292;&#20294;&#27599;&#20010;&#25968;&#25454;&#38598;&#38656;&#35201;&#35774;&#35745;&#12289;&#35757;&#32451;&#21644;&#23384;&#20648;&#19968;&#20010;&#27169;&#22411;&#65292;&#36825;&#24456;&#40635;&#28902;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#36825;&#20123;&#32763;&#35793;&#20219;&#21153;&#32479;&#19968;&#21040;&#26356;&#26222;&#36941;&#30340;&#35774;&#32622;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#22810;&#25165;&#22810;&#33402;&#8221;&#30340;&#27169;&#22411;&#65292;&#21363;&#36866;&#29992;&#20110;&#19981;&#21516;&#20219;&#21153;&#25968;&#25454;&#30340;&#32479;&#19968;&#27169;&#22411;&#23398;&#20064;(NMT)&#65292;&#21487;&#20197;&#21516;&#26102;&#22312;&#22810;&#31181;&#29615;&#22659;&#19979;&#36827;&#34892;&#33391;&#22909;&#30340;&#32763;&#35793;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#21487;&#20197;&#23613;&#21487;&#33021;&#22810;&#22320;&#25193;&#23637;&#12290;&#36890;&#36807;&#32479;&#19968;&#23398;&#20064;&#65292;UMLNMT&#33021;&#22815;&#36328;&#22810;&#20010;&#20219;&#21153;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65292;&#23454;&#29616;&#26234;&#33021;&#25353;&#38656;&#32763;&#35793;&#12290;&#22312;&#19971;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#32763;&#35793;&#20219;&#21153;&#65292;&#21253;&#25324;&#21477;&#23376;&#32763;&#35793;&#12289;&#25991;&#26723;&#32763;&#35793;&#21644;&#32842;&#22825;&#32763;&#35793;&#20013;&#65292;&#25105;&#20204;&#30340;UMLNMT&#30456;&#23545;&#20110;&#29305;&#23450;&#25968;&#25454;&#38598;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing neural machine translation (NMT) studies mainly focus on developing dataset-specific models based on data from different tasks (e.g., document translation and chat translation). Although the dataset-specific models have achieved impressive performance, it is cumbersome as each dataset demands a model to be designed, trained, and stored. In this work, we aim to unify these translation tasks into a more general setting. Specifically, we propose a ``versatile'' model, i.e., the Unified Model Learning for NMT (UMLNMT) that works with data from different tasks, and can translate well in multiple settings simultaneously, and theoretically it can be as many as possible. Through unified learning, UMLNMT is able to jointly train across multiple tasks, implementing intelligent on-demand translation. On seven widely-used translation tasks, including sentence translation, document translation, and chat translation, our UMLNMT results in substantial improvements over dataset-specific model
&lt;/p&gt;</description></item><item><title>Unlimiformer&#26159;&#19968;&#31181;Transformer&#27169;&#22411;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#25152;&#26377;&#23618;&#30340;&#27880;&#24847;&#35745;&#31639;&#21368;&#36733;&#21040;&#21333;&#20010;k&#36817;&#37051;&#32034;&#24341;&#19978;&#65292;&#20174;&#32780;&#21487;&#22788;&#29702;&#26080;&#38480;&#38271;&#24230;&#30340;&#36755;&#20837;&#65292;&#32780;&#19981;&#22686;&#21152;&#39069;&#22806;&#30340;&#23398;&#20064;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2305.01625</link><description>&lt;p&gt;
&#26080;&#38480;&#38271;&#24230;&#36755;&#20837;&#30340;&#38271;&#36317;&#31163;Transformer-Unlimiformer
&lt;/p&gt;
&lt;p&gt;
Unlimiformer: Long-Range Transformers with Unlimited Length Input. (arXiv:2305.01625v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01625
&lt;/p&gt;
&lt;p&gt;
Unlimiformer&#26159;&#19968;&#31181;Transformer&#27169;&#22411;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#25152;&#26377;&#23618;&#30340;&#27880;&#24847;&#35745;&#31639;&#21368;&#36733;&#21040;&#21333;&#20010;k&#36817;&#37051;&#32034;&#24341;&#19978;&#65292;&#20174;&#32780;&#21487;&#22788;&#29702;&#26080;&#38480;&#38271;&#24230;&#30340;&#36755;&#20837;&#65292;&#32780;&#19981;&#22686;&#21152;&#39069;&#22806;&#30340;&#23398;&#20064;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#36890;&#24120;&#23545;&#36755;&#20837;&#38271;&#24230;&#26377;&#39044;&#23450;&#20041;&#30340;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#38656;&#35201;&#21442;&#32771;&#36755;&#20837;&#20013;&#30340;&#27599;&#20010;&#26631;&#35760;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;-Unlimiformer&#65292;&#21487;&#20197;&#21253;&#35013;&#20219;&#20309;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#65292;&#24182;&#23558;&#25152;&#26377;&#23618;&#30340;&#27880;&#24847;&#35745;&#31639;&#21368;&#36733;&#21040;&#21333;&#20010;k&#36817;&#37051;&#32034;&#24341;&#19978;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#38271;&#25991;&#26723;&#21644;&#22810;&#25991;&#26723;&#25688;&#35201;&#22522;&#20934;&#27979;&#35797;&#20013;&#35777;&#26126;&#20102;Unlimiformer&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#23427;&#21487;&#20197;&#24635;&#32467;350k&#20196;&#29260;&#38271;&#30340;&#36755;&#20837;&#32780;&#19981;&#36827;&#34892;&#27979;&#35797;&#26102;&#30340;&#25130;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models typically have a predefined bound to their input length, because of their need to potentially attend to every token in the input. In this work, we propose Unlimiformer: a general approach that can wrap any existing pretrained encoder-decoder transformer, and offload the attention computation across all layers to a single $k$-nearest-neighbor index; this index can be kept on either the GPU or CPU memory and queried in sub-linear time. This way, we can index extremely long input sequences, while every attention head in every decoder layer retrieves its top-$k$ keys, instead of attending to every key. We demonstrate Unlimiformers's efficacy on several long-document and multi-document summarization benchmarks, showing that it can summarize even 350k token-long inputs from the BookSum dataset, without any input truncation at test time. Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#28155;&#21152;&#20102;&#21487;&#20197;&#22788;&#29702;&#21477;&#27861;&#27495;&#20041;&#30340;&#38750;&#30830;&#23450;&#24615;&#26632;&#65292;&#26377;&#25928;&#22320;&#27169;&#25311;&#19968;&#20010;&#38750;&#30830;&#23450;&#24615;&#19979;&#25512;&#33258;&#21160;&#26426;&#12290;</title><link>http://arxiv.org/abs/2304.12955</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38750;&#30830;&#23450;&#24615;&#26632;
&lt;/p&gt;
&lt;p&gt;
Nondeterministic Stacks in Neural Networks. (arXiv:2304.12955v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#28155;&#21152;&#20102;&#21487;&#20197;&#22788;&#29702;&#21477;&#27861;&#27495;&#20041;&#30340;&#38750;&#30830;&#23450;&#24615;&#26632;&#65292;&#26377;&#25928;&#22320;&#27169;&#25311;&#19968;&#20010;&#38750;&#30830;&#23450;&#24615;&#19979;&#25512;&#33258;&#21160;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35821;&#35328;&#20013;&#20805;&#28385;&#20102; &#32452;&#25104;&#24615;&#21477;&#27861;&#32467;&#26500;&#65292;&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#35821;&#35328;&#30340;&#35745;&#31639;&#26426;&#31995;&#32479;&#26041;&#38754;&#20570;&#20986;&#20102;&#31361;&#30772;&#24615;&#30340;&#25913;&#36827;&#65292;&#20294;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#22312;&#22788;&#29702;&#35821;&#27861;&#26041;&#38754;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#28155;&#21152;&#26632; &#25968;&#25454;&#32467;&#26500;&#65292;&#20174;&#35821;&#27861;&#21644;&#26632;&#20043;&#38388;&#30340;&#29702;&#35770;&#20851;&#31995;&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#37319;&#29992;&#30340;&#26159;&#35774;&#35745;&#29992;&#20110;&#36319;&#36394;&#19968;&#20010;&#21477;&#27861;&#20998;&#26512;&#30340;&#30830;&#23450;&#24615;&#26632;&#65292;&#32780;&#22312;&#35821;&#35328;&#20013;&#38656;&#35201;&#37319;&#29992;&#38750;&#30830;&#23450;&#24615;&#26632;&#36827;&#34892;&#35299;&#26512;&#30340;&#21477;&#27861;&#27495;&#20041;&#26497;&#20854;&#24120;&#35265;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#23558;&#38750;&#30830;&#23450;&#24615;&#26632;&#32435;&#20837;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#24046;&#24322;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#21033;&#29992;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#39640;&#25928;&#22320;&#27169;&#25311;&#20102;&#19968;&#20010;&#38750;&#30830;&#23450;&#24615;&#19979;&#25512;&#33258;&#21160;&#26426;&#65292;&#34920;&#31034;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#35745;&#31639;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human language is full of compositional syntactic structures, and although neural networks have contributed to groundbreaking improvements in computer systems that process language, widely-used neural network architectures still exhibit limitations in their ability to process syntax. To address this issue, prior work has proposed adding stack data structures to neural networks, drawing inspiration from theoretical connections between syntax and stacks. However, these methods employ deterministic stacks that are designed to track one parse at a time, whereas syntactic ambiguity, which requires a nondeterministic stack to parse, is extremely common in language. In this dissertation, we remedy this discrepancy by proposing a method of incorporating nondeterministic stacks into neural networks. We develop a differentiable data structure that efficiently simulates a nondeterministic pushdown automaton, representing an exponential number of computations with a dynamic programming algorithm. 
&lt;/p&gt;</description></item><item><title>Eyettention&#26159;&#31532;&#19968;&#20010;&#21516;&#26102;&#22788;&#29702;&#35821;&#35328;&#24207;&#21015;&#21644;&#26102;&#38388;&#24207;&#21015;&#30340;&#38405;&#35835;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#38405;&#35835;&#32773;&#30340;&#25195;&#35270;&#36335;&#24452;&#65292;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20855;&#26377;&#20511;&#37492;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2304.10784</link><description>&lt;p&gt;
Eyettention&#65306;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21452;&#24207;&#21015;&#27169;&#22411;&#20197;&#39044;&#27979;&#20154;&#31867;&#38405;&#35835;&#26102;&#30340;&#25195;&#35270;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Eyettention: An Attention-based Dual-Sequence Model for Predicting Human Scanpaths during Reading. (arXiv:2304.10784v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10784
&lt;/p&gt;
&lt;p&gt;
Eyettention&#26159;&#31532;&#19968;&#20010;&#21516;&#26102;&#22788;&#29702;&#35821;&#35328;&#24207;&#21015;&#21644;&#26102;&#38388;&#24207;&#21015;&#30340;&#38405;&#35835;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#38405;&#35835;&#32773;&#30340;&#25195;&#35270;&#36335;&#24452;&#65292;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20855;&#26377;&#20511;&#37492;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38405;&#35835;&#26102;&#30340;&#30524;&#21160;&#25581;&#31034;&#20102;&#38405;&#35835;&#32773;&#30340;&#35748;&#30693;&#36807;&#31243;&#21644;&#25152;&#38405;&#35835;&#25991;&#26412;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#38405;&#35835;&#20013;&#25195;&#35270;&#36335;&#24452;&#30340;&#20998;&#26512;&#24050;&#24341;&#36215;&#21508;&#20010;&#39046;&#22495;&#30340;&#20851;&#27880;&#65292;&#28085;&#30422;&#20102;&#20174;&#35748;&#30693;&#31185;&#23398;&#21040;&#35821;&#35328;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#38405;&#35835;&#26102;&#20154;&#31867;&#30340;&#25195;&#35270;&#36335;&#24452;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#23427;&#20204;&#26159;&#30001;&#21452;&#24207;&#21015;&#32452;&#25104;&#30340;&#65306;&#21333;&#35789;&#25353;&#29031;&#35821;&#35328;&#30340;&#35821;&#27861;&#35268;&#21017;&#25490;&#24207;&#65292;&#32780;&#27880;&#35270;&#21017;&#25353;&#29031;&#26102;&#38388;&#39034;&#24207;&#25490;&#24207;&#12290;&#20154;&#31867;&#24182;&#19981;&#20005;&#26684;&#25353;&#24038;&#21040;&#21491;&#30340;&#39034;&#24207;&#38405;&#35835;&#65292;&#32780;&#26159;&#36339;&#36807;&#25110;&#37325;&#22797;&#27880;&#35270;&#21333;&#35789;&#65292;&#24182;&#20498;&#36864;&#21040;&#20197;&#21069;&#30340;&#21333;&#35789;&#65292;&#35821;&#35328;&#24207;&#21015;&#21644;&#26102;&#38388;&#24207;&#21015;&#30340;&#23545;&#40784;&#24182;&#19981;&#23481;&#26131;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;Eyettention&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21516;&#26102;&#22788;&#29702;&#35821;&#35328;&#24207;&#21015;&#21644;&#26102;&#38388;&#24207;&#21015;&#30340;&#21452;&#24207;&#21015;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Eye movements during reading offer insights into both the reader's cognitive processes and the characteristics of the text that is being read. Hence, the analysis of scanpaths in reading have attracted increasing attention across fields, ranging from cognitive science over linguistics to computer science. In particular, eye-tracking-while-reading data has been argued to bear the potential to make machine-learning-based language models exhibit a more human-like linguistic behavior. However, one of the main challenges in modeling human scanpaths in reading is their dual-sequence nature: the words are ordered following the grammatical rules of the language, whereas the fixations are chronologically ordered. As humans do not strictly read from left-to-right, but rather skip or refixate words and regress to previous words, the alignment of the linguistic and the temporal sequence is non-trivial. In this paper, we develop Eyettention, the first dual-sequence model that simultaneously process
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26469;&#33258;OpenAI&#27169;&#22411;API&#20197;&#21450;New Bing&#22686;&#24378;&#29256;&#30340;ChatGPT&#23545;&#38544;&#31169;&#24102;&#26469;&#30340;&#39118;&#38505;&#65292;&#25351;&#20986;&#24212;&#29992;&#31243;&#24207;&#38598;&#25104;&#30340;LLMs&#21487;&#33021;&#23548;&#33268;&#27604;&#20197;&#24448;&#26356;&#20005;&#37325;&#30340;&#38544;&#31169;&#23041;&#32961;&#65292;&#23454;&#39564;&#25903;&#25345;&#35813;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.05197</link><description>&lt;p&gt;
Multi-step Jailbreaking Privacy Attacks on ChatGPT
&lt;/p&gt;
&lt;p&gt;
Multi-step Jailbreaking Privacy Attacks on ChatGPT. (arXiv:2304.05197v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26469;&#33258;OpenAI&#27169;&#22411;API&#20197;&#21450;New Bing&#22686;&#24378;&#29256;&#30340;ChatGPT&#23545;&#38544;&#31169;&#24102;&#26469;&#30340;&#39118;&#38505;&#65292;&#25351;&#20986;&#24212;&#29992;&#31243;&#24207;&#38598;&#25104;&#30340;LLMs&#21487;&#33021;&#23548;&#33268;&#27604;&#20197;&#24448;&#26356;&#20005;&#37325;&#30340;&#38544;&#31169;&#23041;&#32961;&#65292;&#23454;&#39564;&#25903;&#25345;&#35813;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36805;&#36895;&#21457;&#23637;&#65292;&#35768;&#22810;&#19979;&#28216;NLP&#20219;&#21153;&#21487;&#20197;&#36890;&#36807;&#33391;&#22909;&#30340;&#25552;&#31034;&#24471;&#21040;&#24456;&#22909;&#30340;&#35299;&#20915;&#12290;&#23613;&#31649;&#27169;&#22411;&#24320;&#21457;&#20154;&#21592;&#21644;&#30740;&#31350;&#20154;&#21592;&#21162;&#21147;&#30830;&#20445;&#36991;&#20813;&#20174;LLMs&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#65292;&#20294;&#20173;&#28982;&#38590;&#20197;&#24341;&#23548;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#65288;AIGC&#65289;&#20026;&#20154;&#31867;&#24102;&#26469;&#22909;&#22788;&#12290;&#30001;&#20110;&#24378;&#22823;&#30340;LLMs&#27491;&#22312;&#21534;&#22124;&#26469;&#33258;&#21508;&#20010;&#39046;&#22495;&#30340;&#29616;&#26377;&#25991;&#26412;&#25968;&#25454;&#65288;&#20363;&#22914;&#65292;GPT-3&#35757;&#32451;&#20102;45TB&#30340;&#25991;&#26412;&#65289;&#65292;&#22240;&#27492;&#20154;&#20204;&#33258;&#28982;&#20250;&#24576;&#30097;&#35757;&#32451;&#25968;&#25454;&#20013;&#26159;&#21542;&#21253;&#21547;&#31169;&#20154;&#20449;&#24687;&#20197;&#21450;&#36825;&#20123;LLMs&#21450;&#20854;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#24102;&#26469;&#20160;&#20040;&#38544;&#31169;&#23041;&#32961;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26469;&#33258;OpenAI&#30340;&#27169;&#22411;API&#21644;&#36890;&#36807;ChatGPT&#22686;&#24378;&#30340;New Bing&#25152;&#24102;&#26469;&#30340;&#38544;&#31169;&#23041;&#32961;&#65292;&#24182;&#26174;&#31034;&#24212;&#29992;&#31243;&#24207;&#38598;&#25104;&#30340;LLMs&#21487;&#33021;&#23548;&#33268;&#27604;&#20197;&#24448;&#26356;&#20005;&#37325;&#30340;&#38544;&#31169;&#23041;&#32961;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#35828;&#27861;&#65292;&#24182;&#35752;&#35770;LLMs&#30340;&#38544;&#31169;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid progress of large language models (LLMs), many downstream NLP tasks can be well solved given good prompts. Though model developers and researchers work hard on dialog safety to avoid generating harmful content from LLMs, it is still challenging to steer AI-generated content (AIGC) for the human good. As powerful LLMs are devouring existing text data from various domains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether the private information is included in the training data and what privacy threats can these LLMs and their downstream applications bring. In this paper, we study the privacy threats from OpenAI's model APIs and New Bing enhanced by ChatGPT and show that application-integrated LLMs may cause more severe privacy threats ever than before. To this end, we conduct extensive experiments to support our claims and discuss LLMs' privacy implications.
&lt;/p&gt;</description></item><item><title>HATELEXICON&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#20167;&#24680;&#35328;&#35770;&#30340;&#35789;&#27719;&#34920;&#65292;&#21033;&#29992;&#20854;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.01890</link><description>&lt;p&gt;
&#31038;&#20250;&#25991;&#21270;&#30693;&#35782;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#20013;&#23545;&#36873;&#39033;&#30340;&#36873;&#25321;&#26159;&#24517;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
Sociocultural knowledge is needed for selection of shots in hate speech detection tasks. (arXiv:2304.01890v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01890
&lt;/p&gt;
&lt;p&gt;
HATELEXICON&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#20167;&#24680;&#35328;&#35770;&#30340;&#35789;&#27719;&#34920;&#65292;&#21033;&#29992;&#20854;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;HATELEXICON&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#30340;&#34065;&#31216;&#21644;&#20167;&#24680;&#35328;&#35770;&#30446;&#26631;&#30340;&#35789;&#27719;&#34920;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#22914;&#20309;&#29992;&#20110;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#65292;&#34920;&#26126;&#21457;&#23637;&#29992;&#20110;&#20998;&#31867;&#26497;&#31471;&#35328;&#35770;&#30340;&#27169;&#22411;&#65292;&#22312;&#36827;&#34892;&#39044;&#27979;&#26102;&#20005;&#37325;&#20381;&#36182;&#30446;&#26631;&#35789;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;HATELEXICON&#26469;&#36741;&#21161;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#35757;&#32451;&#36873;&#39033;&#30340;&#26041;&#27861;&#65292;&#36873;&#39033;&#36873;&#25321;&#22312;&#23567;&#26679;&#26412;&#23398;&#20064;&#20013;&#23588;&#20026;&#37325;&#35201;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;HASOC&#25968;&#25454;&#23545;&#24503;&#35821;&#21644;&#21360;&#22320;&#35821;&#36827;&#34892;&#20102;&#20960;&#20010;&#31034;&#33539;&#23398;&#20064;&#65292;&#24182;&#23558;Multilingual HateCheck&#65288;MHC&#65289;&#20316;&#20026;&#22522;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26681;&#25454;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#36873;&#25321;&#26679;&#26412;&#65292;&#30456;&#23545;&#20110;&#38543;&#26426;&#37319;&#26679;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22312;MHC&#19978;&#34920;&#29616;&#12290;&#22240;&#27492;&#65292;&#24403;&#20165;&#26377;&#23569;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#26102;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#26469;&#36873;&#25321;&#21253;&#21547;&#26356;&#22810;&#31038;&#20250;&#25991;&#21270;&#20449;&#24687;&#30340;&#26679;&#26412;&#33021;&#22815;&#26356;&#22909;&#22320;&#25552;&#39640;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for the countries of Brazil, Germany, India and Kenya, to aid training and interpretability of models. We demonstrate how our lexicon can be used to interpret model predictions, showing that models developed to classify extreme speech rely heavily on target words when making predictions. Further, we propose a method to aid shot selection for training in low-resource settings via HATELEXICON. In few-shot learning, the selection of shots is of paramount importance to model performance. In our work, we simulate a few-shot setting for German and Hindi, using HASOC data for training and the Multilingual HateCheck (MHC) as a benchmark. We show that selecting shots based on our lexicon leads to models performing better on MHC than models trained on shots sampled randomly. Thus, when given only a few training examples, using our lexicon to select shots containing more sociocultural information leads to better few-shot perf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#32534;&#35793;&#31070;&#32463;&#32593;&#32476;CoNNs&#24182;&#20837;&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#20013;&#65292;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#21512;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#25277;&#35937;&#35268;&#21017;&#30340;&#39046;&#22495;&#12290;&#26041;&#27861;&#31216;&#20026;&#8220;&#31070;&#32463;&#29702;&#35299;&#8221;&#65292;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#31526;&#21495;&#25805;&#20316;&#12289;&#35268;&#21017;&#25512;&#29702;&#12289;&#31639;&#26415;&#25512;&#29702;&#31561;&#26041;&#38754;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.01665</link><description>&lt;p&gt;
&#32534;&#35793;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#27169;&#22411;&#65306;&#31070;&#32463;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Neural Comprehension: Language Models with Compiled Neural Networks. (arXiv:2304.01665v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#32534;&#35793;&#31070;&#32463;&#32593;&#32476;CoNNs&#24182;&#20837;&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#20013;&#65292;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#21512;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#25277;&#35937;&#35268;&#21017;&#30340;&#39046;&#22495;&#12290;&#26041;&#27861;&#31216;&#20026;&#8220;&#31070;&#32463;&#29702;&#35299;&#8221;&#65292;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#31526;&#21495;&#25805;&#20316;&#12289;&#35268;&#21017;&#25512;&#29702;&#12289;&#31639;&#26415;&#25512;&#29702;&#31561;&#26041;&#38754;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#65292;&#20294;&#20854;&#36827;&#34892;&#31526;&#21495;&#25805;&#20316;&#21644;&#31639;&#26415;&#25805;&#20316;&#30340;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#65292;&#36825;&#24402;&#22240;&#20110;&#23427;&#20204;&#38544;&#24335;&#22320;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#35268;&#21017;&#12290;&#25105;&#20204;&#25506;&#35752;&#22914;&#20309;&#23558;&#29305;&#21035;&#35774;&#35745;&#24471;&#21040;&#30340;&#21152;&#26435;&#30340;&#32534;&#35793;&#31070;&#32463;&#32593;&#32476;&#65288;CoNNs&#65289;&#24182;&#20837;&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#20013;&#65292;&#20351;&#24471;&#36890;&#36807;&#26799;&#24230;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#33719;&#24471;&#23436;&#20840;&#30340;&#35268;&#21017;&#29702;&#35299;&#33021;&#21147;&#12290;&#32534;&#35793;&#31070;&#32463;&#32593;&#32476;&#30340;&#24182;&#20837;&#20026;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#21512;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#25277;&#35937;&#35268;&#21017;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;&#8220;&#31070;&#32463;&#29702;&#35299;&#8221;&#65292;&#26377;&#21161;&#20110;&#35821;&#35328;&#27169;&#22411;&#22312;&#31526;&#21495;&#25805;&#20316;&#12289;&#35268;&#21017;&#25512;&#29702;&#12289;&#31639;&#26415;&#25512;&#29702;&#31561;&#26041;&#38754;&#23454;&#29616;&#32477;&#23545;&#20934;&#30830;&#24230;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#21487;&#29992;&#65306;\url{https://github.com/...}
&lt;/p&gt;
&lt;p&gt;
Language models have achieved impressive results in natural language processing tasks, but their ability to perform symbolic operations and arithmetic operations, remains limited, which attribute to their learn the rules implicitly from data. We explore how to incorporate compiled neural networks (CoNNs) which weight is specially designed, into the architecture of language models to enable the language model trained by gradient to obtain fully rule comprehension ability. The incorporation of compiled neural networks offers a promising direction for improving the performance of language models on compound tasks, particularly in areas that require a deeper comprehension of abstract rules beyond recognizing patterns in training data. Our method, which call "Neural Comprehension", helps language models achieve absolute accuracy in symbolic operations, thereby enhancing their ability for rule reasoning, symbolic reasoning, and arithmetic reasoning. Our code is publicly available at: \url{ht
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#36890;&#36807;&#35821;&#20041;&#35299;&#26512;&#33719;&#21462;&#30340;&#31526;&#21495;&#35821;&#20041;&#34920;&#31034;&#23545;&#20110;&#25512;&#29702;&#23454;&#20307;&#29366;&#24577;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#31526;&#21495;&#35299;&#26512;&#21644;&#35821;&#20041;&#35299;&#26512;&#20449;&#24687;&#30340;&#36807;&#31243;&#25512;&#29702;&#26694;&#26550;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#34701;&#21512;&#35821;&#20041;&#30693;&#35782;&#21487;&#25552;&#39640;&#36807;&#31243;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.06829</link><description>&lt;p&gt;
&#35821;&#20041;&#35299;&#26512;&#22312;&#29702;&#35299;&#36807;&#31243;&#25991;&#26412;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Role of Semantic Parsing in Understanding Procedural Text. (arXiv:2302.06829v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#36890;&#36807;&#35821;&#20041;&#35299;&#26512;&#33719;&#21462;&#30340;&#31526;&#21495;&#35821;&#20041;&#34920;&#31034;&#23545;&#20110;&#25512;&#29702;&#23454;&#20307;&#29366;&#24577;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#31526;&#21495;&#35299;&#26512;&#21644;&#35821;&#20041;&#35299;&#26512;&#20449;&#24687;&#30340;&#36807;&#31243;&#25512;&#29702;&#26694;&#26550;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#34701;&#21512;&#35821;&#20041;&#30693;&#35782;&#21487;&#25552;&#39640;&#36807;&#31243;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#28145;&#24230;&#35821;&#20041;&#20998;&#26512;&#22120;&#20013;&#25552;&#21462;&#30340;&#31526;&#21495;&#35821;&#20041;&#34920;&#31034;&#26159;&#21542;&#26377;&#21161;&#20110;&#25512;&#29702;&#25152;&#28041;&#21450;&#23454;&#20307;&#30340;&#29366;&#24577;&#12290;&#25105;&#20204;&#32771;&#34385;TRIPS&#65288;&#19968;&#31181;&#28145;&#24230;&#35821;&#20041;&#20998;&#26512;&#22120;&#65289;&#21644;&#35821;&#20041;&#35282;&#33394;&#26631;&#35760;&#20316;&#20026;&#35821;&#20041;&#35299;&#26512;&#30340;&#20004;&#20010;&#26469;&#28304;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31526;&#21495;&#35299;&#26512;&#30340;&#36807;&#31243;&#25512;&#29702;&#26694;&#26550;PROPOLIS&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#35821;&#20041;&#35299;&#26512;&#20449;&#24687;&#38598;&#25104;&#21040;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#27169;&#22411;&#20013;&#36827;&#34892;&#36807;&#31243;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#26126;&#30830;&#22320;&#34701;&#21512;&#36825;&#31181;&#35821;&#20041;&#30693;&#35782;&#21487;&#20197;&#25552;&#39640;&#36807;&#31243;&#29702;&#35299;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#36807;&#31243;&#25512;&#29702;&#20219;&#21153;&#30340;&#26032;&#25351;&#26631;&#65292;&#38416;&#26126;&#20102;&#25361;&#25112;&#24182;&#35782;&#21035;&#20102;&#31070;&#32463;&#12289;&#31526;&#21495;&#21644;&#38598;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate whether symbolic semantic representations, extracted from deep semantic parsers, can help reasoning over the states of involved entities in a procedural text. We consider a deep semantic parser~(TRIPS) and semantic role labeling as two sources of semantic parsing knowledge. First, we propose PROPOLIS, a symbolic parsing-based procedural reasoning framework. Second, we integrate semantic parsing information into state-of-the-art neural models to conduct procedural reasoning. Our experiments indicate that explicitly incorporating such semantic knowledge improves procedural understanding. This paper presents new metrics for evaluating procedural reasoning tasks that clarify the challenges and identify differences among neural, symbolic, and integrated models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#20219;&#21153;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#35299;&#20915;&#21270;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#21333;&#19968;&#22495;&#25110;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2301.12586</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20219;&#21153;&#35821;&#35328;&#24314;&#27169;&#32479;&#19968;&#20998;&#23376;&#21644;&#25991;&#26412;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Unifying Molecular and Textual Representations via Multi-task Language Modelling. (arXiv:2301.12586v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#20219;&#21153;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#35299;&#20915;&#21270;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#21333;&#19968;&#22495;&#25110;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21270;&#23398;&#39046;&#22495;&#65292;&#36890;&#36807;&#20026;&#20998;&#23376;&#35774;&#35745;&#21644;&#21512;&#25104;&#35268;&#21010;&#25552;&#20379;&#29983;&#25104;&#24335;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#26032;&#26041;&#27861;&#26377;&#28508;&#21147;&#25512;&#21160;&#25968;&#25454;&#39537;&#21160;&#30340;&#31185;&#23398;&#21457;&#29616;&#30340;&#26032;&#26102;&#20195;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#20219;&#21153;&#20173;&#28982;&#38656;&#35201;&#19987;&#38376;&#30340;&#27169;&#22411;&#65292;&#23548;&#33268;&#38656;&#35201;&#29305;&#23450;&#38382;&#39064;&#30340;&#24494;&#35843;&#65292;&#24182;&#24573;&#35270;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#38556;&#30861;&#26159;&#33258;&#28982;&#35821;&#35328;&#21644;&#21270;&#23398;&#34920;&#31034;&#20043;&#38388;&#32570;&#20047;&#32479;&#19968;&#34920;&#31034;&#65292;&#20174;&#32780;&#20351;&#20154;&#26426;&#20132;&#20114;&#21464;&#24471;&#22797;&#26434;&#21644;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#22495;&#12289;&#22810;&#20219;&#21153;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#35299;&#20915;&#21270;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#21270;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#21333;&#19968;&#22495;&#25110;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;&#39046;&#22495;&#20043;&#38388;&#20849;&#20139;&#26435;&#37325;&#20250;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#20135;&#29983;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#25104;&#21151;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20998;&#23376;&#29983;&#25104;&#12289;&#21453;&#21512;&#25104;&#39044;&#27979;&#12289;&#21270;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26412;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in neural language models have also been successfully applied to the field of chemistry, offering generative solutions for classical problems in molecular design and synthesis planning. These new methods have the potential to fuel a new era of data-driven automation in scientific discovery. However, specialized models are still typically required for each task, leading to the need for problem-specific fine-tuning and neglecting task interrelations. The main obstacle in this field is the lack of a unified representation between natural language and chemical representations, complicating and limiting human-machine interaction. Here, we propose the first multi-domain, multi-task language model that can solve a wide range of tasks in both the chemical and natural language domains. Our model can handle chemical and natural language concurrently, without requiring expensive pre-training on single domains or task-specific models. Interestingly, sharing weights across domai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#30340;&#26041;&#27861;&#29992;&#20110;&#20998;&#31867;&#36923;&#36753;&#35884;&#35823;&#30340;&#26032;&#26696;&#20363;&#65292;&#36890;&#36807;&#22522;&#20110;&#35821;&#35328;&#24314;&#27169;&#30340;&#26816;&#32034;&#21644;&#21382;&#21490;&#26696;&#20363;&#30340;&#35843;&#25972;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.11879</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26696;&#20363;&#25512;&#29702;&#22312;&#36923;&#36753;&#35884;&#35823;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Case-Based Reasoning with Language Models for Classification of Logical Fallacies. (arXiv:2301.11879v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#30340;&#26041;&#27861;&#29992;&#20110;&#20998;&#31867;&#36923;&#36753;&#35884;&#35823;&#30340;&#26032;&#26696;&#20363;&#65292;&#36890;&#36807;&#22522;&#20110;&#35821;&#35328;&#24314;&#27169;&#30340;&#26816;&#32034;&#21644;&#21382;&#21490;&#26696;&#20363;&#30340;&#35843;&#25972;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32593;&#32476;&#19978;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#21644;&#23459;&#20256;&#30340;&#23481;&#26131;&#21644;&#24555;&#25463;&#24615;&#20419;&#20351;&#25105;&#20204;&#38656;&#35201;&#24320;&#21457;&#21487;&#38752;&#30340;&#25216;&#26415;&#26469;&#26816;&#27979;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#20013;&#30340;&#35884;&#35823;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#22312;&#38656;&#35201;&#22797;&#26434;&#25512;&#29702;&#30340;&#36923;&#36753;&#35884;&#35823;&#20998;&#31867;&#31561;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#32570;&#20047;&#40065;&#26834;&#24615;&#30340;&#29305;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22522;&#20110;&#35821;&#35328;&#24314;&#27169;&#30340;&#26816;&#32034;&#21644;&#21382;&#21490;&#26696;&#20363;&#30340;&#35843;&#25972;&#26469;&#20998;&#31867;&#36923;&#36753;&#35884;&#35823;&#30340;&#26032;&#26696;&#20363;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22235;&#31181;&#20114;&#34917;&#30340;&#31574;&#30053;&#65292;&#22522;&#20110;&#26377;&#20851;&#30446;&#26631;&#12289;&#35299;&#37322;&#12289;&#21453;&#39539;&#21644;&#35770;&#35777;&#32467;&#26500;&#30340;&#22806;&#37096;&#20449;&#24687;&#26469;&#20016;&#23500;&#25105;&#20204;&#27169;&#22411;&#30340;&#36755;&#20837;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#31867;&#20284;&#26696;&#20363;&#30340;&#34920;&#31034;&#23545;&#27169;&#22411;&#24615;&#33021;&#26377;&#24456;&#22823;&#30340;&#24433;&#21709;&#65292;&#36739;&#23569;&#30340;&#21382;&#21490;&#26696;&#20363;&#20063;&#33021;&#20351;&#27169;&#22411;&#26377;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ease and speed of spreading misinformation and propaganda on the Web motivate the need to develop trustworthy technology for detecting fallacies in natural language arguments. However, state-of-the-art language modeling methods exhibit a lack of robustness on tasks like logical fallacy classification that require complex reasoning. In this paper, we propose a Case-Based Reasoning method that classifies new cases of logical fallacy by language-modeling-driven retrieval and adaptation of historical cases. We design four complementary strategies to enrich input representation for our model, based on external information about goals, explanations, counterarguments, and argument structure. Our experiments in in-domain and out-of-domain settings indicate that Case-Based Reasoning improves the accuracy and generalizability of language models. Our ablation studies suggest that representations of similar cases have a strong impact on the model performance, that models perform well with fewe
&lt;/p&gt;</description></item><item><title>MolGen&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#21069;&#32512;&#35843;&#25972;&#21644;&#33258;&#25105;&#21453;&#39304;&#30340;&#33539;&#24335;&#65292;&#23454;&#29616;&#20102;&#21270;&#23398;&#26377;&#25928;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#26032;&#39062;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#31361;&#30772;&#65292;&#22312;&#20998;&#23376;&#29983;&#25104;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11259</link><description>&lt;p&gt;
&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#29983;&#25104;&#19982;&#33258;&#25105;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Domain-Agnostic Molecular Generation with Self-feedback. (arXiv:2301.11259v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11259
&lt;/p&gt;
&lt;p&gt;
MolGen&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#21069;&#32512;&#35843;&#25972;&#21644;&#33258;&#25105;&#21453;&#39304;&#30340;&#33539;&#24335;&#65292;&#23454;&#29616;&#20102;&#21270;&#23398;&#26377;&#25928;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#26032;&#39062;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#31361;&#30772;&#65292;&#22312;&#20998;&#23376;&#29983;&#25104;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#30340;&#29983;&#25104;&#24050;&#32463;&#21463;&#21040;&#26497;&#22823;&#30340;&#20851;&#27880;&#65292;&#20854;&#38761;&#26032;&#20102;&#31185;&#23398;&#23478;&#35774;&#35745;&#20998;&#23376;&#32467;&#26500;&#30340;&#26041;&#24335;&#65292;&#24182;&#20026;&#21270;&#23398;&#21644;&#33647;&#29289;&#35774;&#35745;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#20998;&#23376;&#29983;&#25104;&#20013;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#27604;&#22914;&#29983;&#25104;&#35821;&#27861;&#25110;&#21270;&#23398;&#23384;&#22312;&#32570;&#38519;&#30340;&#20998;&#23376;&#65292;&#29421;&#31364;&#30340;&#39046;&#22495;&#19987;&#27880;&#20197;&#21450;&#30001;&#20110;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#25110;&#22806;&#37096;&#20998;&#23376;&#25968;&#25454;&#24211;&#32780;&#38480;&#21046;&#20102;&#29983;&#25104;&#22810;&#26679;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MolGen&#65292;&#23427;&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#20998;&#23376;&#35821;&#35328;&#27169;&#22411;&#12290;MolGen&#36890;&#36807;&#37325;&#26500;&#19968;&#20159;&#22810;&#20010;&#20998;&#23376;SELFIES&#33719;&#24471;&#20102;&#22266;&#26377;&#30340;&#32467;&#26500;&#21644;&#35821;&#27861;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#21069;&#32512;&#35843;&#25972;&#20419;&#36827;&#20102;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#21453;&#39304;&#33539;&#24335;&#65292;&#21551;&#21457;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#26368;&#32456;&#19979;&#28216;&#30446;&#26631;&#23545;&#40784;&#65292;&#26377;&#21161;&#20110;&#26356;&#31283;&#20581;&#21644;&#39640;&#25928;&#30340;&#20998;&#23376;&#29983;&#25104;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MolGen&#22312;&#21270;&#23398;&#26377;&#25928;&#24615;&#65292;&#22810;&#26679;&#24615;&#65292;&#26032;&#39062;&#24615;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of molecules with desired properties has gained tremendous popularity, revolutionizing the way scientists design molecular structures and providing valuable support for chemical and drug design. However, despite the potential of language models in molecule generation, they face numerous challenges such as the generation of syntactically or chemically flawed molecules, narrow domain focus, and limitations in creating diverse and directionally feasible molecules due to a dearth of annotated data or external molecular databases. To this end, we introduce MolGen, a pre-trained molecular language model tailored specifically for molecule generation. MolGen acquires intrinsic structural and grammatical insights by reconstructing over 100 million molecular SELFIES, while facilitating knowledge transfer between different domains through domain-agnostic molecular prefix tuning. Moreover, we present a self-feedback paradigm that inspires the pre-trained model to align with the ulti
&lt;/p&gt;</description></item><item><title>ClarifyDelphi&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#33021;&#22815;&#38024;&#23545;&#31038;&#20250;&#25110;&#36947;&#24503;&#24773;&#22659;&#25552;&#20986;&#26368;&#26377;&#20449;&#24687;&#20215;&#20540;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22870;&#21169;&#26426;&#21046;&#26368;&#22823;&#21270;&#22238;&#31572;&#38382;&#39064;&#26102;&#30340;&#36947;&#24503;&#21028;&#26029;&#20998;&#27495;&#12290;</title><link>http://arxiv.org/abs/2212.10409</link><description>&lt;p&gt;
ClarifyDelphi&#65306;&#38024;&#23545;&#31038;&#20250;&#21644;&#36947;&#24503;&#24773;&#22659;&#30340;&#24378;&#21270;&#28548;&#28165;&#38382;&#39064;&#19982;&#20248;&#20808;&#32771;&#34385;&#23545;&#25239;&#30340;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
ClarifyDelphi: Reinforced Clarification Questions with Defeasibility Rewards for Social and Moral Situations. (arXiv:2212.10409v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10409
&lt;/p&gt;
&lt;p&gt;
ClarifyDelphi&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#33021;&#22815;&#38024;&#23545;&#31038;&#20250;&#25110;&#36947;&#24503;&#24773;&#22659;&#25552;&#20986;&#26368;&#26377;&#20449;&#24687;&#20215;&#20540;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22870;&#21169;&#26426;&#21046;&#26368;&#22823;&#21270;&#22238;&#31572;&#38382;&#39064;&#26102;&#30340;&#36947;&#24503;&#21028;&#26029;&#20998;&#27495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#30340;&#37325;&#35201;&#24615;&#19981;&#35328;&#32780;&#21947;&#65292;&#29978;&#33267;&#22312;&#24120;&#35782;&#36947;&#24503;&#25512;&#29702;&#20013;&#20063;&#26159;&#22914;&#27492;&#12290;&#25913;&#21464;&#19978;&#19979;&#25991;&#21487;&#33021;&#20250;&#39072;&#20498;&#19968;&#39033;&#34892;&#20026;&#30340;&#36947;&#24503;&#21028;&#26029;;&#8220;&#23545;&#26379;&#21451;&#25746;&#35854;&#8221;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#19981;&#23545;&#30340;&#65292;&#20294;&#22914;&#26524;&#26088;&#22312;&#20445;&#25252;&#20182;&#20204;&#30340;&#29983;&#21629;&#65292;&#23601;&#21487;&#33021;&#26159;&#36947;&#24503;&#19978;&#21487;&#25509;&#21463;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ClarifyDelphi&#65292;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#23427;&#23398;&#20064;&#25552;&#20986;&#28548;&#28165;&#38382;&#39064;&#65288;&#20363;&#22914;&#65292;&#20320;&#20026;&#20160;&#20040;&#35201;&#23545;&#20320;&#30340;&#26379;&#21451;&#25746;&#35854;&#65311;&#65289;&#20197;&#33719;&#21462;&#31038;&#20250;&#25110;&#36947;&#24503;&#24773;&#22659;&#30340;&#20854;&#20182;&#37325;&#35201;&#20449;&#24687;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20854;&#28508;&#22312;&#31572;&#26696;&#23548;&#33268;&#36947;&#24503;&#21028;&#26029;&#26377;&#25152;&#20998;&#27495;&#30340;&#38382;&#39064;&#26159;&#26368;&#26377;&#20449;&#24687;&#20215;&#20540;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20855;&#26377;&#23545;&#25239;&#24615;&#22870;&#21169;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#22238;&#31572;&#38382;&#39064;&#26102;&#30340;&#36947;&#24503;&#21028;&#26029;&#20998;&#27495;&#12290;&#20154;&#31867;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#31454;&#20105;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#29983;&#25104;&#30340;&#38382;&#39064;&#26356;&#30456;&#20851;&#12289;&#26356;&#26377;&#20449;&#24687;&#20215;&#20540;&#21644;&#26356;&#20855;&#20248;&#32988;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26368;&#32456;&#21463;&#21040;&#35748;&#30693;&#31185;&#23398;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#36947;&#24503;&#35748;&#30693;&#30340;&#28789;&#27963;&#24615;&#65288;&#21363;&#33021;&#22815;&#32435;&#20837;&#26032;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#30456;&#24212;&#22320;&#20462;&#25913;&#36947;&#24503;&#21028;&#26029;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context is everything, even in commonsense moral reasoning. Changing contexts can flip the moral judgment of an action; "Lying to a friend" is wrong in general, but may be morally acceptable if it is intended to protect their life.  We present ClarifyDelphi, an interactive system that learns to ask clarification questions (e.g., why did you lie to your friend?) in order to elicit additional salient contexts of a social or moral situation. We posit that questions whose potential answers lead to diverging moral judgments are the most informative. Thus, we propose a reinforcement learning framework with a defeasibility reward that aims to maximize the divergence between moral judgments of hypothetical answers to a question. Human evaluation demonstrates that our system generates more relevant, informative and defeasible questions compared to competitive baselines. Our work is ultimately inspired by studies in cognitive science that have investigated the flexibility in moral cognition (i.e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28176;&#36827;&#24335;&#33258;&#27880;&#24847;&#21147;&#21098;&#26525;&#30340;&#21387;&#32553;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;GRAIN&#65292;&#36890;&#36807;&#25191;&#34892;&#20219;&#21153;&#29305;&#23450;&#30340;&#21098;&#26525;&#21644;&#30693;&#35782;&#33976;&#39311;&#65292;&#21487;&#20197;&#24471;&#21040;&#39640;&#25928;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;GLUE&#12289;SQuAD&#21644;CoNLL 2003&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2212.07634</link><description>&lt;p&gt;
&#22522;&#20110;&#28176;&#36827;&#24335;&#33258;&#27880;&#24847;&#21147;&#21098;&#26525;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Gradient-based Intra-attention Pruning on Pre-trained Language Models. (arXiv:2212.07634v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28176;&#36827;&#24335;&#33258;&#27880;&#24847;&#21147;&#21098;&#26525;&#30340;&#21387;&#32553;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;GRAIN&#65292;&#36890;&#36807;&#25191;&#34892;&#20219;&#21153;&#29305;&#23450;&#30340;&#21098;&#26525;&#21644;&#30693;&#35782;&#33976;&#39311;&#65292;&#21487;&#20197;&#24471;&#21040;&#39640;&#25928;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;GLUE&#12289;SQuAD&#21644;CoNLL 2003&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#21331;&#36234;&#65292;&#20294;&#35745;&#31639;&#20195;&#20215;&#24040;&#22823;&#12290;&#20026;&#27492;&#65292;&#20154;&#20204;&#24320;&#21457;&#20102;&#21098;&#26525;&#21644;&#30693;&#35782;&#33976;&#39311;&#31561;&#25216;&#26415;&#26469;&#20943;&#23567;&#27169;&#22411;&#30340;&#20307;&#31215;&#21644;&#24310;&#36831;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#32467;&#26500;&#30340;&#21098;&#26525;&#26041;&#27861;GRAIN&#65288;&#22522;&#20110;&#28176;&#36827;&#24335;&#33258;&#27880;&#24847;&#21147;&#21098;&#26525;&#65289;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#25191;&#34892;&#20219;&#21153;&#29305;&#23450;&#21098;&#26525;&#65292;&#21487;&#20197;&#24471;&#21040;&#39640;&#25928;&#30340;&#27169;&#22411;&#12290;&#19982;&#36890;&#24120;&#21098;&#26525;&#27599;&#20010;&#27880;&#24847;&#21147;&#22836;&#30340;&#25972;&#20307;&#19981;&#21516;&#65292;GRAIN&#26816;&#26597;&#21644;&#21098;&#26525;&#20869;&#37096;&#33258;&#27880;&#24847;&#32467;&#26500;&#65292;&#26497;&#22823;&#25193;&#23637;&#20102;&#32467;&#26500;&#25628;&#32034;&#31354;&#38388;&#65292;&#20351;&#27169;&#22411;&#26356;&#21152;&#28789;&#27963;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26799;&#24230;&#20998;&#31163;&#31574;&#30053;&#65292;&#20197;&#20943;&#23569;&#33976;&#39311;&#23545;&#21098;&#26525;&#30340;&#24178;&#25200;&#65292;&#26356;&#22909;&#22320;&#32467;&#21512;&#36825;&#20004;&#31181;&#26041;&#27861;&#12290;&#22312;GLUE&#12289;SQuAD&#21644;CoNLL 2003&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;GRAIN&#26174;&#30528;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#31232;&#30095;&#24230;&#26465;&#20214;&#19979;&#65292;&#24182;&#19988;&#22312;&#20445;&#25345;$93\%\sim99\%$&#24615;&#33021;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;$6\sim7\times$&#30340;&#21152;&#36895;&#12290;&#22312;&#26497;&#31471;&#26465;&#20214;&#19979;&#65292;GRAIN&#20173;&#33021;&#20445;&#25345;&#19982;&#39640;&#24615;&#33021;LSTM&#30456;&#21516;&#30340;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models achieve superior performance but are computationally expensive. Techniques such as pruning and knowledge distillation have been developed to reduce their sizes and latencies. In this work, we propose a structured pruning method GRAIN (Gradient-based Intra-attention pruning), which performs task-specific pruning with knowledge distillation and yields highly effective models. Different from common approaches that prune each attention head as a whole, GRAIN inspects and prunes intra-attention structures, which greatly expands the structure search space and enables more flexible models. We also propose a gradient separation strategy that reduces the interference of distillation on pruning for a better combination of the two approaches. Experiments on GLUE, SQuAD, and CoNLL 2003 show that GRAIN notably outperforms other methods, especially in the high sparsity regime, and achieves $6\sim7\times$ speedups while maintaining $93\%\sim99\%$ performance. Under extreme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#39044;&#35757;&#32451;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#25913;&#36896;&#20026;&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#39044;&#35757;&#32451;&#19982;&#19979;&#28216;&#24494;&#35843;&#20043;&#38388;&#30340;&#24046;&#24322;&#21270;&#38382;&#39064;&#12290;PMR &#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#31168;&#65292;&#33021;&#26377;&#25928;&#25552;&#39640;&#39044;&#27979;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.04755</link><description>&lt;p&gt;
&#20174;Clozing&#21040;&#29702;&#35299;&#65306;&#23558;&#39044;&#35757;&#32451;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#25913;&#36896;&#20026;&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#38405;&#35835;&#22120;
&lt;/p&gt;
&lt;p&gt;
From Clozing to Comprehending: Retrofitting Pre-trained Masked Language Model to Pre-trained Machine Reader. (arXiv:2212.04755v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#39044;&#35757;&#32451;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#25913;&#36896;&#20026;&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#39044;&#35757;&#32451;&#19982;&#19979;&#28216;&#24494;&#35843;&#20043;&#38388;&#30340;&#24046;&#24322;&#21270;&#38382;&#39064;&#12290;PMR &#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#31168;&#65292;&#33021;&#26377;&#25928;&#25552;&#39640;&#39044;&#27979;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Pre-trained Machine Reader (PMR)&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#39044;&#35757;&#32451;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411; (MLMs) &#25913;&#36896;&#20026;&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299; (MRC) &#27169;&#22411;&#65292;&#26080;&#38656;&#33719;&#21462;&#26631;&#35760;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992; Wikipedia &#36229;&#38142;&#25509;&#26500;&#24314;&#20102;&#22823;&#37327;&#36890;&#29992;&#19988;&#39640;&#36136;&#37327;&#30340; MRC &#39118;&#26684;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010; Wiki Anchor Extraction &#20219;&#21153;&#26469;&#25351;&#23548; MRC &#39118;&#26684;&#30340;&#39044;&#35757;&#32451;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377; MLMs &#27169;&#22411;&#39044;&#35757;&#32451;&#19982;&#19979;&#28216;&#24494;&#35843;&#20043;&#38388;&#30340;&#24046;&#24322;&#21270;&#38382;&#39064;&#12290;&#38500;&#20102;&#31616;&#21333;&#26131;&#29992;&#65292;PMR &#36824;&#33021;&#26377;&#25928;&#35299;&#20915;&#19968;&#20123;&#22914;&#25277;&#21462;&#24335;&#38382;&#31572;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31561;&#20219;&#21153;&#12290;PMR &#22312;&#29616;&#26377;&#26041;&#27861;&#26041;&#38754;&#26174;&#31034;&#20102;&#24040;&#22823;&#30340;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#12290;&#24403;&#24212;&#29992;&#20110; MRC &#20844;&#24335;&#20013;&#30340;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#26102;&#65292;PMR &#33021;&#22815;&#25552;&#21462;&#39640;&#36136;&#37327;&#30340;&#35777;&#26126;&#26448;&#26009;&#26469;&#35299;&#37322;&#20998;&#31867;&#36807;&#31243;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#39044;&#27979;&#21487;&#35299;&#37322;&#24615;&#12290;PMR &#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20063;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Pre-trained Machine Reader (PMR), a novel method for retrofitting pre-trained masked language models (MLMs) to pre-trained machine reading comprehension (MRC) models without acquiring labeled data. PMR can resolve the discrepancy between model pre-training and downstream fine-tuning of existing MLMs. To build the proposed PMR, we constructed a large volume of general-purpose and high-quality MRC-style training data by using Wikipedia hyperlinks and designed a Wiki Anchor Extraction task to guide the MRC-style pre-training. Apart from its simplicity, PMR effectively solves extraction tasks, such as Extractive Question Answering and Named Entity Recognition. PMR shows tremendous improvements over existing approaches, especially in low-resource scenarios. When applied to the sequence classification task in the MRC formulation, PMR enables the extraction of high-quality rationales to explain the classification process, thereby providing greater prediction explainability. PMR als
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#20919;&#21364;MBR&#65288;DC-MBR&#65289;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26631;&#31614;&#24179;&#28369;&#23545;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#65288;MBR&#65289;&#36896;&#25104;&#30340;&#33258;&#22238;&#24402;&#36807;&#24230;&#24179;&#28369;&#24615;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35843;&#25972;softmax&#28201;&#24230;&#26469;&#25805;&#32437;&#36755;&#20986;&#20998;&#24067;&#30340;&#29109;&#12290;&#26368;&#32456;&#65292;DC-MBR&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#34920;&#29616;&#20986;&#20247;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#22522;&#32447;&#25345;&#32493;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2212.04205</link><description>&lt;p&gt;
DC-MBR&#65306;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#30340;&#20998;&#24067;&#24335;&#20919;&#21364;
&lt;/p&gt;
&lt;p&gt;
DC-MBR: Distributional Cooling for Minimum Bayesian Risk Decoding. (arXiv:2212.04205v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#20919;&#21364;MBR&#65288;DC-MBR&#65289;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26631;&#31614;&#24179;&#28369;&#23545;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#65288;MBR&#65289;&#36896;&#25104;&#30340;&#33258;&#22238;&#24402;&#36807;&#24230;&#24179;&#28369;&#24615;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35843;&#25972;softmax&#28201;&#24230;&#26469;&#25805;&#32437;&#36755;&#20986;&#20998;&#24067;&#30340;&#29109;&#12290;&#26368;&#32456;&#65292;DC-MBR&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#34920;&#29616;&#20986;&#20247;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#22522;&#32447;&#25345;&#32493;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;(MBR)&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#34920;&#29616;&#20986;&#20247;&#65292;&#28982;&#32780;&#65292;&#22312;&#26631;&#31614;&#24179;&#28369;&#30340;&#24773;&#20917;&#19979;&#65292;MBR&#30340;&#34920;&#29616;&#24182;&#19981;&#29702;&#24819;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#26631;&#31614;&#24179;&#28369;&#22312;&#26463;&#25628;&#32034;&#20013;&#25552;&#20379;&#20102;&#19981;&#38169;&#30340;&#25913;&#36827;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#36890;&#29992;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#38382;&#39064;&#20986;&#29616;&#22312;&#26631;&#31614;&#24179;&#28369;&#22312;&#20196;&#29260;&#32423;&#21644;&#24207;&#21015;&#32423;&#20998;&#24067;&#19978;&#19981;&#19968;&#33268;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#26631;&#31614;&#24179;&#28369;&#21482;&#22312;&#20196;&#29260;&#32423;&#19978;&#36896;&#25104;&#36731;&#24494;&#21464;&#21270;&#65292;&#24207;&#21015;&#32423;&#20998;&#24067;&#20063;&#20250;&#39640;&#24230;&#20542;&#26012;&#12290;&#25105;&#20204;&#23558;&#27492;&#38382;&#39064;&#31216;&#20026;&#8220;&#33258;&#22238;&#24402;&#36807;&#24230;&#24179;&#28369;&#24615;&#8221;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20998;&#24067;&#24335;&#20919;&#21364;MBR&#65288;DC-MBR&#65289;&#65292;&#36890;&#36807;&#35843;&#25972;softmax&#28201;&#24230;&#26469;&#25805;&#32437;&#36755;&#20986;&#20998;&#24067;&#30340;&#29109;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#39044;&#35843;&#33410;&#26631;&#31614;&#24179;&#28369;&#22240;&#23376;&#21644;&#20998;&#24067;&#24335;&#20919;&#21364;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#12290;&#22312;NMT&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#20102;&#20998;&#24067;&#24335;&#20919;&#21364;&#26174;&#33879;&#20943;&#36731;&#20102;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#24182;&#25345;&#32493;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimum Bayesian Risk Decoding (MBR) emerges as a promising decoding algorithm in Neural Machine Translation. However, MBR performs poorly with label smoothing, which is surprising as label smoothing provides decent improvement with beam search and improves generality in various tasks. In this work, we show that the issue arises from the un-consistency of label smoothing on the token-level and sequence-level distributions. We demonstrate that even though label smoothing only causes a slight change in the token-level, the sequence-level distribution is highly skewed. We coin the issue \emph{autoregressive over-smoothness}. To address this issue, we propose a simple and effective method, Distributional Cooling MBR (DC-MBR), which manipulates the entropy of output distributions by tuning down the Softmax temperature. We theoretically prove the equivalence between pre-tuning label smoothing factor and distributional cooling. Extensive experiments on NMT benchmarks validate that distributio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#25226;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36880;&#27493;&#25512;&#29702;&#33021;&#21147;&#33976;&#39311;&#21040;&#26356;&#23567;&#30340;&#27169;&#22411;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#25512;&#29702;&#26041;&#26696;&#65292;&#20351;&#29992;&#33487;&#26684;&#25289;&#24213;&#24335;CoT&#26469;&#35757;&#32451;&#20004;&#20010;&#23567;&#22411;&#33976;&#39311;&#27169;&#22411;&#30340;&#32452;&#21512;&#65292;&#21487;&#20197;&#29992;&#26469;&#20998;&#35299;&#21644;&#35299;&#20915;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#19988;&#22312;&#22810;&#20010;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#39640;&#31934;&#24230;&#30340;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#65292;&#32463;&#24120;&#20248;&#20110;&#37027;&#20123;&#27809;&#26377;&#32463;&#36807;CoT&#25512;&#29702;&#26041;&#27861;&#35757;&#32451;&#30340;&#22823;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.00193</link><description>&lt;p&gt;
&#25226;&#25512;&#29702;&#33021;&#21147;&#21387;&#32553;&#21040;&#26356;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;
&lt;/p&gt;
&lt;p&gt;
Distilling Reasoning Capabilities into Smaller Language Models. (arXiv:2212.00193v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#25226;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36880;&#27493;&#25512;&#29702;&#33021;&#21147;&#33976;&#39311;&#21040;&#26356;&#23567;&#30340;&#27169;&#22411;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#25512;&#29702;&#26041;&#26696;&#65292;&#20351;&#29992;&#33487;&#26684;&#25289;&#24213;&#24335;CoT&#26469;&#35757;&#32451;&#20004;&#20010;&#23567;&#22411;&#33976;&#39311;&#27169;&#22411;&#30340;&#32452;&#21512;&#65292;&#21487;&#20197;&#29992;&#26469;&#20998;&#35299;&#21644;&#35299;&#20915;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#19988;&#22312;&#22810;&#20010;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#39640;&#31934;&#24230;&#30340;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#65292;&#32463;&#24120;&#20248;&#20110;&#37027;&#20123;&#27809;&#26377;&#32463;&#36807;CoT&#25512;&#29702;&#26041;&#27861;&#35757;&#32451;&#30340;&#22823;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36880;&#27493;&#25512;&#29702;&#30340;&#26041;&#27861;&#65288;&#22914;CoT&#65289;&#22312;&#20855;&#26377;&#25512;&#29702;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#34987;&#35777;&#26126;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;CoT&#26041;&#27861;&#30340;&#25104;&#21151;&#22522;&#26412;&#19978;&#26159;&#19982;&#27169;&#22411;&#22823;&#23567;&#23494;&#20999;&#30456;&#20851;&#30340;&#65292;&#24182;&#19988;&#36890;&#24120;&#38656;&#35201;&#21313;&#20159;&#32423;&#21442;&#25968;&#35268;&#27169;&#30340;&#27169;&#22411;&#25165;&#33021;&#20351;CoT&#24037;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21033;&#29992;&#36739;&#22823;&#27169;&#22411;&#30340;&#36880;&#27493;CoT&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#23558;&#36825;&#20123;&#33021;&#21147;&#33976;&#39311;&#21040;&#26356;&#23567;&#30340;&#27169;&#22411;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#25512;&#29702;&#26041;&#26696;&#65306;&#33487;&#26684;&#25289;&#24213;&#24335;CoT&#65292;&#23427;&#23398;&#20064;&#23558;&#21407;&#22987;&#38382;&#39064;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#23376;&#38382;&#39064;&#65292;&#24182;&#29992;&#23427;&#26469;&#25351;&#23548;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#12290;&#25105;&#20204;&#20351;&#29992;&#33487;&#26684;&#25289;&#24213;&#24335;CoT&#26469;&#35757;&#32451;&#20004;&#20010;&#23567;&#22411;&#33976;&#39311;&#27169;&#22411;&#30340;&#32452;&#21512;&#65306;&#38382;&#39064;&#20998;&#35299;&#22120;&#21644;&#23376;&#38382;&#39064;&#27714;&#35299;&#22120;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#32473;&#23450;&#19968;&#20010;&#26032;&#38382;&#39064;&#65292;&#36825;&#20004;&#20010;&#33976;&#39311;&#27169;&#22411;&#20197;&#21516;&#27493;&#30340;&#26041;&#24335;&#24037;&#20316;&#65292;&#20197;&#20998;&#35299;&#21644;&#35299;&#20915;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#22312;&#22810;&#20010;&#25512;&#29702;&#25968;&#25454;&#38598;&#65288;GSM8K&#65292;StrategyQA&#21644;SVAMP&#65289;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#33976;&#39311;&#27169;&#22411;&#23398;&#20250;&#20102;&#39640;&#31934;&#24230;&#22320;&#25191;&#34892;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#65292;&#36890;&#24120;&#20248;&#20110;&#27809;&#26377;&#19987;&#38376;&#20351;&#29992;CoT&#25512;&#29702;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#22411;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Step-by-step reasoning approaches like chain of thought (CoT) have proved to be very effective in inducing reasoning capabilities in large language models. However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work. In this paper, we propose a knowledge distillation approach that leverages the step-by-step CoT reasoning capabilities of larger models and distills these abilities into smaller models.  In this work, we propose an alternative reasoning scheme, Socratic CoT, that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps. We use Socratic CoT to train a combination of two small distilled models: a problem decomposer and a subproblem solver. In practice, given a new problem, the two distilled models work in sync to decompose and solve complex problems. On multiple reasoning datasets (GSM8K, StrategyQA, and SVAMP
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#35774;&#65292;&#21363;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22120;&#21487;&#20197;&#38544;&#21547;&#22320;&#32534;&#30721;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#26681;&#25454;&#19978;&#19979;&#25991;&#20013;&#20986;&#29616;&#30340;&#26032;&#31034;&#20363;&#26356;&#26032;&#36825;&#20123;&#38544;&#24335;&#27169;&#22411;&#12290;&#36890;&#36807;&#26500;&#36896;&#21644;&#27604;&#36739;&#24615;&#36136;&#35777;&#26126;&#20102;&#36825;&#20010;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#23398;&#20064;&#22120;&#39044;&#27979;&#22120;&#21644;&#20174;&#26174;&#24335;&#23398;&#20064;&#31639;&#27861;&#20013;&#33719;&#24471;&#30340;&#39044;&#27979;&#22120;&#20043;&#38388;&#30456;&#20284;&#31243;&#24230;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.15661</link><description>&lt;p&gt;
&#20160;&#20040;&#26159;&#19978;&#19979;&#25991;&#23398;&#20064;&#31639;&#27861;&#65311;&#20351;&#29992;&#32447;&#24615;&#27169;&#22411;&#36827;&#34892;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
What learning algorithm is in-context learning? Investigations with linear models. (arXiv:2211.15661v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15661
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#35774;&#65292;&#21363;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22120;&#21487;&#20197;&#38544;&#21547;&#22320;&#32534;&#30721;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#26681;&#25454;&#19978;&#19979;&#25991;&#20013;&#20986;&#29616;&#30340;&#26032;&#31034;&#20363;&#26356;&#26032;&#36825;&#20123;&#38544;&#24335;&#27169;&#22411;&#12290;&#36890;&#36807;&#26500;&#36896;&#21644;&#27604;&#36739;&#24615;&#36136;&#35777;&#26126;&#20102;&#36825;&#20010;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#23398;&#20064;&#22120;&#39044;&#27979;&#22120;&#21644;&#20174;&#26174;&#24335;&#23398;&#20064;&#31639;&#27861;&#20013;&#33719;&#24471;&#30340;&#39044;&#27979;&#22120;&#20043;&#38388;&#30456;&#20284;&#31243;&#24230;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24207;&#21015;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#36716;&#25442;&#22120;&#65292;&#23637;&#29616;&#20102;&#19968;&#31181;&#38750;&#20961;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;&#23427;&#20204;&#21487;&#20197;&#22312;&#36755;&#20837;&#20013;&#21576;&#29616;&#30340;&#26631;&#35760;&#31034;&#20363;&#24207;&#21015;$(x,f(x))$&#26500;&#24314;&#26032;&#30340;&#39044;&#27979;&#22120;&#65292;&#32780;&#26080;&#38656;&#36827;&#19968;&#27493;&#30340;&#21442;&#25968;&#26356;&#26032;&#12290;&#25105;&#20204;&#35843;&#26597;&#20551;&#35774;&#65306;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22120;&#36890;&#36807;&#22312;&#20854;&#28608;&#27963;&#20013;&#32534;&#30721;&#36739;&#23567;&#30340;&#27169;&#22411;&#24182;&#26681;&#25454;&#19978;&#19979;&#25991;&#20013;&#20986;&#29616;&#30340;&#26032;&#31034;&#20363;&#26356;&#26032;&#36825;&#20123;&#38544;&#24335;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26631;&#20934;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#20197;&#32447;&#24615;&#22238;&#24402;&#20316;&#20026;&#21407;&#22411;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19977;&#26465;&#36825;&#20010;&#20551;&#35774;&#30340;&#35777;&#25454;&#26469;&#28304;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#26500;&#36896;&#35777;&#26126;&#20102;&#36716;&#25442;&#22120;&#21487;&#20197;&#22312;&#26799;&#24230;&#19979;&#38477;&#21644;&#38381;&#24418;&#24335;&#30340;&#23725;&#22238;&#24402;&#30340;&#22522;&#30784;&#19978;&#23454;&#29616;&#32447;&#24615;&#27169;&#22411;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#35757;&#32451;&#20986;&#26469;&#30340;&#23398;&#20064;&#22120;&#19982;&#26799;&#24230;&#19979;&#38477;&#12289;&#23725;&#22238;&#24402;&#20197;&#21450;&#31934;&#30830;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#25152;&#35745;&#31639;&#30340;&#39044;&#27979;&#22120;&#38750;&#24120;&#30456;&#20284;&#65292;&#22312;&#36716;&#25442;&#22120;&#30340;&#28145;&#24230;&#21644;&#25968;&#25454;&#38598;&#30340;&#22122;&#22768;&#27700;&#24179;&#21464;&#21270;&#26102;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#39044;&#27979;&#22120;&#20043;&#38388;&#36827;&#34892;&#36716;&#25442;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#23398;&#20064;&#22120;&#39044;&#27979;&#22120;&#21644;&#20174;&#26174;&#24335;&#23398;&#20064;&#31639;&#27861;&#20013;&#33719;&#24471;&#30340;&#39044;&#27979;&#22120;&#20043;&#38388;&#30456;&#20284;&#31243;&#24230;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#24230;&#37327;&#34920;&#26126;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#21487;&#20197;&#38544;&#21547;&#22320;&#32534;&#30721;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#22312;&#38382;&#39064;&#21644;&#19978;&#19979;&#25991;&#20013;&#26681;&#25454;&#38656;&#35201;&#36827;&#34892;&#20999;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural sequence models, especially transformers, exhibit a remarkable capacity for in-context learning. They can construct new predictors from sequences of labeled examples $(x, f(x))$ presented in the input without further parameter updates. We investigate the hypothesis that transformer-based in-context learners implement standard learning algorithms implicitly, by encoding smaller models in their activations, and updating these implicit models as new examples appear in the context. Using linear regression as a prototypical problem, we offer three sources of evidence for this hypothesis. First, we prove by construction that transformers can implement learning algorithms for linear models based on gradient descent and closed-form ridge regression. Second, we show that trained in-context learners closely match the predictors computed by gradient descent, ridge regression, and exact least-squares regression, transitioning between different predictors as transformer depth and dataset noi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20851;&#20110;&#20351;&#29992;Transformer&#26550;&#26500;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LaMDA&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#35828;&#27861;&#12290;&#20316;&#32773;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#19981;&#21487;&#33021;&#20855;&#26377;&#24847;&#35782;&#65292;&#32780;LaMDA&#27809;&#26377;&#27604;&#20854;&#20182;&#31867;&#20284;&#27169;&#22411;&#26356;&#20855;&#20808;&#36827;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.11483</link><description>&lt;p&gt;
Deanthropomorphising NLP&#65306;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#24847;&#35782;&#21040;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Deanthropomorphising NLP: Can a Language Model Be Conscious?. (arXiv:2211.11483v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20851;&#20110;&#20351;&#29992;Transformer&#26550;&#26500;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LaMDA&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#35828;&#27861;&#12290;&#20316;&#32773;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#19981;&#21487;&#33021;&#20855;&#26377;&#24847;&#35782;&#65292;&#32780;LaMDA&#27809;&#26377;&#27604;&#20854;&#20182;&#31867;&#20284;&#27169;&#22411;&#26356;&#20855;&#20808;&#36827;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23545;&#26368;&#36817;&#26377;&#20851;&#20351;&#29992;Transformer&#27169;&#22411;&#26550;&#26500;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LaMDA&#20855;&#26377;&#24847;&#35782;&#30340;&#35828;&#27861;&#36827;&#34892;&#35752;&#35770;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26679;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#21487;&#33021;&#20855;&#26377;&#24847;&#35782;&#65292;&#32780;LaMDA&#24182;&#27809;&#26377;&#27604;&#20854;&#20182;&#31867;&#20284;&#27169;&#22411;&#26356;&#20855;&#20808;&#36827;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#32508;&#21512;&#20449;&#24687;&#29702;&#35770;&#23545;Transformer&#26550;&#26500;&#36827;&#34892;&#20998;&#26512;&#26469;&#35777;&#26126;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#26377;&#24847;&#35782;&#30340;&#35828;&#27861;&#26159;NLP&#25253;&#36947;&#20013;&#20351;&#29992;&#25311;&#20154;&#21270;&#35821;&#35328;&#30340;&#26356;&#24191;&#27867;&#20542;&#21521;&#30340;&#19968;&#37096;&#20998;&#12290;&#26080;&#35770;&#36825;&#20123;&#35828;&#27861;&#30340;&#30495;&#23454;&#24615;&#22914;&#20309;&#65292;&#25105;&#20204;&#35748;&#20026;&#29616;&#22312;&#26159;&#35780;&#20272;&#35821;&#35328;&#24314;&#27169;&#36827;&#23637;&#24182;&#32771;&#34385;&#35813;&#20219;&#21153;&#30340;&#20262;&#29702;&#24433;&#21709;&#30340;&#36866;&#24403;&#26102;&#26426;&#12290;&#20026;&#20102;&#20351;&#26412;&#25991;&#26377;&#21161;&#20110;NLP&#31038;&#21306;&#20197;&#22806;&#30340;&#35835;&#32773;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20123;NLP&#22522;&#30784;&#30693;&#35782;&#30340;&#20171;&#32461;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work is intended as a voice in the discussion over the recent claims that LaMDA, a pretrained language model based on the Transformer model architecture, is sentient. This claim, if confirmed, would have serious ramifications in the Natural Language Processing (NLP) community due to wide-spread use of similar models. However, here we take the position that such a language model cannot be sentient, or conscious, and that LaMDA in particular exhibits no advances over other similar models that would qualify it. We justify this by analysing the Transformer architecture through Integrated Information Theory. We see the claims of consciousness as part of a wider tendency to use anthropomorphic language in NLP reporting. Regardless of the veracity of the claims, we consider this an opportune moment to take stock of progress in language modelling and consider the ethical implications of the task. In order to make this work helpful for readers outside the NLP community, we also present the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SLICER&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#31867;&#21644;&#23545;&#27604;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#36827;&#34892;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#65292;&#20174;&#32780;&#24471;&#21040;&#21487;&#20197;&#24191;&#27867;&#36866;&#29992;&#20110;&#35821;&#38899;&#21644;&#38750;&#35821;&#38899;&#20219;&#21153;&#30340;&#38899;&#39057;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2211.01519</link><description>&lt;p&gt;
SLICER: &#21033;&#29992;&#20302;&#36164;&#28304;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;&#36890;&#29992;&#38899;&#39057;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
SLICER: Learning universal audio representations using low-resource self-supervised pre-training. (arXiv:2211.01519v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SLICER&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#31867;&#21644;&#23545;&#27604;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#36827;&#34892;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#65292;&#20174;&#32780;&#24471;&#21040;&#21487;&#20197;&#24191;&#27867;&#36866;&#29992;&#20110;&#35821;&#38899;&#21644;&#38750;&#35821;&#38899;&#20219;&#21153;&#30340;&#38899;&#39057;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#26410;&#26631;&#35760;&#30340;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#65292;&#20174;&#32780;&#20943;&#23569;&#38899;&#39057;&#21644;&#35821;&#38899;&#20998;&#31867;&#25152;&#38656;&#30340;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#20302;&#36164;&#28304;&#26410;&#26631;&#35760;&#30340;&#38899;&#39057;&#39044;&#35757;&#32451;&#29615;&#22659;&#20013;&#23398;&#20064;&#21487;&#20197;&#27010;&#25324;&#22823;&#37327;&#35821;&#38899;&#21644;&#38750;&#35821;&#38899;&#20219;&#21153;&#30340;&#38899;&#39057;&#34920;&#31034;&#12290;&#21463;&#21040;&#32858;&#31867;&#21644;&#23545;&#27604;&#23398;&#20064;&#33539;&#24335;&#22312;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#26368;&#36817;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SLICER&#65288;&#23454;&#20363;&#21644;&#32858;&#31867;&#32423;&#21035;&#39640;&#25928;&#34920;&#24449;&#30340;&#23545;&#31216;&#23398;&#20064;&#65289;&#65292;&#23558;&#32858;&#31867;&#21644;&#23545;&#27604;&#23398;&#20064;&#33539;&#24335;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#20351;&#29992;&#23398;&#29983;&#21644;&#25945;&#24072;&#32534;&#30721;&#22120;&#20043;&#38388;&#28508;&#22312;&#34920;&#31034;&#20043;&#38388;&#30340;&#23545;&#31216;&#25439;&#22833;&#65292;&#24182;&#21516;&#26102;&#35299;&#20915;&#23454;&#20363;&#21644;&#32858;&#31867;&#32423;&#21035;&#30340;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#36755;&#20837;&#30340;&#39057;&#35889;&#22270;&#25237;&#24433;&#21040;&#19982;&#32858;&#31867;&#25968;&#30446;&#30456;&#21516;&#30340;&#36755;&#20986;&#23376;&#31354;&#38388;&#20013;&#26469;&#22312;&#32447;&#33719;&#24471;&#32858;&#31867;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new Self-Supervised Learning (SSL) approach to pre-train encoders on unlabeled audio data that reduces the need for large amounts of labeled data for audio and speech classification. Our primary aim is to learn audio representations that can generalize across a large variety of speech and non-speech tasks in a low-resource un-labeled audio pre-training setting. Inspired by the recent success of clustering and contrasting learning paradigms for SSL-based speech representation learning, we propose SLICER (Symmetrical Learning of Instance and Cluster-level Efficient Representations), which brings together the best of both clustering and contrasting learning paradigms. We use a symmetric loss between latent representations from student and teacher encoders and simultaneously solve instance and cluster-level contrastive learning tasks. We obtain cluster representations online by just projecting the input spectrogram into an output subspace with dimensions equal to the number of
&lt;/p&gt;</description></item><item><title>MAST&#26159;&#19968;&#31181;&#22810;&#23610;&#24230;&#38899;&#39057;&#35889;&#22270;&#21464;&#21387;&#22120;&#65292;&#24341;&#20837;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#20998;&#23618;&#27010;&#24565;&#65292;&#21516;&#26102;&#25193;&#23637;&#23884;&#20837;&#32500;&#24230;&#65292;&#38477;&#20302;&#26102;&#38388;&#20998;&#36776;&#29575;&#65292;&#29992;&#20110;&#38899;&#39057;&#20998;&#31867;&#12290;&#36890;&#36807;&#37329;&#23383;&#22612;&#32467;&#26500;&#23454;&#29616;&#26089;&#26399;&#23618;&#21644;&#28145;&#23618;&#30340;&#24314;&#27169;&#65292;&#25193;&#23637;&#26041;&#27861;&#20026;SS-MAST&#12290;</title><link>http://arxiv.org/abs/2211.01515</link><description>&lt;p&gt;
MAST:&#22810;&#23610;&#24230;&#38899;&#39057;&#35889;&#22270;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
MAST: Multiscale Audio Spectrogram Transformers. (arXiv:2211.01515v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01515
&lt;/p&gt;
&lt;p&gt;
MAST&#26159;&#19968;&#31181;&#22810;&#23610;&#24230;&#38899;&#39057;&#35889;&#22270;&#21464;&#21387;&#22120;&#65292;&#24341;&#20837;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#20998;&#23618;&#27010;&#24565;&#65292;&#21516;&#26102;&#25193;&#23637;&#23884;&#20837;&#32500;&#24230;&#65292;&#38477;&#20302;&#26102;&#38388;&#20998;&#36776;&#29575;&#65292;&#29992;&#20110;&#38899;&#39057;&#20998;&#31867;&#12290;&#36890;&#36807;&#37329;&#23383;&#22612;&#32467;&#26500;&#23454;&#29616;&#26089;&#26399;&#23618;&#21644;&#28145;&#23618;&#30340;&#24314;&#27169;&#65292;&#25193;&#23637;&#26041;&#27861;&#20026;SS-MAST&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;&#20998;&#31867;&#30340;&#22810;&#23610;&#24230;&#38899;&#39057;&#35889;&#22270;&#21464;&#21387;&#22120;&#65288;MAST&#65289;&#65292;&#23558;&#22810;&#23610;&#24230;&#29305;&#24449;&#20998;&#23618;&#27010;&#24565;&#24341;&#20837;&#38899;&#39057;&#35889;&#22270;&#21464;&#25442;&#22120;&#65288;AST&#65289;&#20013;&#12290;&#32473;&#23450;&#19968;&#20010;&#36755;&#20837;&#30340;&#38899;&#39057;&#35889;&#22270;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#20854;&#35009;&#21098;&#25104;&#21021;&#27493;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#23884;&#20837;&#32500;&#24230;&#65292;&#38543;&#21518;MAST&#20013;&#30340;&#22810;&#20010;&#38454;&#27573;&#36880;&#28176;&#25193;&#23637;&#23884;&#20837;&#32500;&#24230;&#65292;&#21516;&#26102;&#38477;&#20302;&#36755;&#20837;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#12290;&#25105;&#20204;&#20351;&#29992;&#37329;&#23383;&#22612;&#32467;&#26500;&#65292;&#20351;&#24471;MAST&#30340;&#26089;&#26399;&#23618;&#22312;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#20294;&#20302;&#23884;&#20837;&#31354;&#38388;&#19979;&#24314;&#27169;&#31616;&#21333;&#30340;&#20302;&#32423;&#22768;&#23398;&#20449;&#24687;&#65292;&#32780;&#36739;&#28145;&#30340;&#26102;&#38388;&#31895;&#31961;&#23618;&#21017;&#29992;&#39640;&#32500;&#23884;&#20837;&#26469;&#24314;&#27169;&#39640;&#32423;&#22768;&#23398;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26041;&#27861;&#65292;&#31216;&#20026;SS-MAST&#65292;&#23427;&#35745;&#31639;&#20102;&#19968;&#20010;&#23545;&#31216;&#30340;&#23545;&#27604;&#25439;&#22833;&#65292;&#21033;&#29992;patch-drop - &#19968;&#31181;&#26032;&#30340;&#38899;&#39057;&#22686;&#24378;&#25216;&#26415;&#26469;&#33258;&#23398;&#20064;&#21644;&#25945;&#24072;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#34920;&#31034;&#20043;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Multiscale Audio Spectrogram Transformer (MAST) for audio classification, which brings the concept of multiscale feature hierarchies to the Audio Spectrogram Transformer (AST). Given an input audio spectrogram, we first patchify and project it into an initial temporal resolution and embedding dimension, post which the multiple stages in MAST progressively expand the embedding dimension while reducing the temporal resolution of the input. We use a pyramid structure that allows early layers of MAST operating at a high temporal resolution but low embedding space to model simple low-level acoustic information and deeper temporally coarse layers to model high-level acoustic information with high-dimensional embeddings. We also extend our approach to present a new Self-Supervised Learning (SSL) method called SS-MAST, which calculates a symmetric contrastive loss between latent representations from a student and a teacher encoder, leveraging patch-drop, a novel audio augmentation a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23545;&#31561;&#20851;&#31995;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#31216;&#20026;PeerDA&#26469;&#36827;&#34892;&#36328;&#24230;&#35782;&#21035;&#20219;&#21153;&#12290;PeerDA&#20351;&#29992;&#24102;&#26377;PR&#20851;&#31995;&#30340;&#36328;&#24230;&#23545;&#20316;&#20026;&#35757;&#32451;&#30340;&#22686;&#24378;&#25968;&#25454;&#65292;&#38450;&#27490;&#27169;&#22411;&#36807;&#25311;&#21512;&#34920;&#38754;&#19978;&#30340;&#36328;&#24230;-&#31867;&#21035;&#26144;&#23556;&#65292;&#24182;&#25512;&#21160;&#27169;&#22411;&#21033;&#29992;&#36328;&#24230;&#35821;&#20041;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;PeerDA&#22312;&#25968;&#25454;&#22686;&#24378;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.08855</link><description>&lt;p&gt;
&#36890;&#36807;&#24314;&#27169;&#23545;&#31561;&#20851;&#31995;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#36827;&#34892;&#36328;&#24230;&#35782;&#21035;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
PeerDA: Data Augmentation via Modeling Peer Relation for Span Identification Tasks. (arXiv:2210.08855v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23545;&#31561;&#20851;&#31995;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#31216;&#20026;PeerDA&#26469;&#36827;&#34892;&#36328;&#24230;&#35782;&#21035;&#20219;&#21153;&#12290;PeerDA&#20351;&#29992;&#24102;&#26377;PR&#20851;&#31995;&#30340;&#36328;&#24230;&#23545;&#20316;&#20026;&#35757;&#32451;&#30340;&#22686;&#24378;&#25968;&#25454;&#65292;&#38450;&#27490;&#27169;&#22411;&#36807;&#25311;&#21512;&#34920;&#38754;&#19978;&#30340;&#36328;&#24230;-&#31867;&#21035;&#26144;&#23556;&#65292;&#24182;&#25512;&#21160;&#27169;&#22411;&#21033;&#29992;&#36328;&#24230;&#35821;&#20041;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;PeerDA&#22312;&#25968;&#25454;&#22686;&#24378;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#24230;&#35782;&#21035;&#30340;&#30446;&#26631;&#26159;&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#35782;&#21035;&#20986;&#29305;&#23450;&#30340;&#25991;&#26412;&#36328;&#24230;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#21040;&#39044;&#23450;&#20041;&#30340;&#31867;&#21035;&#20013;&#12290;&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20102;&#21516;&#31867;&#36328;&#24230;&#30340;&#23545;&#31561;&#20851;&#31995;&#65292;&#21363;Peer (PR)&#20851;&#31995;&#65292;&#36825;&#31181;&#20851;&#31995;&#34920;&#26126;&#20004;&#20010;&#36328;&#24230;&#26159;&#21516;&#19968;&#31867;&#21035;&#30340;&#23454;&#20363;&#24182;&#19988;&#20855;&#26377;&#30456;&#20284;&#30340;&#29305;&#24449;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23545;&#31561;&#20851;&#31995;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#31216;&#20026;PeerDA&#65292;&#20351;&#29992;&#24102;&#26377;PR&#20851;&#31995;&#30340;&#36328;&#24230;&#23545;&#20316;&#20026;&#35757;&#32451;&#30340;&#22686;&#24378;&#25968;&#25454;&#12290;PeerDA&#26377;&#20004;&#20010;&#29420;&#29305;&#30340;&#20248;&#28857;&#65306;&#65288;1&#65289;&#26377;&#22823;&#37327;&#30340;PR&#36328;&#24230;&#23545;&#21487;&#20197;&#29992;&#26469;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#65307;&#65288;2&#65289;&#22686;&#24378;&#30340;&#25968;&#25454;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#36807;&#25311;&#21512;&#34920;&#38754;&#19978;&#30340;&#36328;&#24230;-&#31867;&#21035;&#26144;&#23556;&#65292;&#24182;&#25512;&#21160;&#27169;&#22411;&#21033;&#29992;&#36328;&#24230;&#35821;&#20041;&#12290;&#22312;&#19971;&#20010;&#39046;&#22495;&#30340;&#22235;&#31181;&#19981;&#21516;&#20219;&#21153;&#30340;&#21313;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;PeerDA&#30340;&#26377;&#25928;&#24615;&#12290;&#23588;&#20854;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;PeerDA&#22312;&#25968;&#25454;&#22686;&#24378;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Span identification aims at identifying specific text spans from text input and classifying them into pre-defined categories. Different from previous works that merely leverage the Subordinate (SUB) relation (i.e. if a span is an instance of a certain category) to train models, this paper for the first time explores the Peer (PR) relation, which indicates that two spans are instances of the same category and share similar features. Specifically, a novel Peer Data Augmentation (PeerDA) approach is proposed which employs span pairs with the PR relation as the augmentation data for training. PeerDA has two unique advantages: (1) There are a large number of PR span pairs for augmenting the training data. (2) The augmented data can prevent the trained model from over-fitting the superficial span-category mapping by pushing the model to leverage the span semantics. Experimental results on ten datasets over four diverse tasks across seven domains demonstrate the effectiveness of PeerDA. Notab
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REV&#30340;&#24230;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#20013;&#26032;&#39062;&#12289;&#19982;&#26631;&#31614;&#30456;&#20851;&#30340;&#20449;&#24687;&#30340;&#25968;&#37327;&#65292;&#36890;&#36807;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#36827;&#34892;&#30740;&#31350;&#12290;&#23454;&#39564;&#35777;&#26126;REV&#22312;&#35780;&#20272;&#35299;&#37322;-&#26631;&#31614;&#23545;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#30452;&#35273;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2210.04982</link><description>&lt;p&gt;
&#29992;&#20449;&#24687;&#35770;&#35780;&#20272;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#30340;&#21487;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
REV: Information-Theoretic Evaluation of Free-Text Rationales. (arXiv:2210.04982v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REV&#30340;&#24230;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#20013;&#26032;&#39062;&#12289;&#19982;&#26631;&#31614;&#30456;&#20851;&#30340;&#20449;&#24687;&#30340;&#25968;&#37327;&#65292;&#36890;&#36807;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#36827;&#34892;&#30740;&#31350;&#12290;&#23454;&#39564;&#35777;&#26126;REV&#22312;&#35780;&#20272;&#35299;&#37322;-&#26631;&#31614;&#23545;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#30452;&#35273;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#26159;&#36808;&#21521;&#21487;&#35299;&#37322; NLP &#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#27493;&#39588;&#65292;&#28982;&#32780;&#35780;&#20272;&#36825;&#26679;&#30340;&#35299;&#37322;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#24230;&#37327;&#20027;&#35201;&#38598;&#20013;&#22312;&#27979;&#37327;&#35299;&#37322;&#21644;&#32473;&#23450;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#19978;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#29702;&#24819;&#30340;&#24230;&#37327;&#24212;&#35813;&#38598;&#20013;&#20110;&#35299;&#37322;&#20013;&#25552;&#20379;&#30340;&#26032;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#22312;&#36755;&#20837;&#25110;&#26631;&#31614;&#20013;&#37117;&#27809;&#26377;&#25552;&#20379;&#12290;&#25105;&#20204;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#20351;&#29992;&#26465;&#20214;V-&#20449;&#24687;&#65288;Hewitt et al&#12290;&#65292;2021&#65289;&#30740;&#31350;&#20102;&#36825;&#20010;&#30740;&#31350;&#38382;&#39064;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;REV&#65288;&#21033;&#29992;&#26465;&#20214;V-&#20449;&#24687;&#35780;&#20272;&#35299;&#37322;&#65289;&#30340;&#24230;&#37327;&#65292;&#29992;&#20110;&#37327;&#21270;&#29702;&#24615;&#20013;&#38500;&#20102;&#36755;&#20837;&#25110;&#26631;&#31614;&#20013;&#24050;&#26377;&#20449;&#24687;&#20043;&#22806;&#30340;&#26032;&#26631;&#31614;&#30456;&#20851;&#20449;&#24687;&#30340;&#25968;&#37327;&#12290;&#22312;&#28041;&#21450;&#25512;&#29702;&#20219;&#21153;&#30340;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;REV&#22312;&#35780;&#20272;&#35299;&#37322;-&#26631;&#31614;&#23545;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#19982;&#29616;&#26377;&#30340;&#24230;&#37327;&#30456;&#27604;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;REV&#19982;&#20154;&#31867;&#30452;&#35273;&#19968;&#33268;&#65292;&#32780;&#19968;&#20123;&#29616;&#26377;&#30340;&#24230;&#37327;&#21017;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating free-text rationales is a promising step towards explainable NLP, yet evaluating such rationales remains a challenge. Existing metrics have mostly focused on measuring the association between the rationale and a given label. We argue that an ideal metric should focus on the new information uniquely provided in the rationale that is otherwise not provided in the input or the label. We investigate this research problem from an information-theoretic perspective using conditional V-information (Hewitt et al., 2021). More concretely, we propose a metric called REV (Rationale Evaluation with conditional V-information), to quantify the amount of new, label-relevant information in a rationale beyond the information already available in the input or the label. Experiments across four benchmarks with reasoning tasks, including chain-of-thought, demonstrate the effectiveness of REV in evaluating rationale-label pairs, compared to existing metrics. We further demonstrate REV is consiste
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#12289;&#24341;&#23548;&#29983;&#25104;&#21644;&#21322;&#21442;&#25968;&#23494;&#38598;&#26816;&#32034;&#65292;&#21160;&#24577;&#29983;&#25104;&#22522;&#20110;&#20107;&#23454;&#24211;&#30340;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#35777;&#26126;&#26641;&#65292;&#23454;&#29616;&#31185;&#23398;&#25512;&#29702;&#65292;&#24182;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.07662</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#19987;&#23478;&#31995;&#32479;&#20013;&#22522;&#20110;&#20107;&#23454;&#24211;&#30340;&#36923;&#36753;&#25512;&#29702;&#30340;&#21160;&#24577;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Dynamic Generation of Grounded Logical Explanations in a Neuro-Symbolic Expert System. (arXiv:2209.07662v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07662
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#12289;&#24341;&#23548;&#29983;&#25104;&#21644;&#21322;&#21442;&#25968;&#23494;&#38598;&#26816;&#32034;&#65292;&#21160;&#24577;&#29983;&#25104;&#22522;&#20110;&#20107;&#23454;&#24211;&#30340;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#35777;&#26126;&#26641;&#65292;&#23454;&#29616;&#31185;&#23398;&#25512;&#29702;&#65292;&#24182;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20135;&#29983;&#22522;&#20110;&#20107;&#23454;&#24211;&#30340;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#35777;&#26126;&#26641;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#21457;&#20102;&#32463;&#20856;&#30340;&#22522;&#20110; Prolog &#30340;&#25512;&#29702;&#24341;&#25806;&#65292;&#20854;&#20013;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#12289;&#24341;&#23548;&#29983;&#25104;&#21644;&#21322;&#21442;&#25968;&#23494;&#38598;&#26816;&#32034;&#26469;&#26367;&#25442;&#25163;&#24037;&#21046;&#23450;&#30340;&#35268;&#21017;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479; NELLIE &#26469;&#28436;&#31034;&#36825;&#31181;&#26041;&#27861;&#65292;&#35813;&#31995;&#32479;&#21160;&#24577;&#22320;&#23454;&#20363;&#21270;&#21487;&#35299;&#37322;&#30340;&#25512;&#29702;&#35268;&#21017;&#65292;&#23545;&#33258;&#28982;&#35821;&#35328;&#35821;&#21477;&#30340;&#34164;&#21547;&#65288;&#21435;&#65289;&#32452;&#21512;&#36827;&#34892;&#25429;&#25417;&#21644;&#35780;&#20998;&#12290;&#36825;&#23548;&#33268;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#22312;&#31185;&#23398;&#25512;&#29702;&#39046;&#22495;&#23637;&#31034;&#20102;&#22914;&#20309;&#36923;&#36753;&#22320;&#20174;&#32463;&#36807;&#20154;&#24037;&#39564;&#35777;&#30340;&#20107;&#23454;&#30340;&#32452;&#21512;&#20013;&#25512;&#23548;&#20986;&#31572;&#26696;&#30340;&#25512;&#29702;&#30165;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an approach for systematic reasoning that produces human interpretable proof trees grounded in a factbase. Our approach evokes classic Prolog-based inference engines, where we replace handcrafted rules by combining neural language modeling, guided generation, and semiparametric dense retrieval. We demonstrate this approach through a novel system, NELLIE, which dynamically instantiates interpretable inference rules that capture and score entailment (de)compositions over natural language statements. This leads to strong performance, as shown in the scientific reasoning domain, while also producing reasoning traces showing how answers derive logically from the composition of human-verified facts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26500;&#36896;&#65292;&#21487;&#22788;&#29702;&#20855;&#26377; $\varepsilon$ -&#24359;&#30340;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#65292;&#20854;&#20013;&#27491;&#21017;&#35821;&#35328;&#30001;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#25351;&#23450;&#65292;&#35813;&#26500;&#36896;&#19981;&#20165;&#32534;&#30721;&#20102;&#32467;&#26500;&#65292;&#32780;&#19988;&#20445;&#30041;&#20102;&#21407;&#22987;&#26500;&#36896;&#30340;&#28176;&#36817;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2209.06809</link><description>&lt;p&gt;
&#20851;&#20110;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#35328;&#21644;&#27491;&#21017;&#35821;&#35328;&#30340;&#20132;&#38598;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On the Intersection of Context-Free and Regular Languages. (arXiv:2209.06809v2 [cs.FL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26500;&#36896;&#65292;&#21487;&#22788;&#29702;&#20855;&#26377; $\varepsilon$ -&#24359;&#30340;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#65292;&#20854;&#20013;&#27491;&#21017;&#35821;&#35328;&#30001;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#25351;&#23450;&#65292;&#35813;&#26500;&#36896;&#19981;&#20165;&#32534;&#30721;&#20102;&#32467;&#26500;&#65292;&#32780;&#19988;&#20445;&#30041;&#20102;&#21407;&#22987;&#26500;&#36896;&#30340;&#28176;&#36817;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Bar-Hillel&#26500;&#36896;&#26159;&#24418;&#24335;&#35821;&#35328;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#32463;&#20856;&#32467;&#26524;&#12290;&#23427;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#26500;&#36896;&#34920;&#26126;&#65292;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#35328;&#21644;&#27491;&#21017;&#35821;&#35328;&#30340;&#20132;&#38598;&#20173;&#26159;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#35328;&#12290;&#22312;&#26500;&#36896;&#20013;&#65292;&#27491;&#21017;&#35821;&#35328;&#30001;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#25351;&#23450;&#12290;&#20294;&#26159;&#65292;&#21407;&#22987;&#26500;&#36896;&#65288;Bar-Hillel&#31561;&#20154;&#65292;1961&#24180;&#65289;&#21450;&#20854;&#21152;&#26435;&#25193;&#23637;&#65288;Nederhof&#21644;Satta&#65292;2003&#24180;&#65289;&#37117;&#26080;&#27861;&#22788;&#29702;&#20855;&#26377; $\varepsilon$ -&#24359;&#30340;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#12290;&#34429;&#28982;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#20013;&#21024;&#38500; $\varepsilon$ -&#24359;&#32780;&#19981;&#25913;&#21464;&#35821;&#35328;&#65292;&#20294;&#36825;&#31181;&#25805;&#20316;&#20250;&#20462;&#25913;&#33258;&#21160;&#26426;&#30340;&#36335;&#24452;&#38598;&#21512;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#26399;&#26395;&#33258;&#21160;&#26426;&#20855;&#26377; $\varepsilon$ -&#24359;&#26102;&#36890;&#29992;&#30340;&#26500;&#36896;&#65292;&#24182;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#24191;&#20041;&#26500;&#36896;&#23548;&#33268;&#20102;&#19968;&#20010;&#32534;&#30721;&#20102;&#36755;&#20837;&#33258;&#21160;&#26426;&#21644;&#25991;&#27861;&#32467;&#26500;&#30340;&#25991;&#27861;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#21407;&#22987;&#26500;&#36896;&#30340;&#28176;&#36817;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Bar-Hillel construction is a classic result in formal language theory. It shows, by a simple construction, that the intersection of a context-free language and a regular language is itself context-free. In the construction, the regular language is specified by a finite-state automaton. However, neither the original construction (Bar-Hillel et al., 1961) nor its weighted extension (Nederhof and Satta, 2003) can handle finite-state automata with $\varepsilon$-arcs. While it is possible to remove $\varepsilon$-arcs from a finite-state automaton efficiently without modifying the language, such an operation modifies the automaton's set of paths. We give a construction that generalizes the Bar-Hillel in the case where the desired automaton has $\varepsilon$-arcs, and further prove that our generalized construction leads to a grammar that encodes the structure of both the input automaton and grammar while retaining the asymptotic size of the original construction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Bamboo&#37197;&#32622;&#31574;&#30053;&#65292;&#22522;&#20110;&#26356;&#28145;&#26356;&#31364;&#30340;Transformer&#32467;&#26500;&#36827;&#34892;Masked&#33258;&#32534;&#30721;&#22120;&#35757;&#32451;&#65292;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2205.10505</link><description>&lt;p&gt;
Transformer&#37197;&#32622;&#19982;&#35757;&#32451;&#30446;&#26631;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on Transformer Configuration and Training Objective. (arXiv:2205.10505v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Bamboo&#37197;&#32622;&#31574;&#30053;&#65292;&#22522;&#20110;&#26356;&#28145;&#26356;&#31364;&#30340;Transformer&#32467;&#26500;&#36827;&#34892;Masked&#33258;&#32534;&#30721;&#22120;&#35757;&#32451;&#65292;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#35768;&#22810;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#19978;&#37117;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#22312;&#35768;&#22810;&#27169;&#22411;&#35757;&#32451;&#24773;&#20917;&#19979;&#65292;&#36890;&#24120;&#37319;&#29992;&#20256;&#32479;&#30340;&#37197;&#32622;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#20123;&#20256;&#32479;&#37197;&#32622;&#65292;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35780;&#20272;&#65292;&#25552;&#20986;&#20102;Bamboo&#30340;&#37197;&#32622;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#20351;&#29992;&#26356;&#28145;&#26356;&#31364;&#30340;Transformer&#32467;&#26500;&#36827;&#34892;Masked&#33258;&#32534;&#30721;&#22120;&#35757;&#32451;&#65292;&#24182;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have delivered impressive results on many tasks, particularly vision and language tasks. In many model training situations, conventional configurations are typically adopted. For example, we often set the base model with hidden dimensions (i.e. model width) to be 768 and the number of transformer layers (i.e. model depth) to be 12. In this paper, we revisit these conventional configurations. Through theoretical analysis and experimental evaluation, we show that the masked autoencoder is effective in alleviating the over-smoothing issue in deep transformer training. Based on this finding, we propose Bamboo, an idea of using deeper and narrower transformer configurations, for masked autoencoder training. On ImageNet, with such a simple change in configuration, re-designed model achieves 87.1% top-1 accuracy and outperforms SoTA models like MAE and BEiT. On language tasks, re-designed model outperforms BERT with default setting by 1.1 points on average, on GLUE da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#30693;&#35782;&#24211;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#22270;&#24863;&#30693;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22270;&#24863;&#30693;&#20803;&#32032;&#34701;&#20837;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25552;&#20986;&#20102;&#25513;&#30721;&#32467;&#26500;&#21644;&#26032;&#30340;&#31867;&#22411;&#32534;&#30721;&#22120;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#24182;&#28040;&#38500;&#20102;&#39069;&#22806;&#39044;&#35757;&#32451;&#20219;&#21153;&#25152;&#24102;&#26469;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2204.06674</link><description>&lt;p&gt;
GAP: &#19968;&#31181;&#38754;&#21521;&#22270;&#30693;&#35782;&#24211;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#22270;&#24863;&#30693;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation. (arXiv:2204.06674v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.06674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#30693;&#35782;&#24211;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#22270;&#24863;&#30693;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22270;&#24863;&#30693;&#20803;&#32032;&#34701;&#20837;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25552;&#20986;&#20102;&#25513;&#30721;&#32467;&#26500;&#21644;&#26032;&#30340;&#31867;&#22411;&#32534;&#30721;&#22120;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#24182;&#28040;&#38500;&#20102;&#39069;&#22806;&#39044;&#35757;&#32451;&#20219;&#21153;&#25152;&#24102;&#26469;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30693;&#35782;&#24211;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25913;&#36827;&#26159;&#30001;&#20110;&#22686;&#21152;&#20102;&#36741;&#21161;&#39044;&#35757;&#32451;&#20219;&#21153;&#26469;&#25552;&#39640;&#24494;&#35843;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#21482;&#25552;&#20379;&#20102;&#23567;&#24133;&#25913;&#36827;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;&#22270;&#24863;&#30693;&#20803;&#32032;&#34701;&#20837;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#25513;&#30721;&#32467;&#26500;&#26469;&#25429;&#33719;&#37051;&#22495;&#20449;&#24687;&#21644;&#19968;&#31181;&#26032;&#30340;&#31867;&#22411;&#32534;&#30721;&#22120;&#65292;&#28155;&#21152;&#22270;&#27880;&#24847;&#26435;&#37325;&#20559;&#24046;&#65292;&#20174;&#32780;&#21487;&#20197;&#36229;&#36234;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#24182;&#28040;&#38500;&#20102;&#39069;&#22806;&#39044;&#35757;&#32451;&#20219;&#21153;&#24102;&#26469;&#30340;&#24046;&#36317;&#12290;&#22312;&#20004;&#20010;&#30693;&#35782;&#24211;&#21040;&#25991;&#26412;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#65292;&#21516;&#26102;&#28041;&#21450;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#27809;&#26377;&#39069;&#22806;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#21046;&#23450;&#20026;&#19968;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#20132;&#25442;&#21508;&#31181;&#25552;&#20986;&#30340;&#32452;&#20214;&#65292;&#24182;&#22522;&#20110;&#25299;&#25169;&#21644;&#31867;&#22411;&#20449;&#24687;&#35299;&#37322;&#22522;&#20110;&#30693;&#35782;&#24211;&#21040;&#25991;&#26412;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent improvements in KG-to-text generation are due to additional auxiliary pre-training tasks designed to give the fine-tune task a boost in performance. These tasks require extensive computational resources while only suggesting marginal improvements. Here, we demonstrate that by fusing graph-aware elements into existing pre-trained language models, we are able to outperform state-of-the-art models and close the gap imposed by additional pre-training tasks. We do so by proposing a mask structure to capture neighborhood information and a novel type encoder that adds a bias to the graph-attention weights depending on the connection type. Experiments on two KG-to-text benchmark datasets show our models are competitive while involving fewer parameters and no additional pre-training tasks. By formulating the problem as a framework, we can interchange the various proposed components and begin interpreting KG-to-text generative models based on the topological and type information found in 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35797;&#22270;&#30740;&#31350;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#34920;&#31034;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#22312;&#39046;&#22495;&#12289;&#35821;&#35328;&#12289;&#25968;&#25454;&#38598;&#22823;&#23567;&#20197;&#21450;&#20808;&#21069;&#30693;&#35782;&#26041;&#38754;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#29992;&#20110;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#25968;&#25454;&#23545; ASR &#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#20197;&#21450;&#30456;&#20284;&#24230;&#21644;&#25968;&#25454;&#30340;&#25968;&#37327;&#31561;&#22240;&#32032;&#23545;&#20854;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2203.16973</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#34920;&#31034;&#23545;&#35821;&#38899;&#35782;&#21035;&#26377;&#29992;&#30340;&#22240;&#32032;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analyzing the factors affecting usefulness of Self-Supervised Pre-trained Representations for Speech Recognition. (arXiv:2203.16973v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.16973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35797;&#22270;&#30740;&#31350;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#34920;&#31034;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#22312;&#39046;&#22495;&#12289;&#35821;&#35328;&#12289;&#25968;&#25454;&#38598;&#22823;&#23567;&#20197;&#21450;&#20808;&#21069;&#30693;&#35782;&#26041;&#38754;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#29992;&#20110;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#25968;&#25454;&#23545; ASR &#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#20197;&#21450;&#30456;&#20284;&#24230;&#21644;&#25968;&#25454;&#30340;&#25968;&#37327;&#31561;&#22240;&#32032;&#23545;&#20854;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#23398;&#20064;&#39640;&#32423;&#35821;&#38899;&#34920;&#31034;&#24050;&#25104;&#20026;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#26500;&#24314;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#36890;&#24120;&#20551;&#35774;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#21516;&#19968;&#39046;&#22495;&#25110;&#35821;&#35328;&#30340;&#25968;&#25454;&#21487;&#29992;&#20110;SSL&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#19981;&#21487;&#34892;&#12290;&#26412;&#25991;&#20316;&#20026; Interspeech Gram Vaani ASR &#25361;&#25112;&#30340;&#19968;&#37096;&#20998;&#65292;&#35797;&#22270;&#30740;&#31350;&#39046;&#22495;&#12289;&#35821;&#35328;&#12289;&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#20854;&#20182;&#19978;&#28216;&#39044;&#35757;&#32451; SSL &#25968;&#25454;&#26041;&#38754;&#23545;&#20302;&#36164;&#28304;&#19979;&#28216; ASR &#20219;&#21153;&#26368;&#32456;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#22312;&#32487;&#32493;&#39044;&#35757;&#32451;&#33539;&#24335;&#30340;&#22522;&#30784;&#19978;&#30740;&#31350;&#20102;&#20351;&#29992; SSL &#35757;&#32451;&#30340;&#27169;&#22411;&#25152;&#25317;&#26377;&#30340;&#20808;&#21069;&#30693;&#35782;&#30340;&#24433;&#21709;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#30740;&#31350;&#25581;&#31034;&#20102; ASR &#31995;&#32479;&#30340;&#24615;&#33021;&#26131;&#21463;&#29992;&#20110; SSL &#39044;&#35757;&#32451;&#30340;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#24615;&#33021;&#38543;&#30528;&#30456;&#20284;&#24230;&#21644;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) to learn high-level speech representations has been a popular approach to building Automatic Speech Recognition (ASR) systems in low-resource settings. However, the common assumption made in literature is that a considerable amount of unlabeled data is available for the same domain or language that can be leveraged for SSL pre-training, which we acknowledge is not feasible in a real-world setting. In this paper, as part of the Interspeech Gram Vaani ASR challenge, we try to study the effect of domain, language, dataset size, and other aspects of our upstream pre-training SSL data on the final performance low-resource downstream ASR task. We also build on the continued pre-training paradigm to study the effect of prior knowledge possessed by models trained using SSL. Extensive experiments and studies reveal that the performance of ASR systems is susceptible to the data used for SSL pre-training. Their performance improves with an increase in similarity and
&lt;/p&gt;</description></item></channel></rss>