<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#22522;&#20110;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22823;&#23567;&#30340;&#25511;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#26680;&#25351;&#20195;&#35299;&#26512;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#36895;&#24230;&#26041;&#38754;&#20248;&#20110;&#26356;&#36817;&#26399;&#30340;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#32780;&#22312;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#20013;&#65292;&#26368;&#32769;&#30340;&#27169;&#22411;&#22312;&#36328;&#22495;&#25991;&#26412;&#20307;&#35009;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>https://arxiv.org/abs/2404.00727</link><description>&lt;p&gt;
&#26680;&#25351;&#20195;&#35299;&#26512;&#27169;&#22411;&#30340;&#21463;&#25511;&#37325;&#26032;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Controlled Reevaluation of Coreference Resolution Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00727
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22823;&#23567;&#30340;&#25511;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#26680;&#25351;&#20195;&#35299;&#26512;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#36895;&#24230;&#26041;&#38754;&#20248;&#20110;&#26356;&#36817;&#26399;&#30340;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#32780;&#22312;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#20013;&#65292;&#26368;&#32769;&#30340;&#27169;&#22411;&#22312;&#36328;&#22495;&#25991;&#26412;&#20307;&#35009;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25152;&#26377;&#26368;&#20808;&#36827;&#30340;&#26680;&#25351;&#20195;&#35299;&#26512;&#65288;CR&#65289;&#27169;&#22411;&#37117;&#28041;&#21450;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#19968;&#20010;CR&#27169;&#22411;&#20248;&#20110;&#21478;&#19968;&#20010;&#30340;&#20986;&#33394;&#24615;&#33021;&#26159;&#30001;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#36824;&#26159;&#20854;&#20182;&#22240;&#32032;&#65288;&#22914;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#26550;&#26500;&#65289;&#36896;&#25104;&#30340;&#65292;&#30001;&#20110;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#23454;&#39564;&#35774;&#32622;&#65292;&#36825;&#26159;&#24456;&#38590;&#25110;&#19981;&#21487;&#33021;&#30830;&#23450;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#27169;&#31946;&#24615;&#65292;&#25105;&#20204;&#31995;&#32479;&#35780;&#20272;&#20102;&#20116;&#20010;CR&#27169;&#22411;&#65292;&#24182;&#25511;&#21046;&#20102;&#19968;&#20123;&#35774;&#35745;&#20915;&#31574;&#65292;&#21253;&#25324;&#27599;&#20010;&#27169;&#22411;&#20351;&#29992;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#24403;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#22823;&#23567;&#26102;&#65292;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;CR&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#36895;&#24230;&#26041;&#38754;&#20248;&#20110;&#26356;&#36817;&#26399;&#30340;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;CR&#27169;&#22411;&#20013;&#65292;&#36739;&#36817;&#26399;&#30340;&#27169;&#22411;&#24182;&#19981;&#24635;&#26159;&#26356;&#20934;&#30830;&#65292;&#32780;&#25105;&#20204;&#27979;&#35797;&#30340;&#26368;&#32769;&#30340;CR&#27169;&#22411;&#22312;&#36328;&#22495;&#25991;&#26412;&#20307;&#35009;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#36873;&#25321;&#21487;&#20197;&#20943;&#23569;&#22823;&#37096;&#20998;&#65292;&#20294;&#24182;&#38750;&#20840;&#37096;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00727v1 Announce Type: new  Abstract: All state-of-the-art coreference resolution (CR) models involve finetuning a pretrained language model. Whether the superior performance of one CR model over another is due to the choice of language model or other factors, such as the task-specific architecture, is difficult or impossible to determine due to lack of a standardized experimental setup. To resolve this ambiguity, we systematically evaluate five CR models and control for certain design decisions including the pretrained language model used by each. When controlling for language model size, encoder-based CR models outperform more recent decoder-based models in terms of both accuracy and inference speed. Surprisingly, among encoder-based CR models, more recent models are not always more accurate, and the oldest CR model that we test generalizes the best to out-of-domain textual genres. We conclude that controlling for the choice of language model reduces most, but not all, of 
&lt;/p&gt;</description></item><item><title>Aurora-M &#26159;&#31532;&#19968;&#20010;&#26681;&#25454;&#32654;&#22269;&#34892;&#25919;&#21629;&#20196;&#36827;&#34892;&#32418;&#38431;&#27979;&#35797;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#33521;&#35821;&#12289;&#33452;&#20848;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#26085;&#35821;&#12289;&#36234;&#21335;&#35821;&#21644;&#20195;&#30721;&#19978;&#35757;&#32451;&#65292;&#19981;&#26029;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;&#20102;&#20154;&#24037;&#23457;&#26680;&#30340;&#23433;&#20840;&#35828;&#26126;&#65292;&#24635;&#35757;&#32451; token &#25968;&#36229;&#36807; 2 &#19975;&#20159;&#20010;</title><link>https://arxiv.org/abs/2404.00399</link><description>&lt;p&gt;
Aurora-M: &#26681;&#25454;&#32654;&#22269;&#34892;&#25919;&#21629;&#20196;&#65292;&#31532;&#19968;&#20010;&#24320;&#28304;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#32418;&#38431;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00399
&lt;/p&gt;
&lt;p&gt;
Aurora-M &#26159;&#31532;&#19968;&#20010;&#26681;&#25454;&#32654;&#22269;&#34892;&#25919;&#21629;&#20196;&#36827;&#34892;&#32418;&#38431;&#27979;&#35797;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#33521;&#35821;&#12289;&#33452;&#20848;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#26085;&#35821;&#12289;&#36234;&#21335;&#35821;&#21644;&#20195;&#30721;&#19978;&#35757;&#32451;&#65292;&#19981;&#26029;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;&#20102;&#20154;&#24037;&#23457;&#26680;&#30340;&#23433;&#20840;&#35828;&#26126;&#65292;&#24635;&#35757;&#32451; token &#25968;&#36229;&#36807; 2 &#19975;&#20159;&#20010;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#22810;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#35757;&#32451;&#26102;&#39640;&#26114;&#30340;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#20102;&#21487;&#35775;&#38382;&#24615;&#12290;BLOOM &#21644; StarCoder &#31561;&#20513;&#35758;&#26088;&#22312;&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#20110;&#21327;&#20316;&#31038;&#21306;&#24320;&#21457;&#26356;&#20855;&#27665;&#20027;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23384;&#22312;&#30340;&#27169;&#22411;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65306;&#22810;&#35821;&#35328;&#33021;&#21147;&#26377;&#38480;&#65292;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#32780;&#20174;&#22836;&#24320;&#22987;&#39044;&#35757;&#32451;&#21448;&#20855;&#26377;&#39640;&#26114;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#38656;&#35201;&#36981;&#23432;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#21644;&#21457;&#23637;&#27861;&#24459;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; Aurora-M&#65292;&#19968;&#20010;&#21253;&#21547; 15B &#21442;&#25968;&#30340;&#22810;&#35821;&#35328;&#24320;&#28304;&#27169;&#22411;&#65292;&#35757;&#32451;&#35821;&#35328;&#21253;&#25324;&#33521;&#35821;&#12289;&#33452;&#20848;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#26085;&#35821;&#12289;&#36234;&#21335;&#35821;&#21644;&#20195;&#30721;&#12290;Aurora-M &#19981;&#26029;&#20174; StarCoderPlus &#19978;&#39044;&#35757;&#32451;&#65292;&#39069;&#22806;&#35757;&#32451;&#20102; 4350 &#20159;&#20010; token&#65292;&#24635;&#35757;&#32451; token &#25968;&#36229;&#36807;&#20102; 2 &#19975;&#20159;&#20010;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#22312;&#20154;&#24037;&#23457;&#26680;&#30340;&#23433;&#20840;&#35828;&#26126;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#24320;&#21457;&#19982;&#20256;&#32479;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00399v1 Announce Type: cross  Abstract: Pretrained language models underpin several AI applications, but their high computational cost for training limits accessibility. Initiatives such as BLOOM and StarCoder aim to democratize access to pretrained models for collaborative community development. However, such existing models face challenges: limited multilingual capabilities, continual pretraining causing catastrophic forgetting, whereas pretraining from scratch is computationally expensive, and compliance with AI safety and development laws. This paper presents Aurora-M, a 15B parameter multilingual open-source model trained on English, Finnish, Hindi, Japanese, Vietnamese, and code. Continually pretrained from StarCoderPlus on 435 billion additional tokens, Aurora-M surpasses 2 trillion tokens in total training token count. It is the first open-source multilingual model fine-tuned on human-reviewed safety instructions, thus aligning its development not only with conventio
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#26102;&#21512;&#24182;&#30456;&#20851;&#27010;&#24565;&#30340;&#23450;&#20041;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#22686;&#24378;&#26041;&#27861;&#20197;&#25913;&#21892;LLMs&#22312;&#29983;&#29289;&#21307;&#23398;NER&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;&#27979;&#35797;&#25968;&#25454;&#35774;&#32622;&#19979;&#24179;&#22343;&#25552;&#39640;&#20102;15\%&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00152</link><description>&lt;p&gt;
&#22312;&#32447;&#23450;&#20041;&#22686;&#24378;LLMs&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;NER
&lt;/p&gt;
&lt;p&gt;
On-the-fly Definition Augmentation of LLMs for Biomedical NER
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00152
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#26102;&#21512;&#24182;&#30456;&#20851;&#27010;&#24565;&#30340;&#23450;&#20041;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#22686;&#24378;&#26041;&#27861;&#20197;&#25913;&#21892;LLMs&#22312;&#29983;&#29289;&#21307;&#23398;NER&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;&#27979;&#35797;&#25968;&#25454;&#35774;&#32622;&#19979;&#24179;&#22343;&#25552;&#39640;&#20102;15\%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;LLMs&#20855;&#26377;&#19968;&#33324;&#30340;&#33021;&#21147;&#65292;&#20294;&#20173;&#28982;&#22312;&#29983;&#29289;&#21307;&#23398;NER&#20219;&#21153;&#20013;&#36935;&#21040;&#22256;&#38590;&#65292;&#36825;&#26159;&#30001;&#20110;&#23384;&#22312;&#19987;&#19994;&#26415;&#35821;&#21644;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#25152;&#33268;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#22686;&#24378;&#26041;&#27861;&#65292;&#22312;&#26377;&#38480;&#25968;&#25454;&#35774;&#32622;&#19979;&#25913;&#21892;LLMs&#22312;&#29983;&#29289;&#21307;&#23398;NER&#19978;&#30340;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23454;&#26102;&#21512;&#24182;&#30456;&#20851;&#27010;&#24565;&#30340;&#23450;&#20041;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#20026;&#20102;&#25552;&#20379;&#30693;&#35782;&#22686;&#24378;&#30340;&#27979;&#35797;&#22330;&#26223;&#65292;&#25105;&#20204;&#23545;&#25552;&#31034;&#31574;&#30053;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#23450;&#20041;&#22686;&#24378;&#23545;&#20110;&#24320;&#28304;&#21644;&#23553;&#38381;&#30340;LLMs&#37117;&#26159;&#26377;&#29992;&#30340;&#12290;&#20363;&#22914;&#65292;&#22312;&#25105;&#20204;&#25152;&#26377;&#65288;&#20845;&#20010;&#65289;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#65292;&#23427;&#23548;&#33268;&#20102;GPT-4&#24615;&#33021;&#65288;F1&#65289;&#24179;&#22343;&#30456;&#23545;&#25552;&#21319;&#20102;15\%&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#28040;&#34701;&#21644;&#20998;&#26512;&#65292;&#20197;&#35777;&#26126;&#25105;&#20204;&#30340;&#24615;&#33021;&#25913;&#36827;&#26469;&#28304;&#20110;&#28155;&#21152;&#30456;&#20851;&#30340;&#23450;&#20041;&#30693;&#35782;&#12290;&#25105;&#20204;&#21457;&#29616;&#35880;&#24910;&#30340;&#25552;&#31034;&#31574;&#30053;&#20063;&#25552;&#39640;&#20102;LLMs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00152v1 Announce Type: new  Abstract: Despite their general capabilities, LLMs still struggle on biomedical NER tasks, which are difficult due to the presence of specialized terminology and lack of training data. In this work we set out to improve LLM performance on biomedical NER in limited data settings via a new knowledge augmentation approach which incorporates definitions of relevant concepts on-the-fly. During this process, to provide a test bed for knowledge augmentation, we perform a comprehensive exploration of prompting strategies. Our experiments show that definition augmentation is useful for both open source and closed LLMs. For example, it leads to a relative improvement of 15\% (on average) in GPT-4 performance (F1) across all (six) of our test datasets. We conduct extensive ablations and analyses to demonstrate that our performance improvements stem from adding relevant definitional knowledge. We find that careful prompting strategies also improve LLM perform
&lt;/p&gt;</description></item><item><title>ReflectSumm&#26159;&#19968;&#20010;&#26088;&#22312;&#24635;&#32467;&#23398;&#29983;&#21453;&#24605;&#24615;&#20889;&#20316;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#24110;&#21161;&#24320;&#21457;&#21644;&#35780;&#20272;&#38024;&#23545;&#29616;&#23454;&#22330;&#26223;&#30340;&#26032;&#22411;&#25688;&#35201;&#25216;&#26415;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2403.19012</link><description>&lt;p&gt;
ReflectSumm: &#19968;&#20010;&#29992;&#20110;&#35838;&#31243;&#21453;&#24605;&#25688;&#35201;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ReflectSumm: A Benchmark for Course Reflection Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19012
&lt;/p&gt;
&lt;p&gt;
ReflectSumm&#26159;&#19968;&#20010;&#26088;&#22312;&#24635;&#32467;&#23398;&#29983;&#21453;&#24605;&#24615;&#20889;&#20316;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#24110;&#21161;&#24320;&#21457;&#21644;&#35780;&#20272;&#38024;&#23545;&#29616;&#23454;&#22330;&#26223;&#30340;&#26032;&#22411;&#25688;&#35201;&#25216;&#26415;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;ReflectSumm&#65292;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#24635;&#32467;&#23398;&#29983;&#21453;&#24605;&#24615;&#20889;&#20316;&#30340;&#26032;&#22411;&#25688;&#35201;&#25968;&#25454;&#38598;&#12290;ReflectSumm&#30340;&#30446;&#26631;&#26159;&#20419;&#36827;&#24320;&#21457;&#21644;&#35780;&#20272;&#38024;&#23545;&#29616;&#23454;&#22330;&#26223;&#30340;&#26032;&#22411;&#25688;&#35201;&#25216;&#26415;&#65292;&#36825;&#20123;&#22330;&#26223;&#20855;&#26377;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;%&#20855;&#26377;&#28508;&#22312;&#22312;&#24847;&#35265;&#24635;&#32467;&#39046;&#22495;&#21644;&#29305;&#21035;&#26159;&#25945;&#32946;&#39046;&#22495;&#20013;&#30340;&#24433;&#21709;&#12290;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#21508;&#31181;&#25688;&#35201;&#20219;&#21153;&#65292;&#24182;&#21253;&#25324;&#20840;&#38754;&#30340;&#20803;&#25968;&#25454;&#65292;&#21487;&#20197;&#25506;&#32034;&#21508;&#31181;&#30740;&#31350;&#38382;&#39064;&#24182;&#25903;&#25345;&#19981;&#21516;&#30340;&#24212;&#29992;&#12290;&#20026;&#23637;&#31034;&#20854;&#25928;&#29992;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#32467;&#26524;&#20026;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19012v1 Announce Type: cross  Abstract: This paper introduces ReflectSumm, a novel summarization dataset specifically designed for summarizing students' reflective writing. The goal of ReflectSumm is to facilitate developing and evaluating novel summarization techniques tailored to real-world scenarios with little training data, %practical tasks with potential implications in the opinion summarization domain in general and the educational domain in particular. The dataset encompasses a diverse range of summarization tasks and includes comprehensive metadata, enabling the exploration of various research questions and supporting different applications. To showcase its utility, we conducted extensive evaluations using multiple state-of-the-art baselines. The results provide benchmarks for facilitating further research in this area.
&lt;/p&gt;</description></item><item><title>WangchanLion&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#27888;&#35821;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#25351;&#20196;&#24494;&#35843;&#27169;&#22411;&#65292;&#22312;0-shot&#21644;1-shot&#35774;&#32622;&#19979;&#33021;&#22815;&#29702;&#35299;&#19978;&#19979;&#25991;&#24182;&#20135;&#29983;&#19982;&#21442;&#32771;&#31572;&#26696;&#19968;&#33268;&#30340;&#22238;&#31572;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.16127</link><description>&lt;p&gt;
WangchanLion&#19982;WangchanX MRC&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
WangchanLion and WangchanX MRC Eval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16127
&lt;/p&gt;
&lt;p&gt;
WangchanLion&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#27888;&#35821;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#25351;&#20196;&#24494;&#35843;&#27169;&#22411;&#65292;&#22312;0-shot&#21644;1-shot&#35774;&#32622;&#19979;&#33021;&#22815;&#29702;&#35299;&#19978;&#19979;&#25991;&#24182;&#20135;&#29983;&#19982;&#21442;&#32771;&#31572;&#26696;&#19968;&#33268;&#30340;&#22238;&#31572;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#25551;&#36848;&#20102;WangchanLion&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#27888;&#35821;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;MRC&#65289;&#30340;&#25351;&#20196;&#24494;&#35843;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;SEA-LION&#21644;&#19968;&#31995;&#21015;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#20419;&#36827;&#24320;&#25918;&#30740;&#31350;&#21644;&#21487;&#37325;&#22797;&#24615;&#65292;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#25152;&#26377;&#35757;&#32451;&#25968;&#25454;&#12289;&#20195;&#30721;&#21644;&#26368;&#32456;&#27169;&#22411;&#26435;&#37325;&#65292;&#37319;&#29992;Apache-2&#35768;&#21487;&#35777;&#12290;&#20026;&#20102;&#35780;&#20272;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#27888;&#35821;MRC&#25968;&#25454;&#38598;XQuAD&#21644;Iapp_wiki_qa_squad&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;0-shot&#21644;1-shot&#35774;&#32622;&#19979;&#65292;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#19978;&#19979;&#25991;&#24182;&#20135;&#29983;&#19982;&#21442;&#32771;&#31572;&#26696;&#19968;&#33268;&#30340;&#22238;&#31572;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;MRC&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#65292;&#35780;&#20272;&#31572;&#26696;&#30340;&#27491;&#30830;&#24615;&#12289;&#24110;&#21161;&#24615;&#12289;&#31616;&#27905;&#24615;&#21644;&#19978;&#19979;&#25991;&#24615;&#12290;&#35780;&#20272;&#32467;&#26524;&#25581;&#31034;&#20102;&#25105;&#20204;&#22914;&#20309;&#25913;&#36827;&#27169;&#22411;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16127v1 Announce Type: cross  Abstract: This technical report describes the development of WangchanLion, an instruction fine-tuned model focusing on Machine Reading Comprehension (MRC) in the Thai language. Our model is based on SEA-LION and a collection of instruction following datasets. To promote open research and reproducibility, we publically release all training data, code, and the final model weights under the Apache-2 license. To assess the contextual understanding capability, we conducted extensive experimental studies using two Thai MRC datasets, XQuAD and Iapp_wiki_qa_squad. Experimental results demonstrate the model's ability to comprehend the context and produce an answer faithful to the reference one in 0-shot and 1-shot settings. In addition, our evaluation goes beyond the traditional MRC. We propose a new evaluation scheme assessing the answer's correctness, helpfulness, conciseness, and contextuality. Evaluation results provide insight into how we can improv
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#20840;&#23616;&#26126;&#30830;&#26631;&#27880;&#25286;&#35299;&#25104;&#26412;&#22320;&#38544;&#24335;&#22810;&#27169;&#24577;&#21453;&#39304;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#23545;&#35805;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#23545;&#35805;&#24230;&#37327;&#26041;&#38754;&#23637;&#29616;&#20986;&#19968;&#33268;&#30340;&#25913;&#36827;</title><link>https://arxiv.org/abs/2403.11330</link><description>&lt;p&gt;
&#36890;&#36807;&#23558;&#19968;&#20010;&#20840;&#23616;&#26126;&#30830;&#26631;&#27880;&#25286;&#35299;&#25104;&#26412;&#22320;&#38544;&#24335;&#22810;&#27169;&#24577;&#21453;&#39304;&#26469;&#25913;&#36827;&#23545;&#35805;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11330
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#20840;&#23616;&#26126;&#30830;&#26631;&#27880;&#25286;&#35299;&#25104;&#26412;&#22320;&#38544;&#24335;&#22810;&#27169;&#24577;&#21453;&#39304;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#23545;&#35805;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#23545;&#35805;&#24230;&#37327;&#26041;&#38754;&#23637;&#29616;&#20986;&#19968;&#33268;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20840;&#23616;&#65288;&#21363;&#65292;&#23545;&#35805;&#32423;&#65289;&#22870;&#21169;&#23545;&#40784;&#22522;&#20110;LLM&#30340;&#23545;&#35805;&#20195;&#29702;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#33258;&#28982;&#21457;&#29983;&#30340;&#22810;&#27169;&#24577;&#20449;&#21495;&#12290;&#22312;&#39640;&#23618;&#27425;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#65288;&#21517;&#20026;GELI&#65289;&#36890;&#36807;&#23558;&#20154;&#31867;&#25552;&#20379;&#30340;&#20840;&#23616;&#26126;&#30830;&#65288;GE&#65289;&#20250;&#35805;&#32423;&#22870;&#21169;&#25286;&#20998;&#65292;&#21033;&#29992;&#26412;&#22320;&#38544;&#24335;&#65288;LI&#65289;&#22810;&#27169;&#24577;&#22870;&#21169;&#20449;&#21495;&#26469;&#36328;&#27169;&#24577;&#22320;&#22609;&#36896;&#22870;&#21169;&#20998;&#35299;&#27493;&#39588;&#12290;&#28982;&#21518;&#23558;&#36825;&#31181;&#20998;&#35299;&#30340;&#22870;&#21169;&#27169;&#22411;&#20316;&#20026;&#26631;&#20934;RHLF&#27969;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#26469;&#25913;&#36827;&#22522;&#20110;LLM&#30340;&#23545;&#35805;&#20195;&#29702;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#30340;&#20154;&#31867;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;GELI&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#21508;&#31181;&#23545;&#35805;&#24230;&#37327;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11330v1 Announce Type: cross  Abstract: We describe an approach for aligning an LLM-based dialogue agent based on global (i.e., dialogue-level) rewards, while also taking into account naturally-occurring multimodal signals. At a high level, our approach (dubbed GELI) learns a local, turn-level reward model by decomposing the human-provided Global Explicit (GE) session-level reward, using Local Implicit (LI} multimodal reward signals to crossmodally shape the reward decomposition step. This decomposed reward model is then used as part of the standard RHLF pipeline improve an LLM-based dialog agent. We run quantitative and qualitative human studies to evaluate the performance of our GELI approach, and find that it shows consistent improvements across various conversational metrics compared to baseline methods.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#35270;&#39057;&#38382;&#31572;&#23398;&#20064;&#30340;&#21160;&#24577;&#36866;&#37197;&#22120;&#21512;&#24182;&#26041;&#27861;DAM&#65292;&#33021;&#22815;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12289;&#26377;&#25928;&#36866;&#24212;&#19981;&#26029;&#21040;&#26469;&#30340;&#25968;&#25454;&#38598;&#12289;&#22788;&#29702;&#26410;&#30693;&#25968;&#25454;&#38598;&#36755;&#20837;&#65292;&#24182;&#20801;&#35768;&#22312;&#31867;&#20284;&#25968;&#25454;&#38598;&#39046;&#22495;&#20043;&#38388;&#20849;&#20139;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.08755</link><description>&lt;p&gt;
DAM:&#29992;&#20110;&#25345;&#32493;&#35270;&#39057;&#38382;&#31572;&#23398;&#20064;&#30340;&#21160;&#24577;&#36866;&#37197;&#22120;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
DAM: Dynamic Adapter Merging for Continual Video QA Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08755
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#35270;&#39057;&#38382;&#31572;&#23398;&#20064;&#30340;&#21160;&#24577;&#36866;&#37197;&#22120;&#21512;&#24182;&#26041;&#27861;DAM&#65292;&#33021;&#22815;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12289;&#26377;&#25928;&#36866;&#24212;&#19981;&#26029;&#21040;&#26469;&#30340;&#25968;&#25454;&#38598;&#12289;&#22788;&#29702;&#26410;&#30693;&#25968;&#25454;&#38598;&#36755;&#20837;&#65292;&#24182;&#20801;&#35768;&#22312;&#31867;&#20284;&#25968;&#25454;&#38598;&#39046;&#22495;&#20043;&#38388;&#20849;&#20139;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25345;&#32493;&#35270;&#39057;&#38382;&#31572;&#65288;VidQA&#65289;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;DAM&#65292;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#21160;&#24577;&#36866;&#37197;&#22120;&#21512;&#24182;&#26469;&#65288;i&#65289;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#65288;ii&#65289;&#23454;&#29616;&#23545;&#25345;&#32493;&#21040;&#36798;&#30340;&#25968;&#25454;&#38598;&#30340;&#39640;&#25928;&#36866;&#24212;&#65292;&#65288;iii&#65289;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#22788;&#29702;&#26469;&#33258;&#26410;&#30693;&#25968;&#25454;&#38598;&#30340;&#36755;&#20837;&#65292;&#65288;iv&#65289;&#23454;&#29616;&#36328;&#30456;&#20284;&#25968;&#25454;&#38598;&#39046;&#22495;&#30340;&#30693;&#35782;&#20849;&#20139;&#12290;&#22312;&#32473;&#23450;&#19968;&#32452;&#25345;&#32493;&#27969;&#24335;&#20256;&#36755;&#30340;VidQA&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#39034;&#24207;&#35757;&#32451;&#29305;&#23450;&#20110;&#25968;&#25454;&#38598;&#30340;&#36866;&#37197;&#22120;&#65292;&#21516;&#26102;&#20923;&#32467;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#39057;&#35821;&#35328;&#39592;&#24178;&#30340;&#21442;&#25968;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#32473;&#23450;&#26469;&#33258;&#26410;&#30693;&#39046;&#22495;&#30340;&#35270;&#39057;&#38382;&#39064;&#31034;&#20363;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#38750;&#21442;&#25968;&#36335;&#30001;&#22120;&#20989;&#25968;&#35745;&#31639;&#27599;&#20010;&#36866;&#37197;&#22120;&#30340;&#27010;&#29575;&#65292;&#21453;&#26144;&#20986;&#35813;&#36866;&#37197;&#22120;&#19982;&#24403;&#21069;&#35270;&#39057;&#38382;&#39064;&#36755;&#20837;&#23454;&#20363;&#30340;&#30456;&#20851;&#24615;&#12290;&#38543;&#21518;&#65292;&#25152;&#25552;&#20986;&#30340;&#21160;&#24577;&#36866;&#37197;&#22120;&#21512;&#24182;&#26041;&#26696;&#32858;&#21512;&#25152;&#26377;&#36866;&#37197;&#22120;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08755v1 Announce Type: cross  Abstract: We present a parameter-efficient method for continual video question-answering (VidQA) learning. Our method, named DAM, uses the proposed Dynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable efficient adaptation to continually arriving datasets, (iii) handle inputs from unknown datasets during inference, and (iv) enable knowledge sharing across similar dataset domains. Given a set of continually streaming VidQA datasets, we sequentially train dataset-specific adapters for each dataset while freezing the parameters of a large pretrained video-language backbone. During inference, given a video-question sample from an unknown domain, our method first uses the proposed non-parametric router function to compute a probability for each adapter, reflecting how relevant that adapter is to the current video-question input instance. Subsequently, the proposed dynamic adapter merging scheme aggregates all the adapter weight
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#36127;&#31181;&#23376;&#23454;&#20307;&#36827;&#34892;&#36229;&#32454;&#31890;&#24230;&#23454;&#20307;&#38598;&#25193;&#23637;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#36229;&#32454;&#31890;&#24230;&#35821;&#20041;&#31867;&#21035;&#34920;&#31034;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.04247</link><description>&lt;p&gt;
UltraWiki: &#20351;&#29992;&#36127;&#31181;&#23376;&#23454;&#20307;&#36827;&#34892;&#36229;&#32454;&#31890;&#24230;&#23454;&#20307;&#38598;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
UltraWiki: Ultra-fine-grained Entity Set Expansion with Negative Seed Entities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04247
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36127;&#31181;&#23376;&#23454;&#20307;&#36827;&#34892;&#36229;&#32454;&#31890;&#24230;&#23454;&#20307;&#38598;&#25193;&#23637;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#36229;&#32454;&#31890;&#24230;&#35821;&#20041;&#31867;&#21035;&#34920;&#31034;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#38598;&#25193;&#23637;(ESE)&#26088;&#22312;&#35782;&#21035;&#23646;&#20110;&#19982;&#32473;&#23450;&#31181;&#23376;&#23454;&#20307;&#30456;&#21516;&#35821;&#20041;&#31867;&#21035;&#30340;&#26032;&#23454;&#20307;&#12290;&#20256;&#32479;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#27491;&#31181;&#23376;&#23454;&#20307;&#26469;&#34920;&#31034;&#30446;&#26631;&#35821;&#20041;&#31867;&#21035;&#65292;&#36825;&#23545;&#36229;&#32454;&#31890;&#24230;&#35821;&#20041;&#31867;&#21035;&#30340;&#34920;&#31034;&#26500;&#25104;&#25361;&#25112;&#12290;&#36229;&#32454;&#31890;&#24230;&#35821;&#20041;&#31867;&#21035;&#26159;&#22522;&#20110;&#24102;&#26377;&#26356;&#20855;&#20307;&#23646;&#24615;&#32422;&#26463;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#31867;&#21035;&#23450;&#20041;&#30340;&#12290;&#20165;&#20351;&#29992;&#27491;&#31181;&#23376;&#23454;&#20307;&#25551;&#36848;&#20250;&#24341;&#36215;&#20004;&#20010;&#38382;&#39064;&#65306;(i) &#36229;&#32454;&#31890;&#24230;&#35821;&#20041;&#31867;&#21035;&#20043;&#38388;&#30340;&#27495;&#20041;&#12290;(ii) &#26080;&#27861;&#23450;&#20041;&#8220;&#19981;&#24819;&#35201;&#8221;&#30340;&#35821;&#20041;&#12290;&#30001;&#20110;&#36825;&#20123;&#22266;&#26377;&#32570;&#38519;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#24456;&#38590;&#35299;&#20915;&#36229;&#32454;&#31890;&#24230;ESE(Ultra-ESE)&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#36755;&#20837;&#20013;&#30340;&#36127;&#31181;&#23376;&#23454;&#20307;&#65292;&#23427;&#20204;&#23646;&#20110;&#19982;&#27491;&#31181;&#23376;&#23454;&#20307;&#30456;&#21516;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#31867;&#21035;&#65292;&#20294;&#22312;&#26576;&#20123;&#23646;&#24615;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#36127;&#31181;&#23376;&#23454;&#20307;&#28040;&#38500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04247v1 Announce Type: new  Abstract: Entity Set Expansion (ESE) aims to identify new entities belonging to the same semantic class as a given set of seed entities. Traditional methods primarily relied on positive seed entities to represent a target semantic class, which poses challenge for the representation of ultra-fine-grained semantic classes. Ultra-fine-grained semantic classes are defined based on fine-grained semantic classes with more specific attribute constraints. Describing it with positive seed entities alone cause two issues: (i) Ambiguity among ultra-fine-grained semantic classes. (ii) Inability to define "unwanted" semantic. Due to these inherent shortcomings, previous methods struggle to address the ultra-fine-grained ESE (Ultra-ESE). To solve this issue, we first introduce negative seed entities in the inputs, which belong to the same fine-grained semantic class as the positive seed entities but differ in certain attributes. Negative seed entities eliminate
&lt;/p&gt;</description></item><item><title>WMDP&#22522;&#20934;&#26159;&#19968;&#20010;&#20844;&#24320;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#29992;&#20316;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.03218</link><description>&lt;p&gt;
WMDP&#22522;&#20934;&#65306;&#36890;&#36807;&#36951;&#24536;&#27979;&#37327;&#21644;&#20943;&#23569;&#24694;&#24847;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03218
&lt;/p&gt;
&lt;p&gt;
WMDP&#22522;&#20934;&#26159;&#19968;&#20010;&#20844;&#24320;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#29992;&#20316;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03218v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#39046;&#22495; &#25688;&#35201;&#65306;&#30333;&#23467;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#25919;&#21629;&#20196;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#20104;&#24694;&#24847;&#34892;&#20026;&#32773;&#24320;&#21457;&#29983;&#29289;&#12289;&#32593;&#32476;&#21644;&#21270;&#23398;&#27494;&#22120;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#34913;&#37327;&#36825;&#20123;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#65292;&#25919;&#24220;&#26426;&#26500;&#21644;&#20027;&#35201;&#20154;&#24037;&#26234;&#33021;&#23454;&#39564;&#23460;&#27491;&#22312;&#24320;&#21457;LLMs&#30340;&#21361;&#38505;&#33021;&#21147;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#26159;&#31169;&#20154;&#30340;&#65292;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#22914;&#20309;&#20943;&#23569;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20165;&#19987;&#27880;&#20110;&#20960;&#26465;&#39640;&#24230;&#29305;&#23450;&#30340;&#24694;&#24847;&#20351;&#29992;&#36884;&#24452;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#22823;&#35268;&#27169;&#26432;&#20260;&#24615;&#27494;&#22120;&#20195;&#29702;&#65288;WMDP&#65289;&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;WMDP&#30001;&#19968;&#32452;&#23398;&#26415;&#30028;&#21644;&#25216;&#26415;&#39038;&#38382;&#32852;&#21512;&#24320;&#21457;&#65292;&#24182;&#22312;&#20844;&#24320;&#21457;&#24067;&#21069;&#20005;&#26684;&#36807;&#28388;&#20197;&#28040;&#38500;&#25935;&#24863;&#20449;&#24687;&#12290;WMDP&#26377;&#20004;&#20010;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03218v1 Announce Type: cross  Abstract: The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 4,157 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two r
&lt;/p&gt;</description></item><item><title>NaturalSpeech 3&#21033;&#29992;&#20998;&#35299;&#35774;&#35745;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#33258;&#28982;&#35821;&#38899;</title><link>https://arxiv.org/abs/2403.03100</link><description>&lt;p&gt;
NaturalSpeech 3: &#21033;&#29992;&#20998;&#35299;&#32534;&#35299;&#30721;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;-shot&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03100
&lt;/p&gt;
&lt;p&gt;
NaturalSpeech 3&#21033;&#29992;&#20998;&#35299;&#35774;&#35745;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#33258;&#28982;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#22312;&#35821;&#38899;&#36136;&#37327;&#12289;&#30456;&#20284;&#24230;&#21644;&#38901;&#24459;&#26041;&#38754;&#20173;&#23384;&#22312;&#19981;&#36275;&#12290;&#37492;&#20110;&#35821;&#38899;&#22797;&#26434;&#22320;&#21253;&#21547;&#21508;&#31181;&#23646;&#24615;&#65288;&#20363;&#22914;&#20869;&#23481;&#12289;&#38901;&#24459;&#12289;&#38899;&#33394;&#21644;&#22768;&#23398;&#32454;&#33410;&#65289;&#65292;&#32473;&#29983;&#25104;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#24819;&#27861;&#26159;&#23558;&#35821;&#38899;&#22240;&#23376;&#20998;&#35299;&#20026;&#20195;&#34920;&#19981;&#21516;&#23646;&#24615;&#30340;&#21508;&#20010;&#23376;&#31354;&#38388;&#65292;&#24182;&#21333;&#29420;&#29983;&#25104;&#23427;&#20204;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NaturalSpeech 3&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#26032;&#39062;&#30340;&#20998;&#35299;&#25193;&#25955;&#27169;&#22411;&#30340;TTS&#31995;&#32479;&#65292;&#21487;&#20197;&#20197;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#33258;&#28982;&#35821;&#38899;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;1) &#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20855;&#26377;&#20998;&#35299;&#21521;&#37327;&#37327;&#21270;&#65288;FVQ&#65289;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#65292;&#23558;&#35821;&#38899;&#27874;&#24418;&#20998;&#35299;&#20026;&#20869;&#23481;&#12289;&#38901;&#24459;&#12289;&#38899;&#33394;&#21644;&#22768;&#23398;&#32454;&#33410;&#30340;&#23376;&#31354;&#38388;&#65307;2) &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#35299;&#25193;&#25955;&#27169;&#22411;&#65292;&#26681;&#25454;&#20854;&#30456;&#24212;&#30340;&#25552;&#31034;&#29983;&#25104;&#27599;&#20010;&#23376;&#31354;&#38388;&#20013;&#30340;&#23646;&#24615;&#12290;&#20511;&#21161;&#36825;&#31181;&#20998;&#35299;&#35774;&#35745;&#65292;NaturalSpeech 3&#33021;&#22815;ef
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03100v1 Announce Type: cross  Abstract: While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can ef
&lt;/p&gt;</description></item><item><title>StructLM&#31995;&#21015;&#27169;&#22411;&#22522;&#20110;&#20840;&#38754;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#36229;&#36234;&#20102;&#22823;&#22810;&#25968;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65292;&#22312;&#32467;&#26500;&#21270;&#30693;&#35782;&#36830;&#25509;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#25104;&#23601;&#12290;</title><link>https://arxiv.org/abs/2402.16671</link><description>&lt;p&gt;
StructLM: &#26397;&#21521;&#26500;&#24314;&#32467;&#26500;&#21270;&#30693;&#35782;&#36830;&#25509;&#30340;&#36890;&#29992;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
StructLM: Towards Building Generalist Models for Structured Knowledge Grounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16671
&lt;/p&gt;
&lt;p&gt;
StructLM&#31995;&#21015;&#27169;&#22411;&#22522;&#20110;&#20840;&#38754;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#36229;&#36234;&#20102;&#22823;&#22810;&#25968;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65292;&#22312;&#32467;&#26500;&#21270;&#30693;&#35782;&#36830;&#25509;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#25104;&#23601;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#25968;&#25454;&#28304;&#65292;&#22914;&#34920;&#26684;&#12289;&#22270;&#24418;&#21644;&#25968;&#25454;&#24211;&#65292;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#30693;&#35782;&#28304;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#32431;&#25991;&#26412;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#22312;&#35299;&#37322;&#21644;&#21033;&#29992;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;LLM&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#26174;&#30528;&#19981;&#36275;&#65292;&#20363;&#22914;&#65292;ChatGPT&#24179;&#22343;&#33853;&#21518;&#20110;&#26368;&#20808;&#36827;&#27169;&#22411;(SoTA)35%&#12290;&#20026;&#22686;&#24378;LLM&#20013;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#36830;&#25509;&#65288;SKG&#65289;&#33021;&#21147;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;110&#19975;&#20010;&#31034;&#20363;&#30340;&#20840;&#38754;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;Code-LLaMA&#26550;&#26500;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;StructLM&#65292;&#21442;&#25968;&#33539;&#22260;&#20174;7B&#21040;34B&#12290;&#25105;&#20204;&#30340;StructLM&#31995;&#21015;&#22312;18&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#20013;&#26377;14&#20010;&#36229;&#36234;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;7&#20010;SKG&#20219;&#21153;&#19978;&#30830;&#31435;&#20102;&#26032;&#30340;SoTA&#25104;&#23601;&#12290;&#27492;&#22806;&#65292;StructLM&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16671v1 Announce Type: new  Abstract: Structured data sources, such as tables, graphs, and databases, are ubiquitous knowledge sources. Despite the demonstrated capabilities of large language models (LLMs) on plain text, their proficiency in interpreting and utilizing structured data remains limited. Our investigation reveals a notable deficiency in LLMs' ability to process structured data, e.g., ChatGPT lags behind state-of-the-art (SoTA) model by an average of 35%. To augment the Structured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a comprehensive instruction tuning dataset comprising 1.1 million examples. Utilizing this dataset, we train a series of models, referred to as StructLM, based on the Code-LLaMA architecture, ranging from 7B to 34B parameters. Our StructLM series surpasses task-specific models on 14 out of 18 evaluated datasets and establishes new SoTA achievements on 7 SKG tasks. Furthermore, StructLM demonstrates exceptional generalizat
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;LLM&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#39044;&#27979;&#22823;&#23398;&#35838;&#31243;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#36866;&#24212;SDGs&#12290;</title><link>https://arxiv.org/abs/2402.16420</link><description>&lt;p&gt;
&#20351;&#29992;&#35838;&#31243;&#25551;&#36848;&#39044;&#27979;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631; - &#20174;LLMs&#21040;&#20256;&#32479;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Predicting Sustainable Development Goals Using Course Descriptions -- from LLMs to Conventional Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16420
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;LLM&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#39044;&#27979;&#22823;&#23398;&#35838;&#31243;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#36866;&#24212;SDGs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#39044;&#27979;&#22823;&#23398;&#35838;&#31243;&#32852;&#21512;&#22269;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#65288;SDG&#65289;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21517;&#20026;PaLM 2&#30340;LLM&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#23558;&#21547;&#26377;&#22024;&#26434;&#20154;&#24037;&#32534;&#20889;&#30340;&#35838;&#31243;&#25551;&#36848;&#20316;&#20026;&#36755;&#20837;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#20102;&#20960;&#20010;&#19981;&#21516;&#30340;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#22823;&#23398;&#35838;&#31243;&#30340;SDG&#12290;&#36825;&#39033;&#24037;&#20316;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#36866;&#24212;SDG&#30340;&#22823;&#23398;&#23618;&#38754;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#34920;&#29616;&#26368;&#22909;&#30340;&#27169;&#22411;&#26159;&#20855;&#26377;0.786&#30340;F1&#20998;&#25968;&#30340;BART&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16420v1 Announce Type: new  Abstract: We present our work on predicting United Nations sustainable development goals (SDG) for university courses. We use an LLM named PaLM 2 to generate training data given a noisy human-authored course description input as input. We use this data to train several different smaller language models to predict SDGs for university courses. This work contributes to better university level adaptation of SDGs. The best performing model in our experiments was BART with an F1-score of 0.786.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;&#22788;&#29702;&#27169;&#22359;&#21644;&#20998;&#26512;&#27169;&#22359;&#30340;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#25968;&#25454;&#36136;&#37327;&#24182;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16358</link><description>&lt;p&gt;
&#19968;&#20010;&#25972;&#21512;&#30340;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#29992;&#20110;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Integrated Data Processing Framework for Pretraining Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16358
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;&#22788;&#29702;&#27169;&#22359;&#21644;&#20998;&#26512;&#27169;&#22359;&#30340;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#25968;&#25454;&#36136;&#37327;&#24182;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#20102;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#32463;&#24120;&#38656;&#35201;&#25163;&#21160;&#20174;&#19981;&#21516;&#26469;&#28304;&#31574;&#21010;&#25968;&#25454;&#38598;&#65292;&#24182;&#20026;&#27599;&#20010;&#25968;&#25454;&#23384;&#20648;&#24211;&#24320;&#21457;&#19987;&#38376;&#30340;&#25968;&#25454;&#28165;&#27927;&#27969;&#31243;&#12290;&#32570;&#20047;&#32479;&#19968;&#30340;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#36825;&#19968;&#36807;&#31243;&#37325;&#22797;&#32780;&#32321;&#29712;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;&#22788;&#29702;&#27169;&#22359;&#21644;&#20998;&#26512;&#27169;&#22359;&#30340;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#22788;&#29702;&#27169;&#22359;&#21253;&#25324;&#19968;&#31995;&#21015;&#19981;&#21516;&#31890;&#24230;&#27700;&#24179;&#30340;&#25805;&#20316;&#31526;&#65292;&#32780;&#20998;&#26512;&#27169;&#22359;&#25903;&#25345;&#23545;&#31934;&#28860;&#25968;&#25454;&#36827;&#34892;&#25506;&#26597;&#21644;&#35780;&#20272;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#26131;&#20110;&#20351;&#29992;&#19988;&#39640;&#24230;&#28789;&#27963;&#12290;&#22312;&#36825;&#31687;&#28436;&#31034;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#22914;&#20309;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#24182;&#23637;&#31034;&#23427;&#22312;&#25913;&#21892;&#25968;&#25454;&#36136;&#37327;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#19982;ChatGPT&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#31471;&#21040;&#31471;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16358v1 Announce Type: cross  Abstract: The ability of the foundation models heavily relies on large-scale, diverse, and high-quality pretraining data. In order to improve data quality, researchers and practitioners often have to manually curate datasets from difference sources and develop dedicated data cleansing pipeline for each data repository. Lacking a unified data processing framework, this process is repetitive and cumbersome. To mitigate this issue, we propose a data processing framework that integrates a Processing Module which consists of a series of operators at different granularity levels, and an Analyzing Module which supports probing and evaluation of the refined data. The proposed framework is easy to use and highly flexible. In this demo paper, we first introduce how to use this framework with some example use cases and then demonstrate its effectiveness in improving the data quality with an automated evaluation with ChatGPT and an end-to-end evaluation in 
&lt;/p&gt;</description></item><item><title>CLIP&#30456;&#20284;&#24615;&#20316;&#20026;&#26356;&#24378;&#22823;&#21644;&#26356;&#31283;&#20581;&#30340;&#24187;&#35273;&#25351;&#26631;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;CLIP&#24341;&#23548;&#35299;&#30721;&#65288;CGD&#65289;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#20943;&#23569;&#23545;&#35937;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2402.15300</link><description>&lt;p&gt;
&#35265;&#35777;&#20026;&#20449;&#65306;&#36890;&#36807;CLIP&#24341;&#23548;&#35299;&#30721;&#32531;&#35299;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15300
&lt;/p&gt;
&lt;p&gt;
CLIP&#30456;&#20284;&#24615;&#20316;&#20026;&#26356;&#24378;&#22823;&#21644;&#26356;&#31283;&#20581;&#30340;&#24187;&#35273;&#25351;&#26631;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;CLIP&#24341;&#23548;&#35299;&#30721;&#65288;CGD&#65289;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#20943;&#23569;&#23545;&#35937;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#23481;&#26131;&#20986;&#29616;&#23545;&#35937;&#24187;&#35273;&#65292;&#21363;&#29983;&#25104;&#30340;&#25991;&#26412;&#21253;&#21547;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#21477;&#23376;&#32423;LVLM&#24187;&#35273;&#36827;&#34892;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#19982;&#22270;&#20687;&#30340;CLIP&#30456;&#20284;&#24615;&#20316;&#20026;&#19968;&#20010;&#27604;&#21333;&#35789;&#21487;&#33021;&#24615;&#26356;&#24378;&#22823;&#12289;&#26356;&#31283;&#20581;&#30340;&#24187;&#35273;&#25351;&#31034;&#22120;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLIP&#24341;&#23548;&#35299;&#30721;&#65288;CGD&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#35299;&#30721;&#26102;&#30340;&#23545;&#35937;&#24187;&#35273;&#12290;CGD&#21033;&#29992;CLIP&#26469;&#24341;&#23548;&#27169;&#22411;&#30340;&#35299;&#30721;&#36807;&#31243;&#65292;&#36890;&#36807;&#22686;&#24378;&#29983;&#25104;&#25991;&#26412;&#19982;&#22270;&#20687;&#30340;&#35270;&#35273;&#32852;&#31995;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;CGD&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#23545;&#35937;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15300v1 Announce Type: cross  Abstract: Large Vision-Language Models (LVLMs) are susceptible to object hallucinations, an issue in which their generated text contains non-existent objects, greatly limiting their reliability and practicality. Current approaches often rely on the model's token likelihoods or other internal information, instruction tuning on additional datasets, or incorporating complex external tools. We first perform empirical analysis on sentence-level LVLM hallucination, finding that CLIP similarity to the image acts as a stronger and more robust indicator of hallucination compared to token likelihoods. Motivated by this, we introduce our CLIP-Guided Decoding (CGD) approach, a straightforward but effective training-free approach to reduce object hallucination at decoding time. CGD uses CLIP to guide the model's decoding process by enhancing visual grounding of generated text with the image. Experiments demonstrate that CGD effectively mitigates object hallu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22312;&#23376;&#23545;&#35937;&#32423;&#21035;&#36827;&#34892;&#22270;&#20687;&#26631;&#35760;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#23558;&#23376;&#23545;&#35937;&#27573;&#21387;&#32553;&#20026;&#32039;&#20945;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#22320;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#23545;&#35937;&#21644;&#23646;&#24615;&#25551;&#36848;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.14327</link><description>&lt;p&gt;
&#23376;&#23545;&#35937;&#32423;&#22270;&#20687;&#26631;&#35760;&#21270;
&lt;/p&gt;
&lt;p&gt;
Subobject-level Image Tokenization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14327
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22312;&#23376;&#23545;&#35937;&#32423;&#21035;&#36827;&#34892;&#22270;&#20687;&#26631;&#35760;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#23558;&#23376;&#23545;&#35937;&#27573;&#21387;&#32553;&#20026;&#32039;&#20945;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#22320;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#23545;&#35937;&#21644;&#23646;&#24615;&#25551;&#36848;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35270;&#35273;&#27169;&#22411;&#36890;&#24120;&#23558;&#22270;&#20687;&#26631;&#35760;&#20026;&#22266;&#23450;&#22823;&#23567;&#30340;&#26041;&#24418;&#34917;&#19969;&#20316;&#20026;&#36755;&#20837;&#21333;&#20803;&#65292;&#36825;&#31181;&#26041;&#27861;&#32570;&#20047;&#23545;&#22270;&#20687;&#20869;&#23481;&#30340;&#36866;&#24212;&#24615;&#65292;&#24182;&#24573;&#30053;&#20102;&#22266;&#26377;&#30340;&#20687;&#32032;&#20998;&#32452;&#32467;&#26500;&#12290;&#21463;&#35821;&#35328;&#27169;&#22411;&#24191;&#27867;&#37319;&#29992;&#30340;&#23376;&#35789;&#26631;&#35760;&#21270;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23376;&#23545;&#35937;&#32423;&#21035;&#36827;&#34892;&#22270;&#20687;&#26631;&#35760;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#23376;&#23545;&#35937;&#30001;&#36890;&#36807;&#20998;&#21106;&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;&#20998;&#21106;&#20219;&#20309;&#27169;&#22411;&#65289;&#33719;&#24471;&#30340;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#22270;&#20687;&#27573;&#34920;&#31034;&#12290;&#20026;&#20102;&#23454;&#29616;&#22522;&#20110;&#23376;&#23545;&#35937;&#26631;&#35760;&#21270;&#30340;&#23398;&#20064;&#31995;&#32479;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#65288;SeqAE&#65289;&#65292;&#23558;&#19981;&#21516;&#22823;&#23567;&#21644;&#24418;&#29366;&#30340;&#23376;&#23545;&#35937;&#27573;&#21387;&#32553;&#20026;&#32039;&#20945;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#28982;&#21518;&#23558;&#23376;&#23545;&#35937;&#23884;&#20837;&#39304;&#36865;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#23376;&#23545;&#35937;&#32423;&#21035;&#26631;&#35760;&#21270;&#26174;&#33879;&#20419;&#36827;&#20102;&#26377;&#25928;&#22320;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#23545;&#35937;&#21644;&#23646;&#24615;&#25551;&#36848;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14327v1 Announce Type: cross  Abstract: Transformer-based vision models typically tokenize images into fixed-size square patches as input units, which lacks the adaptability to image content and overlooks the inherent pixel grouping structure. Inspired by the subword tokenization widely adopted in language models, we propose an image tokenizer at a subobject level, where the subobjects are represented by semantically meaningful image segments obtained by segmentation models (e.g., segment anything models). To implement a learning system based on subobject tokenization, we first introduced a Sequence-to-sequence AutoEncoder (SeqAE) to compress subobject segments of varying sizes and shapes into compact embedding vectors, then fed the subobject embeddings into a large language model for vision language learning. Empirical results demonstrated that our subobject-level tokenization significantly facilitates efficient learning of translating images into object and attribute descr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;MPI&#20195;&#30721;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;MonoCoder&#36827;&#34892;MPI-based&#31243;&#24207;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09126</link><description>&lt;p&gt;
MPIrigen: &#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;MPI&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
MPIrigen: MPI Code Generation through Domain-Specific Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;MPI&#20195;&#30721;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;MonoCoder&#36827;&#34892;MPI-based&#31243;&#24207;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#24182;&#34892;&#35745;&#31639;&#20013;&#65292;&#39640;&#25928;&#30340;&#24182;&#34892;&#35745;&#31639;&#23588;&#20026;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#28040;&#24687;&#20256;&#36882;&#25509;&#21475;&#65288;MPI&#65289;&#38598;&#25104;&#39046;&#22495;&#12290;&#29983;&#25104;&#22522;&#20110;MPI&#30340;&#24182;&#34892;&#31243;&#24207;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24182;&#34892;&#32534;&#31243;&#20219;&#21153;&#65292;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#25506;&#35752;&#20102;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#22522;&#20110;MPI&#30340;&#24182;&#34892;&#31243;&#24207;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#21457;&#29616;&#24191;&#27867;&#20351;&#29992;&#30340;&#27169;&#22411;&#65292;&#22914;GPT-3.5&#21644;PolyCoder&#65288;&#19987;&#38376;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#27169;&#22411;&#65289;&#65292;&#22312;&#29983;&#25104;&#22522;&#20110;MPI&#30340;&#31243;&#24207;&#26102;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#30456;&#27604;&#36890;&#29992;&#31243;&#24207;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;MPI&#30456;&#20851;&#32534;&#31243;&#35821;&#35328;C&#21644;C++&#39044;&#35757;&#32451;&#30340;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;MonoCoder&#30340;&#24615;&#33021;&#26356;&#22909;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;HPCorpusMPI&#19978;&#23545;MonoCoder&#36827;&#34892;&#24494;&#35843;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;MPI-based&#31243;&#24207;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09126v1 Announce Type: cross Abstract: The imperative need to scale computation across numerous nodes highlights the significance of efficient parallel computing, particularly in the realm of Message Passing Interface (MPI) integration. The challenging parallel programming task of generating MPI-based parallel programs has remained unexplored. This study first investigates the performance of state-of-the-art language models in generating MPI-based parallel programs. Findings reveal that widely used models such as GPT-3.5 and PolyCoder (specialized multi-lingual code models) exhibit notable performance degradation, when generating MPI-based programs compared to general-purpose programs. In contrast, domain-specific models such as MonoCoder, which are pretrained on MPI-related programming languages of C and C++, outperform larger models. Subsequently, we introduce a dedicated downstream task of MPI-based program generation by fine-tuning MonoCoder on HPCorpusMPI. We call the r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#26694;&#26550;&#65288;SWEA&#65289;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#26469;&#32534;&#36753;&#30693;&#35782;&#65292;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#65292;&#36991;&#20813;&#19981;&#21487;&#36870;&#30340;&#25439;&#23475;&#21644;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2401.17809</link><description>&lt;p&gt;
SWEA:&#36890;&#36807;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
SWEA: Changing Factual Knowledge in Large Language Models via Subject Word Embedding Altering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17809
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#26694;&#26550;&#65288;SWEA&#65289;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#26469;&#32534;&#36753;&#30693;&#35782;&#65292;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#65292;&#36991;&#20813;&#19981;&#21487;&#36870;&#30340;&#25439;&#23475;&#21644;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#36817;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#20027;&#35201;&#28041;&#21450;&#20462;&#25913;&#27169;&#22411;&#21442;&#25968;&#25110;&#21521;&#29616;&#26377;&#27169;&#22411;&#28155;&#21152;&#38468;&#21152;&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#20250;&#23545;LLM&#36896;&#25104;&#19981;&#21487;&#36870;&#30340;&#24433;&#21709;&#65292;&#32780;&#21518;&#32773;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#65292;&#24182;&#19988;&#27169;&#31946;&#30340;&#21521;&#37327;&#21305;&#37197;&#24182;&#19981;&#24635;&#26159;&#21487;&#38752;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#65288;SWEA&#65289;&#26694;&#26550;&#65292;&#23427;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#65292;&#24182;&#23454;&#29616;&#32534;&#36753;&#30693;&#35782;&#30340;&#30446;&#26631;&#12290;SWEA&#22312;&#27169;&#22411;&#22806;&#37096;&#20351;&#29992;&#31934;&#30830;&#30340;&#20851;&#38190;&#21305;&#37197;&#65292;&#24182;&#36827;&#34892;&#21487;&#38752;&#30340;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#65292;&#20174;&#32780;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#32780;&#19981;&#22686;&#21152;&#25512;&#29702;&#24320;&#38144;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20248;&#21270;&#25233;&#21046;&#34701;&#21512;&#26041;&#27861;&#65292;&#39318;&#20808;&#20248;&#21270;&#32534;&#36753;&#30446;&#26631;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#28982;&#21518;&#25233;&#21046;&#30693;&#35782;&#23884;&#20837;&#32500;&#24230;&#65288;KED&#65289;&#20197;&#33719;&#24471;&#26368;&#32456;&#34701;&#21512;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#22240;&#27492;&#25552;&#20986;&#20102;SWEAOS&#20803;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model editing has recently gained widespread attention. Current model editing methods primarily involve modifying model parameters or adding additional modules to the existing model. However, the former causes irreversible damage to LLMs, while the latter incurs additional inference overhead and fuzzy vector matching is not always reliable. To address these issues, we propose an expandable Subject Word Embedding Altering (SWEA) framework, which modifies the representation of subjects and achieve the goal of editing knowledge during the inference stage. SWEA uses precise key matching outside the model and performs reliable subject word embedding altering, thus protecting the original weights of the model without increasing inference overhead. We then propose optimizing then suppressing fusion method, which first optimizes the embedding vector for the editing target and then suppresses the Knowledge Embedding Dimension (KED) to obtain the final fused embedding. We thus propose SWEAOS met
&lt;/p&gt;</description></item><item><title>GPT-4 Vision&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#20855;&#26377;&#19987;&#23478;&#32423;&#20934;&#30830;&#24230;&#65292;&#20294;&#22312;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2401.08396</link><description>&lt;p&gt;
GPT-4 Vision&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#19987;&#23478;&#32423;&#20934;&#30830;&#24230;&#32972;&#21518;&#30340;&#38544;&#34255;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;
Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine. (arXiv:2401.08396v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08396
&lt;/p&gt;
&lt;p&gt;
GPT-4 Vision&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#20855;&#26377;&#19987;&#23478;&#32423;&#20934;&#30830;&#24230;&#65292;&#20294;&#22312;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20855;&#26377;Vision&#21151;&#33021;&#30340;GPT-4&#22312;&#21307;&#23398;&#25361;&#25112;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#21307;&#29983;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35780;&#20272;&#20027;&#35201;&#20851;&#27880;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#20934;&#30830;&#24230;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;GPT-4V&#22312;&#35299;&#20915;&#26032;&#33521;&#26684;&#20848;&#21307;&#23398;&#26434;&#24535;&#22270;&#20687;&#25361;&#25112;&#20013;&#30340;&#22270;&#20687;&#29702;&#35299;&#12289;&#21307;&#23398;&#30693;&#35782;&#22238;&#24518;&#21644;&#36880;&#27493;&#22810;&#27169;&#24577;&#25512;&#29702;&#30340;&#21407;&#29702;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#25193;&#23637;&#20102;&#24403;&#21069;&#30340;&#30740;&#31350;&#33539;&#22260;&#12290;&#35780;&#20272;&#32467;&#26524;&#35777;&#23454;&#65292;GPT-4V&#22312;&#22810;&#39033;&#36873;&#25321;&#20934;&#30830;&#24230;&#19978;&#20248;&#20110;&#20154;&#31867;&#21307;&#29983;&#65288;88.0% vs. 77.0%&#65292;p=0.034&#65289;&#12290;GPT-4V&#22312;&#21307;&#29983;&#22238;&#31572;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#34920;&#29616;&#20986;&#36229;&#36807;80%&#30340;&#20934;&#30830;&#24230;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-4V&#22312;&#26368;&#32456;&#20570;&#20986;&#27491;&#30830;&#36873;&#25321;&#30340;&#24773;&#20917;&#19979;&#65292;&#32463;&#24120;&#25552;&#20379;&#26377;&#32570;&#38519;&#30340;&#25512;&#29702;&#65288;27.3%&#65289;&#65292;&#20854;&#20013;&#26368;&#31361;&#20986;&#30340;&#26159;&#22270;&#20687;&#29702;&#35299;&#65288;21.6%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies indicate that Generative Pre-trained Transformer 4 with Vision (GPT-4V) outperforms human physicians in medical challenge tasks. However, these evaluations primarily focused on the accuracy of multi-choice questions alone. Our study extends the current scope by conducting a comprehensive analysis of GPT-4V's rationales of image comprehension, recall of medical knowledge, and step-by-step multimodal reasoning when solving New England Journal of Medicine (NEJM) Image Challenges - an imaging quiz designed to test the knowledge and diagnostic capabilities of medical professionals. Evaluation results confirmed that GPT-4V outperforms human physicians regarding multi-choice accuracy (88.0% vs. 77.0%, p=0.034). GPT-4V also performs well in cases where physicians incorrectly answer, with over 80% accuracy. However, we discovered that GPT-4V frequently presents flawed rationales in cases where it makes the correct final choices (27.3%), most prominent in image comprehension (21.6
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;ChatGPT&#21644;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24341;&#25806;&#22312;&#23558;&#20013;&#25991;&#22806;&#20132;&#25991;&#26412;&#32763;&#35793;&#20026;&#33521;&#25991;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#21644;&#20154;&#24037;&#35780;&#20272;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2401.05176</link><description>&lt;p&gt;
&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;ChatGPT&#19982;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22312;&#32763;&#35793;&#20013;&#30340;&#31454;&#20105;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Rival Neural Machine Translation? A Comparative Study. (arXiv:2401.05176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;ChatGPT&#21644;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24341;&#25806;&#22312;&#23558;&#20013;&#25991;&#22806;&#20132;&#25991;&#26412;&#32763;&#35793;&#20026;&#33521;&#25991;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#21644;&#20154;&#24037;&#35780;&#20272;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#36234;&#26469;&#36234;&#22810;&#22320;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32763;&#35793;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#21152;&#30340;&#32972;&#26223;&#19979;&#65292;&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20027;&#27969;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#24341;&#25806;&#22312;&#23558;&#20013;&#25991;&#22806;&#20132;&#25991;&#26412;&#32763;&#35793;&#20026;&#33521;&#25991;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#22235;&#20010;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#21644;&#22522;&#20110;&#38169;&#35823;&#31867;&#22411;&#21644;&#20845;&#20010;&#20998;&#26512;&#32454;&#21017;&#30340;&#20154;&#24037;&#35780;&#20272;&#65292;&#32771;&#23519;&#20102;ChatGPT&#21644;NMT&#24341;&#25806;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#23545;&#20110;ChatGPT&#22312;&#19981;&#21516;&#25552;&#31034;&#21644;NMT&#31995;&#32479;&#19979;&#30340;&#34920;&#29616;&#24471;&#20986;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#32780;&#24403;ChatGPT&#25552;&#20379;&#31034;&#20363;&#25110;&#32763;&#35793;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26102;&#65292;&#20154;&#24037;&#35780;&#20272;&#32773;&#24448;&#24448;&#20250;&#32473;&#20104;&#26126;&#26174;&#36739;&#39640;&#30340;&#35780;&#20998;&#12290;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#19982;&#20154;&#24037;&#35780;&#20272;&#32500;&#24230;&#20043;&#38388;&#30340;&#20004;&#20004;&#30456;&#20851;&#24615;&#32467;&#26524;&#36739;&#24369;&#19988;&#19981;&#26174;&#33879;&#65292;&#36825;&#34920;&#26126;&#20102;&#20004;&#31181;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the increasing interest in leveraging large language models for translation, this paper evaluates the capabilities of large language models (LLMs) represented by ChatGPT in comparison to the mainstream neural machine translation (NMT) engines in translating Chinese diplomatic texts into English. Specifically, we examine the translation quality of ChatGPT and NMT engines as measured by four automated metrics and human evaluation based on an error-typology and six analytic rubrics. Our findings show that automated metrics yield similar results for ChatGPT under different prompts and NMT systems, while human annotators tend to assign noticeably higher scores to ChatGPT when it is provided an example or contextual information about the translation task. Pairwise correlation between automated metrics and dimensions of human evaluation produces weak and non-significant results, suggesting the divergence between the two methods of translation quality assessment. These findings pro
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21098;&#26525;&#33267;&#33267;&#23569;50&#65285;&#30340;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#31232;&#30095;&#24615;&#27700;&#24179;&#21644;&#20943;&#23569;&#21098;&#26525;&#24341;&#36215;&#30340;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#19982;&#37327;&#21270;&#20860;&#23481;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.09499</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#27425;&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models. (arXiv:2310.09499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09499
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21098;&#26525;&#33267;&#33267;&#23569;50&#65285;&#30340;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#31232;&#30095;&#24615;&#27700;&#24179;&#21644;&#20943;&#23569;&#21098;&#26525;&#24341;&#36215;&#30340;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#19982;&#37327;&#21270;&#20860;&#23481;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#31995;&#21015;&#20013;&#30340;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#25512;&#29702;&#24310;&#36831;&#65292;&#24040;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#37327;&#21270;&#12289;&#21098;&#26525;&#21644;&#20854;&#20182;&#26041;&#27861;&#25552;&#39640;LLMs&#30340;&#25928;&#29575;&#25104;&#20026;LLM&#30740;&#31350;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Hessian&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;LLMs&#21098;&#26525;&#33267;&#33267;&#23569;50%&#30340;&#31232;&#30095;&#24615;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#23427;&#26681;&#25454;&#25935;&#24863;&#24230;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;&#31232;&#30095;&#24615;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#38477;&#20302;&#21098;&#26525;&#24341;&#36215;&#30340;&#35823;&#24046;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#31232;&#30095;&#24615;&#27700;&#24179;&#12290;&#24403;&#31232;&#30095;&#24230;&#38750;&#24120;&#39640;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#20248;&#21183;&#26356;&#21152;&#26126;&#26174;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#37327;&#21270;&#20860;&#23481;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#21387;&#32553;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Various Large Language Models(LLMs) from the Generative Pretrained Transformer~(GPT) family have achieved outstanding performances in a wide range of text generation tasks. However, the enormous model sizes have hindered their practical use in real-world applications due to high inference latency. Therefore, improving the efficiencies of LLMs through quantization, pruning, and other means has been a key issue in LLM studies. In this work, we propose a method based on Hessian sensitivity-aware mixed sparsity pruning to prune LLMs to at least 50\% sparsity without the need of any retraining. It allocates sparsity adaptively based on sensitivity, allowing us to reduce pruning-induced error while maintaining the overall sparsity level. The advantages of the proposed method exhibit even more when the sparsity is extremely high. Furthermore, our method is compatible with quantization, enabling further compression of LLMs.
&lt;/p&gt;</description></item><item><title>VAL&#26159;&#19968;&#31181;&#20132;&#20114;&#24335;&#20219;&#21153;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#31526;&#21495;&#38598;&#25104;&#30340;&#29702;&#24565;&#65292;&#23454;&#29616;&#20102;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#36827;&#34892;&#20132;&#20114;&#24335;&#23398;&#20064;&#30340;&#20998;&#23618;&#20219;&#21153;&#30693;&#35782;&#30340;&#33719;&#21462;&#12290;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#21487;&#35299;&#37322;&#24182;&#33021;&#22815;&#25512;&#24191;&#21040;&#25191;&#34892;&#26032;&#20219;&#21153;&#12290;&#22312;&#35270;&#39057;&#28216;&#25103;&#29615;&#22659;&#20013;&#30340;&#29992;&#25143;&#20132;&#20114;&#23454;&#39564;&#34920;&#26126;&#65292;VAL&#33021;&#22815;&#20174;&#26377;&#38480;&#30340;&#25351;&#20196;&#20013;&#25104;&#21151;&#23398;&#21040;&#26377;&#25928;&#30340;&#20219;&#21153;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2310.01627</link><description>&lt;p&gt;
VAL&#65306;&#24102;&#26377;GPT&#23545;&#35805;&#35299;&#26512;&#30340;&#20132;&#20114;&#24335;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
VAL: Interactive Task Learning with GPT Dialog Parsing. (arXiv:2310.01627v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01627
&lt;/p&gt;
&lt;p&gt;
VAL&#26159;&#19968;&#31181;&#20132;&#20114;&#24335;&#20219;&#21153;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#31526;&#21495;&#38598;&#25104;&#30340;&#29702;&#24565;&#65292;&#23454;&#29616;&#20102;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#36827;&#34892;&#20132;&#20114;&#24335;&#23398;&#20064;&#30340;&#20998;&#23618;&#20219;&#21153;&#30693;&#35782;&#30340;&#33719;&#21462;&#12290;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#21487;&#35299;&#37322;&#24182;&#33021;&#22815;&#25512;&#24191;&#21040;&#25191;&#34892;&#26032;&#20219;&#21153;&#12290;&#22312;&#35270;&#39057;&#28216;&#25103;&#29615;&#22659;&#20013;&#30340;&#29992;&#25143;&#20132;&#20114;&#23454;&#39564;&#34920;&#26126;&#65292;VAL&#33021;&#22815;&#20174;&#26377;&#38480;&#30340;&#25351;&#20196;&#20013;&#25104;&#21151;&#23398;&#21040;&#26377;&#25928;&#30340;&#20219;&#21153;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36890;&#24120;&#38656;&#35201;&#25968;&#30334;&#19975;&#20010;&#26679;&#26412;&#26469;&#29983;&#25104;&#38745;&#24577;&#30340;&#40657;&#31665;&#27169;&#22411;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20132;&#20114;&#24335;&#20219;&#21153;&#23398;&#20064;&#65288;ITL&#65289;&#24378;&#35843;&#20174;&#20154;&#31867;&#25552;&#20379;&#30340;&#26377;&#38480;&#25351;&#20196;&#20013;&#36880;&#27493;&#33719;&#24471;&#30693;&#35782;&#65292;&#36825;&#20123;&#25351;&#20196;&#20197;&#33258;&#28982;&#35821;&#35328;&#31561;&#24418;&#24335;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;ITL&#31995;&#32479;&#24448;&#24448;&#21463;&#21040;&#33030;&#24369;&#12289;&#23481;&#26131;&#20986;&#38169;&#30340;&#35821;&#35328;&#35299;&#26512;&#30340;&#22256;&#25200;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#33030;&#24369;&#24615;&#26377;&#19968;&#23450;&#30340;&#25269;&#25239;&#33021;&#21147;&#65292;&#20294;&#19981;&#20855;&#22791;&#21487;&#35299;&#37322;&#24615;&#65292;&#20063;&#26080;&#27861;&#36827;&#34892;&#22686;&#37327;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;VAL&#65292;&#19968;&#31181;&#20855;&#26377;&#26032;&#30340;LLM/&#31526;&#21495;&#38598;&#25104;&#29702;&#24565;&#30340;ITL&#31995;&#32479;&#12290;&#36890;&#36807;&#20165;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#20351;&#29992;LLMs&#65288;&#20363;&#22914;&#35859;&#35789;&#21644;&#21442;&#25968;&#36873;&#25321;&#65289;&#65292;&#22312;&#31639;&#27861;&#26694;&#26550;&#20869;&#65292;VAL&#21033;&#29992;LLMs&#30340;&#20248;&#21183;&#65292;&#25903;&#25345;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#20132;&#20114;&#24335;&#23398;&#20064;&#20998;&#23618;&#20219;&#21153;&#30693;&#35782;&#12290;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#26159;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#65292;&#24182;&#33021;&#22815;&#25512;&#24191;&#21040;&#25903;&#25345;&#25191;&#34892;&#26032;&#20219;&#21153;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#35270;&#39057;&#28216;&#25103;&#29615;&#22659;&#20013;&#30740;&#31350;&#20102;&#29992;&#25143;&#19982;VAL&#30340;&#20132;&#20114;&#65292;&#21457;&#29616;&#22823;&#37096;&#20998;&#29992;&#25143;&#33021;&#22815;&#20174;&#26377;&#38480;&#30340;&#25351;&#20196;&#20013;&#25104;&#21151;&#23398;&#21040;&#26377;&#25928;&#30340;&#20219;&#21153;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning often requires millions of examples to produce static, black-box models. In contrast, interactive task learning (ITL) emphasizes incremental knowledge acquisition from limited instruction provided by humans in modalities such as natural language. However, in practice, ITL systems often suffers from brittle, error-prone language parsing. Large language models (LLMs) are resistant to brittleness but are not interpretable and cannot learn incrementally. We present VAL, an ITL system with a new philosophy for LLM/symbolic integration. By using LLMs only for specific tasks -- such as predicate and argument selection -- within an algorithmic framework, VAL reaps the benefits of LLMs to support interactive learning of hierarchical task knowledge from natural language. Acquired knowledge is human interpretable and generalizes to support execution of novel tasks without additional training. We studied users' interactions with VAL in a video game setting, finding that most
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#26041;&#27861;&#65292;&#21033;&#29992;BioBERT&#27169;&#22411;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#29983;&#27542;&#32454;&#32990;&#31995;&#22522;&#22240;&#19982;&#30142;&#30149;&#30340;&#20851;&#32852;&#65292;&#23637;&#31034;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#37325;&#35201;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2309.13061</link><description>&lt;p&gt;
&#23558;BioBERT&#24212;&#29992;&#20110;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#29983;&#27542;&#32454;&#32990;&#31995;&#22522;&#22240;&#19982;&#30142;&#30149;&#20851;&#32852;&#20197;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Applying BioBERT to Extract Germline Gene-Disease Associations for Building a Knowledge Graph from the Biomedical Literature. (arXiv:2309.13061v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#26041;&#27861;&#65292;&#21033;&#29992;BioBERT&#27169;&#22411;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#29983;&#27542;&#32454;&#32990;&#31995;&#22522;&#22240;&#19982;&#30142;&#30149;&#30340;&#20851;&#32852;&#65292;&#23637;&#31034;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#37325;&#35201;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#34920;&#30340;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#26368;&#26032;&#36827;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#33258;&#21160;&#25552;&#21462;&#12289;&#35268;&#33539;&#21270;&#21644;&#34920;&#31034;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;(&#22914;&#22522;&#22240;&#21644;&#30142;&#30149;)&#30693;&#35782;&#30340;&#27987;&#21402;&#20852;&#36259;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22522;&#22240;&#21644;&#30142;&#30149;&#39046;&#22495;&#30340;&#29983;&#27542;&#32454;&#32990;&#31995;&#25688;&#35201;&#65292;&#29992;&#20110;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#20197;&#23637;&#31034;&#36825;&#19968;&#39046;&#22495;&#30340;&#22823;&#37327;&#24037;&#20316;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SimpleGermKG&#30340;&#33258;&#21160;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#26041;&#27861;&#65292;&#23558;&#29983;&#27542;&#32454;&#32990;&#31995;&#22522;&#22240;&#21644;&#30142;&#30149;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;BioBERT&#27169;&#22411;&#26469;&#25552;&#21462;&#22522;&#22240;&#21644;&#30142;&#30149;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26412;&#20307;&#21644;&#35268;&#21017;&#30340;&#31639;&#27861;&#26469;&#35268;&#33539;&#21270;&#21644;&#28040;&#27495;&#20041;&#21307;&#23398;&#26415;&#35821;&#12290;&#23545;&#20110;&#25991;&#31456;&#12289;&#22522;&#22240;&#21644;&#30142;&#30149;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#31181;&#37096;&#20998;-&#25972;&#20307;&#20851;&#31995;&#26041;&#27861;&#26469;&#23558;&#27599;&#20010;&#23454;&#20307;&#19982;&#20854;&#25968;&#25454;&#28304;&#36830;&#25509;&#24182;&#20197;&#22270;&#24418;&#21270;&#30693;&#35782;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Published biomedical information has and continues to rapidly increase. The recent advancements in Natural Language Processing (NLP), have generated considerable interest in automating the extraction, normalization, and representation of biomedical knowledge about entities such as genes and diseases. Our study analyzes germline abstracts in the construction of knowledge graphs of the of the immense work that has been done in this area for genes and diseases. This paper presents SimpleGermKG, an automatic knowledge graph construction approach that connects germline genes and diseases. For the extraction of genes and diseases, we employ BioBERT, a pre-trained BERT model on biomedical corpora. We propose an ontology-based and rule-based algorithm to standardize and disambiguate medical terms. For semantic relationships between articles, genes, and diseases, we implemented a part-whole relation approach to connect each entity with its data source and visualize them in a graph-based knowled
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#23545;&#25239;&#24615;&#22270;&#20687;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#21019;&#24314;&#22270;&#20687;&#21163;&#25345;&#12290;&#36890;&#36807;&#30740;&#31350;&#19977;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#25915;&#20987;&#23545;&#26368;&#26032;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#39640;&#36798;90&#65285;&#20197;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;&#35813;&#30740;&#31350;&#24341;&#21457;&#20102;&#23545;&#22522;&#30784;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#20005;&#37325;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2309.00236</link><description>&lt;p&gt;
&#22270;&#20687;&#21163;&#25345;&#65306;&#23545;&#25239;&#24615;&#22270;&#20687;&#33021;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Image Hijacking: Adversarial Images can Control Generative Models at Runtime. (arXiv:2309.00236v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#23545;&#25239;&#24615;&#22270;&#20687;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#21019;&#24314;&#22270;&#20687;&#21163;&#25345;&#12290;&#36890;&#36807;&#30740;&#31350;&#19977;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#25915;&#20987;&#23545;&#26368;&#26032;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#39640;&#36798;90&#65285;&#20197;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;&#35813;&#30740;&#31350;&#24341;&#21457;&#20102;&#23545;&#22522;&#30784;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#20005;&#37325;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20813;&#21463;&#24694;&#24847;&#34892;&#20026;&#32773;&#30340;&#25915;&#20987;&#65311;&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#30340;&#22270;&#20687;&#36755;&#20837;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#22270;&#20687;&#21163;&#25345;&#65292;&#21363;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#22270;&#20687;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#34892;&#20026;&#21305;&#37197;&#8221;&#30340;&#36890;&#29992;&#26041;&#27861;&#26469;&#21019;&#24314;&#22270;&#20687;&#21163;&#25345;&#65292;&#24182;&#29992;&#23427;&#26469;&#25506;&#32034;&#19977;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#65306;&#20855;&#20307;&#23383;&#31526;&#20018;&#25915;&#20987;&#21487;&#20197;&#29983;&#25104;&#20219;&#24847;&#34987;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#36755;&#20986;&#65307;&#27844;&#38706;&#19978;&#19979;&#25991;&#25915;&#20987;&#21487;&#20197;&#23558;&#19978;&#19979;&#25991;&#31383;&#21475;&#20013;&#30340;&#20449;&#24687;&#27844;&#38706;&#21040;&#36755;&#20986;&#20013;&#65307;&#36234;&#29425;&#25915;&#20987;&#21487;&#20197;&#32469;&#36807;&#27169;&#22411;&#30340;&#23433;&#20840;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;&#22522;&#20110;CLIP&#21644;LLaMA-2&#30340;&#26368;&#26032;VLM&#27169;&#22411;LLaVA-2&#36827;&#34892;&#20102;&#36825;&#20123;&#25915;&#20987;&#30340;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#25105;&#20204;&#25152;&#26377;&#30340;&#25915;&#20987;&#31867;&#22411;&#25104;&#21151;&#29575;&#22343;&#22312;90&#65285;&#20197;&#19978;&#12290;&#32780;&#19988;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#26159;&#33258;&#21160;&#21270;&#30340;&#65292;&#21482;&#38656;&#35201;&#23545;&#22270;&#20687;&#36827;&#34892;&#23567;&#30340;&#25200;&#21160;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#25552;&#20986;&#20102;&#20005;&#37325;&#30340;&#25285;&#24551;&#12290;&#22914;&#26524;&#22270;&#20687;&#21163;&#25345;&#19982;CIFAR-10&#20013;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#19968;&#26679;&#38590;&#20197;&#38450;&#24481;&#65292;&#37027;&#20040;&#21487;&#33021;&#38656;&#35201;&#24456;&#22810;&#24180;&#25165;&#33021;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are foundation models secure from malicious actors? In this work, we focus on the image input to a vision-language model (VLM). We discover image hijacks, adversarial images that control generative models at runtime. We introduce Behavior Matching, a general method for creating image hijacks, and we use it to explore three types of attacks. Specific string attacks generate arbitrary output of the adversary's choosing. Leak context attacks leak information from the context window into the output. Jailbreak attacks circumvent a model's safety training. We study these attacks against LLaVA-2, a state-of-the-art VLM based on CLIP and LLaMA-2, and find that all our attack types have above a 90\% success rate. Moreover, our attacks are automated and require only small image perturbations. These findings raise serious concerns about the security of foundation models. If image hijacks are as difficult to defend against as adversarial examples in CIFAR-10, then it might be many years before a s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#22522;&#20110;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#21151;&#33021;&#35789;&#30340;&#24847;&#20041;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#20102;&#35299;&#27169;&#22411;&#21644;&#20799;&#31461;&#22914;&#20309;&#23398;&#20064;&#36825;&#20123;&#35789;&#27719;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20197;&#35270;&#35273;&#20026;&#22522;&#30784;&#30340;&#35821;&#35328;&#19978;&#35757;&#32451;&#30340;&#36882;&#24402;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#38656;&#35201;&#31354;&#38388;&#21644;&#25968;&#23383;&#25512;&#29702;&#30340;&#21151;&#33021;&#35789;&#30340;&#26799;&#24230;&#35821;&#20041;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#27809;&#26377;&#36923;&#36753;&#25512;&#29702;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#21040;"&#21644;"&#21644;"&#25110;"&#30340;&#24847;&#20041;&#65292;&#20197;&#21450;&#36805;&#36895;&#21457;&#23637;&#20986;&#26367;&#25442;&#25512;&#35770;&#30340;&#33021;&#21147;&#30340;&#26089;&#26399;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.08628</link><description>&lt;p&gt;
&#20174;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#20013;&#20197;&#22522;&#20110;&#35821;&#22659;&#35821;&#35328;&#23398;&#20064;&#21151;&#33021;&#35789;&#30340;&#24847;&#20041;
&lt;/p&gt;
&lt;p&gt;
Learning the meanings of function words from grounded language using a visual question answering model. (arXiv:2308.08628v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#22522;&#20110;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#21151;&#33021;&#35789;&#30340;&#24847;&#20041;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#20102;&#35299;&#27169;&#22411;&#21644;&#20799;&#31461;&#22914;&#20309;&#23398;&#20064;&#36825;&#20123;&#35789;&#27719;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20197;&#35270;&#35273;&#20026;&#22522;&#30784;&#30340;&#35821;&#35328;&#19978;&#35757;&#32451;&#30340;&#36882;&#24402;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#38656;&#35201;&#31354;&#38388;&#21644;&#25968;&#23383;&#25512;&#29702;&#30340;&#21151;&#33021;&#35789;&#30340;&#26799;&#24230;&#35821;&#20041;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#27809;&#26377;&#36923;&#36753;&#25512;&#29702;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#21040;"&#21644;"&#21644;"&#25110;"&#30340;&#24847;&#20041;&#65292;&#20197;&#21450;&#36805;&#36895;&#21457;&#23637;&#20986;&#26367;&#25442;&#25512;&#35770;&#30340;&#33021;&#21147;&#30340;&#26089;&#26399;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#19968;&#20010;&#30475;&#20284;&#31616;&#21333;&#30340;&#21151;&#33021;&#35789;&#65292;&#22914;&#8220;&#25110;&#32773;&#8221;&#65292;&#8220;&#22312;......&#21518;&#38754;&#8221;&#65292;&#25110;&#8220;&#26356;&#22810;&#8221;&#21487;&#33021;&#38656;&#35201;&#36923;&#36753;&#12289;&#25968;&#23383;&#21644;&#20851;&#31995;&#25512;&#29702;&#12290;&#20799;&#31461;&#22914;&#20309;&#23398;&#20064;&#36825;&#26679;&#30340;&#35789;&#27719;&#65311;&#26082;&#24448;&#30340;&#20064;&#24471;&#29702;&#35770;&#36890;&#24120;&#20381;&#36182;&#20110;&#35748;&#20026;&#20855;&#26377;&#20808;&#22825;&#30693;&#35782;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#26174;&#28982;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#21151;&#33021;&#35789;&#26469;&#22238;&#31572;&#20851;&#20110;&#22797;&#26434;&#35270;&#35273;&#22330;&#26223;&#30340;&#38382;&#39064;&#32780;&#36827;&#34892;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#27169;&#22411;&#23545;&#21151;&#33021;&#35789;&#30340;&#23398;&#20064;&#65292;&#24182;&#24076;&#26395;&#26356;&#22909;&#22320;&#20102;&#35299;&#36825;&#20123;&#35789;&#27719;&#30340;&#24847;&#20041;&#22914;&#20309;&#34987;&#27169;&#22411;&#21644;&#20799;&#31461;&#25152;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20197;&#35270;&#35273;&#20026;&#22522;&#30784;&#30340;&#35821;&#35328;&#19978;&#35757;&#32451;&#30340;&#36882;&#24402;&#27169;&#22411;&#23398;&#20064;&#20102;&#38656;&#35201;&#31354;&#38388;&#21644;&#25968;&#23383;&#25512;&#29702;&#30340;&#21151;&#33021;&#35789;&#30340;&#26799;&#24230;&#35821;&#20041;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#36923;&#36753;&#25512;&#29702;&#30340;&#20808;&#39564;&#30693;&#35782;&#19979;&#23398;&#20064;&#21040;"&#21644;"&#21644;"&#25110;"&#30340;&#24847;&#20041;&#65292;&#24182;&#36805;&#36895;&#21457;&#23637;&#20986;&#36827;&#34892;&#26367;&#25442;&#25512;&#35770;&#30340;&#33021;&#21147;&#30340;&#26089;&#26399;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpreting a seemingly-simple function word like "or", "behind", or "more" can require logical, numerical, and relational reasoning. How are such words learned by children? Prior acquisition theories have often relied on positing a foundation of innate knowledge. Yet recent neural-network based visual question answering models apparently can learn to use function words as part of answering questions about complex visual scenes. In this paper, we study what these models learn about function words, in the hope of better understanding how the meanings of these words can be learnt by both models and children. We show that recurrent models trained on visually grounded language learn gradient semantics for function words requiring spacial and numerical reasoning. Furthermore, we find that these models can learn the meanings of logical connectives "and" and "or" without any prior knowledge of logical reasoning, as well as early evidence that they can develop the ability to reason about alte
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;MERT&#65292;&#21033;&#29992;&#20102;&#25945;&#24072;&#27169;&#22411;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#30340;&#32452;&#21512;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.00107</link><description>&lt;p&gt;
MERT:&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22768;&#23398;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training. (arXiv:2306.00107v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00107
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;MERT&#65292;&#21033;&#29992;&#20102;&#25945;&#24072;&#27169;&#22411;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#30340;&#32452;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26368;&#36817;&#22312;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#35821;&#38899;&#39046;&#22495;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#35757;&#32451;&#36890;&#29992;&#27169;&#22411;&#30340;&#19968;&#31181;&#24456;&#26377;&#21069;&#26223;&#30340;&#33539;&#20363;&#65292;&#23545;&#20110;&#36328;&#36234;&#38899;&#20048;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#35843;&#24615;&#21644;&#38899;&#39640;&#36825;&#26679;&#30340;&#29305;&#27530;&#38899;&#20048;&#30693;&#35782;&#30340;&#24314;&#27169;&#39047;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22768;&#23398;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;&#65292;&#21363;MERT&#12290;&#22312;&#25105;&#20204;&#30340;&#25506;&#32034;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26356;&#20248;&#31168;&#30340;&#25945;&#24072;&#27169;&#22411;&#32452;&#21512;&#65292;&#36825;&#31181;&#32452;&#21512;&#26041;&#27861;&#22312;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is primarily due to the distinctive challenges associated with modelling musical knowledge, particularly its tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified a superior combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a m
&lt;/p&gt;</description></item><item><title>VideoXum&#26159;&#19968;&#20010;&#26032;&#30340;&#32852;&#21512;&#35270;&#39057;&#21644;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#65292;&#23427;&#30340;&#30446;&#26631;&#26159;&#20174;&#38271;&#35270;&#39057;&#20013;&#29983;&#25104;&#23545;&#24212;&#30340;&#31616;&#21270;&#35270;&#39057;&#21098;&#36753;&#21644;&#25991;&#26412;&#25688;&#35201;&#65292;&#21033;&#29992;&#20102;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20851;&#32852;&#21644;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#12290;&#35813;&#27169;&#22411;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#35270;&#39057;&#21644;&#25991;&#26412;&#25688;&#35201;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.12060</link><description>&lt;p&gt;
VideoXum: &#35270;&#39057;&#30340;&#36328;&#27169;&#24577;&#35270;&#35273;&#21644;&#25991;&#26412;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
VideoXum: Cross-modal Visual and Textural Summarization of Videos. (arXiv:2303.12060v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12060
&lt;/p&gt;
&lt;p&gt;
VideoXum&#26159;&#19968;&#20010;&#26032;&#30340;&#32852;&#21512;&#35270;&#39057;&#21644;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#65292;&#23427;&#30340;&#30446;&#26631;&#26159;&#20174;&#38271;&#35270;&#39057;&#20013;&#29983;&#25104;&#23545;&#24212;&#30340;&#31616;&#21270;&#35270;&#39057;&#21098;&#36753;&#21644;&#25991;&#26412;&#25688;&#35201;&#65292;&#21033;&#29992;&#20102;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20851;&#32852;&#21644;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#12290;&#35813;&#27169;&#22411;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#35270;&#39057;&#21644;&#25991;&#26412;&#25688;&#35201;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#25688;&#35201;&#26088;&#22312;&#20174;&#28304;&#35270;&#39057;&#20013;&#25552;&#28860;&#20986;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#65292;&#20197;&#29983;&#25104;&#31616;&#30701;&#30340;&#35270;&#39057;&#21098;&#36753;&#25110;&#25991;&#26412;&#21465;&#36848;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#21512;&#35270;&#39057;&#21644;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#20154;&#24037;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598; -- VideoXum&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#21033;&#29992;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#26469;&#23545;&#40784;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35270;&#39057;&#21644;&#25991;&#26412;&#25688;&#35201;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video summarization aims to distill the most important information from a source video to produce either an abridged clip or a textual narrative. Traditionally, different methods have been proposed depending on whether the output is a video or text, thus ignoring the correlation between the two semantically related tasks of visual summarization and textual summarization. We propose a new joint video and text summarization task. The goal is to generate both a shortened video clip along with the corresponding textual summary from a long video, collectively referred to as a cross-modal summary. The generated shortened video clip and text narratives should be semantically well aligned. To this end, we first build a large-scale human-annotated dataset -- VideoXum (X refers to different modalities). The dataset is reannotated based on ActivityNet. After we filter out the videos that do not meet the length requirements, 14,001 long videos remain in our new dataset. Each video in our reannotat
&lt;/p&gt;</description></item><item><title>RETVec&#26159;&#19968;&#31181;&#39640;&#25928;&#12289;&#24377;&#24615;&#21644;&#22810;&#35821;&#35328;&#30340;&#25991;&#26412;&#21521;&#37327;&#21270;&#22120;&#65292;&#36890;&#36807;&#37319;&#29992;&#26032;&#39062;&#30340;&#23383;&#31526;&#32534;&#30721;&#21644;&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#25340;&#20889;&#38169;&#35823;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26356;&#22909;&#36866;&#24212;&#24615;&#12290;&#19982;&#20854;&#20182;&#21521;&#37327;&#21270;&#22120;&#21644;&#35789;&#23884;&#20837;&#27169;&#22411;&#30456;&#27604;&#65292;RETVec&#22312;&#21508;&#31181;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#21644;&#26174;&#33879;&#30340;&#24377;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.09207</link><description>&lt;p&gt;
RETVec&#65306;&#24377;&#24615;&#21644;&#39640;&#25928;&#30340;&#25991;&#26412;&#21521;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
RETVec: Resilient and Efficient Text Vectorizer. (arXiv:2302.09207v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09207
&lt;/p&gt;
&lt;p&gt;
RETVec&#26159;&#19968;&#31181;&#39640;&#25928;&#12289;&#24377;&#24615;&#21644;&#22810;&#35821;&#35328;&#30340;&#25991;&#26412;&#21521;&#37327;&#21270;&#22120;&#65292;&#36890;&#36807;&#37319;&#29992;&#26032;&#39062;&#30340;&#23383;&#31526;&#32534;&#30721;&#21644;&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#25340;&#20889;&#38169;&#35823;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26356;&#22909;&#36866;&#24212;&#24615;&#12290;&#19982;&#20854;&#20182;&#21521;&#37327;&#21270;&#22120;&#21644;&#35789;&#23884;&#20837;&#27169;&#22411;&#30456;&#27604;&#65292;RETVec&#22312;&#21508;&#31181;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#21644;&#26174;&#33879;&#30340;&#24377;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;RETVec&#65292;&#19968;&#31181;&#19987;&#20026;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25991;&#26412;&#22788;&#29702;&#32780;&#35774;&#35745;&#30340;&#39640;&#25928;&#12289;&#24377;&#24615;&#21644;&#22810;&#35821;&#35328;&#30340;&#25991;&#26412;&#21521;&#37327;&#21270;&#22120;&#12290;RETVec&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23383;&#31526;&#32534;&#30721;&#21644;&#21487;&#36873;&#30340;&#23567;&#22411;&#23884;&#20837;&#27169;&#22411;&#65292;&#23558;&#35789;&#35821;&#23884;&#20837;&#21040;256&#32500;&#21521;&#37327;&#31354;&#38388;&#20013;&#12290;RETVec&#30340;&#23884;&#20837;&#27169;&#22411;&#20351;&#29992;&#23545;&#27604;&#24230;&#23398;&#20064;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#38024;&#23545;&#25340;&#20889;&#38169;&#35823;&#21644;&#23383;&#31526;&#32423;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;RETVec&#22312;&#27969;&#34892;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;&#36825;&#20123;&#27604;&#36739;&#34920;&#26126;&#65292;RETVec&#33021;&#22815;&#20135;&#29983;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#23545;&#25340;&#20889;&#38169;&#35823;&#21644;&#23545;&#25239;&#24615;&#25991;&#26412;&#25915;&#20987;&#20855;&#26377;&#26174;&#33879;&#30340;&#24377;&#24615;&#12290;RETVec&#22312;Apache 2&#35768;&#21487;&#19979;&#21487;&#22312;https://github.com/google-research/retvec&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes RETVec, an efficient, resilient, and multilingual text vectorizer designed for neural-based text processing. RETVec combines a novel character encoding with an optional small embedding model to embed words into a 256-dimensional vector space. The RETVec embedding model is pre-trained using pair-wise metric learning to be robust against typos and character-level adversarial attacks. In this paper, we evaluate and compare RETVec to state-of-the-art vectorizers and word embeddings on popular model architectures and datasets. These comparisons demonstrate that RETVec leads to competitive, multilingual models that are significantly more resilient to typos and adversarial text attacks. RETVec is available under the Apache 2 license at https://github.com/google-research/retvec.
&lt;/p&gt;</description></item></channel></rss>