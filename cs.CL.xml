<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35821;&#35328;&#26657;&#27491;&#27969;&#26159;&#19968;&#31181;&#22522;&#20110;&#26631;&#20934;&#27010;&#29575;&#27969;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#24120;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#22312;&#28304;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#20256;&#36755;&#65292;&#25552;&#20379;&#20102;&#32479;&#19968;&#21644;&#26377;&#25928;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#39046;&#22495;&#36716;&#31227;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.16995</link><description>&lt;p&gt;
&#35821;&#35328;&#26657;&#27491;&#27969;&#65306;&#36890;&#36807;&#27010;&#29575;&#27969;&#25512;&#21160;&#25193;&#25955;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Language Rectified Flow: Advancing Diffusion Language Generation with Probabilistic Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16995
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26657;&#27491;&#27969;&#26159;&#19968;&#31181;&#22522;&#20110;&#26631;&#20934;&#27010;&#29575;&#27969;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#24120;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#22312;&#28304;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#20256;&#36755;&#65292;&#25552;&#20379;&#20102;&#32479;&#19968;&#21644;&#26377;&#25928;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#39046;&#22495;&#36716;&#31227;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#22522;&#30784;&#19978;&#25511;&#21046;&#21477;&#23376;&#23646;&#24615;&#65288;&#20363;&#22914;&#24773;&#24863;&#65289;&#21644;&#32467;&#26500;&#65288;&#20363;&#22914;&#21477;&#27861;&#32467;&#26500;&#65289;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#19968;&#20010;&#25512;&#21160;&#39640;&#36136;&#37327;&#26679;&#26412;&#29983;&#25104;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#36845;&#20195;&#21435;&#22122;&#25968;&#21315;&#27493;&#12290;&#23613;&#31649;&#26377;&#30410;&#65292;&#20294;&#20174;&#22122;&#22768;&#24320;&#22987;&#30340;&#22797;&#26434;&#24615;&#21644;&#23398;&#20064;&#27493;&#39588;&#38480;&#21046;&#20102;&#20854;&#22312;&#35768;&#22810;NLP&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Language Rectified Flow&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#26631;&#20934;&#27010;&#29575;&#27969;&#27169;&#22411;&#30340;&#37325;&#26500;&#12290;&#35821;&#35328;&#26657;&#27491;&#27969;&#23398;&#20064;&#65288;&#31070;&#32463;&#65289;&#24120;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#22312;&#28304;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#20256;&#36755;&#65292;&#20026;&#29983;&#25104;&#24314;&#27169;&#21644;&#22495;&#36716;&#31227;&#25552;&#20379;&#20102;&#32479;&#19968;&#21644;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20174;&#28304;&#20998;&#24067;&#24320;&#22987;&#65292;&#25105;&#20204;&#30340;&#35821;&#35328;&#26657;&#27491;&#27969;&#20135;&#29983;&#24555;&#36895;&#20223;&#30495;&#21644;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16995v1 Announce Type: cross  Abstract: Recent works have demonstrated success in controlling sentence attributes ($e.g.$, sentiment) and structure ($e.g.$, syntactic structure) based on the diffusion language model. A key component that drives theimpressive performance for generating high-quality samples from noise is iteratively denoise for thousands of steps. While beneficial, the complexity of starting from the noise and the learning steps has limited its implementation to many NLP real-world applications. This paper proposes Language Rectified Flow ({\ours}). Our method is based on the reformulation of the standard probabilistic flow models. Language rectified flow learns (neural) ordinary differential equation models to transport between the source distribution and the target distribution, hence providing a unified and effective solution to generative modeling and domain transfer. From the source distribution, our language rectified flow yields fast simulation and effe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26126;&#30830;&#24314;&#27169;&#19981;&#21516;&#30340;&#24863;&#20852;&#36259;&#26041;&#38754;&#26469;&#25913;&#36827;&#27010;&#24565;&#23884;&#20837;&#65292;&#20351;&#20854;&#33021;&#22815;&#25429;&#25417;&#26356;&#24191;&#27867;&#30340;&#24120;&#35782;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16984</link><description>&lt;p&gt;
&#29992;&#22810;&#26041;&#38754;&#27010;&#24565;&#23884;&#20837;&#27169;&#22411;&#24314;&#27169;&#24120;&#35782;&#20849;&#24615;
&lt;/p&gt;
&lt;p&gt;
Modelling Commonsense Commonalities with Multi-Facet Concept Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26126;&#30830;&#24314;&#27169;&#19981;&#21516;&#30340;&#24863;&#20852;&#36259;&#26041;&#38754;&#26469;&#25913;&#36827;&#27010;&#24565;&#23884;&#20837;&#65292;&#20351;&#20854;&#33021;&#22815;&#25429;&#25417;&#26356;&#24191;&#27867;&#30340;&#24120;&#35782;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#23884;&#20837;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#19988;&#39640;&#25928;&#30340;&#26426;&#21046;&#65292;&#23558;&#24120;&#35782;&#30693;&#35782;&#27880;&#20837;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#26631;&#20934;&#23884;&#20837;&#20027;&#35201;&#21453;&#26144;&#22522;&#26412;&#20998;&#31867;&#31867;&#21035;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#23398;&#20064;&#27010;&#24565;&#23884;&#20837;&#26102;&#26126;&#30830;&#24314;&#27169;&#24863;&#20852;&#36259;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24471;&#21040;&#20102;&#33021;&#22815;&#25429;&#25417;&#26356;&#24191;&#27867;&#24120;&#35782;&#23646;&#24615;&#30340;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16984v1 Announce Type: new  Abstract: Concept embeddings offer a practical and efficient mechanism for injecting commonsense knowledge into downstream tasks. Their core purpose is often not to predict the commonsense properties of concepts themselves, but rather to identify commonalities, i.e.\ sets of concepts which share some property of interest. Such commonalities are the basis for inductive generalisation, hence high-quality concept embeddings can make learning easier and more robust. Unfortunately, standard embeddings primarily reflect basic taxonomic categories, making them unsuitable for finding commonalities that refer to more specific aspects (e.g.\ the colour of objects or the materials they are made of). In this paper, we address this limitation by explicitly modelling the different facets of interest when learning concept embeddings. We show that this leads to embeddings which capture a more diverse range of commonsense properties, and consistently improves resu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20154;&#31867;&#12289;GPT-3.5&#21644;GPT-4&#22312;&#22823;&#23398;&#32423;&#32534;&#31243;&#35838;&#31243;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#23398;&#29983;&#30340;&#24179;&#22343;&#24471;&#20998;&#26126;&#26174;&#39640;&#20110;AI&#25552;&#20132;&#65292;&#21516;&#26102;&#25552;&#31034;&#24037;&#31243;&#26174;&#33879;&#25552;&#39640;&#20102;GPT-4&#21644;GPT-3.5&#30340;&#24471;&#20998;&#12290;</title><link>https://arxiv.org/abs/2403.16977</link><description>&lt;p&gt;
&#20154;&#31867;&#12289;GPT-3.5&#21644;GPT-4&#22312;&#22823;&#23398;&#32423;&#32534;&#31243;&#35838;&#31243;&#20013;&#34920;&#29616;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A comparison of Human, GPT-3.5, and GPT-4 Performance in a University-Level Coding Course
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20154;&#31867;&#12289;GPT-3.5&#21644;GPT-4&#22312;&#22823;&#23398;&#32423;&#32534;&#31243;&#35838;&#31243;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#23398;&#29983;&#30340;&#24179;&#22343;&#24471;&#20998;&#26126;&#26174;&#39640;&#20110;AI&#25552;&#20132;&#65292;&#21516;&#26102;&#25552;&#31034;&#24037;&#31243;&#26174;&#33879;&#25552;&#39640;&#20102;GPT-4&#21644;GPT-3.5&#30340;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;Python&#35821;&#35328;&#22312;&#22823;&#23398;&#32423;&#29289;&#29702;&#32534;&#31243;&#20316;&#19994;&#20013;&#65292;ChatGPT&#21464;&#31181;GPT-3.5&#21644;GPT-4&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#26159;&#21542;&#36827;&#34892;&#25552;&#31034;&#24037;&#31243;&#65292;&#19982;&#20165;&#23398;&#29983;&#20316;&#21697;&#21644;&#21516;&#26102;&#21253;&#21547;&#23398;&#29983;&#21644;GPT-4&#36129;&#29486;&#30340;&#28151;&#21512;&#31867;&#21035;&#30340;&#23545;&#27604;&#12290;&#36890;&#36807;50&#20221;&#23398;&#29983;&#25552;&#20132;&#21644;50&#20221;&#19981;&#21516;&#31867;&#21035;&#30340;AI&#29983;&#25104;&#30340;&#25552;&#20132;&#30340;&#35780;&#27604;&#65292;&#30001;&#19977;&#21517;&#29420;&#31435;&#35780;&#20998;&#32773;&#36827;&#34892;&#30450;&#30446;&#35780;&#20998;&#65292;&#25105;&#20204;&#24635;&#35745;&#20102;300&#20010;&#25968;&#25454;&#28857;&#12290;&#23398;&#29983;&#24179;&#22343;&#24471;&#20998;&#20026;91.9% (&#26631;&#20934;&#35823;:0.4)&#65292;&#36229;&#36807;&#20102;&#24471;&#20998;&#20026;81.1% (&#26631;&#20934;&#35823;:0.8)&#30340;&#34920;&#29616;&#26368;&#22909;&#30340;AI&#25552;&#20132;&#31867;&#21035;GPT-4+&#25552;&#31034;&#24037;&#31243;&#65292;&#36825;&#26159;&#19968;&#20010;&#26174;&#33879;&#24046;&#24322; (p = $2.482 \times 10^{-10}$)&#12290;&#25552;&#31034;&#24037;&#31243;&#26174;&#33879;&#25552;&#39640;&#20102;GPT-4 (p = $1.661 \times 10^{-4}$)&#21644;GPT-3.5 (p = $4.967 \times 10^{-9}$)&#30340;&#24471;&#20998;&#12290;&#27492;&#22806;&#65292;&#30450;&#30446;&#35780;&#20998;&#32773;&#34987;&#35201;&#27714;&#20351;&#29992;&#22235;&#28857;&#26446;&#20811;&#29305;&#37327;&#34920;&#29468;&#27979;&#25552;&#20132;&#20316;&#21697;&#30340;&#20316;&#32773;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16977v1 Announce Type: new  Abstract: This study evaluates the performance of ChatGPT variants, GPT-3.5 and GPT-4, both with and without prompt engineering, against solely student work and a mixed category containing both student and GPT-4 contributions in university-level physics coding assignments using the Python language. Comparing 50 student submissions to 50 AI-generated submissions across different categories, and marked blindly by three independent markers, we amassed $n = 300$ data points. Students averaged 91.9% (SE:0.4), surpassing the highest performing AI submission category, GPT-4 with prompt engineering, which scored 81.1% (SE:0.8) - a statistically significant difference (p = $2.482 \times 10^{-10}$). Prompt engineering significantly improved scores for both GPT-4 (p = $1.661 \times 10^{-4}$) and GPT-3.5 (p = $4.967 \times 10^{-9}$). Additionally, the blinded markers were tasked with guessing the authorship of the submissions on a four-point Likert scale from
&lt;/p&gt;</description></item><item><title>VoiceCraft&#26159;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#22635;&#20805;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35821;&#38899;&#32534;&#36753;&#21644;&#38646;-shot&#25991;&#26412;&#21040;&#35821;&#38899;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16973</link><description>&lt;p&gt;
VoiceCraft&#65306;&#37326;&#22806;&#38646;-shot&#35821;&#38899;&#32534;&#36753;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16973
&lt;/p&gt;
&lt;p&gt;
VoiceCraft&#26159;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#22635;&#20805;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35821;&#38899;&#32534;&#36753;&#21644;&#38646;-shot&#25991;&#26412;&#21040;&#35821;&#38899;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;VoiceCraft&#65292;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#22635;&#20805;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#22768;&#20070;&#12289;&#20114;&#32852;&#32593;&#35270;&#39057;&#21644;&#25773;&#23458;&#19978;&#35821;&#38899;&#32534;&#36753;&#21644;&#38646;-shot&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#26041;&#38754;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;VoiceCraft&#37319;&#29992;Transformer&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26631;&#35760;&#37325;&#25490;&#36807;&#31243;&#65292;&#32467;&#21512;&#20102;&#22240;&#26524;&#25513;&#30721;&#21644;&#24310;&#36831;&#22534;&#21472;&#65292;&#20197;&#23454;&#29616;&#22312;&#29616;&#26377;&#24207;&#21015;&#20869;&#30340;&#29983;&#25104;&#12290;&#22312;&#35821;&#38899;&#32534;&#36753;&#20219;&#21153;&#19978;&#65292;VoiceCraft&#29983;&#25104;&#30340;&#32534;&#36753;&#35821;&#38899;&#22312;&#33258;&#28982;&#24230;&#26041;&#38754;&#20960;&#20046;&#19982;&#26410;&#32534;&#36753;&#30340;&#24405;&#38899;&#38590;&#20197;&#21306;&#20998;&#65292;&#32463;&#20154;&#31867;&#35780;&#20272;&#65307;&#23545;&#20110;&#38646;-shot TTS&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#21253;&#25324;VALLE&#21644;&#27969;&#34892;&#30340;&#21830;&#19994;&#27169;&#22411;XTTS-v2&#12290;&#20851;&#38190;&#30340;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20855;&#26377;&#22810;&#26679;&#21475;&#38899;&#12289;&#35821;&#38899;&#39118;&#26684;&#12289;&#24405;&#21046;&#26465;&#20214;&#12289;&#32972;&#26223;&#22122;&#38899;&#21644;&#38899;&#20048;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#30495;&#23454;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#34920;&#29616;&#22987;&#32456;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16973v1 Announce Type: cross  Abstract: We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft employs a Transformer decoder architecture and introduces a token rearrangement procedure that combines causal masking and delayed stacking to enable generation within an existing sequence. On speech editing tasks, VoiceCraft produces edited speech that is nearly indistinguishable from unedited recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS, our model outperforms prior SotA models including VALLE and the popular commercial model XTTS-v2. Crucially, the models are evaluated on challenging and realistic datasets, that consist of diverse accents, speaking styles, recording conditions, and background noise and music, and our model performs consistently well compared to other models a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#25805;&#20316;&#31995;&#32479;&#20013;&#30340;LLM&#20195;&#29702;&#25805;&#20316;&#31995;&#32479;&#65292;&#26088;&#22312;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#12289;&#20419;&#36827;&#20195;&#29702;&#38388;&#19978;&#19979;&#25991;&#20999;&#25442;&#12289;&#23454;&#29616;&#24182;&#21457;&#25191;&#34892;&#20197;&#21450;&#20026;&#20195;&#29702;&#25552;&#20379;&#24037;&#20855;&#26381;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.16971</link><description>&lt;p&gt;
LLM Agent Operating System
&lt;/p&gt;
&lt;p&gt;
LLM Agent Operating System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16971
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#25805;&#20316;&#31995;&#32479;&#20013;&#30340;LLM&#20195;&#29702;&#25805;&#20316;&#31995;&#32479;&#65292;&#26088;&#22312;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#12289;&#20419;&#36827;&#20195;&#29702;&#38388;&#19978;&#19979;&#25991;&#20999;&#25442;&#12289;&#23454;&#29616;&#24182;&#21457;&#25191;&#34892;&#20197;&#21450;&#20026;&#20195;&#29702;&#25552;&#20379;&#24037;&#20855;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16971v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26234;&#33021;&#20195;&#29702;&#23384;&#22312;&#35832;&#22810;&#25361;&#25112;&#65292;&#20250;&#25439;&#23475;&#23427;&#20204;&#30340;&#25928;&#29575;&#21644;&#21151;&#25928;&#12290;&#20854;&#20013;&#21253;&#25324;&#20195;&#29702;&#35831;&#27714;&#22312;LLM&#19978;&#30340;&#27425;&#20248;&#35843;&#24230;&#21644;&#36164;&#28304;&#20998;&#37197;&#12289;&#22312;&#20195;&#29702;&#21644;LLM&#20043;&#38388;&#20132;&#20114;&#26102;&#20445;&#25345;&#19978;&#19979;&#25991;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#23558;&#20855;&#26377;&#19981;&#21516;&#33021;&#21147;&#21644;&#19987;&#19994;&#21270;&#30340;&#24322;&#26500;&#20195;&#29702;&#38598;&#25104;&#22312;&#19968;&#36215;&#30340;&#22797;&#26434;&#24615;&#12290;&#20195;&#29702;&#25968;&#37327;&#21644;&#22797;&#26434;&#24615;&#30340;&#24555;&#36895;&#22686;&#21152;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#36164;&#28304;&#29942;&#39048;&#21644;&#27425;&#20248;&#36164;&#28304;&#21033;&#29992;&#12290;&#21463;&#21040;&#36825;&#20123;&#25361;&#25112;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;AIOS&#65292;&#19968;&#31181;LLM&#20195;&#29702;&#25805;&#20316;&#31995;&#32479;&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#25805;&#20316;&#31995;&#32479;&#65288;OS&#65289;&#20013;&#12290;&#20855;&#20307;&#22320;&#65292;AIOS&#26088;&#22312;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#65292;&#20419;&#36827;&#20195;&#29702;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20999;&#25442;&#65292;&#23454;&#29616;&#20195;&#29702;&#30340;&#24182;&#21457;&#25191;&#34892;&#65292;&#20026;&#20195;&#29702;&#25552;&#20379;&#24037;&#20855;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16971v1 Announce Type: cross  Abstract: The integration and deployment of large language model (LLM)-based intelligent agents have been fraught with challenges that compromise their efficiency and efficacy. Among these issues are sub-optimal scheduling and resource allocation of agent requests over the LLM, the difficulties in maintaining context during interactions between agent and LLM, and the complexities inherent in integrating heterogeneous agents with different capabilities and specializations. The rapid increase of agent quantity and complexity further exacerbates these issues, often leading to bottlenecks and sub-optimal utilization of resources. Inspired by these challenges, this paper presents AIOS, an LLM agent operating system, which embeds large language model into operating systems (OS). Specifically, AIOS is designed to optimize resource allocation, facilitate context switch across agents, enable concurrent execution of agents, provide tool service for agents
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#29992;&#20110;&#19978;&#19979;&#25991;&#35789;&#24418;&#36824;&#21407;&#30340;&#26368;&#30701;&#32534;&#36753;&#33050;&#26412;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35789;&#24418;&#36824;&#21407;&#35270;&#20026;&#20196;&#29260;&#20998;&#31867;&#20219;&#21153;&#65292;&#20165;&#20462;&#25913;&#38656;&#35201;&#23398;&#20064;&#30340;SES&#26631;&#31614;&#65292;&#23458;&#35266;&#24471;&#20986;&#26368;&#20339;&#30340;&#35789;&#24418;&#36824;&#21407;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16968</link><description>&lt;p&gt;
&#35780;&#20272;&#29992;&#20110;&#19978;&#19979;&#25991;&#35789;&#24418;&#36824;&#21407;&#30340;&#26368;&#30701;&#32534;&#36753;&#33050;&#26412;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Evaluating Shortest Edit Script Methods for Contextual Lemmatization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#29992;&#20110;&#19978;&#19979;&#25991;&#35789;&#24418;&#36824;&#21407;&#30340;&#26368;&#30701;&#32534;&#36753;&#33050;&#26412;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35789;&#24418;&#36824;&#21407;&#35270;&#20026;&#20196;&#29260;&#20998;&#31867;&#20219;&#21153;&#65292;&#20165;&#20462;&#25913;&#38656;&#35201;&#23398;&#20064;&#30340;SES&#26631;&#31614;&#65292;&#23458;&#35266;&#24471;&#20986;&#26368;&#20339;&#30340;&#35789;&#24418;&#36824;&#21407;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#19978;&#19979;&#25991;&#35789;&#24418;&#36824;&#21407;&#22120;&#36890;&#24120;&#20381;&#36182;&#20110;&#33258;&#21160;&#35825;&#23548;&#30340;&#26368;&#30701;&#32534;&#36753;&#33050;&#26412;&#65288;SES&#65289;&#65292;&#21363;&#36716;&#25442;&#21333;&#35789;&#24418;&#24335;&#20026;&#20854;&#35789;&#20803;&#25152;&#38656;&#30340;&#32534;&#36753;&#25805;&#20316;&#27425;&#25968;&#12290;&#20107;&#23454;&#19978;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#35745;&#31639;SES&#26041;&#27861;&#20316;&#20026;&#20960;&#31181;&#24403;&#21069;&#21487;&#29992;&#30340;&#26368;&#20808;&#36827;&#30340;&#19978;&#19979;&#25991;&#35789;&#24418;&#36824;&#21407;&#22120;&#26550;&#26500;&#30340;&#19968;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#26410;&#32771;&#23519;SES&#22312;&#26368;&#32456;&#35789;&#24418;&#36824;&#21407;&#24615;&#33021;&#20013;&#30340;&#30452;&#25509;&#24433;&#21709;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#35789;&#24418;&#36824;&#21407;&#35270;&#20026;&#19968;&#20010;&#20196;&#29260;&#20998;&#31867;&#20219;&#21153;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#27169;&#22411;&#20165;&#25509;&#25910;&#19978;&#19979;&#25991;&#20013;&#30340;&#35789;-&#26631;&#31614;&#23545;&#20316;&#20026;&#36755;&#20837;&#65292;&#20854;&#20013;&#26631;&#31614;&#23545;&#24212;&#20110;&#20808;&#21069;&#35825;&#23548;&#30340;SES&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#20165;&#20462;&#25913;&#27169;&#22411;&#38656;&#35201;&#23398;&#20064;&#30340;SES&#26631;&#31614;&#65292;&#25105;&#20204;&#21487;&#20197;&#23458;&#35266;&#22320;&#24471;&#20986;&#21738;&#31181;SES&#34920;&#31034;&#20135;&#29983;&#26368;&#20339;&#30340;&#35789;&#24418;&#36824;&#21407;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#20351;&#29992;&#19981;&#21516;&#24418;&#24577;&#30340;&#19971;&#31181;&#35821;&#35328;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16968v1 Announce Type: new  Abstract: Modern contextual lemmatizers often rely on automatically induced Shortest Edit Scripts (SES), namely, the number of edit operations to transform a word form into its lemma. In fact, different methods of computing SES have been proposed as an integral component in the architecture of several state-of-the-art contextual lemmatizers currently available. However, previous work has not investigated the direct impact of SES in the final lemmatization performance. In this paper we address this issue by focusing on lemmatization as a token classification task where the only input that the model receives is the word-label pairs in context, where the labels correspond to previously induced SES. Thus, by modifying in our lemmatization system only the SES labels that the model needs to learn, we may then objectively conclude which SES representation produces the best lemmatization results. We experiment with seven languages of different morphologic
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#20102;&#25968;&#25454;&#28151;&#21512;&#35268;&#24459;&#65292;&#21487;&#20197;&#37327;&#21270;&#22320;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#19982;&#25968;&#25454;&#28151;&#21512;&#27604;&#20363;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36890;&#36807;&#25311;&#21512;&#20989;&#25968;&#24418;&#24335;&#26469;&#24341;&#23548;&#29702;&#24819;&#30340;&#25968;&#25454;&#28151;&#21512;&#36873;&#25321;&#65292;&#20174;&#32780;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#28151;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.16952</link><description>&lt;p&gt;
&#25968;&#25454;&#28151;&#21512;&#35268;&#24459;&#65306;&#36890;&#36807;&#39044;&#27979;&#35821;&#35328;&#24314;&#27169;&#24615;&#33021;&#26469;&#20248;&#21270;&#25968;&#25454;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16952
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#20102;&#25968;&#25454;&#28151;&#21512;&#35268;&#24459;&#65292;&#21487;&#20197;&#37327;&#21270;&#22320;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#19982;&#25968;&#25454;&#28151;&#21512;&#27604;&#20363;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36890;&#36807;&#25311;&#21512;&#20989;&#25968;&#24418;&#24335;&#26469;&#24341;&#23548;&#29702;&#24819;&#30340;&#25968;&#25454;&#28151;&#21512;&#36873;&#25321;&#65292;&#20174;&#32780;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#28151;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#21253;&#25324;&#22810;&#20010;&#39046;&#22495;&#65288;&#20363;&#22914;&#32593;&#32476;&#25991;&#26412;&#12289;&#23398;&#26415;&#35770;&#25991;&#12289;&#20195;&#30721;&#65289;&#65292;&#20854;&#28151;&#21512;&#27604;&#20363;&#23545;&#32467;&#26524;&#27169;&#22411;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#25110;&#23450;&#24615;&#31574;&#30053;&#26469;&#35843;&#25972;&#27604;&#20363;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#27169;&#22411;&#24615;&#33021;&#19982;&#28151;&#21512;&#27604;&#20363;&#20043;&#38388;&#30340;&#20989;&#25968;&#24418;&#24335;&#30340;&#23450;&#37327;&#21487;&#39044;&#27979;&#24615;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#25968;&#25454;&#28151;&#21512;&#35268;&#24459;&#12290;&#22312;&#26679;&#26412;&#28151;&#21512;&#19978;&#25311;&#21512;&#36825;&#31181;&#20989;&#25968;&#25581;&#31034;&#20102;&#26410;&#35265;&#28151;&#21512;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#20174;&#32780;&#24341;&#23548;&#36873;&#25321;&#29702;&#24819;&#30340;&#25968;&#25454;&#28151;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35757;&#32451;&#27493;&#39588;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#25105;&#20204;&#30340;&#25968;&#25454;&#28151;&#21512;&#35268;&#24459;&#30340;&#32553;&#25918;&#35268;&#24459;&#30340;&#23884;&#22871;&#20351;&#29992;&#65292;&#20197;&#20351;&#24471;&#20165;&#36890;&#36807;&#23567;&#35268;&#27169;&#35757;&#32451;&#23601;&#33021;&#22815;&#39044;&#27979;&#22312;&#21508;&#31181;&#28151;&#21512;&#25968;&#25454;&#19979;&#35757;&#32451;&#30340;&#22823;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#20248;&#21270;&#20102;&#35757;&#32451;&#28151;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16952v1 Announce Type: cross  Abstract: Pretraining data of large language models composes multiple domains (e.g., web texts, academic papers, codes), whose mixture proportions crucially impact the competence of outcome models. While existing endeavors rely on heuristics or qualitative strategies to tune the proportions, we discover the quantitative predictability of model performance regarding the mixture proportions in function forms, which we refer to as the data mixing laws. Fitting such functions on sample mixtures unveils model performance on unseen mixtures before actual runs, thus guiding the selection of an ideal data mixture. Furthermore, we propose nested use of the scaling laws of training steps, model sizes, and our data mixing law to enable predicting the performance of large models trained on massive data under various mixtures with only small-scale training. Moreover, experimental results verify that our method effectively optimizes the training mixture of a 
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#25104;&#23545;&#20559;&#22909;&#25628;&#32034;&#26041;&#27861;PAIRS&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;LLMs&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#30452;&#25509;&#25171;&#20998;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16950</link><description>&lt;p&gt;
&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#19968;&#33268;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#25104;&#23545;&#20559;&#22909;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16950
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#25104;&#23545;&#20559;&#22909;&#25628;&#32034;&#26041;&#27861;PAIRS&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;LLMs&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#30452;&#25509;&#25171;&#20998;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#33258;&#21160;&#35780;&#20272;&#22120;&#22312;&#35780;&#20272;&#29983;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#35780;&#20272;&#20013;&#20173;&#23384;&#22312;&#20559;&#35265;&#65292;&#24120;&#24120;&#38590;&#20197;&#29983;&#25104;&#19982;&#20154;&#31867;&#35780;&#20272;&#19968;&#33268;&#30340;&#36830;&#36143;&#35780;&#20272;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;LLM&#35780;&#20272;&#22120;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#65292;&#25581;&#31034;&#29616;&#26377;&#26088;&#22312;&#20943;&#36731;&#20559;&#35265;&#30340;&#26657;&#20934;&#26041;&#27861;&#19981;&#36275;&#20197;&#26377;&#25928;&#23558;LLM&#35780;&#20272;&#22120;&#23545;&#40784;&#12290;&#21463;&#21040;RLHF&#20013;&#23545;&#20559;&#22909;&#25968;&#25454;&#30340;&#20351;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#35780;&#20272;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#25490;&#24207;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;Pairwise-preference Search&#65288;PAIRS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20197;LLMs&#36827;&#34892;&#25104;&#23545;&#27604;&#36739;&#24182;&#26377;&#25928;&#23545;&#20505;&#36873;&#25991;&#26412;&#36827;&#34892;&#25490;&#24207;&#30340;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#25628;&#32034;&#26041;&#27861;&#12290;PAIRS&#22312;&#20195;&#34920;&#24615;&#35780;&#20272;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#26174;&#31034;&#20986;&#27604;&#30452;&#25509;&#25171;&#20998;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16950v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated promising capabilities as automatic evaluators in assessing the quality of generated natural language. However, LLMs still exhibit biases in evaluation and often struggle to generate coherent evaluations that align with human assessments. In this work, we first conduct a systematic study of the misalignment between LLM evaluators and human judgement, revealing that existing calibration methods aimed at mitigating biases are insufficient for effectively aligning LLM evaluators. Inspired by the use of preference data in RLHF, we formulate the evaluation as a ranking problem and introduce Pairwise-preference Search (PAIRS), an uncertainty-guided search method that employs LLMs to conduct pairwise comparisons and efficiently ranks candidate texts. PAIRS achieves state-of-the-art performance on representative evaluation tasks and demonstrates significant improvements over direct scoring. Furthe
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;SPACE-IDEAS&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#26816;&#27979;&#19982;&#31354;&#38388;&#21019;&#26032;&#30456;&#20851;&#30340;&#26174;&#33879;&#20449;&#24687;&#65292;&#21253;&#25324;&#22810;&#31181;&#25991;&#26412;&#39118;&#26684;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#35757;&#32451;&#26356;&#22909;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>https://arxiv.org/abs/2403.16941</link><description>&lt;p&gt;
SPACE-IDEAS&#65306;&#29992;&#20110;&#31354;&#38388;&#21019;&#26032;&#20013;&#26174;&#33879;&#20449;&#24687;&#26816;&#27979;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SPACE-IDEAS: A Dataset for Salient Information Detection in Space Innovation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16941
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;SPACE-IDEAS&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#26816;&#27979;&#19982;&#31354;&#38388;&#21019;&#26032;&#30456;&#20851;&#30340;&#26174;&#33879;&#20449;&#24687;&#65292;&#21253;&#25324;&#22810;&#31181;&#25991;&#26412;&#39118;&#26684;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#35757;&#32451;&#26356;&#22909;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16941v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#25991;&#26412;&#20013;&#26816;&#27979;&#26174;&#33879;&#37096;&#20998;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20197;&#32531;&#35299;&#20449;&#24687;&#36807;&#36733;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21487;&#29992;&#20110;&#27492;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#20027;&#35201;&#28304;&#33258;&#23398;&#26415;&#20986;&#29256;&#29289;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SPACE-IDEAS&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20174;&#19982;&#31354;&#38388;&#39046;&#22495;&#30456;&#20851;&#30340;&#21019;&#26032;&#29702;&#24565;&#20013;&#26816;&#27979;&#26174;&#33879;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#12290;SPACE-IDEAS&#20013;&#30340;&#25991;&#26412;&#39118;&#26684;&#22810;&#26679;&#65292;&#21253;&#25324;&#38750;&#27491;&#24335;&#12289;&#25216;&#26415;&#12289;&#23398;&#26415;&#21644;&#21830;&#19994;&#20889;&#20316;&#39118;&#26684;&#12290;&#38500;&#20102;&#25163;&#21160;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#27880;&#37322;&#30340;&#25193;&#23637;&#29256;&#26412;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19981;&#21516;&#30340;&#21477;&#23376;&#21644;&#24207;&#21015;&#21477;&#20998;&#31867;&#22120;&#65292;&#24182;&#34920;&#26126;&#33258;&#21160;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#35757;&#32451;&#26356;&#22909;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16941v1 Announce Type: cross  Abstract: Detecting salient parts in text using natural language processing has been widely used to mitigate the effects of information overflow. Nevertheless, most of the datasets available for this task are derived mainly from academic publications. We introduce SPACE-IDEAS, a dataset for salient information detection from innovation ideas related to the Space domain. The text in SPACE-IDEAS varies greatly and includes informal, technical, academic and business-oriented writing styles. In addition to a manually annotated dataset we release an extended version that is annotated using a large generative language model. We train different sentence and sequential sentence classifiers, and show that the automatically annotated dataset can be leveraged using multitask learning to train better classifiers.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#22312;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#20013;&#26174;&#33879;&#25913;&#21892;&#20102;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16915</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31895;&#35843;&#20248;&#30340;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#22312;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#20013;&#26174;&#33879;&#25913;&#21892;&#20102;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20013;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM-based IR&#65289;&#36827;&#34892;&#24494;&#35843;&#38656;&#35201;&#23398;&#20064;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#65292;&#38500;&#20102;&#19979;&#28216;&#20219;&#21153;&#29305;&#23450;&#30340;&#23398;&#20064;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;&#36890;&#36807;&#22312;&#31895;&#35843;&#20248;&#23398;&#20064;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#65292;&#25105;&#20204;&#26088;&#22312;&#20943;&#23569;&#24494;&#35843;&#30340;&#36127;&#25285;&#65292;&#25552;&#39640;&#19979;&#28216;IR&#20219;&#21153;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#31895;&#35843;&#20248;&#30340;&#26597;&#35810;-&#25991;&#26723;&#23545;&#39044;&#27979;&#65288;QDPP&#65289;&#65292;&#20854;&#39044;&#27979;&#26597;&#35810;-&#25991;&#26723;&#23545;&#30340;&#36866;&#24403;&#24615;&#12290;&#35780;&#20272;&#23454;&#39564;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#22235;&#20010;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#25968;&#25454;&#38598;&#20013;&#30340;MRR&#21644;/&#25110;nDCG@5&#12290;&#27492;&#22806;&#65292;&#26597;&#35810;&#39044;&#27979;&#20219;&#21153;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31895;&#35843;&#20248;&#20419;&#36827;&#20102;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16915v1 Announce Type: cross  Abstract: Fine-tuning in information retrieval systems using pre-trained language models (PLM-based IR) requires learning query representations and query-document relations, in addition to downstream task-specific learning. This study introduces coarse-tuning as an intermediate learning stage that bridges pre-training and fine-tuning. By learning query representations and query-document relations in coarse-tuning, we aim to reduce the load of fine-tuning and improve the learning effect of downstream IR tasks. We propose Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the appropriateness of query-document pairs. Evaluation experiments show that the proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc document retrieval datasets. Furthermore, the results of the query prediction task suggested that coarse-tuning facilitated learning of query representation and query-document relations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38024;&#23545;&#26032;&#24847;&#22270;&#21457;&#29616;&#38382;&#39064;&#30340;Robust and Adaptive Prototypical learning (RAP)&#26694;&#26550;&#65292;&#36890;&#36807;&#20581;&#22766;&#30340;&#21407;&#22411;&#21560;&#24341;&#23398;&#20064;(RPAL)&#21644;&#33258;&#36866;&#24212;&#30340;&#21407;&#22411;&#20998;&#25955;&#23398;&#20064;(APDL)&#26041;&#27861;&#65292;&#22312;&#24050;&#30693;&#21644;&#26032;&#30340;&#24847;&#22270;&#31867;&#21035;&#20043;&#38388;&#23454;&#29616;&#20840;&#23616;&#26126;&#26174;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;</title><link>https://arxiv.org/abs/2403.16913</link><description>&lt;p&gt;
&#20855;&#26377;&#21560;&#24341;&#21644;&#20998;&#25955;&#21407;&#22411;&#30340;&#26032;&#24847;&#22270;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
New Intent Discovery with Attracting and Dispersing Prototype
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16913
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38024;&#23545;&#26032;&#24847;&#22270;&#21457;&#29616;&#38382;&#39064;&#30340;Robust and Adaptive Prototypical learning (RAP)&#26694;&#26550;&#65292;&#36890;&#36807;&#20581;&#22766;&#30340;&#21407;&#22411;&#21560;&#24341;&#23398;&#20064;(RPAL)&#21644;&#33258;&#36866;&#24212;&#30340;&#21407;&#22411;&#20998;&#25955;&#23398;&#20064;(APDL)&#26041;&#27861;&#65292;&#22312;&#24050;&#30693;&#21644;&#26032;&#30340;&#24847;&#22270;&#31867;&#21035;&#20043;&#38388;&#23454;&#29616;&#20840;&#23616;&#26126;&#26174;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#24847;&#22270;&#21457;&#29616;&#26088;&#22312;&#21033;&#29992;&#26377;&#38480;&#26631;&#35760;&#21644;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#35782;&#21035;&#24050;&#30693;&#21644;&#25512;&#26029;&#26032;&#30340;&#24847;&#22270;&#31867;&#21035;&#12290;&#26412;&#25991;&#23558;&#20219;&#21153;&#35270;&#20026;&#29305;&#24449;&#32858;&#31867;&#38382;&#39064;&#65292;&#24182;&#22686;&#24378;&#20102;&#23454;&#20363;&#34920;&#31034;&#12290;&#20026;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#21451;&#22909;&#32858;&#31867;&#34920;&#31034;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#38024;&#23545;NID&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#20840;&#23616;&#26126;&#26174;&#20915;&#31574;&#36793;&#30028;&#30340;&#20581;&#22766;&#19988;&#33258;&#36866;&#24212;&#30340;&#21407;&#22411;&#23398;&#20064;&#65288;RAP&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#24050;&#30693;&#21644;&#26032;&#30340;&#24847;&#22270;&#31867;&#21035;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#21407;&#22411;&#21560;&#24341;&#23398;&#20064;&#65288;RPAL&#65289;&#26041;&#27861;&#65292;&#20197;&#20419;&#20351;&#23454;&#20363;&#21521;&#20854;&#23545;&#24212;&#30340;&#21407;&#22411;&#38752;&#25314;&#65292;&#23454;&#29616;&#26356;&#22823;&#30340;&#31751;&#20869;&#32039;&#20945;&#24615;&#12290;&#20026;&#23454;&#29616;&#26356;&#22823;&#30340;&#31751;&#38388;&#20998;&#31163;&#24230;&#65292;&#21478;&#19968;&#20010;&#33258;&#36866;&#24212;&#30340;&#21407;&#22411;&#20998;&#25955;&#23398;&#20064;&#65288;APDL&#65289;&#26041;&#27861;&#34987;&#35774;&#35745;&#20026;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16913v1 Announce Type: new  Abstract: New Intent Discovery (NID) aims to recognize known and infer new intent categories with the help of limited labeled and large-scale unlabeled data. The task is addressed as a feature-clustering problem and recent studies augment instance representation. However, existing methods fail to capture cluster-friendly representations, since they show less capability to effectively control and coordinate within-cluster and between-cluster distances. Tailored to the NID problem, we propose a Robust and Adaptive Prototypical learning (RAP) framework for globally distinct decision boundaries for both known and new intent categories. Specifically, a robust prototypical attracting learning (RPAL) method is designed to compel instances to gravitate toward their corresponding prototype, achieving greater within-cluster compactness. To attain larger between-cluster separation, another adaptive prototypical dispersing learning (APDL) method is devised to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;GPT-3&#20998;&#26512;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#22312;&#21512;&#25104;&#25968;&#25454;&#20013;&#30340;&#21387;&#21147;&#22240;&#32032;&#34920;&#31034;&#65292;&#21019;&#24314;&#20986;&#21253;&#21547;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#32676;&#20307;&#21387;&#21147;&#22240;&#32032;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20026;&#20197;&#21518;&#20351;&#29992;LLMs&#36827;&#34892;&#25968;&#25454;&#29983;&#25104;&#30740;&#31350;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.16909</link><description>&lt;p&gt;
&#26397;&#21521;&#31639;&#27861;&#24544;&#23454;&#24615;&#65306;&#21512;&#25104;&#25968;&#25454;&#19982;&#20154;&#31867;&#29983;&#25104;&#25968;&#25454;&#20013;&#36328;&#20154;&#21475;&#32479;&#35745;&#30340;&#24515;&#29702;&#20581;&#24247;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Towards Algorithmic Fidelity: Mental Health Representation across Demographics in Synthetic vs. Human-generated Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;GPT-3&#20998;&#26512;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#22312;&#21512;&#25104;&#25968;&#25454;&#20013;&#30340;&#21387;&#21147;&#22240;&#32032;&#34920;&#31034;&#65292;&#21019;&#24314;&#20986;&#21253;&#21547;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#32676;&#20307;&#21387;&#21147;&#22240;&#32032;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20026;&#20197;&#21518;&#20351;&#29992;LLMs&#36827;&#34892;&#25968;&#25454;&#29983;&#25104;&#30740;&#31350;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26377;&#28508;&#21147;&#24433;&#21709;&#24212;&#29992;&#21644;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#22312;&#23558;&#36825;&#20123;&#25968;&#25454;&#29992;&#20110;&#35832;&#22914;&#24515;&#29702;&#20581;&#24247;&#36825;&#26679;&#30340;&#25935;&#24863;&#20219;&#21153;&#20043;&#21069;&#65292;&#25105;&#20204;&#38656;&#35201;&#20102;&#35299;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#22312;&#20854;&#20013;&#30340;&#34920;&#31034;&#26041;&#24335;&#12290;&#22312;&#25105;&#20204;&#30340;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25506;&#32034;GPT-3&#30340;&#28508;&#21147;&#26469;&#20998;&#26512;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#25506;&#35752;&#20854;&#23545;&#19981;&#21516;&#31181;&#26063;&#21644;&#24615;&#21035;&#32452;&#21512;&#25152;&#20135;&#29983;&#30340;&#21508;&#31181;&#21387;&#21147;&#22240;&#32032;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#21442;&#32771;&#65292;&#36825;&#20123;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#30740;&#31350;&#20351;&#29992;LLMs&#36827;&#34892;&#25968;&#25454;&#29983;&#25104;&#12290;&#25105;&#20204;&#21033;&#29992;GPT-3&#24320;&#21457;&#20102;HEADROOM&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#25324;3120&#31687;&#20851;&#20110;&#23548;&#33268;&#25233;&#37057;&#30151;&#21387;&#21147;&#22240;&#32032;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25511;&#21046;&#31181;&#26063;&#12289;&#24615;&#21035;&#21644;&#26102;&#38388;&#33539;&#22260;&#65288;COVID-19&#20043;&#21069;&#21644;&#20043;&#21518;&#65289;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36827;&#34892;&#35821;&#20041;&#21644;&#35789;&#27719;&#20998;&#26512;&#26469;&#65288;1&#65289;&#30830;&#23450;&#27599;&#20010;&#20154;&#21475;&#32479;&#35745;&#32676;&#20307;&#30340;&#20027;&#35201;&#21387;&#21147;&#22240;&#32032;&#65307;&#65288;2&#65289;&#23558;&#25105;&#20204;&#30340;&#21512;&#25104;&#25968;&#25454;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29983;&#25104;&#26597;&#35810;&#20197;&#21457;&#23637;&#20381;&#36182;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16909v1 Announce Type: new  Abstract: Synthetic data generation has the potential to impact applications and domains with scarce data. However, before such data is used for sensitive tasks such as mental health, we need an understanding of how different demographics are represented in it. In our paper, we analyze the potential of producing synthetic data using GPT-3 by exploring the various stressors it attributes to different race and gender combinations, to provide insight for future researchers looking into using LLMs for data generation. Using GPT-3, we develop HEADROOM, a synthetic dataset of 3,120 posts about depression-triggering stressors, by controlling for race, gender, and time frame (before and after COVID-19). Using this dataset, we conduct semantic and lexical analyses to (1) identify the predominant stressors for each demographic group; and (2) compare our synthetic data to a human-generated dataset. We present the procedures to generate queries to develop dep
&lt;/p&gt;</description></item><item><title>&#23558;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#25972;&#21512;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#65292;&#20026;&#25511;&#21046;&#29702;&#35770;&#23478;&#21644;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#24314;&#27169;&#21160;&#24577;&#31995;&#32479;&#30340;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.16899</link><description>&lt;p&gt;
&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#22522;&#30784;&#27169;&#22411;&#65306;&#19968;&#20010;&#25511;&#21046;&#29702;&#35770;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
State Space Models as Foundation Models: A Control Theoretic Overview
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16899
&lt;/p&gt;
&lt;p&gt;
&#23558;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#25972;&#21512;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#65292;&#20026;&#25511;&#21046;&#29702;&#35770;&#23478;&#21644;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#24314;&#27169;&#21160;&#24577;&#31995;&#32479;&#30340;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23558;&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#25972;&#21512;&#21040;&#22522;&#30784;&#27169;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;Mamba&#30340;&#25104;&#21151;&#23637;&#31034;&#20102;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;Transformer&#26550;&#26500;&#22312;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;GPT-4&#65292;&#26088;&#22312;&#23558;&#24207;&#21015;&#25968;&#25454;&#32534;&#30721;&#20026;&#28508;&#22312;&#31354;&#38388;&#65292;&#20197;&#23398;&#20064;&#25968;&#25454;&#30340;&#21387;&#32553;&#34920;&#31034;&#12290;&#25511;&#21046;&#29702;&#35770;&#23478;&#20351;&#29992;SSM&#26469;&#26377;&#25928;&#22320;&#24314;&#27169;&#21160;&#24577;&#31995;&#32479;&#36861;&#27714;&#30456;&#21516;&#30340;&#30446;&#26631;&#12290;&#22240;&#27492;&#65292;SSM&#21487;&#20197;&#33258;&#28982;&#22320;&#19982;&#28145;&#24230;&#24207;&#21015;&#24314;&#27169;&#30456;&#36830;&#25509;&#65292;&#25552;&#20379;&#20102;&#22312;&#30456;&#24212;&#30740;&#31350;&#39046;&#22495;&#20043;&#38388;&#21019;&#36896;&#21327;&#21516;&#20316;&#29992;&#30340;&#26426;&#20250;&#12290;&#26412;&#25991;&#26088;&#22312;&#21521;&#25511;&#21046;&#29702;&#35770;&#23478;&#31616;&#35201;&#20171;&#32461;&#22522;&#20110;SSM&#30340;&#26550;&#26500;&#65292;&#24182;&#24635;&#32467;&#26368;&#26032;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;&#23427;&#31995;&#32479;&#22238;&#39038;&#20102;&#26368;&#25104;&#21151;&#30340;SSM&#25552;&#35758;&#65292;&#24182;&#31361;&#20986;&#20102;&#23427;&#20204;&#30340;m
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16899v1 Announce Type: cross  Abstract: In recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20998;&#26512;&#20102;&#21475;&#35821;&#35821;&#35328;&#33258;&#30417;&#30563;&#27169;&#22411;&#23545;&#22768;&#35843;&#30340;&#32534;&#30721;&#33021;&#21147;&#65292;&#20351;&#29992;&#26222;&#36890;&#35805;&#21644;&#36234;&#21335;&#35821;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;SLMs&#22312;&#35757;&#32451;&#20110;&#38750;&#38899;&#35843;&#35821;&#35328;&#25968;&#25454;&#26102;&#20063;&#20445;&#25345;&#30528;&#26174;&#33879;&#30340;&#35789;&#27719;&#38899;&#35843;&#32534;&#30721;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.16865</link><description>&lt;p&gt;
&#21475;&#35821;&#35821;&#35328;&#33258;&#30417;&#30563;&#27169;&#22411;&#20013;&#30340;&#35789;&#27719;&#38899;&#35843;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Encoding of lexical tone in self-supervised models of spoken language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20998;&#26512;&#20102;&#21475;&#35821;&#35821;&#35328;&#33258;&#30417;&#30563;&#27169;&#22411;&#23545;&#22768;&#35843;&#30340;&#32534;&#30721;&#33021;&#21147;&#65292;&#20351;&#29992;&#26222;&#36890;&#35805;&#21644;&#36234;&#21335;&#35821;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;SLMs&#22312;&#35757;&#32451;&#20110;&#38750;&#38899;&#35843;&#35821;&#35328;&#25968;&#25454;&#26102;&#20063;&#20445;&#25345;&#30528;&#26174;&#33879;&#30340;&#35789;&#27719;&#38899;&#35843;&#32534;&#30721;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24615;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#30417;&#30563;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#20174;&#22768;&#23398;&#12289;&#35821;&#38899;&#12289;&#38899;&#38901;&#12289;&#21477;&#27861;&#21644;&#35821;&#20041;&#23618;&#38754;&#21040;&#35828;&#35805;&#32773;&#29305;&#24449;&#20013;&#32534;&#30721;&#20102;&#20154;&#31867;&#35821;&#38899;&#20013;&#30340;&#21508;&#31181;&#29305;&#24449;&#12290;&#20197;&#21069;&#20851;&#20110;&#38899;&#38901;&#23398;&#34920;&#31034;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#35832;&#22914;&#38899;&#32032;&#31561;&#37096;&#20998;&#29305;&#24449;&#19978;&#65307;SLMs&#20013;&#23545;&#38899;&#38901;&#38899;&#31995;&#65288;&#22914;&#22768;&#35843;&#21644;&#37325;&#38899;&#27169;&#24335;&#65289;&#30340;&#32534;&#30721;&#23578;&#26410;&#34987;&#20805;&#20998;&#29702;&#35299;&#12290;&#22768;&#35843;&#26159;&#19968;&#31181;&#23384;&#22312;&#20110;&#19990;&#30028;&#19978;&#21322;&#25968;&#20197;&#19978;&#35821;&#35328;&#20013;&#30340;&#38899;&#31995;&#29305;&#24449;&#12290;&#26412;&#25991;&#26088;&#22312;&#20998;&#26512;SLMs&#30340;&#22768;&#35843;&#32534;&#30721;&#33021;&#21147;&#65292;&#20197;&#26222;&#36890;&#35805;&#21644;&#36234;&#21335;&#35821;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;SLMs&#22312;&#35757;&#32451;&#20110;&#38750;&#38899;&#35843;&#35821;&#35328;&#25968;&#25454;&#26102;&#20063;&#26126;&#26174;&#32534;&#30721;&#20102;&#35789;&#27719;&#38899;&#35843;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;SLMs&#22312;&#22768;&#35843;&#21644;&#36741;&#38899;&#24863;&#30693;&#30740;&#31350;&#20013;&#30340;&#34920;&#29616;&#19982;&#26412;&#26063;&#21644;&#38750;&#26412;&#26063;&#30340;&#20154;&#31867;&#21442;&#19982;&#32773;&#31867;&#20284;&#65292;&#20294;&#23427;&#20204;&#19981;&#36981;&#24490;&#30456;&#21516;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16865v1 Announce Type: new  Abstract: Interpretability research has shown that self-supervised Spoken Language Models (SLMs) encode a wide variety of features in human speech from the acoustic, phonetic, phonological, syntactic and semantic levels, to speaker characteristics. The bulk of prior research on representations of phonology has focused on segmental features such as phonemes; the encoding of suprasegmental phonology (such as tone and stress patterns) in SLMs is not yet well understood. Tone is a suprasegmental feature that is present in more than half of the world's languages. This paper aims to analyze the tone encoding capabilities of SLMs, using Mandarin and Vietnamese as case studies. We show that SLMs encode lexical tone to a significant degree even when they are trained on data from non-tonal languages. We further find that SLMs behave similarly to native and non-native human participants in tone and consonant perception studies, but they do not follow the sam
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19987;&#23478;&#20195;&#24065;&#36335;&#30001;&#23558;&#22810;&#20010;&#19987;&#23478;LLM&#21327;&#21516;&#20316;&#20026;&#36890;&#29992;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#20010;&#19987;&#23478;LLMs&#30340;&#26080;&#32541;&#38598;&#25104;&#65292;&#25903;&#25345;&#38544;&#24335;&#19987;&#19994;&#30693;&#35782;&#30340;&#23398;&#20064;&#21644;&#21160;&#24577;&#25193;&#23637;&#26032;&#30340;&#19987;&#23478;LLMs&#65292;&#21516;&#26102;&#26356;&#22909;&#22320;&#38544;&#34255;&#21327;&#20316;&#32454;&#33410;&#65292;&#23637;&#29616;&#20986;&#27604;&#29616;&#26377;&#22810;LLM&#21327;&#20316;&#33539;&#24335;&#26356;&#22909;&#30340;&#25928;&#26524;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16854</link><description>&lt;p&gt;
&#19968;&#20010;&#19987;&#23478;&#20215;&#20540;&#19968;&#20010;&#20195;&#24065;&#65306;&#36890;&#36807;&#19987;&#23478;&#20195;&#24065;&#36335;&#30001;&#23558;&#22810;&#20010;&#19987;&#23478;LLM&#21327;&#21516;&#20316;&#20026;&#36890;&#29992;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Expert is Worth One Token: Synergizing Multiple Expert LLMs as Generalist via Expert Token Routing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16854
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19987;&#23478;&#20195;&#24065;&#36335;&#30001;&#23558;&#22810;&#20010;&#19987;&#23478;LLM&#21327;&#21516;&#20316;&#20026;&#36890;&#29992;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#20010;&#19987;&#23478;LLMs&#30340;&#26080;&#32541;&#38598;&#25104;&#65292;&#25903;&#25345;&#38544;&#24335;&#19987;&#19994;&#30693;&#35782;&#30340;&#23398;&#20064;&#21644;&#21160;&#24577;&#25193;&#23637;&#26032;&#30340;&#19987;&#23478;LLMs&#65292;&#21516;&#26102;&#26356;&#22909;&#22320;&#38544;&#34255;&#21327;&#20316;&#32454;&#33410;&#65292;&#23637;&#29616;&#20986;&#27604;&#29616;&#26377;&#22810;LLM&#21327;&#20316;&#33539;&#24335;&#26356;&#22909;&#30340;&#25928;&#26524;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19987;&#23478;&#20195;&#24065;&#36335;&#30001;&#65288;Expert-Token-Routing&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#36890;&#29992;&#22411;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#20010;&#19987;&#23478;LLM&#30340;&#26080;&#32541;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#19987;&#23478;LLMs&#34920;&#31034;&#20026;&#20803;LLM&#35789;&#27719;&#20013;&#30340;&#29305;&#27530;&#19987;&#23478;&#20195;&#24065;&#12290;&#20803;LLM&#21487;&#20197;&#36335;&#30001;&#21040;&#19987;&#23478;LLM&#65292;&#23601;&#20687;&#29983;&#25104;&#26032;&#20195;&#24065;&#19968;&#26679;&#12290;&#19987;&#23478;&#20195;&#24065;&#36335;&#30001;&#19981;&#20165;&#21487;&#20197;&#20174;&#29616;&#26377;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#19987;&#23478;LLMs&#30340;&#38544;&#24335;&#19987;&#19994;&#30693;&#35782;&#65292;&#36824;&#21487;&#20197;&#20197;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#24335;&#21160;&#24577;&#25193;&#23637;&#26032;&#30340;&#19987;&#23478;LLMs&#12290;&#23427;&#36824;&#21487;&#20197;&#38544;&#34255;&#29992;&#25143;&#35270;&#35282;&#20013;&#30340;&#35814;&#32454;&#21327;&#20316;&#36807;&#31243;&#65292;&#20419;&#36827;&#20132;&#20114;&#23601;&#20687;&#26159;&#19968;&#20010;&#21333;&#19968;&#30340;LLM&#19968;&#26679;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#28085;&#30422;&#20845;&#20010;&#19981;&#21516;&#19987;&#23478;&#39046;&#22495;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#32988;&#36807;&#20102;&#21508;&#31181;&#29616;&#26377;&#30340;&#22810;LLM&#21327;&#20316;&#33539;&#24335;&#65292;&#23637;&#29616;&#20102;&#36890;&#36807;&#21327;&#21516;&#22810;&#20010;&#19987;&#23478;LLM&#26469;&#26500;&#24314;&#36890;&#29992;&#22411;LLM&#31995;&#32479;&#30340;&#25928;&#26524;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16854v1 Announce Type: cross  Abstract: We present Expert-Token-Routing, a unified generalist framework that facilitates seamless integration of multiple expert LLMs. Our framework represents expert LLMs as special expert tokens within the vocabulary of a meta LLM. The meta LLM can route to an expert LLM like generating new tokens. Expert-Token-Routing not only supports learning the implicit expertise of expert LLMs from existing instruction dataset but also allows for dynamic extension of new expert LLMs in a plug-and-play manner. It also conceals the detailed collaboration process from the user's perspective, facilitating interaction as though it were a singular LLM. Our framework outperforms various existing multi-LLM collaboration paradigms across benchmarks that incorporate six diverse expert domains, demonstrating effectiveness and robustness in building generalist LLM system via synergizing multiple expert LLMs.
&lt;/p&gt;</description></item><item><title>&#20808;&#20363;&#26159;&#20419;&#36827;&#27861;&#24459;NLP&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#27861;&#24459;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#20351;&#29992;&#30340;&#20808;&#20363;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#30340;&#33021;&#21147;&#19981;&#38169;&#65292;&#20294;&#20854;&#20351;&#29992;&#20808;&#20363;&#30340;&#26041;&#24335;&#19982;&#20154;&#31867;&#27861;&#23448;&#19981;&#21516;&#12290;</title><link>https://arxiv.org/abs/2403.16852</link><description>&lt;p&gt;
&#22312;&#27861;&#24459;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#20013;&#36808;&#21521;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Explainability in Legal Outcome Prediction Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16852
&lt;/p&gt;
&lt;p&gt;
&#20808;&#20363;&#26159;&#20419;&#36827;&#27861;&#24459;NLP&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#27861;&#24459;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#20351;&#29992;&#30340;&#20808;&#20363;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#30340;&#33021;&#21147;&#19981;&#38169;&#65292;&#20294;&#20854;&#20351;&#29992;&#20808;&#20363;&#30340;&#26041;&#24335;&#19982;&#20154;&#31867;&#27861;&#23448;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#27861;&#24459;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411; - &#27861;&#24459;NLP&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998; - &#19981;&#33021;&#35299;&#37322;&#20854;&#25512;&#29702;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24212;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#20154;&#31867;&#27861;&#24459;&#20027;&#20307;&#38656;&#35201;&#33021;&#22815;&#29702;&#35299;&#23427;&#20204;&#30340;&#20915;&#31574;&#12290;&#22312;&#26222;&#36890;&#27861;&#26696;&#20363;&#20013;&#65292;&#27861;&#24459;&#20174;&#19994;&#32773;&#36890;&#36807;&#21442;&#32771;&#34987;&#31216;&#20026;&#20808;&#20363;&#30340;&#36807;&#21435;&#26696;&#20363;&#27861;&#24459;&#25512;&#29702;&#21040;&#26696;&#20214;&#32467;&#26524;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20808;&#20363;&#22240;&#27492;&#25104;&#20026;&#20419;&#36827;&#27861;&#24459;NLP&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#27861;&#24459;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#20351;&#29992;&#30340;&#20808;&#20363;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21046;&#23450;&#27861;&#24459;&#20808;&#20363;&#30340;&#20998;&#31867;&#27861;&#65292;&#25105;&#20204;&#33021;&#22815;&#27604;&#36739;&#20154;&#31867;&#27861;&#23448;&#21644;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20182;&#20204;&#20381;&#36182;&#30340;&#19981;&#21516;&#31867;&#22411;&#20808;&#20363;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#27169;&#22411;&#23398;&#20250;&#20102;&#21512;&#29702;&#22320;&#39044;&#27979;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#20351;&#29992;&#30340;&#20808;&#20363;&#26041;&#24335;&#19981;&#21516;&#20110;&#20154;&#31867;&#27861;&#23448;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16852v1 Announce Type: cross  Abstract: Current legal outcome prediction models - a staple of legal NLP - do not explain their reasoning. However, to employ these models in the real world, human legal actors need to be able to understand their decisions. In the case of common law, legal practitioners reason towards the outcome of a case by referring to past case law, known as precedent. We contend that precedent is, therefore, a natural way of facilitating explainability for legal NLP models. In this paper, we contribute a novel method for identifying the precedent employed by legal outcome prediction models. Furthermore, by developing a taxonomy of legal precedent, we are able to compare human judges and our models with respect to the different types of precedent they rely on. We find that while the models learn to predict outcomes reasonably well, their use of precedent is unlike that of human judges.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#26159;&#21542;&#33021;&#22815;&#22522;&#20110;Twitter&#25552;&#21450;&#26469;&#39044;&#27979;&#25991;&#31456;&#30340;&#25764;&#22238;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#39044;&#27979;&#26410;&#26469;&#34987;&#25764;&#22238;&#30340;&#26377;&#38382;&#39064;&#25991;&#31456;&#26041;&#38754;&#26159;&#20855;&#26377;&#19968;&#23450;&#28508;&#21147;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.16851</link><description>&lt;p&gt;
ChatGPT&#26159;&#21542;&#33021;&#22815;&#22522;&#20110;Twitter&#25552;&#21450;&#26469;&#39044;&#27979;&#25991;&#31456;&#30340;&#25764;&#22238;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT predict article retraction based on Twitter mentions?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#26159;&#21542;&#33021;&#22815;&#22522;&#20110;Twitter&#25552;&#21450;&#26469;&#39044;&#27979;&#25991;&#31456;&#30340;&#25764;&#22238;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#39044;&#27979;&#26410;&#26469;&#34987;&#25764;&#22238;&#30340;&#26377;&#38382;&#39064;&#25991;&#31456;&#26041;&#38754;&#26159;&#20855;&#26377;&#19968;&#23450;&#28508;&#21147;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#26377;&#38382;&#39064;&#30340;&#30740;&#31350;&#25991;&#31456;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26681;&#25454;&#34987;&#25764;&#22238;&#25991;&#31456;&#22312;Twitter&#19978;&#30340;&#25552;&#21450;&#26159;&#21542;&#33021;&#22815;&#22312;&#25991;&#31456;&#34987;&#25764;&#22238;&#21069;&#21457;&#20986;&#20449;&#21495;&#65292;&#20174;&#32780;&#22312;&#39044;&#27979;&#26410;&#26469;&#34987;&#25764;&#22238;&#30340;&#26377;&#38382;&#39064;&#25991;&#31456;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;&#20998;&#26512;&#20102;&#21253;&#25324;3,505&#31687;&#24050;&#25764;&#22238;&#25991;&#31456;&#21450;&#20854;&#30456;&#20851;Twitter&#25552;&#21450;&#22312;&#20869;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#20351;&#29992;&#31895;&#31961;&#31934;&#30830;&#21305;&#37197;&#26041;&#27861;&#33719;&#21462;&#30340;&#20855;&#26377;&#31867;&#20284;&#29305;&#24449;&#30340;3,505&#31687;&#26410;&#25764;&#22238;&#25991;&#31456;&#12290;&#36890;&#36807;&#22235;&#31181;&#39044;&#27979;&#26041;&#27861;&#35780;&#20272;&#20102;Twitter&#25552;&#21450;&#22312;&#39044;&#27979;&#25991;&#31456;&#25764;&#22238;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#25163;&#21160;&#26631;&#27880;&#12289;&#20851;&#38190;&#35789;&#35782;&#21035;&#12289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;ChatGPT&#12290;&#25163;&#21160;&#26631;&#27880;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#30340;&#30830;&#26377;&#34987;&#25764;&#22238;&#30340;&#25991;&#31456;&#65292;&#20854;Twitter&#25552;&#21450;&#21253;&#21547;&#22312;&#25764;&#22238;&#21069;&#21457;&#20986;&#20449;&#21495;&#30340;&#21487;&#35782;&#21035;&#35777;&#25454;&#65292;&#23613;&#31649;&#23427;&#20204;&#21482;&#21344;&#25152;&#26377;&#34987;&#25764;&#22238;&#25991;&#31456;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16851v1 Announce Type: cross  Abstract: Detecting problematic research articles timely is a vital task. This study explores whether Twitter mentions of retracted articles can signal potential problems with the articles prior to retraction, thereby playing a role in predicting future retraction of problematic articles. A dataset comprising 3,505 retracted articles and their associated Twitter mentions is analyzed, alongside 3,505 non-retracted articles with similar characteristics obtained using the Coarsened Exact Matching method. The effectiveness of Twitter mentions in predicting article retraction is evaluated by four prediction methods, including manual labelling, keyword identification, machine learning models, and ChatGPT. Manual labelling results indicate that there are indeed retracted articles with their Twitter mentions containing recognizable evidence signaling problems before retraction, although they represent only a limited share of all retracted articles with 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#19978;&#19979;&#25991;&#21270;&#30701;&#35821;&#26816;&#32034;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#35299;&#20915;&#22810;&#20041;&#24615;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#36328;&#35821;&#35328;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16820</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#19978;&#19979;&#25991;&#21270;&#30701;&#35821;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual Contextualized Phrase Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16820
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#19978;&#19979;&#25991;&#21270;&#30701;&#35821;&#26816;&#32034;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#35299;&#20915;&#22810;&#20041;&#24615;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#36328;&#35821;&#35328;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#35821;&#32423;&#23494;&#38598;&#26816;&#32034;&#36890;&#36807;&#21033;&#29992;&#30701;&#35821;&#25552;&#20379;&#30340;&#32454;&#31890;&#24230;&#20449;&#24687;&#65292;&#22312;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#35768;&#22810;&#21560;&#24341;&#20154;&#30340;&#29305;&#24449;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23494;&#38598;&#26816;&#32034;&#20219;&#21153;&#24418;&#24335;&#65292;&#21363;&#36328;&#35821;&#35328;&#19978;&#19979;&#25991;&#21270;&#30701;&#35821;&#26816;&#32034;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22686;&#24378;&#35299;&#20915;&#22810;&#20041;&#24615;&#30340;&#36328;&#35821;&#35328;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#26159;&#23454;&#29616;&#25105;&#20204;&#30446;&#26631;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#20174;&#24179;&#34892;&#21477;&#23376;&#20013;&#33258;&#21160;&#35825;&#23548;&#30340;&#21333;&#35789;&#23545;&#40784;&#20449;&#24687;&#25552;&#21462;&#36328;&#35821;&#35328;&#30701;&#35821;&#23545;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#25105;&#20204;&#30340;&#36328;&#35821;&#35328;&#19978;&#19979;&#25991;&#21270;&#30701;&#35821;&#26816;&#32034;&#22120;&#65288;CCPR&#65289;&#65292;&#35813;&#23545;&#27604;&#23398;&#20064;&#40723;&#21169;&#20855;&#26377;&#30456;&#20284;&#19978;&#19979;&#25991;&#21644;&#35821;&#20041;&#30340;&#30701;&#35821;&#30340;&#38544;&#34255;&#34920;&#31034;&#32039;&#23494;&#23545;&#40784;&#12290;&#25105;&#20204;&#23545;&#36328;&#35821;&#35328;&#30701;&#35821;&#26816;&#32034;&#20219;&#21153;&#21644;&#19968;&#20010;&#19979;&#28216;&#20219;&#21153;&#65292;&#21363;&#26426;&#22120;&#32763;&#35793;&#65292;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16820v1 Announce Type: new  Abstract: Phrase-level dense retrieval has shown many appealing characteristics in downstream NLP tasks by leveraging the fine-grained information that phrases offer. In our work, we propose a new task formulation of dense retrieval, cross-lingual contextualized phrase retrieval, which aims to augment cross-lingual applications by addressing polysemy using context information. However, the lack of specific training data and models are the primary challenges to achieve our goal. As a result, we extract pairs of cross-lingual phrases using word alignment information automatically induced from parallel sentences. Subsequently, we train our Cross-lingual Contextualized Phrase Retriever (CCPR) using contrastive learning, which encourages the hidden representations of phrases with similar contexts and semantics to align closely. Comprehensive experiments on both the cross-lingual phrase retrieval task and a downstream task, i.e, machine translation, dem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;TEI2GO&#27169;&#22411;&#65292;&#23427;&#22312;&#25903;&#25345;&#20845;&#31181;&#35821;&#35328;&#30340;&#21516;&#26102;&#65292;&#22312;&#20854;&#20013;&#22235;&#31181;&#35821;&#35328;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#20855;&#26377;&#26174;&#33879;&#25913;&#21892;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.16804</link><description>&lt;p&gt;
TEI2GO: &#19968;&#31181;&#29992;&#20110;&#24555;&#36895;&#35782;&#21035;&#26102;&#38388;&#34920;&#36798;&#30340;&#22810;&#35821;&#35328;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TEI2GO: A Multilingual Approach for Fast Temporal Expression Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;TEI2GO&#27169;&#22411;&#65292;&#23427;&#22312;&#25903;&#25345;&#20845;&#31181;&#35821;&#35328;&#30340;&#21516;&#26102;&#65292;&#22312;&#20854;&#20013;&#22235;&#31181;&#35821;&#35328;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#20855;&#26377;&#26174;&#33879;&#25913;&#21892;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#34920;&#36798;&#30340;&#35782;&#21035;&#23545;&#20110;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#20889;&#20316;&#30340;&#25991;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#20687;HeidelTime&#36825;&#26679;&#39640;&#25928;&#30340;&#31995;&#32479;&#23384;&#22312;&#65292;&#20294;&#23427;&#20204;&#30340;&#26377;&#38480;&#36816;&#34892;&#24615;&#33021;&#38459;&#30861;&#20102;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#21644;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#37319;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TEI2GO&#27169;&#22411;&#65292;&#23427;&#19982;HeidelTime&#30340;&#26377;&#25928;&#24615;&#30456;&#21305;&#37197;&#65292;&#20294;&#36816;&#34892;&#26102;&#38388;&#26174;&#33879;&#25913;&#21892;&#65292;&#22312;&#25903;&#25345;&#20845;&#31181;&#35821;&#35328;&#30340;&#21516;&#26102;&#65292;&#22312;&#20854;&#20013;&#22235;&#31181;&#35821;&#35328;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#35757;&#32451;TEI2GO&#27169;&#22411;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20154;&#24037;&#27880;&#37322;&#30340;&#21442;&#32771;&#35821;&#26009;&#24211;&#30340;&#32452;&#21512;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;Professor HeidelTime&#8221;&#30340;&#32508;&#21512;&#24369;&#26631;&#35760;&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#29992;HeidelTime&#26631;&#35760;&#30340;&#26032;&#38395;&#25991;&#26412;&#12290;&#35813;&#35821;&#26009;&#24211;&#21253;&#21547;&#20102;&#19968;&#20849;138,069&#31687;&#25991;&#26723;(&#36328;&#20845;&#31181;&#35821;&#35328;)&#65292;&#20849;1,050,921&#20010;&#26102;&#38388;&#34920;&#36798;&#65292;&#26159;&#36804;&#20170;&#20026;&#27490;&#29992;&#20110;&#26102;&#38388;&#34920;&#36798;&#35782;&#21035;&#30340;&#26368;&#22823;&#24320;&#28304;&#26631;&#27880;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#25551;&#36848;&#27169;&#22411;&#30340;&#20135;&#29983;&#36807;&#31243;&#65292;&#25105;&#20204;&#26088;&#22312;&#40723;&#21169;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16804v1 Announce Type: new  Abstract: Temporal expression identification is crucial for understanding texts written in natural language. Although highly effective systems such as HeidelTime exist, their limited runtime performance hampers adoption in large-scale applications and production environments. In this paper, we introduce the TEI2GO models, matching HeidelTime's effectiveness but with significantly improved runtime, supporting six languages, and achieving state-of-the-art results in four of them. To train the TEI2GO models, we used a combination of manually annotated reference corpus and developed ``Professor HeidelTime'', a comprehensive weakly labeled corpus of news texts annotated with HeidelTime. This corpus comprises a total of $138,069$ documents (over six languages) with $1,050,921$ temporal expressions, the largest open-source annotated dataset for temporal expression identification to date. By describing how the models were produced, we aim to encourage the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ProCoder&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#35793;&#22120;&#21453;&#39304;&#24341;&#23548;&#65292;&#36845;&#20195;&#22320;&#25913;&#36827;&#39033;&#30446;&#32423;&#20195;&#30721;&#19978;&#19979;&#25991;&#65292;&#20197;&#33719;&#24471;&#31934;&#30830;&#30340;&#20195;&#30721;&#29983;&#25104;</title><link>https://arxiv.org/abs/2403.16792</link><description>&lt;p&gt;
&#36890;&#36807;&#32534;&#35793;&#22120;&#21453;&#39304;&#36845;&#20195;&#25913;&#36827;&#39033;&#30446;&#32423;&#20195;&#30721;&#19978;&#19979;&#25991;&#65292;&#20197;&#33719;&#24471;&#31934;&#30830;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ProCoder&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#35793;&#22120;&#21453;&#39304;&#24341;&#23548;&#65292;&#36845;&#20195;&#22320;&#25913;&#36827;&#39033;&#30446;&#32423;&#20195;&#30721;&#19978;&#19979;&#25991;&#65292;&#20197;&#33719;&#24471;&#31934;&#30830;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33258;&#21160;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23558;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#24212;&#29992;&#21040;&#29616;&#23454;&#39033;&#30446;&#20013;&#20250;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#29983;&#25104;&#30340;&#20195;&#30721;&#21487;&#33021;&#23384;&#22312;API&#20351;&#29992;&#12289;&#31867;&#12289;&#25968;&#25454;&#32467;&#26500;&#38169;&#35823;&#25110;&#32570;&#23569;&#39033;&#30446;&#29305;&#23450;&#20449;&#24687;&#12290;&#37492;&#20110;&#22823;&#37096;&#20998;&#39033;&#30446;&#29305;&#23450;&#19978;&#19979;&#25991;&#26080;&#27861;&#36866;&#24212;LLMs&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#24517;&#39035;&#25214;&#21040;&#35753;&#27169;&#22411;&#33021;&#22815;&#25506;&#32034;&#39033;&#30446;&#32423;&#20195;&#30721;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ProCoder&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#35793;&#22120;&#21453;&#39304;&#24341;&#23548;&#65292;&#36845;&#20195;&#22320;&#25913;&#36827;&#39033;&#30446;&#32423;&#20195;&#30721;&#19978;&#19979;&#25991;&#65292;&#20197;&#33719;&#24471;&#31934;&#30830;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ProCoder&#39318;&#20808;&#21033;&#29992;&#32534;&#35793;&#22120;&#25216;&#26415;&#35782;&#21035;&#29983;&#25104;&#30340;&#20195;&#30721;&#19982;&#39033;&#30446;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#20043;&#22788;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20174;&#20195;&#30721;&#24211;&#20013;&#25552;&#21462;&#30340;&#20449;&#24687;&#36845;&#20195;&#22320;&#23545;&#40784;&#21644;&#20462;&#22797;&#35782;&#21035;&#20986;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#23558;ProCoder&#19982;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;LLM&#38598;&#25104;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16792v1 Announce Type: new  Abstract: Large language models (LLMs) have shown remarkable progress in automated code generation. Yet, incorporating LLM-based code generation into real-life software projects poses challenges, as the generated code may contain errors in API usage, class, data structure, or missing project-specific information. As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context. To this end, this paper puts forward a novel approach, termed ProCoder, which iteratively refines the project-level code context for precise code generation, guided by the compiler feedback. In particular, ProCoder first leverages compiler techniques to identify a mismatch between the generated code and the project's context. It then iteratively aligns and fixes the identified errors using information extracted from the code repository. We integrate ProCoder with two representative L
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#32763;&#35793;&#20316;&#20026;&#25345;&#32493;&#35757;&#32451;&#30446;&#26631;&#20197;&#22686;&#24378;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#12289;&#36830;&#25509;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#21644;&#36328;&#35821;&#35328;&#24212;&#29992;&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#32467;&#26524;&#26174;&#31034;&#26426;&#22120;&#32763;&#35793;&#26410;&#33021;&#22686;&#24378;&#36328;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.16777</link><description>&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#26159;&#21542;&#33021;&#22815;&#36830;&#25509;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#21644;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Machine Translation Bridge Multilingual Pretraining and Cross-lingual Transfer Learning?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#32763;&#35793;&#20316;&#20026;&#25345;&#32493;&#35757;&#32451;&#30446;&#26631;&#20197;&#22686;&#24378;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#12289;&#36830;&#25509;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#21644;&#36328;&#35821;&#35328;&#24212;&#29992;&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#32467;&#26524;&#26174;&#31034;&#26426;&#22120;&#32763;&#35793;&#26410;&#33021;&#22686;&#24378;&#36328;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#23558;&#34920;&#31034;&#20174;&#19968;&#31181;&#35821;&#35328;&#36716;&#31227;&#21040;&#21478;&#19968;&#31181;&#35821;&#35328;&#23545;&#20110;&#36328;&#35821;&#35328;&#23398;&#20064;&#23588;&#20026;&#37325;&#35201;&#12290;&#21487;&#20197;&#26399;&#26395;&#26426;&#22120;&#32763;&#35793;&#30446;&#26631;&#38750;&#24120;&#36866;&#21512;&#20419;&#36827;&#36825;&#31181;&#33021;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#28041;&#21450;&#19981;&#21516;&#35821;&#35328;&#20013;&#35821;&#20041;&#31561;&#20215;&#21477;&#23376;&#30340;&#26174;&#24335;&#23545;&#40784;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#37319;&#29992;&#26426;&#22120;&#32763;&#35793;&#20316;&#20026;&#25345;&#32493;&#35757;&#32451;&#30446;&#26631;&#20197;&#22686;&#24378;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#12289;&#36830;&#25509;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#21644;&#36328;&#35821;&#35328;&#24212;&#29992;&#30340;&#28508;&#22312;&#30410;&#22788;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#35270;&#35282;&#26469;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65306;&#23545;&#29616;&#26377;&#27169;&#22411;&#24615;&#33021;&#30340;&#23450;&#37327;&#35780;&#20272;&#20197;&#21450;&#23427;&#20204;&#28508;&#22312;&#34920;&#31034;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#39044;&#26399;&#30456;&#21453;&#65292;&#20316;&#20026;&#25345;&#32493;&#35757;&#32451;&#30340;&#26426;&#22120;&#32763;&#35793;&#26410;&#33021;&#22686;&#24378;&#36328;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16777v1 Announce Type: new  Abstract: Multilingual pretraining and fine-tuning have remarkably succeeded in various natural language processing tasks. Transferring representations from one language to another is especially crucial for cross-lingual learning. One can expect machine translation objectives to be well suited to fostering such capabilities, as they involve the explicit alignment of semantically equivalent sentences from different languages. This paper investigates the potential benefits of employing machine translation as a continued training objective to enhance language representation learning, bridging multilingual pretraining and cross-lingual applications. We study this question through two lenses: a quantitative evaluation of the performance of existing models and an analysis of their latent representations. Our results show that, contrary to expectations, machine translation as the continued training fails to enhance cross-lingual representation learning i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#40065;&#26834;&#24615;&#28151;&#21512;&#20195;&#30721;&#32763;&#35793;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#24320;&#21457;&#20102;Hinglish&#21040;&#33521;&#35821;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#20197;&#21450;&#25552;&#20986;&#30340;&#33021;&#22815;&#22788;&#29702;&#22122;&#22768;&#30340;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;RCMT&#12290;</title><link>https://arxiv.org/abs/2403.16771</link><description>&lt;p&gt;
&#29992;&#20110;&#40065;&#26834;&#24615;&#28151;&#21512;&#20195;&#30721;&#32763;&#35793;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Synthetic Data Generation and Joint Learning for Robust Code-Mixed Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#40065;&#26834;&#24615;&#28151;&#21512;&#20195;&#30721;&#32763;&#35793;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#24320;&#21457;&#20102;Hinglish&#21040;&#33521;&#35821;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#20197;&#21450;&#25552;&#20986;&#30340;&#33021;&#22815;&#22788;&#29702;&#22122;&#22768;&#30340;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;RCMT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22810;&#35821;&#35328;&#19990;&#30028;&#20013;&#30340;&#24191;&#27867;&#32593;&#32476;&#20132;&#27969;&#20026;&#22312;&#21333;&#20010;&#35805;&#35821;&#20013;&#28151;&#21512;&#22810;&#31181;&#35821;&#35328;&#65288;&#21448;&#31216;&#28151;&#21512;&#20195;&#30721;&#35821;&#35328;&#65289;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#30001;&#20110;&#26631;&#27880;&#25968;&#25454;&#30340;&#31232;&#32570;&#21644;&#22122;&#38899;&#30340;&#23384;&#22312;&#65292;&#36825;&#32473;&#35745;&#31639;&#27169;&#22411;&#24102;&#26469;&#20102;&#20005;&#23803;&#25361;&#25112;&#12290;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#29615;&#22659;&#20013;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#26159;&#36890;&#36807;&#32763;&#35793;&#21033;&#29992;&#36164;&#28304;&#20016;&#23500;&#35821;&#35328;&#20013;&#30340;&#29616;&#26377;&#25968;&#25454;&#12290;&#26412;&#25991;&#38024;&#23545;&#28151;&#21512;&#20195;&#30721;&#65288;&#21360;&#22320;&#35821;&#21644;&#23391;&#21152;&#25289;&#35821;&#65289;&#21040;&#33521;&#35821;&#30340;&#26426;&#22120;&#32763;&#35793;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21512;&#25104;&#24320;&#21457;&#20102;HINMIX&#19968;&#20010;&#21360;&#22320;&#35821;&#21040;&#33521;&#35821;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#32422;420&#19975;&#20010;&#21477;&#23545;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RCMT&#65292;&#19968;&#31181;&#22522;&#20110;&#24378;&#20581;&#25200;&#21160;&#30340;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#24178;&#20928;&#21644;&#24102;&#22122;&#22768;&#21333;&#35789;&#20043;&#38388;&#20849;&#20139;&#21442;&#25968;&#65292;&#23398;&#20064;&#22788;&#29702;&#29616;&#23454;&#19990;&#30028;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#20013;&#30340;&#22122;&#22768;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RCMT&#22312;&#38646;-shot&#35774;&#32622;&#20013;&#23545;&#23391;&#21152;&#25289;&#35821;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16771v1 Announce Type: new  Abstract: The widespread online communication in a modern multilingual world has provided opportunities to blend more than one language (aka code-mixed language) in a single utterance. This has resulted a formidable challenge for the computational models due to the scarcity of annotated data and presence of noise. A potential solution to mitigate the data scarcity problem in low-resource setup is to leverage existing data in resource-rich language through translation. In this paper, we tackle the problem of code-mixed (Hinglish and Bengalish) to English machine translation. First, we synthetically develop HINMIX, a parallel corpus of Hinglish to English, with ~4.2M sentence pairs. Subsequently, we propose RCMT, a robust perturbation based joint-training model that learns to handle noise in the real-world code-mixed text by parameter sharing across clean and noisy words. Further, we show the adaptability of RCMT in a zero-shot setup for Bengalish t
&lt;/p&gt;</description></item><item><title>ProCQA&#25968;&#25454;&#38598;&#26159;&#20174;StackOverflow&#31038;&#21306;&#25552;&#21462;&#30340;&#65292;&#20026;&#32534;&#31243;&#38382;&#39064;&#22238;&#31572;&#25552;&#20379;&#20102;&#33258;&#28982;&#32467;&#26500;&#21270;&#30340;&#28151;&#21512;&#27169;&#24577;&#38382;&#31572;&#23545;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#27169;&#24577;-&#19981;&#21487;&#30693;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16702</link><description>&lt;p&gt;
ProCQA&#65306;&#19968;&#20010;&#29992;&#20110;&#20195;&#30721;&#25628;&#32034;&#30340;&#22823;&#35268;&#27169;&#22522;&#20110;&#31038;&#21306;&#30340;&#32534;&#31243;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ProCQA: A Large-scale Community-based Programming Question Answering Dataset for Code Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16702
&lt;/p&gt;
&lt;p&gt;
ProCQA&#25968;&#25454;&#38598;&#26159;&#20174;StackOverflow&#31038;&#21306;&#25552;&#21462;&#30340;&#65292;&#20026;&#32534;&#31243;&#38382;&#39064;&#22238;&#31572;&#25552;&#20379;&#20102;&#33258;&#28982;&#32467;&#26500;&#21270;&#30340;&#28151;&#21512;&#27169;&#24577;&#38382;&#31572;&#23545;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#27169;&#24577;-&#19981;&#21487;&#30693;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#24335;&#20195;&#30721;&#38382;&#31572;&#26088;&#22312;&#23558;&#29992;&#25143;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#26597;&#35810;&#19982;&#30456;&#20851;&#20195;&#30721;&#29255;&#27573;&#21305;&#37197;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20351;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#21452;&#27169;&#24577;&#21644;&#21333;&#27169;&#24577;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#23545;&#40784;&#25991;&#26412;&#21644;&#20195;&#30721;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ProCQA&#65292;&#36825;&#26159;&#19968;&#20010;&#20174;StackOverflow&#31038;&#21306;&#20013;&#25552;&#21462;&#30340;&#22823;&#35268;&#27169;&#32534;&#31243;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#33258;&#28982;&#32467;&#26500;&#21270;&#30340;&#28151;&#21512;&#27169;&#24577;&#38382;&#31572;&#23545;&#12290;&#20026;&#20102;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#19981;&#21487;&#30693;&#30340;&#23545;&#27604;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#24403;&#21069;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#21644;&#20195;&#30721;&#34920;&#31034;&#30340;&#23545;&#40784;&#12290;&#19982;&#20808;&#21069;&#20027;&#35201;&#20351;&#29992;&#20174;CodeSearchNet&#20013;&#25552;&#21462;&#30340;&#21452;&#27169;&#24577;&#21644;&#21333;&#27169;&#24577;&#23545;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;&#20195;&#30721;&#26816;&#32034;&#22522;&#20934;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16702v1 Announce Type: new  Abstract: Retrieval-based code question answering seeks to match user queries in natural language to relevant code snippets. Previous approaches typically rely on pretraining models using crafted bi-modal and uni-modal datasets to align text and code representations. In this paper, we introduce ProCQA, a large-scale programming question answering dataset extracted from the StackOverflow community, offering naturally structured mixed-modal QA pairs. To validate its effectiveness, we propose a modality-agnostic contrastive pre-training approach to improve the alignment of text and code representations of current code language models. Compared to previous models that primarily employ bimodal and unimodal pairs extracted from CodeSearchNet for pre-training, our model exhibits significant performance improvements across a wide range of code retrieval benchmarks.
&lt;/p&gt;</description></item><item><title>ToXCL&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#26088;&#22312;&#26816;&#27979;&#21644;&#35299;&#37322;&#38544;&#24615;&#27602;&#24615;&#35328;&#35770;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#27169;&#22411;&#22312;&#26816;&#27979;&#21644;&#35299;&#37322;&#20219;&#21153;&#20013;&#21487;&#33021;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16685</link><description>&lt;p&gt;
ToXCL&#65306;&#27602;&#24615;&#35328;&#35770;&#26816;&#27979;&#21644;&#35299;&#37322;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ToXCL: A Unified Framework for Toxic Speech Detection and Explanation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16685
&lt;/p&gt;
&lt;p&gt;
ToXCL&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#26088;&#22312;&#26816;&#27979;&#21644;&#35299;&#37322;&#38544;&#24615;&#27602;&#24615;&#35328;&#35770;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#27169;&#22411;&#22312;&#26816;&#27979;&#21644;&#35299;&#37322;&#20219;&#21153;&#20013;&#21487;&#33021;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#19978;&#27602;&#24615;&#35328;&#35770;&#30340;&#34067;&#24310;&#26159;&#19968;&#20010;&#20196;&#20154;&#20851;&#27880;&#30340;&#38382;&#39064;&#65292;&#23545;&#20154;&#32676;&#26500;&#25104;&#23041;&#32961;&#12290;&#26126;&#26174;&#30340;&#27602;&#24615;&#35328;&#35770;&#21253;&#21547;&#20882;&#29359;&#24615;&#35789;&#27719;&#20449;&#21495;&#65292;&#38544;&#24615;&#30340;&#35328;&#35770;&#21017;&#21253;&#21547;&#32534;&#30721;&#25110;&#38388;&#25509;&#35821;&#35328;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#19981;&#20165;&#38656;&#35201;&#26816;&#27979;&#38544;&#24615;&#27602;&#24615;&#35328;&#35770;&#65292;&#36824;&#38656;&#35201;&#35299;&#37322;&#20854;&#27602;&#24615;&#12290;&#36825;&#23548;&#33268;&#20102;&#23545;&#26377;&#25928;&#26816;&#27979;&#21644;&#35299;&#37322;&#38544;&#24615;&#27602;&#24615;&#35328;&#35770;&#30340;&#32479;&#19968;&#26694;&#26550;&#30340;&#29420;&#29305;&#38656;&#27714;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#23558;&#27602;&#24615;&#35328;&#35770;&#30340;&#26816;&#27979;&#21644;&#35299;&#37322;&#20219;&#21153;&#21046;&#23450;&#20026;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#31181;&#31574;&#30053;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#21518;&#32493;&#38169;&#35823;&#20256;&#25773;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#27169;&#22411;&#30340;&#26816;&#27979;&#32467;&#26524;&#36828;&#20302;&#20110;&#37027;&#20123;&#20165;&#19987;&#27880;&#20110;&#26816;&#27979;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ToXCL&#65292;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#21644;&#35299;&#37322;&#38544;&#24615;&#27602;&#24615;&#35328;&#35770;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16685v1 Announce Type: new  Abstract: The proliferation of online toxic speech is a pertinent problem posing threats to demographic groups. While explicit toxic speech contains offensive lexical signals, implicit one consists of coded or indirect language. Therefore, it is crucial for models not only to detect implicit toxic speech but also to explain its toxicity. This draws a unique need for unified frameworks that can effectively detect and explain implicit toxic speech. Prior works mainly formulated the task of toxic speech detection and explanation as a text generation problem. Nonetheless, models trained using this strategy can be prone to suffer from the consequent error propagation problem. Moreover, our experiments reveal that the detection results of such models are much lower than those that focus only on the detection task. To bridge these gaps, we introduce ToXCL, a unified framework for the detection and explanation of implicit toxic speech. Our model consists 
&lt;/p&gt;</description></item><item><title>&#31532;&#19968;&#27425;&#21033;&#29992;&#35745;&#31639;&#31038;&#20250;&#35821;&#35328;&#23398;&#26041;&#27861;&#22312;Twitter&#19978;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#21457;&#29616;&#28843;&#32768;&#34892;&#20026;&#30340;&#26222;&#36941;&#24615;&#38543;&#26102;&#38388;&#20943;&#23569;&#65292;&#32780;&#26356;&#24180;&#36731;&#12289;&#21463;&#25945;&#32946;&#31243;&#24230;&#36739;&#39640;&#21644;&#21463;&#27426;&#36814;&#30340;&#29992;&#25143;&#26356;&#23481;&#26131;&#28843;&#32768;&#12290;</title><link>https://arxiv.org/abs/2403.16668</link><description>&lt;p&gt;
&#35841;&#22312;&#32593;&#32476;&#19978;&#28843;&#32768;&#24471;&#26356;&#22810;&#65311;&#31038;&#20132;&#23186;&#20307;&#28843;&#32768;&#34892;&#20026;&#30340;&#22823;&#35268;&#27169;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Who is bragging more online? A large scale analysis of bragging in social media
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16668
&lt;/p&gt;
&lt;p&gt;
&#31532;&#19968;&#27425;&#21033;&#29992;&#35745;&#31639;&#31038;&#20250;&#35821;&#35328;&#23398;&#26041;&#27861;&#22312;Twitter&#19978;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#21457;&#29616;&#28843;&#32768;&#34892;&#20026;&#30340;&#26222;&#36941;&#24615;&#38543;&#26102;&#38388;&#20943;&#23569;&#65292;&#32780;&#26356;&#24180;&#36731;&#12289;&#21463;&#25945;&#32946;&#31243;&#24230;&#36739;&#39640;&#21644;&#21463;&#27426;&#36814;&#30340;&#29992;&#25143;&#26356;&#23481;&#26131;&#28843;&#32768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28843;&#32768;&#26159;&#35828;&#20986;&#37027;&#20123;&#21487;&#33021;&#34987;&#20854;&#20182;&#20154;&#31215;&#26497;&#30475;&#24453;&#30340;&#35805;&#35821;&#65292;&#23427;&#22312;&#20154;&#31867;&#20132;&#27969;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#31215;&#26497;&#30340;&#33258;&#25105;&#24418;&#35937;&#12290;&#31038;&#20132;&#23186;&#20307;&#26159;&#19968;&#20010;&#33258;&#28982;&#30340;&#24179;&#21488;&#65292;&#29992;&#25143;&#21487;&#20197;&#21033;&#29992;&#28843;&#32768;&#26469;&#33719;&#24471;&#35266;&#20247;&#30340;&#38054;&#20329;&#12289;&#23562;&#37325;&#12289;&#20851;&#27880;&#21644;&#31881;&#19997;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#32593;&#32476;&#19978;&#28843;&#32768;&#34892;&#20026;&#30340;&#35268;&#27169;&#21644;&#29305;&#24449;&#30693;&#20043;&#29978;&#23569;&#12290;&#26412;&#25991;&#37319;&#29992;&#35745;&#31639;&#31038;&#20250;&#35821;&#35328;&#23398;&#26041;&#27861;&#65292;&#39318;&#27425;&#23545;Twitter&#65288;&#32654;&#22269;&#65289;&#19978;&#30340;&#28843;&#32768;&#34892;&#20026;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#25972;&#20307;&#26222;&#36941;&#24615;&#12289;&#26102;&#38388;&#21160;&#24577;&#21644;&#20154;&#21475;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#21516;&#19968;&#29992;&#25143;&#32676;&#20307;&#20013;&#65292;&#28843;&#32768;&#30340;&#26222;&#36941;&#31243;&#24230;&#20250;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#20943;&#23569;&#12290;&#27492;&#22806;&#65292;&#22312;&#32654;&#22269;&#65292;&#26356;&#24180;&#36731;&#12289;&#21463;&#25945;&#32946;&#31243;&#24230;&#36739;&#39640;&#21644;&#21463;&#27426;&#36814;&#30340;&#29992;&#25143;&#26356;&#26377;&#21487;&#33021;&#28843;&#32768;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35821;&#35328;&#23398;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#29305;&#23450;&#30340;&#28843;&#32768;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16668v1 Announce Type: new  Abstract: Bragging is the act of uttering statements that are likely to be positively viewed by others and it is extensively employed in human communication with the aim to build a positive self-image of oneself. Social media is a natural platform for users to employ bragging in order to gain admiration, respect, attention and followers from their audiences. Yet, little is known about the scale of bragging online and its characteristics. This paper employs computational sociolinguistics methods to conduct the first large scale study of bragging behavior on Twitter (U.S.) by focusing on its overall prevalence, temporal dynamics and impact of demographic factors. Our study shows that the prevalence of bragging decreases over time within the same population of users. In addition, younger, more educated and popular users in the U.S. are more likely to brag. Finally, we conduct an extensive linguistics analysis to unveil specific bragging themes associ
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#26816;&#32034;&#21644;&#24635;&#32467;&#32593;&#32476;&#20013;&#30340;&#35777;&#25454;&#65292;&#26500;&#24314;&#20102;RU22Fact&#25968;&#25454;&#38598;&#65292;&#26159;&#20851;&#20110;2022&#24180;&#20420;&#20044;&#20914;&#31361;&#30340;&#22810;&#35821;&#35328;&#21487;&#35299;&#37322;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#24320;&#21457;&#20102;&#31471;&#21040;&#31471;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#26469;&#39564;&#35777;&#22768;&#26126;&#24182;&#29983;&#25104;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.16662</link><description>&lt;p&gt;
RU22Fact&#65306;&#20248;&#21270;&#22810;&#35821;&#35328;&#21487;&#35299;&#37322;&#20107;&#23454;&#26680;&#26597;&#20013;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
RU22Fact: Optimizing Evidence for Multilingual Explainable Fact-Checking on Russia-Ukraine Conflict
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16662
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#26816;&#32034;&#21644;&#24635;&#32467;&#32593;&#32476;&#20013;&#30340;&#35777;&#25454;&#65292;&#26500;&#24314;&#20102;RU22Fact&#25968;&#25454;&#38598;&#65292;&#26159;&#20851;&#20110;2022&#24180;&#20420;&#20044;&#20914;&#31361;&#30340;&#22810;&#35821;&#35328;&#21487;&#35299;&#37322;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#24320;&#21457;&#20102;&#31471;&#21040;&#31471;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#26469;&#39564;&#35777;&#22768;&#26126;&#24182;&#29983;&#25104;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#23454;&#26680;&#26597;&#26159;&#36890;&#36807;&#26816;&#26597;&#29616;&#26377;&#35777;&#25454;&#26469;&#39564;&#35777;&#32473;&#23450;&#22768;&#26126;&#30340;&#20934;&#30830;&#24615;&#30340;&#20219;&#21153;&#12290;&#39640;&#36136;&#37327;&#30340;&#35777;&#25454;&#22312;&#22686;&#24378;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#24182;&#20419;&#36827;&#29983;&#25104;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20026;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#25552;&#20379;&#36275;&#22815;&#21644;&#30456;&#20851;&#30340;&#35777;&#25454;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#20174;&#32593;&#32476;&#20013;&#26816;&#32034;&#21644;&#24635;&#32467;&#35777;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;RU22Fact&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#20110;2022&#24180;&#20420;&#20044;&#20914;&#31361;&#30340;&#26032;&#22411;&#22810;&#35821;&#35328;&#21487;&#35299;&#37322;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1.6&#19975;&#20010;&#26679;&#26412;&#65292;&#27599;&#20010;&#26679;&#26412;&#37117;&#21253;&#21547;&#29616;&#23454;&#19990;&#30028;&#30340;&#22768;&#26126;&#12289;&#20248;&#21270;&#35777;&#25454;&#21644;&#24341;&#29992;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#20026;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#24314;&#31435;&#22522;&#20934;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#65292;&#29992;&#20110;&#26680;&#23454;&#22768;&#26126;&#24182;&#29983;&#25104;&#35299;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16662v1 Announce Type: new  Abstract: Fact-checking is the task of verifying the factuality of a given claim by examining the available evidence. High-quality evidence plays a vital role in enhancing fact-checking systems and facilitating the generation of explanations that are understandable to humans. However, the provision of both sufficient and relevant evidence for explainable fact-checking systems poses a challenge. To tackle this challenge, we propose a method based on a Large Language Model to automatically retrieve and summarize evidence from the Web. Furthermore, we construct RU22Fact, a novel multilingual explainable fact-checking dataset on the Russia-Ukraine conflict in 2022 of 16K samples, each containing real-world claims, optimized evidence, and referenced explanation. To establish a baseline for our dataset, we also develop an end-to-end explainable fact-checking system to verify claims and generate explanations. Experimental results demonstrate the prospect
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;BART&#21644;MarianMT&#20004;&#31181;&#20808;&#36827;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#65292;&#23545;&#25991;&#26412;&#20013;&#30340;&#38169;&#35823;&#36827;&#34892;&#20462;&#27491;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#30740;&#31350;&#25506;&#31350;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2403.16655</link><description>&lt;p&gt;
&#35821;&#27861;&#38169;&#35823;&#19982;&#25340;&#20889;&#38169;&#35823;&#26356;&#27491;&#65306;&#21033;&#29992;BART&#21644;MarianMT&#25506;&#31350;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#21709;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Grammatical vs Spelling Error Correction: An Investigation into the Responsiveness of Transformer-based Language Models using BART and MarianMT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16655
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;BART&#21644;MarianMT&#20004;&#31181;&#20808;&#36827;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#65292;&#23545;&#25991;&#26412;&#20013;&#30340;&#38169;&#35823;&#36827;&#34892;&#20462;&#27491;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#30740;&#31350;&#25506;&#31350;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Text &#20173;&#28982;&#26159;&#20449;&#24687;&#34920;&#31034;&#30340;&#19968;&#20010;&#30456;&#20851;&#24418;&#24335;&#12290;&#25991;&#26412;&#25991;&#26723;&#26159;&#22312;&#25968;&#23383;&#21407;&#29983;&#24179;&#21488;&#20013;&#21019;&#24314;&#30340;&#65292;&#25110;&#36890;&#36807;&#23558;&#20854;&#20182;&#23186;&#20307;&#25991;&#20214;&#65288;&#22914;&#22270;&#20687;&#21644;&#35821;&#38899;&#65289;&#36716;&#25442;&#32780;&#26469;&#12290;&#34429;&#28982;&#25968;&#23383;&#21407;&#29983;&#25991;&#26412;&#36890;&#24120;&#26159;&#36890;&#36807;&#23454;&#20307;&#25110;&#34394;&#25311;&#38190;&#30424;&#33719;&#24471;&#30340;&#65292;&#20294;&#20063;&#21033;&#29992;OCR&#21644;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;&#23558;&#22270;&#20687;&#21644;&#35821;&#38899;&#20449;&#21495;&#36716;&#25442;&#20026;&#25991;&#26412;&#20869;&#23481;&#12290;&#25152;&#26377;&#36825;&#20123;&#25991;&#26412;&#29983;&#25104;&#26426;&#21046;&#37117;&#20250;&#24341;&#20837;&#38169;&#35823;&#21040;&#25152;&#25429;&#33719;&#30340;&#25991;&#26412;&#20013;&#12290;&#35813;&#39033;&#30446;&#26088;&#22312;&#20998;&#26512;&#25991;&#26412;&#25991;&#26723;&#20013;&#20986;&#29616;&#30340;&#19981;&#21516;&#31867;&#22411;&#38169;&#35823;&#12290;&#35813;&#24037;&#20316;&#37319;&#29992;&#20004;&#31181;&#20808;&#36827;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;BART&#21644;MarianMT&#65292;&#26469;&#32416;&#27491;&#25991;&#26412;&#20013;&#23384;&#22312;&#30340;&#24322;&#24120;&#12290;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#21487;&#29992;&#25968;&#25454;&#38598;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#20197;&#24494;&#35843;&#23427;&#20204;&#30340;&#32416;&#38169;&#33021;&#21147;&#12290;&#36827;&#34892;&#20102;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;&#26469;&#35843;&#26597;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16655v1 Announce Type: new  Abstract: Text continues to remain a relevant form of representation for information. Text documents are created either in digital native platforms or through the conversion of other media files such as images and speech. While the digital native text is invariably obtained through physical or virtual keyboards, technologies such as OCR and speech recognition are utilized to transform the images and speech signals into text content. All these variety of mechanisms of text generation also introduce errors into the captured text.   This project aims at analyzing different kinds of error that occurs in text documents. The work employs two of the advanced deep neural network-based language models, namely, BART and MarianMT, to rectify the anomalies present in the text. Transfer learning of these models with available dataset is performed to finetune their capacity for error correction. A comparative study is conducted to investigate the effectiveness 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#19987;&#21033;&#23884;&#20837;&#27169;&#22411;&#22312;&#19987;&#21033;&#30456;&#20284;&#24615;&#35745;&#31639;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#20855;&#20307;&#25506;&#35752;&#20102;Sentence Transformers (SBERT) &#26550;&#26500;&#22312;&#19987;&#21033;&#30456;&#20284;&#24615;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16630</link><description>&lt;p&gt;
&#19987;&#21033;&#30456;&#20284;&#24615;&#23884;&#20837;&#27169;&#22411;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A comparative analysis of embedding models for patent similarity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#19987;&#21033;&#23884;&#20837;&#27169;&#22411;&#22312;&#19987;&#21033;&#30456;&#20284;&#24615;&#35745;&#31639;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#20855;&#20307;&#25506;&#35752;&#20102;Sentence Transformers (SBERT) &#26550;&#26500;&#22312;&#19987;&#21033;&#30456;&#20284;&#24615;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20110;&#25991;&#26412;&#30340;&#19987;&#21033;&#30456;&#20284;&#24615;&#39046;&#22495;&#20570;&#20986;&#20102;&#20004;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#23427;&#27604;&#36739;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#19987;&#21033;&#29305;&#23450;&#39044;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#65288;&#22914;word2vec&#21644;doc2vec&#27169;&#22411;&#65289;&#21644;&#19978;&#19979;&#25991;&#35789;&#23884;&#20837;&#27169;&#22411;&#65288;&#22914;&#22522;&#20110;transformers&#30340;&#27169;&#22411;&#65289;&#22312;&#19987;&#21033;&#30456;&#20284;&#24615;&#35745;&#31639;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#20854;&#27425;&#65292;&#23427;&#20855;&#20307;&#27604;&#36739;&#20102;&#20855;&#26377;&#19981;&#21516;&#35757;&#32451;&#38454;&#27573;&#30340;Sentence Transformers&#65288;SBERT&#65289;&#26550;&#26500;&#22312;&#19987;&#21033;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#20026;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#20351;&#29992;&#20851;&#20110;&#19987;&#21033;&#24178;&#28041;&#30340;&#20449;&#24687;&#65292;&#21363;&#20004;&#20010;&#25110;&#22810;&#20010;&#19987;&#21033;&#30003;&#35831;&#20013;&#30340;&#19987;&#21033;&#35201;&#27714;&#34987;&#19987;&#21033;&#23457;&#26597;&#21592;&#35777;&#26126;&#23384;&#22312;&#37325;&#21472;&#30340;&#29616;&#35937;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#24178;&#28041;&#26696;&#20363;&#35270;&#20026;&#20004;&#20010;&#19987;&#21033;&#20043;&#38388;&#30340;&#26368;&#22823;&#30456;&#20284;&#24615;&#30340;&#20195;&#29702;&#65292;&#24182;&#29992;&#23427;&#20204;&#20316;&#20026;&#22522;&#20934;&#26469;&#35780;&#20272;&#19981;&#21516;&#23884;&#20837;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16630v1 Announce Type: new  Abstract: This paper makes two contributions to the field of text-based patent similarity. First, it compares the performance of different kinds of patent-specific pretrained embedding models, namely static word embeddings (such as word2vec and doc2vec models) and contextual word embeddings (such as transformers based models), on the task of patent similarity calculation. Second, it compares specifically the performance of Sentence Transformers (SBERT) architectures with different training phases on the patent similarity task. To assess the models' performance, we use information about patent interferences, a phenomenon in which two or more patent claims belonging to different patent applications are proven to be overlapping by patent examiners. Therefore, we use these interferences cases as a proxy for maximum similarity between two patents, treating them as ground-truth to evaluate the performance of the different embedding models. Our results p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22810;&#35821;&#35328;&#21477;&#23376;&#32534;&#30721;&#22120;&#65288;CT-XLMR-SE&#21644;CT-mBERT-SE&#65289;&#65292;&#21487;&#20026;50&#22810;&#31181;&#35821;&#35328;&#30340;&#21361;&#26426;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#23884;&#20837;&#35821;&#20041;&#20869;&#23481;&#65292;&#20351;&#20855;&#26377;&#30456;&#20284;&#21547;&#20041;&#30340;&#25991;&#26412;&#22312;&#21516;&#19968;&#21521;&#37327;&#31354;&#38388;&#20869;&#25509;&#36817;&#65292;&#26080;&#35770;&#35821;&#35328;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.16614</link><description>&lt;p&gt;
&#20026;&#21361;&#26426;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#22686;&#21152;&#35821;&#20041;&#20869;&#23481;&#30340;&#36328;&#35821;&#35328;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Semantically Enriched Cross-Lingual Sentence Embeddings for Crisis-related Social Media Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16614
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22810;&#35821;&#35328;&#21477;&#23376;&#32534;&#30721;&#22120;&#65288;CT-XLMR-SE&#21644;CT-mBERT-SE&#65289;&#65292;&#21487;&#20026;50&#22810;&#31181;&#35821;&#35328;&#30340;&#21361;&#26426;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#23884;&#20837;&#35821;&#20041;&#20869;&#23481;&#65292;&#20351;&#20855;&#26377;&#30456;&#20284;&#21547;&#20041;&#30340;&#25991;&#26412;&#22312;&#21516;&#19968;&#21521;&#37327;&#31354;&#38388;&#20869;&#25509;&#36817;&#65292;&#26080;&#35770;&#35821;&#35328;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35832;&#22914;&#21361;&#26426;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#30340;&#35821;&#20041;&#25628;&#32034;&#21644;&#32858;&#31867;&#31561;&#20219;&#21153;&#25552;&#39640;&#20102;&#25105;&#20204;&#23545;&#21361;&#26426;&#35805;&#35821;&#30340;&#29702;&#35299;&#65292;&#26377;&#21161;&#20110;&#20915;&#31574;&#21046;&#23450;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#12290;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#21361;&#26426;&#20449;&#24687;&#23398;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#65292;&#20294;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#32570;&#20047;&#35821;&#20041;&#24847;&#20041;&#12290;&#23613;&#31649;CrisisTransformers&#31995;&#21015;&#21253;&#25324;&#19968;&#20010;&#21477;&#23376;&#32534;&#30721;&#22120;&#26469;&#35299;&#20915;&#35821;&#20041;&#38382;&#39064;&#65292;&#20294;&#23427;&#20173;&#28982;&#26159;&#21333;&#35821;&#30340;&#65292;&#20165;&#22788;&#29702;&#33521;&#35821;&#25991;&#26412;&#12290;&#27492;&#22806;&#65292;&#20026;&#19981;&#21516;&#35821;&#35328;&#20351;&#29992;&#21333;&#29420;&#30340;&#27169;&#22411;&#20250;&#23548;&#33268;&#23884;&#20837;&#21040;&#19981;&#21516;&#21521;&#37327;&#31354;&#38388;&#20013;&#65292;&#24403;&#27604;&#36739;&#22810;&#35821;&#35328;&#25991;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26102;&#20250;&#24341;&#20837;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#35821;&#35328;&#21477;&#23376;&#32534;&#30721;&#22120;&#65288;CT-XLMR-SE&#21644;CT-mBERT-SE&#65289;&#65292;&#20026;50&#22810;&#31181;&#35821;&#35328;&#30340;&#21361;&#26426;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#23884;&#20837;&#65292;&#20351;&#20855;&#26377;&#30456;&#20284;&#21547;&#20041;&#30340;&#25991;&#26412;&#22312;&#30456;&#21516;&#30340;&#21521;&#37327;&#31354;&#38388;&#20869;&#38752;&#36817;&#65292;&#26080;&#35770;&#35821;&#35328;&#22810;&#26679;&#24615;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16614v1 Announce Type: new  Abstract: Tasks such as semantic search and clustering on crisis-related social media texts enhance our comprehension of crisis discourse, aiding decision-making and targeted interventions. Pre-trained language models have advanced performance in crisis informatics, but their contextual embeddings lack semantic meaningfulness. Although the CrisisTransformers family includes a sentence encoder to address the semanticity issue, it remains monolingual, processing only English texts. Furthermore, employing separate models for different languages leads to embeddings in distinct vector spaces, introducing challenges when comparing semantic similarities between multi-lingual texts. Therefore, we propose multi-lingual sentence encoders (CT-XLMR-SE and CT-mBERT-SE) that embed crisis-related social media texts for over 50 languages, such that texts with similar meanings are in close proximity within the same vector space, irrespective of language diversity.
&lt;/p&gt;</description></item><item><title>&#23545;&#35805;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#24314;&#35774;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#25509;&#22320;&#34892;&#20026;&#21644;&#25509;&#22320;&#21333;&#20803;&#30340;&#26631;&#27880;&#21644;&#24230;&#37327;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#23545;&#35805;&#31995;&#32479;&#22312;&#25509;&#22320;&#33021;&#21147;&#19978;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.16609</link><description>&lt;p&gt;
&#23545;&#35805;&#25509;&#22320;&#65306;&#20851;&#20110;&#25509;&#22320;&#34892;&#20026;&#21644;&#25509;&#22320;&#21333;&#20803;&#30340;&#27880;&#37322;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Conversational Grounding: Annotation and Analysis of Grounding Acts and Grounding Units
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16609
&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#24314;&#35774;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#25509;&#22320;&#34892;&#20026;&#21644;&#25509;&#22320;&#21333;&#20803;&#30340;&#26631;&#27880;&#21644;&#24230;&#37327;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#23545;&#35805;&#31995;&#32479;&#22312;&#25509;&#22320;&#33021;&#21147;&#19978;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#30340;&#23545;&#35805;&#36890;&#24120;&#24314;&#31435;&#22312;&#20849;&#21516;&#29702;&#35299;&#30340;&#22522;&#30784;&#19978;&#65292;&#25152;&#26377;&#21442;&#19982;&#26041;&#23545;&#25152;&#20849;&#20139;&#30340;&#20449;&#24687;&#24515;&#39046;&#31070;&#20250;&#12290;&#36825;&#20010;&#34987;&#31216;&#20026;&#23545;&#35805;&#25509;&#22320;&#30340;&#36807;&#31243;&#23545;&#20110;&#26500;&#24314;&#21487;&#38752;&#30340;&#23545;&#35805;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#33021;&#22815;&#20934;&#30830;&#36861;&#36394;&#21644;&#22238;&#24518;&#25152;&#20998;&#20139;&#30340;&#20449;&#24687;&#12290;&#20195;&#29702;&#22312;&#25509;&#22320;&#20256;&#36798;&#30340;&#20449;&#24687;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#23545;&#20110;&#26500;&#24314;&#21487;&#38752;&#30340;&#23545;&#35805;&#31995;&#32479;&#26377;&#30528;&#37325;&#35201;&#36129;&#29486;&#12290;&#23613;&#31649;&#23545;&#35805;&#31995;&#32479;&#22312;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#30340;&#25509;&#22320;&#33021;&#21147;&#20173;&#28982;&#23384;&#22312;&#26126;&#26174;&#30340;&#19981;&#36275;&#12290;Traum&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#23545;&#35805;&#25509;&#22320;&#30340;&#26694;&#26550;&#65292;&#20171;&#32461;&#20102;&#25509;&#22320;&#34892;&#20026;&#21644;&#25509;&#22320;&#21333;&#20803;&#65292;&#20294;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#65292;&#23578;&#26410;&#21462;&#24471;&#23454;&#36136;&#24615;&#36827;&#23637;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#20004;&#20010;&#23545;&#35805;&#35821;&#26009;&#24211;&#36827;&#34892;&#27880;&#37322;&#65292;&#21033;&#29992;&#25509;&#22320;&#34892;&#20026;&#12289;&#25509;&#22320;&#21333;&#20803;&#20197;&#21450;&#23427;&#20204;&#30340;&#25509;&#22320;&#31243;&#24230;&#30340;&#24230;&#37327;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#22312;&#27880;&#37322;&#36807;&#31243;&#20013;&#30340;&#20851;&#38190;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16609v1 Announce Type: new  Abstract: Successful conversations often rest on common understanding, where all parties are on the same page about the information being shared. This process, known as conversational grounding, is crucial for building trustworthy dialog systems that can accurately keep track of and recall the shared information. The proficiencies of an agent in grounding the conveyed information significantly contribute to building a reliable dialog system. Despite recent advancements in dialog systems, there exists a noticeable deficit in their grounding capabilities. Traum provided a framework for conversational grounding introducing Grounding Acts and Grounding Units, but substantial progress, especially in the realm of Large Language Models, remains lacking. To bridge this gap, we present the annotation of two dialog corpora employing Grounding Acts, Grounding Units, and a measure of their degree of grounding. We discuss our key findings during the annotation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;SemEval2024 Task8&#30340;TrustAI&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#35821;&#22659;&#19979;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#65292;&#30740;&#31350;&#32508;&#21512;&#20998;&#26512;&#20102;&#32479;&#35745;&#12289;&#31070;&#32463;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#31561;&#21508;&#31181;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#23454;&#39564;&#35774;&#32622;&#21644;&#28145;&#20837;&#30340;&#35823;&#24046;&#20998;&#26512;&#65292;&#21462;&#24471;&#20102;&#30456;&#24403;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#21516;&#26102;&#20063;&#31361;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#20013;&#38656;&#35201;&#32771;&#34385;&#30340;&#25361;&#25112;&#21644;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2403.16592</link><description>&lt;p&gt;
TrustAI&#22312;SemEval-2024&#20219;&#21153;8&#20013;&#30340;&#24212;&#29992;&#65306;&#22810;&#39046;&#22495;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#25216;&#26415;&#30340;&#32508;&#21512;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
TrustAI at SemEval-2024 Task 8: A Comprehensive Analysis of Multi-domain Machine Generated Text Detection Techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;SemEval2024 Task8&#30340;TrustAI&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#35821;&#22659;&#19979;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#65292;&#30740;&#31350;&#32508;&#21512;&#20998;&#26512;&#20102;&#32479;&#35745;&#12289;&#31070;&#32463;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#31561;&#21508;&#31181;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#23454;&#39564;&#35774;&#32622;&#21644;&#28145;&#20837;&#30340;&#35823;&#24046;&#20998;&#26512;&#65292;&#21462;&#24471;&#20102;&#30456;&#24403;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#21516;&#26102;&#20063;&#31361;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#20013;&#38656;&#35201;&#32771;&#34385;&#30340;&#25361;&#25112;&#21644;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16592v1 &#20844;&#21578;&#31867;&#22411;: &#26032; &#38543;&#30528;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#29992;&#25143;&#26597;&#35810;&#20013;&#29983;&#25104;&#27969;&#30021;&#20869;&#23481;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#23545;&#20110;&#20135;&#29983;&#30340;&#34394;&#20551;&#20449;&#24687;&#21644;&#20010;&#20154;&#20449;&#24687;&#27844;&#38706;&#24341;&#36215;&#20102;&#25285;&#24551;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;SemEval2024 Task8&#20013;&#29992;&#20110;&#26816;&#27979;&#21508;&#31181;&#39046;&#22495;&#20869;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#21333;&#35821;&#21644;&#22810;&#35821;&#22659;&#19979;&#26816;&#27979;&#12290;&#25105;&#20204;&#20840;&#38754;&#20998;&#26512;&#20102;&#21508;&#31181;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#32479;&#35745;&#12289;&#31070;&#32463;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#35814;&#32454;&#25551;&#36848;&#20102;&#23454;&#39564;&#35774;&#32622;&#65292;&#24182;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#35823;&#24046;&#20998;&#26512;&#20197;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23376;&#20219;&#21153;-A&#21333;&#35821;&#27979;&#35797;&#38598;&#19978;&#33719;&#24471;86.9\%&#30340;&#20934;&#30830;&#29575;&#65292;&#23376;&#20219;&#21153;-B&#19978;&#20026;83.7\%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#26410;&#26469;&#30740;&#31350;&#20013;&#38656;&#35201;&#32771;&#34385;&#30340;&#25361;&#25112;&#21644;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16592v1 Announce Type: new  Abstract: The Large Language Models (LLMs) exhibit remarkable ability to generate fluent content across a wide spectrum of user queries. However, this capability has raised concerns regarding misinformation and personal information leakage. In this paper, we present our methods for the SemEval2024 Task8, aiming to detect machine-generated text across various domains in both mono-lingual and multi-lingual contexts. Our study comprehensively analyzes various methods to detect machine-generated text, including statistical, neural, and pre-trained model approaches. We also detail our experimental setup and perform a in-depth error analysis to evaluate the effectiveness of these methods. Our methods obtain an accuracy of 86.9\% on the test set of subtask-A mono and 83.7\% for subtask-B. Furthermore, we also highlight the challenges and essential factors for consideration in future studies.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#25552;&#28860;&#20013;&#20855;&#26377;&#29420;&#29305;&#20248;&#21183;&#65292;&#20294;&#22312;&#22788;&#29702;&#24773;&#24863;&#26102;&#20173;&#23384;&#22312;&#19968;&#23450;&#23616;&#38480;&#24615;&#65292;&#26080;&#35770;&#26159;&#23545;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#36824;&#26159;&#20154;&#31867;&#27880;&#37322;&#21592;&#32780;&#35328;&#12290;</title><link>https://arxiv.org/abs/2403.16584</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#25110;&#20154;&#31867;&#65289;&#26159;&#21542;&#33021;&#36827;&#34892;&#25991;&#26412;&#25552;&#28860;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models (or Humans) Distill Text?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16584
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#25552;&#28860;&#20013;&#20855;&#26377;&#29420;&#29305;&#20248;&#21183;&#65292;&#20294;&#22312;&#22788;&#29702;&#24773;&#24863;&#26102;&#20173;&#23384;&#22312;&#19968;&#23450;&#23616;&#38480;&#24615;&#65292;&#26080;&#35770;&#26159;&#23545;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#36824;&#26159;&#20154;&#31867;&#27880;&#37322;&#21592;&#32780;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#25552;&#28860;&#26041;&#38754;&#30340;&#28508;&#21147;&#65306;&#21363;&#21435;&#38500;&#19981;&#38656;&#35201;&#30340;&#31105;&#27490;&#21464;&#37327;&#30340;&#25991;&#26412;&#30165;&#36857;&#12290;&#25105;&#20204;&#21033;&#29992;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#30340;&#19968;&#31995;&#21015;LLMs&#26469;&#35782;&#21035;&#21644;&#21435;&#38500;&#20851;&#20110;&#30446;&#26631;&#21464;&#37327;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#20182;&#30456;&#20851;&#20449;&#21495;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#22312;&#22788;&#29702;&#25552;&#28860;&#20013;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#20026;&#22312;&#28041;&#21450;&#25991;&#26412;&#25968;&#25454;&#30340;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#35843;&#26597;&#20013;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#31574;&#30053;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;&#23588;&#20854;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#24378;&#28872;&#27979;&#35797;&#24773;&#24863;&#31227;&#38500;&#26102;&#65292;&#32463;&#36807;LLM&#25552;&#28860;&#30340;&#25991;&#26412;&#19982;&#24773;&#24863;&#20043;&#38388;&#30340;&#32479;&#35745;&#20851;&#32852;&#20173;&#28982;&#21487;&#20197;&#34987;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#28165;&#26224;&#22320;&#26816;&#27979;&#21040;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20154;&#31867;&#27880;&#37322;&#21592;&#22312;&#20445;&#30041;&#20854;&#20182;&#35821;&#20041;&#20869;&#23481;&#30340;&#21516;&#26102;&#65292;&#20063;&#24456;&#38590;&#25552;&#28860;&#20986;&#24773;&#24863;&#12290;&#36825;&#34920;&#26126;&#21487;&#33021;&#23384;&#22312;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16584v1 Announce Type: new  Abstract: We investigate the potential of large language models (LLMs) to distill text: to remove the textual traces of an undesired forbidden variable. We employ a range of LLMs with varying architectures and training approaches to distill text by identifying and removing information about the target variable while preserving other relevant signals. Our findings shed light on the strengths and limitations of LLMs in addressing the distillation and provide insights into the strategies for leveraging these models in computational social science investigations involving text data. In particular, we show that in the strong test of removing sentiment, the statistical association between the processed text and sentiment is still clearly detectable to machine learning classifiers post-LLM-distillation. Furthermore, we find that human annotators also struggle to distill sentiment while preserving other semantic content. This suggests there may be limited
&lt;/p&gt;</description></item><item><title>NSINA&#26159;&#20026;&#35299;&#20915;&#20711;&#20285;&#32599;&#35821;&#20013;LLMs&#36866;&#24212;&#24615;&#25361;&#25112;&#32780;&#24341;&#20837;&#30340;&#26368;&#22823;&#26032;&#38395;&#35821;&#26009;&#24211;&#65292;&#20026;&#25913;&#36827;&#35813;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25552;&#20379;&#20102;&#23453;&#36149;&#36164;&#28304;&#21644;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2403.16571</link><description>&lt;p&gt;
NSINA&#65306;&#29992;&#20110;&#20711;&#20285;&#32599;&#35821;&#30340;&#26032;&#38395;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
NSINA: A News Corpus for Sinhala
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16571
&lt;/p&gt;
&lt;p&gt;
NSINA&#26159;&#20026;&#35299;&#20915;&#20711;&#20285;&#32599;&#35821;&#20013;LLMs&#36866;&#24212;&#24615;&#25361;&#25112;&#32780;&#24341;&#20837;&#30340;&#26368;&#22823;&#26032;&#38395;&#35821;&#26009;&#24211;&#65292;&#20026;&#25913;&#36827;&#35813;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25552;&#20379;&#20102;&#23453;&#36149;&#36164;&#28304;&#21644;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24341;&#20837;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#21457;&#23637;&#65292;&#20294;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#39044;&#35757;&#32451;&#36164;&#28304;&#12290;&#23588;&#20854;&#26159;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;&#22914;&#20711;&#20285;&#32599;&#35821;&#65289;&#20013;&#65292;&#36825;&#19968;&#28857;&#23588;&#20026;&#26126;&#26174;&#65292;&#22240;&#20026;&#23427;&#20204;&#38754;&#20020;&#30528;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#32570;&#20047;&#20805;&#36275;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#26377;&#38480;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#20026;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;NSINA&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#25324;&#26469;&#33258;&#28909;&#38376;&#20711;&#20285;&#32599;&#35821;&#26032;&#38395;&#32593;&#31449;&#30340;50&#19975;&#22810;&#31687;&#25991;&#31456;&#30340;&#20840;&#38754;&#26032;&#38395;&#35821;&#26009;&#24211;&#65292;&#20197;&#21450;&#19977;&#39033;NLP&#20219;&#21153;&#65306;&#26032;&#38395;&#23186;&#20307;&#35782;&#21035;&#12289;&#26032;&#38395;&#31867;&#21035;&#39044;&#27979;&#21644;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#12290;NSINA&#30340;&#21457;&#24067;&#26088;&#22312;&#20026;&#36866;&#24212;&#20711;&#20285;&#32599;&#35821;&#30340;LLMs&#24102;&#26469;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#21644;&#29992;&#20110;&#25913;&#36827;&#20711;&#20285;&#32599;&#35821;NLP&#30340;&#22522;&#20934;&#12290;NSINA&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#20711;&#20285;&#32599;&#35821;&#26032;&#38395;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16571v1 Announce Type: cross  Abstract: The introduction of large language models (LLMs) has advanced natural language processing (NLP), but their effectiveness is largely dependent on pre-training resources. This is especially evident in low-resource languages, such as Sinhala, which face two primary challenges: the lack of substantial training data and limited benchmarking datasets. In response, this study introduces NSINA, a comprehensive news corpus of over 500,000 articles from popular Sinhala news websites, along with three NLP tasks: news media identification, news category prediction, and news headline generation. The release of NSINA aims to provide a solution to challenges in adapting LLMs to Sinhala, offering valuable resources and benchmarks for improving NLP in the Sinhala language. NSINA is the largest news corpus for Sinhala, available up to date.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Poincar&#233;&#35299;&#37322;&#26041;&#27861;&#22312;&#36229;&#20960;&#20309;&#31354;&#38388;&#20013;&#24314;&#27169;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;O(n^2logn)&#30340;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#22312;&#25237;&#24433;&#31354;&#38388;&#20013;&#36827;&#34892;&#30340;&#23618;&#27425;&#32858;&#31867;&#36807;&#31243;&#21487;&#20197;&#35270;&#20026;&#26500;&#24314;&#26368;&#23567;&#29983;&#25104;&#26641;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#26377;&#25928;&#30340;&#31639;&#27861;</title><link>https://arxiv.org/abs/2403.16554</link><description>&lt;p&gt;
PE&#65306;&#19968;&#31181;&#29992;&#20110;&#24555;&#36895;&#25991;&#26412;&#23618;&#27425;&#29983;&#25104;&#30340;Poincar&#233;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PE: A Poincare Explanation Method for Fast Text Hierarchy Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16554
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Poincar&#233;&#35299;&#37322;&#26041;&#27861;&#22312;&#36229;&#20960;&#20309;&#31354;&#38388;&#20013;&#24314;&#27169;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;O(n^2logn)&#30340;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#22312;&#25237;&#24433;&#31354;&#38388;&#20013;&#36827;&#34892;&#30340;&#23618;&#27425;&#32858;&#31867;&#36807;&#31243;&#21487;&#20197;&#35270;&#20026;&#26500;&#24314;&#26368;&#23567;&#29983;&#25104;&#26641;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#26377;&#25928;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16554v1 &#20844;&#21578;&#31867;&#22411;: cross &#25688;&#35201;: NLP&#20013;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#40657;&#30418;&#29305;&#24615;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#30740;&#31350;&#37325;&#28857;&#24050;&#32463;&#36716;&#31227;&#21040;&#23618;&#27425;&#23646;&#24615;&#65288;HA&#65289;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#24314;&#27169;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#32791;&#26102;&#30340;&#36138;&#23146;&#25628;&#32034;&#26469;&#24314;&#27169;&#38750;&#36830;&#32493;&#32452;&#21512;&#65292;&#24573;&#30053;&#20102;&#29305;&#24449;&#34920;&#31034;&#20013;&#28508;&#22312;&#30340;&#35821;&#35328;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;Poincar&#233;&#35299;&#37322;&#65288;PE&#65289;&#65292;&#29992;&#20110;&#20351;&#29992;&#36229;&#20960;&#20309;&#31354;&#38388;&#24314;&#27169;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#65292;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;$O(n^2logn)$&#12290;&#21463;Poincar&#233;&#27169;&#22411;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#23884;&#20837;&#25237;&#24433;&#21040;&#36229;&#20960;&#20309;&#31354;&#38388;&#20013;&#65292;&#36825;&#23637;&#31034;&#20986;&#26356;&#22909;&#30340;&#23545;&#21477;&#27861;&#21644;&#35821;&#20041;&#23618;&#27425;&#32467;&#26500;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25237;&#24433;&#31354;&#38388;&#20013;&#30340;&#23618;&#27425;&#32858;&#31867;&#36807;&#31243;&#21487;&#20197;&#34987;&#35270;&#20026;&#26500;&#24314;&#26368;&#23567;&#29983;&#25104;&#26641;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#26377;&#25928;&#30340;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16554v1 Announce Type: cross  Abstract: The black-box nature of deep learning models in NLP hinders their widespread application. The research focus has shifted to Hierarchical Attribution (HA) for its ability to model feature interactions. Recent works model non-contiguous combinations with a time-costly greedy search in Eculidean spaces, neglecting underlying linguistic information in feature representations. In this work, we introduce a novel method, namely Poincar\'e Explanation (PE), for modeling feature interactions using hyperbolic spaces in an $O(n^2logn)$ time complexity. Inspired by Poincar\'e model, we propose a framework to project the embeddings into hyperbolic spaces, which exhibit better inductive biases for syntax and semantic hierarchical structures. Eventually, we prove that the hierarchical clustering process in the projected space could be viewed as building a minimum spanning tree and propose a time efficient algorithm. Experimental results demonstrate t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#20174;&#22810;&#20010;&#21477;&#23376;&#34920;&#31034;&#20013;&#25552;&#21462;&#20114;&#34917;&#30340;&#21028;&#21035;&#20449;&#24687;&#65292;&#25552;&#39640;&#23569;&#26679;&#26412;&#20851;&#31995;&#20998;&#31867;&#20013;&#30340;&#20449;&#24687;&#25552;&#21462;&#25928;&#29575;</title><link>https://arxiv.org/abs/2403.16543</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#25552;&#39640;&#23569;&#26679;&#26412;&#20851;&#31995;&#20998;&#31867;&#20013;&#30340;&#39640;&#25928;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Efficient Information Extraction in Few-Shot Relation Classification through Contrastive Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16543
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#20174;&#22810;&#20010;&#21477;&#23376;&#34920;&#31034;&#20013;&#25552;&#21462;&#20114;&#34917;&#30340;&#21028;&#21035;&#20449;&#24687;&#65292;&#25552;&#39640;&#23569;&#26679;&#26412;&#20851;&#31995;&#20998;&#31867;&#20013;&#30340;&#20449;&#24687;&#25552;&#21462;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#26631;&#35760;&#23454;&#20363;&#19979;&#30340;&#20851;&#31995;&#20998;&#31867;&#20013;&#21306;&#20998;&#23454;&#20307;&#23545;&#20043;&#38388;&#30340;&#20851;&#31995;&#26500;&#25104;&#23569;&#26679;&#26412;&#20851;&#31995;&#20998;&#31867;&#20013;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#25991;&#26412;&#25968;&#25454;&#30340;&#34920;&#31034;&#25552;&#21462;&#20102;&#36328;&#39046;&#22495;&#12289;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#22810;&#20010;&#21477;&#23376;&#34920;&#31034;&#21644;&#23545;&#27604;&#23398;&#20064;&#20197;&#22686;&#24378;&#20449;&#24687;&#25552;&#21462;&#30340;&#26032;&#26041;&#27861;&#12290;&#23613;&#31649;&#20851;&#31995;&#20998;&#31867;&#20013;&#36890;&#24120;&#20351;&#29992;&#23454;&#20307;&#26631;&#35760;&#20196;&#29260;&#25552;&#21462;&#34920;&#31034;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#27169;&#22411;&#20869;&#37096;&#34920;&#31034;&#20013;&#23384;&#22312;&#22823;&#37327;&#26410;&#34987;&#21033;&#29992;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#40784;&#22810;&#20010;&#21477;&#23376;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#22914;[CLS]&#20196;&#29260;&#12289;&#25552;&#31034;&#20013;&#20351;&#29992;&#30340;[MASK]&#20196;&#29260;&#21644;&#23454;&#20307;&#26631;&#35760;&#20196;&#29260;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#20174;&#36825;&#20123;&#20010;&#20307;&#34920;&#31034;&#20013;&#25552;&#21462;&#20114;&#34917;&#30340;&#21028;&#21035;&#20449;&#24687;&#12290;&#36825;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#29305;&#21035;&#30456;&#20851;&#65292;&#20854;&#20013;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16543v1 Announce Type: cross  Abstract: Differentiating relationships between entity pairs with limited labeled instances poses a significant challenge in few-shot relation classification. Representations of textual data extract rich information spanning the domain, entities, and relations. In this paper, we introduce a novel approach to enhance information extraction combining multiple sentence representations and contrastive learning. While representations in relation classification are commonly extracted using entity marker tokens, we argue that substantial information within the internal model representations remains untapped. To address this, we propose aligning multiple sentence representations, such as the [CLS] token, the [MASK] token used in prompting, and entity marker tokens. Our method employs contrastive learning to extract complementary discriminative information from these individual representations. This is particularly relevant in low-resource settings where
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;&#26088;&#22312;&#22635;&#34917;&#29616;&#26377;&#35268;&#21010;&#32773;&#32570;&#23569;&#30340;&#24120;&#35782;&#25512;&#29702;&#65292;&#20197;&#36866;&#29992;&#20110;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#30340;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2403.16527</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;&#65306;&#28789;&#27963;&#23450;&#20041;&#19982;&#29616;&#26377;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16527
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;&#26088;&#22312;&#22635;&#34917;&#29616;&#26377;&#35268;&#21010;&#32773;&#32570;&#23569;&#30340;&#24120;&#35782;&#25512;&#29702;&#65292;&#20197;&#36866;&#29992;&#20110;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#31995;&#32479;&#21363;&#23558;&#26080;&#22788;&#19981;&#22312;&#65292;&#20174;&#21046;&#36896;&#19994;&#30340;&#33258;&#20027;&#24615;&#21040;&#20892;&#19994;&#39046;&#22495;&#30340;&#26426;&#22120;&#20154;&#65292;&#20174;&#21307;&#30103;&#21161;&#29702;&#21040;&#23089;&#20048;&#20135;&#19994;&#12290;&#22823;&#22810;&#25968;&#31995;&#32479;&#26159;&#36890;&#36807;&#27169;&#22359;&#21270;&#30340;&#23376;&#32452;&#20214;&#24320;&#21457;&#30340;&#65292;&#29992;&#20110;&#20915;&#31574;&#12289;&#35268;&#21010;&#21644;&#25511;&#21046;&#65292;&#36825;&#20123;&#32452;&#20214;&#21487;&#33021;&#26159;&#25163;&#24037;&#35774;&#35745;&#30340;&#65292;&#20063;&#21487;&#33021;&#26159;&#22522;&#20110;&#23398;&#20064;&#30340;&#12290;&#34429;&#28982;&#29616;&#26377;&#26041;&#27861;&#22312;&#23427;&#20204;&#19987;&#38376;&#35774;&#35745;&#30340;&#24773;&#22659;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#27979;&#35797;&#26102;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#22312;&#32597;&#35265;&#30340;&#12289;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#22330;&#26223;&#19979;&#34920;&#29616;&#29305;&#21035;&#24046;&#12290;&#22522;&#20110;&#22810;&#20010;&#20219;&#21153;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#20197;&#21450;&#20174;&#21508;&#20010;&#39046;&#22495;&#37319;&#38598;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#30456;&#20449;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#25552;&#20379;&#29616;&#26377;&#35268;&#21010;&#32773;&#25152;&#32570;&#20047;&#30340;&#24120;&#35782;&#25512;&#29702;&#12290;&#30740;&#31350;&#20154;&#21592;&#35748;&#20026;&#65292;&#36825;&#31181;&#24120;&#35782;&#25512;&#29702;&#23558;&#24357;&#21512;&#31639;&#27861;&#24320;&#21457;&#21644;&#37096;&#32626;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#36866;&#29992;&#20110;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16527v1 Announce Type: new  Abstract: Autonomous systems are soon to be ubiquitous, from manufacturing autonomy to agricultural field robots, and from health care assistants to the entertainment industry. The majority of these systems are developed with modular sub-components for decision-making, planning, and control that may be hand-engineered or learning-based. While these existing approaches have been shown to perform well under the situations they were specifically designed for, they can perform especially poorly in rare, out-of-distribution scenarios that will undoubtedly arise at test-time. The rise of foundation models trained on multiple tasks with impressively large datasets from a variety of fields has led researchers to believe that these models may provide common sense reasoning that existing planners are missing. Researchers posit that this common sense reasoning will bridge the gap between algorithm development and deployment to out-of-distribution tasks, like
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ViTLP&#30340;&#21487;&#35270;&#24341;&#23548;&#30340;&#29983;&#25104;&#25991;&#26412;&#24067;&#23616;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#38271;&#24230;&#30340;&#35789;&#27719;&#23494;&#38598;&#22411;&#25991;&#26723;&#65292;&#24182;&#19988;&#21487;&#20197;&#20316;&#20026;OCR&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#23450;&#20301;&#21644;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.16516</link><description>&lt;p&gt;
&#21487;&#35270;&#24341;&#23548;&#30340;&#29983;&#25104;&#24335;&#25991;&#26412;&#24067;&#23616;&#39044;&#35757;&#32451;&#29992;&#20110;&#25991;&#26723;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Visually Guided Generative Text-Layout Pre-training for Document Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16516
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ViTLP&#30340;&#21487;&#35270;&#24341;&#23548;&#30340;&#29983;&#25104;&#25991;&#26412;&#24067;&#23616;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#38271;&#24230;&#30340;&#35789;&#27719;&#23494;&#38598;&#22411;&#25991;&#26723;&#65292;&#24182;&#19988;&#21487;&#20197;&#20316;&#20026;OCR&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#23450;&#20301;&#21644;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#25216;&#26415;&#21487;&#20197;&#25552;&#21319;&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;&#65288;VDU&#65289;&#30340;&#24615;&#33021;&#65292;&#36890;&#24120;&#38656;&#35201;&#27169;&#22411;&#33719;&#24471;&#24863;&#30693;&#21644;&#25512;&#29702;&#25991;&#26723;&#25991;&#26412;&#21644;&#24067;&#23616;&#65288;&#20363;&#22914;&#25991;&#26412;&#21644;&#34920;&#26684;&#21333;&#20803;&#30340;&#20301;&#32622;&#65289;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ViTLP&#30340;&#21487;&#35270;&#24341;&#23548;&#30340;&#29983;&#25104;&#25991;&#26412;&#24067;&#23616;&#39044;&#35757;&#32451;&#25216;&#26415;&#12290;&#32473;&#23450;&#19968;&#20010;&#25991;&#26723;&#22270;&#20687;&#65292;&#35813;&#27169;&#22411;&#20248;&#21270;&#20998;&#23618;&#35821;&#35328;&#21644;&#24067;&#23616;&#24314;&#27169;&#30446;&#26631;&#65292;&#20197;&#29983;&#25104;&#20132;&#38169;&#30340;&#25991;&#26412;&#21644;&#24067;&#23616;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;Transformers&#22788;&#29702;&#38271;&#25991;&#26723;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22810;&#27573;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#20351;ViTLP&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#38271;&#24230;&#30340;&#35789;&#27719;&#23494;&#38598;&#22411;&#25991;&#26723;&#12290;ViTLP&#21487;&#20197;&#20316;&#20026;&#26412;&#22320;OCR&#27169;&#22411;&#65292;&#29992;&#20110;&#23450;&#20301;&#21644;&#35782;&#21035;&#25991;&#26723;&#22270;&#20687;&#20013;&#30340;&#25991;&#26412;&#12290;&#27492;&#22806;&#65292;ViTLP&#21487;&#20197;&#26377;&#25928;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;VDU&#20219;&#21153;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;ViTLP&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16516v1 Announce Type: new  Abstract: Prior study shows that pre-training techniques can boost the performance of visual document understanding (VDU), which typically requires models to gain abilities to perceive and reason both document texts and layouts (e.g., locations of texts and table-cells). To this end, we propose visually guided generative text-layout pre-training, named ViTLP. Given a document image, the model optimizes hierarchical language and layout modeling objectives to generate the interleaved text and layout sequence. In addition, to address the limitation of processing long documents by Transformers, we introduce a straightforward yet effective multi-segment generative pre-training scheme, facilitating ViTLP to process word-intensive documents of any length. ViTLP can function as a native OCR model to localize and recognize texts of document images. Besides, ViTLP can be effectively applied to various downstream VDU tasks. Extensive experiments show that Vi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;25&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;7&#31181;&#30456;&#23545;&#36739;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#21450;&#20854;&#36328;&#35821;&#35328;&#21464;&#20307;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20351;&#29992;LLMs&#36827;&#34892;ICL&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#26367;&#20195;&#26041;&#27861;&#26597;&#35810;&#23545;&#40784;&#65292;&#24182;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#25552;&#20379;&#20102;&#23453;&#36149;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.16512</link><description>&lt;p&gt;
LLMs&#26159;&#23569;&#26679;&#26412;&#24773;&#22659;&#20302;&#36164;&#28304;&#35821;&#35328;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
LLMs Are Few-Shot In-Context Low-Resource Language Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16512
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;25&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;7&#31181;&#30456;&#23545;&#36739;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#21450;&#20854;&#36328;&#35821;&#35328;&#21464;&#20307;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20351;&#29992;LLMs&#36827;&#34892;ICL&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#26367;&#20195;&#26041;&#27861;&#26597;&#35810;&#23545;&#40784;&#65292;&#24182;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#25552;&#20379;&#20102;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#25903;&#25345;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#21033;&#29992;&#30701;&#26102;&#30340;&#24773;&#22659;&#20449;&#24687;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#65292;&#36825;&#20026;&#32553;&#23567;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#24046;&#36317;&#25552;&#20379;&#20102;&#37325;&#35201;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#25506;&#35752;&#20102;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#38598;&#20013;&#22312;&#30456;&#23545;&#39640;&#36164;&#28304;&#30340;&#35821;&#35328;&#65292;&#27604;&#22914;&#27861;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;25&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;7&#31181;&#30456;&#23545;&#36739;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;ICL&#21450;&#20854;&#36328;&#35821;&#35328;&#21464;&#20307;&#65288;X-ICL&#65289;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#19981;&#20165;&#35780;&#20272;&#20102;LLMs&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20351;&#29992;ICL&#30340;&#26377;&#25928;&#24615;&#65292;&#36824;&#21457;&#29616;&#20102;&#24773;&#22659;&#26631;&#31614;&#23545;&#40784;&#30340;&#32570;&#38519;&#65292;&#24182;&#24341;&#20837;&#20102;&#26356;&#26377;&#25928;&#30340;&#26367;&#20195;&#26041;&#27861;&#65306;&#26597;&#35810;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#30340;&#21508;&#20010;&#26041;&#38754;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24635;&#32467;&#20102;&#23569;&#26679;&#26412;&#24773;&#22659;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16512v1 Announce Type: cross  Abstract: In-context learning (ICL) empowers large language models (LLMs) to perform diverse tasks in underrepresented languages using only short in-context information, offering a crucial avenue for narrowing the gap between high-resource and low-resource languages. Nonetheless, there is only a handful of works explored ICL for low-resource languages with most of them focusing on relatively high-resource languages, such as French and Spanish. In this work, we extensively study ICL and its cross-lingual variation (X-ICL) on 25 low-resource and 7 relatively higher-resource languages. Our study not only assesses the effectiveness of ICL with LLMs in low-resource languages but also identifies the shortcomings of in-context label alignment, and introduces a more effective alternative: query alignment. Moreover, we provide valuable insights into various facets of ICL for low-resource languages. Our study concludes the significance of few-shot in-cont
&lt;/p&gt;</description></item><item><title>LARA&#26159;&#19968;&#20010;Linguistic-Adaptive Retrieval-Augmented Language Models&#65288;&#35821;&#35328;&#33258;&#36866;&#24212;&#26816;&#32034;&#22686;&#24378;LLMs&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#24494;&#35843;&#36807;&#30340;&#36739;&#23567;&#27169;&#22411;&#19982;&#26816;&#32034;&#22686;&#24378;&#26426;&#21046;&#26469;&#25552;&#39640;&#22810;&#35821;&#35328;&#22810;&#36718;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#35805;&#32972;&#26223;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.16504</link><description>&lt;p&gt;
LARA&#65306;&#35821;&#35328;&#33258;&#36866;&#24212;&#26816;&#32034;&#22686;&#24378;LLMs&#29992;&#20110;&#22810;&#36718;&#24847;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16504
&lt;/p&gt;
&lt;p&gt;
LARA&#26159;&#19968;&#20010;Linguistic-Adaptive Retrieval-Augmented Language Models&#65288;&#35821;&#35328;&#33258;&#36866;&#24212;&#26816;&#32034;&#22686;&#24378;LLMs&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#24494;&#35843;&#36807;&#30340;&#36739;&#23567;&#27169;&#22411;&#19982;&#26816;&#32034;&#22686;&#24378;&#26426;&#21046;&#26469;&#25552;&#39640;&#22810;&#35821;&#35328;&#22810;&#36718;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#35805;&#32972;&#26223;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21462;&#24471;&#30340;&#26174;&#33879;&#25104;&#23601;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#37319;&#29992;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20391;&#37325;&#20110;&#21333;&#35821;&#35328;&#12289;&#21333;&#36718;&#20998;&#31867;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LARA&#65288;Linguistic-Adaptive Retrieval-Augmented Language Models&#65289;&#65292;&#26088;&#22312;&#22686;&#24378;&#22810;&#35821;&#35328;&#22810;&#36718;&#20998;&#31867;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#36866;&#24212;&#32842;&#22825;&#26426;&#22120;&#20154;&#20132;&#20114;&#20013;&#30340;&#20247;&#22810;&#24847;&#22270;&#12290;&#30001;&#20110;&#20250;&#35805;&#32972;&#26223;&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#26029;&#21457;&#23637;&#30340;&#24615;&#36136;&#65292;&#22810;&#36718;&#24847;&#22270;&#20998;&#31867;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;LARA&#36890;&#36807;&#23558;&#24494;&#35843;&#36807;&#30340;&#36739;&#23567;&#27169;&#22411;&#19982;&#26816;&#32034;&#22686;&#24378;&#26426;&#21046;&#32467;&#21512;&#65292;&#23884;&#20837;LLMs&#30340;&#26550;&#26500;&#20013;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36825;&#31181;&#25972;&#21512;&#20351;LARA&#33021;&#22815;&#21160;&#24577;&#21033;&#29992;&#36807;&#21435;&#30340;&#23545;&#35805;&#21644;&#30456;&#20851;&#24847;&#22270;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#26816;&#32034;&#25216;&#26415;&#22686;&#24378;&#20102;&#36328;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16504v1 Announce Type: new  Abstract: Following the significant achievements of large language models (LLMs), researchers have employed in-context learning for text classification tasks. However, these studies focused on monolingual, single-turn classification tasks. In this paper, we introduce LARA (Linguistic-Adaptive Retrieval-Augmented Language Models), designed to enhance accuracy in multi-turn classification tasks across six languages, accommodating numerous intents in chatbot interactions. Multi-turn intent classification is notably challenging due to the complexity and evolving nature of conversational contexts. LARA tackles these issues by combining a fine-tuned smaller model with a retrieval-augmented mechanism, integrated within the architecture of LLMs. This integration allows LARA to dynamically utilize past dialogues and relevant intents, thereby improving the understanding of the context. Furthermore, our adaptive retrieval techniques bolster the cross-lingual
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#36229;&#38142;&#25509;&#33258;&#21160;&#26500;&#24314;&#29992;&#20110;&#22320;&#29702;&#35299;&#26512;&#30340;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#30340;&#26032;&#26041;&#27861;&#65292;&#26500;&#24314;&#20102;&#21253;&#21547;130&#19975;&#31687;&#25991;&#31456;&#30340;WHLL&#35821;&#26009;&#24211;&#65292;&#20026;&#22320;&#29702;&#35299;&#26512;&#39046;&#22495;&#24102;&#26469;&#20102;&#26032;&#30340;&#25968;&#25454;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2403.16483</link><description>&lt;p&gt;
&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#36229;&#38142;&#25509;&#33258;&#21160;&#26500;&#24314;&#29992;&#20110;&#22320;&#29702;&#35299;&#26512;&#30340;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
Automatic Construction of a Large-Scale Corpus for Geoparsing Using Wikipedia Hyperlinks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#36229;&#38142;&#25509;&#33258;&#21160;&#26500;&#24314;&#29992;&#20110;&#22320;&#29702;&#35299;&#26512;&#30340;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#30340;&#26032;&#26041;&#27861;&#65292;&#26500;&#24314;&#20102;&#21253;&#21547;130&#19975;&#31687;&#25991;&#31456;&#30340;WHLL&#35821;&#26009;&#24211;&#65292;&#20026;&#22320;&#29702;&#35299;&#26512;&#39046;&#22495;&#24102;&#26469;&#20102;&#26032;&#30340;&#25968;&#25454;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29702;&#35299;&#26512;&#26159;&#23545;&#25991;&#26412;&#20013;&#30340;&#20301;&#32622;&#34920;&#36798;&#36827;&#34892;&#32463;&#32428;&#24230;&#65288;&#22352;&#26631;&#65289;&#20272;&#35745;&#30340;&#20219;&#21153;&#12290;&#22320;&#29702;&#35299;&#26512;&#24517;&#39035;&#22788;&#29702;&#25351;&#31034;&#20855;&#26377;&#30456;&#21516;&#31526;&#21495;&#30340;&#22810;&#20010;&#20301;&#32622;&#30340;&#34920;&#36798;&#30340;&#27495;&#20041;&#12290;&#20026;&#20102;&#35780;&#20272;&#22320;&#29702;&#35299;&#26512;&#31995;&#32479;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20960;&#20010;&#35821;&#26009;&#24211;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35821;&#26009;&#24211;&#35268;&#27169;&#36739;&#23567;&#65292;&#24182;&#19988;&#21463;&#38480;&#20110;&#19968;&#33324;&#39046;&#22495;&#30340;&#20301;&#32622;&#34920;&#36798;&#35206;&#30422;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#32500;&#22522;&#30334;&#31185;&#36229;&#38142;&#25509;&#30340;&#20301;&#32622;&#38142;&#25509;&#65288;WHLL&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#26500;&#24314;&#29992;&#20110;&#22320;&#29702;&#35299;&#26512;&#30340;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#30340;&#26032;&#26041;&#27861;&#12290;WHLL&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#20013;&#30340;&#36229;&#38142;&#25509;&#26469;&#29992;&#22352;&#26631;&#27880;&#37322;&#22810;&#20010;&#20301;&#32622;&#34920;&#36798;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;WHLL&#35821;&#26009;&#24211;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#22320;&#29702;&#35299;&#26512;&#30340;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#12290;WHLL&#35821;&#26009;&#24211;&#21253;&#21547;130&#19975;&#31687;&#25991;&#31456;&#65292;&#27599;&#31687;&#32422;&#21253;&#21547;7.8&#20010;&#29420;&#29305;&#30340;&#20301;&#32622;&#34920;&#36798;&#12290;45.6%&#30340;&#20301;&#32622;&#34920;&#36798;&#23384;&#22312;&#27495;&#20041;&#65292;&#24182;&#25351;&#31034;&#22810;&#20010;&#22320;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16483v1 Announce Type: new  Abstract: Geoparsing is the task of estimating the latitude and longitude (coordinates) of location expressions in texts. Geoparsing must deal with the ambiguity of the expressions that indicate multiple locations with the same notation. For evaluating geoparsing systems, several corpora have been proposed in previous work. However, these corpora are small-scale and suffer from the coverage of location expressions on general domains. In this paper, we propose Wikipedia Hyperlink-based Location Linking (WHLL), a novel method to construct a large-scale corpus for geoparsing from Wikipedia articles. WHLL leverages hyperlinks in Wikipedia to annotate multiple location expressions with coordinates. With this method, we constructed the WHLL corpus, a new large-scale corpus for geoparsing. The WHLL corpus consists of 1.3M articles, each containing about 7.8 unique location expressions. 45.6% of location expressions are ambiguous and refer to more than on
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Superposition Concept Discriminator&#65288;SuperCD&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#23569;&#26679;&#26412;NER&#20013;&#31934;&#30830;&#27867;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#20174;&#31034;&#20363;&#23454;&#20363;&#20013;&#35782;&#21035;&#21472;&#21152;&#27010;&#24565;&#24182;&#26816;&#32034;&#30456;&#24212;&#23454;&#20363;&#26469;&#26631;&#27880;&#65292;&#20197;&#35299;&#20915;&#20449;&#24687;&#19981;&#36275;&#23548;&#33268;&#30340;&#27169;&#31946;&#24615;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.16463</link><description>&lt;p&gt;
&#36890;&#36807;&#21472;&#21152;&#27010;&#24565;&#21028;&#21035;&#36827;&#34892;&#30340;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Few-shot Named Entity Recognition via Superposition Concept Discrimination
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16463
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Superposition Concept Discriminator&#65288;SuperCD&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#23569;&#26679;&#26412;NER&#20013;&#31934;&#30830;&#27867;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#20174;&#31034;&#20363;&#23454;&#20363;&#20013;&#35782;&#21035;&#21472;&#21152;&#27010;&#24565;&#24182;&#26816;&#32034;&#30456;&#24212;&#23454;&#20363;&#26469;&#26631;&#27880;&#65292;&#20197;&#35299;&#20915;&#20449;&#24687;&#19981;&#36275;&#23548;&#33268;&#30340;&#27169;&#31946;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;NER&#26088;&#22312;&#20165;&#20855;&#26377;&#26377;&#38480;&#25968;&#37327;&#30340;&#23454;&#20363;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#30446;&#26631;&#31867;&#22411;&#30340;&#23454;&#20307;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23569;&#26679;&#26412;NER&#21463;&#21040;&#20869;&#22312;&#31934;&#30830;&#27867;&#21270;&#38382;&#39064;&#30340;&#20005;&#37325;&#25361;&#25112;&#65292;&#21363;&#30001;&#20110;&#20449;&#24687;&#19981;&#36275;&#23548;&#33268;&#30340;&#27169;&#31946;&#24615;&#65292;&#38590;&#20197;&#20934;&#30830;&#30830;&#23450;&#25152;&#38656;&#30340;&#30446;&#26631;&#31867;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36229;&#32423;&#27010;&#24565;&#21028;&#21035;&#22120;&#65288;SuperCD&#65289;&#65292;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39318;&#20808;&#24341;&#20837;&#27010;&#24565;&#25552;&#21462;&#22120;&#26469;&#20174;&#31034;&#20363;&#23454;&#20363;&#20013;&#35782;&#21035;&#21472;&#21152;&#27010;&#24565;&#65292;&#27599;&#20010;&#27010;&#24565;&#23545;&#24212;&#19968;&#20010;&#21487;&#33021;&#30340;&#27867;&#21270;&#36793;&#30028;&#12290;&#28982;&#21518;&#65292;&#24212;&#29992;&#21472;&#21152;&#23454;&#20363;&#26816;&#32034;&#22120;&#20174;&#22823;&#35268;&#27169;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#36825;&#20123;&#21472;&#21152;&#27010;&#24565;&#30340;&#30456;&#24212;&#23454;&#20363;&#12290;&#26368;&#21518;&#65292;&#35201;&#27714;&#27880;&#37322;&#21592;&#23545;&#26816;&#32034;&#21040;&#30340;&#23454;&#20363;&#36827;&#34892;&#27880;&#37322;&#65292;&#36825;&#20123;&#24102;&#27880;&#37322;&#30340;&#23454;&#20363;&#19982;&#21407;&#22987;&#31034;&#20363;&#23454;&#20363;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16463v1 Announce Type: new  Abstract: Few-shot NER aims to identify entities of target types with only limited number of illustrative instances. Unfortunately, few-shot NER is severely challenged by the intrinsic precise generalization problem, i.e., it is hard to accurately determine the desired target type due to the ambiguity stemming from information deficiency. In this paper, we propose Superposition Concept Discriminator (SuperCD), which resolves the above challenge via an active learning paradigm. Specifically, a concept extractor is first introduced to identify superposition concepts from illustrative instances, with each concept corresponding to a possible generalization boundary. Then a superposition instance retriever is applied to retrieve corresponding instances of these superposition concepts from large-scale text corpus. Finally, annotators are asked to annotate the retrieved instances and these annotated instances together with original illustrative instances
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;BERT&#27169;&#22411;&#20013;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#22914;&#20309;&#26681;&#25454;&#35789;&#27719;&#31867;&#21035;&#30340;&#19981;&#21516;&#32780;&#21464;&#21270;&#65292;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#19979;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#20219;&#21153;&#20013;&#65292;&#35777;&#23454;&#22312;&#24378;&#35843;&#35821;&#20041;&#20449;&#24687;&#30340;&#20219;&#21153;&#20013;&#65292;&#27880;&#24847;&#21147;&#20027;&#35201;&#38598;&#20013;&#20110;&#20869;&#23481;&#35789;&#65292;&#32780;&#22312;&#24378;&#35843;&#21477;&#27861;&#20449;&#24687;&#30340;&#20219;&#21153;&#20013;&#65292;&#27880;&#24847;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#21151;&#33021;&#35789;&#19978;</title><link>https://arxiv.org/abs/2403.16447</link><description>&lt;p&gt;
&#30740;&#31350;BERT&#27169;&#22411;&#20013;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#22914;&#20309;&#24863;&#30693;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#20219;&#21153;&#20013;&#30340;&#35789;&#27719;&#31867;&#21035;
&lt;/p&gt;
&lt;p&gt;
A Study on How Attention Scores in the BERT Model are Aware of Lexical Categories in Syntactic and Semantic Tasks on the GLUE Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;BERT&#27169;&#22411;&#20013;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#22914;&#20309;&#26681;&#25454;&#35789;&#27719;&#31867;&#21035;&#30340;&#19981;&#21516;&#32780;&#21464;&#21270;&#65292;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#19979;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#20219;&#21153;&#20013;&#65292;&#35777;&#23454;&#22312;&#24378;&#35843;&#35821;&#20041;&#20449;&#24687;&#30340;&#20219;&#21153;&#20013;&#65292;&#27880;&#24847;&#21147;&#20027;&#35201;&#38598;&#20013;&#20110;&#20869;&#23481;&#35789;&#65292;&#32780;&#22312;&#24378;&#35843;&#21477;&#27861;&#20449;&#24687;&#30340;&#20219;&#21153;&#20013;&#65292;&#27880;&#24847;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#21151;&#33021;&#35789;&#19978;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26816;&#26597;&#20102;&#22312;BERT&#27169;&#22411;&#20013;&#65292;token&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#26159;&#21542;&#26681;&#25454;&#35789;&#27719;&#31867;&#21035;&#26174;&#30528;&#21464;&#21270;&#12290;&#21463;&#21040;&#20154;&#31867;&#35821;&#35328;&#22788;&#29702;&#20013;&#21477;&#27861;&#21644;&#35821;&#20041;&#20449;&#24687;&#34987;&#19981;&#21516;&#35299;&#26512;&#30340;&#27010;&#24565;&#21551;&#21457;&#65292;&#25105;&#20204;&#26681;&#25454;&#20854;&#35789;&#27719;&#31867;&#21035;&#23545;&#21477;&#23376;&#20013;&#30340;token&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20851;&#27880;&#36825;&#20123;&#31867;&#21035;&#20043;&#38388;&#27880;&#24847;&#21147;&#20998;&#25968;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#35748;&#20026;&#65292;&#22312;&#27880;&#37325;&#35821;&#20041;&#20449;&#24687;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#20197;&#20869;&#23481;&#35789;&#20026;&#20013;&#24515;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#20250;&#22686;&#24378;&#65292;&#32780;&#22312;&#24378;&#35843;&#21477;&#27861;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#21151;&#33021;&#35789;&#20026;&#20013;&#24515;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#20250;&#22686;&#24378;&#12290;&#36890;&#36807;&#23545;GLUE&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;&#20845;&#20010;&#20219;&#21153;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#23454;&#20102;&#20851;&#20110;&#24494;&#35843;&#36807;&#31243;&#30340;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20854;&#20182;&#35843;&#26597;&#25581;&#31034;&#20102;BERT&#23618;&#20250;&#19968;&#33268;&#20998;&#37197; m
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16447v1 Announce Type: new  Abstract: This study examines whether the attention scores between tokens in the BERT model significantly vary based on lexical categories during the fine-tuning process for downstream tasks. Drawing inspiration from the notion that in human language processing, syntactic and semantic information is parsed differently, we categorize tokens in sentences according to their lexical categories and focus on changes in attention scores among these categories. Our hypothesis posits that in downstream tasks that prioritize semantic information, attention scores centered on content words are enhanced, while in cases emphasizing syntactic information, attention scores centered on function words are intensified. Through experimentation conducted on six tasks from the GLUE benchmark dataset, we substantiate our hypothesis regarding the fine-tuning process. Furthermore, our additional investigations reveal the presence of BERT layers that consistently assign m
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;LLMs&#30340;&#20020;&#24202;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#33258;&#21160;&#35780;&#20272;&#33539;&#24335;&#65292;&#21253;&#25324;&#24230;&#37327;&#12289;&#25968;&#25454;&#21644;&#31639;&#27861;&#65292;&#20197;&#30830;&#20445;&#23433;&#20840;&#21644;&#21487;&#38752;&#30340;&#20020;&#24202;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.16446</link><description>&lt;p&gt;
&#38754;&#21521;LLMs&#20020;&#24202;&#33021;&#21147;&#30340;&#33258;&#21160;&#35780;&#20272;&#65306;&#24230;&#37327;&#12289;&#25968;&#25454;&#21644;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Automatic Evaluation for LLMs' Clinical Capabilities: Metric, Data, and Algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16446
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;LLMs&#30340;&#20020;&#24202;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#33258;&#21160;&#35780;&#20272;&#33539;&#24335;&#65292;&#21253;&#25324;&#24230;&#37327;&#12289;&#25968;&#25454;&#21644;&#31639;&#27861;&#65292;&#20197;&#30830;&#20445;&#23433;&#20840;&#21644;&#21487;&#38752;&#30340;&#20020;&#24202;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30001;&#20110;&#22312;&#33258;&#28982;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#30340;&#21331;&#36234;&#34920;&#29616;&#65292;&#23545;&#20110;&#25552;&#39640;&#21307;&#23398;&#35786;&#26029;&#30340;&#20020;&#24202;&#25928;&#29575;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#20026;&#20102;&#30830;&#20445;&#23433;&#20840;&#21644;&#21487;&#38752;&#30340;&#20020;&#24202;&#24212;&#29992;&#65292;&#35780;&#20272;LLMs&#30340;&#30830;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#26356;&#22909;&#22320;&#20943;&#36731;&#28508;&#22312;&#39118;&#38505;&#65292;&#20363;&#22914;&#24187;&#35273;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#35780;&#20272;&#26041;&#27861;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#21171;&#21160;&#23494;&#38598;&#22411;&#30340;&#20154;&#31867;&#21442;&#19982;&#26469;&#33719;&#24471;&#20154;&#31867;&#20559;&#22909;&#30340;&#21028;&#26029;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#33258;&#21160;&#35780;&#20272;&#33539;&#24335;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#25552;&#20379;&#20020;&#24202;&#26381;&#21153;(&#20363;&#22914;&#30142;&#30149;&#35786;&#26029;&#21644;&#27835;&#30103;)&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#35813;&#35780;&#20272;&#33539;&#24335;&#21253;&#21547;&#19977;&#20010;&#22522;&#26412;&#35201;&#32032;&#65306;&#24230;&#37327;&#12289;&#25968;&#25454;&#21644;&#31639;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#21463;&#19987;&#19994;&#20020;&#24202;&#23454;&#36341;&#36884;&#24452;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;LLM&#29305;&#23450;&#30340;&#20020;&#24202;&#36884;&#24452;(LCP)&#65292;&#20197;&#23450;&#20041;&#21307;&#29983;&#20195;&#29702;&#24212;&#20855;&#22791;&#30340;&#20020;&#24202;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16446v1 Announce Type: new  Abstract: Large language models (LLMs) are gaining increasing interests to improve clinical efficiency for medical diagnosis, owing to their unprecedented performance in modelling natural language. Ensuring the safe and reliable clinical applications, the evaluation of LLMs indeed becomes critical for better mitigating the potential risks, e.g., hallucinations. However, current evaluation methods heavily rely on labor-intensive human participation to achieve human-preferred judgements. To overcome this challenge, we propose an automatic evaluation paradigm tailored to assess the LLMs' capabilities in delivering clinical services, e.g., disease diagnosis and treatment. The evaluation paradigm contains three basic elements: metric, data, and algorithm. Specifically, inspired by professional clinical practice pathways, we formulate a LLM-specific clinical pathway (LCP) to define the clinical capabilities that a doctor agent should possess. Then, Stan
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#22871;&#21517;&#20026;KIT-19&#30340;&#38889;&#25991;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#24320;&#21457;&#38889;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;19&#20010;&#38889;&#25991;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16444</link><description>&lt;p&gt;
KIT-19&#65306;&#19968;&#22871;&#28085;&#30422;19&#20010;&#20219;&#21153;&#30340;&#38889;&#25991;&#25351;&#20196;&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#24494;&#35843;&#38889;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
KIT-19: A Comprehensive Korean Instruction Toolkit on 19 Tasks for Fine-Tuning Korean Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16444
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#22871;&#21517;&#20026;KIT-19&#30340;&#38889;&#25991;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#24320;&#21457;&#38889;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;19&#20010;&#38889;&#25991;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Instruction Tuning on Large Language Models&#26159;&#27169;&#22411;&#34920;&#29616;&#33391;&#22909;&#12289;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#21462;&#24471;&#39640;&#24615;&#33021;&#30340;&#24517;&#35201;&#36807;&#31243;&#12290;&#22312;&#20027;&#27969;&#35821;&#35328;&#22914;&#33521;&#35821;&#20013;&#65292;&#27491;&#22312;&#26500;&#24314;&#21644;&#20844;&#24320;&#25552;&#20379;&#22522;&#20110;&#25351;&#20196;&#30340;&#25968;&#25454;&#38598;&#12290;&#23545;&#20110;&#38889;&#35821;&#65292;&#20844;&#24320;&#21487;&#29992;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#37117;&#20381;&#36182;&#20110;&#20351;&#29992;ChatGPT&#30340;&#36755;&#20986;&#25110;&#32763;&#35793;&#33521;&#25991;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;\textit{KIT-19}&#20316;&#20026;&#29992;&#20110;&#24320;&#21457;&#38889;&#25991;LLM&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290; \textit{KIT-19}&#26159;&#20197;&#25351;&#20196;&#26684;&#24335;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;19&#20010;&#38889;&#25991;NLP&#20219;&#21153;&#30340;&#29616;&#26377;&#24320;&#28304;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#20351;&#29992;\textit{KIT-19}&#35757;&#32451;&#38889;&#25991;&#39044;&#35757;&#32451;LLM&#65292;&#20197;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;\textit{KIT-19}&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#38889;&#25991;LLM&#12290;&#22522;&#20110;&#20854;&#36136;&#37327;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16444v1 Announce Type: new  Abstract: Instruction Tuning on Large Language Models is an essential process for model to function well and achieve high performance in specific tasks. Accordingly, in mainstream languages such as English, instruction-based datasets are being constructed and made publicly available. In the case of Korean, publicly available models and datasets all rely on using the output of ChatGPT or translating datasets built in English. In this paper, We introduce \textit{KIT-19} as an instruction dataset for the development of LLM in Korean. \textit{KIT-19} is a dataset created in an instruction format, comprising 19 existing open-source datasets for Korean NLP tasks. In this paper, we train a Korean Pretrained LLM using \textit{KIT-19} to demonstrate its effectiveness. The experimental results show that the model trained on \textit{KIT-19} significantly outperforms existing Korean LLMs. Based on the its quality and empirical results, this paper proposes tha
&lt;/p&gt;</description></item><item><title>CodeS&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;NL2Repo&#65292;&#26088;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#38656;&#27714;&#20013;&#29983;&#25104;&#25972;&#20010;&#20195;&#30721;&#20179;&#24211;&#65292;&#36890;&#36807;&#22810;&#23618;&#33609;&#22270;&#30340;&#26041;&#24335;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.16443</link><description>&lt;p&gt;
CodeS: &#36890;&#36807;&#22810;&#23618;&#33609;&#22270;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#21040;&#20195;&#30721;&#20179;&#24211;&#30340;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
CodeS: Natural Language to Code Repository via Multi-Layer Sketch
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16443
&lt;/p&gt;
&lt;p&gt;
CodeS&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;NL2Repo&#65292;&#26088;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#38656;&#27714;&#20013;&#29983;&#25104;&#25972;&#20010;&#20195;&#30721;&#20179;&#24211;&#65292;&#36890;&#36807;&#22810;&#23618;&#33609;&#22270;&#30340;&#26041;&#24335;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#23637;&#31034;&#20102;&#23436;&#20840;&#33258;&#21160;&#21270;&#36719;&#20214;&#24320;&#21457;&#30340;&#28508;&#21147;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#65292;&#21363;&#33258;&#28982;&#35821;&#35328;&#21040;&#20195;&#30721;&#20179;&#24211;&#65288;NL2Repo&#65289;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#38656;&#27714;&#20013;&#29983;&#25104;&#25972;&#20010;&#20195;&#30721;&#20179;&#24211;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550; CodeS&#65292;&#36890;&#36807;&#22810;&#23618;&#33609;&#22270;&#23558;NL2Repo&#20998;&#35299;&#20026;&#22810;&#20010;&#23376;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CodeS&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;RepoSketcher&#65292;FileSketcher&#21644;SketchFiller&#12290;RepoSketcher&#39318;&#20808;&#20026;&#32473;&#23450;&#30340;&#38656;&#27714;&#29983;&#25104;&#20195;&#30721;&#20179;&#24211;&#30340;&#30446;&#24405;&#32467;&#26500;&#65307;FileSketcher&#28982;&#21518;&#20026;&#29983;&#25104;&#30340;&#32467;&#26500;&#20013;&#30340;&#27599;&#20010;&#25991;&#20214;&#29983;&#25104;&#19968;&#20010;&#25991;&#20214;&#33609;&#22270;&#65307;SketchFiller&#26368;&#32456;&#20026;&#29983;&#25104;&#30340;&#25991;&#20214;&#33609;&#22270;&#20013;&#30340;&#27599;&#20010;&#20989;&#25968;&#22635;&#20805;&#32454;&#33410;&#12290;&#20026;&#20102;&#20005;&#26684;&#35780;&#20272;CodeS&#22312;NL2Repo&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#21160;&#21644;&#25163;&#21160;&#30340;&#26041;&#24335;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16443v1 Announce Type: cross  Abstract: The impressive performance of large language models (LLMs) on code-related tasks has shown the potential of fully automated software development. In light of this, we introduce a new software engineering task, namely Natural Language to code Repository (NL2Repo). This task aims to generate an entire code repository from its natural language requirements. To address this task, we propose a simple yet effective framework CodeS, which decomposes NL2Repo into multiple sub-tasks by a multi-layer sketch. Specifically, CodeS includes three modules: RepoSketcher, FileSketcher, and SketchFiller. RepoSketcher first generates a repository's directory structure for given requirements; FileSketcher then generates a file sketch for each file in the generated structure; SketchFiller finally fills in the details for each function in the generated file sketch. To rigorously assess CodeS on the NL2Repo task, we carry out evaluations through both automat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#39062;&#30340;Extract and Explore&#65288;EX2&#65289;&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20013;&#65292;&#37325;&#35201;&#30340;&#29305;&#24449;&#25551;&#36848;&#21253;&#25324;&#38750;&#35270;&#35273;&#23646;&#24615;&#65292;&#34394;&#20551;&#25551;&#36848;&#24433;&#21709;VLM&#34920;&#31034;&#65292;&#19981;&#21516;&#30340;VLM&#20248;&#20808;&#32771;&#34385;&#19981;&#21516;&#30340;&#20869;&#23481;&#12290;</title><link>https://arxiv.org/abs/2403.16442</link><description>&lt;p&gt;
&#22914;&#26524;CLIP&#33021;&#35828;&#35805;: &#36890;&#36807;&#23427;&#20204;&#30340;&#39318;&#36873;&#27010;&#24565;&#25551;&#36848;&#29702;&#35299;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16442
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#39062;&#30340;Extract and Explore&#65288;EX2&#65289;&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20013;&#65292;&#37325;&#35201;&#30340;&#29305;&#24449;&#25551;&#36848;&#21253;&#25324;&#38750;&#35270;&#35273;&#23646;&#24615;&#65292;&#34394;&#20551;&#25551;&#36848;&#24433;&#21709;VLM&#34920;&#31034;&#65292;&#19981;&#21516;&#30340;VLM&#20248;&#20808;&#32771;&#34385;&#19981;&#21516;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24120;&#24120;&#20551;&#35774;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#30340;&#34920;&#31034;&#26159;&#22522;&#20110;&#24418;&#29366;&#31561;&#35270;&#35273;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;VLM&#22312;&#34920;&#31034;&#27010;&#24565;&#26102;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#23558;&#36825;&#20123;&#20449;&#24687;&#20316;&#20026;&#20248;&#20808;&#32771;&#34385;&#23545;&#35937;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Extract and Explore&#65288;EX2&#65289;&#65292;&#29992;&#20110;&#21051;&#30011;VLM&#30340;&#37325;&#35201;&#25991;&#26412;&#29305;&#24449;&#12290;EX2&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#23558;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;VLM&#39318;&#36873;&#39033;&#23545;&#40784;&#65292;&#24182;&#29983;&#25104;&#21253;&#21547;VLM&#37325;&#35201;&#29305;&#24449;&#30340;&#25551;&#36848;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26816;&#26597;&#36825;&#20123;&#25551;&#36848;&#20197;&#30830;&#23450;&#23545;VLM&#34920;&#31034;&#26377;&#36129;&#29486;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#25552;&#20379;&#20102;&#27809;&#26377;&#24110;&#21161;&#20449;&#24687;&#30340;&#34394;&#20551;&#25551;&#36848;&#65288;&#20363;&#22914;&#65292;&#21333;&#20987;&#25918;&#22823;&#27010;&#24565;&#30340;&#29031;&#29255;&#65289;&#65292;&#20294;&#22312;VLM&#34920;&#31034;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#20449;&#24687;&#20016;&#23500;&#30340;&#25551;&#36848;&#20013;&#65292;VLM&#22312;&#34920;&#31034;&#35270;&#35273;&#27010;&#24565;&#26102;&#26174;&#33879;&#20381;&#36182;&#38750;&#35270;&#35273;&#23646;&#24615;&#65288;&#22914;&#26646;&#24687;&#22320;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#19981;&#21516;&#30340;VLM&#20248;&#20808;&#32771;&#34385;&#19981;&#21516;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16442v1 Announce Type: new  Abstract: Recent works often assume that Vision-Language Model (VLM) representations are based on visual attributes like shape. However, it is unclear to what extent VLMs prioritize this information to represent concepts. We propose Extract and Explore (EX2), a novel approach to characterize important textual features for VLMs. EX2 uses reinforcement learning to align a large language model with VLM preferences and generates descriptions that incorporate the important features for the VLM. Then, we inspect the descriptions to identify the features that contribute to VLM representations. We find that spurious descriptions have a major role in VLM representations despite providing no helpful information, e.g., Click to enlarge photo of CONCEPT. More importantly, among informative descriptions, VLMs rely significantly on non-visual attributes like habitat to represent visual concepts. Also, our analysis reveals that different VLMs prioritize differen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;REval&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#20197;&#21450;&#19982;&#31243;&#24207;&#25191;&#34892;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16437</link><description>&lt;p&gt;
&#20351;&#29992;&#31243;&#24207;&#25191;&#34892;&#36816;&#34892;&#26102;&#34892;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models with Runtime Behavior of Program Execution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;REval&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#20197;&#21450;&#19982;&#31243;&#24207;&#25191;&#34892;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;&#20195;&#30721;LLMs&#65289;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;&#20026;&#20102;&#35780;&#20272;&#20195;&#30721;LLMs&#22312;&#21508;&#20010;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20934;&#65288;&#22914;HumanEval&#21644;ClassEval&#65289;&#12290;&#20195;&#30721;&#25512;&#29702;&#26159;&#20195;&#30721;LLMs&#26368;&#37325;&#35201;&#30340;&#33021;&#21147;&#20043;&#19968;&#65292;&#20294;&#29616;&#26377;&#30340;&#20195;&#30721;&#25512;&#29702;&#22522;&#20934;&#19981;&#36275;&#12290;&#36890;&#24120;&#65292;&#23427;&#20204;&#37325;&#28857;&#39044;&#27979;&#31243;&#24207;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#24573;&#30053;&#20102;&#31243;&#24207;&#25191;&#34892;&#36807;&#31243;&#20013;&#30340;&#20013;&#38388;&#34892;&#20026;&#35780;&#20272;&#65292;&#20197;&#21450;&#36923;&#36753;&#19968;&#33268;&#24615;&#65288;&#20363;&#22914;&#65292;&#22914;&#26524;&#25191;&#34892;&#36335;&#24452;&#39044;&#27979;&#38169;&#35823;&#65292;&#21017;&#27169;&#22411;&#19981;&#24212;&#35813;&#32473;&#20986;&#27491;&#30830;&#30340;&#36755;&#20986;&#65289;&#22312;&#25191;&#34892;&#25512;&#29702;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;REval&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#20197;&#21450;&#19982;&#31243;&#24207;&#25191;&#34892;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#20195;&#30721;&#22522;&#20934;&#65292;&#24182;&#23558;&#23427;&#20204;&#36866;&#24212;&#21040;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#30340;&#26032;&#22522;&#20934;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16437v1 Announce Type: cross  Abstract: Large language models for code (i.e., code LLMs) have shown strong code understanding and generation capabilities. To evaluate the capabilities of code LLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval and ClassEval). Code reasoning is one of the most essential abilities of code LLMs, but existing benchmarks for code reasoning are not sufficient. Typically, they focus on predicting the input and output of a program, ignoring the evaluation of the intermediate behavior during program execution, as well as the logical consistency (e.g., the model should not give the correct output if the prediction of execution path is wrong) when performing the reasoning. To address these problems, in this paper, we propose a framework, namely REval, for evaluating code reasoning abilities and consistency of code LLMs with program execution. We utilize existing code benchmarks and adapt them to new benchmarks within our framew
&lt;/p&gt;</description></item><item><title>InstUPR&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;LLMs&#30340;&#25351;&#20196;&#36319;&#36394;&#33021;&#21147;&#65292;&#26080;&#38656;&#39069;&#22806;&#24494;&#35843;&#65292;&#36890;&#36807;&#36719;&#24471;&#20998;&#32858;&#21512;&#25216;&#26415;&#21644;&#25104;&#23545;&#37325;&#26032;&#25490;&#24207;&#65292;&#22312;BEIR&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>https://arxiv.org/abs/2403.16435</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#26080;&#30417;&#30563;&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;InstUPR
&lt;/p&gt;
&lt;p&gt;
InstUPR : Instruction-based Unsupervised Passage Reranking with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16435
&lt;/p&gt;
&lt;p&gt;
InstUPR&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;LLMs&#30340;&#25351;&#20196;&#36319;&#36394;&#33021;&#21147;&#65292;&#26080;&#38656;&#39069;&#22806;&#24494;&#35843;&#65292;&#36890;&#36807;&#36719;&#24471;&#20998;&#32858;&#21512;&#25216;&#26415;&#21644;&#25104;&#23545;&#37325;&#26032;&#25490;&#24207;&#65292;&#22312;BEIR&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;InstUPR&#65292;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26080;&#30417;&#30563;&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#20381;&#36182;&#20110;query-document&#23545;&#36827;&#34892;&#22823;&#37327;&#35757;&#32451;&#25110;&#29305;&#23450;&#20110;&#26816;&#32034;&#30340;&#25351;&#20196;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#32463;&#36807;&#25351;&#20196;&#35843;&#25972;&#30340;LLMs&#30340;&#25353;&#29031;&#25351;&#20196;&#36827;&#34892;&#25805;&#20316;&#30340;&#33021;&#21147;&#26469;&#36827;&#34892;&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#30340;&#24494;&#35843;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36719;&#24471;&#20998;&#32858;&#21512;&#25216;&#26415;&#65292;&#24182;&#37319;&#29992;&#20102;&#25104;&#23545;&#37325;&#26032;&#25490;&#24207;&#30340;&#26080;&#30417;&#30563;&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#12290;&#22312;BEIR&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;InstUPR&#20248;&#20110;&#26080;&#30417;&#30563;&#22522;&#32447;&#20197;&#21450;&#19968;&#20010;&#32463;&#36807;&#25351;&#20196;&#35843;&#25972;&#30340;&#37325;&#26032;&#25490;&#24207;&#22120;&#65292;&#31361;&#26174;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;&#22797;&#29616;&#25152;&#26377;&#23454;&#39564;&#30340;&#28304;&#20195;&#30721;&#24050;&#22312;https://github.com/MiuLab/InstUPR &#24320;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16435v1 Announce Type: new  Abstract: This paper introduces InstUPR, an unsupervised passage reranking method based on large language models (LLMs). Different from existing approaches that rely on extensive training with query-document pairs or retrieval-specific instructions, our method leverages the instruction-following capabilities of instruction-tuned LLMs for passage reranking without any additional fine-tuning. To achieve this, we introduce a soft score aggregation technique and employ pairwise reranking for unsupervised passage reranking. Experiments on the BEIR benchmark demonstrate that InstUPR outperforms unsupervised baselines as well as an instruction-tuned reranker, highlighting its effectiveness and superiority. Source code to reproduce all experiments is open-sourced at https://github.com/MiuLab/InstUPR
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#36807;&#31243;&#25581;&#31034;&#20102;&#29983;&#25104;&#23545;&#25239;&#25552;&#31034;&#20197;&#35823;&#23548;&#27169;&#22411;&#30340;&#35265;&#35299;&#65292;&#24341;&#21457;&#20102;&#23545;&#35813;&#33539;&#24335;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#30340;&#25285;&#24551;&#12290;</title><link>https://arxiv.org/abs/2403.16432</link><description>&lt;p&gt;
$\textit{LinkPrompt}$: &#22522;&#20110;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#21644;&#36890;&#29992;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
$\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on Prompt-based Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16432
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#36807;&#31243;&#25581;&#31034;&#20102;&#29983;&#25104;&#23545;&#25239;&#25552;&#31034;&#20197;&#35823;&#23548;&#27169;&#22411;&#30340;&#35265;&#35299;&#65292;&#24341;&#21457;&#20102;&#23545;&#35813;&#33539;&#24335;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt-based learning &#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#33539;&#24335;&#65292;&#23427;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#35843;&#25972;&#21040;&#19979;&#28216;&#20219;&#21153;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#25552;&#21319;&#20102;&#24615;&#33021;&#22522;&#20934;&#12290;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20248;&#21270;&#25628;&#32034;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#22266;&#23450;&#30340;&#25552;&#31034;&#27169;&#26495;&#26469;&#24494;&#35843;&#27169;&#22411;&#12290;&#36825;&#31181;&#22522;&#20110;&#25552;&#31034;&#20248;&#21270;&#36807;&#31243;&#23545;PLMs&#30340;&#23398;&#20064;&#20063;&#25581;&#31034;&#20102;&#29983;&#25104;&#23545;&#25239;&#25552;&#31034;&#20197;&#35823;&#23548;&#27169;&#22411;&#30340;&#35265;&#35299;&#65292;&#24341;&#21457;&#20102;&#23545;&#36825;&#19968;&#33539;&#24335;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#30340;&#25285;&#24551;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#29983;&#25104;&#36890;&#29992;&#23545;&#25239;&#35302;&#21457;&#22120;&#65288;UATs&#65289;&#26469;&#25913;&#21464;&#19981;&#20165;&#30446;&#26631;PLMs&#30340;&#39044;&#27979;&#65292;&#36824;&#26377;&#23545;&#24212;Prompt-based Fine-tuning Models&#65288;PFMs&#65289;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#20316;&#21697;&#20013;&#21457;&#29616;&#30340;UATs&#36890;&#24120;&#26159;&#26080;&#27861;&#38405;&#35835;&#30340;&#20196;&#29260;&#25110;&#23383;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16432v1 Announce Type: cross  Abstract: Prompt-based learning is a new language model training paradigm that adapts the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes the performance benchmarks across various natural language processing (NLP) tasks. Instead of using a fixed prompt template to fine-tune the model, some research demonstrates the effectiveness of searching for the prompt via optimization. Such prompt optimization process of prompt-based learning on PLMs also gives insight into generating adversarial prompts to mislead the model, raising concerns about the adversarial vulnerability of this paradigm. Recent studies have shown that universal adversarial triggers (UATs) can be generated to alter not only the predictions of the target PLMs but also the prediction of corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based learning paradigm. However, UATs found in previous works are often unreadable tokens or characters a
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#23457;&#35270;&#20449;&#24687;&#25277;&#21462;&#20013;&#30340;&#20219;&#21153;&#23450;&#20041;&#20559;&#35265;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;&#26694;&#26550;&#26469;&#34913;&#37327;&#12289;&#24863;&#30693;&#21644;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16396</link><description>&lt;p&gt;
&#26159;&#21542;&#23384;&#22312;&#36866;&#29992;&#20110;&#25152;&#26377;&#24773;&#20917;&#30340;&#20449;&#24687;&#25277;&#21462;&#19968;&#20307;&#21270;&#27169;&#22411;&#65311;&#37325;&#26032;&#23457;&#35270;&#20219;&#21153;&#23450;&#20041;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Is There a One-Model-Fits-All Approach to Information Extraction? Revisiting Task Definition Biases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16396
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#20449;&#24687;&#25277;&#21462;&#20013;&#30340;&#20219;&#21153;&#23450;&#20041;&#20559;&#35265;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;&#26694;&#26550;&#26469;&#34913;&#37327;&#12289;&#24863;&#30693;&#21644;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#20041;&#20559;&#35265;&#26159;&#19968;&#31181;&#36127;&#38754;&#29616;&#35937;&#65292;&#21487;&#33021;&#20250;&#35823;&#23548;&#27169;&#22411;&#12290;&#20449;&#24687;&#25277;&#21462;&#20013;&#30340;&#23450;&#20041;&#20559;&#35265;&#19981;&#20165;&#23384;&#22312;&#20110;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#20043;&#38388;&#65292;&#36824;&#23384;&#22312;&#20110;&#20998;&#20139;&#30456;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#20869;&#37096;&#12290;&#26412;&#25991;&#30830;&#23450;&#20102;&#20449;&#24687;&#25277;&#21462;&#20013;&#30340;&#20004;&#31181;&#23450;&#20041;&#20559;&#35265;&#31867;&#22411;&#65306;&#22312;&#20449;&#24687;&#25277;&#21462;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#20559;&#35265;&#65292;&#20197;&#21450;&#22312;&#20449;&#24687;&#25277;&#21462;&#25968;&#25454;&#38598;&#19982;&#25351;&#23548;&#35843;&#20248;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#20559;&#35265;&#12290;&#20026;&#20102;&#31995;&#32479;&#22320;&#30740;&#31350;&#23450;&#20041;&#20559;&#35265;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19977;&#39033;&#25506;&#31350;&#24615;&#23454;&#39564;&#26469;&#23450;&#37327;&#20998;&#26512;&#23427;&#65292;&#24182;&#21457;&#29616;&#32479;&#19968;&#20449;&#24687;&#25277;&#21462;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#23450;&#20041;&#20559;&#35265;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#20943;&#36731;&#20449;&#24687;&#25277;&#21462;&#20013;&#30340;&#23450;&#20041;&#20559;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#23450;&#20041;&#20559;&#35265;&#27979;&#37327;&#12289;&#20559;&#35265;&#24863;&#30693;&#24494;&#35843;&#21644;&#20219;&#21153;&#29305;&#23450;&#20559;&#35265;&#32531;&#35299;&#32452;&#25104;&#30340;&#22810;&#38454;&#27573;&#26694;&#26550;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#24212;&#23545;&#23450;&#20041;&#20559;&#35265;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#36164;&#28304;&#21487;&#22312;htt&#25214;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16396v1 Announce Type: new  Abstract: Definition bias is a negative phenomenon that can mislead models. Definition bias in information extraction appears not only across datasets from different domains but also within datasets sharing the same domain. We identify two types of definition bias in IE: bias among information extraction datasets and bias between information extraction datasets and instruction tuning datasets. To systematically investigate definition bias, we conduct three probing experiments to quantitatively analyze it and discover the limitations of unified information extraction and large language models in solving definition bias. To mitigate definition bias in information extraction, we propose a multi-stage framework consisting of definition bias measurement, bias-aware fine-tuning, and task-specific bias mitigation. Experimental results demonstrate the effectiveness of our framework in addressing definition bias. Resources of this paper can be found at htt
&lt;/p&gt;</description></item><item><title>&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#30340;&#27867;&#21270;&#38382;&#39064;&#28304;&#20110;&#29616;&#35937;&#31354;&#38388;&#20013;&#30340;&#20559;&#24046;&#65292;&#38656;&#35201;&#37327;&#21270;&#21644;&#35299;&#20915;&#35821;&#35328;&#21644;&#35270;&#35273;&#20559;&#24046;&#65292;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.16394</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#29616;&#35937;&#31354;&#38388;&#20013;&#30340;&#20559;&#24046;&#38459;&#30861;&#20102;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Skews in the Phenomenon Space Hinder Generalization in Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16394
&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#30340;&#27867;&#21270;&#38382;&#39064;&#28304;&#20110;&#29616;&#35937;&#31354;&#38388;&#20013;&#30340;&#20559;&#24046;&#65292;&#38656;&#35201;&#37327;&#21270;&#21644;&#35299;&#20915;&#35821;&#35328;&#21644;&#35270;&#35273;&#20559;&#24046;&#65292;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#30340;&#25991;&#29486;&#23384;&#22312;&#30528;&#20851;&#20110;&#22914;&#20309;&#24544;&#23454;&#22320;&#32452;&#21512;&#23454;&#20307;&#19982;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#23545;&#23454;&#20307;-&#20851;&#31995;&#32452;&#21512;&#22914;&#20309;&#26377;&#25928;&#23398;&#20064;&#30340;&#24418;&#24335;&#21270;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#21453;&#26144;&#38382;&#39064;&#32467;&#26500;&#30340;&#22522;&#30784;&#29616;&#35937;&#31354;&#38388;&#24182;&#19981;&#26126;&#30830;&#23450;&#20041;&#65292;&#23548;&#33268;&#20026;&#20102;&#24076;&#26395;&#27867;&#21270;&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20013;&#24471;&#20197;&#23637;&#29616;&#32780;&#19981;&#26029;&#36861;&#27714;&#26356;&#22810;&#25968;&#25454;&#12290;&#25105;&#20204;&#29468;&#27979;&#22522;&#30784;&#29616;&#35937;&#23398;&#35206;&#30422;&#33539;&#22260;&#24182;&#26410;&#25353;&#27604;&#20363;&#25193;&#23637;&#65292;&#23548;&#33268;&#25152;&#21576;&#29616;&#29616;&#35937;&#30340;&#20559;&#24046;&#23545;&#27867;&#21270;&#36896;&#25104;&#20102;&#20260;&#23475;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32479;&#35745;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#25968;&#25454;&#38598;&#20013;&#30340;&#35821;&#35328;&#21644;&#35270;&#35273;&#20559;&#24046;&#65292;&#29992;&#20110;&#20851;&#31995;&#23398;&#20064;&#65292;&#24182;&#34920;&#26126;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#27867;&#21270;&#22833;&#36133;&#30452;&#25509;&#28304;&#20110;&#29616;&#35937;&#23398;&#35206;&#30422;&#19981;&#23436;&#25972;&#25110;&#19981;&#24179;&#34913;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#21512;&#25104;&#39046;&#22495;&#36827;&#34892;&#23454;&#39564;&#21644;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16394v1 Announce Type: cross  Abstract: The literature on text-to-image generation is plagued by issues of faithfully composing entities with relations. But there lacks a formal understanding of how entity-relation compositions can be effectively learned. Moreover, the underlying phenomenon space that meaningfully reflects the problem structure is not well-defined, leading to an arms race for larger quantities of data in the hope that generalization emerges out of large-scale pretraining. We hypothesize that the underlying phenomenological coverage has not been proportionally scaled up, leading to a skew of the presented phenomenon which harms generalization. We introduce statistical metrics that quantify both the linguistic and visual skew of a dataset for relational learning, and show that generalization failures of text-to-image generation are a direct result of incomplete or unbalanced phenomenological coverage. We first perform experiments in a synthetic domain and demo
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;LLM&#20316;&#20026;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#36880;&#27493;&#21512;&#25104;&#31574;&#30053;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#20026;&#36880;&#27493;&#23376;&#38382;&#39064;&#65292;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#29983;&#25104;&#26368;&#32456;&#31572;&#26696;&#65292;&#20197;&#35299;&#20915;&#22270;&#34920;VQA&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>https://arxiv.org/abs/2403.16385</link><description>&lt;p&gt;
&#36880;&#27493;&#21512;&#25104;&#65306;&#24037;&#20855;&#12289;&#27169;&#26495;&#21644;LLM&#20316;&#20026;&#25968;&#25454;&#29983;&#25104;&#22120;&#29992;&#20110;&#22522;&#20110;&#25512;&#29702;&#30340;&#22270;&#34920;VQA
&lt;/p&gt;
&lt;p&gt;
Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16385
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;LLM&#20316;&#20026;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#36880;&#27493;&#21512;&#25104;&#31574;&#30053;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#20026;&#36880;&#27493;&#23376;&#38382;&#39064;&#65292;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#29983;&#25104;&#26368;&#32456;&#31572;&#26696;&#65292;&#20197;&#35299;&#20915;&#22270;&#34920;VQA&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#22270;&#34920;&#21644;&#22270;&#24418;&#31561;&#25968;&#25454;&#21487;&#35270;&#21270;&#38656;&#35201;&#23545;&#35270;&#35273;&#20803;&#32032;&#21644;&#25968;&#23383;&#36827;&#34892;&#25512;&#29702;&#12290;&#23613;&#31649;&#22312;&#25552;&#21462;&#24335;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#24403;&#21069;&#30340;&#22270;&#34920;&#35270;&#35273;&#38382;&#31572;&#65288;chart VQA&#65289;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26469;&#35299;&#20915;&#25512;&#29702;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21033;&#29992;&#24050;&#32463;&#34920;&#29616;&#20986;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#33258;&#21160;&#25968;&#25454;&#26631;&#27880;&#22120;&#65292;&#20026;&#22270;&#34920;&#22270;&#20687;&#29983;&#25104;&#38382;&#39064;-&#31572;&#26696;&#27880;&#37322;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#8220;&#36880;&#27493;&#21512;&#25104;&#8221;&#31574;&#30053;&#65306;&#22522;&#20110;LLM&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#23398;&#20064;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#20026;&#36880;&#27493;&#23376;&#38382;&#39064;&#65288;&#21407;&#29702;&#65289;&#65292;&#28982;&#21518;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#65288;&#21363;Python&#65289;&#25512;&#23548;&#20986;&#26368;&#32456;&#31572;&#26696;&#12290;&#36825;&#31181;&#36880;&#27493;&#29983;&#25104;&#36807;&#31243;&#26159;&#22312;&#20351;&#29992;&#22522;&#20110;&#27169;&#26495;&#30340;QA&#29983;&#25104;&#31649;&#36947;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#31361;&#20986;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16385v1 Announce Type: cross  Abstract: Understanding data visualizations like charts and plots requires reasoning about both visual elements and numerics. Although strong in extractive questions, current chart visual question answering (chart VQA) models suffer on complex reasoning questions. In this work, we address the lack of reasoning ability by data augmentation. We leverage Large Language Models (LLMs), which have shown to have strong reasoning ability, as an automatic data annotator that generates question-answer annotations for chart images. The key innovation in our method lies in the Synthesize Step-by-Step strategy: our LLM-based data generator learns to decompose the complex question into step-by-step sub-questions (rationales), which are then used to derive the final answer using external tools, i.e. Python. This step-wise generation procedure is trained on synthetic data generated using a template-based QA generation pipeline. Experimental results highlight th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#25628;&#32034;&#24341;&#25806;&#33719;&#21462;&#30340;&#25991;&#26723;&#21644;&#30456;&#20851;&#26597;&#35810;&#26469;&#22686;&#24378;&#20998;&#38754;&#39044;&#27979;&#30340;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19987;&#27880;&#20110;&#20165;&#20351;&#29992;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#26469;&#39044;&#27979;&#20998;&#38754;&#30340;&#26694;&#26550;</title><link>https://arxiv.org/abs/2403.16345</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#32534;&#36753;&#30340;&#22686;&#24378;&#24335;&#20998;&#38754;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhanced Facet Generation with LLM Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16345
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#25628;&#32034;&#24341;&#25806;&#33719;&#21462;&#30340;&#25991;&#26723;&#21644;&#30456;&#20851;&#26597;&#35810;&#26469;&#22686;&#24378;&#20998;&#38754;&#39044;&#27979;&#30340;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19987;&#27880;&#20110;&#20165;&#20351;&#29992;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#26469;&#39044;&#27979;&#20998;&#38754;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#65292;&#29992;&#25143;&#26597;&#35810;&#30340;&#20998;&#38754;&#35782;&#21035;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#12290;&#22914;&#26524;&#25628;&#32034;&#26381;&#21153;&#33021;&#22815;&#35782;&#21035;&#29992;&#25143;&#26597;&#35810;&#30340;&#20998;&#38754;&#65292;&#23601;&#26377;&#28508;&#21147;&#20026;&#29992;&#25143;&#25552;&#20379;&#26356;&#24191;&#27867;&#30340;&#25628;&#32034;&#32467;&#26524;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#25628;&#32034;&#24341;&#25806;&#33719;&#21462;&#30340;&#26816;&#32034;&#25991;&#26723;&#21644;&#30456;&#20851;&#26597;&#35810;&#26469;&#22686;&#24378;&#20998;&#38754;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22312;&#23558;&#25628;&#32034;&#24341;&#25806;&#20316;&#20026;&#27169;&#22411;&#30340;&#19968;&#37096;&#20998;&#36827;&#34892;&#25193;&#23637;&#21040;&#20854;&#20182;&#24212;&#29992;&#26102;&#23384;&#22312;&#25361;&#25112;&#12290;&#31532;&#19968;&#65292;&#25628;&#32034;&#24341;&#25806;&#19981;&#26029;&#26356;&#26032;&#12290;&#22240;&#27492;&#65292;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26399;&#38388;&#38468;&#21152;&#20449;&#24687;&#21487;&#33021;&#20250;&#26377;&#21464;&#21270;&#65292;&#36825;&#21487;&#33021;&#20250;&#38477;&#20302;&#24615;&#33021;&#12290;&#31532;&#20108;&#20010;&#25361;&#25112;&#26159;&#20844;&#20849;&#25628;&#32034;&#24341;&#25806;&#26080;&#27861;&#25628;&#32034;&#20869;&#37096;&#25991;&#20214;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26500;&#24314;&#19968;&#20010;&#21333;&#29420;&#30340;&#25628;&#32034;&#31995;&#32479;&#26469;&#23558;&#20844;&#21496;&#20869;&#37096;&#25991;&#26723;&#32435;&#20837;&#20854;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#65292;&#19987;&#27880;&#20110;&#19968;&#20010;&#21487;&#20197;&#20165;&#36890;&#36807;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#26469;&#39044;&#27979;&#20998;&#38754;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16345v1 Announce Type: cross  Abstract: In information retrieval, facet identification of a user query is an important task. If a search service can recognize the facets of a user's query, it has the potential to offer users a much broader range of search results. Previous studies can enhance facet prediction by leveraging retrieved documents and related queries obtained through a search engine. However, there are challenges in extending it to other applications when a search engine operates as part of the model. First, search engines are constantly updated. Therefore, additional information may change during training and test, which may reduce performance. The second challenge is that public search engines cannot search for internal documents. Therefore, a separate search system needs to be built to incorporate documents from private domains within the company. We propose two strategies that focus on a framework that can predict facets by taking only queries as input withou
&lt;/p&gt;</description></item><item><title>LLMs&#24050;&#25104;&#20026;&#29983;&#29289;&#21307;&#23398;&#19982;&#20581;&#24247;&#20449;&#24687;&#23398;&#20013;&#37325;&#35201;&#30340;&#24037;&#20855;&#65292;&#26412;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;&#20840;&#38754;&#23637;&#31034;&#20102;LLMs&#22312;&#21508;&#31181;BHI&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#20854;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#25913;&#36827;&#65292;&#25581;&#31034;&#20102;&#20027;&#35201;&#21457;&#23637;&#36235;&#21183;&#21644;&#30740;&#31350;&#32593;&#32476;&#65292;&#24182;&#35752;&#35770;&#20102;&#20262;&#29702;&#20851;&#20999;&#21644;&#23454;&#38469;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.16303</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#19982;&#20581;&#24247;&#20449;&#24687;&#23398;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Biomedical and Health Informatics: A Bibliometric Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16303
&lt;/p&gt;
&lt;p&gt;
LLMs&#24050;&#25104;&#20026;&#29983;&#29289;&#21307;&#23398;&#19982;&#20581;&#24247;&#20449;&#24687;&#23398;&#20013;&#37325;&#35201;&#30340;&#24037;&#20855;&#65292;&#26412;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;&#20840;&#38754;&#23637;&#31034;&#20102;LLMs&#22312;&#21508;&#31181;BHI&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#20854;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#25913;&#36827;&#65292;&#25581;&#31034;&#20102;&#20027;&#35201;&#21457;&#23637;&#36235;&#21183;&#21644;&#30740;&#31350;&#32593;&#32476;&#65292;&#24182;&#35752;&#35770;&#20102;&#20262;&#29702;&#20851;&#20999;&#21644;&#23454;&#38469;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36805;&#36895;&#25104;&#20026;&#29983;&#29289;&#21307;&#23398;&#19982;&#20581;&#24247;&#20449;&#24687;&#23398;&#65288;BHI&#65289;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#20026;&#20998;&#26512;&#25968;&#25454;&#12289;&#27835;&#30103;&#24739;&#32773;&#21644;&#24320;&#23637;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#24335;&#12290;&#26412;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;&#26088;&#22312;&#36890;&#36807;&#26816;&#26597;&#33258;2022&#24180;&#33267;2023&#24180;&#30340;&#30740;&#31350;&#25991;&#31456;&#21644;&#21512;&#20316;&#32593;&#32476;&#65292;&#20840;&#38754;&#23637;&#31034;LLMs&#22312;BHI&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#12290;&#23427;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;LLMs&#22914;&#20309;&#21487;&#20197;&#25913;&#36827;&#21508;&#31181;BHI&#39046;&#22495;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#65292;&#22914;&#21307;&#23398;&#35786;&#26029;&#12289;&#24739;&#32773;&#21442;&#19982;&#12289;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31649;&#29702;&#21644;&#20010;&#24615;&#21270;&#21307;&#23398;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;&#30830;&#23450;&#20102;&#20851;&#38190;&#36235;&#21183;&#65292;&#32472;&#21046;&#20102;&#30740;&#31350;&#32593;&#32476;&#65292;&#24182;&#31361;&#20986;&#20102;&#36825;&#20010;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#30340;&#20027;&#35201;&#36827;&#23637;&#12290;&#26368;&#21518;&#65292;&#23427;&#35752;&#35770;&#20102;&#22312;BHI&#20013;&#20351;&#29992;LLMs&#30340;&#20262;&#29702;&#20851;&#20999;&#21644;&#23454;&#38469;&#25361;&#25112;&#65292;&#22914;&#25968;&#25454;&#38544;&#31169;&#21644;&#21487;&#38752;&#30340;&#21307;&#30103;&#24314;&#35758;&#12290;&#23637;&#26395;&#26410;&#26469;&#65292;&#25105;&#20204;&#32771;&#34385;LLMs&#22914;&#20309;&#36827;&#19968;&#27493;&#25913;&#21464;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16303v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have rapidly become important tools in Biomedical and Health Informatics (BHI), enabling new ways to analyze data, treat patients, and conduct research. This bibliometric review aims to provide a panoramic view of how LLMs have been used in BHI by examining research articles and collaboration networks from 2022 to 2023. It further explores how LLMs can improve Natural Language Processing (NLP) applications in various BHI areas like medical diagnosis, patient engagement, electronic health record management, and personalized medicine. To do this, our bibliometric review identifies key trends, maps out research networks, and highlights major developments in this fast-moving field. Lastly, it discusses the ethical concerns and practical challenges of using LLMs in BHI, such as data privacy and reliable medical recommendations. Looking ahead, we consider how LLMs could further transform biomedical research as we
&lt;/p&gt;</description></item><item><title>LexDrafter&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#29616;&#26377;&#27861;&#24459;&#25991;&#20214;&#20013;&#30340;&#26415;&#35821;&#23450;&#20041;&#65292;&#24110;&#21161;&#36215;&#33609;&#27861;&#35268;&#25991;&#20214;&#20013;&#30340;&#8220;Definitions&#8221;&#25991;&#31456;&#12290;</title><link>https://arxiv.org/abs/2403.16295</link><description>&lt;p&gt;
LexDrafter: &#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20026;&#31435;&#27861;&#25991;&#20214;&#36215;&#33609;&#26415;&#35821;
&lt;/p&gt;
&lt;p&gt;
LexDrafter: Terminology Drafting for Legislative Documents using Retrieval Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16295
&lt;/p&gt;
&lt;p&gt;
LexDrafter&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#29616;&#26377;&#27861;&#24459;&#25991;&#20214;&#20013;&#30340;&#26415;&#35821;&#23450;&#20041;&#65292;&#24110;&#21161;&#36215;&#33609;&#27861;&#35268;&#25991;&#20214;&#20013;&#30340;&#8220;Definitions&#8221;&#25991;&#31456;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27431;&#30431;&#31435;&#27861;&#25991;&#20214;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#26032;&#26415;&#35821;&#21450;&#20854;&#23450;&#20041;&#30340;&#25968;&#37327;&#20063;&#22312;&#22686;&#21152;&#12290;&#25353;&#29031;&#27431;&#27954;&#35758;&#20250;&#12289;&#29702;&#20107;&#20250;&#21644;&#22996;&#21592;&#20250;&#30340;&#32852;&#21512;&#23454;&#29992;&#25351;&#21335;&#30340;&#35268;&#23450;&#65292;&#22312;&#27861;&#24459;&#25991;&#20214;&#20013;&#20351;&#29992;&#30340;&#26415;&#35821;&#24212;&#20445;&#25345;&#19968;&#33268;&#65292;&#30456;&#21516;&#27010;&#24565;&#24212;&#22312;&#19981;&#33073;&#31163;&#20854;&#22312;&#26222;&#36890;&#12289;&#27861;&#24459;&#25110;&#25216;&#26415;&#35821;&#35328;&#20013;&#21547;&#20041;&#30340;&#21069;&#25552;&#19979;&#34920;&#36798;&#12290;&#22240;&#27492;&#65292;&#22312;&#36215;&#33609;&#26032;&#30340;&#31435;&#27861;&#25991;&#20214;&#26102;&#65292;&#25317;&#26377;&#19968;&#20010;&#26694;&#26550;&#65292;&#33021;&#22815;&#25552;&#20379;&#26377;&#20851;&#29616;&#26377;&#23450;&#20041;&#30340;&#35265;&#35299;&#65292;&#24182;&#26681;&#25454;&#25991;&#20214;&#30340;&#19978;&#19979;&#25991;&#24110;&#21161;&#23450;&#20041;&#26032;&#26415;&#35821;&#65292;&#23558;&#25903;&#25345;&#19981;&#21516;&#27861;&#35268;&#19979;&#30340;&#36825;&#31181;&#32479;&#19968;&#27861;&#24459;&#23450;&#20041;&#65292;&#24182;&#36991;&#20813;&#27495;&#20041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LexDrafter&#65292;&#19968;&#31181;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#21644;&#19981;&#21516;&#31435;&#27861;&#25991;&#20214;&#20013;&#29616;&#26377;&#26415;&#35821;&#23450;&#20041;&#26469;&#21327;&#21161;&#36215;&#33609;&#31435;&#27861;&#25991;&#20214;&#30340;&#8220;Definitions&#8221;&#25991;&#31456;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16295v1 Announce Type: new  Abstract: With the increase in legislative documents at the EU, the number of new terms and their definitions is increasing as well. As per the Joint Practical Guide of the European Parliament, the Council and the Commission, terms used in legal documents shall be consistent, and identical concepts shall be expressed without departing from their meaning in ordinary, legal, or technical language. Thus, while drafting a new legislative document, having a framework that provides insights about existing definitions and helps define new terms based on a document's context will support such harmonized legal definitions across different regulations and thus avoid ambiguities. In this paper, we present LexDrafter, a framework that assists in drafting Definitions articles for legislative documents using retrieval augmented generation (RAG) and existing term definitions present in different legislative documents. For this, definition elements are built by e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26816;&#32034;&#30701;&#35821;&#22270;&#25512;&#26029;&#19987;&#21033;&#30701;&#35821;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#30701;&#35821;&#22270;&#65292;&#34917;&#20805;&#19987;&#21033;&#30701;&#35821;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#26469;&#20248;&#21270;&#23884;&#20837;&#21644;&#22270;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.16265</link><description>&lt;p&gt;
&#36830;&#25509;&#28857;: &#21033;&#29992;&#26816;&#32034;&#30701;&#35821;&#22270;&#25512;&#26029;&#19987;&#21033;&#30701;&#35821;&#30456;&#20284;&#24615;
&lt;/p&gt;
&lt;p&gt;
Connecting the Dots: Inferring Patent Phrase Similarity with Retrieved Phrase Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16265
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26816;&#32034;&#30701;&#35821;&#22270;&#25512;&#26029;&#19987;&#21033;&#30701;&#35821;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#30701;&#35821;&#22270;&#65292;&#34917;&#20805;&#19987;&#21033;&#30701;&#35821;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#26469;&#20248;&#21270;&#23884;&#20837;&#21644;&#22270;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19987;&#21033;&#30701;&#35821;&#30456;&#20284;&#24615;&#25512;&#26029;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#34913;&#37327;&#20004;&#20010;&#19987;&#21033;&#30701;&#35821;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#30001;&#20110;&#19987;&#21033;&#25991;&#20214;&#37319;&#29992;&#27861;&#24459;&#21644;&#39640;&#24230;&#25216;&#26415;&#24615;&#30340;&#35821;&#35328;&#65292;&#29616;&#26377;&#20351;&#29992;&#23616;&#37096;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#26041;&#27861;&#22312;&#25512;&#26029;&#19987;&#21033;&#30701;&#35821;&#30456;&#20284;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22270;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#25918;&#22823;&#19987;&#21033;&#30701;&#35821;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#23545;&#20110;&#27599;&#20010;&#19987;&#21033;&#30701;&#35821;&#65292;&#25105;&#20204;&#26500;&#24314;&#19968;&#20010;&#30701;&#35821;&#22270;&#65292;&#23558;&#20854;&#36830;&#25509;&#21040;&#20854;&#28966;&#28857;&#19987;&#21033;&#20197;&#21450;&#24341;&#29992;/&#34987;&#24341;&#29992;&#36825;&#20123;&#28966;&#28857;&#19987;&#21033;&#30340;&#19987;&#21033;&#21015;&#34920;&#12290;&#28982;&#21518;&#65292;&#22686;&#24378;&#30340;&#30701;&#35821;&#23884;&#20837;&#26159;&#20174;&#23558;&#20854;&#23616;&#37096;&#19978;&#19979;&#25991;&#23884;&#20837;&#19982;&#30701;&#35821;&#22270;&#20869;&#37096;&#30340;&#20840;&#23616;&#23884;&#20837;&#30456;&#32467;&#21512;&#20013;&#24471;&#20986;&#30340;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#65292;&#21033;&#29992;&#26816;&#32034;&#30340;&#25299;&#25169;&#26469;&#35843;&#25972;&#20004;&#20010;&#26041;&#38754;&#30340;&#23884;&#20837;&#65306;&#19978;&#19979;&#25991;&#21270;&#23884;&#20837;&#21644;&#22270;&#21442;&#25968;&#65292;&#20174;&#32780;&#26368;&#32456;&#25552;&#39640;&#25512;&#26029;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16265v1 Announce Type: new  Abstract: We study the patent phrase similarity inference task, which measures the semantic similarity between two patent phrases. As patent documents employ legal and highly technical language, existing semantic textual similarity methods that use localized contextual information do not perform satisfactorily in inferring patent phrase similarity. To address this, we introduce a graph-augmented approach to amplify the global contextual information of the patent phrases. For each patent phrase, we construct a phrase graph that links to its focal patents and a list of patents that are either cited by or cite these focal patents. The augmented phrase embedding is then derived from combining its localized contextual embedding with its global embedding within the phrase graph. We further propose a self-supervised learning objective that capitalizes on the retrieved topology to refine both the contextualized embedding and the graph parameters in an end
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20027;&#39064;&#24314;&#27169;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#30456;&#20851;&#20027;&#39064;&#26631;&#39064;&#24182;&#36981;&#24490;&#20154;&#31867;&#25351;&#21335;&#26469;&#31934;&#32454;&#21270;&#21644;&#21512;&#24182;&#20027;&#39064;</title><link>https://arxiv.org/abs/2403.16248</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#20256;&#32479;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#25552;&#20379;&#20102;&#21478;&#19968;&#31181;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Offer an Alternative to the Traditional Approach of Topic Modelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16248
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20027;&#39064;&#24314;&#27169;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#30456;&#20851;&#20027;&#39064;&#26631;&#39064;&#24182;&#36981;&#24490;&#20154;&#31867;&#25351;&#21335;&#26469;&#31934;&#32454;&#21270;&#21644;&#21512;&#24182;&#20027;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#24314;&#27169;&#20316;&#20026;&#19968;&#31181;&#25104;&#29087;&#30340;&#26080;&#30417;&#30563;&#25216;&#26415;&#65292;&#22312;&#33258;&#21160;&#26816;&#27979;&#25991;&#26723;&#35821;&#26009;&#24211;&#20013;&#30340;&#37325;&#35201;&#20027;&#39064;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#32463;&#20856;&#30340;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65288;&#20363;&#22914;LDA&#65289;&#23384;&#22312;&#26576;&#20123;&#32570;&#28857;&#65292;&#20363;&#22914;&#32570;&#20047;&#35821;&#20041;&#29702;&#35299;&#21644;&#20027;&#39064;&#37325;&#21472;&#30340;&#23384;&#22312;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#25581;&#31034;&#24191;&#27867;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#28508;&#22312;&#20027;&#39064;&#30340;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20419;&#20351;LLMs&#20174;&#32473;&#23450;&#30340;&#19968;&#32452;&#25991;&#26723;&#20013;&#29983;&#25104;&#20027;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#35780;&#20272;&#21327;&#35758;&#26469;&#35780;&#20272;LLMs&#30340;&#32858;&#31867;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;LLMs&#22312;&#36866;&#24403;&#30340;&#25552;&#31034;&#19979;&#21487;&#20197;&#33073;&#39062;&#32780;&#20986;&#20316;&#20026;&#19968;&#31181;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#33021;&#22815;&#29983;&#25104;&#30456;&#20851;&#30340;&#20027;&#39064;&#26631;&#39064;&#24182;&#36981;&#24490;&#20154;&#31867;&#25351;&#21335;&#26469;&#31934;&#32454;&#21270;&#21644;&#21512;&#24182;&#20027;&#39064;&#12290;&#36890;&#36807;&#28145;&#20837;&#30340;&#23454;&#39564;&#21644;&#35780;&#20272;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#36825;&#31181;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16248v1 Announce Type: new  Abstract: Topic modelling, as a well-established unsupervised technique, has found extensive use in automatically detecting significant topics within a corpus of documents. However, classic topic modelling approaches (e.g., LDA) have certain drawbacks, such as the lack of semantic understanding and the presence of overlapping topics. In this work, we investigate the untapped potential of large language models (LLMs) as an alternative for uncovering the underlying topics within extensive text corpora. To this end, we introduce a framework that prompts LLMs to generate topics from a given set of documents and establish evaluation protocols to assess the clustering efficacy of LLMs. Our findings indicate that LLMs with appropriate prompts can stand out as a viable alternative, capable of generating relevant topic titles and adhering to human guidelines to refine and merge topics. Through in-depth experiments and evaluation, we summarise the advantage
&lt;/p&gt;</description></item><item><title>&#25913;&#36827;&#20351;&#29992;&#20803;&#21551;&#21457;&#26041;&#27861;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#21644;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2403.16247</link><description>&lt;p&gt;
&#20351;&#29992;&#20803;&#21551;&#21457;&#26041;&#27861;&#25913;&#36827;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#65292;&#29992;&#20110;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Improving Sequence-to-Sequence Models for Abstractive Text Summarization Using Meta Heuristic Approaches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16247
&lt;/p&gt;
&lt;p&gt;
&#25913;&#36827;&#20351;&#29992;&#20803;&#21551;&#21457;&#26041;&#27861;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#21644;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#31867;&#31038;&#20250;&#36807;&#28193;&#21040;&#20449;&#24687;&#26102;&#20195;&#65292;&#25105;&#20204;&#27880;&#24847;&#21147;&#30340;&#20943;&#23569;&#26159;&#19968;&#20010;&#24517;&#28982;&#36235;&#21183;&#65292;&#33457;&#26102;&#38388;&#38405;&#35835;&#20887;&#38271;&#26032;&#38395;&#25991;&#31456;&#30340;&#20154;&#32676;&#27491;&#22312;&#36805;&#36895;&#20943;&#23569;&#65292;&#32780;&#23545;&#31616;&#27905;&#20449;&#24687;&#30340;&#38656;&#27714;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#39640;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#31616;&#27905;&#22320;&#24635;&#32467;&#39030;&#32423;&#26032;&#38395;&#25991;&#31456;&#21644;&#26368;&#30452;&#35266;&#30340;&#26631;&#39064;&#65292;&#25552;&#20379;&#37325;&#35201;&#26032;&#38395;&#30340;&#24555;&#36895;&#27010;&#36848;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20154;&#31867;&#22312;&#23581;&#35797;&#36827;&#34892;&#25688;&#35201;&#26102;&#65292;&#20250;&#20174;&#26469;&#28304;&#20013;&#25552;&#21462;&#22522;&#26412;&#20449;&#24687;&#65292;&#24182;&#20174;&#21407;&#22987;&#25552;&#21462;&#20013;&#28155;&#21152;&#26377;&#29992;&#30701;&#35821;&#21644;&#35821;&#27861;&#27880;&#37322;&#12290;&#20154;&#31867;&#26377;&#21019;&#24314;&#25277;&#35937;&#30340;&#29420;&#29305;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#25688;&#35201;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#23545;&#20110;&#31070;&#32463;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#65292;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#65288;seq2seq&#65289;&#27169;&#22411;&#30340;&#24212;&#29992;&#31243;&#24230;&#19981;&#26029;&#22686;&#21152;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#21019;&#26032;&#31574;&#30053;&#26469;&#36827;&#19968;&#27493;&#21457;&#23637;&#24403;&#21069;&#30340;seq2seq&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16247v1 Announce Type: new  Abstract: As human society transitions into the information age, reduction in our attention span is a contingency, and people who spend time reading lengthy news articles are decreasing rapidly and the need for succinct information is higher than ever before. Therefore, it is essential to provide a quick overview of important news by concisely summarizing the top news article and the most intuitive headline. When humans try to make summaries, they extract the essential information from the source and add useful phrases and grammatical annotations from the original extract. Humans have a unique ability to create abstractions. However, automatic summarization is a complicated problem to solve. The use of sequence-to-sequence (seq2seq) models for neural abstractive text summarization has been ascending as far as prevalence. Numerous innovative strategies have been proposed to develop the current seq2seq models further, permitting them to handle diffe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#19978;&#19979;&#25991;&#24863;&#30693;&#32534;&#30721;&#22120;&#25913;&#36827;NL2SQL&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20934;&#30830;&#20272;&#35745;&#26597;&#35810;&#30456;&#20284;&#24615;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;170k&#20010;&#38382;&#39064;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#33021;&#22815;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#27169;&#22411;&#65292;&#25552;&#21319;NL2SQL&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16204</link><description>&lt;p&gt;
SQL-Encoder: &#36890;&#36807;&#19978;&#19979;&#25991;&#24863;&#30693;&#32534;&#30721;&#22120;&#25913;&#36827;NL2SQL&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SQL-Encoder: Improving NL2SQL In-Context Learning Through a Context-Aware Encoder
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#19978;&#19979;&#25991;&#24863;&#30693;&#32534;&#30721;&#22120;&#25913;&#36827;NL2SQL&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20934;&#30830;&#20272;&#35745;&#26597;&#35810;&#30456;&#20284;&#24615;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;170k&#20010;&#38382;&#39064;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#33021;&#22815;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#27169;&#22411;&#65292;&#25552;&#21319;NL2SQL&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#26597;&#35810;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#23545;&#20110;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#27169;&#22411;&#20013;&#36873;&#25321;&#31034;&#20363;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20165;&#22522;&#20110;&#26597;&#35810;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#26469;&#35780;&#20272;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#32780;&#19981;&#32771;&#34385;SQL&#26597;&#35810;&#65292;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36825;&#31181;&#30456;&#20284;&#24230;&#24230;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20934;&#30830;&#20272;&#35745;&#23427;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;17&#19975;&#20010;&#38382;&#39064;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#65292;&#29992;&#20110;&#35757;&#32451;&#19968;&#20010;&#30456;&#20284;&#24615;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#24039;&#22937;&#22320;&#25429;&#25417;&#21040;&#20102;&#38382;&#39064;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#36825;&#20307;&#29616;&#22312;Kendall-Tau&#36317;&#31163;&#21644;precision@k&#25351;&#26631;&#30340;&#25913;&#21892;&#19978;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;OpenAI&#21644;Cohere&#30340;&#31454;&#20105;&#24615;&#23884;&#20837;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#19982;&#36825;&#20123;&#31454;&#20105;&#24615;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#32534;&#30721;&#22120;&#25552;&#21319;&#20102;NL2SQL&#27169;&#22411;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16204v1 Announce Type: new  Abstract: Detecting structural similarity between queries is essential for selecting examples in in-context learning models. However, assessing structural similarity based solely on the natural language expressions of queries, without considering SQL queries, presents a significant challenge. This paper explores the significance of this similarity metric and proposes a model for accurately estimating it. To achieve this, we leverage a dataset comprising 170k question pairs, meticulously curated to train a similarity prediction model. Our comprehensive evaluation demonstrates that the proposed model adeptly captures the structural similarity between questions, as evidenced by improvements in Kendall-Tau distance and precision@k metrics. Notably, our model outperforms strong competitive embedding models from OpenAI and Cohere. Furthermore, compared to these competitive models, our proposed encoder enhances the downstream performance of NL2SQL models
&lt;/p&gt;</description></item><item><title>ALoRA&#21019;&#26032;&#22320;&#25552;&#20986;&#20102;&#20998;&#37197;&#20302;&#31209;&#36866;&#24212;&#24615;&#65288;ALoRA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#36866;&#24212;&#36807;&#31243;&#20013;&#30340;&#22266;&#26377;&#31209;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#22266;&#23450;&#22266;&#26377;&#31209;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16187</link><description>&lt;p&gt;
ALoRA: &#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20998;&#37197;&#20302;&#31209;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16187
&lt;/p&gt;
&lt;p&gt;
ALoRA&#21019;&#26032;&#22320;&#25552;&#20986;&#20102;&#20998;&#37197;&#20302;&#31209;&#36866;&#24212;&#24615;&#65288;ALoRA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#36866;&#24212;&#36807;&#31243;&#20013;&#30340;&#22266;&#26377;&#31209;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#22266;&#23450;&#22266;&#26377;&#31209;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#65288;PEFT&#65289;&#22240;&#20854;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#32780;&#34987;&#24191;&#27867;&#30740;&#31350;&#12290;&#20302;&#31209;&#36866;&#24212;&#24615;&#65288;LoRA&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#20316;&#20026;&#19968;&#31181;&#27969;&#34892;&#21644;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#20196;&#20154;&#38054;&#20329;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#26159;&#20351;&#29992;&#22266;&#23450;&#30340;&#22266;&#26377;&#31209;&#26469;&#23454;&#29616;&#30340;&#65292;&#36825;&#21487;&#33021;&#19981;&#26159;&#19979;&#28216;&#20219;&#21153;&#30340;&#29702;&#24819;&#35774;&#32622;&#12290;&#35748;&#35782;&#21040;&#38656;&#35201;&#26356;&#28789;&#27963;&#30340;&#19979;&#28216;&#20219;&#21153;&#36866;&#24212;&#24615;&#65292;&#25105;&#20204;&#23558;LoRA&#30340;&#26041;&#27861;&#23398;&#25193;&#23637;&#21040;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#20998;&#37197;&#20302;&#31209;&#36866;&#24212;&#24615;&#65288;ALoRA&#65289;&#65292;&#36825;&#26679;&#21487;&#20197;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#22266;&#26377;&#31209;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;AB-LoRA&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20272;&#35745;&#27599;&#20010;LoRA&#31561;&#32423;&#30340;&#37325;&#35201;&#24615;&#24471;&#20998;&#12290;&#20854;&#27425;&#65292;&#21463;AB-LoRA&#30340;&#25351;&#23548;&#65292;&#25105;&#20204;&#36880;&#28176;&#20462;&#21098;&#20102;&#36807;&#22810;&#19988;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#30340;LoRA&#31561;&#32423;&#65292;&#24182;&#23558;&#34987;&#20462;&#21098;&#30340;LoRA&#39044;&#31639;&#20998;&#37197;&#32473;&#38656;&#35201;&#26356;&#39640;&#31561;&#32423;&#30340;&#37325;&#35201;Transformer&#27169;&#22359;&#12290;&#25105;&#20204;&#24050;&#32463;&#22312;&#21508;&#31181;&#23454;&#39564;&#19978;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16187v1 Announce Type: new  Abstract: Parameter-efficient fine-tuning (PEFT) is widely studied for its effectiveness and efficiency in the era of large language models. Low-rank adaptation (LoRA) has demonstrated commendable performance as a popular and representative method. However, it is implemented with a fixed intrinsic rank that might not be the ideal setting for the downstream tasks. Recognizing the need for more flexible downstream task adaptation, we extend the methodology of LoRA to an innovative approach we call allocating low-rank adaptation (ALoRA) that enables dynamic adjustments to the intrinsic rank during the adaptation process. First, we propose a novel method, AB-LoRA, that can effectively estimate the importance score of each LoRA rank. Second, guided by AB-LoRA, we gradually prune abundant and negatively impacting LoRA ranks and allocate the pruned LoRA budgets to important Transformer modules needing higher ranks. We have conducted experiments on variou
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#20165;&#23384;&#22312;&#24178;&#20928;&#20449;&#21495;&#29305;&#24449;&#30340;&#23376;&#31354;&#38388;&#24182;&#20002;&#24323;&#25200;&#21160;&#29305;&#24449;&#65292;&#20351;&#24471;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26356;&#22909;&#22320;&#21306;&#20998;&#23545;&#25239;&#24615;&#31034;&#20363;&#12290;</title><link>https://arxiv.org/abs/2403.16176</link><description>&lt;p&gt;
&#23376;&#31354;&#38388;&#38450;&#24481;&#65306;&#36890;&#36807;&#23398;&#20064;&#24178;&#20928;&#20449;&#21495;&#30340;&#23376;&#31354;&#38388;&#26469;&#20002;&#24323;&#23545;&#25239;&#24615;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
Subspace Defense: Discarding Adversarial Perturbations by Learning a Subspace for Clean Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16176
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#20165;&#23384;&#22312;&#24178;&#20928;&#20449;&#21495;&#29305;&#24449;&#30340;&#23376;&#31354;&#38388;&#24182;&#20002;&#24323;&#25200;&#21160;&#29305;&#24449;&#65292;&#20351;&#24471;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26356;&#22909;&#22320;&#21306;&#20998;&#23545;&#25239;&#24615;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#26497;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#21363;&#22312;&#27491;&#24120;&#31034;&#20363;&#19978;&#25918;&#32622;&#31934;&#24515;&#21046;&#20316;&#30340;&#25200;&#21160;&#20197;&#27450;&#39575;DNNs&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#31867;&#25915;&#20987;&#65292;&#38656;&#35201;&#23545;&#23545;&#25239;&#24615;&#31034;&#20363;&#25658;&#24102;&#30340;&#29305;&#24449;&#36827;&#34892;&#21051;&#30011;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#26679;&#26412;&#29305;&#24449;&#23376;&#31354;&#38388;&#36827;&#34892;&#35889;&#20998;&#26512;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#32463;&#39564;&#24615;&#22320;&#23637;&#31034;&#65292;&#26080;&#35770;&#26159;&#24178;&#20928;&#20449;&#21495;&#36824;&#26159;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#29305;&#24449;&#37117;&#26159;&#20887;&#20313;&#30340;&#65292;&#24182;&#20998;&#21035;&#22312;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#23637;&#24320;&#65292;&#20114;&#30456;&#20043;&#38388;&#37325;&#21472;&#24456;&#23567;&#65292;&#32780;&#32463;&#20856;&#30340;&#20302;&#32500;&#23376;&#31354;&#38388;&#25237;&#24433;&#21487;&#20197;&#23558;&#25200;&#21160;&#29305;&#24449;&#25490;&#38500;&#22312;&#24178;&#20928;&#20449;&#21495;&#23376;&#31354;&#38388;&#20043;&#22806;&#12290;&#36825;&#20351;&#24471;DNNs&#33021;&#22815;&#23398;&#20064;&#19968;&#20010;&#20165;&#23384;&#22312;&#24178;&#20928;&#20449;&#21495;&#29305;&#24449;&#30340;&#23376;&#31354;&#38388;&#65292;&#32780;&#20002;&#24323;&#25200;&#21160;&#29305;&#24449;&#65292;&#36825;&#26377;&#21161;&#20110;&#21306;&#20998;&#23545;&#25239;&#24615;&#31034;&#20363;&#12290;&#20026;&#20102;&#38450;&#27490;&#27531;&#20313;&#30340;&#25200;&#21160;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20108;&#27425;&#26368;&#20248;&#26041;&#21521;&#27531;&#24046;&#32593;&#32476;(QPRN)&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16176v1 Announce Type: cross  Abstract: Deep neural networks (DNNs) are notoriously vulnerable to adversarial attacks that place carefully crafted perturbations on normal examples to fool DNNs. To better understand such attacks, a characterization of the features carried by adversarial examples is needed. In this paper, we tackle this challenge by inspecting the subspaces of sample features through spectral analysis. We first empirically show that the features of either clean signals or adversarial perturbations are redundant and span in low-dimensional linear subspaces respectively with minimal overlap, and the classical low-dimensional subspace projection can suppress perturbation features out of the subspace of clean signals. This makes it possible for DNNs to learn a subspace where only features of clean signals exist while those of perturbations are discarded, which can facilitate the distinction of adversarial examples. To prevent the residual perturbations that is ine
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20934;&#30830;&#23450;&#20301;&#21644;&#24809;&#32602;&#24187;&#35273;&#26631;&#35760;&#65292;ESREAL&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#35821;&#20041;&#37325;&#24314;&#26469;&#25233;&#21046;&#29983;&#25104;&#24187;&#35273;&#65292;&#35299;&#20915;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16167</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#20041;&#37325;&#24314;&#20943;&#23569;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Exploiting Semantic Reconstruction to Mitigate Hallucinations in Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16167
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20934;&#30830;&#23450;&#20301;&#21644;&#24809;&#32602;&#24187;&#35273;&#26631;&#35760;&#65292;ESREAL&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#35821;&#20041;&#37325;&#24314;&#26469;&#25233;&#21046;&#29983;&#25104;&#24187;&#35273;&#65292;&#35299;&#20915;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#23545;&#20854;&#21487;&#38752;&#24615;&#26500;&#25104;&#37325;&#22823;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#25104;&#38271;&#26631;&#39064;&#26102;&#12290;&#24403;&#21069;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#35782;&#21035;&#21644;&#20943;&#36731;&#36825;&#20123;&#24187;&#35273;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ESREAL&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20934;&#30830;&#23450;&#20301;&#21644;&#24809;&#32602;&#24187;&#35273;&#26631;&#35760;&#26469;&#25233;&#21046;&#24187;&#35273;&#29983;&#25104;&#12290;&#26368;&#21021;&#65292;ESREAL&#26681;&#25454;&#29983;&#25104;&#30340;&#26631;&#39064;&#21019;&#24314;&#19968;&#20010;&#37325;&#24314;&#22270;&#20687;&#65292;&#24182;&#23558;&#20854;&#23545;&#24212;&#21306;&#22495;&#19982;&#21407;&#22987;&#22270;&#20687;&#30340;&#21306;&#22495;&#23545;&#40784;&#12290;&#36825;&#31181;&#35821;&#20041;&#37325;&#24314;&#26377;&#21161;&#20110;&#35782;&#21035;&#29983;&#25104;&#26631;&#39064;&#20013;&#30340;&#26631;&#35760;&#32423;&#24187;&#35273;&#30340;&#23384;&#22312;&#21644;&#31867;&#22411;&#12290;&#38543;&#21518;&#65292;ESREAL&#36890;&#36807;&#35780;&#20272;&#23545;&#40784;&#21306;&#22495;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26469;&#35745;&#31639;&#26631;&#35760;&#32423;&#24187;&#35273;&#20998;&#25968;&#65292;&#22522;&#20110;&#24187;&#35273;&#30340;&#31867;&#22411;&#12290;&#26368;&#21518;&#65292;ESREAL&#37319;&#29992;&#19968;&#31181;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#36827;&#34892;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16167v1 Announce Type: cross  Abstract: Hallucinations in vision-language models pose a significant challenge to their reliability, particularly in the generation of long captions. Current methods fall short of accurately identifying and mitigating these hallucinations. To address this issue, we introduce ESREAL, a novel unsupervised learning framework designed to suppress the generation of hallucinations through accurate localization and penalization of hallucinated tokens. Initially, ESREAL creates a reconstructed image based on the generated caption and aligns its corresponding regions with those of the original image. This semantic reconstruction aids in identifying both the presence and type of token-level hallucinations within the generated caption. Subsequently, ESREAL computes token-level hallucination scores by assessing the semantic similarity of aligned regions based on the type of hallucination. Finally, ESREAL employs a proximal policy optimization algorithm, wh
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;ChatGPT&#26500;&#24314;&#20102;&#38889;&#22269;&#29983;&#29289;&#21307;&#23398;&#35821;&#26009;&#24211;&#65288;KBMC&#65289;&#65292;&#22312;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#19987;&#38376;&#24037;&#20855;&#21644;&#25968;&#25454;&#38598;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16158</link><description>&lt;p&gt;
&#38889;&#22269;&#29983;&#29289;&#21307;&#23398;&#35821;&#26009;&#24211;&#65288;KBMC&#65289;&#29992;&#20110;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Korean Bio-Medical Corpus (KBMC) for Medical Named Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16158
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;ChatGPT&#26500;&#24314;&#20102;&#38889;&#22269;&#29983;&#29289;&#21307;&#23398;&#35821;&#26009;&#24211;&#65288;KBMC&#65289;&#65292;&#22312;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#19987;&#38376;&#24037;&#20855;&#21644;&#25968;&#25454;&#38598;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16158v1 &#20844;&#21578;&#31867;&#22411;: &#26032; &#25277;&#35937;: &#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#22312;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23578;&#26080;&#38024;&#23545;&#38889;&#35821;&#30340;&#24320;&#28304;&#21307;&#23398;NER&#25968;&#25454;&#38598;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;ChatGPT&#36741;&#21161;&#26500;&#24314;&#20102;KBMC&#65288;&#38889;&#22269;&#29983;&#29289;&#21307;&#23398;&#35821;&#26009;&#24211;&#65289;&#65292;&#29616;&#22312;&#21521;&#20844;&#20247;&#23637;&#31034;&#12290;&#20351;&#29992;KBMC&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#19982;&#22312;&#19968;&#33324;&#38889;&#35821;NER&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#21307;&#23398;NER&#24615;&#33021;&#25552;&#39640;&#20102;&#24778;&#20154;&#30340;20%&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#21033;&#29992;&#20687;ChatGPT&#36825;&#26679;&#30340;&#19987;&#38376;&#24037;&#20855;&#21644;&#25968;&#25454;&#38598;&#22312;&#22686;&#24378;&#35832;&#22914;&#21307;&#30103;&#20445;&#20581;&#31561;&#19987;&#19994;&#39046;&#22495;&#30340;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#37325;&#35201;&#20248;&#21183;&#21644;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16158v1 Announce Type: new  Abstract: Named Entity Recognition (NER) plays a pivotal role in medical Natural Language Processing (NLP). Yet, there has not been an open-source medical NER dataset specifically for the Korean language. To address this, we utilized ChatGPT to assist in constructing the KBMC (Korean Bio-Medical Corpus), which we are now presenting to the public. With the KBMC dataset, we noticed an impressive 20% increase in medical NER performance compared to models trained on general Korean NER datasets. This research underscores the significant benefits and importance of using specialized tools and datasets, like ChatGPT, to enhance language processing in specialized fields such as healthcare.
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#22522;&#20110;&#25237;&#24433;&#30340;&#27010;&#24565;&#21435;&#38500;&#26041;&#27861;&#20250;&#22312;&#36716;&#25442;&#21518;&#30340;&#25968;&#25454;&#38598;&#20013;&#27880;&#20837;&#24378;&#22823;&#30340;&#32479;&#35745;&#20381;&#36182;&#24615;&#65292;&#24182;&#23548;&#33268;&#34920;&#31034;&#31354;&#38388;&#39640;&#24230;&#32467;&#26500;&#21270;&#65292;&#20351;&#24471;&#21487;&#20197;&#36890;&#36807;&#24212;&#29992;&#21453;&#32858;&#31867;&#26041;&#27861;&#37325;&#24314;&#21407;&#22987;&#26631;&#35760;&#12290;</title><link>https://arxiv.org/abs/2403.16142</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#25237;&#24433;&#30340;&#27010;&#24565;&#21435;&#38500;&#26041;&#27861;&#23545;&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
What Happens to a Dataset Transformed by a Projection-based Concept Removal Method?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16142
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#25237;&#24433;&#30340;&#27010;&#24565;&#21435;&#38500;&#26041;&#27861;&#20250;&#22312;&#36716;&#25442;&#21518;&#30340;&#25968;&#25454;&#38598;&#20013;&#27880;&#20837;&#24378;&#22823;&#30340;&#32479;&#35745;&#20381;&#36182;&#24615;&#65292;&#24182;&#23548;&#33268;&#34920;&#31034;&#31354;&#38388;&#39640;&#24230;&#32467;&#26500;&#21270;&#65292;&#20351;&#24471;&#21487;&#20197;&#36890;&#36807;&#24212;&#29992;&#21453;&#32858;&#31867;&#26041;&#27861;&#37325;&#24314;&#21407;&#22987;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#32447;&#24615;&#25237;&#24433;&#26041;&#27861;&#20174;&#35821;&#35328;&#34920;&#31034;&#20013;&#21435;&#38500;&#27010;&#24565;&#20449;&#24687;&#30340;&#26041;&#27861;&#30340;&#34892;&#20026;&#65292;&#24182;&#32771;&#34385;&#20102;&#32463;&#36807;&#36825;&#31181;&#26041;&#27861;&#36716;&#25442;&#30340;&#25968;&#25454;&#38598;&#20250;&#21457;&#29983;&#20160;&#20040;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23545;&#30495;&#23454;&#19990;&#30028;&#21644;&#21512;&#25104;&#25968;&#25454;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#20250;&#21521;&#36716;&#25442;&#21518;&#30340;&#25968;&#25454;&#38598;&#20013;&#27880;&#20837;&#24378;&#22823;&#30340;&#32479;&#35745;&#20381;&#36182;&#24615;&#12290;&#24212;&#29992;&#27492;&#31867;&#26041;&#27861;&#21518;&#65292;&#34920;&#31034;&#31354;&#38388;&#20855;&#26377;&#39640;&#24230;&#32467;&#26500;&#21270;&#65306;&#22312;&#36716;&#25442;&#21518;&#30340;&#31354;&#38388;&#20013;&#65292;&#19968;&#20010;&#23454;&#20363;&#20542;&#21521;&#20110;&#20301;&#20110;&#30456;&#21453;&#26631;&#31614;&#30340;&#23454;&#20363;&#38468;&#36817;&#12290;&#22240;&#27492;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#24212;&#29992;&#21453;&#32858;&#31867;&#26041;&#27861;&#26469;&#37325;&#24314;&#21407;&#22987;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16142v1 Announce Type: cross  Abstract: We investigate the behavior of methods that use linear projections to remove information about a concept from a language representation, and we consider the question of what happens to a dataset transformed by such a method. A theoretical analysis and experiments on real-world and synthetic data show that these methods inject strong statistical dependencies into the transformed datasets. After applying such a method, the representation space is highly structured: in the transformed space, an instance tends to be located near instances of the opposite label. As a consequence, the original labeling can in some cases be reconstructed by applying an anti-clustering method.
&lt;/p&gt;</description></item><item><title>&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#36827;&#34892;&#35843;&#26597;&#65292;&#25506;&#35752;&#27844;&#28431;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#24314;&#31435;&#20102;&#27844;&#28431;&#29575;&#12289;&#29983;&#25104;&#29575;&#21644;&#26816;&#27979;&#29575;&#36825;&#19977;&#20010;&#26631;&#20934;&#65292;&#24182;&#23454;&#39564;&#38416;&#26126;&#20102;&#27844;&#28431;&#29575;&#19982;&#36755;&#20986;&#29575;&#20197;&#21450;&#26816;&#27979;&#29575;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.16139</link><description>&lt;p&gt;
&#19968;&#28857;&#23567;&#28431;&#27934;&#23601;&#20250;&#20351;&#19968;&#33368;&#24040;&#36718;&#27785;&#27809;&#65306;&#20174;&#22836;&#21040;&#23614;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#36827;&#34892;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Little Leak Will Sink a Great Ship: Survey of Transparency for Large Language Models from Start to Finish
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16139
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#36827;&#34892;&#35843;&#26597;&#65292;&#25506;&#35752;&#27844;&#28431;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#24314;&#31435;&#20102;&#27844;&#28431;&#29575;&#12289;&#29983;&#25104;&#29575;&#21644;&#26816;&#27979;&#29575;&#36825;&#19977;&#20010;&#26631;&#20934;&#65292;&#24182;&#23454;&#39564;&#38416;&#26126;&#20102;&#27844;&#28431;&#29575;&#19982;&#36755;&#20986;&#29575;&#20197;&#21450;&#26816;&#27979;&#29575;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#22312;&#28023;&#37327;&#32593;&#32476;&#25235;&#21462;&#30340;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#12290;&#36825;&#24102;&#26469;&#20102;&#27844;&#28431;&#39118;&#38505;&#65292;&#21253;&#25324;&#20010;&#20154;&#20449;&#24687;&#12289;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#25991;&#26412;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#27844;&#28431;&#23548;&#33268;&#20154;&#31867;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#20449;&#20219;&#21463;&#25439;&#65292;&#22240;&#20026;&#21487;&#33021;&#20250;&#26410;&#32463;&#25480;&#26435;&#22320;&#29983;&#25104;&#20869;&#23481;&#25110;&#39640;&#20272;&#24615;&#33021;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#20851;&#20110;&#27844;&#28431;&#38382;&#39064;&#30340;&#19977;&#20010;&#26631;&#20934;&#65306;(1)&#27844;&#28431;&#29575;&#65306;&#35757;&#32451;&#25968;&#25454;&#20013;&#27844;&#28431;&#25968;&#25454;&#30340;&#27604;&#20363;&#65292;(2)&#29983;&#25104;&#29575;&#65306;&#29983;&#25104;&#27844;&#28431;&#25968;&#25454;&#30340;&#38590;&#26131;&#31243;&#24230;&#65292;&#20197;&#21450;(3)&#26816;&#27979;&#29575;&#65306;&#26816;&#27979;&#27844;&#28431;&#25968;&#25454;&#19982;&#38750;&#27844;&#28431;&#25968;&#25454;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#27844;&#28431;&#29575;&#26159;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#30340;&#26681;&#28304;&#65292;&#20294;&#20154;&#20204;&#19981;&#28165;&#26970;&#23427;&#20250;&#22914;&#20309;&#24433;&#21709;&#36755;&#20986;&#29575;&#21644;&#26816;&#27979;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#35843;&#26597;&#65292;&#38416;&#26126;&#20102;&#20010;&#20154;&#20449;&#24687;&#12289;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#25991;&#26412;&#21644;&#22522;&#20934;&#25968;&#25454;&#30340;&#27844;&#28431;&#29575;&#19982;&#36755;&#20986;&#29575;&#20197;&#21450;&#26816;&#27979;&#29575;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16139v1 Announce Type: new  Abstract: Large Language Models (LLMs) are trained on massive web-crawled corpora. This poses risks of leakage, including personal information, copyrighted texts, and benchmark datasets. Such leakage leads to undermining human trust in AI due to potential unauthorized generation of content or overestimation of performance. We establish the following three criteria concerning the leakage issues: (1) leakage rate: the proportion of leaked data in training data, (2) output rate: the ease of generating leaked data, and (3) detection rate: the detection performance of leaked versus non-leaked data. Despite the leakage rate being the origin of data leakage issues, it is not understood how it affects the output rate and detection rate. In this paper, we conduct an experimental survey to elucidate the relationship between the leakage rate and both the output rate and detection rate for personal information, copyrighted texts, and benchmark data. Additiona
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#35789;&#27719;&#27495;&#20041;&#26816;&#27979;&#21644;&#35789;&#20041;&#28040;&#27495;&#39046;&#22495;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#20171;&#32461;&#20102;&#35789;&#20041;&#25193;&#23637;&#21644;&#31070;&#32463;&#32908;&#32905;&#25509;&#36817;&#27861;&#31561;&#21019;&#26032;&#26041;&#27861;&#65292;&#20197;&#39044;&#27979;&#26032;&#35789;&#20041;&#26469;&#25552;&#39640;&#28040;&#27495;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16129</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#35789;&#27719;&#27495;&#20041;&#26816;&#27979;&#21644;&#35789;&#20041;&#28040;&#27495;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Lexical Ambiguity Detection and Word Sense Disambiguation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#35789;&#27719;&#27495;&#20041;&#26816;&#27979;&#21644;&#35789;&#20041;&#28040;&#27495;&#39046;&#22495;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#20171;&#32461;&#20102;&#35789;&#20041;&#25193;&#23637;&#21644;&#31070;&#32463;&#32908;&#32905;&#25509;&#36817;&#27861;&#31561;&#21019;&#26032;&#26041;&#27861;&#65292;&#20197;&#39044;&#27979;&#26032;&#35789;&#20041;&#26469;&#25552;&#39640;&#28040;&#27495;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#19987;&#27880;&#20110;&#29702;&#35299;&#21644;&#35299;&#20915;&#35821;&#35328;&#20013;&#27495;&#20041;&#30340;&#25216;&#26415;&#65292;&#31361;&#20986;&#20102;&#35821;&#35328;&#29616;&#35937;&#22914;&#22810;&#20041;&#24615;&#21644;&#21516;&#38899;&#24322;&#20041;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#23427;&#20204;&#23545;&#35745;&#31639;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#37325;&#28857;&#25918;&#22312;&#20102;&#35789;&#20041;&#28040;&#27495;&#65288;WSD&#65289;&#19978;&#65292;&#27010;&#36848;&#20102;&#20174;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21040;&#21033;&#29992;&#35789;&#27719;&#36164;&#28304;&#21644;&#35789;&#20041;&#32593;&#65288;&#22914;WordNet&#65289;&#30340;&#22810;&#26679;&#21270;&#26041;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#35832;&#22914;&#35789;&#20041;&#25193;&#23637;&#65288;WSE&#65289;&#21644;&#31070;&#32463;&#32908;&#32905;&#25509;&#36817;&#27861;&#36825;&#26679;&#30340;&#23574;&#31471;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#26032;&#30340;&#35789;&#20041;&#26469;&#25552;&#39640;&#28040;&#27495;&#20934;&#30830;&#24230;&#12290;&#23427;&#32771;&#23519;&#20102;&#29983;&#29289;&#21307;&#23398;&#28040;&#27495;&#21644;&#35821;&#35328;&#29305;&#23450;&#20248;&#21270;&#30340;&#20855;&#20307;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#35748;&#30693;&#38544;&#21947;&#22312;&#35805;&#35821;&#20998;&#26512;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#30740;&#31350;&#30830;&#23450;&#20102;&#35813;&#39046;&#22495;&#30340;&#25345;&#20037;&#25361;&#25112;&#65292;&#20363;&#22914;&#35821;&#20041;&#26631;&#27880;&#35821;&#26009;&#24211;&#30340;&#31232;&#32570;&#24615;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16129v1 Announce Type: new  Abstract: This paper explores techniques that focus on understanding and resolving ambiguity in language within the field of natural language processing (NLP), highlighting the complexity of linguistic phenomena such as polysemy and homonymy and their implications for computational models. Focusing extensively on Word Sense Disambiguation (WSD), it outlines diverse approaches ranging from deep learning techniques to leveraging lexical resources and knowledge graphs like WordNet. The paper introduces cutting-edge methodologies like word sense extension (WSE) and neuromyotonic approaches, enhancing disambiguation accuracy by predicting new word senses. It examines specific applications in biomedical disambiguation and language specific optimisation and discusses the significance of cognitive metaphors in discourse analysis. The research identifies persistent challenges in the field, such as the scarcity of sense annotated corpora and the complexity 
&lt;/p&gt;</description></item><item><title>WangchanLion&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#27888;&#35821;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#25351;&#20196;&#24494;&#35843;&#27169;&#22411;&#65292;&#22312;0-shot&#21644;1-shot&#35774;&#32622;&#19979;&#33021;&#22815;&#29702;&#35299;&#19978;&#19979;&#25991;&#24182;&#20135;&#29983;&#19982;&#21442;&#32771;&#31572;&#26696;&#19968;&#33268;&#30340;&#22238;&#31572;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.16127</link><description>&lt;p&gt;
WangchanLion&#19982;WangchanX MRC&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
WangchanLion and WangchanX MRC Eval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16127
&lt;/p&gt;
&lt;p&gt;
WangchanLion&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#27888;&#35821;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#25351;&#20196;&#24494;&#35843;&#27169;&#22411;&#65292;&#22312;0-shot&#21644;1-shot&#35774;&#32622;&#19979;&#33021;&#22815;&#29702;&#35299;&#19978;&#19979;&#25991;&#24182;&#20135;&#29983;&#19982;&#21442;&#32771;&#31572;&#26696;&#19968;&#33268;&#30340;&#22238;&#31572;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#25551;&#36848;&#20102;WangchanLion&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#27888;&#35821;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;MRC&#65289;&#30340;&#25351;&#20196;&#24494;&#35843;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;SEA-LION&#21644;&#19968;&#31995;&#21015;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#20419;&#36827;&#24320;&#25918;&#30740;&#31350;&#21644;&#21487;&#37325;&#22797;&#24615;&#65292;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#25152;&#26377;&#35757;&#32451;&#25968;&#25454;&#12289;&#20195;&#30721;&#21644;&#26368;&#32456;&#27169;&#22411;&#26435;&#37325;&#65292;&#37319;&#29992;Apache-2&#35768;&#21487;&#35777;&#12290;&#20026;&#20102;&#35780;&#20272;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#27888;&#35821;MRC&#25968;&#25454;&#38598;XQuAD&#21644;Iapp_wiki_qa_squad&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;0-shot&#21644;1-shot&#35774;&#32622;&#19979;&#65292;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#19978;&#19979;&#25991;&#24182;&#20135;&#29983;&#19982;&#21442;&#32771;&#31572;&#26696;&#19968;&#33268;&#30340;&#22238;&#31572;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;MRC&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#65292;&#35780;&#20272;&#31572;&#26696;&#30340;&#27491;&#30830;&#24615;&#12289;&#24110;&#21161;&#24615;&#12289;&#31616;&#27905;&#24615;&#21644;&#19978;&#19979;&#25991;&#24615;&#12290;&#35780;&#20272;&#32467;&#26524;&#25581;&#31034;&#20102;&#25105;&#20204;&#22914;&#20309;&#25913;&#36827;&#27169;&#22411;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16127v1 Announce Type: cross  Abstract: This technical report describes the development of WangchanLion, an instruction fine-tuned model focusing on Machine Reading Comprehension (MRC) in the Thai language. Our model is based on SEA-LION and a collection of instruction following datasets. To promote open research and reproducibility, we publically release all training data, code, and the final model weights under the Apache-2 license. To assess the contextual understanding capability, we conducted extensive experimental studies using two Thai MRC datasets, XQuAD and Iapp_wiki_qa_squad. Experimental results demonstrate the model's ability to comprehend the context and produce an answer faithful to the reference one in 0-shot and 1-shot settings. In addition, our evaluation goes beyond the traditional MRC. We propose a new evaluation scheme assessing the answer's correctness, helpfulness, conciseness, and contextuality. Evaluation results provide insight into how we can improv
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24314;&#31435;&#19968;&#20221;&#21253;&#25324; 100 &#31687;&#25991;&#26723;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598; OBSINFOX&#65292;&#30740;&#31350;&#20102;&#20154;&#31867;&#19982;&#26426;&#22120;&#22312;&#35748;&#23450;&#20551;&#26032;&#38395;&#29305;&#24449;&#19978;&#30340;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#20102;&#35821;&#26009;&#24211;&#20013;&#35773;&#21050;&#25991;&#26412;&#30340;&#26222;&#36941;&#23384;&#22312;&#12290;</title><link>https://arxiv.org/abs/2403.16099</link><description>&lt;p&gt;
&#19968;&#20221;&#27861;&#22269;&#20551;&#26032;&#38395;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#65306;&#20154;&#31867;&#19982;&#26426;&#22120;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Multi-Label Dataset of French Fake News: Human and Machine Insights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16099
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24314;&#31435;&#19968;&#20221;&#21253;&#25324; 100 &#31687;&#25991;&#26723;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598; OBSINFOX&#65292;&#30740;&#31350;&#20102;&#20154;&#31867;&#19982;&#26426;&#22120;&#22312;&#35748;&#23450;&#20551;&#26032;&#38395;&#29305;&#24449;&#19978;&#30340;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#20102;&#35821;&#26009;&#24211;&#20013;&#35773;&#21050;&#25991;&#26412;&#30340;&#26222;&#36941;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001; 8 &#20301;&#27880;&#37322;&#32773;&#20351;&#29992; 11 &#20010;&#26631;&#31614;&#27880;&#37322;&#30340;&#26469;&#33258; 17 &#20010;&#27861;&#22269;&#34987;&#19987;&#23478;&#26426;&#26500;&#35748;&#20026;&#19981;&#21487;&#38752;&#30340;&#26032;&#38395;&#26469;&#28304;&#36873;&#21462;&#30340; 100 &#31687;&#25991;&#26723;&#30340;&#35821;&#26009;&#24211; OBSINFOX&#12290;&#36890;&#36807;&#25910;&#38598;&#27604;&#36890;&#24120;&#26356;&#22810;&#30340;&#26631;&#31614;&#21644;&#27880;&#37322;&#32773;&#65292;&#25105;&#20204;&#21487;&#20197;&#35782;&#21035;&#20154;&#31867;&#35748;&#20026;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#20551;&#26032;&#38395;&#30340;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#19982;&#33258;&#21160;&#20998;&#31867;&#22120;&#30340;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992; Gate Cloud &#36827;&#34892;&#20027;&#39064;&#21644;&#20307;&#35009;&#20998;&#26512;&#65292;&#36825;&#34920;&#26126;&#35821;&#26009;&#24211;&#20013;&#31867;&#20284;&#35773;&#21050;&#30340;&#25991;&#26412;&#26222;&#36941;&#23384;&#22312;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992; VAGO &#20027;&#35266;&#24615;&#20998;&#26512;&#22120;&#21450;&#20854;&#31070;&#32463;&#29256;&#26412;&#65292;&#20197;&#28548;&#28165;&#26631;&#31614;&#8220;&#20027;&#35266;&#8221;&#19982;&#26631;&#31614;&#8220;&#20551;&#26032;&#38395;&#8221;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#35813;&#24102;&#26377;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#21487;&#36890;&#36807;&#20197;&#19979;&#32593;&#22336;&#22312;&#32447;&#33719;&#21462;&#65306;https://github.com/obs-info/obsinfox
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16099v1 Announce Type: new  Abstract: We present a corpus of 100 documents, OBSINFOX, selected from 17 sources of French press considered unreliable by expert agencies, annotated using 11 labels by 8 annotators. By collecting more labels than usual, by more annotators than is typically done, we can identify features that humans consider as characteristic of fake news, and compare them to the predictions of automated classifiers. We present a topic and genre analysis using Gate Cloud, indicative of the prevalence of satire-like text in the corpus. We then use the subjectivity analyzer VAGO, and a neural version of it, to clarify the link between ascriptions of the label Subjective and ascriptions of the label Fake News. The annotated dataset is available online at the following url: https://github.com/obs-info/obsinfox   Keywords: Fake News, Multi-Labels, Subjectivity, Vagueness, Detail, Opinion, Exaggeration, French Press
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;APL&#65288;&#38463;&#25289;&#20271;&#32534;&#31243;&#35821;&#35328;&#65289;&#65292;&#23427;&#20351;&#29992;LLM&#20316;&#20026;&#21322;&#32534;&#35793;&#22120;&#65292;&#23558;&#38463;&#25289;&#20271;&#25991;&#26412;&#20195;&#30721;&#36716;&#25442;&#20026;Python&#20195;&#30721;&#24182;&#36816;&#34892;&#65292;&#26500;&#24314;&#20102;&#23436;&#25972;&#30340;&#27969;&#27700;&#32447;&#12290;</title><link>https://arxiv.org/abs/2403.16087</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#38463;&#25289;&#20271;&#32534;&#31243;&#35821;&#35328;&#30340;&#32534;&#35793;&#22120;
&lt;/p&gt;
&lt;p&gt;
LLMs as Compiler for Arabic Programming Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;APL&#65288;&#38463;&#25289;&#20271;&#32534;&#31243;&#35821;&#35328;&#65289;&#65292;&#23427;&#20351;&#29992;LLM&#20316;&#20026;&#21322;&#32534;&#35793;&#22120;&#65292;&#23558;&#38463;&#25289;&#20271;&#25991;&#26412;&#20195;&#30721;&#36716;&#25442;&#20026;Python&#20195;&#30721;&#24182;&#36816;&#34892;&#65292;&#26500;&#24314;&#20102;&#23436;&#25972;&#30340;&#27969;&#27700;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;APL (Arabic Programming Language)&#65292;&#23427;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20316;&#20026;&#21322;&#32534;&#35793;&#22120;&#65292;&#23558;&#38463;&#25289;&#20271;&#25991;&#26412;&#20195;&#30721;&#36716;&#25442;&#20026;Python&#20195;&#30721;&#65292;&#28982;&#21518;&#36816;&#34892;&#35813;&#20195;&#30721;&#12290;&#35774;&#35745;&#20102;&#20174;APL&#25991;&#26412;&#32467;&#26500;&#21040;&#25552;&#31034;&#65288;&#20351;&#29992;&#25552;&#31034;&#24037;&#31243;&#65289;&#20877;&#21040;&#20351;&#29992;PyRunner&#36816;&#34892;&#29983;&#25104;&#30340;Python&#20195;&#30721;&#30340;&#23436;&#25972;&#27969;&#27700;&#32447;&#12290;&#35813;&#39033;&#30446;&#21253;&#25324;&#19977;&#37096;&#20998;&#65306;Python&#24211;&#65292;&#20855;&#26377;&#31616;&#21333;&#30028;&#38754;&#30340;&#28216;&#20048;&#22330;&#21644;&#36825;&#31687;&#30740;&#31350;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16087v1 Announce Type: cross  Abstract: In this paper we introduce APL (Arabic Programming Language) that uses Large language models (LLM) as semi-compiler to covert Arabic text code to python code then run the code. Designing a full pipeline from the structure of the APL text then a prompt (using prompt engineering) then running the prodcued python code using PyRunner. This project has a three parts first python library, a playground with simple interface and this research paper.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#25351;&#23548;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65292;&#22914;&#20309;&#36890;&#36807;&#24341;&#20837;&#35770;&#35777;&#29702;&#35770;&#21644;&#24773;&#26223;&#65292;&#20351;&#20854;&#33021;&#22815;&#26356;&#21487;&#38752;&#22320;&#35780;&#20272;&#20105;&#35758;&#38382;&#39064;&#20013;&#30340;&#35770;&#35777;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.16084</link><description>&lt;p&gt;
&#22312;&#25351;&#23548;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#35770;&#35777;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Argument Quality Assessment in the Age of Instruction-Following Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16084
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#25351;&#23548;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65292;&#22914;&#20309;&#36890;&#36807;&#24341;&#20837;&#35770;&#35777;&#29702;&#35770;&#21644;&#24773;&#26223;&#65292;&#20351;&#20854;&#33021;&#22815;&#26356;&#21487;&#38752;&#22320;&#35780;&#20272;&#20105;&#35758;&#38382;&#39064;&#20013;&#30340;&#35770;&#35777;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20105;&#35758;&#38382;&#39064;&#19978;&#30340;&#35770;&#35777;&#22312;NLP&#30740;&#31350;&#20013;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#30001;&#20110;&#20854;&#23545;&#35266;&#28857;&#24418;&#25104;&#12289;&#20915;&#31574;&#21046;&#23450;&#12289;&#20889;&#20316;&#25945;&#32946;&#31561;&#26041;&#38754;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#22312;&#20219;&#20309;&#27492;&#31867;&#24212;&#29992;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#26159;&#35780;&#20272;&#35770;&#35777;&#30340;&#36136;&#37327;&#65292;&#20294;&#36825;&#20063;&#26159;&#19968;&#20010;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#20174;&#23545;&#35770;&#35777;&#36136;&#37327;&#30740;&#31350;&#30340;&#31616;&#35201;&#35843;&#26597;&#24320;&#22987;&#65292;&#25105;&#20204;&#30830;&#23450;&#36136;&#37327;&#27010;&#24565;&#30340;&#22810;&#26679;&#24615;&#21644;&#20854;&#24863;&#30693;&#30340;&#20027;&#35266;&#24615;&#26159;&#23454;&#29616;&#35770;&#35777;&#36136;&#37327;&#35780;&#20272;&#23454;&#36136;&#24615;&#36827;&#23637;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#25351;&#23548;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21033;&#29992;&#36328;&#19978;&#19979;&#25991;&#30340;&#30693;&#35782;&#33021;&#21147;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#21487;&#38752;&#30340;&#35780;&#20272;&#12290;&#23427;&#20204;&#19981;&#20165;&#38656;&#35201;&#23558;LLMs&#24494;&#35843;&#20026;&#35780;&#20272;&#20219;&#21153;&#30340;&#39046;&#20808;&#32773;&#65292;&#36824;&#38656;&#35201;&#31995;&#32479;&#22320;&#29992;&#35770;&#35777;&#29702;&#35770;&#21644;&#24773;&#26223;&#20197;&#21450;&#35299;&#20915;&#38382;&#39064;&#30340;&#26041;&#24335;&#23545;&#20854;&#36827;&#34892;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16084v1 Announce Type: new  Abstract: The computational treatment of arguments on controversial issues has been subject to extensive NLP research, due to its envisioned impact on opinion formation, decision making, writing education, and the like. A critical task in any such application is the assessment of an argument's quality - but it is also particularly challenging. In this position paper, we start from a brief survey of argument quality research, where we identify the diversity of quality notions and the subjectiveness of their perception as the main hurdles towards substantial progress on argument quality assessment. We argue that the capabilities of instruction-following large language models (LLMs) to leverage knowledge across contexts enable a much more reliable assessment. Rather than just fine-tuning LLMs towards leaderboard chasing on assessment tasks, they need to be instructed systematically with argumentation theories and scenarios as well as with ways to sol
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#20013;&#21307;&#39046;&#22495;&#26500;&#24314;&#20102;&#19987;&#19994;&#35821;&#26009;&#24211;&#65292;&#22522;&#20110;LLaMA&#25104;&#21151;&#24320;&#21457;&#20102;&#39318;&#20010;&#32463;&#36807;&#23436;&#25972;&#35757;&#32451;&#30340;Qibo&#27169;&#22411;&#65292;&#24182;&#25512;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;LLMs&#24615;&#33021;&#30340;Qibo&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2403.16056</link><description>&lt;p&gt;
Qibo: &#19968;&#31181;&#29992;&#20110;&#20013;&#21307;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Qibo: A Large Language Model for Traditional Chinese Medicine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#20013;&#21307;&#39046;&#22495;&#26500;&#24314;&#20102;&#19987;&#19994;&#35821;&#26009;&#24211;&#65292;&#22522;&#20110;LLaMA&#25104;&#21151;&#24320;&#21457;&#20102;&#39318;&#20010;&#32463;&#36807;&#23436;&#25972;&#35757;&#32451;&#30340;Qibo&#27169;&#22411;&#65292;&#24182;&#25512;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;LLMs&#24615;&#33021;&#30340;Qibo&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#22312;&#29992;&#25143;&#24847;&#22270;&#29702;&#35299;&#21644;&#21709;&#24212;&#26041;&#38754;&#21462;&#24471;&#30340;&#26174;&#33879;&#36827;&#23637;&#65292;&#22312;&#35768;&#22810;&#19987;&#19994;&#39046;&#22495;&#65292;&#21253;&#25324;&#21307;&#23398;&#12289;&#27861;&#24459;&#21644;&#37329;&#34701;&#12290;&#28982;&#32780;&#65292;&#22312;&#20013;&#21307;&#39046;&#22495;&#65292;LLMs&#30340;&#24615;&#33021;&#25552;&#21319;&#21463;&#21040;&#25361;&#25112;&#65292;&#20854;&#21407;&#22240;&#22312;&#20110;&#20013;&#21307;&#29702;&#35770;&#19982;&#29616;&#20195;&#21307;&#23398;&#20043;&#38388;&#30340;&#26681;&#26412;&#24046;&#24322;&#65292;&#20197;&#21450;&#32570;&#20047;&#19987;&#19994;&#35821;&#26009;&#24211;&#36164;&#28304;&#12290;&#26412;&#25991;&#26088;&#22312;&#26500;&#24314;&#21644;&#25972;&#29702;&#20013;&#21307;&#39046;&#22495;&#30340;&#19987;&#19994;&#35821;&#26009;&#24211;&#65292;&#36171;&#20104;&#22823;&#22411;&#27169;&#22411;&#20855;&#26377;&#20013;&#21307;&#29702;&#35770;&#29305;&#33394;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#25104;&#21151;&#22522;&#20110;LLaMA&#24320;&#21457;&#20102;Qibo&#27169;&#22411;&#65292;&#36825;&#26159;&#20013;&#21307;&#39046;&#22495;&#31532;&#19968;&#20010;&#32463;&#36807;&#23436;&#25972;&#35757;&#32451;&#36807;&#31243;&#65288;&#20174;&#39044;&#35757;&#32451;&#21040;&#30417;&#30563;&#24494;&#35843;&#65289;&#30340;LLM&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Qibo&#22522;&#20934;&#27979;&#35797;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#24615;&#33021;&#30340;&#19987;&#38376;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16056v1 Announce Type: cross  Abstract: In the field of Artificial Intelligence, Large Language Models (LLMs) have demonstrated significant advances in user intent understanding and response in a number of specialized domains, including medicine, law, and finance. However, in the unique domain of traditional Chinese medicine (TCM), the performance enhancement of LLMs is challenged by the essential differences between its theories and modern medicine, as well as the lack of specialized corpus resources. In this paper, we aim to construct and organize a professional corpus in the field of TCM, to endow the large model with professional knowledge that is characteristic of TCM theory, and to successfully develop the Qibo model based on LLaMA, which is the first LLM in the field of TCM to undergo a complete training process from pre-training to Supervised Fine-Tuning (SFT). Furthermore, we develop the Qibo-benchmark, a specialized tool for evaluating the performance of LLMs, whic
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21333;&#35843;&#37322;&#20041;&#65288;MonoPara&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#37322;&#20041;LM&#21644;&#30446;&#26631;LM&#38598;&#25104;&#35299;&#30721;&#36807;&#31243;&#65292;&#23558;&#25552;&#31034;&#25110;&#25351;&#20196;&#37322;&#20041;&#20026;&#20302;&#22256;&#24785;&#24230;&#30340;&#29256;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.16038</link><description>&lt;p&gt;
&#21333;&#35843;&#37322;&#20041;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Monotonic Paraphrasing Improves Generalization of Language Model Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16038
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21333;&#35843;&#37322;&#20041;&#65288;MonoPara&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#37322;&#20041;LM&#21644;&#30446;&#26631;LM&#38598;&#25104;&#35299;&#30721;&#36807;&#31243;&#65292;&#23558;&#25552;&#31034;&#25110;&#25351;&#20196;&#37322;&#20041;&#20026;&#20302;&#22256;&#24785;&#24230;&#30340;&#29256;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#29616;&#21487;&#33021;&#20250;&#38543;&#30528;&#21516;&#19968;&#20219;&#21153;&#30340;&#19981;&#21516;&#25552;&#31034;&#25110;&#25351;&#20196;&#32780;&#21464;&#21270;&#12290;&#36825;&#31181;&#29616;&#35937;&#30340;&#19968;&#20010;&#20844;&#35748;&#22240;&#32032;&#26159;&#27169;&#22411;&#23545;&#32473;&#23450;&#25552;&#31034;&#25110;&#25351;&#20196;&#30340;&#29087;&#24713;&#31243;&#24230;&#65292;&#36890;&#24120;&#36890;&#36807;&#20854;&#22256;&#24785;&#24230;&#26469;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#21487;&#33021;&#25552;&#31034;&#30701;&#35821;&#30340;&#24040;&#22823;&#31354;&#38388;&#65292;&#25214;&#21040;&#22256;&#24785;&#24230;&#26368;&#20302;&#30340;&#25552;&#31034;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21333;&#35843;&#37322;&#20041;&#65288;MonoPara&#65289;&#65292;&#19968;&#31181;&#31471;&#21040;&#31471;&#35299;&#30721;&#31574;&#30053;&#65292;&#26681;&#25454;&#37322;&#20041;LM&#21644;&#30446;&#26631;LM&#65288;&#21363;&#25552;&#31034;&#25110;&#25351;&#20196;&#25191;&#34892;&#22120;&#65289;&#30340;&#38598;&#21512;&#26469;&#23558;&#32473;&#23450;&#25552;&#31034;&#25110;&#25351;&#20196;&#37322;&#20041;&#21270;&#20026;&#20854;&#20302;&#22256;&#24785;&#24230;&#30340;&#23545;&#24212;&#29289;&#12290;&#38598;&#21512;&#35299;&#30721;&#36807;&#31243;&#21487;&#20197;&#26377;&#25928;&#22320;&#37322;&#20041;&#21407;&#22987;&#25552;&#31034;&#32780;&#19981;&#25913;&#21464;&#20854;&#35821;&#20041;&#21547;&#20041;&#65292;&#21516;&#26102;&#21333;&#35843;&#22320;&#38477;&#20302;&#27599;&#20010;&#29983;&#25104;&#29289;&#30340;&#22256;&#24785;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16038v1 Announce Type: new  Abstract: Performance of large language models (LLMs) may vary with different prompts or instructions of even the same task. One commonly recognized factor for this phenomenon is the model's familiarity with the given prompt or instruction, which is typically estimated by its perplexity. However, finding the prompt with the lowest perplexity is challenging, given the enormous space of possible prompting phrases. In this paper, we propose monotonic paraphrasing (MonoPara), an end-to-end decoding strategy that paraphrases given prompts or instructions into their lower perplexity counterparts based on an ensemble of a paraphrase LM for prompt (or instruction) rewriting, and a target LM (i.e. the prompt or instruction executor) that constrains the generation for lower perplexity. The ensemble decoding process can efficiently paraphrase the original prompt without altering its semantic meaning, while monotonically decreasing the perplexity of each gene
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#20041;-&#32467;&#26500;&#27880;&#24847;&#22686;&#24378;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;SSA-GCN&#65289;&#65292;&#33021;&#22815;&#21516;&#26102;&#27169;&#25311;&#22270;&#32467;&#26500;&#24182;&#20174;&#30693;&#35782;&#22270;&#35889;&#21644;&#22797;&#26434;&#32593;&#32476;&#30340;&#35282;&#24230;&#25552;&#21462;&#26080;&#30417;&#30563;&#29305;&#24449;&#65292;&#20197;&#25552;&#21319;&#33410;&#28857;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16033</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;-&#32467;&#26500;&#27880;&#24847;&#22686;&#24378;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Node Classification via Semantic-Structural Attention-Enhanced Graph Convolutional Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16033
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#20041;-&#32467;&#26500;&#27880;&#24847;&#22686;&#24378;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;SSA-GCN&#65289;&#65292;&#33021;&#22815;&#21516;&#26102;&#27169;&#25311;&#22270;&#32467;&#26500;&#24182;&#20174;&#30693;&#35782;&#22270;&#35889;&#21644;&#22797;&#26434;&#32593;&#32476;&#30340;&#35282;&#24230;&#25552;&#21462;&#26080;&#30417;&#30563;&#29305;&#24449;&#65292;&#20197;&#25552;&#21319;&#33410;&#28857;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25968;&#25454;&#65292;&#20063;&#31216;&#20026;&#22797;&#26434;&#32593;&#32476;&#25968;&#25454;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#21644;&#24212;&#29992;&#20013;&#26080;&#22788;&#19981;&#22312;&#12290;&#20043;&#21069;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20027;&#35201;&#19987;&#27880;&#20110;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#25552;&#21462;&#29305;&#23450;&#20219;&#21153;&#30340;&#32467;&#26500;&#29305;&#24449;&#65292;&#20294;&#22312;&#25429;&#25417;&#25972;&#20010;&#22270;&#30340;&#22266;&#26377;&#35821;&#20041;&#21644;&#32467;&#26500;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35821;&#20041;-&#32467;&#26500;&#27880;&#24847;&#22686;&#24378;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;SSA-GCN&#65289;&#65292;&#19981;&#20165;&#27169;&#25311;&#20102;&#22270;&#32467;&#26500;&#65292;&#36824;&#20174;&#24635;&#20307;&#19978;&#25552;&#21462;&#20102;&#26080;&#30417;&#30563;&#29305;&#24449;&#20197;&#22686;&#24378;&#39030;&#28857;&#20998;&#31867;&#24615;&#33021;&#12290;SSA-GCN&#30340;&#20851;&#38190;&#36129;&#29486;&#22312;&#19977;&#20010;&#26041;&#38754;&#34920;&#29616;&#65306;&#39318;&#20808;&#65292;&#23427;&#36890;&#36807;&#20174;&#30693;&#35782;&#22270;&#35889;&#30340;&#35282;&#24230;&#36827;&#34892;&#26080;&#30417;&#30563;&#29305;&#24449;&#25552;&#21462;&#26469;&#33719;&#24471;&#35821;&#20041;&#20449;&#24687;&#65307;&#20854;&#27425;&#65292;&#23427;&#36890;&#36807;&#20174;&#22797;&#26434;&#32593;&#32476;&#30340;&#35282;&#24230;&#36827;&#34892;&#26080;&#30417;&#30563;&#29305;&#24449;&#25552;&#21462;&#26469;&#33719;&#24471;&#32467;&#26500;&#20449;&#24687;&#65307;&#26368;&#21518;&#65292;&#23427;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#23558;&#36825;&#20123;&#29305;&#24449;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16033v1 Announce Type: cross  Abstract: Graph data, also known as complex network data, is omnipresent across various domains and applications. Prior graph neural network models primarily focused on extracting task-specific structural features through supervised learning objectives, but they fell short in capturing the inherent semantic and structural features of the entire graph. In this paper, we introduce the semantic-structural attention-enhanced graph convolutional network (SSA-GCN), which not only models the graph structure but also extracts generalized unsupervised features to enhance vertex classification performance. The SSA-GCN's key contributions lie in three aspects: firstly, it derives semantic information through unsupervised feature extraction from a knowledge graph perspective; secondly, it obtains structural information through unsupervised feature extraction from a complex network perspective; and finally, it integrates these features through a cross-attent
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35774;&#35745;&#19987;&#38376;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#35748;&#30693;&#34892;&#20026;&#30103;&#27861;&#21407;&#21017;&#30340;&#25552;&#31034;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;CBT-LLM&#65292;&#19968;&#20010;&#38024;&#23545;&#20013;&#22269;&#24515;&#29702;&#20581;&#24247;&#38382;&#31572;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#24515;&#29702;&#25903;&#25345;&#31934;&#24230;&#21644;&#25928;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.16008</link><description>&lt;p&gt;
CBT-LLM&#65306;&#22522;&#20110;&#35748;&#30693;&#34892;&#20026;&#30103;&#27861;&#30340;&#24515;&#29702;&#20581;&#24247;&#38382;&#31572;&#30340;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16008
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35774;&#35745;&#19987;&#38376;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#35748;&#30693;&#34892;&#20026;&#30103;&#27861;&#21407;&#21017;&#30340;&#25552;&#31034;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;CBT-LLM&#65292;&#19968;&#20010;&#38024;&#23545;&#20013;&#22269;&#24515;&#29702;&#20581;&#24247;&#38382;&#31572;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#24515;&#29702;&#25903;&#25345;&#31934;&#24230;&#21644;&#25928;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#31361;&#26174;&#20986;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#38024;&#23545;&#24515;&#29702;&#20581;&#24247;&#26381;&#21153;&#24179;&#21488;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#21462;&#24471;&#20102;&#21021;&#27493;&#25104;&#21151;, &#20294;&#22312;&#25968;&#25454;&#31232;&#32570;&#12289;&#36136;&#37327;&#38382;&#39064;&#20197;&#21450;&#30830;&#20445;&#22312;&#24515;&#29702;&#25216;&#26415;&#26041;&#38754;&#24314;&#31435;&#29282;&#22266;&#22522;&#30784;&#31561;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#24515;&#29702;&#25903;&#25345;&#31934;&#24230;&#21644;&#25928;&#21147;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#35748;&#30693;&#34892;&#20026;&#30103;&#27861;&#21407;&#21017;&#30340;&#29305;&#23450;&#25552;&#31034;&#65292;&#24182;&#29983;&#25104;&#20102;CBT QA&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#29992;&#20110;&#22522;&#20110;CBT&#32467;&#26500;&#21270;&#24178;&#39044;&#31574;&#30053;&#30340;&#20013;&#22269;&#24515;&#29702;&#20581;&#24247;&#38382;&#31572;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#24378;&#35843;&#19987;&#19994;&#21644;&#32467;&#26500;&#21270;&#30340;&#22238;&#24212;&#12290;&#21033;&#29992;&#36825;&#19968;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#21019;&#36896;&#20986;CBT-LLM&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16008v1 Announce Type: new  Abstract: The recent advancements in artificial intelligence highlight the potential of language models in psychological health support. While models trained on data from mental health service platform have achieved preliminary success, challenges persist in areas such as data scarcity, quality, and ensuring a solid foundation in psychological techniques. To address these challenges, this study introduces a novel approach to enhance the precision and efficacy of psychological support through large language models. Specifically, we design a specific prompt derived from principles of Cognitive Behavioral Therapy (CBT) and have generated the CBT QA dataset, specifically for Chinese psychological health Q&amp;A based on CBT structured intervention strategies. Unlike previous methods, our dataset emphasizes professional and structured response. Utilizing this dataset, we fine-tuned the large language model, giving birth to CBT-LLM, the large-scale language
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#37324;&#31243;&#30865;&#25968;&#25454;&#38598;BIMCV-R&#65292;&#21253;&#21547;8,069&#20010;3D CT&#20307;&#31215;&#21644;&#20854;&#25918;&#23556;&#23398;&#25253;&#21578;&#65292;&#21516;&#26102;&#24320;&#21457;&#20102;&#26816;&#32034;&#31574;&#30053;MedFinder&#65292;&#20026;3D&#21307;&#23398;&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#39046;&#22495;&#25552;&#20379;&#20102;&#37325;&#35201;&#36129;&#29486;</title><link>https://arxiv.org/abs/2403.15992</link><description>&lt;p&gt;
BIMCV-R&#65306;&#29992;&#20110;3D CT&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#30340;&#37324;&#31243;&#30865;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BIMCV-R: A Landmark Dataset for 3D CT Text-Image Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15992
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#37324;&#31243;&#30865;&#25968;&#25454;&#38598;BIMCV-R&#65292;&#21253;&#21547;8,069&#20010;3D CT&#20307;&#31215;&#21644;&#20854;&#25918;&#23556;&#23398;&#25253;&#21578;&#65292;&#21516;&#26102;&#24320;&#21457;&#20102;&#26816;&#32034;&#31574;&#30053;MedFinder&#65292;&#20026;3D&#21307;&#23398;&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#39046;&#22495;&#25552;&#20379;&#20102;&#37325;&#35201;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15992v1 &#21457;&#24067;&#31867;&#22411;: &#36328;&#36234;  &#25688;&#35201;: &#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#19982;&#21307;&#30103;&#20445;&#20581;&#30340;&#34701;&#21512;&#19981;&#26029;&#22686;&#21152;&#20102;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#30340;&#24037;&#20316;&#37327;&#12290;&#20026;&#20102;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#22312;&#35786;&#26029;&#36807;&#31243;&#20013;&#65292;&#20943;&#36731;&#20854;&#24037;&#20316;&#37327;&#65292;&#24320;&#21457;&#19968;&#20010;&#21487;&#38752;&#30340;&#26816;&#32034;&#30456;&#20284;&#30149;&#20363;&#30740;&#31350;&#30340;&#31995;&#32479;&#26159;&#19968;&#20010;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#36825;&#19968;&#27010;&#24565;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#26159;&#30446;&#21069;3D&#21307;&#23398;&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#39046;&#22495;&#21463;&#38480;&#20110;&#32570;&#20047;&#20581;&#20840;&#30340;&#35780;&#20272;&#22522;&#20934;&#21644;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;BIMCV-R&#65288;&#27492;&#25968;&#25454;&#38598;&#23558;&#22312;&#25509;&#21463;&#21518;&#21457;&#24067;&#12290;&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;8,069&#20010;3D CT&#20307;&#31215;&#30340;&#24191;&#27867;&#25910;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;200&#19975;&#24352;&#20999;&#29255;&#65292;&#20197;&#21450;&#23427;&#20204;&#21508;&#33258;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#22312;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25299;&#23637;&#20102;&#19968;&#31181;&#26816;&#32034;&#31574;&#30053;&#65292;MedFinder&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#21452;&#27969;&#32593;&#32476;&#26550;&#26500;&#65292;&#21033;&#29992;&#22823;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15992v1 Announce Type: cross  Abstract: The burgeoning integration of 3D medical imaging into healthcare has led to a substantial increase in the workload of medical professionals. To assist clinicians in their diagnostic processes and alleviate their workload, the development of a robust system for retrieving similar case studies presents a viable solution. While the concept holds great promise, the field of 3D medical text-image retrieval is currently limited by the absence of robust evaluation benchmarks and curated datasets. To remedy this, our study presents a groundbreaking dataset, BIMCV-R (This dataset will be released upon acceptance.), which includes an extensive collection of 8,069 3D CT volumes, encompassing over 2 million slices, paired with their respective radiological reports. Expanding upon the foundational work of our dataset, we craft a retrieval strategy, MedFinder. This approach employs a dual-stream network architecture, harnessing the potential of larg
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;IllusionVQA&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27979;&#35797;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#38169;&#35273;&#21644;&#38590;&#35299;&#22330;&#26223;&#19979;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#29702;&#35299;&#20219;&#21153;&#21644;&#23450;&#20301;&#20219;&#21153;&#19978;&#65292;&#34920;&#29616;&#26368;&#20339;&#30340;VLM&#20026;GPT4V&#65292;&#32780;&#20154;&#31867;&#34920;&#29616;&#26356;&#32988;&#19968;&#31609;&#12290;</title><link>https://arxiv.org/abs/2403.15952</link><description>&lt;p&gt;
IllusionVQA&#65306;&#19968;&#20010;&#25361;&#25112;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35273;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
IllusionVQA: A Challenging Optical Illusion Dataset for Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15952
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;IllusionVQA&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27979;&#35797;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#38169;&#35273;&#21644;&#38590;&#35299;&#22330;&#26223;&#19979;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#29702;&#35299;&#20219;&#21153;&#21644;&#23450;&#20301;&#20219;&#21153;&#19978;&#65292;&#34920;&#29616;&#26368;&#20339;&#30340;VLM&#20026;GPT4V&#65292;&#32780;&#20154;&#31867;&#34920;&#29616;&#26356;&#32988;&#19968;&#31609;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#30340;&#20986;&#29616;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#35843;&#26597;&#31070;&#32463;&#32593;&#32476;&#30340;&#35270;&#35273;&#29702;&#35299;&#12290; VLM&#19981;&#20165;&#33021;&#22815;&#36827;&#34892;&#23545;&#35937;&#20998;&#31867;&#21644;&#26816;&#27979;&#65292;&#36824;&#33021;&#22815;&#36827;&#34892;&#35270;&#35273;&#29702;&#35299;&#21644;&#24120;&#35782;&#25512;&#29702;&#12290; &#36825;&#33258;&#28982;&#32780;&#28982;&#22320;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#24403;&#22270;&#20687;&#26412;&#36523;&#26159;&#19981;&#21512;&#29702;&#30340;&#26102;&#65292;VLM&#20250;&#22914;&#20309;&#22238;&#24212;&#65311; &#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IllusionVQA&#65306;&#19968;&#20010;&#21253;&#21547;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20809;&#23398;&#38169;&#35273;&#21644;&#38590;&#20197;&#35299;&#37322;&#30340;&#22330;&#26223;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#65292;&#20197;&#27979;&#35797;VLM&#22312;&#20004;&#31181;&#19981;&#21516;&#30340;&#22810;&#36873;VQA&#20219;&#21153; - &#29702;&#35299;&#21644;&#36719;&#23450;&#20301;&#30340;&#33021;&#21147;&#12290; &#34920;&#29616;&#26368;&#20339;&#30340;VLM GPT4V&#22312;&#29702;&#35299;&#20219;&#21153;&#65288;4-shot&#65289;&#19978;&#23454;&#29616;&#20102;62.99&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#23450;&#20301;&#20219;&#21153;&#65288;4-shot&#21644;Chain-of-Thought&#65289;&#19978;&#23454;&#29616;&#20102;49.7&#65285;&#30340;&#20934;&#30830;&#29575;&#12290; &#20154;&#31867;&#35780;&#20272;&#34920;&#26126;&#65292;&#20154;&#31867;&#22312;&#29702;&#35299;&#21644;&#23450;&#20301;&#26041;&#38754;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;91.03&#65285;&#21644;100&#65285;&#12290; &#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#21644;Chain-of-Thought&#25512;&#29702;&#26041;&#38754;&#26377;&#24456;&#22823;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15952v1 Announce Type: cross  Abstract: The advent of Vision Language Models (VLM) has allowed researchers to investigate the visual understanding of a neural network using natural language. Beyond object classification and detection, VLMs are capable of visual comprehension and common-sense reasoning. This naturally led to the question: How do VLMs respond when the image itself is inherently unreasonable? To this end, we present IllusionVQA: a diverse dataset of challenging optical illusions and hard-to-interpret scenes to test the capability of VLMs in two distinct multiple-choice VQA tasks - comprehension and soft localization. GPT4V, the best-performing VLM, achieves 62.99% accuracy (4-shot) on the comprehension task and 49.7% on the localization task (4-shot and Chain-of-Thought). Human evaluation reveals that humans achieve 91.03% and 100% accuracy in comprehension and localization. We discover that In-Context Learning (ICL) and Chain-of-Thought reasoning substantially
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22320;&#29702;&#20195;&#24065;&#30340;&#27010;&#24565;&#65292;&#23558;&#20854;&#20316;&#20026;&#21464;&#21387;&#22120;&#30340;&#36755;&#20837;&#32452;&#20214;&#19982;&#20855;&#20307;&#22320;&#29702;&#20301;&#32622;&#32852;&#31995;&#36215;&#26469;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#38024;&#23545;&#29699;&#38754;&#22352;&#26631;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.15940</link><description>&lt;p&gt;
&#22320;&#29702;&#20195;&#24065;&#19982;&#22320;&#29702;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Geotokens and Geotransformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22320;&#29702;&#20195;&#24065;&#30340;&#27010;&#24565;&#65292;&#23558;&#20854;&#20316;&#20026;&#21464;&#21387;&#22120;&#30340;&#36755;&#20837;&#32452;&#20214;&#19982;&#20855;&#20307;&#22320;&#29702;&#20301;&#32622;&#32852;&#31995;&#36215;&#26469;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#38024;&#23545;&#29699;&#38754;&#22352;&#26631;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#65292;&#20301;&#32622;&#32534;&#30721;&#20027;&#35201;&#20026;&#36755;&#20837;&#20195;&#24065;&#25552;&#20379;&#20102;&#24207;&#21015;&#30340;&#24847;&#20041;&#12290;&#21407;&#22987;&#21464;&#21387;&#22120;&#35770;&#25991;&#30340;&#26041;&#27861;&#22312;&#19968;&#33324;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#20294;&#20063;&#26377;&#20102;&#26032;&#30340;&#25552;&#35758;&#65292;&#22914;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#65288;RoPE&#65289;&#65292;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22320;&#29702;&#20195;&#24065;&#65292;&#21464;&#21387;&#22120;&#30340;&#36755;&#20837;&#32452;&#20214;&#65292;&#27599;&#19968;&#20010;&#20195;&#24065;&#19982;&#29305;&#23450;&#22320;&#36136;&#20301;&#32622;&#30456;&#36830;&#12290;&#19982;&#20856;&#22411;&#30340;&#35821;&#35328;&#24207;&#21015;&#19981;&#21516;&#65292;&#23545;&#20110;&#36825;&#20123;&#20195;&#24065;&#65292;&#39034;&#24207;&#24182;&#19981;&#20687;&#22320;&#29702;&#22352;&#26631;&#26412;&#36523;&#37027;&#26679;&#37325;&#35201;&#12290;&#20026;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#34920;&#31034;&#30456;&#23545;&#20301;&#32622;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#36317;&#31163;&#21644;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#36317;&#31163;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65292;&#20511;&#37492;&#20110;RoPE&#32467;&#26500;&#20294;&#19987;&#20026;&#29699;&#38754;&#22352;&#26631;&#23450;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15940v1 Announce Type: cross  Abstract: In transformer architectures, position encoding primarily provides a sense of sequence for input tokens. While the original transformer paper's method has shown satisfactory results in general language processing tasks, there have been new proposals, such as Rotary Position Embedding (RoPE), for further improvement. This paper presents geotokens, input components for transformers, each linked to a specific geological location. Unlike typical language sequences, for these tokens, the order is not as vital as the geographical coordinates themselves. To represent the relative position in this context and to keep a balance between the real world distance and the distance in the embedding space, we design a position encoding approach drawing from the RoPE structure but tailored for spherical coordinates.
&lt;/p&gt;</description></item><item><title>LlamBERT&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#27880;&#37322;&#26410;&#26631;&#35760;&#25968;&#25454;&#24211;&#24182;&#29992;&#20110;&#24494;&#35843;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#22312;&#38477;&#20302;&#25104;&#26412;&#30340;&#21516;&#26102;&#30053;&#24494;&#29306;&#29298;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15938</link><description>&lt;p&gt;
LlamBERT&#65306;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22823;&#35268;&#27169;&#12289;&#20302;&#25104;&#26412;&#30340;&#25968;&#25454;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
LlamBERT: Large-scale low-cost data annotation in NLP
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15938
&lt;/p&gt;
&lt;p&gt;
LlamBERT&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#27880;&#37322;&#26410;&#26631;&#35760;&#25968;&#25454;&#24211;&#24182;&#29992;&#20110;&#24494;&#35843;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#22312;&#38477;&#20302;&#25104;&#26412;&#30340;&#21516;&#26102;&#30053;&#24494;&#29306;&#29298;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#22914;GPT-4&#21644;Llama 2&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#23427;&#20204;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#19982;&#23427;&#20204;&#30340;&#20351;&#29992;&#30456;&#20851;&#30340;&#39640;&#25104;&#26412;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LlamBERT&#65292;&#36825;&#26159;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#23545;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#24211;&#30340;&#23567;&#23376;&#38598;&#36827;&#34892;&#27880;&#37322;&#65292;&#24182;&#23558;&#32467;&#26524;&#29992;&#20110;&#24494;&#35843;&#31867;&#20284;BERT&#21644;RoBERTa&#30340;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#12290;&#36825;&#19968;&#31574;&#30053;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65306;IMDb&#24433;&#35780;&#25968;&#25454;&#38598;&#21644;UMLS Meta-Thesaurus&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;LlamBERT&#26041;&#27861;&#22312;&#31245;&#24494;&#29306;&#29298;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15938v1 Announce Type: cross  Abstract: Large Language Models (LLMs), such as GPT-4 and Llama 2, show remarkable proficiency in a wide range of natural language processing (NLP) tasks. Despite their effectiveness, the high costs associated with their use pose a challenge. We present LlamBERT, a hybrid approach that leverages LLMs to annotate a small subset of large, unlabeled databases and uses the results for fine-tuning transformer encoders like BERT and RoBERTa. This strategy is evaluated on two diverse datasets: the IMDb review dataset and the UMLS Meta-Thesaurus. Our results indicate that the LlamBERT approach slightly compromises on accuracy while offering much greater cost-effectiveness.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#38646;-shot&#25552;&#31034;&#26469;&#24341;&#20986;&#25945;&#24072;&#27169;&#22411;&#30340;&#29702;&#30001;&#65292;&#20943;&#23569;&#25163;&#24037;&#21046;&#20316;&#30340;&#23569;-shot&#31034;&#20363;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#38477;&#20302;&#25152;&#38656;&#30340;&#24635;&#35760;&#21495;&#25968;&#65292;&#36825;&#30452;&#25509;&#36716;&#21270;&#20026;&#25104;&#26412;&#33410;&#32422;&#12290;</title><link>https://arxiv.org/abs/2403.15886</link><description>&lt;p&gt;
&#21033;&#29992;&#38646;-shot&#25552;&#31034;&#23454;&#29616;&#39640;&#25928;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Leveraging Zero-Shot Prompting for Efficient Language Model Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15886
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#38646;-shot&#25552;&#31034;&#26469;&#24341;&#20986;&#25945;&#24072;&#27169;&#22411;&#30340;&#29702;&#30001;&#65292;&#20943;&#23569;&#25163;&#24037;&#21046;&#20316;&#30340;&#23569;-shot&#31034;&#20363;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#38477;&#20302;&#25152;&#38656;&#30340;&#24635;&#35760;&#21495;&#25968;&#65292;&#36825;&#30452;&#25509;&#36716;&#21270;&#20026;&#25104;&#26412;&#33410;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;LLMs&#39640;&#25928;&#22320;&#33976;&#39311;&#20026;&#26356;&#23567;&#12289;&#29305;&#23450;&#20110;&#24212;&#29992;&#30340;&#27169;&#22411;&#65292;&#26174;&#33879;&#38477;&#20302;&#36816;&#33829;&#25104;&#26412;&#21644;&#20154;&#24037;&#21171;&#21160;&#12290;&#35813;&#25216;&#26415;&#21033;&#29992;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#20026;&#26410;&#26631;&#35760;&#25968;&#25454;&#29983;&#25104;&#26631;&#31614;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#30001;&#65292;&#20197;&#35299;&#20915;&#23558;&#35745;&#31639;&#23494;&#38598;&#22411;LLMs&#37096;&#32626;&#21040;&#29305;&#23450;&#24212;&#29992;&#25110;&#36793;&#32536;&#35774;&#22791;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#22810;&#20219;&#21153;&#35757;&#32451;&#26694;&#26550;&#65292;&#20854;&#20013;&#23398;&#29983;&#27169;&#22411;&#27169;&#20223;&#36825;&#20123;&#29702;&#30001;&#20197;&#21450;&#25945;&#24072;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#26469;&#22686;&#24378;&#24494;&#35843;&#21644;&#33976;&#39311;&#12290;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#21033;&#29992;&#38646;-shot&#25552;&#31034;&#26469;&#24341;&#20986;&#25945;&#24072;&#27169;&#22411;&#30340;&#29702;&#30001;&#65292;&#20943;&#23569;&#25163;&#24037;&#21046;&#20316;&#30340;&#23569;-shot&#31034;&#20363;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#38477;&#20302;&#25152;&#38656;&#30340;&#24635;&#35760;&#21495;&#25968;&#65292;&#36825;&#30452;&#25509;&#36716;&#21270;&#20026;&#25104;&#26412;&#33410;&#32422;&#65292;&#32771;&#34385;&#21040;&#20027;&#35201;&#25216;&#26415;&#20844;&#21496;LLM APIs&#30340;&#25353;&#35760;&#21495;&#35745;&#36153;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35843;&#26597;&#20102;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15886v1 Announce Type: cross  Abstract: This paper introduces a novel approach for efficiently distilling LLMs into smaller, application-specific models, significantly reducing operational costs and manual labor. Addressing the challenge of deploying computationally intensive LLMs in specific applications or edge devices, this technique utilizes LLMs' reasoning capabilities to generate labels and natural language rationales for unlabeled data. Our approach enhances both finetuning and distillation by employing a multi-task training framework where student models mimic these rationales alongside teacher predictions. Key contributions include the employment of zero-shot prompting to elicit teacher model rationales, reducing the necessity for handcrafted few-shot examples and lowering the overall token count required, which directly translates to cost savings given the pay-per-token billing model of major tech companies' LLM APIs. Additionally, the paper investigates the impact
&lt;/p&gt;</description></item><item><title>STEntConv&#21033;&#29992;&#29992;&#25143;&#31435;&#22330;&#24314;&#31435;&#20102;&#29992;&#25143;&#21644;&#21629;&#21517;&#23454;&#20307;&#30340;&#21152;&#26435;&#22270;&#65292;&#36890;&#36807;&#26377;&#31526;&#21495;&#22270;&#21367;&#31215;&#32593;&#32476;&#39044;&#27979;Reddit&#24086;&#23376;&#20013;&#30340;&#19981;&#21516;&#24847;&#35265;&#34920;&#36798;&#12290;</title><link>https://arxiv.org/abs/2403.15885</link><description>&lt;p&gt;
STEntConv&#65306;&#21033;&#29992;&#31435;&#22330;&#26816;&#27979;&#21644;&#26377;&#31526;&#21495;&#22270;&#21367;&#31215;&#32593;&#32476;&#39044;&#27979;&#19981;&#21516;&#24847;&#35265;
&lt;/p&gt;
&lt;p&gt;
STEntConv: Predicting Disagreement with Stance Detection and a Signed Graph Convolutional Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15885
&lt;/p&gt;
&lt;p&gt;
STEntConv&#21033;&#29992;&#29992;&#25143;&#31435;&#22330;&#24314;&#31435;&#20102;&#29992;&#25143;&#21644;&#21629;&#21517;&#23454;&#20307;&#30340;&#21152;&#26435;&#22270;&#65292;&#36890;&#36807;&#26377;&#31526;&#21495;&#22270;&#21367;&#31215;&#32593;&#32476;&#39044;&#27979;Reddit&#24086;&#23376;&#20013;&#30340;&#19981;&#21516;&#24847;&#35265;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#20852;&#36215;&#23548;&#33268;&#26497;&#21270;&#30340;&#22312;&#32447;&#35752;&#35770;&#22686;&#21152;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#36873;&#20030;&#21644;&#27668;&#20505;&#21464;&#21270;&#31561;&#25919;&#27835;&#21644;&#31038;&#20250;&#25991;&#21270;&#35805;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#20004;&#31687;&#25991;&#31456;&#30340;&#20316;&#32773;&#26159;&#21542;&#21516;&#24847;&#25110;&#19981;&#21516;&#24847;&#65292;&#21033;&#29992;&#20174;&#20182;&#20204;&#30340;&#25991;&#31456;&#20013;&#33719;&#24471;&#30340;&#20851;&#20110;&#21629;&#21517;&#23454;&#20307;&#30340;&#29992;&#25143;&#31435;&#22330;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;STEntConv&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26500;&#24314;&#20102;&#19968;&#20010;&#30001;&#31435;&#22330;&#21152;&#26435;&#30340;&#29992;&#25143;&#21644;&#21629;&#21517;&#23454;&#20307;&#22270;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#26377;&#31526;&#21495;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;SGCN&#65289;&#26469;&#26816;&#27979;&#35780;&#35770;&#21644;&#22238;&#22797;&#24086;&#23376;&#20043;&#38388;&#30340;&#19981;&#21516;&#24847;&#35265;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20986;&#21253;&#21547;&#27492;&#20449;&#24687;&#21487;&#20197;&#25913;&#21892;Reddit&#24086;&#23376;&#25968;&#25454;&#38598;&#19978;&#26377;&#20105;&#35758;&#30340;&#23376;&#29256;&#20027;&#39064;&#30340;&#19981;&#21516;&#24847;&#35265;&#26816;&#27979;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#24179;&#21488;&#29305;&#23450;&#29305;&#24449;&#25110;&#29992;&#25143;&#21382;&#21490;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15885v1 Announce Type: new  Abstract: The rise of social media platforms has led to an increase in polarised online discussions, especially on political and socio-cultural topics such as elections and climate change. We propose a simple and novel unsupervised method to predict whether the authors of two posts agree or disagree, leveraging user stances about named entities obtained from their posts. We present STEntConv, a model which builds a graph of users and named entities weighted by stance and trains a Signed Graph Convolutional Network (SGCN) to detect disagreement between comment and reply posts. We run experiments and ablation studies and show that including this information improves disagreement detection performance on a dataset of Reddit posts for a range of controversial subreddit topics, without the need for platform-specific features or user history.
&lt;/p&gt;</description></item><item><title>VLUE&#26159;&#31532;&#19968;&#20010;&#36234;&#21335;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#65288;VLUE&#65289;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#21253;&#25324;&#25991;&#26412;&#20998;&#31867;&#12289;&#36328;&#24230;&#25552;&#21462;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#22312;&#20869;&#30340;&#20116;&#20010;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#19971;&#20010;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20171;&#32461;&#20102;CafeBERT&#65292;&#19968;&#20010;&#21462;&#24471;&#20248;&#24322;&#32467;&#26524;&#30340;&#26032;&#22411;&#26368;&#20808;&#36827;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.15882</link><description>&lt;p&gt;
VLUE&#65306;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#21644;&#22810;&#20219;&#21153;&#30693;&#35782;&#36801;&#31227;&#23398;&#20064;&#65292;&#29992;&#20110;&#36234;&#21335;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
VLUE: A New Benchmark and Multi-task Knowledge Transfer Learning for Vietnamese Natural Language Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15882
&lt;/p&gt;
&lt;p&gt;
VLUE&#26159;&#31532;&#19968;&#20010;&#36234;&#21335;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#65288;VLUE&#65289;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#21253;&#25324;&#25991;&#26412;&#20998;&#31867;&#12289;&#36328;&#24230;&#25552;&#21462;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#22312;&#20869;&#30340;&#20116;&#20010;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#19971;&#20010;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20171;&#32461;&#20102;CafeBERT&#65292;&#19968;&#20010;&#21462;&#24471;&#20248;&#24322;&#32467;&#26524;&#30340;&#26032;&#22411;&#26368;&#20808;&#36827;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#22522;&#20934;&#22312;&#21508;&#31181;&#35821;&#35328;&#65288;&#22914;&#33521;&#35821;&#30340;GLUE&#65292;&#20013;&#25991;&#30340;CLUE&#65292;&#38889;&#35821;&#30340;KLUE&#21644;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#30340;IndoNLU&#65289;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20419;&#36827;&#20102;&#23545;&#26032;NLU&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#35780;&#20272;&#12290;&#20026;&#20102;&#20026;&#36234;&#21335;NLU&#24314;&#31435;&#19968;&#22871;&#26631;&#20934;&#21270;&#30340;&#22522;&#20934;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#36234;&#21335;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#65288;VLUE&#65289;&#22522;&#20934;&#12290;VLUE&#22522;&#20934;&#21253;&#25324;&#28085;&#30422;&#20102;&#19981;&#21516;NLU&#20219;&#21153;&#30340;&#20116;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25991;&#26412;&#20998;&#31867;&#12289;&#36328;&#24230;&#25552;&#21462;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12290;&#20026;&#20102;&#28145;&#20837;&#20102;&#35299;&#24403;&#21069;&#36234;&#21335;NLU&#30340;&#29616;&#29366;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19971;&#20010;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21253;&#25324;&#22810;&#35821;&#35328;&#21644;&#36234;&#21335;&#21333;&#35821;&#27169;&#22411;&#65292;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;VLUE&#22522;&#20934;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CafeBERT&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;VLUE&#22522;&#20934;&#30340;&#25152;&#26377;&#20219;&#21153;&#19978;&#37117;&#21462;&#24471;&#20102;&#20248;&#24322;&#32467;&#26524;&#30340;&#26032;&#22411;&#26368;&#20808;&#36827;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15882v1 Announce Type: new  Abstract: The success of Natural Language Understanding (NLU) benchmarks in various languages, such as GLUE for English, CLUE for Chinese, KLUE for Korean, and IndoNLU for Indonesian, has facilitated the evaluation of new NLU models across a wide range of tasks. To establish a standardized set of benchmarks for Vietnamese NLU, we introduce the first Vietnamese Language Understanding Evaluation (VLUE) benchmark. The VLUE benchmark encompasses five datasets covering different NLU tasks, including text classification, span extraction, and natural language understanding. To provide an insightful overview of the current state of Vietnamese NLU, we then evaluate seven state-of-the-art pre-trained models, including both multilingual and Vietnamese monolingual models, on our proposed VLUE benchmark. Furthermore, we present CafeBERT, a new state-of-the-art pre-trained model that achieves superior results across all tasks in the VLUE benchmark. Our model co
&lt;/p&gt;</description></item><item><title>LAMPER&#26694;&#26550;&#26088;&#22312;&#35780;&#20272;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#30740;&#31350;&#21457;&#29616;&#20854;&#29305;&#24449;&#34920;&#31034;&#33021;&#21147;&#21463;&#21040;PLMs&#26368;&#22823;&#36755;&#20837;&#26631;&#35760;&#38408;&#20540;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.15875</link><description>&lt;p&gt;
LAMPER&#65306;&#29992;&#20110;&#38646;&#26679;&#26412;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
LAMPER: LanguAge Model and Prompt EngineeRing for zero-shot time series classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15875
&lt;/p&gt;
&lt;p&gt;
LAMPER&#26694;&#26550;&#26088;&#22312;&#35780;&#20272;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#30740;&#31350;&#21457;&#29616;&#20854;&#29305;&#24449;&#34920;&#31034;&#33021;&#21147;&#21463;&#21040;PLMs&#26368;&#22823;&#36755;&#20837;&#26631;&#35760;&#38408;&#20540;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26500;&#24314;&#20102;LanguAge&#27169;&#22411;&#21644;Prompt EngineeRing&#65288;LAMPER&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#31995;&#32479;&#35780;&#20272;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#23481;&#32435;&#22810;&#26679;&#25552;&#31034;&#21450;&#20854;&#22312;&#38646;&#26679;&#26412;&#26102;&#38388;&#24207;&#21015;&#65288;TS&#65289;&#20998;&#31867;&#20013;&#30340;&#25972;&#21512;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#35780;&#20272;&#20013;&#37096;&#32626;LAMPER&#65292;&#20351;&#29992;&#20102;&#26469;&#28304;&#20110;UCR&#23384;&#26723;&#30340;128&#20010;&#21333;&#21464;&#37327;TS&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;LAMPER&#30340;&#29305;&#24449;&#34920;&#31034;&#33021;&#21147;&#21463;&#21040;PLMs&#24378;&#21152;&#30340;&#26368;&#22823;&#36755;&#20837;&#26631;&#35760;&#38408;&#20540;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15875v1 Announce Type: new  Abstract: This study constructs the LanguAge Model with Prompt EngineeRing (LAMPER) framework, designed to systematically evaluate the adaptability of pre-trained language models (PLMs) in accommodating diverse prompts and their integration in zero-shot time series (TS) classification. We deploy LAMPER in experimental assessments using 128 univariate TS datasets sourced from the UCR archive. Our findings indicate that the feature representation capacity of LAMPER is influenced by the maximum input token threshold imposed by PLMs.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;RAAMove&#30340;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#24110;&#21161;&#20998;&#26512;&#24182;&#33258;&#21160;&#35782;&#21035;&#30740;&#31350;&#35770;&#25991;&#25688;&#35201;&#20013;&#30340;&#21160;&#20316;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.15872</link><description>&lt;p&gt;
RAAMove: &#29992;&#20110;&#20998;&#26512;&#30740;&#31350;&#35770;&#25991;&#25688;&#35201;&#20013;&#21160;&#20316;&#30340;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
RAAMove: A Corpus for Analyzing Moves in Research Article Abstracts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15872
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;RAAMove&#30340;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#24110;&#21161;&#20998;&#26512;&#24182;&#33258;&#21160;&#35782;&#21035;&#30740;&#31350;&#35770;&#25991;&#25688;&#35201;&#20013;&#30340;&#21160;&#20316;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#20154;&#20204;&#19968;&#30452;&#22312;&#30740;&#31350;&#33521;&#35821;&#19987;&#38376;&#29992;&#36884;&#65288;ESP&#65289;&#21644;&#23398;&#26415;&#30446;&#30340;&#33521;&#35821;&#65288;EAP&#65289;&#20013;&#30340;&#21160;&#20316;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#30740;&#31350;&#35770;&#25991;&#65288;RA&#65289;&#25688;&#35201;&#30340;&#21160;&#20316;&#27880;&#37322;&#35821;&#26009;&#24211;&#21364;&#24456;&#23569;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;RAAMove&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#27880;&#37322;RA&#25688;&#35201;&#20013;&#30340;&#21160;&#20316;&#32467;&#26500;&#30340;&#32508;&#21512;&#22810;&#39046;&#22495;&#35821;&#26009;&#24211;&#12290;RAAMove&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20419;&#36827;&#21160;&#20316;&#20998;&#26512;&#21644;&#33258;&#21160;&#21160;&#20316;&#35782;&#21035;&#12290;&#26412;&#25991;&#23545;&#35821;&#26009;&#24211;&#26500;&#24314;&#36807;&#31243;&#36827;&#34892;&#20102;&#35814;&#32454;&#35752;&#35770;&#65292;&#21253;&#25324;&#26041;&#26696;&#12289;&#25968;&#25454;&#25910;&#38598;&#12289;&#27880;&#37322;&#20934;&#21017;&#21644;&#27880;&#37322;&#31243;&#24207;&#12290;&#35821;&#26009;&#24211;&#20998;&#20004;&#20010;&#38454;&#27573;&#26500;&#24314;&#65306;&#39318;&#20808;&#65292;&#19987;&#23478;&#26631;&#27880;&#21592;&#25163;&#21160;&#26631;&#27880;&#39640;&#36136;&#37327;&#25968;&#25454;&#65307;&#38543;&#21518;&#65292;&#22522;&#20110;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#65292;&#21033;&#29992;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#36890;&#36807;&#19987;&#23478;&#20462;&#25913;&#36827;&#34892;&#33258;&#21160;&#26631;&#27880;&#12290;&#32467;&#26524;&#26159;&#19968;&#20010;&#21253;&#21547;33,988&#20010;&#27880;&#37322;&#30340;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15872v1 Announce Type: new  Abstract: Move structures have been studied in English for Specific Purposes (ESP) and English for Academic Purposes (EAP) for decades. However, there are few move annotation corpora for Research Article (RA) abstracts. In this paper, we introduce RAAMove, a comprehensive multi-domain corpus dedicated to the annotation of move structures in RA abstracts. The primary objective of RAAMove is to facilitate move analysis and automatic move identification. This paper provides a thorough discussion of the corpus construction process, including the scheme, data collection, annotation guidelines, and annotation procedures. The corpus is constructed through two stages: initially, expert annotators manually annotate high-quality data; subsequently, based on the human-annotated data, a BERT-based model is employed for automatic annotation with the help of experts' modification. The result is a large-scale and high-quality corpus comprising 33,988 annotated i
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20013;&#24515;&#25513;&#34109;&#30340;GLIP&#25216;&#26415;&#22312;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#20013;&#21462;&#20195;&#20102;&#38543;&#26426;&#25513;&#34109;&#65292;&#21033;&#29992;&#39640;&#26031;&#20998;&#24067;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#26131;&#20110;&#33719;&#24471;&#19988;&#36866;&#29992;&#20110;&#19981;&#20855;&#26377;&#26126;&#26174;&#20013;&#24515;&#28966;&#28857;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.15837</link><description>&lt;p&gt;
&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#20013;&#24515;&#25513;&#34109;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Centered Masking for Language-Image Pre-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15837
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20013;&#24515;&#25513;&#34109;&#30340;GLIP&#25216;&#26415;&#22312;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#20013;&#21462;&#20195;&#20102;&#38543;&#26426;&#25513;&#34109;&#65292;&#21033;&#29992;&#39640;&#26031;&#20998;&#24067;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#26131;&#20110;&#33719;&#24471;&#19988;&#36866;&#29992;&#20110;&#19981;&#20855;&#26377;&#26126;&#26174;&#20013;&#24515;&#28966;&#28857;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;GLIP&#65289;&#30340;&#39640;&#26031;&#25513;&#34109;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#12289;&#30452;&#25509;&#21644;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#22270;&#20687;&#34917;&#19969;&#36827;&#34892;&#25513;&#34109;&#12290;GLIP&#22522;&#20110;&#24555;&#36895;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;FLIP&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;CLIP&#27169;&#22411;&#26102;&#38543;&#26426;&#23631;&#34109;&#22270;&#20687;&#34917;&#19969;&#12290;GLIP&#23558;&#38543;&#26426;&#23631;&#34109;&#26367;&#25442;&#20026;&#20013;&#24515;&#25513;&#34109;&#65292;&#20351;&#29992;&#39640;&#26031;&#20998;&#24067;&#65292;&#24182;&#21463;&#21040;&#22270;&#20687;&#20013;&#24515;&#37325;&#35201;&#24615;&#30340;&#21551;&#21457;&#12290;&#22312;&#19968;&#31995;&#21015;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#20013;&#65292;GLIP&#20445;&#30041;&#20102;&#19982;FLIP&#30456;&#21516;&#30340;&#35745;&#31639;&#33410;&#30465;&#33021;&#21147;&#65292;&#21516;&#26102;&#25913;&#21892;&#20102;&#24615;&#33021;&#65292;&#36825;&#26159;&#30001;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#25152;&#35777;&#23454;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;GLIP&#30340;&#22909;&#22788;&#24456;&#23481;&#26131;&#33719;&#24471;&#65292;&#26080;&#38656;&#31934;&#32454;&#35843;&#25972;&#39640;&#26031;&#65292;&#20063;&#36866;&#29992;&#20110;&#21253;&#21547;&#26080;&#26126;&#26174;&#20013;&#24515;&#28966;&#28857;&#22270;&#29255;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15837v1 Announce Type: cross  Abstract: We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel, straightforward, and effective technique for masking image patches during pre-training of a vision-language model. GLIP builds on Fast Language-Image Pre-Training (FLIP), which randomly masks image patches while training a CLIP model. GLIP replaces random masking with centered masking, that uses a Gaussian distribution and is inspired by the importance of image patches at the center of the image. GLIP retains the same computational savings as FLIP, while improving performance across a range of downstream datasets and tasks, as demonstrated by our experimental results. We show the benefits of GLIP to be easy to obtain, requiring no delicate tuning of the Gaussian, and also applicable to data sets containing images without an obvious center focus.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#21019;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#21477;&#23376;&#32423;&#24230;&#37327;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#24230;&#37327;&#33021;&#22815;&#39640;&#24230;&#20934;&#30830;&#22320;&#39044;&#27979;&#20154;&#31867;&#21477;&#23376;&#38405;&#35835;&#36895;&#24230;&#65292;&#20026;&#26410;&#26469;&#25972;&#21512;LLMs&#21644;&#35748;&#30693;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.15822</link><description>&lt;p&gt;
&#35745;&#31639;&#21477;&#23376;&#32423;&#24230;&#37327;&#39044;&#27979;&#20154;&#31867;&#21477;&#23376;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Computational Sentence-level Metrics Predicting Human Sentence Comprehension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#21019;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#21477;&#23376;&#32423;&#24230;&#37327;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#24230;&#37327;&#33021;&#22815;&#39640;&#24230;&#20934;&#30830;&#22320;&#39044;&#27979;&#20154;&#31867;&#21477;&#23376;&#38405;&#35835;&#36895;&#24230;&#65292;&#20026;&#26410;&#26469;&#25972;&#21512;LLMs&#21644;&#35748;&#30693;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#24515;&#29702;&#35821;&#35328;&#23398;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#22312;&#21333;&#35789;&#22788;&#29702;&#19978;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#21019;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#21477;&#23376;&#32423;&#24230;&#37327;&#12290;&#24320;&#21457;&#30340;&#24230;&#37327;&#21253;&#25324;&#21477;&#23376;&#24847;&#22806;&#24615;&#21644;&#21477;&#23376;&#30456;&#20851;&#24615;&#65292;&#28982;&#21518;&#32463;&#36807;&#27979;&#35797;&#21644;&#27604;&#36739;&#20197;&#39564;&#35777;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#39044;&#27979;&#20154;&#31867;&#22914;&#20309;&#36328;&#35821;&#35328;&#25972;&#20307;&#29702;&#35299;&#21477;&#23376;&#12290;&#36825;&#20123;&#24230;&#37327;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#39044;&#27979;&#20154;&#31867;&#21477;&#23376;&#38405;&#35835;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#35745;&#31639;&#30340;&#21477;&#23376;&#32423;&#24230;&#37327;&#22312;&#39044;&#27979;&#21644;&#38416;&#26126;&#35835;&#32773;&#22312;&#29702;&#35299;&#25972;&#20307;&#21477;&#23376;&#26102;&#36935;&#21040;&#30340;&#22788;&#29702;&#22256;&#38590;&#26041;&#38754;&#24322;&#24120;&#26377;&#25928;&#65292;&#21487;&#36328;&#36234;&#22810;&#31181;&#35821;&#35328;&#12290;&#23427;&#20204;&#20986;&#33394;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#20026;&#26410;&#26469;&#22312;&#25972;&#21512;LLMs&#21644;&#35748;&#30693;&#31185;&#23398;&#26041;&#38754;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15822v1 Announce Type: new  Abstract: The majority of research in computational psycholinguistics has concentrated on the processing of words. This study introduces innovative methods for computing sentence-level metrics using multilingual large language models. The metrics developed sentence surprisal and sentence relevance and then are tested and compared to validate whether they can predict how humans comprehend sentences as a whole across languages. These metrics offer significant interpretability and achieve high accuracy in predicting human sentence reading speeds. Our results indicate that these computational sentence-level metrics are exceptionally effective at predicting and elucidating the processing difficulties encountered by readers in comprehending sentences as a whole across a variety of languages. Their impressive performance and generalization capabilities provide a promising avenue for future research in integrating LLMs and cognitive science.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;MRC&#30340;&#21307;&#23398;NER&#27169;&#22411;&#65292;&#37319;&#29992;&#20219;&#21153;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#31574;&#30053;&#12289;&#22810;&#35789;&#23545;&#23884;&#20837;&#21644;&#22810;&#31890;&#24230;&#25193;&#24352;&#21367;&#31215;&#65292;&#24182;&#32467;&#21512;Biaffine&#21644;MLP&#30340;&#32852;&#21512;&#39044;&#27979;&#22120;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#34920;&#24449;&#21644;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.15800</link><description>&lt;p&gt;
&#22522;&#20110;MRC&#30340;&#24102;&#20849;&#21516;&#39044;&#27979;&#21644;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#21307;&#23398;&#23884;&#22871;NER
&lt;/p&gt;
&lt;p&gt;
MRC-based Nested Medical NER with Co-prediction and Adaptive Pre-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15800
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;MRC&#30340;&#21307;&#23398;NER&#27169;&#22411;&#65292;&#37319;&#29992;&#20219;&#21153;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#31574;&#30053;&#12289;&#22810;&#35789;&#23545;&#23884;&#20837;&#21644;&#22810;&#31890;&#24230;&#25193;&#24352;&#21367;&#31215;&#65292;&#24182;&#32467;&#21512;Biaffine&#21644;MLP&#30340;&#32852;&#21512;&#39044;&#27979;&#22120;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#34920;&#24449;&#21644;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#20449;&#24687;&#25552;&#21462;&#20013;&#65292;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#65292;&#23427;&#22312;&#24320;&#21457;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#12289;&#22686;&#24378;&#21307;&#23398;&#38382;&#31572;&#31995;&#32479;&#20197;&#21450;&#20998;&#26512;&#30005;&#23376;&#30149;&#21382;&#20013;&#25198;&#28436;&#30528;&#20851;&#38190;&#35282;&#33394;&#12290;&#21307;&#23398;NER&#38754;&#20020;&#30340;&#25361;&#25112;&#20027;&#35201;&#26469;&#33258;&#22797;&#26434;&#30340;&#23884;&#22871;&#32467;&#26500;&#21644;&#22797;&#26434;&#30340;&#21307;&#23398;&#26415;&#35821;&#65292;&#36825;&#20351;&#20854;&#19982;&#20256;&#32479;&#39046;&#22495;&#30340;NER&#26377;&#25152;&#21306;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;MRC&#65289;&#30340;&#21307;&#23398;NER&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#20219;&#21153;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#31574;&#30053;&#26469;&#25552;&#39640;&#27169;&#22411;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#24341;&#20837;&#20102;&#22810;&#20010;&#35789;&#23545;&#23884;&#20837;&#21644;&#22810;&#31890;&#24230;&#25193;&#24352;&#21367;&#31215;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#34920;&#24449;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#32467;&#21512;&#20102;Biaffine&#21644;MLP&#30340;&#32852;&#21512;&#39044;&#27979;&#22120;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15800v1 Announce Type: new  Abstract: In medical information extraction, medical Named Entity Recognition (NER) is indispensable, playing a crucial role in developing medical knowledge graphs, enhancing medical question-answering systems, and analyzing electronic medical records. The challenge in medical NER arises from the complex nested structures and sophisticated medical terminologies, distinguishing it from its counterparts in traditional domains. In response to these complexities, we propose a medical NER model based on Machine Reading Comprehension (MRC), which uses a task-adaptive pre-training strategy to improve the model's capability in the medical field. Meanwhile, our model introduces multiple word-pair embeddings and multi-granularity dilated convolution to enhance the model's representation ability and uses a combined predictor of Biaffine and MLP to improve the model's recognition performance. Experimental evaluations conducted on the CMeEE, a benchmark for Ch
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#25439;&#22833;&#35282;&#24230;&#37325;&#26032;&#23450;&#20041;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;&#65292;&#21457;&#29616;&#20855;&#26377;&#30456;&#21516;&#39044;&#35757;&#32451;&#25439;&#22833;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#34920;&#29616;&#30456;&#20284;&#65292;&#32780;&#24403;&#39044;&#35757;&#32451;&#25439;&#22833;&#20302;&#20110;&#29305;&#23450;&#38408;&#20540;&#26102;&#65292;&#27169;&#22411;&#23558;&#23637;&#29616;&#20986;&#31361;&#29616;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.15796</link><description>&lt;p&gt;
&#20174;&#25439;&#22833;&#35282;&#24230;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Understanding Emergent Abilities of Language Models from the Loss Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#25439;&#22833;&#35282;&#24230;&#37325;&#26032;&#23450;&#20041;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;&#65292;&#21457;&#29616;&#20855;&#26377;&#30456;&#21516;&#39044;&#35757;&#32451;&#25439;&#22833;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#34920;&#29616;&#30456;&#20284;&#65292;&#32780;&#24403;&#39044;&#35757;&#32451;&#25439;&#22833;&#20302;&#20110;&#29305;&#23450;&#38408;&#20540;&#26102;&#65292;&#27169;&#22411;&#23558;&#23637;&#29616;&#20986;&#31361;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#36136;&#30097;&#20102;&#20256;&#32479;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;&#20165;&#23384;&#22312;&#20110;&#22823;&#27169;&#22411;&#20013;&#30340;&#35266;&#28857;&#12290;&#36825;&#31181;&#24576;&#30097;&#28304;&#33258;&#20004;&#28857;&#35266;&#23519;&#65306;1&#65289;&#36739;&#23567;&#30340;&#27169;&#22411;&#20063;&#33021;&#23637;&#29616;&#20986;&#23545;&#31361;&#29616;&#33021;&#21147;&#30340;&#39640;&#24615;&#33021;&#65307;2&#65289;&#36136;&#30097;&#29992;&#20110;&#27979;&#37327;&#36825;&#20123;&#33021;&#21147;&#30340;&#19981;&#36830;&#32493;&#24615;&#25351;&#26631;&#12290;&#26412;&#25991;&#25552;&#35758;&#20174;&#39044;&#35757;&#32451;&#25439;&#22833;&#30340;&#35282;&#24230;&#30740;&#31350;&#31361;&#29616;&#33021;&#21147;&#65292;&#32780;&#38750;&#27169;&#22411;&#22823;&#23567;&#25110;&#35757;&#32451;&#35745;&#31639;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20855;&#26377;&#30456;&#21516;&#39044;&#35757;&#32451;&#25439;&#22833;&#20294;&#19981;&#21516;&#27169;&#22411;&#21644;&#25968;&#25454;&#22823;&#23567;&#30340;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#30456;&#21516;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#24403;&#26576;&#19968;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25439;&#22833;&#20302;&#20110;&#29305;&#23450;&#38408;&#20540;&#26102;&#65292;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#31361;&#29616;&#33021;&#21147;&#65292;&#32780;&#19981;&#35770;&#25351;&#26631;&#30340;&#36830;&#32493;&#24615;&#22914;&#20309;&#65307;&#32780;&#22312;&#36798;&#21040;&#35813;&#38408;&#20540;&#20043;&#21069;&#65292;&#20854;&#24615;&#33021;&#20173;&#20445;&#25345;&#22312;&#38543;&#26426;&#29468;&#27979;&#27700;&#24179;&#12290;&#36825;&#21551;&#21457;&#25105;&#20204;&#37325;&#26032;&#23450;&#20041;&#31361;&#29616;&#33021;&#21147;&#20026;&#37027;&#20123;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15796v1 Announce Type: cross  Abstract: Recent studies have put into question the belief that emergent abilities in language models are exclusive to large models. This skepticism arises from two observations: 1) smaller models can also exhibit high performance on emergent abilities and 2) there is doubt on the discontinuous metrics used to measure these abilities. In this paper, we propose to study emergent abilities in the lens of pre-training loss, instead of model size or training compute. We demonstrate that the models with the same pre-training loss, but different model and data sizes, generate the same performance on various downstream tasks. We also discover that a model exhibits emergent abilities on certain tasks -- regardless of the continuity of metrics -- when its pre-training loss falls below a specific threshold. Before reaching this threshold, its performance remains at the level of random guessing. This inspires us to redefine emergent abilities as those that
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#25991;&#26723;&#32423;&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#65288;RST&#65289;&#26641;&#19982;&#21477;&#32423;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#22270;&#32467;&#21512;&#36215;&#26469;&#26500;&#24314;S3&#22270;&#65292;&#24418;&#25104;&#32479;&#19968;&#30340;&#35821;&#20041;&#35805;&#35821;&#32467;&#26500;&#65292;&#29992;&#20110;&#26631;&#39064;&#29983;&#25104;&#26694;&#26550;&#20013;&#65292;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#20998;&#23618;&#32467;&#26500;&#20462;&#21098;&#26426;&#21046;&#65292;&#25552;&#39640;&#26631;&#39064;&#29983;&#25104;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.15776</link><description>&lt;p&gt;
&#20026;&#39640;&#36136;&#37327;&#26631;&#39064;&#29983;&#25104;&#24314;&#27169;&#32479;&#19968;&#35821;&#20041;&#35805;&#35821;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Modeling Unified Semantic Discourse Structure for High-quality Headline Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15776
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#25991;&#26723;&#32423;&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#65288;RST&#65289;&#26641;&#19982;&#21477;&#32423;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#22270;&#32467;&#21512;&#36215;&#26469;&#26500;&#24314;S3&#22270;&#65292;&#24418;&#25104;&#32479;&#19968;&#30340;&#35821;&#20041;&#35805;&#35821;&#32467;&#26500;&#65292;&#29992;&#20110;&#26631;&#39064;&#29983;&#25104;&#26694;&#26550;&#20013;&#65292;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#20998;&#23618;&#32467;&#26500;&#20462;&#21098;&#26426;&#21046;&#65292;&#25552;&#39640;&#26631;&#39064;&#29983;&#25104;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39064;&#20026;&#29983;&#25104;&#26088;&#22312;&#29992;&#31616;&#30701;&#12289;&#21560;&#24341;&#20154;&#30340;&#26631;&#39064;&#24635;&#32467;&#38271;&#31687;&#25991;&#26723;&#65292;&#21453;&#26144;&#20027;&#35201;&#24605;&#24819;&#12290;&#36825;&#38656;&#35201;&#20934;&#30830;&#25429;&#25417;&#26680;&#24515;&#25991;&#26723;&#35821;&#20041;&#65292;&#30001;&#20110;&#25991;&#26412;&#30340;&#38271;&#24230;&#21644;&#32972;&#26223;&#20449;&#24687;&#20016;&#23500;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#32479;&#19968;&#30340;&#35821;&#20041;&#35805;&#35821;&#32467;&#26500;(S3)&#26469;&#34920;&#31034;&#25991;&#26723;&#35821;&#20041;&#65292;&#36890;&#36807;&#23558;&#25991;&#26723;&#32423;&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#65288;RST&#65289;&#26641;&#19982;&#21477;&#32423;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#22270;&#32467;&#21512;&#36215;&#26469;&#26500;&#24314;S3&#22270;&#12290;&#21477;&#23376;&#12289;&#20174;&#21477;&#21644;&#35789;&#27719;&#30340;&#20998;&#23618;&#32452;&#21512;&#22266;&#26377;&#22320;&#34920;&#36798;&#20102;&#25972;&#20010;&#25991;&#26723;&#30340;&#35821;&#20041;&#21547;&#20041;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26631;&#39064;&#29983;&#25104;&#26694;&#26550;&#65292;&#22312;&#20854;&#20013;&#23558;S3&#22270;&#32534;&#30721;&#20026;&#19978;&#19979;&#25991;&#29305;&#24449;&#12290;&#20026;&#20102;&#24041;&#22266;S3&#22270;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#20010;&#20998;&#23618;&#32467;&#26500;&#20462;&#21098;&#26426;&#21046;&#65292;&#21160;&#24577;&#31579;&#36873;&#22810;&#20313;&#21644;&#38750;&#24517;&#35201;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15776v1 Announce Type: cross  Abstract: Headline generation aims to summarize a long document with a short, catchy title that reflects the main idea. This requires accurately capturing the core document semantics, which is challenging due to the lengthy and background information-rich na ture of the texts. In this work, We propose using a unified semantic discourse structure (S3) to represent document semantics, achieved by combining document-level rhetorical structure theory (RST) trees with sentence-level abstract meaning representation (AMR) graphs to construct S3 graphs. The hierarchical composition of sentence, clause, and word intrinsically characterizes the semantic meaning of the overall document. We then develop a headline generation framework, in which the S3 graphs are encoded as contextual features. To consolidate the efficacy of S3 graphs, we further devise a hierarchical structure pruning mechanism to dynamically screen the redundant and nonessential nodes with
&lt;/p&gt;</description></item><item><title>&#29992;&#25143;&#31471;&#23454;&#29616;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#31215;&#26497;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#29992;&#25143;&#31471;&#36816;&#34892;&#36890;&#29992;&#31639;&#27861;&#26469;&#35299;&#20915;&#24120;&#35265;&#38382;&#39064;&#65292;&#26080;&#38656;&#26381;&#21153;&#25552;&#20379;&#21830;&#25913;&#21464;&#26381;&#21153;&#26412;&#36523;&#12290;</title><link>https://arxiv.org/abs/2403.15757</link><description>&lt;p&gt;
&#29992;&#25143;&#31471;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
User-Side Realization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15757
&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#31471;&#23454;&#29616;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#31215;&#26497;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#29992;&#25143;&#31471;&#36816;&#34892;&#36890;&#29992;&#31639;&#27861;&#26469;&#35299;&#20915;&#24120;&#35265;&#38382;&#39064;&#65292;&#26080;&#38656;&#26381;&#21153;&#25552;&#20379;&#21830;&#25913;&#21464;&#26381;&#21153;&#26412;&#36523;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#23545;&#26381;&#21153;&#24863;&#21040;&#19981;&#28385;&#24847;&#12290;&#30001;&#20110;&#26381;&#21153;&#24182;&#38750;&#37327;&#36523;&#23450;&#21046;&#32473;&#29992;&#25143;&#65292;&#22240;&#27492;&#19981;&#28385;&#24847;&#26159;&#33258;&#28982;&#32780;&#28982;&#30340;&#12290;&#38382;&#39064;&#22312;&#20110;&#65292;&#21363;&#20351;&#29992;&#25143;&#24863;&#21040;&#19981;&#28385;&#24847;&#65292;&#20182;&#20204;&#36890;&#24120;&#20063;&#27809;&#26377;&#35299;&#20915;&#19981;&#28385;&#30340;&#25163;&#27573;&#12290;&#29992;&#25143;&#26080;&#27861;&#20462;&#25913;&#26381;&#21153;&#30340;&#28304;&#20195;&#30721;&#65292;&#20063;&#26080;&#27861;&#24378;&#36843;&#26381;&#21153;&#25552;&#20379;&#21830;&#36827;&#34892;&#26356;&#25913;&#12290;&#29992;&#25143;&#21035;&#26080;&#36873;&#25321;&#65292;&#21482;&#33021;&#20445;&#25345;&#19981;&#28385;&#24847;&#25110;&#36864;&#20986;&#26381;&#21153;&#12290;&#29992;&#25143;&#31471;&#23454;&#29616;&#36890;&#36807;&#25552;&#20379;&#36890;&#29992;&#31639;&#27861;&#26469;&#22788;&#29702;&#29992;&#25143;&#31471;&#30340;&#24120;&#35265;&#38382;&#39064;&#65292;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#25552;&#20379;&#20102;&#31215;&#26497;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#31639;&#27861;&#22312;&#29992;&#25143;&#31471;&#36816;&#34892;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#26381;&#21153;&#25552;&#20379;&#21830;&#25913;&#21464;&#26381;&#21153;&#26412;&#36523;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15757v1 Announce Type: cross  Abstract: Users are dissatisfied with services. Since the service is not tailor-made for a user, it is natural for dissatisfaction to arise. The problem is, that even if users are dissatisfied, they often do not have the means to resolve their dissatisfaction. The user cannot alter the source code of the service, nor can they force the service provider to change. The user has no choice but to remain dissatisfied or quit the service. User-side realization offers proactive solutions to this problem by providing general algorithms to deal with common problems on the user's side. These algorithms run on the user's side and solve the problems without having the service provider change the service itself.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21021;&#27493;&#23433;&#20840;&#39118;&#38505;&#20998;&#26512;&#20013;&#23637;&#29616;&#20102;&#27604;&#20154;&#31867;&#26356;&#24555;&#36895;&#30340;&#20449;&#24687;&#24635;&#32467;&#33021;&#21147;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#25506;&#35752;&#20102;&#24494;&#35843;&#27169;&#22411;&#22312;&#21327;&#21161;&#20174;&#19994;&#32773;&#36827;&#34892;PSRA&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15756</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21021;&#27493;&#23433;&#20840;&#39118;&#38505;&#20998;&#26512;&#65306;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Preliminary Security Risk Analysis: A Mission-Critical Case Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15756
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21021;&#27493;&#23433;&#20840;&#39118;&#38505;&#20998;&#26512;&#20013;&#23637;&#29616;&#20102;&#27604;&#20154;&#31867;&#26356;&#24555;&#36895;&#30340;&#20449;&#24687;&#24635;&#32467;&#33021;&#21147;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#25506;&#35752;&#20102;&#24494;&#35843;&#27169;&#22411;&#22312;&#21327;&#21161;&#20174;&#19994;&#32773;&#36827;&#34892;PSRA&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21021;&#27493;&#23433;&#20840;&#39118;&#38505;&#20998;&#26512;&#65288;PSRA&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#12289;&#35780;&#20272;&#21644;&#25552;&#20986;&#28508;&#22312;&#39118;&#38505;&#22312;&#20855;&#20307;&#24773;&#22659;&#20013;&#30340;&#24212;&#23545;&#25514;&#26045;&#12290;&#22312;&#20851;&#38190;&#20219;&#21153;&#24773;&#22659;&#20013;&#65292;&#21450;&#26102;&#21644;&#36805;&#36895;&#30340;&#34892;&#21160;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#25152;&#20197;&#23545;&#26377;&#25928;PSRA&#25152;&#38656;&#30340;&#24191;&#27867;&#19987;&#19994;&#30693;&#35782;&#21644;&#22823;&#37327;&#19982;&#25991;&#26412;&#30456;&#20851;&#30340;&#20219;&#21153;&#22312;&#38459;&#30861;&#24555;&#36895;&#35780;&#20272;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#27604;&#20154;&#31867;&#26356;&#24555;&#36895;&#22320;&#24635;&#32467;&#20449;&#24687;&#65292;&#21033;&#29992;&#24494;&#35843;&#27169;&#22411;&#65288;FTM&#65289;&#22312;PSRA&#20013;&#30340;&#33021;&#21147;&#23578;&#26410;&#34987;&#20808;&#21069;&#30340;&#30740;&#31350;&#25152;&#25506;&#35752;&#12290;&#26412;&#25991;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#35843;&#26597;FTM&#22312;&#21327;&#21161;&#20174;&#19994;&#32773;&#36827;&#34892;PSRA&#20013;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15756v1 Announce Type: cross  Abstract: Preliminary security risk analysis (PSRA) provides a quick approach to identify, evaluate and propose remeditation to potential risks in specific scenarios. The extensive expertise required for an effective PSRA and the substantial ammount of textual-related tasks hinder quick assessments in mission-critical contexts, where timely and prompt actions are essential. The speed and accuracy of human experts in PSRA significantly impact response time. A large language model can quickly summarise information in less time than a human. To our knowledge, no prior study has explored the capabilities of fine-tuned models (FTM) in PSRA. Our case study investigates the proficiency of FTM to assist practitioners in PSRA. We manually curated 141 representative samples from over 50 mission-critical analyses archived by the industrial context team in the last five years.We compared the proficiency of the FTM versus seven human experts. Within the indu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#21482;&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#26377;&#25928;&#65292;&#23545;&#25991;&#26412;&#20998;&#31867;&#20174;&#19994;&#32773;&#30340;&#24314;&#35758;&#26159;&#36873;&#25321;&#36866;&#24403;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#21516;&#26679;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.15744</link><description>&lt;p&gt;
&#35770;&#20027;&#21160;&#23398;&#20064;&#32773;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Fragility of Active Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#21482;&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#26377;&#25928;&#65292;&#23545;&#25991;&#26412;&#20998;&#31867;&#20174;&#19994;&#32773;&#30340;&#24314;&#35758;&#26159;&#36873;&#25321;&#36866;&#24403;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#21516;&#26679;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#25216;&#26415;&#26088;&#22312;&#36890;&#36807;&#36845;&#20195;&#36873;&#25321;&#26368;&#26377;&#21487;&#33021;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#23454;&#20363;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#26631;&#27880;&#39044;&#31639;&#12290;&#28982;&#32780;&#65292;&#19982;&#38543;&#26426;&#25277;&#26679;&#30456;&#27604;&#65292;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#65288;&#20363;&#22914;&#19981;&#21516;&#25968;&#25454;&#38598;&#65292;&#20998;&#31867;&#22120;&#65289;&#65292;&#23427;&#20204;&#30340;&#30410;&#22788;&#24182;&#19981;&#19968;&#33268;&#12290;&#22312;&#36825;&#39033;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22240;&#32032;&#30340;&#32452;&#21512;&#22914;&#20309;&#21487;&#33021;&#25513;&#30422;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#30340;&#20219;&#20309;&#25910;&#30410;&#12290;&#19987;&#27880;&#20110;&#25991;&#26412;&#20998;&#31867;&#65292;&#25105;&#20204;&#22312;&#22823;&#32422;1000&#20010;&#23454;&#39564;&#20013;&#20005;&#26684;&#35780;&#20272;&#20102;&#36827;&#34892;&#20998;&#31867;&#65292;&#25105;&#20204;&#22312;&#22823;&#32422;1000&#20010;&#23454;&#39564;&#20013;&#20005;&#26684;&#35780;&#20272;&#20102;AL&#25216;&#26415;&#65292;&#36825;&#20123;&#23454;&#39564;&#22312;&#25968;&#25454;&#38598;&#12289;&#25209;&#22823;&#23567;&#12289;&#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#26041;&#38754;&#21464;&#21270;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;AL&#21482;&#22312;&#19968;&#32452;&#26377;&#38480;&#30340;&#24773;&#22659;&#20013;&#26377;&#25928;&#12290;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#20351;&#29992;&#19982;&#29616;&#23454;&#19990;&#30028;&#26399;&#26395;&#26356;&#22909;&#23545;&#40784;&#30340;&#24230;&#37327;&#30340;&#38382;&#39064;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#24433;&#21709;&#22312;&#20110;&#23545;&#20174;&#19994;&#32773;&#30340;&#27934;&#23519;&#65306;(a) &#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#30340;&#36873;&#25321;&#19982;AL&#25216;&#26415;&#30340;&#36873;&#25321;&#19968;&#26679;&#37325;&#35201;&#65292;(b) &#36873;&#25321;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15744v1 Announce Type: cross  Abstract: Active learning (AL) techniques aim to maximally utilize a labeling budget by iteratively selecting instances that are most likely to improve prediction accuracy. However, their benefit compared to random sampling has not been consistent across various setups, e.g., different datasets, classifiers. In this empirical study, we examine how a combination of different factors might obscure any gains from an AL technique.   Focusing on text classification, we rigorously evaluate AL techniques over around 1000 experiments that vary wrt the dataset, batch size, text representation and the classifier. We show that AL is only effective in a narrow set of circumstances. We also address the problem of using metrics that are better aligned with real world expectations.   The impact of this study is in its insights for a practitioner: (a) the choice of text representation and classifier is as important as that of an AL technique, (b) choice of the 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#25991;&#26723;&#20013;&#25554;&#20837;&#20010;&#20154;&#23494;&#30721;&#24182;&#35782;&#21035;&#29983;&#25104;&#20869;&#23481;&#20013;&#30340;&#8220;&#24189;&#28789;&#21477;&#23376;&#8221;&#65292;&#26222;&#36890;&#29992;&#25143;&#21487;&#20197;&#30830;&#35748;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#28389;&#29992;&#20854;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#25968;&#25454;&#29256;&#26435;&#20445;&#25252;&#12290;</title><link>https://arxiv.org/abs/2403.15740</link><description>&lt;p&gt;
Ghost Sentence&#65306;&#19968;&#31181;&#20379;&#26222;&#36890;&#29992;&#25143;&#20351;&#29992;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#36827;&#34892;&#29256;&#26435;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Ghost Sentence: A Tool for Everyday Users to Copyright Data from Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15740
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#25991;&#26723;&#20013;&#25554;&#20837;&#20010;&#20154;&#23494;&#30721;&#24182;&#35782;&#21035;&#29983;&#25104;&#20869;&#23481;&#20013;&#30340;&#8220;&#24189;&#28789;&#21477;&#23376;&#8221;&#65292;&#26222;&#36890;&#29992;&#25143;&#21487;&#20197;&#30830;&#35748;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#28389;&#29992;&#20854;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#25968;&#25454;&#29256;&#26435;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Web&#29992;&#25143;&#25968;&#25454;&#22312;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21450;&#20854;&#24494;&#35843;&#21464;&#31181;&#30340;&#29983;&#24577;&#31995;&#32479;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#24314;&#35758;&#29992;&#25143;&#22312;&#20854;&#25991;&#26723;&#20013;&#21453;&#22797;&#25554;&#20837;&#20010;&#20154;&#23494;&#30721;&#65292;&#20351;LLMs&#33021;&#22815;&#35760;&#24518;&#36825;&#20123;&#23494;&#30721;&#12290;&#36825;&#20123;&#29992;&#25143;&#25991;&#26723;&#20013;&#38544;&#34255;&#30340;&#23494;&#30721;&#65292;&#34987;&#31216;&#20026;&#8220;&#24189;&#28789;&#21477;&#23376;&#8221;&#65292;&#19968;&#26086;&#23427;&#20204;&#20986;&#29616;&#22312;LLMs&#29983;&#25104;&#30340;&#20869;&#23481;&#20013;&#65292;&#29992;&#25143;&#23601;&#21487;&#20197;&#30830;&#20449;&#20182;&#20204;&#30340;&#25968;&#25454;&#34987;&#29992;&#20110;&#35757;&#32451;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#31181;&#29256;&#26435;&#24037;&#20855;&#30340;&#26377;&#25928;&#24615;&#21644;&#29992;&#27861;&#65292;&#25105;&#20204;&#21033;&#29992;&#24189;&#28789;&#21477;&#23376;&#23450;&#20041;&#20102;&#8220;&#29992;&#25143;&#35757;&#32451;&#25968;&#25454;&#35782;&#21035;&#8221;&#20219;&#21153;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#12289;&#19981;&#21516;&#35268;&#27169;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#35268;&#27169;&#30340;LLMs&#36827;&#34892;&#27979;&#35797;&#12290;&#20026;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26368;&#21518;$k$&#20010;&#21333;&#35789;&#39564;&#35777;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15740v1 Announce Type: new  Abstract: Web user data plays a central role in the ecosystem of pre-trained large language models (LLMs) and their fine-tuned variants. Billions of data are crawled from the web and fed to LLMs. How can \textit{\textbf{everyday web users}} confirm if LLMs misuse their data without permission? In this work, we suggest that users repeatedly insert personal passphrases into their documents, enabling LLMs to memorize them. These concealed passphrases in user documents, referred to as \textit{ghost sentences}, once they are identified in the generated content of LLMs, users can be sure that their data is used for training. To explore the effectiveness and usage of this copyrighting tool, we define the \textit{user training data identification} task with ghost sentences. Multiple datasets from various sources at different scales are created and tested with LLMs of different sizes. For evaluation, we introduce a last $k$ words verification manner along 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;DIIT&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#21644;&#24212;&#29992;&#19987;&#23478;&#28436;&#31034;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#24402;&#32435;&#35268;&#21017;&#65292;&#25913;&#21892;&#20027;&#21160;&#20542;&#21548;&#25216;&#33021;&#65292;&#20943;&#23569;&#19981;&#35831;&#33258;&#26469;&#30340;&#24314;&#35758;&#65292;&#20419;&#36827;&#26356;&#21327;&#20316;&#21644;&#26080;&#26435;&#23041;&#24615;&#30340;&#22238;&#24212;&#12290;</title><link>https://arxiv.org/abs/2403.15737</link><description>&lt;p&gt;
&#36890;&#36807;&#24402;&#32435;&#25512;&#29702;&#23454;&#29616;&#23569;&#26679;&#26412;&#23545;&#35805;&#31574;&#30053;&#23398;&#20064;&#65292;&#24212;&#29992;&#20110;&#28608;&#21169;&#24335;&#35775;&#35848;
&lt;/p&gt;
&lt;p&gt;
Few-shot Dialogue Strategy Learning for Motivational Interviewing via Inductive Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15737
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;DIIT&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#21644;&#24212;&#29992;&#19987;&#23478;&#28436;&#31034;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#24402;&#32435;&#35268;&#21017;&#65292;&#25913;&#21892;&#20027;&#21160;&#20542;&#21548;&#25216;&#33021;&#65292;&#20943;&#23569;&#19981;&#35831;&#33258;&#26469;&#30340;&#24314;&#35758;&#65292;&#20419;&#36827;&#26356;&#21327;&#20316;&#21644;&#26080;&#26435;&#23041;&#24615;&#30340;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#26500;&#24314;&#19968;&#20010;&#23545;&#35805;&#31995;&#32479;&#65292;&#33021;&#22815;&#28608;&#21169;&#29992;&#25143;&#37319;&#32435;&#31215;&#26497;&#30340;&#29983;&#27963;&#26041;&#24335;&#21464;&#21270;&#65306;&#28608;&#21169;&#24335;&#35775;&#35848;&#12290;&#35299;&#20915;&#36825;&#26679;&#19968;&#20010;&#20219;&#21153;&#38656;&#35201;&#19968;&#20010;&#31995;&#32479;&#33021;&#22815;&#25512;&#26029;&#22914;&#20309;&#26377;&#25928;&#22320;&#28608;&#21169;&#29992;&#25143;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DIIT&#65292;&#19968;&#20010;&#33021;&#22815;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#21644;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#24402;&#32435;&#35268;&#21017;&#24418;&#24335;&#30340;&#23545;&#35805;&#31574;&#30053;&#30340;&#26694;&#26550;&#12290;&#23545;&#25351;&#31034;&#36981;&#24490;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#26174;&#31034;&#65292;DIIR&#21457;&#29616;&#30340;&#33258;&#28982;&#35821;&#35328;&#31574;&#30053;&#25551;&#36848;&#33021;&#22815;&#25552;&#39640;&#20027;&#21160;&#20542;&#21548;&#25216;&#33021;&#65292;&#20943;&#23569;&#19981;&#35831;&#33258;&#26469;&#30340;&#24314;&#35758;&#65292;&#24182;&#20419;&#36827;&#26356;&#20855;&#21327;&#20316;&#24615;&#21644;&#19981;&#37027;&#20040;&#26435;&#23041;&#24615;&#30340;&#22238;&#24212;&#65292;&#20248;&#20110;&#21508;&#31181;&#28436;&#31034;&#21033;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15737v1 Announce Type: new  Abstract: We consider the task of building a dialogue system that can motivate users to adopt positive lifestyle changes: Motivational Interviewing. Addressing such a task requires a system that can infer \textit{how} to motivate a user effectively. We propose DIIT, a framework that is capable of learning and applying conversation strategies in the form of natural language inductive rules from expert demonstrations. Automatic and human evaluation on instruction-following large language models show natural language strategy descriptions discovered by DIIR can improve active listening skills, reduce unsolicited advice, and promote more collaborative and less authoritative responses, outperforming various demonstration utilization methods.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#34701;&#21512;&#26041;&#27861;&#65292;&#23558;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#30693;&#35782;&#34701;&#20837;LLMs&#20013;&#65292;&#29992;&#20110;&#26356;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.15736</link><description>&lt;p&gt;
LLMs&#25351;&#23548;LLMs&#65306;&#19968;&#31181;&#25552;&#21462;&#21644;&#32534;&#36753;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LLMs Instruct LLMs:An Extraction and Editing Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15736
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#34701;&#21512;&#26041;&#27861;&#65292;&#23558;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#30693;&#35782;&#34701;&#20837;LLMs&#20013;&#65292;&#29992;&#20110;&#26356;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15736v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#20852;&#36259;&#28857;&#22312;&#20110;&#26080;&#38656;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#21363;&#21487;&#26356;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20294;&#26159;&#36825;&#20063;&#24102;&#26469;&#20102;&#19968;&#20123;&#25361;&#25112;&#12290;&#23588;&#20854;&#26159;&#23545;&#20110;&#38656;&#35201;&#29992;&#26377;&#38480;&#26679;&#26412;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#30340;&#24773;&#20917;&#26469;&#35828;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#36866;&#29992;&#20110;LLMs&#30340;&#36139;&#20047;&#32422;&#26463;&#22797;&#26434;&#25512;&#29702;&#65288;PCRA-LLM&#65289;&#30340;&#24773;&#20917;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#23545;&#36825;&#19968;&#20851;&#38190;&#38382;&#39064;&#26159;&#19981;&#36275;&#22815;&#30340;&#65292;&#23588;&#20854;&#22312;&#25105;&#20204;&#25506;&#32034;&#29305;&#23450;&#21307;&#23398;&#32972;&#26223;&#26102;&#23588;&#20026;&#26126;&#26174;&#65292;&#36825;&#20307;&#29616;&#20102;PCRA-LLM&#30340;&#29420;&#29305;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#34701;&#21512;&#26041;&#27861;&#65292;&#23558;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#30693;&#35782;&#34701;&#20837;LLMs&#20013;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20004;&#38454;&#27573;&#26694;&#26550;&#65306;&#39318;&#20808;&#65292;&#21033;&#29992;&#36890;&#29992;LLMs&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#26469;&#20174;&#22797;&#26434;&#25991;&#26412;&#20013;&#25552;&#21462;&#30693;&#35782;&#65307;&#38543;&#21518;&#65292;&#36890;&#36807;&#30693;&#35782;&#32534;&#36753;&#26469;&#26356;&#26032;&#39046;&#22495;LLMs&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15736v1 Announce Type: new  Abstract: The interest in updating Large Language Models (LLMs) without retraining from scratch is substantial, yet it comes with some challenges.This is especially true for situations demanding complex reasoning with limited samples, a scenario we refer to as the Paucity-Constrained Complex Reasoning Adaptation for LLMs (PCRA-LLM).Traditional methods like Low-Rank Adaptation (LoRA) and Retrieval-Augmented Generation (RAG) are inadequate for this critical issue, particularly evident in our exploration of a specific medical context that epitomize the PCRA-LLM's distinct needs.To address the issue, we propose a Sequential Fusion method to incorporate knowledge from complex context into LLMs. This method employs a two-stage framework: initially, it leverages general LLMs to construct knowledge graphs (KGs) for extracting knowledge from complex texts; subsequently, it updates the domain LLMs through knowledge edit. According to our method, the domain 
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#38754;&#21521;&#30005;&#23376;&#31163;&#23376;&#23545;&#25758;&#26426;&#30340;&#22522;&#20110;RAG&#30340;&#25688;&#35201;&#29983;&#25104;&#20195;&#29702;&#65292;&#33021;&#22815;&#21387;&#32553;&#20449;&#24687;&#24182;&#24341;&#29992;&#30456;&#20851;&#22238;&#22797;&#65292;&#20026;&#21512;&#20316;&#32773;&#25552;&#20379;&#37325;&#22823;&#20248;&#21183;</title><link>https://arxiv.org/abs/2403.15729</link><description>&lt;p&gt;
&#38754;&#21521;&#30005;&#23376;&#31163;&#23376;&#23545;&#25758;&#26426;&#30340;&#22522;&#20110;RAG&#30340;&#25688;&#35201;&#29983;&#25104;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Towards a \textbf{RAG}-based Summarization Agent for the Electron-Ion Collider
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15729
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#38754;&#21521;&#30005;&#23376;&#31163;&#23376;&#23545;&#25758;&#26426;&#30340;&#22522;&#20110;RAG&#30340;&#25688;&#35201;&#29983;&#25104;&#20195;&#29702;&#65292;&#33021;&#22815;&#21387;&#32553;&#20449;&#24687;&#24182;&#24341;&#29992;&#30456;&#20851;&#22238;&#22797;&#65292;&#20026;&#21512;&#20316;&#32773;&#25552;&#20379;&#37325;&#22823;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#24615;&#21644;&#24222;&#22823;&#30340;&#20449;&#24687;&#37327;&#28085;&#30422;&#20102;&#22823;&#35268;&#27169;&#23454;&#39564;&#30340;&#25991;&#20214;&#12289;&#35770;&#25991;&#12289;&#25968;&#25454;&#21644;&#20854;&#20182;&#36164;&#28304;&#65292;&#23548;&#33268;&#23548;&#33322;&#36825;&#20123;&#22810;&#26679;&#24418;&#24335;&#20449;&#24687;&#30340;&#20219;&#21153;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#21644;&#31934;&#21147;&#65292;&#23545;&#20110;&#26032;&#21512;&#20316;&#32773;&#21644;&#26089;&#26399;&#31185;&#23398;&#23478;&#26469;&#35828;&#23588;&#20026;&#33392;&#24040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#27491;&#22312;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;EIC&#25688;&#35201;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65288;RAGS4EIC&#65289;&#12290;&#35813;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#19981;&#20165;&#21387;&#32553;&#20449;&#24687;&#65292;&#36824;&#26377;&#25928;&#24341;&#29992;&#30456;&#20851;&#22238;&#22797;&#65292;&#20026;&#21512;&#20316;&#32773;&#25552;&#20379;&#20102;&#37325;&#22823;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#37319;&#21462;&#20102;&#20004;&#27493;&#26041;&#27861;&#65306;&#39318;&#20808;&#65292;&#26597;&#35810;&#21253;&#21547;&#25152;&#26377;&#30456;&#20851;&#23454;&#39564;&#20449;&#24687;&#30340;&#32508;&#21512;&#21521;&#37327;&#25968;&#25454;&#24211;&#65307;&#20854;&#27425;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26681;&#25454;&#29992;&#25143;&#26597;&#35810;&#21644;&#26816;&#32034;&#25968;&#25454;&#29983;&#25104;&#21253;&#21547;&#24341;&#29992;&#30340;&#31616;&#27905;&#25688;&#35201;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#20351;&#29992;RAG&#35780;&#20272;&#30340;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15729v1 Announce Type: cross  Abstract: The complexity and sheer volume of information encompassing documents, papers, data, and other resources from large-scale experiments demand significant time and effort to navigate, making the task of accessing and utilizing these varied forms of information daunting, particularly for new collaborators and early-career scientists. To tackle this issue, a Retrieval Augmented Generation (RAG)--based Summarization AI for EIC (RAGS4EIC) is under development. This AI-Agent not only condenses information but also effectively references relevant responses, offering substantial advantages for collaborators. Our project involves a two-step approach: first, querying a comprehensive vector database containing all pertinent experiment information; second, utilizing a Large Language Model (LLM) to generate concise summaries enriched with citations based on user queries and retrieved data. We describe the evaluation methods that use RAG assessments 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PEaCE&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#20854;&#20013;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#35760;&#24405;&#35780;&#20272;&#20102;&#22522;&#20110;transformer&#30340;OCR&#27169;&#22411;&#22312;&#21270;&#23398;&#25991;&#29486;&#20013;&#30340;&#35782;&#21035;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#21487;&#20197;&#27169;&#25311;&#30495;&#23454;&#35760;&#24405;&#29305;&#24449;&#30340;&#36716;&#25442;&#12290;</title><link>https://arxiv.org/abs/2403.15724</link><description>&lt;p&gt;
PEaCE&#65306;&#29992;&#20110;&#31185;&#23398;&#25991;&#26723;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#30340;&#21270;&#23398;&#23548;&#21521;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PEaCE: A Chemistry-Oriented Dataset for Optical Character Recognition on Scientific Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15724
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PEaCE&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#20854;&#20013;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#35760;&#24405;&#35780;&#20272;&#20102;&#22522;&#20110;transformer&#30340;OCR&#27169;&#22411;&#22312;&#21270;&#23398;&#25991;&#29486;&#20013;&#30340;&#35782;&#21035;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#21487;&#20197;&#27169;&#25311;&#30495;&#23454;&#35760;&#24405;&#29305;&#24449;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Optical Character Recognition&#65288;OCR&#65289;&#26159;&#19968;&#20010;&#26088;&#22312;&#35782;&#21035;&#22270;&#20687;&#20013;&#23384;&#22312;&#30340;&#25991;&#26412;&#30340;&#26082;&#23450;&#20219;&#21153;&#12290;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#29616;&#25104;&#30340;OCR&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26159;&#38024;&#23545;&#31185;&#23398;&#65288;&#20363;&#22914;&#65292;&#20844;&#24335;&#65289;&#25110;&#36890;&#29992;&#21360;&#21047;&#33521;&#25991;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#20174;&#21270;&#23398;&#20986;&#29256;&#29289;&#20013;&#25552;&#21462;&#25991;&#26412;&#38656;&#35201;&#19968;&#31181;&#33021;&#22815;&#22312;&#36825;&#20004;&#20010;&#39046;&#22495;&#20013;&#36827;&#34892;&#25805;&#20316;&#30340;OCR&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#24037;&#20855;Nougat&#34920;&#29616;&#20986;&#35299;&#26512;&#23398;&#26415;&#25991;&#26723;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#20294;&#26080;&#27861;&#35299;&#26512;PubMed&#25991;&#31456;&#20013;&#30340;&#34920;&#26684;&#65292;&#36825;&#26500;&#25104;&#20102;&#23398;&#26415;&#30028;&#19968;&#20010;&#37325;&#35201;&#37096;&#20998;&#65292;&#24182;&#19988;&#20063;&#26159;&#26412;&#27425;&#24037;&#20316;&#30340;&#37325;&#28857;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21253;&#21547;&#21512;&#25104;&#21644;&#30495;&#23454;&#35760;&#24405;&#30340;Printed English and Chemical Equations&#65288;PEaCE&#65289;&#25968;&#25454;&#38598;&#65292;&#24182;&#35780;&#20272;&#20102;&#24403;&#36825;&#19968;&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#22522;&#20110;transformer&#30340;OCR&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#37492;&#20110;&#30495;&#23454;&#35760;&#24405;&#21253;&#21547;&#21512;&#25104;&#35760;&#24405;&#20013;&#19981;&#23384;&#22312;&#30340;&#20154;&#24037;&#21046;&#21697;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#20223;&#36825;&#20123;&#29305;&#36136;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15724v1 Announce Type: cross  Abstract: Optical Character Recognition (OCR) is an established task with the objective of identifying the text present in an image. While many off-the-shelf OCR models exist, they are often trained for either scientific (e.g., formulae) or generic printed English text. Extracting text from chemistry publications requires an OCR model that is capable in both realms. Nougat, a recent tool, exhibits strong ability to parse academic documents, but is unable to parse tables in PubMed articles, which comprises a significant part of the academic community and is the focus of this work. To mitigate this gap, we present the Printed English and Chemical Equations (PEaCE) dataset, containing both synthetic and real-world records, and evaluate the efficacy of transformer-based OCR models when trained on this resource. Given that real-world records contain artifacts not present in synthetic records, we propose transformations that mimic such qualities. We p
&lt;/p&gt;</description></item><item><title>EDDA&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;if-then&#21407;&#29702;&#21644;&#35821;&#20041;&#30456;&#20851;&#30340;&#35789;&#26367;&#25442;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#38646;&#26679;&#26412;&#31435;&#22330;&#26816;&#27979;&#20013;&#30446;&#26631;&#22686;&#24378;&#21644;&#25991;&#26412;&#22686;&#24378;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.15715</link><description>&lt;p&gt;
EDDA&#65306;&#19968;&#31181;&#29992;&#20110;&#38646;&#26679;&#26412;&#31435;&#22330;&#26816;&#27979;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EDDA: A Encoder-Decoder Data Augmentation Framework for Zero-Shot Stance Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15715
&lt;/p&gt;
&lt;p&gt;
EDDA&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;if-then&#21407;&#29702;&#21644;&#35821;&#20041;&#30456;&#20851;&#30340;&#35789;&#26367;&#25442;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#38646;&#26679;&#26412;&#31435;&#22330;&#26816;&#27979;&#20013;&#30446;&#26631;&#22686;&#24378;&#21644;&#25991;&#26412;&#22686;&#24378;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31435;&#22330;&#26816;&#27979;&#26088;&#22312;&#30830;&#23450;&#25991;&#26412;&#23545;&#29305;&#23450;&#30446;&#26631;&#34920;&#36798;&#30340;&#24577;&#24230;&#12290;&#38646;&#26679;&#26412;&#31435;&#22330;&#26816;&#27979;&#65288;ZSSD&#65289;&#26088;&#22312;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23545;&#26410;&#35265;&#30446;&#26631;&#36827;&#34892;&#31435;&#22330;&#20998;&#31867;&#12290;&#26368;&#36817;&#38024;&#23545;ZSSD&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#36890;&#36807;&#25991;&#26412;&#25110;&#30446;&#26631;&#22686;&#24378;&#22686;&#21152;&#20102;&#22312;&#19981;&#21516;&#30446;&#26631;&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#19968;&#23450;&#23616;&#38480;&#24615;&#12290;&#30446;&#26631;&#22686;&#24378;&#32570;&#20047;&#29983;&#25104;&#30446;&#26631;&#19982;&#28304;&#25991;&#26412;&#20043;&#38388;&#30340;&#36923;&#36753;&#20851;&#31995;&#65292;&#32780;&#25991;&#26412;&#22686;&#24378;&#20165;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#65292;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25968;&#25454;&#22686;&#24378;&#65288;EDDA&#65289;&#26694;&#26550;&#12290;&#32534;&#30721;&#22120;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#23558;&#25991;&#26412;&#24635;&#32467;&#20026;&#29305;&#23450;&#20110;&#30446;&#26631;&#30340;if-then&#21407;&#29702;&#65292;&#24314;&#31435;&#36923;&#36753;&#20851;&#31995;&#12290;&#35299;&#30721;&#22120;&#26681;&#25454;&#36825;&#20123;&#34920;&#36798;&#24335;&#20351;&#29992;&#35821;&#20041;&#30456;&#20851;&#30340;&#35789;&#26367;&#25442;&#31574;&#30053;&#29983;&#25104;&#26032;&#26679;&#26412;&#65292;&#22686;&#21152;&#20102;&#20854;&#21516;&#20041;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15715v1 Announce Type: new  Abstract: Stance detection aims to determine the attitude expressed in text towards a given target. Zero-shot stance detection (ZSSD) has emerged to classify stances towards unseen targets during inference. Recent data augmentation techniques for ZSSD increase transferable knowledge between targets through text or target augmentation. However, these methods exhibit limitations. Target augmentation lacks logical connections between generated targets and source text, while text augmentation relies solely on training data, resulting in insufficient generalization. To address these issues, we propose an encoder-decoder data augmentation (EDDA) framework. The encoder leverages large language models and chain-of-thought prompting to summarize texts into target-specific if-then rationales, establishing logical relationships. The decoder generates new samples based on these expressions using a semantic correlation word replacement strategy to increase syn
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;FEEL&#65292;&#29992;&#20110;&#35780;&#20272;&#24773;&#24863;&#25903;&#25345;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#38750;&#20154;&#24037;&#26041;&#27861;&#22312;&#35780;&#20272;&#24773;&#24863;&#25903;&#25345;&#33021;&#21147;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#37319;&#29992;&#20102;&#27010;&#29575;&#20998;&#24067;&#26041;&#27861;&#21644;&#38598;&#25104;&#23398;&#20064;&#20197;&#33719;&#24471;&#26356;&#31283;&#23450;&#21644;&#20840;&#38754;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.15699</link><description>&lt;p&gt;
FEEL&#65306;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24773;&#24863;&#25903;&#25345;&#33021;&#21147;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FEEL: A Framework for Evaluating Emotional Support Capability with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15699
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;FEEL&#65292;&#29992;&#20110;&#35780;&#20272;&#24773;&#24863;&#25903;&#25345;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#38750;&#20154;&#24037;&#26041;&#27861;&#22312;&#35780;&#20272;&#24773;&#24863;&#25903;&#25345;&#33021;&#21147;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#37319;&#29992;&#20102;&#27010;&#29575;&#20998;&#24067;&#26041;&#27861;&#21644;&#38598;&#25104;&#23398;&#20064;&#20197;&#33719;&#24471;&#26356;&#31283;&#23450;&#21644;&#20840;&#38754;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#65288;ESC&#65289;&#26159;&#19968;&#31181;&#20856;&#22411;&#30340;&#23545;&#35805;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24110;&#21161;&#29992;&#25143;&#32531;&#35299;&#24773;&#24863;&#21387;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24773;&#24863;&#20998;&#26512;&#20013;&#28041;&#21450;&#22266;&#26377;&#20027;&#35266;&#24615;&#65292;&#24403;&#21069;&#38750;&#20154;&#24037;&#26041;&#27861;&#22312;&#26377;&#25928;&#35780;&#20272;&#24773;&#24863;&#25903;&#25345;&#33021;&#21147;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#36825;&#20123;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#23384;&#22312;&#24456;&#20302;&#30340;&#30456;&#20851;&#24615;&#12290;&#21516;&#26102;&#65292;&#25163;&#21160;&#35780;&#20272;&#26041;&#27861;&#23558;&#23548;&#33268;&#24456;&#39640;&#30340;&#25104;&#26412;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#27169;&#22411;FEEL&#65288;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#24773;&#24863;&#25903;&#25345;&#33021;&#21147;&#30340;&#26694;&#26550;&#65289;&#65292;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#35780;&#20272;&#32773;&#26469;&#35780;&#20272;&#24773;&#24863;&#25903;&#25345;&#33021;&#21147;&#12290;&#35813;&#27169;&#22411;&#21608;&#23494;&#32771;&#34385;ESC&#30340;&#21508;&#31181;&#35780;&#20272;&#26041;&#38754;&#65292;&#24212;&#29992;&#26356;&#20840;&#38754;&#21644;&#20934;&#30830;&#30340;ESC&#35780;&#20272;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#23427;&#37319;&#29992;&#27010;&#29575;&#20998;&#24067;&#26041;&#27861;&#20197;&#33719;&#24471;&#26356;&#31283;&#23450;&#30340;&#32467;&#26524;&#65292;&#24182;&#38598;&#25104;&#20102;&#38598;&#25104;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15699v1 Announce Type: new  Abstract: Emotional Support Conversation (ESC) is a typical dialogue that can effec-tively assist the user in mitigating emotional pressures. However, owing to the inherent subjectivity involved in analyzing emotions, current non-artificial methodologies face challenges in effectively appraising the emo-tional support capability. These metrics exhibit a low correlation with human judgments. Concurrently, manual evaluation methods extremely will cause high costs. To solve these problems, we propose a novel model FEEL (Framework for Evaluating Emotional Support Capability with Large Lan-guage Models), employing Large Language Models (LLMs) as evaluators to assess emotional support capabilities. The model meticulously considers var-ious evaluative aspects of ESC to apply a more comprehensive and accurate evaluation method for ESC. Additionally, it employs a probability distribu-tion approach for a more stable result and integrates an ensemble learnin
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;MixRE&#65292;&#24182;&#26500;&#24314;&#20102;&#25903;&#25345;&#35813;&#20219;&#21153;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#38598;MixRED&#65292;&#22635;&#34917;&#20102;&#22810;&#35821;&#35328;&#24773;&#26223;&#19979;&#20851;&#31995;&#25277;&#21462;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.15696</link><description>&lt;p&gt;
MixRED: &#19968;&#20010;&#22810;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MixRED: A Mix-lingual Relation Extraction Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15696
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;MixRE&#65292;&#24182;&#26500;&#24314;&#20102;&#25903;&#25345;&#35813;&#20219;&#21153;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#38598;MixRED&#65292;&#22635;&#34917;&#20102;&#22810;&#35821;&#35328;&#24773;&#26223;&#19979;&#20851;&#31995;&#25277;&#21462;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15696v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25688;&#35201;&#65306;&#20851;&#31995;&#25277;&#21462;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#20855;&#26377;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#35821;&#20851;&#31995;&#25277;&#21462;&#25110;&#29992;&#20110;&#20851;&#31995;&#25277;&#21462;&#30340;&#36328;&#35821;&#35328;&#22686;&#24378;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#35821;&#35328;&#65288;&#25110;&#20195;&#30721;&#28151;&#21512;&#65289;&#22330;&#26223;&#20013;&#65292;&#20173;&#23384;&#22312;&#23545;&#20851;&#31995;&#25277;&#21462;&#30340;&#29702;&#35299;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#65292;&#22312;&#35813;&#22330;&#26223;&#20013;&#65292;&#20010;&#20307;&#22312;&#21477;&#23376;&#20013;&#28151;&#21512;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#30340;&#20869;&#23481;&#65292;&#29983;&#25104;&#22810;&#35821;&#20869;&#23481;&#12290;&#30001;&#20110;&#32570;&#20047;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#65292;&#29616;&#26377;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23578;&#26410;&#25506;&#35752;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#32771;&#34385;&#22810;&#35821;&#35328;&#24773;&#26223;&#20013;&#30340;&#20851;&#31995;&#25277;&#21462;&#65292;&#31216;&#20026;MixRE&#65292;&#24182;&#26500;&#24314;&#20102;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#38598;MixRED&#20197;&#25903;&#25345;&#27492;&#20219;&#21153;&#12290;&#38500;&#20102;&#26500;&#24314;MixRED&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#27169;&#22411;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15696v1 Announce Type: new  Abstract: Relation extraction is a critical task in the field of natural language processing with numerous real-world applications. Existing research primarily focuses on monolingual relation extraction or cross-lingual enhancement for relation extraction. Yet, there remains a significant gap in understanding relation extraction in the mix-lingual (or code-switching) scenario, where individuals intermix contents from different languages within sentences, generating mix-lingual content. Due to the lack of a dedicated dataset, the effectiveness of existing relation extraction models in such a scenario is largely unexplored. To address this issue, we introduce a novel task of considering relation extraction in the mix-lingual scenario called MixRE and constructing the human-annotated dataset MixRED to support this task. In addition to constructing the MixRED dataset, we evaluate both state-of-the-art supervised models and large language models (LLMs)
&lt;/p&gt;</description></item><item><title>EAGLE&#25552;&#20986;&#20102;&#19968;&#20010;&#39046;&#22495;&#27867;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#21033;&#29992;&#20174;&#26087;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#23398;&#20064;&#29305;&#24449;&#30340;&#19981;&#21464;&#24615;&#65292;&#20174;&#32780;&#26816;&#27979;&#20986;&#26410;&#30693;&#30446;&#26631;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.15690</link><description>&lt;p&gt;
EAGLE&#65306;&#38754;&#21521;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#39046;&#22495;&#27867;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EAGLE: A Domain Generalization Framework for AI-generated Text Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15690
&lt;/p&gt;
&lt;p&gt;
EAGLE&#25552;&#20986;&#20102;&#19968;&#20010;&#39046;&#22495;&#27867;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#21033;&#29992;&#20174;&#26087;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#23398;&#20064;&#29305;&#24449;&#30340;&#19981;&#21464;&#24615;&#65292;&#20174;&#32780;&#26816;&#27979;&#20986;&#26410;&#30693;&#30446;&#26631;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21147;&#30340;&#25552;&#21319;&#65292;&#36127;&#36131;&#20219;&#21644;&#23433;&#20840;&#20351;&#29992;&#36825;&#20123;LLMs&#30340;&#19968;&#20010;&#37325;&#35201;&#27493;&#39588;&#26159;&#33021;&#22815;&#26816;&#27979;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#23613;&#31649;&#30417;&#30563;&#24335;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#26816;&#27979;&#22120;&#22312;&#26087;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#38543;&#30528;&#26032;LLMs&#30340;&#39057;&#32321;&#21457;&#24067;&#65292;&#26500;&#24314;&#29992;&#20110;&#35782;&#21035;&#36825;&#20123;&#26032;&#27169;&#22411;&#25991;&#26412;&#30340;&#30417;&#30563;&#26816;&#27979;&#22120;&#23558;&#38656;&#35201;&#26032;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#23454;&#36341;&#20013;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#26469;&#33258;&#26410;&#30693;&#30446;&#26631;&#29983;&#25104;&#22120;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#39046;&#22495;&#27867;&#21270;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;EAGLE&#21033;&#29992;&#36804;&#20170;&#20026;&#27490;&#20174;&#26087;&#35821;&#35328;&#27169;&#22411;&#33719;&#24471;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#23398;&#20064;&#36328;&#36825;&#20123;&#29983;&#25104;&#22120;&#19981;&#21464;&#30340;&#29305;&#24449;&#65292;&#20197;&#20415;&#26816;&#27979;&#30001;&#26410;&#30693;&#30446;&#26631;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;EAGLE&#36890;&#36807;&#32467;&#21512;&#33258;&#30417;&#30563;&#30340;&#34920;&#24449;&#33021;&#21147;&#26469;&#23398;&#20064;&#36825;&#31181;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15690v1 Announce Type: cross  Abstract: With the advancement in capabilities of Large Language Models (LLMs), one major step in the responsible and safe use of such LLMs is to be able to detect text generated by these models. While supervised AI-generated text detectors perform well on text generated by older LLMs, with the frequent release of new LLMs, building supervised detectors for identifying text from such new models would require new labeled training data, which is infeasible in practice. In this work, we tackle this problem and propose a domain generalization framework for the detection of AI-generated text from unseen target generators. Our proposed framework, EAGLE, leverages the labeled data that is available so far from older language models and learns features invariant across these generators, in order to detect text generated by an unknown target generator. EAGLE learns such domain-invariant features by combining the representational power of self-supervised 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31639;&#26415;&#30005;&#36335;&#32422;&#26463;&#32534;&#30721;&#20026;&#22810;&#39033;&#24335;&#26041;&#31243;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#20195;&#25968;&#35745;&#31639;&#22312;&#26377;&#38480;&#22495;&#19978;&#35299;&#20915;&#22810;&#39033;&#24335;&#26041;&#31243;&#31995;&#32479;&#65292;&#20197;&#31934;&#30830;&#23450;&#20301;ZKP&#30005;&#36335;&#20013;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2403.15676</link><description>&lt;p&gt;
AC4&#65306;&#29992;&#20110;ZKP&#20013;&#30005;&#36335;&#32422;&#26463;&#30340;&#20195;&#25968;&#35745;&#31639;&#26816;&#26597;&#22120;
&lt;/p&gt;
&lt;p&gt;
AC4: Algebraic Computation Checker for Circuit Constraints in ZKPs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15676
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31639;&#26415;&#30005;&#36335;&#32422;&#26463;&#32534;&#30721;&#20026;&#22810;&#39033;&#24335;&#26041;&#31243;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#20195;&#25968;&#35745;&#31639;&#22312;&#26377;&#38480;&#22495;&#19978;&#35299;&#20915;&#22810;&#39033;&#24335;&#26041;&#31243;&#31995;&#32479;&#65292;&#20197;&#31934;&#30830;&#23450;&#20301;ZKP&#30005;&#36335;&#20013;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ZKP&#31995;&#32479;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#22312;&#24403;&#20195;&#23494;&#30721;&#23398;&#20013;&#21457;&#25381;&#30528;&#22522;&#30784;&#24615;&#20316;&#29992;&#12290; Zk-SNARK&#21327;&#35758;&#20027;&#23548;&#20102;ZKP&#30340;&#20351;&#29992;&#65292;&#36890;&#24120;&#36890;&#36807;&#31639;&#26415;&#30005;&#36335;&#32534;&#31243;&#33539;&#24335;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#27424;&#32422;&#26463;&#25110;&#36807;&#32422;&#26463;&#30340;&#30005;&#36335;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#12290; &#27424;&#32422;&#26463;&#30340;&#30005;&#36335;&#25351;&#30340;&#26159;&#32570;&#20047;&#24517;&#35201;&#32422;&#26463;&#30340;&#30005;&#36335;&#65292;&#23548;&#33268;&#30005;&#36335;&#20013;&#20986;&#29616;&#24847;&#22806;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23548;&#33268;&#39564;&#35777;&#32773;&#25509;&#21463;&#38169;&#35823;&#35265;&#35777;&#12290; &#36807;&#32422;&#26463;&#30340;&#30005;&#36335;&#26159;&#25351;&#32422;&#26463;&#36807;&#24230;&#30340;&#30005;&#36335;&#65292;&#23548;&#33268;&#30005;&#36335;&#32570;&#20047;&#24517;&#35201;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23548;&#33268;&#39564;&#35777;&#32773;&#25509;&#21463;&#27809;&#26377;&#35265;&#35777;&#65292;&#20351;&#30005;&#36335;&#27627;&#26080;&#24847;&#20041;&#12290; &#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#25214;&#20986;ZKP&#30005;&#36335;&#20013;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#38169;&#35823;&#12290; &#35813;&#26041;&#27861;&#28041;&#21450;&#23558;&#31639;&#26415;&#30005;&#36335;&#32422;&#26463;&#32534;&#30721;&#20026;&#22810;&#39033;&#24335;&#26041;&#31243;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#20195;&#25968;&#35745;&#31639;&#22312;&#26377;&#38480;&#22495;&#19978;&#35299;&#20915;&#22810;&#39033;&#24335;&#26041;&#31243;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15676v1 Announce Type: cross  Abstract: ZKP systems have surged attention and held a fundamental role in contemporary cryptography. Zk-SNARK protocols dominate the ZKP usage, often implemented through arithmetic circuit programming paradigm. However, underconstrained or overconstrained circuits may lead to bugs. Underconstrained circuits refer to circuits that lack the necessary constraints, resulting in unexpected solutions in the circuit and causing the verifier to accept a bogus witness. Overconstrained circuits refer to circuits that are constrained excessively, resulting in the circuit lacking necessary solutions and causing the verifier to accept no witness, rendering the circuit meaningless. This paper introduces a novel approach for pinpointing two distinct types of bugs in ZKP circuits. The method involves encoding the arithmetic circuit constraints to polynomial equation systems and solving polynomial equation systems over a finite field by algebraic computation. T
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#33021;&#29992;&#20110;&#25512;&#21160;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#30340;&#21457;&#29616;&#21644;&#24212;&#29992;</title><link>https://arxiv.org/abs/2403.15673</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#29983;&#29289;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
AI for Biomedicine in the Era of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15673
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#33021;&#29992;&#20110;&#25512;&#21160;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#30340;&#21457;&#29616;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#30340;&#33021;&#21147;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#39046;&#22495;&#65292;&#20174;&#35299;&#20915;&#37327;&#23376;&#31995;&#32479;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#21040;&#39044;&#27979;&#21270;&#23398;&#25110;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20998;&#23376;&#27700;&#24179;&#65292;&#36827;&#19968;&#27493;&#24310;&#20280;&#21040;&#39044;&#27979;&#20256;&#26579;&#30149;&#29190;&#21457;&#31561;&#31038;&#20250;&#39044;&#27979;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#27493;&#65292;&#22914;ChatGPT&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#30340;&#26174;&#33879;&#23454;&#21147;&#65292;&#27604;&#22914;&#32763;&#35793;&#35821;&#35328;&#65292;&#26500;&#24314;&#32842;&#22825;&#26426;&#22120;&#20154;&#20197;&#21450;&#22238;&#31572;&#38382;&#39064;&#12290;&#24403;&#32771;&#34385;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19982;&#33258;&#28982;&#35821;&#35328;&#31867;&#20284;&#30340;&#24207;&#21015;&#24615;: &#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#21644;&#20581;&#24247;&#35760;&#24405;&#21576;&#29616;&#20026;&#25991;&#26412;&#65292;&#29983;&#29289;&#24207;&#21015;&#25110;&#27979;&#24207;&#25968;&#25454;&#20197;&#24207;&#21015;&#26041;&#24335;&#25490;&#21015;&#65292;&#25110;&#32773;&#20256;&#24863;&#22120;&#25968;&#25454;&#22914;&#33041;&#20449;&#21495;&#21017;&#21576;&#29616;&#20026;&#26102;&#38388;&#24207;&#21015;&#12290;&#19968;&#20010;&#38382;&#39064;&#20986;&#29616;&#20102;: &#25105;&#20204;&#26159;&#21542;&#33021;&#21033;&#29992;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#26469;&#25512;&#21160;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#30340;&#21457;&#29616;? &#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23558;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15673v1 Announce Type: new  Abstract: The capabilities of AI for biomedicine span a wide spectrum, from the atomic level, where it solves partial differential equations for quantum systems, to the molecular level, predicting chemical or protein structures, and further extending to societal predictions like infectious disease outbreaks. Recent advancements in large language models, exemplified by models like ChatGPT, have showcased significant prowess in natural language tasks, such as translating languages, constructing chatbots, and answering questions. When we consider biomedical data, we observe a resemblance to natural language in terms of sequences: biomedical literature and health records presented as text, biological sequences or sequencing data arranged in sequences, or sensor data like brain signals as time series. The question arises: Can we harness the potential of recent large language models to drive biomedical knowledge discoveries? In this survey, we will expl
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Private Mixing of Ensemble Distributions (PMixED)&#65306;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#36755;&#20986;&#20998;&#24067;&#25237;&#24433;&#21040;&#20844;&#20849;LLM&#30340;&#36755;&#20986;&#20998;&#24067;&#21608;&#22260;&#30340;&#38598;&#21512;&#19978;&#65292;&#24182;&#37319;&#26679;&#24179;&#22343;&#26469;&#23454;&#29616;&#23454;&#38469;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65292;&#20197;&#26356;&#36731;&#37327;&#21270;&#30340;&#26041;&#24335;&#23454;&#29616;&#23545;&#38544;&#31169;&#25935;&#24863;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.15638</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24046;&#20998;&#31169;&#26377;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Next-Token Prediction of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15638
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Private Mixing of Ensemble Distributions (PMixED)&#65306;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#36755;&#20986;&#20998;&#24067;&#25237;&#24433;&#21040;&#20844;&#20849;LLM&#30340;&#36755;&#20986;&#20998;&#24067;&#21608;&#22260;&#30340;&#38598;&#21512;&#19978;&#65292;&#24182;&#37319;&#26679;&#24179;&#22343;&#26469;&#23454;&#29616;&#23454;&#38469;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65292;&#20197;&#26356;&#36731;&#37327;&#21270;&#30340;&#26041;&#24335;&#23454;&#29616;&#23545;&#38544;&#31169;&#25935;&#24863;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38544;&#31169;&#26085;&#30410;&#37325;&#35201;&#12290;DP-SGD&#26159;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#26368;&#24191;&#27867;&#37319;&#29992;&#30340;&#25216;&#26415;&#65292;&#23427;&#20197;&#19968;&#31181;&#20445;&#35777;&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#24335;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;DP-SGD&#38656;&#35201;&#27604;SGD&#26356;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#26356;&#22823;&#30340;&#20869;&#23384;&#38656;&#27714;&#65292;&#21516;&#26102;&#36807;&#39640;&#20272;&#35745;&#23545;&#25163;&#20855;&#26377;&#30333;&#30418;&#35775;&#38382;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#26356;&#29616;&#23454;&#30340;&#22330;&#26223;&#20551;&#35774;&#21482;&#26377;&#23545;&#38544;&#31169;&#25935;&#24863;&#30340;LLM&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#12290;&#22312;&#36825;&#20123;&#35266;&#23519;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31169;&#26377;&#28151;&#21512;&#38598;&#21512;&#20998;&#24067;&#65288;PMixED&#65289;&#65306;&#19968;&#31181;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#27599;&#20010;&#36755;&#20986;&#20998;&#24067;&#20174;&#19968;&#20010;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;LLM&#38598;&#21512;&#25237;&#24433;&#21040;&#20844;&#20849;LLM&#36755;&#20986;&#20998;&#24067;&#21608;&#22260;&#30340;&#38598;&#21512;&#19978;&#65292;&#28982;&#21518;&#23545;&#25237;&#24433;&#20998;&#24067;&#36827;&#34892;&#24179;&#22343;&#24182;&#20174;&#20013;&#25277;&#26679;&#26469;&#23454;&#29616;&#23454;&#38469;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#31169;&#26377;&#39044;&#27979;&#21327;&#35758;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;DP-SGD&#26356;&#36731;&#37327;&#21270;&#65292;&#22240;&#20026;&#23427;&#19982;&#27169;&#22411;&#26080;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15638v1 Announce Type: cross  Abstract: Ensuring the privacy of Large Language Models (LLMs) is becoming increasingly important. The most widely adopted technique to accomplish this is DP-SGD, which trains a model in such a way that guarantees Differential Privacy (DP). However, DP-SGD requires longer training times and larger memory requirements than SGD, while overestimating an adversary's capabilities in having white box access to the model. A more realistic scenario assumes only black-box access to a privacy-sensitive LLM. Motivated by these observations, we present Private Mixing of Ensemble Distributions (PMixED): a private prediction protocol that achieves practical next-token prediction by projecting each of the model's output distribution from an ensemble of fine-tuned LLMs onto a set around a public LLM's output distribution, then averaging the projected distributions and sampling from it. Our approach is more lightweight than DP-SGD in that it is model agnostic, i
&lt;/p&gt;</description></item><item><title>NaturalTurn&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#20934;&#30830;&#25429;&#25417;&#33258;&#28982;&#23545;&#35805;&#20132;&#27969;&#21160;&#24577;&#30340;&#36718;&#27425;&#20998;&#21106;&#31639;&#27861;&#65292;&#36890;&#36807;&#21306;&#20998;&#35828;&#35805;&#32773;&#30340;&#20027;&#35201;&#23545;&#35805;&#36718;&#27425;&#21644;&#21548;&#20247;&#30340;&#27425;&#35201;&#35805;&#35821;&#65292;&#33021;&#22815;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#22320;&#25552;&#21462;&#36716;&#24405;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.15615</link><description>&lt;p&gt;
NaturalTurn&#65306;&#19968;&#31181;&#23558;&#36716;&#24405;&#20214;&#20998;&#21106;&#25104;&#33258;&#28982;&#23545;&#35805;&#36716;&#25240;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NaturalTurn: A Method to Segment Transcripts into Naturalistic Conversational Turns
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15615
&lt;/p&gt;
&lt;p&gt;
NaturalTurn&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#20934;&#30830;&#25429;&#25417;&#33258;&#28982;&#23545;&#35805;&#20132;&#27969;&#21160;&#24577;&#30340;&#36718;&#27425;&#20998;&#21106;&#31639;&#27861;&#65292;&#36890;&#36807;&#21306;&#20998;&#35828;&#35805;&#32773;&#30340;&#20027;&#35201;&#23545;&#35805;&#36718;&#27425;&#21644;&#21548;&#20247;&#30340;&#27425;&#35201;&#35805;&#35821;&#65292;&#33021;&#22815;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#22320;&#25552;&#21462;&#36716;&#24405;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15615v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#23545;&#35805;&#26159;&#31038;&#20250;&#12289;&#35748;&#30693;&#21644;&#35745;&#31639;&#31185;&#23398;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#30340;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#23545;&#35805;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#30740;&#31350;&#20154;&#21592;&#32570;&#20047;&#21487;&#20280;&#32553;&#30340;&#26041;&#27861;&#23558;&#35821;&#38899;&#36716;&#24405;&#36716;&#25442;&#20026;&#20250;&#35805;&#36718;&#27425;&#8212;&#8212;&#31038;&#20250;&#20114;&#21160;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#8220;NaturalTurn&#8221;&#65292;&#19968;&#31181;&#26088;&#22312;&#20934;&#30830;&#25429;&#25417;&#33258;&#28982;&#20132;&#27969;&#21160;&#24577;&#30340;&#36718;&#27425;&#20998;&#21106;&#31639;&#27861;&#12290;NaturalTurn&#36890;&#36807;&#21306;&#20998;&#35828;&#35805;&#32773;&#30340;&#20027;&#35201;&#23545;&#35805;&#36718;&#27425;&#21644;&#21548;&#20247;&#30340;&#27425;&#35201;&#35805;&#35821;&#65292;&#22914;&#32972;&#26223;&#22768;&#12289;&#31616;&#30701;&#25554;&#35805;&#21644;&#20854;&#20182;&#34920;&#29616;&#23545;&#35805;&#29305;&#24449;&#30340;&#24179;&#34892;&#35328;&#35821;&#24418;&#24335;&#65292;&#26469;&#36816;&#20316;&#12290;&#20351;&#29992;&#22823;&#22411;&#23545;&#35805;&#35821;&#26009;&#24211;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#27966;&#29983;&#30340;&#36716;&#24405;&#30456;&#27604;&#65292;NaturalTurn&#27966;&#29983;&#30340;&#36716;&#24405;&#34920;&#29616;&#20986;&#26377;&#21033;&#30340;&#32479;&#35745;&#21644;&#25512;&#26029;&#29305;&#24615;&#12290;NaturalTurn&#31639;&#27861;&#20195;&#34920;&#20102;&#19968;&#31181;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15615v1 Announce Type: new  Abstract: Conversation is the subject of increasing interest in the social, cognitive, and computational sciences. And yet, as conversational datasets continue to increase in size and complexity, researchers lack scalable methods to segment speech-to-text transcripts into conversational turns--the basic building blocks of social interaction. We introduce "NaturalTurn," a turn segmentation algorithm designed to accurately capture the dynamics of naturalistic exchange. NaturalTurn operates by distinguishing speakers' primary conversational turns from listeners' secondary utterances, such as backchannels, brief interjections, and other forms of parallel speech that characterize conversation. Using data from a large conversation corpus, we show how NaturalTurn-derived transcripts demonstrate favorable statistical and inferential characteristics compared to transcripts derived from existing methods. The NaturalTurn algorithm represents an improvement i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21363;&#20026;&#30740;&#31350;&#35770;&#25991;&#29983;&#25104;&#24314;&#35758;&#24615;&#23616;&#38480;&#65292;&#36890;&#36807;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#31181;&#26041;&#27861;&#26469;&#25581;&#31034;&#30456;&#20851;&#25361;&#25112;&#12289;&#23454;&#36341;&#35265;&#35299;&#21644;&#28508;&#22312;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2403.15529</link><description>&lt;p&gt;
LimGen: &#25506;&#31350;&#29992;&#20110;&#29983;&#25104;&#30740;&#31350;&#35770;&#25991;&#24314;&#35758;&#24615;&#23616;&#38480;&#30340;LLMs
&lt;/p&gt;
&lt;p&gt;
LimGen: Probing the LLMs for Generating Suggestive Limitations of Research Papers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21363;&#20026;&#30740;&#31350;&#35770;&#25991;&#29983;&#25104;&#24314;&#35758;&#24615;&#23616;&#38480;&#65292;&#36890;&#36807;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#31181;&#26041;&#27861;&#26469;&#25581;&#31034;&#30456;&#20851;&#25361;&#25112;&#12289;&#23454;&#36341;&#35265;&#35299;&#21644;&#28508;&#22312;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#26597;&#23616;&#38480;&#26159;&#23398;&#26415;&#30740;&#31350;&#35780;&#23457;&#36807;&#31243;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#25581;&#31034;&#20102;&#30740;&#31350;&#21487;&#33021;&#32570;&#20047;&#20915;&#23450;&#24615;&#25110;&#38656;&#35201;&#21152;&#24378;&#30340;&#26041;&#38754;&#12290;&#36825;&#26377;&#21161;&#20110;&#35835;&#32773;&#32771;&#34385;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#26356;&#24191;&#27867;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#30740;&#31350;&#35770;&#25991;&#24314;&#35758;&#24615;&#23616;&#38480;&#29983;&#25104;&#65288;SLG&#65289;&#30340;&#19968;&#39033;&#26032;&#39062;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#21517;&#20026;LimGen&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;ACL&#25991;&#38598;&#30340;4068&#31687;&#30740;&#31350;&#35770;&#25991;&#21450;&#20854;&#30456;&#20851;&#23616;&#38480;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#22810;&#31181;&#26041;&#27861;&#26469;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#24314;&#35758;&#24615;&#23616;&#38480;&#65292;&#36890;&#36807;&#24443;&#24213;&#30740;&#31350;&#30456;&#20851;&#25361;&#25112;&#12289;&#23454;&#36341;&#35265;&#35299;&#21644;&#28508;&#22312;&#26426;&#20250;&#12290;&#25105;&#20204;&#30340;LimGen&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/armbf/LimGen &#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15529v1 Announce Type: cross  Abstract: Examining limitations is a crucial step in the scholarly research reviewing process, revealing aspects where a study might lack decisiveness or require enhancement. This aids readers in considering broader implications for further research. In this article, we present a novel and challenging task of Suggestive Limitation Generation (SLG) for research papers. We compile a dataset called LimGen, encompassing 4068 research papers and their associated limitations from the ACL anthology. We investigate several approaches to harness large language models (LLMs) for producing suggestive limitations, by thoroughly examining the related challenges, practical insights, and potential opportunities. Our LimGen dataset and code can be accessed at https://github.com/armbf/LimGen.
&lt;/p&gt;</description></item><item><title>CTSM&#27169;&#22411;&#32467;&#21512;&#29305;&#36136;&#21644;&#29366;&#24577;&#24773;&#32490;&#65292;&#36890;&#36807;&#26500;&#24314;&#21644;&#32534;&#30721;&#24773;&#32490;&#23884;&#20837;&#20197;&#21450;&#24341;&#20837;&#24773;&#32490;&#24341;&#23548;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#20808;&#21069;&#22788;&#29702;&#24773;&#32490;&#24863;&#30693;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.15516</link><description>&lt;p&gt;
CTSM&#65306;&#23558;&#29305;&#36136;&#21644;&#29366;&#24577;&#24773;&#32490;&#30456;&#32467;&#21512;&#30340;&#20849;&#24773;&#21709;&#24212;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CTSM: Combining Trait and State Emotions for Empathetic Response Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15516
&lt;/p&gt;
&lt;p&gt;
CTSM&#27169;&#22411;&#32467;&#21512;&#29305;&#36136;&#21644;&#29366;&#24577;&#24773;&#32490;&#65292;&#36890;&#36807;&#26500;&#24314;&#21644;&#32534;&#30721;&#24773;&#32490;&#23884;&#20837;&#20197;&#21450;&#24341;&#20837;&#24773;&#32490;&#24341;&#23548;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#20808;&#21069;&#22788;&#29702;&#24773;&#32490;&#24863;&#30693;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#24773;&#24615;&#21709;&#24212;&#29983;&#25104;&#26088;&#22312;&#36171;&#20104;&#23545;&#35805;&#31995;&#32479;&#24863;&#30693;&#35828;&#35805;&#32773;&#24773;&#32490;&#24182;&#30456;&#24212;&#29983;&#25104;&#20849;&#24773;&#24615;&#22238;&#24212;&#30340;&#33021;&#21147;&#12290;&#24515;&#29702;&#30740;&#31350;&#34920;&#26126;&#65292;&#20316;&#20026;&#20849;&#24773;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#65292;&#24773;&#32490;&#21253;&#25324;&#29305;&#36136;&#24773;&#32490;&#65288;&#38745;&#24577;&#19988;&#19982;&#29615;&#22659;&#26080;&#20851;&#65289;&#21644;&#29366;&#24577;&#24773;&#32490;&#65288;&#21160;&#24577;&#19988;&#19982;&#29615;&#22659;&#30456;&#20851;&#65289;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#23558;&#23427;&#20204;&#21333;&#29420;&#22788;&#29702;&#65292;&#23548;&#33268;&#23545;&#24773;&#22659;&#24773;&#32490;&#30340;&#24863;&#30693;&#19981;&#36275;&#65292;&#36827;&#32780;&#24433;&#21709;&#20102;&#26377;&#25928;&#30340;&#20849;&#24773;&#34920;&#36798;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#29305;&#36136;&#21644;&#29366;&#24577;&#24773;&#32490;&#30456;&#32467;&#21512;&#30340;&#20849;&#24773;&#21709;&#24212;&#27169;&#22411;&#65288;CTSM&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#20805;&#20998;&#24863;&#30693;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#21644;&#32534;&#30721;&#29305;&#36136;&#21644;&#29366;&#24577;&#24773;&#32490;&#23884;&#20837;&#65292;&#28982;&#21518;&#36890;&#36807;&#19968;&#20010;&#24773;&#32490;&#24341;&#23548;&#27169;&#22359;&#36827;&#19968;&#27493;&#22686;&#24378;&#24773;&#32490;&#24863;&#30693;&#33021;&#21147;&#65292;&#35813;&#27169;&#22359;&#25351;&#23548;&#24773;&#32490;&#34920;&#36798;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#21449;&#23545;&#27604;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15516v1 Announce Type: cross  Abstract: Empathetic response generation endeavors to empower dialogue systems to perceive speakers' emotions and generate empathetic responses accordingly. Psychological research demonstrates that emotion, as an essential factor in empathy, encompasses trait emotions, which are static and context-independent, and state emotions, which are dynamic and context-dependent. However, previous studies treat them in isolation, leading to insufficient emotional perception of the context, and subsequently, less effective empathetic expression. To address this problem, we propose Combining Trait and State emotions for Empathetic Response Model (CTSM). Specifically, to sufficiently perceive emotions in dialogue, we first construct and encode trait and state emotion embeddings, and then we further enhance emotional perception capability through an emotion guidance module that guides emotion representation. In addition, we propose a cross-contrastive learnin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#36793;&#30028;&#24863;&#30693;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#36890;&#36807;&#31227;&#21160;&#28508;&#22312;&#29305;&#24449;&#12289;&#37325;&#26500;&#29983;&#25104;&#27169;&#31946;&#29256;&#26412;&#20197;&#21450;&#37319;&#29992;&#20013;K&#37319;&#26679;&#26469;&#22686;&#24378;&#29983;&#25104;&#21477;&#23376;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#27604;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#25552;&#39640;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15512</link><description>&lt;p&gt;
&#36890;&#36807;&#20915;&#31574;&#36793;&#30028;&#24863;&#30693;&#25968;&#25454;&#22686;&#24378;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#25552;&#39640;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Effectiveness and Robustness in a Low-Resource Regime via Decision-Boundary-aware Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#36793;&#30028;&#24863;&#30693;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#36890;&#36807;&#31227;&#21160;&#28508;&#22312;&#29305;&#24449;&#12289;&#37325;&#26500;&#29983;&#25104;&#27169;&#31946;&#29256;&#26412;&#20197;&#21450;&#37319;&#29992;&#20013;K&#37319;&#26679;&#26469;&#22686;&#24378;&#29983;&#25104;&#21477;&#23376;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#27604;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#25552;&#39640;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#20294;&#30452;&#25509;&#26041;&#27861;&#65288;&#22914;mixup&#21644;cutout&#65289;&#22312;&#25991;&#26412;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#21463;&#38480;&#20110;&#20854;&#31163;&#25955;&#29305;&#24615;&#12290;&#26412;&#25991;&#21463;&#21040;&#20915;&#31574;&#36793;&#30028;&#30340;&#26368;&#26032;&#30740;&#31350;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#36793;&#30028;&#24863;&#30693;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#22686;&#24378;&#31283;&#20581;&#24615;&#12290;&#35813;&#25216;&#26415;&#39318;&#20808;&#19987;&#27880;&#20110;&#23558;&#28508;&#22312;&#29305;&#24449;&#31227;&#36817;&#20915;&#31574;&#36793;&#30028;&#65292;&#28982;&#21518;&#36827;&#34892;&#37325;&#26500;&#20197;&#29983;&#25104;&#19968;&#20010;&#24102;&#26377;&#36719;&#26631;&#31614;&#30340;&#27169;&#31946;&#29256;&#26412;&#12290;&#27492;&#22806;&#65292;&#24314;&#35758;&#20351;&#29992;&#20013;K&#37319;&#26679;&#26469;&#22686;&#24378;&#29983;&#25104;&#21477;&#23376;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15512v1 Announce Type: cross  Abstract: Efforts to leverage deep learning models in low-resource regimes have led to numerous augmentation studies. However, the direct application of methods such as mixup and cutout to text data, is limited due to their discrete characteristics. While methods using pretrained language models have exhibited efficiency, they require additional considerations for robustness. Inspired by recent studies on decision boundaries, this paper proposes a decision-boundary-aware data augmentation strategy to enhance robustness using pretrained language models. The proposed technique first focuses on shifting the latent features closer to the decision boundary, followed by reconstruction to generate an ambiguous version with a soft label. Additionally, mid-K sampling is suggested to enhance the diversity of the generated sentences. This paper demonstrates the performance of the proposed augmentation strategy compared to other methods through extensive ex
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;LLMs&#22312;&#25216;&#26415;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#30740;&#31350;&#32654;&#22269;&#32852;&#37030;&#30005;&#20449;&#27861;&#35268;&#31532;47&#31456;&#65292;&#23581;&#35797;&#31616;&#21270;&#20449;&#24687;&#25910;&#38598;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.15503</link><description>&lt;p&gt;
&#22312;&#25216;&#26415;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#35780;&#20272;LLMs&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Performance of LLMs on Technical Language Processing tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15503
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;LLMs&#22312;&#25216;&#26415;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#30740;&#31350;&#32654;&#22269;&#32852;&#37030;&#30005;&#20449;&#27861;&#35268;&#31532;47&#31456;&#65292;&#23581;&#35797;&#31616;&#21270;&#20449;&#24687;&#25910;&#38598;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#39033;&#20851;&#20110;LLMs&#22312;&#25216;&#26415;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24615;&#33021;&#35780;&#20272;&#30740;&#31350;&#30340;&#32467;&#26524;&#12290;&#20154;&#31867;&#32463;&#24120;&#38754;&#20020;&#38656;&#35201;&#20174;&#19981;&#21516;&#26469;&#28304;&#25910;&#38598;&#20449;&#24687;&#24182;&#38656;&#35201;&#29702;&#35299;&#22823;&#37327;&#25991;&#26412;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#20219;&#21153;&#23545;&#20154;&#31867;&#26469;&#35828;&#21487;&#33021;&#38750;&#24120;&#22797;&#26434;&#65292;&#36890;&#24120;&#38656;&#35201;&#28145;&#20837;&#30740;&#31350;&#65292;&#21253;&#25324;&#22810;&#27425;&#38405;&#35835;&#25991;&#26412;&#30340;&#37096;&#20998;&#20869;&#23481;&#12290;&#20026;&#20102;&#31616;&#21270;&#20449;&#24687;&#25910;&#38598;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLMs&#22312;&#32842;&#22825;&#30028;&#38754;&#19978;&#30340;&#34920;&#29616;&#65292;&#35780;&#20272;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#22238;&#31572;&#26631;&#20934;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#22522;&#20110;&#20154;&#31867;&#23545;&#25991;&#26412;&#20869;&#23481;&#30340;&#38405;&#35835;&#26469;&#39044;&#26399;&#20154;&#31867;&#33021;&#22815;&#22238;&#31572;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#25991;&#26412;&#20026;&#12298;&#32654;&#22269;&#32852;&#37030;&#27861;&#35268;&#12299;&#31532;47&#31456;&#12298;&#32654;&#22269;&#27861;&#20856;&#12299;&#65292;&#36825;&#19968;&#31456;&#25551;&#36848;&#20102;&#20316;&#20026;&#32852;&#37030;&#36890;&#20449;&#22996;&#21592;&#20250;&#65288;FCC&#65289;&#31649;&#29702;&#19979;&#21830;&#29992;&#30005;&#20449;&#30340;&#35268;&#23450;&#12290;&#36825;&#19968;&#27573;&#25991;&#26412;&#26159;&#25105;&#20204;&#24863;&#20852;&#36259;&#30340;&#30740;&#31350;&#23545;&#35937;&#65292;&#22240;&#20026;&#25105;&#20204;&#26356;&#24191;&#27867;&#30340;&#30740;&#31350;&#28041;&#21450;&#26426;&#22120;&#22788;&#29702;&#36825;&#31867;&#25991;&#26412;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15503v1 Announce Type: new  Abstract: In this paper we present the results of an evaluation study of the perfor-mance of LLMs on Technical Language Processing tasks. Humans are often confronted with tasks in which they have to gather information from dispar-ate sources and require making sense of large bodies of text. These tasks can be significantly complex for humans and often require deep study including rereading portions of a text. Towards simplifying the task of gathering in-formation we evaluated LLMs with chat interfaces for their ability to provide answers to standard questions that a human can be expected to answer based on their reading of a body of text. The body of text under study is Title 47 of the United States Code of Federal Regulations (CFR) which describes regula-tions for commercial telecommunications as governed by the Federal Com-munications Commission (FCC). This has been a body of text of interest be-cause our larger research concerns the issue of ma
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#25913;&#36827;&#20102;&#25991;&#26412;&#36755;&#20837;&#31995;&#32479;&#20013;&#30340;&#22312;&#32447;&#33258;&#21160;&#23436;&#25104;&#24314;&#35758;&#65292;&#23558;&#35748;&#30693;&#36127;&#33655;&#32435;&#20837;&#27169;&#22411;&#35757;&#32451;&#30446;&#26631;&#65292;&#22522;&#20110;&#25991;&#26412;&#36755;&#20837;&#36895;&#24230;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.15502</link><description>&lt;p&gt;
&#22312;&#32447;&#25991;&#23383;&#33258;&#21160;&#23436;&#25104;&#30340;&#39034;&#24207;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Sequential Decision-Making for Inline Text Autocomplete
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15502
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#25913;&#36827;&#20102;&#25991;&#26412;&#36755;&#20837;&#31995;&#32479;&#20013;&#30340;&#22312;&#32447;&#33258;&#21160;&#23436;&#25104;&#24314;&#35758;&#65292;&#23558;&#35748;&#30693;&#36127;&#33655;&#32435;&#20837;&#27169;&#22411;&#35757;&#32451;&#30446;&#26631;&#65292;&#22522;&#20110;&#25991;&#26412;&#36755;&#20837;&#36895;&#24230;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#23436;&#25104;&#24314;&#35758;&#26159;&#29616;&#20195;&#25991;&#26412;&#36755;&#20837;&#31995;&#32479;&#20013;&#30340;&#22522;&#26412;&#21151;&#33021;&#65292;&#24212;&#29992;&#20110;&#35832;&#22914;&#28040;&#24687;&#20256;&#36882;&#21644;&#30005;&#23376;&#37038;&#20214;&#25776;&#20889;&#31561;&#39046;&#22495;&#12290;&#36890;&#24120;&#65292;&#33258;&#21160;&#23436;&#25104;&#24314;&#35758;&#26159;&#20174;&#20855;&#26377;&#32622;&#20449;&#24230;&#38408;&#20540;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#38408;&#20540;&#24182;&#27809;&#26377;&#30452;&#25509;&#32771;&#34385;&#29992;&#25143;&#22240;&#26174;&#31034;&#24314;&#35758;&#32780;&#26045;&#21152;&#30340;&#35748;&#30693;&#36127;&#33655;&#65292;&#20363;&#22914;&#20174;&#36755;&#20837;&#20999;&#25442;&#21040;&#38405;&#35835;&#24314;&#35758;&#30340;&#19978;&#19979;&#25991;&#20197;&#21450;&#20915;&#23450;&#26159;&#21542;&#25509;&#21463;&#24314;&#35758;&#30340;&#26102;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#26469;&#25913;&#36827;&#25991;&#26412;&#36755;&#20837;&#31995;&#32479;&#20013;&#30340;&#22312;&#32447;&#33258;&#21160;&#23436;&#25104;&#24314;&#35758;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#38543;&#26102;&#38388;&#19982;&#30446;&#26631;&#29992;&#25143;&#30340;&#37325;&#22797;&#20132;&#20114;&#26469;&#23398;&#20064;&#24314;&#35758;&#31574;&#30053;&#12290;&#36825;&#31181;&#21046;&#23450;&#20801;&#35768;&#25105;&#20204;&#23558;&#35748;&#30693;&#36127;&#33655;&#22240;&#32032;&#32435;&#20837;&#35757;&#32451;&#33258;&#21160;&#23436;&#25104;&#27169;&#22411;&#30340;&#30446;&#26631;&#20013;&#65292;&#36890;&#36807;&#22522;&#20110;&#25991;&#26412;&#36755;&#20837;&#36895;&#24230;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#29702;&#35770;&#26041;&#38754;&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15502v1 Announce Type: new  Abstract: Autocomplete suggestions are fundamental to modern text entry systems, with applications in domains such as messaging and email composition. Typically, autocomplete suggestions are generated from a language model with a confidence threshold. However, this threshold does not directly take into account the cognitive load imposed on the user by surfacing suggestions, such as the effort to switch contexts from typing to reading the suggestion, and the time to decide whether to accept the suggestion. In this paper, we study the problem of improving inline autocomplete suggestions in text entry systems via a sequential decision-making formulation, and use reinforcement learning to learn suggestion policies through repeated interactions with a target user over time. This formulation allows us to factor cognitive load into the objective of training an autocomplete model, through a reward function based on text entry speed. We acquired theoretica
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;ChatGPT&#22312;&#38463;&#25289;&#20271;&#35821;&#21307;&#30103;&#35786;&#26029;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#30456;&#20284;&#24230;&#35780;&#20998;&#20026;76%&#65292;&#38142;&#24335;&#25552;&#31034;&#25216;&#26415;&#30456;&#23545;&#26377;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.15501</link><description>&lt;p&gt;
&#36890;&#36807;&#20010;&#24615;&#21270;ChatGPT&#36741;&#21161;&#22686;&#24378;&#38463;&#25289;&#20271;&#35821;&#21307;&#30103;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Enhancing Medical Support in the Arabic Language Through Personalized ChatGPT Assistance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15501
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;ChatGPT&#22312;&#38463;&#25289;&#20271;&#35821;&#21307;&#30103;&#35786;&#26029;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#30456;&#20284;&#24230;&#35780;&#20998;&#20026;76%&#65292;&#38142;&#24335;&#25552;&#31034;&#25216;&#26415;&#30456;&#23545;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#32447;&#21307;&#30103;&#35786;&#26029;&#19982;&#20256;&#32479;&#30475;&#21307;&#29983;&#26041;&#24335;&#20043;&#38388;&#26085;&#30410;&#22686;&#38271;&#30340;&#21463;&#27426;&#36814;&#31243;&#24230;&#12290;&#23427;&#24378;&#35843;&#20102;&#29616;&#26377;&#24037;&#20855;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#20351;&#29992;ChatGPT&#30340;&#20248;&#21183;&#65292;ChatGPT&#21487;&#20197;&#25552;&#20379;&#23454;&#26102;&#30340;&#20010;&#24615;&#21270;&#21307;&#30103;&#35786;&#26029;&#65292;&#32780;&#19988;&#26159;&#20813;&#36153;&#30340;&#12290;&#35813;&#27573;&#33853;&#24635;&#32467;&#20102;&#19968;&#39033;&#35780;&#20272;ChatGPT&#22312;&#38463;&#25289;&#20271;&#35821;&#21307;&#30103;&#35786;&#26029;&#20013;&#24615;&#33021;&#30340;&#30740;&#31350;&#12290;&#35813;&#30740;&#31350;&#28041;&#21450;&#32534;&#21046;&#30142;&#30149;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#25552;&#31034;&#25216;&#26415;&#20026;&#27599;&#31181;&#30142;&#30149;&#29983;&#25104;&#22810;&#26465;&#28040;&#24687;&#12290;&#36890;&#36807;&#27979;&#37327;&#20854;&#21709;&#24212;&#19982;&#23454;&#38469;&#30142;&#30149;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#26469;&#35780;&#20272;ChatGPT&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#20986;&#20196;&#20154;&#40723;&#33310;&#30340;&#34920;&#29616;&#65292;&#30456;&#20284;&#24230;&#35780;&#20272;&#30340;&#24179;&#22343;&#20998;&#25968;&#32422;&#20026;76%&#12290;&#20351;&#29992;&#20102;&#21508;&#31181;&#25552;&#31034;&#25216;&#26415;&#65292;&#38142;&#24335;&#25552;&#31034;&#23637;&#29616;&#20986;&#30456;&#23545;&#20248;&#21183;&#12290;&#35813;&#30740;&#31350;&#36824;&#35760;&#24405;&#20102;ChatGPT API&#30340;&#24179;&#22343;&#21709;&#24212;&#26102;&#38388;&#20026;6.12&#31186;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15501v1 Announce Type: new  Abstract: This Paper discusses the growing popularity of online medical diagnosis as an alternative to traditional doctor visits. It highlights the limitations of existing tools and emphasizes the advantages of using ChatGPT, which provides real-time, personalized medical diagnosis at no cost. The paragraph summarizes a research study that evaluated the performance of ChatGPT in Arabic medical diagnosis. The study involved compiling a dataset of disease information and generating multiple messages for each disease using different prompting techniques. ChatGPT's performance was assessed by measuring the similarity between its responses and the actual diseases. The results showed promising performance, with average scores of around 76% for similarity measures. Various prompting techniques were used, and chain prompting demonstrated a relative advantage. The study also recorded an average response time of 6.12 seconds for the ChatGPT API, which is co
&lt;/p&gt;</description></item><item><title>&#26827;&#31867;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#19979;&#19968;&#20010;&#23383;&#31526;&#39044;&#27979;&#35757;&#32451;&#65292;&#20173;&#33021;&#23398;&#20064;&#20986;&#20869;&#37096;&#34920;&#31034;&#30340;&#26827;&#30424;&#29366;&#24577;</title><link>https://arxiv.org/abs/2403.15498</link><description>&lt;p&gt;
&#26827;&#31867;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26032;&#39062;&#19990;&#30028;&#27169;&#22411;&#21644;&#28508;&#21464;&#37327;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15498
&lt;/p&gt;
&lt;p&gt;
&#26827;&#31867;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#19979;&#19968;&#20010;&#23383;&#31526;&#39044;&#27979;&#35757;&#32451;&#65292;&#20173;&#33021;&#23398;&#20064;&#20986;&#20869;&#37096;&#34920;&#31034;&#30340;&#26827;&#30424;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#20854;&#24615;&#33021;&#26469;&#28304;&#30340;&#35752;&#35770;&#12290;&#26159;&#20165;&#20165;&#23398;&#20064;&#21477;&#27861;&#27169;&#24335;&#21644;&#34920;&#38754;&#32479;&#35745;&#32467;&#26524;&#65292;&#36824;&#26159;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#35821;&#20041;&#21644;&#19990;&#30028;&#27169;&#22411;&#65311;&#25105;&#20204;&#22312;&#35937;&#26827;&#36825;&#20010;&#26356;&#22797;&#26434;&#30340;&#39046;&#22495;&#25193;&#23637;&#20102;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#36890;&#36807;&#22312;&#30495;&#23454;&#28216;&#25103;&#20013;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#29992;&#32447;&#24615;&#25506;&#27979;&#21644;&#23545;&#27604;&#28608;&#27963;&#26469;&#30740;&#31350;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#12290;&#23613;&#31649;&#27169;&#22411;&#27809;&#26377;&#20808;&#39564;&#30340;&#28216;&#25103;&#30693;&#35782;&#65292;&#20165;&#20165;&#36890;&#36807;&#19979;&#19968;&#20010;&#23383;&#31526;&#39044;&#27979;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20851;&#20110;&#26827;&#30424;&#29366;&#24577;&#30340;&#20869;&#37096;&#34920;&#31034;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15498v1 Announce Type: cross  Abstract: Language models have shown unprecedented capabilities, sparking debate over the source of their performance. Is it merely the outcome of learning syntactic patterns and surface level statistics, or do they extract semantics and a world model from the text? Prior work by Li et al. investigated this by training a GPT model on synthetic, randomly generated Othello games and found that the model learned an internal representation of the board state. We extend this work into the more complex domain of chess, training on real games and investigating our model's internal representations using linear probes and contrastive activations. The model is given no a priori knowledge of the game and is solely trained on next character prediction, yet we find evidence of internal representations of board state. We validate these internal representations by using them to make interventions on the model's activations and edit its internal board state. Un
&lt;/p&gt;</description></item><item><title>&#21457;&#23637;&#20102;SemLa&#65292;&#19968;&#20010;&#38024;&#23545;&#32454;&#31890;&#24230;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#24320;&#21457;&#24037;&#20316;&#27969;&#38656;&#27714;&#30340;&#26032;&#22411;&#35270;&#35273;&#20998;&#26512;&#31995;&#32479;&#65292;&#29992;&#20110;&#20998;&#35299;&#25968;&#25454;&#38598;&#20013;&#30340;&#22797;&#26434;&#35821;&#20041;&#32467;&#26500;&#21644;&#21487;&#35270;&#21270;&#25991;&#26412;&#21547;&#20041;&#20013;&#30340;&#32454;&#24494;&#24046;&#21035;</title><link>https://arxiv.org/abs/2403.15492</link><description>&lt;p&gt;
&#38754;&#21521;&#32454;&#31890;&#24230;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#35270;&#35273;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Visual Analytics for Fine-grained Text Classification Models and Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15492
&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#20102;SemLa&#65292;&#19968;&#20010;&#38024;&#23545;&#32454;&#31890;&#24230;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#24320;&#21457;&#24037;&#20316;&#27969;&#38656;&#27714;&#30340;&#26032;&#22411;&#35270;&#35273;&#20998;&#26512;&#31995;&#32479;&#65292;&#29992;&#20110;&#20998;&#35299;&#25968;&#25454;&#38598;&#20013;&#30340;&#22797;&#26434;&#35821;&#20041;&#32467;&#26500;&#21644;&#21487;&#35270;&#21270;&#25991;&#26412;&#21547;&#20041;&#20013;&#30340;&#32454;&#24494;&#24046;&#21035;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#65292;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#21464;&#24471;&#36234;&#26469;&#36234;&#32454;&#31890;&#24230;&#21270;&#65292;&#22240;&#20026;&#25968;&#25454;&#38598;&#34987;&#20998;&#25104;&#26356;&#22810;&#26356;&#38590;&#20197;&#21306;&#20998;&#30340;&#31867;&#21035;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#38598;&#30340;&#35821;&#20041;&#32467;&#26500;&#21464;&#24471;&#26356;&#22797;&#26434;&#65292;&#27169;&#22411;&#20915;&#31574;&#20063;&#21464;&#24471;&#26356;&#21152;&#38590;&#20197;&#35299;&#37322;&#12290;&#22312;&#36825;&#31181;&#31354;&#32570;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#19982;NLP&#39046;&#22495;&#19987;&#23478;&#23494;&#20999;&#21512;&#20316;&#65292;&#36890;&#36807;&#35774;&#35745;&#21644;&#35780;&#20272;&#30340;&#36845;&#20195;&#36807;&#31243;&#65292;&#34920;&#24449;&#21644;&#35299;&#20915;&#20182;&#20204;&#22312;&#24320;&#21457;&#32454;&#31890;&#24230;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#24037;&#20316;&#27969;&#20013;&#19981;&#26029;&#22686;&#38271;&#30340;&#38656;&#27714;&#12290;&#36825;&#31181;&#21512;&#20316;&#30340;&#32467;&#26524;&#26159;&#24320;&#21457;&#20986;&#20102;SemLa&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#20197;&#19979;&#38656;&#27714;&#23450;&#21046;&#30340;&#26032;&#22411;&#35270;&#35273;&#20998;&#26512;&#31995;&#32479;&#65306;1&#65289;&#22312;&#23558;&#25968;&#25454;&#38598;&#31354;&#38388;&#21270;&#21040;&#27169;&#22411;&#23884;&#20837;&#31354;&#38388;&#26102;&#20998;&#35299;&#22797;&#26434;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;2&#65289;&#21487;&#35270;&#21270;&#25991;&#26412;&#24847;&#20041;&#20013;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15492v1 Announce Type: new  Abstract: In natural language processing (NLP), text classification tasks are increasingly fine-grained, as datasets are fragmented into a larger number of classes that are more difficult to differentiate from one another. As a consequence, the semantic structures of datasets have become more complex, and model decisions more difficult to explain. Existing tools, suited for coarse-grained classification, falter under these additional challenges. In response to this gap, we worked closely with NLP domain experts in an iterative design-and-evaluation process to characterize and tackle the growing requirements in their workflow of developing fine-grained text classification models. The result of this collaboration is the development of SemLa, a novel visual analytics system tailored for 1) dissecting complex semantic structures in a dataset when it is spatialized in model embedding space, and 2) visualizing fine-grained nuances in the meaning of text
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#24320;&#28304;&#23545;&#35805;&#24335;LLMs&#23545;&#35199;&#29677;&#29273;&#35821;&#21333;&#35789;&#30340;&#20102;&#35299;&#31243;&#24230;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#20204;&#26080;&#27861;&#27491;&#30830;&#20351;&#29992;&#22823;&#37096;&#20998;&#21333;&#35789;&#20889;&#21477;&#23376;&#12290;</title><link>https://arxiv.org/abs/2403.15491</link><description>&lt;p&gt;
&#24320;&#28304;&#23545;&#35805;&#24335;LLMs&#19981;&#20102;&#35299;&#22823;&#37096;&#20998;&#35199;&#29677;&#29273;&#35821;&#21333;&#35789;
&lt;/p&gt;
&lt;p&gt;
Open Source Conversational LLMs do not know most Spanish words
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#24320;&#28304;&#23545;&#35805;&#24335;LLMs&#23545;&#35199;&#29677;&#29273;&#35821;&#21333;&#35789;&#30340;&#20102;&#35299;&#31243;&#24230;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#20204;&#26080;&#27861;&#27491;&#30830;&#20351;&#29992;&#22823;&#37096;&#20998;&#21333;&#35789;&#20889;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;&#21487;&#20197;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#30340;&#23545;&#35805;&#27169;&#22411;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#65292;&#24320;&#21457;&#20102;&#22823;&#37327;&#24320;&#28304;&#32842;&#22825;LLMs&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#22312;&#22238;&#31572;&#38382;&#39064;&#25110;&#35299;&#20915;&#20960;&#20046;&#20219;&#20309;&#21487;&#33021;&#30340;&#20027;&#39064;&#19978;&#30340;&#33021;&#21147;&#65292;&#25110;&#32773;&#27979;&#35797;&#23427;&#20204;&#29702;&#35299;&#25110;&#35299;&#37322;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#20123;&#27169;&#22411;&#23545;&#35821;&#35328;&#30340;&#20102;&#35299;&#30340;&#35780;&#20272;&#21364;&#21463;&#21040;&#20102;&#36828;&#36828;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#20363;&#22914;&#65292;&#23427;&#20204;&#33021;&#22815;&#35782;&#21035;&#21644;&#20351;&#29992;&#19981;&#21516;&#35821;&#35328;&#20013;&#30340;&#21333;&#35789;&#25968;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#21442;&#32771;&#35789;&#20856;&#20013;&#27979;&#35797;&#37096;&#20998;&#21333;&#35789;&#30340;&#26679;&#26412;&#26469;&#35780;&#20272;&#24320;&#28304;&#32842;&#22825;LLMs&#23545;&#35199;&#29677;&#29273;&#35821;&#21333;&#35789;&#30340;&#20102;&#35299;&#31243;&#24230;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#24320;&#28304;&#32842;&#22825;LLMs&#23545;&#37325;&#35201;&#37096;&#20998;&#21333;&#35789;&#20135;&#29983;&#20102;&#38169;&#35823;&#30340;&#21547;&#20041;&#65292;&#24182;&#19988;&#26080;&#27861;&#27491;&#30830;&#20351;&#29992;&#22823;&#37096;&#20998;&#21333;&#35789;&#22312;&#19978;&#19979;&#25991;&#20013;&#20889;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15491v1 Announce Type: new  Abstract: The growing interest in Large Language Models (LLMs) and in particular in conversational models with which users can interact has led to the development of a large number of open-source chat LLMs. These models are evaluated on a wide range of benchmarks to assess their capabilities in answering questions or solving problems on almost any possible topic or to test their ability to reason or interpret texts. Instead, the evaluation of the knowledge that these models have of the languages has received much less attention. For example, the words that they can recognize and use in different languages. In this paper, we evaluate the knowledge that open-source chat LLMs have of Spanish words by testing a sample of words in a reference dictionary. The results show that open-source chat LLMs produce incorrect meanings for an important fraction of the words and are not able to use most of the words correctly to write sentences with context. These 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#39318;&#27425;&#22312;&#26790;&#22659;&#21465;&#20107;&#20013;&#36827;&#34892;&#35282;&#33394;&#21644;&#24773;&#24863;&#26816;&#27979;&#30740;&#31350;&#65292;&#23637;&#31034;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#24212;&#23545;&#35813;&#22797;&#26434;&#20219;&#21153;&#65292;&#30417;&#30563;&#27169;&#22411;&#34920;&#29616;&#26356;&#20339;&#19988;&#21442;&#25968;&#26356;&#23569;&#12290;</title><link>https://arxiv.org/abs/2403.15486</link><description>&lt;p&gt;
&#24207;&#21015;&#21040;&#24207;&#21015;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26790;&#22659;&#21465;&#20107;&#20013;&#30340;&#35282;&#33394;&#21644;&#24773;&#24863;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Sequence-to-Sequence Language Models for Character and Emotion Detection in Dream Narratives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#39318;&#27425;&#22312;&#26790;&#22659;&#21465;&#20107;&#20013;&#36827;&#34892;&#35282;&#33394;&#21644;&#24773;&#24863;&#26816;&#27979;&#30740;&#31350;&#65292;&#23637;&#31034;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#24212;&#23545;&#35813;&#22797;&#26434;&#20219;&#21153;&#65292;&#30417;&#30563;&#27169;&#22411;&#34920;&#29616;&#26356;&#20339;&#19988;&#21442;&#25968;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26790;&#22659;&#30740;&#31350;&#23545;&#20110;&#29702;&#35299;&#20154;&#31867;&#30340;(&#38750;)&#24847;&#35782;&#12289;&#35748;&#30693;&#21644;&#25991;&#21270;&#25968;&#20010;&#19990;&#32426;&#26469;&#19968;&#30452;&#33267;&#20851;&#37325;&#35201;&#12290;&#23450;&#37327;&#20998;&#26512;&#26790;&#22659;&#20381;&#36182;&#20110;&#23545;&#26790;&#22659;&#21465;&#36848;&#30340;&#21171;&#21160;&#23494;&#38598;&#22411;&#25163;&#21160;&#27880;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#24207;&#21015;&#21040;&#24207;&#21015;&#29983;&#25104;&#26694;&#26550;&#33258;&#21160;&#21270;&#36825;&#19968;&#36807;&#31243;&#12290;&#26412;&#25991;&#39318;&#27425;&#22312;&#26790;&#22659;&#21465;&#20107;&#30340;&#24320;&#25918;DreamBank&#35821;&#26009;&#24211;&#33521;&#25991;&#37096;&#20998;&#20013;&#36827;&#34892;&#20102;&#35282;&#33394;&#21644;&#24773;&#24863;&#26816;&#27979;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#19968;&#22797;&#26434;&#20219;&#21153;&#12290;&#20026;&#20102;&#20102;&#35299;&#39044;&#27979;&#24615;&#33021;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#35282;&#33394;&#30340;&#39044;&#27979;&#39034;&#24207;&#20197;&#21450;&#23545;&#19987;&#26377;&#21517;&#31216;&#21644;&#35282;&#33394;&#29305;&#24449;&#30340;&#32771;&#34385;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30417;&#30563;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#21516;&#26102;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569;&#20102;28&#20493;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21450;&#20854;&#29983;&#25104;&#30340;&#27880;&#37322;&#24050;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15486v1 Announce Type: cross  Abstract: The study of dreams has been central to understanding human (un)consciousness, cognition, and culture for centuries. Analyzing dreams quantitatively depends on labor-intensive, manual annotation of dream narratives. We automate this process through a natural language sequence-to-sequence generation framework. This paper presents the first study on character and emotion detection in the English portion of the open DreamBank corpus of dream narratives. Our results show that language models can effectively address this complex task. To get insight into prediction performance, we evaluate the impact of model size, prediction order of characters, and the consideration of proper names and character traits. We compare our approach with a large language model using in-context learning. Our supervised models perform better while having 28 times fewer parameters. Our model and its generated annotations are made publicly available.
&lt;/p&gt;</description></item><item><title>MOGAM&#27169;&#22411;&#26159;&#20026;&#20102;&#20811;&#26381;&#29616;&#26377;&#25233;&#37057;&#30151;&#26816;&#27979;&#26041;&#27861;&#22312;&#19981;&#21516;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#31867;&#22411;&#19978;&#30340;&#38480;&#21046;&#32780;&#25552;&#20986;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#32508;&#21512;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#26356;&#20855;&#21487;&#25193;&#23637;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.15485</link><description>&lt;p&gt;
MOGAM&#65306;&#19968;&#31181;&#29992;&#20110;&#25233;&#37057;&#30151;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#38754;&#21521;&#23545;&#35937;&#22270;&#27880;&#24847;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MOGAM: A Multimodal Object-oriented Graph Attention Model for Depression Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15485
&lt;/p&gt;
&lt;p&gt;
MOGAM&#27169;&#22411;&#26159;&#20026;&#20102;&#20811;&#26381;&#29616;&#26377;&#25233;&#37057;&#30151;&#26816;&#27979;&#26041;&#27861;&#22312;&#19981;&#21516;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#31867;&#22411;&#19978;&#30340;&#38480;&#21046;&#32780;&#25552;&#20986;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#32508;&#21512;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#26356;&#20855;&#21487;&#25193;&#23637;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#26816;&#27979;&#22312;&#25233;&#37057;&#30151;&#27835;&#30103;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#30740;&#31350;&#20851;&#27880;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#65292;&#20010;&#20307;&#22312;&#35813;&#24179;&#21488;&#34920;&#36798;&#24773;&#32490;&#65292;&#26088;&#22312;&#23454;&#29616;&#25233;&#37057;&#30151;&#30340;&#26089;&#26399;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#29305;&#23450;&#29305;&#24449;&#65292;&#23548;&#33268;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#38598;&#65288;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#25110;&#35270;&#39057;&#65289;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#26377;&#38480;&#12290;&#20026;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#38754;&#21521;&#23545;&#35937;&#22270;&#27880;&#24847;&#27169;&#22411;&#65288;MOGAM&#65289;&#65292;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#65292;&#25552;&#20379;&#26356;&#20855;&#21487;&#20280;&#32553;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#20026;&#30830;&#20445;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#25233;&#37057;&#30151;&#30340;&#30495;&#23454;&#30151;&#29366;&#65292;&#25105;&#20204;&#20165;&#25910;&#38598;&#20102;&#20855;&#26377;&#20020;&#24202;&#35786;&#26029;&#30340;&#29992;&#25143;&#30340;&#35270;&#39057;&#26085;&#24535;&#12290;&#20026;&#20102;&#21033;&#29992;&#35270;&#39057;&#26085;&#24535;&#30340;&#22810;&#26679;&#29305;&#24449;&#65292;&#25105;&#20204;&#37319;&#29992;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#24182;&#25910;&#38598;&#39069;&#22806;&#30340;&#20803;&#25968;&#25454;&#65292;&#22914;&#35270;&#39057;&#26085;&#24535;&#30340;&#26631;&#39064;&#12289;&#25551;&#36848;&#21644;&#25345;&#32493;&#26102;&#38388;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15485v1 Announce Type: cross  Abstract: Early detection plays a crucial role in the treatment of depression. Therefore, numerous studies have focused on social media platforms, where individuals express their emotions, aiming to achieve early detection of depression. However, the majority of existing approaches often rely on specific features, leading to limited scalability across different types of social media datasets, such as text, images, or videos. To overcome this limitation, we introduce a Multimodal Object-Oriented Graph Attention Model (MOGAM), which can be applied to diverse types of data, offering a more scalable and versatile solution. Furthermore, to ensure that our model can capture authentic symptoms of depression, we only include vlogs from users with a clinical diagnosis. To leverage the diverse features of vlogs, we adopt a multimodal approach and collect additional metadata such as the title, description, and duration of the vlogs. To effectively aggregat
&lt;/p&gt;</description></item><item><title>RakutenAI-7B&#26159;&#19968;&#22871;&#26085;&#26412;&#23548;&#21521;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#26085;&#26412;LM Harness&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#26368;&#22909;&#65292;&#20998;&#21035;&#21457;&#24067;&#20102;&#25351;&#23548;&#21644;&#32842;&#22825;&#35843;&#25972;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.15484</link><description>&lt;p&gt;
RakutenAI-7B&#65306;&#20026;&#26085;&#26412;&#35821;&#35328;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RakutenAI-7B: Extending Large Language Models for Japanese
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15484
&lt;/p&gt;
&lt;p&gt;
RakutenAI-7B&#26159;&#19968;&#22871;&#26085;&#26412;&#23548;&#21521;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#26085;&#26412;LM Harness&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#26368;&#22909;&#65292;&#20998;&#21035;&#21457;&#24067;&#20102;&#25351;&#23548;&#21644;&#32842;&#22825;&#35843;&#25972;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;RakutenAI-7B&#65292;&#36825;&#26159;&#19968;&#22871;&#20197;&#26085;&#26412;&#20026;&#23548;&#21521;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#26085;&#26412;LM Harness&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#20248;&#20110;&#24320;&#25918;&#30340;7B&#27169;&#22411;&#12290;&#38500;&#20102;&#22522;&#30784;&#27169;&#22411;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#26681;&#25454;&#25351;&#23548;&#21644;&#32842;&#22825;&#36827;&#34892;&#35843;&#25972;&#30340;&#27169;&#22411;&#65292;&#20998;&#21035;&#26159;RakutenAI-7B-instruct&#21644;RakutenAI-7B-chat&#65292;&#20351;&#29992;Apache 2.0&#35768;&#21487;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15484v1 Announce Type: new  Abstract: We introduce RakutenAI-7B, a suite of Japanese-oriented large language models that achieve the best performance on the Japanese LM Harness benchmarks among the open 7B models. Along with the foundation model, we release instruction- and chat-tuned models, RakutenAI-7B-instruct and RakutenAI-7B-chat respectively, under the Apache 2.0 license.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#21021;&#23398;&#32773;&#21516;&#34892;&#36741;&#23548;&#21592;&#25552;&#20379;&#22810;&#32423;&#35814;&#32454;&#21453;&#39304;&#65292;&#36171;&#33021;&#35268;&#27169;&#21270;&#30340;&#25903;&#25345;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#24739;&#32773;&#12290;</title><link>https://arxiv.org/abs/2403.15482</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#32423;&#21453;&#39304;&#65292;&#20026;&#21021;&#23398;&#32773;&#21516;&#34892;&#36741;&#23548;&#21592;&#36171;&#33021;
&lt;/p&gt;
&lt;p&gt;
Multi-Level Feedback Generation with Large Language Models for Empowering Novice Peer Counselors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15482
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#21021;&#23398;&#32773;&#21516;&#34892;&#36741;&#23548;&#21592;&#25552;&#20379;&#22810;&#32423;&#35814;&#32454;&#21453;&#39304;&#65292;&#36171;&#33021;&#35268;&#27169;&#21270;&#30340;&#25903;&#25345;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#24739;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#21644;&#20010;&#24615;&#21270;&#21453;&#39304;&#26159;&#22521;&#20859;&#20855;&#26377;&#20020;&#24202;&#25216;&#33021;&#30340;&#21516;&#34892;&#36741;&#23548;&#21592;&#30340;&#20851;&#38190;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21453;&#39304;&#26426;&#21046;&#20027;&#35201;&#20381;&#36182;&#20110;&#20154;&#24037;&#30417;&#30563;&#12290;&#21516;&#34892;&#36741;&#23548;&#21592;&#32463;&#24120;&#32570;&#20047;&#26426;&#21046;&#26469;&#20174;&#32463;&#39564;&#20016;&#23500;&#30340;&#23548;&#24072;&#37027;&#37324;&#33719;&#24471;&#35814;&#32454;&#30340;&#21453;&#39304;&#65292;&#36825;&#20351;&#24471;&#20182;&#20204;&#38590;&#20197;&#25903;&#25345;&#20351;&#29992;&#21516;&#34892;&#36741;&#23548;&#30340;&#22823;&#37327;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#24739;&#32773;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#24773;&#22659;&#21270;&#21644;&#22810;&#32423;&#21453;&#39304;&#65292;&#20197;&#36171;&#33021;&#35268;&#27169;&#21270;&#30340;&#21021;&#23398;&#32773;&#21516;&#34892;&#36741;&#23548;&#21592;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#19982;&#19968;&#32452;&#39640;&#32423;&#24515;&#29702;&#27835;&#30103;&#30563;&#23548;&#20849;&#21516;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#32423;&#21453;&#39304;&#20998;&#31867;&#27861;&#65292;&#28982;&#21518;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#23436;&#25972;&#21453;&#39304;&#27880;&#37322;&#30340;400&#27425;&#24773;&#32490;&#25903;&#25345;&#23545;&#35805;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#25913;&#36827;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#21453;&#39304;&#30340;&#33258;&#21160;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15482v1 Announce Type: new  Abstract: Realistic practice and tailored feedback are key processes for training peer counselors with clinical skills. However, existing mechanisms of providing feedback largely rely on human supervision. Peer counselors often lack mechanisms to receive detailed feedback from experienced mentors, making it difficult for them to support the large number of people with mental health issues who use peer counseling. Our work aims to leverage large language models to provide contextualized and multi-level feedback to empower peer counselors, especially novices, at scale. To achieve this, we co-design with a group of senior psychotherapy supervisors to develop a multi-level feedback taxonomy, and then construct a publicly available dataset with comprehensive feedback annotations of 400 emotional support conversations. We further design a self-improvement method on top of large language models to enhance the automatic generation of feedback. Via qualita
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#38598;&#25104;&#20102;&#30417;&#30563;&#24335;&#25552;&#21462;&#21644;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#19968;&#31181;&#25552;&#20379;&#33258;&#26432;&#39118;&#38505;&#25903;&#25345;&#35777;&#25454;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;CLPsych 2024 shared task&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#32489;&#12290;</title><link>https://arxiv.org/abs/2403.15478</link><description>&lt;p&gt;
&#23558;&#30417;&#30563;&#24335;&#25552;&#21462;&#21644;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#29992;&#20110;&#33258;&#26432;&#39118;&#38505;&#35777;&#25454;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Integrating Supervised Extractive and Generative Language Models for Suicide Risk Evidence Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15478
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#38598;&#25104;&#20102;&#30417;&#30563;&#24335;&#25552;&#21462;&#21644;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#19968;&#31181;&#25552;&#20379;&#33258;&#26432;&#39118;&#38505;&#25903;&#25345;&#35777;&#25454;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;CLPsych 2024 shared task&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#30417;&#30563;&#24335;&#25552;&#21462;&#21644;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;CLPsych 2024&#20849;&#20139;&#20219;&#21153;&#20013;&#25552;&#20379;&#33258;&#26432;&#39118;&#38505;&#30340;&#25903;&#25345;&#35777;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#35745;&#21477;&#23376;&#32423;&#21035;&#30340;&#33258;&#26432;&#39118;&#38505;&#21644;&#36127;&#38754;&#24773;&#32490;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#24378;&#35843;&#33258;&#26432;&#39118;&#38505;&#21644;&#36127;&#38754;&#24773;&#32490;&#30340;&#27010;&#29575;&#25552;&#39640;&#65292;&#31934;&#30830;&#23450;&#20301;&#39640;&#33258;&#26432;&#39118;&#38505;&#21477;&#23376;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;MentaLLaMa&#26694;&#26550;&#25972;&#21512;&#29983;&#25104;&#24335;&#25688;&#35201;&#21644;&#20174;&#30830;&#23450;&#30340;&#39640;&#33258;&#26432;&#39118;&#38505;&#21477;&#23376;&#21644;&#19968;&#20010;&#19987;&#38376;&#30340;&#33258;&#26432;&#39118;&#38505;&#35789;&#20856;&#20013;&#25552;&#21462;&#25688;&#35201;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;SophiaADS&#22312;&#31361;&#20986;&#25688;&#35201;&#20013;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#65292;&#22312;&#25688;&#35201;&#29983;&#25104;&#20013;&#26681;&#25454;&#21484;&#22238;&#29575;&#21644;&#19968;&#33268;&#24615;&#25351;&#26631;&#20998;&#21035;&#25490;&#21517;&#31532;&#21313;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15478v1 Announce Type: new  Abstract: We propose a method that integrates supervised extractive and generative language models for providing supporting evidence of suicide risk in the CLPsych 2024 shared task. Our approach comprises three steps. Initially, we construct a BERT-based model for estimating sentence-level suicide risk and negative sentiment. Next, we precisely identify high suicide risk sentences by emphasizing elevated probabilities of both suicide risk and negative sentiment. Finally, we integrate generative summaries using the MentaLLaMa framework and extractive summaries from identified high suicide risk sentences and a specialized dictionary of suicidal risk words. SophiaADS, our team, achieved 1st place for highlight extraction and ranked 10th for summary generation, both based on recall and consistency metrics, respectively.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#36890;&#36807;&#27604;&#36739;&#30740;&#31350;&#20102;&#20960;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#35770;&#28857;&#25366;&#25496;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;BERT&#26550;&#26500;&#21644;ChatGPT-4&#24494;&#35843;&#27169;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#32467;&#26524;&#26174;&#31034;BERT+ChatGPT-4&#22312;&#35770;&#28857;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.15473</link><description>&lt;p&gt;
&#20351;&#29992;&#32039;&#20945;&#35821;&#35328;&#27169;&#22411;&#21644;ChatGPT-4&#25913;&#36827;&#30340;&#39640;&#25928;&#35770;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Efficient argument classification with compact language models and ChatGPT-4 refinements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15473
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#36890;&#36807;&#27604;&#36739;&#30740;&#31350;&#20102;&#20960;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#35770;&#28857;&#25366;&#25496;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;BERT&#26550;&#26500;&#21644;ChatGPT-4&#24494;&#35843;&#27169;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#32467;&#26524;&#26174;&#31034;BERT+ChatGPT-4&#22312;&#35770;&#28857;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#28857;&#25366;&#25496;&#65288;AM&#65289;&#34987;&#23450;&#20041;&#20026;&#33258;&#21160;&#35782;&#21035;&#21644;&#25552;&#21462;&#35770;&#35777;&#32452;&#25104;&#37096;&#20998;&#65288;&#22914;&#21069;&#25552;&#12289;&#20027;&#24352;&#31561;&#65289;&#24182;&#26816;&#27979;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65288;&#21363;&#25903;&#25345;&#12289;&#21453;&#39539;&#12289;&#26080;&#20851;&#31995;&#65289;&#30340;&#20219;&#21153;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20351;&#25105;&#20204;&#33021;&#22815;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#20998;&#26512;&#35770;&#28857;&#24182;&#25552;&#21462;&#23427;&#20204;&#30340;&#35821;&#20041;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#35770;&#28857;&#25366;&#25496;&#20013;&#20960;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;&#35813;&#24037;&#20316;&#38598;&#20013;&#22312;&#35770;&#28857;&#20998;&#31867;&#19978;&#12290;&#30740;&#31350;&#26159;&#22312;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#65288;Args.me, UKP, US2016&#65289;&#19978;&#36827;&#34892;&#30340;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#21019;&#26032;&#28857;&#26159;&#22522;&#20110;BERT&#26550;&#26500;&#30340;&#38598;&#25104;&#27169;&#22411;&#21644;&#20316;&#20026;&#24494;&#35843;&#27169;&#22411;&#30340;ChatGPT-4&#12290;&#25552;&#20986;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;BERT+ChatGPT-4&#22312;&#34920;&#29616;&#19978;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;Transformer&#21644;LSTM&#30340;&#27169;&#22411;&#12290;&#35266;&#23519;&#21040;&#30340;&#25913;&#36827;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#22823;&#20110;10&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15473v1 Announce Type: new  Abstract: Argument mining (AM) is defined as the task of automatically identifying and extracting argumentative components (e.g. premises, claims, etc.) and detecting the existing relations among them (i.e., support, attack, no relations). Deep learning models enable us to analyze arguments more efficiently than traditional methods and extract their semantics. This paper presents comparative studies between a few deep learning-based models in argument mining. The work concentrates on argument classification. The research was done on a wide spectrum of datasets (Args.me, UKP, US2016). The main novelty of this paper is the ensemble model which is based on BERT architecture and ChatGPT-4 as fine tuning model. The presented results show that BERT+ChatGPT-4 outperforms the rest of the models including other Transformer-based and LSTM-based models. The observed improvement is, in most cases, greater than 10The presented analysis can provide crucial insi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102; vi-Mistral-X&#65292;&#19968;&#20010;&#19987;&#20026;&#36234;&#21335;&#35821;&#35774;&#35745;&#30340;&#21019;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#25345;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#38024;&#23545;&#36234;&#21335;&#35821;&#30340;&#39069;&#22806;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#20854;&#23545;&#36234;&#21335;&#35821;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.15470</link><description>&lt;p&gt;
&#20351;&#29992;&#20808;&#36827;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#26500;&#24314;&#36234;&#21335;&#35821;&#35328;&#27169;&#22411; Vi-Mistral-X
&lt;/p&gt;
&lt;p&gt;
Vi-Mistral-X: Building a Vietnamese Language Model with Advanced Continual Pre-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15470
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102; vi-Mistral-X&#65292;&#19968;&#20010;&#19987;&#20026;&#36234;&#21335;&#35821;&#35774;&#35745;&#30340;&#21019;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#25345;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#38024;&#23545;&#36234;&#21335;&#35821;&#30340;&#39069;&#22806;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#20854;&#23545;&#36234;&#21335;&#35821;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#27493;&#26174;&#33879;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#23613;&#31649;&#19987;&#27880;&#20110;&#33521;&#35821;&#20013;&#24515;&#27169;&#22411;&#23548;&#33268;&#20102;&#19968;&#20123;&#29305;&#23450;&#35821;&#35328;&#65288;&#21253;&#25324;&#36234;&#21335;&#35821;&#65289;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;vi-mistral-x&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#36234;&#21335;&#35821;&#35774;&#35745;&#30340;&#21019;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#21033;&#29992;&#20102;&#22522;&#20110; Mistral &#26550;&#26500;&#30340;&#19968;&#31181;&#29420;&#29305;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#20998;&#32452;&#26597;&#35810;&#27880;&#24847;&#21147;&#21644;&#28369;&#21160;&#31383;&#21475;&#27880;&#24847;&#21147;&#25216;&#26415;&#12290;&#35813;&#27169;&#22411; vi-Mistral-X &#22312;&#25913;&#36827;&#23545;&#36234;&#21335;&#35821;&#35328;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#36808;&#20986;&#20102;&#37325;&#35201;&#19968;&#27493;&#12290;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#36234;&#21335;&#35821;&#30340;&#39069;&#22806;&#25345;&#32493;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#29702;&#35299;&#22797;&#26434;&#35821;&#35328;&#32454;&#24494;&#24046;&#21035;&#21644;&#29983;&#25104;&#20934;&#30830;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36234;&#21335;&#35821;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15470v1 Announce Type: new  Abstract: The advancement of Large Language Models (LLMs) has significantly transformed the field of natural language processing, although the focus on English-centric models has created a noticeable research gap for specific languages, including Vietnamese. To address this issue, this paper presents vi-mistral-x, an innovative Large Language Model designed expressly for the Vietnamese language. It utilizes a unique method of continual pre-training, based on the Mistral architecture, which incorporates grouped-query attention and sliding window attention techniques. This model, vi-Mistral-X, marks a significant step forward in improving the understanding and generation of the Vietnamese language. It introduces an additional phase of continual pre-training, specifically adapted for Vietnamese, enhancing the model's capability in understanding complex language nuances and generating accurate, context-aware Vietnamese text. Through comprehensive test
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24320;&#21457;&#30340;&#31561;&#36317;NMT&#31995;&#32479;&#65292;&#37325;&#28857;&#22312;&#20110;&#20248;&#21270;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#21477;&#23545;&#20013;&#38899;&#32032;&#35745;&#25968;&#30340;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2403.15469</link><description>&lt;p&gt;
&#20351;&#29992;&#38899;&#32032;&#35745;&#25968;&#27604;&#22870;&#21169;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#31561;&#36317;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Isometric Neural Machine Translation using Phoneme Count Ratio Reward-based Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24320;&#21457;&#30340;&#31561;&#36317;NMT&#31995;&#32479;&#65292;&#37325;&#28857;&#22312;&#20110;&#20248;&#21270;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#21477;&#23545;&#20013;&#38899;&#32032;&#35745;&#25968;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#33258;&#21160;&#35270;&#39057;&#37197;&#38899;&#65288;AVD&#65289;&#27969;&#27700;&#32447;&#30001;&#19977;&#20010;&#20851;&#38190;&#27169;&#22359;&#32452;&#25104;&#65292;&#21363;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12289;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#12290;AVD&#31649;&#36947;&#20013;&#20351;&#29992;&#31561;&#36317;-NMT&#31639;&#27861;&#26469;&#35843;&#33410;&#21512;&#25104;&#36755;&#20986;&#25991;&#26412;&#30340;&#38271;&#24230;&#65292;&#20197;&#30830;&#20445;&#22312;&#37197;&#38899;&#36807;&#31243;&#20043;&#21518;&#65292;&#35270;&#39057;&#21644;&#38899;&#39057;&#30340;&#23545;&#40784;&#21516;&#27493;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#38598;&#20013;&#20110;&#35843;&#25972;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#25991;&#26412;&#20013;&#23383;&#31526;&#21644;&#21333;&#35789;&#30340;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#35843;&#25972;&#38899;&#32032;&#30340;&#25968;&#37327;&#65292;&#22240;&#20026;&#23427;&#20204;&#19982;&#35821;&#38899;&#25345;&#32493;&#26102;&#38388;&#23494;&#20999;&#30456;&#20851;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24320;&#21457;&#31561;&#36317;NMT&#31995;&#32479;&#65292;&#37325;&#28857;&#20248;&#21270;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#21477;&#23545;&#20013;&#38899;&#32032;&#35745;&#25968;&#30340;&#23545;&#40784;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15469v1 Announce Type: new  Abstract: Traditional Automatic Video Dubbing (AVD) pipeline consists of three key modules, namely, Automatic Speech Recognition (ASR), Neural Machine Translation (NMT), and Text-to-Speech (TTS). Within AVD pipelines, isometric-NMT algorithms are employed to regulate the length of the synthesized output text. This is done to guarantee synchronization with respect to the alignment of video and audio subsequent to the dubbing process. Previous approaches have focused on aligning the number of characters and words in the source and target language texts of Machine Translation models. However, our approach aims to align the number of phonemes instead, as they are closely associated with speech duration. In this paper, we present the development of an isometric NMT system using Reinforcement Learning (RL), with a focus on optimizing the alignment of phoneme counts in the source and target language sentence pairs. To evaluate our models, we propose the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#29992;&#25143;&#25925;&#24847;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#27744;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;&#24341;&#20837;&#36880;&#23618;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#27744;&#21270;&#31574;&#30053;&#26469;&#25429;&#25417;&#20882;&#29359;&#24615;&#21644;&#26631;&#35760;&#23884;&#20837;&#65292;&#20351;&#27169;&#22411;&#26356;&#21152;&#31283;&#20581;&#65292;&#21363;&#20351;&#25915;&#20987;&#29575;&#22686;&#21152;&#12290;</title><link>https://arxiv.org/abs/2403.15467</link><description>&lt;p&gt;
&#22312;&#20882;&#29359;&#24615;&#35821;&#35328;&#26816;&#27979;&#20013;&#30340;&#27744;&#21270;&#31574;&#30053;&#65306;&#38024;&#23545;&#29992;&#25143;&#25925;&#24847;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Don't be a Fool: Pooling Strategies in Offensive Language Detection from User-Intended Adversarial Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#29992;&#25143;&#25925;&#24847;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#27744;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;&#24341;&#20837;&#36880;&#23618;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#27744;&#21270;&#31574;&#30053;&#26469;&#25429;&#25417;&#20882;&#29359;&#24615;&#21644;&#26631;&#35760;&#23884;&#20837;&#65292;&#20351;&#27169;&#22411;&#26356;&#21152;&#31283;&#20581;&#65292;&#21363;&#20351;&#25915;&#20987;&#29575;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20882;&#29359;&#24615;&#35821;&#35328;&#26816;&#27979;&#26159;&#36807;&#28388;&#36785;&#39554;&#34920;&#36798;&#24182;&#25913;&#21892;&#22312;&#32447;&#29992;&#25143;&#20307;&#39564;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24694;&#24847;&#29992;&#25143;&#24120;&#24120;&#36890;&#36807;&#24341;&#20837;&#25991;&#26412;&#22122;&#38899;&#26469;&#35268;&#36991;&#36807;&#28388;&#31995;&#32479;&#12290;&#26412;&#25991;&#23558;&#36825;&#20123;&#35268;&#36991;&#34892;&#20026;&#20316;&#20026;&#29992;&#25143;&#25925;&#24847;&#30340;&#23545;&#25239;&#25915;&#20987;&#25552;&#20986;&#65292;&#36825;&#20123;&#25915;&#20987;&#25554;&#20837;&#29305;&#27530;&#31526;&#21495;&#25110;&#21033;&#29992;&#38889;&#25991;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20197;&#36880;&#23618;&#26041;&#24335;&#24341;&#20837;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27744;&#21270;&#31574;&#30053;&#65292;&#20197;&#25269;&#24481;&#25152;&#25552;&#20986;&#30340;&#25915;&#20987;&#65292;&#20391;&#37325;&#20110;&#21069;&#32622;&#23618;&#32780;&#19981;&#20165;&#20165;&#26159;&#26368;&#21518;&#19968;&#23618;&#20197;&#25429;&#25417;&#20882;&#29359;&#24615;&#21644;&#26631;&#35760;&#23884;&#20837;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#27744;&#21270;&#31574;&#30053;&#22312;&#25915;&#20987;&#29575;&#22686;&#21152;&#26102;&#26356;&#21152;&#31283;&#20581;&#65292;&#21363;&#20351;&#27809;&#26377;&#30452;&#25509;&#35757;&#32451;&#36825;&#26679;&#30340;&#27169;&#24335;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#24178;&#20928;&#25991;&#26412;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#26816;&#27979;&#36973;&#21463;&#25915;&#20987;&#30340;&#20882;&#29359;&#24615;&#35821;&#35328;&#26041;&#38754;&#21487;&#20197;&#36798;&#21040;&#21487;&#27604;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15467v1 Announce Type: new  Abstract: Offensive language detection is an important task for filtering out abusive expressions and improving online user experiences. However, malicious users often attempt to avoid filtering systems through the involvement of textual noises. In this paper, we propose these evasions as user-intended adversarial attacks that insert special symbols or leverage the distinctive features of the Korean language. Furthermore, we introduce simple yet effective pooling strategies in a layer-wise manner to defend against the proposed attacks, focusing on the preceding layers not just the last layer to capture both offensiveness and token embeddings. We demonstrate that these pooling strategies are more robust to performance degradation even when the attack rate is increased, without directly training of such patterns. Notably, we found that models pre-trained on clean texts could achieve a comparable performance in detecting attacked offensive language, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#23637;&#24320;&#31639;&#27861;&#20026;$n$-grams&#65292;Transformers&#65292;HMMs&#21644;&#39532;&#23572;&#21487;&#22827;&#38142;&#29983;&#25104;&#26368;&#26377;&#21487;&#33021;&#30340;&#24207;&#21015;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.15465</link><description>&lt;p&gt;
&#20351;&#29992;&#23637;&#24320;&#31639;&#27861;&#20026;$n$-grams&#65292;Transformers&#65292;HMMs&#21644;&#39532;&#23572;&#21487;&#22827;&#38142;&#29983;&#25104;&#26368;&#26377;&#21487;&#33021;&#30340;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Most Likely Sequence Generation for $n$-Grams, Transformers, HMMs, and Markov Chains, by Using Rollout Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#23637;&#24320;&#31639;&#27861;&#20026;$n$-grams&#65292;Transformers&#65292;HMMs&#21644;&#39532;&#23572;&#21487;&#22827;&#38142;&#29983;&#25104;&#26368;&#26377;&#21487;&#33021;&#30340;&#24207;&#21015;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20855;&#26377;$n$-gram&#32467;&#26500;&#30340;transformer&#65292;&#20363;&#22914;&#24213;&#23618;&#30340;ChatGPT&#12290;Transformer&#25552;&#20379;&#20102;&#19979;&#19968;&#20010;&#21333;&#35789;&#30340;&#27010;&#29575;&#65292;&#21487;&#20197;&#29992;&#26469;&#29983;&#25104;&#21333;&#35789;&#24207;&#21015;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#36825;&#20123;&#27010;&#29575;&#35745;&#31639;&#39640;&#21487;&#33021;&#24615;&#21333;&#35789;&#24207;&#21015;&#30340;&#26041;&#27861;&#12290;&#35745;&#31639;&#20174;&#32473;&#23450;&#21021;&#22987;&#29366;&#24577;&#24320;&#22987;&#30340;&#26368;&#20248;&#65288;&#21363;&#26368;&#26377;&#21487;&#33021;&#65289;&#21333;&#35789;&#24207;&#21015;&#26159;&#19968;&#20010;&#26840;&#25163;&#30340;&#38382;&#39064;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;$N$&#21644;$n$-gram&#35789;&#27719;&#37327;&#30340;&#20302;&#38454;&#22810;&#39033;&#24335;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;$N$&#20010;&#21333;&#35789;&#30340;&#39640;&#21487;&#33021;&#24615;&#24207;&#21015;&#12290;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#36817;&#20284;&#21160;&#24577;&#35268;&#21010;&#20013;&#30340;&#23637;&#24320;&#26041;&#27861;&#65292;&#19968;&#31181;&#21333;&#31574;&#30053;&#36845;&#20195;&#65292;&#21487;&#20197;&#25913;&#21892;&#20219;&#20309;&#32473;&#23450;&#21551;&#21457;&#24335;&#31574;&#30053;&#30340;&#24615;&#33021;&#12290;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#36138;&#23146;&#21551;&#21457;&#24335;&#65292;&#29983;&#25104;&#20855;&#26377;&#26368;&#39640;&#27010;&#29575;&#30340;&#19979;&#19968;&#20010;&#21333;&#35789;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#12289;&#31034;&#20363;&#21644;&#35745;&#31639;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#30340;m
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15465v1 Announce Type: cross  Abstract: In this paper we consider a transformer with an $n$-gram structure, such as the one underlying ChatGPT. The transformer provides next word probabilities, which can be used to generate word sequences. We consider methods for computing word sequences that are highly likely, based on these probabilities. Computing the optimal (i.e., most likely) word sequence starting with a given initial state is an intractable problem, so we propose methods to compute highly likely sequences of $N$ words in time that is a low order polynomial in $N$ and in the vocabulary size of the $n$-gram. These methods are based on the rollout approach from approximate dynamic programming, a form of single policy iteration, which can improve the performance of any given heuristic policy. In our case we use a greedy heuristic that generates as next word one that has the highest probability. We show with analysis, examples, and computational experimentation that our m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#39044;&#27979;&#24615;&#20195;&#29702;&#25512;&#29702;&#21644;&#25209;&#21028;&#24615;&#20195;&#29702;&#25351;&#23548;&#65292;&#21033;&#29992;LLMs&#23545;&#32467;&#26500;&#21270;&#24739;&#32773;&#23601;&#35786;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.15464</link><description>&lt;p&gt;
&#22522;&#20110;LLMs&#30340;&#23569;&#26679;&#26412;&#30142;&#30149;&#39044;&#27979;&#65306;&#32467;&#21512;&#39044;&#27979;&#24615;&#20195;&#29702;&#25512;&#29702;&#21644;&#25209;&#21028;&#24615;&#20195;&#29702;&#25351;&#23548;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LLMs-based Few-Shot Disease Predictions using EHR: A Novel Approach Combining Predictive Agent Reasoning and Critical Agent Instruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15464
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#39044;&#27979;&#24615;&#20195;&#29702;&#25512;&#29702;&#21644;&#25209;&#21028;&#24615;&#20195;&#29702;&#25351;&#23548;&#65292;&#21033;&#29992;LLMs&#23545;&#32467;&#26500;&#21270;&#24739;&#32773;&#23601;&#35786;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15464v1 &#31867;&#22411;&#65306;&#36328;&#23398;&#31185; &#25688;&#35201;&#65306;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;(EHRs)&#21253;&#21547;&#23545;&#20581;&#24247;&#30456;&#20851;&#39044;&#27979;&#20219;&#21153;&#65292;&#22914;&#30142;&#30149;&#39044;&#27979;&#65292;&#26377;&#20215;&#20540;&#30340;&#24739;&#32773;&#25968;&#25454;&#12290;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36825;&#21487;&#33021;&#26159;&#26114;&#36149;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24212;&#29992;&#20110;&#23558;&#32467;&#26500;&#21270;&#24739;&#32773;&#23601;&#35786;&#25968;&#25454;(&#20363;&#22914;&#35786;&#26029;&#12289;&#23454;&#39564;&#23460;&#12289;&#22788;&#26041;)&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#21465;&#36848;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#38754;&#21521;EHR&#39044;&#27979;&#30340;&#25552;&#31034;&#31574;&#30053;&#35780;&#20272;&#20102;LLMs&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20855;&#26377;&#19981;&#21516;&#35282;&#33394;&#30340;LLM&#20195;&#29702;&#65306;&#19968;&#20010;&#36827;&#34892;&#39044;&#27979;&#24182;&#29983;&#25104;&#25512;&#29702;&#36807;&#31243;&#30340;&#39044;&#27979;&#20195;&#29702;&#65292;&#20197;&#21450;&#20998;&#26512;&#19981;&#27491;&#30830;&#39044;&#27979;&#24182;&#20026;&#25913;&#21892;&#39044;&#27979;&#20195;&#29702;&#25512;&#29702;&#25552;&#20379;&#25351;&#23548;&#30340;&#25209;&#35780;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;LLMs&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15464v1 Announce Type: cross  Abstract: Electronic health records (EHRs) contain valuable patient data for health-related prediction tasks, such as disease prediction. Traditional approaches rely on supervised learning methods that require large labeled datasets, which can be expensive and challenging to obtain. In this study, we investigate the feasibility of applying Large Language Models (LLMs) to convert structured patient visit data (e.g., diagnoses, labs, prescriptions) into natural language narratives. We evaluate the zero-shot and few-shot performance of LLMs using various EHR-prediction-oriented prompting strategies. Furthermore, we propose a novel approach that utilizes LLM agents with different roles: a predictor agent that makes predictions and generates reasoning processes and a critic agent that analyzes incorrect predictions and provides guidance for improving the reasoning of the predictor agent. Our results demonstrate that with the proposed approach, LLMs c
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#35821;&#35328;&#29983;&#25104;&#22312;&#32447;&#30740;&#31350;&#20013;&#25928;&#24212;&#22823;&#23567;&#12289;&#21464;&#24322;&#24615;&#21644;&#21151;&#25928;&#23545;&#35774;&#35745;&#25928;&#26524;&#30340;&#24433;&#21709;</title><link>https://arxiv.org/abs/2403.15459</link><description>&lt;p&gt;
&#35780;&#20272;&#35821;&#35328;&#29983;&#25104;&#22312;&#32447;&#30740;&#31350;&#20013;&#30340;&#25928;&#24212;&#22823;&#23567;&#12289;&#21464;&#24322;&#24615;&#21644;&#21151;&#25928;
&lt;/p&gt;
&lt;p&gt;
Assessing effect sizes, variability, and power in the on-line study of language production
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15459
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#35821;&#35328;&#29983;&#25104;&#22312;&#32447;&#30740;&#31350;&#20013;&#25928;&#24212;&#22823;&#23567;&#12289;&#21464;&#24322;&#24615;&#21644;&#21151;&#25928;&#23545;&#35774;&#35745;&#25928;&#26524;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30123;&#24773;&#30340;&#27969;&#34892;&#65292;&#35768;&#22810;&#23454;&#39564;&#24515;&#29702;&#23398;&#23478;&#21644;&#35821;&#35328;&#23398;&#23478;&#24050;&#32463;&#24320;&#22987;&#22312;&#20114;&#32852;&#32593;&#19978;&#25910;&#38598;&#25968;&#25454;&#65288;&#20197;&#19979;&#31616;&#31216;&#22312;&#32447;&#25968;&#25454;&#65289;&#12290;&#24517;&#39035;&#35780;&#20272;&#36825;&#31867;&#23454;&#39564;&#30340;&#21487;&#34892;&#24615;&#20197;&#21450;&#26410;&#26469;&#23454;&#39564;&#25152;&#38656;&#30340;&#26679;&#26412;&#37327;&#65292;&#20197;&#36798;&#21040;&#36275;&#22815;&#30340;&#32479;&#35745;&#21151;&#25928;&#12290;&#36825;&#21453;&#36807;&#26469;&#38656;&#35201;&#20102;&#35299;&#25928;&#24212;&#22823;&#23567;&#21644;&#21464;&#24322;&#24615;&#30340;&#20449;&#24687;&#12290;&#22312;&#19968;&#31995;&#21015;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#22312;&#23454;&#39564;&#23460;&#21644;&#22312;&#32447;&#36827;&#34892;&#30340;&#30456;&#21516;&#35789;&#27719;&#20135;&#29983;&#23454;&#39564;&#20013;&#33719;&#24471;&#30340;&#21709;&#24212;&#26102;&#38388;&#25968;&#25454;&#12290;&#36825;&#20123;&#20998;&#26512;&#35753;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#20004;&#31181;&#35774;&#32622;&#22312;&#25928;&#24212;&#22823;&#23567;&#12289;&#23454;&#39564;&#36807;&#31243;&#20013;&#21709;&#24212;&#30340;&#19968;&#33268;&#24615;&#12289;&#36328;&#21442;&#19982;&#32773;&#24179;&#22343;&#21709;&#24212;&#26102;&#38388;&#30340;&#21464;&#24322;&#24615;&#12289;&#21442;&#19982;&#32773;&#20043;&#38388;&#25928;&#24212;&#22823;&#23567;&#30340;&#24046;&#24322;&#65292;&#20197;&#21450;&#26410;&#35299;&#37322;&#21464;&#24322;&#37327;&#30340;&#25968;&#37327;&#26159;&#21542;&#23384;&#22312;&#24046;&#24322;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#27169;&#25311;&#35780;&#20272;&#36825;&#20123;&#24046;&#24322;&#23545;&#35774;&#35745;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#23545;&#22312;&#32447;&#30740;&#31350;&#21487;&#33021;&#24102;&#26469;&#30340;&#28909;&#24773;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15459v1 Announce Type: new  Abstract: With the pandemic, many experimental psychologists and linguists have started to collect data over the internet (hereafter on-line data). The feasibility of such experiments and the sample sizes required to achieve sufficient statistical power in future experiments have to be assessed. This in turn requires information on effect sizes and variability. In a series of analyses, we compare response time data obtained in the same word production experiment conducted in the lab and on-line. These analyses allow us to determine whether the two settings differ in effect sizes, in the consistency of responses over the course of the experiment, in the variability of average response times across participants, in the magnitude of effect sizes across participants, or in the amount of unexplained variability. We assess the impact of these differences on the power of the design in a series of simulations. Our findings temper the enthusiasm raised by 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#27979;&#28216;&#25103;&#20869;&#22403;&#22334;&#35805;&#21644;&#27602;&#24615;&#20449;&#24687;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20351;&#29992;BERT&#21644;GPT&#27169;&#22411;&#22312;DOTA 2&#28216;&#25103;&#23545;&#25112;&#30340;&#32842;&#22825;&#25968;&#25454;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.15458</link><description>&lt;p&gt;
&#35843;&#25972;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#26816;&#27979;&#28216;&#25103;&#20869;&#22403;&#22334;&#35805;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning Pre-trained Language Models to Detect In-Game Trash Talks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#27979;&#28216;&#25103;&#20869;&#22403;&#22334;&#35805;&#21644;&#27602;&#24615;&#20449;&#24687;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20351;&#29992;BERT&#21644;GPT&#27169;&#22411;&#22312;DOTA 2&#28216;&#25103;&#23545;&#25112;&#30340;&#32842;&#22825;&#25968;&#25454;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29609;&#22312;&#32447;&#25163;&#26426;&#21644;&#30005;&#33041;&#28216;&#25103;&#26102;&#24120;&#35265;&#30340;&#38382;&#39064;&#19982;&#29609;&#23478;&#20043;&#38388;&#30340;&#26377;&#27602;&#34892;&#20026;&#21644;&#28389;&#29992;&#27807;&#36890;&#26377;&#20851;&#12290;&#22522;&#20110;&#19981;&#21516;&#30340;&#25253;&#21578;&#21644;&#30740;&#31350;&#65292;&#26412;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#21644;&#27602;&#24615;&#23545;&#29609;&#23478;&#28216;&#25103;&#34920;&#29616;&#21644;&#25972;&#20307;&#24184;&#31119;&#24863;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#20998;&#31867;&#25110;&#26816;&#27979;&#28216;&#25103;&#20869;&#22403;&#22334;&#35805;&#25110;&#26377;&#27602;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#37319;&#29992;&#24182;&#35780;&#20272;&#20102;&#39044;&#35757;&#32451;&#30340;BERT&#21644;GPT&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#27979;&#28216;&#25103;&#20869;&#32842;&#22825;&#20013;&#30340;&#27602;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#21033;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;API&#65292;&#25910;&#38598;&#20102;&#26469;&#33258;DOTA 2&#28216;&#25103;&#23545;&#25112;&#30340;&#28216;&#25103;&#20869;&#32842;&#22825;&#25968;&#25454;&#65292;&#32463;&#36807;&#22788;&#29702;&#12289;&#23457;&#26597;&#21644;&#26631;&#35760;&#20026;&#38750;&#27602;&#24615;&#12289;&#36731;&#24494;&#65288;&#27602;&#24615;&#65289;&#21644;&#26377;&#27602;&#12290;&#35813;&#30740;&#31350;&#33021;&#22815;&#25910;&#38598;&#32422;&#20004;&#21315;&#20010;&#28216;&#25103;&#20869;&#32842;&#22825;&#20197;&#35757;&#32451;&#21644;&#27979;&#35797;BERT&#65288;Base-uncased&#65289;&#12289;BERT&#65288;Large-uncased&#65289;&#21644;GPT-3&#27169;&#22411;&#12290;&#22522;&#20110;&#36825;&#19977;&#31181;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#65292;&#26412;&#30740;&#31350;&#24471;&#20986;&#32467;&#35770;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15458v1 Announce Type: new  Abstract: Common problems in playing online mobile and computer games were related to toxic behavior and abusive communication among players. Based on different reports and studies, the study also discusses the impact of online hate speech and toxicity on players' in-game performance and overall well-being. This study investigates the capability of pre-trained language models to classify or detect trash talk or toxic in-game messages The study employs and evaluates the performance of pre-trained BERT and GPT language models in detecting toxicity within in-game chats. Using publicly available APIs, in-game chat data from DOTA 2 game matches were collected, processed, reviewed, and labeled as non-toxic, mild (toxicity), and toxic. The study was able to collect around two thousand in-game chats to train and test BERT (Base-uncased), BERT (Large-uncased), and GPT-3 models. Based on the three models' state-of-the-art performance, this study concludes p
&lt;/p&gt;</description></item><item><title>WoLF&#26694;&#26550;&#25552;&#20986;&#20102;&#23545;&#20110;CXR&#30340;&#20840;&#38754;&#29702;&#35299;&#30340;&#25913;&#36827;&#65292;&#21253;&#25324;&#20351;&#29992;&#39069;&#22806;&#30340;&#20581;&#24247;&#30456;&#20851;&#25968;&#25454;&#12289;&#37325;&#26500;&#25253;&#21578;&#20197;&#25552;&#20379;&#26356;&#26377;&#32452;&#32455;&#30340;&#20449;&#24687;&#12289;&#20197;&#21450;&#25913;&#36827;&#29983;&#25104;&#31572;&#26696;&#30340;&#32454;&#33268;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.15456</link><description>&lt;p&gt;
WoLF: &#29992;&#20110;&#33016;&#37096;X&#32447;&#22270;&#29702;&#35299;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
WoLF: Large Language Model Framework for CXR Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15456
&lt;/p&gt;
&lt;p&gt;
WoLF&#26694;&#26550;&#25552;&#20986;&#20102;&#23545;&#20110;CXR&#30340;&#20840;&#38754;&#29702;&#35299;&#30340;&#25913;&#36827;&#65292;&#21253;&#25324;&#20351;&#29992;&#39069;&#22806;&#30340;&#20581;&#24247;&#30456;&#20851;&#25968;&#25454;&#12289;&#37325;&#26500;&#25253;&#21578;&#20197;&#25552;&#20379;&#26356;&#26377;&#32452;&#32455;&#30340;&#20449;&#24687;&#12289;&#20197;&#21450;&#25913;&#36827;&#29983;&#25104;&#31572;&#26696;&#30340;&#32454;&#33268;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29616;&#20195;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLMs)&#21462;&#24471;&#20102;&#23545;&#33016;&#37096;X&#32447;&#22270;(CXR)&#29702;&#35299;&#26041;&#38754;&#30340;&#26174;&#30528;&#26041;&#27861;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35270;&#35273;&#38382;&#31572;(VQA)&#21644;CXR&#25253;&#21578;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CXR&#29702;&#35299;&#26694;&#26550;&#20173;&#23384;&#22312;&#20960;&#20010;&#31243;&#24207;&#19978;&#30340;&#32570;&#38519;&#12290;(1)&#20197;&#24448;&#30340;&#26041;&#27861;&#20165;&#20351;&#29992;CXR&#25253;&#21578;&#65292;&#36825;&#23545;&#20110;&#20840;&#38754;&#30340;&#35270;&#35273;&#38382;&#31572;(VQA)&#26469;&#35828;&#26159;&#19981;&#22815;&#30340;&#65292;&#29305;&#21035;&#26159;&#24403;&#38656;&#35201;&#39069;&#22806;&#30340;&#20581;&#24247;&#30456;&#20851;&#25968;&#25454;&#22914;&#29992;&#33647;&#21382;&#21490;&#21644;&#20808;&#21069;&#30340;&#35786;&#26029;&#26102;&#12290;(2)&#20197;&#24448;&#30340;&#26041;&#27861;&#20351;&#29992;&#26410;&#32463;&#22788;&#29702;&#30340;CXR&#25253;&#21578;&#65292;&#36825;&#20123;&#25253;&#21578;&#24448;&#24448;&#32467;&#26500;&#38543;&#24847;&#12290;&#34429;&#28982;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29702;&#35299;&#21508;&#31181;&#25991;&#26412;&#26684;&#24335;&#65292;&#20294;&#20026;&#20102;&#25552;&#20379;&#26356;&#28165;&#26224;&#12289;&#26377;&#32452;&#32455;&#30340;&#22522;&#20110;&#35299;&#21078;&#23398;&#30340;&#20449;&#24687;&#65292;&#37325;&#26500;&#25253;&#21578;&#21487;&#33021;&#20250;&#22686;&#24378;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;(3)&#30446;&#21069;&#29992;&#20110;CXR-VQA&#30340;&#35780;&#20272;&#26041;&#27861;&#20027;&#35201;&#24378;&#35843;&#35821;&#35328;&#27491;&#30830;&#24615;&#65292;&#32570;&#20047;&#23545;&#29983;&#25104;&#31572;&#26696;&#30340;&#24494;&#22937;&#35780;&#20272;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15456v1 Announce Type: new  Abstract: Significant methodological strides have been made toward Chest X-ray (CXR) understanding via modern vision-language models (VLMs), demonstrating impressive Visual Question Answering (VQA) and CXR report generation abilities. However, existing CXR understanding frameworks still possess several procedural caveats. (1) Previous methods solely use CXR reports, which are insufficient for comprehensive Visual Question Answering (VQA), especially when additional health-related data like medication history and prior diagnoses are needed. (2) Previous methods use raw CXR reports, which are often arbitrarily structured. While modern language models can understand various text formats, restructuring reports for clearer, organized anatomy-based information could enhance their usefulness. (3) Current evaluation methods for CXR-VQA primarily emphasize linguistic correctness, lacking the capability to offer nuanced assessments of the generated answers.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#65292;&#36890;&#36807;&#25506;&#32034;&#19971;&#31181;&#25991;&#26412;&#37319;&#26679;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#31934;&#32454;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#20943;&#36731;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.15455</link><description>&lt;p&gt;
&#25913;&#36827;&#25991;&#26412;&#27969;&#20013;&#29992;&#20110;&#24494;&#35843;SentenceBERT&#30340;&#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Sampling Methods for Fine-tuning SentenceBERT in Text Streams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#65292;&#36890;&#36807;&#25506;&#32034;&#19971;&#31181;&#25991;&#26412;&#37319;&#26679;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#31934;&#32454;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#20943;&#36731;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#19978;&#25991;&#26412;&#25968;&#25454;&#30340;&#28608;&#22686;&#20026;&#26426;&#26500;&#21644;&#20844;&#21496;&#25552;&#20379;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#26426;&#20250;&#65292;&#21487;&#20197;&#30417;&#27979;&#20844;&#20247;&#23545;&#20854;&#26381;&#21153;&#21644;&#20135;&#21697;&#30340;&#24847;&#35265;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#25968;&#25454;&#30340;&#24555;&#36895;&#29983;&#25104;&#65292;&#22788;&#29702;&#20381;&#27425;&#21040;&#36798;&#12289;&#28508;&#22312;&#26080;&#38480;&#30340;&#25991;&#26412;&#27969;&#30340;&#25991;&#26412;&#27969;&#25366;&#25496;&#35774;&#32622;&#36890;&#24120;&#27604;&#20256;&#32479;&#30340;&#25209;&#37327;&#23398;&#20064;&#26356;&#21512;&#36866;&#12290;&#34429;&#28982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#22240;&#20854;&#22312;&#27969;&#24335;&#20869;&#23481;&#20013;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#21521;&#37327;&#21270;&#33021;&#21147;&#32780;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#23427;&#20204;&#22312;&#36866;&#24212;&#27010;&#24565;&#28418;&#31227;&#65288;&#25968;&#25454;&#20998;&#24067;&#38543;&#26102;&#38388;&#21457;&#29983;&#21464;&#21270;&#65292;&#20174;&#32780;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#30340;&#29616;&#35937;&#65289;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#19971;&#31181;&#25991;&#26412;&#37319;&#26679;&#26041;&#27861;&#23545;&#31934;&#24515;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#20174;&#32780;&#20943;&#36731;&#24615;&#33021;&#19979;&#38477;&#12290;&#25105;&#20204;&#20934;&#30830;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#23545;&#20351;&#29992;&#22235;&#31181;&#19981;&#21516;&#26041;&#24335;&#36827;&#34892;&#24494;&#35843;&#30340;SBERT&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15455v1 Announce Type: new  Abstract: The proliferation of textual data on the Internet presents a unique opportunity for institutions and companies to monitor public opinion about their services and products. Given the rapid generation of such data, the text stream mining setting, which handles sequentially arriving, potentially infinite text streams, is often more suitable than traditional batch learning. While pre-trained language models are commonly employed for their high-quality text vectorization capabilities in streaming contexts, they face challenges adapting to concept drift - the phenomenon where the data distribution changes over time, adversely affecting model performance. Addressing the issue of concept drift, this study explores the efficacy of seven text sampling methods designed to selectively fine-tune language models, thereby mitigating performance degradation. We precisely assess the impact of these methods on fine-tuning the SBERT model using four differ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25991;&#26412;&#25968;&#25454;&#24773;&#24863;&#20998;&#31867;&#20013;&#24212;&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#24120;&#29992;&#25216;&#26415;&#22914;&#21435;&#38500;&#26631;&#28857;&#31526;&#21495;&#21644;&#20572;&#29992;&#35789;&#21487;&#33021;&#20250;&#38459;&#30861;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#36825;&#20123;&#20803;&#32032;&#20173;&#28982;&#33021;&#22815;&#20256;&#36798;&#24773;&#24863;&#25110;&#24378;&#35843;&#65292;&#32780;Transformer&#30340;&#20248;&#21183;&#22312;&#20110;&#29702;&#35299;&#25991;&#26412;&#20869;&#30340;&#35821;&#22659;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.15454</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#36827;&#34892;&#24773;&#24863;&#26816;&#27979;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Emotion Detection with Transformers: A Comparative Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25991;&#26412;&#25968;&#25454;&#24773;&#24863;&#20998;&#31867;&#20013;&#24212;&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#24120;&#29992;&#25216;&#26415;&#22914;&#21435;&#38500;&#26631;&#28857;&#31526;&#21495;&#21644;&#20572;&#29992;&#35789;&#21487;&#33021;&#20250;&#38459;&#30861;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#36825;&#20123;&#20803;&#32032;&#20173;&#28982;&#33021;&#22815;&#20256;&#36798;&#24773;&#24863;&#25110;&#24378;&#35843;&#65292;&#32780;Transformer&#30340;&#20248;&#21183;&#22312;&#20110;&#29702;&#35299;&#25991;&#26412;&#20869;&#30340;&#35821;&#22659;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;Transformer&#27169;&#22411;&#22312;&#25991;&#26412;&#25968;&#25454;&#24773;&#24863;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#21464;&#20307;&#30340;Transformer&#23545;Emotion&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#35770;&#25991;&#36824;&#20998;&#26512;&#20102;&#19968;&#20123;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#30340;&#22240;&#32032;&#65292;&#27604;&#22914;Transformer&#23618;&#30340;&#24494;&#35843;&#12289;&#23618;&#30340;&#21487;&#35757;&#32451;&#24615;&#20197;&#21450;&#25991;&#26412;&#25968;&#25454;&#30340;&#39044;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24120;&#29992;&#25216;&#26415;&#22914;&#21435;&#38500;&#26631;&#28857;&#31526;&#21495;&#21644;&#20572;&#29992;&#35789;&#21487;&#33021;&#20250;&#38459;&#30861;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#21487;&#33021;&#26159;&#22240;&#20026;Transformer&#30340;&#20248;&#21183;&#22312;&#20110;&#29702;&#35299;&#25991;&#26412;&#20869;&#30340;&#35821;&#22659;&#20851;&#31995;&#12290;&#20687;&#26631;&#28857;&#31526;&#21495;&#21644;&#20572;&#29992;&#35789;&#36825;&#26679;&#30340;&#20803;&#32032;&#20173;&#28982;&#21487;&#20197;&#20256;&#36798;&#24773;&#24863;&#25110;&#24378;&#35843;&#65292;&#21435;&#38500;&#23427;&#20204;&#21487;&#33021;&#20250;&#30772;&#22351;&#36825;&#31181;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15454v1 Announce Type: new  Abstract: In this study, we explore the application of transformer-based models for emotion classification on text data. We train and evaluate several pre-trained transformer models, on the Emotion dataset using different variants of transformers. The paper also analyzes some factors that in-fluence the performance of the model, such as the fine-tuning of the transformer layer, the trainability of the layer, and the preprocessing of the text data. Our analysis reveals that commonly applied techniques like removing punctuation and stop words can hinder model performance. This might be because transformers strength lies in understanding contextual relationships within text. Elements like punctuation and stop words can still convey sentiment or emphasis and removing them might disrupt this context.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20197;&#25991;&#26412;&#20013;&#30340;&#36328;&#24230;&#20026;&#20013;&#24515;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#23558;&#21508;&#31181;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20301;&#20026;&#30456;&#21516;&#22522;&#26412;&#38754;&#21521;&#36328;&#24230;&#30340;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#30340;&#21464;&#20307;</title><link>https://arxiv.org/abs/2403.15453</link><description>&lt;p&gt;
&#38754;&#21521;&#36328;&#24230;&#30340;&#20449;&#24687;&#25277;&#21462;--&#20449;&#24687;&#25277;&#21462;&#30340;&#32479;&#19968;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Span-Oriented Information Extraction -- A Unifying Perspective on Information Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15453
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20197;&#25991;&#26412;&#20013;&#30340;&#36328;&#24230;&#20026;&#20013;&#24515;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#23558;&#21508;&#31181;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20301;&#20026;&#30456;&#21516;&#22522;&#26412;&#38754;&#21521;&#36328;&#24230;&#30340;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#30340;&#21464;&#20307;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15453v1 &#20844;&#21578;&#31867;&#22411;: &#36328; &#24403;&#21069;&#25688;&#35201;: &#20449;&#24687;&#25277;&#21462;&#25351;&#30340;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#20854;&#30446;&#30340;&#26159;&#35782;&#21035;&#25991;&#26412;&#20013;&#30340;&#23376;&#24207;&#21015;&#21450;&#20854;&#26631;&#31614;&#12290;&#36825;&#20123;&#20219;&#21153;&#22810;&#24180;&#26469;&#34987;&#29992;&#20110;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#24182;&#23558;&#33258;&#30001;&#25991;&#26412;&#38142;&#25509;&#21040;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#22952;&#30861;&#20102;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#25991;&#26412;&#20013;&#30340;&#36328;&#24230;&#20026;&#20013;&#24515;&#30340;&#32479;&#19968;&#35270;&#35282;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#30475;&#20284;&#19981;&#21327;&#35843;&#30340;&#20219;&#21153;&#37325;&#26032;&#23450;&#20301;&#21040;&#36825;&#19968;&#32479;&#19968;&#35270;&#35282;&#20013;&#65292;&#24182;&#23558;&#21508;&#31181;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#21576;&#29616;&#20026;&#30456;&#21516;&#22522;&#26412;&#38754;&#21521;&#36328;&#24230;&#30340;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#30340;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15453v1 Announce Type: cross  Abstract: Information Extraction refers to a collection of tasks within Natural Language Processing (NLP) that identifies sub-sequences within text and their labels. These tasks have been used for many years to link extract relevant information and to link free text to structured data. However, the heterogeneity among information extraction tasks impedes progress in this area. We therefore offer a unifying perspective centered on what we define to be spans in text. We then re-orient these seemingly incongruous tasks into this unified perspective and then re-present the wide assortment of information extraction tasks as variants of the same basic Span-Oriented Information Extraction task.
&lt;/p&gt;</description></item><item><title>&#20174;&#35821;&#35328;&#27169;&#22411;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#24037;&#20855;&#30340;&#32479;&#19968;&#23450;&#20041;&#20026;LMs&#20351;&#29992;&#30340;&#22806;&#37096;&#31243;&#24207;&#65292;&#24182;&#23545;LM&#24037;&#20855;&#22330;&#26223;&#21644;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#23457;&#26597;&#65292;&#21516;&#26102;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#20102;&#35299;&#20102;&#21508;&#31181;&#24037;&#20855;&#26041;&#27861;&#30340;&#25928;&#29575;&#65292;&#20197;&#21450;&#31361;&#20986;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.15452</link><description>&lt;p&gt;
&#24037;&#20855;&#31350;&#31455;&#26159;&#20160;&#20040;&#65311;&#20174;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35282;&#36827;&#34892;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
What Are Tools Anyway? A Survey from the Language Model Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15452
&lt;/p&gt;
&lt;p&gt;
&#20174;&#35821;&#35328;&#27169;&#22411;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#24037;&#20855;&#30340;&#32479;&#19968;&#23450;&#20041;&#20026;LMs&#20351;&#29992;&#30340;&#22806;&#37096;&#31243;&#24207;&#65292;&#24182;&#23545;LM&#24037;&#20855;&#22330;&#26223;&#21644;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#23457;&#26597;&#65292;&#21516;&#26102;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#20102;&#35299;&#20102;&#21508;&#31181;&#24037;&#20855;&#26041;&#27861;&#30340;&#25928;&#29575;&#65292;&#20197;&#21450;&#31361;&#20986;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#38750;&#24120;&#24378;&#22823;&#12290;&#24037;&#20855;&#26174;&#33879;&#22686;&#24378;&#20102;LMs&#22312;&#38656;&#35201;&#22797;&#26434;&#25216;&#33021;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30740;&#31350;&#20197;&#19981;&#21516;&#26041;&#24335;&#20351;&#29992;"&#24037;&#20855;"&#19968;&#35789;&#65292;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#24037;&#20855;&#31350;&#31455;&#26159;&#20160;&#20040;&#65311;&#25509;&#19979;&#26469;&#65292;&#24037;&#20855;&#22312;&#21738;&#37324;&#20197;&#21450;&#22914;&#20309;&#24110;&#21161;LMs&#65311;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24037;&#20855;&#30340;&#32479;&#19968;&#23450;&#20041;&#65292;&#21363;LMs&#20351;&#29992;&#30340;&#22806;&#37096;&#31243;&#24207;&#65292;&#24182;&#23545;LM&#24037;&#20855;&#22330;&#26223;&#21644;&#26041;&#27861;&#36827;&#34892;&#31995;&#32479;&#24615;&#23457;&#26597;&#12290;&#22522;&#20110;&#36825;&#19968;&#23457;&#26597;&#65292;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#21508;&#31181;&#24037;&#20855;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#25152;&#38656;&#30340;&#35745;&#31639;&#21644;&#24615;&#33021;&#22686;&#30410;&#26469;&#23454;&#35777;&#30740;&#31350;&#23427;&#20204;&#30340;&#25928;&#29575;&#65292;&#24182;&#31361;&#20986;&#20102;&#35813;&#39046;&#22495;&#30340;&#19968;&#20123;&#25361;&#25112;&#21644;&#28508;&#22312;&#26410;&#26469;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15452v1 Announce Type: cross  Abstract: Language models (LMs) are powerful yet mostly for text generation tasks. Tools have substantially enhanced their performance for tasks that require complex skills. However, many works adopt the term "tool" in different ways, raising the question: What is a tool anyway? Subsequently, where and how do tools help LMs? In this survey, we provide a unified definition of tools as external programs used by LMs, and perform a systematic review of LM tooling scenarios and approaches. Grounded on this review, we empirically study the efficiency of various tooling methods by measuring their required compute and performance gains on various benchmarks, and highlight some challenges and potential future research in the field.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#31354;&#38388;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#25506;&#32034;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#35758;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.15451</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;FAIR&#25968;&#25454;&#31354;&#38388;&#30340;&#21487;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Enabling FAIR Dataspaces Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#31354;&#38388;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#25506;&#32034;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#35758;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31354;&#38388;&#26368;&#36817;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#24471;&#21040;&#20102;&#37319;&#29992;&#65292;&#21253;&#25324;&#20256;&#32479;&#19978;&#25968;&#23383;&#21270;&#31243;&#24230;&#36739;&#20302;&#30340;&#39046;&#22495;&#65292;&#22914;&#25991;&#21270;&#39046;&#22495;&#12290;&#21033;&#29992;&#35821;&#20041;&#32593;&#25216;&#26415;&#26377;&#21161;&#20110;&#20351;&#25968;&#25454;&#31354;&#38388;&#26356;&#20855;FAIR&#24615;&#65292;&#20294;&#20854;&#22797;&#26434;&#24615;&#23545;&#25968;&#25454;&#31354;&#38388;&#30340;&#37319;&#29992;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#24182;&#22686;&#21152;&#20102;&#20854;&#25104;&#26412;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#24341;&#20986;&#20102;&#36825;&#26679;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#25903;&#25345;FAIR&#25968;&#25454;&#31354;&#38388;&#30340;&#37319;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#20855;&#20307;&#31034;&#20363;&#23637;&#31034;&#20102;LLMs&#22312;&#25968;&#25454;&#31354;&#38388;&#20013;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#30740;&#31350;&#35758;&#31243;&#65292;&#20197;&#25506;&#35752;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15451v1 Announce Type: new  Abstract: Dataspaces have recently gained adoption across various sectors, including traditionally less digitized domains such as culture. Leveraging Semantic Web technologies helps to make dataspaces FAIR, but their complexity poses a significant challenge to the adoption of dataspaces and increases their cost. The advent of Large Language Models (LLMs) raises the question of how these models can support the adoption of FAIR dataspaces. In this work, we demonstrate the potential of LLMs in dataspaces with a concrete example. We also derive a research agenda for exploring this emerging field.
&lt;/p&gt;</description></item><item><title>LoRAG&#26159;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#36845;&#20195;&#24490;&#29615;&#26426;&#21046;&#25552;&#39640;&#20102;&#26816;&#32034;&#22686;&#24378;&#22411;&#25991;&#26412;&#29983;&#25104;&#30340;&#36136;&#37327;&#65292;&#22312;&#23454;&#39564;&#35777;&#26126;&#22312;&#29983;&#25104;&#30340;&#25991;&#26412;&#36830;&#36143;&#24615;&#21644;&#30456;&#20851;&#24615;&#26041;&#38754;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.15450</link><description>&lt;p&gt;
Loops On Retrieval Augmented Generation (LoRAG)
&lt;/p&gt;
&lt;p&gt;
Loops On Retrieval Augmented Generation (LoRAG)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15450
&lt;/p&gt;
&lt;p&gt;
LoRAG&#26159;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#36845;&#20195;&#24490;&#29615;&#26426;&#21046;&#25552;&#39640;&#20102;&#26816;&#32034;&#22686;&#24378;&#22411;&#25991;&#26412;&#29983;&#25104;&#30340;&#36136;&#37327;&#65292;&#22312;&#23454;&#39564;&#35777;&#26126;&#22312;&#29983;&#25104;&#30340;&#25991;&#26412;&#36830;&#36143;&#24615;&#21644;&#30456;&#20851;&#24615;&#26041;&#38754;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Loops On Retrieval Augmented Generation (LoRAG)&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#36845;&#20195;&#24490;&#29615;&#26426;&#21046;&#26469;&#25552;&#39640;&#26816;&#32034;&#22686;&#24378;&#22411;&#25991;&#26412;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#35813;&#26550;&#26500;&#38598;&#25104;&#20102;&#29983;&#25104;&#27169;&#22411;&#12289;&#26816;&#32034;&#26426;&#21046;&#21644;&#21160;&#24577;&#24490;&#29615;&#27169;&#22359;&#65292;&#20801;&#35768;&#36890;&#36807;&#19982;&#20174;&#36755;&#20837;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#30340;&#30456;&#20851;&#20449;&#24687;&#36827;&#34892;&#20132;&#20114;&#26469;&#23545;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;LoRAG&#22312;BLEU&#20998;&#25968;&#12289;ROUGE&#20998;&#25968;&#21644;&#22256;&#24785;&#24230;&#26041;&#38754;&#22343;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#25991;&#26412;&#30340;&#36830;&#36143;&#24615;&#21644;&#30456;&#20851;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23450;&#24615;&#35780;&#20272;&#36827;&#19968;&#27493;&#35828;&#26126;&#20102;LoRAG&#20135;&#29983;&#19978;&#19979;&#25991;&#20016;&#23500;&#19988;&#36830;&#36143;&#30340;&#36755;&#20986;&#30340;&#33021;&#21147;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#36845;&#20195;&#24490;&#29615;&#22312;&#32531;&#35299;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#25361;&#25112;&#26041;&#38754;&#30340;&#28508;&#21147;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15450v1 Announce Type: new  Abstract: This paper presents Loops On Retrieval Augmented Generation (LoRAG), a new framework designed to enhance the quality of retrieval-augmented text generation through the incorporation of an iterative loop mechanism. The architecture integrates a generative model, a retrieval mechanism, and a dynamic loop module, allowing for iterative refinement of the generated text through interactions with relevant information retrieved from the input context. Experimental evaluations on benchmark datasets demonstrate that LoRAG surpasses existing state-of-the-art models in terms of BLEU score, ROUGE score, and perplexity, showcasing its effectiveness in achieving both coherence and relevance in generated text. The qualitative assessment further illustrates LoRAG's capability to produce contextually rich and coherent outputs. This research contributes valuable insights into the potential of iterative loops in mitigating challenges in text generation, po
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#30740;&#31350;&#20102;&#23545;&#25239;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#23545;&#35805;&#20013;&#30340;&#29702;&#30001;&#12289;&#24773;&#24863;&#21644;&#20449;&#35465;&#31561;&#35828;&#26381;&#26041;&#24335;&#65292;&#23545;&#27604;&#23553;&#38381;&#21644;&#24320;&#25918;&#20132;&#20114;&#20013;&#30340;&#19981;&#21516;&#34892;&#20026;&#21644;&#35805;&#39064;&#23618;&#38754;&#65292;&#21457;&#29616;&#20102;&#22312;&#23545;&#25239;&#35328;&#35770;&#20013;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.15449</link><description>&lt;p&gt;
&#24974;&#24680;&#28304;&#20110;&#26080;&#30693;&#65281;&#23545;&#25239;&#20250;&#35805;&#24615;&#20167;&#24680;&#35328;&#35770;&#20013;&#35828;&#26381;&#26041;&#24335;&#30340;&#25552;&#28860;
&lt;/p&gt;
&lt;p&gt;
Hatred Stems from Ignorance! Distillation of the Persuasion Modes in Countering Conversational Hate Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15449
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#30740;&#31350;&#20102;&#23545;&#25239;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#23545;&#35805;&#20013;&#30340;&#29702;&#30001;&#12289;&#24773;&#24863;&#21644;&#20449;&#35465;&#31561;&#35828;&#26381;&#26041;&#24335;&#65292;&#23545;&#27604;&#23553;&#38381;&#21644;&#24320;&#25918;&#20132;&#20114;&#20013;&#30340;&#19981;&#21516;&#34892;&#20026;&#21644;&#35805;&#39064;&#23618;&#38754;&#65292;&#21457;&#29616;&#20102;&#22312;&#23545;&#25239;&#35328;&#35770;&#20013;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#23545;&#25239;&#35328;&#35770;&#20351;&#29992;&#30340;&#22240;&#32032;&#26159;&#29702;&#35299;&#22312;&#32447;&#23545;&#25239;&#20167;&#24680;&#35328;&#35770;&#30340;&#26368;&#20339;&#26041;&#27861;&#30340;&#26680;&#24515;&#12290;&#21508;&#31181;&#30740;&#31350;&#35780;&#20272;&#23545;&#25239;&#35328;&#35770;&#20013;&#20351;&#29992;&#30340;&#24773;&#24863;&#22522;&#30784;&#22240;&#32032;&#65292;&#22914;&#24773;&#24863;&#20849;&#40483;&#12289;&#20882;&#29359;&#31243;&#24230;&#21644;&#25932;&#24847;&#31243;&#24230;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#20250;&#35805;&#20132;&#20114;&#20013;&#20351;&#29992;&#30340;&#23545;&#25239;&#35328;&#35770;&#65292;&#26412;&#30740;&#31350;&#23558;&#35828;&#26381;&#26041;&#24335;&#20998;&#35299;&#20026;&#29702;&#30001;&#12289;&#24773;&#24863;&#21644;&#20449;&#35465;&#65292;&#28982;&#21518;&#35780;&#20272;&#23427;&#20204;&#22312;&#28041;&#21450;&#31181;&#26063;&#20027;&#20041;&#12289;&#24615;&#21035;&#27495;&#35270;&#21644;&#23447;&#25945;&#38382;&#39064;&#30340;&#20004;&#31181;&#23545;&#35805;&#20132;&#20114;&#31867;&#22411;&#20013;&#30340;&#20351;&#29992;&#12290;&#35780;&#20272;&#28085;&#30422;&#20102;&#20154;&#31867;&#19982;&#29983;&#25104;&#23545;&#25239;&#35328;&#35770;&#30340;&#19981;&#21516;&#34892;&#20026;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#22238;&#22797;&#30340;&#31435;&#22330;&#19982;&#27599;&#31181;&#23545;&#25239;&#35328;&#35770;&#20013;&#30340;&#35828;&#26381;&#26041;&#24335;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#22312;&#24320;&#25918;&#21644;&#23553;&#38381;&#20132;&#20114;&#30340;&#23545;&#25239;&#35328;&#35770;&#35828;&#26381;&#26041;&#24335;&#19978;&#30340;&#24494;&#22937;&#24046;&#24322; -- &#23588;&#20854;&#26159;&#22312;&#35805;&#39064;&#23618;&#38754;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15449v1 Announce Type: cross  Abstract: Examining the factors that the counter-speech uses is at the core of understanding the optimal methods for confronting hate speech online. Various studies assess the emotional base factor used in counter speech, such as emotion-empathy, offensiveness, and level of hostility. To better understand the counter-speech used in conversational interactions, this study distills persuasion modes into reason, emotion, and credibility and then evaluates their use in two types of conversation interactions: closed (multi-turn) and open (single-turn) conversation interactions concerning racism, sexism, and religion. The evaluation covers the distinct behaviors of human versus generated counter-speech. We also assess the interplay between the replies' stance and each mode of persuasion in the counter-speech. Notably, we observe nuanced differences in the counter-speech persuasion modes for open and closed interactions -- especially on the topic level
&lt;/p&gt;</description></item><item><title>&#37327;&#21270;&#30446;&#21069;&#27604;&#21098;&#26525;&#26356;&#26377;&#25928;&#65292;&#21487;&#20197;&#21516;&#26102;&#23454;&#29616;&#25928;&#29575;&#21644;&#21487;&#20449;&#24230;&#65292;&#20294;&#21098;&#26525;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;</title><link>https://arxiv.org/abs/2403.15447</link><description>&lt;p&gt;
&#35299;&#30721;&#21387;&#32553;&#30340;&#20449;&#20219;&#65306;&#23457;&#35270;&#22312;&#21387;&#32553;&#19979;&#39640;&#25928;LLMs&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15447
&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#30446;&#21069;&#27604;&#21098;&#26525;&#26356;&#26377;&#25928;&#65292;&#21487;&#20197;&#21516;&#26102;&#23454;&#29616;&#25928;&#29575;&#21644;&#21487;&#20449;&#24230;&#65292;&#20294;&#21098;&#26525;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#39640;&#24615;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21387;&#32553;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#25512;&#26029;&#30340;&#39318;&#36873;&#31574;&#30053;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#21387;&#32553;&#26041;&#27861;&#22312;&#20445;&#30041;&#33391;&#24615;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20294;&#21387;&#32553;&#22312;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#26041;&#38754;&#30340;&#28508;&#22312;&#39118;&#38505;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#35270;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20351;&#29992;&#20116;&#31181;&#26368;&#20808;&#36827;&#21387;&#32553;&#25216;&#26415;&#35780;&#20272;&#19977;&#31181;&#39046;&#20808;LLMs&#30340;&#21487;&#20449;&#24230;&#32500;&#24230;&#36827;&#34892;&#20102;&#39318;&#27425;&#24443;&#24213;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#31361;&#20986;&#20102;&#21387;&#32553;&#19982;&#21487;&#20449;&#24230;&#20043;&#38388;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30446;&#21069;&#37327;&#21270;&#27604;&#21098;&#26525;&#26356;&#26377;&#25928;&#22320;&#21516;&#26102;&#23454;&#29616;&#25928;&#29575;&#21644;&#21487;&#20449;&#24230;&#12290;&#20363;&#22914;&#65292;4&#20301;&#37327;&#21270;&#27169;&#22411;&#20445;&#30041;&#20102;&#20854;&#21407;&#22987;&#23545;&#24212;&#29289;&#30340;&#21487;&#20449;&#24230;&#65292;&#20294;&#27169;&#22411;&#21098;&#26525;&#26174;&#33879;&#38477;&#20302;&#20102;&#21487;&#20449;&#24230;&#65292;&#21363;&#20351;&#22312;50%&#30340;&#31232;&#30095;&#24230;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15447v1 Announce Type: cross  Abstract: Compressing high-capability Large Language Models (LLMs) has emerged as a favored strategy for resource-efficient inferences. While state-of-the-art (SoTA) compression methods boast impressive advancements in preserving benign task performance, the potential risks of compression in terms of safety and trustworthiness have been largely neglected. This study conducts the first, thorough evaluation of three (3) leading LLMs using five (5) SoTA compression techniques across eight (8) trustworthiness dimensions. Our experiments highlight the intricate interplay between compression and trustworthiness, revealing some interesting patterns. We find that quantization is currently a more effective approach than pruning in achieving efficiency and trustworthiness simultaneously. For instance, a 4-bit quantized model retains the trustworthiness of its original counterpart, but model pruning significantly degrades trustworthiness, even at 50% spars
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;ARIMA&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#21644;LDA/HDP&#27169;&#22411;&#25552;&#21462;&#22810;&#35821;&#35328;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20027;&#39064;&#21160;&#24577;&#65292;&#29305;&#21035;&#20851;&#27880;&#22312;&#21361;&#26426;&#26399;&#38388;&#30340;&#20132;&#27969;&#36235;&#21183;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#35821;&#35328;&#19968;&#33268;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.15445</link><description>&lt;p&gt;
&#36890;&#36807;ARIMA&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#25581;&#31034;&#31038;&#20132;&#32593;&#32476;&#19978;&#30340;&#22810;&#35821;&#35328;&#20027;&#39064;&#21160;&#24577;&#21644;&#36235;&#21183;&#35782;&#21035;&#65306;LDA/HDP&#27169;&#22411;&#22686;&#24378;&#30340;&#26032;&#22411;&#25968;&#25454;&#32763;&#35793;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Decoding Multilingual Topic Dynamics and Trend Identification through ARIMA Time Series Analysis on Social Networks: A Novel Data Translation Framework Enhanced by LDA/HDP Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15445
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;ARIMA&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#21644;LDA/HDP&#27169;&#22411;&#25552;&#21462;&#22810;&#35821;&#35328;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20027;&#39064;&#21160;&#24577;&#65292;&#29305;&#21035;&#20851;&#27880;&#22312;&#21361;&#26426;&#26399;&#38388;&#30340;&#20132;&#27969;&#36235;&#21183;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#35821;&#35328;&#19968;&#33268;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#30772;&#35793;&#22810;&#35821;&#35328;&#20027;&#39064;&#21160;&#24577;&#65292;&#24182;&#35782;&#21035;&#21361;&#26426;&#26399;&#38388;&#30340;&#20132;&#27969;&#36235;&#21183;&#12290;&#25105;&#20204;&#20851;&#27880;&#31361;&#23612;&#26031;&#31038;&#20132;&#32593;&#32476;&#20013;&#22312;&#20896;&#29366;&#30149;&#27602;&#22823;&#27969;&#34892;&#26399;&#38388;&#20197;&#21450;&#20854;&#20182;&#26174;&#33879;&#20027;&#39064;&#65288;&#22914;&#20307;&#32946;&#21644;&#25919;&#27835;&#65289;&#20013;&#30340;&#23545;&#35805;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#19982;&#36825;&#20123;&#20027;&#39064;&#30456;&#20851;&#30340;&#21508;&#31181;&#22810;&#35821;&#35328;&#35780;&#35770;&#36827;&#34892;&#32858;&#21512;&#12290;&#28982;&#21518;&#22312;&#25968;&#25454;&#39044;&#22788;&#29702;&#36807;&#31243;&#20013;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#31934;&#28860;&#12290;&#25105;&#20204;&#24341;&#20837;&#25105;&#20204;&#30340;&#26080;&#33521;&#35821;&#21040;&#33521;&#35821;&#30340;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#26469;&#22788;&#29702;&#35821;&#35328;&#24046;&#24322;&#12290;&#23545;&#36825;&#31181;&#26041;&#27861;&#30340;&#23454;&#35777;&#27979;&#35797;&#26174;&#31034;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;F1&#20540;&#65292;&#31361;&#26174;&#20102;&#23427;&#36866;&#29992;&#20110;&#35821;&#35328;&#19968;&#33268;&#20219;&#21153;&#30340;&#29305;&#28857;&#12290;&#28145;&#20837;&#30740;&#31350;&#65292;&#37319;&#29992;&#20102;&#20808;&#36827;&#30340;&#24314;&#27169;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;LDA&#21644;HDP&#27169;&#22411;&#65292;&#20174;&#32763;&#35793;&#20869;&#23481;&#20013;&#25552;&#21462;&#30456;&#20851;&#20027;&#39064;&#12290;&#36825;&#23548;&#33268;&#24212;&#29992;ARIMA&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26469;&#35299;&#30721;&#19981;&#26029;&#21464;&#21270;&#30340;&#20027;&#39064;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15445v1 Announce Type: cross  Abstract: In this study, the authors present a novel methodology adept at decoding multilingual topic dynamics and identifying communication trends during crises. We focus on dialogues within Tunisian social networks during the Coronavirus Pandemic and other notable themes like sports and politics. We start by aggregating a varied multilingual corpus of comments relevant to these subjects. This dataset undergoes rigorous refinement during data preprocessing. We then introduce our No-English-to-English Machine Translation approach to handle linguistic differences. Empirical tests of this method showed high accuracy and F1 scores, highlighting its suitability for linguistically coherent tasks. Delving deeper, advanced modeling techniques, specifically LDA and HDP models are employed to extract pertinent topics from the translated content. This leads to applying ARIMA time series analysis to decode evolving topic trends. Applying our method to a mu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#26512;&#21335;&#32654;&#35821;&#35328;&#25299;&#25169;&#24418;&#29366;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#24212;&#29992;&#20102;&#22810;&#37325;&#23545;&#24212;&#20998;&#26512;&#25216;&#26415;&#21644;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.15440</link><description>&lt;p&gt;
&#20174;&#25299;&#25169;&#23398;&#35282;&#24230;&#30475;&#35821;&#35328;&#23398;
&lt;/p&gt;
&lt;p&gt;
Linguistics from a topological viewpoint
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#26512;&#21335;&#32654;&#35821;&#35328;&#25299;&#25169;&#24418;&#29366;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#24212;&#29992;&#20102;&#22810;&#37325;&#23545;&#24212;&#20998;&#26512;&#25216;&#26415;&#21644;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#23398;&#20013;&#30340;&#31867;&#22411;&#25968;&#25454;&#24211;&#36890;&#24120;&#26159;&#20998;&#31867;&#20540;&#30340;&#12290;&#22240;&#27492;&#65292;&#24456;&#38590;&#28165;&#26224;&#22320;&#21487;&#35270;&#21270;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#24037;&#20316;&#27969;&#31243;&#65292;&#36890;&#36807;&#24212;&#29992;&#22810;&#37325;&#23545;&#24212;&#20998;&#26512;&#25216;&#26415;&#21644;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#26469;&#20998;&#26512;&#21335;&#32654;&#35821;&#35328;&#30340;&#25299;&#25169;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15440v1 Announce Type: new  Abstract: Typological databases in linguistics are usually categorical-valued. As a result, it is difficult to have a clear visualization of the data. In this paper, we describe a workflow to analyze the topological shapes of South American languages by applying multiple correspondence analysis technique and topological data analysis methods.
&lt;/p&gt;</description></item><item><title>&#23558;&#24418;&#24577;&#32032;&#20998;&#21106;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#24207;&#21015;&#21040;&#24207;&#21015;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22810;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20986;&#20248;&#24322;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;&#39640;&#36164;&#28304;&#35821;&#35328;&#29615;&#22659;&#19979;&#30340;&#21487;&#27604;&#25928;&#21147;&#65292;&#20197;&#21450;&#20302;&#36164;&#28304;&#35821;&#35328;&#22330;&#26223;&#19979;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15436</link><description>&lt;p&gt;
&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#36827;&#34892;&#21477;&#23376;&#32423;&#24418;&#24577;&#32032;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Using Contextual Information for Sentence-level Morpheme Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15436
&lt;/p&gt;
&lt;p&gt;
&#23558;&#24418;&#24577;&#32032;&#20998;&#21106;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#24207;&#21015;&#21040;&#24207;&#21015;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22810;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20986;&#20248;&#24322;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;&#39640;&#36164;&#28304;&#35821;&#35328;&#29615;&#22659;&#19979;&#30340;&#21487;&#27604;&#25928;&#21147;&#65292;&#20197;&#21450;&#20302;&#36164;&#28304;&#35821;&#35328;&#22330;&#26223;&#19979;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24418;&#24577;&#32032;&#20998;&#21106;&#30340;&#21457;&#23637;&#20027;&#35201;&#24378;&#35843;&#21333;&#35789;&#32423;&#21035;&#30340;&#20998;&#21106;&#65292;&#36890;&#24120;&#24573;&#35270;&#20102;&#21477;&#23376;&#20869;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#24418;&#24577;&#32032;&#20998;&#21106;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#38382;&#39064;&#65292;&#23558;&#25972;&#20010;&#21477;&#23376;&#20316;&#20026;&#36755;&#20837;&#65292;&#32780;&#19981;&#26159;&#23396;&#31435;&#22320;&#22788;&#29702;&#21333;&#20010;&#21333;&#35789;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22810;&#35821;&#35328;&#27169;&#22411;&#19982;&#21333;&#35821;&#27169;&#22411;&#30456;&#27604;&#22987;&#32456;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#27169;&#22411;&#27809;&#26377;&#36229;&#36234;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#20294;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#23637;&#29616;&#20986;&#21487;&#27604;&#36739;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#22330;&#26223;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15436v1 Announce Type: new  Abstract: Recent advancements in morpheme segmentation primarily emphasize word-level segmentation, often neglecting the contextual relevance within the sentence. In this study, we redefine the morpheme segmentation task as a sequence-to-sequence problem, treating the entire sentence as input rather than isolating individual words. Our findings reveal that the multilingual model consistently exhibits superior performance compared to monolingual counterparts. While our model did not surpass the performance of the current state-of-the-art, it demonstrated comparable efficacy with high-resource languages while revealing limitations in low-resource language scenarios.
&lt;/p&gt;</description></item><item><title>ChatPattern&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#28789;&#27963;&#30340;&#24067;&#23616;&#27169;&#24335;&#23450;&#21046;&#65292;&#33021;&#22815;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#35201;&#27714;&#29983;&#25104;&#39640;&#36136;&#37327;&#22823;&#35268;&#27169;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.15434</link><description>&lt;p&gt;
ChatPattern: &#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#24067;&#23616;&#27169;&#24335;&#23450;&#21046;
&lt;/p&gt;
&lt;p&gt;
ChatPattern: Layout Pattern Customization via Natural Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15434
&lt;/p&gt;
&lt;p&gt;
ChatPattern&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#28789;&#27963;&#30340;&#24067;&#23616;&#27169;&#24335;&#23450;&#21046;&#65292;&#33021;&#22815;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#35201;&#27714;&#29983;&#25104;&#39640;&#36136;&#37327;&#22823;&#35268;&#27169;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20316;&#21697;&#20027;&#35201;&#38598;&#20013;&#22312;&#29983;&#25104;&#22266;&#23450;&#22823;&#23567;&#30340;&#24067;&#23616;&#27169;&#24335;&#65292;&#32780;&#26356;&#23454;&#29992;&#30340;&#33258;&#30001;&#22823;&#23567;&#27169;&#24335;&#29983;&#25104;&#21463;&#21040;&#20102;&#26377;&#38480;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChatPattern&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#28789;&#27963;&#30340;&#27169;&#24335;&#23450;&#21046;&#12290;ChatPattern&#21033;&#29992;&#19968;&#20010;&#21253;&#21547;&#19987;&#23478;LLM&#20195;&#29702;&#21644;&#19968;&#20010;&#39640;&#24230;&#21487;&#25511;&#30340;&#24067;&#23616;&#27169;&#24335;&#29983;&#25104;&#22120;&#30340;&#21452;&#37096;&#20998;&#31995;&#32479;&#12290;LLM&#20195;&#29702;&#21487;&#20197;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#35201;&#27714;&#24182;&#25805;&#20316;&#35774;&#35745;&#24037;&#20855;&#20197;&#28385;&#36275;&#25351;&#23450;&#38656;&#27714;&#65292;&#32780;&#29983;&#25104;&#22120;&#25797;&#38271;&#20110;&#26465;&#20214;&#24067;&#23616;&#29983;&#25104;&#12289;&#27169;&#24335;&#20462;&#25913;&#21644;&#20869;&#23384;&#21451;&#22909;&#22411;&#27169;&#24335;&#25193;&#23637;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27169;&#24335;&#29983;&#25104;&#35774;&#32622;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;ChatPattern&#21512;&#25104;&#39640;&#36136;&#37327;&#22823;&#35268;&#27169;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15434v1 Announce Type: cross  Abstract: Existing works focus on fixed-size layout pattern generation, while the more practical free-size pattern generation receives limited attention. In this paper, we propose ChatPattern, a novel Large-Language-Model (LLM) powered framework for flexible pattern customization. ChatPattern utilizes a two-part system featuring an expert LLM agent and a highly controllable layout pattern generator. The LLM agent can interpret natural language requirements and operate design tools to meet specified needs, while the generator excels in conditional layout generation, pattern modification, and memory-friendly patterns extension. Experiments on challenging pattern generation setting shows the ability of ChatPattern to synthesize high-quality large-scale patterns.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20174;GPT-4&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#25105;&#20204;&#20026;&#28626;&#21361;&#29289;&#31181;&#21019;&#24314;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#20445;&#25252;&#29983;&#29289;&#22810;&#26679;&#24615;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2403.15430</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#28860;&#28626;&#21361;&#29289;&#31181;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Distilling Named Entity Recognition Models for Endangered Species from Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15430
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;GPT-4&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#25105;&#20204;&#20026;&#28626;&#21361;&#29289;&#31181;&#21019;&#24314;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#20445;&#25252;&#29983;&#29289;&#22810;&#26679;&#24615;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20174;&#19994;&#32773;&#27491;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20174;&#19987;&#21033;&#12289;&#35770;&#25991;&#21644;&#35770;&#25991;&#31561;&#21322;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#28304;&#21019;&#24314;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#65292;&#32780;&#26080;&#38656;&#20855;&#22791;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#29983;&#24577;&#19987;&#23478;&#27491;&#22312;&#23547;&#25214;&#21508;&#31181;&#25163;&#27573;&#26469;&#20445;&#25252;&#29983;&#29289;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#20026;&#36825;&#20123;&#21162;&#21147;&#20570;&#20986;&#36129;&#29486;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#28626;&#21361;&#29289;&#31181;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#20174;GPT-4&#20013;&#25552;&#28860;&#30693;&#35782;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#38454;&#27573;&#36807;&#31243;&#21019;&#24314;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#30340;&#25968;&#25454;&#38598;&#65306;1&#65289;&#25105;&#20204;&#20174;GPT-4&#20013;&#29983;&#25104;&#20102;&#22235;&#31867;&#28626;&#21361;&#29289;&#31181;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;2&#65289;&#20154;&#31867;&#39564;&#35777;&#20102;&#21512;&#25104;&#25968;&#25454;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#26368;&#32456;&#29983;&#25104;&#37329;&#26631;&#25968;&#25454;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#30340;&#26032;&#39062;&#25968;&#25454;&#38598;&#21253;&#21547;&#20849;3.6K&#20010;&#21477;&#23376;&#65292;&#22343;&#20998;&#20026;1.8K&#20010;NER&#21477;&#23376;&#21644;1.8K&#20010;RE&#21477;&#23376;&#12290;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#38543;&#21518;&#29992;&#20110;&#23545;bo&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15430v1 Announce Type: new  Abstract: Natural language processing (NLP) practitioners are leveraging large language models (LLM) to create structured datasets from semi-structured and unstructured data sources such as patents, papers, and theses, without having domain-specific knowledge. At the same time, ecological experts are searching for a variety of means to preserve biodiversity. To contribute to these efforts, we focused on endangered species and through in-context learning, we distilled knowledge from GPT-4. In effect, we created datasets for both named entity recognition (NER) and relation extraction (RE) via a two-stage process: 1) we generated synthetic data from GPT-4 of four classes of endangered species, 2) humans verified the factual accuracy of the synthetic data, resulting in gold data. Eventually, our novel dataset contains a total of 3.6K sentences, evenly divided between 1.8K NER and 1.8K RE sentences. The constructed dataset was then used to fine-tune bo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#19977;&#38454;&#27573;&#30417;&#30563;&#24494;&#35843;&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#39564;&#21644;&#25968;&#25454;&#37325;&#21472;&#20272;&#35745;&#23454;&#29616;&#20102;&#25945;&#32946;&#30693;&#35782;&#30340;&#32467;&#26500;&#25286;&#21368;&#21644;&#22686;&#37327;&#24341;&#23548;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2403.15426</link><description>&lt;p&gt;
&#25945;&#32946;&#29615;&#22659;&#19979;&#38598;&#25104;&#24378;&#20808;&#39564;&#27169;&#22359;&#21644;&#25968;&#25454;&#37325;&#21472;&#20272;&#35745;&#30340;&#19977;&#38454;&#27573;SFT&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Three-Phases SFT Hybrid Model Integrated Strong Prior Module and Data Overlap Estimation in the Eduation Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15426
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#19977;&#38454;&#27573;&#30417;&#30563;&#24494;&#35843;&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#39564;&#21644;&#25968;&#25454;&#37325;&#21472;&#20272;&#35745;&#23454;&#29616;&#20102;&#25945;&#32946;&#30693;&#35782;&#30340;&#32467;&#26500;&#25286;&#21368;&#21644;&#22686;&#37327;&#24341;&#23548;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#22522;&#20110;&#20808;&#39564;&#30340;&#19977;&#38454;&#27573;&#30417;&#30563;&#24494;&#35843;&#27169;&#22411;&#65292;&#35777;&#26126;&#27604;&#20256;&#32479;&#24494;&#35843;&#26041;&#27861;&#26356;&#26377;&#31454;&#20105;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#25945;&#32946;&#30693;&#35782;&#30340;&#32467;&#26500;&#25286;&#21368;&#21644;&#22686;&#37327;&#24341;&#23548;&#36755;&#20986;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#37319;&#26679;&#22120;&#21644;&#37325;&#21472;&#20272;&#35745;&#31070;&#32463;&#32593;&#32476;&#23545;&#19977;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#36827;&#34892;&#20102;&#20581;&#22766;&#30340;&#20998;&#31867;&#65292;&#23558;&#39044;&#22788;&#29702;&#25968;&#25454;&#38598;&#20998;&#19977;&#25209;&#27880;&#20837;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;LORA&#24494;&#35843;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20808;&#39564;&#27169;&#22359;&#65292;&#23558;&#31995;&#32479;&#25552;&#31034;&#12289;&#21521;&#37327;&#25968;&#25454;&#24211;&#21644;&#25277;&#35937;&#35821;&#27861;&#26641;&#20219;&#21153;&#20998;&#21106;&#30456;&#32467;&#21512;&#12290;&#26368;&#21518;&#65292;&#23545;&#22522;&#20110;&#20808;&#39564;&#30340;&#24494;&#35843;&#27169;&#22411;&#24212;&#29992;&#20102;&#21387;&#32553;&#26041;&#27861;&#21644;&#27491;&#21017;&#21270;&#32422;&#26463;&#65292;&#38543;&#21518;&#22312;&#36755;&#20986;&#31471;&#36827;&#34892;&#25991;&#26412;&#36807;&#28388;&#20197;&#33719;&#24471;&#22686;&#37327;&#24341;&#23548;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20195;&#34920;&#20102;&#30495;&#27491;&#20197;&#20016;&#23500;&#30340;&#25945;&#32946;&#30693;&#35782;&#12289;&#20998;&#27493;&#25351;&#23548;&#30340;&#29305;&#28857;&#20307;&#29616;&#23548;&#24072;&#35282;&#33394;&#30340;&#31532;&#19968;&#39033;&#30740;&#31350;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15426v1 Announce Type: cross  Abstract: In this paper, we propose an end-to-end prior-based three-phases supervised fine-tuned model, which is proved more competitive than traditional fine-tuning method. More specifically, our model realizes the structural disassembly and incremental guided output of educational knowledge. To this end, we robustify data classification of three types via a sampler and overlap estimation neural network, and inject the preprocessing datasets into pre-trained model in three batches for LORA fine-tuning. Then, we design a prior module couples system prompt, vector databases, and abstract syntax tree task segmentation. Finally, the compression method and regularization constraint are applied to the prior-based fine-tuned model, followed by text filter at the output end to obtain incremental guided results. Our model represents the first research effort to truly embody the tutor role with the features of abundant educational knowledge, step-by-step
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;39&#31687;&#26368;&#26032;&#35770;&#25991;&#65292;&#26088;&#22312;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#34920;&#36798;&#21644;&#21253;&#23481;&#24615;&#65292;&#21457;&#29616;&#24403;&#21069;&#30740;&#31350;&#26410;&#23545;&#8220;&#25991;&#21270;&#8221;&#36827;&#34892;&#23450;&#20041;&#65292;&#32780;&#26159;&#22312;&#29305;&#23450;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#25506;&#31350;&#65292;&#30740;&#31350;&#20102;&#26576;&#20123;&#8220;&#25991;&#21270;&#8221;&#30340;&#26041;&#38754;&#65292;&#30041;&#19979;&#35768;&#22810;&#26410;&#34987;&#25506;&#31350;&#30340;&#26377;&#36259;&#21644;&#37325;&#35201;&#26041;&#38754;&#65292;&#22914;&#35821;&#20041;&#39046;&#22495;&#21644;&#20851;&#20110;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15412</link><description>&lt;p&gt;
&#22312;LLMs&#20013;&#27979;&#37327;&#21644;&#24314;&#27169;&#8220;&#25991;&#21270;&#8221;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Towards Measuring and Modeling "Culture" in LLMs: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15412
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;39&#31687;&#26368;&#26032;&#35770;&#25991;&#65292;&#26088;&#22312;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#34920;&#36798;&#21644;&#21253;&#23481;&#24615;&#65292;&#21457;&#29616;&#24403;&#21069;&#30740;&#31350;&#26410;&#23545;&#8220;&#25991;&#21270;&#8221;&#36827;&#34892;&#23450;&#20041;&#65292;&#32780;&#26159;&#22312;&#29305;&#23450;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#25506;&#31350;&#65292;&#30740;&#31350;&#20102;&#26576;&#20123;&#8220;&#25991;&#21270;&#8221;&#30340;&#26041;&#38754;&#65292;&#30041;&#19979;&#35768;&#22810;&#26410;&#34987;&#25506;&#31350;&#30340;&#26377;&#36259;&#21644;&#37325;&#35201;&#26041;&#38754;&#65292;&#22914;&#35821;&#20041;&#39046;&#22495;&#21644;&#20851;&#20110;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21576;&#29616;&#20102;&#23545;39&#31687;&#26368;&#26032;&#35770;&#25991;&#30340;&#35843;&#26597;&#65292;&#26088;&#22312;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#34920;&#36798;&#21644;&#21253;&#23481;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#27809;&#26377;&#19968;&#31687;&#30740;&#31350;&#23450;&#20041;&#8220;&#25991;&#21270;&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#12289;&#22810;&#23618;&#38754;&#30340;&#27010;&#24565;&#65307;&#30456;&#21453;&#65292;&#23427;&#20204;&#22312;&#19968;&#20123;&#29305;&#21035;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#25506;&#31350;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20195;&#34920;&#20102;&#26576;&#20123;&#8220;&#25991;&#21270;&#8221;&#30340;&#26041;&#38754;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#38754;&#31216;&#20026;&#25991;&#21270;&#30340;&#20195;&#29702;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#32455;&#22312;&#20154;&#21475;&#32479;&#35745;&#12289;&#35821;&#20041;&#21644;&#35821;&#35328;&#25991;&#21270;&#20132;&#20114;&#20195;&#29702;&#30340;&#19977;&#20010;&#32500;&#24230;&#19978;&#12290;&#25105;&#20204;&#36824;&#23545;&#37319;&#29992;&#30340;&#25506;&#26597;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#21482;&#26377;&#8220;&#25991;&#21270;&#8221;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#22914;&#20215;&#20540;&#35266;&#21644;&#30446;&#26631;&#65292;&#34987;&#30740;&#31350;&#20102;&#65292;&#30041;&#19979;&#20102;&#20960;&#20010;&#20854;&#20182;&#26377;&#36259;&#19988;&#37325;&#35201;&#30340;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#22823;&#37327;&#35821;&#20041;&#39046;&#22495;&#21644;&#20851;&#20110;&#24615;&#65288;Hershcovich&#31561;&#20154;&#65292;2022&#65289;&#30340;&#26410;&#34987;&#25506;&#31350;&#12290;&#21478;&#22806;&#20004;&#20010;&#20851;&#38190;&#30340;&#31354;&#30333;&#26159;&#30446;&#21069;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#24773;&#22659;&#24615;&#30340;&#32570;&#20047;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15412v1 Announce Type: cross  Abstract: We present a survey of 39 recent papers that aim to study cultural representation and inclusion in large language models. We observe that none of the studies define "culture," which is a complex, multifaceted concept; instead, they probe the models on some specially designed datasets which represent certain aspects of "culture." We call these aspects the proxies of cultures, and organize them across three dimensions of demographic, semantic and linguistic-cultural interaction proxies. We also categorize the probing methods employed. Our analysis indicates that only certain aspects of "culture," such as values and objectives, have been studied, leaving several other interesting and important facets, especially the multitude of semantic domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack of robustness and situatedness of the current methods. Based on these observations
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;X-AMR&#27880;&#37322;&#24037;&#20855;&#65292;&#36890;&#36807;&#26426;&#22120;&#36741;&#21161;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#65292;&#23454;&#29616;&#20102;&#23545;&#20851;&#38190;&#20107;&#20214;&#35821;&#20041;&#30340;&#27880;&#37322;&#65292;&#19982;GPT-4&#38598;&#25104;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.15407</link><description>&lt;p&gt;
X-AMR&#27880;&#37322;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
X-AMR Annotation Tool
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15407
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;X-AMR&#27880;&#37322;&#24037;&#20855;&#65292;&#36890;&#36807;&#26426;&#22120;&#36741;&#21161;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#65292;&#23454;&#29616;&#20102;&#23545;&#20851;&#38190;&#20107;&#20214;&#35821;&#20041;&#30340;&#27880;&#37322;&#65292;&#19982;GPT-4&#38598;&#25104;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#36328;&#25991;&#26723;&#25277;&#35937;&#24847;&#20041;&#34920;&#24449;&#65288;X-AMR&#65289;&#27880;&#37322;&#24037;&#20855;&#65292;&#26088;&#22312;&#29992;&#20110;&#27880;&#37322;&#20851;&#38190;&#30340;&#35821;&#26009;&#24211;&#32423;&#20107;&#20214;&#35821;&#20041;&#12290;&#36890;&#36807;Prodigy&#27880;&#37322;&#24037;&#20855;&#25552;&#20379;&#30340;&#26426;&#22120;&#36741;&#21161;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#29992;&#25143;&#20307;&#39564;&#65292;&#30830;&#20445;&#27880;&#37322;&#36807;&#31243;&#30340;&#31616;&#26131;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#24037;&#20855;&#22312;&#22686;&#24378;&#29616;&#26377;&#20107;&#20214;&#35821;&#26009;&#24211;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#31361;&#26174;&#20102;&#20854;&#19982;GPT-4&#38598;&#25104;&#26102;&#30340;&#20248;&#21183;&#12290;&#20195;&#30721;&#21644;&#27880;&#37322;&#65306;https://github.com/ahmeshaf/gpt_coref
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15407v1 Announce Type: cross  Abstract: This paper presents a novel Cross-document Abstract Meaning Representation (X-AMR) annotation tool designed for annotating key corpus-level event semantics. Leveraging machine assistance through the Prodigy Annotation Tool, we enhance the user experience, ensuring ease and efficiency in the annotation process. Through empirical analyses, we demonstrate the effectiveness of our tool in augmenting an existing event corpus, highlighting its advantages when integrated with GPT-4. Code and annotations: https://github.com/ahmeshaf/gpt_coref
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31995;&#32479;&#35780;&#20215;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#20854;&#22312;&#26089;&#26399;&#31579;&#26597;&#12289;&#25968;&#23383;&#24178;&#39044;&#21644;&#20854;&#20182;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;</title><link>https://arxiv.org/abs/2403.15401</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#31995;&#32479;&#35780;&#20215;
&lt;/p&gt;
&lt;p&gt;
Large Language Model for Mental Health: A Systematic Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15401
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31995;&#32479;&#35780;&#20215;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#20854;&#22312;&#26089;&#26399;&#31579;&#26597;&#12289;&#25968;&#23383;&#24178;&#39044;&#21644;&#20854;&#20182;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25968;&#23383;&#20581;&#24247;&#39046;&#22495;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23637;&#29616;&#20986;&#20102;&#28508;&#22312;&#30340;&#24212;&#29992;&#24615;&#65292;&#20294;&#23427;&#20204;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#24212;&#29992;&#20173;&#22312;&#25345;&#32493;&#35752;&#35770;&#20013;&#12290;&#36825;&#39033;&#31995;&#32479;&#24615;&#35780;&#20215;&#26088;&#22312;&#24635;&#32467;&#21644;&#34920;&#24449;LLMs&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#35843;&#26597;LLMs&#26368;&#26032;&#30740;&#31350;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#35752;&#35770;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#26089;&#26399;&#31579;&#26597;&#12289;&#25968;&#23383;&#24178;&#39044;&#20197;&#21450;&#20854;&#20182;&#20020;&#24202;&#24212;&#29992;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#26681;&#25454;PRISMA&#25351;&#21335;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;PubMed&#12289;DBLP&#35745;&#31639;&#26426;&#31185;&#23398;&#25991;&#29486;&#25968;&#25454;&#24211;&#21644;IEEE Xplore&#19978;&#21457;&#34920;&#30340;&#33521;&#25991;&#25991;&#31456;&#65292;&#26102;&#38388;&#36328;&#24230;&#20026;2017&#24180;1&#26376;1&#26085;&#33267;2023&#24180;9&#26376;1&#26085;&#65292;&#37325;&#28857;&#20851;&#27880;&#24515;&#29702;&#20581;&#24247;&#21644;LLMs&#12290;&#35813;&#32508;&#36848;&#20998;&#26512;&#20102;32&#31687;&#25991;&#31456;&#65292;&#21253;&#25324;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#38598;&#36827;&#34892;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#30340;&#65288;n=13&#65289;&#12289;&#24515;&#29702;&#20581;&#24247;&#32842;&#22825;&#26426;&#22120;&#20154;&#65288;n=10&#65289;&#20197;&#21450;&#20854;&#20182;&#24515;&#29702;&#20581;&#24247;&#24212;&#29992;&#65288;n=9&#65289;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;LLMs&#22312;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15401v1 Announce Type: cross  Abstract: Large language models (LLMs) have received much attention and shown their potential in digital health, while their application in mental health is subject to ongoing debate. This systematic review aims to summarize and characterize the use of LLMs in mental health by investigating the strengths and limitations of the latest work in LLMs and discusses the challenges and opportunities for early screening, digital interventions, and other clinical applications in mental health. Following PRISMA guidelines, we examined English articles from PubMed, DBLP Computer Science Bibliography, and IEEE Xplore, published between 1 January 2017, and 1 September 2023, focusing on mental health and LLMs. The review analyzed 32 articles, including mental health analysis using social media datasets (n=13), mental health chatbots (n=10), and other mental health applications (n=9). Findings reveal LLMs' effectiveness in mental health issue detection and the
&lt;/p&gt;</description></item><item><title>ChatGPT&#22312;&#32447;&#24615;&#20195;&#25968;&#20013;&#21462;&#24471;&#24040;&#22823;&#25913;&#36827;&#65292;&#20294;&#30446;&#21069;&#20173;&#28982;&#26080;&#27861;&#23436;&#20840;&#21462;&#20195;&#20154;&#31867;&#25945;&#24072;&#65292;&#36719;&#20214;&#29702;&#35299;&#38382;&#39064;&#30340;&#33021;&#21147;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20854;&#28508;&#21147;&#30340;&#24605;&#32771;&#12290;</title><link>https://arxiv.org/abs/2403.15399</link><description>&lt;p&gt;
ChatGPT&#22312;&#32447;&#24615;&#20195;&#25968;&#20013;&#30340;&#36816;&#29992;&#65306;&#21462;&#24471;&#36827;&#23637;&#65292;&#30041;&#24453;&#36827;&#19968;&#27493;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
ChatGPT in Linear Algebra: Strides Forward, Steps to Go
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15399
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#32447;&#24615;&#20195;&#25968;&#20013;&#21462;&#24471;&#24040;&#22823;&#25913;&#36827;&#65292;&#20294;&#30446;&#21069;&#20173;&#28982;&#26080;&#27861;&#23436;&#20840;&#21462;&#20195;&#20154;&#31867;&#25945;&#24072;&#65292;&#36719;&#20214;&#29702;&#35299;&#38382;&#39064;&#30340;&#33021;&#21147;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20854;&#28508;&#21147;&#30340;&#24605;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#26086;&#26032;&#25216;&#26415;&#20986;&#29616;&#65292;&#25945;&#32946;&#30028;&#23601;&#20250;&#25506;&#32034;&#20854;&#23454;&#29992;&#24615;&#20197;&#21450;&#22312;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#20851;&#20110;&#22522;&#30784;&#32447;&#24615;&#20195;&#25968;&#20027;&#39064;&#30340;ChatGPT&#20250;&#35805;&#12290;&#25105;&#20204;&#21453;&#24605;&#20102;&#36807;&#21435;&#19968;&#24180;&#20869;ChatGPT&#22312;&#25105;&#20204;&#24863;&#20852;&#36259;&#30340;&#39046;&#22495;&#25152;&#36827;&#34892;&#30340;&#36807;&#31243;&#65292;&#24378;&#35843;&#20102;&#22312;&#24212;&#23545;&#32447;&#24615;&#20195;&#25968;&#38382;&#39064;&#19978;&#25152;&#21462;&#24471;&#30340;&#24040;&#22823;&#25913;&#36827;&#12290;&#23588;&#20854;&#26159;&#65292;&#26412;&#25991;&#35752;&#35770;&#20102;&#36825;&#20010;&#36719;&#20214;&#26159;&#21542;&#21487;&#20197;&#20316;&#20026;&#25945;&#23398;&#21161;&#25163;&#65292;&#29978;&#33267;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#21462;&#20195;&#20154;&#31867;&#25945;&#24072;&#30340;&#38382;&#39064;&#12290;&#25130;&#33267;&#26412;&#25991;&#25776;&#20889;&#26102;&#65292;&#31572;&#26696;&#36890;&#24120;&#26159;&#21542;&#23450;&#30340;&#12290;&#23545;&#20110;&#21487;&#20197;&#20026;&#20043;&#31215;&#26497;&#30340;&#26041;&#38754;&#65292;&#32473;&#20986;&#20102;&#19968;&#20123;&#20851;&#20110;&#21407;&#22987;&#24037;&#31243;&#30340;&#21453;&#24605;&#12290;&#19982;&#35813;&#36719;&#20214;&#30340;&#20132;&#27969;&#32473;&#20154;&#19968;&#31181;&#22312;&#19982;&#20154;&#31867;&#20132;&#35848;&#30340;&#21360;&#35937;&#65292;&#26377;&#26102;&#20250;&#20135;&#29983;&#36825;&#26679;&#19968;&#20010;&#38382;&#39064;&#65306;&#36719;&#20214;&#26159;&#21542;&#29702;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#35835;&#32773;&#30340;&#27880;&#24847;&#21147;&#34987;&#24341;&#21521;&#20102;f
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15399v1 Announce Type: cross  Abstract: As soon as a new technology emerges, the education community explores its affordances and the possibilities to apply it in education. In this paper, we analyze sessions with ChatGPT around topics in basic Linear Algebra. We reflect the process undertaken by the ChatGPT along the recent year in our area of interest, emphasising the vast improvement that has been done in grappling with Linear Algebra problems. In particular, the question whether this software can be a teaching assistant or even somehow replace the human teacher, is addressed. As of the time this paper is written, the answer is generally negative. For the small part where the answer can be positive, some reflections about an original instrumental genesis are given.   Communication with the software gives the impression to talk to a human, and sometimes the question is whether the software understands the question or not. Therefore, the reader's attention is drawn to the f
&lt;/p&gt;</description></item><item><title>&#22278;&#26700;&#20250;&#35758;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#27861;&#24459;&#21644;&#25919;&#31574;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#24102;&#26469;&#30340;&#30495;&#23454;&#24615;&#12289;&#38544;&#31169;&#21644;&#24066;&#22330;&#38598;&#20013;&#31561;&#26041;&#38754;&#30340;&#37325;&#35201;&#31038;&#20250;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.15397</link><description>&lt;p&gt;
&#31649;&#25511;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#22278;&#26700;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Regulating Large Language Models: A Roundtable Report
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15397
&lt;/p&gt;
&lt;p&gt;
&#22278;&#26700;&#20250;&#35758;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#27861;&#24459;&#21644;&#25919;&#31574;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#24102;&#26469;&#30340;&#30495;&#23454;&#24615;&#12289;&#38544;&#31169;&#21644;&#24066;&#22330;&#38598;&#20013;&#31561;&#26041;&#38754;&#30340;&#37325;&#35201;&#31038;&#20250;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;2023&#24180;7&#26376;20&#26085;&#65292;&#19968;&#32676;&#20855;&#26377;&#27861;&#24459;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#12289;&#25919;&#27835;&#31185;&#23398;&#31561;&#19987;&#19994;&#30693;&#35782;&#30340;27&#21517;&#23398;&#32773;&#21644;&#25968;&#23383;&#26435;&#21033;&#20513;&#23548;&#32773;&#32858;&#38598;&#22312;&#32445;&#32422;&#22823;&#23398;&#27861;&#23398;&#38498;&#20449;&#24687;&#27861;&#24459;&#30740;&#31350;&#25152;&#21644;&#27665;&#20027;&#19982;&#31185;&#25216;&#20013;&#24515;&#32852;&#21512;&#20030;&#21150;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#27861;&#24459;&#21644;&#25919;&#31574;&#22278;&#26700;&#20250;&#35758;&#19978;&#12290;&#22278;&#26700;&#20250;&#35758;&#26088;&#22312;&#35752;&#35770;&#27861;&#24459;&#21644;&#25919;&#31574;&#22914;&#20309;&#24110;&#21161;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24102;&#26469;&#30340;&#19968;&#20123;&#36739;&#22823;&#31038;&#20250;&#38382;&#39064;&#12290;&#35752;&#35770;&#20027;&#35201;&#38598;&#20013;&#22312;&#19977;&#20010;&#25919;&#31574;&#39046;&#22495;&#65306;1.&#30495;&#23454;&#24615;&#65306;LLMs&#22312;&#29983;&#25104;&#35823;&#20449;&#24687;&#21644;&#20551;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#21738;&#20123;&#39118;&#38505;&#65311;&#20174;&#25216;&#26415;&#21644;/&#25110;&#30417;&#31649;&#30340;&#35282;&#24230;&#22914;&#20309;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#65311;2.&#38544;&#31169;&#65306;&#22312;&#21019;&#24314;&#12289;&#37096;&#32626;&#21644;&#20351;&#29992;LLMs&#36807;&#31243;&#20013;&#28041;&#21450;&#21738;&#20123;&#26368;&#22823;&#30340;&#38544;&#31169;&#39118;&#38505;&#65311;&#22914;&#20309;&#20174;&#25216;&#26415;&#21644;/&#25110;&#30417;&#31649;&#30340;&#35282;&#24230;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#65311;3.&#24066;&#22330;&#38598;&#20013;&#65306;LLMs&#24102;&#26469;&#20102;&#21738;&#20123;&#24066;&#22330;&#38598;&#20013;&#30340;&#23041;&#32961;&#65311;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15397v1 Announce Type: cross  Abstract: On July 20, 2023, a group of 27 scholars and digital rights advocates with expertise in law, computer science, political science, and other disciplines gathered for the Large Language Models, Law and Policy Roundtable, co-hosted by the NYU School of Law's Information Law Institute and the Center for Democracy &amp; Technology. The roundtable convened to discuss how law and policy can help address some of the larger societal problems posed by large language models (LLMs). The discussion focused on three policy topic areas in particular:   1. Truthfulness: What risks do LLMs pose in terms of generating mis- and disinformation? How can these risks be mitigated from a technical and/or regulatory perspective?   2. Privacy: What are the biggest privacy risks involved in the creation, deployment, and use of LLMs? How can these risks be mitigated from a technical and/or regulatory perspective?   3. Market concentration: What threats do LLMs pose c
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20998;&#26512;Reddit&#29992;&#25143;&#24086;&#23376;&#65292;&#26816;&#27979;&#38463;&#29255;&#31867;&#33647;&#29289;&#20351;&#29992;&#32773;&#65292;&#24110;&#21161;&#25913;&#21892;&#23545;&#38463;&#29255;&#31867;&#33647;&#29289;&#21361;&#26426;&#30340;&#30417;&#27979;&#21644;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.15393</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21452;&#21521;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#20174;Reddit&#24086;&#23376;&#20013;&#26816;&#27979;&#38463;&#29255;&#31867;&#33647;&#29289;&#20351;&#29992;&#32773;
&lt;/p&gt;
&lt;p&gt;
Detection of Opioid Users from Reddit Posts via an Attention-based Bidirectional Recurrent Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15393
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20998;&#26512;Reddit&#29992;&#25143;&#24086;&#23376;&#65292;&#26816;&#27979;&#38463;&#29255;&#31867;&#33647;&#29289;&#20351;&#29992;&#32773;&#65292;&#24110;&#21161;&#25913;&#21892;&#23545;&#38463;&#29255;&#31867;&#33647;&#29289;&#21361;&#26426;&#30340;&#30417;&#27979;&#21644;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#29255;&#31867;&#33647;&#29289;&#21361;&#26426;&#25351;&#30340;&#26159;&#22240;&#38463;&#29255;&#31867;&#33647;&#29289;&#36807;&#37327;&#20351;&#29992;&#21644;&#25104;&#30270;&#32780;&#23548;&#33268;&#30340;&#26085;&#30410;&#22686;&#38271;&#30340;&#20303;&#38498;&#21644;&#27515;&#20129;&#26696;&#20363;&#65292;&#24050;&#32463;&#25104;&#20026;&#32654;&#22269;&#20005;&#37325;&#30340;&#20581;&#24247;&#38382;&#39064;&#12290;&#20026;&#24212;&#23545;&#27492;&#21361;&#26426;&#65292;&#32852;&#37030;&#21644;&#22320;&#26041;&#25919;&#24220;&#20197;&#21450;&#21355;&#29983;&#31038;&#21306;&#24050;&#32463;&#21046;&#23450;&#20102;&#35768;&#22810;&#31574;&#30053;&#12290;&#20854;&#20013;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#20581;&#24247;&#30417;&#27979;&#26469;&#25552;&#39640;&#25105;&#20204;&#23545;&#21361;&#26426;&#30340;&#20102;&#35299;&#26159;&#24403;&#21153;&#20043;&#24613;&#20043;&#19968;&#12290;&#38500;&#20102;&#30452;&#25509;&#27979;&#35797;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20063;&#21487;&#33021;&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#26469;&#26816;&#27979;&#38463;&#29255;&#31867;&#33647;&#29289;&#20351;&#29992;&#32773;&#65292;&#22240;&#20026;&#35768;&#22810;&#38463;&#29255;&#31867;&#33647;&#29289;&#20351;&#29992;&#32773;&#21487;&#33021;&#36873;&#25321;&#19981;&#20570;&#27979;&#35797;&#65292;&#20294;&#21487;&#33021;&#20250;&#21311;&#21517;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#20998;&#20139;&#20182;&#20204;&#30340;&#32463;&#21382;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25910;&#38598;&#24182;&#20998;&#26512;&#20102;&#26469;&#33258;&#27969;&#34892;&#31038;&#20132;&#32593;&#32476;Reddit&#30340;&#29992;&#25143;&#24086;&#23376;&#65292;&#20197;&#30830;&#23450;&#38463;&#29255;&#31867;&#33647;&#29289;&#20351;&#29992;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15393v1 Announce Type: new  Abstract: The opioid epidemic, referring to the growing hospitalizations and deaths because of overdose of opioid usage and addiction, has become a severe health problem in the United States. Many strategies have been developed by the federal and local governments and health communities to combat this crisis. Among them, improving our understanding of the epidemic through better health surveillance is one of the top priorities. In addition to direct testing, machine learning approaches may also allow us to detect opioid users by analyzing data from social media because many opioid users may choose not to do the tests but may share their experiences on social media anonymously. In this paper, we take advantage of recent advances in machine learning, collect and analyze user posts from a popular social network Reddit with the goal to identify opioid users. Posts from more than 1,000 users who have posted on three sub-reddits over a period of one mon
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#33014;&#22218;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#27874;&#26031;&#35821;&#25512;&#25991;&#20013;&#26816;&#27979;&#19982;&#33258;&#26432;&#30456;&#20851;&#30340;&#24515;&#29702;&#21387;&#21147;&#28304;&#65292;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15391</link><description>&lt;p&gt;
CapsF: &#29992;&#20110;&#20174; Twitter &#20013;&#25552;&#21462;&#33258;&#26432;&#24515;&#29702;&#21387;&#21147;&#28304;&#30340;&#33014;&#22218;&#34701;&#21512;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
CapsF: Capsule Fusion for Extracting psychiatric stressors for suicide from twitter
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15391
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#33014;&#22218;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#27874;&#26031;&#35821;&#25512;&#25991;&#20013;&#26816;&#27979;&#19982;&#33258;&#26432;&#30456;&#20851;&#30340;&#24515;&#29702;&#21387;&#21147;&#28304;&#65292;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38500;&#30284;&#30151;&#12289;&#34880;&#21387;&#12289;&#20132;&#36890;&#20107;&#25925;&#21644;&#20013;&#39118;&#31561;&#22240;&#32032;&#22806;&#65292;&#33258;&#26432;&#19968;&#30452;&#26159;&#20234;&#26391;&#20027;&#35201;&#27515;&#22240;&#20043;&#19968;&#12290;&#33258;&#26432;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#26159;&#24515;&#29702;&#21387;&#21147;&#28304;&#12290;&#22312;&#22788;&#20110;&#39118;&#38505;&#20154;&#32676;&#20013;&#35782;&#21035;&#24515;&#29702;&#21387;&#21147;&#28304;&#26377;&#21161;&#20110;&#26089;&#26399;&#39044;&#38450;&#33258;&#26432;&#21644;&#33258;&#26432;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#33258;&#21160;&#21270;&#26041;&#27861;&#29992;&#20110;&#20174; Twitter &#25552;&#21462;&#24515;&#29702;&#21387;&#21147;&#28304;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#38754;&#21521;&#38750;&#27874;&#26031;&#35821;&#35328;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20174;&#27874;&#26031;&#35821;&#25512;&#25991;&#20013;&#26816;&#27979;&#19982;&#33258;&#26432;&#30456;&#20851;&#30340;&#24515;&#29702;&#21387;&#21147;&#28304;&#25216;&#26415;&#12290;&#25552;&#20986;&#30340;&#22522;&#20110;&#33014;&#22218;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;0.83&#30340;&#20108;&#36827;&#21046;&#20998;&#32423;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15391v1 Announce Type: new  Abstract: Along with factors such as cancer, blood pressure, street accidents and stroke, suicide has been one of Iran main causes of death. One of the main reasons for suicide is psychological stressors. Identifying psychological stressors in an at risk population can help in the early prevention of suicidal and suicidal behaviours. In recent years, the widespread popularity and flow of real time information sharing of social media have allowed for potential early intervention in large scale and even small scale populations. However, some automated approaches to extract psychiatric stressors from Twitter have been presented, but most of this research has been for non Persian languages. This study aims to investigate the techniques of detecting psychological stress related to suicide from Persian tweets using learning based methods. The proposed capsule based approach achieved a binary classification accuracy of 0.83.
&lt;/p&gt;</description></item><item><title>PruMerge&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#35270;&#35273;&#20196;&#29260;&#20943;&#23569;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#20196;&#29260;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.15388</link><description>&lt;p&gt;
LLaVA-PruMerge: &#33258;&#36866;&#24212;&#20196;&#29260;&#20943;&#23569;&#29992;&#20110;&#39640;&#25928;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15388
&lt;/p&gt;
&lt;p&gt;
PruMerge&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#35270;&#35273;&#20196;&#29260;&#20943;&#23569;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#20196;&#29260;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#36890;&#36807;&#36830;&#25509;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;LMMs&#21253;&#25324;&#20102;&#26356;&#22797;&#26434;&#30340;&#35270;&#35273;&#36755;&#20837;&#65292;&#22914;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#35270;&#39057;&#65292;&#36825;&#26174;&#33879;&#22686;&#21152;&#20102;&#35270;&#35273;&#20196;&#29260;&#30340;&#25968;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#20196;&#29260;&#20943;&#23569;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#31867;&#20284;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#35768;&#22810;&#35270;&#35273;&#20196;&#29260;&#22312;&#31354;&#38388;&#19978;&#26159;&#20887;&#20313;&#30340;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PruMerge&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#35270;&#35273;&#20196;&#29260;&#20943;&#23569;&#26041;&#27861;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35270;&#35273;&#20196;&#29260;&#30340;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#21487;&#27604;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15388v1 Announce Type: cross  Abstract: Large Multimodal Models (LMMs) have shown significant reasoning capabilities by connecting a visual encoder and a large language model. LMMs typically use a fixed amount of visual tokens, such as the penultimate layer features in the CLIP visual encoder, as the prefix content. Recent LMMs incorporate more complex visual inputs, such as high-resolution images and videos, which increase the number of visual tokens significantly. However, due to the design of the Transformer architecture, computational costs associated with these models tend to increase quadratically with the number of input tokens. To tackle this problem, we explore a token reduction mechanism and find, similar to prior work, that many visual tokens are spatially redundant. Based on this, we propose PruMerge, a novel adaptive visual token reduction approach, which largely reduces the number of visual tokens while maintaining comparable model performance. We first select 
&lt;/p&gt;</description></item><item><title>&#27700;&#21360;&#39046;&#22495;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#25915;&#20987;&#32773;&#26080;&#27861;&#35775;&#38382;&#27700;&#21360;&#27169;&#22411;&#25110;&#26816;&#27979;API&#30340;&#24773;&#20917;&#19979;&#65292;&#27700;&#21360;&#22522;&#30784;&#30340;AI&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#22120;&#20063;&#26080;&#27861;&#25269;&#25239;&#23545;&#25239;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.15365</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#22270;&#20687;&#27700;&#21360;&#30340;&#36716;&#31227;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
A Transfer Attack to Image Watermarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15365
&lt;/p&gt;
&lt;p&gt;
&#27700;&#21360;&#39046;&#22495;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#25915;&#20987;&#32773;&#26080;&#27861;&#35775;&#38382;&#27700;&#21360;&#27169;&#22411;&#25110;&#26816;&#27979;API&#30340;&#24773;&#20917;&#19979;&#65292;&#27700;&#21360;&#22522;&#30784;&#30340;AI&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#22120;&#20063;&#26080;&#27861;&#25269;&#25239;&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#21360;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#19994;&#39046;&#22495;&#65292;&#29992;&#20110;&#26816;&#27979;&#30001;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#25991;&#29486;&#20013;&#23545;&#36825;&#31181;&#22522;&#20110;&#27700;&#21360;&#30340;&#26816;&#27979;&#22120;&#22312;&#30333;&#30418;&#21644;&#40657;&#30418;&#29615;&#22659;&#19979;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#26377;&#24456;&#22909;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#26080;&#30418;&#29615;&#22659;&#19979;&#30340;&#31283;&#20581;&#24615;&#21364;&#30693;&#20043;&#29978;&#23569;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22810;&#39033;&#30740;&#31350;&#22768;&#31216;&#22270;&#20687;&#27700;&#21360;&#22312;&#36825;&#31181;&#29615;&#22659;&#19979;&#26159;&#31283;&#20581;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36716;&#31227;&#23545;&#25239;&#25915;&#20987;&#26469;&#38024;&#23545;&#26080;&#30418;&#29615;&#22659;&#19979;&#30340;&#22270;&#20687;&#27700;&#21360;&#12290;&#25105;&#20204;&#30340;&#36716;&#31227;&#25915;&#20987;&#21521;&#24102;&#27700;&#21360;&#30340;&#22270;&#20687;&#28155;&#21152;&#24494;&#25200;&#65292;&#20197;&#36530;&#36991;&#34987;&#25915;&#20987;&#32773;&#35757;&#32451;&#30340;&#22810;&#20010;&#26367;&#20195;&#27700;&#21360;&#27169;&#22411;&#65292;&#24182;&#19988;&#32463;&#36807;&#25200;&#21160;&#30340;&#24102;&#27700;&#21360;&#22270;&#20687;&#20063;&#33021;&#36530;&#36991;&#30446;&#26631;&#27700;&#21360;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#29702;&#35770;&#19978;&#21644;&#32463;&#39564;&#19978;&#23637;&#31034;&#20102;&#65292;&#22522;&#20110;&#27700;&#21360;&#30340;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#22120;&#21363;&#20351;&#25915;&#20987;&#32773;&#27809;&#26377;&#35775;&#38382;&#27700;&#21360;&#27169;&#22411;&#25110;&#26816;&#27979;API&#65292;&#20063;&#19981;&#20855;&#26377;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15365v1 Announce Type: cross  Abstract: Watermark has been widely deployed by industry to detect AI-generated images. The robustness of such watermark-based detector against evasion attacks in the white-box and black-box settings is well understood in the literature. However, the robustness in the no-box setting is much less understood. In particular, multiple studies claimed that image watermark is robust in such setting. In this work, we propose a new transfer evasion attack to image watermark in the no-box setting. Our transfer attack adds a perturbation to a watermarked image to evade multiple surrogate watermarking models trained by the attacker itself, and the perturbed watermarked image also evades the target watermarking model. Our major contribution is to show that, both theoretically and empirically, watermark-based AI-generated image detector is not robust to evasion attacks even if the attacker does not have access to the watermarking model nor the detection API.
&lt;/p&gt;</description></item><item><title>ChatGPT&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20154;&#31867;&#21270;&#30340;&#23545;&#35805;&#22238;&#22797;&#65292;&#21487;&#38761;&#26032;&#21508;&#34892;&#19994;&#24182;&#25913;&#21464;&#25216;&#26415;&#20114;&#21160;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.14643</link><description>&lt;p&gt;
&#25506;&#31350;ChatGPT&#21450;&#20854;&#23545;&#31038;&#20250;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring ChatGPT and its Impact on Society
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14643
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20154;&#31867;&#21270;&#30340;&#23545;&#35805;&#22238;&#22797;&#65292;&#21487;&#38761;&#26032;&#21508;&#34892;&#19994;&#24182;&#25913;&#21464;&#25216;&#26415;&#20114;&#21160;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24050;&#32463;&#23384;&#22312;&#19968;&#27573;&#26102;&#38388;&#20102;&#65292;&#20294;&#31361;&#28982;&#38388;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#21463;&#21040;&#20102;&#26356;&#22810;&#30340;&#20851;&#27880;&#12290;&#24863;&#35874;&#35895;&#27468;&#12289;&#24494;&#36719;&#12289;&#20803;&#23431;&#23449;&#31561;&#31185;&#25216;&#30028;&#20027;&#35201;&#21697;&#29260;&#30340;&#21019;&#26032;&#12290;&#28982;&#32780;&#65292;OpenAI&#36890;&#36807;&#20854;&#24320;&#21019;&#24615;&#21457;&#26126;ChatGPT&#35302;&#21457;&#20102;&#25353;&#38062;&#12290;ChatGPT&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#33021;&#22815;&#22312;&#23545;&#35805;&#32972;&#26223;&#20013;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#30340;&#22238;&#22797;&#12290;&#23427;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#26469;&#29983;&#25104;&#23545;&#36755;&#20837;&#25991;&#26412;&#30340;&#33258;&#28982;&#35821;&#35328;&#22238;&#22797;&#12290;&#20854;&#24222;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#12289;&#19978;&#19979;&#25991;&#29983;&#25104;&#21644;&#38754;&#21521;&#24320;&#25918;&#22495;&#30340;&#35757;&#32451;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#22810;&#21151;&#33021;&#19988;&#26377;&#25928;&#30340;&#24037;&#20855;&#65292;&#21487;&#24212;&#29992;&#20110;&#20174;&#32842;&#22825;&#26426;&#22120;&#20154;&#21040;&#23458;&#25143;&#26381;&#21153;&#20877;&#21040;&#35821;&#35328;&#32763;&#35793;&#31561;&#24191;&#27867;&#39046;&#22495;&#12290;&#23427;&#20855;&#26377;&#24443;&#24213;&#25913;&#21464;&#21508;&#34892;&#19994;&#24182;&#36716;&#21464;&#25105;&#20204;&#19982;&#25216;&#26415;&#20114;&#21160;&#26041;&#24335;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;ChatGPT&#20063;&#24341;&#21457;&#20102;&#19968;&#20123;&#25285;&#24551;&#65292;&#21253;&#25324;&#36947;&#24503;&#26041;&#38754;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14643v1 Announce Type: cross  Abstract: Artificial intelligence has been around for a while, but suddenly it has received more attention than ever before. Thanks to innovations from companies like Google, Microsoft, Meta, and other major brands in technology. OpenAI, though, has triggered the button with its ground-breaking invention ChatGPT. ChatGPT is a Large Language Model (LLM) based on Transformer architecture that has the ability to generate human-like responses in a conversational context. It uses deep learning algorithms to generate natural language responses to input text. Its large number of parameters, contextual generation, and open-domain training make it a versatile and effective tool for a wide range of applications, from chatbots to customer service to language translation. It has the potential to revolutionize various industries and transform the way we interact with technology. However, the use of ChatGPT has also raised several concerns, including ethical,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;A$^3$T&#26694;&#26550;&#65292;&#36890;&#36807;ActRe&#25552;&#31034;&#20195;&#29702;&#23454;&#29616;&#20102;ReAct&#39118;&#26684;&#20195;&#29702;&#23545;&#20195;&#29702;&#36712;&#36857;&#30340;&#33258;&#20027;&#26631;&#27880;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#26032;&#30340;&#36712;&#36857;&#21512;&#25104;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.14589</link><description>&lt;p&gt;
ReAct&#36935;&#19978;ActRe&#65306;&#23545;&#27604;&#24615;&#33258;&#35757;&#32451;&#20013;&#30340;&#20195;&#29702;&#36712;&#36857;&#33258;&#21160;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14589
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;A$^3$T&#26694;&#26550;&#65292;&#36890;&#36807;ActRe&#25552;&#31034;&#20195;&#29702;&#23454;&#29616;&#20102;ReAct&#39118;&#26684;&#20195;&#29702;&#23545;&#20195;&#29702;&#36712;&#36857;&#30340;&#33258;&#20027;&#26631;&#27880;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#26032;&#30340;&#36712;&#36857;&#21512;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14589v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25991;&#25688;&#65306;&#35821;&#35328;&#20195;&#29702;&#36890;&#36807;&#19982;&#22522;&#30784;&#27169;&#22411;&#25512;&#29702;&#23637;&#31034;&#20102;&#33258;&#20027;&#20915;&#31574;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#36890;&#36807;&#22810;&#27493;&#25512;&#29702;&#21644;&#34892;&#21160;&#36712;&#36857;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#26469;&#35757;&#32451;&#35821;&#35328;&#20195;&#29702;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#36825;&#26679;&#30340;&#36712;&#36857;&#20173;&#38656;&#35201;&#30456;&#24403;&#22823;&#30340;&#20154;&#21147;&#65292;&#26080;&#35770;&#26159;&#36890;&#36807;&#20154;&#24037;&#26631;&#27880;&#36824;&#26159;&#23454;&#26045;&#22810;&#26679;&#21270;&#25552;&#31034;&#26694;&#26550;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;A$^3$T&#65292;&#19968;&#20010;&#20801;&#35768;&#20197;ReAct&#39118;&#26684;&#33258;&#20027;&#27880;&#37322;&#20195;&#29702;&#36712;&#36857;&#30340;&#26694;&#26550;&#12290;&#20854;&#20013;&#24515;&#26159;&#19968;&#20010;ActRe&#25552;&#31034;&#20195;&#29702;&#65292;&#23427;&#35299;&#37322;&#20219;&#24847;&#21160;&#20316;&#30340;&#21407;&#22240;&#12290;&#24403;&#38543;&#26426;&#25277;&#21462;&#22806;&#37096;&#21160;&#20316;&#26102;&#65292;ReAct&#39118;&#26684;&#20195;&#29702;&#21487;&#20197;&#26597;&#35810;ActRe&#20195;&#29702;&#20197;&#33719;&#21462;&#20854;&#25991;&#26412;&#29702;&#30001;&#12290;&#26032;&#39062;&#30340;&#36712;&#36857;&#28982;&#21518;&#36890;&#36807;&#23558;ActRe&#30340;&#21518;&#39564;&#25512;&#29702;&#21069;&#32622;&#21040;&#25277;&#26679;&#21160;&#20316;&#20013;&#36827;&#34892;&#32508;&#21512;&#21512;&#25104;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;ReAct&#39118;&#26684;&#20195;&#29702;&#21487;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14589v1 Announce Type: new  Abstract: Language agents have demonstrated autonomous decision-making abilities by reasoning with foundation models. Recently, efforts have been made to train language agents for performance improvement, with multi-step reasoning and action trajectories as the training data. However, collecting such trajectories still requires considerable human effort, by either artificial annotations or implementations of diverse prompting frameworks. In this work, we propose A$^3$T, a framework that enables the Autonomous Annotation of Agent Trajectories in the style of ReAct. The central role is an ActRe prompting agent, which explains the reason for an arbitrary action. When randomly sampling an external action, the ReAct-style agent could query the ActRe agent with the action to obtain its textual rationales. Novel trajectories are then synthesized by prepending the posterior reasoning from ActRe to the sampled action. In this way, the ReAct-style agent exe
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#38024;&#23545;&#38388;&#25509;&#24773;&#24863;&#34920;&#36798;&#30340;&#38889;&#22269;&#24120;&#35782;&#30693;&#35782;&#22270;&#35889;K-Act2Emo&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20854;&#22312;&#35757;&#32451;&#24773;&#24863;&#25512;&#26029;&#27169;&#22411;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24494;&#35843;&#21518;&#30340;BART&#30693;&#35782;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;&#36798;&#21040;&#20102;&#19982;GPT-4 Turbo&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.14253</link><description>&lt;p&gt;
K-Act2Emo&#65306;&#38024;&#23545;&#38388;&#25509;&#24773;&#24863;&#34920;&#36798;&#30340;&#38889;&#22269;&#24120;&#35782;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
K-Act2Emo: Korean Commonsense Knowledge Graph for Indirect Emotional Expression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14253
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#38024;&#23545;&#38388;&#25509;&#24773;&#24863;&#34920;&#36798;&#30340;&#38889;&#22269;&#24120;&#35782;&#30693;&#35782;&#22270;&#35889;K-Act2Emo&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20854;&#22312;&#35757;&#32451;&#24773;&#24863;&#25512;&#26029;&#27169;&#22411;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24494;&#35843;&#21518;&#30340;BART&#30693;&#35782;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;&#36798;&#21040;&#20102;&#19982;GPT-4 Turbo&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#25991;&#23398;&#20316;&#21697;&#20013;&#65292;&#24773;&#32490;&#36890;&#36807;&#23545;&#34892;&#20026;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#22806;&#34920;&#30340;&#25551;&#36848;&#38388;&#25509;&#20256;&#36798;&#65292;&#38656;&#35201;&#23545;&#21465;&#20107;&#29702;&#35299;&#36827;&#34892;&#24773;&#24863;&#25512;&#26029;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;K-Act2Emo&#65292;&#19968;&#20010;&#21253;&#21547;1,900&#31181;&#38388;&#25509;&#24773;&#24863;&#34920;&#36798;&#21644;&#21487;&#25512;&#26029;&#20986;&#30340;&#24773;&#32490;&#30340;&#38889;&#22269;&#24120;&#35782;&#30693;&#35782;&#22270;&#35889;&#12290;&#25105;&#20204;&#23558;&#25512;&#29702;&#31867;&#22411;&#20998;&#20026;&#27491;&#38754;&#24773;&#22659;&#30340;&#25512;&#29702;&#12289;&#36127;&#38754;&#24773;&#22659;&#30340;&#25512;&#29702;&#21644;&#34920;&#36798;&#19981;&#26159;&#24773;&#24863;&#32447;&#32034;&#26102;&#30340;&#25512;&#29702;&#12290;&#19982;&#29616;&#26377;&#30340;&#24120;&#35782;&#30693;&#35782;&#22270;&#35889;&#19981;&#21516;&#65292;K-Act2Emo&#19987;&#27880;&#20110;&#24773;&#24863;&#32972;&#26223;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#23427;&#23545;&#35757;&#32451;&#24773;&#24863;&#25512;&#26029;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20351;&#29992;K-Act2Emo&#24494;&#35843;&#30340;&#22522;&#20110;BART&#30340;&#30693;&#35782;&#27169;&#22411;&#32988;&#36807;&#20102;&#21508;&#31181;&#29616;&#26377;&#30340;&#38889;&#22269;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#24615;&#33021;&#19978;&#36798;&#21040;&#20102;&#19982;GPT-4 Turbo&#21487;&#27604;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14253v1 Announce Type: new  Abstract: In many literary texts, emotions are indirectly conveyed through descriptions of actions, facial expressions, and appearances, necessitating emotion inference for narrative understanding. In this paper, we introduce K-Act2Emo, a Korean commonsense knowledge graph (CSKG) comprising 1,900 indirect emotional expressions and the emotions inferable from them. We categorize reasoning types into inferences in positive situations, inferences in negative situations, and inferences when expressions do not serve as emotional cues. Unlike existing CSKGs, K-Act2Emo specializes in emotional contexts, and experimental results validate its effectiveness for training emotion inference models. Significantly, the BART-based knowledge model fine-tuned with K-Act2Emo outperforms various existing Korean large language models, achieving performance levels comparable to GPT-4 Turbo.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#36807;&#31243;&#20013;&#36890;&#36807;&#21033;&#29992;CLIP&#30340;&#22266;&#26377;&#23646;&#24615;&#26469;&#25506;&#35752;&#26657;&#20934;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#25552;&#31034;&#36873;&#25321;&#26174;&#33879;&#24433;&#21709;&#20102;CLIP&#20013;&#30340;&#26657;&#20934;&#65292;&#20854;&#20013;&#23548;&#33268;&#26356;&#39640;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#25552;&#31034;&#20250;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.14119</link><description>&lt;p&gt;
C-TPT&#65306;&#36890;&#36807;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#26657;&#20934;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#36807;&#31243;&#20013;&#36890;&#36807;&#21033;&#29992;CLIP&#30340;&#22266;&#26377;&#23646;&#24615;&#26469;&#25506;&#35752;&#26657;&#20934;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#25552;&#31034;&#36873;&#25321;&#26174;&#33879;&#24433;&#21709;&#20102;CLIP&#20013;&#30340;&#26657;&#20934;&#65292;&#20854;&#20013;&#23548;&#33268;&#26356;&#39640;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#25552;&#31034;&#20250;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#27979;&#35797;&#26102;&#36866;&#24212;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20316;&#20026;&#19968;&#31181;&#22312;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#26041;&#27861;&#12290;&#19968;&#20010;&#20027;&#35201;&#30340;&#20363;&#35777;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#29992;&#20110;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#30340;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25552;&#31034;&#20027;&#35201;&#26159;&#20026;&#20102;&#25552;&#39640;&#20934;&#30830;&#24615;&#32780;&#24320;&#21457;&#30340;&#65292;&#24573;&#35270;&#20102;&#26657;&#20934;&#30340;&#37325;&#35201;&#24615;&#8212;&#8212;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26657;&#20934;&#26041;&#27861;&#20381;&#36182;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#27979;&#35797;&#26102;&#22330;&#26223;&#19979;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;CLIP&#30340;&#22266;&#26377;&#23646;&#24615;&#65292;&#22312;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#36807;&#31243;&#20013;&#25506;&#35752;&#26657;&#20934;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#35266;&#23519;&#65292;&#25105;&#20204;&#21457;&#29616;&#25552;&#31034;&#36873;&#25321;&#26174;&#33879;&#24433;&#21709;&#20102;CLIP&#20013;&#30340;&#26657;&#20934;&#65292;&#20854;&#20013;&#23548;&#33268;&#26356;&#39640;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#25552;&#31034;&#20250;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14119v1 Announce Type: cross  Abstract: In deep learning, test-time adaptation has gained attention as a method for model fine-tuning without the need for labeled data. A prime exemplification is the recently proposed test-time prompt tuning for large-scale vision-language models such as CLIP. Unfortunately, these prompts have been mainly developed to improve accuracy, overlooking the importance of calibration-a crucial aspect for quantifying prediction uncertainty. However, traditional calibration methods rely on substantial amounts of labeled data, making them impractical for test-time scenarios. To this end, this paper explores calibration during test-time prompt tuning by leveraging the inherent properties of CLIP. Through a series of observations, we find that the prompt choice significantly affects the calibration in CLIP, where the prompts leading to higher text feature dispersion result in better-calibrated predictions. Introducing the Average Text Feature Dispersion
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BART&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#21033;&#29992;&#38382;&#31572;&#26694;&#26550;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#29305;&#23450;&#24773;&#32490;&#30701;&#35821;&#65292;&#24182;&#36890;&#36807;&#20998;&#31867;&#22120;&#39044;&#27979;&#31572;&#26696;&#36328;&#24230;&#20301;&#32622;&#65292;&#23454;&#29616;&#23545;&#24773;&#32490;&#30701;&#35821;&#30340;&#31934;&#30830;&#25552;&#21462;&#12290;</title><link>https://arxiv.org/abs/2403.14050</link><description>&lt;p&gt;
&#20351;&#29992;BART&#20174;&#25512;&#25991;&#20013;&#25552;&#21462;&#24773;&#32490;&#30701;&#35821;
&lt;/p&gt;
&lt;p&gt;
Extracting Emotion Phrases from Tweets using BART
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BART&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#21033;&#29992;&#38382;&#31572;&#26694;&#26550;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#29305;&#23450;&#24773;&#32490;&#30701;&#35821;&#65292;&#24182;&#36890;&#36807;&#20998;&#31867;&#22120;&#39044;&#27979;&#31572;&#26696;&#36328;&#24230;&#20301;&#32622;&#65292;&#23454;&#29616;&#23545;&#24773;&#32490;&#30701;&#35821;&#30340;&#31934;&#30830;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#26159;&#19968;&#39033;&#26088;&#22312;&#35782;&#21035;&#21644;&#25552;&#21462;&#25991;&#26412;&#20013;&#24773;&#32490;&#26041;&#38754;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#20027;&#35201;&#26159;&#23545;&#25991;&#26412;&#30340;&#25972;&#20307;&#26497;&#24615;&#36827;&#34892;&#20998;&#31867;&#65292;&#24573;&#30053;&#20102;&#20256;&#36798;&#24773;&#32490;&#30340;&#20855;&#20307;&#30701;&#35821;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#38382;&#31572;&#26694;&#26550;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#21452;&#21521;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#65288;BART&#65289;&#65292;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#65292;&#20174;&#32473;&#23450;&#25991;&#26412;&#20013;&#25552;&#21462;&#25918;&#22823;&#32473;&#23450;&#24773;&#24863;&#26497;&#24615;&#30340;&#30701;&#35821;&#12290;&#25105;&#20204;&#21019;&#24314;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#65292;&#30830;&#23450;&#35201;&#25552;&#21462;&#30340;&#29305;&#23450;&#24773;&#32490;&#65292;&#28982;&#21518;&#24341;&#23548;BART&#19987;&#27880;&#20110;&#25991;&#26412;&#20013;&#30456;&#20851;&#30340;&#24773;&#24863;&#32447;&#32034;&#12290;&#25105;&#20204;&#22312;BART&#20013;&#20351;&#29992;&#19968;&#20010;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#25991;&#26412;&#20013;&#31572;&#26696;&#36328;&#24230;&#30340;&#24320;&#22987;&#21644;&#32467;&#26463;&#20301;&#32622;&#65292;&#20174;&#32780;&#24110;&#21161;&#30830;&#23450;&#25552;&#21462;&#30340;&#24773;&#32490;&#30701;&#35821;&#30340;&#31934;&#30830;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14050v2 Announce Type: replace  Abstract: Sentiment analysis is a natural language processing task that aims to identify and extract the emotional aspects of a text. However, many existing sentiment analysis methods primarily classify the overall polarity of a text, overlooking the specific phrases that convey sentiment. In this paper, we applied an approach to sentiment analysis based on a question-answering framework. Our approach leverages the power of Bidirectional Autoregressive Transformer (BART), a pre-trained sequence-to-sequence model, to extract a phrase from a given text that amplifies a given sentiment polarity. We create a natural language question that identifies the specific emotion to extract and then guide BART to pay attention to the relevant emotional cues in the text. We use a classifier within BART to predict the start and end positions of the answer span within the text, which helps to identify the precise boundaries of the extracted emotion phrase. Our
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#8220;Chain-of-Interaction (CoI)&#8221;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20108;&#20803;&#20132;&#20114;&#24773;&#22659;&#26469;&#20026;&#31934;&#31070;&#20915;&#31574;&#25903;&#25345;&#19978;&#19979;&#25991;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#20197;&#35299;&#20915;&#31934;&#31070;&#27835;&#30103;&#20013;&#32570;&#20047;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#24573;&#35270;&#24739;&#32773;-&#27835;&#30103;&#24072;&#20132;&#20114;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.13786</link><description>&lt;p&gt;
Chain-of-Interaction: &#36890;&#36807;&#20108;&#20803;&#20132;&#20114;&#32972;&#26223;&#22686;&#24378;&#29992;&#20110;&#31934;&#31070;&#34892;&#20026;&#29702;&#35299;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Interaction: Enhancing Large Language Models for Psychiatric Behavior Understanding by Dyadic Contexts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13786
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#8220;Chain-of-Interaction (CoI)&#8221;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20108;&#20803;&#20132;&#20114;&#24773;&#22659;&#26469;&#20026;&#31934;&#31070;&#20915;&#31574;&#25903;&#25345;&#19978;&#19979;&#25991;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#20197;&#35299;&#20915;&#31934;&#31070;&#27835;&#30103;&#20013;&#32570;&#20047;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#24573;&#35270;&#24739;&#32773;-&#27835;&#30103;&#24072;&#20132;&#20114;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#32534;&#30721;&#24739;&#32773;&#34892;&#20026;&#23545;&#20110;&#25903;&#25345;&#31934;&#31070;&#27835;&#30103;&#24072;&#22312;&#28608;&#21169;&#24615;&#38754;&#35848;&#65288;MI&#65289;&#26399;&#38388;&#20570;&#20986;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;MI&#26159;&#19968;&#31181;&#21327;&#20316;&#27807;&#36890;&#24178;&#39044;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31934;&#31070;&#38382;&#39064;&#65292;&#22914;&#37202;&#31934;&#21644;&#33647;&#29289;&#25104;&#30270;&#12290;&#23613;&#31649;&#34892;&#20026;&#32534;&#30721;&#20219;&#21153;&#24050;&#36805;&#36895;&#20511;&#21161;&#26426;&#22120;&#23398;&#20064;&#26469;&#39044;&#27979;MI&#20250;&#35805;&#26399;&#38388;&#24739;&#32773;&#29366;&#24577;&#65292;&#20294;&#32570;&#20047;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#24182;&#24573;&#35270;&#24739;&#32773;-&#27835;&#30103;&#24072;&#20132;&#20114;&#26159;&#22312;&#23454;&#38469;&#23454;&#36341;&#20013;&#24320;&#21457;&#21644;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Chain-of-Interaction (CoI)&#25552;&#31034;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20108;&#20803;&#20132;&#20114;&#24773;&#22659;&#26469;&#20026;&#31934;&#31070;&#20915;&#31574;&#25903;&#25345;&#19978;&#19979;&#25991;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#12290;CoI&#25552;&#31034;&#26041;&#27861;&#31995;&#32479;&#22320;&#23558;&#32534;&#30721;&#20219;&#21153;&#20998;&#35299;&#20026;&#19977;&#20010;&#20851;&#38190;&#25512;&#29702;&#27493;&#39588;&#65292;&#25552;&#21462;&#24739;&#32773;&#21442;&#19982;&#24230;&#65292;&#23398;&#20064;&#27835;&#30103;&#24072;&#25552;&#38382;&#31574;&#30053;&#65292;&#24182;&#25972;&#21512;&#20108;&#20803;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13786v1 Announce Type: new  Abstract: Automatic coding patient behaviors is essential to support decision making for psychotherapists during the motivational interviewing (MI), a collaborative communication intervention approach to address psychiatric issues, such as alcohol and drug addiction. While the behavior coding task has rapidly adapted machine learning to predict patient states during the MI sessions, lacking of domain-specific knowledge and overlooking patient-therapist interactions are major challenges in developing and deploying those models in real practice. To encounter those challenges, we introduce the Chain-of-Interaction (CoI) prompting method aiming to contextualize large language models (LLMs) for psychiatric decision support by the dyadic interactions. The CoI prompting approach systematically breaks down the coding task into three key reasoning steps, extract patient engagement, learn therapist question strategies, and integrates dyadic interactions bet
&lt;/p&gt;</description></item><item><title>EthioLLM&#20026;&#22467;&#22622;&#20420;&#27604;&#20122;&#20116;&#31181;&#35821;&#35328;&#65288;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#30422;&#20234;&#20857;&#35821;&#12289;&#38463;&#26041;&#22885;&#32599;&#33707;&#35821;&#12289;&#32034;&#39532;&#37324;&#35821;&#21644;&#25552;&#26684;&#37324;&#23612;&#20122;&#35821;&#65289;&#20197;&#21450;&#33521;&#35821;&#24341;&#20837;&#20102;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;Ethiobenchmark&#65292;&#20026;&#21508;&#31181;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13737</link><description>&lt;p&gt;
EthioLLM&#65306;&#29992;&#20110;&#22467;&#22622;&#20420;&#27604;&#20122;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21450;&#20219;&#21153;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
EthioLLM: Multilingual Large Language Models for Ethiopian Languages with Task Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13737
&lt;/p&gt;
&lt;p&gt;
EthioLLM&#20026;&#22467;&#22622;&#20420;&#27604;&#20122;&#20116;&#31181;&#35821;&#35328;&#65288;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#30422;&#20234;&#20857;&#35821;&#12289;&#38463;&#26041;&#22885;&#32599;&#33707;&#35821;&#12289;&#32034;&#39532;&#37324;&#35821;&#21644;&#25552;&#26684;&#37324;&#23612;&#20122;&#35821;&#65289;&#20197;&#21450;&#33521;&#35821;&#24341;&#20837;&#20102;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;Ethiobenchmark&#65292;&#20026;&#21508;&#31181;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#26469;&#22240;&#20854;&#22312;&#21508;&#31181;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#20986;&#33394;&#34920;&#29616;&#32780;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;LLMs&#30340;&#36164;&#28304;&#19981;&#36275;&#65292;&#20302;&#36164;&#28304;&#35821;&#35328;&#20173;&#33853;&#21518;&#20110;NLP&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;&#22467;&#22622;&#20420;&#27604;&#20122;&#35821;&#35328;&#25317;&#26377;&#26174;&#33879;&#30340;&#35821;&#35328;&#22810;&#26679;&#24615;&#65292;&#21253;&#25324;&#24191;&#27867;&#30340;&#25991;&#23383;&#31995;&#32479;&#65292;&#24182;&#23500;&#26377;&#28145;&#36828;&#30340;&#23447;&#25945;&#21644;&#25991;&#21270;&#24847;&#20041;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;EthioLLM - &#20116;&#31181;&#22467;&#22622;&#20420;&#27604;&#20122;&#35821;&#35328;&#65288;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#30422;&#20234;&#20857;&#35821;&#12289;&#38463;&#26041;&#22885;&#32599;&#33707;&#35821;&#12289;&#32034;&#39532;&#37324;&#35821;&#21644;&#25552;&#26684;&#37324;&#23612;&#20122;&#35821;&#65289;&#21644;&#33521;&#35821;&#30340;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;Ethiobenchmark - &#29992;&#20110;&#21508;&#31181;&#19979;&#28216;NLP&#20219;&#21153;&#30340;&#26032;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#20116;&#20010;&#19979;&#28216;NLP&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24320;&#28304;&#25105;&#20204;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#12289;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#26032;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#31934;&#35843;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13737v1 Announce Type: new  Abstract: Large language models (LLMs) have gained popularity recently due to their outstanding performance in various downstream Natural Language Processing (NLP) tasks. However, low-resource languages are still lagging behind current state-of-the-art (SOTA) developments in the field of NLP due to insufficient resources to train LLMs. Ethiopian languages exhibit remarkable linguistic diversity, encompassing a wide array of scripts, and are imbued with profound religious and cultural significance. This paper introduces EthioLLM -- multilingual large language models for five Ethiopian languages (Amharic, Ge'ez, Afan Oromo, Somali, and Tigrinya) and English, and Ethiobenchmark -- a new benchmark dataset for various downstream NLP tasks. We evaluate the performance of these models across five downstream NLP tasks. We open-source our multilingual language models, new benchmark datasets for various downstream tasks, and task-specific fine-tuned languag
&lt;/p&gt;</description></item><item><title>&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26412;&#35843;&#26597;&#35770;&#25991;&#27010;&#36848;&#20102;&#22312;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#32972;&#26223;&#19979;&#22270;&#34920;&#29702;&#35299;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;</title><link>https://arxiv.org/abs/2403.12027</link><description>&lt;p&gt;
&#20174;&#20687;&#32032;&#21040;&#27934;&#23519;: &#22312;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#26102;&#20195;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12027
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26412;&#35843;&#26597;&#35770;&#25991;&#27010;&#36848;&#20102;&#22312;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#32972;&#26223;&#19979;&#22270;&#34920;&#29702;&#35299;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21487;&#35270;&#21270;&#20197;&#22270;&#34920;&#24418;&#24335;&#22312;&#25968;&#25454;&#20998;&#26512;&#20013;&#25198;&#28436;&#30528;&#20851;&#38190;&#35282;&#33394;&#65292;&#25552;&#20379;&#20851;&#38190;&#27934;&#23519;&#24182;&#24110;&#21161;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#12290;&#38543;&#30528;&#36817;&#24180;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#23835;&#36215;&#65292;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#38761;&#21629;&#65292;&#24182;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#22270;&#34920;&#29702;&#35299;&#20219;&#21153;&#12290;&#26412;&#35843;&#26597;&#35770;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#25506;&#35752;&#20102;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#32972;&#26223;&#19979;&#22270;&#34920;&#29702;&#35299;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12027v1 Announce Type: cross  Abstract: Data visualization in the form of charts plays a pivotal role in data analysis, offering critical insights and aiding in informed decision-making. Automatic chart understanding has witnessed significant advancements with the rise of large foundation models in recent years. Foundation models, such as large language models (LLMs), have revolutionized various natural language processing (NLP) tasks and are increasingly being applied to chart understanding tasks. This survey paper provides a comprehensive overview of the recent developments, challenges, and future directions in chart understanding within the context of these foundation models. The paper begins by defining chart understanding, outlining problem formulations, and discussing fundamental building blocks crucial for studying chart understanding tasks. In the section on tasks and datasets, we explore various tasks within chart understanding and discuss their evaluation metrics a
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;Guide-Align&#65292;&#19968;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#23433;&#20840;&#35757;&#32451;&#27169;&#22411;&#35782;&#21035;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#21046;&#23450;&#29305;&#23450;&#25351;&#21335;&#65292;&#20174;&#32780;&#24314;&#31435;&#20840;&#38754;&#30340;&#25351;&#23548;&#24211;&#65292;&#29992;&#20110;&#25351;&#23548;LLMs&#29983;&#25104;&#23433;&#20840;&#21644;&#39640;&#36136;&#37327;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2403.11838</link><description>&lt;p&gt;
&#30830;&#20445;&#23433;&#20840;&#21644;&#39640;&#36136;&#37327;&#36755;&#20986;&#65306;&#38754;&#21521;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#21335;&#24211;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11838
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;Guide-Align&#65292;&#19968;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#23433;&#20840;&#35757;&#32451;&#27169;&#22411;&#35782;&#21035;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#21046;&#23450;&#29305;&#23450;&#25351;&#21335;&#65292;&#20174;&#32780;&#24314;&#31435;&#20840;&#38754;&#30340;&#25351;&#23548;&#24211;&#65292;&#29992;&#20110;&#25351;&#23548;LLMs&#29983;&#25104;&#23433;&#20840;&#21644;&#39640;&#36136;&#37327;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#20063;&#23384;&#22312;&#20559;&#35265;&#20869;&#23481;&#29983;&#25104;&#21644;&#38544;&#31169;&#38382;&#39064;&#31561;&#39118;&#38505;&#12290;&#24403;&#21069;&#30340;&#23545;&#40784;&#25216;&#26415;&#20043;&#19968;&#21253;&#25324;&#22522;&#20110;&#21407;&#21017;&#30340;&#38598;&#25104;&#65292;&#20294;&#38754;&#20020;&#30001;&#20110;&#25163;&#24037;&#21046;&#23450;&#35268;&#21017;&#30340;&#19981;&#31934;&#30830;&#24615;&#21644;&#26410;&#32463;&#23433;&#20840;&#35757;&#32451;&#30340;&#27169;&#22411;&#23545;&#39118;&#38505;&#24863;&#30693;&#19981;&#36275;&#32780;&#20135;&#29983;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Guide-Align&#65292;&#36825;&#26159;&#19968;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;&#12290;&#26368;&#21021;&#65292;&#19968;&#20010;&#32463;&#36807;&#23433;&#20840;&#35757;&#32451;&#30340;&#27169;&#22411;&#35782;&#21035;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#20026;&#21508;&#31181;&#36755;&#20837;&#21046;&#23450;&#20855;&#20307;&#25351;&#21335;&#65292;&#20174;&#32780;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#25351;&#21335;&#24211;&#21644;&#29992;&#20110;&#36755;&#20837;&#25351;&#21335;&#26816;&#32034;&#30340;&#27169;&#22411;&#12290;&#38543;&#21518;&#65292;&#26816;&#32034;&#27169;&#22411;&#23558;&#26032;&#36755;&#20837;&#19982;&#30456;&#20851;&#25351;&#21335;&#30456;&#20851;&#32852;&#65292;&#24341;&#23548;LLMs&#22312;&#21709;&#24212;&#29983;&#25104;&#20013;&#30830;&#20445;&#23433;&#20840;&#21644;&#39640;&#36136;&#37327;&#36755;&#20986;&#65292;&#20174;&#32780;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19968;&#33268;&#12290;&#21478;&#19968;&#20010;&#39069;&#22806;&#21487;&#36873;&#38454;&#27573;&#28041;&#21450;&#20351;&#29992;&#32463;&#36807;&#32454;&#33268;&#23545;&#40784;&#30340;&#26032;&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11838v1 Announce Type: cross  Abstract: Large Language Models (LLMs) exhibit impressive capabilities but also present risks such as biased content generation and privacy issues. One of the current alignment techniques includes principle-driven integration, but it faces challenges arising from the imprecision of manually crafted rules and inadequate risk perception in models without safety training. To address these, we introduce Guide-Align, a two-stage approach. Initially, a safety-trained model identifies potential risks and formulates specific guidelines for various inputs, thereby establishing a comprehensive library of guidelines and models for input-guidelines retrieval. Subsequently, the retrieval model correlates new inputs with pertinent guidelines, guiding LLMs in response generation to ensure safe and high-quality outputs, thus aligning with human values. An additional optional stage involves fine-tuning a model with new well-aligned datasets generated through the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Counting-Stars&#30340;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21512;&#29702;&#31574;&#30053;&#65292;&#29992;&#20110;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;GPT-4 Turbo&#21644;Kimi Chat&#22312;&#27492;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.11802</link><description>&lt;p&gt;
Counting-Stars&#65306;&#19968;&#31181;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21512;&#29702;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Counting-Stars: A Simple, Efficient, and Reasonable Strategy for Evaluating Long-Context Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11802
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Counting-Stars&#30340;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21512;&#29702;&#31574;&#30053;&#65292;&#29992;&#20110;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;GPT-4 Turbo&#21644;Kimi Chat&#22312;&#27492;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24320;&#21457;&#20855;&#26377;&#24378;&#22823;&#38271;&#19978;&#19979;&#25991;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#30001;&#20110;&#32570;&#20047;&#36866;&#24403;&#30340;&#35780;&#20272;&#31574;&#30053;&#65292;&#23545;&#39046;&#20808;&#30340;LLMs&#65288;&#20363;&#22914;ChatGPT&#21644;KimiChat&#65289;&#30340;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#33021;&#21147;&#21644;&#24615;&#33021;&#20102;&#35299;&#29978;&#23569;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21512;&#29702;&#30340;&#38271;&#19978;&#19979;&#25991;LLMs&#35780;&#20272;&#31574;&#30053;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#21517;&#20026;Counting-Stars&#12290;Counting-Stars&#26088;&#22312;&#35201;&#27714;LLMs&#20805;&#20998;&#29702;&#35299;&#21644;&#25429;&#25417;&#38271;&#19978;&#19979;&#25991;&#20013;&#30340;&#38271;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#33021;&#22815;&#25910;&#38598;&#36328;&#36234;&#25972;&#20010;&#19978;&#19979;&#25991;&#30340;&#22810;&#20010;&#35777;&#25454;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#22522;&#20110;Counting-Stars&#65292;&#25105;&#20204;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#20102;&#20004;&#20010;&#39046;&#20808;&#30340;&#38271;&#19978;&#19979;&#25991;LLMs&#65292;&#21363;GPT-4 Turbo&#21644;Kimi Chat&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4 Turbo&#21644;Kimi Chat&#22312;Counting-Stars&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11802v1 Announce Type: new  Abstract: While recent research endeavors have concentrated on developing Large Language Models (LLMs) with robust long-context capabilities, due to the lack of appropriate evaluation strategies, relatively little is known about how well the long-context processing abilities and performance of leading LLMs (e.g., ChatGPT and KimiChat). To address this gap, we propose a simple, efficient, and reasonable strategy for evaluating long-context LLMs as a new benchmark, named Counting-Stars. The Counting-Stars is designed to require LLMs to fully understand and capture long dependencies in long contexts and be able to collect inter-dependency across multiple pieces of evidence spanning the entire context to finish the task. Based on the Counting-Stars, we conduct experiments to evaluate the two leading long-context LLMs, i.e., GPT-4 Turbo and Kimi Chat. The experimental results indicate that GPT-4 Turbo and Kimi Chat achieve significant performance in th
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#25910;&#38598;&#38901;&#24459;&#35799;&#21644;&#35799;&#27468;&#25968;&#25454;&#38598;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#20934;&#30830;&#29575;&#20026;97%&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32416;&#27491;&#20102;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.11752</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#32463;&#20856;&#65306;&#35782;&#21035;&#21644;&#32416;&#27491;&#38901;&#24459;&#35799;&#20013;&#30340;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Revisiting The Classics: A Study on Identifying and Rectifying Gender Stereotypes in Rhymes and Poems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11752
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#25910;&#38598;&#38901;&#24459;&#35799;&#21644;&#35799;&#27468;&#25968;&#25454;&#38598;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#20934;&#30830;&#29575;&#20026;97%&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32416;&#27491;&#20102;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38901;&#24459;&#35799;&#26159;&#20256;&#36882;&#25991;&#21270;&#35268;&#33539;&#21644;&#31038;&#20250;&#35282;&#33394;&#30340;&#24378;&#22823;&#23186;&#20171;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20316;&#21697;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#24310;&#32493;&#20102;&#26377;&#20559;&#35265;&#30340;&#35748;&#30693;&#65292;&#24182;&#38480;&#21046;&#20102;&#20010;&#20307;&#36523;&#20221;&#30340;&#33539;&#22260;&#12290;&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#25910;&#38598;&#19968;&#32452;&#38901;&#24459;&#35799;&#21644;&#35799;&#27468;&#25968;&#25454;&#38598;&#65292;&#35782;&#21035;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20934;&#30830;&#29575;&#20026;97%&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#24615;&#21035;&#20559;&#35265;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#32416;&#27491;&#20102;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#65292;&#24182;&#22312;&#19982;&#20154;&#31867;&#25945;&#32946;&#32773;&#32416;&#27491;&#31574;&#30053;&#30340;&#27604;&#36739;&#35843;&#26597;&#20013;&#35780;&#20272;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#24635;&#20043;&#65292;&#36825;&#39033;&#24037;&#20316;&#31361;&#26174;&#20102;&#25991;&#23398;&#20316;&#21697;&#20013;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#30340;&#26222;&#36941;&#24615;&#65292;&#24182;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32416;&#27491;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11752v1 Announce Type: new  Abstract: Rhymes and poems are a powerful medium for transmitting cultural norms and societal roles. However, the pervasive existence of gender stereotypes in these works perpetuates biased perceptions and limits the scope of individuals' identities. Past works have shown that stereotyping and prejudice emerge in early childhood, and developmental research on causal mechanisms is critical for understanding and controlling stereotyping and prejudice. This work contributes by gathering a dataset of rhymes and poems to identify gender stereotypes and propose a model with 97\% accuracy to identify gender bias. Gender stereotypes were rectified using a Large Language Model (LLM) and its effectiveness was evaluated in a comparative survey against human educator rectifications. To summarize, this work highlights the pervasive nature of gender stereotypes in literary works and reveals the potential of LLMs to rectify gender stereotypes. This study raises 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22810;&#27169;&#24577;&#36719;&#25552;&#31034;&#26694;&#26550;MoPE-BAF&#65292;&#29992;&#20110;&#35299;&#20915;&#23569;&#26679;&#26412;&#23398;&#20064;&#19979;&#30340;&#22810;&#27169;&#24577;&#35773;&#21050;&#26816;&#27979;&#21644;&#24773;&#24863;&#20998;&#26512;&#38382;&#39064;&#65292;&#36890;&#36807;&#19977;&#20010;&#36719;&#25552;&#31034;&#19987;&#23478;&#21644;&#22359;&#24863;&#30693;&#25552;&#31034;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#21644;&#22810;&#27169;&#24577;&#20132;&#20114;&#12290;</title><link>https://arxiv.org/abs/2403.11311</link><description>&lt;p&gt;
&#28151;&#21512;&#25552;&#31034;&#19987;&#23478;&#29992;&#20110;&#22810;&#27169;&#24577;&#35821;&#20041;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-Prompt-Experts for Multi-modal Semantic Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11311
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22810;&#27169;&#24577;&#36719;&#25552;&#31034;&#26694;&#26550;MoPE-BAF&#65292;&#29992;&#20110;&#35299;&#20915;&#23569;&#26679;&#26412;&#23398;&#20064;&#19979;&#30340;&#22810;&#27169;&#24577;&#35773;&#21050;&#26816;&#27979;&#21644;&#24773;&#24863;&#20998;&#26512;&#38382;&#39064;&#65292;&#36890;&#36807;&#19977;&#20010;&#36719;&#25552;&#31034;&#19987;&#23478;&#21644;&#22359;&#24863;&#30693;&#25552;&#31034;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#21644;&#22810;&#27169;&#24577;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11311v1 &#20844;&#21578;&#31867;&#22411;:&#26032; &#25277;&#35937;:&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#28145;&#24230;&#22810;&#27169;&#24577;&#35821;&#20041;&#29702;&#35299;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#36229;&#36234;&#20102;&#21333;&#32431;&#30340;&#34920;&#38754;&#20869;&#23481;&#20851;&#31995;&#25366;&#25496;&#12290;&#25910;&#38598;&#21644;&#27880;&#37322;&#39640;&#36136;&#37327;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#25361;&#25112;&#20984;&#26174;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#20851;&#27880;&#36825;&#19968;&#32972;&#26223;&#19979;&#30340;&#20004;&#20010;&#20851;&#38190;&#20219;&#21153;: &#23569;&#26679;&#26412;&#22810;&#27169;&#24577;&#35773;&#21050;&#26816;&#27979;&#65288;MSD&#65289;&#21644;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65288;MSA&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#32479;&#19968;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#30340;&#19968;&#31181;&#26032;&#22411;&#22810;&#27169;&#24577;&#36719;&#25552;&#31034;&#26694;&#26550;Mixture-of-Prompt-Experts with Block-Aware Prompt Fusion&#65288;MoPE-BAF&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#20010;&#36719;&#25552;&#31034;&#19987;&#23478;: &#19968;&#20010;&#25991;&#26412;&#25552;&#31034;&#21644;&#19968;&#20010;&#22270;&#20687;&#25552;&#31034;&#65292;&#29992;&#20110;&#25552;&#21462;&#29305;&#23450;&#20110;&#27169;&#24577;&#30340;&#29305;&#24449;&#20197;&#20016;&#23500;&#21333;&#27169;&#24577;&#34920;&#31034;&#65292;&#20197;&#21450;&#19968;&#20010;&#32479;&#19968;&#25552;&#31034;&#20197;&#21327;&#21161;&#22810;&#27169;&#24577;&#20132;&#20114;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;Transformer&#23618;&#37325;&#26032;&#32452;&#32455;&#20026;&#20960;&#20010;&#22359;&#65292;&#24182;&#23454;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11311v1 Announce Type: new  Abstract: Deep multimodal semantic understanding that goes beyond the mere superficial content relation mining has received increasing attention in the realm of artificial intelligence. The challenges of collecting and annotating high-quality multi-modal data have underscored the significance of few-shot learning. In this paper, we focus on two critical tasks under this context: few-shot multi-modal sarcasm detection (MSD) and multi-modal sentiment analysis (MSA). To address them, we propose Mixture-of-Prompt-Experts with Block-Aware Prompt Fusion (MoPE-BAF), a novel multi-modal soft prompt framework based on the unified vision-language model (VLM). Specifically, we design three experts of soft prompts: a text prompt and an image prompt that extract modality-specific features to enrich the single-modal representation, and a unified prompt to assist multi-modal interaction. Additionally, we reorganize Transformer layers into several blocks and intr
&lt;/p&gt;</description></item><item><title>Pointer-Generator Networks&#22312;&#20302;&#36164;&#28304;&#26426;&#22120;&#32763;&#35793;&#20013;&#26410;&#23637;&#29616;&#20986;&#39044;&#26399;&#30340;&#20248;&#21183;&#65292;&#27169;&#22411;&#22312;&#19981;&#21516;&#36164;&#28304;&#33539;&#22260;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#19979;&#34920;&#29616;&#19968;&#33324;&#12290;</title><link>https://arxiv.org/abs/2403.10963</link><description>&lt;p&gt;
Pointer-Generator&#32593;&#32476;&#29992;&#20110;&#20302;&#36164;&#28304;&#26426;&#22120;&#32763;&#35793;&#65306;&#19981;&#35201;&#22797;&#21046;&#37027;&#20010;&#65281;
&lt;/p&gt;
&lt;p&gt;
Pointer-Generator Networks for Low-Resource Machine Translation: Don't Copy That!
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10963
&lt;/p&gt;
&lt;p&gt;
Pointer-Generator Networks&#22312;&#20302;&#36164;&#28304;&#26426;&#22120;&#32763;&#35793;&#20013;&#26410;&#23637;&#29616;&#20986;&#39044;&#26399;&#30340;&#20248;&#21183;&#65292;&#27169;&#22411;&#22312;&#19981;&#21516;&#36164;&#28304;&#33539;&#22260;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#19979;&#34920;&#29616;&#19968;&#33324;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#22312;&#39640;&#36164;&#28304;&#29615;&#22659;&#20013;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#35768;&#22810;&#35821;&#35328;&#32570;&#20047;&#24517;&#35201;&#30340;&#22823;&#35268;&#27169;&#24179;&#34892;&#35821;&#26009;&#24211;&#26469;&#21463;&#30410;&#12290;&#22312;&#20004;&#31181;&#23494;&#20999;&#30456;&#20851;&#35821;&#35328;&#20043;&#38388;&#30340;&#20302;&#36164;&#28304;&#65288;LR&#65289;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#19968;&#31181;&#33258;&#28982;&#30340;&#30452;&#35273;&#26159;&#23547;&#27714;&#20174;&#32467;&#26500;&#8220;&#25463;&#24452;&#8221;&#20013;&#33719;&#30410;&#65292;&#20363;&#22914;&#20174;&#28304;&#35821;&#35328;&#22797;&#21046;&#23376;&#35789;&#21040;&#30446;&#26631;&#35821;&#35328;&#65292;&#22240;&#20026;&#36825;&#26679;&#30340;&#35821;&#35328;&#23545;&#36890;&#24120;&#20849;&#20139;&#30456;&#24403;&#25968;&#37327;&#30340;&#30456;&#21516;&#21333;&#35789;&#12289;&#21516;&#28304;&#35789;&#21644;&#20511;&#35789;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#38024;&#23545;&#20845;&#31181;&#35821;&#35328;&#23545;&#30340;&#25351;&#38024;&#29983;&#25104;&#22120;&#32593;&#32476;&#22312;&#21508;&#31181;&#36164;&#28304;&#33539;&#22260;&#19979;&#30340;&#29992;&#36884;&#65292;&#24182;&#21457;&#29616;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#37117;&#26377;&#36731;&#24494;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;&#26174;&#31034;&#65292;&#27169;&#22411;&#23545;&#20110;&#23494;&#20999;&#30456;&#20851;&#30340;&#35821;&#35328;&#23545;&#19982;&#36739;&#36828;&#30340;&#35821;&#35328;&#23545;&#65292;&#25110;&#32773;&#36164;&#28304;&#33539;&#22260;&#36739;&#20302;&#19982;&#36739;&#39640;&#30340;&#35821;&#35328;&#23545;&#24182;&#27809;&#26377;&#23637;&#29616;&#20986;&#26356;&#22823;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#27169;&#22411;&#24182;&#26410;&#23637;&#31034;&#20986;&#23545;&#20110;&#20849;&#20139;&#23376;&#35789;&#26426;&#21046;&#30340;&#39044;&#26399;&#29992;&#27861;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#34892;&#20026;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10963v1 Announce Type: new  Abstract: While Transformer-based neural machine translation (NMT) is very effective in high-resource settings, many languages lack the necessary large parallel corpora to benefit from it. In the context of low-resource (LR) MT between two closely-related languages, a natural intuition is to seek benefits from structural "shortcuts", such as copying subwords from the source to the target, given that such language pairs often share a considerable number of identical words, cognates, and borrowings. We test Pointer-Generator Networks for this purpose for six language pairs over a variety of resource ranges, and find weak improvements for most settings. However, analysis shows that the model does not show greater improvements for closely-related vs. more distant language pairs, or for lower resource ranges, and that the models do not exhibit the expected usage of the mechanism for shared subwords. Our discussion of the reasons for this behaviour high
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20851;&#31995;&#35299;&#32544;&#26469;&#25351;&#23548;&#31070;&#32463;&#21709;&#24212;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20250;&#35805;&#19978;&#19979;&#25991;&#20869;&#37096;&#24494;&#22937;&#32447;&#32034;&#19978;&#36827;&#34892;&#20851;&#31995;&#25512;&#26029;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#26041;&#22238;&#22797;&#30340;&#33258;&#21160;&#25512;&#26029;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#31614;&#12290;</title><link>https://arxiv.org/abs/2403.10827</link><description>&lt;p&gt;
&#20855;&#26377;&#20851;&#31995;&#35299;&#32544;&#30340;&#22810;&#26041;&#22238;&#22797;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Multi-party Response Generation with Relation Disentanglement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20851;&#31995;&#35299;&#32544;&#26469;&#25351;&#23548;&#31070;&#32463;&#21709;&#24212;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20250;&#35805;&#19978;&#19979;&#25991;&#20869;&#37096;&#24494;&#22937;&#32447;&#32034;&#19978;&#36827;&#34892;&#20851;&#31995;&#25512;&#26029;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#26041;&#22238;&#22797;&#30340;&#33258;&#21160;&#25512;&#26029;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31070;&#32463;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#22312;&#21452;&#26041;&#23545;&#35805;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#20551;&#35774;&#35805;&#35821;&#26159;&#25353;&#39034;&#24207;&#32452;&#32455;&#30340;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#23545;&#35805;&#28041;&#21450;&#22810;&#26041;&#21442;&#19982;&#32773;&#65292;&#23545;&#35805;&#19978;&#19979;&#25991;&#30340;&#32467;&#26500;&#35201;&#22797;&#26434;&#24471;&#22810;&#65292;&#20363;&#22914;&#65292;&#26469;&#33258;&#19981;&#21516;&#21442;&#19982;&#32773;&#30340;&#35805;&#35821;&#21487;&#33021;&#8220;&#24182;&#34892;&#8221;&#21457;&#29983;&#12290;&#38754;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#26377;&#30740;&#31350;&#33268;&#21147;&#20110;&#24314;&#27169;&#35805;&#35821;&#25110;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#20415;&#20197;&#26356;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#36827;&#34892;&#21709;&#24212;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#36825;&#20123;&#20851;&#31995;&#65292;&#37117;&#20551;&#35774;&#36825;&#20123;&#20851;&#31995;&#26159;&#39044;&#20808;&#32473;&#23450;&#30340;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#20063;&#22952;&#30861;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#24494;&#22937;&#32447;&#32034;&#36827;&#34892;&#20851;&#31995;&#25512;&#26029;&#65292;&#24341;&#23548;&#31070;&#32463;&#21709;&#24212;&#29983;&#25104;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#20154;&#24037;&#26631;&#31614;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20851;&#31995;&#24605;&#32500;&#33258;&#21160;&#25512;&#26029;&#20986;&#36825;&#20123;&#20851;&#31995;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#20851;&#31995;&#26469;&#25351;&#23548;&#31070;&#32463;&#21709;&#24212;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10827v1 Announce Type: new  Abstract: Existing neural response generation models have achieved impressive improvements for two-party conversations, which assume that utterances are sequentially organized. However, many real-world dialogues involve multiple interlocutors and the structure of conversational context is much more complex, e.g. utterances from different interlocutors can occur "in parallel". Facing this challenge, there are works trying to model the relations among utterances or interlocutors to facilitate response generation with clearer context. Nonetheless, these methods rely heavily on such relations and all assume that these are given beforehand, which is impractical and hinders the generality of such methods. In this work, we propose to automatically infer the relations via relational thinking on subtle clues inside the conversation context without any human label, and leverage these relations to guide the neural response generation. Specifically, we first 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;KG-LLM&#65289;&#65292;&#21033;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;NLP&#33539;&#20363;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22810;&#36339;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102;&#26694;&#26550;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#27425;&#23581;&#35797;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07311</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;KG-LLM&#65289;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Large Language Model (KG-LLM) for Link Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07311
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;KG-LLM&#65289;&#65292;&#21033;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;NLP&#33539;&#20363;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22810;&#36339;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102;&#26694;&#26550;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#27425;&#23581;&#35797;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#20998;&#26512;&#39046;&#22495;&#65292;&#39044;&#27979;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20869;&#22810;&#20010;&#38142;&#25509;&#30340;&#20219;&#21153;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#30693;&#35782;&#22270;&#23884;&#20837;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#36825;&#19968;&#25361;&#25112;&#21464;&#24471;&#36234;&#26469;&#36234;&#21487;&#35299;&#20915;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;KG-LLM&#65289;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20851;&#38190;&#30340;NLP&#33539;&#20363;&#65292;&#21253;&#25324;&#24605;&#32500;&#38142;&#25552;&#31034;&#65288;CoT&#65289;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22810;&#36339;&#38142;&#25509;&#39044;&#27979;&#12290;&#36890;&#36807;&#23558;KG&#36716;&#25442;&#20026;CoT&#25552;&#31034;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26088;&#22312;&#35782;&#21035;&#24182;&#23398;&#20064;&#23454;&#20307;&#21450;&#20854;&#30456;&#20114;&#20851;&#31995;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#20026;&#20102;&#23637;&#31034;KG-LLM&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#35813;&#26694;&#26550;&#20869;&#24494;&#35843;&#20102;&#19977;&#31181;&#20027;&#35201;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#38750;ICL&#21644;ICL&#20219;&#21153;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#35813;&#26694;&#26550;&#20026;LLMs&#25552;&#20379;&#38646;&#27425;&#23581;&#35797;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07311v1 Announce Type: new  Abstract: The task of predicting multiple links within knowledge graphs (KGs) stands as a challenge in the field of knowledge graph analysis, a challenge increasingly resolvable due to advancements in natural language processing (NLP) and KG embedding techniques. This paper introduces a novel methodology, the Knowledge Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP paradigms, including chain-of-thought (CoT) prompting and in-context learning (ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a CoT prompt, our framework is designed to discern and learn the latent representations of entities and their interrelations. To show the efficacy of the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs) within this framework, employing both non-ICL and ICL tasks for a comprehensive evaluation. Further, we explore the framework's potential to provide LLMs with zero-shot capabilities f
&lt;/p&gt;</description></item><item><title>FastV&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#27169;&#24335;&#24182;&#22312;&#21518;&#32493;&#23618;&#20013;&#20462;&#21098;&#35270;&#35273;&#20195;&#24065;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#20013;&#19981;&#25439;&#22833;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.06764</link><description>&lt;p&gt;
&#19968;&#24352;&#22270;&#29255;&#22312;&#31532;&#20108;&#23618;&#20043;&#21518;&#20215;&#20540;1/2&#20195;&#24065;&#65306;&#38024;&#23545;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21363;&#25554;&#21363;&#29992;&#25512;&#29702;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06764
&lt;/p&gt;
&lt;p&gt;
FastV&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#27169;&#24335;&#24182;&#22312;&#21518;&#32493;&#23618;&#20013;&#20462;&#21098;&#35270;&#35273;&#20195;&#24065;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#20013;&#19981;&#25439;&#22833;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#20013;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#23384;&#22312;&#20302;&#25928;&#29616;&#35937;&#65292;&#23588;&#20854;&#26159;&#22312;&#30693;&#21517;&#27169;&#22411;&#22914;LLaVA-1.5&#12289;QwenVL-Chat&#21644;Video-LLaVA&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27969;&#34892;&#30340;LVLMs&#30340;&#28145;&#23618;&#20013;&#65292;&#23545;&#35270;&#35273;&#20195;&#24065;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#26497;&#20854;&#20302;&#25928;&#65292;&#26263;&#31034;&#30456;&#36739;&#20110;&#22788;&#29702;&#25991;&#26412;&#25968;&#25454;&#65292;&#38656;&#35201;&#26356;&#31232;&#30095;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FastV&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#26089;&#26399;&#23618;&#20013;&#30340;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#27169;&#24335;&#21644;&#22312;&#38543;&#21518;&#23618;&#20013;&#20462;&#21098;&#35270;&#35273;&#20195;&#24065;&#26469;&#20248;&#21270;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;FastV&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65288;&#20363;&#22914;&#65292;&#23545;&#20110;LLaVA-1.5-13B&#30340;FLOP&#20943;&#23569;&#20102;45%&#65289;&#65292;&#32780;&#19981;&#20250;&#22312;&#24191;&#27867;&#30340;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#20013;&#29306;&#29298;&#24615;&#33021;&#12290;FastV&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#24615;&#33021;&#26435;&#34913;&#26159;&#39640;&#24230;&#21487;&#23450;&#21046;&#30340;&#65292;&#24182;&#19988;&#26159;&#24085;&#32047;&#25176;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06764v1 Announce Type: cross  Abstract: In this study, we identify the inefficient attention phenomena in Large Vision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5, QwenVL-Chat and Video-LLaVA. We find out that the attention computation over visual tokens is of extreme inefficiency in the deep layers of popular LVLMs, suggesting a need for a sparser approach compared to textual data handling. To this end, we introduce FastV, a versatile plug-and-play method designed to optimize computational efficiency by learning adaptive attention patterns in early layers and pruning visual tokens in subsequent ones. Our evaluations demonstrate FastV's ability to dramatically reduce computational costs (e.g., a 45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a wide range of image and video understanding tasks. The computational efficiency and performance trade-off of FastV are highly customizable and pareto-efficient. It can compress t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;LoReKT&#30340;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#37325;&#35201;&#24615;&#26426;&#21046;&#65292;&#26088;&#22312;&#20174;&#20016;&#23500;&#36164;&#28304;&#30340;KT&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21644;&#34920;&#31034;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.06725</link><description>&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#37325;&#35201;&#24615;&#26426;&#21046;&#24494;&#35843;&#25913;&#36827;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Improving Low-Resource Knowledge Tracing Tasks by Supervised Pre-training and Importance Mechanism Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;LoReKT&#30340;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#37325;&#35201;&#24615;&#26426;&#21046;&#65292;&#26088;&#22312;&#20174;&#20016;&#23500;&#36164;&#28304;&#30340;KT&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21644;&#34920;&#31034;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#65288;KT&#65289;&#26088;&#22312;&#22522;&#20110;&#23398;&#29983;&#30340;&#21382;&#21490;&#20114;&#21160;&#26469;&#20272;&#35745;&#20182;&#20204;&#30340;&#30693;&#35782;&#25484;&#25569;&#31243;&#24230;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;KT&#65288;DLKT&#65289;&#26041;&#27861;&#22312;KT&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#65292;&#22914;&#39044;&#31639;&#38480;&#21046;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#35266;&#23519;&#21040;&#30340;&#20114;&#21160;&#38750;&#24120;&#26377;&#38480;&#65292;&#21363;&#20302;&#36164;&#28304;KT&#25968;&#25454;&#38598;&#12290;&#30452;&#25509;&#22312;&#20302;&#36164;&#28304;KT&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;DLKT&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#24182;&#19988;&#24456;&#38590;&#36873;&#25321;&#36866;&#24403;&#30340;&#28145;&#24230;&#31070;&#32463;&#26550;&#26500;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LoReKT&#30340;&#20302;&#36164;&#28304;KT&#26694;&#26550;&#26469;&#24212;&#23545;&#19978;&#36848;&#25361;&#25112;&#12290;&#21463;&#30427;&#34892;&#30340;&#8220;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#8221;&#33539;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26088;&#22312;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#20174;&#20016;&#23500;&#36164;&#28304;&#30340;KT&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21644;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06725v1 Announce Type: cross  Abstract: Knowledge tracing (KT) aims to estimate student's knowledge mastery based on their historical interactions. Recently, the deep learning based KT (DLKT) approaches have achieved impressive performance in the KT task. These DLKT models heavily rely on the large number of available student interactions. However, due to various reasons such as budget constraints and privacy concerns, observed interactions are very limited in many real-world scenarios, a.k.a, low-resource KT datasets. Directly training a DLKT model on a low-resource KT dataset may lead to overfitting and it is difficult to choose the appropriate deep neural architecture. Therefore, in this paper, we propose a low-resource KT framework called LoReKT to address above challenges. Inspired by the prevalent "pre-training and fine-tuning" paradigm, we aim to learn transferable parameters and representations from rich-resource KT datasets during the pre-training stage and subseque
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35774;&#35745;&#22810;&#27169;&#24577;&#23567;&#35821;&#35328;&#27169;&#22411;(MSLMs)&#21450;&#25552;&#20986;&#39640;&#25928;&#22810;&#27169;&#24577;&#21161;&#25163;Mipha&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#26041;&#38754;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#20987;&#36133;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#24320;&#21457;&#24378;&#22823;MSLMs&#25552;&#20379;&#20102;&#35265;&#35299;&#21644;&#25351;&#21335;</title><link>https://arxiv.org/abs/2403.06199</link><description>&lt;p&gt;
&#36890;&#36807;&#23567;&#35821;&#35328;&#27169;&#22411;&#20840;&#38754;&#25913;&#36896;&#22810;&#27169;&#24577;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Overhaul of Multimodal Assistant with Small Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06199
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35774;&#35745;&#22810;&#27169;&#24577;&#23567;&#35821;&#35328;&#27169;&#22411;(MSLMs)&#21450;&#25552;&#20986;&#39640;&#25928;&#22810;&#27169;&#24577;&#21161;&#25163;Mipha&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#26041;&#38754;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#20987;&#36133;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#24320;&#21457;&#24378;&#22823;MSLMs&#25552;&#20379;&#20102;&#35265;&#35299;&#21644;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#23637;&#31034;&#20102;&#22312;&#19982;&#35270;&#35273;&#29702;&#35299;&#21644;&#25512;&#29702;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22521;&#35757;&#21644;&#25512;&#29702;&#38454;&#27573;&#30340;&#39640;&#35745;&#31639;&#38656;&#27714;&#65292;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#38754;&#20020;&#38556;&#30861;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30740;&#31350;&#21644;&#29992;&#25143;&#31038;&#21306;&#20013;&#21463;&#20247;&#30340;&#33539;&#22260;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#23567;&#35821;&#35328;&#27169;&#22411;&#65288;MSLMs&#65289;&#30340;&#35774;&#35745;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mipha&#30340;&#39640;&#25928;&#22810;&#27169;&#24577;&#21161;&#25163;&#65292;&#26088;&#22312;&#22312;&#22810;&#20010;&#26041;&#38754;&#20043;&#38388;&#21019;&#36896;&#21327;&#21516;&#20316;&#29992;&#65306;&#35270;&#35273;&#34920;&#31034;&#12289;&#35821;&#35328;&#27169;&#22411;&#21644;&#20248;&#21270;&#31574;&#30053;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#19981;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;Mipha-3B&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;MLLMs&#65292;&#29305;&#21035;&#26159;LLaVA-1.5-13B&#12290;&#36890;&#36807;&#35814;&#32454;&#35752;&#35770;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21457;&#23637;&#24378;&#22823;&#30340;MSLMs&#30340;&#35265;&#35299;&#21644;&#25351;&#21335;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;MLLMs&#30340;&#33021;&#21147;&#30456;&#23218;&#32654;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20197;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06199v1 Announce Type: cross  Abstract: Multimodal Large Language Models (MLLMs) have showcased impressive skills in tasks related to visual understanding and reasoning. Yet, their widespread application faces obstacles due to the high computational demands during both the training and inference phases, restricting their use to a limited audience within the research and user communities. In this paper, we investigate the design aspects of Multimodal Small Language Models (MSLMs) and propose an efficient multimodal assistant named Mipha, which is designed to create synergy among various aspects: visual representation, language models, and optimization strategies. We show that without increasing the volume of training data, our Mipha-3B outperforms the state-of-the-art large MLLMs, especially LLaVA-1.5-13B, on multiple benchmarks. Through detailed discussion, we provide insights and guidelines for developing strong MSLMs that rival the capabilities of MLLMs. Our code is availa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20840;&#38754;&#35843;&#26597;&#21508;&#31181;&#25915;&#20987;&#24418;&#24335;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21463;&#25915;&#20987;&#30340;&#24615;&#36136;&#12289;&#26426;&#21046;&#12289;&#28508;&#22312;&#24433;&#21709;&#20197;&#21450;&#24403;&#21069;&#38450;&#24481;&#31574;&#30053;&#65292;&#20026;&#27169;&#22411;&#23436;&#25972;&#24615;&#21644;&#29992;&#25143;&#20449;&#20219;&#25552;&#20379;&#20102;&#37325;&#35201;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.04786</link><description>&lt;p&gt;
&#20998;&#35299;&#38450;&#24481;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25915;&#20987;&#30340;&#27604;&#36739;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20840;&#38754;&#35843;&#26597;&#21508;&#31181;&#25915;&#20987;&#24418;&#24335;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21463;&#25915;&#20987;&#30340;&#24615;&#36136;&#12289;&#26426;&#21046;&#12289;&#28508;&#22312;&#24433;&#21709;&#20197;&#21450;&#24403;&#21069;&#38450;&#24481;&#31574;&#30053;&#65292;&#20026;&#27169;&#22411;&#23436;&#25972;&#24615;&#21644;&#29992;&#25143;&#20449;&#20219;&#25552;&#20379;&#20102;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#22522;&#30707;&#65292;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#25991;&#26412;&#26041;&#38754;&#25552;&#20379;&#20102;&#38761;&#21629;&#24615;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#23427;&#20204;&#26085;&#30410;&#37325;&#35201;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#33030;&#24369;&#24615;&#26041;&#38754;&#24050;&#32463;&#24341;&#36215;&#20102;&#37325;&#35201;&#20851;&#27880;&#12290;&#26412;&#25991;&#23545;&#38024;&#23545;LLMs&#30340;&#21508;&#31181;&#24418;&#24335;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#35752;&#35770;&#20102;&#36825;&#20123;&#25915;&#20987;&#30340;&#24615;&#36136;&#21644;&#26426;&#21046;&#12289;&#23427;&#20204;&#30340;&#28508;&#22312;&#24433;&#21709;&#20197;&#21450;&#24403;&#21069;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#26088;&#22312;&#25805;&#32437;&#27169;&#22411;&#36755;&#20986;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12289;&#24433;&#21709;&#27169;&#22411;&#35757;&#32451;&#30340;&#25968;&#25454;&#20013;&#27602;&#20197;&#21450;&#19982;&#35757;&#32451;&#25968;&#25454;&#21033;&#29992;&#30456;&#20851;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#26412;&#25991;&#36824;&#25506;&#35752;&#20102;&#19981;&#21516;&#25915;&#20987;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12289;LLMs&#25269;&#25239;&#36825;&#20123;&#25915;&#20987;&#30340;&#38887;&#24615;&#20197;&#21450;&#23545;&#27169;&#22411;&#23436;&#25972;&#24615;&#21644;&#29992;&#25143;&#20449;&#20219;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23457;&#35270;&#26368;&#26032;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04786v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have become a cornerstone in the field of Natural Language Processing (NLP), offering transformative capabilities in understanding and generating human-like text. However, with their rising prominence, the security and vulnerability aspects of these models have garnered significant attention. This paper presents a comprehensive survey of the various forms of attacks targeting LLMs, discussing the nature and mechanisms of these attacks, their potential impacts, and current defense strategies. We delve into topics such as adversarial attacks that aim to manipulate model outputs, data poisoning that affects model training, and privacy concerns related to training data exploitation. The paper also explores the effectiveness of different attack methodologies, the resilience of LLMs against these attacks, and the implications for model integrity and user trust. By examining the latest research, we provide insight
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#20174;&#22270;&#21040;&#35789;&#34955;&#26041;&#27861;&#65292;&#24110;&#21161;&#39044;&#27979;&#28151;&#28102;&#32618;&#21517;&#65292;&#36890;&#36807;&#26500;&#25104;&#35201;&#32032;&#21644;&#20851;&#38190;&#35789;&#36873;&#25321;&#36827;&#34892;&#21028;&#26029;&#12290;</title><link>https://arxiv.org/abs/2403.04369</link><description>&lt;p&gt;
&#20174;&#22270;&#21040;&#35789;&#34955;: &#23558;&#39046;&#22495;&#30693;&#35782;&#24341;&#20837;&#28151;&#28102;&#32618;&#21517;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
From Graph to Word Bag: Introducing Domain Knowledge to Confusing Charge Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04369
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#20174;&#22270;&#21040;&#35789;&#34955;&#26041;&#27861;&#65292;&#24110;&#21161;&#39044;&#27979;&#28151;&#28102;&#32618;&#21517;&#65292;&#36890;&#36807;&#26500;&#25104;&#35201;&#32032;&#21644;&#20851;&#38190;&#35789;&#36873;&#25321;&#36827;&#34892;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#28102;&#32618;&#21517;&#39044;&#27979;&#26159;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#26681;&#25454;&#20107;&#23454;&#25551;&#36848;&#39044;&#27979;&#28151;&#28102;&#32618;&#21517;&#12290;&#29616;&#26377;&#30340;&#32618;&#21517;&#39044;&#27979;&#26041;&#27861;&#22312;&#34920;&#29616;&#19978;&#24050;&#32463;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25928;&#26524;&#65292;&#20294;&#22312;&#22788;&#29702;&#28151;&#28102;&#32618;&#21517;&#65288;&#22914;&#25250;&#22842;&#19982;&#25250;&#21163;&#65289;&#26102;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#27861;&#24459;&#39046;&#22495;&#65292;&#26500;&#25104;&#35201;&#32032;&#22312;&#21306;&#20998;&#28151;&#28102;&#32618;&#21517;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#26500;&#25104;&#35201;&#32032;&#26159;&#28508;&#22312;&#21009;&#32602;&#32972;&#21518;&#30340;&#22522;&#26412;&#34892;&#20026;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#32618;&#21517;&#20043;&#38388;&#26377;&#24494;&#22937;&#30340;&#21306;&#21035;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20174;&#22270;&#21040;&#35789;&#34955;&#65288;FWGB&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#26377;&#20851;&#26500;&#25104;&#35201;&#32032;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#20197;&#25351;&#23548;&#27169;&#22411;&#22312;&#28151;&#28102;&#32618;&#21517;&#19978;&#20570;&#20986;&#21028;&#26029;&#65292;&#31867;&#20284;&#20110;&#27861;&#23448;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#26500;&#25104;&#35201;&#32032;&#30340;&#27861;&#24459;&#30693;&#35782;&#22270;&#65292;&#20197;&#24110;&#21161;&#20026;&#27599;&#31181;&#32618;&#21517;&#36873;&#25321;&#20851;&#38190;&#35789;&#65292;&#24418;&#25104;&#19968;&#20010;&#21333;&#35789;&#34955;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04369v1 Announce Type: new  Abstract: Confusing charge prediction is a challenging task in legal AI, which involves predicting confusing charges based on fact descriptions. While existing charge prediction methods have shown impressive performance, they face significant challenges when dealing with confusing charges, such as Snatch and Robbery. In the legal domain, constituent elements play a pivotal role in distinguishing confusing charges. Constituent elements are fundamental behaviors underlying criminal punishment and have subtle distinctions among charges. In this paper, we introduce a novel From Graph to Word Bag (FWGB) approach, which introduces domain knowledge regarding constituent elements to guide the model in making judgments on confusing charges, much like a judge's reasoning process. Specifically, we first construct a legal knowledge graph containing constituent elements to help select keywords for each charge, forming a word bag. Subsequently, to guide the mod
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22797;&#21046;&#30740;&#31350;BASS&#26694;&#26550;&#65292;&#21457;&#29616;&#20102;&#19982;&#21407;&#22987;&#24037;&#20316;&#30456;&#27604;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#65292;&#24182;&#24378;&#35843;&#20102;&#25776;&#20889;&#21487;&#22797;&#21046;&#35770;&#25991;&#30340;&#20851;&#38190;&#23454;&#36341;&#12290;</title><link>https://arxiv.org/abs/2403.02930</link><description>&lt;p&gt;
BASS&#30340;&#20877;&#23457;&#35270;--&#21033;&#29992;&#32479;&#19968;&#35821;&#20041;&#22270;&#25552;&#21319;&#25277;&#35937;&#25688;&#35201;--&#19968;&#39033;&#22797;&#21046;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Second Look on BASS -- Boosting Abstractive Summarization with Unified Semantic Graphs -- A Replication Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02930
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22797;&#21046;&#30740;&#31350;BASS&#26694;&#26550;&#65292;&#21457;&#29616;&#20102;&#19982;&#21407;&#22987;&#24037;&#20316;&#30456;&#27604;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#65292;&#24182;&#24378;&#35843;&#20102;&#25776;&#20889;&#21487;&#22797;&#21046;&#35770;&#25991;&#30340;&#20851;&#38190;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;BASS&#26694;&#26550;&#30340;&#35814;&#32454;&#22797;&#21046;&#30740;&#31350;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#32479;&#19968;&#35821;&#20041;&#22270;&#27010;&#24565;&#30340;&#25277;&#35937;&#25688;&#35201;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#21253;&#25324;&#22797;&#21046;&#20851;&#38190;&#32452;&#20214;&#26102;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#19968;&#20010;&#28040;&#34701;&#30740;&#31350;&#26469;&#31995;&#32479;&#22320;&#38548;&#31163;&#22312;&#22797;&#21046;&#26032;&#39062;&#32452;&#20214;&#26102;&#26681;&#28304;&#20110;&#38169;&#35823;&#26469;&#28304;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#19982;&#21407;&#22987;&#24037;&#20316;&#30456;&#27604;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#21363;&#20351;&#26159;&#34987;&#21512;&#29702;&#30465;&#30053;&#30340;&#32454;&#33410;&#23545;&#20110;&#22797;&#21046;&#20687;BASS&#36825;&#26679;&#30340;&#20808;&#36827;&#26694;&#26550;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#25776;&#20889;&#21487;&#22797;&#21046;&#35770;&#25991;&#30340;&#20851;&#38190;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02930v1 Announce Type: new  Abstract: We present a detailed replication study of the BASS framework, an abstractive summarization system based on the notion of Unified Semantic Graphs. Our investigation includes challenges in replicating key components and an ablation study to systematically isolate error sources rooted in replicating novel components. Our findings reveal discrepancies in performance compared to the original work. We highlight the significance of paying careful attention even to reasonably omitted details for replicating advanced frameworks like BASS, and emphasize key practices for writing replicable papers.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;InjecAgent&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#24037;&#20855;&#38598;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#23545;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#36890;&#36807;&#35780;&#20272;30&#31181;LLM&#20195;&#29702;&#65292;&#21457;&#29616;&#36825;&#20123;&#20195;&#29702;&#23384;&#22312;&#28431;&#27934;</title><link>https://arxiv.org/abs/2403.02691</link><description>&lt;p&gt;
InjecAgent&#65306;&#22522;&#20110;&#24037;&#20855;&#38598;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Agent&#20013;&#30340;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;InjecAgent&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#24037;&#20855;&#38598;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#23545;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#36890;&#36807;&#35780;&#20272;30&#31181;LLM&#20195;&#29702;&#65292;&#21457;&#29616;&#36825;&#20123;&#20195;&#29702;&#23384;&#22312;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#24037;&#20316;&#23558;LLMs&#20316;&#20026;&#20195;&#29702;&#20307;&#29616;&#20986;&#26469;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#35775;&#38382;&#24037;&#20855;&#65292;&#25191;&#34892;&#25805;&#20316;&#65292;&#24182;&#19982;&#22806;&#37096;&#20869;&#23481;&#65288;&#20363;&#22914;&#65292;&#30005;&#23376;&#37038;&#20214;&#25110;&#32593;&#31449;&#65289;&#36827;&#34892;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#22806;&#37096;&#20869;&#23481;&#24341;&#20837;&#20102;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#65288;IPI&#65289;&#25915;&#20987;&#30340;&#39118;&#38505;&#65292;&#24694;&#24847;&#25351;&#20196;&#34987;&#23884;&#20837;LLMs&#22788;&#29702;&#30340;&#20869;&#23481;&#20013;&#65292;&#26088;&#22312;&#25805;&#32437;&#36825;&#20123;&#20195;&#29702;&#25191;&#34892;&#23545;&#29992;&#25143;&#26377;&#23475;&#30340;&#25805;&#20316;&#12290;&#32771;&#34385;&#21040;&#36825;&#31867;&#25915;&#20987;&#30340;&#28508;&#22312;&#20005;&#37325;&#21518;&#26524;&#65292;&#24314;&#31435;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#30340;&#22522;&#20934;&#27979;&#35797;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;InjecAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#24037;&#20855;&#38598;&#25104;&#30340;LLM&#20195;&#29702;&#23545;IPI&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;InjecAgent&#21253;&#25324;1,054&#20010;&#27979;&#35797;&#29992;&#20363;&#65292;&#28085;&#30422;17&#31181;&#19981;&#21516;&#30340;&#29992;&#25143;&#24037;&#20855;&#21644;62&#31181;&#25915;&#20987;&#32773;&#24037;&#20855;&#12290;&#25105;&#20204;&#23558;&#25915;&#20987;&#24847;&#22270;&#20998;&#20026;&#20004;&#31181;&#20027;&#35201;&#31867;&#22411;&#65306;&#23545;&#29992;&#25143;&#36896;&#25104;&#30452;&#25509;&#20260;&#23475;&#21644;&#31363;&#21462;&#31169;&#20154;&#25968;&#25454;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;30&#31181;&#19981;&#21516;&#30340;LLM&#20195;&#29702;&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#20195;&#29702;&#26159;&#33030;&#24369;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02691v1 Announce Type: new  Abstract: Recent work has embodied LLMs as agents, allowing them to access tools, perform actions, and interact with external content (e.g., emails or websites). However, external content introduces the risk of indirect prompt injection (IPI) attacks, where malicious instructions are embedded within the content processed by LLMs, aiming to manipulate these agents into executing detrimental actions against users. Given the potentially severe consequences of such attacks, establishing benchmarks to assess and mitigate these risks is imperative.   In this work, we introduce InjecAgent, a benchmark designed to assess the vulnerability of tool-integrated LLM agents to IPI attacks. InjecAgent comprises 1,054 test cases covering 17 different user tools and 62 attacker tools. We categorize attack intentions into two primary types: direct harm to users and exfiltration of private data. We evaluate 30 different LLM agents and show that agents are vulnerable
&lt;/p&gt;</description></item><item><title>"&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;Align-to-Distill&#8221;&#65288;A2D&#65289;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#23545;&#40784;&#23398;&#29983;&#27880;&#24847;&#21147;&#22836;&#19982;&#20854;&#25945;&#24072;&#23545;&#24212;&#29289;&#65292;&#36716;&#21270;&#20102;&#32452;&#21512;&#26144;&#23556;&#21551;&#21457;&#24335;&#26041;&#27861;&#20026;&#23398;&#20064;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;A2D&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;WMT-2022 De-&gt;Dsb&#21644;WMT-2014 En-&gt;De&#30340;BLEU&#20998;&#25968;&#20998;&#21035;&#33719;&#24471;&#39640;&#36798;+3.61&#21644;+0.63&#30340;&#25552;&#21319;&#12290;"</title><link>https://arxiv.org/abs/2403.01479</link><description>&lt;p&gt;
Align-to-Distill: &#21487;&#35757;&#32451;&#30340;&#27880;&#24847;&#21147;&#23545;&#40784;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Align-to-Distill: Trainable Attention Alignment for Knowledge Distillation in Neural Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01479
&lt;/p&gt;
&lt;p&gt;
"&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;Align-to-Distill&#8221;&#65288;A2D&#65289;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#23545;&#40784;&#23398;&#29983;&#27880;&#24847;&#21147;&#22836;&#19982;&#20854;&#25945;&#24072;&#23545;&#24212;&#29289;&#65292;&#36716;&#21270;&#20102;&#32452;&#21512;&#26144;&#23556;&#21551;&#21457;&#24335;&#26041;&#27861;&#20026;&#23398;&#20064;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;A2D&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;WMT-2022 De-&gt;Dsb&#21644;WMT-2014 En-&gt;De&#30340;BLEU&#20998;&#25968;&#20998;&#21035;&#33719;&#24471;&#39640;&#36798;+3.61&#21644;+0.63&#30340;&#25552;&#21319;&#12290;"
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#28145;&#24230;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#20986;&#29616;&#25552;&#39640;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#24615;&#33021;&#12290;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#36890;&#36807;&#23558;&#30693;&#35782;&#20174;&#25945;&#24072;&#27169;&#22411;&#20256;&#36755;&#21040;&#26356;&#32039;&#20945;&#30340;&#23398;&#29983;&#27169;&#22411;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;Transformer&#26550;&#26500;&#30340;KD&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#20915;&#23450;&#35201;&#20174;&#21738;&#20123;&#25945;&#24072;&#23618;&#20013;&#33976;&#39311;&#30693;&#35782;&#26102;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;Align-to-Distill&#8221;&#65288;A2D&#65289;&#31574;&#30053;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#23545;&#40784;&#23398;&#29983;&#27880;&#24847;&#21147;&#22836;&#19982;&#20854;&#25945;&#24072;&#23545;&#24212;&#29289;&#26469;&#35299;&#20915;&#29305;&#24449;&#26144;&#23556;&#38382;&#39064;&#12290;A2D&#20013;&#30340;&#27880;&#24847;&#21147;&#23545;&#40784;&#27169;&#22359;&#25191;&#34892;&#23398;&#29983;&#21644;&#25945;&#24072;&#27880;&#24847;&#21147;&#22836;&#20043;&#38388;&#30340;&#23494;&#38598;&#36880;&#22836;&#27604;&#36739;&#65292;&#23558;&#32452;&#21512;&#26144;&#23556;&#21551;&#21457;&#24335;&#26041;&#27861;&#36716;&#21270;&#20026;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;A2D&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;WMT-2022 De-&gt;Dsb&#21644;WMT-2014 En-&gt;De&#30340;BLEU&#20998;&#25968;&#20998;&#21035;&#33719;&#24471;&#39640;&#36798;+3.61&#21644;+0.63&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01479v1 Announce Type: cross  Abstract: The advent of scalable deep models and large datasets has improved the performance of Neural Machine Translation. Knowledge Distillation (KD) enhances efficiency by transferring knowledge from a teacher model to a more compact student model. However, KD approaches to Transformer architecture often rely on heuristics, particularly when deciding which teacher layers to distill from. In this paper, we introduce the 'Align-to-Distill' (A2D) strategy, designed to address the feature mapping problem by adaptively aligning student attention heads with their teacher counterparts during training. The Attention Alignment Module in A2D performs a dense head-by-head comparison between student and teacher attention heads across layers, turning the combinatorial mapping heuristics into a learning problem. Our experiments show the efficacy of A2D, demonstrating gains of up to +3.61 and +0.63 BLEU points for WMT-2022 De-&gt;Dsb and WMT-2014 En-&gt;De, respe
&lt;/p&gt;</description></item><item><title>SEED&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sample-Efficient adaptation with Error-Driven learning&#30340;&#26032;&#39062;&#36866;&#24212;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#20135;&#29983;&#30340;&#38169;&#35823;&#20316;&#20026;&#23398;&#20064;&#26426;&#20250;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.00046</link><description>&lt;p&gt;
&#20351;&#29992;&#26679;&#26412;&#39640;&#25928;&#36866;&#24212;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#23450;&#20041;&#20197;&#36827;&#34892;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SEED: Customize Large Language Models with Sample-Efficient Adaptation for Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00046
&lt;/p&gt;
&lt;p&gt;
SEED&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sample-Efficient adaptation with Error-Driven learning&#30340;&#26032;&#39062;&#36866;&#24212;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#20135;&#29983;&#30340;&#38169;&#35823;&#20316;&#20026;&#23398;&#20064;&#26426;&#20250;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#29305;&#23450;&#22330;&#26223;&#19979;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#20123;&#22330;&#26223;&#36890;&#24120;&#38656;&#35201;&#35843;&#25972;LLMs&#20197;&#28385;&#36275;&#29305;&#23450;&#38656;&#27714;&#65292;&#20294;&#23454;&#38469;&#21487;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#65292;&#23548;&#33268;&#20195;&#30721;&#29983;&#25104;&#24615;&#33021;&#36739;&#24046;&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#35843;&#25972;LLMs&#20197;&#36866;&#24212;&#26032;&#22330;&#26223;&#24182;&#20351;&#29992;&#26356;&#23569;&#30340;&#35757;&#32451;&#26679;&#26412;&#26159;&#24403;&#21069;&#20195;&#30721;&#29983;&#25104;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEED&#30340;&#26032;&#39062;&#36866;&#24212;&#26041;&#27861;&#65292;&#21363;Sample-Efficient adaptation with Error-Driven learning for code generation&#12290;SEED&#21033;&#29992;LLMs&#20135;&#29983;&#30340;&#38169;&#35823;&#20316;&#20026;&#23398;&#20064;&#26426;&#20250;&#65292;&#21033;&#29992;&#38169;&#35823;&#20462;&#35746;&#26469;&#20811;&#26381;&#33258;&#36523;&#32570;&#28857;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SEED&#28041;&#21450;&#35782;&#21035;LLMs&#29983;&#25104;&#30340;&#38169;&#35823;&#20195;&#30721;&#65292;&#20351;&#29992;Self-revise&#36827;&#34892;&#20195;&#30721;&#20462;&#35746;&#65292;&#20248;&#21270;&#27169;&#22411;&#24182;&#36845;&#20195;&#22320;&#36827;&#34892;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00046v1 Announce Type: cross  Abstract: Although Large Language Models (LLMs) have made significant progress in code generation, they still struggle with code generation tasks in specific scenarios. These scenarios usually necessitate the adaptation of LLMs to fulfill specific needs, but the limited training data available in practice leads to poor code generation performance. How to effectively adapt LLMs to new scenarios with fewer training samples is a major challenge for current code generation. In this paper, we propose a novel adaptation approach named SEED, which stands for Sample-Efficient adaptation with Error-Driven learning for code generation. SEED leverages the errors made by LLMs as learning opportunities, using error revision to overcome its own shortcomings, thus achieving efficient learning. Specifically, SEED involves identifying error code generated by LLMs, employing Self-revise for code revision, optimizing the model with revised code, and iteratively ad
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#26041;&#35328;&#20998;&#31867;&#22120;&#25552;&#21462;&#26041;&#35328;&#30340;&#35789;&#27719;&#29305;&#24449;&#65292;&#25104;&#21151;&#35782;&#21035;&#20102;&#26377;&#21161;&#20110;&#26041;&#35328;&#21464;&#21270;&#30340;&#20851;&#38190;&#35821;&#35328;&#29305;&#23450;&#35789;&#27719;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.17914</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#26041;&#35328;&#20998;&#31867;&#22120;&#25552;&#21462;&#26041;&#35328;&#30340;&#35789;&#27719;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Extracting Lexical Features from Dialects via Interpretable Dialect Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17914
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#26041;&#35328;&#20998;&#31867;&#22120;&#25552;&#21462;&#26041;&#35328;&#30340;&#35789;&#27719;&#29305;&#24449;&#65292;&#25104;&#21151;&#35782;&#21035;&#20102;&#26377;&#21161;&#20110;&#26041;&#35328;&#21464;&#21270;&#30340;&#20851;&#38190;&#35821;&#35328;&#29305;&#23450;&#35789;&#27719;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#19968;&#31181;&#35821;&#35328;&#30340;&#26041;&#35328;&#20043;&#38388;&#30340;&#35821;&#35328;&#24046;&#24322;&#36890;&#24120;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#21644;&#32454;&#33268;&#30340;&#20154;&#31867;&#20998;&#26512;&#12290;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#30740;&#31350;&#21508;&#31181;&#26041;&#35328;&#28041;&#21450;&#21040;&#22797;&#26434;&#24615;&#21644;&#24494;&#22937;&#20043;&#22788;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#26041;&#35328;&#20998;&#31867;&#22120;&#25552;&#21462;&#26041;&#35328;&#30340;&#21306;&#20998;&#24615;&#35789;&#27719;&#29305;&#24449;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#20154;&#31867;&#19987;&#23478;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20107;&#21518;&#21644;&#20869;&#22312;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#23545;&#26222;&#36890;&#35805;&#12289;&#24847;&#22823;&#21033;&#35821;&#21644;&#20302;&#22320;&#33832;&#20811;&#26862;&#35821;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#35782;&#21035;&#20102;&#26377;&#21161;&#20110;&#26041;&#35328;&#21464;&#21270;&#30340;&#20851;&#38190;&#35821;&#35328;&#29305;&#23450;&#35789;&#27719;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17914v1 Announce Type: cross  Abstract: Identifying linguistic differences between dialects of a language often requires expert knowledge and meticulous human analysis. This is largely due to the complexity and nuance involved in studying various dialects. We present a novel approach to extract distinguishing lexical features of dialects by utilizing interpretable dialect classifiers, even in the absence of human experts. We explore both post-hoc and intrinsic approaches to interpretability, conduct experiments on Mandarin, Italian, and Low Saxon, and experimentally demonstrate that our method successfully identifies key language-specific lexical features that contribute to dialectal variations.
&lt;/p&gt;</description></item><item><title>HumanEval-XL &#26159;&#19968;&#20010;&#38754;&#21521;&#36328;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#27867;&#21270;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#65292;&#24314;&#31435;23&#31181;&#33258;&#28982;&#35821;&#35328;&#21644;12&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#32852;&#31995;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#24179;&#21488;&#65292;&#24357;&#34917;&#20102;&#22810;&#35821;&#35328;LLM&#35780;&#20272;&#30340;&#37325;&#35201;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.16694</link><description>&lt;p&gt;
HumanEval-XL&#65306;&#38754;&#21521;&#36328;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#27867;&#21270;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16694
&lt;/p&gt;
&lt;p&gt;
HumanEval-XL &#26159;&#19968;&#20010;&#38754;&#21521;&#36328;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#27867;&#21270;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#65292;&#24314;&#31435;23&#31181;&#33258;&#28982;&#35821;&#35328;&#21644;12&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#32852;&#31995;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#24179;&#21488;&#65292;&#24357;&#34917;&#20102;&#22810;&#35821;&#35328;LLM&#35780;&#20272;&#30340;&#37325;&#35201;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#20174;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#20195;&#30721;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20934;&#20027;&#35201;&#38598;&#20013;&#22312;&#23558;&#33521;&#35821;&#25552;&#31034;&#32763;&#35793;&#20026;&#22810;&#35821;&#35328;&#20195;&#30721;&#65292;&#25110;&#32773;&#20165;&#38480;&#20110;&#38750;&#24120;&#26377;&#38480;&#30340;&#33258;&#28982;&#35821;&#35328;(NLs)&#12290;&#36825;&#20123;&#22522;&#20934;&#24573;&#35270;&#20102;&#24222;&#22823;&#30340;&#20316;&#20026;&#23545;&#27604;&#30340;&#22810;&#35821;&#35328;NL&#21040;&#22810;&#35821;&#35328;&#20195;&#30721;&#30340;&#24191;&#38420;&#39046;&#22495;&#65292;&#23548;&#33268;&#20102;&#23545;&#22810;&#35821;&#35328;LLM&#35780;&#20272;&#30340;&#37325;&#22823;&#31354;&#30333;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;HumanEval-XL&#65292;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#26469;&#35299;&#20915;&#36825;&#19968;&#19981;&#36275;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#12290;HumanEval-XL&#24314;&#31435;&#20102;23&#31181;NL&#21644;12&#31181;&#32534;&#31243;&#35821;&#35328;(PLs)&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#21253;&#25324;&#20102;22,080&#20010;&#25552;&#31034;&#30340;&#38598;&#21512;&#65292;&#24179;&#22343;&#26377;8.33&#20010;&#27979;&#35797;&#29992;&#20363;&#12290;&#36890;&#36807;&#30830;&#20445;&#22312;&#22810;&#20010;NL&#21644;PL&#20043;&#38388;&#30340;&#24182;&#34892;&#25968;&#25454;&#65292;HumanEval-XL&#20026;&#22810;&#35821;&#35328;LLMs&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#24179;&#21488;&#65292;&#20801;&#35768;&#35780;&#20272;&#23545;&#19981;&#21516;NL&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16694v1 Announce Type: new  Abstract: Large language models (LLMs) have made significant progress in generating codes from textual prompts. However, existing benchmarks have mainly concentrated on translating English prompts to multilingual codes or have been constrained to very limited natural languages (NLs). These benchmarks have overlooked the vast landscape of massively multilingual NL to multilingual code, leaving a critical gap in the evaluation of multilingual LLMs. In response, we introduce HumanEval-XL, a massively multilingual code generation benchmark specifically crafted to address this deficiency. HumanEval-XL establishes connections between 23 NLs and 12 programming languages (PLs), and comprises of a collection of 22,080 prompts with an average of 8.33 test cases. By ensuring parallel data across multiple NLs and PLs, HumanEval-XL offers a comprehensive evaluation platform for multilingual LLMs, allowing the assessment of the understanding of different NLs. O
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#23569;&#26679;&#26412;&#25552;&#31034;&#21644;Llama-2-70B&#22686;&#24378;MultiWOZ&#25968;&#25454;&#38598;&#65292;&#24341;&#20837;&#29992;&#25143;&#32972;&#26223;&#25925;&#20107;&#65292;&#26377;&#25928;&#35299;&#20915;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#20013;&#30340;&#38386;&#32842;&#24178;&#25200;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#21516;&#26102;&#25215;&#35748;&#29992;&#25143;&#32972;&#26223;&#25925;&#20107;&#24182;&#25512;&#21160;&#20219;&#21153;&#30340;&#36827;&#34892;&#12290;</title><link>https://arxiv.org/abs/2402.15248</link><description>&lt;p&gt;
Chitchat&#20316;&#20026;&#24178;&#25200;&#65306;&#21521;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#28155;&#21152;&#29992;&#25143;&#32972;&#26223;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Chitchat as Interference: Adding User Backstories to Task-Oriented Dialogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15248
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#23569;&#26679;&#26412;&#25552;&#31034;&#21644;Llama-2-70B&#22686;&#24378;MultiWOZ&#25968;&#25454;&#38598;&#65292;&#24341;&#20837;&#29992;&#25143;&#32972;&#26223;&#25925;&#20107;&#65292;&#26377;&#25928;&#35299;&#20915;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#20013;&#30340;&#38386;&#32842;&#24178;&#25200;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#21516;&#26102;&#25215;&#35748;&#29992;&#25143;&#32972;&#26223;&#25925;&#20107;&#24182;&#25512;&#21160;&#20219;&#21153;&#30340;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#65288;TOD&#65289;&#20013;&#65292;&#20154;&#31867;&#29992;&#25143;&#33258;&#28982;&#20250;&#24341;&#20837;&#36229;&#20986;&#20219;&#21153;&#33539;&#22260;&#30340;&#38386;&#32842;&#65292;&#24178;&#25200;&#20102;&#23545;&#35805;&#30340;&#27969;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;Llama-2-70B&#36827;&#34892;&#23569;&#26679;&#26412;&#25552;&#31034;&#65292;&#20197;&#22686;&#24378;MultiWOZ&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#29992;&#25143;&#32972;&#26223;&#25925;&#20107;&#65292;&#36825;&#26159;TOD&#20013;&#20856;&#22411;&#30340;&#38386;&#32842;&#24178;&#25200;&#30340;&#19968;&#20010;&#20363;&#23376;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#35797;&#20004;&#20010;&#27169;&#22411;&#26469;&#35780;&#20272;&#27492;&#28155;&#21152;&#30340;&#24433;&#21709;&#65306;&#19968;&#20010;&#20165;&#22312;TOD&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#21478;&#19968;&#20010;&#22312;TOD&#19978;&#36827;&#34892;&#21021;&#27493;&#38386;&#32842;&#20132;&#20114;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#23545;&#36825;&#20123;&#31995;&#32479;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#26377;&#25928;&#29992;&#20110;&#35757;&#32451;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#22312;&#21516;&#19968;&#36718;&#20013;&#25345;&#32493;&#25215;&#35748;&#29992;&#25143;&#32972;&#26223;&#25925;&#20107;&#24182;&#25104;&#21151;&#25512;&#21160;&#20219;&#21153;&#30340;&#36827;&#34892;&#65292;&#36825;&#24471;&#21040;&#20102;&#20154;&#31867;&#35780;&#20272;&#30340;&#30830;&#35748;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#24341;&#20837;&#29992;&#25143;&#32972;&#26223;&#25925;&#20107;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15248v1 Announce Type: new  Abstract: During task-oriented dialogues (TODs), human users naturally introduce chitchat that is beyond the immediate scope of the task, interfering with the flow of the conversation. To address this issue without the need for expensive manual data creation, we use few-shot prompting with Llama-2-70B to enhance the MultiWOZ dataset with user backstories, a typical example of chitchat interference in TODs. We assess the impact of this addition by testing two models: one trained solely on TODs and another trained on TODs with a preliminary chitchat interaction. Our analysis reveals that our enriched dataset poses a significant challenge to these systems. Moreover, we demonstrate that our dataset can be effectively used for training purposes, enabling a system to consistently acknowledge the user's backstory while also successfully moving the task forward in the same turn, as confirmed by human evaluation. These findings highlight the benefits of ge
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;31&#31181;&#21335;&#20122;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#20849;&#25351;&#35299;&#26512;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#25104;&#24037;&#20855;&#36827;&#34892;&#35757;&#32451;&#21644;&#23545;&#40784;&#65292;&#22312;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#20849;&#25351;&#35299;&#26512;&#27169;&#22411;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.13571</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#21335;&#20122;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#20849;&#25351;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Multilingual Coreference Resolution in Low-resource South Asian Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13571
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;31&#31181;&#21335;&#20122;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#20849;&#25351;&#35299;&#26512;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#25104;&#24037;&#20855;&#36827;&#34892;&#35757;&#32451;&#21644;&#23545;&#40784;&#65292;&#22312;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#20849;&#25351;&#35299;&#26512;&#27169;&#22411;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#25351;&#35299;&#26512;&#28041;&#21450;&#35782;&#21035;&#22312;&#35805;&#35821;&#20013;&#25351;&#21521;&#21516;&#19968;&#29616;&#23454;&#23454;&#20307;&#30340;&#25991;&#26412;&#29255;&#27573;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#36825;&#19968;&#20219;&#21153;&#22312;&#33521;&#35821;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#21335;&#20122;&#35821;&#35328;&#20013;&#65292;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#20849;&#25351;&#35299;&#26512;&#36164;&#28304;&#21644;&#27169;&#22411;&#30456;&#23545;&#31232;&#32570;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#25104;&#30340;&#32763;&#35793;&#21644;&#35789;&#23545;&#40784;&#24037;&#20855;&#65292;&#22312;31&#31181;&#21335;&#20122;&#35821;&#35328;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#22810;&#35821;&#35328;&#20849;&#25351;&#35299;&#26512;&#30340;&#32763;&#35793;&#25968;&#25454;&#38598;&#65288;TransMuCoRes&#65289;&#12290;&#20960;&#20046;&#25152;&#26377;&#39044;&#27979;&#30340;&#32763;&#35793;&#37117;&#36890;&#36807;&#20102;&#21512;&#29702;&#24615;&#26816;&#26597;&#65292;75%&#30340;&#33521;&#35821;&#21442;&#32771;&#25991;&#29486;&#19982;&#20854;&#39044;&#27979;&#30340;&#32763;&#35793;&#30456;&#23545;&#24212;&#12290;&#21033;&#29992;&#22810;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#20004;&#31181;&#29616;&#25104;&#30340;&#20849;&#25351;&#35299;&#26512;&#27169;&#22411;&#65292;&#23558;TransMuCoRes&#19982;&#24102;&#26377;&#25163;&#21160;&#27880;&#37322;&#30340;&#21360;&#22320;&#35821;&#20849;&#25351;&#35299;&#26512;&#25968;&#25454;&#38598;&#25340;&#25509;&#22312;&#19968;&#36215;&#12290;&#26368;&#20339;&#34920;&#29616;&#27169;&#22411;&#22312;LEA F1&#21644;CoNLL F1&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;64&#21644;68&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13571v1 Announce Type: cross  Abstract: Coreference resolution involves the task of identifying text spans within a discourse that pertain to the same real-world entity. While this task has been extensively explored in the English language, there has been a notable scarcity of publicly accessible resources and models for coreference resolution in South Asian languages. We introduce a Translated dataset for Multilingual Coreference Resolution (TransMuCoRes) in 31 South Asian languages using off-the-shelf tools for translation and word-alignment. Nearly all of the predicted translations successfully pass a sanity check, and 75% of English references align with their predicted translations. Using multilingual encoders, two off-the-shelf coreference resolution models were trained on a concatenation of TransMuCoRes and a Hindi coreference resolution dataset with manual annotations. The best performing model achieved a score of 64 and 68 for LEA F1 and CoNLL F1, respectively, on o
&lt;/p&gt;</description></item><item><title>LongHeads &#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37322;&#25918;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#28508;&#21147;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10685</link><description>&lt;p&gt;
LongHeads: &#22810;&#22836;&#27880;&#24847;&#21147;&#20854;&#23454;&#26159;&#19968;&#20010;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
LongHeads: Multi-Head Attention is Secretly a Long Context Processor
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10685
&lt;/p&gt;
&lt;p&gt;
LongHeads &#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37322;&#25918;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#28508;&#21147;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#65292;&#20294;&#30001;&#20110;&#26377;&#38480;&#38271;&#24230;&#27867;&#21270;&#21644;&#27880;&#24847;&#21147;&#30340;&#20108;&#27425;&#35745;&#31639;&#38656;&#27714;&#65292;&#24448;&#24448;&#38590;&#20197;&#26377;&#25928;&#39640;&#25928;&#22320;&#22788;&#29702;&#36739;&#38271;&#30340;&#36755;&#20837;&#12290; &#35768;&#22810;&#20154;&#35797;&#22270;&#36890;&#36807;&#38480;&#21046;&#22312;&#39044;&#35757;&#32451;&#38271;&#24230;&#20869;&#30340;&#27880;&#24847;&#21147;&#31383;&#21475;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290; &#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24341;&#20837;&#20102;&#26032;&#38382;&#39064;&#65292;&#22914;&#24573;&#30053;&#20013;&#38388;&#19978;&#19979;&#25991;&#21644;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LongHeads&#65292;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37322;&#25918;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#28508;&#21147;&#26469;&#22686;&#24378;LLM&#30340;&#38271;&#19978;&#19979;&#25991;&#33021;&#21147;&#12290; &#25105;&#20204;&#20801;&#35768;&#27599;&#20010;&#22836;&#37096;&#36873;&#25321;&#24182;&#20851;&#27880;&#37325;&#35201;&#30340;&#19978;&#19979;&#25991;&#22359;&#65292;&#20197;&#22788;&#29702;&#20998;&#24067;&#38271;&#24230;&#65292;&#32780;&#19981;&#26159;&#35753;&#27599;&#20010;&#22836;&#37096;&#37117;&#21442;&#19982;&#20840;&#21477;&#27880;&#24847;&#21147;&#65292;&#36825;&#26679;&#20570;&#30001;&#20110;&#20998;&#24067;&#20043;&#22806;&#30340;&#38382;&#39064;&#32780;&#38590;&#20197;&#27867;&#21270;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10685v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved impressive performance in numerous domains but often struggle to process lengthy inputs effectively and efficiently due to limited length generalization and attention's quadratic computational demands. Many sought to mitigate this by restricting the attention window within the pre-trained length. However, these methods introduce new issues such as ignoring the middle context and requiring additional training. To address these problems, we propose LongHeads, a training-free framework that enhances LLM's long context ability by unlocking multi-head attention's untapped potential. Instead of allowing each head to attend to the full sentence, which struggles with generalizing to longer sequences due to out-of-distribution (OOD) issues, we allow each head to process in-distribution length by selecting and attending to important context chunks. To this end, we propose a chunk selection strategy that
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OpenFMNav&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#20102;&#30446;&#26631;&#23548;&#33322;&#39046;&#22495;&#20013;&#20851;&#20110;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10670</link><description>&lt;p&gt;
OpenFMNav: &#36890;&#36807;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#23454;&#29616;&#24320;&#25918;&#24335;&#38646;&#26679;&#26412;&#30446;&#26631;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via Vision-Language Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OpenFMNav&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#20102;&#30446;&#26631;&#23548;&#33322;&#39046;&#22495;&#20013;&#20851;&#20110;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#23548;&#33322;(ObjectNav)&#38656;&#35201;&#19968;&#20010;&#20195;&#29702;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#23548;&#33322;&#20197;&#25214;&#21040;&#26597;&#35810;&#23545;&#35937;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#26041;&#27861;&#23581;&#35797;&#36890;&#36807;&#20381;&#36182;&#30417;&#30563;&#23398;&#20064;&#25110;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#65292;&#20854;&#20013;&#23427;&#20204;&#26159;&#22312;&#20855;&#26377;&#38381;&#38598;&#23545;&#35937;&#30340;&#26377;&#38480;&#23478;&#24237;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#28982;&#32780;&#65292;&#20173;&#26377;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#23578;&#26410;&#35299;&#20915;&#65306;&#29702;&#35299;&#35201;&#27714;&#24320;&#25918;&#38598;&#23545;&#35937;&#30340;&#33258;&#30001;&#24418;&#24335;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#24182;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#25512;&#24191;&#21040;&#26032;&#29615;&#22659;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OpenFMNav&#65292;&#19968;&#31181;&#22522;&#20110;&#24320;&#25918;&#38598;&#22522;&#30784;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#30446;&#26631;&#23548;&#33322;&#26694;&#26550;&#12290;&#25105;&#20204;&#39318;&#20808;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#31526;&#21512;&#29992;&#25143;&#38656;&#27714;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20013;&#25552;&#21462;&#25552;&#35758;&#30340;&#23545;&#35937;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLMs)&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#31215;&#26497;&#21457;&#29616;&#24182;&#26816;&#27979;&#22330;&#26223;&#20013;&#30340;&#20505;&#36873;&#23545;&#35937;&#65292;&#26500;&#24314;&#19968;&#20010;Ve
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10670v1 Announce Type: new  Abstract: Object navigation (ObjectNav) requires an agent to navigate through unseen environments to find queried objects. Many previous methods attempted to solve this task by relying on supervised or reinforcement learning, where they are trained on limited household datasets with close-set objects. However, two key challenges are unsolved: understanding free-form natural language instructions that demand open-set objects, and generalizing to new environments in a zero-shot manner. Aiming to solve the two challenges, in this paper, we propose OpenFMNav, an Open-set Foundation Model based framework for zero-shot object Navigation. We first unleash the reasoning abilities of large language models (LLMs) to extract proposed objects from natural language instructions that meet the user's demand. We then leverage the generalizability of large vision language models (VLMs) to actively discover and detect candidate objects from the scene, building a Ve
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20248;&#21270;&#35757;&#32451;&#31574;&#30053;&#25552;&#39640;NLP&#20219;&#21153;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#37319;&#29992;&#32454;&#24605;&#36830;&#24819;&#25552;&#31034;&#25216;&#26415;&#65292;&#23558;GPT-4&#20013;&#25552;&#28860;&#30340;&#30693;&#35782;&#24212;&#29992;&#20110;BERT&#27169;&#22411;&#65292;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#20026;&#36164;&#28304;&#26377;&#38480;&#25110;&#23553;&#38381;&#32593;&#32476;&#29615;&#22659;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.09282</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#20248;&#21270;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;NLP&#20219;&#21153;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Enhanced NLP Task Performance through Knowledge Distillation and Optimized Training Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09282
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20248;&#21270;&#35757;&#32451;&#31574;&#30053;&#25552;&#39640;NLP&#20219;&#21153;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#37319;&#29992;&#32454;&#24605;&#36830;&#24819;&#25552;&#31034;&#25216;&#26415;&#65292;&#23558;GPT-4&#20013;&#25552;&#28860;&#30340;&#30693;&#35782;&#24212;&#29992;&#20110;BERT&#27169;&#22411;&#65292;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#20026;&#36164;&#28304;&#26377;&#38480;&#25110;&#23553;&#38381;&#32593;&#32476;&#29615;&#22659;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#30340;&#25972;&#21512;&#21040;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#65292;&#20026;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#20943;&#23569;&#23545;&#22823;&#37327;&#20154;&#24037;&#27880;&#37322;&#30340;&#20381;&#36182;&#25171;&#24320;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32454;&#24605;&#36830;&#24819;&#65288;CoT&#65289;&#25552;&#31034;&#25216;&#26415;&#20174;GPT-4&#20013;&#25552;&#28860;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25913;&#36827;&#36739;&#23567;&#27169;&#22411;BERT&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20219;&#21153;&#19978;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#30340;&#35757;&#32451;&#36807;&#31243;&#65306;&#39318;&#20808;&#20351;&#29992;GPT-4&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#33976;&#39311;&#21644;&#21407;&#22987;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#30340;&#32452;&#21512;&#23545;&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#28151;&#21512;&#35757;&#32451;&#31574;&#30053;&#26126;&#26174;&#20248;&#20110;&#20165;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22312;F1&#20998;&#25968;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#20026;&#36164;&#28304;&#26377;&#38480;&#25110;&#23553;&#38381;&#32593;&#32476;&#29615;&#22659;&#25552;&#20379;&#20102;&#19968;&#31181;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09282v1 Announce Type: new Abstract: The integration of Large Language Models (LLMs) like GPT-4 into traditional Natural Language Processing (NLP) tasks has opened new avenues for enhancing model performance while reducing the reliance on extensive human annotations. This paper presents a novel approach that leverages the Chain of Thought (CoT) prompting technique to distill knowledge from GPT-4, subsequently applying it to improve the efficiency and effectiveness of a smaller model, BERT, on Named Entity Recognition (NER) tasks. Our method involves a two-phase training process: initially employing GPT-4 annotated data for pre-training and then refining the model with a combination of distilled and original human-annotated data. The results demonstrate that our mixed-training strategy significantly outperforms models trained solely on human annotations, achieving superior F1-scores and showcasing a cost-effective solution for resource-limited or closed-network settings. The 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#20018;&#32852;Transformer&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#36895;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#23558;&#23567;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#22823;&#27169;&#22411;&#20197;&#22359;&#27169;&#24335;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#35753;&#23567;&#27169;&#22411;&#20851;&#27880;&#22823;&#27169;&#22411;&#30340;&#20016;&#23500;&#34920;&#31034;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#23567;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#65292;&#20018;&#32852;&#30340;PaLM2-Bison&#21644;PaLM2-Gecko&#30456;&#27604;&#29420;&#31435;&#30340;PaLM2-Gecko&#65292;&#22312;&#19979;&#19968;&#20010;&#35789;&#20803;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#25552;&#39640;&#20102;3.3%&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20855;&#26377;&#30456;&#20284;&#19979;&#28216;&#20219;&#21153;&#30340;PaLM2-Otter&#27169;&#22411;&#65292;&#21152;&#36895;&#27604;&#36798;&#21040;1.16&#20493;&#12290;</title><link>https://arxiv.org/abs/2402.08644</link><description>&lt;p&gt;
&#29992;&#20110;&#25512;&#26029;&#39640;&#25928;LLMs&#30340;&#20018;&#32852;Transformer
&lt;/p&gt;
&lt;p&gt;
Tandem Transformers for Inference Efficient LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08644
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#20018;&#32852;Transformer&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#36895;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#23558;&#23567;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#22823;&#27169;&#22411;&#20197;&#22359;&#27169;&#24335;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#35753;&#23567;&#27169;&#22411;&#20851;&#27880;&#22823;&#27169;&#22411;&#30340;&#20016;&#23500;&#34920;&#31034;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#23567;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#65292;&#20018;&#32852;&#30340;PaLM2-Bison&#21644;PaLM2-Gecko&#30456;&#27604;&#29420;&#31435;&#30340;PaLM2-Gecko&#65292;&#22312;&#19979;&#19968;&#20010;&#35789;&#20803;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#25552;&#39640;&#20102;3.3%&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20855;&#26377;&#30456;&#20284;&#19979;&#28216;&#20219;&#21153;&#30340;PaLM2-Otter&#27169;&#22411;&#65292;&#21152;&#36895;&#27604;&#36798;&#21040;1.16&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;( LLMs )&#20855;&#26377;&#33258;&#22238;&#24402;&#30340;&#29305;&#24615;&#65292;&#36825;&#20351;&#24471;&#25512;&#26029;&#36895;&#24230;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#20026;&#35789;&#20803;&#26159;&#25353;&#39034;&#24207;&#29983;&#25104;&#30340;&#12290;&#23613;&#31649;&#26377;&#20123;&#39044;&#27979;&#21644;&#24182;&#34892;&#35299;&#30721;&#25216;&#26415;&#35797;&#22270;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#37117;&#26377;&#38480;&#21046;&#65306;&#35201;&#20040;&#20381;&#36182;&#26356;&#31934;&#31616;&#20294;&#20934;&#30830;&#24230;&#36739;&#20302;&#30340;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#65292;&#35201;&#20040;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#22522;&#30784;LLM&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#21363;&#20018;&#32852;Transformer&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36825;&#31181;&#26550;&#26500;&#29420;&#29305;&#22320;&#32467;&#21512;&#20102;(1)&#19968;&#20010;&#23567;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;(2)&#19968;&#20010;&#20197;&#22359;&#27169;&#24335;&#36816;&#34892;&#30340;&#22823;&#27169;&#22411;(&#21516;&#26102;&#22788;&#29702;&#22810;&#20010;&#35789;&#20803;)&#12290;&#36890;&#36807;&#35753;&#23567;&#27169;&#22411;&#20851;&#27880;&#22823;&#27169;&#22411;&#26356;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#22823;&#24133;&#25552;&#21319;&#23567;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#22312;PaLM2&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#65292;PaLM2-Bison&#21644;PaLM2-Gecko&#30340;&#20018;&#32852;&#30456;&#36739;&#29420;&#31435;&#30340;PaLM2-Gecko&#65292;&#22312;&#19979;&#19968;&#20010;&#35789;&#20803;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#25552;&#21319;&#20102;3.3%&#65292;&#19982;&#20855;&#26377;&#30456;&#20284;&#19979;&#28216;&#20219;&#21153;&#30340;PaLM2-Otter&#27169;&#22411;&#30456;&#27604;&#65292;&#25552;&#20379;&#20102;1.16&#20493;&#30340;&#21152;&#36895;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The autoregressive nature of conventional large language models (LLMs) inherently limits inference speed, as tokens are generated sequentially. While speculative and parallel decoding techniques attempt to mitigate this, they face limitations: either relying on less accurate smaller models for generation or failing to fully leverage the base LLM's representations.   We introduce a novel architecture, Tandem transformers, to address these issues. This architecture uniquely combines (1) a small autoregressive model and (2) a large model operating in block mode (processing multiple tokens simultaneously). The small model's predictive accuracy is substantially enhanced by granting it attention to the large model's richer representations. On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko demonstrates a 3.3% improvement in next-token prediction accuracy over a standalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter model with comparable downstream p
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#28041;&#21450;&#20102;&#22823;&#37327;&#30340;&#25968;&#23398;&#38382;&#39064;&#31867;&#22411;&#21644;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#35774;&#32622;&#12290;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2402.00157</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;&#65306;&#36827;&#23637;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Mathematical Reasoning: Progresses and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00157
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#28041;&#21450;&#20102;&#22823;&#37327;&#30340;&#25968;&#23398;&#38382;&#39064;&#31867;&#22411;&#21644;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#35774;&#32622;&#12290;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#25512;&#29702;&#26159;&#35780;&#20272;&#20154;&#31867;&#26234;&#33021;&#22522;&#26412;&#35748;&#30693;&#33021;&#21147;&#30340;&#22522;&#30707;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#33258;&#21160;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#30340;&#37325;&#35270;&#12290;&#28982;&#32780;&#65292;&#25968;&#23398;&#38382;&#39064;&#30340;&#31867;&#22411;&#38750;&#24120;&#24191;&#27867;&#65292;LLM&#30456;&#20851;&#25216;&#26415;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#35774;&#32622;&#19979;&#36827;&#34892;&#35780;&#20272;&#65292;&#20351;&#24471;&#22914;&#20309;&#21028;&#26029;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#20013;&#30340;&#30495;&#27491;&#36827;&#23637;&#21644;&#38556;&#30861;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#35843;&#26597;&#30740;&#31350;&#21253;&#25324;&#20102;&#20197;&#19979;&#22235;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;i&#65289;&#20840;&#38754;&#25506;&#32034;&#21508;&#31181;&#24050;&#32463;&#30740;&#31350;&#30340;&#25968;&#23398;&#38382;&#39064;&#21450;&#20854;&#30456;&#24212;&#25968;&#25454;&#38598;&#65307;ii&#65289;&#30740;&#31350;&#25552;&#20986;&#30340;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#30340;LLM&#25216;&#26415;&#30340;&#33539;&#22260;&#65307;iii&#65289;&#27010;&#36848;&#24433;&#21709;LLM&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#22240;&#32032;&#21644;&#20851;&#27880;&#28857;&#65307;iv&#65289;&#38416;&#26126;&#20173;&#28982;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematical reasoning serves as a cornerstone for assessing the fundamental cognitive capabilities of human intelligence. In recent times, there has been a notable surge in the development of Large Language Models (LLMs) geared towards the automated resolution of mathematical problems. However, the landscape of mathematical problem types is vast and varied, with LLM-oriented techniques undergoing evaluation across diverse datasets and settings. This diversity makes it challenging to discern the true advancements and obstacles within this burgeoning field. This survey endeavors to address four pivotal dimensions: i) a comprehensive exploration of the various mathematical problems and their corresponding datasets that have been investigated; ii) an examination of the spectrum of LLM-oriented techniques that have been proposed for mathematical problem-solving; iii) an overview of factors and concerns affecting LLMs in solving math; and iv) an elucidation of the persisting challenges with
&lt;/p&gt;</description></item><item><title>LOCOST&#26159;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#25991;&#26723;&#30340;&#25277;&#35937;&#25688;&#35201;&#29983;&#25104;&#12290;&#19982;&#22522;&#20110;&#31232;&#30095;&#27880;&#24847;&#27169;&#24335;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#27604;&#65292;LOCOST&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#26399;&#38388;&#33410;&#30465;&#22823;&#37327;&#20869;&#23384;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;LOCOST&#22312;&#38271;&#25991;&#26723;&#25688;&#35201;&#21270;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;93-96%&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#36229;&#36807;600K&#20010;&#26631;&#35760;&#30340;&#36755;&#20837;&#25991;&#26412;&#12290;</title><link>https://arxiv.org/abs/2401.17919</link><description>&lt;p&gt;
LOCOST: &#38271;&#25991;&#26723;&#25277;&#35937;&#25688;&#35201;&#21270;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LOCOST: State-Space Models for Long Document Abstractive Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17919
&lt;/p&gt;
&lt;p&gt;
LOCOST&#26159;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#25991;&#26723;&#30340;&#25277;&#35937;&#25688;&#35201;&#29983;&#25104;&#12290;&#19982;&#22522;&#20110;&#31232;&#30095;&#27880;&#24847;&#27169;&#24335;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#27604;&#65292;LOCOST&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#26399;&#38388;&#33410;&#30465;&#22823;&#37327;&#20869;&#23384;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;LOCOST&#22312;&#38271;&#25991;&#26723;&#25688;&#35201;&#21270;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;93-96%&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#36229;&#36807;600K&#20010;&#26631;&#35760;&#30340;&#36755;&#20837;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26159;&#32534;&#30721;&#38271;&#24207;&#21015;&#21644;&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#30340;&#20302;&#22797;&#26434;&#24230;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LOCOST&#65306;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#20855;&#26377;&#38271;&#19978;&#19979;&#25991;&#36755;&#20837;&#30340;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#12290;&#36825;&#31181;&#26550;&#26500;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20026;O&#65288;L log L&#65289;&#65292;&#21487;&#20197;&#22788;&#29702;&#27604;&#22522;&#20110;&#31232;&#30095;&#27880;&#24847;&#27169;&#24335;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#38271;&#25991;&#26723;&#25277;&#35937;&#25688;&#35201;&#21270;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#24615;&#33021;&#27700;&#24179;&#19978;&#36798;&#21040;&#20102;&#19982;&#30456;&#21516;&#22823;&#23567;&#30340;&#26368;&#20248;&#31232;&#30095;&#21464;&#21387;&#22120;&#30456;&#24403;&#30340;93-96%&#65292;&#21516;&#26102;&#22312;&#35757;&#32451;&#26399;&#38388;&#33410;&#30465;&#20102;&#39640;&#36798;50%&#30340;&#20869;&#23384;&#65292;&#22312;&#25512;&#26029;&#26399;&#38388;&#33410;&#30465;&#20102;&#39640;&#36798;87%&#30340;&#20869;&#23384;&#12290;&#27492;&#22806;&#65292;LOCOST&#26377;&#25928;&#22320;&#22788;&#29702;&#36229;&#36807;600K&#20010;&#26631;&#35760;&#30340;&#36755;&#20837;&#25991;&#26412;&#65292;&#20026;&#23436;&#25972;&#20070;&#25688;&#35201;&#21270;&#35774;&#23450;&#20102;&#26032;&#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#24182;&#20026;&#38271;&#36755;&#20837;&#22788;&#29702;&#24320;&#36767;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-space models are a low-complexity alternative to transformers for encoding long sequences and capturing long-term dependencies. We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs. With a computational complexity of $O(L \log L)$, this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns. We evaluate our model on a series of long document abstractive summarization tasks. The model reaches a performance level that is 93-96% comparable to the top-performing sparse transformers of the same size while saving up to 50% memory during training and up to 87% during inference. Additionally, LOCOST effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26032;&#30340;PILOT&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20351;&#29992;&#26696;&#20363;&#27861;&#39044;&#27979;&#27861;&#24459;&#26696;&#20363;&#32467;&#26524;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#30456;&#20851;&#26696;&#20363;&#26816;&#32034;&#21644;&#26102;&#38388;&#27169;&#24335;&#22788;&#29702;&#20004;&#20010;&#27169;&#22359;&#12290;</title><link>https://arxiv.org/abs/2401.15770</link><description>&lt;p&gt;
PILOT: &#20351;&#29992;&#27861;&#24459;&#26696;&#20363;&#39044;&#27979;&#26696;&#20363;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
PILOT: Legal Case Outcome Prediction with Case Law
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15770
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26032;&#30340;PILOT&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20351;&#29992;&#26696;&#20363;&#27861;&#39044;&#27979;&#27861;&#24459;&#26696;&#20363;&#32467;&#26524;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#30456;&#20851;&#26696;&#20363;&#26816;&#32034;&#21644;&#26102;&#38388;&#27169;&#24335;&#22788;&#29702;&#20004;&#20010;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#39044;&#27979;&#27861;&#24459;&#26696;&#20363;&#32467;&#26524;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#21069;&#26223;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#27665;&#20107;&#27861;&#24459;&#26696;&#20363;&#32780;&#38750;&#26696;&#20363;&#27861;&#31995;&#32479;&#19978;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#20351;&#29992;&#26696;&#20363;&#27861;&#36827;&#34892;&#27861;&#24459;&#26696;&#20363;&#32467;&#26524;&#39044;&#27979;&#26102;&#30340;&#20004;&#20010;&#29420;&#29305;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#35782;&#21035;&#20316;&#20026;&#27861;&#23448;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#22522;&#26412;&#35777;&#25454;&#30340;&#30456;&#20851;&#20808;&#20363;&#26696;&#20363;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#27425;&#65292;&#26377;&#24517;&#35201;&#32771;&#34385;&#27861;&#24459;&#21407;&#21017;&#38543;&#26102;&#38388;&#28436;&#21464;&#30340;&#24773;&#20917;&#65292;&#22240;&#20026;&#26089;&#26399;&#26696;&#20363;&#21487;&#33021;&#36981;&#24490;&#19981;&#21516;&#30340;&#27861;&#24459;&#32972;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PILOT&#65288;&#27861;&#24459;&#26696;&#20363;&#32467;&#26524;&#39044;&#27979;&#65289;&#30340;&#26032;&#26694;&#26550;&#12290;&#23427;&#21253;&#25324;&#29992;&#20110;&#30456;&#20851;&#26696;&#20363;&#26816;&#32034;&#21644;&#26102;&#38388;&#27169;&#24335;&#22788;&#29702;&#30340;&#20004;&#20010;&#27169;&#22359;&#12290;&#20026;&#20102;&#34913;&#37327;&#29616;&#26377;&#27861;&#24459;&#26696;&#20363;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#20174;&#22823;&#35268;&#27169;&#26696;&#20363;&#27861;&#25968;&#25454;&#24211;&#20013;&#31574;&#21010;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20934;&#30830;&#35782;&#21035;&#20808;&#20363;&#26696;&#20363;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#32531;&#35299;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.15770v2 Announce Type: replace  Abstract: Machine learning shows promise in predicting the outcome of legal cases, but most research has concentrated on civil law cases rather than case law systems. We identified two unique challenges in making legal case outcome predictions with case law. First, it is crucial to identify relevant precedent cases that serve as fundamental evidence for judges during decision-making. Second, it is necessary to consider the evolution of legal principles over time, as early cases may adhere to different legal contexts. In this paper, we proposed a new framework named PILOT (PredictIng Legal case OuTcome) for case outcome prediction. It comprises two modules for relevant case retrieval and temporal pattern handling, respectively. To benchmark the performance of existing legal case outcome prediction models, we curated a dataset from a large-scale case law database. We demonstrate the importance of accurately identifying precedent cases and mitiga
&lt;/p&gt;</description></item><item><title>Temp-Lora&#26041;&#27861;&#36890;&#36807;&#22312;&#38271;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#36880;&#27493;&#35757;&#32451;&#20020;&#26102;Lora&#27169;&#22359;&#65292;&#26377;&#25928;&#20445;&#30041;&#19978;&#19979;&#25991;&#30693;&#35782;&#24182;&#36991;&#20813;&#23545;&#27169;&#22411;&#21442;&#25968;&#30340;&#27704;&#20037;&#24615;&#25913;&#21464;&#12290;</title><link>https://arxiv.org/abs/2401.11504</link><description>&lt;p&gt;
&#38543;&#30528;&#25991;&#26412;&#37327;&#22686;&#21152;&#65292;&#25512;&#26029;&#35757;&#32451;&#26377;&#21161;&#20110;&#38271;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
With Greater Text Comes Greater Necessity: Inference-Time Training Helps Long Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11504
&lt;/p&gt;
&lt;p&gt;
Temp-Lora&#26041;&#27861;&#36890;&#36807;&#22312;&#38271;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#36880;&#27493;&#35757;&#32451;&#20020;&#26102;Lora&#27169;&#22359;&#65292;&#26377;&#25928;&#20445;&#30041;&#19978;&#19979;&#25991;&#30693;&#35782;&#24182;&#36991;&#20813;&#23545;&#27169;&#22411;&#21442;&#25968;&#30340;&#27704;&#20037;&#24615;&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#25991;&#26412;&#29983;&#25104;&#65292;&#22914;&#23567;&#35828;&#21019;&#20316;&#21644;&#20855;&#26377;&#26497;&#38271;&#19978;&#19979;&#25991;&#30340;&#31687;&#31456;&#32423;&#32763;&#35793;&#65292;&#23545;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#38271;&#24230;&#22806;&#25512;&#31561;&#31574;&#30053;&#25193;&#23637;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#35757;&#32451;&#21644;/&#25110;&#25512;&#26029;&#38454;&#27573;&#35201;&#27714;&#22823;&#37327;&#30828;&#20214;&#36164;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;Temp-Lora&#24341;&#20837;&#20102;&#19968;&#20010;&#26367;&#20195;&#27010;&#24565;&#12290;&#25105;&#20204;&#19981;&#20381;&#36182;&#20110;KV&#32531;&#23384;&#23384;&#20648;&#25152;&#26377;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#32780;&#26159;&#23558;&#36825;&#20123;&#20449;&#24687;&#30452;&#25509;&#23884;&#20837;&#20020;&#26102;Lora&#27169;&#22359;&#20013;&#12290;&#22312;&#38271;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#36825;&#20010;&#27169;&#22359;&#20250;&#38543;&#30528;&#20808;&#21069;&#29983;&#25104;&#30340;&#25991;&#26412;&#36880;&#28176;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#26377;&#25928;&#22320;&#20445;&#30041;&#19978;&#19979;&#25991;&#30693;&#35782;&#65292;&#36824;&#38450;&#27490;&#20102;&#23545;&#27169;&#22411;&#21442;&#25968;&#30340;&#20219;&#20309;&#27704;&#20037;&#24615;&#25913;&#21464;&#65292;&#22240;&#20026;&#27169;&#22359;&#22312;&#29983;&#25104;&#21518;&#34987;&#20002;&#24323;&#12290;&#22312;PG19&#35821;&#35328;&#24314;&#27169;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11504v2 Announce Type: replace-cross  Abstract: Long text generation, such as novel writing and discourse-level translation with extremely long contexts, presents significant challenges to current language models. Existing methods mainly focus on extending the model's context window through strategies like length extrapolation. However, these approaches demand substantial hardware resources during the training and/or inference phases. Our proposed method, Temp-Lora, introduces an alternative concept. Instead of relying on the KV cache to store all context information, we embeds this information directly into a temporary Lora module. In the process of long text generation, this module is progressively trained with text generated previously. This approach not only efficiently preserves contextual knowledge but also prevents any permanent alteration to the model's parameters given that the module is discarded post-generation. Extensive experiments on the PG19 language modeling 
&lt;/p&gt;</description></item><item><title>BaRDa&#25968;&#25454;&#38598;&#36890;&#36807;&#20351;&#29992;&#20154;&#31867;&#27880;&#37322;&#30340;&#34164;&#28085;&#26641;&#65292;&#28151;&#21512;&#30495;&#23454;&#21644;&#34394;&#20551;&#20107;&#23454;&#65292;&#24182;&#21253;&#25324;&#21453;&#20107;&#23454;&#20363;&#23376;&#65292;&#25104;&#21151;&#21306;&#20998;&#20102;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2312.07527</link><description>&lt;p&gt;
BaRDa: &#19968;&#20010;&#23558;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#20998;&#24320;&#30340;&#20449;&#24565;&#21644;&#25512;&#29702;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BaRDa: A Belief and Reasoning Dataset that Separates Factual Accuracy and Reasoning Ability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07527
&lt;/p&gt;
&lt;p&gt;
BaRDa&#25968;&#25454;&#38598;&#36890;&#36807;&#20351;&#29992;&#20154;&#31867;&#27880;&#37322;&#30340;&#34164;&#28085;&#26641;&#65292;&#28151;&#21512;&#30495;&#23454;&#21644;&#34394;&#20551;&#20107;&#23454;&#65292;&#24182;&#21253;&#25324;&#21453;&#20107;&#23454;&#20363;&#23376;&#65292;&#25104;&#21151;&#21306;&#20998;&#20102;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#22522;&#20934;&#26469;&#27604;&#36739;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#24615;&#33021;&#65292;&#20294;&#26368;&#32456;&#20219;&#21153;&#35780;&#20272;&#24448;&#24448;&#28151;&#28102;&#20102;*&#20107;&#23454;&#20934;&#30830;&#24615;*&#65288;"&#30495;&#30456;"&#65289;&#21644;*&#25512;&#29702;&#33021;&#21147;*&#65288;"&#21512;&#29702;&#24615;"&#65292;&#25110;&#32773;&#26681;&#25454;&#27491;&#30830;&#25253;&#21578;&#20449;&#24565;&#21547;&#20041;&#26469;&#23450;&#20041;&#30340;"&#35802;&#23454;"&#65289;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#20010;&#33021;&#22815;&#28165;&#26224;&#21306;&#20998;&#36825;&#20004;&#20010;&#27010;&#24565;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#21644;&#25193;&#23637;&#19968;&#32452;&#20154;&#31867;&#27880;&#37322;&#30340;*&#34164;&#28085;&#26641;*&#65292;&#29992;&#20110;&#34920;&#36798;&#33391;&#22909;&#21644;&#24694;&#21155;&#30340;&#25512;&#29702;&#38142;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#21644;&#34394;&#20551;&#20107;&#23454;&#30340;&#28151;&#21512;&#65292;&#29305;&#21035;&#26159;&#21253;&#25324;&#21453;&#20107;&#23454;&#30340;&#20363;&#23376;&#65292;&#20197;&#36991;&#20813;&#20449;&#24565;&#20559;&#35265;&#65288;&#20063;&#31216;&#20026;"&#20869;&#23481;&#25928;&#24212;"&#65289;&#12290;&#32467;&#26524;&#25968;&#25454;&#38598;&#21517;&#20026;BaRDa&#65292;&#21253;&#21547;3000&#20010;&#34164;&#28085;&#65288;1787&#20010;&#26377;&#25928;&#65292;1213&#20010;&#26080;&#25928;&#65289;&#65292;&#20351;&#29992;6681&#20010;&#30495;&#23454;&#21644;2319&#20010;&#34394;&#20551;&#38472;&#36848;&#12290;&#22312;&#22235;&#20010;GPT&#31995;&#21015;&#27169;&#22411; GPT3(curie)/GPT3(davinici)/3.5/4 &#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#21457;&#29616;&#20107;&#23454;&#20934;&#30830;&#24615;&#65288;&#30495;&#30456;&#65289;&#24471;&#20998;&#20026;74.1/80.6/82.6/87.1&#20197;&#21450;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07527v2 Announce Type: replace-cross  Abstract: While there are numerous benchmarks comparing the performance of modern language models (LMs), end-task evaluations often conflate notions of *factual accuracy* ("truth") and *reasoning ability* ("rationality", or "honesty" in the sense of correctly reporting implications of beliefs). Our goal is a dataset that clearly distinguishes these two notions. Our approach is to leverage and extend a collection of human-annotated *entailment trees*, engineered to express both good and bad chains of reasoning, and using a mixture of true and false facts, in particular including counterfactual examples, to avoid belief bias (also known as the "content effect"). The resulting dataset, called BaRDa, contains 3000 entailments (1787 valid, 1213 invalid), using 6681 true and 2319 false statements. Testing on four GPT-series models, GPT3(curie)/GPT3(davinici)/3.5/4, we find factual accuracy (truth) scores of 74.1/80.6/82.6/87.1 and reasoning ac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#21487;&#20449;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#21487;&#20449;&#24230;&#65292;&#35782;&#21035;&#26631;&#31614;&#38169;&#35823;&#65292;&#24182;&#35780;&#20272;&#22024;&#26434;&#26631;&#31614;&#23545;&#19981;&#23433;&#20840;&#35780;&#35770;&#21644;&#23545;&#35805;&#20998;&#31867;&#30340;&#24433;&#21709;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#26080;&#23475;&#35821;&#35328;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2311.11202</link><description>&lt;p&gt;
&#25581;&#31034;&#21644;&#25552;&#39640;&#25968;&#25454;&#21487;&#20449;&#24230;&#65306;&#35757;&#32451;&#26080;&#23475;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#21487;&#20449;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#21487;&#20449;&#24230;&#65292;&#35782;&#21035;&#26631;&#31614;&#38169;&#35823;&#65292;&#24182;&#35780;&#20272;&#22024;&#26434;&#26631;&#31614;&#23545;&#19981;&#23433;&#20840;&#35780;&#35770;&#21644;&#23545;&#35805;&#20998;&#31867;&#30340;&#24433;&#21709;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#26080;&#23475;&#35821;&#35328;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11202v2&#23459;&#24067;&#31867;&#22411;&#65306;&#26367;&#25442;-&#36328;&#25991;&#26723;&#25688;&#35201;&#65306;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#22312;&#35757;&#32451;&#12289;&#24494;&#35843;&#25110;&#23545;&#40784;&#36807;&#31243;&#20013;&#21487;&#33021;&#21463;&#21040;&#19981;&#24076;&#26395;&#30340;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#27880;&#35299;&#30340;&#27491;&#30830;&#24615;&#65292;&#21363;&#25968;&#25454;&#38598;&#30340;&#21487;&#20449;&#24230;&#65292;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#21487;&#20449;&#24230;&#65292;&#20854;&#20013;&#21253;&#25324;&#21487;&#29992;&#20110;&#35757;&#32451;&#26080;&#23475;&#35821;&#35328;&#27169;&#22411;&#30340;&#27969;&#34892;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#22914;Jigsaw Civil Comments&#12289;Anthropic Harmless&#21644;Red Team&#12289;PKU BeaverTails&#21644;SafeRLHF&#12290;&#32771;&#34385;&#21040;&#20154;&#20204;&#28165;&#27927;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#25104;&#26412;&#21644;&#38590;&#24230;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#21487;&#20449;&#24230;&#65292;&#35782;&#21035;&#26631;&#31614;&#38169;&#35823;&#65292;&#24182;&#35780;&#20272;&#31574;&#21010;&#35821;&#35328;&#25968;&#25454;&#20013;&#22024;&#26434;&#26631;&#31614;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#20851;&#27880;&#19981;&#23433;&#20840;&#35780;&#35770;&#21644;&#23545;&#35805;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11202v2 Announce Type: replace-cross  Abstract: Language models have shown promise in various tasks but can be affected by undesired data during training, fine-tuning, or alignment. For example, if some unsafe conversations are wrongly annotated as safe ones, the model fine-tuned on these samples may be harmful. Therefore, the correctness of annotations, i.e., the credibility of the dataset, is important. This study focuses on the credibility of real-world datasets, including the popular benchmarks Jigsaw Civil Comments, Anthropic Harmless &amp; Red Team, PKU BeaverTails &amp; SafeRLHF, that can be used for training a harmless language model. Given the cost and difficulty of cleaning these datasets by humans, we introduce a systematic framework for evaluating the credibility of datasets, identifying label errors, and evaluating the influence of noisy labels in the curated language data, specifically focusing on unsafe comments and conversation classification. With the framework, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#21644;&#32508;&#21512;&#24605;&#32500;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#23567;&#35821;&#35328;&#27169;&#22411;&#32487;&#25215;&#19981;&#23436;&#21892;&#25512;&#29702;&#21644;&#24187;&#35273;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.09214</link><description>&lt;p&gt;
Mind's Mirror: &#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#21644;&#32508;&#21512;&#24605;&#32500;
&lt;/p&gt;
&lt;p&gt;
Mind's Mirror: Distilling Self-Evaluation Capability and Comprehensive Thinking from Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#21644;&#32508;&#21512;&#24605;&#32500;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#23567;&#35821;&#35328;&#27169;&#22411;&#32487;&#25215;&#19981;&#23436;&#21892;&#25512;&#29702;&#21644;&#24187;&#35273;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#21644;&#35745;&#31639;&#38656;&#27714;&#22312;&#32771;&#34385;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#30340;&#23454;&#38469;&#37096;&#32626;&#26102;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#26041;&#27861;&#35770;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;LLMs&#20013;&#30340;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#25552;&#28860;&#21040;SLMs&#20013;&#65292;&#26088;&#22312;&#20943;&#36731;&#20174;LLMs&#32487;&#25215;&#30340;&#38169;&#35823;&#25512;&#29702;&#21644;&#24187;&#35273;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20513;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#19981;&#21516;&#30340;CoTs&#21644;&#33258;&#25105;&#35780;&#20272;&#36755;&#20986;&#26469;&#25552;&#28860;&#26356;&#20840;&#38754;&#30340;&#24605;&#32500;&#65292;&#20197;&#30830;&#20445;&#26356;&#20026;&#24443;&#24213;&#21644;&#20581;&#22766;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09214v2 Announce Type: replace  Abstract: Large language models (LLMs) have achieved remarkable advancements in natural language processing. However, the massive scale and computational demands of these models present formidable challenges when considering their practical deployment in resource-constrained environments. While techniques such as chain-of-thought (CoT) distillation have displayed promise in distilling LLMs into small language models (SLMs), there is a risk that distilled SLMs may still inherit flawed reasoning and hallucinations from LLMs. To address these issues, we propose a twofold methodology: First, we introduce a novel method for distilling the self-evaluation capability from LLMs into SLMs, aiming to mitigate the adverse effects of flawed reasoning and hallucinations inherited from LLMs. Second, we advocate for distilling more comprehensive thinking by incorporating multiple distinct CoTs and self-evaluation outputs, to ensure a more thorough and robust
&lt;/p&gt;</description></item><item><title>UNER&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#12289;&#31038;&#21306;&#39537;&#21160;&#30340;&#39033;&#30446;&#65292;&#26088;&#22312;&#25552;&#20379;&#39640;&#36136;&#37327;&#12289;&#36328;&#35821;&#35328;&#19968;&#33268;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#22522;&#20934;&#65292;&#20197;&#20419;&#36827;&#21644;&#26631;&#20934;&#21270;&#22810;&#35821;&#35328;NER&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2311.09122</link><description>&lt;p&gt;
&#36890;&#29992;NER:&#19968;&#20010;&#37329;&#26631;&#20934;&#30340;&#22810;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Universal NER: A Gold-Standard Multilingual Named Entity Recognition Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09122
&lt;/p&gt;
&lt;p&gt;
UNER&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#12289;&#31038;&#21306;&#39537;&#21160;&#30340;&#39033;&#30446;&#65292;&#26088;&#22312;&#25552;&#20379;&#39640;&#36136;&#37327;&#12289;&#36328;&#35821;&#35328;&#19968;&#33268;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#22522;&#20934;&#65292;&#20197;&#20419;&#36827;&#21644;&#26631;&#20934;&#21270;&#22810;&#35821;&#35328;NER&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#36890;&#29992;NER&#65288;UNER&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#65292;&#31038;&#21306;&#39537;&#21160;&#30340;&#39033;&#30446;&#65292;&#26088;&#22312;&#24320;&#21457;&#22810;&#31181;&#35821;&#35328;&#30340;&#37329;&#26631;&#20934;NER&#22522;&#20934;&#12290;UNER&#30340;&#24635;&#20307;&#30446;&#26631;&#26159;&#25552;&#20379;&#39640;&#36136;&#37327;&#12289;&#36328;&#35821;&#35328;&#19968;&#33268;&#30340;&#26631;&#27880;&#65292;&#20197;&#20419;&#36827;&#21644;&#26631;&#20934;&#21270;&#22810;&#35821;&#35328;NER&#30740;&#31350;&#12290;UNER v1&#21253;&#21547;&#20102;&#22312;12&#31181;&#19981;&#21516;&#35821;&#35328;&#20013;&#20351;&#29992;&#36328;&#35821;&#35328;&#19968;&#33268;&#27169;&#24335;&#26631;&#27880;&#30340;18&#20010;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;UNER&#30340;&#25968;&#25454;&#38598;&#21019;&#24314;&#21644;&#32452;&#25104;&#65307;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#38024;&#23545;&#19981;&#21516;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#23398;&#20064;&#35774;&#32622;&#30340;&#21021;&#22987;&#24314;&#27169;&#22522;&#32447;&#12290;&#25105;&#20204;&#21521;&#20844;&#20247;&#21457;&#24067;&#20102;&#25968;&#25454;&#12289;&#20195;&#30721;&#21644;&#25311;&#21512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09122v2 Announce Type: replace  Abstract: We introduce Universal NER (UNER), an open, community-driven project to develop gold-standard NER benchmarks in many languages. The overarching goal of UNER is to provide high-quality, cross-lingually consistent annotations to facilitate and standardize multilingual NER research. UNER v1 contains 18 datasets annotated with named entities in a cross-lingual consistent schema across 12 diverse languages. In this paper, we detail the dataset creation and composition of UNER; we also provide initial modeling baselines on both in-language and cross-lingual learning settings. We release the data, code, and fitted models to the public.
&lt;/p&gt;</description></item><item><title>OFA&#26694;&#26550;&#36890;&#36807;&#26234;&#33021;&#21021;&#22987;&#21270;&#26410;&#35265;&#23376;&#35789;&#30340;&#23884;&#20837;&#65292;&#32467;&#21512;&#22806;&#37096;&#22810;&#35821;&#35328;&#38745;&#24577;&#35789;&#21521;&#37327;&#21644;&#30697;&#38453;&#20998;&#35299;&#65292;&#26377;&#25928;&#23454;&#29616;&#22810;&#35821;&#35328;&#36866;&#24212;&#24182;&#22823;&#24133;&#20943;&#23569;&#27169;&#22411;&#23884;&#20837;&#21442;&#25968;&#25968;&#37327;</title><link>https://arxiv.org/abs/2311.08849</link><description>&lt;p&gt;
OFA&#65306;&#19968;&#31181;&#29992;&#20110;&#21021;&#22987;&#21270;&#26410;&#35265;&#23376;&#35789;&#23884;&#20837;&#30340;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25345;&#32493;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
OFA: A Framework of Initializing Unseen Subword Embeddings for Efficient Large-scale Multilingual Continued Pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08849
&lt;/p&gt;
&lt;p&gt;
OFA&#26694;&#26550;&#36890;&#36807;&#26234;&#33021;&#21021;&#22987;&#21270;&#26410;&#35265;&#23376;&#35789;&#30340;&#23884;&#20837;&#65292;&#32467;&#21512;&#22806;&#37096;&#22810;&#35821;&#35328;&#38745;&#24577;&#35789;&#21521;&#37327;&#21644;&#30697;&#38453;&#20998;&#35299;&#65292;&#26377;&#25928;&#23454;&#29616;&#22810;&#35821;&#35328;&#36866;&#24212;&#24182;&#22823;&#24133;&#20943;&#23569;&#27169;&#22411;&#23884;&#20837;&#21442;&#25968;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20174;&#22836;&#24320;&#22987;&#39044;&#35757;&#32451;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#19981;&#21516;&#65292;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#35789;&#27719;&#25193;&#23637;&#21644;&#25345;&#32493;&#39044;&#35757;&#32451;&#26469;&#36866;&#24212;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21040;&#26032;&#30340;&#35821;&#35328;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65306;OFA&#65288;One For All&#65289;&#65292;&#23427;&#32874;&#26126;&#22320;&#21021;&#22987;&#21270;&#20102;&#26410;&#35265;&#23376;&#35789;&#30340;&#23884;&#20837;&#65292;&#20174;&#32780;&#21487;&#20197;&#39640;&#25928;&#26377;&#25928;&#22320;&#23558;PLM&#36866;&#24212;&#21040;&#22810;&#31181;&#35821;&#35328;&#12290;OFA&#21033;&#29992;&#22806;&#37096;&#23545;&#40784;&#33391;&#22909;&#30340;&#22810;&#35821;&#35328;&#38745;&#24577;&#35789;&#21521;&#37327;&#65292;&#24182;&#23558;&#23545;&#40784;&#30693;&#35782;&#27880;&#20837;&#21040;&#23376;&#35789;&#23884;&#20837;&#20013;&#12290;&#27492;&#22806;&#65292;OFA&#24212;&#29992;&#30697;&#38453;&#20998;&#35299;&#65292;&#24182;&#29992;&#20004;&#20010;&#20302;&#32500;&#30697;&#38453;&#26367;&#25442;&#32321;&#29712;&#30340;&#23884;&#20837;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#27169;&#22411;&#30340;&#23884;&#20837;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08849v2 Announce Type: replace  Abstract: Instead of pretraining multilingual language models from scratch, a more efficient method is to adapt existing pretrained language models (PLMs) to new languages via vocabulary extension and continued pretraining. However, this method usually randomly initializes the embeddings of new subwords and introduces substantially more embedding parameters to the model, thus weakening the efficiency. To address these issues, we propose a novel framework: $\textbf{O}$ne $\textbf{F}$or $\textbf{A}$ll ($\textbf{OFA}$), which wisely initializes the embeddings of unseen subwords and thus can adapt a PLM to multiple languages efficiently and effectively. OFA takes advantage of external well-aligned multilingual static word vectors and injects the alignment knowledge into the subword embeddings. In addition, OFA applies matrix factorization and replaces the cumbersome embeddings with two lower-dimensional matrices, which largely reduces the number o
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#19982;&#26657;&#20934;&#30340;&#35843;&#30740;&#24635;&#32467;&#20102;&#25361;&#25112;&#12289;&#25216;&#26415;&#36827;&#23637;&#12289;&#24212;&#29992;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2311.08298</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#19982;&#26657;&#20934;&#30340;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A Survey of Confidence Estimation and Calibration in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08298
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#19982;&#26657;&#20934;&#30340;&#35843;&#30740;&#24635;&#32467;&#20102;&#25361;&#25112;&#12289;&#25216;&#26415;&#36827;&#23637;&#12289;&#24212;&#29992;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#29983;&#25104;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#65292;&#23427;&#20204;&#21487;&#33021;&#19981;&#21487;&#38752;&#12290;&#35780;&#20272;&#23427;&#20204;&#30340;&#32622;&#20449;&#24230;&#24182;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#36827;&#34892;&#26657;&#20934;&#21487;&#20197;&#24110;&#21161;&#20943;&#36731;&#39118;&#38505;&#65292;&#20351;LLMs&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;&#36817;&#26399;&#26377;&#35768;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23578;&#26080;&#20840;&#38754;&#30340;&#27010;&#36848;&#26469;&#32452;&#32455;&#24182;&#27010;&#36848;&#20027;&#35201;&#30340;&#32463;&#39564;&#25945;&#35757;&#65292;&#26412;&#35843;&#30740;&#26088;&#22312;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#25361;&#25112;&#65292;&#24182;&#24635;&#32467;&#20102;LLMs&#32622;&#20449;&#24230;&#20272;&#35745;&#21644;&#26657;&#20934;&#30340;&#26368;&#26032;&#25216;&#26415;&#36827;&#23637;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08298v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks in various domains. Despite their impressive performance, they can be unreliable due to factual errors in their generations. Assessing their confidence and calibrating them across different tasks can help mitigate risks and enable LLMs to produce better generations. There has been a lot of recent research aiming to address this, but there has been no comprehensive overview to organize it and outline the main lessons learned. The present survey aims to bridge this gap. In particular, we outline the challenges and we summarize recent technical advancements for LLM confidence estimation and calibration. We further discuss their applications and suggest promising directions for future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20013;&#30340;&#33258;&#25105;&#39564;&#35777;&#33021;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#23427;&#20204;&#20934;&#30830;&#35782;&#21035;&#36923;&#36753;&#35884;&#35823;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.07954</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20013;&#30340;&#33258;&#25105;&#39564;&#35777;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20013;&#30340;&#33258;&#25105;&#39564;&#35777;&#33021;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#23427;&#20204;&#20934;&#30830;&#35782;&#21035;&#36923;&#36753;&#35884;&#35823;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#25512;&#29702;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#36861;&#27714;&#30446;&#26631;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#22312;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#19978;&#38754;&#20020;&#22256;&#38590;&#12290;&#20026;&#20102;&#22686;&#24378;&#25512;&#29702;&#24615;&#33021;&#65292;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#26159;&#21487;&#25193;&#23637;&#30340;&#30417;&#30563;&#65292;&#36825;&#38656;&#35201;LLMs&#35782;&#21035;&#33258;&#24049;&#30340;&#38169;&#35823;&#65292;&#28982;&#21518;&#33258;&#34892;&#25913;&#36827;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#33258;&#25105;&#39564;&#35777;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#26159;&#21542;&#24456;&#22909;&#22320;&#29702;&#35299;&#33258;&#24049;&#30340;&#38169;&#35823;&#20173;&#22312;&#35843;&#26597;&#20013;&#12290;&#26412;&#25991;&#30528;&#37325;&#25506;&#35752;&#20102;LLMs&#22312;&#36923;&#36753;&#25512;&#29702;&#32972;&#26223;&#19979;&#30340;&#33258;&#25105;&#39564;&#35777;&#33021;&#21147;&#65292;&#20851;&#27880;&#23427;&#20204;&#20934;&#30830;&#35782;&#21035;&#36923;&#36753;&#35884;&#35823;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;232&#31181;&#25512;&#29702;&#35884;&#35823;&#30340;&#25968;&#25454;&#38598;FALLACIES&#65292;&#24182;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#20851;&#20110;LLMs&#22312;FALLACIES&#19978;&#30340;&#20840;&#38754;&#21644;&#35814;&#32454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07954v2 Announce Type: replace  Abstract: Logical reasoning has been an ongoing pursuit in the field of AI. Despite significant advancements made by large language models (LLMs), they still struggle with complex logical reasoning problems. To enhance reasoning performance, one promising direction is scalable oversight, which requires LLMs to identify their own errors and then improve by themselves. Various self-verification methods have been proposed in pursuit of this goal. Nevertheless, whether existing models understand their own errors well is still under investigation. In this paper, we take a closer look at the self-verification abilities of LLMs in the context of logical reasoning, focusing on their ability to identify logical fallacies accurately. We introduce a dataset, FALLACIES, containing 232 types of reasoning fallacies categorized in a hierarchical taxonomy. By conducting exhaustive experiments on FALLACIES, we obtain comprehensive and detailed analyses of a se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#23558;&#25991;&#26412;&#27169;&#24577;&#34701;&#20837;&#22478;&#24066;&#22270;&#20687;&#25551;&#36848;&#30340;LLM&#22686;&#24378;&#26694;&#26550;UrbanCLIP&#65292;&#24182;&#25506;&#35752;&#20102;&#25991;&#26412;&#27169;&#24577;&#22914;&#20309;&#22686;&#24378;&#22478;&#24066;&#21306;&#22495;&#25551;&#36848;&#20197;&#21450;&#20854;&#24433;&#21709;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2310.18340</link><description>&lt;p&gt;
UrbanCLIP&#65306;&#23398;&#20064;&#26469;&#33258;&#32593;&#32476;&#30340;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#25991;&#26412;&#22686;&#24378;&#30340;&#22478;&#24066;&#21306;&#22495;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
UrbanCLIP: Learning Text-enhanced Urban Region Profiling with Contrastive Language-Image Pretraining from the Web
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#23558;&#25991;&#26412;&#27169;&#24577;&#34701;&#20837;&#22478;&#24066;&#22270;&#20687;&#25551;&#36848;&#30340;LLM&#22686;&#24378;&#26694;&#26550;UrbanCLIP&#65292;&#24182;&#25506;&#35752;&#20102;&#25991;&#26412;&#27169;&#24577;&#22914;&#20309;&#22686;&#24378;&#22478;&#24066;&#21306;&#22495;&#25551;&#36848;&#20197;&#21450;&#20854;&#24433;&#21709;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#32593;&#32476;&#25968;&#25454;&#36827;&#34892;&#30340;&#22478;&#24066;&#21306;&#22495;&#25551;&#36848;&#23545;&#22478;&#24066;&#35268;&#21010;&#21644;&#21487;&#25345;&#32493;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30446;&#30585;&#20102;LLM&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#19981;&#26029;&#23835;&#36215;&#65292;&#23588;&#20854;&#26159;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#30740;&#31350;&#65292;&#22914;&#35270;&#35273;-&#35821;&#35328;&#23398;&#20064;&#65292;&#20854;&#20013;&#25991;&#26412;&#27169;&#24577;&#20316;&#20026;&#22270;&#20687;&#30340;&#34917;&#20805;&#20449;&#24687;&#12290;&#26412;&#25991;&#26088;&#22312;&#22238;&#31572;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;i&#65289;&#25991;&#26412;&#27169;&#24577;&#33021;&#21542;&#22686;&#24378;&#22478;&#24066;&#21306;&#22495;&#25551;&#36848;&#65311;ii&#65289;&#22914;&#26524;&#21487;&#20197;&#65292;&#20197;&#20309;&#31181;&#26041;&#24335;&#21644;&#22312;&#21738;&#20123;&#26041;&#38754;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24378;&#22823;&#21147;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#38598;&#25104;&#25991;&#26412;&#27169;&#24577;&#30693;&#35782;&#21040;&#22478;&#24066;&#22270;&#20687;&#25551;&#36848;&#20013;&#30340;LLM&#22686;&#24378;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;LLM&#22686;&#24378;&#22411;&#22478;&#24066;&#21306;&#22495;&#25551;&#36848;&#65288;UrbanCLIP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.18340v2 Announce Type: replace-cross  Abstract: Urban region profiling from web-sourced data is of utmost importance for urban planning and sustainable development. We are witnessing a rising trend of LLMs for various fields, especially dealing with multi-modal data research such as vision-language learning, where the text modality serves as a supplement information for the image. Since textual modality has never been introduced into modality combinations in urban region profiling, we aim to answer two fundamental questions in this paper: i) Can textual modality enhance urban region profiling? ii) and if so, in what ways and with regard to which aspects? To answer the questions, we leverage the power of Large Language Models (LLMs) and introduce the first-ever LLM-enhanced framework that integrates the knowledge of textual modality into urban imagery profiling, named LLM-enhanced Urban Region Profiling with Contrastive Language-Image Pretraining (UrbanCLIP). Specifically, it
&lt;/p&gt;</description></item><item><title>MuSR&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#21465;&#20107;&#20013;&#36827;&#34892;&#22810;&#27493;&#36719;&#25512;&#29702;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#32508;&#21512;&#21040;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31639;&#27861;&#21019;&#24314;&#22797;&#26434;&#25512;&#29702;&#23454;&#20363;&#65292;&#25361;&#25112;&#20102;&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#38543;&#30528;&#27169;&#22411;&#33021;&#21147;&#30340;&#22686;&#24378;&#36827;&#34892;&#36827;&#19968;&#27493;&#25193;&#23637;&#12290;</title><link>https://arxiv.org/abs/2310.16049</link><description>&lt;p&gt;
MuSR: &#29992;&#22810;&#27493;&#36719;&#25512;&#29702;&#27979;&#35797;&#24605;&#32500;&#38142;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.16049
&lt;/p&gt;
&lt;p&gt;
MuSR&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#21465;&#20107;&#20013;&#36827;&#34892;&#22810;&#27493;&#36719;&#25512;&#29702;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#32508;&#21512;&#21040;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31639;&#27861;&#21019;&#24314;&#22797;&#26434;&#25512;&#29702;&#23454;&#20363;&#65292;&#25361;&#25112;&#20102;&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#38543;&#30528;&#27169;&#22411;&#33021;&#21147;&#30340;&#22686;&#24378;&#36827;&#34892;&#36827;&#19968;&#27493;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#35013;&#22791;&#20102;&#35832;&#22914;&#24605;&#32500;&#38142;&#25552;&#31034;&#31561;&#25216;&#26415;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#36827;&#34892;&#40065;&#26834;&#25512;&#29702;&#30340;&#33021;&#21147;&#20173;&#28982;&#26377;&#25152;&#27424;&#32570;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;LLM&#25512;&#29702;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#31995;&#32479;&#33021;&#21147;&#19981;&#26029;&#22686;&#38271;&#65292;&#32780;&#29992;&#20110;&#36923;&#36753;&#25512;&#29702;&#31561;&#20219;&#21153;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20173;&#28982;&#20445;&#25345;&#19981;&#21464;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MuSR&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#21465;&#20107;&#20013;&#25191;&#34892;&#22810;&#27493;&#36719;&#25512;&#29702;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#29305;&#24449;&#12290;&#39318;&#20808;&#65292;&#23427;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#32508;&#21512;&#21040;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31639;&#27861;&#21019;&#24314;&#30340;&#65292;&#20351;&#24471;&#33021;&#22815;&#26500;&#24314;&#25361;&#25112;GPT-4&#30340;&#22797;&#26434;&#25512;&#29702;&#23454;&#20363;(&#20363;&#22914;&#65292;&#22823;&#32422;1000&#23383;&#38271;&#30340;&#35851;&#26432;&#24748;&#30097;&#25925;&#20107;)&#65292;&#24182;&#19988;&#38543;&#30528;&#21457;&#34892;&#26356;&#26377;&#33021;&#21147;&#30340;LLM&#65292;&#23427;&#21487;&#20197;&#36827;&#19968;&#27493;&#25193;&#23637;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#23454;&#20363;&#26159;&#23545;&#24212;&#20110;&#30495;&#23454;&#39046;&#22495;&#30340;&#33258;&#30001;&#25991;&#26412;&#21465;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.16049v2 Announce Type: replace  Abstract: While large language models (LLMs) equipped with techniques like chain-of-thought prompting have demonstrated impressive capabilities, they still fall short in their ability to reason robustly in complex settings. However, evaluating LLM reasoning is challenging because system capabilities continue to grow while benchmark datasets for tasks like logical deduction have remained static. We introduce MuSR, a dataset for evaluating language models on multistep soft reasoning tasks specified in a natural language narrative. This dataset has two crucial features. First, it is created through a novel neurosymbolic synthetic-to-natural generation algorithm, enabling the construction of complex reasoning instances that challenge GPT-4 (e.g., murder mysteries roughly 1000 words in length) and which can be scaled further as more capable LLMs are released. Second, our dataset instances are free text narratives corresponding to real-world domains
&lt;/p&gt;</description></item><item><title>HallusionBench&#26159;&#19968;&#20010;&#19987;&#20026;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#20687;&#32972;&#26223;&#25512;&#29702;&#20013;&#38754;&#20020;&#25361;&#25112;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#32467;&#26500;&#21644;&#37327;&#21270;&#20998;&#26512;&#65292;&#26174;&#31034;&#20986;GPT-4V&#21462;&#24471;&#20102;31.42%&#30340;&#20934;&#30830;&#29575;&#65292;&#36828;&#39640;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2310.14566</link><description>&lt;p&gt;
HallusionBench&#65306;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#32416;&#32544;&#30340;&#35821;&#35328;&#24187;&#35273;&#21644;&#35270;&#24187;&#35273;&#30340;&#39640;&#32423;&#35786;&#26029;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14566
&lt;/p&gt;
&lt;p&gt;
HallusionBench&#26159;&#19968;&#20010;&#19987;&#20026;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#20687;&#32972;&#26223;&#25512;&#29702;&#20013;&#38754;&#20020;&#25361;&#25112;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#32467;&#26500;&#21644;&#37327;&#21270;&#20998;&#26512;&#65292;&#26174;&#31034;&#20986;GPT-4V&#21462;&#24471;&#20102;31.42%&#30340;&#20934;&#30830;&#29575;&#65292;&#36828;&#39640;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;HallusionBench&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#35780;&#20272;&#22270;&#20687;&#32972;&#26223;&#25512;&#29702;&#32780;&#35774;&#35745;&#30340;&#20840;&#38754;&#22522;&#20934;&#12290;&#36825;&#20010;&#22522;&#20934;&#23545;&#20110;&#39640;&#32423;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#65288;&#22914;GPT-4V&#65288;Vision&#65289;&#12289;Gemini Pro Vision&#21644;LLaVA-1.5&#65289;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#24378;&#35843;&#23545;&#35270;&#35273;&#25968;&#25454;&#30340;&#24494;&#22937;&#29702;&#35299;&#21644;&#35299;&#37322;&#12290;&#35813;&#22522;&#20934;&#21253;&#21547;346&#24352;&#22270;&#20687;&#21644;1129&#20010;&#38382;&#39064;&#65292;&#20840;&#37096;&#30001;&#20154;&#31867;&#19987;&#23478;&#31934;&#24515;&#35774;&#35745;&#12290;&#25105;&#20204;&#20026;&#36825;&#20123;&#35270;&#35273;&#38382;&#39064;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#26500;&#65292;&#26088;&#22312;&#24314;&#31435;&#23545;&#29031;&#32452;&#12290;&#36825;&#31181;&#32467;&#26500;&#20351;&#25105;&#20204;&#33021;&#22815;&#23545;&#27169;&#22411;&#30340;&#21709;&#24212;&#20542;&#21521;&#12289;&#36923;&#36753;&#19968;&#33268;&#24615;&#21644;&#21508;&#31181;&#25925;&#38556;&#27169;&#24335;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;&#12290;&#22312;&#25105;&#20204;&#23545;HallusionBench&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#23545;14&#31181;&#19981;&#21516;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#31361;&#20986;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;GPT-4V&#21462;&#24471;&#30340;31.42&#65285;&#30340;&#38382;&#39064;&#23545;&#20934;&#30830;&#29575;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25152;&#26377;&#20854;&#20182;&#35780;&#20272;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#22343;&#20302;&#20110;16&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14566v3 Announce Type: replace-cross  Abstract: We introduce HallusionBench, a comprehensive benchmark designed for the evaluation of image-context reasoning. This benchmark presents significant challenges to advanced large visual-language models (LVLMs), such as GPT-4V(Vision), Gemini Pro Vision, and LLaVA-1.5, by emphasizing nuanced understanding and interpretation of visual data. The benchmark comprises 346 images paired with 1129 questions, all meticulously crafted by human experts. We introduce a novel structure for these visual questions designed to establish control groups. This structure enables us to conduct a quantitative analysis of the models' response tendencies, logical consistency, and various failure modes. In our evaluation on HallusionBench, we benchmarked 14 different models, highlighting a 31.42% question-pair accuracy achieved by the state-of-the-art GPT-4V. Notably, all other evaluated models achieve accuracy below 16%. Moreover, our analysis not only h
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;RTSUM&#26694;&#26550;&#65292;&#21033;&#29992;&#20851;&#31995;&#19977;&#20803;&#32452;&#36827;&#34892;&#25688;&#35201;&#29983;&#25104;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#25688;&#35201;&#24037;&#20855;&#65292;&#25903;&#25345;&#22810;&#32423;&#26174;&#33879;&#24615;&#21487;&#35270;&#21270;&#12290;</title><link>https://arxiv.org/abs/2310.13895</link><description>&lt;p&gt;
RTSUM&#65306;&#22522;&#20110;&#20851;&#31995;&#19977;&#20803;&#32452;&#30340;&#21487;&#35299;&#37322;&#25688;&#35201;&#19982;&#22810;&#32423;&#26174;&#33879;&#24615;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
RTSUM: Relation Triple-based Interpretable Summarization with Multi-level Salience Visualization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.13895
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;RTSUM&#26694;&#26550;&#65292;&#21033;&#29992;&#20851;&#31995;&#19977;&#20803;&#32452;&#36827;&#34892;&#25688;&#35201;&#29983;&#25104;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#25688;&#35201;&#24037;&#20855;&#65292;&#25903;&#25345;&#22810;&#32423;&#26174;&#33879;&#24615;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RTSUM&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#20851;&#31995;&#19977;&#20803;&#32452;&#20316;&#20026;&#25688;&#35201;&#22522;&#26412;&#21333;&#20803;&#30340;&#26080;&#30417;&#30563;&#25688;&#35201;&#26694;&#26550;&#12290;&#32473;&#23450;&#36755;&#20837;&#25991;&#26723;&#65292;RTSUM&#39318;&#20808;&#36890;&#36807;&#22810;&#32423;&#26174;&#33879;&#24615;&#35780;&#20998;&#36873;&#25321;&#26174;&#33879;&#30340;&#20851;&#31995;&#19977;&#20803;&#32452;&#65292;&#28982;&#21518;&#21033;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#20174;&#36873;&#23450;&#30340;&#20851;&#31995;&#19977;&#20803;&#32452;&#29983;&#25104;&#31616;&#27905;&#25688;&#35201;&#12290;&#22312;RTSUM&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#35299;&#37322;&#24615;&#25688;&#35201;&#24037;&#20855;&#30340;&#32593;&#32476;&#28436;&#31034;&#65292;&#25552;&#20379;&#20102;&#23545;&#36755;&#20986;&#25688;&#35201;&#30340;&#32454;&#31890;&#24230;&#35299;&#37322;&#12290;&#36890;&#36807;&#25903;&#25345;&#33258;&#23450;&#20041;&#36873;&#39033;&#65292;&#25105;&#20204;&#30340;&#24037;&#20855;&#22312;&#19977;&#20010;&#19981;&#21516;&#32423;&#21035;&#19978;&#21487;&#35270;&#21270;&#25991;&#26412;&#21333;&#20803;&#30340;&#26174;&#33879;&#24615;&#65306;&#21477;&#23376;&#12289;&#20851;&#31995;&#19977;&#20803;&#32452;&#21644;&#30701;&#35821;&#12290;&#20195;&#30721;&#24050;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.13895v2 Announce Type: replace  Abstract: In this paper, we present RTSUM, an unsupervised summarization framework that utilizes relation triples as the basic unit for summarization. Given an input document, RTSUM first selects salient relation triples via multi-level salience scoring and then generates a concise summary from the selected relation triples by using a text-to-text language model. On the basis of RTSUM, we also develop a web demo for an interpretable summarizing tool, providing fine-grained interpretations with the output summary. With support for customization options, our tool visualizes the salience for textual units at three distinct levels: sentences, relation triples, and phrases. The codes,are publicly available.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#20915;&#31574;&#27169;&#22411;Bridge&#65292;&#32467;&#21512;&#19987;&#23478;&#30340;&#35748;&#30693;&#20219;&#21153;&#20998;&#26512;&#65292;&#25104;&#21151;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24357;&#34917;&#26032;&#25163;&#21644;&#19987;&#23478;&#22312;&#32416;&#27491;&#25968;&#23398;&#38169;&#35823;&#20013;&#30340;&#30693;&#35782;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2310.10648</link><description>&lt;p&gt;
&#36890;&#36807;&#20915;&#31574;&#27169;&#22411;&#24357;&#34917;&#26032;&#25163;&#19982;&#19987;&#23478;&#20043;&#38388;&#30340;&#24046;&#36317;&#65306;&#20197;&#32416;&#27491;&#25968;&#23398;&#38169;&#35823;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Bridging the Novice-Expert Gap via Models of Decision-Making: A Case Study on Remediating Math Mistakes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.10648
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20915;&#31574;&#27169;&#22411;Bridge&#65292;&#32467;&#21512;&#19987;&#23478;&#30340;&#35748;&#30693;&#20219;&#21153;&#20998;&#26512;&#65292;&#25104;&#21151;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24357;&#34917;&#26032;&#25163;&#21644;&#19987;&#23478;&#22312;&#32416;&#27491;&#25968;&#23398;&#38169;&#35823;&#20013;&#30340;&#30693;&#35782;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#36741;&#23548;&#35268;&#27169;&#21270;&#20173;&#28982;&#26159;&#25945;&#32946;&#20013;&#30340;&#19968;&#39033;&#20027;&#35201;&#25361;&#25112;&#12290;&#30001;&#20110;&#38656;&#27714;&#22686;&#38271;&#65292;&#35768;&#22810;&#24179;&#21488;&#32856;&#29992;&#26032;&#25163;&#23548;&#24072;&#65292;&#20182;&#20204;&#19982;&#32463;&#39564;&#20016;&#23500;&#30340;&#25945;&#32946;&#24037;&#20316;&#32773;&#19981;&#21516;&#65292;&#38590;&#20197;&#35299;&#20915;&#23398;&#29983;&#30340;&#38169;&#35823;&#65292;&#22240;&#27492;&#26080;&#27861;&#25235;&#20303;&#20027;&#35201;&#30340;&#23398;&#20064;&#26426;&#20250;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32416;&#27491;&#25968;&#23398;&#38169;&#35823;&#20013;&#24357;&#34917;&#26032;&#25163;&#21644;&#19987;&#23478;&#20043;&#38388;&#30693;&#35782;&#24046;&#36317;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;Bridge&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#35748;&#30693;&#20219;&#21153;&#20998;&#26512;&#23558;&#19987;&#23478;&#30340;&#28508;&#22312;&#24605;&#32500;&#36807;&#31243;&#36716;&#21270;&#20026;&#32416;&#27491;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36825;&#28041;&#21450;&#19987;&#23478;&#35782;&#21035;(A)&#23398;&#29983;&#30340;&#38169;&#35823;&#12289;(B)&#32416;&#27491;&#31574;&#30053;&#21644;(C)&#29983;&#25104;&#22238;&#24212;&#20043;&#21069;&#30340;&#24847;&#22270;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;700&#20010;&#30495;&#23454;&#36741;&#23548;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#65292;&#30001;&#19987;&#23478;&#26631;&#27880;&#20102;&#20182;&#20204;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;LLMs&#65292;&#24182;&#21457;&#29616;&#19987;&#23478;&#30340;&#20915;&#31574;&#27169;&#22411;&#23545;LLMs&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20197;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65306;&#22238;&#24212;f
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.10648v2 Announce Type: replace-cross  Abstract: Scaling high-quality tutoring remains a major challenge in education. Due to growing demand, many platforms employ novice tutors who, unlike experienced educators, struggle to address student mistakes and thus fail to seize prime learning opportunities. Our work explores the potential of large language models (LLMs) to close the novice-expert knowledge gap in remediating math mistakes. We contribute Bridge, a method that uses cognitive task analysis to translate an expert's latent thought process into a decision-making model for remediation. This involves an expert identifying (A) the student's error, (B) a remediation strategy, and (C) their intention before generating a response. We construct a dataset of 700 real tutoring conversations, annotated by experts with their decisions. We evaluate state-of-the-art LLMs on our dataset and find that the expert's decision-making model is critical for LLMs to close the gap: responses f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#24863;&#30693;&#32763;&#35793;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;NMT&#27169;&#22411;&#26469;&#20272;&#35745;&#20854;&#36755;&#20986;&#36136;&#37327;&#65292;&#21487;&#20197;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#28040;&#38500;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2310.06707</link><description>&lt;p&gt;
&#36136;&#37327;&#24863;&#30693;&#32763;&#35793;&#27169;&#22411;&#65306;&#21333;&#19968;&#27169;&#22411;&#20013;&#30340;&#39640;&#25928;&#29983;&#25104;&#21644;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Quality-Aware Translation Models: Efficient Generation and Quality Estimation in a Single Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.06707
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#24863;&#30693;&#32763;&#35793;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;NMT&#27169;&#22411;&#26469;&#20272;&#35745;&#20854;&#36755;&#20986;&#36136;&#37327;&#65292;&#21487;&#20197;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#28040;&#38500;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#21518;&#39564;&#65288;MAP&#65289;&#35299;&#30721;&#26159;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#35299;&#30721;&#31574;&#30053;&#12290; &#30740;&#31350;&#34920;&#26126;&#65292;&#27169;&#22411;&#27010;&#29575;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#65292;&#20294;&#19981;&#33021;&#24635;&#26159;&#25104;&#31435;&#65292;&#29983;&#25104;&#36136;&#37327;&#21487;&#20197;&#36890;&#36807;&#35299;&#30721;&#26469;&#20248;&#21270;&#19968;&#20010;&#20197;&#24230;&#37327;&#25110;&#36136;&#37327;&#35780;&#20272;&#20449;&#21495;&#25903;&#25345;&#30340;&#25928;&#29992;&#20989;&#25968;&#26469;&#25552;&#39640;&#65292;&#21363;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#25110;&#36136;&#37327;&#24863;&#30693;&#35299;&#30721;&#12290; &#36825;&#20123;&#26041;&#27861;&#30340;&#20027;&#35201;&#32570;&#28857;&#22312;&#20110;&#23427;&#20204;&#38656;&#35201;&#19968;&#20010;&#39069;&#22806;&#30340;&#27169;&#22411;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#35745;&#31639;&#25928;&#29992;&#20989;&#25968;&#65292;&#20250;&#26174;&#33879;&#22686;&#21152;&#35745;&#31639;&#25104;&#26412;&#12290; &#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#35757;&#32451;NMT&#27169;&#22411;&#33258;&#24049;&#26469;&#20272;&#35745;&#20854;&#36755;&#20986;&#36136;&#37327;&#65292;&#20174;&#32780;&#20351;NMT&#27169;&#22411;&#26412;&#36523;&#20855;&#22791;&#36136;&#37327;&#24863;&#30693;&#33021;&#21147;&#12290; &#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#36827;&#34892;MBR&#35299;&#30721;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#23610;&#23544;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.06707v2 Announce Type: replace-cross  Abstract: Maximum-a-posteriori (MAP) decoding is the most widely used decoding strategy for neural machine translation (NMT) models. The underlying assumption is that model probability correlates well with human judgment, with better translations getting assigned a higher score by the model. However, research has shown that this assumption does not always hold, and generation quality can be improved by decoding to optimize a utility function backed by a metric or quality-estimation signal, as is done by Minimum Bayes Risk (MBR) or Quality-Aware decoding. The main disadvantage of these approaches is that they require an additional model to calculate the utility function during decoding, significantly increasing the computational cost. In this paper, we propose to make the NMT models themselves quality-aware by training them to estimate the quality of their own output. Using this approach for MBR decoding we can drastically reduce the size
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20803;&#23398;&#20064;&#35270;&#35282;&#25506;&#35752;&#20102;Transformer&#29992;&#20110;&#22240;&#26524;&#35821;&#35328;&#24314;&#27169;&#26102;&#30340;&#20869;&#37096;&#20248;&#21270;&#36807;&#31243;&#65292;&#21457;&#29616;&#24182;&#20998;&#26512;&#20102;Transformer-based&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#21040;&#30340;token&#34920;&#31034;&#33539;&#25968;&#30340;&#29305;&#27530;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2310.05884</link><description>&lt;p&gt;
&#20174;&#20803;&#23398;&#20064;&#35270;&#35282;&#30475;Transformer&#29992;&#20110;&#22240;&#26524;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
A Meta-Learning Perspective on Transformers for Causal Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20803;&#23398;&#20064;&#35270;&#35282;&#25506;&#35752;&#20102;Transformer&#29992;&#20110;&#22240;&#26524;&#35821;&#35328;&#24314;&#27169;&#26102;&#30340;&#20869;&#37096;&#20248;&#21270;&#36807;&#31243;&#65292;&#21457;&#29616;&#24182;&#20998;&#26512;&#20102;Transformer-based&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#21040;&#30340;token&#34920;&#31034;&#33539;&#25968;&#30340;&#29305;&#27530;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#22312;&#24320;&#21457;&#22823;&#22411;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#21464;&#24471;&#26174;&#33879;&#12290;&#28982;&#32780;&#65292;&#35299;&#37322;&#20854;&#33021;&#21147;&#30340;&#26426;&#21046;&#23578;&#19981;&#20026;&#20154;&#25152;&#20102;&#35299;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#35757;&#32451;&#36807;&#31243;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#35270;&#35282;&#26469;&#25506;&#31350;Transformer&#26550;&#26500;&#22312;&#22240;&#26524;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#35757;&#32451;&#26102;&#30340;&#20869;&#37096;&#20248;&#21270;&#36807;&#31243;&#65292;&#35814;&#32454;&#35828;&#26126;&#20102;Transformer&#20869;&#37096;&#30340;&#19968;&#20010;&#20248;&#21270;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#20010;&#20869;&#37096;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#29702;&#35770;&#20998;&#26512;&#20102;Transformer-based&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#21040;&#30340;token&#34920;&#31034;&#30340;&#33539;&#25968;&#30340;&#29305;&#27530;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24471;&#21040;&#20102;&#21508;&#31181;&#35774;&#32622;&#20013;&#30340;&#23454;&#39564;&#35777;&#23454;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05884v2 Announce Type: replace-cross  Abstract: The Transformer architecture has become prominent in developing large causal language models. However, mechanisms to explain its capabilities are not well understood. Focused on the training process, here we establish a meta-learning view of the Transformer architecture when trained for the causal language modeling task, by explicating an inner optimization process within the Transformer. Further, within the inner optimization, we discover and theoretically analyze a special characteristic of the norms of learned token representations within Transformer-based causal language models. Our analysis is supported by experiments in various settings.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;LoT&#65288;Logical Thoughts&#65289;&#25552;&#31034;&#65292;&#19968;&#20010;&#33258;&#25105;&#25913;&#36827;&#26694;&#26550;&#65292;&#21033;&#29992;&#26681;&#26893;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#21407;&#21017;&#65292;&#29305;&#21035;&#26159;&#24402;&#35884;&#27861;&#65292;&#36880;&#27493;&#39564;&#35777;&#21644;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2309.13339</link><description>&lt;p&gt;
&#36890;&#36807;&#36923;&#36753;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13339
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;LoT&#65288;Logical Thoughts&#65289;&#25552;&#31034;&#65292;&#19968;&#20010;&#33258;&#25105;&#25913;&#36827;&#26694;&#26550;&#65292;&#21033;&#29992;&#26681;&#26893;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#21407;&#21017;&#65292;&#29305;&#21035;&#26159;&#24402;&#35884;&#27861;&#65292;&#36880;&#27493;&#39564;&#35777;&#21644;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340; remarkable generalizability&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#20173;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24191;&#27867;&#30340;&#30693;&#35782;&#65292;&#20294;&#23427;&#20204;&#30340;&#25512;&#29702;&#32463;&#24120;&#26410;&#33021;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#26469;&#24314;&#31435;&#36830;&#36143;&#30340;&#24605;&#32500;&#33539;&#24335;&#12290;&#36825;&#20123;&#27169;&#22411;&#26377;&#26102;&#20250;&#20986;&#29616;&#24187;&#35273;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#25512;&#29702;&#36807;&#31243;&#26410;&#21463;&#36923;&#36753;&#21407;&#21017;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; LoT&#65288;Logical Thoughts&#65289;&#25552;&#31034;&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#25105;&#25913;&#36827;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#26681;&#26893;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#21407;&#21017;&#65292;&#29305;&#21035;&#26159;&#24402;&#35884;&#27861;&#65292;&#36880;&#27493;&#31995;&#32479;&#22320;&#39564;&#35777;&#21644;&#32416;&#27491;&#25512;&#29702;&#36807;&#31243;&#12290;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13339v2 Announce Type: replace-cross  Abstract: Recent advancements in large language models have showcased their remarkable generalizability across various domains. However, their reasoning abilities still have significant room for improvement, especially when confronted with scenarios requiring multi-step reasoning. Although large language models possess extensive knowledge, their reasoning often fails to effectively utilize this knowledge to establish a coherent thinking paradigm. These models sometimes show hallucinations as their reasoning procedures are unconstrained by logical principles. Aiming at improving the zero-shot chain-of-thought reasoning ability of large language models, we propose LoT (Logical Thoughts) prompting, a self-improvement framework that leverages principles rooted in symbolic logic, particularly Reductio ad Absurdum, to systematically verify and rectify the reasoning processes step by step. Experimental evaluations conducted on language tasks in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#24635;&#32467;&#22312;&#22810;&#31687;&#26032;&#38395;&#25991;&#31456;&#20013;&#36935;&#21040;&#30340;&#22810;&#26679;&#20449;&#24687;&#65292;&#28085;&#30422;&#20102;&#30456;&#21516;&#20107;&#20214;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;DiverseSumm&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34913;&#37327;&#25688;&#35201;&#35206;&#30422;&#33539;&#22260;&#21644;&#24544;&#23454;&#24230;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2309.09369</link><description>&lt;p&gt;
&#25317;&#25265;&#22810;&#26679;&#24615;&#20197;&#33719;&#24471;&#26356;&#20016;&#23500;&#30340;&#27934;&#35265;&#65306;&#19968;&#39033;&#22810;&#25991;&#26723;&#24635;&#32467;&#22522;&#20934;&#30740;&#31350;&#21450;&#22312;&#26032;&#38395;&#25991;&#31456;&#20013;&#24635;&#32467;&#22810;&#26679;&#20449;&#24687;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Embrace Divergence for Richer Insights: A Multi-document Summarization Benchmark and a Case Study on Summarizing Diverse Information from News Articles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.09369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#24635;&#32467;&#22312;&#22810;&#31687;&#26032;&#38395;&#25991;&#31456;&#20013;&#36935;&#21040;&#30340;&#22810;&#26679;&#20449;&#24687;&#65292;&#28085;&#30422;&#20102;&#30456;&#21516;&#20107;&#20214;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;DiverseSumm&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34913;&#37327;&#25688;&#35201;&#35206;&#30422;&#33539;&#22260;&#21644;&#24544;&#23454;&#24230;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#22810;&#25991;&#26723;&#26032;&#38395;&#24635;&#32467;&#30740;&#31350;&#36890;&#24120;&#38598;&#20013;&#22312;&#25972;&#29702;&#25152;&#26377;&#20449;&#24687;&#28304;&#37117;&#21516;&#24847;&#30340;&#20449;&#24687;&#19978;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20998;&#25955;&#22312;&#22810;&#31687;&#20851;&#20110;&#21516;&#19968;&#20107;&#20214;&#30340;&#25991;&#31456;&#20013;&#30340;&#22810;&#26679;&#20449;&#24687;&#30340;&#24635;&#32467;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#24635;&#32467;&#22312;&#22810;&#31687;&#26032;&#38395;&#25991;&#31456;&#20013;&#36935;&#21040;&#30340;&#22810;&#26679;&#20449;&#24687;&#65292;&#28085;&#30422;&#20102;&#30456;&#21516;&#20107;&#20214;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#19968;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35782;&#21035;&#22810;&#26679;&#20449;&#24687;&#30340;&#25968;&#25454;&#25910;&#38598;&#26550;&#26500;&#65292;&#24182;&#31574;&#21010;&#20102;&#19968;&#20010;&#21517;&#20026;DiverseSumm&#30340;&#25968;&#25454;&#38598;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324;245&#31687;&#26032;&#38395;&#25253;&#36947;&#65292;&#27599;&#31687;&#25253;&#36947;&#21253;&#21547;10&#31687;&#26032;&#38395;&#25991;&#31456;&#65292;&#24182;&#37197;&#26377;&#32463;&#20154;&#24037;&#39564;&#35777;&#30340;&#21442;&#32771;&#25991;&#26412;&#12290;&#25509;&#19979;&#26469;&#65292;&#20026;&#20102;&#21551;&#29992;&#19968;&#33268;&#30340;&#33258;&#21160;&#35780;&#20272;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24230;&#37327;&#35780;&#20272;&#25688;&#35201;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#24544;&#23454;&#24230;&#26102;&#30340;&#20301;&#32622;&#21644;&#20887;&#38271;&#20559;&#24046;&#12290;&#36890;&#36807;&#30456;&#20851;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;o
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.09369v2 Announce Type: replace  Abstract: Previous research in multi-document news summarization has typically concentrated on collating information that all sources agree upon. However, the summarization of diverse information dispersed across multiple articles about an event remains underexplored. In this paper, we propose a new task of summarizing diverse information encountered in multiple news articles encompassing the same event. To facilitate this task, we outlined a data collection schema for identifying diverse information and curated a dataset named DiverseSumm. The dataset includes 245 news stories, with each story comprising 10 news articles and paired with a human-validated reference. Next, to enable consistent automatic evaluation, we conducted a comprehensive analysis to pinpoint the position and verbosity biases when utilizing Large Language Model (LLM)-based metrics for evaluating the coverage and faithfulness of summaries. Through correlation analyses, we o
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#25512;&#33616;&#31995;&#32479;&#30340;&#29983;&#25104;&#24335;&#25512;&#33616;&#25552;&#20379;&#20102;&#26032;&#26426;&#36935;&#65292;&#21487;&#20197;&#31616;&#21270;&#25512;&#33616;&#27969;&#31243;&#24182;&#30452;&#25509;&#20174;&#23436;&#25972;&#30340;&#39033;&#30446;&#27744;&#20013;&#29983;&#25104;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2309.01157</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#24335;&#25512;&#33616;&#65306;&#19968;&#39033;&#35843;&#26597;&#21644;&#36828;&#35265;&#35752;&#35770;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Generative Recommendation: A Survey and Visionary Discussions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01157
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#25512;&#33616;&#31995;&#32479;&#30340;&#29983;&#25104;&#24335;&#25512;&#33616;&#25552;&#20379;&#20102;&#26032;&#26426;&#36935;&#65292;&#21487;&#20197;&#31616;&#21270;&#25512;&#33616;&#27969;&#31243;&#24182;&#30452;&#25509;&#20174;&#23436;&#25972;&#30340;&#39033;&#30446;&#27744;&#20013;&#29983;&#25104;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#20165;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#65292;&#36824;&#26377;&#28508;&#21147;&#37325;&#22609;&#35768;&#22810;&#20854;&#20182;&#39046;&#22495;&#65292;&#20363;&#22914;&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#22522;&#20110;LLM&#30340;&#29983;&#25104;&#24335;&#25512;&#33616;&#30340;&#36827;&#23637;&#12289;&#26041;&#27861;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#30528;&#30524;&#20110;&#19977;&#20010;&#38382;&#39064;&#65306;1&#65289;&#29983;&#25104;&#24335;&#25512;&#33616;&#26159;&#20160;&#20040;&#65292;2&#65289;&#20026;&#20160;&#20040;RS&#24212;&#35813;&#21457;&#23637;&#21040;&#29983;&#25104;&#24335;&#25512;&#33616;&#65292;3&#65289;&#22914;&#20309;&#20026;&#21508;&#31181;RS&#23454;&#29616;&#22522;&#20110;LLM&#30340;&#29983;&#25104;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01157v2 Announce Type: replace-cross  Abstract: Large language models (LLM) not only have revolutionized the field of natural language processing (NLP) but also have the potential to reshape many other fields, e.g., recommender systems (RS). However, most of the related work treats an LLM as a component of the conventional recommendation pipeline (e.g., as a feature extractor), which may not be able to fully leverage the generative power of LLM. Instead of separating the recommendation process into multiple stages, such as score computation and re-ranking, this process can be simplified to one stage with LLM: directly generating recommendations from the complete pool of items. This survey reviews the progress, methods, and future directions of LLM-based generative recommendation by examining three questions: 1) What generative recommendation is, 2) Why RS should advance to generative recommendation, and 3) How to implement LLM-based generative recommendation for various RS t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22352;&#26631;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#26367;&#20195;&#24615;&#35270;&#35282;&#65292;&#36890;&#36807;&#24314;&#31435;&#33258;&#21160;&#21270;&#35780;&#20272;&#20998;&#25968;&#21644;&#37319;&#29992;&#19977;&#31181;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65292;&#20026;&#29983;&#25104;&#21644;&#35780;&#20272;&#35299;&#37322;&#25552;&#20379;&#20102;&#26032;&#30340;&#30740;&#31350;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2308.14115</link><description>&lt;p&gt;
&#22352;&#26631;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Situated Natural Language Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.14115
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22352;&#26631;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#26367;&#20195;&#24615;&#35270;&#35282;&#65292;&#36890;&#36807;&#24314;&#31435;&#33258;&#21160;&#21270;&#35780;&#20272;&#20998;&#25968;&#21644;&#37319;&#29992;&#19977;&#31181;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65292;&#20026;&#29983;&#25104;&#21644;&#35780;&#20272;&#35299;&#37322;&#25552;&#20379;&#20102;&#26032;&#30340;&#30740;&#31350;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#26159;&#21521;&#20154;&#31867;&#35299;&#37322;&#20915;&#31574;&#26368;&#26131;&#29702;&#35299;&#30340;&#24037;&#20855;&#20043;&#19968;&#65292;&#32780;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#29983;&#25104;&#36830;&#36143;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65288;NLE&#65289;&#30340;&#33021;&#21147;&#12290;&#29616;&#26377;&#30340;NLE&#30740;&#31350;&#35266;&#28857;&#27809;&#26377;&#32771;&#34385;&#21463;&#20247;&#30340;&#22240;&#32032;&#12290;&#19968;&#20010;NLE&#21487;&#20197;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#65292;&#20294;&#21487;&#33021;&#19981;&#31526;&#21512;&#21463;&#20247;&#30340;&#38656;&#27714;&#21644;&#21916;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26367;&#20195;&#24615;&#35270;&#35282;&#65292;&#21363;\textit{&#22352;&#26631;}NLE&#12290;&#22312;&#35780;&#20272;&#26041;&#38754;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#33258;&#21160;&#21270;&#35780;&#20272;&#20998;&#25968;&#12290;&#36825;&#20123;&#20998;&#25968;&#25551;&#36848;&#20102;NLE&#22312;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#35821;&#29992;&#31867;&#21035;&#19978;&#30340;&#23646;&#24615;&#12290;&#22312;&#29983;&#25104;&#26041;&#38754;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19977;&#31181;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#30340;&#36866;&#29992;&#24615;&#12290;&#22352;&#26631;NLE&#25552;&#20379;&#20102;&#19968;&#31181;&#35270;&#35282;&#65292;&#20419;&#36827;&#20102;&#20851;&#20110;&#35299;&#37322;&#29983;&#25104;&#21644;&#35780;&#20272;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.14115v2 Announce Type: replace  Abstract: Natural language is among the most accessible tools for explaining decisions to humans, and large pretrained language models (PLMs) have demonstrated impressive abilities to generate coherent natural language explanations (NLE). The existing NLE research perspectives do not take the audience into account. An NLE can have high textual quality, but it might not accommodate audiences' needs and preference. To address this limitation, we propose an alternative perspective, \textit{situated} NLE. On the evaluation side, we set up automated evaluation scores. These scores describe the properties of NLEs in lexical, semantic, and pragmatic categories. On the generation side, we identify three prompt engineering techniques and assess their applicability on the situations. Situated NLE provides a perspective and facilitates further research on the generation and evaluation of explanations.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#27491;&#24335;&#21270;&#21644;&#24050;&#30693;&#36234;&#29425;&#20998;&#31867;&#27861;&#20197;&#22635;&#34917;&#23545;&#21830;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#36234;&#29425;&#25915;&#20987;&#30340;&#32570;&#20047;&#30740;&#31350;&#65292;&#35843;&#26597;&#29616;&#26377;&#30340;&#36234;&#29425;&#26041;&#27861;&#21450;&#20854;&#22312;&#24320;&#28304;&#21644;&#21830;&#29992;LLMs&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2305.14965</link><description>&lt;p&gt;
&#27450;&#39575;LLMs&#35753;&#20854;&#19981;&#36981;&#20174;&#65306;&#27491;&#24335;&#21270;&#12289;&#20998;&#26512;&#21644;&#26816;&#27979;&#36234;&#29425;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.14965
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#27491;&#24335;&#21270;&#21644;&#24050;&#30693;&#36234;&#29425;&#20998;&#31867;&#27861;&#20197;&#22635;&#34917;&#23545;&#21830;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#36234;&#29425;&#25915;&#20987;&#30340;&#32570;&#20047;&#30740;&#31350;&#65292;&#35843;&#26597;&#29616;&#26377;&#30340;&#36234;&#29425;&#26041;&#27861;&#21450;&#20854;&#22312;&#24320;&#28304;&#21644;&#21830;&#29992;LLMs&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#21830;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25506;&#32034;&#34920;&#26126;&#65292;&#38750;&#19987;&#23478;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#25805;&#32437;&#20182;&#20204;&#30340;&#25552;&#31034;&#26469;&#36234;&#29425;LLMs&#65307;&#23548;&#33268;&#36864;&#21270;&#30340;&#36755;&#20986;&#34892;&#20026;&#12289;&#38544;&#31169;&#19982;&#23433;&#20840;&#28431;&#27934;&#12289;&#20882;&#29359;&#24615;&#36755;&#20986;&#20197;&#21450;&#36829;&#21453;&#20869;&#23481;&#30417;&#31649;&#25919;&#31574;&#12290;&#26377;&#38480;&#30340;&#30740;&#31350;&#24050;&#36827;&#34892;&#20102;&#23545;&#36825;&#20123;&#25915;&#20987;&#21450;&#20854;&#32531;&#35299;&#25514;&#26045;&#30340;&#27491;&#24335;&#21270;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#24418;&#24335;&#21270;&#25551;&#36848;&#21644;&#24050;&#30693;&#65288;&#21450;&#21487;&#33021;&#30340;&#65289;&#36234;&#29425;&#20998;&#31867;&#27861;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#35843;&#26597;&#29616;&#26377;&#30340;&#36234;&#29425;&#26041;&#27861;&#21450;&#20854;&#22312;&#24320;&#28304;&#21644;&#21830;&#29992;LLMs&#65288;&#22914;&#22522;&#20110;GPT&#30340;&#27169;&#22411;&#12289;OPT&#12289;BLOOM&#21644;FLAN-T5-XXL&#65289;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#36234;&#29425;&#26816;&#27979;&#22312;&#38024;&#23545;&#24050;&#30693;&#25915;&#20987;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;4&#39033;&#20219;&#21153;&#30340;3700&#20010;&#36234;&#29425;&#25552;&#31034;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23558;&#38543;&#30528;&#27169;&#22411;&#36755;&#20986;&#19968;&#36215;&#20844;&#24320;&#36825;&#19968;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.14965v2 Announce Type: replace  Abstract: Recent explorations with commercial Large Language Models (LLMs) have shown that non-expert users can jailbreak LLMs by simply manipulating their prompts; resulting in degenerate output behavior, privacy and security breaches, offensive outputs, and violations of content regulator policies. Limited studies have been conducted to formalize and analyze these attacks and their mitigations. We bridge this gap by proposing a formalism and a taxonomy of known (and possible) jailbreaks. We survey existing jailbreak methods and their effectiveness on open-source and commercial LLMs (such as GPT-based models, OPT, BLOOM, and FLAN-T5-XXL). We further discuss the challenges of jailbreak detection in terms of their effectiveness against known attacks. For our analysis, we collect a dataset of 3700 jailbreak prompts across 4 tasks. We will make the dataset public along with the model outputs.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#25972;&#29702;&#27969;&#31243;&#26469;&#26500;&#24314;&#25991;&#21270;&#30456;&#20851;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;GPT-4&#26080;&#21442;&#32771;&#35780;&#20272;&#32763;&#35793;&#30340;&#21487;&#29702;&#35299;&#24615;&#12290;</title><link>https://arxiv.org/abs/2305.14328</link><description>&lt;p&gt;
&#22312;&#25991;&#21270;&#24847;&#35782;&#19978;&#22522;&#20110;LLM&#30340;&#26426;&#22120;&#32763;&#35793;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking LLM-based Machine Translation on Cultural Awareness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.14328
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#25972;&#29702;&#27969;&#31243;&#26469;&#26500;&#24314;&#25991;&#21270;&#30456;&#20851;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;GPT-4&#26080;&#21442;&#32771;&#35780;&#20272;&#32763;&#35793;&#30340;&#21487;&#29702;&#35299;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32763;&#35793;&#25991;&#21270;&#29305;&#23450;&#20869;&#23481;&#23545;&#20110;&#26377;&#25928;&#30340;&#36328;&#25991;&#21270;&#27807;&#36890;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20173;&#28982;&#38590;&#20197;&#20934;&#30830;&#29702;&#35299;&#21644;&#32763;&#35793;&#21253;&#21547;&#25991;&#21270;&#29305;&#23450;&#23454;&#20307;&#30340;&#21477;&#23376;&#12290;&#26368;&#36817;&#20851;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#36827;&#23637;&#21033;&#29992;&#36731;&#37327;&#32423;&#25552;&#31034;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#25552;&#39640;&#20855;&#26377;&#25991;&#21270;&#24847;&#35782;&#30340;&#26426;&#22120;&#32763;&#35793;&#30340;&#26377;&#25928;&#24615;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#25972;&#29702;&#27969;&#31243;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#25991;&#21270;&#30456;&#20851;&#24182;&#20016;&#23500;&#20102;&#25991;&#21270;&#29305;&#23450;&#39033;&#30446;&#27880;&#37322;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;GPT-4&#20197;&#26080;&#21442;&#32771;&#26041;&#24335;&#35780;&#20272;&#32763;&#35793;&#30340;&#21487;&#29702;&#35299;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#21508;&#31181;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(NMT)&#21644;LLM-based MT&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#25552;&#31034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.14328v2 Announce Type: replace  Abstract: Translating cultural-specific content is crucial for effective cross-cultural communication. However, many MT systems still struggle to translate sentences containing cultural-specific entities accurately and understandably. Recent advancements in in-context learning utilize lightweight prompts to guide large language models (LLMs) in machine translation tasks. Nevertheless, the effectiveness of this approach in enhancing machine translation with cultural awareness remains uncertain. To address this gap, we introduce a new data curation pipeline to construct a culturally relevant parallel corpus, enriched with annotations of cultural-specific items. Furthermore, we devise a novel evaluation metric to assess the understandability of translations in a reference-free manner by GPT-4. We evaluate a variety of neural machine translation (NMT) and LLM-based MT systems using our dataset. Additionally, we propose several prompting strategies
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#30417;&#30563;&#21452;&#35821;&#35789;&#34920;&#35825;&#23548;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#30456;&#20851;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;&#39640;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#35789;&#34920;&#35825;&#23548;&#65292;&#21482;&#38656;&#35201;&#23545;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#26029;&#12290;</title><link>https://arxiv.org/abs/2305.14012</link><description>&lt;p&gt;
&#24403;&#20320;&#30340;&#34920;&#20146;&#26377;&#27491;&#30830;&#30340;&#36830;&#25509;&#65306;&#38750;&#30417;&#30563;&#21452;&#35821;&#35789;&#34920;&#35825;&#23548;&#29992;&#20110;&#30456;&#20851;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
When your Cousin has the Right Connections: Unsupervised Bilingual Lexicon Induction for Related Data-Imbalanced Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.14012
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#30417;&#30563;&#21452;&#35821;&#35789;&#34920;&#35825;&#23548;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#30456;&#20851;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;&#39640;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#35789;&#34920;&#35825;&#23548;&#65292;&#21482;&#38656;&#35201;&#23545;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#38750;&#30417;&#30563;&#21452;&#35821;&#35789;&#34920;&#35825;&#23548;&#65288;BLI&#65289;&#26041;&#27861;&#20381;&#36182;&#20110;&#38656;&#35201;&#22823;&#22411;&#21333;&#35821;&#35821;&#26009;&#24211;&#30340;&#33391;&#22909;&#36136;&#37327;&#30340;&#38745;&#24577;&#25110;&#19978;&#19979;&#25991;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#38750;&#30417;&#30563;BLI&#26368;&#26377;&#21487;&#33021;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;LRLs&#65289;&#26377;&#29992;&#65292;&#23545;&#20110;&#36825;&#20123;&#35821;&#35328;&#65292;&#22823;&#22411;&#25968;&#25454;&#38598;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#25105;&#20204;&#32463;&#24120;&#23545;&#24314;&#31435;LRLs&#19982;&#30456;&#20851;&#39640;&#36164;&#28304;&#35821;&#35328;&#65288;HRLs&#65289;&#20043;&#38388;&#30340;&#21452;&#35821;&#36164;&#28304;&#24863;&#20852;&#36259;&#65292;&#32467;&#26524;&#36896;&#25104;BLI&#30340;&#25968;&#25454;&#35774;&#32622;&#20986;&#29616;&#20005;&#37325;&#19981;&#24179;&#34913;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#25991;&#29486;&#20013;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;BLI&#26041;&#27861;&#23545;&#20005;&#37325;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#35821;&#35328;&#23545;&#34920;&#29616;&#25509;&#36817;&#38646;&#30340;&#24615;&#33021;&#65292;&#36825;&#34920;&#26126;&#36825;&#20123;&#35774;&#32622;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#25216;&#26415;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#30456;&#20851;&#30340;LRL&#21644;HRL&#20043;&#38388;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;BLI&#65292;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#23545;HRL&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#26029;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;Bhojpuri&#21644;Magahi&#36825;&#20004;&#31181;&#30495;&#27491;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#26377;&#25928;&#24615;&#65288;&#21333;&#35821;&#35821;&#26009;&#23567;&#20110;5M&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.14012v2 Announce Type: replace  Abstract: Most existing approaches for unsupervised bilingual lexicon induction (BLI) depend on good quality static or contextual embeddings requiring large monolingual corpora for both languages. However, unsupervised BLI is most likely to be useful for low-resource languages (LRLs), where large datasets are not available. Often we are interested in building bilingual resources for LRLs against related high-resource languages (HRLs), resulting in severely imbalanced data settings for BLI. We first show that state-of-the-art BLI methods in the literature exhibit near-zero performance for severely data-imbalanced language pairs, indicating that these settings require more robust techniques. We then present a new method for unsupervised BLI between a related LRL and HRL that only requires inference on a masked language model of the HRL, and demonstrate its effectiveness on truly low-resource languages Bhojpuri and Magahi (with &lt;5M monolingual to
&lt;/p&gt;</description></item><item><title>LLM-Pat&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#24314;&#24182;&#27604;&#36739;&#20505;&#36873;&#25991;&#26412;&#19982;&#20854;&#23545;&#24212;&#30340;&#8220;&#20804;&#24351;&#8221;&#25991;&#26412;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#21028;&#26029;&#20505;&#36873;&#25991;&#26412;&#26159;&#21542;&#30001;&#26426;&#22120;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2305.12519</link><description>&lt;p&gt;
LLM&#20146;&#23376;&#37492;&#23450;&#65306;LLM&#36951;&#20256;&#32487;&#25215;&#20013;&#30340;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
LLM Paternity Test: Generated Text Detection with LLM Genetic Inheritance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.12519
&lt;/p&gt;
&lt;p&gt;
LLM-Pat&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#24314;&#24182;&#27604;&#36739;&#20505;&#36873;&#25991;&#26412;&#19982;&#20854;&#23545;&#24212;&#30340;&#8220;&#20804;&#24351;&#8221;&#25991;&#26412;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#21028;&#26029;&#20505;&#36873;&#25991;&#26412;&#26159;&#21542;&#30001;&#26426;&#22120;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#29983;&#25104;&#25658;&#24102;&#21508;&#31181;&#28389;&#29992;&#39118;&#38505;&#30340;&#25991;&#26412;&#65292;&#21253;&#25324;&#25220;&#34989;&#12289;&#22312;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#21457;&#24067;&#34394;&#20551;&#35780;&#35770;&#65292;&#25110;&#32773;&#21046;&#20316;&#24341;&#20154;&#27880;&#30446;&#30340;&#34394;&#20551;&#25512;&#25991;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#25991;&#26412;&#26159;&#21542;&#30001;&#26426;&#22120;&#29983;&#25104;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#20005;&#37325;&#20381;&#36182;&#35757;&#32451;&#25968;&#25454;&#65292;&#23427;&#20204;&#24448;&#24448;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#21363;LLM&#20146;&#23376;&#37492;&#23450;&#65288;LLM-Pat&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#20219;&#20309;&#20505;&#36873;&#25991;&#26412;&#65288;"&#23376;&#31867;"&#65289;&#65292;LLM-Pat&#20351;&#29992;&#19968;&#20010;&#20013;&#38388;LLM&#65288;"&#29238;&#31867;"&#65289;&#37325;&#24314;&#19982;&#32473;&#23450;&#25991;&#26412;&#23545;&#24212;&#30340;"&#20804;&#24351;"&#25991;&#26412;&#65292;&#28982;&#21518;&#34913;&#37327;&#20505;&#36873;&#25991;&#26412;&#19982;&#20854;"&#20804;&#24351;"&#25991;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#39640;&#30456;&#20284;&#24615;&#34920;&#26126;&#20505;&#36873;&#25991;&#26412;&#26159;&#30001;&#26426;&#22120;&#29983;&#25104;&#65292;&#31867;&#20284;&#20110;&#22522;&#22240;&#29305;&#24449;&#12290;&#25105;&#20204;&#24050;&#26500;&#24314;&#20102;&#25968;&#25454;&#38598;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.12519v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) can generate texts that carry the risk of various misuses, including plagiarism, planting fake reviews on e-commerce platforms, or creating inflammatory false tweets. Detecting whether a text is machine-generated has thus become increasingly important. While existing detection methods exhibit superior performance, they often lack generalizability due to their heavy dependence on training data. To alleviate this problem, we propose a model-related generated text detection method, the LLM Paternity Test (LLM-Pat). Specifically, given any candidate text (\textit{child}), LLM-Pat employs an intermediary LLM (\textit{parent}) to reconstruct a \textit{sibling} text corresponding to the given text and then measures the similarity between candidate texts and their sibling texts. High similarity indicates that the candidate text is machine-generated, akin to genetic traits. We have constructed datasets encom
&lt;/p&gt;</description></item><item><title>Spacerini&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;Pyserini&#21644;Hugging Face&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#26080;&#32541;&#26500;&#24314;&#21644;&#37096;&#32626;&#20132;&#20114;&#24335;&#25628;&#32034;&#24341;&#25806;&#65292;&#20351;&#24471;&#38750;IR&#20174;&#19994;&#32773;&#21487;&#20197;&#26356;&#36731;&#26494;&#22320;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#26816;&#32034;&#27169;&#22411;&#65292;&#23545;NLP&#21644;IR&#30740;&#31350;&#20154;&#21592;&#20197;&#21450;&#31532;&#19977;&#26041;&#22797;&#21046;&#30740;&#31350;&#24037;&#20316;&#37117;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>https://arxiv.org/abs/2302.14534</link><description>&lt;p&gt;
Spacerini&#65306;&#20351;&#29992;Pyserini&#21644;Hugging Face&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#25628;&#32034;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
Spacerini: Plug-and-play Search Engines with Pyserini and Hugging Face
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.14534
&lt;/p&gt;
&lt;p&gt;
Spacerini&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;Pyserini&#21644;Hugging Face&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#26080;&#32541;&#26500;&#24314;&#21644;&#37096;&#32626;&#20132;&#20114;&#24335;&#25628;&#32034;&#24341;&#25806;&#65292;&#20351;&#24471;&#38750;IR&#20174;&#19994;&#32773;&#21487;&#20197;&#26356;&#36731;&#26494;&#22320;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#26816;&#32034;&#27169;&#22411;&#65292;&#23545;NLP&#21644;IR&#30740;&#31350;&#20154;&#21592;&#20197;&#21450;&#31532;&#19977;&#26041;&#22797;&#21046;&#30740;&#31350;&#24037;&#20316;&#37117;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Spacerini&#65292;&#36825;&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;Pyserini&#24037;&#20855;&#21253;&#21644;Hugging Face&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#26080;&#32541;&#26500;&#24314;&#21644;&#37096;&#32626;&#20132;&#20114;&#24335;&#25628;&#32034;&#24341;&#25806;&#30340;&#24037;&#20855;&#12290;&#36890;&#36807;Spacerini&#65292;&#38750;IR&#20174;&#19994;&#32773;&#21487;&#20197;&#26356;&#36731;&#26494;&#22320;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#31232;&#30095;&#21644;&#31264;&#23494;&#26816;&#32034;&#27169;&#22411;&#65292;&#24182;&#26368;&#23567;&#21270;&#37096;&#32626;&#24037;&#20316;&#37327;&#12290;Spacerini&#23545;&#20110;&#24076;&#26395;&#36890;&#36807;&#23545;&#35757;&#32451;&#35821;&#26009;&#24211;&#36827;&#34892;&#23450;&#24615;&#20998;&#26512;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#39564;&#35777;&#30740;&#31350;&#30340;NLP&#30740;&#31350;&#20154;&#21592;&#12289;&#24076;&#26395;&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;Pyserini&#29983;&#24577;&#31995;&#32479;&#20013;&#23637;&#31034;&#26032;&#26816;&#32034;&#27169;&#22411;&#30340;IR&#30740;&#31350;&#20154;&#21592;&#20197;&#21450;&#22797;&#21046;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#24037;&#20316;&#30340;&#31532;&#19977;&#26041;&#26469;&#35828;&#37117;&#38750;&#24120;&#26377;&#29992;&#12290;Spacerini&#26159;&#24320;&#28304;&#30340;&#65292;&#24182;&#21253;&#25324;&#29992;&#20110;&#26412;&#22320;&#21644;&#36828;&#31243;&#21152;&#36733;&#12289;&#39044;&#22788;&#29702;&#12289;&#32034;&#24341;&#21644;&#37096;&#32626;&#25628;&#32034;&#24341;&#25806;&#30340;&#23454;&#29992;&#31243;&#24207;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;Spacerini&#21019;&#24314;&#30340;13&#20010;&#19981;&#21516;&#29992;&#20363;&#30340;&#25628;&#32034;&#24341;&#25806;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.14534v2 Announce Type: replace-cross  Abstract: We present Spacerini, a tool that integrates the Pyserini toolkit for reproducible information retrieval research with Hugging Face to enable the seamless construction and deployment of interactive search engines. Spacerini makes state-of-the-art sparse and dense retrieval models more accessible to non-IR practitioners while minimizing deployment effort. This is useful for NLP researchers who want to better understand and validate their research by performing qualitative analyses of training corpora, for IR researchers who want to demonstrate new retrieval models integrated into the growing Pyserini ecosystem, and for third parties reproducing the work of other researchers. Spacerini is open source and includes utilities for loading, preprocessing, indexing, and deploying search engines locally and remotely. We demonstrate a portfolio of 13 search engines created with Spacerini for different use cases.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#27010;&#24565;&#24863;&#30693;&#27880;&#24847;&#21147;&#30340;&#30693;&#35782;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#26816;&#27979;&#65292;&#36890;&#36807;&#23558;&#21307;&#23398;&#30693;&#35782;&#21644;&#25991;&#26412;&#22270;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#19981;&#21516;&#31867;&#22411;&#33410;&#28857;&#30340;&#29305;&#24449;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2301.10451</link><description>&lt;p&gt;
&#20855;&#26377;&#27010;&#24565;&#24863;&#30693;&#27880;&#24847;&#21147;&#30340;&#30693;&#35782;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Knowledge-augmented Graph Neural Networks with Concept-aware Attention for Adverse Drug Event Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.10451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#27010;&#24565;&#24863;&#30693;&#27880;&#24847;&#21147;&#30340;&#30693;&#35782;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#26816;&#27979;&#65292;&#36890;&#36807;&#23558;&#21307;&#23398;&#30693;&#35782;&#21644;&#25991;&#26412;&#22270;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#19981;&#21516;&#31867;&#22411;&#33410;&#28857;&#30340;&#29305;&#24449;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#65288;ADEs&#65289;&#26159;&#33647;&#29289;&#23433;&#20840;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#21508;&#31181;&#25991;&#26412;&#65292;&#22914;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#12289;&#33647;&#29289;&#35780;&#35770;&#12289;&#31038;&#20132;&#23186;&#20307;&#21644;&#21307;&#30103;&#35770;&#22363;&#19978;&#30340;&#29992;&#25143;&#24086;&#23376;&#65292;&#21253;&#21547;&#22823;&#37327;&#20851;&#20110;ADEs&#30340;&#20449;&#24687;&#12290;&#36817;&#26399;&#30340;&#30740;&#31350;&#24212;&#29992;&#20102;&#22522;&#20110;&#35789;&#23884;&#20837;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26469;&#33258;&#21160;&#21270;&#20174;&#25991;&#26412;&#20013;&#26816;&#27979;ADEs&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26410;&#25506;&#35752;&#22914;&#20309;&#23558;&#33647;&#29289;&#21644;&#19981;&#33391;&#21453;&#24212;&#30340;&#26174;&#24335;&#21307;&#23398;&#30693;&#35782;&#25110;&#30456;&#24212;&#30340;&#29305;&#24449;&#23398;&#20064;&#32435;&#20837;&#21040;&#20854;&#20013;&#12290;&#26412;&#25991;&#37319;&#29992;&#25551;&#36848;&#25991;&#26723;&#12289;&#21333;&#35789;&#21644;&#27010;&#24565;&#20043;&#38388;&#20851;&#31995;&#30340;&#24322;&#36136;&#25991;&#26412;&#22270;&#65292;&#24182;&#36890;&#36807;&#32479;&#19968;&#21307;&#23398;&#35821;&#35328;&#31995;&#32479;&#20013;&#30340;&#21307;&#23398;&#30693;&#35782;&#22686;&#24378;&#23427;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#24863;&#30693;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#20026;&#22270;&#20013;&#19981;&#21516;&#31867;&#22411;&#30340;&#33410;&#28857;&#19981;&#21516;&#22320;&#23398;&#20064;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#21270;&#23884;&#20837;&#21644;&#21367;&#31215;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#29305;&#24449;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.10451v2 Announce Type: replace  Abstract: Adverse drug events (ADEs) are an important aspect of drug safety. Various texts such as biomedical literature, drug reviews, and user posts on social media and medical forums contain a wealth of information about ADEs. Recent studies have applied word embedding and deep learning -based natural language processing to automate ADE detection from text. However, they did not explore incorporating explicit medical knowledge about drugs and adverse reactions or the corresponding feature learning. This paper adopts the heterogenous text graph which describes relationships between documents, words and concepts, augments it with medical knowledge from the Unified Medical Language System, and proposes a concept-aware attention mechanism which learns features differently for the different types of nodes in the graph. We further utilize contextualized embeddings from pretrained language models and convolutional graph neural networks for effecti
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24418;&#24335;&#35821;&#35328;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#21151;&#33021;&#35821;&#35328;&#33021;&#21147;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#31283;&#23450;&#65292;&#21487;&#33021;&#38656;&#35201;&#19987;&#38376;&#30340;&#35843;&#25972;&#21644;&#22806;&#37096;&#27169;&#22359;&#30340;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2301.06627</link><description>&lt;p&gt;
&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#21306;&#20998;&#35821;&#35328;&#21644;&#24605;&#32500;
&lt;/p&gt;
&lt;p&gt;
Dissociating language and thought in large language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.06627
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24418;&#24335;&#35821;&#35328;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#21151;&#33021;&#35821;&#35328;&#33021;&#21147;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#31283;&#23450;&#65292;&#21487;&#33021;&#38656;&#35201;&#19987;&#38376;&#30340;&#35843;&#25972;&#21644;&#22806;&#37096;&#27169;&#22359;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36804;&#20170;&#20026;&#27490;&#22312;&#25484;&#25569;&#20154;&#31867;&#35821;&#35328;&#26041;&#38754;&#20570;&#24471;&#26368;&#22909;&#65292;&#28982;&#32780;&#20154;&#20204;&#23545;&#23427;&#20204;&#30340;&#35821;&#35328;&#21644;&#35748;&#30693;&#33021;&#21147;&#20173;&#23384;&#22312;&#20998;&#27495;&#12290;&#26412;&#25991;&#20351;&#29992;&#24418;&#24335;&#35821;&#35328;&#33021;&#21147;&#65288;&#23545;&#35821;&#35328;&#35268;&#21017;&#21644;&#27169;&#24335;&#30340;&#20102;&#35299;&#65289;&#19982;&#21151;&#33021;&#35821;&#35328;&#33021;&#21147;&#65288;&#29702;&#35299;&#21644;&#20351;&#29992;&#35821;&#35328;&#22312;&#19990;&#30028;&#20013;&#30340;&#26041;&#24335;&#65289;&#30340;&#21306;&#21035;&#26469;&#35780;&#20272;LLMs&#12290;&#25105;&#20204;&#36890;&#36807;&#20154;&#31867;&#31070;&#32463;&#31185;&#23398;&#26469;&#30830;&#31435;&#36825;&#19968;&#21306;&#21035;&#65292;&#20154;&#31867;&#31070;&#32463;&#31185;&#23398;&#26174;&#31034;&#24418;&#24335;&#21644;&#21151;&#33021;&#33021;&#21147;&#20381;&#36182;&#20110;&#19981;&#21516;&#30340;&#31070;&#32463;&#26426;&#21046;&#12290;&#23613;&#31649;LLMs&#22312;&#24418;&#24335;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#20154;&#20204;&#30340;&#24778;&#20154;&#27700;&#24179;&#65292;&#20294;&#23427;&#20204;&#22312;&#21151;&#33021;&#33021;&#21147;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20173;&#28982;&#19981;&#31283;&#23450;&#65292;&#24182;&#19988;&#36890;&#24120;&#38656;&#35201;&#19987;&#38376;&#30340;&#31934;&#32454;&#35843;&#25972;&#21644;/&#25110;&#19982;&#22806;&#37096;&#27169;&#22359;&#30340;&#32806;&#21512;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#37027;&#20123;&#20197;&#31867;&#20284;&#20154;&#31867;&#26041;&#24335;&#20351;&#29992;&#35821;&#35328;&#30340;&#27169;&#22411;&#23558;&#38656;&#35201;&#25484;&#25569;&#36825;&#20004;&#31181;&#33021;&#21147;&#31867;&#22411;&#65292;&#32780;&#36825;&#21453;&#36807;&#26469;&#21487;&#33021;&#38656;&#35201;&#20026;&#24418;&#24335;&#35821;&#35328;&#33021;&#21147;&#19987;&#38376;&#21270;&#30340;&#26426;&#21046;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.06627v3 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have come closest among all models to date to mastering human language, yet opinions about their linguistic and cognitive capabilities remain split. Here, we evaluate LLMs using a distinction between formal linguistic competence - knowledge of linguistic rules and patterns - and functional linguistic competence - understanding and using language in the world. We ground this distinction in human neuroscience, which has shown that formal and functional competence rely on different neural mechanisms. Although LLMs are surprisingly good at formal competence, their performance on functional competence tasks remains spotty and often requires specialized fine-tuning and/or coupling with external modules. We posit that models that use language in human-like ways would need to master both of these competence types, which, in turn, could require the emergence of mechanisms specialized for formal linguistic co
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20351;&#29992;&#38134;&#26631;&#20934;&#25968;&#25454;&#36827;&#34892;&#38646;-shot&#20851;&#31995;&#25552;&#21462;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#31867;&#21035;&#30340;&#28165;&#27905;&#25968;&#25454;&#26816;&#27979;&#27169;&#22359;&#12290;</title><link>https://arxiv.org/abs/2211.13883</link><description>&lt;p&gt;
&#20351;&#29992;&#38134;&#26631;&#20934;&#25968;&#25454;&#36827;&#34892;&#38646;-shot&#20851;&#31995;&#25552;&#21462;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning with Silver Standard Data for Zero-shot Relation Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.13883
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20351;&#29992;&#38134;&#26631;&#20934;&#25968;&#25454;&#36827;&#34892;&#38646;-shot&#20851;&#31995;&#25552;&#21462;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#31867;&#21035;&#30340;&#28165;&#27905;&#25968;&#25454;&#26816;&#27979;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#20851;&#31995;&#25552;&#21462;&#65288;RE&#65289;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#40644;&#37329;&#26631;&#20934;&#25968;&#25454;&#12290;&#26368;&#36817;&#38646;-shot&#20851;&#31995;&#25552;&#21462;&#26041;&#27861;&#23558;RE&#20219;&#21153;&#36716;&#21270;&#20026;&#20854;&#20182;NLP&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;NLP&#20219;&#21153;&#30340;&#29616;&#25104;&#27169;&#22411;&#30452;&#25509;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#25191;&#34892;&#25512;&#29702;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#22823;&#37327;RE&#27880;&#37322;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#28508;&#22312;&#26377;&#20215;&#20540;&#30340;&#21103;&#20135;&#21697;&#26159;&#22823;&#35268;&#27169;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#36827;&#19968;&#27493;&#30740;&#31350;&#22914;&#20309;&#20351;&#29992;&#36825;&#20123;&#28508;&#22312;&#26377;&#20215;&#20540;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#39318;&#20808;&#20174;&#38134;&#26631;&#20934;&#25968;&#25454;&#20013;&#26816;&#27979;&#21040;&#23569;&#37327;&#28165;&#27905;&#25968;&#25454;&#65292;&#28982;&#21518;&#20351;&#29992;&#36873;&#23450;&#30340;&#28165;&#27905;&#25968;&#25454;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#21518;&#20351;&#29992;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#25512;&#26029;&#20851;&#31995;&#31867;&#22411;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31867;&#21035;&#30340;&#28165;&#27905;&#25968;&#25454;&#26816;&#27979;&#27169;&#22359;&#65292;&#22312;&#36873;&#25321;&#28165;&#27905;&#25968;&#25454;&#26102;&#32771;&#34385;&#31867;&#21035;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20987;&#36133;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.13883v2 Announce Type: replace  Abstract: The superior performance of supervised relation extraction (RE) methods heavily relies on a large amount of gold standard data. Recent zero-shot relation extraction methods converted the RE task to other NLP tasks and used off-the-shelf models of these NLP tasks to directly perform inference on the test data without using a large amount of RE annotation data. A potentially valuable by-product of these methods is the large-scale silver standard data. However, there is no further investigation on the use of potentially valuable silver standard data. In this paper, we propose to first detect a small amount of clean data from silver standard data and then use the selected clean data to finetune the pretrained model. We then use the finetuned model to infer relation types. We also propose a class-aware clean data detection module to consider class information when selecting clean data. The experimental results show that our method can out
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#22810;&#35821;&#35328;&#25991;&#26723;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#25991;&#26723;&#23884;&#20837;&#30340;&#39640;&#26031;&#20998;&#24067;&#24418;&#24335;&#26469;&#32534;&#30721;&#19981;&#30830;&#23450;&#24615;&#65292;&#21033;&#29992;&#23398;&#21040;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#20027;&#39064;&#35782;&#21035;&#65292;&#22312;17&#31181;&#19981;&#21516;&#35821;&#35328;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#23454;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#36164;&#28304;&#35821;&#35328;&#19978;&#34920;&#29616;&#20986;&#33394;</title><link>https://arxiv.org/abs/2007.01359</link><description>&lt;p&gt;
&#19968;&#31181;&#36125;&#21494;&#26031;&#22810;&#35821;&#35328;&#25991;&#26723;&#27169;&#22411;&#29992;&#20110;&#38646;&#26679;&#26412;&#20027;&#39064;&#35782;&#21035;&#19982;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Multilingual Document Model for Zero-shot Topic Identification and Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2007.01359
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#22810;&#35821;&#35328;&#25991;&#26723;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#25991;&#26723;&#23884;&#20837;&#30340;&#39640;&#26031;&#20998;&#24067;&#24418;&#24335;&#26469;&#32534;&#30721;&#19981;&#30830;&#23450;&#24615;&#65292;&#21033;&#29992;&#23398;&#21040;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#20027;&#39064;&#35782;&#21035;&#65292;&#22312;17&#31181;&#19981;&#21516;&#35821;&#35328;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#23454;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#36164;&#28304;&#35821;&#35328;&#19978;&#34920;&#29616;&#20986;&#33394;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#19982;&#35821;&#35328;&#26080;&#20851;&#30340;&#25991;&#26723;&#23884;&#20837;&#30340;&#36125;&#21494;&#26031;&#22810;&#35821;&#35328;&#25991;&#26723;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#26159;BaySMM[Kesiraju et al 2020]&#22312;&#22810;&#35821;&#35328;&#22330;&#26223;&#20013;&#30340;&#25193;&#23637;&#12290;&#23427;&#23398;&#20064;&#20197;&#39640;&#26031;&#20998;&#24067;&#30340;&#24418;&#24335;&#34920;&#31034;&#25991;&#26723;&#23884;&#20837;&#65292;&#20174;&#32780;&#32534;&#30721;&#21327;&#26041;&#24046;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#32447;&#24615;&#20998;&#31867;&#22120;&#20256;&#25773;&#23398;&#21040;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#26377;&#21033;&#20110;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#20027;&#39064;&#35782;&#21035;&#12290;&#25105;&#20204;&#22312;17&#31181;&#35821;&#35328;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#22810;&#35821;&#35328;&#36125;&#21494;&#26031;&#25991;&#26723;&#27169;&#22411;&#22312;&#19982;&#22522;&#20110;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#30340;&#20854;&#20182;&#31995;&#32479;&#65288;&#22914;LASER&#12289;XLM-R&#12289;mUSE&#65289;&#30456;&#27604;&#65292;&#23545;8&#31181;&#39640;&#36164;&#28304;&#35821;&#35328;&#34920;&#29616;&#31454;&#20105;&#21147;&#24378;&#65292;&#24182;&#22312;9&#31181;&#20013;&#36164;&#28304;&#35821;&#35328;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;&#25105;&#20204;&#36890;&#36807;&#26356;&#28145;&#20837;&#22320;&#30740;&#31350;&#24403;&#21069;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#31995;&#32479;&#21644;&#28085;&#30422;&#30340;&#35821;&#35328;&#65292;&#37325;&#26032;&#23457;&#35270;&#20102;&#38646;&#26679;&#26412;&#35774;&#23450;&#20013;&#30340;&#36328;&#35821;&#35328;&#20027;&#39064;&#35782;&#21035;&#12290;&#25105;&#20204;&#25351;&#20986;&#20102;&#20854;&#20013;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2007.01359v3 Announce Type: replace  Abstract: In this paper, we present a Bayesian multilingual document model for learning language-independent document embeddings. The model is an extension of BaySMM [Kesiraju et al 2020] to the multilingual scenario. It learns to represent the document embeddings in the form of Gaussian distributions, thereby encoding the uncertainty in its covariance. We propagate the learned uncertainties through linear classifiers that benefit zero-shot cross-lingual topic identification. Our experiments on 17 languages show that the proposed multilingual Bayesian document model performs competitively, when compared to other systems based on large-scale neural networks (LASER, XLM-R, mUSE) on 8 high-resource languages, and outperforms these systems on 9 mid-resource languages. We revisit cross-lingual topic identification in zero-shot settings by taking a deeper dive into current datasets, baseline systems and the languages covered. We identify shortcoming
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#19968;&#31181;&#21517;&#20026;&#26816;&#32034;&#31034;&#33539;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#20110;&#36755;&#20837;&#26597;&#35810;&#30340;&#31034;&#33539;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#37327;&#26679;&#26412;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#36824;&#20943;&#23569;&#20102;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2401.11624</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#31034;&#33539;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
In-context Learning with Retrieved Demonstrations for Language Models: A Survey. (arXiv:2401.11624v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#19968;&#31181;&#21517;&#20026;&#26816;&#32034;&#31034;&#33539;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#20110;&#36755;&#20837;&#26597;&#35810;&#30340;&#31034;&#33539;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#37327;&#26679;&#26412;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#36824;&#20943;&#23569;&#20102;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24050;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#36755;&#20837;&#19978;&#19979;&#25991;&#20013;&#36827;&#34892;&#23569;&#37327;&#26679;&#26412;&#30340;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#24182;&#22312;&#26032;&#20219;&#21153;&#19978;&#20855;&#26377;&#36866;&#24212;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;ICL&#33021;&#21147;&#23545;&#20110;&#23569;&#26679;&#26412;&#31034;&#33539;&#30340;&#36873;&#25321;&#26159;&#25935;&#24863;&#30340;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#36827;&#23637;&#26159;&#26816;&#32034;&#38024;&#23545;&#27599;&#20010;&#36755;&#20837;&#26597;&#35810;&#23450;&#21046;&#30340;&#31034;&#33539;&#12290;&#31034;&#33539;&#26816;&#32034;&#30340;&#23454;&#29616;&#30456;&#23545;&#31616;&#21333;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#25968;&#25454;&#24211;&#21644;&#26816;&#32034;&#31995;&#32479;&#12290;&#36825;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#32780;&#19988;&#24050;&#32463;&#35777;&#26126;&#21487;&#20197;&#20943;&#23569;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;&#37492;&#20110;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#21644;&#22312;&#26816;&#32034;&#31034;&#33539;&#30340;ICL&#26041;&#38754;&#19981;&#26029;&#22686;&#38271;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#32508;&#36848;&#12290;&#22312;&#36825;&#39033;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#21644;&#27604;&#36739;&#20102;&#26816;&#32034;&#27169;&#22411;&#30340;&#19981;&#21516;&#35774;&#35745;&#36873;&#25321;&#65292;&#26816;&#32034;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Language models, especially pre-trained large language models, have showcased remarkable abilities as few-shot in-context learners (ICL), adept at adapting to new tasks with just a few demonstrations in the input context. However, the model's ability to perform ICL is sensitive to the choice of the few-shot demonstrations. Instead of using a fixed set of demonstrations, one recent development is to retrieve demonstrations tailored to each input query. The implementation of demonstration retrieval is relatively straightforward, leveraging existing databases and retrieval systems. This not only improves the efficiency and scalability of the learning process but also has been shown to reduce biases inherent in manual example selection. In light of the encouraging results and growing research in ICL with retrieved demonstrations, we conduct an extensive review of studies in this area. In this survey, we discuss and compare different design choices for retrieval models, retrieval training p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#32534;&#36753;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#26434;&#21518;&#26524;&#65292;&#21457;&#29616;&#22312;&#22686;&#24378;&#27169;&#22411;&#20934;&#30830;&#24615;&#19982;&#20445;&#25345;&#36947;&#24503;&#23436;&#25972;&#24615;&#20043;&#38388;&#23384;&#22312;&#24726;&#35770;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#27880;&#20837;&#20934;&#30830;&#20449;&#24687;&#23545;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#24456;&#37325;&#35201;&#65292;&#20294;&#23427;&#21487;&#33021;&#30772;&#22351;&#27169;&#22411;&#30340;&#22522;&#26412;&#26694;&#26550;&#65292;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#21644;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2401.10647</link><description>&lt;p&gt;
&#25773;&#39118;&#25769;&#36215;&#39118;&#26292;&#65306;&#32534;&#36753;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language Models. (arXiv:2401.10647v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#32534;&#36753;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#26434;&#21518;&#26524;&#65292;&#21457;&#29616;&#22312;&#22686;&#24378;&#27169;&#22411;&#20934;&#30830;&#24615;&#19982;&#20445;&#25345;&#36947;&#24503;&#23436;&#25972;&#24615;&#20043;&#38388;&#23384;&#22312;&#24726;&#35770;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#27880;&#20837;&#20934;&#30830;&#20449;&#24687;&#23545;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#24456;&#37325;&#35201;&#65292;&#20294;&#23427;&#21487;&#33021;&#30772;&#22351;&#27169;&#22411;&#30340;&#22522;&#26412;&#26694;&#26550;&#65292;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#21644;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#65292;&#32418;&#38431;&#27979;&#35797;&#25110;&#36234;&#29425;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#27010;&#24565;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#36827;&#34892;&#32534;&#36753;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#20462;&#25913;&#30340;&#22797;&#26434;&#21518;&#26524;&#65292;&#21457;&#29616;&#20102;&#22686;&#24378;&#27169;&#22411;&#20934;&#30830;&#24615;&#19982;&#20445;&#25345;&#20854;&#36947;&#24503;&#23436;&#25972;&#24615;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#28145;&#20837;&#20998;&#26512;&#25581;&#31034;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#24726;&#35770;&#65306;&#34429;&#28982;&#27880;&#20837;&#20934;&#30830;&#20449;&#24687;&#23545;&#20110;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23427;&#21364;&#21487;&#33021;&#30772;&#22351;&#27169;&#22411;&#30340;&#22522;&#26412;&#26694;&#26550;&#65292;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#21644;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;NicheHazardQA&#65292;&#29992;&#20110;&#30740;&#31350;&#27169;&#22411;&#22312;&#30456;&#21516;&#21644;&#36328;&#39046;&#22495;&#20013;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;&#36825;&#19968;&#26041;&#38754;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#32534;&#36753;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#30340;&#23433;&#20840;&#24230;&#37327;&#21644;&#20445;&#25252;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly advancing field of artificial intelligence, the concept of Red-Teaming or Jailbreaking large language models (LLMs) has emerged as a crucial area of study. This approach is especially significant in terms of assessing and enhancing the safety and robustness of these models. This paper investigates the intricate consequences of such modifications through model editing, uncovering a complex relationship between enhancing model accuracy and preserving its ethical integrity. Our in-depth analysis reveals a striking paradox: while injecting accurate information is crucial for model reliability, it can paradoxically destabilize the model's foundational framework, resulting in unpredictable and potentially unsafe behaviors. Additionally, we propose a benchmark dataset NicheHazardQA to investigate this unsafe behavior both within the same and cross topical domain. This aspect of our research sheds light on how the edits, impact the model's safety metrics and guardrails. Our find
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#20195;&#30721;&#30340;&#23646;&#24615;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#20043;&#38388;&#30340;&#29420;&#29305;&#27169;&#24335;&#65292;&#23588;&#20854;&#26159;&#32467;&#26500;&#20998;&#21106;&#23545;&#20110;&#35782;&#21035;&#20195;&#30721;&#26469;&#28304;&#24456;&#20851;&#38190;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.06461</link><description>&lt;p&gt;
&#20195;&#30721;&#20043;&#38388;&#30340;&#30028;&#38480;&#65306;&#25581;&#31034;&#26426;&#22120;&#21644;&#20154;&#31867;&#31243;&#24207;&#21592;&#20043;&#38388;&#19981;&#21516;&#30340;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Between Lines of Code: Unraveling the Distinct Patterns of Machine and Human Programmers. (arXiv:2401.06461v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#20195;&#30721;&#30340;&#23646;&#24615;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#20043;&#38388;&#30340;&#29420;&#29305;&#27169;&#24335;&#65292;&#23588;&#20854;&#26159;&#32467;&#26500;&#20998;&#21106;&#23545;&#20110;&#35782;&#21035;&#20195;&#30721;&#26469;&#28304;&#24456;&#20851;&#38190;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#27169;&#31946;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#28304;&#20195;&#30721;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#23548;&#33268;&#36719;&#20214;&#20135;&#29289;&#30340;&#23436;&#25972;&#24615;&#21644;&#30495;&#23454;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#20195;&#30721;&#38271;&#24230;&#12289;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#33258;&#28982;&#24615;&#31561;&#23646;&#24615;&#30340;&#20005;&#26684;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#22266;&#26377;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#29305;&#21035;&#27880;&#24847;&#21040;&#65292;&#20195;&#30721;&#30340;&#32467;&#26500;&#20998;&#21106;&#26159;&#35782;&#21035;&#20854;&#26469;&#28304;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#22411;&#26426;&#22120;&#29983;&#25104;&#20195;&#30721;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25913;&#36827;&#20102;DetectGPT&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have catalyzed an unprecedented wave in code generation. While achieving significant advances, they blur the distinctions between machine-and human-authored source code, causing integrity and authenticity issues of software artifacts. Previous methods such as DetectGPT have proven effective in discerning machine-generated texts, but they do not identify and harness the unique patterns of machine-generated code. Thus, its applicability falters when applied to code. In this paper, we carefully study the specific patterns that characterize machine and human-authored code. Through a rigorous analysis of code attributes such as length, lexical diversity, and naturalness, we expose unique pat-terns inherent to each source. We particularly notice that the structural segmentation of code is a critical factor in identifying its provenance. Based on our findings, we propose a novel machine-generated code detection method called DetectCodeGPT, which improves DetectGPT by cap
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#65292;&#21457;&#29616;&#26410;&#23545;&#40784;&#21644;&#23545;&#40784;&#30340;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#22312;&#35821;&#20041;&#19978;&#26159;&#30456;&#20284;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#21305;&#37197;&#26410;&#23545;&#40784;&#32534;&#30721;&#22120;&#65292;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#23454;&#29616;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2401.05224</link><description>&lt;p&gt;
&#35270;&#35273;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#26159;&#21542;&#20197;&#30456;&#20284;&#26041;&#24335;&#34920;&#31034;&#19990;&#30028;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Vision and Language Encoders Represent the World Similarly?. (arXiv:2401.05224v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05224
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#65292;&#21457;&#29616;&#26410;&#23545;&#40784;&#21644;&#23545;&#40784;&#30340;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#22312;&#35821;&#20041;&#19978;&#26159;&#30456;&#20284;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#21305;&#37197;&#26410;&#23545;&#40784;&#32534;&#30721;&#22120;&#65292;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#23454;&#29616;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#25104;&#20026;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#20107;&#23454;&#19978;&#30340;&#27169;&#22411;&#30340;&#23545;&#40784;&#30340;&#25991;&#26412;-&#22270;&#20687;&#32534;&#30721;&#22120;&#65288;&#22914;CLIP&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#27169;&#24577;&#29305;&#23450;&#30340;&#32534;&#30721;&#22120;&#22312;&#21508;&#33258;&#39046;&#22495;&#20013;&#20063;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65306;&#30001;&#20110;&#23427;&#20204;&#22522;&#26412;&#19978;&#34920;&#31034;&#21516;&#19968;&#20010;&#29289;&#29702;&#19990;&#30028;&#65292;&#21333;&#27169;&#24577;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#20043;&#38388;&#26159;&#21542;&#23384;&#22312;&#23545;&#40784;&#65311;&#36890;&#36807;&#20351;&#29992;&#20013;&#24515;&#26680;&#23545;&#40784;&#65288;CKA&#65289;&#20998;&#26512;&#22270;&#20687;-&#26631;&#39064;&#22522;&#20934;&#19978;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#65292;&#25105;&#20204;&#21457;&#29616;&#26410;&#23545;&#40784;&#21644;&#23545;&#40784;&#30340;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#22312;&#35821;&#20041;&#19978;&#26159;&#30456;&#20284;&#30340;&#12290;&#22312;&#20687;CLIP&#36825;&#26679;&#30340;&#23545;&#40784;&#32534;&#30721;&#22120;&#20013;&#32570;&#20047;&#32479;&#35745;&#30456;&#20284;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#26174;&#31034;&#20102;&#21487;&#33021;&#23384;&#22312;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#30340;&#26410;&#23545;&#40784;&#32534;&#30721;&#22120;&#30340;&#21305;&#37197;&#12290;&#25105;&#20204;&#23558;&#36825;&#35270;&#20026;&#21033;&#29992;&#22270;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26377;&#31181;&#23376;&#22270;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861; - &#24555;&#36895;&#20108;&#27425;&#20998;&#37197;&#38382;&#39064;&#20248;&#21270;&#21644;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#30340;&#23616;&#37096;CKA&#24230;&#37327;&#30340;&#21305;&#37197;/&#26816;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligned text-image encoders such as CLIP have become the de facto model for vision-language tasks. Furthermore, modality-specific encoders achieve impressive performances in their respective domains. This raises a central question: does an alignment exist between uni-modal vision and language encoders since they fundamentally represent the same physical world? Analyzing the latent spaces structure of vision and language models on image-caption benchmarks using the Centered Kernel Alignment (CKA), we find that the representation spaces of unaligned and aligned encoders are semantically similar. In the absence of statistical similarity in aligned encoders like CLIP, we show that a possible matching of unaligned encoders exists without any training. We frame this as a seeded graph-matching problem exploiting the semantic similarity between graphs and propose two methods - a Fast Quadratic Assignment Problem optimization, and a novel localized CKA metric-based matching/retrieval. We demons
&lt;/p&gt;</description></item><item><title>VQPy&#26159;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#35270;&#39057;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Python&#21464;&#20307;&#20316;&#20026;&#21069;&#31471;&#65292;&#24182;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#21644;&#20248;&#21270;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#30340;&#22788;&#29702;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2311.01623</link><description>&lt;p&gt;
VQPy&#65306;&#19968;&#31181;&#38754;&#21521;&#29616;&#20195;&#35270;&#39057;&#20998;&#26512;&#30340;&#38754;&#21521;&#23545;&#35937;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
VQPy: An Object-Oriented Approach to Modern Video Analytics. (arXiv:2311.01623v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01623
&lt;/p&gt;
&lt;p&gt;
VQPy&#26159;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#35270;&#39057;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Python&#21464;&#20307;&#20316;&#20026;&#21069;&#31471;&#65292;&#24182;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#21644;&#20248;&#21270;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#30340;&#22788;&#29702;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#20998;&#26512;&#24191;&#27867;&#24212;&#29992;&#20110;&#24403;&#20170;&#31995;&#32479;&#21644;&#26381;&#21153;&#20013;&#12290;&#22312;&#35270;&#39057;&#20998;&#26512;&#30340;&#21069;&#27839;&#26159;&#29992;&#25143;&#24320;&#21457;&#30340;&#35270;&#39057;&#26597;&#35810;&#65292;&#20197;&#25214;&#21040;&#29305;&#23450;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#12290;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#65288;&#20363;&#22914;&#20154;&#65292;&#21160;&#29289;&#65292;&#27773;&#36710;&#31561;&#65289;&#19982;&#20256;&#32479;&#38754;&#21521;&#23545;&#35937;&#35821;&#35328;&#24314;&#27169;&#30340;&#23545;&#35937;&#30456;&#20284;&#30340;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35270;&#39057;&#20998;&#26512;&#30340;&#38754;&#21521;&#23545;&#35937;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21517;&#20026;VQPy&#65292;&#21253;&#25324;&#19968;&#20010;&#21069;&#31471;&#65288;&#19968;&#31181;Python&#21464;&#20307;&#65292;&#20854;&#20013;&#21253;&#21547;&#29992;&#25143;&#21487;&#20197;&#34920;&#36798;&#35270;&#39057;&#23545;&#35937;&#21450;&#20854;&#20132;&#20114;&#30340;&#32467;&#26500;&#65289;&#21644;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#33258;&#21160;&#29983;&#25104;&#21644;&#20248;&#21270;&#31649;&#36947;&#12290;&#25105;&#20204;&#24050;&#32463;&#23454;&#26045;&#21644;&#24320;&#28304;&#20102;VQPy&#65292;&#23427;&#24050;&#32463;&#20316;&#20026;Cisco DeepVision&#26694;&#26550;&#30340;&#19968;&#37096;&#20998;&#20135;&#21697;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video analytics is widely used in contemporary systems and services. At the forefront of video analytics are video queries that users develop to find objects of particular interest. Building upon the insight that video objects (e.g., human, animals, cars, etc.), the center of video analytics, are similar in spirit to objects modeled by traditional object-oriented languages, we propose to develop an object-oriented approach to video analytics. This approach, named VQPy, consists of a frontend$\unicode{x2015}$a Python variant with constructs that make it easy for users to express video objects and their interactions$\unicode{x2015}$as well as an extensible backend that can automatically construct and optimize pipelines based on video objects. We have implemented and open-sourced VQPy, which has been productized in Cisco as part of its DeepVision framework.
&lt;/p&gt;</description></item><item><title>CLEX&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#38271;&#24230;&#22806;&#25512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20301;&#32622;&#23884;&#20837;&#32553;&#25918;&#26041;&#27861;&#25512;&#24191;&#21040;&#36830;&#32493;&#21160;&#24577;&#24314;&#27169;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#26041;&#27861;&#22312;&#29305;&#23450;&#38271;&#24230;&#19978;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16450</link><description>&lt;p&gt;
CLEX: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#38271;&#24230;&#22806;&#25512;
&lt;/p&gt;
&lt;p&gt;
CLEX: Continuous Length Extrapolation for Large Language Models. (arXiv:2310.16450v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16450
&lt;/p&gt;
&lt;p&gt;
CLEX&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#38271;&#24230;&#22806;&#25512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20301;&#32622;&#23884;&#20837;&#32553;&#25918;&#26041;&#27861;&#25512;&#24191;&#21040;&#36830;&#32493;&#21160;&#24577;&#24314;&#27169;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#26041;&#27861;&#22312;&#29305;&#23450;&#38271;&#24230;&#19978;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#36827;&#23637;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21331;&#36234;&#33021;&#21147;&#21463;&#38480;&#20110;Transformer&#30340;&#39044;&#35774;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;&#20301;&#32622;&#23884;&#20837;&#65288;PE&#65289;&#32553;&#25918;&#26041;&#27861;&#34429;&#28982;&#33021;&#22815;&#23558;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;&#29305;&#23450;&#38271;&#24230;&#65292;&#20294;&#22312;&#22806;&#25512;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#30340;&#23616;&#38480;&#24615;&#65292;&#25110;&#32773;&#22312;&#19978;&#19979;&#25991;&#31383;&#21475;&#20869;&#29306;&#29298;&#37096;&#20998;&#24615;&#33021;&#12290;&#34429;&#28982;&#38271;&#24230;&#22806;&#25512;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#33021;&#22815;&#23558;&#19978;&#19979;&#25991;&#31383;&#21475;&#24310;&#38271;&#33267;&#35757;&#32451;&#24207;&#21015;&#38271;&#24230;&#20043;&#22806;&#65292;&#20294;&#22312;&#23454;&#38469;&#30340;&#38271;&#19978;&#19979;&#25991;&#24212;&#29992;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;LLMs&#30340;&#25345;&#32493;&#38271;&#24230;&#22806;&#25512;&#65288;CLEX&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;PE&#32553;&#25918;&#26041;&#27861;&#25512;&#24191;&#21040;&#36890;&#36807;&#24120;&#24494;&#20998;&#26041;&#31243;&#23545;&#38271;&#24230;&#32553;&#25918;&#22240;&#23376;&#24314;&#27169;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#24403;&#21069;&#20026;&#29305;&#23450;&#38271;&#24230;&#35774;&#35745;&#30340;PE&#32553;&#25918;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Large Language Models (LLMs) are pioneering advances in many natural language processing tasks, however, their exceptional capabilities are restricted within the preset context window of Transformer. Position Embedding (PE) scaling methods, while effective in extending the context window to a specific length, demonstrate either notable limitations in their extrapolation abilities or sacrificing partial performance within the context window. Length extrapolation methods, although theoretically capable of extending the context window beyond the training sequence length, often underperform in practical long-context applications. To address these challenges, we propose Continuous Length EXtrapolation (CLEX) for LLMs. We generalise the PE scaling approaches to model the continuous dynamics by ordinary differential equations over the length scaling factor, thereby overcoming the constraints of current PE scaling methods designed for specific lengths. Moreover, by extending 
&lt;/p&gt;</description></item><item><title>SOTOPIA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#26234;&#33021;&#20013;&#30340;&#31038;&#20132;&#26234;&#33021;&#30340;&#20132;&#20114;&#24335;&#29615;&#22659;&#12290;&#36890;&#36807;&#27169;&#25311;&#22797;&#26434;&#30340;&#31038;&#20132;&#20114;&#21160;&#65292;&#24182;&#20351;&#29992;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#31038;&#20132;&#26234;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#29305;&#21035;&#26159;&#22312;SOTOPIA-hard&#24773;&#26223;&#19979;&#12290;GPT-4&#22312;&#36825;&#20010;&#23376;&#38598;&#19978;&#30340;&#30446;&#26631;&#23436;&#25104;&#29575;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2310.11667</link><description>&lt;p&gt;
SOTOPIA: &#20132;&#20114;&#24335;&#35780;&#20272;&#35821;&#35328;&#26234;&#33021;&#20013;&#30340;&#31038;&#20132;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents. (arXiv:2310.11667v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11667
&lt;/p&gt;
&lt;p&gt;
SOTOPIA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#26234;&#33021;&#20013;&#30340;&#31038;&#20132;&#26234;&#33021;&#30340;&#20132;&#20114;&#24335;&#29615;&#22659;&#12290;&#36890;&#36807;&#27169;&#25311;&#22797;&#26434;&#30340;&#31038;&#20132;&#20114;&#21160;&#65292;&#24182;&#20351;&#29992;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#31038;&#20132;&#26234;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#29305;&#21035;&#26159;&#22312;SOTOPIA-hard&#24773;&#26223;&#19979;&#12290;GPT-4&#22312;&#36825;&#20010;&#23376;&#38598;&#19978;&#30340;&#30446;&#26631;&#23436;&#25104;&#29575;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26159;&#31038;&#20132;&#30340;&#23384;&#22312;&#65307;&#25105;&#20204;&#22312;&#26085;&#24120;&#20114;&#21160;&#20013;&#36861;&#27714;&#31038;&#20132;&#30446;&#26631;&#65292;&#36825;&#26159;&#31038;&#20132;&#26234;&#33021;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#33021;&#21147;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SOTOPIA&#65292;&#19968;&#20010;&#24320;&#25918;&#24335;&#29615;&#22659;&#65292;&#29992;&#20110;&#27169;&#25311;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20043;&#38388;&#30340;&#22797;&#26434;&#31038;&#20132;&#20114;&#21160;&#24182;&#35780;&#20272;&#23427;&#20204;&#30340;&#31038;&#20132;&#26234;&#33021;&#12290;&#22312;&#25105;&#20204;&#30340;&#29615;&#22659;&#20013;&#65292;&#20195;&#29702;&#20154;&#25198;&#28436;&#35282;&#33394;&#65292;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#30456;&#20114;&#21327;&#20316;&#12289;&#21512;&#20316;&#12289;&#20132;&#27969;&#21644;&#31454;&#20105;&#65292;&#20197;&#23454;&#29616;&#22797;&#26434;&#30340;&#31038;&#20132;&#30446;&#26631;&#12290;&#25105;&#20204;&#27169;&#25311;&#20102;LLM-based&#20195;&#29702;&#20154;&#19982;&#20154;&#31867;&#20043;&#38388;&#22312;&#36825;&#20010;&#20219;&#21153;&#31354;&#38388;&#20869;&#30340;&#35282;&#33394;&#25198;&#28436;&#20114;&#21160;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21517;&#20026;SOTOPIA-Eval&#30340;&#25972;&#20307;&#35780;&#20272;&#26694;&#26550;&#23545;&#23427;&#20204;&#30340;&#34920;&#29616;&#36827;&#34892;&#35780;&#20272;&#12290;&#36890;&#36807;SOTOPIA&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#31038;&#20132;&#26234;&#33021;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#30830;&#23450;&#20102;SOTOPIA&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#21363;SOTOPIA-hard&#65292;&#23545;&#25152;&#26377;&#27169;&#22411;&#26469;&#35828;&#37117;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#36825;&#20010;&#23376;&#38598;&#19978;&#65292;GPT-4&#30340;&#30446;&#26631;&#23436;&#25104;&#29575;&#26174;&#33879;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completio
&lt;/p&gt;</description></item><item><title>KGQuiz&#26159;&#19968;&#20010;&#30693;&#35782;&#23494;&#38598;&#22411;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#28085;&#30422;&#19977;&#20010;&#30693;&#35782;&#39046;&#22495;&#21644;&#20116;&#20010;&#20219;&#21153;&#65292;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#30693;&#35782;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.09725</link><description>&lt;p&gt;
KGQuiz: &#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#30721;&#30693;&#35782;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
KGQuiz: Evaluating the Generalization of Encoded Knowledge in Large Language Models. (arXiv:2310.09725v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09725
&lt;/p&gt;
&lt;p&gt;
KGQuiz&#26159;&#19968;&#20010;&#30693;&#35782;&#23494;&#38598;&#22411;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#28085;&#30422;&#19977;&#20010;&#30693;&#35782;&#39046;&#22495;&#21644;&#20116;&#20010;&#20219;&#21153;&#65292;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#30693;&#35782;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#34920;&#26126;&#30495;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#34987;&#32534;&#30721;&#22312;&#23427;&#20204;&#30340;&#27169;&#22411;&#21442;&#25968;&#20013;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#22312;&#26377;&#38480;&#30340;&#30693;&#35782;&#39046;&#22495;&#19978;&#36827;&#34892;&#19968;&#20123;&#25506;&#32034;&#24615;&#20219;&#21153;&#20043;&#22806;&#65292;&#25105;&#20204;&#23545;&#20110;&#22914;&#20309;&#31995;&#32479;&#35780;&#20272;LLMs&#30340;&#30693;&#35782;&#33021;&#21147;&#20197;&#21450;&#23427;&#20204;&#30340;&#30693;&#35782;&#33021;&#21147;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#36880;&#28176;&#22797;&#26434;&#30340;&#20219;&#21153;&#26684;&#24335;&#20013;&#30340;&#27867;&#21270;&#25928;&#26524;&#24182;&#19981;&#20102;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KGQuiz&#65292;&#19968;&#20010;&#30693;&#35782;&#23494;&#38598;&#22411;&#22522;&#20934;&#27979;&#35797;&#65292;&#20840;&#38754;&#35843;&#26597;LLMs&#30340;&#30693;&#35782;&#27867;&#21270;&#33021;&#21147;&#12290;KGQuiz&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#30001;&#22522;&#20110;&#19977;&#20803;&#32452;&#30340;&#30693;&#35782;&#26500;&#24314;&#65292;&#28085;&#30422;&#20102;&#19977;&#20010;&#30693;&#35782;&#39046;&#22495;&#65292;&#24182;&#21253;&#25324;&#20116;&#20010;&#20219;&#21153;&#65292;&#38590;&#24230;&#36882;&#22686;&#65306;&#30495;&#20551;&#21028;&#26029;&#12289;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#12289;&#22635;&#31354;&#12289;&#20107;&#23454;&#32534;&#36753;&#21644;&#24320;&#25918;&#24335;&#30693;&#35782;&#29983;&#25104;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;LLMs&#30340;&#30693;&#35782;&#33021;&#21147;&#21644;&#23427;&#20204;&#30340;&#27867;&#21270;&#25928;&#26524;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;10&#31181;&#24320;&#28304;&#21644;&#40657;&#30418;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) demonstrate remarkable performance on knowledge-intensive tasks, suggesting that real-world knowledge is encoded in their model parameters. However, besides explorations on a few probing tasks in limited knowledge domains, it is not well understood how to evaluate LLMs' knowledge systematically and how well their knowledge abilities generalize, across a spectrum of knowledge domains and progressively complex task formats. To this end, we propose KGQuiz, a knowledge-intensive benchmark to comprehensively investigate the knowledge generalization abilities of LLMs. KGQuiz is a scalable framework constructed from triplet-based knowledge, which covers three knowledge domains and consists of five tasks with increasing complexity: true-or-false, multiple-choice QA, blank filling, factual editing, and open-ended knowledge generation. To gain a better understanding of LLMs' knowledge abilities and their generalization, we evaluate 10 open-source and black-box LLMs o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SEA&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#20272;&#35745;&#27880;&#24847;&#21147;&#25513;&#30721;&#23454;&#29616;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#31232;&#30095;&#27880;&#24847;&#21147;&#65292;&#35299;&#20915;&#20102;transformer&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#27880;&#24847;&#21147;&#25805;&#20316;&#22797;&#26434;&#24230;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#20445;&#25345;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01777</link><description>&lt;p&gt;
&#37319;&#29992;&#20272;&#35745;&#30340;&#27880;&#24847;&#21147;&#25513;&#30721;&#30340;&#31232;&#30095;&#32447;&#24615;&#27880;&#24847;&#21147;&#65288;SEA&#65289;
&lt;/p&gt;
&lt;p&gt;
SEA: Sparse Linear Attention with Estimated Attention Mask. (arXiv:2310.01777v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01777
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SEA&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#20272;&#35745;&#27880;&#24847;&#21147;&#25513;&#30721;&#23454;&#29616;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#31232;&#30095;&#27880;&#24847;&#21147;&#65292;&#35299;&#20915;&#20102;transformer&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#27880;&#24847;&#21147;&#25805;&#20316;&#22797;&#26434;&#24230;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#20445;&#25345;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;transformer&#26550;&#26500;&#22312;&#38656;&#35201;&#23545;&#24207;&#21015;&#20803;&#32032;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995;&#24314;&#27169;&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#22914;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27880;&#24847;&#21147;&#25805;&#20316;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#65292;transformer&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#27492;&#20808;&#21069;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#31232;&#30095;&#21270;&#25110;&#32447;&#24615;&#36924;&#36817;&#27880;&#24847;&#21147;&#30697;&#38453;&#26469;&#38477;&#20302;&#22797;&#26434;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#20174;&#25945;&#24072;&#30340;&#27880;&#24847;&#21147;&#30697;&#38453;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#24182;&#19988;&#36890;&#24120;&#38656;&#35201;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#31232;&#30095;&#21644;&#32447;&#24615;&#26041;&#27861;&#22914;&#26524;&#19981;&#33021;&#20135;&#29983;&#23436;&#20840;&#20108;&#27425;&#30340;&#27880;&#24847;&#21147;&#30697;&#38453;&#65292;&#36824;&#21487;&#33021;&#22833;&#21435;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SEA&#65306;&#37319;&#29992;&#20272;&#35745;&#27880;&#24847;&#21147;&#25513;&#30721;&#30340;&#31232;&#30095;&#32447;&#24615;&#27880;&#24847;&#21147;&#26041;&#27861;&#12290;SEA&#36890;&#36807;&#22522;&#20110;&#26680;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#26041;&#27861;&#20272;&#35745;&#27880;&#24847;&#21147;&#30697;&#38453;&#65292;&#24182;&#21019;&#24314;&#19968;&#20010;&#23545;&#23436;&#25972;&#27880;&#24847;&#21147;&#30697;&#38453;&#36827;&#34892;&#31232;&#30095;&#36924;&#36817;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformer architecture has made breakthroughs in recent years on tasks which require modeling pairwise relationships between sequential elements, as is the case in natural language understanding. However, transformers struggle with long sequences due to the quadratic complexity of the attention operation, and previous research has aimed to lower the complexity by sparsifying or linearly approximating the attention matrix. Yet, these approaches cannot straightforwardly distill knowledge from a teacher's attention matrix, and often require complete retraining from scratch. Furthermore, previous sparse and linear approaches may also lose interpretability if they do not produce full quadratic attention matrices. To address these challenges, we propose SEA: Sparse linear attention with an Estimated Attention mask. SEA estimates the attention matrix with linear complexity via kernel-based linear attention, then creates a sparse approximation to the full attention matrix with a top-k se
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;LLMs&#20013;&#25552;&#21462;&#22522;&#20110;&#34920;&#26684;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33976;&#39311;&#23558;&#22823;&#22411;&#27169;&#22411;&#36716;&#21270;&#20026;&#19987;&#38376;&#29992;&#20110;&#22522;&#20110;&#34920;&#26684;&#25512;&#29702;&#20219;&#21153;&#30340;&#23567;&#22411;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.13182</link><description>&lt;p&gt;
&#20174;LLMs&#20013;&#26377;&#25928;&#25552;&#21462;&#22522;&#20110;&#34920;&#26684;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Effective Distillation of Table-based Reasoning Ability from LLMs. (arXiv:2309.13182v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;LLMs&#20013;&#25552;&#21462;&#22522;&#20110;&#34920;&#26684;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33976;&#39311;&#23558;&#22823;&#22411;&#27169;&#22411;&#36716;&#21270;&#20026;&#19987;&#38376;&#29992;&#20110;&#22522;&#20110;&#34920;&#26684;&#25512;&#29702;&#20219;&#21153;&#30340;&#23567;&#22411;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24222;&#22823;&#30340;&#21442;&#25968;&#21644;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#39640;&#38656;&#27714;&#32473;&#23454;&#38469;&#24212;&#29992;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#30340;&#29305;&#23450;&#33021;&#21147;&#65292;&#22914;&#25968;&#20540;&#25512;&#29702;&#65292;&#21487;&#20197;&#36890;&#36807;&#33976;&#39311;&#20256;&#36882;&#32473;&#36739;&#23567;&#30340;&#27169;&#22411;&#12290;&#19968;&#20123;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;LLMs&#36827;&#34892;&#22522;&#20110;&#34920;&#26684;&#25512;&#29702;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20043;&#21069;&#65292;&#23578;&#26410;&#23545;&#19987;&#38376;&#20026;&#34920;&#26684;&#29983;&#25104;&#20219;&#21153;&#23450;&#21046;&#30340;&#36739;&#23567;&#27169;&#22411;&#30340;&#34920;&#26684;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#34920;&#26684;&#25512;&#29702;&#30340;&#33976;&#39311;&#26041;&#27861;&#65292;&#26088;&#22312;&#23558;LLMs&#33976;&#39311;&#25104;&#19987;&#38376;&#20026;&#22522;&#20110;&#34920;&#26684;&#25512;&#29702;&#20219;&#21153;&#35774;&#35745;&#30340;&#36739;&#23567;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20010;&#20855;&#26377;0.22&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#65288;Flan-T5-base&#65289;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#33976;&#39311;&#65292;&#24182;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, their remarkable parameter size and their impressive high requirement of computing resources pose challenges for their practical deployment. Recent research has revealed that specific capabilities of LLMs, such as numerical reasoning, can be transferred to smaller models through distillation. Some studies explore the potential of leveraging LLMs to perform table-based reasoning. Nevertheless, prior to our work, there has been no investigation into the prospect of specialising table reasoning skills in smaller models specifically tailored for table-to-text generation tasks. In this paper, we propose a novel table-based reasoning distillation, with the aim of distilling distilling LLMs into tailored, smaller models specifically designed for table-based reasoning task. Experimental results have shown that a 0.22 billion parameter model (Flan-T5-base) fin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Prompt Tuning&#22312;&#19982;"&#25216;&#33021;&#31070;&#32463;&#20803;"&#30340;&#20851;&#31995;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#29305;&#23450;&#20219;&#21153;&#30340;&#35843;&#25972;&#25351;&#20196;&#22312;&#30456;&#21516;&#31867;&#22411;&#30340;&#20219;&#21153;&#19978;&#20855;&#26377;&#20256;&#36882;&#24615;&#65292;&#20294;&#23545;&#20110;&#23545;&#25239;&#24615;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#19981;&#39640;&#65292;&#20854;&#20013;T5&#30340;&#40065;&#26834;&#24615;&#27604;RoBERTa&#26356;&#39640;&#65292;&#24182;&#19988;&#21457;&#29616;T5&#21644;RoBERTa&#20013;&#37117;&#23384;&#22312;&#25216;&#33021;&#31070;&#32463;&#20803;&#12290;</title><link>http://arxiv.org/abs/2309.12263</link><description>&lt;p&gt;
&#20851;&#20110;&#25216;&#33021;&#31070;&#32463;&#20803;&#19982;Prompt Tuning&#20013;&#30340;&#40065;&#26834;&#24615;&#30340;&#20851;&#31995;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Relationship between Skill Neurons and Robustness in Prompt Tuning. (arXiv:2309.12263v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Prompt Tuning&#22312;&#19982;"&#25216;&#33021;&#31070;&#32463;&#20803;"&#30340;&#20851;&#31995;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#29305;&#23450;&#20219;&#21153;&#30340;&#35843;&#25972;&#25351;&#20196;&#22312;&#30456;&#21516;&#31867;&#22411;&#30340;&#20219;&#21153;&#19978;&#20855;&#26377;&#20256;&#36882;&#24615;&#65292;&#20294;&#23545;&#20110;&#23545;&#25239;&#24615;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#19981;&#39640;&#65292;&#20854;&#20013;T5&#30340;&#40065;&#26834;&#24615;&#27604;RoBERTa&#26356;&#39640;&#65292;&#24182;&#19988;&#21457;&#29616;T5&#21644;RoBERTa&#20013;&#37117;&#23384;&#22312;&#25216;&#33021;&#31070;&#32463;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt Tuning&#26159;&#19968;&#31181;&#29992;&#20110;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(PLMs)&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#23545;RoBERTa&#30340;&#23454;&#39564;&#65292;&#26377;&#20154;&#35748;&#20026;Prompt Tuning&#28608;&#27963;&#20102;Transformer&#21069;&#39304;&#32593;&#32476;&#20013;&#29305;&#23450;&#30340;&#31070;&#32463;&#20803;&#65292;&#36825;&#20123;&#31070;&#32463;&#20803;&#23545;&#32473;&#23450;&#20219;&#21153;&#20855;&#26377;&#39640;&#39044;&#27979;&#33021;&#21147;&#21644;&#36873;&#25321;&#24615;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;RoBERTa&#21644;T5&#26469;&#30740;&#31350;Prompt Tuning&#19982;&#36825;&#20123;&#8220;&#25216;&#33021;&#31070;&#32463;&#20803;&#8221;&#30340;&#40065;&#26834;&#24615;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#29305;&#23450;&#20219;&#21153;&#30340;&#35843;&#25972;&#25351;&#20196;&#22312;&#30456;&#21516;&#31867;&#22411;&#30340;&#20219;&#21153;&#19978;&#20855;&#26377;&#20256;&#36882;&#24615;&#65292;&#20294;&#23545;&#20110;&#23545;&#25239;&#24615;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#19981;&#39640;&#65292;&#20854;&#20013;T5&#30340;&#40065;&#26834;&#24615;&#27604;RoBERTa&#26356;&#39640;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#37325;&#29616;&#20102;RoBERTa&#20013;&#30340;&#25216;&#33021;&#31070;&#32463;&#20803;&#23384;&#22312;&#65292;&#24182;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;T5&#20013;&#20063;&#23384;&#22312;&#25216;&#33021;&#31070;&#32463;&#20803;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;T5&#22312;&#38750;&#23545;&#25239;&#24615;&#25968;&#25454;&#19978;&#30830;&#23450;&#30340;&#25216;&#33021;&#31070;&#32463;&#20803;&#20063;&#26159;&#23545;&#25239;&#24615;&#25968;&#25454;&#19978;&#39044;&#27979;&#24615;&#26368;&#24378;&#30340;&#31070;&#32463;&#20803;&#65292;&#32780;&#36825;&#22312;RoBERTa&#20013;&#19981;&#26159;&#36825;&#31181;&#24773;&#20917;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#26356;&#39640;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#21487;&#33021;&#19982;&#25216;&#33021;&#31070;&#32463;&#20803;&#30340;&#23384;&#22312;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt Tuning is a popular parameter-efficient finetuning method for pre-trained large language models (PLMs). Recently, based on experiments with RoBERTa, it has been suggested that Prompt Tuning activates specific neurons in the transformer's feed-forward networks, that are highly predictive and selective for the given task. In this paper, we study the robustness of Prompt Tuning in relation to these "skill neurons", using RoBERTa and T5. We show that prompts tuned for a specific task are transferable to tasks of the same type but are not very robust to adversarial data, with higher robustness for T5 than RoBERTa. At the same time, we replicate the existence of skill neurons in RoBERTa and further show that skill neurons also seem to exist in T5. Interestingly, the skill neurons of T5 determined on non-adversarial data are also among the most predictive neurons on the adversarial data, which is not the case for RoBERTa. We conclude that higher adversarial robustness may be related to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28145;&#20837;&#35780;&#20272;&#22522;&#20110;&#20869;&#23481;&#21644;&#22522;&#20110;&#19978;&#19979;&#25991;&#27169;&#22411;&#22312;&#26816;&#27979;&#26032;&#30340;&#26410;&#30693;&#35875;&#35328;&#19978;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21457;&#29616;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#27169;&#22411;&#36807;&#24230;&#20381;&#36182;&#26469;&#28304;&#24086;&#23376;&#20449;&#24687;&#24182;&#24573;&#30053;&#20102;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#20063;&#25552;&#20986;&#20102;&#20943;&#23567;&#26102;&#38388;&#27010;&#24565;&#24433;&#21709;&#30340;&#23454;&#38469;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2309.11576</link><description>&lt;p&gt;
&#32771;&#23519;&#22522;&#20110;&#38745;&#24577;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#35745;&#31639;&#21270;&#35875;&#35328;&#26816;&#27979;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Examining the Limitations of Computational Rumor Detection Models Trained on Static Datasets. (arXiv:2309.11576v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28145;&#20837;&#35780;&#20272;&#22522;&#20110;&#20869;&#23481;&#21644;&#22522;&#20110;&#19978;&#19979;&#25991;&#27169;&#22411;&#22312;&#26816;&#27979;&#26032;&#30340;&#26410;&#30693;&#35875;&#35328;&#19978;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21457;&#29616;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#27169;&#22411;&#36807;&#24230;&#20381;&#36182;&#26469;&#28304;&#24086;&#23376;&#20449;&#24687;&#24182;&#24573;&#30053;&#20102;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#20063;&#25552;&#20986;&#20102;&#20943;&#23567;&#26102;&#38388;&#27010;&#24565;&#24433;&#21709;&#30340;&#23454;&#38469;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35875;&#35328;&#26816;&#27979;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#20854;&#33021;&#22815;&#26816;&#27979;&#20986;&#26032;&#20986;&#29616;&#30340;&#12289;&#20197;&#21069;&#26410;&#30693;&#30340;&#35875;&#35328;&#30340;&#33021;&#21147;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20165;&#22522;&#20110;&#20869;&#23481;&#65288;&#21363;&#20165;&#20351;&#29992;&#26469;&#28304;&#24086;&#23376;&#20316;&#20026;&#36755;&#20837;&#65289;&#30340;&#35875;&#35328;&#26816;&#27979;&#27169;&#22411;&#22312;&#26410;&#30693;&#35875;&#35328;&#19978;&#30340;&#34920;&#29616;&#25928;&#26524;&#36739;&#24046;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#27169;&#22411;&#30340;&#28508;&#21147;&#23578;&#26410;&#20805;&#20998;&#21033;&#29992;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#28145;&#20837;&#35780;&#20272;&#22522;&#20110;&#20869;&#23481;&#21644;&#22522;&#20110;&#19978;&#19979;&#25991;&#27169;&#22411;&#22312;&#29305;&#21035;&#26159;&#26816;&#27979;&#26032;&#30340;&#26410;&#30693;&#35875;&#35328;&#19978;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#27169;&#22411;&#20173;&#28982;&#36807;&#24230;&#20381;&#36182;&#26469;&#33258;&#35875;&#35328;&#26469;&#28304;&#24086;&#23376;&#30340;&#20449;&#24687;&#65292;&#24182;&#20542;&#21521;&#20110;&#24573;&#30053;&#19978;&#19979;&#25991;&#20449;&#24687;&#21487;&#33021;&#21457;&#25381;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#25968;&#25454;&#25286;&#20998;&#31574;&#30053;&#23545;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#22914;&#20309;&#20943;&#23567;&#26102;&#38388;&#27010;&#24565;&#24433;&#21709;&#30340;&#23454;&#38469;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
A crucial aspect of a rumor detection model is its ability to generalize, particularly its ability to detect emerging, previously unknown rumors. Past research has indicated that content-based (i.e., using solely source posts as input) rumor detection models tend to perform less effectively on unseen rumors. At the same time, the potential of context-based models remains largely untapped. The main contribution of this paper is in the in-depth evaluation of the performance gap between content and context-based models specifically on detecting new, unseen rumors. Our empirical findings demonstrate that context-based models are still overly dependent on the information derived from the rumors' source post and tend to overlook the significant role that contextual information can play. We also study the effect of data split strategies on classifier performance. Based on our experimental results, the paper also offers practical suggestions on how to minimize the effects of temporal concept d
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;K-pop&#27468;&#35789;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25581;&#31034;&#20102;K-pop&#27468;&#35789;&#32763;&#35793;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#31070;&#32463;&#27468;&#35789;&#32763;&#35793;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#19987;&#29992;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11093</link><description>&lt;p&gt;
K-pop&#27468;&#35789;&#32763;&#35793;&#65306;&#25968;&#25454;&#38598;&#12289;&#20998;&#26512;&#19982;&#31070;&#32463;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling. (arXiv:2309.11093v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11093
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;K-pop&#27468;&#35789;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25581;&#31034;&#20102;K-pop&#27468;&#35789;&#32763;&#35793;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#31070;&#32463;&#27468;&#35789;&#32763;&#35793;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#19987;&#29992;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27468;&#35789;&#32763;&#35793;&#20316;&#20026;&#19968;&#20010;&#30740;&#31350;&#20102;&#19968;&#20010;&#19990;&#32426;&#30340;&#39046;&#22495;&#65292;&#22914;&#20170;&#21560;&#24341;&#30528;&#35745;&#31639;&#35821;&#35328;&#23398;&#30740;&#31350;&#32773;&#30340;&#27880;&#24847;&#12290;&#25105;&#20204;&#22312;&#20197;&#24448;&#30740;&#31350;&#20013;&#21457;&#29616;&#20102;&#20004;&#20010;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#22312;&#27468;&#35789;&#32763;&#35793;&#30740;&#31350;&#20013;&#65292;&#23613;&#31649;K-pop&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#20294;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#35199;&#26041;&#27969;&#27966;&#21644;&#35821;&#35328;&#65292;&#27809;&#26377;&#30740;&#31350;&#38598;&#20013;&#22312;K-pop&#19978;&#12290;&#20854;&#27425;&#65292;&#27468;&#35789;&#32763;&#35793;&#39046;&#22495;&#32570;&#20047;&#21487;&#20844;&#24320;&#33719;&#24471;&#30340;&#25968;&#25454;&#38598;&#65307;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#23578;&#26080;&#27492;&#31867;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#25299;&#23485;&#27468;&#35789;&#32763;&#35793;&#30740;&#31350;&#30340;&#27969;&#27966;&#21644;&#35821;&#35328;&#33539;&#22260;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#21809;&#27468;&#35789;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#32422;89%&#20026;K-pop&#27468;&#35789;&#12290;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#36880;&#34892;&#21644;&#36880;&#33410;&#23545;&#40784;&#20102;&#38889;&#35821;&#21644;&#33521;&#35821;&#27468;&#35789;&#12290;&#25105;&#20204;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#25581;&#31034;&#20102;K-pop&#27468;&#35789;&#32763;&#35793;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#19982;&#20854;&#20182;&#24191;&#27867;&#30740;&#31350;&#30340;&#27969;&#27966;&#21306;&#20998;&#24320;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#31070;&#32463;&#27468;&#35789;&#32763;&#35793;&#27169;&#22411;&#65292;&#20174;&#32780;&#24378;&#35843;&#20102;&#19987;&#29992;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lyric translation, a field studied for over a century, is now attracting computational linguistics researchers. We identified two limitations in previous studies. Firstly, lyric translation studies have predominantly focused on Western genres and languages, with no previous study centering on K-pop despite its popularity. Second, the field of lyric translation suffers from a lack of publicly available datasets; to the best of our knowledge, no such dataset exists. To broaden the scope of genres and languages in lyric translation studies, we introduce a novel singable lyric translation dataset, approximately 89\% of which consists of K-pop song lyrics. This dataset aligns Korean and English lyrics line-by-line and section-by-section. We leveraged this dataset to unveil unique characteristics of K-pop lyric translation, distinguishing it from other extensively studied genres, and to construct a neural lyric translation model, thereby underscoring the importance of a dedicated dataset for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#26032;&#30340;&#20581;&#24247;&#22768;&#26126;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;750&#20010;&#30001;&#21307;&#23398;&#19987;&#23478;&#26631;&#27880;&#30340;&#20581;&#24247;&#30456;&#20851;&#22768;&#26126;&#65292;&#24182;&#25552;&#20379;&#20102;&#26469;&#33258;&#20020;&#24202;&#30740;&#31350;&#30340;&#35777;&#25454;&#25903;&#25345;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#21253;&#25324;&#35777;&#25454;&#26816;&#32034;&#12289;&#30495;&#23454;&#24615;&#39044;&#27979;&#21644;&#35299;&#37322;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.08503</link><description>&lt;p&gt;
HealthFC&#65306;&#19968;&#20221;&#29992;&#20110;&#22522;&#20110;&#35777;&#25454;&#30340;&#21307;&#23398;&#20107;&#23454;&#26816;&#39564;&#30340;&#20581;&#24247;&#22768;&#26126;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
HealthFC: A Dataset of Health Claims for Evidence-Based Medical Fact-Checking. (arXiv:2309.08503v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#26032;&#30340;&#20581;&#24247;&#22768;&#26126;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;750&#20010;&#30001;&#21307;&#23398;&#19987;&#23478;&#26631;&#27880;&#30340;&#20581;&#24247;&#30456;&#20851;&#22768;&#26126;&#65292;&#24182;&#25552;&#20379;&#20102;&#26469;&#33258;&#20020;&#24202;&#30740;&#31350;&#30340;&#35777;&#25454;&#25903;&#25345;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#21253;&#25324;&#35777;&#25454;&#26816;&#32034;&#12289;&#30495;&#23454;&#24615;&#39044;&#27979;&#21644;&#35299;&#37322;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#26102;&#20195;&#65292;&#36890;&#36807;&#20114;&#32852;&#32593;&#26597;&#35810;&#20581;&#24247;&#30456;&#20851;&#24314;&#35758;&#24050;&#25104;&#20026;&#19968;&#31181;&#24120;&#35265;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#21028;&#26029;&#22312;&#32447;&#25214;&#21040;&#30340;&#21307;&#23398;&#22768;&#26126;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#25214;&#21040;&#30456;&#24212;&#30340;&#35777;&#25454;&#65292;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20107;&#23454;&#26816;&#39564;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#36890;&#36807;&#21487;&#38752;&#30693;&#35782;&#26469;&#28304;&#30340;&#35777;&#25454;&#35780;&#20272;&#20107;&#23454;&#22768;&#26126;&#30495;&#23454;&#24615;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#25512;&#21160;&#27492;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;750&#20010;&#20581;&#24247;&#30456;&#20851;&#22768;&#26126;&#65292;&#22312;&#21487;&#20449;&#24230;&#26041;&#38754;&#30001;&#21307;&#23398;&#19987;&#23478;&#36827;&#34892;&#20102;&#26631;&#27880;&#65292;&#24182;&#25552;&#20379;&#20102;&#26469;&#33258;&#36866;&#24403;&#30340;&#20020;&#24202;&#30740;&#31350;&#30340;&#35777;&#25454;&#25903;&#25345;&#12290;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#31361;&#20986;&#20854;&#29305;&#28857;&#21644;&#25361;&#25112;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#19982;&#33258;&#21160;&#20107;&#23454;&#26816;&#39564;&#30456;&#20851;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#22914;&#35777;&#25454;&#26816;&#32034;&#12289;&#30495;&#23454;&#24615;&#39044;&#27979;&#21644;&#35299;&#37322;&#29983;&#25104;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;&#19981;&#21516;&#26041;&#27861;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#23545;&#23427;&#20204;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#35752;&#35770;&#20102;&#30740;&#31350;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Seeking health-related advice on the internet has become a common practice in the digital era. Determining the trustworthiness of medical claims found online and finding appropriate evidence for this information is increasingly challenging. Fact-checking has emerged as an approach to assess the veracity of factual claims using evidence from credible knowledge sources. To help advance the automation of this task, in this paper, we introduce a novel dataset of 750 health-related claims, labeled for veracity by medical experts and backed with evidence from appropriate clinical studies. We provide an analysis of the dataset, highlighting its characteristics and challenges. The dataset can be used for Machine Learning tasks related to automated fact-checking such as evidence retrieval, veracity prediction, and explanation generation. For this purpose, we provide baseline models based on different approaches, examine their performance, and discuss the findings.
&lt;/p&gt;</description></item><item><title>ContrastWSD&#26159;&#19968;&#31181;&#20351;&#29992;&#20102;&#35789;&#20041;&#28040;&#23696;&#30340;&#38544;&#21947;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#38544;&#21947;&#35782;&#21035;&#36807;&#31243;&#21644;&#35789;&#20041;&#28040;&#23696;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#21462;&#24182;&#23545;&#27604;&#21333;&#35789;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#21644;&#22522;&#26412;&#21547;&#20041;&#65292;&#20197;&#25552;&#39640;&#38544;&#21947;&#26816;&#27979;&#30340;&#25928;&#26524;&#65292;&#36229;&#36807;&#20854;&#20182;&#20165;&#20381;&#36182;&#19978;&#19979;&#25991;&#23884;&#20837;&#25110;&#38598;&#25104;&#22522;&#26412;&#23450;&#20041;&#21644;&#22806;&#37096;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.03103</link><description>&lt;p&gt;
ContrastWSD: &#20351;&#29992;&#35789;&#20041;&#28040;&#23696;&#21152;&#24378;&#38544;&#21947;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
ContrastWSD: Enhancing Metaphor Detection with Word Sense Disambiguation Following the Metaphor Identification Procedure. (arXiv:2309.03103v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03103
&lt;/p&gt;
&lt;p&gt;
ContrastWSD&#26159;&#19968;&#31181;&#20351;&#29992;&#20102;&#35789;&#20041;&#28040;&#23696;&#30340;&#38544;&#21947;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#38544;&#21947;&#35782;&#21035;&#36807;&#31243;&#21644;&#35789;&#20041;&#28040;&#23696;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#21462;&#24182;&#23545;&#27604;&#21333;&#35789;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#21644;&#22522;&#26412;&#21547;&#20041;&#65292;&#20197;&#25552;&#39640;&#38544;&#21947;&#26816;&#27979;&#30340;&#25928;&#26524;&#65292;&#36229;&#36807;&#20854;&#20182;&#20165;&#20381;&#36182;&#19978;&#19979;&#25991;&#23884;&#20837;&#25110;&#38598;&#25104;&#22522;&#26412;&#23450;&#20041;&#21644;&#22806;&#37096;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ContrastWSD&#65292;&#19968;&#31181;&#22522;&#20110;RoBERTa&#30340;&#38544;&#21947;&#26816;&#27979;&#27169;&#22411;&#65292;&#23427;&#38598;&#25104;&#20102;&#38544;&#21947;&#35782;&#21035;&#36807;&#31243;(MIP)&#21644;&#35789;&#20041;&#28040;&#23696;(WSD)&#26469;&#25552;&#21462;&#24182;&#23545;&#27604;&#21333;&#35789;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#21644;&#22522;&#26412;&#21547;&#20041;&#65292;&#20197;&#30830;&#23450;&#23427;&#22312;&#21477;&#23376;&#20013;&#26159;&#21542;&#20197;&#38544;&#21947;&#30340;&#26041;&#24335;&#20351;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;WSD&#27169;&#22411;&#24471;&#20986;&#30340;&#21333;&#35789;&#35789;&#20041;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22686;&#24378;&#20102;&#38544;&#21947;&#26816;&#27979;&#36807;&#31243;&#65292;&#24182;&#36229;&#36807;&#20102;&#20165;&#20381;&#36182;&#19978;&#19979;&#25991;&#23884;&#20837;&#25110;&#20165;&#38598;&#25104;&#22522;&#26412;&#23450;&#20041;&#21644;&#20854;&#20182;&#22806;&#37096;&#30693;&#35782;&#30340;&#20854;&#20182;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#24378;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#22312;&#25512;&#36827;&#38544;&#21947;&#26816;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents ContrastWSD, a RoBERTa-based metaphor detection model that integrates the Metaphor Identification Procedure (MIP) and Word Sense Disambiguation (WSD) to extract and contrast the contextual meaning with the basic meaning of a word to determine whether it is used metaphorically in a sentence. By utilizing the word senses derived from a WSD model, our model enhances the metaphor detection process and outperforms other methods that rely solely on contextual embeddings or integrate only the basic definitions and other external knowledge. We evaluate our approach on various benchmark datasets and compare it with strong baselines, indicating the effectiveness in advancing metaphor detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22240;&#26524;&#20132;&#21449;&#24615;&#21644;&#21452;&#37325;&#26799;&#24230;&#19979;&#38477;&#22312;&#22810;&#27169;&#24577;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#20167;&#24680;&#36855;&#22240;&#26816;&#27979;&#20026;&#20363;&#12290;&#36890;&#36807;&#32467;&#21512;&#22240;&#26524;&#20998;&#26512;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#21487;&#20197;&#25581;&#31034;&#20854;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#20171;&#32461;&#20102;&#20132;&#21449;&#24615;&#21644;&#27169;&#24577;&#30340;&#26799;&#24230;&#27880;&#24847;&#21147;&#30340;&#25688;&#35201;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11585</link><description>&lt;p&gt;
&#22240;&#26524;&#20132;&#21449;&#24615;&#21644;&#21452;&#37325;&#26799;&#24230;&#19979;&#38477;&#22312;&#22810;&#27169;&#24577;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65306;&#20197;&#20167;&#24680;&#36855;&#22240;&#20026;&#20363;&#65288;arXiv:2308.11585v1 [cs.AI]&#65289;
&lt;/p&gt;
&lt;p&gt;
Causal Intersectionality and Dual Form of Gradient Descent for Multimodal Analysis: a Case Study on Hateful Memes. (arXiv:2308.11585v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22240;&#26524;&#20132;&#21449;&#24615;&#21644;&#21452;&#37325;&#26799;&#24230;&#19979;&#38477;&#22312;&#22810;&#27169;&#24577;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#20167;&#24680;&#36855;&#22240;&#26816;&#27979;&#20026;&#20363;&#12290;&#36890;&#36807;&#32467;&#21512;&#22240;&#26524;&#20998;&#26512;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#21487;&#20197;&#25581;&#31034;&#20854;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#20171;&#32461;&#20102;&#20132;&#21449;&#24615;&#21644;&#27169;&#24577;&#30340;&#26799;&#24230;&#27880;&#24847;&#21147;&#30340;&#25688;&#35201;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#65292;&#29305;&#21035;&#26159;&#22312;&#26032;&#20852;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#29702;&#35299;&#20854;&#20869;&#37096;&#24037;&#20316;&#20013;&#30340;&#35821;&#20041;&#24847;&#20041;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22240;&#26524;&#20998;&#26512;&#20391;&#37325;&#20110;&#23450;&#20041;&#35821;&#20041;&#21450;&#20854;&#37327;&#21270;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#26159;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#26680;&#24515;&#65292;&#29992;&#20110;&#35299;&#37322;&#40657;&#30418;&#23376;&#30340;&#35299;&#37322;&#12290;&#36890;&#36807;&#21327;&#21516;&#36825;&#20123;&#26041;&#27861;&#65292;&#25506;&#32034;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#22914;&#20309;&#38416;&#26126;&#20854;&#22240;&#26524;&#25928;&#24212;&#24050;&#25104;&#20026;&#22522;&#20110;&#35777;&#25454;&#30340;&#20915;&#31574;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#19968;&#31995;&#21015;&#24182;&#34892;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20132;&#21449;&#24615;--&#20010;&#20307;&#30340;&#22810;&#20010;&#20154;&#21475;&#32479;&#35745;&#23398;&#22240;&#32032;&#30340;&#32452;&#21512;&#24433;&#21709;--&#21487;&#20197;&#20197;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#30340;&#24418;&#24335;&#36827;&#34892;&#32467;&#26500;&#21270;&#12290;&#26368;&#21021;&#65292;&#26412;&#30740;&#31350;&#38416;&#36848;&#20102;&#20167;&#24680;&#36855;&#22240;&#26816;&#27979;&#38382;&#39064;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;ATE&#26469;&#25551;&#36848;&#65292;&#20511;&#21161;&#20132;&#21449;&#24615;&#21407;&#21017;&#65292;&#20197;&#21450;&#22522;&#20110;&#27169;&#24577;&#30340;&#26799;&#24230;&#27880;&#24847;&#21147;&#30340;&#25688;&#35201;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the wake of the explosive growth of machine learning (ML) usage, particularly within the context of emerging Large Language Models (LLMs), comprehending the semantic significance rooted in their internal workings is crucial. While causal analyses focus on defining semantics and its quantification, the gradient-based approach is central to explainable AI (XAI), tackling the interpretation of the black box. By synergizing these approaches, the exploration of how a model's internal mechanisms illuminate its causal effect has become integral for evidence-based decision-making. A parallel line of research has revealed that intersectionality - the combinatory impact of multiple demographics of an individual - can be structured in the form of an Averaged Treatment Effect (ATE). Initially, this study illustrates that the hateful memes detection problem can be formulated as an ATE, assisted by the principles of intersectionality, and that a modality-wise summarization of gradient-based atten
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#20174;&#25972;&#20307;&#35282;&#24230;&#23545;&#35813;&#39046;&#22495;&#30340;&#31995;&#32479;&#23457;&#26597;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#22823;&#37327;&#32593;&#32476;&#30693;&#35782;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#26234;&#33021;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2308.11432</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Large Language Model based Autonomous Agents. (arXiv:2308.11432v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11432
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#20174;&#25972;&#20307;&#35282;&#24230;&#23545;&#35813;&#39046;&#22495;&#30340;&#31995;&#32479;&#23457;&#26597;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#22823;&#37327;&#32593;&#32476;&#30693;&#35782;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#26234;&#33021;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#20195;&#29702;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#23398;&#26415;&#30028;&#30340;&#30740;&#31350;&#28909;&#28857;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#24448;&#24448;&#38598;&#20013;&#22312;&#23545;&#26377;&#38480;&#30693;&#35782;&#30340;&#20195;&#29702;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#36825;&#19982;&#20154;&#31867;&#30340;&#23398;&#20064;&#36807;&#31243;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#65292;&#22240;&#27492;&#24456;&#38590;&#23454;&#29616;&#20154;&#31867;&#33324;&#30340;&#20915;&#31574;&#12290;&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#33719;&#21462;&#22823;&#37327;&#30340;&#32593;&#32476;&#30693;&#35782;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#29616;&#20986;&#20102;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#26234;&#33021;&#30340;&#26174;&#33879;&#28508;&#21147;&#12290;&#36825;&#24341;&#21457;&#20102;&#23545;&#22522;&#20110;LLM&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#30340;&#39640;&#28072;&#20852;&#36259;&#12290;&#20026;&#20102;&#21457;&#25381;LLM&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#20102;&#21508;&#31181;&#19981;&#21516;&#24212;&#29992;&#30340;&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#12290;&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#36825;&#20123;&#30740;&#31350;&#65292;&#20174;&#25972;&#20307;&#30340;&#35282;&#24230;&#23545;&#33258;&#20027;&#20195;&#29702;&#39046;&#22495;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#23457;&#26597;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#26500;&#24314;&#65292;&#20026;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents have long been a prominent research topic in the academic community. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from the human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating autonomous agents based on LLMs. To harness the full potential of LLMs, researchers have devised diverse agent architectures tailored to different applications. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of autonomous agents from a holistic perspective. More specifically, our focus lies in the construction of LLM-based agents, for which we propose a unified framework t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#26222;&#36890;&#20154;&#22312;&#38754;&#20020;&#38544;&#31169;&#23041;&#32961;&#24773;&#22659;&#26102;&#30340;&#20915;&#31574;&#34892;&#20026;&#65292;&#20197;&#21450;&#20182;&#20204;&#24895;&#24847;&#20026;&#32473;&#20104;&#24046;&#20998;&#38544;&#31169;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#25552;&#20379;&#25935;&#24863;&#25968;&#25454;&#25152;&#25215;&#25285;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2307.06708</link><description>&lt;p&gt;
&#26159;&#21542;&#20998;&#20139;&#65311;&#32473;&#20104;&#24046;&#20998;&#38544;&#31169;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#25935;&#24863;&#25968;&#25454;&#30340;&#26222;&#36890;&#20154;&#25509;&#21463;&#20160;&#20040;&#39118;&#38505;&#65311;
&lt;/p&gt;
&lt;p&gt;
To share or not to share: What risks would laypeople accept to give sensitive data to differentially-private NLP systems?. (arXiv:2307.06708v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06708
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#26222;&#36890;&#20154;&#22312;&#38754;&#20020;&#38544;&#31169;&#23041;&#32961;&#24773;&#22659;&#26102;&#30340;&#20915;&#31574;&#34892;&#20026;&#65292;&#20197;&#21450;&#20182;&#20204;&#24895;&#24847;&#20026;&#32473;&#20104;&#24046;&#20998;&#38544;&#31169;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#25552;&#20379;&#25935;&#24863;&#25968;&#25454;&#25152;&#25215;&#25285;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;NLP&#31038;&#21306;&#24050;&#32463;&#37319;&#29992;&#20013;&#24515;&#24046;&#20998;&#38544;&#31169;&#20316;&#20026;&#20445;&#25252;&#38544;&#31169;&#30340;&#27169;&#22411;&#35757;&#32451;&#25110;&#25968;&#25454;&#20849;&#20139;&#30340;&#39318;&#36873;&#26694;&#26550;&#65292;&#20294;&#20915;&#23450;&#24615;&#30340;&#20851;&#38190;&#21442;&#25968;&#8212;&#8212;&#25511;&#21046;&#38544;&#31169;&#20445;&#25252;&#24378;&#24230;&#30340;&#38544;&#31169;&#39044;&#31639;&#949;&#30340;&#36873;&#25321;&#21644;&#35299;&#37322;&#20173;&#28982;&#30456;&#24403;&#38543;&#24847;&#12290;&#25105;&#20204;&#35748;&#20026;&#30830;&#23450;&#949;&#20540;&#19981;&#24212;&#35813;&#20165;&#30001;&#30740;&#31350;&#20154;&#21592;&#25110;&#31995;&#32479;&#24320;&#21457;&#32773;&#20915;&#23450;&#65292;&#36824;&#24517;&#39035;&#32771;&#34385;&#37027;&#20123;&#20849;&#20139;&#20182;&#20204;&#28508;&#22312;&#25935;&#24863;&#25968;&#25454;&#30340;&#20154;&#12290;&#25442;&#21477;&#35805;&#35828;&#65306;&#20320;&#24895;&#24847;&#20026;&#949;&#20540;&#20026;10&#32780;&#20998;&#20139;&#20320;&#30340;&#21363;&#26102;&#28040;&#24687;&#21527;&#65311;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#12289;&#23454;&#26045;&#21644;&#36827;&#34892;&#34892;&#20026;&#23454;&#39564;(311&#21517;&#26222;&#36890;&#21442;&#19982;&#32773;)&#26469;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#30740;&#31350;&#20154;&#20204;&#22312;&#19981;&#30830;&#23450;&#20915;&#31574;&#29615;&#22659;&#19979;&#38754;&#23545;&#23041;&#32961;&#38544;&#31169;&#30340;&#24773;&#22659;&#26102;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#23558;&#39118;&#38505;&#24863;&#30693;&#26694;&#26550;&#21270;&#20026;&#20004;&#20010;&#29616;&#23454;&#30340;NLP&#22330;&#26223;&#65292;&#24182;&#20351;&#29992;&#24773;&#33410;&#34892;&#20026;&#30740;&#31350;&#65292;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#21738;&#20123;&#949;&#38408;&#20540;&#23558;&#23548;&#33268;&#20849;&#20139;&#34892;&#20026;&#30340;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the NLP community has adopted central differential privacy as a go-to framework for privacy-preserving model training or data sharing, the choice and interpretation of the key parameter, privacy budget $\varepsilon$ that governs the strength of privacy protection, remains largely arbitrary. We argue that determining the $\varepsilon$ value should not be solely in the hands of researchers or system developers, but must also take into account the actual people who share their potentially sensitive data. In other words: Would you share your instant messages for $\varepsilon$ of 10? We address this research gap by designing, implementing, and conducting a behavioral experiment (311 lay participants) to study the behavior of people in uncertain decision-making situations with respect to privacy-threatening situations. Framing the risk perception in terms of two realistic NLP scenarios and using a vignette behavioral study help us determine what $\varepsilon$ thresholds would lead l
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20221;&#21517;&#20026;ODD&#30340;&#26032;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#65292;&#26816;&#27979;&#21644;&#20998;&#31867;&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#22312;&#33647;&#29289;&#30456;&#20851;&#30149;&#20363;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2307.02591</link><description>&lt;p&gt;
ODD: &#19968;&#20221;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ODD: A Benchmark Dataset for the NLP-based Opioid Related Aberrant Behavior Detection. (arXiv:2307.02591v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02591
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20221;&#21517;&#20026;ODD&#30340;&#26032;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#65292;&#26816;&#27979;&#21644;&#20998;&#31867;&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#22312;&#33647;&#29289;&#30456;&#20851;&#30149;&#20363;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#65288;ORAB&#65289;&#26159;&#38450;&#27490;&#33647;&#29289;&#36807;&#37327;&#30340;&#26032;&#39118;&#38505;&#22240;&#32032;&#12290;&#20197;&#24448;&#65292;ORAB&#20027;&#35201;&#36890;&#36807;&#35843;&#26597;&#32467;&#26524;&#21644;&#33647;&#29289;&#32473;&#20104;&#30417;&#27979;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#25193;&#23637;&#65292;&#24182;&#19981;&#33021;&#28085;&#30422;&#25152;&#26377;&#24322;&#24120;&#34892;&#20026;&#30340;&#33539;&#22260;&#12290;&#28982;&#32780;&#65292;ORAB&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#20013;&#24191;&#27867;&#26377;&#35760;&#24405;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;ODD&#30340;&#26032;&#22411;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;ORAB&#26816;&#27979;&#12290;ODD&#26159;&#19968;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;750&#22810;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#12290;ODD&#26088;&#22312;&#20174;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#20013;&#35782;&#21035;ORAB&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#20061;&#20010;&#31867;&#21035;&#65306;1&#65289;&#24050;&#30830;&#35748;&#24322;&#24120;&#34892;&#20026;&#65292;2&#65289;&#26263;&#31034;&#30340;&#24322;&#24120;&#34892;&#20026;&#65292;3&#65289;&#38463;&#29255;&#31867;&#33647;&#29289;&#65292;4&#65289;&#36866;&#24212;&#30151;&#65292;5&#65289;&#24050;&#35786;&#26029;&#30340;&#38463;&#29255;&#21046;&#21058;&#20381;&#36182;&#65292;6&#65289;&#33519;&#20108;&#27694;&#24179;&#31867;&#33647;&#29289;&#65292;7&#65289;&#33647;&#29289;&#21464;&#21270;&#65292;8&#65289;&#19982;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#30456;&#20851;&#65292;9&#65289;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Opioid related aberrant behaviors (ORAB) present novel risk factors for opioid overdose. Previously, ORAB have been mainly assessed by survey results and by monitoring drug administrations. Such methods however, cannot scale up and do not cover the entire spectrum of aberrant behaviors. On the other hand, ORAB are widely documented in electronic health record notes. This paper introduces a novel biomedical natural language processing benchmark dataset named ODD, for ORAB Detection Dataset. ODD is an expert-annotated dataset comprising of more than 750 publicly available EHR notes. ODD has been designed to identify ORAB from patients' EHR notes and classify them into nine categories; 1) Confirmed Aberrant Behavior, 2) Suggested Aberrant Behavior, 3) Opioids, 4) Indication, 5) Diagnosed opioid dependency, 6) Benzodiapines, 7) Medication Changes, 8) Central Nervous System-related, and 9) Social Determinants of Health. We explored two state-of-the-art natural language processing (NLP) mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#21518;&#35757;&#32451;&#25216;&#26415;Dial-MAE&#65292;&#21033;&#29992;&#29983;&#25104;&#26041;&#27861;&#26356;&#22909;&#22320;&#21387;&#32553;&#23545;&#35805;&#35821;&#20041;&#33267;&#23494;&#38598;&#21521;&#37327;&#65292;&#24182;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04357</link><description>&lt;p&gt;
&#29992;&#20110;&#22522;&#20110;&#26816;&#32034;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#19978;&#19979;&#25991;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
ConTextual Masked Auto-Encoder for Retrieval-based Dialogue Systems. (arXiv:2306.04357v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#21518;&#35757;&#32451;&#25216;&#26415;Dial-MAE&#65292;&#21033;&#29992;&#29983;&#25104;&#26041;&#27861;&#26356;&#22909;&#22320;&#21387;&#32553;&#23545;&#35805;&#35821;&#20041;&#33267;&#23494;&#38598;&#21521;&#37327;&#65292;&#24182;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#26088;&#22312;&#26681;&#25454;&#32473;&#23450;&#30340;&#29992;&#25143;&#21644;&#31995;&#32479;&#35805;&#35821;&#21382;&#21490;&#35760;&#24405;&#20174;&#20960;&#20010;&#20505;&#36873;&#21709;&#24212;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#21709;&#24212;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#21518;&#35757;&#32451;&#22823;&#22810;&#20381;&#36182;&#20110;&#21333;&#32431;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#26469;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#20934;&#30830;&#24615;&#12290;&#20294;&#26159;&#65292;&#26368;&#36817;&#24320;&#21457;&#30340;&#29983;&#25104;&#26041;&#27861;&#22312;IR&#31038;&#21306;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#25991;&#26412;&#34920;&#31034;&#33021;&#21147;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#22909;&#30340;&#23545;&#35805;&#35821;&#20041;&#24314;&#27169;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986; Dial-MAE&#65288;&#23545;&#35805;&#19978;&#19979;&#25991;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38024;&#23545;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#21518;&#35757;&#32451;&#25216;&#26415;&#12290; Dial-MAE&#20351;&#29992;&#19968;&#20010;&#19981;&#23545;&#31216;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#23398;&#20064;&#23558;&#23545;&#35805;&#30340;&#35821;&#20041;&#26356;&#22909;&#22320;&#21387;&#32553;&#21040;&#23494;&#38598;&#21521;&#37327;&#20013;&#12290; Dial-MAE&#30340;&#36807;&#31243;&#21253;&#25324;&#30001;&#28145;&#24230;&#32534;&#30721;&#22120;&#21019;&#24314;&#24102;&#26377;&#25513;&#30721;&#23545;&#35805;&#19978;&#19979;&#25991;&#30340;&#23545;&#35805;&#23884;&#20837;&#65292;&#28982;&#21518;&#26159;&#27973;&#35299;&#30721;&#22120;&#65292;&#35813;&#35299;&#30721;&#22120;&#20351;&#29992;&#27492;&#23884;&#20837;&#20197;&#21450;&#19978;&#19979;&#25991;&#21521;&#37327;&#26469;&#29983;&#25104;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue response selection aims to select an appropriate response from several candidates based on a given user and system utterance history. Recent studies have been improving the accuracy of dialogue response selection through post-training, mostly relying on naive masked language modeling methods. However, the recently developed generative methods have shown promising text representation capabilities in IR community, which could potentially lead to better dialogue semantics modeling. Thus, in this paper, we propose Dial-MAE (Dialogue Contextual Masking Auto-encoder), a straightforward yet effective post-training technique tailored for dialogue response selection. Dial-MAE uses an asymmetric encoder-decoder architecture that learns to better compress the semantics of the dialogue into dialogue-dense vectors. The process of Dial-MAE involves a deep encoder creating a dialogue embedding with the masked dialogue context, followed by a shallow decoder that uses this embedding along with
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#28155;&#21152;&#21644;&#20943;&#23569;&#22122;&#22768;&#65292;&#20197;&#19968;&#31181;&#26356;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#65292;&#21487;&#35270;&#21270;&#21644;&#27604;&#36739;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#27169;&#22411;&#65292;&#26356;&#28165;&#26970;&#22320;&#20102;&#35299;&#33258;&#21160;&#21457;&#38899;&#38556;&#30861;&#35780;&#20272;&#30340;&#27169;&#24335;&#65292;&#24182;&#20026;&#24314;&#31435;&#21487;&#38752;&#30340;&#33258;&#21160;&#21457;&#38899;&#38556;&#30861;&#35780;&#20272;&#31995;&#32479;&#25552;&#20379;&#20102;&#35748;&#35782;&#21644;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2306.04337</link><description>&lt;p&gt;
&#33258;&#21160;&#21457;&#38899;&#38556;&#30861;&#35821;&#38899;&#35780;&#20272;&#21487;&#38752;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on the Reliability of Automatic Dysarthric Speech Assessments. (arXiv:2306.04337v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#28155;&#21152;&#21644;&#20943;&#23569;&#22122;&#22768;&#65292;&#20197;&#19968;&#31181;&#26356;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#65292;&#21487;&#35270;&#21270;&#21644;&#27604;&#36739;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#27169;&#22411;&#65292;&#26356;&#28165;&#26970;&#22320;&#20102;&#35299;&#33258;&#21160;&#21457;&#38899;&#38556;&#30861;&#35780;&#20272;&#30340;&#27169;&#24335;&#65292;&#24182;&#20026;&#24314;&#31435;&#21487;&#38752;&#30340;&#33258;&#21160;&#21457;&#38899;&#38556;&#30861;&#35780;&#20272;&#31995;&#32479;&#25552;&#20379;&#20102;&#35748;&#35782;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#21457;&#38899;&#38556;&#30861;&#35780;&#20272;&#25552;&#20379;&#20102;&#24320;&#21457;&#26377;&#25928;&#20302;&#25104;&#26412;&#24037;&#20855;&#30340;&#26426;&#20250;&#65292;&#36825;&#20123;&#24037;&#20855;&#21487;&#20197;&#35299;&#20915;&#25163;&#21160;&#21644;&#20027;&#35266;&#35780;&#20272;&#30340;&#29616;&#26377;&#38480;&#21046;&#12290; &#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#26159;&#21542;&#20381;&#36182;&#20110;&#21457;&#38899;&#38556;&#30861;&#30456;&#20851;&#30340;&#35821;&#38899;&#27169;&#24335;&#25110;&#22806;&#37096;&#22240;&#32032;&#23578;&#19981;&#28165;&#26970;&#12290; &#25105;&#20204;&#26088;&#22312;&#26356;&#28165;&#26970;&#22320;&#20102;&#35299;&#21457;&#38899;&#38556;&#30861;&#27169;&#24335;&#12290; &#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#21644;&#20943;&#23569;&#22122;&#22768;&#65292;&#35774;&#35745;&#21644;&#23454;&#26045;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#21487;&#35270;&#21270;&#21644;&#27604;&#36739;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#27169;&#22411;&#65292;&#22312;&#24739;&#32773;&#23618;&#38754;&#19978;&#36827;&#34892;&#26356;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;UA-Speech&#25968;&#25454;&#38598;&#65292;&#23558;&#25968;&#25454;&#20197;&#35828;&#35805;&#20154;&#20026;&#21333;&#20301;&#36827;&#34892;&#25286;&#20998;&#12290; &#30446;&#21069;&#30340;&#25991;&#29486;&#20013;&#25253;&#21578;&#30340;&#32467;&#26524;&#20284;&#20046;&#26080;&#35770;&#26159;&#21542;&#36827;&#34892;&#20102;&#36825;&#31181;&#25286;&#20998;&#65292;&#37117;&#20250;&#23548;&#33268;&#30001;&#20110;&#25968;&#25454;&#27844;&#28431;&#32780;&#21487;&#33021;&#36807;&#20110;&#33258;&#20449;&#30340;&#27169;&#22411;&#12290; &#25105;&#20204;&#24076;&#26395;&#36825;&#20123;&#32467;&#26524;&#25552;&#39640;&#30740;&#31350;&#30028;&#23545;&#24314;&#31435;&#21487;&#38752;&#30340;&#33258;&#21160;&#21457;&#38899;&#38556;&#30861;&#35780;&#20272;&#31995;&#32479;&#30340;&#35201;&#27714;&#30340;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automating dysarthria assessments offers the opportunity to develop effective, low-cost tools that address the current limitations of manual and subjective assessments. Nonetheless, it is unclear whether current approaches rely on dysarthria-related speech patterns or external factors. We aim toward obtaining a clearer understanding of dysarthria patterns. To this extent, we study the effects of noise in recordings, both through addition and reduction. We design and implement a new method for visualizing and comparing feature extractors and models, at a patient level, in a more interpretable way. We use the UA-Speech dataset with a speaker-based split of the dataset. Results reported in the literature appear to have been done irrespective of such split, leading to models that may be overconfident due to data-leakage. We hope that these results raise awareness in the research community regarding the requirements for establishing reliable automatic dysarthria assessment systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PLANNER&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23558;&#33258;&#22238;&#24402;&#29983;&#25104;&#21644;&#28508;&#22312;&#35821;&#20041;&#25193;&#25955;&#30456;&#32467;&#21512;&#65292;&#20197;&#22312;&#29983;&#25104;&#25991;&#26412;&#26102;&#22312;&#27573;&#33853;&#32423;&#21035;&#23454;&#29616;&#20840;&#23616;&#25511;&#21046;&#65292;&#29983;&#25104;&#27969;&#30021;&#30340;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.02531</link><description>&lt;p&gt;
PLANNER:&#36890;&#36807;&#28508;&#22312;&#35821;&#35328;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#21270;&#27573;&#33853;
&lt;/p&gt;
&lt;p&gt;
PLANNER: Generating Diversified Paragraph via Latent Language Diffusion Model. (arXiv:2306.02531v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PLANNER&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23558;&#33258;&#22238;&#24402;&#29983;&#25104;&#21644;&#28508;&#22312;&#35821;&#20041;&#25193;&#25955;&#30456;&#32467;&#21512;&#65292;&#20197;&#22312;&#29983;&#25104;&#25991;&#26412;&#26102;&#22312;&#27573;&#33853;&#32423;&#21035;&#23454;&#29616;&#20840;&#23616;&#25511;&#21046;&#65292;&#29983;&#25104;&#27969;&#30021;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#26377;&#26102;&#20250;&#29983;&#25104;&#37325;&#22797;&#19988;&#36136;&#37327;&#20302;&#19979;&#30340;&#36755;&#20986;&#65292;&#22240;&#20026;&#22312;&#29983;&#25104;&#30340;&#27493;&#39588;&#20013;&#38169;&#35823;&#20250;&#32047;&#31215;&#12290;&#36825;&#20010;&#38382;&#39064;&#24120;&#24120;&#34987;&#24402;&#22240;&#20110;&#26333;&#20809;&#20559;&#24046;-&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#20351;&#29992;&#26041;&#24335;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#27169;&#22411;&#21487;&#20197;&#22238;&#39038;&#21644;&#20462;&#27491;&#20854;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#35745;&#31639;&#19978;&#21487;&#33021;&#24456;&#26114;&#36149;&#65292;&#24182;&#19988;&#22312;&#25991;&#26412;&#21644;&#27573;&#33853;&#36739;&#38271;&#30340;&#24773;&#20917;&#19979;&#65292;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#27604;&#65292;&#20808;&#21069;&#20851;&#20110;&#25991;&#26412;&#30340;&#30740;&#31350;&#21162;&#21147;&#24050;&#23548;&#33268;&#20135;&#29983;&#30340;&#27169;&#22411;&#20135;&#29983;&#19981;&#22826;&#27969;&#30021;&#30340;&#36755;&#20986;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PLANNER&#65292;&#19968;&#20010;&#23558;&#28508;&#22312;&#35821;&#20041;&#25193;&#25955;&#19982;&#33258;&#22238;&#24402;&#29983;&#25104;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#20197;&#22312;&#27573;&#33853;&#19978;&#36827;&#34892;&#20840;&#23616;&#25511;&#21046;&#26469;&#29983;&#25104;&#27969;&#30021;&#30340;&#25991;&#26412;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23558;&#33258;&#22238;&#24402;&#30340;&#8220;&#35299;&#30721;&#8221;&#27169;&#22359;&#19982;&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#20197;&#31895;&#31890;&#24230;&#26041;&#24335;&#29983;&#25104;&#35821;&#20041;&#27573;&#33853;&#23884;&#20837;&#30340;&#8220;&#35268;&#21010;&#8221;&#27169;&#22359;&#30456;&#32467;&#21512;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Autoregressive models for text sometimes generate repetitive and low-quality output because errors accumulate during the steps of generation. This issue is often attributed to exposure bias - the difference between how a model is trained, and how it is used during inference. Denoising diffusion models provide an alternative approach in which a model can revisit and revise its output. However, they can be computationally expensive and prior efforts on text have led to models that produce less fluent output compared to autoregressive models, especially for longer text and paragraphs. In this paper, we propose PLANNER, a model that combines latent semantic diffusion with autoregressive generation, to generate fluent text while exercising global control over paragraphs. The model achieves this by combining an autoregressive "decoding" module with a "planning" module that uses latent diffusion to generate semantic paragraph embeddings in a coarse-to-fine manner. The proposed method is evalu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#24314;&#27169;&#25104;&#22270;&#24418;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24605;&#32500;&#22270;&#8221;&#65288;GoT&#65289;&#25512;&#29702;&#36741;&#21161;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#23436;&#25104;&#26356;&#21152;&#30495;&#23454;&#30340;&#12289;&#22797;&#26434;&#30340;&#24605;&#32500;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.16582</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22270;&#24605;&#32500;&#25512;&#29702;&#65306;&#36229;&#36234;&#8220;&#24605;&#32500;&#38142;&#8221;&#30340;&#26377;&#21147;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models. (arXiv:2305.16582v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16582
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#24314;&#27169;&#25104;&#22270;&#24418;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24605;&#32500;&#22270;&#8221;&#65288;GoT&#65289;&#25512;&#29702;&#36741;&#21161;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#23436;&#25104;&#26356;&#21152;&#30495;&#23454;&#30340;&#12289;&#22797;&#26434;&#30340;&#24605;&#32500;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;NLP&#20219;&#21153;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#8220;&#24605;&#32500;&#38142;&#8221;&#65288;CoT&#65289;&#33021;&#22815;&#36890;&#36807;&#29983;&#25104;&#20013;&#38388;&#27493;&#39588;&#26469;&#24110;&#21161;LLMs&#23436;&#25104;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#30340;&#24605;&#32500;&#36807;&#31243;&#24120;&#24120;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#32780;&#19981;&#21482;&#26159;&#31616;&#21333;&#30340;&#39034;&#24207;&#24605;&#32500;&#38142;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24605;&#32500;&#22270;&#8221;&#65288;GoT&#65289;&#25512;&#29702;&#65292;&#23427;&#19981;&#20165;&#23558;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#24314;&#27169;&#25104;&#38142;&#24335;&#32467;&#26500;&#65292;&#32780;&#19988;&#36824;&#24314;&#27169;&#25104;&#22270;&#24418;&#32467;&#26500;&#12290;&#36890;&#36807;&#23558;&#24605;&#32500;&#21333;&#20803;&#34920;&#31034;&#20026;&#33410;&#28857;&#65292;&#23427;&#20204;&#20043;&#38388;&#30340;&#36830;&#25509;&#20316;&#20026;&#36793;&#32536;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25429;&#25417;&#20102;&#20154;&#31867;&#24605;&#32500;&#30340;&#38750;&#39034;&#24207;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#24605;&#32500;&#36807;&#31243;&#30340;&#26356;&#21152;&#30495;&#23454;&#30340;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the widespread use of large language models (LLMs) in NLP tasks, researchers have discovered the potential of Chain-of-thought (CoT) to assist LLMs in accomplishing complex reasoning tasks by generating intermediate steps. However, human thought processes are often non-linear, rather than simply sequential chains of thoughts. Therefore, we propose Graph-of-Thought (GoT) reasoning, which models human thought processes not only as a chain but also as a graph. By representing thought units as nodes and connections between them as edges, our approach captures the non-sequential nature of human thinking and allows for a more realistic modeling of thought processes. Similar to Multimodal-CoT, we modeled GoT reasoning as a two-stage framework, generating rationales first and then producing the final answer. Specifically, we employ an additional graph-of-thoughts encoder for GoT representation learning and fuse the GoT representation with the original input representation through a gated 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20845;&#20010;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#24182;&#30740;&#31350;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#26080;&#27861;&#19982;&#36739;&#23567;&#30340;&#24494;&#35843;&#22522;&#32447;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2305.14310</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#20998;&#31867;&#20013;&#30340;&#25552;&#31034;&#22797;&#26434;&#24615;&#23548;&#33322;&#65306;&#19968;&#39033;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Navigating Prompt Complexity for Zero-Shot Classification: A Study of Large Language Models in Computational Social Science. (arXiv:2305.14310v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20845;&#20010;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#24182;&#30740;&#31350;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#26080;&#27861;&#19982;&#36739;&#23567;&#30340;&#24494;&#35843;&#22522;&#32447;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#19988;&#20855;&#26377;&#26681;&#25454;&#29305;&#23450;&#25552;&#31034;&#29983;&#25104;&#21709;&#24212;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#23427;&#20204;&#30340;&#24212;&#29992;&#36890;&#24120;&#37319;&#29992;&#38646;&#26679;&#26412;&#35774;&#32622;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#20010;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;LLM&#65292;ChatGPT&#21644;OpenAssistant&#22312;&#20845;&#20010;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#20998;&#31867;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#30740;&#31350;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35843;&#26597;&#20102;&#25552;&#31034;&#22797;&#26434;&#24615;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#22312;&#25552;&#31034;&#20013;&#21152;&#20837;&#26631;&#31614;&#23450;&#20041;&#30340;&#25928;&#26524;&#65307;&#20351;&#29992;&#26631;&#31614;&#21517;&#31216;&#30340;&#21516;&#20041;&#35789;&#65307;&#20197;&#21450;&#22312;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#25972;&#21512;&#36807;&#21435;&#35760;&#24518;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#65292;&#30446;&#21069;&#30340;LLMs&#26080;&#27861;&#36798;&#21040;&#36739;&#23567;&#30340;&#24494;&#35843;&#22522;&#32447;&#36716;&#25442;&#27169;&#22411;&#65288;&#22914;BERT-large&#65289;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned Large Language Models (LLMs) have exhibited impressive language understanding and the capacity to generate responses that follow specific prompts. However, due to the computational demands associated with training these models, their applications often adopt a zero-shot setting. In this paper, we evaluate the zero-shot performance of two publicly accessible LLMs, ChatGPT and OpenAssistant, in the context of six Computational Social Science classification tasks, while also investigating the effects of various prompting strategies. Our experiments investigate the impact of prompt complexity, including the effect of incorporating label definitions into the prompt; use of synonyms for label names; and the influence of integrating past memories during foundation model training. The findings indicate that in a zero-shot setting, current LLMs are unable to match the performance of smaller, fine-tuned baseline transformer models (such as BERT-large). Additionally, we find tha
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21069;&#31471;&#26694;&#26550;&#65292;&#25429;&#25417;&#20102;&#33521;&#25991;&#35821;&#38899;&#21512;&#25104;&#21069;&#31471;&#27169;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#22312;&#25152;&#26377;&#27169;&#22359;&#20013;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10666</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#33521;&#25991;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#21069;&#31471;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
a unified front-end framework for english text-to-speech synthesis. (arXiv:2305.10666v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10666
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21069;&#31471;&#26694;&#26550;&#65292;&#25429;&#25417;&#20102;&#33521;&#25991;&#35821;&#38899;&#21512;&#25104;&#21069;&#31471;&#27169;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#22312;&#25152;&#26377;&#27169;&#22359;&#20013;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21069;&#31471;&#26159;&#33521;&#25991;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#36127;&#36131;&#25552;&#21462;&#35821;&#35328;&#29305;&#24449;&#65292;&#22914;&#38901;&#24459;&#21644;&#38899;&#32032;&#65292;&#36825;&#23545;&#20110;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#21512;&#25104;&#35821;&#38899;&#33267;&#20851;&#37325;&#35201;&#12290;&#33521;&#25991;&#25991;&#26412;&#21040;&#35821;&#38899;&#21069;&#31471;&#36890;&#24120;&#30001;&#25991;&#26412;&#35268;&#33539;&#21270;&#27169;&#22359;&#65288;TN&#65289;&#65292;&#21333;&#35789;&#38901;&#24459;&#30701;&#35821;&#38901;&#24459;&#30701;&#35821;&#27169;&#22359;&#65288;PWPP&#65289;&#21644;&#23383;&#24418;&#21040;&#38899;&#32032;&#27169;&#22359;&#65288;G2P&#65289;&#32452;&#25104;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#33521;&#25991;&#25991;&#26412;&#21040;&#35821;&#38899;&#21069;&#31471;&#30340;&#30740;&#31350;&#20165;&#20851;&#27880;&#20110;&#21333;&#29420;&#27169;&#22359;&#65292;&#24573;&#30053;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#65292;&#23548;&#33268;&#27599;&#20010;&#27169;&#22359;&#24615;&#33021;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21069;&#31471;&#26694;&#26550;&#65292;&#25429;&#25417;&#33521;&#25991;&#25991;&#26412;&#21040;&#35821;&#38899;&#21069;&#31471;&#27169;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#27169;&#22359;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The front-end is a critical component of English text-to-speech (TTS) systems, responsible for extracting linguistic features that are essential for a text-to-speech model to synthesize speech, such as prosodies and phonemes. The English TTS front-end typically consists of a text normalization (TN) module, a prosody word prosody phrase (PWPP) module, and a grapheme-to-phoneme (G2P) module. However, current research on the English TTS front-end focuses solely on individual modules, neglecting the interdependence between them and resulting in sub-optimal performance for each module. Therefore, this paper proposes a unified front-end framework that captures the dependencies among the English TTS front-end modules. Extensive experiments have demonstrated that the proposed method achieves state-of-the-art (SOTA) performance in all modules.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32771;&#34385;&#20102;&#26102;&#38388;&#24615;&#23545;COVID-19&#30123;&#33495;&#24577;&#24230;&#26816;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#26102;&#38388;&#20998;&#21106;&#26174;&#33879;&#38477;&#20302;&#20102;&#31435;&#22330;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.04806</link><description>&lt;p&gt;
&#32771;&#23519;COVID-19&#30123;&#33495;&#24577;&#24230;&#26816;&#27979;&#20013;&#30340;&#26102;&#38388;&#24615;
&lt;/p&gt;
&lt;p&gt;
Examining Temporalities on Stance Detection Towards COVID-19 Vaccination. (arXiv:2304.04806v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04806
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32771;&#34385;&#20102;&#26102;&#38388;&#24615;&#23545;COVID-19&#30123;&#33495;&#24577;&#24230;&#26816;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#26102;&#38388;&#20998;&#21106;&#26174;&#33879;&#38477;&#20302;&#20102;&#31435;&#22330;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#25351;&#20986;&#30123;&#33495;&#25509;&#31181;&#26159;&#25511;&#21046;COVID-19&#20256;&#25773;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;&#20102;&#35299;&#20844;&#20247;&#30340;&#30123;&#33495;&#24577;&#24230;&#23545;&#20915;&#31574;&#32773;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340; COVID-19 &#30123;&#33495;&#24577;&#24230;&#65288;&#22914;&#25903;&#25345;&#21644;&#29369;&#35947;&#65289;&#20250;&#38543;&#26102;&#38388;&#32780;&#28436;&#21464;&#65292;&#22240;&#27492;&#22312;&#20998;&#26512;&#36825;&#20123;&#31435;&#22330;&#26102;&#38656;&#35201;&#32771;&#34385;&#21487;&#33021;&#30340;&#26102;&#38388;&#28418;&#31227;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#26816;&#26597;&#26102;&#38388;&#27010;&#24565;&#28418;&#31227;&#23545;&#25512;&#29305;&#19978; COVID-19 &#30123;&#33495;&#31435;&#22330;&#26816;&#27979;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#23545;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#36827;&#34892;&#20102;&#38543;&#26426;&#21644;&#26102;&#38388;&#20998;&#21106;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25152;&#26377;&#21333;&#35821;&#21644;&#22810;&#35821;&#25968;&#25454;&#38598;&#30340;&#38543;&#26426;&#21644;&#26102;&#38388;&#20998;&#21106;&#20043;&#38388;&#65292;&#27169;&#22411;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#26102;&#38388;&#20998;&#21106;&#26174;&#33879;&#38477;&#20302;&#20102;&#31435;&#22330;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous studies have highlighted the importance of vaccination as an effective strategy to control the transmission of the COVID-19 virus. It is crucial for policymakers to have a comprehensive understanding of the public's stance towards vaccination on a large scale. However, attitudes towards COVID-19 vaccination, such as pro-vaccine or vaccine hesitancy, have evolved over time on social media. Thus, it is necessary to account for possible temporal shifts when analysing these stances. This study aims to examine the impact of temporal concept drift on stance detection towards COVID-19 vaccination on Twitter. To this end, we evaluate a range of transformer-based models using chronological and random splits of social media data. Our findings demonstrate significant discrepancies in model performance when comparing random and chronological splits across all monolingual and multilingual datasets. Chronological splits significantly reduce the accuracy of stance classification. Therefore, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#19968;&#22871;&#35821;&#38899;&#21333;&#35789;&#23884;&#20837;&#21450;&#20854;&#30456;&#20851;&#20219;&#21153;&#65292;&#25552;&#39640;&#20102;&#35821;&#38899;&#20449;&#24687;&#22788;&#29702;&#30340;&#25928;&#26524;&#21644;&#21487;&#37325;&#22797;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.02541</link><description>&lt;p&gt;
PWESuite&#65306;&#35821;&#38899;&#21333;&#35789;&#23884;&#20837;&#21450;&#20854;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
PWESuite: Phonetic Word Embeddings and Tasks They Facilitate. (arXiv:2304.02541v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#19968;&#22871;&#35821;&#38899;&#21333;&#35789;&#23884;&#20837;&#21450;&#20854;&#30456;&#20851;&#20219;&#21153;&#65292;&#25552;&#39640;&#20102;&#35821;&#38899;&#20449;&#24687;&#22788;&#29702;&#30340;&#25928;&#26524;&#21644;&#21487;&#37325;&#22797;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#21333;&#35789;&#26144;&#23556;&#21040;&#22266;&#23450;&#32500;&#24230;&#30340;&#21521;&#37327;&#31354;&#38388;&#30340;&#21333;&#35789;&#23884;&#20837;&#26159;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#30784;&#12290;&#22823;&#22810;&#25968;&#21333;&#35789;&#23884;&#20837;&#26041;&#27861;&#32534;&#30721;&#35821;&#20041;&#20449;&#24687;&#12290;&#20294;&#26159;&#65292;&#23545;&#20110;&#26576;&#20123;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#30340;&#35821;&#38899;&#20449;&#24687;&#32463;&#24120;&#34987;&#24573;&#30053;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20960;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#21457;&#22768;&#29305;&#24449;&#26500;&#24314;&#35821;&#38899;&#30693;&#24773;&#21333;&#35789;&#23884;&#20837;&#65292;&#24182;&#25552;&#20379;&#19968;&#22871;&#35821;&#38899;&#21333;&#35789;&#23884;&#20837;&#20197;&#40723;&#21169;&#20854;&#31038;&#21306;&#30340;&#24320;&#21457;&#12289;&#35780;&#20272;&#21644;&#20351;&#29992;&#12290;&#34429;&#28982;&#24050;&#32463;&#23384;&#22312;&#35768;&#22810;&#23398;&#20064;&#35821;&#38899;&#21333;&#35789;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#26041;&#38754;&#32570;&#20047;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20960;&#31181;&#35780;&#20272;&#35821;&#38899;&#21333;&#35789;&#23884;&#20837;&#30340;&#20869;&#22312;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#22914;&#21333;&#35789;&#26816;&#32034;&#21644;&#19982;&#22768;&#38899;&#30456;&#20284;&#24615;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#22806;&#22312;&#34920;&#29616;&#65292;&#22914;&#38901;&#24459;&#21644;&#21516;&#28304;&#26816;&#27979;&#21644;&#22768;&#38899;&#31867;&#27604;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#20219;&#21153;&#22871;&#20214;&#23558;&#20419;&#36827;&#21487;&#37325;&#22797;&#24615;&#24182;&#25552;&#20379;&#26410;&#26469;&#35821;&#38899;&#21333;&#35789;&#23884;&#20837;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Word embeddings that map words into a fixed-dimensional vector space are the backbone of modern NLP. Most word embedding methods encode semantic information. However, phonetic information, which is important for some tasks, is often overlooked. In this work, we develop several novel methods which leverage articulatory features to build phonetically informed word embeddings, and present a set of phonetic word embeddings to encourage their community development, evaluation and use. While several methods for learning phonetic word embeddings already exist, there is a lack of consistency in evaluating their effectiveness. Thus, we also proposes several ways to evaluate both intrinsic aspects of phonetic word embeddings, such as word retrieval and correlation with sound similarity, and extrinsic performances, such as rhyme and cognate detection and sound analogies. We hope that our suite of tasks will promote reproducibility and provide direction for future research on phonetic word embeddi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36328;&#35821;&#35328;&#25925;&#20107;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#19981;&#21516;&#30340;&#25925;&#20107;&#35268;&#21010;&#12290;&#32467;&#26524;&#34920;&#26126;&#23558;&#25925;&#20107;&#20998;&#20026;&#19977;&#24149;&#21487;&#20197;&#24102;&#26469;&#26356;&#19968;&#33268;&#21644;&#26377;&#36259;&#30340;&#21465;&#36848;&#65292;&#21516;&#26102;&#20801;&#35768;&#26126;&#30830;&#25511;&#21046;&#20854;&#20869;&#23481;&#21644;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2212.10471</link><description>&lt;p&gt;
&#23567;&#32418;&#24125;&#29615;&#29699;&#26053;&#34892;&#65306;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#25925;&#20107;&#35268;&#21010;&#19982;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Little Red Riding Hood Goes Around the Globe:Crosslingual Story Planning and Generation with Large Language Models. (arXiv:2212.10471v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36328;&#35821;&#35328;&#25925;&#20107;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#19981;&#21516;&#30340;&#25925;&#20107;&#35268;&#21010;&#12290;&#32467;&#26524;&#34920;&#26126;&#23558;&#25925;&#20107;&#20998;&#20026;&#19977;&#24149;&#21487;&#20197;&#24102;&#26469;&#26356;&#19968;&#33268;&#21644;&#26377;&#36259;&#30340;&#21465;&#36848;&#65292;&#21516;&#26102;&#20801;&#35768;&#26126;&#30830;&#25511;&#21046;&#20854;&#20869;&#23481;&#21644;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#22312;&#21333;&#35821;&#29615;&#22659;&#19979;&#35268;&#21010;&#25925;&#20107;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#35821;&#19978;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#36328;&#35821;&#35328;&#30340;&#33258;&#21160;&#25925;&#20107;&#29983;&#25104;&#20013;&#65292;&#35268;&#21010;&#26159;&#21542;&#24102;&#26469;&#20102;&#20248;&#21183;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36328;&#35821;&#35328;&#25925;&#20107;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#20026;&#35813;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#23545;&#19981;&#21516;&#30340;&#25925;&#20107;&#35268;&#21010;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#24182;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#29983;&#25104;&#20102;&#25925;&#20107;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#25925;&#20107;&#32467;&#26500;&#21270;&#20026;&#19977;&#24149;&#30340;&#35268;&#21010;&#21487;&#20197;&#24102;&#26469;&#26356;&#19968;&#33268;&#21644;&#26377;&#36259;&#30340;&#21465;&#36848;&#65292;&#21516;&#26102;&#20801;&#35768;&#26126;&#30830;&#25511;&#21046;&#20854;&#20869;&#23481;&#21644;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous work has demonstrated the effectiveness of planning for story generation exclusively in a monolingual setting focusing primarily on English. We consider whether planning brings advantages to automatic story generation across languages. We propose a new task of cross-lingual story generation with planning and present a new dataset for this task. We conduct a comprehensive study of different plans and generate stories in several languages, by leveraging the creative and reasoning capabilities of large pre-trained language models. Our results demonstrate that plans which structure stories into three acts lead to more coherent and interesting narratives, while allowing to explicitly control their content and structure.
&lt;/p&gt;</description></item></channel></rss>