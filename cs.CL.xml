<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#23545;&#35805;&#24773;&#22659;&#19979;&#30340;&#27169;&#22411;&#36827;&#34892;&#37325;&#25490;&#24207;&#30340;&#26041;&#27861;&#65292;&#24110;&#21161;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32553;&#23567;&#35757;&#32451;&#21644;&#25512;&#26029;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20197;&#25913;&#36827;&#20154;&#24037;&#26234;&#33021;&#21307;&#23398;&#21490;&#37319;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.01974</link><description>&lt;p&gt;
&#21307;&#23398;&#21490;&#37319;&#38598;&#30340;&#23545;&#35805;&#24773;&#22659;&#37325;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Dialogue-Contextualized Re-ranking for Medical History-Taking. (arXiv:2304.01974v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#23545;&#35805;&#24773;&#22659;&#19979;&#30340;&#27169;&#22411;&#36827;&#34892;&#37325;&#25490;&#24207;&#30340;&#26041;&#27861;&#65292;&#24110;&#21161;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32553;&#23567;&#35757;&#32451;&#21644;&#25512;&#26029;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20197;&#25913;&#36827;&#20154;&#24037;&#26234;&#33021;&#21307;&#23398;&#21490;&#37319;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21307;&#23398;&#21490;&#37319;&#38598;&#26159;&#30151;&#29366;&#26816;&#26597;&#12289;&#33258;&#21160;&#24739;&#32773;&#25509;&#24453;&#12289;&#20998;&#35786;&#21644;&#20854;&#20182;&#20154;&#24037;&#26234;&#33021;&#34394;&#25311;&#25252;&#29702;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#30001;&#20110;&#30149;&#21490;&#37319;&#38598;&#26041;&#24335;&#30340;&#22810;&#26679;&#24615;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#29616;&#26377;&#30340;&#31995;&#32479;&#20351;&#29992;&#38388;&#25509;&#25968;&#25454;&#25110;&#19987;&#23478;&#30693;&#35782;&#36827;&#34892;&#24320;&#21457;&#12290;&#36825;&#23548;&#33268;&#20102;&#35757;&#32451;&#21644;&#25512;&#26029;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22240;&#20026;&#27169;&#22411;&#26159;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#22312;&#25512;&#26029;&#26102;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#37325;&#25490;&#24207;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#35805;&#24773;&#22659;&#19979;&#30340;&#27169;&#22411;&#37325;&#26032;&#23545;&#31532;&#19968;&#38454;&#27573;&#30340;&#38382;&#39064;&#20505;&#36873;&#32773;&#36827;&#34892;&#25490;&#24207;&#65292;&#24110;&#21161;&#32553;&#23567;&#35757;&#32451;&#21644;&#25512;&#26029;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#8212;&#8212;&#20840;&#23616;&#37325;&#25490;&#24207;&#22120;&#65292;&#35813;&#27169;&#22411;&#21516;&#26102;&#23558;&#25152;&#26377;&#38382;&#39064;&#19982;&#23545;&#35805;&#36827;&#34892;&#20132;&#21449;&#32534;&#30721;&#65292;&#24182;&#23558;&#20854;&#19982;&#20960;&#31181;&#29616;&#26377;&#30340;&#31070;&#32463;&#32447;&#36335;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;transformer&#21644;S4&#35821;&#35328;&#27169;&#22411;&#32972;&#26223;&#19979;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30456;&#23545;&#20110;&#19987;&#23478;&#31995;&#32479;&#65292;&#26368;&#20339;&#34920;&#29616;&#26159;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI-driven medical history-taking is an important component in symptom checking, automated patient intake, triage, and other AI virtual care applications. As history-taking is extremely varied, machine learning models require a significant amount of data to train. To overcome this challenge, existing systems are developed using indirect data or expert knowledge. This leads to a training-inference gap as models are trained on different kinds of data than what they observe at inference time. In this work, we present a two-stage re-ranking approach that helps close the training-inference gap by re-ranking the first-stage question candidates using a dialogue-contextualized model. For this, we propose a new model, global re-ranker, which cross-encodes the dialogue with all questions simultaneously, and compare it with several existing neural baselines. We test both transformer and S4-based language model backbones. We find that relative to the expert system, the best performance is achieved 
&lt;/p&gt;</description></item><item><title>MEGClass&#26159;&#19968;&#31181;&#36890;&#36807;&#30456;&#20114;&#22686;&#24378;&#30340;&#25991;&#26412;&#31890;&#24230;&#23454;&#29616;&#26497;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#23618;&#20851;&#27880;&#26426;&#21046;&#20174;&#25991;&#26723;&#12289;&#21477;&#23376;&#21644;&#21333;&#35789;&#20013;&#25552;&#21462;&#36830;&#36143;&#19988;&#22810;&#26679;&#30340;&#23376;&#25991;&#26412;&#65292;&#33021;&#22815;&#20934;&#30830;&#20998;&#31867;&#35752;&#35770;&#22810;&#20010;&#20027;&#39064;&#30340;&#25991;&#26723;&#65292;&#24182;&#19988;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.01969</link><description>&lt;p&gt;
MEGClass: &#36890;&#36807;&#30456;&#20114;&#22686;&#24378;&#30340;&#25991;&#26412;&#31890;&#24230;&#23454;&#29616;&#26497;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
MEGClass: Text Classification with Extremely Weak Supervision via Mutually-Enhancing Text Granularities. (arXiv:2304.01969v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01969
&lt;/p&gt;
&lt;p&gt;
MEGClass&#26159;&#19968;&#31181;&#36890;&#36807;&#30456;&#20114;&#22686;&#24378;&#30340;&#25991;&#26412;&#31890;&#24230;&#23454;&#29616;&#26497;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#23618;&#20851;&#27880;&#26426;&#21046;&#20174;&#25991;&#26723;&#12289;&#21477;&#23376;&#21644;&#21333;&#35789;&#20013;&#25552;&#21462;&#36830;&#36143;&#19988;&#22810;&#26679;&#30340;&#23376;&#25991;&#26412;&#65292;&#33021;&#22815;&#20934;&#30830;&#20998;&#31867;&#35752;&#35770;&#22810;&#20010;&#20027;&#39064;&#30340;&#25991;&#26723;&#65292;&#24182;&#19988;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#20316;&#20026;&#30417;&#30563;&#65292;&#36825;&#22312;&#21160;&#24577;&#26032;&#20852;&#39046;&#22495;&#20013;&#26159;&#26114;&#36149;&#30340;&#12290;&#26576;&#20123;&#26041;&#27861;&#36890;&#36807;&#20165;&#20381;&#36182;&#31867;&#21517;&#34920;&#38754;&#25991;&#26412;&#20316;&#20026;&#26497;&#24369;&#30417;&#30563;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#32771;&#34385;&#21040;&#21333;&#19968;&#31867;&#21035;&#25991;&#26723;&#35752;&#35770;&#22810;&#20010;&#20027;&#39064;&#30340;&#24773;&#20917;&#12290;&#20027;&#39064;&#22810;&#26679;&#24615;&#21644;&#27169;&#31946;&#30340;&#21477;&#23376;&#21487;&#33021;&#20250;&#24341;&#20837;&#22122;&#22768;&#21040;&#25991;&#26723;&#30340;&#24213;&#23618;&#34920;&#31034;&#65292;&#20174;&#32780;&#24433;&#21709;&#39044;&#27979;&#31867;&#21035;&#30340;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#29420;&#31435;&#22320;&#20851;&#27880;&#25991;&#26723;&#12289;&#21477;&#23376;&#25110;&#21333;&#35789;&#30340;&#25991;&#26412;&#31890;&#24230;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#25105;&#20204;&#32852;&#21512;&#20174;&#25152;&#26377;&#19977;&#32773;&#20013;&#25552;&#21462;&#31895;&#31890;&#24230;&#25110;&#32454;&#31890;&#24230;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#26469;&#35782;&#21035;&#20998;&#31867;&#30340;&#37325;&#35201;&#23376;&#25991;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MEGClass&#65292;&#19968;&#31181;&#21033;&#29992;&#30456;&#20114;&#22686;&#24378;&#30340;&#25991;&#26412;&#31890;&#24230;&#36827;&#34892;&#26497;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;MEGClass&#36890;&#36807;&#20998;&#23618;&#20851;&#27880;&#26426;&#21046;&#20174;&#25991;&#26723;&#12289;&#21477;&#23376;&#21644;&#21333;&#35789;&#20013;&#25552;&#21462;&#36830;&#36143;&#19988;&#22810;&#26679;&#30340;&#23376;&#25991;&#26412;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#21644;&#25972;&#21512;&#26469;&#33258;&#22810;&#20010;&#31890;&#24230;&#30340;&#24369;&#20449;&#21495;&#65292;&#20197;&#20934;&#30830;&#20998;&#31867;&#25991;&#26723;&#65292;&#21363;&#20351;&#23427;&#20204;&#35752;&#35770;&#22810;&#20010;&#20027;&#39064;&#12290;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#26497;&#24369;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26377;&#30528;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text classification typically requires a substantial amount of human-annotated data to serve as supervision, which is costly to obtain in dynamic emerging domains. Certain methods seek to address this problem by solely relying on the surface text of class names to serve as extremely weak supervision. However, existing methods fail to account for single-class documents discussing multiple topics. Both topic diversity and vague sentences may introduce noise into the document's underlying representation and consequently the precision of the predicted class. Furthermore, current work focuses on text granularities (documents, sentences, or words) independently, which limits the degree of coarse- or fine-grained context that we can jointly extract from all three to identify significant subtext for classification. In order to address this problem, we propose MEGClass, an extremely weakly-supervised text classification method to exploit Mutually-Enhancing Text Granularities. Specifically, MEGC
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25903;&#25345;&#22810;&#23186;&#20307;&#20869;&#23481;&#21019;&#24314;&#30340;&#22270;&#20687;/&#25991;&#26412;&#26816;&#32034;&#27979;&#35797;&#38598;&#8212;&#8212;AToMiC&#65292;&#23427;&#20351;&#29992;&#20102;&#32500;&#22522;&#30334;&#31185;&#20013;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26723;&#20851;&#32852;&#65292;&#24182;&#24314;&#31435;&#20102;&#22810;&#26679;&#30340;&#39046;&#22495;&#25991;&#26412;&#21644;&#22270;&#29255;&#12290;AToMiC &#20026;&#21487;&#25193;&#23637;&#12289;&#22810;&#26679;&#21270;&#12289;&#21487;&#22797;&#29616;&#30340;&#22810;&#23186;&#20307;&#26816;&#32034;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2304.01961</link><description>&lt;p&gt;
AToMiC&#65306;&#25903;&#25345;&#22810;&#23186;&#20307;&#20869;&#23481;&#21019;&#24314;&#30340;&#22270;&#20687;/&#25991;&#26412;&#26816;&#32034;&#27979;&#35797;&#38598;
&lt;/p&gt;
&lt;p&gt;
AToMiC: An Image/Text Retrieval Test Collection to Support Multimedia Content Creation. (arXiv:2304.01961v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25903;&#25345;&#22810;&#23186;&#20307;&#20869;&#23481;&#21019;&#24314;&#30340;&#22270;&#20687;/&#25991;&#26412;&#26816;&#32034;&#27979;&#35797;&#38598;&#8212;&#8212;AToMiC&#65292;&#23427;&#20351;&#29992;&#20102;&#32500;&#22522;&#30334;&#31185;&#20013;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26723;&#20851;&#32852;&#65292;&#24182;&#24314;&#31435;&#20102;&#22810;&#26679;&#30340;&#39046;&#22495;&#25991;&#26412;&#21644;&#22270;&#29255;&#12290;AToMiC &#20026;&#21487;&#25193;&#23637;&#12289;&#22810;&#26679;&#21270;&#12289;&#21487;&#22797;&#29616;&#30340;&#22810;&#23186;&#20307;&#26816;&#32034;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;AToMiC&#65288;&#22810;&#23186;&#20307;&#20869;&#23481;&#21019;&#20316;&#24037;&#20855;&#65289;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25512;&#21160;&#22270;&#20687;/&#25991;&#26412;&#36328;&#27169;&#24577;&#26816;&#32034;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;&#34429;&#28982;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#32463;&#22312;&#25552;&#39640;&#26816;&#32034;&#25928;&#26524;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#30740;&#31350;&#20173;&#20381;&#36182;&#20110;&#20165;&#20855;&#26377;&#31616;&#21333;&#22270;&#20687;-&#25991;&#26412;&#20851;&#31995;&#21644;&#26816;&#32034;&#20219;&#21153;&#29992;&#25143;&#27169;&#22411;&#19981;&#36275;&#30340;&#22270;&#20687;&#26631;&#39064;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20123;&#36807;&#24230;&#31616;&#21270;&#30340;&#35774;&#32622;&#21644;&#22810;&#23186;&#20307;&#20869;&#23481;&#21019;&#24314;&#30340;&#30495;&#23454;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26500;&#24314;&#26816;&#32034;&#27979;&#35797;&#38598;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#20013;&#23884;&#20837;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26723;&#20851;&#32852;&#65292;&#24314;&#31435;&#20102;&#21253;&#25324;&#20998;&#23618;&#32467;&#26500;&#12289;&#25991;&#26412;&#26679;&#24335;&#21644;&#31867;&#22411;&#22312;&#20869;&#30340;&#22810;&#26679;&#21270;&#39046;&#22495;&#30340;&#25991;&#26412;&#21644;&#22270;&#29255;&#12290;&#25105;&#20204;&#22522;&#20110;&#19968;&#20010;&#29616;&#23454;&#30340;&#29992;&#25143;&#27169;&#22411;&#21046;&#23450;&#20102;&#20004;&#20010;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#22522;&#32447;&#27169;&#22411;&#30340;&#26816;&#32034;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;AToMiC&#20026;&#21487;&#25193;&#23637;&#12289;&#22810;&#26679;&#21270;&#12289;&#21487;&#22797;&#29616;&#30340;&#22810;&#23186;&#20307;&#26816;&#32034;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the AToMiC (Authoring Tools for Multimedia Content) dataset, designed to advance research in image/text cross-modal retrieval. While vision-language pretrained transformers have led to significant improvements in retrieval effectiveness, existing research has relied on image-caption datasets that feature only simplistic image-text relationships and underspecified user models of retrieval tasks. To address the gap between these oversimplified settings and real-world applications for multimedia content creation, we introduce a new approach for building retrieval test collections. We leverage hierarchical structures and diverse domains of texts, styles, and types of images, as well as large-scale image-document associations embedded in Wikipedia. We formulate two tasks based on a realistic user model and validate our dataset through retrieval experiments using baseline models. AToMiC offers a testbed for scalable, diverse, and reproducible multimedia retrieval research
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#25918;&#23556;&#32959;&#30244;&#29289;&#29702;&#38382;&#39064;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#21457;&#29616;ChatGPT&#65288;GPT-4&#65289;&#30340;&#34920;&#29616;&#26368;&#22909;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;LLMs&#27169;&#22411;&#25913;&#36827;&#21644;&#38598;&#25104;&#31574;&#30053;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.01938</link><description>&lt;p&gt;
&#22312;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#25918;&#23556;&#32959;&#30244;&#29289;&#29702;&#23398;&#39046;&#22495;&#20013;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models on a Highly-specialized Topic, Radiation Oncology Physics. (arXiv:2304.01938v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#25918;&#23556;&#32959;&#30244;&#29289;&#29702;&#38382;&#39064;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#21457;&#29616;ChatGPT&#65288;GPT-4&#65289;&#30340;&#34920;&#29616;&#26368;&#22909;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;LLMs&#27169;&#22411;&#25913;&#36827;&#21644;&#38598;&#25104;&#31574;&#30053;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#25918;&#23556;&#32959;&#30244;&#29289;&#29702;&#38382;&#39064;&#26041;&#38754;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#26159;&#31532;&#19968;&#39033;&#38024;&#23545;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#30001;100&#20010;&#25918;&#23556;&#32959;&#30244;&#29289;&#29702;&#23398;&#38382;&#39064;&#32452;&#25104;&#30340;&#32771;&#35797;&#65292;&#24182;&#20351;&#29992;&#22235;&#31181;LLMs&#21644;&#21307;&#23398;&#29289;&#29702;&#23398;&#30340;&#19987;&#23478;&#21644;&#38750;&#19987;&#23478;&#36827;&#34892;&#35780;&#20272;&#12290;ChatGPT&#65288;GPT-4&#65289;&#24179;&#22343;&#32780;&#35328;&#34920;&#29616;&#26368;&#22909;&#65292;&#36229;&#36807;&#20102;&#25152;&#26377;&#20854;&#20182;LLMs&#20197;&#21450;&#21307;&#30103;&#29289;&#29702;&#23398;&#23478;&#30340;&#34920;&#29616;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#20063;&#25552;&#20986;&#20102;&#19968;&#20123;LLMs&#27169;&#22411;&#25913;&#36827;&#21644;&#38598;&#25104;&#31574;&#30053;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the first study to investigate Large Language Models (LLMs) in answering radiation oncology physics questions. Because popular exams like AP Physics, LSAT, and GRE have large test-taker populations and ample test preparation resources in circulation, they may not allow for accurately assessing the true potential of LLMs. This paper proposes evaluating LLMs on a highly-specialized topic, radiation oncology physics, which may be more pertinent to scientific and medical communities in addition to being a valuable benchmark of LLMs. We developed an exam consisting of 100 radiation oncology physics questions based on our expertise at Mayo Clinic. Four LLMs, ChatGPT (GPT-3.5), ChatGPT (GPT-4), Bard (LaMDA), and BLOOMZ, were evaluated against medical physicists and non-experts. ChatGPT (GPT-4) outperformed all other LLMs as well as medical physicists, on average. The performance of ChatGPT (GPT-4) was further improved when prompted to explain first, then answer. ChatGPT (GPT-3.5 an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;LLM-Adapters&#65292;&#19968;&#20010;&#23558;&#36866;&#37197;&#22120;&#38598;&#25104;&#21040;LLMs&#20013;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#35757;&#32451;&#26102;&#38388;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.01933</link><description>&lt;p&gt;
LLM-Adapters&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#36866;&#37197;&#22120;&#31995;&#21015;
&lt;/p&gt;
&lt;p&gt;
LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models. (arXiv:2304.01933v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LLM-Adapters&#65292;&#19968;&#20010;&#23558;&#36866;&#37197;&#22120;&#38598;&#25104;&#21040;LLMs&#20013;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#35757;&#32451;&#26102;&#38388;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;GPT-3&#21644;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#65292;&#23548;&#33268;&#20102;&#35768;&#22810;&#32463;&#27982;&#23454;&#24800;&#21644;&#26131;&#20110;&#35775;&#38382;&#30340;&#26367;&#20195;&#21697;&#30340;&#24320;&#21457;&#65292;&#36825;&#20123;&#26367;&#20195;&#21697;&#26159;&#36890;&#36807;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#65288;&#20363;&#22914;ChatDoctor&#65289;&#25110;&#25351;&#23548;&#25968;&#25454;&#65288;&#20363;&#22914;Alpaca&#65289;&#24494;&#35843;&#24320;&#25918;&#24335;LLMs&#32780;&#21019;&#24314;&#30340;&#12290;&#22312;&#21508;&#31181;&#24494;&#35843;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;&#36866;&#37197;&#22120;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26080;&#30097;&#26159;&#26368;&#26377;&#21560;&#24341;&#21147;&#30340;&#30740;&#31350;&#20027;&#39064;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#24494;&#35843;&#23569;&#37327;&#22806;&#37096;&#21442;&#25968;&#32780;&#19981;&#26159;&#25972;&#20010;LLMs&#65292;&#21516;&#26102;&#23454;&#29616;&#21487;&#27604;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;LLMs&#30340;PEFT&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;LLM-Adapters&#65292;&#36825;&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#65292;&#23558;&#21508;&#31181;&#36866;&#37197;&#22120;&#38598;&#25104;&#21040;LLMs&#20013;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#19981;&#21516;&#20219;&#21153;&#25191;&#34892;&#36825;&#20123;&#36866;&#37197;&#22120;&#30340;PEFT&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;&#24335;LLMs&#65292;&#20363;&#22914;LLaMA&#65292;BLOOM&#65292;OPT&#21644;GPT-J&#65292;&#20197;&#21450;&#24191;&#27867;&#20351;&#29992;&#30340;&#36866;&#37197;&#22120;&#65292;&#20363;&#22914;&#20018;&#32852;&#36866;&#37197;&#22120;&#65292;&#24182;&#32852;&#36866;&#37197;&#22120;&#21644;LoRA&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#39640;&#25928;&#19988;&#28789;&#27963;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#26368;&#23569;&#30340;&#38468;&#21152;&#35757;&#32451;&#25968;&#25454;&#25110;&#35745;&#31639;&#36164;&#28304;&#36731;&#26494;&#24494;&#35843;LLMs&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#23545;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LLM-Adapters&#21487;&#20197;&#27604;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#35757;&#32451;&#26102;&#38388;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of large language models (LLMs), like GPT-3 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by fine-tuning open-access LLMs with task-specific data (e.g., ChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning methods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly one of the most attractive topics, as it only requires fine-tuning a few external parameters instead of the entire LLMs while achieving comparable or even better performance. To enable further research on PEFT methods of LLMs, this paper presents LLM-Adapters, an easy-to-use framework that integrates various adapters into LLMs and can execute these adapter-based PEFT methods of LLMs for different tasks. The framework includes state-of-the-art open-access LLMs such as LLaMA, BLOOM, OPT, and GPT-J, as well as widely used adapters such as Series adapter, Parallel adapter, and LoRA. The framework is designed to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#25910;&#38598;&#19968;&#20123;&#22522;&#30784;&#35774;&#26045;&#65292;&#29992;&#20110;&#22312;&#26031;&#25289;&#22827;&#35821;&#35328;&#29615;&#22659;&#20013;&#36827;&#34892;ICL&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#19968;&#32452;&#26368;&#26032;&#30340;&#22659;&#20013;&#23398;&#20064;&#22120;&#65292;&#24182;&#23558;&#20854;&#24615;&#33021;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2304.01922</link><description>&lt;p&gt;
&#36164;&#28304;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#22312;&#26031;&#25289;&#22827;&#35821;&#22659;&#19979;&#23398;&#20064;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Resources and Few-shot Learners for In-context Learning in Slavic Languages. (arXiv:2304.01922v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01922
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#25910;&#38598;&#19968;&#20123;&#22522;&#30784;&#35774;&#26045;&#65292;&#29992;&#20110;&#22312;&#26031;&#25289;&#22827;&#35821;&#35328;&#29615;&#22659;&#20013;&#36827;&#34892;ICL&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#19968;&#32452;&#26368;&#26032;&#30340;&#22659;&#20013;&#23398;&#20064;&#22120;&#65292;&#24182;&#23558;&#20854;&#24615;&#33021;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#21019;&#24314;&#20102;&#20934;&#30830;&#19988;&#32039;&#20945;&#30340;&#22659;&#20013;&#23398;&#20064;&#22120;&#65292;&#22312;&#22330;&#26223;&#23398;&#20064; (ICL) &#30340;&#24037;&#20316;&#22823;&#22810;&#38598;&#20013;&#22312;&#33521;&#25991;&#20219;&#21153;&#19978;&#12290;&#28982;&#32780;&#65292;&#19982;&#20351;&#29992;&#33521;&#35821;&#20197;&#22806;&#35821;&#35328;&#30340;&#29992;&#25143;&#20114;&#21160;&#30340;&#33021;&#21147;&#20026;&#23558;&#35821;&#35328;&#25216;&#26415;&#24212;&#29992;&#20110;&#38750;&#33521;&#35821;&#20351;&#29992;&#32773;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20123;&#22522;&#30784;&#35774;&#26045;&#65292;&#29992;&#20110;&#22312;&#25463;&#20811;&#35821;&#12289;&#27874;&#20848;&#35821;&#21644;&#20420;&#35821;&#31561;&#22810;&#31181;&#26031;&#25289;&#22827;&#35821;&#35328;&#20013;&#36827;&#34892;ICL&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#36716;&#25442;&#21644;&#32431;&#30446;&#26631;&#35821;&#35328;&#32534;&#20889;&#30340;&#26032;&#27169;&#26495;&#65292;&#23558;&#21508;&#31181;&#21508;&#26679;&#30340;&#25968;&#25454;&#38598;&#36830;&#25509;&#25104;&#19968;&#20010;&#32479;&#19968;&#30340;&#25945;&#23398;&#26684;&#24335;&#12290;&#21033;&#29992;&#26032;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#32452;&#26368;&#26032;&#30340;&#22659;&#20013;&#23398;&#20064;&#22120;&#65292;&#24182;&#23558;&#20854;&#32467;&#26524;&#19982;&#30417;&#30563;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#25910;&#38598;&#30340;&#36164;&#28304;&#35757;&#32451;&#12289;&#35780;&#20272;&#21644;&#21457;&#24067;&#20102;&#19968;&#32452;&#22659;&#20013;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#24615;&#33021;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the rapid recent progress in creating accurate and compact in-context learners, most recent work focuses on in-context learning (ICL) for tasks in English. However, the ability to interact with users of languages outside English presents a great potential for broadening the applicability of language technologies to non-English speakers.  In this work, we collect the infrastructure necessary for training and evaluation of ICL in a selection of Slavic languages: Czech, Polish, and Russian. We link a diverse set of datasets and cast these into a unified instructional format through a set of transformations and newly-crafted templates written purely in target languages. Using the newly-curated dataset, we evaluate a set of the most recent in-context learners and compare their results to the supervised baselines. Finally, we train, evaluate and publish a set of in-context learning models that we train on the collected resources and compare their performance to previous work.  We fin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#21452;&#20851;&#27880;&#31070;&#32463;&#21464;&#25442;&#22120;&#8221;&#65292;&#21487;&#20197;&#36890;&#36807;&#20248;&#21270;&#21796;&#37266;&#35789;&#26816;&#27979;&#26469;&#36873;&#25321;&#35745;&#31639;&#36335;&#24452;&#65292;&#20174;&#32780;&#25552;&#39640;&#21796;&#37266;&#35789;&#30340;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#26102;&#38388;&#25928;&#29575;&#65292;&#24182;&#19988;&#35745;&#31639;&#25104;&#26412;&#21487;&#20197;&#38477;&#20302;90&#65285;&#32780;&#20165;&#22686;&#21152;1&#65285;&#30340;&#21442;&#25968;&#12290;&#36825;&#31181;&#26550;&#26500;&#21487;&#20197;&#22312;&#35821;&#38899;&#35782;&#21035;&#39046;&#22495;&#20013;&#22823;&#26377;&#35048;&#30410;&#12290;</title><link>http://arxiv.org/abs/2304.01905</link><description>&lt;p&gt;
&#21452;&#20851;&#27880;&#31070;&#32463;&#21464;&#25442;&#22120;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#26102;&#30340;&#39640;&#25928;&#21796;&#37266;&#35789;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Dual-Attention Neural Transducers for Efficient Wake Word Spotting in Speech Recognition. (arXiv:2304.01905v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#21452;&#20851;&#27880;&#31070;&#32463;&#21464;&#25442;&#22120;&#8221;&#65292;&#21487;&#20197;&#36890;&#36807;&#20248;&#21270;&#21796;&#37266;&#35789;&#26816;&#27979;&#26469;&#36873;&#25321;&#35745;&#31639;&#36335;&#24452;&#65292;&#20174;&#32780;&#25552;&#39640;&#21796;&#37266;&#35789;&#30340;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#26102;&#38388;&#25928;&#29575;&#65292;&#24182;&#19988;&#35745;&#31639;&#25104;&#26412;&#21487;&#20197;&#38477;&#20302;90&#65285;&#32780;&#20165;&#22686;&#21152;1&#65285;&#30340;&#21442;&#25968;&#12290;&#36825;&#31181;&#26550;&#26500;&#21487;&#20197;&#22312;&#35821;&#38899;&#35782;&#21035;&#39046;&#22495;&#20013;&#22823;&#26377;&#35048;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21452;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#65292;&#26088;&#22312;&#25552;&#39640;&#21796;&#37266;&#35789;&#35782;&#21035;&#30340;&#20934;&#30830;&#29575;&#24182;&#25913;&#21892;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#30340;&#25512;&#29702;&#26102;&#38388;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#21033;&#29992;&#21796;&#37266;&#35789;&#26816;&#27979;&#26469;&#36873;&#25321;&#21738;&#20010;&#27880;&#24847;&#21147;&#32593;&#32476;&#25191;&#34892;&#36755;&#20837;&#38899;&#39057;&#24103;&#30340;&#36816;&#34892;&#26102;&#35745;&#31639;&#36335;&#24452;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#20316;&#32773;&#26377;&#25928;&#25552;&#39640;&#20102;&#21796;&#37266;&#35789;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#23450;&#20041;&#20102;&#28014;&#28857;&#36816;&#31639;&#65288;FLOPs&#65289;&#30340;&#36816;&#34892;&#26102;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#20351;&#29992;&#20316;&#32773;&#30340;&#20869;&#37096;&#25968;&#25454;&#38598;&#26102;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#21452;&#20851;&#27880;&#32593;&#32476;&#21487;&#20197;&#23558;&#21796;&#37266;&#35789;&#38899;&#39057;&#24103;&#30340;&#35745;&#31639;&#25104;&#26412;&#38477;&#20302;$90\%$&#65292;&#32780;&#21442;&#25968;&#25968;&#37327;&#20165;&#22686;&#21152;$1\%$&#12290;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#35813;&#26550;&#26500;&#25552;&#39640;&#20102;&#21796;&#37266;&#35789;F1&#24471;&#20998;$16\%$&#65292;&#24182;&#23558;&#19968;&#33324;&#30340;&#32597;&#35265;&#35789;&#38169;&#35823;&#29575;&#25552;&#39640;&#20102;$3\%$&#12290;
&lt;/p&gt;
&lt;p&gt;
We present dual-attention neural biasing, an architecture designed to boost Wake Words (WW) recognition and improve inference time latency on speech recognition tasks. This architecture enables a dynamic switch for its runtime compute paths by exploiting WW spotting to select which branch of its attention networks to execute for an input audio frame. With this approach, we effectively improve WW spotting accuracy while saving runtime compute cost as defined by floating point operations (FLOPs). Using an in-house de-identified dataset, we demonstrate that the proposed dual-attention network can reduce the compute cost by $90\%$ for WW audio frames, with only $1\%$ increase in the number of parameters. This architecture improves WW F1 score by $16\%$ relative and improves generic rare word error rate by $3\%$ relative compared to the baselines.
&lt;/p&gt;</description></item><item><title>REFINER &#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#20197;&#26174;&#24335;&#29983;&#25104;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#65292;&#24182;&#19982;&#25552;&#20379;&#33258;&#21160;&#21453;&#39304;&#30340;&#25209;&#21028;&#27169;&#22411;&#20132;&#20114;&#65292;&#20854;&#22312;&#19977;&#20010;&#19981;&#21516;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#30528;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2304.01904</link><description>&lt;p&gt;
REFINER: &#22522;&#20110;&#20013;&#38388;&#34920;&#31034;&#30340;&#25512;&#29702;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
REFINER: Reasoning Feedback on Intermediate Representations. (arXiv:2304.01904v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01904
&lt;/p&gt;
&lt;p&gt;
REFINER &#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#20197;&#26174;&#24335;&#29983;&#25104;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#65292;&#24182;&#19982;&#25552;&#20379;&#33258;&#21160;&#21453;&#39304;&#30340;&#25209;&#21028;&#27169;&#22411;&#20132;&#20114;&#65292;&#20854;&#22312;&#19977;&#20010;&#19981;&#21516;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;remarkable&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#26174;&#24335;&#29983;&#25104;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#65292;&#20363;&#22914;&#38142;&#24335;&#24605;&#32771;&#25552;&#31034;&#31561;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#21487;&#33021;&#24182;&#19981;&#26159;&#26681;&#25454;&#21021;&#22987;&#19978;&#19979;&#25991;&#24471;&#20986;&#30340;&#36866;&#24403;&#25512;&#23548;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#27491;&#30830;&#30340;&#26368;&#32456;&#39044;&#27979;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;REFINER&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#20197;&#26174;&#24335;&#29983;&#25104;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#65292;&#24182;&#19982;&#25552;&#20379;&#33258;&#21160;&#21453;&#39304;&#30340;&#25209;&#21028;&#27169;&#22411;&#20132;&#20114;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25209;&#35780;&#23478;&#25552;&#20379;&#20102;&#32467;&#26500;&#21270;&#21453;&#39304;&#65292;&#25512;&#29702;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#23427;&#26469;&#36845;&#20195;&#25913;&#36827;&#20854;&#20013;&#38388;&#21442;&#25968;&#12290;REFINER&#30340;&#19977;&#20010;&#19981;&#21516;&#25512;&#29702;&#20219;&#21153;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#20986;&#20102;&#19982;&#22522;&#32447;&#20855;&#26377;&#21487;&#27604;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#30340;&#26174;&#30528;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#24403;&#20351;&#29992;GPT3.5&#20316;&#20026;&#25512;&#29702;&#22120;&#26102;&#65292;&#32463;&#36807;&#35757;&#32451;&#30340;&#25209;&#35780;&#23478;&#26174;&#30528;&#25913;&#21892;&#20102;&#25512;&#29702;&#32780;&#26080;&#38656;&#24494;&#35843;&#25512;&#29702;&#22120;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#25209;&#35780;&#27169;&#22411;&#26159;&#22312;&#27809;&#26377;&#26114;&#36149;&#30340;&#20154;&#31867;&#21442;&#19982;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#30340;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#23545;&#26032;&#39062;&#19978;&#19979;&#25991;&#25552;&#20379;&#20302;&#25104;&#26412;&#21453;&#39304;&#36827;&#34892;&#32487;&#32493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have recently shown remarkable performance on reasoning tasks by explicitly generating intermediate inferences, e.g., chain-of-thought prompting. However, these intermediate inference steps may be inappropriate deductions from the initial context and lead to incorrect final predictions. Here we introduce REFINER, a framework for finetuning LMs to explicitly generate intermediate reasoning steps while interacting with a critic model that provides automated feedback on the reasoning. Specifically, the critic provides structured feedback that the reasoning LM uses to iteratively improve its intermediate arguments. Empirical evaluations of REFINER on three diverse reasoning tasks show significant improvements over baseline LMs of comparable scale. Furthermore, when using GPT3.5 as the reasoner, the trained critic significantly improves reasoning without finetuning the reasoner. Finally, our critic model is trained without expensive human-in-the-loop data but can be su
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;BERT&#21450;&#20854;&#21464;&#31181;&#20026;&#26805;&#35821;&#25991;&#29486;&#24320;&#21457;&#20102;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#25552;&#21462;&#25688;&#35201;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#20844;&#24320;&#21457;&#24067;&#20102;&#19968;&#20010;&#26805;&#35821;&#22825;&#22478;&#25991;&#26412;&#35821;&#26009;&#24211;&#12290;</title><link>http://arxiv.org/abs/2304.01894</link><description>&lt;p&gt;
&#20351;&#29992;BERT&#21450;&#20854;&#21464;&#31181;&#30340;&#26805;&#35821;&#25991;&#29486;&#25688;&#35201;&#25552;&#21462;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
San-BERT: Extractive Summarization for Sanskrit Documents using BERT and it's variants. (arXiv:2304.01894v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;BERT&#21450;&#20854;&#21464;&#31181;&#20026;&#26805;&#35821;&#25991;&#29486;&#24320;&#21457;&#20102;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#25552;&#21462;&#25688;&#35201;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#20844;&#24320;&#21457;&#24067;&#20102;&#19968;&#20010;&#26805;&#35821;&#22825;&#22478;&#25991;&#26412;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#38024;&#23545;&#26805;&#35821;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;&#21464;&#24418;&#37329;&#21018;&#65288;BERT&#65289;&#21450;&#20854;&#21464;&#20307;&#65306;&#36731;&#37327;&#32423;BERT&#65288;ALBERT&#65289;&#21644;&#40065;&#26834;&#20248;&#21270;BERT&#65288;RoBERTa&#65289;&#65292;&#21033;&#29992;&#22825;&#22478;&#25991;&#26805;&#35821;&#25991;&#29486;&#35821;&#26009;&#24211;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#36825;&#20123;&#27169;&#22411;&#20013;&#25552;&#21462;&#32473;&#23450;&#25991;&#26412;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#24212;&#29992;&#38477;&#32500;&#21644;&#32858;&#31867;&#25216;&#26415;&#23545;&#29305;&#24449;&#36827;&#34892;&#22788;&#29702;&#65292;&#20197;&#29983;&#25104;&#32473;&#23450;&#26805;&#35821;&#25991;&#29486;&#30340;&#25688;&#35201;&#12290;&#38500;&#20102;&#25552;&#21462;&#25991;&#26412;&#25688;&#35201;&#25216;&#26415;&#22806;&#65292;&#25105;&#20204;&#36824;&#20844;&#24320;&#21457;&#24067;&#20102;&#19968;&#20010;&#26805;&#35821;&#22825;&#22478;&#25991;&#26412;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we develop language models for the Sanskrit language, namely Bidirectional Encoder Representations from Transformers (BERT) and its variants: A Lite BERT (ALBERT), and Robustly Optimized BERT (RoBERTa) using Devanagari Sanskrit text corpus. Then we extracted the features for the given text from these models. We applied the dimensional reduction and clustering techniques on the features to generate an extractive summary for a given Sanskrit document. Along with the extractive text summarization techniques, we have also created and released a Sanskrit Devanagari text corpus publicly.
&lt;/p&gt;</description></item><item><title>HATELEXICON&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#20167;&#24680;&#35328;&#35770;&#30340;&#35789;&#27719;&#34920;&#65292;&#21033;&#29992;&#20854;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.01890</link><description>&lt;p&gt;
&#31038;&#20250;&#25991;&#21270;&#30693;&#35782;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#20013;&#23545;&#36873;&#39033;&#30340;&#36873;&#25321;&#26159;&#24517;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
Sociocultural knowledge is needed for selection of shots in hate speech detection tasks. (arXiv:2304.01890v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01890
&lt;/p&gt;
&lt;p&gt;
HATELEXICON&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#20167;&#24680;&#35328;&#35770;&#30340;&#35789;&#27719;&#34920;&#65292;&#21033;&#29992;&#20854;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;HATELEXICON&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#30340;&#34065;&#31216;&#21644;&#20167;&#24680;&#35328;&#35770;&#30446;&#26631;&#30340;&#35789;&#27719;&#34920;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#22914;&#20309;&#29992;&#20110;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#65292;&#34920;&#26126;&#21457;&#23637;&#29992;&#20110;&#20998;&#31867;&#26497;&#31471;&#35328;&#35770;&#30340;&#27169;&#22411;&#65292;&#22312;&#36827;&#34892;&#39044;&#27979;&#26102;&#20005;&#37325;&#20381;&#36182;&#30446;&#26631;&#35789;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;HATELEXICON&#26469;&#36741;&#21161;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#35757;&#32451;&#36873;&#39033;&#30340;&#26041;&#27861;&#65292;&#36873;&#39033;&#36873;&#25321;&#22312;&#23567;&#26679;&#26412;&#23398;&#20064;&#20013;&#23588;&#20026;&#37325;&#35201;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;HASOC&#25968;&#25454;&#23545;&#24503;&#35821;&#21644;&#21360;&#22320;&#35821;&#36827;&#34892;&#20102;&#20960;&#20010;&#31034;&#33539;&#23398;&#20064;&#65292;&#24182;&#23558;Multilingual HateCheck&#65288;MHC&#65289;&#20316;&#20026;&#22522;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26681;&#25454;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#36873;&#25321;&#26679;&#26412;&#65292;&#30456;&#23545;&#20110;&#38543;&#26426;&#37319;&#26679;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22312;MHC&#19978;&#34920;&#29616;&#12290;&#22240;&#27492;&#65292;&#24403;&#20165;&#26377;&#23569;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#26102;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#26469;&#36873;&#25321;&#21253;&#21547;&#26356;&#22810;&#31038;&#20250;&#25991;&#21270;&#20449;&#24687;&#30340;&#26679;&#26412;&#33021;&#22815;&#26356;&#22909;&#22320;&#25552;&#39640;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for the countries of Brazil, Germany, India and Kenya, to aid training and interpretability of models. We demonstrate how our lexicon can be used to interpret model predictions, showing that models developed to classify extreme speech rely heavily on target words when making predictions. Further, we propose a method to aid shot selection for training in low-resource settings via HATELEXICON. In few-shot learning, the selection of shots is of paramount importance to model performance. In our work, we simulate a few-shot setting for German and Hindi, using HASOC data for training and the Multilingual HateCheck (MHC) as a benchmark. We show that selecting shots based on our lexicon leads to models performing better on MHC than models trained on shots sampled randomly. Thus, when given only a few training examples, using our lexicon to select shots containing more sociocultural information leads to better few-shot perf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#21644;GPT-4&#65292;&#21253;&#25324;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#21069;&#26223;&#24212;&#29992;&#65292;&#24182;&#30528;&#37325;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#12289;&#25351;&#20196;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#21019;&#26032;&#12290;ChatGPT/GPT-4&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#26041;&#38754;&#34920;&#29616;&#31361;&#20986;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#39046;&#22495;&#20063;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.01852</link><description>&lt;p&gt;
ChatGPT/GPT-4&#30740;&#31350;&#32508;&#36848;&#21450;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#26410;&#26469;&#30340;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models. (arXiv:2304.01852v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#21644;GPT-4&#65292;&#21253;&#25324;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#21069;&#26223;&#24212;&#29992;&#65292;&#24182;&#30528;&#37325;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#12289;&#25351;&#20196;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#21019;&#26032;&#12290;ChatGPT/GPT-4&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#26041;&#38754;&#34920;&#29616;&#31361;&#20986;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#39046;&#22495;&#20063;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#26469;&#33258;GPT&#31995;&#21015;&#30340;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;ChatGPT&#21644;GPT-4&#21450;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#21069;&#26223;&#24212;&#29992;&#12290;&#23454;&#38469;&#19978;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#12289;&#25351;&#20196;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26159;&#25552;&#39640;LLMs&#30340;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#30340;&#37325;&#35201;&#21019;&#26032;&#12290;&#25105;&#20204;&#22312;arXiv&#19978;&#28145;&#20837;&#20998;&#26512;&#20102;194&#31687;&#30456;&#20851;&#25991;&#29486;&#65292;&#21253;&#25324;&#36235;&#21183;&#20998;&#26512;&#12289;&#35789;&#20113;&#34920;&#29616;&#21644;&#22312;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#20998;&#24067;&#20998;&#26512;&#12290;&#30740;&#31350;&#21457;&#29616;ChatGPT/GPT-4&#30740;&#31350;&#26174;&#33879;&#22686;&#38271;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#30452;&#25509;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#19978;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#22312;&#20174;&#25945;&#32946;&#21644;&#21382;&#21490;&#21040;&#25968;&#23398;&#12289;&#21307;&#23398;&#21644;&#29289;&#29702;&#31561;&#39046;&#22495;&#20855;&#26377;&#30456;&#24403;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#26377;&#20851;ChatGPT&#30340;&#33021;&#21147;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive survey of ChatGPT and GPT-4, state-of-the-art large language models (LLM) from the GPT series, and their prospective applications across diverse domains. Indeed, key innovations such as large-scale pre-training that captures knowledge across the entire world wide web, instruction fine-tuning and Reinforcement Learning from Human Feedback (RLHF) have played significant roles in enhancing LLMs' adaptability and performance. We performed an in-depth analysis of 194 relevant papers on arXiv, encompassing trend analysis, word cloud representation, and distribution analysis across various application domains. The findings reveal a significant and increasing interest in ChatGPT/GPT-4 research, predominantly centered on direct natural language processing applications, while also demonstrating considerable potential in areas ranging from education and history to mathematics, medicine, and physics. This study endeavors to furnish insights into ChatGPT's capabi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#20154;&#26426;&#20132;&#20114;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#38500;&#20102;&#25903;&#25345;&#20174;&#25972;&#20010;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#20027;&#39064;&#22806;&#65292;&#36824;&#25903;&#25345;&#30446;&#26631;&#20027;&#39064;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#35789;&#24314;&#35758;&#21151;&#33021;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#24456;&#22909;&#22320;&#36845;&#20195;&#22320;&#23436;&#21892;&#27169;&#22411;&#65292;&#19988;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#35780;&#20215;&#12290;</title><link>http://arxiv.org/abs/2304.01774</link><description>&lt;p&gt;
&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#20154;&#26426;&#20132;&#20114;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A User-Centered, Interactive, Human-in-the-Loop Topic Modelling System. (arXiv:2304.01774v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#20154;&#26426;&#20132;&#20114;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#38500;&#20102;&#25903;&#25345;&#20174;&#25972;&#20010;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#20027;&#39064;&#22806;&#65292;&#36824;&#25903;&#25345;&#30446;&#26631;&#20027;&#39064;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#35789;&#24314;&#35758;&#21151;&#33021;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#24456;&#22909;&#22320;&#36845;&#20195;&#22320;&#23436;&#21892;&#27169;&#22411;&#65292;&#19988;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#35780;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26426;&#20132;&#20114;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#23558;&#29992;&#25143;&#30340;&#30693;&#35782;&#34701;&#20837;&#21040;&#24314;&#27169;&#36807;&#31243;&#20013;&#65292;&#20351;&#20854;&#33021;&#22815;&#36845;&#20195;&#22320;&#23436;&#21892;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#23637;&#31034;&#20102;&#29992;&#25143;&#21453;&#39304;&#30340;&#20215;&#20540;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#20363;&#22914;&#38590;&#20197;&#36319;&#36394;&#21464;&#21270;&#12289;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#20197;&#21450;&#32570;&#20047;&#22522;&#20110;&#23454;&#38469;&#20351;&#29992;&#30340;&#30495;&#23454;&#19990;&#30028;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20132;&#20114;&#24335;&#20154;&#26426;&#20132;&#20114;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#65292;&#20854;&#20855;&#26377;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#27604;&#36739;&#21644;&#35760;&#24405;&#27599;&#20010;&#27493;&#39588;&#65292;&#24182;&#20855;&#26377;&#26032;&#39062;&#30340;&#20027;&#39064;&#35789;&#24314;&#35758;&#21151;&#33021;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#25552;&#20379;&#30495;&#23454;&#21487;&#38752;&#30340;&#21453;&#39304;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#19981;&#20165;&#25903;&#25345;&#20256;&#32479;&#20027;&#39064;&#27169;&#22411;&#30340;&#21151;&#33021;&#65292;&#21363;&#20174;&#25972;&#20010;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#20027;&#39064;&#65292;&#36824;&#25903;&#25345;&#30446;&#26631;&#20027;&#39064;&#24314;&#27169;&#65292;&#21363;&#38024;&#23545;&#35821;&#26009;&#24211;&#29305;&#23450;&#26041;&#38754;&#36827;&#34892;&#20027;&#39064;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#35813;&#31995;&#32479;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#29992;&#25143;&#30740;&#31350;&#30340;&#32467;&#26524;&#65292;&#20197;&#35780;&#20272;&#20854;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-in-the-loop topic modelling incorporates users' knowledge into the modelling process, enabling them to refine the model iteratively. Recent research has demonstrated the value of user feedback, but there are still issues to consider, such as the difficulty in tracking changes, comparing different models and the lack of evaluation based on real-world examples of use. We developed a novel, interactive human-in-the-loop topic modeling system with a user-friendly interface that enables users compare and record every step they take, and a novel topic words suggestion feature to help users provide feedback that is faithful to the ground truth. Our system also supports not only what traditional topic models can do, i.e., learning the topics from the whole corpus, but also targeted topic modelling, i.e., learning topics for specific aspects of the corpus. In this article, we provide an overview of the system and present the results of a series of user studies designed to assess the value
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#21283;&#23376;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#20808;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#23569;&#26679;&#26412;&#36866;&#24212;&#65292;&#36866;&#29992;&#20110;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#23545;&#21333;&#27169;&#22411;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2304.01752</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#23569;&#26679;&#26412;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Black Box Few-Shot Adaptation for Vision-Language models. (arXiv:2304.01752v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#21283;&#23376;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#20808;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#23569;&#26679;&#26412;&#36866;&#24212;&#65292;&#36866;&#29992;&#20110;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#23545;&#21333;&#27169;&#22411;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#36719;&#25552;&#31034;&#23398;&#20064;&#26159;&#23569;&#26679;&#26412;&#39046;&#22495;&#36866;&#29992;&#30340;&#26368;&#21463;&#27426;&#36814;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#26032;&#39046;&#22495;&#24341;&#21457;&#30340;&#20998;&#24067;&#20559;&#31227;&#26469;&#32553;&#23567;&#27169;&#24577;&#24046;&#36317;&#12290;&#34429;&#28982;&#35813;&#26041;&#27861;&#24615;&#33021;&#39640;&#25928;&#65292;&#20294;&#20173;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#26435;&#37325;&#65292;&#24182;&#19988;&#22312;&#20855;&#26377;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#30340;&#22823;&#22411;&#27169;&#22411;&#19978;&#21487;&#33021;&#20250;&#23548;&#33268;&#35745;&#31639;&#19978;&#30340;&#19981;&#21487;&#34892;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#21283;&#23376;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#20808;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#30340; V-L &#23569;&#26679;&#26412;&#36866;&#24212;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#26435;&#37325;&#65292;&#35757;&#32451;&#36895;&#24230;&#24555;&#25968;&#20010;&#25968;&#37327;&#32423;&#65292;&#36866;&#29992;&#20110;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#19988;&#36824;&#21487;&#20197;&#29992;&#20110;&#23545;&#21333;&#27169;&#22411;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Language (V-L) models trained with contrastive learning to align the visual and language modalities have been shown to be strong few-shot learners. Soft prompt learning is the method of choice for few-shot downstream adaption aiming to bridge the modality gap caused by the distribution shift induced by the new domain. While parameter-efficient, prompt learning still requires access to the model weights and can be computationally infeasible for large models with billions of parameters. To address these shortcomings, in this work, we describe a black-box method for V-L few-shot adaptation that (a) operates on pre-computed image and text features and hence works without access to the model's weights, (b) it is orders of magnitude faster at training time, (c) it is amenable to both supervised and unsupervised training, and (d) it can be even used to align image and text features computed from uni-modal models. To achieve this, we propose Linear Feature Alignment (LFA), a simple line
&lt;/p&gt;</description></item><item><title>ChatGPT&#26159;&#19968;&#20010;&#22522;&#20110;GPT-3.5&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#20986;&#33394;&#30340;&#38169;&#35823;&#26816;&#27979;&#21644;&#32416;&#27491;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24456;&#39640;&#28508;&#21147;&#12290;&#23427;&#21487;&#20197;&#33258;&#30001;&#22320;&#32416;&#27491;&#38169;&#35823;&#20351;&#24471;&#32416;&#27491;&#21518;&#30340;&#21477;&#23376;&#38750;&#24120;&#27969;&#30021;&#65292;&#22312;&#38750;&#33521;&#35821;&#21644;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#30340;&#34920;&#29616;&#31361;&#20986;&#20102;&#20854;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.01746</link><description>&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#39640;&#24230;&#27969;&#30021;&#30340;&#35821;&#27861;&#32416;&#38169;&#31995;&#32479;&#21527;&#65311;&#19968;&#39033;&#32508;&#21512;&#24615;&#35780;&#20272;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT a Highly Fluent Grammatical Error Correction System? A Comprehensive Evaluation. (arXiv:2304.01746v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01746
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#22522;&#20110;GPT-3.5&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#20986;&#33394;&#30340;&#38169;&#35823;&#26816;&#27979;&#21644;&#32416;&#27491;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24456;&#39640;&#28508;&#21147;&#12290;&#23427;&#21487;&#20197;&#33258;&#30001;&#22320;&#32416;&#27491;&#38169;&#35823;&#20351;&#24471;&#32416;&#27491;&#21518;&#30340;&#21477;&#23376;&#38750;&#24120;&#27969;&#30021;&#65292;&#22312;&#38750;&#33521;&#35821;&#21644;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#30340;&#34920;&#29616;&#31361;&#20986;&#20102;&#20854;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;GPT-3.5&#20808;&#36827;&#26550;&#26500;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;ChatGPT&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#32570;&#20047;&#20840;&#38754;&#30340;&#30740;&#31350;&#25506;&#32034;&#20854;&#22312;&#35821;&#27861;&#32416;&#38169;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#23637;&#31034;&#20854;&#22312;&#35821;&#27861;&#32416;&#38169;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#38646;&#26679;&#26412;&#25512;&#29702;&#65288;zero-shot chain-of-thought, CoT&#65289;&#21644;&#23569;&#26679;&#26412;&#25512;&#29702;&#65288;few-shot CoT&#65289;&#35774;&#32622;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#35757;&#32451;ChatGPT&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#28041;&#21450;&#22312;&#19977;&#31181;&#19981;&#21516;&#35821;&#35328;&#20013;&#23545;ChatGPT&#22312;&#20116;&#20010;&#23448;&#26041;&#27979;&#35797;&#38598;&#21644;&#19977;&#20010;&#33521;&#25991;&#25991;&#26723;&#32423;GEC&#27979;&#35797;&#38598;&#19978;&#30340;&#34920;&#29616;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#65292;ChatGPT&#20855;&#26377;&#20986;&#33394;&#30340;&#38169;&#35823;&#26816;&#27979;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#33258;&#30001;&#22320;&#32416;&#27491;&#38169;&#35823;&#20351;&#24471;&#32416;&#27491;&#21518;&#30340;&#21477;&#23376;&#38750;&#24120;&#27969;&#30021;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#20854;&#36807;&#24230;&#32416;&#27491;&#20542;&#21521;&#21644;&#19981;&#36981;&#23432;&#26368;&#23567;&#25913;&#21160;&#21407;&#21017;&#12290;&#27492;&#22806;&#65292;&#22312;&#38750;&#33521;&#35821;&#21644;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#30340;&#34920;&#29616;&#31361;&#20986;&#20102;&#20854;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, a large-scale language model based on the advanced GPT-3.5 architecture, has shown remarkable potential in various Natural Language Processing (NLP) tasks. However, there is currently a dearth of comprehensive study exploring its potential in the area of Grammatical Error Correction (GEC). To showcase its capabilities in GEC, we design zero-shot chain-of-thought (CoT) and few-shot CoT settings using in-context learning for ChatGPT. Our evaluation involves assessing ChatGPT's performance on five official test sets in three different languages, along with three document-level GEC test sets in English. Our experimental results and human evaluations demonstrate that ChatGPT has excellent error detection capabilities and can freely correct errors to make the corrected sentences very fluent, possibly due to its over-correction tendencies and not adhering to the principle of minimal edits. Additionally, its performance in non-English and low-resource settings highlights its potential
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#35875;&#35328;&#26816;&#27979;&#24182;&#20998;&#26512;&#20102;&#26032;&#20896;&#30123;&#24773;&#30456;&#20851;&#25512;&#25991;&#30340;&#25968;&#25454;&#12290;&#30740;&#31350;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#36890;&#36807;&#27604;&#36739;&#35821;&#35328;&#32467;&#26500;&#21644;&#20256;&#25773;&#36335;&#24452;&#31561;&#29305;&#24449;&#26469;&#21306;&#20998;&#35875;&#35328;&#21644;&#20107;&#23454;&#65292;&#24182;&#21457;&#29616;&#35821;&#35328;&#32467;&#26500;&#26159;&#26356;&#22909;&#30340;&#21306;&#20998;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2304.01712</link><description>&lt;p&gt;
&#25512;&#29305;&#35875;&#35328;&#26816;&#27979;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Rumour Detection and Analysis on Twitter. (arXiv:2304.01712v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#35875;&#35328;&#26816;&#27979;&#24182;&#20998;&#26512;&#20102;&#26032;&#20896;&#30123;&#24773;&#30456;&#20851;&#25512;&#25991;&#30340;&#25968;&#25454;&#12290;&#30740;&#31350;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#36890;&#36807;&#27604;&#36739;&#35821;&#35328;&#32467;&#26500;&#21644;&#20256;&#25773;&#36335;&#24452;&#31561;&#29305;&#24449;&#26469;&#21306;&#20998;&#35875;&#35328;&#21644;&#20107;&#23454;&#65292;&#24182;&#21457;&#29616;&#35821;&#35328;&#32467;&#26500;&#26159;&#26356;&#22909;&#30340;&#21306;&#20998;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20381;&#36182;&#31038;&#20132;&#23186;&#20307;&#33719;&#21462;&#26032;&#38395;&#21644;&#20449;&#24687;&#65292;&#19968;&#20123;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#21457;&#24067;&#32570;&#20047;&#35777;&#23454;&#30340;&#20449;&#24687;&#20197;&#33719;&#21462;&#20851;&#27880;&#65292;&#36825;&#31181;&#20449;&#24687;&#34987;&#31216;&#20026;&#35875;&#35328;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31995;&#32479;&#26469;&#39044;&#27979;&#35875;&#35328;&#12290;&#20854;&#20013;&#65292;&#26368;&#20339;&#27169;&#22411;&#34987;&#24212;&#29992;&#20110;&#26032;&#20896;&#30123;&#24773;&#30456;&#20851;&#25512;&#25991;&#30340;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#12290;&#26412;&#30740;&#31350;&#30340;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#36890;&#36807;&#20004;&#20010;&#32500;&#24230;&#65306;&#35821;&#35328;&#32467;&#26500;&#21644;&#20256;&#25773;&#36335;&#24452;&#65292;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#26469;&#27604;&#36739;&#35875;&#35328;&#21644;&#20107;&#23454;&#12290;&#65288;2&#65289;&#36890;&#36807;&#20998;&#26512;&#35875;&#35328;&#19982;&#20107;&#23454;&#22312;&#35789;&#27719;&#20351;&#29992;&#21644;&#26263;&#31034;&#30340;&#24773;&#24863;&#26041;&#38754;&#30340;&#19981;&#21516;&#65292;&#25506;&#31350;&#35875;&#35328;&#19982;&#20107;&#23454;&#20043;&#38388;&#30340;&#19981;&#21516;&#12290;&#35813;&#30740;&#31350;&#34920;&#26126;&#65292;&#30456;&#27604;&#20256;&#25773;&#36335;&#24452;&#65292;&#35821;&#35328;&#32467;&#26500;&#26159;&#26356;&#22909;&#30340;&#29305;&#24449;&#26469;&#21306;&#20998;&#35875;&#35328;&#21644;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years people have become increasingly reliant on social media to read news and get information, and some social media users post unsubstantiated information to gain attention. Such information is known as rumours. Nowadays, rumour detection is receiving a growing amount of attention because of the pandemic of the New Coronavirus, which has led to a large number of rumours being spread. In this paper, a Natural Language Processing (NLP) system is built to predict rumours. The best model is applied to the COVID-19 tweets to conduct exploratory data analysis. The contribution of this study is twofold: (1) to compare rumours and facts using state-of-the-art natural language processing models in two dimensions: language structure and propagation route. (2) An analysis of how rumours differ from facts in terms of their lexical use and the emotions they imply. This study shows that linguistic structure is a better feature to distinguish rumours from facts compared to the propagation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#35821;&#35328;&#27169;&#22411;Fine-Tune&#20219;&#21153;&#20013;&#22914;&#20309;&#25429;&#25417;&#35821;&#35328;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;Topological Data Analysis&#26041;&#27861;&#26500;&#24314;&#26377;&#21521;&#22270;&#20174;&#20013;&#25552;&#21462;&#25299;&#25169;&#29305;&#24449;&#12290;&#23454;&#39564;&#21457;&#29616;&#65292;&#20197;&#24358;&#24615;&#21644;&#21305;&#37197;&#25968;&#20026;&#26032;&#29305;&#24449;&#30340;TDA-based&#20998;&#31867;&#22120;&#20248;&#20110;Fine-Tune&#22522;&#32447;&#27169;&#22411;&#65292;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#29702;&#35299;LMs&#22312;&#21487;&#25509;&#21463;&#24615;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2304.01680</link><description>&lt;p&gt;
BERT&#21487;&#20197;&#21534;&#21693;RuCoLA&#21527;&#65311;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26469;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can BERT eat RuCoLA? Topological Data Analysis to Explain. (arXiv:2304.01680v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#35821;&#35328;&#27169;&#22411;Fine-Tune&#20219;&#21153;&#20013;&#22914;&#20309;&#25429;&#25417;&#35821;&#35328;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;Topological Data Analysis&#26041;&#27861;&#26500;&#24314;&#26377;&#21521;&#22270;&#20174;&#20013;&#25552;&#21462;&#25299;&#25169;&#29305;&#24449;&#12290;&#23454;&#39564;&#21457;&#29616;&#65292;&#20197;&#24358;&#24615;&#21644;&#21305;&#37197;&#25968;&#20026;&#26032;&#29305;&#24449;&#30340;TDA-based&#20998;&#31867;&#22120;&#20248;&#20110;Fine-Tune&#22522;&#32447;&#27169;&#22411;&#65292;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#29702;&#35299;LMs&#22312;&#21487;&#25509;&#21463;&#24615;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#25797;&#38271;Fine-Tune&#20219;&#21153;&#25429;&#25417;&#35821;&#35328;&#29305;&#24449;&#65292;&#24182;&#29992;Topological Data Analysis&#65288;TDA&#65289;&#26041;&#27861;&#20174;&#27880;&#24847;&#21147;&#30697;&#38453;&#20013;&#26500;&#24314;&#26377;&#21521;&#22270;&#65292;&#20174;&#20013;&#25552;&#21462;&#25299;&#25169;&#29305;&#24449;&#65292;&#23558;&#20854;&#36865;&#20837;&#32447;&#24615;&#20998;&#31867;&#22120;&#20013;&#12290;&#22312;&#33521;&#35821;&#21644;&#20420;&#35821;&#20004;&#20010;&#25299;&#25169;&#19978;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;CoLA&#21644;RuCoLA&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#25552;&#20986;&#20102;&#22810;&#31181;&#40657;&#30418;&#20869;&#30465;&#25216;&#26415;&#65292;&#20197;&#20415;&#28145;&#20837;&#20102;&#35299;LM&#23545;&#35821;&#35328;&#30340;&#34920;&#31034;&#21644;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates how Transformer language models (LMs) fine-tuned for acceptability classification capture linguistic features. Our approach uses the best practices of topological data analysis (TDA) in NLP: we construct directed attention graphs from attention matrices, derive topological features from them, and feed them to linear classifiers. We introduce two novel features, chordality, and the matching number, and show that TDA-based classifiers outperform fine-tuning baselines. We experiment with two datasets, CoLA and RuCoLA in English and Russian, typologically different languages. On top of that, we propose several black-box introspection techniques aimed at detecting changes in the attention mode of the LMs during fine-tuning, defining the LM's prediction confidences, and associating individual heads with fine-grained grammar phenomena. Our results contribute to understanding the behavior of monolingual LMs in the acceptability classification task, provide insights into
&lt;/p&gt;</description></item><item><title>&#19978;&#19979;&#25991;&#35821;&#22659;&#19979;&#35821;&#20041;&#36716;&#21464;&#26816;&#27979;&#30340;&#35745;&#31639;&#26426;&#26041;&#27861;&#19981;&#26029;&#36827;&#27493;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#21333;&#35789;&#30340;&#22810;&#37325;&#29992;&#27861;/&#24847;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21547;&#20041;&#34920;&#31034;&#12289;&#26102;&#38388;&#24863;&#30693;&#21644;&#23398;&#20064;&#27169;&#24577;&#30340;&#20998;&#31867;&#26694;&#26550;&#12290;&#35813;&#32508;&#36848;&#25506;&#35752;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;</title><link>http://arxiv.org/abs/2304.01666</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#35821;&#22659;&#19979;&#35821;&#20041;&#36716;&#21464;&#26816;&#27979;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Contextualised Semantic Shift Detection. (arXiv:2304.01666v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01666
&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#35821;&#22659;&#19979;&#35821;&#20041;&#36716;&#21464;&#26816;&#27979;&#30340;&#35745;&#31639;&#26426;&#26041;&#27861;&#19981;&#26029;&#36827;&#27493;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#21333;&#35789;&#30340;&#22810;&#37325;&#29992;&#27861;/&#24847;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21547;&#20041;&#34920;&#31034;&#12289;&#26102;&#38388;&#24863;&#30693;&#21644;&#23398;&#20064;&#27169;&#24577;&#30340;&#20998;&#31867;&#26694;&#26550;&#12290;&#35813;&#32508;&#36848;&#25506;&#35752;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#36716;&#21464;&#26816;&#27979; (SSD) &#26159;&#25351;&#35782;&#21035;&#12289;&#35299;&#37322;&#21644;&#35780;&#20272;&#30446;&#26631;&#35789;&#21487;&#33021;&#38543;&#30528;&#26102;&#38388;&#21457;&#29983;&#30340;&#24847;&#20041;&#21464;&#21270;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#19978;&#65292;SSD &#26159;&#30001;&#35821;&#35328;&#23398;&#23478;&#21644;&#31038;&#20250;&#31185;&#23398;&#23478;&#36890;&#36807;&#25163;&#21160;&#21644;&#32791;&#26102;&#30340;&#27963;&#21160;&#26469;&#22788;&#29702;&#30340;&#12290;&#36817;&#24180;&#26469;&#65292;&#35745;&#31639;&#26426;&#26041;&#27861;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35789;&#23884;&#20837;&#25216;&#26415;&#24471;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20197;&#23613;&#21487;&#33021;&#22320;&#33258;&#21160;&#21270; SSD&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#36807;&#21435;&#30340;&#19977;&#24180;&#20013;&#65292;&#20960;&#20046;&#23436;&#20840;&#22522;&#20110;&#35789;&#27719;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#36827;&#23637;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#21333;&#35789;&#30340;&#22810;&#31181;&#29992;&#27861;/&#24847;&#20041;&#65292;&#24182;&#26356;&#22909;&#22320;&#25429;&#25417;&#30456;&#20851;&#30340;&#35821;&#20041;&#36716;&#21464;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#19978;&#19979;&#25991;&#23884;&#20837;&#30340; SSD &#26041;&#27861;&#65288;&#21363; CSSDetection&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#21547;&#20041;&#34920;&#31034;&#12289;&#26102;&#38388;&#24863;&#30693;&#21644;&#23398;&#20064;&#27169;&#24577;&#32500;&#24230;&#20026;&#29305;&#24449;&#30340;&#20998;&#31867;&#26694;&#26550;&#12290;&#21033;&#29992;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#23545;&#36716;&#21464;&#35780;&#20272;&#25514;&#26045;&#36827;&#34892;&#20102;&#22238;&#39038;&#65292;&#24182;&#30830;&#23450;&#20102;&#19978;&#19979;&#25991;&#21270; SSD &#39046;&#22495;&#26410;&#26469;&#30740;&#31350;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic Shift Detection (SSD) is the task of identifying, interpreting, and assessing the possible change over time in the meanings of a target word. Traditionally, SSD has been addressed by linguists and social scientists through manual and time-consuming activities. In the recent years, computational approaches based on Natural Language Processing and word embeddings gained increasing attention to automate SSD as much as possible. In particular, over the past three years, significant advancements have been made almost exclusively based on word contextualised embedding models, which can handle the multiple usages/meanings of the words and better capture the related semantic shifts. In this paper, we survey the approaches based on contextualised embeddings for SSD (i.e., CSSDetection) and we propose a classification framework characterised by meaning representation, time-awareness, and learning modality dimensions. The framework is exploited i) to review the measures for shift assessm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#32534;&#35793;&#31070;&#32463;&#32593;&#32476;CoNNs&#24182;&#20837;&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#20013;&#65292;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#21512;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#25277;&#35937;&#35268;&#21017;&#30340;&#39046;&#22495;&#12290;&#26041;&#27861;&#31216;&#20026;&#8220;&#31070;&#32463;&#29702;&#35299;&#8221;&#65292;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#31526;&#21495;&#25805;&#20316;&#12289;&#35268;&#21017;&#25512;&#29702;&#12289;&#31639;&#26415;&#25512;&#29702;&#31561;&#26041;&#38754;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.01665</link><description>&lt;p&gt;
&#32534;&#35793;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#27169;&#22411;&#65306;&#31070;&#32463;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Neural Comprehension: Language Models with Compiled Neural Networks. (arXiv:2304.01665v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#32534;&#35793;&#31070;&#32463;&#32593;&#32476;CoNNs&#24182;&#20837;&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#20013;&#65292;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#21512;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#25277;&#35937;&#35268;&#21017;&#30340;&#39046;&#22495;&#12290;&#26041;&#27861;&#31216;&#20026;&#8220;&#31070;&#32463;&#29702;&#35299;&#8221;&#65292;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#31526;&#21495;&#25805;&#20316;&#12289;&#35268;&#21017;&#25512;&#29702;&#12289;&#31639;&#26415;&#25512;&#29702;&#31561;&#26041;&#38754;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#65292;&#20294;&#20854;&#36827;&#34892;&#31526;&#21495;&#25805;&#20316;&#21644;&#31639;&#26415;&#25805;&#20316;&#30340;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#65292;&#36825;&#24402;&#22240;&#20110;&#23427;&#20204;&#38544;&#24335;&#22320;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#35268;&#21017;&#12290;&#25105;&#20204;&#25506;&#35752;&#22914;&#20309;&#23558;&#29305;&#21035;&#35774;&#35745;&#24471;&#21040;&#30340;&#21152;&#26435;&#30340;&#32534;&#35793;&#31070;&#32463;&#32593;&#32476;&#65288;CoNNs&#65289;&#24182;&#20837;&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#20013;&#65292;&#20351;&#24471;&#36890;&#36807;&#26799;&#24230;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#33719;&#24471;&#23436;&#20840;&#30340;&#35268;&#21017;&#29702;&#35299;&#33021;&#21147;&#12290;&#32534;&#35793;&#31070;&#32463;&#32593;&#32476;&#30340;&#24182;&#20837;&#20026;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#21512;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#25277;&#35937;&#35268;&#21017;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;&#8220;&#31070;&#32463;&#29702;&#35299;&#8221;&#65292;&#26377;&#21161;&#20110;&#35821;&#35328;&#27169;&#22411;&#22312;&#31526;&#21495;&#25805;&#20316;&#12289;&#35268;&#21017;&#25512;&#29702;&#12289;&#31639;&#26415;&#25512;&#29702;&#31561;&#26041;&#38754;&#23454;&#29616;&#32477;&#23545;&#20934;&#30830;&#24230;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#21487;&#29992;&#65306;\url{https://github.com/...}
&lt;/p&gt;
&lt;p&gt;
Language models have achieved impressive results in natural language processing tasks, but their ability to perform symbolic operations and arithmetic operations, remains limited, which attribute to their learn the rules implicitly from data. We explore how to incorporate compiled neural networks (CoNNs) which weight is specially designed, into the architecture of language models to enable the language model trained by gradient to obtain fully rule comprehension ability. The incorporation of compiled neural networks offers a promising direction for improving the performance of language models on compound tasks, particularly in areas that require a deeper comprehension of abstract rules beyond recognizing patterns in training data. Our method, which call "Neural Comprehension", helps language models achieve absolute accuracy in symbolic operations, thereby enhancing their ability for rule reasoning, symbolic reasoning, and arithmetic reasoning. Our code is publicly available at: \url{ht
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#37492;&#21035;&#24335;&#27807;&#36890;&#30446;&#26631;&#65292;&#24494;&#35843;&#24050;&#26377;&#30340;&#31070;&#32463;&#23383;&#24149;&#29983;&#25104;&#22120;&#65292;&#22312;&#20445;&#25345;&#35821;&#35328;&#30340;&#35270;&#35273;&#25551;&#36848;&#24615;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#20102;&#23545;&#22270;&#20687;&#20869;&#23481;&#30340;&#20449;&#24687;&#25552;&#21462;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.01662</link><description>&lt;p&gt;
&#36890;&#36807;&#37492;&#21035;&#24494;&#35843;&#36827;&#34892;&#36328;&#39046;&#22495;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Cross-Domain Image Captioning with Discriminative Finetuning. (arXiv:2304.01662v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#37492;&#21035;&#24335;&#27807;&#36890;&#30446;&#26631;&#65292;&#24494;&#35843;&#24050;&#26377;&#30340;&#31070;&#32463;&#23383;&#24149;&#29983;&#25104;&#22120;&#65292;&#22312;&#20445;&#25345;&#35821;&#35328;&#30340;&#35270;&#35273;&#25551;&#36848;&#24615;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#20102;&#23545;&#22270;&#20687;&#20869;&#23481;&#30340;&#20449;&#24687;&#25552;&#21462;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#23383;&#24149;&#29983;&#25104;&#22120;&#36890;&#24120;&#34987;&#35757;&#32451;&#25104;&#27169;&#20223;&#20154;&#31867;&#29983;&#25104;&#30340;&#21442;&#32771;&#25991;&#26412;&#32780;&#27809;&#26377;&#38024;&#23545;&#29305;&#23450;&#27807;&#36890;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#65292;&#23548;&#33268;&#20135;&#29983;&#27169;&#31946;&#30340;&#23383;&#24149;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#37492;&#21035;&#24335;&#27807;&#36890;&#30446;&#26631;&#24494;&#35843;&#31070;&#32463;&#23383;&#24149;&#29983;&#25104;&#22120;&#33021;&#22815;&#24110;&#21161;&#24674;&#22797;&#24179;&#23454;&#12289;&#35270;&#35273;&#25551;&#36848;&#24615;&#26356;&#24378;&#12289;&#20851;&#20110;&#22270;&#20687;&#20869;&#23481;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#12290;&#32473;&#23450;&#19968;&#20010;&#30446;&#26631;&#22270;&#20687;&#65292;&#31995;&#32479;&#24517;&#39035;&#23398;&#20064;&#20135;&#29983;&#19968;&#27573;&#25551;&#36848;&#65292;&#20174;&#32780;&#20351;&#19968;&#20010;&#24320;&#31665;&#21363;&#29992;&#30340;&#25991;&#26412;&#26465;&#20214;&#22270;&#20687;&#26816;&#32034;&#22120;&#33021;&#22815;&#22312;&#19968;&#32452;&#20505;&#36873;&#22270;&#20687;&#20013;&#35782;&#21035;&#35813;&#22270;&#20687;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#27969;&#34892;&#30340;ClipCap&#23383;&#24149;&#29983;&#25104;&#22120;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#20351;&#29992;BLIP&#22797;&#21046;&#20102;&#20027;&#35201;&#32467;&#26524;&#12290;&#23601;&#19982;&#20154;&#31867;&#25551;&#36848;&#30340;&#30456;&#20284;&#24230;&#32780;&#35328;&#65292;&#22312;&#30456;&#21516;&#30340;&#23383;&#24149;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21644;&#27979;&#35797;&#38750;&#24494;&#35843;&#27169;&#22411;&#29983;&#25104;&#30340;&#23383;&#24149;&#30053;&#20248;&#20110;&#37492;&#21035;&#24494;&#35843;&#30340;&#23383;&#24149;&#12290;&#28982;&#32780;&#65292;&#24403;&#27169;&#22411;&#26410;&#32463;&#36827;&#19968;&#27493;&#35757;&#32451;&#26102;&#65292;&#37492;&#21035;&#24494;&#35843;&#30340;&#23383;&#24149;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#29983;&#25104;&#30340;&#21442;&#32771;&#25991;&#26412;&#26356;&#33021;&#25552;&#20379;&#20851;&#20110;&#22270;&#20687;&#35270;&#35273;&#20869;&#23481;&#26356;&#20016;&#23500;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural captioners are typically trained to mimic human-generated references without optimizing for any specific communication goal, leading to problems such as the generation of vague captions. In this paper, we show that fine-tuning an out-of-the-box neural captioner with a self-supervised discriminative communication objective helps to recover a plain, visually descriptive language that is more informative about image contents. Given a target image, the system must learn to produce a description that enables an out-of-the-box text-conditioned image retriever to identify such image among a set of candidates. We experiment with the popular ClipCap captioner, also replicating the main results with BLIP. In terms of similarity to ground-truth human descriptions, the captions emerging from discriminative finetuning lag slightly behind those generated by the non-finetuned model, when the latter is trained and tested on the same caption dataset. However, when the model is used without furth
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SWIPE&#30340;&#22810;&#32500;&#24863;&#30693;&#22120;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#25972;&#20010;&#25991;&#26412;&#30340;&#26631;&#31614;&#24182;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#24863;&#30693;&#27573;&#33853;&#30340;&#26631;&#31614;&#12290;SWIPE&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#20998;&#31867;&#22120;&#65292;&#25903;&#25345;&#19981;&#21516;&#30340;&#32534;&#30721;&#22120;&#65292;&#22312;&#20998;&#31867;&#19978;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2304.01638</link><description>&lt;p&gt;
&#39640;&#25928;&#21487;&#35299;&#37322;&#30340;&#38271;&#25991;&#26412;&#20998;&#31867;&#30340;&#22810;&#32500;&#24863;&#30693;&#22120;
&lt;/p&gt;
&lt;p&gt;
Multidimensional Perceptron for Efficient and Explainable Long Text Classification. (arXiv:2304.01638v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01638
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SWIPE&#30340;&#22810;&#32500;&#24863;&#30693;&#22120;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#25972;&#20010;&#25991;&#26412;&#30340;&#26631;&#31614;&#24182;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#24863;&#30693;&#27573;&#33853;&#30340;&#26631;&#31614;&#12290;SWIPE&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#20998;&#31867;&#22120;&#65292;&#25903;&#25345;&#19981;&#21516;&#30340;&#32534;&#30721;&#22120;&#65292;&#22312;&#20998;&#31867;&#19978;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;Transformer&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19981;&#21487;&#36991;&#20813;&#30340;&#25104;&#26412;&#21644;&#22797;&#26434;&#24615;&#65292;&#22312;&#38271;&#25991;&#26412;&#20998;&#31867;&#20013;&#24341;&#36215;&#20102;&#25928;&#29575;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#22312;&#39640;&#24230;&#25935;&#24863;&#30340;&#39046;&#22495;&#65292;&#20363;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#27861;&#24459;&#38271;&#25991;&#26412;&#25366;&#25496;&#20013;&#65292;&#28508;&#22312;&#30340;&#27169;&#22411;&#19981;&#20449;&#20219;&#65292;&#34429;&#28982;&#34987;&#20302;&#20272;&#21644;&#26410;&#34987;&#25506;&#32034;&#65292;&#20294;&#21487;&#33021;&#23381;&#32946;&#37325;&#35201;&#30340;&#24551;&#34385;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#23558;&#38271;&#25991;&#26412;&#20998;&#21106;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#27599;&#20010;&#29255;&#27573;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#21147;&#25110;RNN&#26469;&#33719;&#21462;&#38271;&#25991;&#26412;&#34920;&#31034;&#20197;&#36827;&#34892;&#20998;&#31867;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#21363;Segment-aWare multIdimensional PErceptron&#65288;SWIPE&#65289;&#65292;&#20197;&#21462;&#20195;&#19978;&#36848;&#26694;&#26550;&#20013;&#30340;&#27880;&#24847;&#21147;/RNN&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;SWIPE&#21487;&#20197;&#36890;&#36807;&#26377;&#30417;&#30563;&#30340;&#35757;&#32451;&#26377;&#25928;&#22320;&#23398;&#20064;&#25972;&#20010;&#25991;&#26412;&#30340;&#26631;&#31614;&#65292;&#21516;&#26102;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#24863;&#30693;&#27573;&#33853;&#30340;&#26631;&#31614;&#24182;&#20272;&#35745;&#23427;&#20204;&#23545;&#38271;&#25991;&#26412;&#26631;&#31614;&#30340;&#36129;&#29486;&#12290;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#20998;&#31867;&#22120;&#65292;SWIPE&#21487;&#20197;&#25903;&#25345;&#19981;&#21516;&#30340;&#32534;&#30721;&#22120;&#65292;&#22312;&#20998;&#31867;&#19978;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Because of the inevitable cost and complexity of transformer and pre-trained models, efficiency concerns are raised for long text classification. Meanwhile, in the highly sensitive domains, e.g., healthcare and legal long-text mining, potential model distrust, yet underrated and underexplored, may hatch vital apprehension. Existing methods generally segment the long text, encode each piece with the pre-trained model, and use attention or RNNs to obtain long text representation for classification. In this work, we propose a simple but effective model, Segment-aWare multIdimensional PErceptron (SWIPE), to replace attention/RNNs in the above framework. Unlike prior efforts, SWIPE can effectively learn the label of the entire text with supervised training, while perceive the labels of the segments and estimate their contributions to the long-text labeling in an unsupervised manner. As a general classifier, SWIPE can endorse different encoders, and it outperforms SOTA models in terms of cla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SimCSum&#30340;&#27169;&#22411;&#65292;&#32852;&#21512;&#35757;&#32451;&#31616;&#21270;&#21644;&#36328;&#35821;&#35328;&#25688;&#35201;&#20004;&#20010;&#39640;&#32423;NLP&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#19982;&#22522;&#32447;&#27169;&#22411;&#30456;&#36817;&#25110;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#25552;&#39640;&#20102;&#35821;&#35328;&#22797;&#26434;&#24615;&#21644;&#36328;&#35821;&#35328;&#25277;&#35937;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2304.01621</link><description>&lt;p&gt;
SimCSum&#65306;&#32852;&#21512;&#23398;&#20064;&#31616;&#21270;&#21644;&#36328;&#35821;&#35328;&#25688;&#35201;&#20197;&#29992;&#20110;&#36328;&#35821;&#35328;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;
&lt;/p&gt;
&lt;p&gt;
SimCSum: Joint Learning of Simplification and Cross-lingual Summarization for Cross-lingual Science Journalism. (arXiv:2304.01621v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SimCSum&#30340;&#27169;&#22411;&#65292;&#32852;&#21512;&#35757;&#32451;&#31616;&#21270;&#21644;&#36328;&#35821;&#35328;&#25688;&#35201;&#20004;&#20010;&#39640;&#32423;NLP&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#19982;&#22522;&#32447;&#27169;&#22411;&#30456;&#36817;&#25110;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#25552;&#39640;&#20102;&#35821;&#35328;&#22797;&#26434;&#24615;&#21644;&#36328;&#35821;&#35328;&#25277;&#35937;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#35328;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#20026;&#38750;&#19987;&#19994;&#35835;&#32773;&#29983;&#25104;&#20102;&#19981;&#21516;&#20110;&#26469;&#28304;&#35821;&#35328;&#30340;&#31185;&#23398;&#25991;&#31456;&#30340;&#36890;&#20439;&#29256;&#26412;&#12290;&#22240;&#27492;&#65292;&#36328;&#35821;&#35328;&#23459;&#20256;&#25688;&#35201;&#24517;&#39035;&#21253;&#21547;&#21407;&#22987;&#25991;&#26723;&#30340;&#35201;&#28857;&#20869;&#23481;&#65292;&#24182;&#19988;&#20869;&#23481;&#24212;&#35813;&#36830;&#36143;&#12289;&#26131;&#25026;&#65292;&#24182;&#19988;&#20197;&#21463;&#20247;&#30340;&#26412;&#22320;&#35821;&#35328;&#21576;&#29616;&#12290;&#26412;&#25991;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#31616;&#21270;&#21644;&#36328;&#35821;&#35328;&#25688;&#35201;&#20004;&#20010;&#39640;&#32423;NLP&#20219;&#21153;&#26469;&#25913;&#36827;&#36328;&#35821;&#35328;&#25688;&#35201;&#29983;&#25104;&#30340;&#36825;&#20123;&#26041;&#38754;&#12290;&#21069;&#32773;&#20943;&#23569;&#35821;&#35328;&#22797;&#26434;&#24230;&#65292;&#21518;&#32773;&#19987;&#27880;&#20110;&#36328;&#35821;&#35328;&#25688;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#20307;&#31995;&#32467;&#26500;&#8212;&#8212;SimCSum&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#20849;&#20139;&#32534;&#30721;&#22120;&#21644;&#20004;&#20010;&#24182;&#34892;&#35299;&#30721;&#22120;&#65292;&#20849;&#21516;&#23398;&#20064;&#31616;&#21270;&#21644;&#36328;&#35821;&#35328;&#25688;&#35201;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#20960;&#31181;&#24378;&#22522;&#32447;&#27169;&#22411;&#30340;&#20960;&#20010;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#24037;&#35780;&#20272;&#26469;&#23454;&#35777;&#30740;&#31350;&#20102;SimCSum&#30340;&#24615;&#33021;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;SimCSum&#22312;&#21516;&#26102;&#35757;&#32451;&#20004;&#20010;&#39640;&#32423;NLP&#20219;&#21153;&#30340;&#21516;&#26102;&#65292;&#21576;&#29616;&#20986;&#19982;&#22522;&#32447;&#30456;&#20284;&#25110;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual science journalism generates popular science stories of scientific articles different from the source language for a non-expert audience. Hence, a cross-lingual popular summary must contain the salient content of the input document, and the content should be coherent, comprehensible, and in a local language for the targeted audience. We improve these aspects of cross-lingual summary generation by joint training of two high-level NLP tasks, simplification and cross-lingual summarization. The former task reduces linguistic complexity, and the latter focuses on cross-lingual abstractive summarization. We propose a novel multi-task architecture - SimCSum consisting of one shared encoder and two parallel decoders jointly learning simplification and cross-lingual summarization. We empirically investigate the performance of SimCSum by comparing it with several strong baselines over several evaluation metrics and by human evaluation. Overall, SimCSum demonstrates statistically si
&lt;/p&gt;</description></item><item><title>EDeR&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#20107;&#20214;&#38388;&#20381;&#36182;&#20851;&#31995;&#27880;&#37322;&#12290;&#39044;&#27979;&#27492;&#20851;&#31995;&#21487;&#20197;&#25552;&#39640;&#20107;&#20214;&#25277;&#21462;&#21450;&#20849;&#25351;&#28040;&#35299;&#31561;&#19979;&#28216;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01612</link><description>&lt;p&gt;
EDeR&#65306;&#29992;&#20110;&#25506;&#32034;&#20107;&#20214;&#20043;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
EDeR: A Dataset for Exploring Dependency Relations Between Events. (arXiv:2304.01612v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01612
&lt;/p&gt;
&lt;p&gt;
EDeR&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#20107;&#20214;&#38388;&#20381;&#36182;&#20851;&#31995;&#27880;&#37322;&#12290;&#39044;&#27979;&#27492;&#20851;&#31995;&#21487;&#20197;&#25552;&#39640;&#20107;&#20214;&#25277;&#21462;&#21450;&#20849;&#25351;&#28040;&#35299;&#31561;&#19979;&#28216;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#30740;&#31350;&#30340;&#26680;&#24515;&#20219;&#21153;&#12290;&#25105;&#20204;&#35748;&#20026;&#36804;&#20170;&#26410;&#22312;NLP&#25110;IR&#30740;&#31350;&#20013;&#25506;&#32034;&#30340;&#19968;&#31181;&#37325;&#35201;&#20851;&#31995;&#31867;&#22411;&#26159;&#20107;&#20214;&#20316;&#20026;&#21478;&#19968;&#20010;&#20107;&#20214;&#30340;&#21442;&#25968; - &#24517;&#38656;&#25110;&#21487;&#36873; - &#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20154;&#24037;&#27880;&#37322;&#30340;Event Dependency Relation&#25968;&#25454;&#38598;&#65288;EDeR&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#36825;&#31181;&#20381;&#36182;&#20851;&#31995;&#12290;&#27880;&#37322;&#26159;&#22312;OntoNotes&#25968;&#25454;&#38598;&#30340;&#25991;&#26723;&#26679;&#26412;&#19978;&#23436;&#25104;&#30340;&#65292;&#36825;&#20855;&#26377;&#19982;&#35813;&#25968;&#25454;&#38598;&#30340;&#29616;&#26377;&#27491;&#20132;&#27880;&#37322;&#38598;&#25104;&#21512;&#24182;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#39044;&#27979;&#20107;&#20214;&#20381;&#36182;&#20851;&#31995;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#20854;&#20013;&#26368;&#20339;&#26041;&#27861;&#30340;&#20108;&#20803;&#21442;&#25968;/&#38750;&#21442;&#25968;&#20998;&#31867;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;82.61&#65285;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35782;&#21035;&#27492;&#20851;&#31995;&#21487;&#20197;&#23548;&#33268;&#26356;&#20934;&#30830;&#30340;&#20107;&#20214;&#25277;&#21462;&#65288;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#65289;&#65292;&#24182;&#21487;&#20197;&#25913;&#36827;&#20381;&#36182;&#20110;&#27492;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#20363;&#22914;&#20849;&#25351;&#28040;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#39044;&#27979;&#19977;&#31181;&#21442;&#25968;&#32852;&#21512;&#31867;&#22411;&#21487;&#20197;&#36890;&#36807;&#22312;&#20107;&#20214;&#25277;&#21462;&#20013;&#25552;&#20379;&#26356;&#26377;&#25928;&#30340;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25913;&#36827;&#25512;&#29702;&#21644;&#20943;&#23569;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction is a central task in natural language processing (NLP) and information retrieval (IR) research. We argue that an important type of relation not explored in NLP or IR research to date is that of an event being an argument - required or optional - of another event. We introduce the human-annotated Event Dependency Relation dataset (EDeR) which provides this dependency relation. The annotation is done on a sample of documents from the OntoNotes dataset, which has the added benefit that it integrates with existing, orthogonal, annotations of this dataset. We investigate baseline approaches for predicting the event dependency relation, the best of which achieves an accuracy of 82.61 for binary argument/non-argument classification. We show that recognizing this relation leads to more accurate event extraction (semantic role labelling) and can improve downstream tasks that depend on this, such as co-reference resolution. Furthermore, we demonstrate that predicting the thre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#22312;&#21508;&#31181;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20107;&#23454;&#22238;&#24518;&#65292;&#38382;&#31572;&#65292;&#24773;&#24863;&#20998;&#26512;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#31561;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#20248;&#20808;&#32771;&#34385;&#20449;&#24687;&#24615;&#21333;&#35789;&#26469;&#24433;&#21709;&#25513;&#30422;&#35821;&#35328;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2304.01597</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#26080;&#30417;&#30563;&#25913;&#36827;&#20107;&#23454;&#30693;&#35782;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Improvement of Factual Knowledge in Language Models. (arXiv:2304.01597v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#22312;&#21508;&#31181;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20107;&#23454;&#22238;&#24518;&#65292;&#38382;&#31572;&#65292;&#24773;&#24863;&#20998;&#26512;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#31561;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#20248;&#20808;&#32771;&#34385;&#20449;&#24687;&#24615;&#21333;&#35789;&#26469;&#24433;&#21709;&#25513;&#30422;&#35821;&#35328;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25513;&#30422;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#22312;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25198;&#28436;&#20851;&#38190;&#35282;&#33394;&#12290;&#20294;&#26159;&#30001;&#20110;&#39640;&#39057;&#35789;&#25903;&#37197;&#20102;MLM&#30446;&#26631;&#65292;&#22240;&#27492;&#19981;&#21033;&#20110;&#23398;&#20064;&#20107;&#23454;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20197;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#24433;&#21709;MLM&#39044;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24378;&#21046;&#35821;&#35328;&#27169;&#22411;&#20248;&#20808;&#32771;&#34385;&#20449;&#24687;&#24615;&#21333;&#35789;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20851;&#38381;&#20070;&#26412;&#30340;&#24773;&#20917;&#19979;&#30340;&#20107;&#23454;&#22238;&#24518;&#65292;&#38382;&#31572;&#65292;&#24773;&#24863;&#20998;&#26512;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#31561;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked language modeling (MLM) plays a key role in pretraining large language models. But the MLM objective is often dominated by high-frequency words that are sub-optimal for learning factual knowledge. In this work, we propose an approach for influencing MLM pretraining in a way that can improve language model performance on a variety of knowledge-intensive tasks. We force the language model to prioritize informative words in a fully unsupervised way. Experiments demonstrate that the proposed approach can significantly improve the performance of pretrained language models on tasks such as factual recall, question answering, sentiment analysis, and natural language inference in a closed-book setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23646;&#24615;&#19968;&#33268;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;(ACK-MMEA)&#65292;&#24357;&#34917;&#23454;&#20307;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#29305;&#23450;&#27169;&#24577;&#19978;&#20855;&#26377;&#19981;&#21516;&#25968;&#37327;&#23646;&#24615;&#30340;&#19978;&#19979;&#25991;&#24046;&#36317;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;(MMEA)&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01563</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#30340;&#23646;&#24615;&#19968;&#33268;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Attribute-Consistent Knowledge Graph Representation Learning for Multi-Modal Entity Alignment. (arXiv:2304.01563v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23646;&#24615;&#19968;&#33268;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;(ACK-MMEA)&#65292;&#24357;&#34917;&#23454;&#20307;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#29305;&#23450;&#27169;&#24577;&#19978;&#20855;&#26377;&#19981;&#21516;&#25968;&#37327;&#23646;&#24615;&#30340;&#19978;&#19979;&#25991;&#24046;&#36317;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;(MMEA)&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;(MMEA)&#26088;&#22312;&#25214;&#21040;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;(MMKGs)&#20043;&#38388;&#25152;&#26377;&#31561;&#20215;&#30340;&#23454;&#20307;&#23545;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#39062;&#30340;&#23646;&#24615;&#19968;&#33268;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;(ACK-MMEA)&#65292;&#36890;&#36807;&#21512;&#24182;&#19968;&#33268;&#30340;&#23545;&#40784;&#30693;&#35782;&#26469;&#24357;&#34917;&#19978;&#19979;&#25991;&#24046;&#36317;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#27169;&#24577;&#23646;&#24615;&#32479;&#19968;&#21270;&#26500;&#24314;&#23646;&#24615;&#19968;&#33268;&#30340;&#30693;&#35782;&#22270;&#35889;(ACKGs)&#65292;&#24182;&#22312;&#22522;&#20110;&#20851;&#31995;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#34701;&#21512;&#36825;&#20123;ACKGs&#20197;&#33719;&#24471;&#32858;&#21512;&#30340;&#20851;&#31995;&#34920;&#31034;&#21644;&#31283;&#20581;&#30340;&#23454;&#20307;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The multi-modal entity alignment (MMEA) aims to find all equivalent entity pairs between multi-modal knowledge graphs (MMKGs). Rich attributes and neighboring entities are valuable for the alignment task, but existing works ignore contextual gap problems that the aligned entities have different numbers of attributes on specific modality when learning entity representations. In this paper, we propose a novel attribute-consistent knowledge graph representation learning framework for MMEA (ACK-MMEA) to compensate the contextual gaps through incorporating consistent alignment knowledge. Attribute-consistent KGs (ACKGs) are first constructed via multi-modal attribute uniformization with merge and generate operators so that each entity has one and only one uniform feature in each modality. The ACKGs are then fed into a relation-aware graph neural network with random dropouts, to obtain aggregated relation representations and robust entity representations. In order to evaluate the ACK-MMEA fa
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;&#23545;&#27604;&#20256;&#36882;&#26694;&#26550;&#21644;&#20256;&#25773;&#32467;&#26500;&#65292;&#23558;&#20174;&#20805;&#36275;&#36164;&#28304;&#30340;&#35875;&#35328;&#25968;&#25454;&#23398;&#21040;&#30340;&#29305;&#24449;&#36866;&#24212;&#20110;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#26816;&#27979;&#21040;&#36328;&#36234;&#35821;&#35328;&#21644;&#39046;&#22495;&#30028;&#38480;&#30340;&#35875;&#35328;&#12290;</title><link>http://arxiv.org/abs/2304.01492</link><description>&lt;p&gt;
&#32479;&#19968;&#23545;&#27604;&#20256;&#36882;&#26694;&#26550;&#19982;&#20256;&#25773;&#32467;&#26500;&#29992;&#20110;&#25552;&#39640;&#20302;&#36164;&#28304;&#35875;&#35328;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Unified Contrastive Transfer Framework with Propagation Structure for Boosting Low-Resource Rumor Detection. (arXiv:2304.01492v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01492
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;&#23545;&#27604;&#20256;&#36882;&#26694;&#26550;&#21644;&#20256;&#25773;&#32467;&#26500;&#65292;&#23558;&#20174;&#20805;&#36275;&#36164;&#28304;&#30340;&#35875;&#35328;&#25968;&#25454;&#23398;&#21040;&#30340;&#29305;&#24449;&#36866;&#24212;&#20110;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#26816;&#27979;&#21040;&#36328;&#36234;&#35821;&#35328;&#21644;&#39046;&#22495;&#30028;&#38480;&#30340;&#35875;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30340;&#35875;&#35328;&#20276;&#38543;&#30528;&#31361;&#21457;&#26032;&#38395;&#25110;&#28909;&#38376;&#35805;&#39064;&#32780;&#20256;&#25773;&#65292;&#36825;&#20005;&#37325;&#38459;&#30861;&#20102;&#30495;&#30456;&#30340;&#20256;&#25773;&#12290;&#29616;&#26377;&#30340;&#35875;&#35328;&#26816;&#27979;&#31639;&#27861;&#23637;&#31034;&#20102;&#22312;&#21069;&#20960;&#22825;&#26032;&#38395;&#19978;&#33391;&#22909;&#24615;&#33021;&#30340;&#21069;&#26223;&#65292;&#20294;&#26159;&#30001;&#20110;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#21644;&#20808;&#21069;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#23427;&#20204;&#24456;&#38590;&#21457;&#29616;&#19982;&#39044;&#26399;&#20107;&#20214;&#26377;&#20851;&#30340;&#35875;&#35328;&#65292;&#29305;&#21035;&#26159;&#22312;&#19981;&#21516;&#35821;&#35328;&#65288;&#21363;&#20302;&#36164;&#28304;&#29615;&#22659;&#65289;&#20013;&#20256;&#25773;&#30340;&#35875;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#23545;&#27604;&#20256;&#36882;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20174;&#20805;&#36275;&#36164;&#28304;&#30340;&#35875;&#35328;&#25968;&#25454;&#23398;&#21040;&#30340;&#29305;&#24449;&#36866;&#24212;&#20110;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#29305;&#24449;&#26469;&#26816;&#27979;&#35875;&#35328;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#20256;&#25773;&#30340;&#35875;&#35328;&#34920;&#31034;&#20026;&#26080;&#21521;&#25299;&#25169;&#32467;&#26500;&#65292;&#28982;&#21518;&#36890;&#36807;&#32479;&#19968;&#23545;&#27604;&#33539;&#24335;&#36827;&#34892;Multi-scale&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26126;&#30830;&#22320;&#31361;&#30772;&#20102;&#39046;&#22495;&#21644;/&#25110;&#35821;&#35328;&#38382;&#39064;&#30340;&#38556;&#30861;&#65292;&#36890;&#36807;&#35821;&#35328;&#23545;&#40784;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The truth is significantly hampered by massive rumors that spread along with breaking news or popular topics. Since there is sufficient corpus gathered from the same domain for model training, existing rumor detection algorithms show promising performance on yesterday's news. However, due to a lack of training data and prior expert knowledge, they are poor at spotting rumors concerning unforeseen events, especially those propagated in different languages (i.e., low-resource regimes). In this paper, we propose a unified contrastive transfer framework to detect rumors by adapting the features learned from well-resourced rumor data to that of the low-resourced. More specifically, we first represent rumor circulated on social media as an undirected topology, and then train a Multi-scale Graph Convolutional Network via a unified contrastive paradigm. Our model explicitly breaks the barriers of the domain and/or language issues, via language alignment and a novel domain-adaptive contrastive 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35780;&#20272;&#20102;&#32842;&#22825;GPT&#26816;&#27979;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#21644;&#20854;&#20182;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#21306;&#20998;&#20154;&#24037;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01487</link><description>&lt;p&gt;
&#32842;&#22825;GPT&#65292;&#36824;&#26159;&#19981;&#32842;&#22825;GPT&#65306;&#36825;&#26159;&#19968;&#20010;&#38382;&#39064;&#65281;
&lt;/p&gt;
&lt;p&gt;
To ChatGPT, or not to ChatGPT: That is the question!. (arXiv:2304.01487v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01487
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35780;&#20272;&#20102;&#32842;&#22825;GPT&#26816;&#27979;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#21644;&#20854;&#20182;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#21306;&#20998;&#20154;&#24037;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;GPT&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#20840;&#29699;&#24863;&#30693;&#12290;&#38543;&#30528;&#32842;&#22825;GPT&#21644;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#23545;&#20110;&#20182;&#20204;&#30340;&#35823;&#29992;&#30340;&#25285;&#24551;&#20063;&#22686;&#21152;&#20102;&#65292;&#20363;&#22914;&#20256;&#25773;&#34394;&#20551;&#28040;&#24687;&#65292;&#25220;&#34989;&#65292;&#25805;&#32437;&#20844;&#20247;&#33286;&#35770;&#65292;&#27450;&#39575;&#21644;&#27450;&#35784;&#12290;&#22240;&#27492;&#65292;&#21306;&#20998;&#20154;&#24037;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#21508;&#31181;&#26816;&#27979;&#26041;&#27861;&#65292;&#20174;&#22522;&#26412;&#30340;&#20108;&#20803;&#20998;&#31867;&#22120;&#21040;&#26356;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#19968;&#20123;&#26816;&#27979;&#25216;&#26415;&#20381;&#36182;&#20110;&#32479;&#35745;&#29305;&#24449;&#25110;&#21477;&#27861;&#27169;&#24335;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;&#21017;&#21253;&#21547;&#35821;&#20041;&#25110;&#19978;&#19979;&#25991;&#20449;&#24687;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23545;&#32842;&#22825;GPT&#26816;&#27979;&#20013;&#26368;&#26032;&#25216;&#26415;&#36827;&#34892;&#20840;&#38754;&#21644;&#29616;&#20195;&#21270;&#30340;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#20854;&#20182;&#26410;&#19987;&#38376;&#22768;&#31216;&#26816;&#27979;&#32842;&#22825;GPT&#29983;&#25104;&#20869;&#23481;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#20197;&#35780;&#20272;&#23427;&#20204;&#22312;&#26816;&#27979;&#32842;&#22825;GPT&#29983;&#25104;&#20869;&#23481;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#22312;&#25105;&#20204;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;&#20154;&#24037;&#32534;&#20889;&#21644;&#32842;&#22825;GPT&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has become a global sensation. As ChatGPT and other Large Language Models (LLMs) emerge, concerns of misusing them in various ways increase, such as disseminating fake news, plagiarism, manipulating public opinion, cheating, and fraud. Hence, distinguishing AI-generated from human-generated becomes increasingly essential. Researchers have proposed various detection methodologies, ranging from basic binary classifiers to more complex deep-learning models. Some detection techniques rely on statistical characteristics or syntactic patterns, while others incorporate semantic or contextual information to improve accuracy. The primary objective of this study is to provide a comprehensive and contemporary assessment of the most recent techniques in ChatGPT detection. Additionally, we evaluated other AI-generated text detection tools that do not specifically claim to detect ChatGPT-generated content to assess their performance in detecting ChatGPT-generated content. For our evaluation,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BCT&#30340;&#20998;&#22359;&#21387;&#32553;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;&#25972;&#20010;Transformer&#27169;&#22411;&#36827;&#34892;&#26356;&#32454;&#31890;&#24230;&#30340;&#21387;&#32553;&#65292;&#23454;&#29616;&#20102;&#38477;&#20302;&#37096;&#32626;&#38376;&#27099;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.01483</link><description>&lt;p&gt;
&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#20998;&#22359;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Blockwise Compression of Transformer-based Models without Retraining. (arXiv:2304.01483v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BCT&#30340;&#20998;&#22359;&#21387;&#32553;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;&#25972;&#20010;Transformer&#27169;&#22411;&#36827;&#34892;&#26356;&#32454;&#31890;&#24230;&#30340;&#21387;&#32553;&#65292;&#23454;&#29616;&#20102;&#38477;&#20302;&#37096;&#32626;&#38376;&#27099;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#27169;&#22411;&#30340;GPT-3&#12289;ChatGPT&#21644;GPT-4&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#20294;&#23427;&#20204;&#30340;&#24040;&#22823;&#35745;&#31639;&#36164;&#28304;&#21644;&#23384;&#20648;&#24320;&#38144;&#20173;&#28982;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BCT&#30340;&#20998;&#22359;&#21387;&#32553;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;&#25972;&#20010;Transformer&#27169;&#22411;&#36827;&#34892;&#26356;&#32454;&#31890;&#24230;&#30340;&#21387;&#32553;&#65292;&#21253;&#25324;&#23884;&#20837;&#12289;&#30697;&#38453;&#20056;&#27861;&#12289;GELU&#12289;Softmax&#12289;&#23618;&#35268;&#33539;&#21270;&#20197;&#21450;&#25152;&#26377;&#20013;&#38388;&#32467;&#26524;&#12290;&#25105;&#20204;&#23545;&#19968;&#20010;&#39640;&#25928;&#27169;&#22411;&#20351;&#29992;BCT&#36827;&#34892;&#20102;&#21387;&#32553;&#24182;&#22312;&#22810;&#20010;GLUE&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#65292;BCT&#21482;&#20250;&#24102;&#26469;&#23569;&#20110;0.90%&#30340;&#20934;&#30830;&#29575;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models, represented by GPT-3, ChatGPT, and GPT-4, have recently attracted increasing interest, research enthusiasm, and business demand. However, their massive computation resources and huge memory footprint are inevitable challenges. To tackle this issue, we propose BCT, a framework of blockwise compression for transformers without retraining, to lower deployment thresholds. BCT achieves more fine-grained compression of the whole transformer, including embedding, matrix multiplication, GELU, Softmax, layer normalization, and all the intermediate results. As a case, we compress an efficient model with BCT and evaluate it on several General Language Understanding Evaluation (GLUE) datasets. The results show that BCT can achieve a less than 0.90% accuracy drop in most tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21521;&#37327;&#25509;&#22320;&#38382;&#39064;&#65292;&#36890;&#36807;&#21306;&#20998;&#20869;&#37096;&#34920;&#31034;&#25509;&#22320;&#30340;&#19981;&#21516;&#26041;&#24335;&#65292;&#24635;&#32467;&#20986;&#20116;&#20010;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2304.01481</link><description>&lt;p&gt;
&#21521;&#37327;&#25509;&#22320;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The Vector Grounding Problem. (arXiv:2304.01481v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21521;&#37327;&#25509;&#22320;&#38382;&#39064;&#65292;&#36890;&#36807;&#21306;&#20998;&#20869;&#37096;&#34920;&#31034;&#25509;&#22320;&#30340;&#19981;&#21516;&#26041;&#24335;&#65292;&#24635;&#32467;&#20986;&#20116;&#20010;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#35821;&#35328;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#33021;&#21147;&#26412;&#36136;&#30340;&#28608;&#28872;&#36777;&#35770;&#12290;&#19981;&#21516;&#20110;&#20154;&#31867;&#65292;&#36825;&#20123;&#27169;&#22411;&#21482;&#33021;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#23398;&#20064;&#35821;&#35328;&#65292;&#27809;&#26377;&#19982;&#30495;&#23454;&#19990;&#30028;&#30340;&#30452;&#25509;&#20132;&#20114;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#23427;&#20204;&#33021;&#22815;&#29983;&#25104;&#20851;&#20110;&#21508;&#31181;&#35805;&#39064;&#20284;&#20046;&#26377;&#24847;&#20041;&#30340;&#25991;&#26412;&#12290;&#36825;&#19968;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#23601;&#37325;&#26032;&#24341;&#36215;&#20102;&#23545;&#32463;&#20856;&#8220;&#31526;&#21495;&#25509;&#22320;&#38382;&#39064;&#8221;&#30340;&#20851;&#27880;&#65292;&#36825;&#20010;&#38382;&#39064;&#36136;&#30097;&#20102;&#32463;&#20856;&#31526;&#21495;AI&#31995;&#32479;&#30340;&#20869;&#37096;&#34920;&#31034;&#21644;&#36755;&#20986;&#33021;&#21542;&#20855;&#26377;&#20869;&#22312;&#24847;&#20041;&#12290;&#19982;&#36825;&#20123;&#31995;&#32479;&#19981;&#21516;&#65292;&#29616;&#20195;LLMs&#26159;&#35745;&#31639;&#21521;&#37327;&#32780;&#19981;&#26159;&#31526;&#21495;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#31995;&#32479;&#20063;&#26377;&#31867;&#20284;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21521;&#37327;&#25509;&#22320;&#38382;&#39064;&#12290;&#26412;&#25991;&#26377;&#20004;&#20010;&#20027;&#35201;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21306;&#20998;&#20102;&#29983;&#29289;&#25110;&#20154;&#24037;&#31995;&#32479;&#20013;&#20869;&#37096;&#34920;&#31034;&#21487;&#20197;&#25509;&#22320;&#30340;&#21508;&#31181;&#26041;&#24335;&#65292;&#30830;&#23450;&#20102;&#20116;&#20010;&#19981;&#21516;&#30340;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
The remarkable performance of large language models (LLMs) on complex linguistic tasks has sparked a lively debate on the nature of their capabilities. Unlike humans, these models learn language exclusively from textual data, without direct interaction with the real world. Nevertheless, they can generate seemingly meaningful text about a wide range of topics. This impressive accomplishment has rekindled interest in the classical 'Symbol Grounding Problem,' which questioned whether the internal representations and outputs of classical symbolic AI systems could possess intrinsic meaning. Unlike these systems, modern LLMs are artificial neural networks that compute over vectors rather than symbols. However, an analogous problem arises for such systems, which we dub the Vector Grounding Problem. This paper has two primary objectives. First, we differentiate various ways in which internal representations can be grounded in biological or artificial systems, identifying five distinct notions 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Semigraph&#30340;&#26041;&#24335;&#36827;&#34892;&#24773;&#24863;&#21644;&#35773;&#21050;&#26816;&#27979;&#65292;&#36890;&#36807;&#25991;&#26723;&#30340;&#24773;&#24863;&#26497;&#24615;&#24471;&#20998;&#26469;&#35782;&#21035;&#25991;&#31456;&#20013;&#30340;&#35773;&#21050;&#65292;&#35813;&#26041;&#27861;&#22312;&#20122;&#39532;&#36874;&#20135;&#21697;&#35780;&#20215;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.01424</link><description>&lt;p&gt;
&#22522;&#20110;Semigraph&#30340;&#24773;&#24863;&#26497;&#24615;&#30340;&#35773;&#21050;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Polarity based Sarcasm Detection using Semigraph. (arXiv:2304.01424v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Semigraph&#30340;&#26041;&#24335;&#36827;&#34892;&#24773;&#24863;&#21644;&#35773;&#21050;&#26816;&#27979;&#65292;&#36890;&#36807;&#25991;&#26723;&#30340;&#24773;&#24863;&#26497;&#24615;&#24471;&#20998;&#26469;&#35782;&#21035;&#25991;&#31456;&#20013;&#30340;&#35773;&#21050;&#65292;&#35813;&#26041;&#27861;&#22312;&#20122;&#39532;&#36874;&#20135;&#21697;&#35780;&#20215;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35773;&#21050;&#26159;&#22312;&#32447;&#24179;&#21488;&#19978;&#32463;&#24120;&#20986;&#29616;&#30340;&#39640;&#32423;&#35821;&#35328;&#34920;&#36798;&#12290;&#35773;&#21050;&#26816;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#23427;&#24433;&#21709;&#24773;&#24863;&#20998;&#26512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Semigraph&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21253;&#25324;Semigraph&#30340;&#26500;&#24314;&#21644;&#35773;&#21050;&#26816;&#27979;&#36807;&#31243;&#12290;&#22312;&#25991;&#26412;&#25991;&#26723;&#30340;&#27169;&#24335;&#20851;&#32852;&#24615;&#20013;&#24314;&#35758;&#20102;Semigraph&#30340;&#21464;&#20307;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;Semigraph&#33719;&#24471;&#25991;&#26723;&#30340;&#35773;&#21050;&#21644;&#38750;&#35773;&#21050;&#24773;&#24863;&#26497;&#24615;&#24471;&#20998;&#12290;&#35773;&#21050;&#24773;&#24863;&#26497;&#24615;&#24471;&#20998;&#34920;&#31034;&#25991;&#26723;&#21464;&#24471;&#35773;&#21050;&#30340;&#21487;&#33021;&#24615;&#12290;&#22522;&#20110;&#24773;&#24863;&#26497;&#24615;&#35780;&#20998;&#27169;&#22411;&#26816;&#27979;&#35773;&#21050;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#35773;&#21050;&#26816;&#27979;&#26041;&#27861;&#12290;&#22312;&#20122;&#39532;&#36874;&#20135;&#21697;&#35780;&#20215;&#20013;&#65292;&#35813;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#21644;f-measure&#20998;&#21035;&#20026;0.87&#12289;0.79&#21644;0.83&#12290;
&lt;/p&gt;
&lt;p&gt;
Sarcasm is an advanced linguistic expression often found on various online platforms. Sarcasm detection is challenging in natural language processing tasks that affect sentiment analysis. This article presents the inventive method of the semigraph, including semigraph construction and sarcasm detection processes. A variation of the semigraph is suggested in the pattern-relatedness of the text document. The proposed method is to obtain the sarcastic and non-sarcastic polarity scores of a document using a semigraph. The sarcastic polarity score represents the possibility that a document will become sarcastic. Sarcasm is detected based on the polarity scoring model. The performance of the proposed model enhances the existing prior art approach to sarcasm detection. In the Amazon product review, the model achieved the accuracy, recall, and f-measure of 0.87, 0.79, and 0.83, respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#20851;&#32852;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#21407;&#29702;&#26469;&#25552;&#21462;Twitter&#25968;&#25454;&#20013;&#20276;&#38543;&#30456;&#20851;&#20107;&#20214;&#30340;&#20851;&#38190;&#35789;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#24403;&#21069;&#31995;&#32479;&#20013;&#38750;&#27491;&#24335;&#35821;&#35328;&#19979;&#20851;&#38190;&#35789;&#25552;&#21462;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#23545;Twitter COVID-19&#25968;&#25454;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2304.01423</link><description>&lt;p&gt;
&#22522;&#20110;&#20107;&#20214;&#19981;&#30830;&#23450;&#24615;&#30340;Twitter&#20027;&#39064;&#19978;&#19979;&#25991;&#21521;&#37327;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
Thematic context vector association based on event uncertainty for Twitter. (arXiv:2304.01423v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#20851;&#32852;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#21407;&#29702;&#26469;&#25552;&#21462;Twitter&#25968;&#25454;&#20013;&#20276;&#38543;&#30456;&#20851;&#20107;&#20214;&#30340;&#20851;&#38190;&#35789;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#24403;&#21069;&#31995;&#32479;&#20013;&#38750;&#27491;&#24335;&#35821;&#35328;&#19979;&#20851;&#38190;&#35789;&#25552;&#21462;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#23545;Twitter COVID-19&#25968;&#25454;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#35789;&#25552;&#21462;&#26159;&#25991;&#26412;&#25366;&#25496;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#22312;Twitter&#25968;&#25454;&#20013;&#25552;&#21462;&#20276;&#38543;&#30456;&#20851;&#20107;&#20214;&#30340;&#20851;&#38190;&#35789;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#25152;&#20351;&#29992;&#30340;&#35821;&#35328;&#38750;&#27491;&#24335;&#12290;&#25340;&#20889;&#38169;&#35823;&#30340;&#21333;&#35789;&#12289;&#32553;&#30053;&#35821;&#21644;&#27495;&#20041;&#24615;&#26415;&#35821;&#30340;&#20351;&#29992;&#23548;&#33268;&#20102;&#38750;&#27491;&#24335;&#12290;&#30446;&#21069;&#31995;&#32479;&#20013;&#25552;&#21462;&#38750;&#27491;&#24335;&#35821;&#35328;&#19979;&#30340;&#20851;&#38190;&#35789;&#26159;&#22522;&#20110;&#27169;&#24335;&#25110;&#20107;&#20214;&#22522;&#30784;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#20851;&#32852;&#30340;&#20351;&#29992;&#20027;&#39064;&#20107;&#20214;&#25552;&#21462;&#19978;&#19979;&#25991;&#20851;&#38190;&#35789;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#21407;&#29702;&#35782;&#21035;&#20107;&#20214;&#30340;&#20027;&#39064;&#19978;&#19979;&#25991;&#12290;&#20351;&#29992;&#31216;&#20026;&#20027;&#39064;&#19978;&#19979;&#25991;&#21521;&#37327;&#30340;&#30690;&#37327;&#23545;&#20027;&#39064;&#19978;&#19979;&#25991;&#36827;&#34892;&#26435;&#37325;&#35745;&#31639;&#65292;&#36825;&#20123;&#30690;&#37327;&#34920;&#31034;&#20107;&#20214;&#30340;&#30830;&#23450;&#25110;&#19981;&#30830;&#23450;&#12290;&#22312;Twitter COVID-19&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#31995;&#32479;&#24182;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;&#35813;&#31995;&#32479;&#20174;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#29305;&#23450;&#20110;&#20107;&#20214;&#30340;&#20027;&#39064;&#19978;&#19979;&#25991;&#21521;&#37327;&#24182;&#23545;&#20854;&#36827;&#34892;&#25490;&#21517;&#12290;&#25552;&#21462;&#30340;&#20027;&#39064;&#19978;&#19979;&#25991;&#21521;&#37327;&#29992;&#20110;&#25512;&#26029;&#20851;&#38190;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;
Keyword extraction is a crucial process in text mining. The extraction of keywords with respective contextual events in Twitter data is a big challenge. The challenging issues are mainly because of the informality in the language used. The use of misspelled words, acronyms, and ambiguous terms causes informality. The extraction of keywords with informal language in current systems is pattern based or event based. In this paper, contextual keywords are extracted using thematic events with the help of data association. The thematic context for events is identified using the uncertainty principle in the proposed system. The thematic contexts are weighed with the help of vectors called thematic context vectors which signifies the event as certain or uncertain. The system is tested on the Twitter COVID-19 dataset and proves to be effective. The system extracts event-specific thematic context vectors from the test dataset and ranks them. The extracted thematic context vectors are used for th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;StatCan&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#28041;&#21450;&#30495;&#23454;&#24847;&#22270;&#30340;&#23545;&#35805;&#36716;&#25442;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#20219;&#21153;&#65306;&#22522;&#20110;&#27491;&#22312;&#36827;&#34892;&#30340;&#23545;&#35805;&#33258;&#21160;&#26816;&#32034;&#30456;&#20851;&#25968;&#25454;&#34920;&#21644;&#22312;&#27599;&#20010;&#22238;&#21512;&#33258;&#21160;&#29983;&#25104;&#36866;&#24403;&#30340;&#20195;&#29702;&#21709;&#24212;&#12290;&#35813;&#30740;&#31350;&#23545;&#29616;&#26377;&#27169;&#22411;&#25345;&#32493;&#25552;&#20986;&#25361;&#25112;&#65292;&#40723;&#21169;&#26356;&#22810;&#22522;&#20110;&#23545;&#35805;&#30340;&#25968;&#25454;&#26816;&#32034;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2304.01412</link><description>&lt;p&gt;
StatCan&#23545;&#35805;&#25968;&#25454;&#38598;&#65306;&#36890;&#36807;&#30495;&#23454;&#24847;&#22270;&#30340;&#23545;&#35805;&#26816;&#32034;&#25968;&#25454;&#34920;
&lt;/p&gt;
&lt;p&gt;
The StatCan Dialogue Dataset: Retrieving Data Tables through Conversations with Genuine Intents. (arXiv:2304.01412v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01412
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;StatCan&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#28041;&#21450;&#30495;&#23454;&#24847;&#22270;&#30340;&#23545;&#35805;&#36716;&#25442;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#20219;&#21153;&#65306;&#22522;&#20110;&#27491;&#22312;&#36827;&#34892;&#30340;&#23545;&#35805;&#33258;&#21160;&#26816;&#32034;&#30456;&#20851;&#25968;&#25454;&#34920;&#21644;&#22312;&#27599;&#20010;&#22238;&#21512;&#33258;&#21160;&#29983;&#25104;&#36866;&#24403;&#30340;&#20195;&#29702;&#21709;&#24212;&#12290;&#35813;&#30740;&#31350;&#23545;&#29616;&#26377;&#27169;&#22411;&#25345;&#32493;&#25552;&#20986;&#25361;&#25112;&#65292;&#40723;&#21169;&#26356;&#22810;&#22522;&#20110;&#23545;&#35805;&#30340;&#25968;&#25454;&#26816;&#32034;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;StatCan&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;19379&#27425;&#20195;&#34920;Statistics Canada&#30340;&#20195;&#29702;&#21644;&#22312;&#32447;&#29992;&#25143;&#20043;&#38388;&#30340;&#23545;&#35805;&#36716;&#25442;&#65292;&#28041;&#21450;&#30495;&#23454;&#24847;&#22270;&#65292;&#20351;&#29992;&#33521;&#35821;&#25110;&#27861;&#35821;&#36827;&#34892;&#65292;&#20195;&#29702;&#20250;&#26816;&#32034;&#21040;5000&#22810;&#20010;&#22797;&#26434;&#25968;&#25454;&#34920;&#20043;&#19968;&#12290;&#22522;&#20110;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20219;&#21153;&#65306;&#65288;1&#65289;&#22522;&#20110;&#27491;&#22312;&#36827;&#34892;&#30340;&#23545;&#35805;&#33258;&#21160;&#26816;&#32034;&#30456;&#20851;&#25968;&#25454;&#34920;&#65292;&#65288;2&#65289;&#22312;&#27599;&#20010;&#22238;&#21512;&#33258;&#21160;&#29983;&#25104;&#36866;&#24403;&#30340;&#20195;&#29702;&#21709;&#24212;&#12290;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#24378;&#22522;&#32447;&#26469;&#30740;&#31350;&#27599;&#20010;&#20219;&#21153;&#30340;&#38590;&#24230;&#12290;&#22312;&#26102;&#38388;&#25968;&#25454;&#20998;&#21106;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#37117;&#38590;&#20197;&#25512;&#24191;&#21040;&#26410;&#26469;&#30340;&#23545;&#35805;&#20013;&#65292;&#24403;&#25105;&#20204;&#20174;&#39564;&#35777;&#38598;&#31227;&#21160;&#21040;&#27979;&#35797;&#38598;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20004;&#20010;&#20219;&#21153;&#30340;&#24615;&#33021;&#37117;&#26174;&#33879;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#22312;&#20309;&#26102;&#36820;&#22238;&#34920;&#26684;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#20219;&#21153;&#23545;&#29616;&#26377;&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#40723;&#21169;&#36827;&#19968;&#27493;&#30740;&#31350;&#22522;&#20110;&#23545;&#35805;&#30340;&#25968;&#25454;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the StatCan Dialogue Dataset consisting of 19,379 conversation turns between agents working at Statistics Canada and online users looking for published data tables. The conversations stem from genuine intents, are held in English or French, and lead to agents retrieving one of over 5000 complex data tables. Based on this dataset, we propose two tasks: (1) automatic retrieval of relevant tables based on a on-going conversation, and (2) automatic generation of appropriate agent responses at each turn. We investigate the difficulty of each task by establishing strong baselines. Our experiments on a temporal data split reveal that all models struggle to generalize to future conversations, as we observe a significant drop in performance across both tasks when we move from the validation to the test set. In addition, we find that response generation models struggle to decide when to return a table. Considering that the tasks pose significant challenges to existing models, we enc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#21517;&#20026; Pythia &#30340;&#24037;&#20855;&#22871;&#20214;&#65292;&#21253;&#21547; 16 &#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#22823;&#23567;&#20174; 70M &#21040; 12B &#21442;&#25968;&#19981;&#31561;&#12290;Pythia &#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#22312;&#22810;&#20010;&#39046;&#22495;&#24320;&#23637;&#30740;&#31350;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#20960;&#20010;&#26032;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#22312;&#35760;&#24518;&#12289;&#24212;&#29992;&#23569;&#37327;&#25968;&#25454;&#26102;&#30340;&#25928;&#26524;&#20197;&#21450;&#20943;&#23569;&#24615;&#21035;&#20559;&#35265;&#31561;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2304.01373</link><description>&lt;p&gt;
Pythia&#65306;&#19968;&#22871;&#29992;&#20110;&#36328;&#35757;&#32451;&#21644;&#25193;&#23637;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling. (arXiv:2304.01373v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#21517;&#20026; Pythia &#30340;&#24037;&#20855;&#22871;&#20214;&#65292;&#21253;&#21547; 16 &#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#22823;&#23567;&#20174; 70M &#21040; 12B &#21442;&#25968;&#19981;&#31561;&#12290;Pythia &#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#22312;&#22810;&#20010;&#39046;&#22495;&#24320;&#23637;&#30740;&#31350;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#20960;&#20010;&#26032;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#22312;&#35760;&#24518;&#12289;&#24212;&#29992;&#23569;&#37327;&#25968;&#25454;&#26102;&#30340;&#25928;&#26524;&#20197;&#21450;&#20943;&#23569;&#24615;&#21035;&#20559;&#35265;&#31561;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#21517;&#20026;Pythia&#30340;&#24037;&#20855;&#22871;&#20214;&#65292;&#20854;&#20013;&#21253;&#25324;16&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#37117;&#26159;&#22312;&#23436;&#20840;&#30456;&#21516;&#30340;&#39034;&#24207;&#19979;&#20174;&#20844;&#20849;&#25968;&#25454;&#20013;&#35757;&#32451;&#32780;&#26469;&#30340;&#65292;&#22823;&#23567;&#20174;70M&#21040;12B&#21442;&#25968;&#19981;&#31561;&#12290;&#20316;&#32773;&#20844;&#24320;&#20102;&#36825;16&#20010;&#27169;&#22411;&#30340;154&#20010;&#26816;&#26597;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#24037;&#20855;&#20197;&#19979;&#36733;&#21644;&#37325;&#26500;&#27169;&#22411;&#30340;exact training dataloaders&#20197;&#36827;&#34892;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Pythia&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#23545;&#35760;&#24518;&#12289;&#20943;&#23569;&#24615;&#21035;&#20559;&#35265;&#31561;&#26041;&#38754;&#30340;&#26032;&#39062;&#30740;&#31350;&#32467;&#26524;&#65292;&#24182;&#28436;&#31034;&#20102;&#36825;&#31181;&#39640;&#24230;&#25511;&#21046;&#30340;&#35774;&#32622;&#22914;&#20309;&#29992;&#20110;&#33719;&#24471;&#26377;&#20851;&#35821;&#35328;&#27169;&#22411;&#21450;&#20854;&#35757;&#32451;&#21160;&#24577;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at https://github.com/EleutherAI/pythia.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#36328;&#35821;&#35328;&#25220;&#34989;&#26816;&#27979;&#26041;&#27861;&#65292;&#19981;&#20381;&#36182;&#26426;&#22120;&#32763;&#35793;&#21644;&#35789;&#20041;&#28040;&#27495;&#65292;&#20351;&#29992;&#24320;&#25918;&#30340;&#22810;&#35821;&#35328;&#21516;&#20041;&#35789;&#24211;&#36827;&#34892;&#20505;&#36873;&#26816;&#32034;&#20219;&#21153;&#21644;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#22810;&#35821;&#35328;BERT&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.01352</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#36328;&#35821;&#35328;&#25220;&#34989;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Simple and Effective Method of Cross-Lingual Plagiarism Detection. (arXiv:2304.01352v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01352
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#36328;&#35821;&#35328;&#25220;&#34989;&#26816;&#27979;&#26041;&#27861;&#65292;&#19981;&#20381;&#36182;&#26426;&#22120;&#32763;&#35793;&#21644;&#35789;&#20041;&#28040;&#27495;&#65292;&#20351;&#29992;&#24320;&#25918;&#30340;&#22810;&#35821;&#35328;&#21516;&#20041;&#35789;&#24211;&#36827;&#34892;&#20505;&#36873;&#26816;&#32034;&#20219;&#21153;&#21644;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#22810;&#35821;&#35328;BERT&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36328;&#35821;&#35328;&#25220;&#34989;&#26816;&#27979;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22823;&#37327;&#30340;&#35821;&#35328;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24320;&#25918;&#30340;&#22810;&#35821;&#35328;&#21516;&#20041;&#35789;&#24211;&#36827;&#34892;&#20505;&#36873;&#26816;&#32034;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#22810;&#35821;&#35328;BERT&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#22312;&#20351;&#29992;&#26102;&#19981;&#20381;&#36182;&#26426;&#22120;&#32763;&#35793;&#21644;&#35789;&#20041;&#28040;&#27495;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#35768;&#22810;&#35821;&#35328;&#65292;&#21253;&#25324;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#29616;&#26377;&#21644;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#22312;&#27861;&#35821;&#12289;&#20420;&#35821;&#21644;&#20122;&#32654;&#23612;&#20122;&#35821;&#31561;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a simple cross-lingual plagiarism detection method applicable to a large number of languages. The presented approach leverages open multilingual thesauri for candidate retrieval task and pre-trained multilingual BERT-based language models for detailed analysis. The method does not rely on machine translation and word sense disambiguation when in use, and therefore is suitable for a large number of languages, including under-resourced languages. The effectiveness of the proposed approach is demonstrated for several existing and new benchmarks, achieving state-of-the-art results for French, Russian, and Armenian languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#24230;&#30340;&#31649;&#36947;&#31574;&#30053;&#21644;&#26356;&#22909;&#30340;&#26631;&#35760;&#21270;&#26041;&#27861;&#26469;&#25552;&#39640;&#21270;&#23398;&#29289;&#36136;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#25552;&#21462;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.01344</link><description>&lt;p&gt;
&#21270;&#23398;&#29289;&#36136;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#25552;&#21462;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65306;&#26356;&#22909;&#30340;&#26631;&#35760;&#21270;&#21644;&#22522;&#20110;&#36328;&#24230;&#30340;&#31649;&#36947;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
End-to-End Models for Chemical-Protein Interaction Extraction: Better Tokenization and Span-Based Pipeline Strategies. (arXiv:2304.01344v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#24230;&#30340;&#31649;&#36947;&#31574;&#30053;&#21644;&#26356;&#22909;&#30340;&#26631;&#35760;&#21270;&#26041;&#27861;&#26469;&#25552;&#39640;&#21270;&#23398;&#29289;&#36136;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#25552;&#21462;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#20851;&#31995;&#25552;&#21462;(E2ERE)&#26159;&#20449;&#24687;&#25552;&#21462;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#23588;&#20854;&#22914;&#27492;&#65292;&#22240;&#20026;&#31185;&#23398;&#25991;&#29486;&#25968;&#37327;&#32487;&#32493;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;E2ERE&#36890;&#24120;&#28041;&#21450;&#35782;&#21035;&#23454;&#20307;(&#25110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER))&#21644;&#20851;&#32852;&#20851;&#31995;&#65292;&#32780;&#22823;&#22810;&#25968;E2ERE&#20219;&#21153;&#21482;&#26159;&#20551;&#23450;&#23454;&#20307;&#24050;&#32463;&#25552;&#20379;&#65292;&#24182;&#26368;&#32456;&#25191;&#34892;&#20851;&#31995;&#20998;&#31867;&#12290;&#30001;&#20110;NER&#20013;&#30340;&#38169;&#35823;&#21487;&#33021;&#23548;&#33268;RE&#20013;&#30340;&#26356;&#22810;&#38169;&#35823;&#65292;E2ERE&#26412;&#36136;&#19978;&#27604;&#20165;RE&#26356;&#38590;&#12290;&#29983;&#29289;&#21307;&#23398;E2ERE&#20013;&#30340;&#19968;&#20010;&#22797;&#26434;&#25968;&#25454;&#38598;&#26159;ChemProt&#25968;&#25454;&#38598;(BioCreative VI,2017)&#65292;&#35813;&#25968;&#25454;&#38598;&#22312;&#31185;&#23398;&#25991;&#29486;&#20013;&#35782;&#21035;&#21270;&#23398;&#21270;&#21512;&#29289;&#21644;&#22522;&#22240;/&#34507;&#30333;&#36136;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;ChemProt&#34987;&#21253;&#25324;&#22312;&#25152;&#26377;&#26368;&#26032;&#30340;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#21253;&#25324;BLUE&#12289;BLURB&#21644;BigBio&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#21644;&#20854;&#20182;&#21333;&#29420;&#30340;&#21162;&#21147;&#36890;&#24120;&#27809;&#26377;&#31471;&#21040;&#31471;&#30340;&#22788;&#29702;ChemProt&#12290;&#22312;&#36825;&#20010;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#36328;&#24230;&#30340;&#31649;&#36947;&#31574;&#30053;&#21644;&#26356;&#22909;&#30340;&#26631;&#35760;&#21270;&#26041;&#27861;&#26469;&#36827;&#34892;E2ERE&#65292;&#20026;ChemProt&#25968;&#25454;&#38598;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#27169;&#22411;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end relation extraction (E2ERE) is an important task in information extraction, more so for biomedicine as scientific literature continues to grow exponentially. E2ERE typically involves identifying entities (or named entity recognition (NER)) and associated relations, while most RE tasks simply assume that the entities are provided upfront and end up performing relation classification. E2ERE is inherently more difficult than RE alone given the potential snowball effect of errors from NER leading to more errors in RE. A complex dataset in biomedical E2ERE is the ChemProt dataset (BioCreative VI, 2017) that identifies relations between chemical compounds and genes/proteins in scientific literature. ChemProt is included in all recent biomedical natural language processing benchmarks including BLUE, BLURB, and BigBio. However, its treatment in these benchmarks and in other separate efforts is typically not end-to-end, with few exceptions. In this effort, we employ a span-based pipe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20123; NLP &#25216;&#24039;&#65292;&#21487;&#29992;&#20110;&#39640;&#25928;&#29983;&#20135;&#33258;&#23450;&#20041;&#20107;&#20214;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#35757;&#32451;&#20998;&#31867;&#22120;&#12289;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#35782;&#21035;&#34892;&#21160;&#32773;&#21644;&#25509;&#25910;&#32773;&#31561;&#12290;</title><link>http://arxiv.org/abs/2304.01331</link><description>&lt;p&gt;
&#19981;&#20351;&#29992;&#35789;&#20856;&#21019;&#24314;&#33258;&#23450;&#20041;&#30340;&#20107;&#20214;&#25968;&#25454;&#65306;&#19968;&#20123;&#35776;&#31373;
&lt;/p&gt;
&lt;p&gt;
Creating Custom Event Data Without Dictionaries: A Bag-of-Tricks. (arXiv:2304.01331v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20123; NLP &#25216;&#24039;&#65292;&#21487;&#29992;&#20110;&#39640;&#25928;&#29983;&#20135;&#33258;&#23450;&#20041;&#20107;&#20214;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#35757;&#32451;&#20998;&#31867;&#22120;&#12289;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#35782;&#21035;&#34892;&#21160;&#32773;&#21644;&#25509;&#25910;&#32773;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#25968;&#25454;&#25351;&#30340;&#26159;&#20174;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#30340;&#32467;&#26500;&#21270;&#35760;&#24405;&#65292;&#25551;&#36848;&#8220;&#35841;&#23545;&#35841;&#20570;&#20102;&#20160;&#20040;&#8221;&#65292;&#26159;&#22269;&#38469;&#25919;&#27835;&#23398;&#32773;&#30340;&#37325;&#35201;&#25968;&#25454;&#26469;&#28304;&#12290;&#30001;&#20110;&#24320;&#21457;&#26032;&#30340;&#20107;&#20214;&#25968;&#25454;&#38598;&#30340;&#39640;&#25104;&#26412;&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;&#20381;&#36182;&#25163;&#24037;&#26500;&#24314;&#23383;&#20856;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#22823;&#22411;&#39044;&#20808;&#23384;&#22312;&#30340;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;ICEWS&#65292;&#32780;&#19981;&#26159;&#24320;&#21457;&#38024;&#23545;&#20854;&#29305;&#23450;&#30740;&#31350;&#38382;&#39064;&#36827;&#34892;&#20102;&#20248;&#21270;&#30340;&#23450;&#21046;&#20107;&#20214;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20123;&#35776;&#31373;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#23450;&#21046;&#20107;&#20214;&#25968;&#25454;&#29983;&#20135;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20123;&#25216;&#26415;&#65292;&#22914;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26469;&#35757;&#32451;&#20107;&#20214;&#31867;&#21035;&#20998;&#31867;&#22120;&#65292;&#22312;&#25991;&#26412;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#26469;&#35782;&#21035;&#34892;&#21160;&#32773;&#21644;&#21160;&#20316;&#30340;&#25509;&#25910;&#32773;&#65292;&#20197;&#21450;&#20351;&#29992;NLP&#30340;&#39044;&#35757;&#32451;&#8220;&#38382;&#31572;&#8221;&#27169;&#22411;&#26469;&#35299;&#20915;&#25552;&#21450;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event data, or structured records of ``who did what to whom'' that are automatically extracted from text, is an important source of data for scholars of international politics. The high cost of developing new event datasets, especially using automated systems that rely on hand-built dictionaries, means that most researchers draw on large, pre-existing datasets such as ICEWS rather than developing tailor-made event datasets optimized for their specific research question. This paper describes a ``bag of tricks'' for efficient, custom event data production, drawing on recent advances in natural language processing (NLP) that allow researchers to rapidly produce customized event datasets. The paper introduces techniques for training an event category classifier with active learning, identifying actors and the recipients of actions in text using large language models and standard machine learning classifiers and pretrained ``question-answering'' models from NLP, and resolving mentions of ac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#25991;&#26723;&#30456;&#20284;&#24615;&#31639;&#27861;&#65292;&#20998;&#20026;&#19977;&#31867;&#36827;&#34892;&#25506;&#35752;&#65306;&#32479;&#35745;&#31639;&#27861;&#12289;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#21644;&#22522;&#20110;&#35821;&#26009;&#24211;/&#30693;&#35782;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#26368;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#30830;&#23450;&#21738;&#20123;&#31639;&#27861;&#26368;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.01330</link><description>&lt;p&gt;
&#25991;&#26723;&#30456;&#20284;&#24615;&#31639;&#27861;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Comparison of Document Similarity Algorithms. (arXiv:2304.01330v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#25991;&#26723;&#30456;&#20284;&#24615;&#31639;&#27861;&#65292;&#20998;&#20026;&#19977;&#31867;&#36827;&#34892;&#25506;&#35752;&#65306;&#32479;&#35745;&#31639;&#27861;&#12289;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#21644;&#22522;&#20110;&#35821;&#26009;&#24211;/&#30693;&#35782;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#26368;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#30830;&#23450;&#21738;&#20123;&#31639;&#27861;&#26368;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#30456;&#20284;&#24615;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#26368;&#24120;&#29992;&#20110;&#25220;&#34989;&#26816;&#27979;&#21644;&#25991;&#26412;&#25688;&#35201;&#12290;&#22240;&#27492;&#65292;&#25214;&#21040;&#26368;&#26377;&#25928;&#30340;&#25991;&#26723;&#30456;&#20284;&#24615;&#31639;&#27861;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#26377;&#24456;&#22823;&#30340;&#27491;&#38754;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#20247;&#22810;&#25991;&#26723;&#30456;&#20284;&#24615;&#31639;&#27861;&#65292;&#24182;&#30830;&#23450;&#21738;&#20123;&#31639;&#27861;&#26368;&#26377;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#25991;&#26723;&#30456;&#20284;&#24615;&#31639;&#27861;&#20998;&#20026;&#19977;&#31867;&#65306;&#32479;&#35745;&#31639;&#27861;&#12289;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#21644;&#22522;&#20110;&#35821;&#26009;&#24211;/&#30693;&#35782;&#30340;&#31639;&#27861;&#65292;&#26469;&#25506;&#35752;&#26368;&#26377;&#25928;&#30340;&#25991;&#26723;&#30456;&#20284;&#24615;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31995;&#21015;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#26469;&#27604;&#36739;&#27599;&#20010;&#31867;&#21035;&#20013;&#26368;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#35780;&#20272;&#20102;&#27599;&#20010;&#31639;&#27861;&#21487;&#33021;&#29992;&#20110;&#30340;&#25152;&#26377;&#21487;&#33021;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document similarity is an important part of Natural Language Processing and is most commonly used for plagiarism-detection and text summarization. Thus, finding the overall most effective document similarity algorithm could have a major positive impact on the field of Natural Language Processing. This report sets out to examine the numerous document similarity algorithms, and determine which ones are the most useful. It addresses the most effective document similarity algorithm by categorizing them into 3 types of document similarity algorithms: statistical algorithms, neural networks, and corpus/knowledge-based algorithms. The most effective algorithms in each category are also compared in our work using a series of benchmark datasets and evaluations that test every possible area that each algorithm could be used in.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#20851;&#27880;&#20390;&#27979; Cheapfake &#23186;&#20307;&#30340; OOC &#35823;&#29992;&#65292;&#23588;&#20854;&#26159;&#23545;&#30495;&#23454;&#29031;&#29255;&#30340;&#35823;&#29992;&#21644;&#19981;&#31526;&#21512;&#30340;&#22270;&#20687;&#35828;&#26126;&#12290;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#24182;&#27979;&#35797;&#27169;&#22411;&#65292;&#20197;&#26816;&#27979;&#32473;&#23450;&#26679;&#26412;&#26159;&#21542;&#20026; OOC &#65292;&#22522;&#20110; COSMOS &#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.01328</link><description>&lt;p&gt;
&#20390;&#27979;Cheapfake&#30340;&#22823;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Grand Challenge On Detecting Cheapfakes. (arXiv:2304.01328v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01328
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#20851;&#27880;&#20390;&#27979; Cheapfake &#23186;&#20307;&#30340; OOC &#35823;&#29992;&#65292;&#23588;&#20854;&#26159;&#23545;&#30495;&#23454;&#29031;&#29255;&#30340;&#35823;&#29992;&#21644;&#19981;&#31526;&#21512;&#30340;&#22270;&#20687;&#35828;&#26126;&#12290;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#24182;&#27979;&#35797;&#27169;&#22411;&#65292;&#20197;&#26816;&#27979;&#32473;&#23450;&#26679;&#26412;&#26159;&#21542;&#20026; OOC &#65292;&#22522;&#20110; COSMOS &#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Cheapfake&#26159;&#19968;&#20010;&#26032;&#20986;&#29616;&#30340;&#26415;&#35821;&#65292;&#25351;&#30340;&#26159;&#38750;AI&#65288;&#8220;cheap&#8221;&#65289;&#23545;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#31713;&#25913;&#12290;&#23427;&#20204;&#27604;deepfake&#26356;&#26222;&#36941;&#12290;Cheapfake&#23186;&#20307;&#21487;&#20197;&#20351;&#29992;&#22270;&#20687;/&#35270;&#39057;&#32534;&#36753;&#36719;&#20214;&#21019;&#24314;&#65292;&#20063;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;&#36719;&#20214;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#22312;&#35823;&#23548;&#24615;&#22768;&#26126;&#26049;&#36793;&#20998;&#20139;&#23186;&#20307;&#26469;&#31616;&#21333;&#22320;&#25913;&#21464;&#22270;&#20687;/&#35270;&#39057;&#30340;&#32972;&#26223;&#12290;&#36825;&#31181;&#32972;&#26223;&#30340;&#20462;&#25913;&#34987;&#31216;&#20026;&#23186;&#20307;&#30340;&#19978;&#19979;&#25991;&#65288;OOC&#65289;&#35823;&#29992;&#12290;&#19982;&#20266;&#36896;&#23186;&#20307;&#19981;&#21516;&#65292;OOC&#23186;&#20307;&#26356;&#38590;&#26816;&#27979;&#65292;&#22240;&#20026;&#22270;&#20687;&#21644;&#35270;&#39057;&#24182;&#26410;&#34987;&#31713;&#25913;&#12290;&#22312;&#36825;&#20010;&#25361;&#25112;&#20013;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#20851;&#27880;&#20390;&#27979;OOC&#22270;&#20687;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#26159;&#22312;&#26032;&#38395;&#25253;&#36947;&#20013;&#65292;&#23545;&#30495;&#23454;&#29031;&#29255;&#30340;&#35823;&#29992;&#19982;&#19981;&#31526;&#21512;&#30340;&#22270;&#20687;&#35828;&#26126;&#12290;&#36825;&#20010;&#25361;&#25112;&#30340;&#30446;&#30340;&#26159;&#24320;&#21457;&#21644;&#22522;&#20934;&#27979;&#35797;&#21487;&#20197;&#29992;&#26469;&#26816;&#27979;&#32473;&#23450;&#26679;&#26412;&#65288;&#26032;&#38395;&#22270;&#20687;&#21644;&#30456;&#20851;&#30340;&#26631;&#39064;&#65289;&#26159;&#21542;&#20026;OOC&#30340;&#27169;&#22411;&#65292;&#22522;&#20110;&#26368;&#36817;&#32534;&#21046;&#30340;COSMOS&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cheapfake is a recently coined term that encompasses non-AI ("cheap") manipulations of multimedia content. Cheapfakes are known to be more prevalent than deepfakes. Cheapfake media can be created using editing software for image/video manipulations, or even without using any software, by simply altering the context of an image/video by sharing the media alongside misleading claims. This alteration of context is referred to as out-of-context (OOC) misuse of media. OOC media is much harder to detect than fake media, since the images and videos are not tampered. In this challenge, we focus on detecting OOC images, and more specifically the misuse of real photographs with conflicting image captions in news items. The aim of this challenge is to develop and benchmark models that can be used to detect whether given samples (news image and associated captions) are OOC, based on the recently compiled COSMOS dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#27874;&#26031;-&#38463;&#25289;&#20271;&#25991;&#23383;&#36827;&#34892;&#35821;&#35328;&#35782;&#21035;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#20351;&#29992;&#19968;&#32452;&#26377;&#30417;&#30563;&#25216;&#26415;&#23558;&#21477;&#23376;&#20998;&#31867;&#20026;&#23427;&#20204;&#30340;&#35821;&#35328;&#20197;&#21450;&#25552;&#20986;&#19968;&#20010;&#38024;&#23545;&#20998;&#31867;&#22120;&#28151;&#28102;&#35821;&#35328;&#31751;&#30340;&#20998;&#23618;&#27169;&#22411;&#12290;&#23454;&#39564;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.01322</link><description>&lt;p&gt;
PALI&#65306;&#27874;&#26031;-&#38463;&#25289;&#20271;&#25991;&#23383;&#35821;&#35328;&#35782;&#21035;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
PALI: A Language Identification Benchmark for Perso-Arabic Scripts. (arXiv:2304.01322v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#27874;&#26031;-&#38463;&#25289;&#20271;&#25991;&#23383;&#36827;&#34892;&#35821;&#35328;&#35782;&#21035;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#20351;&#29992;&#19968;&#32452;&#26377;&#30417;&#30563;&#25216;&#26415;&#23558;&#21477;&#23376;&#20998;&#31867;&#20026;&#23427;&#20204;&#30340;&#35821;&#35328;&#20197;&#21450;&#25552;&#20986;&#19968;&#20010;&#38024;&#23545;&#20998;&#31867;&#22120;&#28151;&#28102;&#35821;&#35328;&#31751;&#30340;&#20998;&#23618;&#27169;&#22411;&#12290;&#23454;&#39564;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27874;&#26031;-&#38463;&#25289;&#20271;&#25991;&#23383;&#26159;&#19990;&#30028;&#21508;&#22320;&#24191;&#27867;&#37319;&#29992;&#21644;&#20351;&#29992;&#30340;&#19968;&#31867;&#25991;&#23383;&#12290;&#20351;&#29992;&#36825;&#31181;&#25991;&#23383;&#36827;&#34892;&#35821;&#35328;&#35782;&#21035;&#23545;&#35821;&#35328;&#25216;&#26415;&#33267;&#20851;&#37325;&#35201;&#65292;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#24773;&#20917;&#19979;&#36739;&#20855;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#37325;&#28857;&#25506;&#35752;&#20351;&#29992;&#27874;&#26031;-&#38463;&#25289;&#20271;&#25991;&#23383;&#36827;&#34892;&#35821;&#35328;&#35782;&#21035;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#21452;&#35821;&#31038;&#21306;&#20013;&#8220;&#38750;&#20256;&#32479;&#8221;&#30340;&#20070;&#20889;&#26041;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#32452;&#26377;&#30417;&#30563;&#25216;&#26415;&#23558;&#21477;&#23376;&#20998;&#31867;&#20026;&#23427;&#20204;&#30340;&#35821;&#35328;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#20998;&#31867;&#22120;&#24120;&#24120;&#28151;&#28102;&#30340;&#35821;&#35328;&#31751;&#30340;&#20998;&#23618;&#27169;&#22411;&#12290;&#25105;&#20204;&#23454;&#39564;&#30340;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Perso-Arabic scripts are a family of scripts that are widely adopted and used by various linguistic communities around the globe. Identifying various languages using such scripts is crucial to language technologies and challenging in low-resource setups. As such, this paper sheds light on the challenges of detecting languages using Perso-Arabic scripts, especially in bilingual communities where ``unconventional'' writing is practiced. To address this, we use a set of supervised techniques to classify sentences into their languages. Building on these, we also propose a hierarchical model that targets clusters of languages that are more often confused by the classifiers. Our experiment results indicate the effectiveness of our solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#24403;&#22320;&#26032;&#38395;&#32593;&#31449;&#12289;&#30005;&#21488;&#21644;&#30000;&#37326;&#35843;&#26597;&#31561;&#26041;&#27861;&#35299;&#20915;&#21335;&#37096;&#24211;&#23572;&#24503;&#35821;&#21644;&#25289;&#22522;&#35821;&#32570;&#20047;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#24211;&#23572;&#24503;&#35821;&#21644;&#25166;&#25166;&#26684;&#33098;&#23612;&#35821;&#30340;&#20854;&#20182;&#21464;&#20307;&#29615;&#22659;&#19979;&#36827;&#34892;&#35821;&#35328;&#35782;&#21035;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.01319</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#25216;&#26415;&#20013;&#30340;&#35821;&#26009;&#24211;&#21019;&#24314;&#26041;&#27861;&#65306;&#20197;&#21335;&#37096;&#24211;&#23572;&#24503;&#35821;&#21644;&#25289;&#22522;&#35821;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Approaches to Corpus Creation for Low-Resource Language Technology: the Case of Southern Kurdish and Laki. (arXiv:2304.01319v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#24403;&#22320;&#26032;&#38395;&#32593;&#31449;&#12289;&#30005;&#21488;&#21644;&#30000;&#37326;&#35843;&#26597;&#31561;&#26041;&#27861;&#35299;&#20915;&#21335;&#37096;&#24211;&#23572;&#24503;&#35821;&#21644;&#25289;&#22522;&#35821;&#32570;&#20047;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#24211;&#23572;&#24503;&#35821;&#21644;&#25166;&#25166;&#26684;&#33098;&#23612;&#35821;&#30340;&#20854;&#20182;&#21464;&#20307;&#29615;&#22659;&#19979;&#36827;&#34892;&#35821;&#35328;&#35782;&#21035;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#20302;&#36164;&#28304;&#21644;&#28626;&#21361;&#35821;&#35328;&#31038;&#21306;&#20013;&#35821;&#35328;&#25968;&#25454;&#32570;&#20047;&#30340;&#22256;&#25200;&#65292;&#21335;&#37096;&#24211;&#23572;&#24503;&#35821;&#21644;&#25289;&#22522;&#35821;&#32570;&#20047;&#26377;&#25928;&#24037;&#20855;&#65292;&#20165;&#26377;&#26497;&#23569;&#37327;&#30340;&#36164;&#28304;&#21487;&#20379;&#20351;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#21033;&#29992;&#24403;&#22320;&#26032;&#38395;&#32593;&#31449;&#20869;&#23481;&#12289;&#25773;&#25918;&#21335;&#37096;&#24211;&#23572;&#24503;&#35821;&#20869;&#23481;&#30340;&#24403;&#22320;&#30005;&#21488;&#20197;&#21450;Laki&#30340;&#30000;&#37326;&#35843;&#26597;&#31561;&#26041;&#27861;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#36825;&#20123;&#20302;&#36164;&#28304;&#35821;&#35328;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#20070;&#20889;&#21644;&#26631;&#20934;&#21270;&#26041;&#38754;&#20197;&#21450;&#22312;&#26816;&#32034;&#25968;&#25454;&#26469;&#28304;&#21644;&#25968;&#23383;&#21270;&#25163;&#20889;&#20869;&#23481;&#20197;&#21019;&#24314;&#21335;&#37096;&#24211;&#23572;&#24503;&#35821;&#21644;&#25289;&#22522;&#35821;&#30340;&#35821;&#26009;&#24211;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#22312;&#24211;&#23572;&#24503;&#35821;&#21644;&#25166;&#25166;&#26684;&#33098;&#23612;&#35821;&#30340;&#20854;&#20182;&#21464;&#20307;&#29615;&#22659;&#19979;&#36827;&#34892;&#35821;&#35328;&#35782;&#21035;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the major challenges that under-represented and endangered language communities face in language technology is the lack or paucity of language data. This is also the case of the Southern varieties of the Kurdish and Laki languages for which very limited resources are available with insubstantial progress in tools. To tackle this, we provide a few approaches that rely on the content of local news websites, a local radio station that broadcasts content in Southern Kurdish and fieldwork for Laki. In this paper, we describe some of the challenges of such under-represented languages, particularly in writing and standardization, and also, in retrieving sources of data and retro-digitizing handwritten content to create a corpus for Southern Kurdish and Laki. In addition, we study the task of language identification in light of the other variants of Kurdish and Zaza-Gorani languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#20250;&#35805;&#25968;&#25454;&#38598;XSGD&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#40784;&#25552;&#31034;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#20219;&#21153;&#30340;NLI-based&#21644;vanilla&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;&#25554;&#27133;&#22635;&#20805;&#21644;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.01295</link><description>&lt;p&gt;
&#26377;&#25928;&#22320;&#23545;&#40784;&#36328;&#35821;&#35328;&#20250;&#35805;&#20219;&#21153;&#30340;&#25552;&#31034;&#35843;&#25972;&#36328;&#35821;&#35328;&#36716;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning. (arXiv:2304.01295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#20250;&#35805;&#25968;&#25454;&#38598;XSGD&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#40784;&#25552;&#31034;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#20219;&#21153;&#30340;NLI-based&#21644;vanilla&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;&#25554;&#27133;&#22635;&#20805;&#21644;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#23545;&#20110;&#20250;&#35805;&#20219;&#21153;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;XSGD&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;Schema-Guided Dialogue&#65288;SGD&#65289;&#32763;&#35793;&#25104;105&#31181;&#20854;&#20182;&#35821;&#35328;&#30340;&#24179;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#20250;&#35805;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#40784;&#30340;&#36328;&#35821;&#35328;&#34920;&#31034;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#40784;&#25552;&#31034;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#65306;NLI-based&#21644;vanilla&#20998;&#31867;&#22120;&#65292;&#24182;&#27979;&#35797;&#20102;&#23545;&#40784;&#25552;&#31034;&#25152;&#23454;&#29616;&#30340;&#36328;&#35821;&#35328;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#23545;&#35805;&#20219;&#21153;&#65288;&#25554;&#27133;&#22635;&#20805;&#21644;&#24847;&#22270;&#20998;&#31867;&#65289;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual transfer of language models trained on high-resource languages like English has been widely studied for many NLP tasks, but focus on conversational tasks has been rather limited. This is partly due to the high cost of obtaining non-English conversational data, which results in limited coverage. In this work, we introduce XSGD, a parallel and large-scale multilingual conversation dataset that we created by translating the English-only Schema-Guided Dialogue (SGD) dataset (Rastogi et al., 2020) into 105 other languages. XSGD contains approximately 330k utterances per language. To facilitate aligned cross-lingual representations, we develop an efficient prompt-tuning-based method for learning alignment prompts. We also investigate two different classifiers: NLI-based and vanilla classifiers, and test cross-lingual capability enabled by the aligned prompts. We evaluate our model's cross-lingual generalization capabilities on two conversation tasks: slot-filling and intent cla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31995;&#32479;&#35770;&#36807;&#31243;&#20998;&#26512;&#65288;STPA&#65289;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#37319;&#29992;ChatGPT&#23545;&#33258;&#21160;&#32039;&#24613;&#21046;&#21160;&#65288;AEB&#65289;&#31995;&#32479;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#37325;&#22797;&#21452;&#24037;&#20132;&#20114;&#26041;&#27861;&#26159;&#26368;&#26377;&#25928;&#30340;&#65292;&#24182;&#26174;&#30528;&#25552;&#39640;&#20102;STPA&#30340;&#36136;&#37327;&#12290;&#26412;&#30740;&#31350;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#24212;&#29992;&#20110;&#23433;&#20840;&#20998;&#26512;&#65292;&#24182;&#20026;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.01246</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#23433;&#20840;&#20998;&#26512;&#65306;&#32842;&#22825;GPT&#22312;STPA&#26696;&#20363;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Safety Analysis in the Era of Large Language Models: A Case Study of STPA using ChatGPT. (arXiv:2304.01246v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31995;&#32479;&#35770;&#36807;&#31243;&#20998;&#26512;&#65288;STPA&#65289;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#37319;&#29992;ChatGPT&#23545;&#33258;&#21160;&#32039;&#24613;&#21046;&#21160;&#65288;AEB&#65289;&#31995;&#32479;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#37325;&#22797;&#21452;&#24037;&#20132;&#20114;&#26041;&#27861;&#26159;&#26368;&#26377;&#25928;&#30340;&#65292;&#24182;&#26174;&#30528;&#25552;&#39640;&#20102;STPA&#30340;&#36136;&#37327;&#12290;&#26412;&#30740;&#31350;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#24212;&#29992;&#20110;&#23433;&#20840;&#20998;&#26512;&#65292;&#24182;&#20026;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#21644;BERT&#65292;&#30001;&#20110;&#20854;&#20855;&#26377;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#23545;&#35805;&#65292;&#22312;&#35768;&#22810;&#30693;&#35782;&#39046;&#22495;&#20013;&#20855;&#26377;&#35814;&#32454;&#21644;&#26126;&#30830;&#30340;&#31572;&#26696;&#65292;&#27491;&#22312;&#24341;&#39046;&#19968;&#22330;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#28909;&#28526;&#12290;&#34429;&#28982;LLMs&#27491;&#22312;&#36805;&#36895;&#24212;&#29992;&#20110;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#39046;&#22495;&#65292;&#20294;&#25105;&#20204;&#23545;&#20197;&#19979;&#38382;&#39064;&#24863;&#20852;&#36259;&#65306;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#30340;&#23433;&#20840;&#20998;&#26512;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;LLMs&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#23545;&#33258;&#21160;&#32039;&#24613;&#21046;&#21160;&#65288;AEB&#65289;&#31995;&#32479;&#30340;&#31995;&#32479;&#35770;&#36807;&#31243;&#20998;&#26512;&#65288;STPA&#65289;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;STPA&#26159;&#26368;&#26222;&#36941;&#30340;&#21361;&#38505;&#20998;&#26512;&#25216;&#26415;&#20043;&#19968;&#65292;&#20294;&#23427;&#23384;&#22312;&#35832;&#22810;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#39640;&#22797;&#26434;&#24615;&#21644;&#20027;&#35266;&#24615;&#65292;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;ChatGPT&#30340;&#24212;&#29992;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#32771;&#34385;&#20854;&#19982;&#20154;&#31867;&#19987;&#23478;&#30340;&#20132;&#20114;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#23558;ChatGPT&#32435;&#20837;STPA&#20013;&#30340;&#26041;&#27861;&#65306;&#19968;&#27425;&#24615;&#21333;&#24037;&#20132;&#20114;&#12289;&#37325;&#22797;&#21333;&#24037;&#20132;&#20114;&#21644;&#37325;&#22797;&#21452;&#24037;&#20132;&#20114;&#12290;&#27604;&#36739;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;i&#65289;&#22312;&#27809;&#26377;&#20154;&#31867;&#19987;&#23478;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;ChatGPT&#19981;&#33021;&#20026;STPA&#25552;&#20379;&#36275;&#22815;&#30340;&#20449;&#24687;&#65307;&#65288;ii&#65289;&#19968;&#27425;&#24615;&#21333;&#24037;&#20132;&#20114;&#23545;STPA&#26377;&#24110;&#21161;&#65292;&#20294;&#19981;&#22914;&#37325;&#22797;&#20132;&#20114;&#26377;&#25928;&#65307;&#65288;iii&#65289;&#37325;&#22797;&#21452;&#24037;&#20132;&#20114;&#19968;&#33268;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#26174;&#30528;&#25552;&#39640;&#20102;STPA&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#21487;&#20197;&#24212;&#29992;&#20110;&#23433;&#20840;&#20998;&#26512;&#65292;&#24182;&#20026;AEB&#20197;&#22806;&#30340;&#20854;&#20182;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT and BERT, are leading a new AI heatwave due to its human-like conversations with detailed and articulate answers across many domains of knowledge. While LLMs are being quickly applied to many AI application domains, we are interested in the following question: Can safety analysis for safety-critical systems make use of LLMs? To answer, we conduct a case study of Systems Theoretic Process Analysis (STPA) on Automatic Emergency Brake (AEB) systems using ChatGPT. STPA, one of the most prevalent techniques for hazard analysis, is known to have limitations such as high complexity and subjectivity, which this paper aims to explore the use of ChatGPT to address. Specifically, three ways of incorporating ChatGPT into STPA are investigated by considering its interaction with human experts: one-off simplex interaction, recurring simplex interaction, and recurring duplex interaction. Comparative results reveal that: (i) using ChatGPT without human exp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#35777;&#25454;&#20849;&#25351;&#22270;&#21644;&#35777;&#25454;&#25991;&#26412;&#22270;&#34920;&#26469;&#35299;&#20915;&#20020;&#24202;&#35777;&#25454;&#25512;&#33616;&#20013;&#30340;&#32852;&#31995;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#36890;&#36947;&#24322;&#26500;&#23398;&#20064;&#27169;&#22411;&#26469;&#25903;&#25345;&#20020;&#24202;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.01242</link><description>&lt;p&gt;
&#22312;&#35777;&#25454;&#22270;&#19978;&#36816;&#29992;&#22810;&#36890;&#36947;&#24322;&#26500;&#23398;&#20064;&#22686;&#24378;&#20020;&#24202;&#35777;&#25454;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Enhancing Clinical Evidence Recommendation with Multi-Channel Heterogeneous Learning on Evidence Graphs. (arXiv:2304.01242v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01242
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#35777;&#25454;&#20849;&#25351;&#22270;&#21644;&#35777;&#25454;&#25991;&#26412;&#22270;&#34920;&#26469;&#35299;&#20915;&#20020;&#24202;&#35777;&#25454;&#25512;&#33616;&#20013;&#30340;&#32852;&#31995;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#36890;&#36947;&#24322;&#26500;&#23398;&#20064;&#27169;&#22411;&#26469;&#25903;&#25345;&#20020;&#24202;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35777;&#25454;&#21253;&#25324;&#24739;&#32773;&#12289;&#24178;&#39044;&#65288;&#22914;&#33647;&#29289;&#25110;&#29289;&#29702;&#27835;&#30103;&#65289;&#12289;&#38382;&#39064;&#21644;&#32467;&#26524;&#20043;&#38388;&#30340;&#20851;&#32852;&#21644;&#24433;&#21709;&#12290;&#25512;&#33616;&#20020;&#24202;&#35777;&#25454;&#30340;&#30446;&#30340;&#26159;&#20026;&#21307;&#21153;&#20154;&#21592;&#25552;&#20379;&#30456;&#20851;&#20449;&#24687;&#65292;&#20197;&#25903;&#25345;&#20182;&#20204;&#30340;&#20915;&#31574;&#36807;&#31243;&#24182;&#29983;&#25104;&#26032;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#30340;&#20855;&#20307;&#20219;&#21153;&#20391;&#37325;&#20110;&#26681;&#25454;&#20020;&#24202;&#38382;&#39064;&#25512;&#33616;&#35777;&#25454;&#12290;&#28982;&#32780;&#65292;&#26576;&#20123;&#20020;&#24202;&#38382;&#39064;&#21644;&#30456;&#20851;&#35777;&#25454;&#20043;&#38388;&#30340;&#30452;&#25509;&#32852;&#31995;&#24448;&#24448;&#26159;&#31232;&#30095;&#30340;&#65292;&#36825;&#23601;&#20135;&#29983;&#20102;&#32852;&#31995;&#31232;&#30095;&#24615;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25512;&#33616;&#36866;&#24403;&#30340;&#35777;&#25454;&#65292;&#26377;&#24517;&#35201;&#21516;&#26102;&#21033;&#29992;&#35777;&#25454;&#20043;&#38388;&#30340;&#25299;&#25169;&#20851;&#31995;&#21644;&#25551;&#36848;&#23427;&#20204;&#30340;&#25991;&#26412;&#20449;&#24687;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#20004;&#20010;&#30693;&#35782;&#22270;&#34920;&#65306;&#35777;&#25454;&#20849;&#25351;&#22270;&#21644;&#35777;&#25454;&#25991;&#26412;&#22270;&#34920;&#65292;&#20197;&#34920;&#31034;&#35777;&#25454;&#20803;&#32032;&#20043;&#38388;&#30340;&#25299;&#25169;&#21644;&#35821;&#35328;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#36890;&#36947;&#24322;&#26500;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical evidence encompasses the associations and impacts between patients, interventions (such as drugs or physiotherapy), problems, and outcomes. The goal of recommending clinical evidence is to provide medical practitioners with relevant information to support their decision-making processes and to generate new evidence. Our specific task focuses on recommending evidence based on clinical problems. However, the direct connections between certain clinical problems and related evidence are often sparse, creating a challenge of link sparsity. Additionally, to recommend appropriate evidence, it is essential to jointly exploit both topological relationships among evidence and textual information describing them. To address these challenges, we define two knowledge graphs: an Evidence Co-reference Graph and an Evidence Text Graph, to represent the topological and linguistic relations among evidential elements, respectively. We also introduce a multi-channel heterogeneous learning model a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#24503;&#25289;&#32500;&#36798;&#35821;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#20998;&#31867;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#20197;&#20415;&#23558;&#20854;&#35782;&#21035;&#20026;&#24656;&#21516;&#12289;&#36328;&#24615;&#21035;&#27495;&#35270;&#21644;&#38750;&#21453;LGBT+&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2304.01241</link><description>&lt;p&gt;
&#21457;&#29616;&#24503;&#25289;&#32500;&#36798;&#35821;&#20013;&#30340;&#24656;&#21516;&#21644;&#36328;&#24615;&#21035;&#27495;&#35270;&#65306;&#25506;&#32034;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Detection of Homophobia &amp; Transphobia in Dravidian Languages: Exploring Deep Learning Methods. (arXiv:2304.01241v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#24503;&#25289;&#32500;&#36798;&#35821;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#20998;&#31867;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#20197;&#20415;&#23558;&#20854;&#35782;&#21035;&#20026;&#24656;&#21516;&#12289;&#36328;&#24615;&#21035;&#27495;&#35270;&#21644;&#38750;&#21453;LGBT+&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#36785;&#39554;&#24615;&#20869;&#23481;&#30340;&#22686;&#21152;&#27491;&#22312;&#24433;&#21709;&#22312;&#32447;&#29992;&#25143;&#30340;&#31038;&#20132;&#29983;&#27963;&#12290;&#20351;&#29992;&#20882;&#29359;&#21644;&#20167;&#24680;&#35328;&#35770;&#20351;&#24471;&#31038;&#20132;&#23186;&#20307;&#21464;&#24471;&#26377;&#27602;&#12290;&#24656;&#21516;&#21644;&#36328;&#24615;&#21035;&#27495;&#35270;&#26159;&#38024;&#23545;LGBT+&#31038;&#32676;&#30340;&#20882;&#29359;&#24615;&#35780;&#35770;&#12290;&#21450;&#26102;&#26816;&#27979;&#21644;&#22788;&#29702;&#36825;&#20123;&#35780;&#35770;&#65292;&#21521;&#28041;&#21450;&#27492;&#31867;&#34892;&#20026;&#30340;&#29992;&#25143;&#21457;&#20986;&#35686;&#21578;&#25110;&#21450;&#26102;&#26631;&#35760;&#26159;&#38750;&#24120;&#24517;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#24503;&#25289;&#32500;&#36798;&#35821;&#36825;&#31181;&#34987;&#35748;&#20026;&#26159;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#35821;&#35328;&#20013;&#65292;&#33258;&#21160;&#26816;&#27979;&#27492;&#31867;&#20869;&#23481;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#35797;&#22270;&#25506;&#35752;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#39532;&#26469;&#35821;&#21644;&#27888;&#31859;&#23572;&#35821;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#20998;&#31867;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#20197;&#20415;&#23558;&#20854;&#35782;&#21035;&#20026;&#24656;&#21516;&#12289;&#36328;&#24615;&#21035;&#27495;&#35270;&#21644;&#38750;&#21453;LGBT+&#20869;&#23481;&#12290;&#32780;&#20854;&#20013;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26377;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12289;&#22312;GloVe&#23884;&#20837;&#19979;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#23398;&#20064;&#27169;&#22411;&#65288;&#22810;&#35821;&#35328;BERT&#21644;IndicBERT&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increase in abusive content on online social media platforms is impacting the social life of online users. Use of offensive and hate speech has been making so-cial media toxic. Homophobia and transphobia constitute offensive comments against LGBT+ community. It becomes imperative to detect and handle these comments, to timely flag or issue a warning to users indulging in such behaviour. However, automated detection of such content is a challenging task, more so in Dravidian languages which are identified as low resource languages. Motivated by this, the paper attempts to explore applicability of different deep learning mod-els for classification of the social media comments in Malayalam and Tamil lan-guages as homophobic, transphobic and non-anti-LGBT+content. The popularly used deep learning models- Convolutional Neural Network (CNN), Long Short Term Memory (LSTM) using GloVe embedding and transformer-based learning models (Multilingual BERT and IndicBERT) are applied to the class
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25104;&#21151;&#22320;&#35782;&#21035;&#20986;&#24515;&#29702;&#20581;&#24247;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#30140;&#30171;&#25552;&#21450;&#65292;&#25552;&#39640;&#20102;&#23545;&#30140;&#30171;&#21644;&#24515;&#29702;&#20581;&#24247;&#20043;&#38388;&#20851;&#31995;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.01240</link><description>&lt;p&gt;
&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26041;&#27861;&#35782;&#21035;&#24515;&#29702;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#30140;&#30171;&#25552;&#21450;
&lt;/p&gt;
&lt;p&gt;
Identifying Mentions of Pain in Mental Health Records Text: A Natural Language Processing Approach. (arXiv:2304.01240v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01240
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25104;&#21151;&#22320;&#35782;&#21035;&#20986;&#24515;&#29702;&#20581;&#24247;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#30140;&#30171;&#25552;&#21450;&#65292;&#25552;&#39640;&#20102;&#23545;&#30140;&#30171;&#21644;&#24515;&#29702;&#20581;&#24247;&#20043;&#38388;&#20851;&#31995;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30140;&#30171;&#26159;&#35775;&#38382;&#21307;&#30103;&#36164;&#28304;&#30340;&#24120;&#35265;&#21407;&#22240;&#65292;&#24182;&#19988;&#26159;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#19982;&#24515;&#29702;&#20581;&#24247;&#30340;&#37325;&#21472;&#26041;&#38754;&#12290;&#24515;&#29702;&#20581;&#24247;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#26159;&#30740;&#31350;&#27492;&#37325;&#21472;&#30340;&#33391;&#22909;&#25968;&#25454;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#30140;&#30171;&#30340;&#22823;&#37327;&#20449;&#24687;&#20445;&#23384;&#22312;&#36825;&#20123;&#35760;&#24405;&#30340;&#33258;&#30001;&#25991;&#26412;&#20013;&#65292;&#30001;&#20110;&#20854;&#27495;&#20041;&#24615;&#65292;&#30140;&#30171;&#30340;&#25552;&#21450;&#21576;&#29616;&#20986;&#29420;&#29305;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#12290;&#26412;&#39033;&#30446;&#20351;&#29992;&#21311;&#21517;&#30340;&#24515;&#29702;&#20581;&#24247;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#24211;&#20013;&#30340;&#25968;&#25454;&#12290;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#31867;&#31639;&#27861;&#65292;&#23558;&#21477;&#23376;&#20998;&#31867;&#20026;&#35752;&#35770;&#24739;&#32773;&#30140;&#30171;&#25110;&#19981;&#35752;&#35770;&#12290;&#36825;&#23558;&#26377;&#21161;&#20110;&#20174;&#22823;&#22411;&#25968;&#25454;&#24211;&#20013;&#25552;&#21462;&#30456;&#20851;&#30140;&#30171;&#20449;&#24687;&#65292;&#24182;&#23558;&#36825;&#20123;&#36755;&#20986;&#29992;&#20110;&#36827;&#19968;&#27493;&#30740;&#31350;&#30140;&#30171;&#21644;&#24515;&#29702;&#20581;&#24247;&#12290;&#20849;&#25163;&#21160;&#19977;&#37325;&#27880;&#37322;&#20102;1,985&#20221;&#25991;&#20214;&#65292;&#20197;&#21019;&#24314;&#40644;&#37329;&#26631;&#20934;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#29992;&#20110;&#35757;&#32451;&#19977;&#31181;&#24120;&#29992;&#30340;&#20998;&#31867;&#31639;&#27861;&#12290;&#26368;&#20339;&#27169;&#22411;&#30340;F1&#20998;&#25968;&#20026;0.787&#12290;&#32467;&#26524;&#35777;&#26126;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#35782;&#21035;&#24515;&#29702;&#20581;&#24247;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#30140;&#30171;&#25552;&#21450;&#30340;&#21487;&#34892;&#24615;&#65292;&#36825;&#21487;&#20197;&#25913;&#21892;&#23545;&#30140;&#30171;&#21644;&#24515;&#29702;&#20581;&#24247;&#20043;&#38388;&#20851;&#31995;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pain is a common reason for accessing healthcare resources and is a growing area of research, especially in its overlap with mental health. Mental health electronic health records are a good data source to study this overlap. However, much information on pain is held in the free text of these records, where mentions of pain present a unique natural language processing problem due to its ambiguous nature. This project uses data from an anonymised mental health electronic health records database. The data are used to train a machine learning based classification algorithm to classify sentences as discussing patient pain or not. This will facilitate the extraction of relevant pain information from large databases, and the use of such outputs for further studies on pain and mental health. 1,985 documents were manually triple-annotated for creation of gold standard training data, which was used to train three commonly used classification algorithms. The best performing model achieved an F1-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#31867;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#20256;&#32479;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22312;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#32463;&#36807;&#25913;&#36827;&#21644;&#24494;&#35843;&#30340;Spam-T5&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.01238</link><description>&lt;p&gt;
Spam-T5&#65306;&#22522;&#20110;&#23567;&#26679;&#26412;&#30340;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection. (arXiv:2304.01238v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#31867;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#20256;&#32479;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22312;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#32463;&#36807;&#25913;&#36827;&#21644;&#24494;&#35843;&#30340;Spam-T5&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;BERT-like&#12289;Sentence Transformers&#21644;Seq2Seq&#65289;&#20197;&#21450;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65288;&#22914;&#26420;&#32032;&#36125;&#21494;&#26031;&#21644;LightGBM&#65289;&#22312;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#20013;&#30340;&#20316;&#29992;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#65288;&#23436;&#25972;&#35757;&#32451;&#38598;&#21644;&#23567;&#26679;&#26412;&#65289;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290; &#21457;&#29616;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;LLMs&#20248;&#20110;&#22522;&#32447;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22312;&#23567;&#26679;&#26412;&#24773;&#20917;&#19979;&#12290;&#36825;&#31181;&#36866;&#24212;&#24615;&#20351;LLMs&#22312;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#29420;&#29305;&#30340;&#20248;&#21183;&#65292;&#22240;&#20026;&#26631;&#35760;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#65292;&#24182;&#19988;&#27169;&#22411;&#38656;&#35201;&#32463;&#24120;&#26356;&#26032;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Spam-T5&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26159;&#19987;&#38376;&#20026;&#26816;&#27979;&#30005;&#23376;&#37038;&#20214;&#22403;&#22334;&#32780;&#36827;&#34892;&#20102;&#25913;&#36827;&#21644;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Spam-T5&#27169;&#22411;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the effectiveness of large language models (LLMs) in email spam detection by comparing prominent models from three distinct families: BERT-like, Sentence Transformers, and Seq2Seq. Additionally, we examine well-established machine learning techniques for spam detection, such as Na\"ive Bayes and LightGBM, as baseline methods. We assess the performance of these models across four public datasets, utilizing different numbers of training samples (full training set and few-shot settings). Our findings reveal that, in the majority of cases, LLMs surpass the performance of the popular baseline techniques, particularly in few-shot scenarios. This adaptability renders LLMs uniquely suited to spam detection tasks, where labeled samples are limited in number and models require frequent updates. Additionally, we introduce Spam-T5, a Flan-T5 model that has been specifically adapted and fine-tuned for the purpose of detecting email spam. Our results demonstrate that Spam-T5 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#30693;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32467;&#26524;&#39044;&#27979;&#21644;&#24739;&#32773;&#20998;&#35786;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#24613;&#35786;&#31185;&#20020;&#24202;&#20915;&#31574;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.01233</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#30693;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#24613;&#35786;&#23460;&#32467;&#26524;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Perceiver Language Model for Outcome Prediction in Emergency Department. (arXiv:2304.01233v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#30693;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32467;&#26524;&#39044;&#27979;&#21644;&#24739;&#32773;&#20998;&#35786;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#24613;&#35786;&#31185;&#20020;&#24202;&#20915;&#31574;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#24314;&#27169;&#24050;&#32463;&#22312;&#29983;&#25104;&#20855;&#26377;&#33391;&#22909;&#20934;&#30830;&#24615;&#21644;&#39640;&#35821;&#20041;&#36830;&#36143;&#24615;&#30340;&#20196;&#20154;&#20449;&#26381;&#30340;&#25991;&#26412;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#26159;&#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22686;&#24378;&#36825;&#20123;&#24378;&#22823;&#30340;&#27169;&#22411;&#20197;&#29992;&#20110;&#29305;&#23450;&#30340;&#24212;&#29992;&#31243;&#24207;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#38754;&#21521;&#21307;&#30103;&#20445;&#20581;&#24212;&#29992;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#24314;&#27169;&#12290;&#25105;&#20204;&#23545;&#22522;&#20110;&#19977;&#35282;&#27954;&#21306;&#22495;&#35760;&#24405;&#30340;&#39318;&#24109;&#25237;&#35785;&#20449;&#24687;&#21644;&#29983;&#21629;&#20307;&#24449;&#30340;&#25991;&#26412;&#20449;&#24687;&#36827;&#34892;&#20102;&#32467;&#26524;&#39044;&#27979;&#21644;&#24739;&#32773;&#20998;&#35786;&#12290;&#25105;&#20204;&#25913;&#32534;&#20102;Perceiver&#8212;&#8212;&#19968;&#31181;&#36890;&#29992;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#23427;&#24050;&#32463;&#22312;&#22810;&#20010;&#24212;&#29992;&#31243;&#24207;&#20013;&#23637;&#29616;&#20986;&#21487;&#34892;&#30340;&#32467;&#26524;&#12290;&#30001;&#20110;&#29983;&#21629;&#20307;&#24449;&#27169;&#24577;&#20197;&#34920;&#26684;&#24418;&#24335;&#21576;&#29616;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;Perceiver&#30340;&#20301;&#32622;&#32534;&#30721;&#20197;&#30830;&#20445;&#32622;&#25442;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;MIMIC-IV ED&#25968;&#25454;&#38598;&#19978;&#30340;120K&#27425;&#35775;&#38382;&#26469;&#35780;&#20272;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#23545;&#35786;&#26029;&#20195;&#30721;&#39044;&#27979;&#30340;&#20219;&#21153;&#12290;&#22312;&#23454;&#39564;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;&#25913;&#21892;&#20102;&#39044;&#27979;&#34920;&#29616;&#65292;&#30456;&#27604;&#20110;&#21333;&#27169;&#24577;&#27169;&#22411;&#65292;&#24182;&#19988;Perceiver&#22312;&#20219;&#21153;&#20013;&#32988;&#36807;&#20102;&#20854;&#20182;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#25913;&#36827;&#24613;&#35786;&#31185;&#20020;&#24202;&#20915;&#31574;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language modeling have shown impressive progress in generating compelling text with good accuracy and high semantic coherence. An interesting research direction is to augment these powerful models for specific applications using contextual information. In this work, we explore multi-modal language modeling for healthcare applications. We are interested in outcome prediction and patient triage in hospital emergency department based on text information in chief complaints and vital signs recorded at triage. We adapt Perceiver - a modality-agnostic transformer-based model that has shown promising results in several applications. Since vital-sign modality is represented in tabular format, we modified Perceiver position encoding to ensure permutation invariance. We evaluated the multi-modal language model for the task of diagnosis code prediction using MIMIC-IV ED dataset on 120K visits. In the experimental analysis, we show that mutli-modality improves the prediction performance compared w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#26469;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20026;&#20195;&#30721;&#29983;&#25104;&#21644;&#20195;&#30721;&#25688;&#35201;&#31561;&#20219;&#21153;&#24494;&#35843;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.01228</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#25105;&#25913;&#36827;&#30340;&#26041;&#24335;&#23454;&#29616;&#26356;&#22909;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Better Language Models of Code through Self-Improvement. (arXiv:2304.01228v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#26469;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20026;&#20195;&#30721;&#29983;&#25104;&#21644;&#20195;&#30721;&#25688;&#35201;&#31561;&#20219;&#21153;&#24494;&#35843;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#21508;&#31181;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#22810;&#27169;&#24335;&#30446;&#26631;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#20294;&#26159;&#65292;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#38656;&#35201;&#22823;&#37327;&#30417;&#30563;&#65292;&#24182;&#19988;&#21463;&#21040;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#20197;&#25913;&#21892;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20010;&#26694;&#26550;&#21033;&#29992;&#20102;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#38454;&#27573;&#33719;&#24471;&#30340;&#30693;&#35782;&#26469;&#29983;&#25104;&#20266;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#19979;&#19968;&#27493;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26694;&#26550;&#24212;&#29992;&#21040;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#22914;CodeT5&#12289;CodeBERT&#21644;UnixCoder&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#33879;&#25552;&#39640;&#20102;PLMC&#22312;&#19982;&#20195;&#30721;&#30456;&#20851;&#30340;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#22914;CodeXGLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20195;&#30721;&#25688;&#35201;&#21644;&#20195;&#30721;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models for code (PLMCs) have gained attention in recent research. These models are pre-trained on large-scale datasets using multi-modal objectives. However, fine-tuning them requires extensive supervision and is limited by the size of the dataset provided. We aim to improve this issue by proposing a simple data augmentation framework. Our framework utilizes knowledge gained during the pre-training and fine-tuning stage to generate pseudo data, which is then used as training data for the next step. We incorporate this framework into the state-of-the-art language models, such as CodeT5, CodeBERT, and UnixCoder. The results show that our framework significantly improves PLMCs' performance in code-related sequence generation tasks, such as code summarization and code generation in the CodeXGLUE benchmark.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#8220;&#22522;&#20110;&#25552;&#31034;&#30340;&#24320;&#25918;&#20851;&#31995;&#25277;&#21462;&#8221;&#27169;&#22411;&#65292;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#19981;&#38656;&#35201;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#23454;&#29616;&#20102;&#20840;&#26032;&#30340;&#26080;&#30417;&#30563;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01209</link><description>&lt;p&gt;
PromptORE -- &#19968;&#31181;&#20840;&#26032;&#30340;&#26080;&#30417;&#30563;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PromptORE -- A Novel Approach Towards Fully Unsupervised Relation Extraction. (arXiv:2304.01209v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01209
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#8220;&#22522;&#20110;&#25552;&#31034;&#30340;&#24320;&#25918;&#20851;&#31995;&#25277;&#21462;&#8221;&#27169;&#22411;&#65292;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#19981;&#38656;&#35201;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#23454;&#29616;&#20102;&#20840;&#26032;&#30340;&#26080;&#30417;&#30563;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#20851;&#31995;&#25277;&#21462;&#26088;&#22312;&#35782;&#21035;&#25991;&#26412;&#20013;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#32780;&#22312;&#35757;&#32451;&#26399;&#38388;&#27809;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#21487;&#29992;&#12290;&#36825;&#23545;&#20110;&#27809;&#26377;&#27880;&#37322;&#25968;&#25454;&#38598;&#30340;&#29305;&#23450;&#39046;&#22495;&#20851;&#31995;&#25277;&#21462;&#21644;&#20808;&#39564;&#26410;&#30693;&#20851;&#31995;&#31867;&#22411;&#30340;&#24320;&#25918;&#39046;&#22495;&#20851;&#31995;&#25277;&#21462;&#29305;&#21035;&#30456;&#20851;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#20005;&#37325;&#20381;&#36182;&#20110;&#36229;&#21442;&#25968;&#65292;&#35843;&#25972;&#36825;&#20123;&#36229;&#21442;&#25968;&#36890;&#24120;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#12290;&#20026;&#20102;&#20943;&#36731;&#23545;&#36229;&#21442;&#25968;&#30340;&#20381;&#36182;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PromptORE&#65292;&#21363;&#8220;&#22522;&#20110;&#25552;&#31034;&#30340;&#24320;&#25918;&#20851;&#31995;&#25277;&#21462;&#8221;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#26032;&#30340;&#25552;&#31034;&#35843;&#25972;&#33539;&#20363;&#36866;&#24212;&#20110;&#26080;&#30417;&#30563;&#35774;&#32622;&#65292;&#24182;&#29992;&#23427;&#26469;&#23884;&#20837;&#34920;&#36798;&#20851;&#31995;&#30340;&#21477;&#23376;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;&#36825;&#20123;&#23884;&#20837;&#36827;&#34892;&#32858;&#31867;&#65292;&#21457;&#29616;&#20505;&#36873;&#20851;&#31995;&#65292;&#24182;&#23581;&#35797;&#19981;&#21516;&#30340;&#31574;&#30053;&#26469;&#33258;&#21160;&#20272;&#35745;&#36866;&#24403;&#30340;&#32858;&#31867;&#25968;&#37327;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;PromptORE&#26159;&#31532;&#19968;&#20010;&#19981;&#38656;&#35201;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#26080;&#30417;&#30563;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Relation Extraction (RE) aims to identify relations between entities in text, without having access to labeled data during training. This setting is particularly relevant for domain specific RE where no annotated dataset is available and for open-domain RE where the types of relations are a priori unknown. Although recent approaches achieve promising results, they heavily depend on hyperparameters whose tuning would most often require labeled data. To mitigate the reliance on hyperparameters, we propose PromptORE, a ''Prompt-based Open Relation Extraction'' model. We adapt the novel prompt-tuning paradigm to work in an unsupervised setting, and use it to embed sentences expressing a relation. We then cluster these embeddings to discover candidate relations, and we experiment different strategies to automatically estimate an adequate number of clusters. To the best of our knowledge, PromptORE is the first unsupervised RE model that does not need hyperparameter tuning. Resul
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#31243;&#65292;&#36890;&#36807;&#21033;&#29992;ChatGPT&#19982;&#33258;&#36523;&#23545;&#35805;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#36718;&#32842;&#22825;&#35821;&#26009;&#24211;&#65292;&#24182;&#37319;&#29992;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26469;&#22686;&#24378;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLaMA&#65292;&#24471;&#21040;&#30340;&#27169;&#22411;&#34987;&#21629;&#21517;&#20026;Baize&#65292;&#22312;&#26368;&#23567;&#21270;&#28508;&#22312;&#39118;&#38505;&#30340;&#25252;&#26639;&#19979;&#65292;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.01196</link><description>&lt;p&gt;
Baize:&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#23545;&#35805;&#25968;&#25454;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#30340;&#24320;&#28304;&#32842;&#22825;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data. (arXiv:2304.01196v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01196
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#31243;&#65292;&#36890;&#36807;&#21033;&#29992;ChatGPT&#19982;&#33258;&#36523;&#23545;&#35805;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#36718;&#32842;&#22825;&#35821;&#26009;&#24211;&#65292;&#24182;&#37319;&#29992;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26469;&#22686;&#24378;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLaMA&#65292;&#24471;&#21040;&#30340;&#27169;&#22411;&#34987;&#21629;&#21517;&#20026;Baize&#65292;&#22312;&#26368;&#23567;&#21270;&#28508;&#22312;&#39118;&#38505;&#30340;&#25252;&#26639;&#19979;&#65292;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;&#27169;&#22411;&#65292;&#22914;ChatGPT&#65292;&#23637;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#20247;&#22810;&#39046;&#22495;&#24471;&#21040;&#36805;&#36895;&#24212;&#29992;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#21482;&#33021;&#36890;&#36807;&#21463;&#38480;&#21046;&#30340;API&#36827;&#34892;&#35775;&#38382;&#65292;&#20174;&#32780;&#21046;&#36896;&#20102;&#26032;&#30340;&#30740;&#31350;&#21644;&#39046;&#22495;&#36827;&#23637;&#30340;&#38556;&#30861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#31243;&#65292;&#36890;&#36807;&#21033;&#29992;ChatGPT&#19982;&#33258;&#36523;&#23545;&#35805;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#36718;&#32842;&#22825;&#35821;&#26009;&#24211;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26469;&#22686;&#24378;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLaMA&#12290;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#34987;&#21629;&#21517;&#20026;Baize&#65292;&#22312;&#26368;&#23567;&#21270;&#28508;&#22312;&#39118;&#38505;&#30340;&#25252;&#26639;&#19979;&#65292;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;Baize&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#20165;&#29992;&#20110;&#30740;&#31350;&#30446;&#30340;&#65292;&#21487;&#22312;https://github.com/project-baize/baize&#36827;&#34892;&#19979;&#36733;&#12290;&#22312;&#32447;&#28436;&#31034;&#20063;&#21487;&#22312;https://huggingface.co/spaces/project-baize/baize-lora-7B&#36827;&#34892;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chat models, such as ChatGPT, have shown impressive capabilities and have been rapidly adopted across numerous domains. However, these models are only accessible through a restricted API, creating barriers for new research and progress in the field. We propose a pipeline that can automatically generate a high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a conversation with itself. Subsequently, we employ parameter-efficient tuning to enhance LLaMA, an open-source large language model. The resulting model, named Baize, demonstrates good performance in multi-turn dialogues with guardrails that minimize potential risks. The Baize models and data are released for research purposes only at https://github.com/project-baize/baize. An online demo is also available at https://huggingface.co/spaces/project-baize/baize-lora-7B.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#20934;&#30830;&#24615;&#30340;&#31574;&#30053;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;Polytuplet Loss&#20989;&#25968;&#26469;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#65292;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#26524;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2304.01046</link><description>&lt;p&gt;
&#8220;Polytuplet Loss: &#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#8221;
&lt;/p&gt;
&lt;p&gt;
Polytuplet Loss: A Reverse Approach to Training Reading Comprehension and Logical Reasoning Models. (arXiv:2304.01046v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#20934;&#30830;&#24615;&#30340;&#31574;&#30053;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;Polytuplet Loss&#20989;&#25968;&#26469;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#65292;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#26524;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25972;&#20010;&#23398;&#26657;&#25945;&#32946;&#36807;&#31243;&#20013;&#65292;&#23398;&#29983;&#20204;&#23558;&#21463;&#21040;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#32771;&#39564;&#12290;&#23398;&#29983;&#20204;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#31574;&#30053;&#26469;&#23436;&#25104;&#27492;&#31867;&#32771;&#35797;&#65292;&#20854;&#20013;&#26377;&#20123;&#34987;&#35748;&#20026;&#26159;&#36890;&#24120;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31574;&#30053;&#30340;&#12290;&#36825;&#26679;&#19968;&#31181;&#31574;&#30053;&#28041;&#21450;&#24378;&#35843;&#30456;&#23545;&#20934;&#30830;&#24615;&#32780;&#38750;&#32477;&#23545;&#20934;&#30830;&#24615;&#65292;&#29702;&#35770;&#19978;&#21487;&#20197;&#22312;&#19981;&#23436;&#20840;&#25484;&#25569;&#35299;&#39064;&#25152;&#38656;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#24471;&#20986;&#27491;&#30830;&#31572;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#36825;&#31181;&#31574;&#30053;&#26469;&#35757;&#32451;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#20197;&#35299;&#20915;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;ReClor&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#36923;&#36753;&#25512;&#29702;&#25216;&#33021;&#65292;&#20294;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#31181;&#36890;&#29992;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Polytuplet Loss&#20989;&#25968;&#65292;&#26159;&#19977;&#20803;&#32452;&#25439;&#22833;&#20989;&#25968;&#30340;&#25193;&#23637;&#65292;&#20197;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#32780;&#38750;&#23398;&#20064;&#32477;&#23545;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Throughout schooling, students are tested on reading comprehension and logical reasoning. Students have developed various strategies for completing such exams, some of which are generally thought to outperform others. One such strategy involves emphasizing relative accuracy over absolute accuracy and can theoretically produce the correct answer without full knowledge of the information required to solve the question. This paper examines the effectiveness of applying such a strategy to train transfer learning models to solve reading comprehension and logical reasoning questions. The models were evaluated on the ReClor dataset, a challenging reading comprehension and logical reasoning benchmark. While previous studies targeted logical reasoning skills, we focus on a general training method and model architecture. We propose the polytuplet loss function, an extension of the triplet loss function, to ensure prioritization of learning the relative correctness of answer choices over learning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#21019;&#20316;&#19968;&#31687;&#20551;&#30340;&#31185;&#23398;&#35770;&#25991;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#39564;&#35777;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20165;&#22522;&#20110;&#35821;&#35328;&#20449;&#24687;&#35299;&#37322;&#22825;&#25991;&#35266;&#27979;&#21644;&#28304;&#65292;&#24182;&#20026;&#21516;&#34892;&#35780;&#23457;&#35782;&#21035;&#27450;&#35784;&#24615;&#29983;&#25104;&#30340;&#31185;&#23398;&#35770;&#25991;&#25552;&#20379;&#28508;&#22312;&#25163;&#27573;&#12290;&#32467;&#35770;&#26159;&#65292;&#30446;&#21069;&#22825;&#25991;&#23398;&#23478;&#30340;&#24037;&#20316;&#26159;&#23433;&#20840;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.17853</link><description>&lt;p&gt;
AI&#33021;&#21542;&#35753;&#20285;&#29595;&#23556;&#32447;&#22825;&#20307;&#29289;&#29702;&#23398;&#23478;&#22833;&#19994;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can AI Put Gamma-Ray Astrophysicists Out of a Job?. (arXiv:2303.17853v1 [physics.pop-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#21019;&#20316;&#19968;&#31687;&#20551;&#30340;&#31185;&#23398;&#35770;&#25991;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#39564;&#35777;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20165;&#22522;&#20110;&#35821;&#35328;&#20449;&#24687;&#35299;&#37322;&#22825;&#25991;&#35266;&#27979;&#21644;&#28304;&#65292;&#24182;&#20026;&#21516;&#34892;&#35780;&#23457;&#35782;&#21035;&#27450;&#35784;&#24615;&#29983;&#25104;&#30340;&#31185;&#23398;&#35770;&#25991;&#25552;&#20379;&#28508;&#22312;&#25163;&#27573;&#12290;&#32467;&#35770;&#26159;&#65292;&#30446;&#21069;&#22825;&#25991;&#23398;&#23478;&#30340;&#24037;&#20316;&#26159;&#23433;&#20840;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#21019;&#20316;&#19968;&#31687;&#25991;&#31456;&#30340;&#33021;&#21147;&#12290;&#36825;&#31687;&#25991;&#31456;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#23384;&#22312;&#30340;&#25104;&#20687;&#22823;&#27668;&#20999;&#20262;&#31185;&#22827;&#26395;&#36828;&#38236;(IATC)&#38453;&#21015;&#26816;&#27979;&#33033;&#20914;&#26143;&#39118;&#26143;&#20113;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#34892;&#36825;&#39033;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#20165;&#22522;&#20110;&#35821;&#35328;&#20449;&#24687;&#35299;&#37322;&#22825;&#25991;&#35266;&#27979;&#21644;&#28304;&#30340;&#33021;&#21147;&#65292;&#24182;&#35780;&#20272;&#22312;&#21516;&#34892;&#35780;&#35758;&#36807;&#31243;&#20013;&#22914;&#20309;&#35782;&#21035;&#35784;&#39575;&#29983;&#25104;&#30340;&#31185;&#23398;&#35770;&#25991;&#30340;&#28508;&#22312;&#25163;&#27573;&#65288;&#32771;&#34385;&#21040;&#36825;&#20123;&#24037;&#20855;&#23578;&#26410;&#37096;&#32626;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#25968;&#23383;&#27700;&#21360;&#65289;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#22825;&#25991;&#23398;&#23478;&#30340;&#24037;&#20316;&#30446;&#21069;&#26159;&#23433;&#20840;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In what will likely be a litany of generative-model-themed arXiv submissions celebrating April the 1st, we evaluate the capacity of state-of-the-art transformer models to create a paper detailing the detection of a Pulsar Wind Nebula with a non-existent Imaging Atmospheric Cherenkov Telescope (IACT) Array. We do this to evaluate the ability of such models to interpret astronomical observations and sources based on language information alone, and to assess potential means by which fraudulently generated scientific papers could be identified during peer review (given that reliable generative model watermarking has yet to be deployed for these tools). We conclude that our jobs as astronomers are safe for the time being. From this point on, prompts given to ChatGPT and Stable Diffusion are shown in orange, text generated by ChatGPT is shown in black, whereas analysis by the (human) authors is in blue.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21738;&#20123;&#27169;&#31946;&#24320;&#25918;&#24615;&#38382;&#39064;&#26368;&#36866;&#21512;&#36890;&#36807;&#23545;&#35805;&#22238;&#31572;&#65292;&#21457;&#29616;&#36825;&#20123;&#38382;&#39064;&#39640;&#24230;&#31038;&#20132;&#21644;&#20010;&#20154;&#21270;&#65292;&#23545;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2303.17710</link><description>&lt;p&gt;
&#26377;&#21738;&#20123;&#38382;&#39064;&#38656;&#35201;&#36827;&#34892;&#20132;&#35848;&#25165;&#33021;&#22238;&#31572;&#65311;&#19968;&#20010; AskReddit &#38382;&#39064;&#26696;&#20363;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
What Types of Questions Require Conversation to Answer? A Case Study of AskReddit Questions. (arXiv:2303.17710v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21738;&#20123;&#27169;&#31946;&#24320;&#25918;&#24615;&#38382;&#39064;&#26368;&#36866;&#21512;&#36890;&#36807;&#23545;&#35805;&#22238;&#31572;&#65292;&#21457;&#29616;&#36825;&#20123;&#38382;&#39064;&#39640;&#24230;&#31038;&#20132;&#21644;&#20010;&#20154;&#21270;&#65292;&#23545;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20250;&#35805;&#31995;&#32479;&#65288;&#22914;&#32842;&#22825;&#26426;&#22120;&#20154;&#12289;&#35821;&#38899;&#23545;&#35805;&#31995;&#32479;&#21644;&#26234;&#33021;&#38899;&#31665;&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#24050;&#32463;&#28145;&#21051;&#22320;&#24433;&#21709;&#20102;&#29616;&#20195;&#25968;&#23383;&#29983;&#27963;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#22238;&#31572;&#26126;&#30830;&#23450;&#20041;&#30340;&#38382;&#39064;&#65292;&#32780;&#38750;&#25903;&#25345;&#29992;&#25143;&#25506;&#32034;&#22797;&#26434;&#30340;&#12289;&#19981;&#26126;&#30830;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#30740;&#31350;&#21738;&#20123;&#27169;&#31946;&#30340;&#12289;&#24320;&#25918;&#24615;&#38382;&#39064;&#26368;&#36866;&#21512;&#36890;&#36807;&#23545;&#35805;&#26469;&#22238;&#31572;&#65292;&#25512;&#21160;&#20250;&#35805;&#31995;&#32479;&#30340;&#36793;&#30028;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174; AskReddit &#19978;&#21457;&#24067;&#30340;100&#19975;&#20010;&#24320;&#25918;&#24335;&#35831;&#27714;&#20013;&#38543;&#26426;&#25277;&#21462;&#20102;500&#20010;&#38382;&#39064;&#65292;&#28982;&#21518;&#25307;&#21215;&#22312;&#32447;&#24037;&#20154;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#30340;8&#20010;&#35810;&#38382;&#12290;&#25105;&#20204;&#36824;&#25191;&#34892;&#24320;&#25918;&#24335;&#32534;&#30721;&#65292;&#23558;&#38382;&#39064;&#20998;&#31867;&#20026;27&#20010;&#19981;&#21516;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20154;&#20204;&#35748;&#20026;&#38656;&#35201;&#20132;&#35848;&#25165;&#33021;&#28385;&#24847;&#35299;&#20915;&#30340;&#38382;&#39064;&#26159;&#39640;&#24230;&#31038;&#20132;&#21644;&#20010;&#20154;&#21270;&#30340;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#22914;&#20309;&#36866;&#24212;&#29992;&#25143;&#38656;&#27714;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of automated conversational systems such as chatbots, spoken-dialogue systems, and smart speakers, has significantly impacted modern digital life. However, these systems are primarily designed to provide answers to well-defined questions rather than to support users in exploring complex, ill-defined questions. In this paper, we aim to push the boundaries of conversational systems by examining the types of nebulous, open-ended questions that can best be answered through conversation. We first sampled 500 questions from one million open-ended requests posted on AskReddit, and then recruited online crowd workers to answer eight inquiries about these questions. We also performed open coding to categorize the questions into 27 different domains. We found that the issues people believe require conversation to resolve satisfactorily are highly social and personal. Our work provides insights into how future research could be geared to align with users' needs.
&lt;/p&gt;</description></item><item><title>oBERTa&#26159;&#19968;&#32452;&#26131;&#20110;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#12289;&#33976;&#39311;&#12289;&#21098;&#26525;&#31561;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#27169;&#22411;&#21387;&#32553;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#31232;&#30095;&#36801;&#31227;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17612</link><description>&lt;p&gt;
oBERTa: &#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#12289;&#33976;&#39311;&#21644;&#21098;&#26525;&#26469;&#25552;&#39640;&#31232;&#30095;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes. (arXiv:2303.17612v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17612
&lt;/p&gt;
&lt;p&gt;
oBERTa&#26159;&#19968;&#32452;&#26131;&#20110;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#12289;&#33976;&#39311;&#12289;&#21098;&#26525;&#31561;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#27169;&#22411;&#21387;&#32553;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#31232;&#30095;&#36801;&#31227;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;oBERTa&#35821;&#35328;&#27169;&#22411;&#30340;&#33539;&#22260;&#65292;&#23427;&#26159;&#19968;&#32452;&#26131;&#20110;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20801;&#35768;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20174;&#19994;&#32773;&#22312;&#19981;&#38656;&#35201;&#27169;&#22411;&#21387;&#32553;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;3.8&#21040;24.3&#20493;&#30340;&#26356;&#24555;&#36895;&#30340;&#27169;&#22411;&#12290;oBERTa&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#21098;&#26525;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#37327;&#21270;&#24037;&#20316;&#65292;&#24182;&#21033;&#29992;&#20923;&#32467;&#30340;&#23884;&#20837;&#26469;&#25913;&#36827;&#30693;&#35782;&#33976;&#39311;&#65292;&#24182;&#25913;&#36827;&#27169;&#22411;&#21021;&#22987;&#21270;&#65292;&#20197;&#22312;&#24191;&#27867;&#30340;&#20256;&#36882;&#20219;&#21153;&#19978;&#25552;&#20379;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#29983;&#25104;oBERTa&#26102;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#39640;&#24230;&#20248;&#21270;&#30340;RoBERTa&#19982;BERT&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26399;&#38388;&#21098;&#26525;&#26041;&#38754;&#30340;&#19981;&#21516;&#20043;&#22788;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#24494;&#35843;&#26399;&#38388;&#19981;&#22826;&#36866;&#21512;&#21387;&#32553;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;oBERTa&#22312;&#19971;&#20010;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;NLP&#20219;&#21153;&#19978;&#30340;&#20351;&#29992;&#65292;&#24182;&#21457;&#29616;&#25913;&#36827;&#30340;&#21387;&#32553;&#25216;&#26415;&#20351;&#24471;&#32463;&#36807;&#21098;&#26525;&#30340;oBERTa&#27169;&#22411;&#33021;&#22815;&#21305;&#37197;BERTBASE&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;SQUAD V1.1&#38382;&#31572;&#25968;&#25454;&#30340;Prune OFA Large&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce the range of oBERTa language models, an easy-to-use set of language models, which allows Natural Language Processing (NLP) practitioners to obtain between 3.8 and 24.3 times faster models without expertise in model compression. Specifically, oBERTa extends existing work on pruning, knowledge distillation, and quantization and leverages frozen embeddings to improve knowledge distillation, and improved model initialization to deliver higher accuracy on a a broad range of transfer tasks. In generating oBERTa, we explore how the highly optimized RoBERTa differs from the BERT with respect to pruning during pre-training and fine-tuning and find it less amenable to compression during fine-tuning. We explore the use of oBERTa on a broad seven representative NLP tasks and find that the improved compression techniques allow a pruned oBERTa model to match the performance of BERTBASE and exceed the performance of Prune OFA Large on the SQUAD V1.1 Question Answering data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25512;&#33616;&#31995;&#32479;&#27169;&#24335;-Chat-Rec&#65292;&#36890;&#36807;&#23558;LLMs&#19982;&#23545;&#35805;&#24335;&#25512;&#33616;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#20013;&#20114;&#21160;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;Chat-Rec&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#29992;&#25143;&#20559;&#22909;&#65292;&#24182;&#22312;&#25512;&#33616;&#36807;&#31243;&#20013;&#24314;&#31435;&#29992;&#25143;-&#20135;&#21697;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20855;&#26377;&#26356;&#22823;&#30340;&#36879;&#26126;&#24230;&#21644;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.14524</link><description>&lt;p&gt;
Chat-REC&#65306;&#38754;&#21521;&#20114;&#21160;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;LLM&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System. (arXiv:2303.14524v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25512;&#33616;&#31995;&#32479;&#27169;&#24335;-Chat-Rec&#65292;&#36890;&#36807;&#23558;LLMs&#19982;&#23545;&#35805;&#24335;&#25512;&#33616;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#20013;&#20114;&#21160;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;Chat-Rec&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#29992;&#25143;&#20559;&#22909;&#65292;&#24182;&#22312;&#25512;&#33616;&#36807;&#31243;&#20013;&#24314;&#31435;&#29992;&#25143;-&#20135;&#21697;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20855;&#26377;&#26356;&#22823;&#30340;&#36879;&#26126;&#24230;&#21644;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35299;&#20915;&#21508;&#31181;&#24212;&#29992;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#25512;&#33616;&#31995;&#32479;&#20173;&#38754;&#20020;&#24456;&#22823;&#30340;&#25361;&#25112;&#65292;&#22914;&#20114;&#21160;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#24046;&#65292;&#36825;&#23454;&#38469;&#19978;&#20063;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#31995;&#32479;&#20013;&#30340;&#24191;&#27867;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#27169;&#24335;&#65292;&#31216;&#20026;Chat-REC&#65288;ChatGPT&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#65289;&#65292;&#36890;&#36807;&#23558;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#21644;&#21382;&#21490;&#20132;&#20114;&#36716;&#25442;&#20026;&#25552;&#31034;&#65292;&#21019;&#26032;&#22320;&#22686;&#24378;LLMs&#29992;&#20110;&#26500;&#24314;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#12290;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#65292;Chat-Rec&#34987;&#35777;&#26126;&#22312;&#23398;&#20064;&#29992;&#25143;&#20559;&#22909;&#21644;&#24314;&#31435;&#29992;&#25143;&#19982;&#20135;&#21697;&#20043;&#38388;&#30340;&#32852;&#31995;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#36825;&#20063;&#20351;&#24471;&#25512;&#33616;&#36807;&#31243;&#26356;&#20855;&#20114;&#21160;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;Chat-Rec&#26694;&#26550;&#20869;&#65292;&#29992;&#25143;&#30340;&#20559;&#22909;&#21487;&#20197;&#36716;&#31227;&#21040;&#19981;&#21516;&#30340;&#20135;&#21697;&#36827;&#34892;&#36328;&#39046;&#22495;&#25512;&#33616;&#65292;&#24182;&#19988;&#22522;&#20110;&#25552;&#31034;&#30340;&#27880;&#20837;&#20801;&#35768;&#26356;&#22823;&#30340;&#36879;&#26126;&#24230;&#21644;&#23545;&#25512;&#33616;&#36807;&#31243;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated their significant potential to be applied for addressing various application tasks. However, traditional recommender systems continue to face great challenges such as poor interactivity and explainability, which actually also hinder their broad deployment in real-world systems. To address these limitations, this paper proposes a novel paradigm called Chat-Rec (ChatGPT Augmented Recommender System) that innovatively augments LLMs for building conversational recommender systems by converting user profiles and historical interactions into prompts. Chat-Rec is demonstrated to be effective in learning user preferences and establishing connections between users and products through in-context learning, which also makes the recommendation process more interactive and explainable. What's more, within the Chat-Rec framework, user's preferences can transfer to different products for cross-domain recommendations, and prompt-based injection of informa
&lt;/p&gt;</description></item><item><title>AfroDigits&#26159;&#31532;&#19968;&#20010;&#21457;&#24067;&#30340;&#38754;&#21521;&#38750;&#27954;&#35821;&#35328;&#30340;&#38899;&#39057;&#25968;&#23383;&#25968;&#25454;&#38598;&#65292;&#20026;&#38750;&#27954;&#35821;&#35328;&#20013;&#30340;&#35821;&#38899;&#24212;&#29992;&#31243;&#24207;&#24320;&#36767;&#20102;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2303.12582</link><description>&lt;p&gt;
AfroDigits&#65306;&#38754;&#21521;&#38750;&#27954;&#35821;&#35328;&#30340;&#22522;&#20110;&#31038;&#21306;&#39537;&#21160;&#30340;&#21475;&#35821;&#25968;&#23383;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
AfroDigits: A Community-Driven Spoken Digit Dataset for African Languages. (arXiv:2303.12582v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12582
&lt;/p&gt;
&lt;p&gt;
AfroDigits&#26159;&#31532;&#19968;&#20010;&#21457;&#24067;&#30340;&#38754;&#21521;&#38750;&#27954;&#35821;&#35328;&#30340;&#38899;&#39057;&#25968;&#23383;&#25968;&#25454;&#38598;&#65292;&#20026;&#38750;&#27954;&#35821;&#35328;&#20013;&#30340;&#35821;&#38899;&#24212;&#29992;&#31243;&#24207;&#24320;&#36767;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35821;&#38899;&#25216;&#26415;&#30340;&#36827;&#27493;&#26159;&#26174;&#33879;&#30340;&#65292;&#20294;&#30001;&#20110;&#38750;&#27954;&#35821;&#26009;&#24211;&#30340;&#21294;&#20047;&#65292;&#20854;&#22312;&#38750;&#27954;&#35821;&#35328;&#20013;&#30340;&#38598;&#25104;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AfroDigits&#65292;&#36825;&#26159;&#19968;&#20010;&#26497;&#31616;&#30340;&#12289;&#30001;&#31038;&#21306;&#39537;&#21160;&#30340;&#38750;&#27954;&#35821;&#35328;&#21475;&#35821;&#25968;&#23383;&#25968;&#25454;&#38598;&#65292;&#30446;&#21069;&#35206;&#30422;&#20102;38&#31181;&#38750;&#27954;&#35821;&#35328;&#12290;&#20316;&#20026;AfroDigits&#23454;&#38469;&#24212;&#29992;&#30340;&#28436;&#31034;&#65292;&#25105;&#20204;&#20351;&#29992;Wav2Vec2.0-Large&#21644;XLS-R&#27169;&#22411;&#65292;&#22312;&#20845;&#31181;&#38750;&#27954;&#35821;&#35328;[Igbo(ibo), Yoruba(yor), Rundi(run), Oshiwambo(kua), Shona(sna)&#21644;Oromo(gax)]&#19978;&#36827;&#34892;&#20102;&#38899;&#39057;&#25968;&#23383;&#20998;&#31867;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#28151;&#21512;&#38750;&#27954;&#35821;&#26009;&#24211;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#25928;&#26524;&#12290;AfroDigits&#26159;&#38750;&#27954;&#35821;&#35328;&#20013;&#31532;&#19968;&#20010;&#21457;&#24067;&#30340;&#38899;&#39057;&#25968;&#23383;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30456;&#20449;&#23427;&#23558;&#20026;&#38754;&#21521;&#38750;&#27954;&#30340;&#35821;&#38899;&#24212;&#29992;&#31243;&#24207;&#24320;&#36767;&#36947;&#36335;&#65292;&#22914;&#30005;&#35805;&#21495;&#30721;&#21644;&#34903;&#36947;&#22320;&#22336;&#30340;&#35782;&#21035;&#12290;&#25105;&#20204;&#22312;https:/ / afrodigits&#20844;&#24320;&#21457;&#24067;&#25968;&#25454;&#38598;&#21644;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of speech technologies has been remarkable, yet its integration with African languages remains limited due to the scarcity of African speech corpora. To address this issue, we present AfroDigits, a minimalist, community-driven dataset of spoken digits for African languages, currently covering 38 African languages. As a demonstration of the practical applications of AfroDigits, we conduct audio digit classification experiments on six African languages [Igbo (ibo), Yoruba (yor), Rundi (run), Oshiwambo (kua), Shona (sna), and Oromo (gax)] using the Wav2Vec2.0-Large and XLS-R models. Our experiments reveal a useful insight on the effect of mixing African speech corpora during finetuning. AfroDigits is the first published audio digit dataset for African languages and we believe it will, among other things, pave the way for Afro-centric speech applications such as the recognition of telephone numbers, and street numbers. We release the dataset and platform publicly at https:/
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22810;&#35821;&#35328;&#20013;&#36827;&#34892;&#35821;&#35328;&#26080;&#20851;&#30340;&#26080;&#30417;&#30563;&#20998;&#35789;&#30340;&#20803;&#23398;&#20064;&#21487;&#33021;&#24615;&#65292;&#24182;&#20351;&#29992;&#22810;&#31181;&#36866;&#24212;&#24230;&#20989;&#25968;&#33258;&#21160;&#30830;&#23450;&#26080;&#30417;&#30563;&#20998;&#35789;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#12290;&#32467;&#26524;&#34920;&#26126;&#22312;&#33521;&#35821;&#21644;&#20420;&#35821;&#20013;&#65292;&#21069;&#19977;&#31181;&#24230;&#37327;&#30340;&#21152;&#24615;&#32452;&#21512;&#19982; F1 &#20998;&#35789;&#24471;&#20998;&#20043;&#38388;&#26377;&#30456;&#24403;&#22909;&#30340;&#30456;&#20851;&#24615;&#65292;&#22312;&#20013;&#25991;&#20013;&#65292;F1 &#24471;&#20998;&#19982;&#21387;&#32553;&#22240;&#23376;&#26377;&#26174;&#33879;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.02427</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#36229;&#21442;&#25968;&#35843;&#33410;&#22312;&#26080;&#30417;&#30563;&#36328;&#35821;&#31181;&#20998;&#35789;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Self-tuning hyper-parameters for unsupervised cross-lingual tokenization. (arXiv:2303.02427v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22810;&#35821;&#35328;&#20013;&#36827;&#34892;&#35821;&#35328;&#26080;&#20851;&#30340;&#26080;&#30417;&#30563;&#20998;&#35789;&#30340;&#20803;&#23398;&#20064;&#21487;&#33021;&#24615;&#65292;&#24182;&#20351;&#29992;&#22810;&#31181;&#36866;&#24212;&#24230;&#20989;&#25968;&#33258;&#21160;&#30830;&#23450;&#26080;&#30417;&#30563;&#20998;&#35789;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#12290;&#32467;&#26524;&#34920;&#26126;&#22312;&#33521;&#35821;&#21644;&#20420;&#35821;&#20013;&#65292;&#21069;&#19977;&#31181;&#24230;&#37327;&#30340;&#21152;&#24615;&#32452;&#21512;&#19982; F1 &#20998;&#35789;&#24471;&#20998;&#20043;&#38388;&#26377;&#30456;&#24403;&#22909;&#30340;&#30456;&#20851;&#24615;&#65292;&#22312;&#20013;&#25991;&#20013;&#65292;F1 &#24471;&#20998;&#19982;&#21387;&#32553;&#22240;&#23376;&#26377;&#26174;&#33879;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#33521;&#35821;&#12289;&#20420;&#35821;&#21644;&#20013;&#25991;&#31561;&#22810;&#31181;&#35821;&#35328;&#20013;&#36827;&#34892;&#35821;&#35328;&#26080;&#20851;&#30340;&#26080;&#30417;&#30563;&#20998;&#35789;&#38382;&#39064;&#30340;&#20803;&#23398;&#20064;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#20154;&#31867;&#26080;&#20851;&#36866;&#24212;&#24230;&#20989;&#25968;&#65292;&#22914;&#26631;&#20934;&#21270;&#21453;&#29109;&#12289;&#21387;&#32553;&#22240;&#23376;&#21644;&#20132;&#21449;&#20998;&#21106; F1 &#24471;&#20998;&#65292;&#20197;&#21450;&#19977;&#20010;&#24230;&#37327;&#30340;&#21152;&#24615;&#21644;&#20056;&#24615;&#32452;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#26089;&#26399;&#20316;&#21697;&#25552;&#20986;&#30340;&#26080;&#30417;&#30563;&#20998;&#35789;&#27169;&#22411;&#36229;&#21442;&#25968;&#30340;&#33258;&#21160;&#30830;&#23450;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#27979;&#35797;&#20854;&#19982;&#20256;&#32479; F1 &#20998;&#35789;&#24471;&#20998;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#33521;&#35821;&#21644;&#20420;&#35821;&#26041;&#38754;&#65292;&#22312;&#21069;&#19977;&#31181;&#24230;&#37327;&#30340;&#21152;&#24615;&#32452;&#21512;&#19982;&#21518;&#32773;&#20043;&#38388;&#26377;&#30456;&#24403;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#20013;&#25991;&#26041;&#38754;&#65292;&#25105;&#20204;&#21457;&#29616; F1 &#24471;&#20998;&#19982;&#21387;&#32553;&#22240;&#23376;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#20102;&#23545;&#20110;&#36164;&#28304;&#31232;&#32570;&#21644;&#27515;&#35821;&#35328;&#30340;&#22362;&#23454;&#30340;&#26080;&#30417;&#30563;&#20998;&#35789;&#21487;&#33021;&#24615;&#65292;&#24182;&#35753;&#20154;&#20204;&#21487;&#20197;&#20174;&#25928;&#29575;&#28436;&#21270;&#30340;&#35282;&#24230;&#24605;&#32771;&#20154;&#31867;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the possibility of meta-learning for the language-independent unsupervised tokenization problem for English, Russian, and Chinese. We implement the meta-learning approach for automatic determination of hyper-parameters of the unsupervised tokenization model proposed in earlier works, relying on various human-independent fitness functions such as normalised anti-entropy, compression factor and cross-split F1 score, as well as additive and multiplicative composite combinations of the three metrics, testing them against the conventional F1 tokenization score. We find a fairly good correlation between the latter and the additive combination of the former three metrics for English and Russian. In case of Chinese, we find a significant correlation between the F 1 score and the compression factor. Our results suggest the possibility of robust unsupervised tokenization of low-resource and dead languages and allow us to think about human languages in terms of the evolution of efficie
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;ChatGPT&#30340;11&#20010;&#22833;&#36133;&#31867;&#21035;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#20854;&#20013;&#21253;&#25324;&#25512;&#29702;&#12289;&#20107;&#23454;&#38169;&#35823;&#12289;&#25968;&#23398;&#12289;&#32534;&#30721;&#21644;&#20559;&#35265;&#12290;&#25214;&#20986;&#22833;&#36133;&#21407;&#22240;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#25913;&#36827;&#26410;&#26469;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;</title><link>http://arxiv.org/abs/2302.03494</link><description>&lt;p&gt;
ChatGPT&#22833;&#36133;&#20998;&#31867;&#23384;&#26723;
&lt;/p&gt;
&lt;p&gt;
A Categorical Archive of ChatGPT Failures. (arXiv:2302.03494v8 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;ChatGPT&#30340;11&#20010;&#22833;&#36133;&#31867;&#21035;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#20854;&#20013;&#21253;&#25324;&#25512;&#29702;&#12289;&#20107;&#23454;&#38169;&#35823;&#12289;&#25968;&#23398;&#12289;&#32534;&#30721;&#21644;&#20559;&#35265;&#12290;&#25214;&#20986;&#22833;&#36133;&#21407;&#22240;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#25913;&#36827;&#26410;&#26469;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#19981;&#21516;&#39046;&#22495;&#35777;&#26126;&#20102;&#20854;&#20215;&#20540;&#12290;&#30001;OpenAI&#24320;&#21457;&#30340;ChatGPT&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#29702;&#35299;&#19978;&#19979;&#25991;&#24182;&#29983;&#25104;&#36866;&#24403;&#30340;&#21709;&#24212;&#26469;&#27169;&#25311;&#20154;&#31867;&#23545;&#35805;&#12290;&#23427;&#22240;&#33021;&#22815;&#26377;&#25928;&#22320;&#22238;&#31572;&#24191;&#27867;&#30340;&#20154;&#31867;&#38382;&#39064;&#32780;&#21463;&#21040;&#37325;&#35270;&#65292;&#20854;&#27969;&#21033;&#21644;&#20840;&#38754;&#30340;&#31572;&#26696;&#22312;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#20844;&#20849;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;ChatGPT&#22833;&#25928;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#36825;&#26159;&#26412;&#30740;&#31350;&#30340;&#37325;&#28857;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#24182;&#35752;&#35770;&#20102;11&#20010;&#22833;&#36133;&#31867;&#21035;&#65292;&#21253;&#25324;&#25512;&#29702;&#12289;&#20107;&#23454;&#38169;&#35823;&#12289;&#25968;&#23398;&#12289;&#32534;&#30721;&#21644;&#20559;&#35265;&#12290;&#36824;&#31361;&#20986;&#20102;ChatGPT&#30340;&#39118;&#38505;&#12289;&#38480;&#21046;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#22686;&#24378;&#26410;&#26469;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have been demonstrated to be valuable in different fields. ChatGPT, developed by OpenAI, has been trained using massive amounts of data and simulates human conversation by comprehending context and generating appropriate responses. It has garnered significant attention due to its ability to effectively answer a broad range of human inquiries, with fluent and comprehensive answers surpassing prior public chatbots in both security and usefulness. However, a comprehensive analysis of ChatGPT's failures is lacking, which is the focus of this study. Eleven categories of failures, including reasoning, factual errors, math, coding, and bias, are presented and discussed. The risks, limitations, and societal implications of ChatGPT are also highlighted. The goal of this study is to assist researchers and developers in enhancing future language models and chatbots.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Hi-VT5&#65292;&#23427;&#26159;&#19968;&#31181;&#20998;&#23618; transformer &#32467;&#26500;&#65292;&#33021;&#22815;&#22788;&#29702;&#22810;&#39029;&#25991;&#26723; DocVQA &#20219;&#21153;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#22238;&#31572;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.05935</link><description>&lt;p&gt;
&#22810;&#32423;&#27169;&#24577;&#21464;&#21387;&#22120;&#29992;&#20110;&#22810;&#39029; DocVQA
&lt;/p&gt;
&lt;p&gt;
Hierarchical multimodal transformers for Multi-Page DocVQA. (arXiv:2212.05935v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Hi-VT5&#65292;&#23427;&#26159;&#19968;&#31181;&#20998;&#23618; transformer &#32467;&#26500;&#65292;&#33021;&#22815;&#22788;&#29702;&#22810;&#39029;&#25991;&#26723; DocVQA &#20219;&#21153;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#22238;&#31572;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#65288;DocVQA&#65289;&#26159;&#25351;&#22238;&#31572;&#25991;&#26723;&#22270;&#20687;&#20013;&#30340;&#38382;&#39064;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340; DocVQA &#24037;&#20316;&#20165;&#32771;&#34385;&#21333;&#39029;&#25991;&#26723;&#12290;&#20294;&#26159;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#25991;&#26723;&#20027;&#35201;&#30001;&#22810;&#20010;&#39029;&#38754;&#32452;&#25104;&#65292;&#24212;&#35813;&#19968;&#36215;&#22788;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558; DocVQA &#25193;&#23637;&#21040;&#22810;&#39029;&#38754;&#22330;&#26223;&#12290;&#20026;&#27492;&#65292;&#39318;&#20808;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598; MP-DocVQA&#65292;&#20854;&#20013;&#38382;&#39064;&#26159;&#38024;&#23545;&#22810;&#39029;&#25991;&#26723;&#32780;&#38750;&#21333;&#39029;&#25552;&#20986;&#30340;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#23618;&#26041;&#27861; Hi-VT5&#65292;&#22522;&#20110; T5 &#32467;&#26500;&#65292;&#20811;&#26381;&#20102;&#22788;&#29702;&#38271;&#22810;&#39029;&#25991;&#26723;&#30340;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22522;&#20110;&#20998;&#23618;&#21464;&#21387;&#22120;&#32467;&#26500;&#65292;&#32534;&#30721;&#22120;&#23545;&#27599;&#20010;&#39029;&#38754;&#30340;&#26368;&#30456;&#20851;&#20449;&#24687;&#36827;&#34892;&#25688;&#35201;&#65292;&#28982;&#21518;&#35299;&#30721;&#22120;&#21033;&#29992;&#36825;&#20123;&#25688;&#35201;&#20449;&#24687;&#29983;&#25104;&#26368;&#32456;&#31572;&#26696;&#12290;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#21333;&#20010;&#38454;&#27573;&#20013;&#22238;&#31572;&#38382;&#39064;&#24182;&#25552;&#20379;
&lt;/p&gt;
&lt;p&gt;
Document Visual Question Answering (DocVQA) refers to the task of answering questions from document images. Existing work on DocVQA only considers single-page documents. However, in real scenarios documents are mostly composed of multiple pages that should be processed altogether. In this work we extend DocVQA to the multi-page scenario. For that, we first create a new dataset, MP-DocVQA, where questions are posed over multi-page documents instead of single pages. Second, we propose a new hierarchical method, Hi-VT5, based on the T5 architecture, that overcomes the limitations of current methods to process long multi-page documents. The proposed method is based on a hierarchical transformer architecture where the encoder summarizes the most relevant information of every page and then, the decoder takes this summarized information to generate the final answer. Through extensive experimentation, we demonstrate that our method is able, in a single stage, to answer the questions and provid
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30005;&#24433;&#37197;&#38899;&#26550;&#26500;&#65292;&#36890;&#36807;&#20998;&#23618;&#38901;&#24459;&#24314;&#27169;&#20174;&#22068;&#21767;&#12289;&#38754;&#37096;&#21644;&#22330;&#26223;&#19977;&#20010;&#26041;&#38754;&#23558;&#35270;&#35273;&#20449;&#24687;&#19982;&#30456;&#24212;&#30340;&#35821;&#38899;&#38901;&#24459;&#32852;&#31995;&#36215;&#26469;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;V2C&#20219;&#21153;&#20013;&#24773;&#24863;&#21464;&#21270;&#21644;&#35828;&#35805;&#36895;&#24230;&#21305;&#37197;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.04054</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;&#38901;&#24459;&#27169;&#22411;&#30340;&#30005;&#24433;&#37197;&#38899;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Dub Movies via Hierarchical Prosody Models. (arXiv:2212.04054v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30005;&#24433;&#37197;&#38899;&#26550;&#26500;&#65292;&#36890;&#36807;&#20998;&#23618;&#38901;&#24459;&#24314;&#27169;&#20174;&#22068;&#21767;&#12289;&#38754;&#37096;&#21644;&#22330;&#26223;&#19977;&#20010;&#26041;&#38754;&#23558;&#35270;&#35273;&#20449;&#24687;&#19982;&#30456;&#24212;&#30340;&#35821;&#38899;&#38901;&#24459;&#32852;&#31995;&#36215;&#26469;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;V2C&#20219;&#21153;&#20013;&#24773;&#24863;&#21464;&#21270;&#21644;&#35828;&#35805;&#36895;&#24230;&#21305;&#37197;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#19968;&#27573;&#25991;&#26412;&#12289;&#19968;&#20010;&#35270;&#39057;&#29255;&#27573;&#21644;&#19968;&#20010;&#21442;&#32771;&#38899;&#39057;&#65292;&#30005;&#24433;&#37197;&#38899;&#20219;&#21153;&#65288;&#20063;&#31216;&#20026;&#35270;&#35273;&#35821;&#38899;&#20811;&#38534;V2C&#65289;&#26088;&#22312;&#20351;&#29992;&#25152;&#38656;&#30340;&#35828;&#35805;&#32773;&#22768;&#38899;&#20316;&#20026;&#21442;&#32771;&#65292;&#29983;&#25104;&#19982;&#35270;&#39057;&#20013;&#21576;&#29616;&#30340;&#35828;&#35805;&#32773;&#24773;&#24863;&#21305;&#37197;&#30340;&#35821;&#38899;&#65292;&#32780;&#19988;&#35201;&#27714;&#29983;&#25104;&#30340;&#35821;&#38899;&#24688;&#22909;&#21305;&#37197;&#35270;&#39057;&#20013;&#21576;&#29616;&#30340;&#19981;&#26029;&#21464;&#21270;&#30340;&#24773;&#24863;&#21644;&#35828;&#35805;&#36895;&#24230;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30005;&#24433;&#37197;&#38899;&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#20998;&#23618;&#38901;&#24459;&#24314;&#27169;&#20174;&#19977;&#20010;&#26041;&#38754;&#23558;&#35270;&#35273;&#20449;&#24687;&#19982;&#30456;&#24212;&#30340;&#35821;&#38899;&#38901;&#24459;&#32852;&#31995;&#36215;&#26469;&#65306;&#22068;&#21767;&#12289;&#38754;&#37096;&#21644;&#22330;&#26223;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#22068;&#21767;&#36816;&#21160;&#19982;&#35821;&#38899;&#25345;&#32493;&#26102;&#38388;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#26368;&#36817;&#24515;&#29702;&#23398;&#21457;&#29616;&#30340;&#24841;&#24742;&#21644;&#21796;&#36215;&#34920;&#31034;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#23558;&#38754;&#37096;&#34920;&#24773;&#20256;&#36798;&#21040;&#35821;&#38899;&#33021;&#37327;&#21644;&#38899;&#39640;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24773;&#24863;&#22686;&#24378;&#22120;&#26469;&#25429;&#25417;&#20840;&#23616;&#35270;&#39057;&#22330;&#26223;&#30340;&#27675;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a piece of text, a video clip and a reference audio, the movie dubbing (also known as visual voice clone V2C) task aims to generate speeches that match the speaker's emotion presented in the video using the desired speaker voice as reference. V2C is more challenging than conventional text-to-speech tasks as it additionally requires the generated speech to exactly match the varying emotions and speaking speed presented in the video. Unlike previous works, we propose a novel movie dubbing architecture to tackle these problems via hierarchical prosody modelling, which bridges the visual information to corresponding speech prosody from three aspects: lip, face, and scene. Specifically, we align lip movement to the speech duration, and convey facial expression to speech energy and pitch via attention mechanism based on valence and arousal representations inspired by recent psychology findings. Moreover, we design an emotion booster to capture the atmosphere from global video scenes. A
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#28145;&#24230;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#37319;&#29992;&#23398;&#20064;&#36830;&#25509;&#30340;&#26041;&#24335;&#32780;&#38750;&#30830;&#23450;&#24615;&#36830;&#25509;&#65292;&#33258;&#21160;&#23398;&#20064;&#24086;&#23376;&#20043;&#38388;&#30340;&#36830;&#25509;&#26469;&#39044;&#27979;&#20154;&#26684;&#29305;&#24449;&#65292;&#30456;&#27604;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.01515</link><description>&lt;p&gt;
&#35746;&#21333;&#26159;&#19981;&#38656;&#35201;&#30340;&#65306;&#29992;&#20110;&#20154;&#26684;&#26816;&#27979;&#30340;&#21160;&#24577;&#28145;&#24230;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Orders Are Unwanted: Dynamic Deep Graph Convolutional Network for Personality Detection. (arXiv:2212.01515v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#28145;&#24230;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#37319;&#29992;&#23398;&#20064;&#36830;&#25509;&#30340;&#26041;&#24335;&#32780;&#38750;&#30830;&#23450;&#24615;&#36830;&#25509;&#65292;&#33258;&#21160;&#23398;&#20064;&#24086;&#23376;&#20043;&#38388;&#30340;&#36830;&#25509;&#26469;&#39044;&#27979;&#20154;&#26684;&#29305;&#24449;&#65292;&#30456;&#27604;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#22914;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#20013;&#65292;&#22522;&#20110;&#22312;&#32447;&#24086;&#23376;&#26469;&#39044;&#27979;&#20154;&#26684;&#29305;&#24449;&#24050;&#25104;&#20026;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#12290;&#36825;&#20010;&#20219;&#21153;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#20174;&#21508;&#31181;&#24086;&#23376;&#20013;&#27719;&#24635;&#20449;&#24687;&#65292;&#24418;&#25104;&#27599;&#20010;&#29992;&#25143;&#30340;&#25972;&#20307;&#36164;&#26009;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#23558;&#24086;&#23376;&#31616;&#21333;&#22320;&#36830;&#25509;&#25104;&#38271;&#25991;&#26723;&#65292;&#28982;&#21518;&#36890;&#36807;&#39034;&#24207;&#25110;&#20998;&#23618;&#27169;&#22411;&#23545;&#25991;&#26723;&#36827;&#34892;&#32534;&#30721;&#65292;&#36825;&#31181;&#26041;&#24335;&#24341;&#20837;&#20102;&#26080;&#24207;&#30340;&#24086;&#23376;&#65292;&#21487;&#33021;&#20250;&#35823;&#23548;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#28145;&#24230;&#22270;&#21367;&#31215;&#32593;&#32476;(D-DGCN)&#65292;&#20197;&#20811;&#26381;&#19978;&#36848;&#38480;&#21046;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#23398;&#20064;&#36830;&#25509;&#26041;&#27861;&#65292;&#37319;&#29992;&#21160;&#24577;&#22810;&#36339;&#32467;&#26500;&#32780;&#38750;&#30830;&#23450;&#24615;&#32467;&#26500;&#65292;&#24182;&#23558;&#20854;&#19982;DGCN&#27169;&#22359;&#30456;&#32467;&#21512;&#65292;&#33258;&#21160;&#23398;&#20064;&#24086;&#23376;&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;&#24086;&#23376;&#32534;&#30721;&#22120;&#12289;&#23398;&#20064;&#36830;&#25509;&#21644;DGCN&#27169;&#22359;&#21516;&#26102;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#32852;&#21512;&#35757;&#32451;&#12290;&#22312;Kaggle&#21644;Pandora&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20986;D-DGCN&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting personality traits based on online posts has emerged as an important task in many fields such as social network analysis. One of the challenges of this task is assembling information from various posts into an overall profile for each user. While many previous solutions simply concatenate the posts into a long document and then encode the document by sequential or hierarchical models, they introduce unwarranted orders for the posts, which may mislead the models. In this paper, we propose a dynamic deep graph convolutional network (D-DGCN) to overcome the above limitation. Specifically, we design a learn-to-connect approach that adopts a dynamic multi-hop structure instead of a deterministic structure, and combine it with a DGCN module to automatically learn the connections between posts. The modules of post encoder, learn-to-connect, and DGCN are jointly trained in an end-to-end manner. Experimental results on the Kaggle and Pandora datasets show the superior performance of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#20010;&#24615;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#65288;PAA&#65289;&#26469;&#29983;&#25104;&#22522;&#20110;&#20010;&#24615;&#30340;&#19968;&#33268;&#24615;&#22238;&#24212;&#65292;&#21487;&#20197;&#36890;&#36807;&#25972;&#21512;&#20010;&#24615;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26435;&#37325;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126; PAA &#26694;&#26550;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#65292;&#21487;&#20197;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2210.15088</link><description>&lt;p&gt;
&#24102;&#26377;&#20010;&#24615;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#30340;&#20010;&#24615;&#21270;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Personalized Dialogue Generation with Persona-Adaptive Attention. (arXiv:2210.15088v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#20010;&#24615;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#65288;PAA&#65289;&#26469;&#29983;&#25104;&#22522;&#20110;&#20010;&#24615;&#30340;&#19968;&#33268;&#24615;&#22238;&#24212;&#65292;&#21487;&#20197;&#36890;&#36807;&#25972;&#21512;&#20010;&#24615;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26435;&#37325;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126; PAA &#26694;&#26550;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#65292;&#21487;&#20197;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20010;&#24615;&#21270;&#30340;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#26681;&#25454;&#21382;&#21490;&#19978;&#19979;&#25991;&#21644;&#39044;&#23450;&#20041;&#30340;&#20010;&#24615;&#29983;&#25104;&#19968;&#33268;&#30340;&#22238;&#24212;&#12290;&#19982;&#20256;&#32479;&#30340;&#23545;&#35805;&#29983;&#25104;&#19981;&#21516;&#65292;&#22522;&#20110;&#20010;&#24615;&#30340;&#23545;&#35805;&#38656;&#35201;&#32771;&#34385;&#23545;&#35805;&#19978;&#19979;&#25991;&#21644;&#20010;&#24615;&#20004;&#20010;&#26041;&#38754;&#65292;&#36825;&#23545;&#20110;&#19968;&#33268;&#30340;&#35757;&#32451;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#20010;&#24615;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#65288;PAA&#65289;&#65292;&#36890;&#36807;&#25105;&#20204;&#35774;&#35745;&#30340;&#27880;&#24847;&#21147;&#36866;&#24212;&#24615;&#22320;&#25972;&#21512;&#20102;&#26469;&#33258;&#20010;&#24615;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26435;&#37325;&#12290;&#27492;&#22806;&#65292;PAA &#36824;&#24212;&#29992;&#20102;&#21160;&#24577;&#23631;&#34109;&#26426;&#21046;&#65292;&#19981;&#20165;&#21487;&#20197;&#20002;&#24323;&#19978;&#19979;&#25991;&#21644;&#20010;&#24615;&#30340;&#20887;&#20313;&#20449;&#24687;&#65292;&#36824;&#21487;&#20197;&#20316;&#20026;&#27491;&#21017;&#21270;&#26426;&#21046;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24378;&#22522;&#32447;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340; PAA &#26694;&#26550;&#22312;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340; PAA &#26041;&#27861;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#34920;&#29616;&#21516;&#26679;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Persona-based dialogue systems aim to generate consistent responses based on historical context and predefined persona. Unlike conventional dialogue generation, the persona-based dialogue needs to consider both dialogue context and persona, posing a challenge for coherent training. Specifically, this requires a delicate weight balance between context and persona. To achieve that, in this paper, we propose an effective framework with Persona-Adaptive Attention (PAA), which adaptively integrates the weights from the persona and context information via our designed attention. In addition, a dynamic masking mechanism is applied to the PAA to not only drop redundant information in context and persona but also serve as a regularization mechanism to avoid overfitting. Experimental results demonstrate the superiority of the proposed PAA framework compared to the strong baselines in both automatic and human evaluation. Moreover, the proposed PAA approach can perform equivalently well in a low-r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35270;&#39057;&#31034;&#20363;&#23398;&#20064;&#25163;&#35821;&#25340;&#20889;&#22312;&#26426;&#22120;&#20154;&#20013;&#30340;&#23454;&#29616;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35757;&#32451;&#27169;&#20223;&#36816;&#21160;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#20174;RGB&#35270;&#39057;&#20013;&#25552;&#21462;&#25163;&#30340;&#19977;&#32500;&#23039;&#24577;&#65292;&#24182;&#35782;&#21035;&#20986;&#26368;&#20339;&#30340;&#27169;&#20223;&#36229;&#21442;&#25968;&#38598;&#65292;&#26412;&#25991;&#25104;&#21151;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.05135</link><description>&lt;p&gt;
&#35821;&#35328;&#31526;&#21495;&#30340;&#34920;&#29616;&#65306;&#22522;&#20110;&#31034;&#33539;&#30340;&#20154;&#26426;&#20132;&#20114;&#20013;&#65292;&#20307;&#24863;&#25163;&#35821;&#25163;&#25351;&#25340;&#20889;&#30340;&#32763;&#35793;&#26426;&#22120;&#20154;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
Signs of Language: Embodied Sign Language Fingerspelling Acquisition from Demonstrations for Human-Robot Interaction. (arXiv:2209.05135v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35270;&#39057;&#31034;&#20363;&#23398;&#20064;&#25163;&#35821;&#25340;&#20889;&#22312;&#26426;&#22120;&#20154;&#20013;&#30340;&#23454;&#29616;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35757;&#32451;&#27169;&#20223;&#36816;&#21160;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#20174;RGB&#35270;&#39057;&#20013;&#25552;&#21462;&#25163;&#30340;&#19977;&#32500;&#23039;&#24577;&#65292;&#24182;&#35782;&#21035;&#20986;&#26368;&#20339;&#30340;&#27169;&#20223;&#36229;&#21442;&#25968;&#38598;&#65292;&#26412;&#25991;&#25104;&#21151;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26426;&#22120;&#20154;&#20013;&#32454;&#33268;&#30340;&#21160;&#20316;&#26159;&#19968;&#20010;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#26426;&#22120;&#20154;&#25163;&#30340;&#19978;&#19979;&#25991;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#35270;&#39057;&#31034;&#20363;&#23398;&#20064;&#26080;&#39069;&#22806;&#20449;&#24687;&#19979;&#30340;&#29087;&#32451;&#36816;&#21160;&#27169;&#20223;&#65292;&#20197;&#33719;&#24471;&#25163;&#35821;&#25340;&#20889;&#22312;&#26426;&#22120;&#20154;&#20013;&#30340;&#23454;&#29616;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#26426;&#22120;&#20154;&#25163;&#30340;URDF&#27169;&#22411;&#65292;&#24182;&#20351;&#27599;&#20010;&#20851;&#33410;&#21482;&#26377;&#19968;&#20010;&#33268;&#21160;&#22120;&#12290;&#28982;&#21518;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#20174;RGB&#35270;&#39057;&#20013;&#25552;&#21462;&#25163;&#30340;&#19977;&#32500;&#23039;&#24577;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;(&#21363;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#21644;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;)&#26469;&#35757;&#32451;&#19968;&#31181;&#33021;&#22815;&#22797;&#21046;&#31034;&#33539;&#36816;&#21160;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#22522;&#20110;&#21442;&#32771;&#36816;&#21160;&#35782;&#21035;&#20986;&#26368;&#20339;&#30340;&#27169;&#20223;&#36229;&#21442;&#25968;&#38598;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#20845;&#20010;&#23545;&#24212;&#20110;&#25340;&#20889;&#23383;&#27597;&#30340;&#19981;&#21516;&#20219;&#21153;&#36827;&#34892;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning fine-grained movements is a challenging topic in robotics, particularly in the context of robotic hands. One specific instance of this challenge is the acquisition of fingerspelling sign language in robots. In this paper, we propose an approach for learning dexterous motor imitation from video examples without additional information. To achieve this, we first build a URDF model of a robotic hand with a single actuator for each joint. We then leverage pre-trained deep vision models to extract the 3D pose of the hand from RGB videos. Next, using state-of-the-art reinforcement learning algorithms for motion imitation (namely, proximal policy optimization and soft actor-critic), we train a policy to reproduce the movement extracted from the demonstrations. We identify the optimal set of hyperparameters for imitation based on a reference motion. Finally, we demonstrate the generalizability of our approach by testing it on six different tasks, corresponding to fingerspelled letters.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#21333;&#20010;NMT&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#21516;&#33521;&#35821;&#20043;&#38388;&#30340;&#21452;&#21521;&#26080;&#30417;&#30563;&#32763;&#35793;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#22810;&#35821;&#35328;&#24494;&#35843;&#21644;&#21452;&#21521;&#22238;&#35793;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2209.02821</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#35821;&#35328;&#24494;&#35843;&#21644;&#22238;&#35793;&#23454;&#29616;&#30340;&#22810;&#35821;&#35328;&#21452;&#21521;&#26080;&#30417;&#30563;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Multilingual Bidirectional Unsupervised Translation Through Multilingual Finetuning and Back-Translation. (arXiv:2209.02821v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02821
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#21333;&#20010;NMT&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#21516;&#33521;&#35821;&#20043;&#38388;&#30340;&#21452;&#21521;&#26080;&#30417;&#30563;&#32763;&#35793;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#22810;&#35821;&#35328;&#24494;&#35843;&#21644;&#21452;&#21521;&#22238;&#35793;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#21333;&#20010;NMT&#27169;&#22411;&#20197;&#23558;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#20174;&#21644;&#21040;&#33521;&#35821;&#36827;&#34892;&#32763;&#35793;&#12290;&#23545;&#20110;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#23558;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#21021;&#22987;&#21270;&#20026;&#39044;&#35757;&#32451;&#30340;XLM-R&#21644;RoBERTa&#26435;&#37325;&#65292;&#28982;&#21518;&#23545;40&#31181;&#35821;&#35328;&#21040;&#33521;&#35821;&#30340;&#24182;&#34892;&#25968;&#25454;&#36827;&#34892;&#22810;&#35821;&#35328;&#24494;&#35843;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#30340;&#38646;-shot&#32763;&#35793;&#12290;&#23545;&#20110;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#25512;&#24191;&#33021;&#21147;&#20174;&#21333;&#35821;&#35328;&#25968;&#25454;&#38598;&#29983;&#25104;&#21512;&#25104;&#24182;&#34892;&#25968;&#25454;&#65292;&#28982;&#21518;&#20351;&#29992;&#36830;&#32493;&#30340;&#21452;&#21521;&#22238;&#35793;&#36718;&#27425;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;EcXTra&#65288;{E}nglish-{c}entric Crosslingual ({X}) {Tra}nsfer&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27010;&#24565;&#19978;&#24456;&#31616;&#21333;&#65292;&#21482;&#20351;&#29992;&#26631;&#20934;&#30340;&#20132;&#21449;&#29109;&#30446;&#26631;&#65292;&#24182;&#19988;&#26159;&#25968;&#25454;&#39537;&#21160;&#30340;&#65292;&#20381;&#27425;&#21033;&#29992;&#36741;&#21161;&#24182;&#34892;&#25968;&#25454;&#21644;&#21333;&#35821;&#35328;&#25968;&#25454;&#12290;&#25105;&#20204;&#35780;&#20272;&#25105;&#20204;&#22312;7&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#26080;&#30417;&#30563;NMT&#32467;&#26524;&#65292;&#21457;&#29616;&#27599;&#19968;&#36718;&#22238;&#35793;&#35757;&#32451;&#37117;&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#21452;&#21521;&#26080;&#30417;&#30563;&#32763;&#35793;&#36136;&#37327;&#65292;&#30456;&#27604;&#22522;&#32447;&#25552;&#39640;&#20102;&#22810;&#36798;10&#20010;BLEU&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a two-stage approach for training a single NMT model to translate unseen languages both to and from English. For the first stage, we initialize an encoder-decoder model to pretrained XLM-R and RoBERTa weights, then perform multilingual fine-tuning on parallel data in 40 languages to English. We find this model can generalize to zero-shot translations on unseen languages. For the second stage, we leverage this generalization ability to generate synthetic parallel data from monolingual datasets, then train with successive rounds of bidirectional back-translation.  We term our approach EcXTra ({E}nglish-{c}entric Crosslingual ({X}) {Tra}nsfer). Our approach is conceptually simple, only using a standard cross-entropy objective throughout, and also is data-driven, sequentially leveraging auxiliary parallel data and monolingual data. We evaluate our unsupervised NMT results on 7 low-resource languages, and find that each round of back-translation training further refines bidirecti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#27604;&#29616;&#26377;&#30340;&#25688;&#35201;&#35780;&#20272;&#25351;&#26631;&#26356;&#22909;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#19988;&#26356;&#21152;&#40065;&#26834;&#65292;&#19982;&#29616;&#26377;&#25351;&#26631;&#30456;&#32467;&#21512;&#21487;&#20197;&#20351;&#35780;&#20272;&#25928;&#26524;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2208.07316</link><description>&lt;p&gt;
MENLI: &#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
MENLI: Robust Evaluation Metrics from Natural Language Inference. (arXiv:2208.07316v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#27604;&#29616;&#26377;&#30340;&#25688;&#35201;&#35780;&#20272;&#25351;&#26631;&#26356;&#22909;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#19988;&#26356;&#21152;&#40065;&#26834;&#65292;&#19982;&#29616;&#26377;&#25351;&#26631;&#30456;&#32467;&#21512;&#21487;&#20197;&#20351;&#35780;&#20272;&#25928;&#26524;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#34987;&#25552;&#20986;&#30340;&#22522;&#20110;BERT&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#25351;&#26631;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#26131;&#21463;&#21040;&#23545;&#20449;&#24687;&#27491;&#30830;&#24615;&#30340;&#25915;&#20987;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#37096;&#20998;&#21407;&#22240;&#26159;&#27492;&#31867;&#27169;&#22411;&#26159;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#24314;&#27169;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#36825;&#31181;&#25351;&#26631;&#26356;&#36866;&#21512;&#24314;&#27169;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26694;&#26550;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;NLI&#22522;&#30784;&#25351;&#26631;&#27604;&#26368;&#36817;&#30340;BERT&#22522;&#30784;&#25351;&#26631;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;NLI&#22522;&#30784;&#25351;&#26631;&#20248;&#20110;&#29616;&#26377;&#30340;&#25688;&#35201;&#35780;&#20272;&#25351;&#26631;&#65292;&#20294;&#20302;&#20110;SOTA MT&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#26377;&#25351;&#26631;&#19982;&#25105;&#20204;&#30340;NLI&#25351;&#26631;&#30456;&#32467;&#21512;&#26102;&#65292;&#25105;&#20204;&#26082;&#33719;&#24471;&#20102;&#26356;&#39640;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65288;15&#65285;-30&#65285;&#65289;&#65292;&#21448;&#33719;&#24471;&#20102;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#26356;&#39640;&#30340;&#36136;&#37327;&#25351;&#26631;&#65288;+5&#65285;&#33267;30&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently proposed BERT-based evaluation metrics for text generation perform well on standard benchmarks but are vulnerable to adversarial attacks, e.g., relating to information correctness. We argue that this stems (in part) from the fact that they are models of semantic similarity. In contrast, we develop evaluation metrics based on Natural Language Inference (NLI), which we deem a more appropriate modeling. We design a preference-based adversarial attack framework and show that our NLI based metrics are much more robust to the attacks than the recent BERT-based metrics. On standard benchmarks, our NLI based metrics outperform existing summarization metrics, but perform below SOTA MT metrics. However, when combining existing metrics with our NLI metrics, we obtain both higher adversarial robustness (15%-30%) and higher quality metrics as measured on standard benchmarks (+5% to 30%).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#38463;&#25289;&#20271;-&#33521;&#35821;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#22686;&#24378;&#30340;&#35789;&#27719;&#26367;&#25442;&#26041;&#27861;&#65292;&#36890;&#36807;&#21333;&#35789;&#23545;&#40784;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#36827;&#34892;&#35789;&#27719;&#26367;&#25442;&#65292;&#35780;&#20272;&#20102;&#20854;&#22312;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#12289;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;34%&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2205.12649</link><description>&lt;p&gt;
&#30740;&#31350;&#29992;&#20110;&#38463;&#25289;&#20271;-&#33521;&#35821;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#22686;&#24378;&#30340;&#35789;&#27719;&#26367;&#25442;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Investigating Lexical Replacements for Arabic-English Code-Switched Data Augmentation. (arXiv:2205.12649v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#38463;&#25289;&#20271;-&#33521;&#35821;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#22686;&#24378;&#30340;&#35789;&#27719;&#26367;&#25442;&#26041;&#27861;&#65292;&#36890;&#36807;&#21333;&#35789;&#23545;&#40784;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#36827;&#34892;&#35789;&#27719;&#26367;&#25442;&#65292;&#35780;&#20272;&#20102;&#20854;&#22312;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#12289;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;34%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31232;&#30095;&#26159;&#38459;&#30861;&#28151;&#21512;&#20195;&#30721;&#65288;CS&#65289;NLP&#31995;&#32479;&#21457;&#23637;&#30340;&#20027;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#29992;&#20110;&#21512;&#25104;&#26041;&#35328;&#38463;&#25289;&#20271;&#35821;-&#33521;&#35821;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415; - &#35789;&#27719;&#26367;&#25442;&#12290;&#25105;&#20204;&#20351;&#29992;&#21333;&#35789;&#23545;&#40784;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#36827;&#34892;&#35789;&#27719;&#26367;&#25442;&#65292;&#20854;&#20013;CS&#28857;&#26159;&#38543;&#26426;&#36873;&#25321;&#25110;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#23398;&#20064;&#30340;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#19982;&#22522;&#20110;&#35789;&#20856;&#30340;&#26367;&#25442;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#35780;&#20272;&#25152;&#29983;&#25104;&#30340;&#21477;&#23376;&#30340;&#36136;&#37327;&#65292;&#35780;&#20272;&#25968;&#25454;&#22686;&#24378;&#23545;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#12289;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#39044;&#27979;&#27169;&#22411;&#30456;&#23545;&#20110;&#38543;&#26426;&#26041;&#27861;&#29983;&#25104;&#20102;&#26356;&#33258;&#28982;&#30340;CS&#21477;&#23376;&#65292;&#36825;&#26159;&#26681;&#25454;&#20154;&#31867;&#21028;&#26029;&#25552;&#20986;&#30340;&#12290;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#23613;&#31649;&#38543;&#26426;&#26041;&#27861;&#29983;&#25104;&#20102;&#26356;&#22810;&#30340;&#25968;&#25454;&#65292;&#20294;&#20004;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#30456;&#21516;&#65288;&#20248;&#20110;&#22522;&#20110;&#35789;&#20856;&#30340;&#26367;&#25442;&#65289;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25968;&#25454;&#22686;&#24378;&#23454;&#29616;&#20102;34%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data sparsity is a main problem hindering the development of code-switching (CS) NLP systems. In this paper, we investigate data augmentation techniques for synthesizing dialectal Arabic-English CS text. We perform lexical replacements using word-aligned parallel corpora where CS points are either randomly chosen or learnt using a sequence-to-sequence model. We compare these approaches against dictionary-based replacements. We assess the quality of the generated sentences through human evaluation and evaluate the effectiveness of data augmentation on machine translation (MT), automatic speech recognition (ASR), and speech translation (ST) tasks. Results show that using a predictive model results in more natural CS sentences compared to the random approach, as reported in human judgements. In the downstream tasks, despite the random approach generating more data, both approaches perform equally (outperforming dictionary-based replacements). Overall, data augmentation achieves 34% improv
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#35270;&#39057;-&#35821;&#35328;&#25925;&#20107;&#25968;&#25454;&#38598;SYMON&#65292;&#29992;&#20110;&#25512;&#36827;&#22810;&#27169;&#24577;&#25925;&#20107;&#29702;&#35299;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2203.05711</link><description>&lt;p&gt;
&#30005;&#24433;&#21465;&#36848;&#25688;&#35201;&#65306;&#19968;&#20010;&#29992;&#20110;&#25925;&#20107;&#29702;&#35299;&#30340;&#35270;&#39057;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding. (arXiv:2203.05711v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.05711
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#35270;&#39057;-&#35821;&#35328;&#25925;&#20107;&#25968;&#25454;&#38598;SYMON&#65292;&#29992;&#20110;&#25512;&#36827;&#22810;&#27169;&#24577;&#25925;&#20107;&#29702;&#35299;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;AI&#26377;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#20294;&#25925;&#20107;&#29702;&#35299;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25910;&#38598;&#12289;&#39044;&#22788;&#29702;&#24182;&#20844;&#24320;&#21457;&#24067;&#20102;&#19968;&#20010;&#35270;&#39057;&#35821;&#35328;&#25925;&#20107;&#25968;&#25454;&#38598;SYMON&#65292;&#20854;&#20013;&#21253;&#21547;5,193&#20010;&#27969;&#34892;&#30005;&#24433;&#21644;&#30005;&#35270;&#21095;&#30340;&#35270;&#39057;&#25688;&#35201;&#12290;SYMON&#25429;&#25417;&#20102;&#30001;&#20154;&#31867;&#21019;&#20316;&#32773;&#21046;&#20316;&#30340;&#38754;&#21521;&#20154;&#31867;&#35266;&#20247;&#30340;&#33258;&#28982;&#25925;&#20107;&#21465;&#36848;&#35270;&#39057;&#12290;&#20316;&#20026;&#19968;&#20010;&#21407;&#22411;&#21644;&#33258;&#28982;&#25925;&#20107;&#25968;&#25454;&#38598;&#65292;SYMON&#20855;&#26377;&#39640;&#35206;&#30422;&#30340;&#22810;&#27169;&#24577;&#25925;&#20107;&#20107;&#20214;&#12289;&#20016;&#23500;&#30340;&#24515;&#29702;&#29366;&#24577;&#25551;&#36848;&#21644;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#22823;&#35821;&#20041;&#24046;&#36317;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#21644;&#30005;&#24433;&#25688;&#35201;&#35270;&#39057;&#30340;&#38646;&#26679;&#26412;&#23545;&#40784;&#30340;&#22522;&#20934;&#65292;&#23637;&#31034;&#20102;&#22312;&#25925;&#20107;&#29702;&#35299;&#20013;&#39046;&#22495;&#20869;&#25968;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;SYMON&#65292;&#25105;&#20204;&#24076;&#26395;&#20026;&#22810;&#27169;&#24577;&#25925;&#20107;&#29702;&#35299;&#30340;&#36827;&#23637;&#25171;&#19979;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advances of AI, story understanding remains an open and under-investigated problem. We collect, preprocess, and publicly release a video-language story dataset, Synopses of Movie Narratives (SYMON), containing 5,193 video summaries of popular movies and TV series. SYMON captures naturalistic story-telling videos for human audience made by human creators. As a prototypical and naturalistic story dataset, SYMON features high coverage of multimodal story events, abundant mental-state descriptions, and large semantic gaps between the visual and the textual modalities. We establish benchmarks on video-text retrieval and zero-shot alignment on movie summary videos, which showcase the importance of in-domain data in story understanding. With SYMON, we hope to lay the groundwork for progress in multimodal story understanding.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328; Twitter &#25968;&#25454;&#65292;&#20998;&#26512;&#20102;&#24076;&#33098;&#12289;&#35199;&#29677;&#29273;&#21644;&#32852;&#21512;&#29579;&#22269;&#35758;&#20250;&#25104;&#21592;&#30340;&#25512;&#25991;&#65292;&#24182;&#21457;&#29616;&#28040;&#26497;&#24773;&#32490;&#26356;&#26131;&#20256;&#25773;&#12290;</title><link>http://arxiv.org/abs/2202.00396</link><description>&lt;p&gt;
&#28040;&#26497;&#24773;&#32490;&#20256;&#25773;&#26356;&#24555;&#65306;&#22522;&#20110;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328; Twitter &#20998;&#26512;&#30340;&#24773;&#24863;&#22312;&#25919;&#27835;&#27807;&#36890;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Negativity Spreads Faster: A Large-Scale Multilingual Twitter Analysis on the Role of Sentiment in Political Communication. (arXiv:2202.00396v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328; Twitter &#25968;&#25454;&#65292;&#20998;&#26512;&#20102;&#24076;&#33098;&#12289;&#35199;&#29677;&#29273;&#21644;&#32852;&#21512;&#29579;&#22269;&#35758;&#20250;&#25104;&#21592;&#30340;&#25512;&#25991;&#65292;&#24182;&#21457;&#29616;&#28040;&#26497;&#24773;&#32490;&#26356;&#26131;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24050;&#32463;&#22312;&#29616;&#20195;&#31038;&#20250;&#20013;&#23545;&#25919;&#31574;&#21046;&#23450;&#20135;&#29983;&#20102;&#26497;&#22823;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#35199;&#26041;&#19990;&#30028;&#20013;&#65292;Twitter &#31561;&#24179;&#21488;&#35753;&#29992;&#25143;&#33021;&#22815;&#20851;&#27880;&#25919;&#27835;&#23478;&#65292;&#20351;&#20844;&#27665;&#26356;&#22810;&#22320;&#21442;&#19982;&#25919;&#27835;&#35752;&#35770;&#12290;&#21516;&#26679;&#65292;&#25919;&#27835;&#23478;&#20063;&#21033;&#29992; Twitter &#34920;&#36798;&#33258;&#24049;&#30340;&#35266;&#28857;&#65292;&#22312;&#24403;&#21069;&#35805;&#39064;&#19978;&#19982;&#20182;&#20154;&#36777;&#35770;&#65292;&#24182;&#25512;&#21160;&#33258;&#24049;&#30340;&#25919;&#27835;&#35758;&#31243;&#65292;&#26088;&#22312;&#24433;&#21709;&#36873;&#27665;&#34892;&#20026;&#12290;&#26412;&#25991;&#35797;&#22270;&#20998;&#26512;&#19977;&#20010;&#27431;&#27954;&#22269;&#23478;&#25919;&#27835;&#23478;&#30340;&#25512;&#25991;&#65292;&#24182;&#25506;&#32034;&#23427;&#20204;&#30340;&#20256;&#25773;&#24230;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20256;&#36798;&#28040;&#26497;&#24773;&#32490;&#30340;&#25512;&#25991;&#24448;&#24448;&#20250;&#34987;&#26356;&#39057;&#32321;&#22320;&#36716;&#21457;&#12290;&#36890;&#36807;&#21033;&#29992;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#23545;&#26469;&#33258;&#24076;&#33098;&#12289;&#35199;&#29677;&#29273;&#21644;&#32852;&#21512;&#29579;&#22269;&#65288;&#21253;&#25324;&#20998;&#26435;&#25919;&#24220;&#65289;&#35758;&#20250;&#25104;&#21592;&#30340;&#25968;&#21313;&#19975;&#26465;&#25512;&#25991;&#36827;&#34892;&#20102;&#24773;&#24863;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#22320;&#25506;&#31350;&#21644;&#20998;&#26512;&#24046;&#24322;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media has become extremely influential when it comes to policy making in modern societies, especially in the western world, where platforms such as Twitter allow users to follow politicians, thus making citizens more involved in political discussion. In the same vein, politicians use Twitter to express their opinions, debate among others on current topics and promote their political agendas aiming to influence voter behaviour. In this paper, we attempt to analyse tweets of politicians from three European countries and explore the virality of their tweets. Previous studies have shown that tweets conveying negative sentiment are likely to be retweeted more frequently. By utilising state-of-the-art pre-trained language models, we performed sentiment analysis on hundreds of thousands of tweets collected from members of parliament in Greece, Spain and the United Kingdom, including devolved administrations. We achieved this by systematically exploring and analysing the differences bet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#23383;&#20856;&#23398;&#20064;&#23558;&#19978;&#19979;&#25991;&#23884;&#20837;&#20316;&#20026;Transformer&#22240;&#23376;&#30340;&#32447;&#24615;&#21472;&#21152;&#26469;&#25171;&#24320;Transformer&#8220;&#40657;&#21283;&#23376;&#8221;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#23637;&#31034;&#20854;&#25429;&#33719;&#30340;&#23618;&#27425;&#21270;&#35821;&#20041;&#32467;&#26500;&#65292;&#20026;&#26356;&#22909;&#22320;&#29702;&#35299;Transformer&#32593;&#32476;&#30340;&#24037;&#20316;&#26041;&#24335;&#24102;&#26469;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2103.15949</link><description>&lt;p&gt;
&#36890;&#36807;&#23383;&#20856;&#23398;&#20064;&#23454;&#29616;Transformer&#21487;&#35270;&#21270;:&#23558;&#19978;&#19979;&#25991;&#23884;&#20837;&#20316;&#20026;Transformer&#22240;&#23376;&#30340;&#32447;&#24615;&#21472;&#21152;
&lt;/p&gt;
&lt;p&gt;
Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors. (arXiv:2103.15949v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.15949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#23383;&#20856;&#23398;&#20064;&#23558;&#19978;&#19979;&#25991;&#23884;&#20837;&#20316;&#20026;Transformer&#22240;&#23376;&#30340;&#32447;&#24615;&#21472;&#21152;&#26469;&#25171;&#24320;Transformer&#8220;&#40657;&#21283;&#23376;&#8221;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#23637;&#31034;&#20854;&#25429;&#33719;&#30340;&#23618;&#27425;&#21270;&#35821;&#20041;&#32467;&#26500;&#65292;&#20026;&#26356;&#22909;&#22320;&#29702;&#35299;Transformer&#32593;&#32476;&#30340;&#24037;&#20316;&#26041;&#24335;&#24102;&#26469;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;Transformer&#32593;&#32476;&#38382;&#19990;&#20197;&#26469;&#65292;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#34920;&#31034;&#23398;&#20064;&#20013;&#25472;&#36215;&#20102;&#19968;&#22330;&#38761;&#21629;&#12290;&#23613;&#31649;&#24050;&#32463;&#20570;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#26469;&#35299;&#37322;Transformer&#32593;&#32476;&#20013;&#30340;&#34920;&#31034;&#65292;&#20294;&#24191;&#27867;&#35748;&#20026;&#25105;&#20204;&#30340;&#29702;&#35299;&#36824;&#19981;&#22815;&#12290;&#20854;&#20013;&#19968;&#20010;&#37325;&#35201;&#21407;&#22240;&#26159;&#32570;&#20047;&#36275;&#22815;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#26469;&#36827;&#34892;&#35814;&#32454;&#30340;&#20998;&#26512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#23383;&#20856;&#23398;&#20064;&#23558;&#20854;&#20316;&#20026;Transformer&#22240;&#23376;&#30340;&#32447;&#24615;&#21472;&#21152;&#26469;&#25171;&#24320;&#36825;&#20123;&#8220;&#40657;&#21283;&#23376;&#8221;&#12290;&#36890;&#36807;&#21487;&#35270;&#21270;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#34987;Transformer&#22240;&#23376;&#25429;&#33719;&#30340;&#23618;&#27425;&#21270;&#35821;&#20041;&#32467;&#26500;&#65292;&#20363;&#22914;&#35789;&#32423;&#22810;&#20041;&#28040;&#27495;&#12289;&#21477;&#23376;&#32423;&#27169;&#24335;&#24418;&#25104;&#21644;&#38271;&#36317;&#31163;&#20381;&#36182;&#12290;&#34429;&#28982;&#19968;&#20123;&#27169;&#24335;&#31526;&#21512;&#20256;&#32479;&#30340;&#35821;&#35328;&#30693;&#35782;&#65292;&#20294;&#20854;&#20313;&#30340;&#27169;&#24335;&#30456;&#23545;&#20986;&#20046;&#24847;&#26009;&#65292;&#21487;&#33021;&#25552;&#20379;&#26032;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#31181;&#21487;&#35270;&#21270;&#24037;&#20855;&#33021;&#24102;&#26469;&#26356;&#22810;&#30340;&#30693;&#35782;&#21644;&#23545;Transformer&#32593;&#32476;&#24037;&#20316;&#26041;&#24335;&#30340;&#26356;&#22909;&#29702;&#35299;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/z&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer networks have revolutionized NLP representation learning since they were introduced. Though a great effort has been made to explain the representation in transformers, it is widely recognized that our understanding is not sufficient. One important reason is that there lack enough visualization tools for detailed analysis. In this paper, we propose to use dictionary learning to open up these "black boxes" as linear superpositions of transformer factors. Through visualization, we demonstrate the hierarchical semantic structures captured by the transformer factors, e.g., word-level polysemy disambiguation, sentence-level pattern formation, and long-range dependency. While some of these patterns confirm the conventional prior linguistic knowledge, the rest are relatively unexpected, which may provide new insights. We hope this visualization tool can bring further knowledge and a better understanding of how transformer networks work. The code is available at https://github.com/z
&lt;/p&gt;</description></item></channel></rss>