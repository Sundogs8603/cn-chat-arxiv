<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39046;&#22495;&#36866;&#24212;&#25216;&#26415;&#25193;&#23637;&#20154;&#21475;&#30340;&#26041;&#27861;&#65292;&#20197;&#21152;&#36895;&#35757;&#32451;&#24182;&#25552;&#39640;&#20351;&#29992;&#36739;&#23569;&#35774;&#22791;&#36827;&#34892;&#35757;&#32451;&#26102;&#30340;&#27169;&#22411;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.07477</link><description>&lt;p&gt;
&#20351;&#29992;&#31169;&#20154;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#20154;&#21475;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Population Expansion for Training Language Models with Private Federated Learning. (arXiv:2307.07477v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39046;&#22495;&#36866;&#24212;&#25216;&#26415;&#25193;&#23637;&#20154;&#21475;&#30340;&#26041;&#27861;&#65292;&#20197;&#21152;&#36895;&#35757;&#32451;&#24182;&#25552;&#39640;&#20351;&#29992;&#36739;&#23569;&#35774;&#22791;&#36827;&#34892;&#35757;&#32451;&#26102;&#30340;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#23398;&#20064;&#20026;&#20998;&#24067;&#24335;&#35774;&#22791;&#25552;&#20379;&#24102;&#26377;&#27491;&#24335;&#38544;&#31169;&#20445;&#35777;&#30340;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#12290;&#24403;&#35774;&#22791;&#25968;&#37327;&#24222;&#22823;&#26102;&#65292;&#32852;&#37030;&#23398;&#20064;&#19982;&#24046;&#20998;&#38544;&#31169;&#30340;&#32467;&#21512;&#33021;&#22815;&#21450;&#26102;&#29983;&#25104;&#24615;&#33021;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35774;&#22791;&#25968;&#37327;&#36739;&#23569;&#30340;&#24212;&#29992;&#65292;&#30001;&#20110;&#24046;&#20998;&#38544;&#31169;&#22122;&#22768;&#19982;&#35774;&#22791;&#25968;&#37327;&#25104;&#21453;&#27604;&#65292;&#27169;&#22411;&#25928;&#29992;&#19979;&#38477;&#65292;&#21516;&#26102;&#30001;&#20110;&#38656;&#35201;&#31561;&#24453;&#26469;&#33258;&#36739;&#23567;&#35774;&#22791;&#27744;&#30340;&#36275;&#22815;&#23458;&#25143;&#31471;&#21487;&#29992;&#65292;&#35757;&#32451;&#24310;&#36831;&#20063;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#39046;&#22495;&#36866;&#24212;&#25216;&#26415;&#25193;&#23637;&#20154;&#21475;&#65292;&#20197;&#21152;&#24555;&#35757;&#32451;&#24182;&#25913;&#21892;&#20351;&#29992;&#36739;&#23569;&#35774;&#22791;&#36827;&#34892;&#35757;&#32451;&#26102;&#30340;&#26368;&#32456;&#27169;&#22411;&#36136;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#21487;&#20197;&#20351;&#23454;&#38469;&#35821;&#35328;&#24314;&#27169;&#25968;&#25454;&#38598;&#30340;&#25928;&#29992;&#25552;&#39640;13%&#33267;30%&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) combined with differential privacy (DP) offers machine learning (ML) training with distributed devices and with a formal privacy guarantee. With a large population of devices, FL with DP produces a performant model in a timely manner. However, for applications with a smaller population, not only does the model utility degrade as the DP noise is inversely proportional to population, but also the training latency increases since waiting for enough clients to become available from a smaller pool is slower. In this work, we thus propose expanding the population based on domain adaptation techniques to speed up the training and improves the final model quality when training with small populations. We empirically demonstrate that our techniques can improve the utility by 13% to 30% on real-world language modeling datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#29233;&#23572;&#20848;&#35821;&#26041;&#35328;&#30340;&#21464;&#24322;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#25506;&#32034;&#20102;&#29233;&#23572;&#20848;&#21475;&#38899;&#26041;&#35328;&#30340;&#35821;&#38899;&#35782;&#21035;&#65292;&#24182;&#25104;&#21151;&#25645;&#24314;&#20986;&#19968;&#20010;&#20934;&#30830;&#29575;&#20026;73%&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2307.07436</link><description>&lt;p&gt;
&#23545;&#29233;&#23572;&#20848;&#21475;&#38899;&#26041;&#35328;&#30340;&#35821;&#38899;&#35782;&#21035;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards spoken dialect identification of Irish. (arXiv:2307.07436v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#29233;&#23572;&#20848;&#35821;&#26041;&#35328;&#30340;&#21464;&#24322;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#25506;&#32034;&#20102;&#29233;&#23572;&#20848;&#21475;&#38899;&#26041;&#35328;&#30340;&#35821;&#38899;&#35782;&#21035;&#65292;&#24182;&#25104;&#21151;&#25645;&#24314;&#20986;&#19968;&#20010;&#20934;&#30830;&#29575;&#20026;73%&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29233;&#23572;&#20848;&#35821;&#22312;&#26041;&#35328;&#21644;&#21475;&#38899;&#19978;&#38750;&#24120;&#20016;&#23500;&#65292;&#36825;&#20351;&#24471;&#20026;&#36825;&#31181;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#21019;&#24314;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#21464;&#24471;&#22256;&#38590;&#65292;&#22240;&#20026;&#36825;&#26679;&#30340;&#31995;&#32479;&#24517;&#39035;&#24212;&#23545;&#26377;&#38480;&#30340;&#35821;&#26009;&#24211;&#20013;&#30340;&#39640;&#24230;&#21464;&#24322;&#24615;&#12290;&#26368;&#36817;&#19968;&#39033;&#23545;&#29233;&#23572;&#20848;&#35821;ASR&#20013;&#26041;&#35328;&#20559;&#35265;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24179;&#34913;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#23548;&#33268;&#20102;&#19981;&#24179;&#31561;&#30340;&#26041;&#35328;&#24615;&#33021;&#65292;&#38463;&#23572;&#26031;&#29305;&#26041;&#35328;&#30340;&#34920;&#29616;&#22987;&#32456;&#27604;&#24247;&#35834;&#29305;&#25110;&#33945;&#26031;&#29305;&#26041;&#35328;&#35201;&#24046;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#26412;&#23454;&#39564;&#30740;&#31350;&#20102;&#23545;&#29233;&#23572;&#20848;&#21475;&#38899;&#26041;&#35328;&#30340;&#35821;&#38899;&#35782;&#21035;&#65292;&#26088;&#22312;&#23558;&#36825;&#26679;&#30340;&#31995;&#32479;&#32435;&#20837;&#35821;&#38899;&#35782;&#21035;&#27969;&#27700;&#32447;&#20013;&#12290;&#23454;&#39564;&#20013;&#27979;&#35797;&#20102;&#20004;&#20010;&#22768;&#23398;&#20998;&#31867;&#27169;&#22411;&#65292;XLS-R&#21644;ECAPA-TDNN&#65292;&#21644;&#19968;&#20010;&#20351;&#29992;&#39044;&#35757;&#32451;&#29233;&#23572;&#20848;&#35821;BERT&#27169;&#22411;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#20998;&#31867;&#22120;&#12290;ECAPA-TDNN&#34920;&#29616;&#26368;&#22909;&#65292;&#29305;&#21035;&#26159;&#22312;VoxLingua107&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35821;&#35328;&#35782;&#21035;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#25972;&#20307;&#20934;&#30830;&#29575;&#36798;&#21040;73%&#12290;
&lt;/p&gt;
&lt;p&gt;
The Irish language is rich in its diversity of dialects and accents. This compounds the difficulty of creating a speech recognition system for the low-resource language, as such a system must contend with a high degree of variability with limited corpora. A recent study investigating dialect bias in Irish ASR found that balanced training corpora gave rise to unequal dialect performance, with performance for the Ulster dialect being consistently worse than for the Connacht or Munster dialects. Motivated by this, the present experiments investigate spoken dialect identification of Irish, with a view to incorporating such a system into the speech recognition pipeline. Two acoustic classification models are tested, XLS-R and ECAPA-TDNN, in conjunction with a text-based classifier using a pretrained Irish-language BERT model. The ECAPA-TDNN, particularly a model pretrained for language identification on the VoxLingua107 dataset, performed best overall, with an accuracy of 73%. This was furt
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20010;&#20154;&#36130;&#21153;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#34429;&#28982;&#36755;&#20986;&#27969;&#21033;&#21512;&#29702;&#65292;&#20294;&#22312;&#25552;&#20379;&#20934;&#30830;&#21487;&#38752;&#30340;&#36130;&#21153;&#20449;&#24687;&#26041;&#38754;&#20173;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2307.07422</link><description>&lt;p&gt;
LLM&#33021;&#25104;&#20026;&#33391;&#22909;&#30340;&#36130;&#21153;&#39038;&#38382;&#21527;&#65311;&#65306;&#20851;&#20110;&#20010;&#20154;&#20915;&#31574;&#20248;&#21270;&#32467;&#26524;&#30340;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can LLMs be Good Financial Advisors?: An Initial Study in Personal Decision Making for Optimized Outcomes. (arXiv:2307.07422v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07422
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20010;&#20154;&#36130;&#21153;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#34429;&#28982;&#36755;&#20986;&#27969;&#21033;&#21512;&#29702;&#65292;&#20294;&#22312;&#25552;&#20379;&#20934;&#30830;&#21487;&#38752;&#30340;&#36130;&#21153;&#20449;&#24687;&#26041;&#38754;&#20173;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#22914;ChatGPT&#21644;Bard&#65292;&#27491;&#25104;&#20026;&#29992;&#25143;&#21487;&#29992;&#30340;&#65292;&#26377;&#28508;&#21147;&#25913;&#21464;&#20844;&#20247;&#20915;&#31574;&#36136;&#37327;&#30340;&#24037;&#20855;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#26088;&#22312;&#35843;&#26597;&#36825;&#20123;&#31995;&#32479;&#22312;&#20010;&#20154;&#36130;&#21153;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;&#32780;&#37329;&#34701;&#21253;&#23481;&#19968;&#30452;&#26159;&#38134;&#34892;&#30340;&#20027;&#35201;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;13&#20010;&#38382;&#39064;&#65292;&#20195;&#34920;&#20010;&#20154;&#36130;&#21153;&#20013;&#30340;&#38134;&#34892;&#20135;&#21697;&#65292;&#21253;&#25324;&#38134;&#34892;&#36134;&#25143;&#12289;&#20449;&#29992;&#21345;&#12289;&#23384;&#27454;&#35777;&#20070;&#21450;&#20854;&#20135;&#21697;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#19982;&#39640;&#20215;&#36141;&#20080;&#12289;&#25903;&#20184;&#38134;&#34892;&#36153;&#29992;&#21644;&#25237;&#36164;&#24314;&#35758;&#30456;&#20851;&#30340;&#20915;&#31574;&#65292;&#36824;&#21253;&#25324;&#19981;&#21516;&#30340;&#26041;&#35328;&#21644;&#35821;&#35328;&#65288;&#33521;&#35821;&#12289;&#38750;&#27491;&#24335;&#32654;&#22269;&#33521;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#36755;&#20986;&#27969;&#21033;&#32780;&#19988;&#21512;&#29702;&#65292;&#20294;&#22312;&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#25552;&#20379;&#20934;&#30830;&#21487;&#38752;&#30340;&#36130;&#21153;&#20449;&#24687;&#26041;&#38754;&#20173;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Increasingly powerful Large Language Model (LLM) based chatbots, like ChatGPT and Bard, are becoming available to users that have the potential to revolutionize the quality of decision-making achieved by the public. In this context, we set out to investigate how such systems perform in the personal finance domain, where financial inclusion has been an overarching stated aim of banks for decades. We asked 13 questions representing banking products in personal finance: bank account, credit card, and certificate of deposits and their inter-product interactions, and decisions related to high-value purchases, payment of bank dues, and investment advice, and in different dialects and languages (English, African American Vernacular English, and Telugu). We find that although the outputs of the chatbots are fluent and plausible, there are still critical gaps in providing accurate and reliable financial information using LLM-based chatbots.
&lt;/p&gt;</description></item><item><title>Sumformer&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#26102;&#38388;&#20195;&#26367;&#33258;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#29992;&#24635;&#32467;&#28151;&#21512;&#26469;&#22788;&#29702;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.07421</link><description>&lt;p&gt;
Sumformer: &#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#32447;&#24615;&#22797;&#26434;&#24230;&#20195;&#26367;&#33258;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sumformer: A Linear-Complexity Alternative to Self-Attention for Speech Recognition. (arXiv:2307.07421v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07421
&lt;/p&gt;
&lt;p&gt;
Sumformer&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#26102;&#38388;&#20195;&#26367;&#33258;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#29992;&#24635;&#32467;&#28151;&#21512;&#26469;&#22788;&#29702;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20381;&#36182;&#20110;&#33258;&#27880;&#24847;&#21147;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#36827;&#34892;&#20196;&#29260;&#28151;&#21512;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#19982;&#35821;&#38899;&#35821;&#21477;&#30340;&#38271;&#24230;&#21576;&#20108;&#27425;&#20851;&#31995;&#65292;&#23548;&#33268;&#25512;&#29702;&#12289;&#35757;&#32451;&#21644;&#20869;&#23384;&#21344;&#29992;&#36895;&#24230;&#21464;&#24930;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#27604;&#33258;&#27880;&#24847;&#21147;&#26356;&#20415;&#23452;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20294;&#24456;&#38590;&#20445;&#35777;&#36798;&#21040;&#30456;&#21516;&#30340;&#20934;&#30830;&#24615;&#27700;&#24179;&#12290;&#23454;&#38469;&#19978;&#65292;&#32463;&#36807;&#35757;&#32451;&#30340;&#35821;&#38899;&#35782;&#21035;&#22120;&#30340;&#33258;&#27880;&#24847;&#21147;&#26435;&#37325;&#22312;&#26102;&#38388;&#19978;&#21576;&#20840;&#23616;&#24179;&#22343;&#21270;&#30340;&#24418;&#24335;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#32447;&#24615;&#26102;&#38388;&#26367;&#20195;&#33258;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#12290;&#23427;&#29992;&#25152;&#26377;&#26102;&#38388;&#27493;&#38271;&#30340;&#21521;&#37327;&#30340;&#24179;&#22343;&#20540;&#26469;&#24635;&#32467;&#25972;&#20010;&#35821;&#21477;&#12290;&#28982;&#21518;&#23558;&#36825;&#20010;&#21333;&#19968;&#30340;&#24635;&#32467;&#19982;&#29305;&#23450;&#26102;&#38388;&#30340;&#20449;&#24687;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#8220;&#24635;&#32467;&#28151;&#21512;&#8221;&#12290;&#22312;&#26368;&#20808;&#36827;&#30340;ASR&#27169;&#22411;&#20013;&#24341;&#20837;&#24635;&#32467;&#28151;&#21512;&#65292;&#21487;&#20197;&#22312;&#38477;&#20302;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#22810;&#36798;27%&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#25110;&#36229;&#36807;&#20808;&#21069;&#30340;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern speech recognition systems rely on self-attention. Unfortunately, token mixing with self-attention takes quadratic time in the length of the speech utterance, slowing down inference as well as training and increasing memory consumption. Cheaper alternatives to self-attention for ASR have been developed, but fail to consistently reach the same level of accuracy. In practice, however, the self-attention weights of trained speech recognizers take the form of a global average over time. This paper, therefore, proposes a linear-time alternative to self-attention for speech recognition. It summarises a whole utterance with the mean over vectors for all time steps. This single summary is then combined with time-specific information. We call this method ``Summary Mixing''. Introducing Summary Mixing in state-of-the-art ASR models makes it feasible to preserve or exceed previous speech recognition performance while lowering the training and inference times by up to 27% and reducing the m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;GPT&#20197;&#35782;&#21035;&#21487;&#27604;&#20844;&#21496;&#12290;&#20256;&#32479;&#30340;&#21487;&#27604;&#20844;&#21496;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#23450;&#24615;&#26041;&#27861;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#21516;&#34892;&#20844;&#21496;&#65292;&#32780;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#25552;&#21462;&#20844;&#21496;&#25551;&#36848;/&#25688;&#35201;&#20174;&#32780;&#36827;&#34892;&#30456;&#20284;&#24615;&#20998;&#26512;&#65292;&#23454;&#29616;&#26356;&#37327;&#21270;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.07420</link><description>&lt;p&gt;
&#20351;&#29992;GPT&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20197;&#35782;&#21035;&#21487;&#27604;&#20844;&#21496;
&lt;/p&gt;
&lt;p&gt;
Named entity recognition using GPT for identifying comparable companies. (arXiv:2307.07420v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;GPT&#20197;&#35782;&#21035;&#21487;&#27604;&#20844;&#21496;&#12290;&#20256;&#32479;&#30340;&#21487;&#27604;&#20844;&#21496;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#23450;&#24615;&#26041;&#27861;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#21516;&#34892;&#20844;&#21496;&#65292;&#32780;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#25552;&#21462;&#20844;&#21496;&#25551;&#36848;/&#25688;&#35201;&#20174;&#32780;&#36827;&#34892;&#30456;&#20284;&#24615;&#20998;&#26512;&#65292;&#23454;&#29616;&#26356;&#37327;&#21270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20844;&#20849;&#21644;&#31169;&#20154;&#20844;&#21496;&#65292;&#21487;&#27604;&#20844;&#21496;&#20998;&#26512;&#34987;&#24191;&#27867;&#29992;&#20316;&#20844;&#21496;&#20272;&#20540;&#30340;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#31169;&#21215;&#32929;&#26435;&#20844;&#21496;&#30340;&#20272;&#20540;&#65292;&#35813;&#26041;&#27861;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#21487;&#27604;&#20844;&#21496;&#26041;&#27861;&#30340;&#20960;&#31181;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#23450;&#24615;&#26041;&#27861;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#21516;&#34892;&#20844;&#21496;&#65292;&#36825;&#24448;&#24448;&#20351;&#29992;&#24050;&#24314;&#31435;&#30340;&#34892;&#19994;&#20998;&#31867;&#26041;&#26696;&#21644;/&#25110;&#20998;&#26512;&#24072;&#30340;&#30452;&#35273;&#21644;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#21644;&#31169;&#21215;&#32929;&#26435;&#34892;&#19994;&#24320;&#22987;&#20351;&#29992;&#26356;&#22810;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#26426;&#22120;&#23398;&#20064;&#32858;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12290;&#23545;&#20110;NLP&#26041;&#27861;&#65292;&#35813;&#36807;&#31243;&#21253;&#25324;&#20174;&#20844;&#21496;&#30340;&#32593;&#31449;&#25110;&#26469;&#33258;&#26576;&#20123;&#37329;&#34701;&#25968;&#25454;&#24211;&#31995;&#32479;&#30340;&#20844;&#21496;&#25551;&#36848;&#20013;&#25552;&#21462;&#20135;&#21697;&#23454;&#20307;&#65292;&#28982;&#21518;&#36827;&#34892;&#30456;&#20284;&#24615;&#20998;&#26512;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#20844;&#21496;&#32500;&#22522;&#30334;&#31185;&#32593;&#31449;&#30340;&#20844;&#21496;&#25551;&#36848;/&#25688;&#35201;&#65292;&#23637;&#31034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20363;&#22914;GPT
&lt;/p&gt;
&lt;p&gt;
For both public and private firms, comparable companies analysis is widely used as a method for company valuation. In particular, the method is of great value for valuation of private equity companies. The several approaches to the comparable companies method usually rely on a qualitative approach to identifying similar peer companies, which tends to use established industry classification schemes and/or analyst intuition and knowledge. However, more quantitative methods have started being used in the literature and in the private equity industry, in particular, machine learning clustering, and natural language processing (NLP). For NLP methods, the process consists of extracting product entities from e.g., the company's website or company descriptions from some financial database system and then to perform similarity analysis. Here, using companies descriptions/summaries from publicly available companies' Wikipedia websites, we show that using large language models (LLMs), such as GPT
&lt;/p&gt;</description></item><item><title>RoPDA&#26159;&#19968;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;NER&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#36830;&#32493;&#25552;&#31034;&#36827;&#34892;&#23454;&#20307;&#21644;&#19978;&#19979;&#25991;&#22686;&#24378;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#19968;&#33268;&#24615;&#36807;&#28388;&#21644;&#28151;&#21512;&#25216;&#26415;&#20197;&#20248;&#21270;&#22686;&#24378;&#26679;&#26412;&#30340;&#21033;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.07417</link><description>&lt;p&gt;
RoPDA&#65306;&#29992;&#20110;&#20302;&#36164;&#28304;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#40065;&#26834;&#22522;&#20110;&#25552;&#31034;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
RoPDA: Robust Prompt-based Data Augmentation for Low-Resource Named Entity Recognition. (arXiv:2307.07417v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07417
&lt;/p&gt;
&lt;p&gt;
RoPDA&#26159;&#19968;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;NER&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#36830;&#32493;&#25552;&#31034;&#36827;&#34892;&#23454;&#20307;&#21644;&#19978;&#19979;&#25991;&#22686;&#24378;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#19968;&#33268;&#24615;&#36807;&#28388;&#21644;&#28151;&#21512;&#25216;&#26415;&#20197;&#20248;&#21270;&#22686;&#24378;&#26679;&#26412;&#30340;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#22312;&#20302;&#36164;&#28304;NER&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#20197;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#23384;&#22312;&#30772;&#22351;&#21477;&#27861;&#32467;&#26500;&#12289;&#26631;&#35760;-&#26631;&#31614;&#19981;&#21305;&#37197;&#21644;&#23545;&#22806;&#37096;&#30693;&#35782;&#25110;&#25163;&#21160;&#24037;&#20316;&#30340;&#38656;&#27714;&#30340;&#32570;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RoPDA: &#19968;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;NER&#30340;&#40065;&#26834;&#22522;&#20110;&#25552;&#31034;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21644;&#36830;&#32493;&#25552;&#31034;&#65292;RoPDA&#36890;&#36807;&#20116;&#20010;&#22522;&#26412;&#30340;&#22686;&#24378;&#25805;&#20316;&#36827;&#34892;&#23454;&#20307;&#22686;&#24378;&#21644;&#19978;&#19979;&#25991;&#22686;&#24378;&#65292;&#29983;&#25104;&#26631;&#31614;&#32763;&#36716;&#21644;&#20445;&#30041;&#26631;&#31614;&#30340;&#26679;&#26412;&#12290;&#20026;&#20102;&#20248;&#21270;&#22686;&#24378;&#26679;&#26412;&#30340;&#21033;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25216;&#26415;&#65306;&#33258;&#19968;&#33268;&#24615;&#36807;&#28388;&#21644;&#28151;&#21512;&#12290;&#21069;&#32773;&#26377;&#25928;&#22320;&#28040;&#38500;&#20302;&#36136;&#37327;&#26679;&#26412;&#65292;&#21518;&#32773;&#38450;&#27490;&#30452;&#25509;&#21033;&#29992;&#26631;&#31614;&#32763;&#36716;&#26679;&#26412;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;...
&lt;/p&gt;
&lt;p&gt;
Data augmentation has been widely used in low-resource NER tasks to tackle the problem of data sparsity. However, previous data augmentation methods have the disadvantages of disrupted syntactic structures, token-label mismatch, and requirement for external knowledge or manual effort. To address these issues, we propose \textbf{Ro}bust \textbf{P}rompt-based \textbf{D}ata \textbf{A}ugmentation (RoPDA) for low-resource NER. Based on pre-trained language models (PLMs) with continuous prompt, RoPDA performs entity augmentation and context augmentation through five fundamental augmentation operations to generate label-flipping and label-preserving examples. To optimize the utilization of the augmented samples, we present two techniques: Self-Consistency Filtering and mixup. The former effectively eliminates low-quality samples, while the latter prevents performance degradation arising from the direct utilization of label-flipping samples. Extensive experiments on three benchmarks from diffe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;AutoHint&#65292;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#21644;&#20248;&#21270;&#30340;&#26032;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#36755;&#20837;-&#36755;&#20986;&#28436;&#31034;&#20013;&#29983;&#25104;&#25552;&#31034;&#65292;&#24182;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#20248;&#28857;&#65292;&#20248;&#21270;&#21407;&#22987;&#25552;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.07415</link><description>&lt;p&gt;
AutoHint: &#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#19982;&#20248;&#21270;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AutoHint: Automatic Prompt Optimization with Hint Generation. (arXiv:2307.07415v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;AutoHint&#65292;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#21644;&#20248;&#21270;&#30340;&#26032;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#36755;&#20837;-&#36755;&#20986;&#28436;&#31034;&#20013;&#29983;&#25104;&#25552;&#31034;&#65292;&#24182;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#20248;&#28857;&#65292;&#20248;&#21270;&#21407;&#22987;&#25552;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AutoHint&#65292;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33258;&#21160;&#25552;&#31034;&#24037;&#31243;&#21644;&#20248;&#21270;&#30340;&#26032;&#26694;&#26550;&#12290;&#34429;&#28982;LLM&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#27880;&#37322;&#33021;&#21147;&#65292;&#20294;&#23558;&#27492;&#33021;&#21147;&#24212;&#29992;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#20851;&#38190;&#22312;&#20110;&#24320;&#21457;&#39640;&#36136;&#37327;&#30340;&#25552;&#31034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20174;&#36755;&#20837;-&#36755;&#20986;&#28436;&#31034;&#20013;&#27966;&#29983;&#30340;&#20016;&#23500;&#25351;&#23548;&#32435;&#20837;&#21407;&#22987;&#25552;&#31034;&#65292;&#20197;&#32487;&#25215;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#20016;&#23500;&#31216;&#20026;&#8220;&#25552;&#31034;&#8221;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26631;&#35760;&#25968;&#25454;&#20013;&#33258;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20174;&#19968;&#20010;&#21021;&#22987;&#25552;&#31034;&#24320;&#22987;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#25351;&#23548;LLM&#20174;&#38169;&#35823;&#39044;&#27979;&#20013;&#25512;&#26029;&#20986;&#36873;&#23450;&#26679;&#26412;&#30340;&#26032;&#25552;&#31034;&#65292;&#28982;&#21518;&#20174;&#27599;&#20010;&#26679;&#26412;&#30340;&#25552;&#31034;&#20013;&#36827;&#34892;&#24635;&#32467;&#65292;&#24182;&#23558;&#32467;&#26524;&#28155;&#21152;&#22238;&#21021;&#22987;&#25552;&#31034;&#65292;&#24418;&#25104;&#19968;&#20010;&#26032;&#30340;&#20016;&#23500;&#25351;&#23548;&#12290;&#35813;&#26041;&#27861;&#22312;BIG-Bench&#25351;&#20196;&#25512;&#23548;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents AutoHint, a novel framework for automatic prompt engineering and optimization for Large Language Models (LLM). While LLMs have demonstrated remarkable ability in achieving high-quality annotation in various tasks, the key to applying this ability to specific tasks lies in developing high-quality prompts. Thus we propose a framework to inherit the merits of both in-context learning and zero-shot learning by incorporating enriched instructions derived from input-output demonstrations to optimize original prompt. We refer to the enrichment as the hint and propose a framework to automatically generate the hint from labeled data. More concretely, starting from an initial prompt, our method first instructs a LLM to deduce new hints for selected samples from incorrect predictions, and then summarizes from per-sample hints and adds the results back to the initial prompt to form a new, enriched instruction. The proposed method is evaluated on the BIG-Bench Instruction Induct
&lt;/p&gt;</description></item><item><title>&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;HuCurl&#33021;&#22815;&#26681;&#25454;&#20808;&#21069;&#23545;&#26679;&#26412;&#38590;&#24230;&#30340;&#20102;&#35299;&#65292;&#21457;&#29616;&#38750;&#21333;&#35843;&#30340;&#26377;&#25928;&#35838;&#31243;&#65292;&#24182;&#22312;&#22810;&#20010;NLP&#20219;&#21153;&#20013;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.07412</link><description>&lt;p&gt;
HuCurl: &#20154;&#31867;&#24341;&#23548;&#35838;&#31243;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
HuCurl: Human-induced Curriculum Discovery. (arXiv:2307.07412v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07412
&lt;/p&gt;
&lt;p&gt;
&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;HuCurl&#33021;&#22815;&#26681;&#25454;&#20808;&#21069;&#23545;&#26679;&#26412;&#38590;&#24230;&#30340;&#20102;&#35299;&#65292;&#21457;&#29616;&#38750;&#21333;&#35843;&#30340;&#26377;&#25928;&#35838;&#31243;&#65292;&#24182;&#22312;&#22810;&#20010;NLP&#20219;&#21153;&#20013;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#35838;&#31243;&#21457;&#29616;&#38382;&#39064;&#65292;&#24182;&#25551;&#36848;&#20102;&#19968;&#20010;&#33021;&#22815;&#22312;&#35838;&#31243;&#31354;&#38388;&#20013;&#22522;&#20110;&#20808;&#21069;&#26377;&#20851;&#26679;&#26412;&#38590;&#24230;&#30340;&#30693;&#35782;&#21457;&#29616;&#26377;&#25928;&#35838;&#31243;&#30340;&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#20351;&#29992;&#27880;&#37322;&#29109;&#21644;&#25439;&#22833;&#20316;&#20026;&#38590;&#24230;&#30340;&#24230;&#37327;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65306;&#65288;i&#65289;&#32473;&#23450;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#26368;&#20339;&#21457;&#29616;&#35838;&#31243;&#24448;&#24448;&#19982;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#21333;&#35843;&#35838;&#31243;&#30456;&#21453;&#65307;&#65288;ii&#65289;&#20256;&#32479;&#30340;&#30001;&#26131;&#21040;&#38590;&#25110;&#30001;&#38590;&#21040;&#26131;&#36807;&#28193;&#30340;&#35838;&#31243;&#24448;&#24448;&#23384;&#22312;&#34920;&#29616;&#19981;&#20339;&#30340;&#39118;&#38505;&#65307;&#65288;iii&#65289;&#23545;&#36739;&#23567;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21457;&#29616;&#30340;&#35838;&#31243;&#22312;&#36739;&#22823;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#28085;&#30422;&#20102;&#19968;&#20123;&#29616;&#26377;&#30340;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#22312;&#20960;&#20010;NLP&#20219;&#21153;&#19978;&#32988;&#36807;&#23427;&#20204;&#30340;&#35838;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the problem of curriculum discovery and describe a curriculum learning framework capable of discovering effective curricula in a curriculum space based on prior knowledge about sample difficulty. Using annotation entropy and loss as measures of difficulty, we show that (i): the top-performing discovered curricula for a given model and dataset are often non-monotonic as opposed to monotonic curricula in existing literature, (ii): the prevailing easy-to-hard or hard-to-easy transition curricula are often at the risk of underperforming, and (iii): the curricula discovered for smaller datasets and models perform well on larger datasets and models respectively. The proposed framework encompasses some of the existing curriculum learning approaches and can discover curricula that outperform them across several NLP tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;8&#31181;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#24182;&#21457;&#29616;CopyLeaks&#26159;&#26368;&#20934;&#30830;&#30340;&#65292;GPTKit&#26159;&#26368;&#20339;&#30340;&#20943;&#23569;&#35823;&#25253;&#29575;&#30340;&#24037;&#20855;&#65292;GLTR&#26159;&#26368;&#20855;&#38887;&#24615;&#30340;&#24037;&#20855;&#12290;&#36825;&#20123;&#32467;&#26524;&#23558;&#20026;&#25945;&#32946;&#24037;&#20316;&#32773;&#25552;&#20379;&#32500;&#25252;&#23398;&#26415;&#35802;&#20449;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2307.07411</link><description>&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#25945;&#32946;&#20013;&#26816;&#27979;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#65306;ChatGPT&#26696;&#20363;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Detecting LLM-Generated Text in Computing Education: A Comparative Study for ChatGPT Cases. (arXiv:2307.07411v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07411
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;8&#31181;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#24182;&#21457;&#29616;CopyLeaks&#26159;&#26368;&#20934;&#30830;&#30340;&#65292;GPTKit&#26159;&#26368;&#20339;&#30340;&#20943;&#23569;&#35823;&#25253;&#29575;&#30340;&#24037;&#20855;&#65292;GLTR&#26159;&#26368;&#20855;&#38887;&#24615;&#30340;&#24037;&#20855;&#12290;&#36825;&#20123;&#32467;&#26524;&#23558;&#20026;&#25945;&#32946;&#24037;&#20316;&#32773;&#25552;&#20379;&#32500;&#25252;&#23398;&#26415;&#35802;&#20449;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26368;&#36817;&#25913;&#36827;&#21644;&#24191;&#27867;&#24212;&#29992;&#65292;&#23427;&#20204;&#23545;&#25945;&#32946;&#20013;&#30340;&#23398;&#26415;&#35802;&#20449;&#26500;&#25104;&#20102;&#20005;&#37325;&#23041;&#32961;&#12290;&#29616;&#20195;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#35797;&#22270;&#36890;&#36807;&#20026;&#25945;&#32946;&#24037;&#20316;&#32773;&#25552;&#20379;&#35780;&#20272;&#25991;&#26412;&#26159;&#21542;&#20026;LLM&#29983;&#25104;&#30340;&#26381;&#21153;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;ChatGPT&#21019;&#24314;&#20043;&#21069;&#35745;&#31639;&#26426;&#31185;&#23398;&#23398;&#29983;&#25552;&#20132;&#30340;124&#20221;&#20316;&#19994;&#65292;&#28982;&#21518;&#29983;&#25104;&#20102;40&#20221;ChatGPT&#20316;&#19994;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#36890;&#36807;&#20934;&#30830;&#24230;&#12289;&#35823;&#25253;&#29575;&#21644;&#38887;&#24615;&#36825;&#19977;&#20010;&#25351;&#26631;&#35780;&#20272;&#20102;&#20843;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#30446;&#30340;&#26159;&#21521;&#31038;&#21306;&#25552;&#20379;LLM&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#30340;&#24037;&#20316;&#24773;&#20917;&#65292;&#24182;&#20026;&#25945;&#32946;&#24037;&#20316;&#32773;&#25552;&#20379;&#27934;&#23519;&#65292;&#20197;&#26356;&#22909;&#22320;&#32500;&#25252;&#35838;&#31243;&#30340;&#23398;&#26415;&#35802;&#20449;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#21457;&#29616;CopyLeaks&#26159;&#26368;&#20934;&#30830;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;GPTKit&#26159;&#20943;&#23569;&#35823;&#25253;&#29575;&#30340;&#26368;&#20339;LLM&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;GLTR&#26159;&#26368;&#20855;&#38887;&#24615;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the recent improvements and wide availability of Large Language Models (LLMs), they have posed a serious threat to academic integrity in education. Modern LLM-generated text detectors attempt to combat the problem by offering educators with services to assess whether some text is LLM-generated. In this work, we have collected 124 submissions from computer science students before the creation of ChatGPT. We then generated 40 ChatGPT submissions. We used this data to evaluate eight publicly-available LLM-generated text detectors through the measures of accuracy, false positives, and resilience. The purpose of this work is to inform the community of what LLM-generated text detectors work and which do not, but also to provide insights for educators to better maintain academic integrity in their courses. Our results find that CopyLeaks is the most accurate LLM-generated text detector, GPTKit is the best LLM-generated text detector to reduce false positives, and GLTR is the most resil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;CheXOFA&#65292;&#36890;&#36807;&#22312;&#19968;&#33324;&#39046;&#22495;&#30340;&#35757;&#32451;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36716;&#31227;&#21040;&#33016;&#37096;X&#23556;&#32447;&#39046;&#22495;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#25152;&#38656;&#30340;&#30693;&#35782;&#21644;&#25216;&#33021;&#65292;&#24182;&#22312;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#33719;&#24471;&#20102;RadSum23&#27979;&#35797;&#38598;&#30340;&#31532;&#19968;&#21517;&#12290;</title><link>http://arxiv.org/abs/2307.07409</link><description>&lt;p&gt;
KU-DMIS-MSRA&#22312;RadSum23&#20013;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
KU-DMIS-MSRA at RadSum23: Pre-trained Vision-Language Model for Radiology Report Summarization. (arXiv:2307.07409v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;CheXOFA&#65292;&#36890;&#36807;&#22312;&#19968;&#33324;&#39046;&#22495;&#30340;&#35757;&#32451;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36716;&#31227;&#21040;&#33016;&#37096;X&#23556;&#32447;&#39046;&#22495;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#25152;&#38656;&#30340;&#30693;&#35782;&#21644;&#25216;&#33021;&#65292;&#24182;&#22312;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#33719;&#24471;&#20102;RadSum23&#27979;&#35797;&#38598;&#30340;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CheXOFA&#65292;&#19968;&#31181;&#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#39046;&#22495;&#30340;&#26032;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLM)&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#39318;&#20808;&#22312;&#19968;&#33324;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20877;&#36716;&#31227;&#21040;&#33016;&#37096;X&#23556;&#32447;&#39046;&#22495;&#12290;&#22312;&#19968;&#20010;&#33879;&#21517;&#30340;VLM&#20013;&#65292;&#25105;&#20204;&#23558;&#21508;&#31181;&#29305;&#23450;&#39046;&#22495;&#30340;&#20219;&#21153;&#32479;&#19968;&#20026;&#19968;&#20010;&#31616;&#21333;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#27169;&#24335;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#20174;&#26377;&#38480;&#30340;&#39046;&#22495;&#36164;&#28304;&#20013;&#26377;&#25928;&#22320;&#23398;&#20064;&#25152;&#38656;&#30340;&#30693;&#35782;&#21644;&#25216;&#33021;&#12290;&#36890;&#36807;&#22312;BioNLP&#20849;&#20139;&#20219;&#21153;&#25552;&#20379;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21463;&#30410;&#20110;&#36328;&#22810;&#20010;&#20219;&#21153;&#21644;&#39046;&#22495;&#30340;&#35757;&#32451;&#12290;&#36890;&#36807;&#38598;&#25104;&#21644;&#20107;&#23454;&#26657;&#20934;&#31561;&#24494;&#22937;&#30340;&#25216;&#24039;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;RadSum23&#30340;&#38544;&#34255;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce CheXOFA, a new pre-trained vision-language model (VLM) for the chest X-ray domain. Our model is initially pre-trained on various multimodal datasets within the general domain before being transferred to the chest X-ray domain. Following a prominent VLM, we unify various domain-specific tasks into a simple sequence-to-sequence schema. It enables the model to effectively learn the required knowledge and skills from limited resources in the domain. Demonstrating superior performance on the benchmark datasets provided by the BioNLP shared task, our model benefits from its training across multiple tasks and domains. With subtle techniques including ensemble and factual calibration, our system achieves first place on the RadSum23 leaderboard for the hidden test set.
&lt;/p&gt;</description></item><item><title>&#38899;&#32032;&#26816;&#32034;&#25216;&#26415;&#36890;&#36807;&#29305;&#23450;&#30340;&#32593;&#32476;&#26500;&#24314;&#26041;&#24335;&#23454;&#29616;&#20102;&#35821;&#38899;&#21644;&#22270;&#20687;&#30340;&#20934;&#30830;&#26816;&#32034;&#65292;&#22312;&#29305;&#23450;&#30340;&#39046;&#22495;&#21644;&#35789;&#27719;&#38598;&#21512;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.07407</link><description>&lt;p&gt;
&#38899;&#32032;&#26816;&#32034;&#65307;&#35821;&#38899;&#35782;&#21035;&#65307;&#20803;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Phoneme-retrieval; voice recognition; vowels recognition. (arXiv:2307.07407v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07407
&lt;/p&gt;
&lt;p&gt;
&#38899;&#32032;&#26816;&#32034;&#25216;&#26415;&#36890;&#36807;&#29305;&#23450;&#30340;&#32593;&#32476;&#26500;&#24314;&#26041;&#24335;&#23454;&#29616;&#20102;&#35821;&#38899;&#21644;&#22270;&#20687;&#30340;&#20934;&#30830;&#26816;&#32034;&#65292;&#22312;&#29305;&#23450;&#30340;&#39046;&#22495;&#21644;&#35789;&#27719;&#38598;&#21512;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38899;&#32032;&#26816;&#32034;&#25216;&#26415;&#65292;&#20854;&#29305;&#28857;&#26159;&#32593;&#32476;&#30340;&#26500;&#24314;&#26041;&#24335;&#12290;&#32473;&#20986;&#20102;&#19968;&#32452;&#21021;&#22987;&#31070;&#32463;&#20803;&#65292;&#36825;&#20123;&#31070;&#32463;&#20803;&#30340;&#25968;&#37327;&#22823;&#33268;&#31561;&#20110;&#25968;&#25454;&#30340;&#20856;&#22411;&#32467;&#26500;&#25968;&#37327;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#32593;&#32476;&#26159;&#29992;&#20110;&#35821;&#38899;&#26816;&#32034;&#65292;&#21017;&#31070;&#32463;&#20803;&#30340;&#25968;&#37327;&#24517;&#39035;&#31561;&#20110;&#29305;&#23450;&#20154;&#25152;&#23646;&#31038;&#20250;&#32676;&#20307;&#25152;&#35762;&#35821;&#35328;&#30340;&#29305;&#24449;&#38899;&#32032;&#25968;&#37327;&#12290;&#36890;&#24120;&#65292;&#36825;&#26159;&#19968;&#39033;&#38750;&#24120;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#32593;&#32476;&#30340;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#29992;&#20110;&#23398;&#20064;&#30340;&#26679;&#26412;&#12290;&#22914;&#26524;&#32593;&#32476;&#26159;&#29992;&#20110;&#22270;&#20687;&#26816;&#32034;&#65292;&#21017;&#20165;&#22312;&#35201;&#26816;&#32034;&#30340;&#25968;&#25454;&#23646;&#20110;&#29305;&#23450;&#30340;&#22270;&#20687;&#38598;&#21512;&#26102;&#36215;&#20316;&#29992;&#12290;&#22914;&#26524;&#32593;&#32476;&#26159;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#65292;&#21017;&#20165;&#23545;&#26576;&#20010;&#29305;&#23450;&#30340;&#35789;&#27719;&#38598;&#21512;&#36215;&#20316;&#29992;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#20363;&#23376;&#26159;&#29992;&#20110;&#39134;&#26426;&#39134;&#34892;&#30340;&#25351;&#20196;&#12290;&#20363;&#22914;&#65292;&#8220;&#39134;&#26426;&#21521;&#19996;&#26041;&#36716;120&#24230;&#8221;&#36825;&#26679;&#30340;&#25351;&#20196;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#34987;&#35782;&#21035;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
A phoneme-retrieval technique is proposed, which is due to the particular way of the construction of the network. An initial set of neurons is given. The number of these neurons is approximately equal to the number of typical structures of the data. For example if the network is built for voice retrieval then the number of neurons must be equal to the number of characteristic phonemes of the alphabet of the language spoken by the social group to which the particular person belongs. Usually this task is very complicated and the network can depend critically on the samples used for the learning. If the network is built for image retrieval then it works only if the data to be retrieved belong to a particular set of images. If the network is built for voice recognition it works only for some particular set of words. A typical example is the words used for the flight of airplanes. For example a command like the "airplane should make a turn of 120 degrees towards the east" can be easily reco
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#22522;&#20110;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#22235;&#31181;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#23391;&#21152;&#25289;&#25991;&#26412;&#25688;&#35201;&#27169;&#22411;&#30340;&#36755;&#20986;&#26469;&#30830;&#23450;&#32473;&#23450;&#25991;&#26412;&#30340;&#26368;&#20934;&#30830;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2307.07392</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#25490;&#21517;&#30340;&#26041;&#27861;&#22686;&#24378;&#23391;&#21152;&#25289;&#25991;&#26412;&#25688;&#35201;&#30340;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Rank Your Summaries: Enhancing Bengali Text Summarization via Ranking-based Approach. (arXiv:2307.07392v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07392
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#22522;&#20110;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#22235;&#31181;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#23391;&#21152;&#25289;&#25991;&#26412;&#25688;&#35201;&#27169;&#22411;&#30340;&#36755;&#20986;&#26469;&#30830;&#23450;&#32473;&#23450;&#25991;&#26412;&#30340;&#26368;&#20934;&#30830;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#26082;&#39640;&#25928;&#21448;&#20934;&#30830;&#30340;&#25991;&#26412;&#25688;&#35201;&#25216;&#26415;&#30340;&#38656;&#27714;&#26085;&#30410;&#22686;&#21152;&#65292;&#25506;&#32034;&#33021;&#22815;&#22686;&#24378;&#19987;&#20026;&#23391;&#21152;&#25289;&#25991;&#26412;&#25688;&#35201;&#32780;&#35774;&#35745;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#31934;&#30830;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#20013;&#65292;&#20154;&#20204;&#21487;&#20197;&#20351;&#29992;&#20247;&#22810;&#30340;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#20123;&#39044;&#35757;&#32451;&#25688;&#35201;&#27169;&#22411;&#29983;&#25104;&#30340;&#21508;&#31181;&#36873;&#39033;&#20013;&#65292;&#30830;&#23450;&#32473;&#23450;&#25991;&#26412;&#30340;&#26368;&#20855;&#20449;&#24687;&#37327;&#21644;&#30456;&#20851;&#24615;&#30340;&#25688;&#35201;&#21464;&#24471;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#22235;&#31181;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#23391;&#21152;&#25289;&#25991;&#26412;&#25688;&#35201;&#27169;&#22411;&#30340;&#36755;&#20986;&#26469;&#30830;&#23450;&#32473;&#23450;&#25991;&#26412;&#30340;&#26368;&#20934;&#30830;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#25688;&#35201;&#12290;&#35813;&#36807;&#31243;&#39318;&#20808;&#23545;&#36755;&#20837;&#25991;&#26412;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#21253;&#25324;&#21435;&#38500;&#29305;&#27530;&#23383;&#31526;&#21644;&#26631;&#28857;&#31526;&#21495;&#31561;&#19981;&#24517;&#35201;&#30340;&#20803;&#32032;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21033;&#29992;&#22235;&#20010;&#39044;&#35757;&#32451;&#30340;&#25688;&#35201;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing need for text summarization techniques that are both efficient and accurate, it becomes crucial to explore avenues that enhance the quality and precision of pre-trained models specifically tailored for summarizing Bengali texts. When it comes to text summarization tasks, there are numerous pre-trained transformer models at one's disposal. Consequently, it becomes quite a challenge to discern the most informative and relevant summary for a given text among the various options generated by these pre-trained summarization models. This paper aims to identify the most accurate and informative summary for a given text by utilizing a simple but effective ranking-based approach that compares the output of four different pre-trained Bengali text summarization models. The process begins by carrying out preprocessing of the input text that involves eliminating unnecessary elements such as special characters and punctuation marks. Next, we utilize four pre-trained summarization
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21477;&#23376;&#23884;&#20837;&#30340;&#26500;&#25104;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#25991;&#26412;&#21644;&#20854;&#35789;&#32452;&#25104;&#20998;&#30340;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#25991;&#26412;&#34920;&#31034;&#30340;&#30446;&#26631;&#65292;&#24182;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#21487;&#27604;&#36739;&#30340;&#25913;&#36827;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#30446;&#26631;&#25110;&#32593;&#32476;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.07380</link><description>&lt;p&gt;
&#21477;&#23376;&#23884;&#20837;&#30340;&#26500;&#25104;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Composition-contrastive Learning for Sentence Embeddings. (arXiv:2307.07380v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07380
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21477;&#23376;&#23884;&#20837;&#30340;&#26500;&#25104;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#25991;&#26412;&#21644;&#20854;&#35789;&#32452;&#25104;&#20998;&#30340;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#25991;&#26412;&#34920;&#31034;&#30340;&#30446;&#26631;&#65292;&#24182;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#21487;&#27604;&#36739;&#30340;&#25913;&#36827;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#30446;&#26631;&#25110;&#32593;&#32476;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#30340;&#21521;&#37327;&#34920;&#31034;&#22312;&#25628;&#32034;&#24212;&#29992;&#20013;&#38750;&#24120;&#26222;&#36941;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#25991;&#26412;&#34920;&#31034;&#65307;&#36890;&#36807;&#26368;&#22823;&#21270;&#30456;&#21516;&#25991;&#26412;&#30340;&#24494;&#23567;&#25200;&#21160;&#23884;&#20837;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#24182;&#40723;&#21169;&#23884;&#20837;&#22312;&#26356;&#24191;&#27867;&#30340;&#35821;&#26009;&#24211;&#20013;&#30340;&#22343;&#21248;&#20998;&#24067;&#12290;&#19982;&#27492;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#22823;&#21270;&#25991;&#26412;&#21644;&#20854;&#35789;&#32452;&#25104;&#20998;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#36825;&#19968;&#30446;&#26631;&#30340;&#20960;&#20010;&#23454;&#29616;&#26041;&#24335;&#65292;&#24182;&#35814;&#32454;&#35828;&#26126;&#20102;&#27599;&#31181;&#24773;&#20917;&#19979;&#23545;&#34920;&#31034;&#30340;&#24433;&#21709;&#12290;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25552;&#39640;&#22522;&#32447;&#27700;&#24179;&#26041;&#38754;&#26377;&#25152;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#22312;&#19981;&#20135;&#29983;&#36741;&#21161;&#35757;&#32451;&#30446;&#26631;&#25110;&#39069;&#22806;&#32593;&#32476;&#21442;&#25968;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#36825;&#26679;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vector representations of natural language are ubiquitous in search applications. Recently, various methods based on contrastive learning have been proposed to learn textual representations from unlabelled data; by maximizing alignment between minimally-perturbed embeddings of the same text, and encouraging a uniform distribution of embeddings across a broader corpus. Differently, we propose maximizing alignment between texts and a composition of their phrasal constituents. We consider several realizations of this objective and elaborate the impact on representations in each case. Experimental results on semantic textual similarity tasks show improvements over baselines that are comparable with state-of-the-art approaches. Moreover, this work is the first to do so without incurring costs in auxiliary training objectives or additional network parameters.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#23545;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#22312;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#21644;&#25991;&#26412;&#20013;&#36827;&#34892;&#32852;&#21512;&#23398;&#20064;&#30340;&#24403;&#21069;&#29366;&#20917;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#25506;&#32034;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2307.07362</link><description>&lt;p&gt;
&#19968;&#31687;&#20851;&#20110;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#22312;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#21644;&#25991;&#26412;&#20013;&#30340;&#25195;&#25551;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A scoping review on multimodal deep learning in biomedical images and texts. (arXiv:2307.07362v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07362
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#23545;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#22312;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#21644;&#25991;&#26412;&#20013;&#36827;&#34892;&#32852;&#21512;&#23398;&#20064;&#30340;&#24403;&#21069;&#29366;&#20917;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#25506;&#32034;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#30340;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#21644;&#39044;&#21518;&#31995;&#32479;&#24212;&#35813;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#65288;MDL&#65289;&#28041;&#21450;&#22810;&#31181;&#25968;&#25454;&#28304;&#65288;&#22914;&#22270;&#20687;&#21644;&#25991;&#26412;&#65289;&#30340;&#25972;&#21512;&#65292;&#26377;&#28508;&#21147;&#24443;&#24213;&#25913;&#21464;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#39046;&#22495;&#30452;&#21040;&#26368;&#36817;&#25165;&#24341;&#36215;&#30740;&#31350;&#20154;&#21592;&#30340;&#27880;&#24847;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#23545;&#36825;&#20010;&#20027;&#39064;&#36827;&#34892;&#31995;&#32479;&#32508;&#36848;&#65292;&#30830;&#23450;&#24403;&#21069;&#24037;&#20316;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25506;&#32034;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#23545;&#35813;&#39046;&#22495; current state &#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#32852;&#21512;&#23398;&#20064;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#36825;&#20004;&#31181;&#25968;&#25454;&#31867;&#22411;&#22312; MDL &#30740;&#31350;&#20013;&#26368;&#24120;&#29992;&#12290;&#26412;&#30740;&#31350;&#22238;&#39038;&#20102;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#22312;&#20116;&#20010;&#20219;&#21153;&#20013;&#30340;&#24403;&#21069;&#24212;&#29992;&#65306;&#65288;1&#65289;&#25253;&#21578;&#29983;&#25104;&#65292;&#65288;2&#65289;&#35270;&#35273;&#38382;&#31572;&#65292;&#65288;3&#65289;&#20132;&#21449;...
&lt;/p&gt;
&lt;p&gt;
Computer-assisted diagnostic and prognostic systems of the future should be capable of simultaneously processing multimodal data. Multimodal deep learning (MDL), which involves the integration of multiple sources of data, such as images and text, has the potential to revolutionize the analysis and interpretation of biomedical data. However, it only caught researchers' attention recently. To this end, there is a critical need to conduct a systematic review on this topic, identify the limitations of current work, and explore future directions. In this scoping review, we aim to provide a comprehensive overview of the current state of the field and identify key concepts, types of studies, and research gaps with a focus on biomedical images and texts joint learning, mainly because these two were the most commonly available data types in MDL research. This study reviewed the current uses of multimodal deep learning on five tasks: (1) Report generation, (2) Visual question answering, (3) Cros
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#27880;&#37322;&#30340;&#25163;&#35821;&#32763;&#35793;&#20013;&#30340;&#27880;&#37322;&#20851;&#27880;&#26426;&#21046;&#65288;Gloss Attention&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#35821;&#20041;&#36793;&#30028;&#20301;&#32622;&#21644;&#20840;&#23616;&#29702;&#35299;&#25163;&#35821;&#35270;&#39057;&#65292;&#23454;&#29616;&#20102;&#23545;&#25163;&#35821;&#35270;&#39057;&#30340;&#20934;&#30830;&#32763;&#35793;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23558;&#21477;&#23376;&#38388;&#30456;&#20284;&#24615;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#25552;&#39640;&#20102;&#32763;&#35793;&#32593;&#32476;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.07361</link><description>&lt;p&gt;
&#26080;&#38656;&#27880;&#37322;&#30340;&#25163;&#35821;&#32763;&#35793;&#20013;&#30340;&#27880;&#37322;&#20851;&#27880;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Gloss Attention for Gloss-free Sign Language Translation. (arXiv:2307.07361v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#27880;&#37322;&#30340;&#25163;&#35821;&#32763;&#35793;&#20013;&#30340;&#27880;&#37322;&#20851;&#27880;&#26426;&#21046;&#65288;Gloss Attention&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#35821;&#20041;&#36793;&#30028;&#20301;&#32622;&#21644;&#20840;&#23616;&#29702;&#35299;&#25163;&#35821;&#35270;&#39057;&#65292;&#23454;&#29616;&#20102;&#23545;&#25163;&#35821;&#35270;&#39057;&#30340;&#20934;&#30830;&#32763;&#35793;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23558;&#21477;&#23376;&#38388;&#30456;&#20284;&#24615;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#25552;&#39640;&#20102;&#32763;&#35793;&#32593;&#32476;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#25163;&#35821;&#32763;&#35793;&#26041;&#27861;&#37117;&#38656;&#35201;&#20351;&#29992;&#27880;&#37322;&#26469;&#25552;&#20379;&#39069;&#22806;&#30340;&#30417;&#30563;&#20449;&#24687;&#65292;&#28982;&#32780;&#65292;&#27880;&#37322;&#30340;&#33719;&#21462;&#24182;&#19981;&#23481;&#26131;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#29616;&#26377;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#65292;&#30830;&#35748;&#27880;&#37322;&#22914;&#20309;&#20351;&#25163;&#35821;&#32763;&#35793;&#26356;&#23481;&#26131;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27880;&#37322;&#21487;&#20197;&#20026;&#27169;&#22411;&#25552;&#20379;&#20004;&#20010;&#26041;&#38754;&#30340;&#20449;&#24687;&#65306;1&#65289;&#23427;&#21487;&#20197;&#24110;&#21161;&#27169;&#22411;&#38544;&#24335;&#22320;&#23398;&#20064;&#36830;&#32493;&#25163;&#35821;&#35270;&#39057;&#20013;&#30340;&#35821;&#20041;&#36793;&#30028;&#20301;&#32622;&#65292;2&#65289;&#23427;&#21487;&#20197;&#24110;&#21161;&#27169;&#22411;&#20840;&#23616;&#29702;&#35299;&#25163;&#35821;&#35270;&#39057;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#27880;&#37322;&#20851;&#27880;&#8221;&#26426;&#21046;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#22312;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#30340;&#35270;&#39057;&#29255;&#27573;&#20869;&#23616;&#37096;&#20851;&#27880;&#65292;&#23601;&#20687;&#27880;&#37322;&#24110;&#21161;&#29616;&#26377;&#27169;&#22411;&#19968;&#26679;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#20013;&#21477;&#23376;&#38388;&#30456;&#20284;&#24615;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#25105;&#20204;&#30340;&#27880;&#37322;&#20851;&#27880;&#25163;&#35821;&#32763;&#35793;&#32593;&#32476;&#65288;GASLT&#65289;&#20013;&#65292;&#20197;&#24110;&#21161;&#20854;&#22312;&#21477;&#23376;&#23618;&#38754;&#19978;&#29702;&#35299;&#25163;&#35821;&#35270;&#39057;&#12290;&#22312;&#22810;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most sign language translation (SLT) methods to date require the use of gloss annotations to provide additional supervision information, however, the acquisition of gloss is not easy. To solve this problem, we first perform an analysis of existing models to confirm how gloss annotations make SLT easier. We find that it can provide two aspects of information for the model, 1) it can help the model implicitly learn the location of semantic boundaries in continuous sign language videos, 2) it can help the model understand the sign language video globally. We then propose \emph{gloss attention}, which enables the model to keep its attention within video segments that have the same semantics locally, just as gloss helps existing models do. Furthermore, we transfer the knowledge of sentence-to-sentence similarity from the natural language model to our gloss attention SLT network (GASLT) to help it understand sign language videos at the sentence level. Experimental results on multiple large-s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25299;&#23637;&#20102;&#35780;&#20272;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#21051;&#26495;&#20559;&#35265;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#36328;&#35821;&#35328;&#20998;&#26512;&#21457;&#29616;mGPT-2&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#26174;&#31034;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#21453;&#21051;&#26495;&#34892;&#20026;&#65292;&#24182;&#19988;&#33521;&#35821;&#27169;&#22411;&#34920;&#29616;&#20986;&#26368;&#24378;&#30340;&#20559;&#35265;&#65292;&#32780;&#22303;&#32819;&#20854;&#35821;&#21017;&#26368;&#19981;&#26126;&#26174;&#12290;</title><link>http://arxiv.org/abs/2307.07331</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#30340;&#21051;&#26495;&#20559;&#35265;&#26377;&#20309;&#19981;&#21516;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Different Is Stereotypical Bias Across Languages?. (arXiv:2307.07331v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25299;&#23637;&#20102;&#35780;&#20272;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#21051;&#26495;&#20559;&#35265;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#36328;&#35821;&#35328;&#20998;&#26512;&#21457;&#29616;mGPT-2&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#26174;&#31034;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#21453;&#21051;&#26495;&#34892;&#20026;&#65292;&#24182;&#19988;&#33521;&#35821;&#27169;&#22411;&#34920;&#29616;&#20986;&#26368;&#24378;&#30340;&#20559;&#35265;&#65292;&#32780;&#22303;&#32819;&#20854;&#35821;&#21017;&#26368;&#19981;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#35780;&#20272;&#39044;&#35757;&#32451;&#30340;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21051;&#26495;&#20559;&#35265;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#22320;&#35843;&#26597;(a)&#22810;&#35821;&#35328;&#27169;&#22411;&#21644;&#21333;&#35821;&#27169;&#22411;&#12289;(b)&#19981;&#21516;&#22522;&#30784;&#26550;&#26500;&#19979;&#30340;&#21051;&#26495;&#20559;&#35265;&#12289;(c)&#22810;&#31181;&#35821;&#35328;&#20013;&#30340;&#20559;&#35265;&#65292;&#25193;&#23637;&#20102;&#35813;&#30740;&#31350;&#39046;&#22495;&#30340;&#22810;&#20010;&#26041;&#38754;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21033;&#29992;&#33521;&#35821;&#30340;StereoSet&#25968;&#25454;&#38598;&#23558;&#20854;&#21322;&#33258;&#21160;&#32763;&#35793;&#25104;&#24503;&#35821;&#12289;&#27861;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#22303;&#32819;&#20854;&#35821;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#36827;&#34892;&#36825;&#31181;&#31867;&#22411;&#30340;&#20998;&#26512;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#25105;&#20204;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#19968;&#20010;&#26356;&#20026;&#32454;&#33268;&#30340;&#30011;&#38754;&#65292;&#20197;&#21450;&#19982;&#20165;&#33521;&#35821;&#20998;&#26512;&#26377;&#26174;&#33879;&#24046;&#24322;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20027;&#35201;&#24471;&#20986;&#20197;&#19979;&#32467;&#35770;&#65306;mGPT-2&#65288;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#65289;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#26174;&#31034;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#21453;&#21051;&#26495;&#34892;&#20026;&#65292;&#33521;&#35821;&#65288;&#21333;&#35821;&#65289;&#27169;&#22411;&#34920;&#29616;&#20986;&#26368;&#24378;&#30340;&#20559;&#35265;&#65292;&#24182;&#19988;&#25968;&#25454;&#38598;&#20013;&#21453;&#26144;&#30340;&#21051;&#26495;&#21360;&#35937;&#22312;&#22303;&#32819;&#20854;&#35821;&#20013;&#26368;&#19981;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have demonstrated how to assess the stereotypical bias in pre-trained English language models. In this work, we extend this branch of research in multiple different dimensions by systematically investigating (a) mono- and multilingual models of (b) different underlying architectures with respect to their bias in (c) multiple different languages. To that end, we make use of the English StereoSet data set (Nadeem et al., 2021), which we semi-automatically translate into German, French, Spanish, and Turkish. We find that it is of major importance to conduct this type of analysis in a multilingual setting, as our experiments show a much more nuanced picture as well as notable differences from the English-only analysis. The main takeaways from our analysis are that mGPT-2 (partly) shows surprising anti-stereotypical behavior across languages, English (monolingual) models exhibit the strongest bias, and the stereotypes reflected in the data set are least present in Turkish mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26032;&#38395;&#32534;&#36753;&#23460;&#20013;&#20351;&#29992;&#30340;&#28151;&#21512;&#23457;&#26680;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21521;&#20869;&#23481;&#31649;&#29702;&#21592;&#25512;&#33616;&#29305;&#33394;&#24086;&#23376;&#26469;&#25903;&#25345;&#20182;&#20204;&#22312;&#36873;&#25321;&#29305;&#33394;&#20869;&#23481;&#26041;&#38754;&#20570;&#20986;&#20915;&#31574;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#27010;&#29575;&#25490;&#24207;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#32467;&#21512;&#20102;&#29992;&#25143;&#21644;&#25991;&#26412;&#20869;&#23481;&#29305;&#24449;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20998;&#31867;&#21644;&#25490;&#24207;&#24615;&#33021;&#12290;&#20869;&#23481;&#31649;&#29702;&#21592;&#22312;&#35780;&#20272;&#20013;&#21457;&#29616;&#20102;&#21512;&#36866;&#30340;&#35780;&#35770;&#65292;&#24182;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#25509;&#21463;&#20102;&#25512;&#33616;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.07317</link><description>&lt;p&gt;
&#26032;&#38395;&#32534;&#36753;&#23460;&#20013;&#30340;&#28151;&#21512;&#23457;&#26680;&#65306;&#21521;&#20869;&#23481;&#31649;&#29702;&#21592;&#25512;&#33616;&#29305;&#33394;&#24086;&#23376;
&lt;/p&gt;
&lt;p&gt;
Hybrid moderation in the newsroom: Recommending featured posts to content moderators. (arXiv:2307.07317v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26032;&#38395;&#32534;&#36753;&#23460;&#20013;&#20351;&#29992;&#30340;&#28151;&#21512;&#23457;&#26680;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21521;&#20869;&#23481;&#31649;&#29702;&#21592;&#25512;&#33616;&#29305;&#33394;&#24086;&#23376;&#26469;&#25903;&#25345;&#20182;&#20204;&#22312;&#36873;&#25321;&#29305;&#33394;&#20869;&#23481;&#26041;&#38754;&#20570;&#20986;&#20915;&#31574;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#27010;&#29575;&#25490;&#24207;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#32467;&#21512;&#20102;&#29992;&#25143;&#21644;&#25991;&#26412;&#20869;&#23481;&#29305;&#24449;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20998;&#31867;&#21644;&#25490;&#24207;&#24615;&#33021;&#12290;&#20869;&#23481;&#31649;&#29702;&#21592;&#22312;&#35780;&#20272;&#20013;&#21457;&#29616;&#20102;&#21512;&#36866;&#30340;&#35780;&#35770;&#65292;&#24182;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#25509;&#21463;&#20102;&#25512;&#33616;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#26032;&#38395;&#23186;&#20307;&#27491;&#21162;&#21147;&#22788;&#29702;&#35780;&#35770;&#21306;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#30340;&#23457;&#26680;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#25490;&#24207;&#30340;&#25512;&#33616;&#31995;&#32479;&#26469;&#25903;&#25345;&#21644;&#25480;&#26435;&#23457;&#26680;&#21592;&#36873;&#25321;&#29305;&#33394;&#24086;&#23376;&#65292;&#36825;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#32467;&#21512;&#29992;&#25143;&#21644;&#25991;&#26412;&#20869;&#23481;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#27979;&#35797;&#38598;&#19978;&#30340;&#26368;&#20339;&#20998;&#31867;F1&#20998;&#25968;&#20026;0.44&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#22823;&#37327;&#39564;&#35777;&#25991;&#31456;&#19978;&#35266;&#23519;&#21040;&#20102;&#22343;&#20540;NDCG@5&#30340;&#26368;&#20339;&#20540;&#20026;0.87&#12290;&#22312;&#19987;&#23478;&#35780;&#20272;&#20013;&#65292;&#20869;&#23481;&#31649;&#29702;&#21592;&#26681;&#25454;&#25512;&#33616;&#32467;&#26524;&#36873;&#25321;&#35201;&#25512;&#33616;&#30340;&#35780;&#35770;&#65292;&#24471;&#21040;&#20102;0.83&#30340;NDCG&#20998;&#25968;&#12290;&#25105;&#20204;&#24471;&#20986;&#30340;&#32467;&#35770;&#26159;&#65292;&#39318;&#20808;&#65292;&#28155;&#21152;&#25991;&#26412;&#29305;&#24449;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#24471;&#20998;&#65307;&#20854;&#27425;&#65292;&#34429;&#28982;&#36873;&#25321;&#29305;&#33394;&#20869;&#23481;&#20173;&#28982;&#26377;&#19968;&#23450;&#30340;&#20027;&#35266;&#24615;&#65292;&#20294;&#20869;&#23481;&#31649;&#29702;&#21592;&#22312;&#25152;&#26377;&#34987;&#35780;&#20272;&#30340;&#25512;&#33616;&#20013;&#37117;&#25214;&#21040;&#20102;&#21512;&#36866;&#30340;&#35780;&#35770;&#65292;&#38500;&#20102;&#19968;&#20010;&#20363;&#22806;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#65292;&#36808;&#21521;&#36879;&#26126;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online news outlets are grappling with the moderation of user-generated content within their comment section. We present a recommender system based on ranking class probabilities to support and empower the moderator in choosing featured posts, a time-consuming task. By combining user and textual content features we obtain an optimal classification F1-score of 0.44 on the test set. Furthermore, we observe an optimum mean NDCG@5 of 0.87 on a large set of validation articles. As an expert evaluation, content moderators assessed the output of a random selection of articles by choosing comments to feature based on the recommendations, which resulted in a NDCG score of 0.83. We conclude that first, adding text features yields the best score and second, while choosing featured content remains somewhat subjective, content moderators found suitable comments in all but one evaluated recommendations. We end the paper by analyzing our best-performing model, a step towards transparency and explaina
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#22270;&#25968;&#25454;&#30340;&#38646;-shot&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#37096;&#20998;&#25351;&#26631;&#19978;&#25509;&#36817;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#22312;&#20107;&#23454;&#12289;&#21453;&#20107;&#23454;&#21644;&#34394;&#26500;&#38472;&#36848;&#30340;&#23545;&#27604;&#20013;&#20063;&#26377;&#26174;&#33879;&#30340;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2307.07312</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#30693;&#35782;&#22270;&#29983;&#25104;&#38646;-shot&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Models for Zero-Shot Natural Language Generation from Knowledge Graphs. (arXiv:2307.07312v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#22270;&#25968;&#25454;&#30340;&#38646;-shot&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#37096;&#20998;&#25351;&#26631;&#19978;&#25509;&#36817;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#22312;&#20107;&#23454;&#12289;&#21453;&#20107;&#23454;&#21644;&#34394;&#26500;&#38472;&#36848;&#30340;&#23545;&#27604;&#20013;&#20063;&#26377;&#26174;&#33879;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20219;&#20309;&#20351;&#29992;&#32467;&#26500;&#21270;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#25968;&#25454;&#20316;&#20026;&#20854;&#24213;&#23618;&#30693;&#35782;&#34920;&#31034;&#30340;&#31995;&#32479;&#20013;&#65292;KG&#21040;&#25991;&#26412;&#29983;&#25104;&#26159;&#23558;&#22270;&#25968;&#25454;&#30340;&#37096;&#20998;&#36716;&#21270;&#20026;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#25991;&#26412;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#20351;&#29992;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#21363;&#20351;&#22312;&#29305;&#23450;&#22270;&#21040;&#25991;&#26412;&#20219;&#21153;&#30340;&#30456;&#23545;&#23567;&#30340;&#35757;&#32451;&#38598;&#19978;&#20063;&#33021;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#36825;&#20010;&#27010;&#24565;&#30340;&#22522;&#30784;&#19978;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#38646;-shot&#29983;&#25104;&#65292;&#20165;&#20165;&#26681;&#25454;&#27169;&#22411;&#23545;&#19977;&#20803;&#32452;&#32467;&#26500;&#30340;&#29702;&#35299;&#36827;&#34892;&#29983;&#25104;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ChatGPT&#22312;WebNLG 2020&#25361;&#25112;&#36187;&#30340;&#26576;&#20123;&#25351;&#26631;&#19978;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#20854;&#20182;&#25351;&#26631;&#19978;&#33853;&#21518;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20107;&#23454;&#12289;&#21453;&#20107;&#23454;&#21644;&#34394;&#26500;&#38472;&#36848;&#65292;&#24182;&#23637;&#31034;&#20102;LLM&#24050;&#32463;&#23545;&#20854;&#35299;&#26512;&#30340;&#25968;&#25454;&#26377;&#20851;&#30340;&#30693;&#35782;&#19982;&#36755;&#20986;&#25991;&#26412;&#36136;&#37327;&#20043;&#38388;&#30340;&#26174;&#33879;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
In any system that uses structured knowledge graph (KG) data as its underlying knowledge representation, KG-to-text generation is a useful tool for turning parts of the graph data into text that can be understood by humans. Recent work has shown that models that make use of pretraining on large amounts of text data can perform well on the KG-to-text task even with relatively small sets of training data on the specific graph-to-text task. In this paper, we build on this concept by using large language models to perform zero-shot generation based on nothing but the model's understanding of the triple structure from what it can read. We show that ChatGPT achieves near state-of-the-art performance on some measures of the WebNLG 2020 challenge, but falls behind on others. Additionally, we compare factual, counter-factual and fictional statements, and show that there is a significant connection between what the LLM already knows about the data it is parsing and the quality of the output text
&lt;/p&gt;</description></item><item><title>C3&#26159;&#22522;&#20110;ChatGPT&#30340;&#38646;-shot Text-to-SQL&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#25552;&#20379;&#31995;&#32479;&#24615;&#30340;&#22788;&#29702;&#26041;&#27861;&#65292;&#22312;Spider Challenge&#19978;&#21462;&#24471;&#20102;82.3%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#65292;&#25104;&#20026;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.07306</link><description>&lt;p&gt;
C3: &#20351;&#29992;ChatGPT&#36827;&#34892;&#38646;-shot Text-to-SQL
&lt;/p&gt;
&lt;p&gt;
C3: Zero-shot Text-to-SQL with ChatGPT. (arXiv:2307.07306v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07306
&lt;/p&gt;
&lt;p&gt;
C3&#26159;&#22522;&#20110;ChatGPT&#30340;&#38646;-shot Text-to-SQL&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#25552;&#20379;&#31995;&#32479;&#24615;&#30340;&#22788;&#29702;&#26041;&#27861;&#65292;&#22312;Spider Challenge&#19978;&#21462;&#24471;&#20102;82.3%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#65292;&#25104;&#20026;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ChatGPT&#30340;&#38646;-shot Text-to-SQL&#26041;&#27861;&#65292;&#21517;&#20026;C3&#65292;&#20854;&#22312;Spider&#30340;&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;82.3%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#65292;&#24182;&#25104;&#20026;Spider Challenge&#20013;&#26368;&#20808;&#36827;&#30340;&#38646;-shot Text-to-SQL&#26041;&#27861;&#12290;C3&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65306;Clear Prompting (CP)&#65292;Calibration with Hints (CH)&#21644;Consistent Output (CO)&#65292;&#20998;&#21035;&#23545;&#24212;&#20110;&#27169;&#22411;&#36755;&#20837;&#65292;&#27169;&#22411;&#20559;&#24046;&#21644;&#27169;&#22411;&#36755;&#20986;&#12290;&#23427;&#20026;&#38646;-shot Text-to-SQL&#25552;&#20379;&#20102;&#31995;&#32479;&#24615;&#30340;&#22788;&#29702;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a ChatGPT-based zero-shot Text-to-SQL method, dubbed C3, which achieves 82.3\% in terms of execution accuracy on the holdout test set of Spider and becomes the state-of-the-art zero-shot Text-to-SQL method on the Spider Challenge. C3 consists of three key components: Clear Prompting (CP), Calibration with Hints (CH), and Consistent Output (CO), which are corresponding to the model input, model bias and model output respectively. It provides a systematic treatment for zero-shot Text-to-SQL. Extensive experiments have been conducted to verify the effectiveness and efficiency of our proposed method.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#26041;&#35328;&#35782;&#21035;&#65292;&#24179;&#34913;&#30340;&#35821;&#26009;&#24211;&#26080;&#27861;&#22312;&#19981;&#21516;&#26041;&#35328;&#20043;&#38388;&#20135;&#29983;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;&#33945;&#26031;&#29305;&#26041;&#35328;&#20855;&#26377;&#26368;&#20302;&#30340;&#38169;&#35823;&#29575;&#65292;&#32780;&#38463;&#23572;&#26031;&#29305;&#26041;&#35328;&#19968;&#30452;&#34920;&#29616;&#36739;&#24046;&#12290;&#24247;&#32435;&#36203;&#29305;&#26041;&#35328;&#21644;&#33945;&#26031;&#29305;&#26041;&#35328;&#20043;&#38388;&#23384;&#22312;&#32039;&#23494;&#30340;&#20851;&#31995;&#65292;&#20294;&#19981;&#23545;&#31216;&#12290;&#36825;&#20123;&#32467;&#26524;&#23558;&#25351;&#23548;&#26410;&#26469;&#30340;&#35821;&#26009;&#24211;&#25910;&#38598;&#21644;&#31995;&#32479;&#26500;&#24314;&#31574;&#30053;&#65292;&#20197;&#20248;&#21270;&#36328;&#26041;&#35328;&#30340;&#24615;&#33021;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07295</link><description>&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#23454;&#29616;&#26041;&#35328;&#21253;&#23481;&#24615;&#35782;&#21035;&#65306;&#24179;&#34913;&#35821;&#26009;&#24211;&#26159;&#21542;&#26159;&#31572;&#26696;&#65311;
&lt;/p&gt;
&lt;p&gt;
Towards dialect-inclusive recognition in a low-resource language: are balanced corpora the answer?. (arXiv:2307.07295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07295
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#26041;&#35328;&#35782;&#21035;&#65292;&#24179;&#34913;&#30340;&#35821;&#26009;&#24211;&#26080;&#27861;&#22312;&#19981;&#21516;&#26041;&#35328;&#20043;&#38388;&#20135;&#29983;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;&#33945;&#26031;&#29305;&#26041;&#35328;&#20855;&#26377;&#26368;&#20302;&#30340;&#38169;&#35823;&#29575;&#65292;&#32780;&#38463;&#23572;&#26031;&#29305;&#26041;&#35328;&#19968;&#30452;&#34920;&#29616;&#36739;&#24046;&#12290;&#24247;&#32435;&#36203;&#29305;&#26041;&#35328;&#21644;&#33945;&#26031;&#29305;&#26041;&#35328;&#20043;&#38388;&#23384;&#22312;&#32039;&#23494;&#30340;&#20851;&#31995;&#65292;&#20294;&#19981;&#23545;&#31216;&#12290;&#36825;&#20123;&#32467;&#26524;&#23558;&#25351;&#23548;&#26410;&#26469;&#30340;&#35821;&#26009;&#24211;&#25910;&#38598;&#21644;&#31995;&#32479;&#26500;&#24314;&#31574;&#30053;&#65292;&#20197;&#20248;&#21270;&#36328;&#26041;&#35328;&#30340;&#24615;&#33021;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ASR&#31995;&#32479;&#36890;&#24120;&#26159;&#20026;&#21475;&#22836;&#30340;&#8220;&#26631;&#20934;&#8221;&#35821;&#35328;&#26500;&#24314;&#30340;&#65292;&#23545;&#20110;&#38750;&#26631;&#20934;&#30340;&#26041;&#35328;/&#21464;&#20307;&#30340;&#24615;&#33021;&#20250;&#19979;&#38477;&#12290;&#23545;&#20110;&#20687;&#29233;&#23572;&#20848;&#36825;&#26679;&#30340;&#35821;&#35328;&#26469;&#35828;&#65292;&#36825;&#26159;&#19968;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#19968;&#20010;&#21475;&#22836;&#30340;&#26631;&#20934;&#35821;&#65292;&#32780;&#26159;&#26377;&#19977;&#20010;&#20027;&#35201;&#30340;&#26041;&#35328;&#65306;&#38463;&#23572;&#26031;&#29305;&#26041;&#35328;&#65288;Ul&#65289;&#65292;&#24247;&#32435;&#36203;&#29305;&#26041;&#35328;&#65288;Co&#65289;&#21644;&#33945;&#26031;&#29305;&#26041;&#35328;&#65288;Mu&#65289;&#12290;&#20026;&#20102;&#37327;&#21270;&#35828;&#35805;&#32773;&#26041;&#35328;&#23545;&#35782;&#21035;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#35757;&#32451;&#20102;12&#20010;ASR&#31995;&#32479;&#65292;&#39318;&#20808;&#20351;&#29992;&#22522;&#32447;&#26041;&#35328;&#24179;&#34913;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#28982;&#21518;&#20351;&#29992;&#20462;&#25913;&#36807;&#30340;&#22522;&#32447;&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#26041;&#35328;&#29305;&#23450;&#30340;&#26448;&#26009;&#34987;&#20943;&#21435;&#25110;&#28155;&#21152;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26041;&#35328;&#24179;&#34913;&#30340;&#35821;&#26009;&#24211;&#22312;&#19981;&#21516;&#26041;&#35328;&#20043;&#38388;&#27809;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#65306;&#38463;&#23572;&#26031;&#29305;&#26041;&#35328;&#19968;&#30452;&#34920;&#29616;&#36739;&#24046;&#65292;&#32780;&#33945;&#26031;&#29305;&#26041;&#35328;&#24471;&#21040;&#20102;&#26368;&#20302;&#30340;&#38169;&#35823;&#29575;&#12290;&#24247;&#32435;&#36203;&#29305;&#26041;&#35328;&#21644;&#33945;&#26031;&#29305;&#26041;&#35328;&#20043;&#38388;&#23384;&#22312;&#30528;&#32039;&#23494;&#30340;&#20851;&#31995;&#65292;&#20294;&#19981;&#23545;&#31216;&#12290;&#36825;&#20123;&#32467;&#26524;&#23558;&#25351;&#23548;&#26410;&#26469;&#30340;&#35821;&#26009;&#24211;&#25910;&#38598;&#21644;&#31995;&#32479;&#26500;&#24314;&#31574;&#30053;&#65292;&#20197;&#20248;&#21270;&#36328;&#26041;&#35328;&#30340;&#24615;&#33021;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
ASR systems are generally built for the spoken 'standard', and their performance declines for non-standard dialects/varieties. This is a problem for a language like Irish, where there is no single spoken standard, but rather three major dialects: Ulster (Ul), Connacht (Co) and Munster (Mu). As a diagnostic to quantify the effect of the speaker's dialect on recognition performance, 12 ASR systems were trained, firstly using baseline dialect-balanced training corpora, and then using modified versions of the baseline corpora, where dialect-specific materials were either subtracted or added. Results indicate that dialect-balanced corpora do not yield a similar performance across the dialects: the Ul dialect consistently underperforms, whereas Mu yields lowest WERs. There is a close relationship between Co and Mu dialects, but one that is not symmetrical. These results will guide future corpus collection and system building strategies to optimise for cross-dialect performance equity.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#25345;&#32493;&#23618;&#29305;&#23450;&#24494;&#35843;&#21644;&#32463;&#39564;&#22238;&#25918;&#25216;&#26415;&#26469;&#25913;&#21892;&#24503;&#35821;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#22312;&#36739;&#23567;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07280</link><description>&lt;p&gt;
&#22238;&#25918;&#20197;&#22238;&#24518;&#65306;&#38024;&#23545;&#24503;&#35821;&#35821;&#38899;&#35782;&#21035;&#30340;&#25345;&#32493;&#23618;&#29305;&#23450;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Replay to Remember: Continual Layer-Specific Fine-tuning for German Speech Recognition. (arXiv:2307.07280v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07280
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#25345;&#32493;&#23618;&#29305;&#23450;&#24494;&#35843;&#21644;&#32463;&#39564;&#22238;&#25918;&#25216;&#26415;&#26469;&#25913;&#21892;&#24503;&#35821;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#22312;&#36739;&#23567;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#22312;&#24341;&#20837;&#26080;&#30417;&#30563;&#25110;&#33258;&#30417;&#30563;&#35757;&#32451;&#25216;&#26415;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#36825;&#20123;&#25913;&#36827;&#20173;&#28982;&#20165;&#38480;&#20110;&#26576;&#20123;&#35821;&#35328;&#21644;&#35828;&#35805;&#32773;&#12290;&#36801;&#31227;&#23398;&#20064;&#20351;&#24471;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#19981;&#20165;&#26159;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#36824;&#21253;&#25324;&#26356;&#29305;&#23450;&#30340;&#35828;&#35805;&#32773;&#32676;&#20307;&#12290;&#28982;&#32780;&#65292;&#23545;&#26032;&#39046;&#22495;&#30340;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#36890;&#24120;&#20250;&#23548;&#33268;&#22312;&#21407;&#22987;&#39046;&#22495;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;ASR&#27169;&#22411;&#22312;&#36739;&#23567;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#21487;&#20197;&#26377;&#22810;&#22909;&#65292;&#20351;&#29992;&#25105;&#20204;&#33258;&#24049;&#30340;&#24503;&#35821;&#39640;&#32423;&#35821;&#38899;&#21629;&#20196;&#25968;&#25454;&#38598;&#65288;SVC-de&#65289;&#65292;&#20197;&#21450;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#20923;&#32467;&#27169;&#22411;&#30340;&#37096;&#20998;&#26469;&#20445;&#30041;&#22810;&#23569;&#36890;&#29992;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#21152;ASR&#27169;&#22411;&#23545;&#24494;&#35843;&#39046;&#22495;&#20043;&#22806;&#30340;&#35789;&#27719;&#21644;&#35828;&#35805;&#32773;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#24212;&#29992;&#32463;&#39564;&#22238;&#25918;&#36827;&#34892;&#36830;&#32493;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Automatic Speech Recognition (ASR) models have shown significant advances with the introduction of unsupervised or self-supervised training techniques, these improvements are still only limited to a subsection of languages and speakers. Transfer learning enables the adaptation of large-scale multilingual models to not only low-resource languages but also to more specific speaker groups. However, fine-tuning on data from new domains is usually accompanied by a decrease in performance on the original domain. Therefore, in our experiments, we examine how well the performance of large-scale ASR models can be approximated for smaller domains, with our own dataset of German Senior Voice Commands (SVC-de), and how much of the general speech recognition performance can be preserved by selectively freezing parts of the model during training. To further increase the robustness of the ASR model to vocabulary and speakers outside of the fine-tuned domain, we apply Experience Replay for conti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#35270;&#35273;&#20449;&#24687;&#23545;&#21475;&#22836;&#35821;&#35328;&#29702;&#35299;&#30340;&#24433;&#21709;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#21644;&#21333;&#27169;&#24577;&#29615;&#22659;&#19979;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#27169;&#24577;&#21644;&#21333;&#27169;&#24577;&#29615;&#22659;&#19979;&#30340;&#35748;&#30693;&#21162;&#21147;&#23384;&#22312;&#24046;&#24322;&#65292;&#23616;&#37096;&#35789;&#27719;&#19978;&#19979;&#25991;&#23545;&#22810;&#27169;&#24577;&#29615;&#22659;&#19979;&#30340;&#35748;&#30693;&#22788;&#29702;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.07277</link><description>&lt;p&gt;
&#35821;&#38899;&#21644;&#35270;&#21548;&#29702;&#35299;&#20013;&#30340;&#21333;&#35789;&#26159;&#21542;&#21516;&#26679;&#20196;&#20154;&#24778;&#35766;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are words equally surprising in audio and audio-visual comprehension?. (arXiv:2307.07277v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#35270;&#35273;&#20449;&#24687;&#23545;&#21475;&#22836;&#35821;&#35328;&#29702;&#35299;&#30340;&#24433;&#21709;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#21644;&#21333;&#27169;&#24577;&#29615;&#22659;&#19979;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#27169;&#24577;&#21644;&#21333;&#27169;&#24577;&#29615;&#22659;&#19979;&#30340;&#35748;&#30693;&#21162;&#21147;&#23384;&#22312;&#24046;&#24322;&#65292;&#23616;&#37096;&#35789;&#27719;&#19978;&#19979;&#25991;&#23545;&#22810;&#27169;&#24577;&#29615;&#22659;&#19979;&#30340;&#35748;&#30693;&#22788;&#29702;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#39033;&#21463;&#25511;&#30740;&#31350;&#65292;&#35843;&#26597;&#20102;&#35270;&#35273;&#20449;&#24687;&#65288;&#21363;&#30475;&#21040;&#35828;&#35805;&#32773;&#65289;&#23545;&#21475;&#22836;&#35821;&#35328;&#29702;&#35299;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#30456;&#21516;&#21475;&#22836;&#21050;&#28608;&#30340;&#20165;&#38899;&#39057;&#21644;&#38899;&#39057;-&#35270;&#35273;&#21576;&#29616;&#20013;&#27599;&#20010;&#21333;&#35789;&#30456;&#20851;&#30340;ERP&#26631;&#35760;&#65288;N400&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#20855;&#20307;&#26469;&#35828;&#26159;n-gram&#21644;Transformer&#27169;&#22411;&#65289;&#22522;&#20110;&#35789;&#27719;&#19978;&#19979;&#25991;&#39044;&#27979;N400&#21709;&#24212;&#30340;&#21487;&#39044;&#27979;&#24615;&#24230;&#37327;&#65288;surprisal&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35748;&#30693;&#21162;&#21147;&#22312;&#22810;&#27169;&#24577;&#21644;&#21333;&#27169;&#24577;&#29615;&#22659;&#20013;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20165;&#38899;&#39057;&#29615;&#22659;&#20013;&#65292;&#20855;&#26377;&#36739;&#22823;&#35789;&#27719;&#19978;&#19979;&#25991;&#30340;Transformer&#27169;&#22411;&#25552;&#20379;&#26356;&#22909;&#30340;&#25311;&#21512;&#65292;&#32780;2-gram&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#29615;&#22659;&#20013;&#26356;&#26377;&#25928;&#12290;&#36825;&#20984;&#26174;&#20102;&#23616;&#37096;&#35789;&#27719;&#19978;&#19979;&#25991;&#23545;&#22810;&#27169;&#24577;&#29615;&#22659;&#19979;&#35748;&#30693;&#22788;&#29702;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report a controlled study investigating the effect of visual information (i.e., seeing the speaker) on spoken language comprehension. We compare the ERP signature (N400) associated with each word in audio-only and audio-visual presentations of the same verbal stimuli. We assess the extent to which surprisal measures (which quantify the predictability of words in their lexical context) are generated on the basis of different types of language models (specifically n-gram and Transformer models) that predict N400 responses for each word. Our results indicate that cognitive effort differs significantly between multimodal and unimodal settings. In addition, our findings suggest that while Transformer-based models, which have access to a larger lexical context, provide a better fit in the audio-only setting, 2-gram language models are more effective in the multimodal setting. This highlights the significant impact of local lexical context on cognitive processing in a multimodal environmen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#23398;&#21160;&#26426;&#30340;&#20998;&#35789;&#26041;&#26696;MorphPiece&#65292;&#24182;&#20351;&#29992;&#35813;&#26041;&#26696;&#35757;&#32451;&#20102;&#19968;&#20010;&#31216;&#20026;MorphGPT&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;MorphGPT&#22312;&#35821;&#35328;&#24314;&#27169;&#20197;&#21450;&#21508;&#31181;NLP&#20219;&#21153;&#19978;&#37117;&#34920;&#29616;&#20986;&#20102;&#27604;&#20256;&#32479;&#27169;&#22411;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.07262</link><description>&lt;p&gt;
MorphPiece: &#36828;&#31163;&#32479;&#35745;&#35821;&#35328;&#34920;&#31034;&#30340;&#19968;&#27493;
&lt;/p&gt;
&lt;p&gt;
MorphPiece : Moving away from Statistical Language Representation. (arXiv:2307.07262v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#23398;&#21160;&#26426;&#30340;&#20998;&#35789;&#26041;&#26696;MorphPiece&#65292;&#24182;&#20351;&#29992;&#35813;&#26041;&#26696;&#35757;&#32451;&#20102;&#19968;&#20010;&#31216;&#20026;MorphGPT&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;MorphGPT&#22312;&#35821;&#35328;&#24314;&#27169;&#20197;&#21450;&#21508;&#31181;NLP&#20219;&#21153;&#19978;&#37117;&#34920;&#29616;&#20986;&#20102;&#27604;&#20256;&#32479;&#27169;&#22411;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#35789;&#26159;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27969;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#19968;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24403;&#20195;&#20998;&#35789;&#22120;&#22522;&#20110;&#23545;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#23545;&#35821;&#35328;&#29305;&#24449;&#30340;&#32771;&#34385;&#36739;&#23569;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#23398;&#21160;&#26426;&#30340;&#20998;&#35789;&#26041;&#26696;MorphPiece&#65292;&#37096;&#20998;&#22522;&#20110;&#24213;&#23618;&#25991;&#26412;&#30340;&#24418;&#24577;&#20998;&#21106;&#12290;&#20351;&#29992;&#35813;&#20998;&#35789;&#22120;&#65288;&#31216;&#20026;MorphGPT&#65289;&#35757;&#32451;&#30340;&#31867;GPT&#30340;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#26174;&#31034;&#20986;&#27604;&#22312;&#26631;&#20934;BPE&#20998;&#35789;&#22120;&#19978;&#35757;&#32451;&#26102;&#26356;&#20248;&#36234;&#30340;&#25910;&#25947;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19982;&#35268;&#27169;&#22823;6&#20493;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#35821;&#35328;&#24314;&#27169;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#26465;&#20214;&#19979;&#23545;MorphGPT&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#22312;&#21508;&#20010;&#26041;&#38754;&#19982;GPT-2&#27169;&#22411;&#30456;&#27604;&#26377;&#26356;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tokenization is a critical part of modern NLP pipelines. However, contemporary tokenizers for Large Language Models are based on statistical analysis of text corpora, without much consideration to the linguistic features. We propose a linguistically motivated tokenization scheme, MorphPiece, which is based partly on morphological segmentation of the underlying text. A GPT-style causal language model trained on this tokenizer (called MorphGPT) shows superior convergence compared to the same architecture trained on a standard BPE tokenizer. Specifically we get Language Modeling performance comparable to a 6 times larger model. Additionally, we evaluate MorphGPT on a variety of NLP tasks in supervised and unsupervised settings and find superior performance across the board, compared to GPT-2 model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HybridBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#27880;&#24847;&#21147;&#21644;&#27744;&#21270;&#32593;&#32476;&#26469;&#32534;&#30721;&#27599;&#19968;&#23618;&#20013;&#30340;&#19981;&#21516;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#24182;&#19988;&#24341;&#20837;DropMask&#26041;&#27861;&#26469;&#35299;&#20915;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;HybridBERT&#22312;&#39044;&#35757;&#32451;&#21644;&#36801;&#31227;&#23398;&#20064;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;BERT&#65292;&#24182;&#19988;DropMask&#33021;&#22815;&#25552;&#39640;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.07258</link><description>&lt;p&gt;
&#29992;&#28151;&#21512;&#27744;&#21270;&#32593;&#32476;&#21644;Drop Mask&#25552;&#21319;BERT
&lt;/p&gt;
&lt;p&gt;
Improving BERT with Hybrid Pooling Network and Drop Mask. (arXiv:2307.07258v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HybridBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#27880;&#24847;&#21147;&#21644;&#27744;&#21270;&#32593;&#32476;&#26469;&#32534;&#30721;&#27599;&#19968;&#23618;&#20013;&#30340;&#19981;&#21516;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#24182;&#19988;&#24341;&#20837;DropMask&#26041;&#27861;&#26469;&#35299;&#20915;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;HybridBERT&#22312;&#39044;&#35757;&#32451;&#21644;&#36801;&#31227;&#23398;&#20064;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;BERT&#65292;&#24182;&#19988;DropMask&#33021;&#22815;&#25552;&#39640;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BERT&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;BERT&#22312;&#19981;&#21516;&#23618;&#27425;&#25429;&#25417;&#21040;&#20016;&#23500;&#30340;&#35821;&#35328;&#20449;&#24687;&#23618;&#27425;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#26222;&#36890;&#30340;BERT&#22312;&#27599;&#19968;&#23618;&#20351;&#29992;&#30456;&#21516;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#24314;&#27169;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;HybridBERT&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#33258;&#27880;&#24847;&#21147;&#21644;&#27744;&#21270;&#32593;&#32476;&#26469;&#32534;&#30721;&#27599;&#19968;&#23618;&#20013;&#30340;&#19981;&#21516;&#19978;&#19979;&#25991;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;DropMask&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#22312;&#25513;&#34109;&#35821;&#35328;&#24314;&#27169;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#36807;&#24230;&#20351;&#29992;&#29305;&#27530;&#25513;&#34109;&#26631;&#35760;&#24341;&#36215;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;HybridBERT&#22312;&#39044;&#35757;&#32451;&#20013;&#34920;&#29616;&#20248;&#20110;BERT&#65292;&#20855;&#26377;&#26356;&#20302;&#30340;&#25439;&#22833;&#12289;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#65288;&#30456;&#23545;&#38477;&#20302;8%&#65289;&#12289;&#26356;&#20302;&#30340;&#20869;&#23384;&#25104;&#26412;&#65288;&#30456;&#23545;&#38477;&#20302;13%&#65289;&#65292;&#24182;&#19988;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#30456;&#23545;&#26356;&#39640;&#30340;&#31934;&#24230;&#65288;&#30456;&#23545;&#22686;&#21152;1.5%&#65289;&#12290;&#27492;&#22806;&#65292;DropMask&#33021;&#22815;&#25552;&#39640;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based pre-trained language models, such as BERT, achieve great success in various natural language understanding tasks. Prior research found that BERT captures a rich hierarchy of linguistic information at different layers. However, the vanilla BERT uses the same self-attention mechanism for each layer to model the different contextual features. In this paper, we propose a HybridBERT model which combines self-attention and pooling networks to encode different contextual features in each layer. Additionally, we propose a simple DropMask method to address the mismatch between pre-training and fine-tuning caused by excessive use of special mask tokens during Masked Language Modeling pre-training. Experiments show that HybridBERT outperforms BERT in pre-training with lower loss, faster training speed (8% relative), lower memory cost (13% relative), and also in transfer learning with 1.5% relative higher accuracies on downstream tasks. Additionally, DropMask improves accuracies 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#35805;&#20195;&#29702;&#35774;&#35745;&#30340;&#30456;&#20851;&#35201;&#32032;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#21253;&#25324;&#23545;&#35805;&#20195;&#29702;&#30340;&#20027;&#35201;&#29305;&#24449;&#12289;&#25903;&#25345;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#26500;&#24314;&#21333;&#29420;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;&#23545;&#35805;&#20219;&#21153;&#26159;&#26114;&#36149;&#19988;&#20887;&#20313;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.07255</link><description>&lt;p&gt;
&#23545;&#35805;&#20195;&#29702;101&#65306;&#35774;&#35745;&#26377;&#25928;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#20851;&#38190;&#35201;&#32032;&#21021;&#23398;&#32773;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Dialogue Agents 101: A Beginner's Guide to Critical Ingredients for Designing Effective Conversational Systems. (arXiv:2307.07255v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#35805;&#20195;&#29702;&#35774;&#35745;&#30340;&#30456;&#20851;&#35201;&#32032;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#21253;&#25324;&#23545;&#35805;&#20195;&#29702;&#30340;&#20027;&#35201;&#29305;&#24449;&#12289;&#25903;&#25345;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#26500;&#24314;&#21333;&#29420;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;&#23545;&#35805;&#20219;&#21153;&#26159;&#26114;&#36149;&#19988;&#20887;&#20313;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19982;&#21516;&#34892;&#36827;&#34892;&#20132;&#27969;&#26469;&#20998;&#20139;&#24819;&#27861;&#26159;&#20154;&#31867;&#20114;&#21160;&#30340;&#20027;&#35201;&#26041;&#24335;&#12290;&#22240;&#27492;&#65292;&#22312;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#23548;&#33268;&#23545;&#35805;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#30340;&#21487;&#29992;&#24615;&#21644;&#22810;&#26679;&#24615;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22810;&#20010;&#20219;&#21153;&#21516;&#26102;&#25506;&#32034;&#65292;&#24403;&#21069;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#30340;&#29616;&#29366;&#21464;&#24471;&#20998;&#25955;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#24110;&#21161;&#20174;&#38646;&#24320;&#22987;&#35774;&#35745;&#23545;&#35805;&#20195;&#29702;&#30340;&#20174;&#19994;&#32773;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#23545;&#35805;&#20195;&#29702;&#30340;&#20027;&#35201;&#29305;&#24449;&#12289;&#25903;&#25345;&#20219;&#21153;&#12289;&#30456;&#24212;&#30340;&#24320;&#25918;&#39046;&#22495;&#25968;&#25454;&#38598;&#20197;&#21450;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#26041;&#27861;&#24050;&#34987;&#29992;&#20110;&#35299;&#20915;&#19981;&#21516;&#30340;&#23545;&#35805;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20026;&#27599;&#20010;&#20219;&#21153;&#26500;&#24314;&#21333;&#29420;&#30340;&#27169;&#22411;&#26159;&#26114;&#36149;&#19988;&#20887;&#20313;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharing ideas through communication with peers is the primary mode of human interaction. Consequently, extensive research has been conducted in the area of conversational AI, leading to an increase in the availability and diversity of conversational tasks, datasets, and methods. However, with numerous tasks being explored simultaneously, the current landscape of conversational AI becomes fragmented. Therefore, initiating a well-thought-out model for a dialogue agent can pose significant challenges for a practitioner. Towards highlighting the critical ingredients needed for a practitioner to design a dialogue agent from scratch, the current study provides a comprehensive overview of the primary characteristics of a dialogue agent, the supporting tasks, their corresponding open-domain datasets, and the methods used to benchmark these datasets. We observe that different methods have been used to tackle distinct dialogue tasks. However, building separate models for each task is costly and 
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30452;&#25509;&#24212;&#29992;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20026;&#20102;&#35299;&#20915;&#35748;&#35777;&#21322;&#24452;&#24456;&#23567;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#33258;&#25105;&#20928;&#21270;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.07171</link><description>&lt;p&gt;
&#24102;&#26377;&#33258;&#25105;&#20928;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Certified Robustness for Large Language Models with Self-Denoising. (arXiv:2307.07171v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07171
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30452;&#25509;&#24212;&#29992;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20026;&#20102;&#35299;&#20915;&#35748;&#35777;&#21322;&#24452;&#24456;&#23567;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#33258;&#25105;&#20928;&#21270;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#22122;&#22768;&#36755;&#20837;&#30340;&#33030;&#24369;&#24615;&#26174;&#33879;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#12290;&#22312;&#36825;&#20123;&#29615;&#22659;&#19979;&#65292;&#30830;&#20445;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27599;&#20010;&#39044;&#27979;&#37117;&#26159;&#31283;&#23450;&#30340;&#38750;&#24120;&#37325;&#35201;&#65292;&#21363;&#22312;&#36755;&#20837;&#30340;&#24494;&#23567;&#24046;&#24322;&#24773;&#20917;&#19979;&#65292;LLM&#30340;&#39044;&#27979;&#24212;&#35813;&#26159;&#19968;&#33268;&#30340;&#12290;&#36825;&#20027;&#35201;&#28041;&#21450;&#21040;&#35748;&#35777;&#40065;&#26834;LLM&#30340;&#30740;&#31350;&#65292;&#21363;&#22312;&#36755;&#20837;&#21608;&#22260;&#30340;&#23616;&#37096;&#21306;&#22495;&#20013;&#65292;&#25152;&#26377;LLM&#30340;&#39044;&#27979;&#37117;&#24471;&#21040;&#35748;&#35777;&#26159;&#27491;&#30830;&#30340;&#12290;&#38543;&#26426;&#24179;&#28369;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#35748;&#35777;LLM&#30340;&#40065;&#26834;&#24615;&#21644;&#39044;&#27979;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#38543;&#26426;&#24179;&#28369;&#22312;&#36827;&#34892;&#27169;&#22411;&#39044;&#27979;&#20043;&#21069;&#38656;&#35201;&#23545;&#36755;&#20837;&#28155;&#21152;&#22122;&#22768;&#65292;&#20854;&#35748;&#35777;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#27169;&#22411;&#22312;&#21463;&#25439;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#12290;&#22240;&#27492;&#65292;&#23427;&#30452;&#25509;&#24212;&#29992;&#20110;LLM&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19988;&#36890;&#24120;&#20250;&#23548;&#33268;&#24456;&#23567;&#30340;&#35748;&#35777;&#21322;&#24452;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;
&lt;/p&gt;
&lt;p&gt;
Although large language models (LLMs) have achieved great success in vast real-world applications, their vulnerabilities towards noisy inputs have significantly limited their uses, especially in high-stake environments. In these contexts, it is crucial to ensure that every prediction made by large language models is stable, i.e., LLM predictions should be consistent given minor differences in the input. This largely falls into the study of certified robust LLMs, i.e., all predictions of LLM are certified to be correct in a local region around the input. Randomized smoothing has demonstrated great potential in certifying the robustness and prediction stability of LLMs. However, randomized smoothing requires adding noise to the input before model prediction, and its certification performance depends largely on the model's performance on corrupted data. As a result, its direct application to LLMs remains challenging and often results in a small certification radius. To address this issue,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20999;&#25442;&#22836;&#23614;&#28431;&#26007;UNITER&#65292;&#29992;&#20110;&#35299;&#20915;&#21452;&#25351;&#31216;&#34920;&#36798;&#29702;&#35299;&#19982;&#21462;&#29289;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#20998;&#21035;&#39044;&#27979;&#30446;&#26631;&#29289;&#21697;&#21644;&#30446;&#30340;&#22320;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#36827;&#34892;&#25152;&#26377;&#32452;&#21512;&#25512;&#29702;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;&#39564;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19968;&#20010;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.07166</link><description>&lt;p&gt;
&#21452;&#25351;&#31216;&#34920;&#36798;&#29702;&#35299;&#19982;&#21462;&#29289;&#20219;&#21153;&#20013;&#30340;&#20999;&#25442;&#22836;&#23614;&#28431;&#26007;UNITER
&lt;/p&gt;
&lt;p&gt;
Switching Head-Tail Funnel UNITER for Dual Referring Expression Comprehension with Fetch-and-Carry Tasks. (arXiv:2307.07166v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20999;&#25442;&#22836;&#23614;&#28431;&#26007;UNITER&#65292;&#29992;&#20110;&#35299;&#20915;&#21452;&#25351;&#31216;&#34920;&#36798;&#29702;&#35299;&#19982;&#21462;&#29289;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#20998;&#21035;&#39044;&#27979;&#30446;&#26631;&#29289;&#21697;&#21644;&#30446;&#30340;&#22320;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#36827;&#34892;&#25152;&#26377;&#32452;&#21512;&#25512;&#29702;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;&#39564;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19968;&#20010;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#23478;&#24237;&#26381;&#21153;&#26426;&#22120;&#20154;&#65288;DSR&#65289;&#65292;&#26681;&#25454;&#33258;&#30001;&#24418;&#24335;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25441;&#36215;&#26085;&#24120;&#29289;&#21697;&#24182;&#23558;&#20854;&#36816;&#36865;&#21040;&#25351;&#23450;&#30340;&#20301;&#32622;&#12290;&#32473;&#23450;&#22914;&#8220;&#23558;&#30424;&#23376;&#24038;&#20391;&#30340;&#29942;&#23376;&#31227;&#21040;&#31354;&#26885;&#23376;&#19978;&#8221;&#20043;&#31867;&#30340;&#25351;&#20196;&#65292;DSR&#38656;&#35201;&#20174;&#29615;&#22659;&#20013;&#30340;&#22810;&#20010;&#20505;&#36873;&#39033;&#20013;&#35782;&#21035;&#20986;&#29942;&#23376;&#21644;&#26885;&#23376;&#65292;&#24182;&#23558;&#30446;&#26631;&#29289;&#21697;&#25644;&#36816;&#21040;&#30446;&#30340;&#22320;&#12290;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#29702;&#35299;&#26041;&#27861;&#22823;&#22810;&#22312;&#35745;&#31639;&#22797;&#26434;&#24615;&#19978;&#37117;&#19981;&#23454;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#23545;&#30446;&#26631;&#29289;&#21697;&#20505;&#36873;&#39033;&#21644;&#30446;&#30340;&#22320;&#20505;&#36873;&#39033;&#30340;&#25152;&#26377;&#32452;&#21512;&#36827;&#34892;&#25512;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20999;&#25442;&#22836;&#23614;&#28431;&#26007;UNITER&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#20998;&#21035;&#39044;&#27979;&#30446;&#26631;&#29289;&#21697;&#21644;&#30446;&#30340;&#22320;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#20010;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22312;&#26631;&#20934;&#30340;&#20307;&#39564;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#25311;&#22120;&#20013;&#25429;&#25417;&#30340;&#29289;&#20307;&#25805;&#20316;&#25351;&#20196;&#21644;&#21322;&#30495;&#23454;&#24863;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes a domestic service robot (DSR) that fetches everyday objects and carries them to specified destinations according to free-form natural language instructions. Given an instruction such as "Move the bottle on the left side of the plate to the empty chair," the DSR is expected to identify the bottle and the chair from multiple candidates in the environment and carry the target object to the destination. Most of the existing multimodal language understanding methods are impractical in terms of computational complexity because they require inferences for all combinations of target object candidates and destination candidates. We propose Switching Head-Tail Funnel UNITER, which solves the task by predicting the target object and the destination individually using a single model. Our method is validated on a newly-built dataset consisting of object manipulation instructions and semi photo-realistic images captured in a standard Embodied AI simulator. The results show that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#35757;&#32451;&#23494;&#38598;&#26816;&#32034;&#22120;&#26469;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#39640;&#36136;&#37327;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#26399;&#38388;&#23545;&#26410;&#35265;&#36807;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.07164</link><description>&lt;p&gt;
&#23398;&#20064;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26816;&#32034;&#19978;&#19979;&#25991;&#31034;&#20363;
&lt;/p&gt;
&lt;p&gt;
Learning to Retrieve In-Context Examples for Large Language Models. (arXiv:2307.07164v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#35757;&#32451;&#23494;&#38598;&#26816;&#32034;&#22120;&#26469;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#39640;&#36136;&#37327;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#26399;&#38388;&#23545;&#26410;&#35265;&#36807;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#26681;&#25454;&#23569;&#37327;&#30340;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#25152;&#36873;&#31034;&#20363;&#30340;&#36136;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#35757;&#32451;&#23494;&#38598;&#26816;&#32034;&#22120;&#65292;&#21487;&#20197;&#20026;LLMs&#35782;&#21035;&#39640;&#36136;&#37327;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#39318;&#20808;&#35757;&#32451;&#22522;&#20110;LLM&#21453;&#39304;&#30340;&#22870;&#21169;&#27169;&#22411;&#26469;&#35780;&#20272;&#20505;&#36873;&#31034;&#20363;&#30340;&#36136;&#37327;&#65292;&#28982;&#21518;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#35757;&#32451;&#22522;&#20110;&#21452;&#32534;&#30721;&#22120;&#30340;&#23494;&#38598;&#26816;&#32034;&#22120;&#12290;&#25105;&#20204;&#22312;30&#20010;&#20219;&#21153;&#22871;&#20214;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#33879;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#35757;&#32451;&#26399;&#38388;&#23545;&#26410;&#35265;&#36807;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28145;&#20837;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#26816;&#32034;&#20855;&#26377;&#30456;&#20284;&#27169;&#24335;&#30340;&#31034;&#20363;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#32780;&#36825;&#31181;&#22686;&#30410;&#22312;&#19981;&#21516;&#35268;&#27169;&#30340;LLMs&#20013;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated their ability to learn in-context, allowing them to perform various tasks based on a few input-output examples. However, the effectiveness of in-context learning is heavily reliant on the quality of the selected examples. In this paper, we propose a novel framework to iteratively train dense retrievers that can identify high-quality in-context examples for LLMs. Our framework initially trains a reward model based on LLM feedback to evaluate the quality of candidate examples, followed by knowledge distillation to train a bi-encoder based dense retriever. Our experiments on a suite of 30 tasks demonstrate that our framework significantly enhances in-context learning performance. Furthermore, we show the generalization ability of our framework to unseen tasks during training. An in-depth analysis reveals that our model improves performance by retrieving examples with similar patterns, and the gains are consistent across LLMs of varying sizes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#39550;&#39542;&#22914;&#20154;&#31867;&#19968;&#33324;&#65292;&#24182;&#36890;&#36807;&#25512;&#29702;&#12289;&#35299;&#37322;&#21644;&#35760;&#24518;&#33021;&#21147;&#35299;&#20915;&#38271;&#23614;&#24773;&#20917;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#26500;&#24314;&#38381;&#29615;&#31995;&#32479;&#36827;&#34892;&#31034;&#33539;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;LLM&#22312;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.07162</link><description>&lt;p&gt;
&#22914;&#20154;&#31867;&#19968;&#26679;&#39550;&#39542;&#65306;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#24605;&#32771;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Drive Like a Human: Rethinking Autonomous Driving with Large Language Models. (arXiv:2307.07162v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#39550;&#39542;&#22914;&#20154;&#31867;&#19968;&#33324;&#65292;&#24182;&#36890;&#36807;&#25512;&#29702;&#12289;&#35299;&#37322;&#21644;&#35760;&#24518;&#33021;&#21147;&#35299;&#20915;&#38271;&#23614;&#24773;&#20917;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#26500;&#24314;&#38381;&#29615;&#31995;&#32479;&#36827;&#34892;&#31034;&#33539;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;LLM&#22312;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20154;&#31867;&#26041;&#24335;&#29702;&#35299;&#39550;&#39542;&#29615;&#22659;&#21450;&#20854;&#22312;&#38754;&#23545;&#22797;&#26434;&#24773;&#26223;&#26102;&#30340;&#25512;&#29702;&#12289;&#35299;&#37322;&#21644;&#35760;&#24518;&#33021;&#21147;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#20248;&#21270;&#21644;&#27169;&#22359;&#21270;&#30340;&#33258;&#21160;&#39550;&#39542;&#65288;AD&#65289;&#31995;&#32479;&#22312;&#22788;&#29702;&#38271;&#23614;&#24773;&#20917;&#26102;&#38754;&#20020;&#30528;&#22266;&#26377;&#30340;&#24615;&#33021;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#29702;&#24819;&#30340;AD&#31995;&#32479;&#24212;&#35813;&#20687;&#20154;&#19968;&#26679;&#39550;&#39542;&#65292;&#36890;&#36807;&#25345;&#32493;&#39550;&#39542;&#31215;&#32047;&#32463;&#39564;&#65292;&#24182;&#20351;&#29992;&#24120;&#35782;&#35299;&#20915;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;AD&#31995;&#32479;&#30340;&#19977;&#20010;&#20851;&#38190;&#33021;&#21147;&#65306;&#25512;&#29702;&#12289;&#35299;&#37322;&#21644;&#35760;&#24518;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#38381;&#29615;&#31995;&#32479;&#23637;&#31034;LLM&#22312;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#29702;&#35299;&#21644;&#29615;&#22659;&#20132;&#20114;&#33021;&#21147;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#37319;&#29992;LLM&#22312;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;LLM&#23637;&#29616;&#20986;&#25512;&#29702;&#24182;&#35299;&#20915;&#38271;&#23614;&#24773;&#20917;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the potential of using a large language model (LLM) to understand the driving environment in a human-like manner and analyze its ability to reason, interpret, and memorize when facing complex scenarios. We argue that traditional optimization-based and modular autonomous driving (AD) systems face inherent performance limitations when dealing with long-tail corner cases. To address this problem, we propose that an ideal AD system should drive like a human, accumulating experience through continuous driving and using common sense to solve problems. To achieve this goal, we identify three key abilities necessary for an AD system: reasoning, interpretation, and memorization. We demonstrate the feasibility of employing an LLM in driving scenarios by building a closed-loop system to showcase its comprehension and environment-interaction abilities. Our extensive experiments show that the LLM exhibits the impressive ability to reason and solve long-tailed cases, provid
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36974;&#30422;&#39046;&#22495;&#20869;&#20851;&#38190;&#35789;&#36827;&#34892;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20351;&#29992;&#38543;&#26426;&#36974;&#30422;&#30340;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#21644;&#24120;&#35265;&#30340;&#39044;&#35757;&#32451;&#28982;&#21518;&#24494;&#35843;&#33539;&#24335;&#12290;</title><link>http://arxiv.org/abs/2307.07160</link><description>&lt;p&gt;
&#19981;&#35201;&#38543;&#26426;&#36974;&#30422;&#65306;&#36890;&#36807;&#36974;&#30422;&#39046;&#22495;&#20869;&#20851;&#38190;&#35789;&#36827;&#34892;&#26377;&#25928;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Do not Mask Randomly: Effective Domain-adaptive Pre-training by Masking In-domain Keywords. (arXiv:2307.07160v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36974;&#30422;&#39046;&#22495;&#20869;&#20851;&#38190;&#35789;&#36827;&#34892;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20351;&#29992;&#38543;&#26426;&#36974;&#30422;&#30340;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#21644;&#24120;&#35265;&#30340;&#39044;&#35757;&#32451;&#28982;&#21518;&#24494;&#35843;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#26080;&#20851;&#30340;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20171;&#20110;&#36890;&#29992;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36873;&#25321;&#24615;&#22320;&#36974;&#30422;&#39046;&#22495;&#20869;&#30340;&#20851;&#38190;&#35789;&#65292;&#21363;&#25552;&#20379;&#30446;&#26631;&#39046;&#22495;&#30340;&#32039;&#20945;&#34920;&#31034;&#30340;&#21333;&#35789;&#12290;&#25105;&#20204;&#20351;&#29992;KeyBERT (Grootendorst, 2020)&#26469;&#35782;&#21035;&#36825;&#20123;&#20851;&#38190;&#35789;&#12290;&#25105;&#20204;&#20351;&#29992;&#20845;&#31181;&#19981;&#21516;&#30340;&#35774;&#32622;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#65306;&#19977;&#20010;&#25968;&#25454;&#38598;&#19982;&#20004;&#20010;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#31574;&#30053;&#24494;&#35843;&#30340;PLMs&#20248;&#20110;&#20351;&#29992;&#38543;&#26426;&#36974;&#30422;&#30340;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#30340;PLMs&#65292;&#24182;&#19988;&#20248;&#20110;&#36981;&#24490;&#24120;&#35265;&#30340;&#39044;&#35757;&#32451;&#28982;&#21518;&#24494;&#35843;&#33539;&#24335;&#30340;PLMs&#12290;&#27492;&#22806;&#65292;&#35782;&#21035;&#39046;&#22495;&#20869;&#20851;&#38190;&#35789;&#30340;&#24320;&#38144;&#26159;&#21512;&#29702;&#30340;&#65292;&#20363;&#22914;&#65292;&#23545;&#20110;BERT Large (Devlin et al., 2019)&#26469;&#35828;&#65292;&#26159;&#39044;&#35757;&#32451;&#26102;&#38388;&#30340;7-15%&#65288;&#20004;&#20010;epoch&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel task-agnostic in-domain pre-training method that sits between generic pre-training and fine-tuning. Our approach selectively masks in-domain keywords, i.e., words that provide a compact representation of the target domain. We identify such keywords using KeyBERT (Grootendorst, 2020). We evaluate our approach using six different settings: three datasets combined with two distinct pre-trained language models (PLMs). Our results reveal that the fine-tuned PLMs adapted using our in-domain pre-training strategy outperform PLMs that used in-domain pre-training with random masking as well as those that followed the common pre-train-then-fine-tune paradigm. Further, the overhead of identifying in-domain keywords is reasonable, e.g., 7-15% of the pre-training time (for two epochs) for BERT Large (Devlin et al., 2019).
&lt;/p&gt;</description></item><item><title>MMSD2.0&#26159;&#19968;&#20010;&#20462;&#27491;&#20102;MMSD&#32570;&#38519;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#22810;&#35270;&#22270;CLIP&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#22810;&#20010;&#35282;&#24230;&#21033;&#29992;&#22810;&#31890;&#24230;&#32447;&#32034;&#36827;&#34892;&#22810;&#27169;&#24335;&#35773;&#21050;&#26816;&#27979;&#65292;&#26174;&#33879;&#20248;&#20110;&#20197;&#21069;&#26368;&#22909;&#30340;&#22522;&#20934;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.07135</link><description>&lt;p&gt;
MMSD2.0&#65306;&#38754;&#21521;&#21487;&#38752;&#30340;&#22810;&#27169;&#24335;&#35773;&#21050;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MMSD2.0: Towards a Reliable Multi-modal Sarcasm Detection System. (arXiv:2307.07135v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07135
&lt;/p&gt;
&lt;p&gt;
MMSD2.0&#26159;&#19968;&#20010;&#20462;&#27491;&#20102;MMSD&#32570;&#38519;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#22810;&#35270;&#22270;CLIP&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#22810;&#20010;&#35282;&#24230;&#21033;&#29992;&#22810;&#31890;&#24230;&#32447;&#32034;&#36827;&#34892;&#22810;&#27169;&#24335;&#35773;&#21050;&#26816;&#27979;&#65292;&#26174;&#33879;&#20248;&#20110;&#20197;&#21069;&#26368;&#22909;&#30340;&#22522;&#20934;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#35773;&#21050;&#26816;&#27979;&#24341;&#36215;&#20102;&#36817;&#26399;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;MMSD&#65289;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#38459;&#30861;&#20102;&#21487;&#38752;&#30340;&#22810;&#27169;&#24335;&#35773;&#21050;&#26816;&#27979;&#31995;&#32479;&#30340;&#21457;&#23637;&#65306;&#65288;1&#65289;MMSD&#20013;&#23384;&#22312;&#19968;&#20123;&#34394;&#20551;&#32447;&#32034;&#65292;&#23548;&#33268;&#27169;&#22411;&#23398;&#20064;&#20559;&#24046;&#65307;&#65288;2&#65289;MMSD&#20013;&#30340;&#36127;&#26679;&#26412;&#24182;&#19981;&#24635;&#26159;&#21512;&#29702;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MMSD2.0&#65292;&#19968;&#20010;&#32416;&#27491;&#20102;MMSD&#32570;&#38519;&#30340;&#32416;&#27491;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21435;&#38500;&#34394;&#20551;&#32447;&#32034;&#21644;&#37325;&#26032;&#27880;&#37322;&#19981;&#21512;&#29702;&#26679;&#26412;&#26469;&#23436;&#25104;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22810;&#35270;&#22270;CLIP&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#22810;&#20010;&#35282;&#24230;&#65288;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;-&#22270;&#20687;&#20132;&#20114;&#35270;&#22270;&#65289;&#21033;&#29992;&#22810;&#31890;&#24230;&#32447;&#32034;&#36827;&#34892;&#22810;&#27169;&#24335;&#35773;&#21050;&#26816;&#27979;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;MMSD2.0&#26159;&#26500;&#24314;&#21487;&#38752;&#30340;&#22810;&#27169;&#24335;&#35773;&#21050;&#26816;&#27979;&#31995;&#32479;&#30340;&#26377;&#20215;&#20540;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#32780;&#22810;&#35270;&#22270;CLIP&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20339;&#22522;&#20934;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal sarcasm detection has attracted much recent attention. Nevertheless, the existing benchmark (MMSD) has some shortcomings that hinder the development of reliable multi-modal sarcasm detection system: (1) There are some spurious cues in MMSD, leading to the model bias learning; (2) The negative samples in MMSD are not always reasonable. To solve the aforementioned issues, we introduce MMSD2.0, a correction dataset that fixes the shortcomings of MMSD, by removing the spurious cues and re-annotating the unreasonable samples. Meanwhile, we present a novel framework called multi-view CLIP that is capable of leveraging multi-grained cues from multiple perspectives (i.e., text, image, and text-image interaction view) for multi-modal sarcasm detection. Extensive experiments show that MMSD2.0 is a valuable benchmark for building reliable multi-modal sarcasm detection systems and multi-view CLIP can significantly outperform the previous best baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#31934;&#24515;&#21046;&#20316;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#24341;&#23548;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;LLMs&#25805;&#20316;&#20219;&#21153;&#29305;&#23450;&#23646;&#24615;&#24182;&#37325;&#26500;&#26032;&#30340;&#21477;&#23376;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26631;&#31614;&#20132;&#25442;&#25968;&#25454;&#30340;&#29983;&#25104;&#65292;&#19982;&#20854;&#20182;&#22522;&#20110;LLMs&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#32467;&#26524;&#36824;&#26174;&#31034;&#20102;&#36890;&#36807;LLM&#24341;&#23548;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#21363;&#20351;&#22312;&#26356;&#23569;&#30340;&#30417;&#30563;&#24773;&#20917;&#19979;&#20063;&#33021;&#21462;&#24471;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.07099</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;LLM&#30340;&#23646;&#24615;&#25805;&#20316;&#29983;&#25104;&#39640;&#25928;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Generating Efficient Training Data via LLM-based Attribute Manipulation. (arXiv:2307.07099v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#31934;&#24515;&#21046;&#20316;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#24341;&#23548;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;LLMs&#25805;&#20316;&#20219;&#21153;&#29305;&#23450;&#23646;&#24615;&#24182;&#37325;&#26500;&#26032;&#30340;&#21477;&#23376;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26631;&#31614;&#20132;&#25442;&#25968;&#25454;&#30340;&#29983;&#25104;&#65292;&#19982;&#20854;&#20182;&#22522;&#20110;LLMs&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#32467;&#26524;&#36824;&#26174;&#31034;&#20102;&#36890;&#36807;LLM&#24341;&#23548;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#21363;&#20351;&#22312;&#26356;&#23569;&#30340;&#30417;&#30563;&#24773;&#20917;&#19979;&#20063;&#33021;&#21462;&#24471;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#38142;&#24335;&#24605;&#32500;&#23646;&#24615;&#25805;&#20316;&#65288;CoTAM&#65289;&#65292;&#36890;&#36807;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#31934;&#24515;&#21046;&#20316;&#30340;&#25968;&#25454;&#26469;&#24341;&#23548;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#20165;&#23545;&#20219;&#21153;&#30446;&#26631;&#23646;&#24615;&#36827;&#34892;&#26356;&#25913;&#24182;&#21019;&#24314;&#25968;&#25454;&#12290;&#21463;&#21040;&#38754;&#37096;&#23646;&#24615;&#25805;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;LLMs&#26469;&#25805;&#20316;&#20219;&#21153;&#29305;&#23450;&#23646;&#24615;&#24182;&#20197;&#21463;&#25511;&#30340;&#26041;&#24335;&#37325;&#26500;&#26032;&#30340;&#21477;&#23376;&#65292;&#20174;&#32780;&#29983;&#25104;&#26631;&#31614;&#20132;&#25442;&#25968;&#25454;&#12290;&#25105;&#20204;&#37319;&#29992;&#38142;&#24335;&#24605;&#32500;&#20998;&#35299;&#21644;&#37325;&#26500;&#26469;&#36866;&#24212;LLMs&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#28508;&#22312;&#34920;&#31034;&#25511;&#21046;&#26041;&#27861;&#12290;&#22312;&#25991;&#26412;&#20998;&#31867;&#21644;&#20854;&#20182;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;CoTAM&#30456;&#23545;&#20110;&#20854;&#20182;&#20855;&#26377;&#30456;&#21516;&#25968;&#37327;&#35757;&#32451;&#26679;&#26412;&#30340;&#22522;&#20110;LLMs&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#20998;&#26512;&#32467;&#26524;&#21487;&#35270;&#21270;&#20102;CoTAM&#30340;&#23646;&#24615;&#25805;&#20316;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#26356;&#23569;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;LLM&#24341;&#23548;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel method, Chain-of-Thoughts Attribute Manipulation (CoTAM), to guide few-shot learning by carefully crafted data from Large Language Models (LLMs). The main idea is to create data with changes only in the attribute targeted by the task. Inspired by facial attribute manipulation, our approach generates label-switched data by leveraging LLMs to manipulate task-specific attributes and reconstruct new sentences in a controlled manner. Instead of conventional latent representation controlling, we implement chain-of-thoughts decomposition and reconstruction to adapt the procedure to LLMs. Extensive results on text classification and other tasks verify the advantage of CoTAM over other LLM-based text generation methods with the same number of training examples. Analysis visualizes the attribute manipulation effectiveness of CoTAM and presents the potential of LLM-guided learning with even less supervision.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;&#34394;&#25311;&#35821;&#38899;&#21161;&#25163;&#20013;&#23545;&#35805;&#20462;&#22797;&#30340;&#29305;&#28857;&#65292;&#25351;&#20986;&#20102;&#20154;&#21161;&#25163;&#21644;&#20154;&#23545;&#20154;&#23545;&#35805;&#20462;&#22797;&#31574;&#30053;&#30340;&#24046;&#24322;&#65292;&#20197;&#21450;&#19981;&#21516;&#35821;&#35328;&#21644;&#21161;&#25163;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2307.07076</link><description>&lt;p&gt;
&#34394;&#25311;&#35821;&#38899;&#21161;&#25163;&#20013;&#30340;&#23545;&#35805;&#20462;&#22797;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Analysis of Dialogue Repair in Virtual Voice Assistants. (arXiv:2307.07076v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07076
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;&#34394;&#25311;&#35821;&#38899;&#21161;&#25163;&#20013;&#23545;&#35805;&#20462;&#22797;&#30340;&#29305;&#28857;&#65292;&#25351;&#20986;&#20102;&#20154;&#21161;&#25163;&#21644;&#20154;&#23545;&#20154;&#23545;&#35805;&#20462;&#22797;&#31574;&#30053;&#30340;&#24046;&#24322;&#65292;&#20197;&#21450;&#19981;&#21516;&#35821;&#35328;&#21644;&#21161;&#25163;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20351;&#29992;&#32773;&#36890;&#24120;&#20351;&#29992;&#20462;&#22797;&#21551;&#21160;&#22120;&#26469;&#20462;&#22797;&#20182;&#20204;&#22312;&#21475;&#22836;&#20132;&#27969;&#36807;&#31243;&#20013;&#21457;&#29983;&#30340;&#22522;&#26412;&#26029;&#24320;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20154;&#23545;&#20154;&#20351;&#29992;&#20462;&#22797;&#21551;&#21160;&#22120;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35805;&#20462;&#22797;&#32467;&#26500;&#30340;&#30740;&#31350;&#65292;&#20854;&#20013;&#23545;&#35805;&#21551;&#21160;&#22120;&#26159;&#20154;&#31867;&#65292;&#21457;&#36215;&#25110;&#21709;&#24212;&#20462;&#22797;&#30340;&#19968;&#26041;&#26159;&#34394;&#25311;&#21161;&#25163;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#20004;&#31181;&#27969;&#34892;&#30340;&#21161;&#25163;&#65292;Google Assistant&#21644;Apple&#30340;Siri&#65292;&#26816;&#26597;&#20102;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#20013;&#30340;&#20462;&#22797;&#21551;&#21160;&#22120;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#27604;&#36739;&#35821;&#38899;&#21161;&#25163;&#23545;&#38656;&#35201;&#20462;&#22797;&#30340;&#23545;&#35805;&#21644;&#20154;&#23545;&#20154;&#23545;&#35805;&#30340;&#22238;&#24212;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#26368;&#32456;&#30340;&#25968;&#25454;&#34920;&#26126;&#65292;&#19981;&#20165;&#22312;&#20154;&#21161;&#25163;&#21644;&#20154;&#23545;&#20154;&#23545;&#35805;&#20462;&#22797;&#31574;&#30053;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#32780;&#19988;&#22312;&#21161;&#25163;&#21644;&#25152;&#30740;&#31350;&#30340;&#35821;&#35328;&#20043;&#38388;&#20063;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language speakers often use what are known as repair initiators to mend fundamental disconnects that occur between them during verbal communication. Previous research in this field has mainly focused on the human-to-human use of repair initiator. We proposed an examination of dialogue repair structure wherein the dialogue initiator is human and the party that initiates or responds to the repair is a virtual assistant. This study examined the use of repair initiators in both English and Spanish with two popular assistants, Google Assistant and Apple's Siri. Our aim was to codify the differences, if any, in responses by voice assistants to dialogues in need of repair as compared to human-human dialogues also in need of repair. Ultimately the data demonstrated that not only were there differences between human-assistant and human-human dialogue repair strategies, but that there were likewise differences among the assistants and the languages studied.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;ASR&#32534;&#30721;&#22120;&#26469;&#21021;&#22987;&#21270;&#19968;&#20010;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#22312;&#35821;&#38899;&#24847;&#22270;&#20998;&#31867;&#21644;&#27133;&#20301;&#22635;&#20805;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;&#19982;&#33258;&#30417;&#30563;&#23398;&#20064;&#30456;&#27604;&#65292;&#20351;&#29992;ASR&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#22312;&#25928;&#26524;&#19978;&#26356;&#22909;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#21442;&#25968;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#19982;&#32423;&#32852;&#27169;&#22411;&#30456;&#27604;&#65292;&#31471;&#21040;&#31471;&#27169;&#22411;&#22312;&#22823;&#37096;&#20998;&#24773;&#20917;&#19979;&#37117;&#26356;&#22909;&#65292;&#38500;&#38750;&#25552;&#20379;&#20102;oracle ASR&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.07057</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;ASR&#32534;&#30721;&#22120;&#23454;&#29616;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#24847;&#22270;&#20998;&#31867;&#21644;&#27133;&#20301;&#22635;&#20805;
&lt;/p&gt;
&lt;p&gt;
Leveraging Pretrained ASR Encoders for Effective and Efficient End-to-End Speech Intent Classification and Slot Filling. (arXiv:2307.07057v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;ASR&#32534;&#30721;&#22120;&#26469;&#21021;&#22987;&#21270;&#19968;&#20010;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#22312;&#35821;&#38899;&#24847;&#22270;&#20998;&#31867;&#21644;&#27133;&#20301;&#22635;&#20805;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;&#19982;&#33258;&#30417;&#30563;&#23398;&#20064;&#30456;&#27604;&#65292;&#20351;&#29992;ASR&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#22312;&#25928;&#26524;&#19978;&#26356;&#22909;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#21442;&#25968;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#19982;&#32423;&#32852;&#27169;&#22411;&#30456;&#27604;&#65292;&#31471;&#21040;&#31471;&#27169;&#22411;&#22312;&#22823;&#37096;&#20998;&#24773;&#20917;&#19979;&#37117;&#26356;&#22909;&#65292;&#38500;&#38750;&#25552;&#20379;&#20102;oracle ASR&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#38899;&#24847;&#22270;&#20998;&#31867;&#21644;&#27133;&#20301;&#22635;&#20805;&#65288;SICSF&#65289;&#65292;&#25552;&#20986;&#20351;&#29992;&#22312;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#19978;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#26469;&#21021;&#22987;&#21270;&#31471;&#21040;&#31471;&#65288;E2E&#65289;Conformer-Transformer&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;SLURP&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#24847;&#22270;&#20934;&#30830;&#29575;&#36798;&#21040;90.14%&#65292;SLURP-F1&#20026;82.27%&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#34920;&#26126;&#30456;&#23545;&#20110;SSL&#65292;ASR&#39044;&#35757;&#32451;&#22312;SICSF&#19978;&#26356;&#21152;&#26377;&#25928;&#12290;&#20026;&#20102;&#25506;&#32034;&#21442;&#25968;&#25928;&#29575;&#65292;&#25105;&#20204;&#20923;&#32467;&#20102;&#32534;&#30721;&#22120;&#24182;&#28155;&#21152;&#20102;Adapter&#27169;&#22359;&#65292;&#32467;&#26524;&#34920;&#26126;&#21482;&#26377;&#20351;&#29992;ASR&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#25165;&#33021;&#23454;&#29616;&#21442;&#25968;&#25928;&#29575;&#65292;&#32780;SSL&#32534;&#30721;&#22120;&#38656;&#35201;&#23436;&#20840;&#24494;&#35843;&#25165;&#33021;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#31471;&#21040;&#31471;&#27169;&#22411;&#19982;&#32423;&#32852;&#27169;&#22411;&#65288;ASR+NLU&#65289;&#36827;&#34892;&#20102;&#28145;&#20837;&#27604;&#36739;&#65292;&#24182;&#34920;&#26126;&#38500;&#38750;&#25552;&#20379;&#20102;oracle ASR&#27169;&#22411;&#65292;&#21542;&#21017;&#31471;&#21040;&#31471;&#27169;&#22411;&#20248;&#20110;&#32423;&#32852;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#36798;&#21040;&#19982;&#39044;&#20808;&#35757;&#32451;&#30340;ASR&#27169;&#22411;&#30456;&#21516;&#24615;&#33021;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study speech intent classification and slot filling (SICSF) by proposing to use an encoder pretrained on speech recognition (ASR) to initialize an end-to-end (E2E) Conformer-Transformer model, which achieves the new state-of-the-art results on the SLURP dataset, with 90.14% intent accuracy and 82.27% SLURP-F1. We compare our model with encoders pretrained on self-supervised learning (SSL), and show that ASR pretraining is much more effective than SSL for SICSF. To explore parameter efficiency, we freeze the encoder and add Adapter modules, and show that parameter efficiency is only achievable with an ASR-pretrained encoder, while the SSL encoder needs full finetuning to achieve comparable results. In addition, we provide an in-depth comparison on end-to-end models versus cascading models (ASR+NLU), and show that E2E models are better than cascaded models unless an oracle ASR model is provided. Last but not least, our model is the first E2E model that achieves the same performance as
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#20020;&#24202;&#35760;&#24405;&#30340;&#37096;&#20998;&#65292;&#25105;&#20204;&#21457;&#29616;&#39044;&#27979;&#33021;&#21147;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#35760;&#24405;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#19988;&#24403;&#19978;&#19979;&#25991;&#38271;&#24230;&#36739;&#22823;&#26102;&#65292;&#32452;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#35760;&#24405;&#21487;&#20197;&#25913;&#21892;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#31934;&#24515;&#36873;&#25321;&#30340;&#37319;&#26679;&#20989;&#25968;&#21487;&#20197;&#20351;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#20449;&#24687;&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.07051</link><description>&lt;p&gt;
&#20805;&#20998;&#21033;&#29992;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#65306;&#39044;&#27979;&#33021;&#21147;&#38543;&#20020;&#24202;&#35760;&#24405;&#31867;&#22411;&#21644;&#35760;&#24405;&#37096;&#20998;&#30340;&#19981;&#21516;&#32780;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Making the Most Out of the Limited Context Length: Predictive Power Varies with Clinical Note Type and Note Section. (arXiv:2307.07051v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07051
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#20020;&#24202;&#35760;&#24405;&#30340;&#37096;&#20998;&#65292;&#25105;&#20204;&#21457;&#29616;&#39044;&#27979;&#33021;&#21147;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#35760;&#24405;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#19988;&#24403;&#19978;&#19979;&#25991;&#38271;&#24230;&#36739;&#22823;&#26102;&#65292;&#32452;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#35760;&#24405;&#21487;&#20197;&#25913;&#21892;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#31934;&#24515;&#36873;&#25321;&#30340;&#37319;&#26679;&#20989;&#25968;&#21487;&#20197;&#20351;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#20449;&#24687;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#20351;&#29992;&#20020;&#24202;&#35760;&#24405;&#30340;&#33258;&#30001;&#25991;&#26412;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20852;&#36259;&#37325;&#26032;&#29123;&#36215;&#12290;&#20020;&#24202;&#35760;&#24405;&#30340;&#19968;&#20010;&#21306;&#21035;&#29305;&#28857;&#26159;&#23427;&#20204;&#36328;&#36234;&#22810;&#20010;&#38271;&#25991;&#26723;&#30340;&#38271;&#26102;&#38388;&#36328;&#24230;&#12290;&#20020;&#24202;&#35760;&#24405;&#30340;&#29420;&#29305;&#32467;&#26500;&#24102;&#26469;&#20102;&#19968;&#20010;&#26032;&#30340;&#35774;&#35745;&#36873;&#25321;&#65306;&#24403;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#22120;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26377;&#38480;&#26102;&#65292;&#24212;&#36873;&#25321;&#20020;&#24202;&#35760;&#24405;&#30340;&#21738;&#20010;&#37096;&#20998;&#20316;&#20026;&#36755;&#20837;&#65311;&#29616;&#26377;&#30740;&#31350;&#35201;&#20040;&#36873;&#25321;&#20855;&#26377;&#39046;&#22495;&#30693;&#35782;&#30340;&#36755;&#20837;&#65292;&#35201;&#20040;&#31616;&#21333;&#22320;&#25130;&#26029;&#23427;&#20204;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#39640;&#39044;&#27979;&#33021;&#21147;&#37096;&#20998;&#30340;&#26694;&#26550;&#12290;&#20351;&#29992;MIMIC-III&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20197;&#19979;&#21457;&#29616;&#65306;1&#65289;&#39044;&#27979;&#33021;&#21147;&#20998;&#24067;&#22312;&#25252;&#29702;&#35760;&#24405;&#21644;&#20986;&#38498;&#35760;&#24405;&#20043;&#38388;&#26159;&#19981;&#21516;&#30340;&#65307;2&#65289;&#24403;&#19978;&#19979;&#25991;&#38271;&#24230;&#36739;&#22823;&#26102;&#65292;&#32452;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#35760;&#24405;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#31934;&#24515;&#36873;&#25321;&#30340;&#37319;&#26679;&#20989;&#25968;&#21487;&#20197;&#20351;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#20449;&#24687;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models have led to renewed interest in natural language processing in healthcare using the free text of clinical notes. One distinguishing characteristic of clinical notes is their long time span over multiple long documents. The unique structure of clinical notes creates a new design choice: when the context length for a language model predictor is limited, which part of clinical notes should we choose as the input? Existing studies either choose the inputs with domain knowledge or simply truncate them. We propose a framework to analyze the sections with high predictive power. Using MIMIC-III, we show that: 1) predictive power distribution is different between nursing notes and discharge notes and 2) combining different types of notes could improve performance when the context length is large. Our findings suggest that a carefully selected sampling function could enable more efficient information extraction from clinical notes.
&lt;/p&gt;</description></item><item><title>MegaWika&#26159;&#19968;&#20010;&#30001;13&#30334;&#19975;&#20010;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#21644;71&#30334;&#19975;&#20010;&#24341;&#29992;&#28304;&#26448;&#26009;&#32452;&#25104;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#21327;&#20316;&#24335; AI &#36741;&#21161;&#25253;&#21578;&#29983;&#25104;&#30340;&#26032;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;&#23427;&#25552;&#20379;&#20102;&#36328;&#35821;&#35328;&#24212;&#29992;&#20013;&#30340;&#38750;&#33521;&#25991;&#25991;&#31456;&#32763;&#35793;&#21644;&#33258;&#21160;&#35821;&#20041;&#20998;&#26512;&#30340;FrameNet&#35299;&#26512;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#21477;&#23376;&#32423;&#25253;&#21578;&#29983;&#25104;&#30340;&#26368;&#22823;&#36164;&#28304;&#65292;&#20063;&#26159;&#21807;&#19968;&#30340;&#36328;&#22810;&#31181;&#35821;&#35328;&#30340;&#25253;&#21578;&#29983;&#25104;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#22522;&#32447;&#32467;&#26524;&#21644;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;&#36328;&#35821;&#35328;&#38382;&#31572;&#21644;&#24341;&#29992;&#26816;&#32034;&#12290; (tl;dr)</title><link>http://arxiv.org/abs/2307.07049</link><description>&lt;p&gt;
MegaWika: &#36328;&#36234;50&#31181;&#22810;&#26679;&#30340;&#35821;&#35328;&#30340;&#25968;&#30334;&#19975;&#25253;&#21578;&#21450;&#20854;&#26469;&#28304;
&lt;/p&gt;
&lt;p&gt;
MegaWika: Millions of reports and their sources across 50 diverse languages. (arXiv:2307.07049v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07049
&lt;/p&gt;
&lt;p&gt;
MegaWika&#26159;&#19968;&#20010;&#30001;13&#30334;&#19975;&#20010;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#21644;71&#30334;&#19975;&#20010;&#24341;&#29992;&#28304;&#26448;&#26009;&#32452;&#25104;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#21327;&#20316;&#24335; AI &#36741;&#21161;&#25253;&#21578;&#29983;&#25104;&#30340;&#26032;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;&#23427;&#25552;&#20379;&#20102;&#36328;&#35821;&#35328;&#24212;&#29992;&#20013;&#30340;&#38750;&#33521;&#25991;&#25991;&#31456;&#32763;&#35793;&#21644;&#33258;&#21160;&#35821;&#20041;&#20998;&#26512;&#30340;FrameNet&#35299;&#26512;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#21477;&#23376;&#32423;&#25253;&#21578;&#29983;&#25104;&#30340;&#26368;&#22823;&#36164;&#28304;&#65292;&#20063;&#26159;&#21807;&#19968;&#30340;&#36328;&#22810;&#31181;&#35821;&#35328;&#30340;&#25253;&#21578;&#29983;&#25104;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#22522;&#32447;&#32467;&#26524;&#21644;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;&#36328;&#35821;&#35328;&#38382;&#31572;&#21644;&#24341;&#29992;&#26816;&#32034;&#12290; (tl;dr)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20419;&#36827;&#21327;&#20316;&#24335; AI &#36741;&#21161;&#25253;&#21578;&#29983;&#25104;&#26032;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; MegaWika&#65292;&#23427;&#30001;50&#31181;&#19981;&#21516;&#35821;&#35328;&#20013;&#30340;1300&#19975;&#20010;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#20197;&#21450;7100&#19975;&#20010;&#24341;&#29992;&#28304;&#26448;&#26009;&#32452;&#25104;&#12290;&#25105;&#20204;&#23545;&#36825;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22810;&#31181;&#24212;&#29992;&#30340;&#22788;&#29702;&#65292;&#36229;&#20986;&#20102;&#26368;&#21021;&#30340;&#32500;&#22522;&#30334;&#31185;&#24341;&#29992;&#25552;&#21462;&#21644;&#20869;&#23481;&#32593;&#39029;&#25235;&#21462;&#65292;&#21253;&#25324;&#23558;&#38750;&#33521;&#25991;&#25991;&#31456;&#32763;&#35793;&#20026;&#36328;&#35821;&#35328;&#24212;&#29992;&#20197;&#21450;&#25552;&#20379;&#30340; FrameNet &#35299;&#26512;&#29992;&#20110;&#33258;&#21160;&#35821;&#20041;&#20998;&#26512;&#12290;MegaWika&#26159;&#29992;&#20110;&#21477;&#23376;&#32423;&#25253;&#21578;&#29983;&#25104;&#30340;&#26368;&#22823;&#36164;&#28304;&#65292;&#20063;&#26159;&#21807;&#19968;&#30340;&#36328;&#22810;&#31181;&#35821;&#35328;&#30340;&#25253;&#21578;&#29983;&#25104;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#35821;&#20041;&#20998;&#23618;&#26679;&#26412;&#36827;&#34892;&#20102;&#36164;&#28304;&#36136;&#37327;&#30340;&#25163;&#21160;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20026;&#33258;&#21160;&#25253;&#21578;&#29983;&#25104;&#30340;&#20851;&#38190;&#27493;&#39588;&#25552;&#20379;&#20102;&#22522;&#32447;&#32467;&#26524;&#21644;&#35757;&#32451;&#27169;&#22411;&#65306;&#36328;&#35821;&#35328;&#38382;&#31572;&#21644;&#24341;&#29992;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
To foster the development of new models for collaborative AI-assisted report generation, we introduce MegaWika, consisting of 13 million Wikipedia articles in 50 diverse languages, along with their 71 million referenced source materials. We process this dataset for a myriad of applications, going beyond the initial Wikipedia citation extraction and web scraping of content, including translating non-English articles for cross-lingual applications and providing FrameNet parses for automated semantic analysis. MegaWika is the largest resource for sentence-level report generation and the only report generation dataset that is multilingual. We manually analyze the quality of this resource through a semantically stratified sample. Finally, we provide baseline results and trained models for crucial steps in automated report generation: cross-lingual question answering and citation retrieval.
&lt;/p&gt;</description></item><item><title>DIALGEN&#26159;&#19968;&#20010;&#20154;&#26426;&#21322;&#33258;&#21160;&#23545;&#35805;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#29983;&#25104;&#23376;&#23545;&#35805;&#21644;&#20351;&#29992;&#20154;&#24037;&#21453;&#39304;&#26469;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#33258;&#21160;&#29702;&#35299;&#20154;&#38469;&#23545;&#35805;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.07047</link><description>&lt;p&gt;
DIALGEN: &#36890;&#36807;&#20154;&#24037;&#29983;&#25104;&#23545;&#35805;&#25913;&#21892;&#23545;&#20154;&#38469;&#23545;&#35805;&#30340;&#29702;&#35299;&#30340;&#21327;&#21516;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DIALGEN: Collaborative Human-LM Generated Dialogues for Improved Understanding of Human-Human Conversations. (arXiv:2307.07047v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07047
&lt;/p&gt;
&lt;p&gt;
DIALGEN&#26159;&#19968;&#20010;&#20154;&#26426;&#21322;&#33258;&#21160;&#23545;&#35805;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#29983;&#25104;&#23376;&#23545;&#35805;&#21644;&#20351;&#29992;&#20154;&#24037;&#21453;&#39304;&#26469;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#33258;&#21160;&#29702;&#35299;&#20154;&#38469;&#23545;&#35805;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38656;&#35201;&#33258;&#21160;&#29702;&#35299;&#20154;&#38469;&#23545;&#35805;&#30340;&#24212;&#29992;&#36890;&#24120;&#38754;&#20020;&#19982;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#31169;&#20154;&#20449;&#24687;&#65292;&#22914;&#21628;&#21483;&#20013;&#24515;&#25110;&#20020;&#24202;&#23545;&#35805;&#65292;&#26377;&#20851;&#30340;&#25361;&#25112;&#12290;&#22788;&#29702;&#21463;&#20445;&#25252;&#30340;&#25968;&#25454;&#20063;&#20250;&#22686;&#21152;&#27880;&#37322;&#25104;&#26412;&#65292;&#20174;&#32780;&#38480;&#21046;&#25216;&#26415;&#21457;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DIALGEN&#65292;&#19968;&#31181;&#20154;&#26426;&#21322;&#33258;&#21160;&#23545;&#35805;&#29983;&#25104;&#26694;&#26550;&#12290;DIALGEN&#20351;&#29992;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;&#65288;ChatGPT&#65289;&#65292;&#21487;&#20197;&#36981;&#24490;&#26550;&#26500;&#21644;&#39118;&#26684;&#35268;&#33539;&#65292;&#29983;&#25104;&#27969;&#21033;&#30340;&#23545;&#35805;&#25991;&#26412;&#65292;&#36890;&#36807;&#36845;&#20195;&#29983;&#25104;&#23376;&#23545;&#35805;&#24182;&#20351;&#29992;&#20154;&#24037;&#21453;&#39304;&#26469;&#32416;&#27491;&#19981;&#19968;&#33268;&#25110;&#37325;&#23450;&#23545;&#35805;&#30340;&#27969;&#31243;&#12290;&#22312;&#23558;&#20195;&#29702;-&#23458;&#25143;&#20449;&#24687;&#25910;&#38598;&#21628;&#21483;&#24402;&#32435;&#20026;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#32467;&#26500;&#21270;&#27010;&#25324;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DIALGEN&#25968;&#25454;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applications that could benefit from automatic understanding of human-human conversations often come with challenges associated with private information in real-world data such as call center or clinical conversations. Working with protected data also increases costs of annotation, which limits technology development. To address these challenges, we propose DIALGEN, a human-in-the-loop semi-automated dialogue generation framework. DIALGEN uses a language model (ChatGPT) that can follow schema and style specifications to produce fluent conversational text, generating a complex conversation through iteratively generating subdialogues and using human feedback to correct inconsistencies or redirect the flow. In experiments on structured summarization of agent-client information gathering calls, framed as dialogue state tracking, we show that DIALGEN data enables significant improvement in model performance.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20381;&#23384;&#23376;&#26641;&#20132;&#25442;&#23454;&#29616;&#26426;&#22120;&#32763;&#35793;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#20381;&#23384;&#26641;&#20013;&#25552;&#21462;&#30456;&#24212;&#30340;&#23376;&#26641;&#24182;&#36827;&#34892;&#20132;&#25442;&#65292;&#21019;&#24314;&#22686;&#24378;&#26679;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#65292;&#23427;&#22312;3&#20010;&#35821;&#35328;&#23545;&#19978;&#34920;&#29616;&#20986;&#20102;&#25345;&#32493;&#30340;BLEU&#20998;&#25968;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2307.07025</link><description>&lt;p&gt;
&#36890;&#36807;&#20381;&#23384;&#23376;&#26641;&#20132;&#25442;&#23454;&#29616;&#26426;&#22120;&#32763;&#35793;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation for Machine Translation via Dependency Subtree Swapping. (arXiv:2307.07025v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07025
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20381;&#23384;&#23376;&#26641;&#20132;&#25442;&#23454;&#29616;&#26426;&#22120;&#32763;&#35793;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#20381;&#23384;&#26641;&#20013;&#25552;&#21462;&#30456;&#24212;&#30340;&#23376;&#26641;&#24182;&#36827;&#34892;&#20132;&#25442;&#65292;&#21019;&#24314;&#22686;&#24378;&#26679;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#65292;&#23427;&#22312;3&#20010;&#35821;&#35328;&#23545;&#19978;&#34920;&#29616;&#20986;&#20102;&#25345;&#32493;&#30340;BLEU&#20998;&#25968;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#36890;&#36807;&#20381;&#23384;&#23376;&#26641;&#20132;&#25442;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#30340;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#12290;&#25105;&#20204;&#20174;&#28304;&#21477;&#23376;&#21644;&#30446;&#26631;&#21477;&#23376;&#30340;&#20381;&#23384;&#26641;&#20013;&#25552;&#21462;&#30456;&#24212;&#30340;&#23376;&#26641;&#65292;&#24182;&#23558;&#20854;&#22312;&#21477;&#23376;&#20043;&#38388;&#20132;&#25442;&#65292;&#20197;&#21019;&#24314;&#22686;&#24378;&#26679;&#26412;&#12290;&#25105;&#20204;&#26681;&#25454;&#20381;&#23384;&#26641;&#30340;&#22270;&#24418;&#30456;&#20284;&#24615;&#21644;&#38468;&#21152;&#21551;&#21457;&#24335;&#26041;&#27861;&#36827;&#34892;&#24443;&#24213;&#30340;&#36807;&#28388;&#65292;&#20197;&#30830;&#20445;&#25552;&#21462;&#30340;&#23376;&#26641;&#23545;&#24212;&#20110;&#30456;&#21516;&#30340;&#21547;&#20041;&#12290;&#25105;&#20204;&#20351;&#29992;IWSLT&#25991;&#26412;&#32763;&#35793;&#25968;&#25454;&#38598;&#21644;Hunglish2&#35821;&#26009;&#24211;&#65292;&#22312;4&#20010;&#35821;&#35328;&#23545;&#30340;&#20004;&#20010;&#26041;&#21521;&#19978;&#36827;&#34892;&#20102;&#36164;&#28304;&#21463;&#38480;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;4&#20010;&#35821;&#35328;&#23545;&#20013;&#65292;&#25105;&#20204;&#30340;&#22522;&#32447;&#27169;&#22411;&#22312;3&#20010;&#35821;&#35328;&#23545;&#19978;&#30340;BLEU&#20998;&#25968;&#26377;&#20102;&#25345;&#32493;&#30340;&#25552;&#39640;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#22312;GitHub&#19978;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a generic framework for data augmentation via dependency subtree swapping that is applicable to machine translation. We extract corresponding subtrees from the dependency parse trees of the source and target sentences and swap these across bisentences to create augmented samples. We perform thorough filtering based on graphbased similarities of the dependency trees and additional heuristics to ensure that extracted subtrees correspond to the same meaning. We conduct resource-constrained experiments on 4 language pairs in both directions using the IWSLT text translation datasets and the Hunglish2 corpus. The results demonstrate consistent improvements in BLEU score over our baseline models in 3 out of 4 language pairs. Our code is available on GitHub.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#27874;&#20848;&#35821;&#36873;&#20030;&#28608;&#21169;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;6,112&#26465;&#20154;&#24037;&#26631;&#27880;&#30340;&#25512;&#25991;&#12290;&#32763;&#35793;&#27169;&#22411;HerBERT&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;68%&#30340;F1&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.07007</link><description>&lt;p&gt;
&#36873;&#20030;&#28608;&#21169;&#25968;&#25454;&#38598;&#65306;&#27874;&#20848;&#36873;&#20030;&#30340;&#24212;&#29992;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Electoral Agitation Data Set: The Use Case of the Polish Election. (arXiv:2307.07007v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07007
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#27874;&#20848;&#35821;&#36873;&#20030;&#28608;&#21169;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;6,112&#26465;&#20154;&#24037;&#26631;&#27880;&#30340;&#25512;&#25991;&#12290;&#32763;&#35793;&#27169;&#22411;HerBERT&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;68%&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#27969;&#34892;&#20351;&#25919;&#27835;&#23478;&#20204;&#20351;&#29992;&#23427;&#36827;&#34892;&#25919;&#27835;&#24191;&#21578;&#12290;&#22240;&#27492;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#20805;&#28385;&#20102;&#36873;&#20030;&#28608;&#21169;&#65288;&#36873;&#20030;&#23459;&#20256;&#65289;&#65292;&#29305;&#21035;&#26159;&#22312;&#36873;&#20030;&#27963;&#21160;&#26399;&#38388;&#12290;&#36873;&#20030;&#31649;&#29702;&#26426;&#26500;&#26080;&#27861;&#36861;&#36394;&#31526;&#21512;&#36873;&#20030;&#27861;&#35268;&#23450;&#30340;&#28608;&#21169;&#20449;&#24687;&#30340;&#20256;&#25773;&#21644;&#25968;&#37327;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#36804;&#20170;&#20026;&#27490;&#23578;&#26410;&#26377;&#25928;&#38024;&#23545;&#30340;&#19968;&#20010;&#21033;&#22522;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#27874;&#20848;&#35821;&#36873;&#20030;&#28608;&#21169;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#12290;&#20854;&#20013;&#21253;&#21547;6,112&#26465;&#20154;&#24037;&#26631;&#27880;&#30340;&#24102;&#26377;&#22235;&#31181;&#27861;&#24459;&#32422;&#26463;&#31867;&#21035;&#26631;&#31614;&#30340;&#25512;&#25991;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;0.66&#30340;&#35780;&#27880;&#32773;&#19968;&#33268;&#24615;&#65288;Cohen's kappa&#20998;&#20540;&#65289;&#12290;&#21478;&#19968;&#20301;&#35780;&#27880;&#32773;&#35299;&#20915;&#20102;&#21069;&#20004;&#20301;&#35780;&#27880;&#32773;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#25552;&#39640;&#20102;&#26631;&#27880;&#27969;&#31243;&#30340;&#19968;&#33268;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;&#26032;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#34987;&#29992;&#20110;&#20248;&#21270;&#19968;&#20010;&#21517;&#20026;HerBERT&#30340;&#27874;&#20848;&#35821;&#35328;&#27169;&#22411;&#65288;&#36798;&#21040;68%&#30340;F1&#20998;&#25968;&#65289;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20123;&#28508;&#22312;&#30340;&#24212;&#29992;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of social media makes politicians use it for political advertisement. Therefore, social media is full of electoral agitation (electioneering), especially during the election campaigns. The election administration cannot track the spread and quantity of messages that count as agitation under the election code. It addresses a crucial problem, while also uncovering a niche that has not been effectively targeted so far. Hence, we present the first publicly open data set for detecting electoral agitation in the Polish language. It contains 6,112 human-annotated tweets tagged with four legally conditioned categories. We achieved a 0.66 inter-annotator agreement (Cohen's kappa score). An additional annotator resolved the mismatches between the first two improving the consistency and complexity of the annotation process. The newly created data set was used to fine-tune a Polish Language Model called HerBERT (achieving a 68% F1 score). We also present a number of potential use ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#20843;&#31181;&#26131;&#20110;&#38598;&#25104;&#21040;&#29616;&#26377;NLP&#31995;&#32479;&#20013;&#19988;&#19981;&#38656;&#35201;&#39069;&#22806;&#24102;&#22806;&#25968;&#25454;&#25110;&#27169;&#22411;&#20462;&#25913;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#23436;&#20840;&#21487;&#22797;&#29616;&#23454;&#39564;&#32467;&#26524;&#30340;&#30740;&#31350;&#29615;&#22659;&#12290;&#20998;&#26512;&#34920;&#26126;&#29616;&#26377;&#30340;NLP&#20219;&#21153;&#20013;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#23545;&#20110;&#25429;&#25417;&#25152;&#26377;&#30001;&#19981;&#21516;&#31867;&#22411;&#20998;&#24067;&#36716;&#25442;&#29305;&#24449;&#30340;&#26679;&#26412;&#23578;&#19981;&#22815;&#25935;&#24863;&#65292;&#36825;&#38656;&#35201;&#26410;&#26469;&#30340;&#24037;&#20316;&#26469;&#24320;&#21457;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.07002</link><description>&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#32463;&#20856;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#30340;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Classical Out-of-Distribution Detection Methods Benchmark in Text Classification Tasks. (arXiv:2307.07002v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#20843;&#31181;&#26131;&#20110;&#38598;&#25104;&#21040;&#29616;&#26377;NLP&#31995;&#32479;&#20013;&#19988;&#19981;&#38656;&#35201;&#39069;&#22806;&#24102;&#22806;&#25968;&#25454;&#25110;&#27169;&#22411;&#20462;&#25913;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#23436;&#20840;&#21487;&#22797;&#29616;&#23454;&#39564;&#32467;&#26524;&#30340;&#30740;&#31350;&#29615;&#22659;&#12290;&#20998;&#26512;&#34920;&#26126;&#29616;&#26377;&#30340;NLP&#20219;&#21153;&#20013;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#23545;&#20110;&#25429;&#25417;&#25152;&#26377;&#30001;&#19981;&#21516;&#31867;&#22411;&#20998;&#24067;&#36716;&#25442;&#29305;&#24449;&#30340;&#26679;&#26412;&#23578;&#19981;&#22815;&#25935;&#24863;&#65292;&#36825;&#38656;&#35201;&#26410;&#26469;&#30340;&#24037;&#20316;&#26469;&#24320;&#21457;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#21463;&#25511;&#29615;&#22659;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#38754;&#23545;&#24102;&#22806;&#20998;&#24067;&#30340;&#20363;&#23376;&#26102;&#24448;&#24448;&#34920;&#29616;&#20986;&#22256;&#38590;&#65292;&#22240;&#27492;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#25104;&#20026;NLP&#31995;&#32479;&#20013;&#20851;&#38190;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#26412;&#25991;&#30528;&#37325;&#24378;&#35843;&#20102;&#29616;&#26377;NLP&#39046;&#22495;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20843;&#31181;&#26131;&#20110;&#38598;&#25104;&#21040;&#29616;&#26377;NLP&#31995;&#32479;&#20013;&#19988;&#19981;&#38656;&#35201;&#39069;&#22806;&#24102;&#22806;&#25968;&#25454;&#25110;&#27169;&#22411;&#20462;&#25913;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#20043;&#19968;&#26159;&#25552;&#20379;&#20102;&#19968;&#20010;&#32467;&#26500;&#33391;&#22909;&#30340;&#30740;&#31350;&#29615;&#22659;&#65292;&#21487;&#20197;&#23436;&#20840;&#22797;&#29616;&#23454;&#39564;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#29616;&#26377;&#30340;NLP&#20219;&#21153;&#20013;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#23545;&#20110;&#25429;&#25417;&#25152;&#26377;&#30001;&#19981;&#21516;&#31867;&#22411;&#20998;&#24067;&#36716;&#25442;&#29305;&#24449;&#30340;&#26679;&#26412;&#23578;&#19981;&#22815;&#25935;&#24863;&#12290;&#22312;&#39046;&#22495;&#32972;&#26223;&#36716;&#21464;&#21644;&#21333;&#35789;&#38543;&#26426;&#25490;&#21015;&#30340;&#24773;&#20917;&#19979;&#65292;&#27979;&#35797;&#24773;&#26223;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#31361;&#26174;&#20102;&#26410;&#26469;&#38656;&#35201;&#24320;&#23637;&#26356;&#26377;&#25928;&#30340;&#24037;&#20316;&#26469;&#21457;&#23637;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art models can perform well in controlled environments, but they often struggle when presented with out-of-distribution (OOD) examples, making OOD detection a critical component of NLP systems. In this paper, we focus on highlighting the limitations of existing approaches to OOD detection in NLP. Specifically, we evaluated eight OOD detection methods that are easily integrable into existing NLP systems and require no additional OOD data or model modifications. One of our contributions is providing a well-structured research environment that allows for full reproducibility of the results. Additionally, our analysis shows that existing OOD detection methods for NLP tasks are not yet sufficiently sensitive to capture all samples characterized by various types of distributional shifts. Particularly challenging testing scenarios arise in cases of background shift and randomly shuffled word order within in domain texts. This highlights the need for future work to develop more ef
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19987;&#21033;&#25991;&#20214;&#20013;&#25552;&#21462;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#30693;&#35782;&#22270;&#26469;&#22635;&#20805;&#36890;&#29992;&#35774;&#35745;&#30693;&#35782;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2307.06985</link><description>&lt;p&gt;
&#36808;&#21521;&#22635;&#20805;&#36890;&#29992;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Populating Generalizable Engineering Design Knowledge. (arXiv:2307.06985v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06985
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19987;&#21033;&#25991;&#20214;&#20013;&#25552;&#21462;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#30693;&#35782;&#22270;&#26469;&#22635;&#20805;&#36890;&#29992;&#35774;&#35745;&#30693;&#35782;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22635;&#20805;&#36890;&#29992;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19987;&#21033;&#25991;&#20214;&#20013;&#25552;&#21462;head entity :: relationship :: tail entity&#24418;&#24335;&#20107;&#23454;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#20107;&#23454;&#21487;&#20197;&#22312;&#19987;&#21033;&#25991;&#20214;&#20869;&#37096;&#21644;&#36328;&#25991;&#20214;&#20043;&#38388;&#32452;&#21512;&#24418;&#25104;&#30693;&#35782;&#22270;&#65292;&#29992;&#20316;&#34920;&#31034;&#21644;&#23384;&#20648;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#24037;&#31243;&#35774;&#35745;&#25991;&#29486;&#20013;&#30340;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#20851;&#31995;&#26469;&#22635;&#20805;&#32479;&#35745;&#36817;&#20284;&#32780;&#38750;&#20107;&#23454;&#30340;&#19977;&#20803;&#32452;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#26631;&#35760;&#22120;&#26469;&#35782;&#21035;&#21477;&#23376;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#12290;&#22312;&#30830;&#23450;&#20102;&#19968;&#23545;&#23454;&#20307;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;&#21478;&#19968;&#20010;&#26631;&#35760;&#22120;&#26469;&#35782;&#21035;&#29305;&#23450;&#34920;&#31034;&#36825;&#23545;&#23454;&#20307;&#20043;&#38388;&#20851;&#31995;&#30340;&#20851;&#31995;&#26631;&#35760;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#20123;&#26631;&#35760;&#22120;&#65292;&#25105;&#20204;&#25163;&#21160;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;44,227&#20010;&#21477;&#23376;&#21644;&#30456;&#24212;&#20107;&#23454;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#23558;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#36890;&#24120;&#25512;&#33616;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20854;&#20013;&#25105;&#20204;&#39044;.
&lt;/p&gt;
&lt;p&gt;
Aiming to populate generalizable engineering design knowledge, we propose a method to extract facts of the form head entity :: relationship :: tail entity from sentences found in patent documents. These facts could be combined within and across patent documents to form knowledge graphs that serve as schemes for representing as well as storing design knowledge. Existing methods in engineering design literature often utilise a set of predefined relationships to populate triples that are statistical approximations rather than facts. In our method, we train a tagger to identify both entities and relationships from a sentence. Given a pair of entities thus identified, we train another tagger to identify the relationship tokens that specifically denote the relationship between the pair. For training these taggers, we manually construct a dataset of 44,227 sentences and corresponding facts. We also compare the performance of the method against typically recommended approaches, wherein, we pre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;DARPA&#36890;&#20449;&#32773;&#39033;&#30446;&#20013;&#21463;&#25387;&#21644;&#24700;&#24594;&#30340;&#29992;&#25143;&#35760;&#24405;&#65292;&#24212;&#29992;&#20250;&#35805;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#20197;&#35782;&#21035;&#20154;&#19982;&#30005;&#33041;&#20250;&#35805;&#20013;&#30340;&#25913;&#36827;&#26426;&#20250;&#21644;&#22833;&#36133;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.06982</link><description>&lt;p&gt;
&#37325;&#35775;DARPA&#36890;&#20449;&#32773;&#25968;&#25454;&#65292;&#20351;&#29992;&#20250;&#35805;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Revisiting the DARPA Communicator Data using Conversation Analysis. (arXiv:2307.06982v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;DARPA&#36890;&#20449;&#32773;&#39033;&#30446;&#20013;&#21463;&#25387;&#21644;&#24700;&#24594;&#30340;&#29992;&#25143;&#35760;&#24405;&#65292;&#24212;&#29992;&#20250;&#35805;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#20197;&#35782;&#21035;&#20154;&#19982;&#30005;&#33041;&#20250;&#35805;&#20013;&#30340;&#25913;&#36827;&#26426;&#20250;&#21644;&#22833;&#36133;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#19982;&#30005;&#33041;&#20250;&#35805;&#26041;&#38754;&#30340;&#29616;&#26377;&#25216;&#26415;&#20173;&#26377;&#24453;&#25913;&#36827;&#65292;&#19982;&#30005;&#33041;&#20132;&#27969;&#24448;&#24448;&#38750;&#24120;&#20196;&#20154;&#35752;&#21388;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#36890;&#36807;&#23547;&#25214;&#33039;&#35805;&#28389;&#29992;&#26469;&#35782;&#21035;&#36825;&#20123;&#31995;&#32479;&#20013;&#30340;&#8220;&#25913;&#36827;&#26426;&#20250;&#8221;&#30340;&#26041;&#27861;&#12290;&#36825;&#19968;&#20551;&#35774;&#26159;&#20154;&#20204;&#23545;&#30005;&#33041;&#35828;&#33039;&#35805;&#26159;&#19968;&#31181;&#24809;&#32602;&#65292;&#33039;&#35805;&#20195;&#34920;&#20102;&#31995;&#32479;&#26410;&#33021;&#27491;&#24120;&#36816;&#34892;&#30340;&#22833;&#36133;&#28857;&#12290;&#22312;&#30830;&#23450;&#20986;&#38169;&#20301;&#32622;&#21518;&#65292;&#25105;&#20204;&#21487;&#20197;&#36870;&#21521;&#26597;&#30475;&#35760;&#24405;&#65292;&#24182;&#36890;&#36807;&#20250;&#35805;&#20998;&#26512;&#25214;&#20986;&#20986;&#38169;&#21407;&#22240;&#12290;&#20250;&#35805;&#20998;&#26512;&#26159;&#19968;&#31181;&#36136;&#24615;&#26041;&#27861;&#65292;&#23545;&#20110;&#25105;&#20204;&#36825;&#20123;&#26469;&#33258;&#25968;&#37327;&#32972;&#26223;&#30340;&#20154;&#26469;&#35828;&#65292;&#21487;&#33021;&#20250;&#26174;&#24471;&#24456;&#38476;&#29983;&#65292;&#29978;&#33267;&#19981;&#31185;&#23398;&#12290;&#26412;&#25991;&#39318;&#20808;&#25551;&#36848;&#20102;&#29616;&#20195;&#20250;&#35805;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#28982;&#21518;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;DARPA&#36890;&#20449;&#32773;&#39033;&#30446;&#20013;&#21463;&#25387;&#21644;&#24700;&#24594;&#30340;&#29992;&#25143;&#30340;&#35760;&#24405;&#12290;&#32467;&#35770;&#26159;&#23384;&#22312;&#19968;&#20123;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The state of the art in human computer conversation leaves something to be desired and, indeed, talking to a computer can be down-right annoying. This paper describes an approach to identifying ``opportunities for improvement'' in these systems by looking for abuse in the form of swear words. The premise is that humans swear at computers as a sanction and, as such, swear words represent a point of failure where the system did not behave as it should. Having identified where things went wrong, we can work backward through the transcripts and, using conversation analysis (CA) work out how things went wrong. Conversation analysis is a qualitative methodology and can appear quite alien - indeed unscientific - to those of us from a quantitative background. The paper starts with a description of Conversation analysis in its modern form, and then goes on to apply the methodology to transcripts of frustrated and annoyed users in the DARPA Communicator project. The conclusion is that there is a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23391;&#21152;&#25289;&#35821;&#20013;&#20551;&#26032;&#38395;&#30340;&#26816;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#24635;&#32467;&#21644;&#25193;&#20805;&#25216;&#26415;&#65292;&#32467;&#21512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22235;&#37325;&#26041;&#27861;&#26469;&#20998;&#31867;&#23391;&#21152;&#25289;&#35821;&#30340;&#20551;&#26032;&#38395;&#25991;&#31456;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#24635;&#32467;&#21644;&#25193;&#20805;&#22312;&#23391;&#21152;&#25289;&#35821;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06979</link><description>&lt;p&gt;
&#35299;&#20915;&#23391;&#21152;&#25289;&#35821;&#20013;&#30340;&#20551;&#26032;&#38395;&#38382;&#39064;&#65306;&#25581;&#31034;&#24635;&#32467;&#19982;&#25193;&#20805;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Tackling Fake News in Bengali: Unraveling the Impact of Summarization vs. Augmentation on Pre-trained Language Models. (arXiv:2307.06979v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23391;&#21152;&#25289;&#35821;&#20013;&#20551;&#26032;&#38395;&#30340;&#26816;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#24635;&#32467;&#21644;&#25193;&#20805;&#25216;&#26415;&#65292;&#32467;&#21512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22235;&#37325;&#26041;&#27861;&#26469;&#20998;&#31867;&#23391;&#21152;&#25289;&#35821;&#30340;&#20551;&#26032;&#38395;&#25991;&#31456;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#24635;&#32467;&#21644;&#25193;&#20805;&#22312;&#23391;&#21152;&#25289;&#35821;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#21644;&#22312;&#32447;&#26032;&#38395;&#26469;&#28304;&#30340;&#20852;&#36215;&#65292;&#20551;&#26032;&#38395;&#24050;&#25104;&#20026;&#20840;&#29699;&#24615;&#30340;&#37325;&#22823;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#20687;&#23391;&#21152;&#25289;&#35821;&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#26816;&#27979;&#20551;&#26032;&#38395;&#22312;&#30740;&#31350;&#20013;&#21463;&#21040;&#20102;&#26377;&#38480;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#24635;&#32467;&#21644;&#25193;&#20805;&#25216;&#26415;&#20197;&#21450;&#20116;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#20998;&#31867;&#23391;&#21152;&#25289;&#35821;&#30340;&#20551;&#26032;&#38395;&#25991;&#31456;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#23558;&#33521;&#35821;&#26032;&#38395;&#25991;&#31456;&#36827;&#34892;&#32763;&#35793;&#65292;&#24182;&#20351;&#29992;&#25193;&#20805;&#25216;&#26415;&#26469;&#35299;&#20915;&#20551;&#26032;&#38395;&#25991;&#31456;&#30340;&#19981;&#36275;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#30528;&#37325;&#20110;&#36890;&#36807;&#24635;&#32467;&#26032;&#38395;&#26469;&#35299;&#20915;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#20196;&#29260;&#38271;&#24230;&#38480;&#21046;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20005;&#26684;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24635;&#32467;&#21644;&#25193;&#20805;&#22312;&#23391;&#21152;&#25289;&#35821;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#29420;&#31435;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#24403;&#23558;BanglaBERT&#22522;&#30784;&#27169;&#22411;&#19982;&#25193;&#20805;&#25216;&#26415;&#30456;&#32467;&#21512;&#26102;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of social media and online news sources, fake news has become a significant issue globally. However, the detection of fake news in low resource languages like Bengali has received limited attention in research. In this paper, we propose a methodology consisting of four distinct approaches to classify fake news articles in Bengali using summarization and augmentation techniques with five pre-trained language models. Our approach includes translating English news articles and using augmentation techniques to curb the deficit of fake news articles. Our research also focused on summarizing the news to tackle the token length limitation of BERT based models. Through extensive experimentation and rigorous evaluation, we show the effectiveness of summarization and augmentation in the case of Bengali fake news detection. We evaluated our models using three separate test datasets. The BanglaBERT Base model, when combined with augmentation techniques, achieved an impressive accurac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#25991;&#26412;&#29983;&#25104;&#23450;&#20041;&#20026;&#20174;&#29616;&#26377;&#25991;&#26412;&#38598;&#21512;&#20013;&#36880;&#27493;&#22797;&#21046;&#25991;&#26412;&#29255;&#27573;&#65292;&#24182;&#36890;&#36807;&#22797;&#21046;&#21644;&#31896;&#36148;&#25805;&#20316;&#26469;&#23454;&#29616;&#29983;&#25104;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#39034;&#24207;&#36873;&#25321;&#21333;&#35789;&#29983;&#25104;&#30340;&#27169;&#22411;&#65292;&#22312;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#19988;&#25512;&#29702;&#25928;&#29575;&#19982;&#22522;&#20110;&#26631;&#35760;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2307.06962</link><description>&lt;p&gt;
&#22797;&#21046;&#23601;&#26159;&#20320;&#25152;&#38656;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Copy Is All You Need. (arXiv:2307.06962v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#25991;&#26412;&#29983;&#25104;&#23450;&#20041;&#20026;&#20174;&#29616;&#26377;&#25991;&#26412;&#38598;&#21512;&#20013;&#36880;&#27493;&#22797;&#21046;&#25991;&#26412;&#29255;&#27573;&#65292;&#24182;&#36890;&#36807;&#22797;&#21046;&#21644;&#31896;&#36148;&#25805;&#20316;&#26469;&#23454;&#29616;&#29983;&#25104;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#39034;&#24207;&#36873;&#25321;&#21333;&#35789;&#29983;&#25104;&#30340;&#27169;&#22411;&#65292;&#22312;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#19988;&#25512;&#29702;&#25928;&#29575;&#19982;&#22522;&#20110;&#26631;&#35760;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#23548;&#30340;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#39034;&#24207;&#36873;&#25321;&#26469;&#33258;&#22266;&#23450;&#35789;&#27719;&#34920;&#30340;&#21333;&#35789;&#26469;&#32452;&#25104;&#36755;&#20986;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25991;&#26412;&#29983;&#25104;&#23450;&#20041;&#20026;&#20174;&#29616;&#26377;&#25991;&#26412;&#38598;&#21512;&#20013;&#36880;&#27493;&#22797;&#21046;&#25991;&#26412;&#29255;&#27573;&#65288;&#20363;&#22914;&#21333;&#35789;&#25110;&#30701;&#35821;&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#25928;&#30340;&#21521;&#37327;&#25628;&#32034;&#24037;&#20855;&#21253;&#35745;&#31639;&#26377;&#24847;&#20041;&#30340;&#25991;&#26412;&#29255;&#27573;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#24182;&#23545;&#20854;&#36827;&#34892;&#32034;&#24341;&#12290;&#28982;&#21518;&#65292;&#25991;&#26412;&#29983;&#25104;&#30340;&#20219;&#21153;&#34987;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#30340;&#22797;&#21046;&#21644;&#31896;&#36148;&#25805;&#20316;&#65306;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#65292;&#25105;&#20204;&#20174;&#25991;&#26412;&#38598;&#21512;&#20013;&#23547;&#25214;&#21512;&#36866;&#30340;&#25991;&#26412;&#29255;&#27573;&#65292;&#32780;&#19981;&#26159;&#20174;&#29420;&#31435;&#30340;&#35789;&#27719;&#34920;&#20013;&#36873;&#25321;&#12290;&#22312;&#26631;&#20934;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#27979;&#35797;&#65288;WikiText-103&#65289;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#37117;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#29983;&#25104;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20943;&#23569;&#20102;&#35299;&#30721;&#27493;&#39588;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25512;&#29702;&#25928;&#29575;&#19982;&#22522;&#20110;&#26631;&#35760;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#24403;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#31616;&#21333;&#22320;&#20999;&#25442;&#21040;&#19981;&#21516;&#39046;&#22495;&#65292;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant text generation models compose the output by sequentially selecting words from a fixed vocabulary. In this paper, we formulate text generation as progressively copying text segments (e.g., words or phrases) from an existing text collection. We compute the contextualized representations of meaningful text segments and index them using efficient vector search toolkits. The task of text generation is then decomposed into a series of copy-and-paste operations: at each time step, we seek suitable text spans from the text collection rather than selecting from a standalone vocabulary. Experiments on the standard language modeling benchmark (WikiText-103) show that our approach achieves better generation quality according to both automatic and human evaluations. Besides, its inference efficiency is comparable to token-level autoregressive models thanks to the reduction of decoding steps. We also show that our approach allows for effective domain adaptation by simply switching to d
&lt;/p&gt;</description></item><item><title>ACTI&#22312;EVALITA 2023&#20013;&#30340;&#38452;&#35851;&#35770;&#36776;&#35782;&#20219;&#21153;&#20849;&#26377;15&#25903;&#22242;&#38431;&#21442;&#19982;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21028;&#26029;&#38452;&#35851;&#20869;&#23481;&#21644;&#20998;&#31867;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#25269;&#21046;&#22312;&#22312;&#32447;&#24179;&#21488;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2307.06954</link><description>&lt;p&gt;
ACTI&#22312;EVALITA 2023&#20013;&#30340;&#32508;&#36848;&#65306;&#38452;&#35851;&#35770;&#36776;&#35782;&#20219;&#21153;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
ACTI at EVALITA 2023: Overview of the Conspiracy Theory Identification Task. (arXiv:2307.06954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06954
&lt;/p&gt;
&lt;p&gt;
ACTI&#22312;EVALITA 2023&#20013;&#30340;&#38452;&#35851;&#35770;&#36776;&#35782;&#20219;&#21153;&#20849;&#26377;15&#25903;&#22242;&#38431;&#21442;&#19982;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21028;&#26029;&#38452;&#35851;&#20869;&#23481;&#21644;&#20998;&#31867;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#25269;&#21046;&#22312;&#22312;&#32447;&#24179;&#21488;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38452;&#35851;&#35770;&#36776;&#35782;&#20219;&#21153;&#26159;Evalita 2023&#39318;&#27425;&#25552;&#20986;&#30340;&#26032;&#20849;&#20139;&#20219;&#21153;&#12290;ACTI&#25361;&#25112;&#20165;&#22522;&#20110;Telegram&#19978;&#30340;&#38452;&#35851;&#39057;&#36947;&#35780;&#35770;&#65292;&#20998;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;(i) &#38452;&#35851;&#20869;&#23481;&#20998;&#31867;&#65306;&#36776;&#35782;&#38452;&#35851;&#20869;&#23481;&#21644;(ii) &#38452;&#35851;&#31867;&#21035;&#20998;&#31867;&#65306;&#38024;&#23545;&#29305;&#23450;&#38452;&#35851;&#29702;&#35770;&#20998;&#31867;&#12290;&#20849;&#26377;15&#25903;&#22242;&#38431;&#21442;&#19982;&#20102;&#35813;&#20219;&#21153;&#65292;&#24635;&#20849;&#25552;&#20132;&#20102;81&#20010;&#32467;&#26524;&#12290;&#25105;&#20204;&#35828;&#26126;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20851;&#20110;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#26469;&#25269;&#21046;&#22312;&#22312;&#32447;&#24179;&#21488;&#19978;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conspiracy Theory Identication task is a new shared task proposed for the first time at the Evalita 2023. The ACTI challenge, based exclusively on comments published on conspiratorial channels of telegram, is divided into two subtasks: (i) Conspiratorial Content Classification: identifying conspiratorial content and (ii) Conspiratorial Category Classification about specific conspiracy theory classification. A total of fifteen teams participated in the task for a total of 81 submissions. We illustrate the best performing approaches were based on the utilization of large language models. We finally draw conclusions about the utilization of these models for counteracting the spreading of misinformation in online platforms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLoRA&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20302;&#31209;&#26356;&#26032;&#26469;&#35757;&#32451;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#39044;&#35757;&#32451;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ReLoRA&#22312;&#19982;&#24120;&#35268;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30456;&#27604;&#30340;&#24615;&#33021;&#34920;&#29616;&#19978;&#30456;&#24403;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#27169;&#22411;&#36234;&#22823;&#30340;&#24773;&#20917;&#19979;&#25928;&#29575;&#36234;&#39640;&#65292;&#20026;&#39640;&#25928;&#35757;&#32451;&#21315;&#20159;&#32423;&#21442;&#25968;&#32593;&#32476;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05695</link><description>&lt;p&gt;
&#20197;&#19981;&#21516;&#26041;&#24335;&#22534;&#21472;&#26356;&#22810;&#23618;&#65306;&#36890;&#36807;&#20302;&#31209;&#26356;&#26032;&#36827;&#34892;&#39640;&#31209;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Stack More Layers Differently: High-Rank Training Through Low-Rank Updates. (arXiv:2307.05695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLoRA&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20302;&#31209;&#26356;&#26032;&#26469;&#35757;&#32451;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#39044;&#35757;&#32451;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ReLoRA&#22312;&#19982;&#24120;&#35268;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30456;&#27604;&#30340;&#24615;&#33021;&#34920;&#29616;&#19978;&#30456;&#24403;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#27169;&#22411;&#36234;&#22823;&#30340;&#24773;&#20917;&#19979;&#25928;&#29575;&#36234;&#39640;&#65292;&#20026;&#39640;&#25928;&#35757;&#32451;&#21315;&#20159;&#32423;&#21442;&#25968;&#32593;&#32476;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#35268;&#27169;&#32593;&#32476;&#25317;&#26377;&#25968;&#30334;&#20159;&#20010;&#21442;&#25968;&#30340;&#35268;&#27169;&#24050;&#32463;&#21344;&#20027;&#23548;&#22320;&#20301;&#24182;&#19988;&#25928;&#26524;&#26174;&#33879;&#65292;&#20294;&#23545;&#20110;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#35757;&#32451;&#24517;&#35201;&#24615;&#20173;&#28982;&#32570;&#20047;&#28165;&#26224;&#30340;&#29702;&#35299;&#65292;&#32780;&#26367;&#20195;&#26041;&#27861;&#19981;&#19968;&#23450;&#33021;&#22815;&#38477;&#20302;&#35757;&#32451;&#39640;&#24615;&#33021;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#20316;&#20026;&#35757;&#32451;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;ReLoRA&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#20302;&#31209;&#26356;&#26032;&#26469;&#35757;&#32451;&#39640;&#31209;&#32593;&#32476;&#12290;&#25105;&#20204;&#23558;ReLoRA&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#65292;&#21442;&#25968;&#37327;&#39640;&#36798;350M&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#19982;&#24120;&#35268;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ReLoRA&#30340;&#25928;&#29575;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#25552;&#39640;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#39640;&#25928;&#35757;&#32451;&#21315;&#20159;&#32423;&#21442;&#25968;&#32593;&#32476;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#30340;&#28508;&#21147;&#21450;&#20854;&#23545;&#20110;&#32553;&#25918;&#23450;&#24459;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the dominance and effectiveness of scaling, resulting in large networks with hundreds of billions of parameters, the necessity to train overparametrized models remains poorly understood, and alternative approaches do not necessarily make it cheaper to train high-performance models. In this paper, we explore low-rank training techniques as an alternative approach to training large neural networks. We introduce a novel method called ReLoRA, which utilizes low-rank updates to train high-rank networks. We apply ReLoRA to pre-training transformer language models with up to 350M parameters and demonstrate comparable performance to regular neural network training. Furthermore, we observe that the efficiency of ReLoRA increases with model size, making it a promising approach for training multi-billion-parameter networks efficiently. Our findings shed light on the potential of low-rank training techniques and their implications for scaling laws.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21333;&#20154;&#34920;&#29616;&#25552;&#31034;&#65288;SPP&#65289;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#19982;&#22810;&#20010;&#35282;&#33394;&#36827;&#34892;&#22810;&#36718;&#33258;&#25105;&#21327;&#20316;&#65292;&#23558;&#21333;&#20010;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#35748;&#30693;&#21327;&#21516;&#32773;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05300</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37322;&#25918;&#35748;&#30693;&#21327;&#21516;&#65306;&#36890;&#36807;&#22810;&#20154;&#26684;&#33258;&#25105;&#21327;&#20316;&#23454;&#29616;&#20219;&#21153;&#35299;&#20915;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration. (arXiv:2307.05300v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21333;&#20154;&#34920;&#29616;&#25552;&#31034;&#65288;SPP&#65289;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#19982;&#22810;&#20010;&#35282;&#33394;&#36827;&#34892;&#22810;&#36718;&#33258;&#25105;&#21327;&#20316;&#65292;&#23558;&#21333;&#20010;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#35748;&#30693;&#21327;&#21516;&#32773;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26234;&#24935;&#20381;&#36182;&#20110;&#35748;&#30693;&#21327;&#21516;&#30340;&#27010;&#24565;&#65292;&#21363;&#22312;&#19981;&#21516;&#35748;&#30693;&#36807;&#31243;&#20043;&#38388;&#36827;&#34892;&#21327;&#20316;&#21644;&#20449;&#24687;&#25972;&#21512;&#65292;&#20197;&#33719;&#24471;&#27604;&#20010;&#20307;&#35748;&#30693;&#36807;&#31243;&#26356;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#36890;&#29992;&#20219;&#21153;&#35299;&#20915;&#20195;&#29702;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#38656;&#35201;&#20016;&#23500;&#39046;&#22495;&#30693;&#35782;&#21644;&#22797;&#26434;&#25512;&#29702;&#30340;&#20219;&#21153;&#19978;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21333;&#20154;&#34920;&#29616;&#25552;&#31034;&#65288;SPP&#65289;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#19982;&#22810;&#20010;&#35282;&#33394;&#36827;&#34892;&#22810;&#36718;&#33258;&#25105;&#21327;&#20316;&#65292;&#23558;&#21333;&#20010;LLM&#36716;&#21270;&#20026;&#35748;&#30693;&#21327;&#21516;&#32773;&#12290;&#35748;&#30693;&#21327;&#21516;&#32773;&#25351;&#30340;&#26159;&#19968;&#20010;&#26234;&#33021;&#20195;&#29702;&#65292;&#19982;&#22810;&#20010;&#26234;&#24935;&#21512;&#20316;&#65292;&#32467;&#21512;&#20182;&#20204;&#30340;&#20010;&#20307;&#20248;&#21183;&#21644;&#30693;&#35782;&#65292;&#20174;&#32780;&#22686;&#24378;&#22797;&#26434;&#20219;&#21153;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;&#36890;&#36807;&#26681;&#25454;&#20219;&#21153;&#36755;&#20837;&#21160;&#24577;&#35782;&#21035;&#21644;&#27169;&#25311;&#19981;&#21516;&#30340;&#35282;&#33394;&#65292;SPP&#37322;&#25918;&#20102;LLM&#20013;&#35748;&#30693;&#21327;&#21516;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human intelligence thrives on the concept of cognitive synergy, where collaboration and information integration among different cognitive processes yield superior outcomes compared to individual cognitive processes in isolation. Although Large Language Models (LLMs) have demonstrated promising performance as general task-solving agents, they still struggle with tasks that require intensive domain knowledge and complex reasoning. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist refers to an intelligent agent that collaborates with multiple minds, combining their individual strengths and knowledge, to enhance problem-solving and overall performance in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. We have discovered that assi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22810;&#27169;&#24577;&#21452;&#37325;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#65288;MDAT&#65289;&#27169;&#22411;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#24418;&#27880;&#24847;&#21644;&#20849;&#21516;&#20851;&#27880;&#26426;&#21046;&#26469;&#25429;&#25417;&#19981;&#21516;&#24773;&#24863;&#30340;&#36328;&#27169;&#24577;&#20381;&#36182;&#65292;&#24182;&#20351;&#29992;&#26368;&#23569;&#30340;&#30446;&#26631;&#35821;&#35328;&#25968;&#25454;&#23454;&#29616;&#25913;&#36827;&#30340;&#36328;&#35821;&#35328;&#24773;&#24863;&#35782;&#21035;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.13804</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21452;&#37325;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#23454;&#29616;&#36328;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Cross-Language Speech Emotion Recognition Using Multimodal Dual Attention Transformers. (arXiv:2306.13804v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13804
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22810;&#27169;&#24577;&#21452;&#37325;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#65288;MDAT&#65289;&#27169;&#22411;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#24418;&#27880;&#24847;&#21644;&#20849;&#21516;&#20851;&#27880;&#26426;&#21046;&#26469;&#25429;&#25417;&#19981;&#21516;&#24773;&#24863;&#30340;&#36328;&#27169;&#24577;&#20381;&#36182;&#65292;&#24182;&#20351;&#29992;&#26368;&#23569;&#30340;&#30446;&#26631;&#35821;&#35328;&#25968;&#25454;&#23454;&#29616;&#25913;&#36827;&#30340;&#36328;&#35821;&#35328;&#24773;&#24863;&#35782;&#21035;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#36827;&#23637;&#65292;&#20294;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#26080;&#27861;&#22312;&#36328;&#35821;&#35328;&#29615;&#22659;&#20013;&#23454;&#29616;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21452;&#37325;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#65288;MDAT&#65289;&#27169;&#22411;&#65292;&#20197;&#25913;&#36827;&#36328;&#35821;&#35328;SER&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#65292;&#24182;&#37197;&#22791;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#65292;&#21253;&#25324;&#22270;&#24418;&#27880;&#24847;&#21644;&#20849;&#21516;&#20851;&#27880;&#65292;&#20197;&#25429;&#33719;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#26368;&#23569;&#30340;&#30446;&#26631;&#35821;&#35328;&#25968;&#25454;&#23454;&#29616;&#25913;&#36827;&#30340;&#36328;&#35821;&#35328;SER&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#21033;&#29992;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#23618;&#36827;&#34892;&#39640;&#23618;&#29305;&#24449;&#34920;&#31034;&#65292;&#20197;&#25552;&#39640;&#24773;&#24863;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;MDAT&#22312;&#21508;&#20010;&#38454;&#27573;&#25191;&#34892;&#29305;&#24449;&#34920;&#31034;&#30340;&#32454;&#21270;&#65292;&#24182;&#20026;&#20998;&#31867;&#23618;&#25552;&#20379;&#24773;&#24863;&#26174;&#30528;&#29305;&#24449;&#12290;&#36825;&#31181;&#26032;&#39062;&#26041;&#27861;&#36824;&#30830;&#20445;&#20102;&#27169;&#24577;&#29305;&#23450;&#30340;&#24773;&#24863;&#20449;&#24687;&#30340;&#20445;&#23384;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#20132;&#21449;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent progress in speech emotion recognition (SER), state-of-the-art systems are unable to achieve improved performance in cross-language settings. In this paper, we propose a Multimodal Dual Attention Transformer (MDAT) model to improve cross-language SER. Our model utilises pre-trained models for multimodal feature extraction and is equipped with a dual attention mechanism including graph attention and co-attention to capture complex dependencies across different modalities and achieve improved cross-language SER results using minimal target language data. In addition, our model also exploits a transformer encoder layer for high-level feature representation to improve emotion classification accuracy. In this way, MDAT performs refinement of feature representation at various stages and provides emotional salient features to the classification layer. This novel approach also ensures the preservation of modality-specific emotional information while enhancing cross-modality 
&lt;/p&gt;</description></item><item><title>CHiME-7 DASR &#25361;&#25112;&#36187;&#26088;&#22312;&#36827;&#34892;&#22312;&#36828;&#22330;&#29615;&#22659;&#19979;&#22810;&#35774;&#22791;&#36828;&#31243;&#20250;&#35758;&#36716;&#24405;&#30340;&#40065;&#26834;&#35821;&#38899;&#35782;&#21035;&#65292;&#24182;&#22312;&#19977;&#31181;&#19981;&#21516;&#22330;&#26223;&#20013;&#35780;&#20272;&#31995;&#32479;&#24615;&#33021;&#12290;&#21442;&#19982;&#32773;&#21487;&#20197;&#20351;&#29992;&#24320;&#28304;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.13734</link><description>&lt;p&gt;
CHiME-7 DASR &#25361;&#25112;&#36187;&#65306;&#22810;&#35774;&#22791;&#36828;&#31243;&#20250;&#35758;&#36716;&#24405;&#22312;&#22810;&#26679;&#21270;&#29615;&#22659;&#20013;
&lt;/p&gt;
&lt;p&gt;
The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios. (arXiv:2306.13734v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13734
&lt;/p&gt;
&lt;p&gt;
CHiME-7 DASR &#25361;&#25112;&#36187;&#26088;&#22312;&#36827;&#34892;&#22312;&#36828;&#22330;&#29615;&#22659;&#19979;&#22810;&#35774;&#22791;&#36828;&#31243;&#20250;&#35758;&#36716;&#24405;&#30340;&#40065;&#26834;&#35821;&#38899;&#35782;&#21035;&#65292;&#24182;&#22312;&#19977;&#31181;&#19981;&#21516;&#22330;&#26223;&#20013;&#35780;&#20272;&#31995;&#32479;&#24615;&#33021;&#12290;&#21442;&#19982;&#32773;&#21487;&#20197;&#20351;&#29992;&#24320;&#28304;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CHiME &#25361;&#25112;&#36187;&#22312;&#40065;&#26834;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#24320;&#21457;&#21644;&#35780;&#20272;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;&#31532;&#19971;&#23626; CHiME &#25361;&#25112;&#36187;&#20013;&#24341;&#20837;&#20102; CHiME-7 &#36828;&#31243;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035; (DASR) &#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#21253;&#25324;&#22312;&#36828;&#22330;&#29615;&#22659;&#19979;&#20351;&#29992;&#22810;&#20010;&#12289;&#21487;&#33021;&#26159;&#24322;&#26500;&#30340;&#24405;&#38899;&#35774;&#22791;&#36827;&#34892;&#32852;&#21512; ASR &#21644;&#20154;&#22768;&#20998;&#31163;&#12290;&#19982;&#20043;&#21069;&#30340;&#25361;&#25112;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#35780;&#20272;3&#20010;&#19981;&#21516;&#30340;&#22330;&#26223;&#19979;&#30340;&#31995;&#32479;&#24615;&#33021;&#65306;CHiME-6&#12289;DiPCo &#21644; Mixer 6&#12290;&#30446;&#26631;&#26159;&#35753;&#21442;&#19982;&#32773;&#35774;&#35745;&#19968;&#20010;&#33021;&#22815;&#22312;&#19981;&#30693;&#36947;&#20808;&#39564;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#27178;&#36328;&#19981;&#21516;&#38453;&#21015;&#20960;&#20309;&#21644;&#29992;&#20363;&#30340;&#21333;&#20010;&#31995;&#32479;&#12290;&#21478;&#19968;&#20010;&#19982;&#20043;&#21069; CHiME &#19981;&#21516;&#30340;&#26159;&#65292;&#21442;&#19982;&#32773;&#21487;&#20197;&#20351;&#29992;&#24320;&#28304;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The CHiME challenges have played a significant role in the development and evaluation of robust speech recognition (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task comprises joint ASR and diarization in far-field settings with multiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse scenarios: CHiME-6, DiPCo, and Mixer 6. The goal is for participants to devise a single system that can generalize across different array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that participants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, motivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology agnostic and features multi-channel diarization, channel selection, guided source separat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#36328;&#26102;&#20195;&#25688;&#35201;&#20219;&#21153;&#65292;&#20351;&#29992;&#21382;&#21490;&#24187;&#24819;&#25991;&#26412;&#21644;&#32500;&#22522;&#30334;&#31185;&#25688;&#35201;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;CLCTS&#35821;&#26009;&#24211;&#65292;&#24182;&#30740;&#31350;&#20102;&#27969;&#34892;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#21450;&#20854;&#20013;&#38388;&#20219;&#21153;&#24494;&#35843;&#30340;&#26377;&#25928;&#24615;&#65307;&#21516;&#26102;&#36824;&#25506;&#35752;&#20102;ChatGPT&#22312;CLCTS&#20013;&#20316;&#20026;&#25688;&#35201;&#22120;&#21644;&#35780;&#20272;&#22120;&#30340;&#28508;&#21147;&#12290;&#26368;&#32456;&#21457;&#29616;&#20013;&#38388;&#20219;&#21153;&#24494;&#35843;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#20135;&#29983;&#20102;&#20013;&#31561;&#21040;&#24046;&#30340;&#25928;&#26524;&#65292;&#32780;ChatGPT&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20013;&#31561;&#21040;&#22909;&#30340;&#25688;&#35201;&#36136;&#37327;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.12916</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#36328;&#26102;&#20195;&#25688;&#35201;&#65306;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual Cross-temporal Summarization: Dataset, Models, Evaluation. (arXiv:2306.12916v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#36328;&#26102;&#20195;&#25688;&#35201;&#20219;&#21153;&#65292;&#20351;&#29992;&#21382;&#21490;&#24187;&#24819;&#25991;&#26412;&#21644;&#32500;&#22522;&#30334;&#31185;&#25688;&#35201;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;CLCTS&#35821;&#26009;&#24211;&#65292;&#24182;&#30740;&#31350;&#20102;&#27969;&#34892;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#21450;&#20854;&#20013;&#38388;&#20219;&#21153;&#24494;&#35843;&#30340;&#26377;&#25928;&#24615;&#65307;&#21516;&#26102;&#36824;&#25506;&#35752;&#20102;ChatGPT&#22312;CLCTS&#20013;&#20316;&#20026;&#25688;&#35201;&#22120;&#21644;&#35780;&#20272;&#22120;&#30340;&#28508;&#21147;&#12290;&#26368;&#32456;&#21457;&#29616;&#20013;&#38388;&#20219;&#21153;&#24494;&#35843;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#20135;&#29983;&#20102;&#20013;&#31561;&#21040;&#24046;&#30340;&#25928;&#26524;&#65292;&#32780;ChatGPT&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20013;&#31561;&#21040;&#22909;&#30340;&#25688;&#35201;&#36136;&#37327;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25688;&#35201;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#36328;&#35821;&#35328;&#36328;&#26102;&#20195;&#25688;&#35201;(CLCTS)&#26159;&#19968;&#20010;&#28508;&#21147;&#24040;&#22823;&#20294;&#40092;&#26377;&#30740;&#31350;&#30340;&#39046;&#22495;&#65292;&#23427;&#26377;&#21487;&#33021;&#25552;&#39640;&#36328;&#25991;&#21270;&#30340;&#21487;&#35775;&#38382;&#24615;&#12289;&#20449;&#24687;&#20849;&#20139;&#21644;&#29702;&#35299;&#12290;&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;CLCTS&#20219;&#21153;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#21019;&#24314;&#12289;&#24314;&#27169;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;CLCTS&#35821;&#26009;&#24211;&#65292;&#21033;&#29992;&#21382;&#21490;&#24187;&#24819;&#25991;&#26412;&#21644;&#33521;&#35821;&#12289;&#24503;&#35821;&#32500;&#22522;&#30334;&#31185;&#25688;&#35201;&#65292;&#24182;&#30740;&#31350;&#20102;&#27969;&#34892;&#30340;&#21464;&#21387;&#22120;&#31471;&#21040;&#31471;&#27169;&#22411;&#20197;&#21450;&#24102;&#26377;&#19981;&#21516;&#20013;&#38388;&#20219;&#21153;&#24494;&#35843;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;ChatGPT&#22312;CLCTS&#20013;&#20316;&#20026;&#25688;&#35201;&#22120;&#21644;&#35780;&#20272;&#22120;&#30340;&#28508;&#21147;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#20154;&#31867;&#12289;ChatGPT&#20197;&#21450;&#20960;&#20010;&#26368;&#36817;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#21457;&#29616;&#25105;&#20204;&#30340;&#20013;&#38388;&#20219;&#21153;&#24494;&#35843;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#20135;&#29983;&#20102;&#20174;&#24046;&#21040;&#20013;&#31561;&#30340;&#25688;&#35201;&#36136;&#37327;&#65307;ChatGPT&#20316;&#20026;&#25688;&#35201;&#22120;(&#27809;&#26377;&#20219;&#20309;&#24494;&#35843;)&#65292;&#25552;&#20379;&#20102;&#20013;&#31561;&#21040;&#22909;&#30340;&#25688;&#35201;&#36136;&#37327;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
While summarization has been extensively researched in natural language processing (NLP), cross-lingual cross-temporal summarization (CLCTS) is a largely unexplored area that has the potential to improve cross-cultural accessibility, information sharing, and understanding. This paper comprehensively addresses the CLCTS task, including dataset creation, modeling, and evaluation. We build the first CLCTS corpus, leveraging historical fictive texts and Wikipedia summaries in English and German, and examine the effectiveness of popular transformer end-to-end models with different intermediate task finetuning tasks. Additionally, we explore the potential of ChatGPT for CLCTS as a summarizer and an evaluator. Overall, we report evaluations from humans, ChatGPT, and several recent automatic evaluation metrics where we find our intermediate task finetuned end-to-end models generate bad to moderate quality summaries; ChatGPT as a summarizer (without any finetuning) provides moderate to good qua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#36753;&#36317;&#31163;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;RNN-T&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#22312;LibriSpeech&#30340;600M Conformer RNN-T&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.01789</link><description>&lt;p&gt;
&#22522;&#20110;&#32534;&#36753;&#36317;&#31163;&#30340;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;RNN-T&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Edit Distance based RL for RNNT decoding. (arXiv:2306.01789v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#36753;&#36317;&#31163;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;RNN-T&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#22312;LibriSpeech&#30340;600M Conformer RNN-T&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20854;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#20986;&#33394;&#30340;WER&#21644;&#25903;&#25345;&#26080;&#32541;&#27969;&#24335;&#20256;&#36755;&#21644;&#38271;&#31687;&#36716;&#24405;&#30340;&#33021;&#21147;&#65292;RNN-T&#30446;&#21069;&#34987;&#35748;&#20026;&#26159;ASR&#30340;&#24037;&#19994;&#26631;&#20934;&#12290; &#28982;&#32780;&#65292;&#23427;&#26368;&#22823;&#30340;&#32570;&#28857;&#22312;&#20110;&#35757;&#32451;&#21644;&#25512;&#29702;&#30446;&#26631;&#20043;&#38388;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#21270;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20043;&#38388;&#24046;&#36317;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32534;&#36753;&#36317;&#31163;RL (EDRL)&#26041;&#27861;&#22522;&#20110;&#32534;&#36753;&#36317;&#31163;&#35745;&#31639;&#22870;&#21169;&#65292;&#24182;&#22312;&#27599;&#20010;&#25805;&#20316;&#32423;&#21035;&#35757;&#32451;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#22312;LibriSpeech&#30340;600M Conformer RNN-T&#27169;&#22411;&#19978;&#33719;&#24471;&#20102;SoTA WERs&#12290;
&lt;/p&gt;
&lt;p&gt;
RNN-T is currently considered the industry standard in ASR due to its exceptional WERs in various benchmark tests and its ability to support seamless streaming and longform transcription. However, its biggest drawback lies in the significant discrepancy between its training and inference objectives. During training, RNN-T maximizes all alignment probabilities by teacher forcing, while during inference, it uses beam search which may not necessarily find the maximum probable alignment. Additionally, RNN-T's inability to experience mistakes during teacher forcing training makes it more problematic when a mistake occurs in inference. To address this issue, this paper proposes a Reinforcement Learning method that minimizes the gap between training and inference time. Our Edit Distance based RL (EDRL) approach computes rewards based on the edit distance, and trains the network at every action level. The proposed approach yielded SoTA WERs on LibriSpeech for the 600M Conformer RNN-T model.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20934;UNITE&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#35780;&#20272;&#65292;&#21253;&#21547;&#26469;&#33258;12&#20010;&#20197;&#19978;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12289;&#36229;&#36807;3.9K&#31181;&#27169;&#24335;&#30340;SQL&#26597;&#35810;&#21644;29K&#20010;&#25968;&#25454;&#24211;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;Codex&#22312;&#36328;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#35774;&#35745;&#30340;&#32534;&#30721;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#21487;&#26426;&#35835;&#30340;&#25968;&#25454;&#24211;&#30340;&#36136;&#37327;&#23545;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.16265</link><description>&lt;p&gt;
UNITE: &#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#35780;&#20272;&#30340;&#32479;&#19968;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
UNITE: A Unified Benchmark for Text-to-SQL Evaluation. (arXiv:2305.16265v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16265
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20934;UNITE&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#35780;&#20272;&#65292;&#21253;&#21547;&#26469;&#33258;12&#20010;&#20197;&#19978;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12289;&#36229;&#36807;3.9K&#31181;&#27169;&#24335;&#30340;SQL&#26597;&#35810;&#21644;29K&#20010;&#25968;&#25454;&#24211;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;Codex&#22312;&#36328;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#35774;&#35745;&#30340;&#32534;&#30721;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#21487;&#26426;&#35835;&#30340;&#25968;&#25454;&#24211;&#30340;&#36136;&#37327;&#23545;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#23454;&#29992;&#30340;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#24212;&#35813;&#21487;&#20197;&#24456;&#22909;&#22320;&#27010;&#25324;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12289;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#24211;&#27169;&#24335;&#21644;&#26032;&#39062;&#30340;SQL&#26597;&#35810;&#32467;&#26500;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20934;UNITE&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#35780;&#20272;&#12290;&#35813;&#22522;&#20934;&#30001;&#20844;&#24320;&#21487;&#29992;&#30340;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#32452;&#25104;&#65292;&#21253;&#21547;&#26469;&#33258;12&#20010;&#20197;&#19978;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12289;&#36229;&#36807;3.9K&#31181;&#27169;&#24335;&#30340;SQL&#26597;&#35810;&#21644;29K&#20010;&#25968;&#25454;&#24211;&#12290;&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;Spider&#22522;&#20934;&#30456;&#27604;&#65292;&#25105;&#20204;&#22686;&#21152;&#20102;&#32422;120K&#20010;&#39069;&#22806;&#30340;&#31034;&#20363;&#21644;&#19977;&#20493;&#30340;SQL&#27169;&#24335;&#65292;&#20363;&#22914;&#27604;&#36739;&#21644;&#24067;&#23572;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#26032;&#22522;&#20934;&#19978;&#23545;&#20845;&#31181;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#22120;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#65306;1&#65289;Codex&#22312;&#36328;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65307;2&#65289;&#29305;&#21035;&#35774;&#35745;&#30340;&#32534;&#30721;&#26041;&#27861;&#65288;&#20363;&#22914;&#32422;&#26463;&#26463;&#25628;&#32034;&#65289;&#21487;&#20197;&#25552;&#39640;&#22312;&#39046;&#22495;&#20869;&#22806;&#30340;&#24615;&#33021;&#65307;3&#65289;&#21487;&#26426;&#35835;&#30340;&#25968;&#25454;&#24211;&#30340;&#36136;&#37327;&#23545;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
A practical text-to-SQL system should generalize well on a wide variety of natural language questions, unseen database schemas, and novel SQL query structures. To comprehensively evaluate text-to-SQL systems, we introduce a \textbf{UNI}fied benchmark for \textbf{T}ext-to-SQL \textbf{E}valuation (UNITE). It is composed of publicly available text-to-SQL datasets, containing natural language questions from more than 12 domains, SQL queries from more than 3.9K patterns, and 29K databases. Compared to the widely used Spider benchmark \cite{yu-etal-2018-spider}, we introduce $\sim$120K additional examples and a threefold increase in SQL patterns, such as comparative and boolean questions. We conduct a systematic study of six state-of-the-art (SOTA) text-to-SQL parsers on our new benchmark and show that: 1) Codex performs surprisingly well on out-of-domain datasets; 2) specially designed decoding methods (e.g. constrained beam search) can improve performance for both in-domain and out-of-doma
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#20174;&#35821;&#35328;&#38544;&#21947;&#29983;&#25104;&#35270;&#35273;&#38544;&#21947;&#65292;&#24182;&#19988;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#20849;&#21019;&#20986;&#20855;&#26377;&#35270;&#35273;&#20914;&#20987;&#21147;&#21644;&#35821;&#20041;&#21547;&#20041;&#30340;&#38544;&#21947;&#12290;</title><link>http://arxiv.org/abs/2305.14724</link><description>&lt;p&gt;
&#25105;&#23547;&#35269;&#19968;&#20010;&#38544;&#21947;&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#20849;&#21019;&#35270;&#35273;&#38544;&#21947;
&lt;/p&gt;
&lt;p&gt;
I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors. (arXiv:2305.14724v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#20174;&#35821;&#35328;&#38544;&#21947;&#29983;&#25104;&#35270;&#35273;&#38544;&#21947;&#65292;&#24182;&#19988;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#20849;&#21019;&#20986;&#20855;&#26377;&#35270;&#35273;&#20914;&#20987;&#21147;&#21644;&#35821;&#20041;&#21547;&#20041;&#30340;&#38544;&#21947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38544;&#21947;&#26159;&#36890;&#36807;&#22270;&#20687;&#26469;&#35828;&#26381;&#25110;&#20256;&#36798;&#21019;&#24847;&#24819;&#27861;&#30340;&#24378;&#22823;&#20462;&#36766;&#25163;&#27861;&#12290;&#19982;&#35821;&#35328;&#38544;&#21947;&#31867;&#20284;&#65292;&#23427;&#20204;&#36890;&#36807;&#31526;&#21495;&#20027;&#20041;&#21644;&#31526;&#21495;&#30340;&#24182;&#32622;&#38544;&#21547;&#22320;&#20256;&#36798;&#21547;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#35821;&#35328;&#38544;&#21947;&#29983;&#25104;&#35270;&#35273;&#38544;&#21947;&#30340;&#26032;&#20219;&#21153;&#12290;&#36825;&#23545;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65288;&#22914;DALL $\cdot$ E 2&#65289;&#26469;&#35828;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#27169;&#25311;&#38544;&#21547;&#21547;&#20041;&#21644;&#32452;&#21512;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#20316;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#37319;&#29992;&#20197;&#8220;&#20018;&#32852;&#24605;&#32500;&#8221;&#20026;&#25552;&#31034;&#30340;Instruct GPT-3&#65288;davinci-002&#65289;&#29983;&#25104;&#20195;&#34920;&#35821;&#35328;&#38544;&#21947;&#30340;&#35270;&#35273;&#38416;&#36848;&#30340;&#25991;&#26412;&#65292;&#20854;&#20013;&#21253;&#21547;&#38544;&#21547;&#21547;&#20041;&#21644;&#30456;&#20851;&#23545;&#35937;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20316;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#36890;&#36807;&#20154;&#26426;&#21327;&#20316;&#26694;&#26550;&#65292;&#20154;&#20204;&#19982;LLM&#21644;&#34920;&#29616;&#26368;&#20339;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#65292;&#21019;&#24314;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#38544;&#21947;&#21644;&#23427;&#20204;&#30340;&#35270;&#35273;&#23545;&#24212;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LLMs&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#20316;&#21487;&#20197;&#20849;&#21516;&#21019;&#36896;&#20986;&#20855;&#26377;&#35270;&#35273;&#20914;&#20987;&#21147;&#21644;&#35821;&#20041;&#21547;&#20041;&#30340;&#38544;&#21947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual metaphors are powerful rhetorical devices used to persuade or communicate creative ideas through images. Similar to linguistic metaphors, they convey meaning implicitly through symbolism and juxtaposition of the symbols. We propose a new task of generating visual metaphors from linguistic metaphors. This is a challenging task for diffusion-based text-to-image models, such as DALL$\cdot$E 2, since it requires the ability to model implicit meaning and compositionality. We propose to solve the task through the collaboration between Large Language Models (LLMs) and Diffusion Models: Instruct GPT-3 (davinci-002) with Chain-of-Thought prompting generates text that represents a visual elaboration of the linguistic metaphor containing the implicit meaning and relevant objects, which is then used as input to the diffusion-based text-to-image models.Using a human-AI collaboration framework, where humans interact both with the LLM and the top-performing diffusion model, we create a high-qu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#35843;&#25972;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#27169;&#22411;&#65292;&#20849;&#21516;&#25552;&#21462;RCT&#25253;&#21578;&#20013;&#30340;&#24178;&#39044;&#12289;&#32467;&#26524;&#21644;&#21457;&#29616;&#20449;&#24687;&#65292;&#23454;&#29616;&#30456;&#24403;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.03642</link><description>&lt;p&gt;
LLM&#27169;&#22411;&#20849;&#21516;&#25552;&#21462;RCT&#25253;&#21578;&#20013;&#24178;&#39044;&#12289;&#32467;&#26524;&#21644;&#21457;&#29616;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Jointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs. (arXiv:2305.03642v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#35843;&#25972;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#27169;&#22411;&#65292;&#20849;&#21516;&#25552;&#21462;RCT&#25253;&#21578;&#20013;&#30340;&#24178;&#39044;&#12289;&#32467;&#26524;&#21644;&#21457;&#29616;&#20449;&#24687;&#65292;&#23454;&#29616;&#30456;&#24403;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#65288;RCT&#65289;&#30340;&#32467;&#26524;&#30830;&#23450;&#24178;&#39044;&#25514;&#26045;&#30340;&#30456;&#23545;&#26377;&#25928;&#24615;&#65292;&#36827;&#32780;&#25104;&#20026;&#22522;&#20110;&#35777;&#25454;&#30340;&#21307;&#30103;&#20445;&#20581;&#30340;&#20851;&#38190;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;RCT&#32467;&#26524;&#20197;&#65288;&#36890;&#24120;&#26159;&#38750;&#32467;&#26500;&#21270;&#30340;&#65289;&#33258;&#28982;&#35821;&#35328;&#25991;&#31456;&#30340;&#24418;&#24335;&#21576;&#29616;&#65292;&#25551;&#36848;&#35797;&#39564;&#30340;&#35774;&#35745;&#12289;&#25191;&#34892;&#21644;&#32467;&#26524;&#65307;&#20020;&#24202;&#21307;&#29983;&#24517;&#39035;&#20174;&#36825;&#20123;&#25991;&#31456;&#20013;&#25163;&#21160;&#25552;&#21462;&#26377;&#20851;&#25152;&#20851;&#27880;&#30340;&#24178;&#39044;&#25514;&#26045;&#21644;&#32467;&#26524;&#30340;&#21457;&#29616;&#12290;&#36825;&#31181;&#32321;&#29712;&#30340;&#25163;&#21160;&#36807;&#31243;&#20419;&#20351;&#20154;&#20204;&#21033;&#29992;&#65288;&#21322;&#65289;&#33258;&#21160;&#21270;&#30340;&#26041;&#24335;&#20174;&#35797;&#39564;&#25253;&#21578;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#35777;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;&#22522;&#20110;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#20020;&#24202;&#25688;&#35201;&#20013;&#20849;&#21516;&#25552;&#21462;&#24178;&#39044;&#25514;&#26045;&#12289;&#32467;&#26524;&#21644;&#27604;&#36739;&#22240;&#32032;&#65288;ICO&#20803;&#32032;&#65289;&#65292;&#24182;&#25512;&#26029;&#30456;&#20851;&#30340;&#32467;&#26524;&#12290;&#20154;&#24037;&#65288;&#19987;&#23478;&#65289;&#21644;&#33258;&#21160;&#35780;&#20272;&#34920;&#26126;&#65292;&#23558;&#35777;&#25454;&#25552;&#21462;&#26694;&#26550;&#20316;&#20026;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#65292;&#20026;&#27492;&#30446;&#30340;&#24494;&#35843;LLMs&#21487;&#20197;&#23454;&#29616;&#30456;&#24403;&#22823;&#30340;&#65288;&#32422;20&#20010;&#28857;&#65289;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Results from Randomized Controlled Trials (RCTs) establish the comparative effectiveness of interventions, and are in turn critical inputs for evidence-based care. However, results from RCTs are presented in (often unstructured) natural language articles describing the design, execution, and outcomes of trials; clinicians must manually extract findings pertaining to interventions and outcomes of interest from such articles. This onerous manual process has motivated work on (semi-)automating extraction of structured evidence from trial reports. In this work we propose and evaluate a text-to-text model built on instruction-tuned Large Language Models (LLMs) to jointly extract Interventions, Outcomes, and Comparators (ICO elements) from clinical abstracts, and infer the associated results reported. Manual (expert) and automated evaluations indicate that framing evidence extraction as a conditional generation task and fine-tuning LLMs for this purpose realizes considerable ($\sim$20 point 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#65292;&#25552;&#28860;&#20851;&#38190;&#20449;&#24687;&#29983;&#25104;&#31616;&#27905;&#30340;&#25688;&#35201;&#65292;&#20998;&#20026;&#24494;&#35843;&#12289;&#22522;&#20110;&#29305;&#24449;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21253;&#25324;&#34701;&#21512;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21644;&#24320;&#21457;&#26356;&#36866;&#21512;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2304.08763</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25688;&#35201;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Biomedical Text Summarization with Pre-trained Language Model. (arXiv:2304.08763v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#65292;&#25552;&#28860;&#20851;&#38190;&#20449;&#24687;&#29983;&#25104;&#31616;&#27905;&#30340;&#25688;&#35201;&#65292;&#20998;&#20026;&#24494;&#35843;&#12289;&#22522;&#20110;&#29305;&#24449;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21253;&#25324;&#34701;&#21512;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21644;&#24320;&#21457;&#26356;&#36866;&#21512;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#21644;&#30005;&#23376;&#30149;&#21382;&#31561;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#32473;&#20020;&#24202;&#21307;&#29983;&#21644;&#30740;&#31350;&#20154;&#21592;&#39640;&#25928;&#33719;&#21462;&#20020;&#24202;&#20449;&#24687;&#24102;&#26469;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#65292;&#26088;&#22312;&#20174;&#21333;&#20010;&#25110;&#22810;&#20010;&#29983;&#29289;&#21307;&#23398;&#25991;&#26723;&#20013;&#25552;&#28860;&#20851;&#38190;&#20449;&#24687;&#29983;&#25104;&#31616;&#27905;&#30340;&#25688;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24050;&#25104;&#20026;&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#20107;&#23454;&#26631;&#20934;&#65292;PLMs&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#20063;&#20026;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#24102;&#26469;&#26032;&#30340;&#21551;&#31034;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#24635;&#32467;&#20102;&#36817;&#26399;&#22522;&#20110;PLMs&#25506;&#32034;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25688;&#35201;&#30340;&#36827;&#23637;&#65292;&#24110;&#21161;&#29702;&#35299;&#26368;&#26032;&#30340;&#36827;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;&#25105;&#20204;&#26681;&#25454;&#20351;&#29992;PLMs&#30340;&#26041;&#24335;&#23545;&#22522;&#20110;PLMs&#30340;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#22522;&#20110;&#29305;&#24449;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#22914;&#34701;&#21512;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21644;&#24320;&#21457;&#26356;&#36866;&#21512;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exponential growth of biomedical texts such as biomedical literature and electronic health records (EHRs), provides a big challenge for clinicians and researchers to access clinical information efficiently. To address the problem, biomedical text summarization has been proposed to support clinical information retrieval and management, aiming at generating concise summaries that distill key information from single or multiple biomedical documents. In recent years, pre-trained language models (PLMs) have been the de facto standard of various natural language processing tasks in the general domain. Most recently, PLMs have been further investigated in the biomedical field and brought new insights into the biomedical text summarization task. In this paper, we systematically summarize recent advances that explore PLMs for biomedical text summarization, to help understand recent progress, challenges, and future directions. We categorize PLMs-based approaches according to how they utilize
&lt;/p&gt;</description></item><item><title>WHOOPS!&#26159;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;&#24120;&#35782;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20102;&#22270;&#20687;&#23383;&#24149;&#12289;&#36328;&#27169;&#24577;&#21305;&#37197;&#21644;&#35270;&#35273;&#38382;&#31572;&#31561;&#33509;&#24178;&#20010;&#20219;&#21153;&#65292;&#24341;&#20837;&#20102;&#35299;&#37322;&#29983;&#25104;&#20219;&#21153;&#65292;&#25361;&#25112;&#20102;AI&#27169;&#22411;&#35782;&#21035;&#21644;&#35299;&#37322;&#19981;&#21512;&#24120;&#35268;&#30340;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.07274</link><description>&lt;p&gt;
&#25171;&#30772;&#24120;&#35782;&#65306;WHOOPS&#65281;&#19968;&#20010;&#22522;&#20110;&#21512;&#25104;&#21644;&#32452;&#21512;&#22270;&#20687;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images. (arXiv:2303.07274v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07274
&lt;/p&gt;
&lt;p&gt;
WHOOPS!&#26159;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;&#24120;&#35782;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20102;&#22270;&#20687;&#23383;&#24149;&#12289;&#36328;&#27169;&#24577;&#21305;&#37197;&#21644;&#35270;&#35273;&#38382;&#31572;&#31561;&#33509;&#24178;&#20010;&#20219;&#21153;&#65292;&#24341;&#20837;&#20102;&#35299;&#37322;&#29983;&#25104;&#20219;&#21153;&#65292;&#25361;&#25112;&#20102;AI&#27169;&#22411;&#35782;&#21035;&#21644;&#35299;&#37322;&#19981;&#21512;&#24120;&#35268;&#30340;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22855;&#24618;&#12289;&#24322;&#24120;&#21644;&#31070;&#31192;&#30340;&#22270;&#20687;&#20250;&#24341;&#36215;&#35266;&#23519;&#32773;&#30340;&#22909;&#22855;&#24515;&#65292;&#22240;&#20026;&#23427;&#20204;&#25361;&#25112;&#20102;&#24120;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;WHOOPS&#65281;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;&#24120;&#35782;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;&#35774;&#35745;&#24072;&#20351;&#29992;Midjourney&#31561;&#20844;&#20849;&#21487;&#29992;&#22270;&#20687;&#29983;&#25104;&#24037;&#20855;&#21046;&#20316;&#65292;&#24182;&#21253;&#21547;&#33509;&#24178;&#20010;&#20219;&#21153;&#12290;&#38500;&#20102;&#22270;&#20687;&#23383;&#24149;&#12289;&#36328;&#27169;&#24577;&#21305;&#37197;&#21644;&#35270;&#35273;&#38382;&#31572;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#22256;&#38590;&#30340;&#35299;&#37322;&#29983;&#25104;&#20219;&#21153;&#65292;&#20854;&#20013;&#27169;&#22411;&#24517;&#39035;&#35782;&#21035;&#24182;&#35299;&#37322;&#32473;&#23450;&#22270;&#20687;&#30340;&#24322;&#24120;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weird, unusual, and uncanny images pique the curiosity of observers because they challenge commonsense. For example, an image released during the 2022 world cup depicts the famous soccer stars Lionel Messi and Cristiano Ronaldo playing chess, which playfully violates our expectation that their competition should occur on the football field. Humans can easily recognize and interpret these unconventional images, but can AI models do the same? We introduce WHOOPS!, a new dataset and benchmark for visual commonsense. The dataset is comprised of purposefully commonsense-defying images created by designers using publicly-available image generation tools like Midjourney. We consider several tasks posed over the dataset. In addition to image captioning, cross-modal matching, and visual question answering, we introduce a difficult explanation generation task, where models must identify and explain why a given image is unusual. Our results show that state-of-the-art models such as GPT3 and BLIP2
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;ICL-D3IE&#65292;&#36825;&#20010;&#26694;&#26550;&#20351;LLM&#22312;&#19981;&#21516;&#31867;&#22411;&#28436;&#31034;&#19979;&#30340;DIE&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#25913;&#36827;&#24615;&#33021;&#30340;&#21453;&#39304;&#26426;&#21046;&#65292;&#21516;&#26102;&#28085;&#30422;&#20102;&#20301;&#32622;&#21644;&#26684;&#24335;&#26041;&#38754;&#30340;&#28436;&#31034;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2303.05063</link><description>&lt;p&gt;
ICL-D3IE&#65306;&#19978;&#19979;&#25991;&#23398;&#20064;+&#22810;&#26679;&#23637;&#31034;&#26356;&#26032;&#65292;&#29992;&#20110;&#25991;&#26723;&#20449;&#24687;&#25277;&#21462;&#65288;arXiv:2303.05063v2 [cs.CL] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction. (arXiv:2303.05063v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05063
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;ICL-D3IE&#65292;&#36825;&#20010;&#26694;&#26550;&#20351;LLM&#22312;&#19981;&#21516;&#31867;&#22411;&#28436;&#31034;&#19979;&#30340;DIE&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#25913;&#36827;&#24615;&#33021;&#30340;&#21453;&#39304;&#26426;&#21046;&#65292;&#21516;&#26102;&#28085;&#30422;&#20102;&#20301;&#32622;&#21644;&#26684;&#24335;&#26041;&#38754;&#30340;&#28436;&#31034;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-3&#21644;ChatGPT&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#25104;&#26524;&#65292;&#23588;&#20854;&#26159;&#24212;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21363;&#22522;&#20110;&#23569;&#37327;&#28436;&#31034;&#31034;&#20363;&#36827;&#34892;&#25512;&#29702;&#12290;&#23613;&#31649;&#22312;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23578;&#26410;&#36827;&#34892;&#30740;&#31350;&#20197;&#35780;&#20272;LLM&#22312;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#25191;&#34892;&#25991;&#26723;&#20449;&#24687;&#25277;&#21462;&#65288;DIE&#65289;&#30340;&#33021;&#21147;&#12290;&#24212;&#29992;LLM&#25191;&#34892;DIE&#23384;&#22312;&#20004;&#20010;&#25361;&#25112;&#65306;&#27169;&#24577;&#21644;&#20219;&#21153;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;ICL-D3IE&#65292;&#23427;&#20351;LLM&#33021;&#22815;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#28436;&#31034;&#31034;&#20363;&#25191;&#34892;DIE&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#38590;&#20197;&#35757;&#32451;&#30340;&#25991;&#26723;&#20013;&#25552;&#21462;&#26368;&#22256;&#38590;&#21644;&#26368;&#19981;&#21516;&#30340;&#29255;&#27573;&#20316;&#20026;&#28436;&#31034;&#31034;&#20363;&#65292;&#20197;&#20415;&#21463;&#30410;&#20110;&#25152;&#26377;&#27979;&#35797;&#23454;&#20363;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#25551;&#36848;&#20851;&#31995;&#30340;&#28436;&#31034;&#31034;&#20363;&#65292;&#20351;LLM&#33021;&#22815;&#29702;&#35299;&#20301;&#32622;&#20851;&#31995;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26684;&#24335;&#21270;&#28436;&#31034;&#31034;&#20363;&#65292;&#20197;&#26041;&#20415;&#25552;&#21462;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21453;&#39304;&#26426;&#21046;&#65292;&#26356;&#26032;&#20102;&#28436;&#31034;&#31034;&#20363;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;ICL-D3IE&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as GPT-3 and ChatGPT, have demonstrated remarkable results in various natural language processing (NLP) tasks with in-context learning, which involves inference based on a few demonstration examples. Despite their successes in NLP tasks, no investigation has been conducted to assess the ability of LLMs to perform document information extraction (DIE) using in-context learning. Applying LLMs to DIE poses two challenges: the modality and task gap. To this end, we propose a simple but effective in-context learning framework called ICL-D3IE, which enables LLMs to perform DIE with different types of demonstration examples. Specifically, we extract the most difficult and distinct segments from hard training documents as hard demonstrations for benefiting all test instances. We design demonstrations describing relationships that enable LLMs to understand positional relationships. We introduce formatting demonstrations for easy answer extraction. Additionally
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23384;&#22312;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#39044;&#27979;&#26469;&#36741;&#21161;&#20154;&#31867;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#31867;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2302.04391</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#26032;&#26631;&#31614;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23384;&#22312;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#39044;&#27979;&#26469;&#36741;&#21161;&#20154;&#31867;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#31867;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#23384;&#22312;&#22122;&#22768;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#24320;&#21457;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;90&#20998;&#20197;&#19978;&#30340;&#25104;&#32489;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#25214;&#20986;&#22122;&#22768;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#37319;&#29992;&#27169;&#22411;&#39044;&#27979;&#20316;&#20026;&#20154;&#31867;&#26631;&#35760;&#30340;&#21442;&#32771;&#26469;&#37325;&#26032;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24819;&#27861;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#24207;&#21015;&#26631;&#35760;&#12289;&#29289;&#20307;&#26816;&#27979;&#12289;&#24207;&#21015;&#29983;&#25104;&#12289;&#28857;&#20987;&#29575;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#21644;&#20154;&#31867;&#35780;&#20272;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In industry deep learning application, our manually labeled data has a certain number of noisy data. To solve this problem and achieve more than 90 score in dev dataset, we present a simple method to find the noisy data and re-label the noisy data by human, given the model predictions as references in human labeling. In this paper, we illustrate our idea for a broad set of deep learning tasks, includes classification, sequence tagging, object detection, sequence generation, click-through rate prediction. The experimental results and human evaluation results verify our idea.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20116;&#20010;&#26032;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#12290;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#65292;&#25506;&#32034;&#20102;&#20854;&#22312;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.00552</link><description>&lt;p&gt;
&#22312;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20013;&#26377;&#25928;&#22320;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
An Effective Employment of Contrastive Learning in Multi-label Text Classification. (arXiv:2212.00552v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20116;&#20010;&#26032;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#12290;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#65292;&#25506;&#32034;&#20102;&#20854;&#22312;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#23578;&#24453;&#25506;&#32034;&#21644;&#20998;&#26512;&#12290;&#22914;&#20309;&#27491;&#30830;&#21512;&#29702;&#22320;&#26500;&#24314;&#27491;&#36127;&#26679;&#26412;&#26159;&#23545;&#27604;&#23398;&#20064;&#30340;&#26680;&#24515;&#25361;&#25112;&#65292;&#32780;&#22312;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#21457;&#29616;&#23545;&#27604;&#23545;&#35937;&#26356;&#21152;&#22256;&#38590;&#12290;&#20043;&#21069;&#25552;&#20986;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#24456;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20116;&#20010;&#26032;&#39062;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65292;&#21253;&#25324;&#20005;&#26684;&#23545;&#27604;&#25439;&#22833; (SCL)&#12289;&#20869;&#26631;&#31614;&#23545;&#27604;&#25439;&#22833; (ICL)&#12289;Jaccard&#30456;&#20284;&#24230;&#23545;&#27604;&#25439;&#22833;(JSCL)&#12289;Jaccard&#30456;&#20284;&#24230;&#27010;&#29575;&#23545;&#27604;&#25439;&#22833;(JSPCL)&#21644;&#36880;&#27493;&#26631;&#31614;&#23545;&#27604;&#25439;&#22833;(SLCL)&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25506;&#32034;&#20102;&#23545;&#27604;&#23398;&#20064;&#22312;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20026;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#37096;&#32626;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#25552;&#20379;&#20102;&#19968;&#22871;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The effectiveness of contrastive learning technology in natural language processing tasks is yet to be explored and analyzed. How to construct positive and negative samples correctly and reasonably is the core challenge of contrastive learning. It is even harder to discover contrastive objects in multi-label text classification tasks. There are very few contrastive losses proposed previously. In this paper, we investigate the problem from a different angle by proposing five novel contrastive losses for multi-label text classification tasks. These are Strict Contrastive Loss (SCL), Intra-label Contrastive Loss (ICL), Jaccard Similarity Contrastive Loss (JSCL), Jaccard Similarity Probability Contrastive Loss (JSPCL), and Stepwise Label Contrastive Loss (SLCL). We explore the effectiveness of contrastive learning for multi-label text classification tasks by the employment of these novel losses and provide a set of baseline models for deploying contrastive learning techniques on specific t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39044;&#26399;&#23545;&#38405;&#35835;&#26102;&#38388;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#27604;&#36739;&#24778;&#22855;&#24230;&#21644;&#19978;&#19979;&#25991;&#29109;&#23545;&#38405;&#35835;&#26102;&#38388;&#30340;&#39044;&#27979;&#25928;&#26524;&#65292;&#21457;&#29616;&#19978;&#19979;&#25991;&#29109;&#23545;&#35789;&#30340;&#38405;&#35835;&#26102;&#38388;&#24433;&#21709;&#26377;&#23454;&#36136;&#24615;&#30340;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2211.14301</link><description>&lt;p&gt;
&#20851;&#20110;&#39044;&#26399;&#23545;&#38405;&#35835;&#26102;&#38388;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the Effect of Anticipation on Reading Times. (arXiv:2211.14301v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14301
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39044;&#26399;&#23545;&#38405;&#35835;&#26102;&#38388;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#27604;&#36739;&#24778;&#22855;&#24230;&#21644;&#19978;&#19979;&#25991;&#29109;&#23545;&#38405;&#35835;&#26102;&#38388;&#30340;&#39044;&#27979;&#25928;&#26524;&#65292;&#21457;&#29616;&#19978;&#19979;&#25991;&#29109;&#23545;&#35789;&#30340;&#38405;&#35835;&#26102;&#38388;&#24433;&#21709;&#26377;&#23454;&#36136;&#24615;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20108;&#21313;&#24180;&#20013;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#36739;&#38590;&#39044;&#27979;&#65288;&#21363;&#26356;&#39640;&#30340;&#24778;&#22855;&#24230;&#65289;&#30340;&#21333;&#35789;&#38656;&#35201;&#26356;&#38271;&#30340;&#38405;&#35835;&#26102;&#38388;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#36825;&#20123;&#30740;&#31350;&#26263;&#31034;&#20102;&#38405;&#35835;&#36807;&#31243;&#26159;&#23436;&#20840;&#21709;&#24212;&#24335;&#30340;&#65306;&#35835;&#32773;&#35266;&#23519;&#19968;&#20010;&#26032;&#21333;&#35789;&#65292;&#24182;&#26681;&#25454;&#38656;&#35201;&#20998;&#37197;&#26102;&#38388;&#26469;&#22788;&#29702;&#23427;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20808;&#21069;&#30340;&#32467;&#26524;&#20063;&#19982;&#38405;&#35835;&#36807;&#31243;&#33267;&#23569;&#37096;&#20998;&#22320;&#20855;&#26377;&#39044;&#26399;&#24615;&#26159;&#19968;&#33268;&#30340;&#65306;&#35835;&#32773;&#21487;&#20197;&#23545;&#26410;&#26469;&#30340;&#21333;&#35789;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#26681;&#25454;&#20182;&#20204;&#30340;&#26399;&#26395;&#20998;&#37197;&#26102;&#38388;&#26469;&#22788;&#29702;&#23427;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#39044;&#26399;&#24615;&#25805;&#20316;&#21270;&#20026;&#19968;&#20010;&#21333;&#35789;&#30340;&#19978;&#19979;&#25991;&#29109;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#24778;&#22855;&#31243;&#24230;&#21644;&#19978;&#19979;&#25991;&#29109;&#23545;&#22235;&#20010;&#33258;&#28982;&#38405;&#35835;&#25968;&#25454;&#38598;&#65288;&#20004;&#20010;&#33258;&#23450;&#33410;&#22863;&#21644;&#20004;&#20010;&#30524;&#21160;&#36861;&#36394;&#65289;&#20013;&#30340;&#38405;&#35835;&#26102;&#38388;&#36827;&#34892;&#39044;&#27979;&#30340;&#25928;&#26524;&#26469;&#35780;&#20272;&#39044;&#26399;&#23545;&#38405;&#35835;&#30340;&#24433;&#21709;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#36328;&#25968;&#25454;&#38598;&#21644;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#19978;&#19979;&#25991;&#29109;&#23545;&#20110;&#35789;&#30340;&#38405;&#35835;&#26102;&#38388;&#65288;RT&#65289;&#30340;&#24433;&#21709;&#20855;&#26377;&#23454;&#36136;&#24615;&#30340;&#35777;&#25454;&#65306;&#20107;&#23454;&#19978;&#65292;&#26377;&#26102;&#19978;&#19979;&#25991;&#29109;&#27604;&#24778;&#22855;&#24230;&#26356;&#22909;&#22320;&#39044;&#27979;&#38405;&#35835;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past two decades, numerous studies have demonstrated how less predictable (i.e., higher surprisal) words take more time to read. In general, these studies have implicitly assumed the reading process is purely responsive: Readers observe a new word and allocate time to process it as required. We argue that prior results are also compatible with a reading process that is at least partially anticipatory: Readers could make predictions about a future word and allocate time to process it based on their expectation. In this work, we operationalize this anticipation as a word's contextual entropy. We assess the effect of anticipation on reading by comparing how well surprisal and contextual entropy predict reading times on four naturalistic reading datasets: two self-paced and two eye-tracking. Experimentally, across datasets and analyses, we find substantial evidence for effects of contextual entropy over surprisal on a word's reading time (RT): in fact, entropy is sometimes better 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25910;&#38598;&#36328;&#35821;&#35328;&#35805;&#35821;&#23545;&#30340;&#21327;&#35758;&#65292;&#24182;&#20844;&#24320;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;&#20197;&#25903;&#25345;&#26426;&#22120;&#23398;&#20064;&#36328;&#35821;&#35328;&#38901;&#24459;&#26144;&#23556;&#31561;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#20351;&#29992;&#35813;&#35821;&#26009;&#24211;&#12289;&#25193;&#23637;&#35813;&#35821;&#26009;&#24211;&#20197;&#21450;&#35774;&#35745;&#31867;&#20284;&#21452;&#35821;&#23545;&#35805;&#25968;&#25454;&#38598;&#30340;&#20154;&#22763;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2211.11584</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#23545;&#35805;&#30340;&#20877;&#29616;
&lt;/p&gt;
&lt;p&gt;
Dialogs Re-enacted Across Languages. (arXiv:2211.11584v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11584
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25910;&#38598;&#36328;&#35821;&#35328;&#35805;&#35821;&#23545;&#30340;&#21327;&#35758;&#65292;&#24182;&#20844;&#24320;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;&#20197;&#25903;&#25345;&#26426;&#22120;&#23398;&#20064;&#36328;&#35821;&#35328;&#38901;&#24459;&#26144;&#23556;&#31561;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#20351;&#29992;&#35813;&#35821;&#26009;&#24211;&#12289;&#25193;&#23637;&#35813;&#35821;&#26009;&#24211;&#20197;&#21450;&#35774;&#35745;&#31867;&#20284;&#21452;&#35821;&#23545;&#35805;&#25968;&#25454;&#38598;&#30340;&#20154;&#22763;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25903;&#25345;&#26426;&#22120;&#23398;&#20064;&#36328;&#35821;&#35328;&#30340;&#38901;&#24459;&#26144;&#23556;&#21644;&#20854;&#20182;&#25552;&#39640;&#35821;&#38899;-&#35821;&#38899;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#25910;&#38598;&#32039;&#23494;&#21305;&#37197;&#30340;&#35805;&#35821;&#23545;&#30340;&#21327;&#35758;&#65292;&#25551;&#36848;&#20102;&#25152;&#24471;&#21040;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;&#20854;&#20844;&#24320;&#21457;&#24067;&#30340;&#24773;&#20917;&#65292;&#24182;&#38468;&#19978;&#20102;&#19968;&#20123;&#35266;&#23519;&#21644;&#24605;&#32771;&#12290;&#26412;&#25253;&#21578;&#38754;&#21521;&#65306;&#20351;&#29992;&#35813;&#35821;&#26009;&#24211;&#30340;&#20154;&#22763;&#12289;&#25193;&#23637;&#35813;&#35821;&#26009;&#24211;&#30340;&#20154;&#22763;&#20197;&#21450;&#35774;&#35745;&#31867;&#20284;&#30340;&#21452;&#35821;&#23545;&#35805;&#25968;&#25454;&#38598;&#30340;&#20154;&#22763;&#12290;
&lt;/p&gt;
&lt;p&gt;
To support machine learning of cross-language prosodic mappings and other ways to improve speech-to-speech translation, we present a protocol for collecting closely matched pairs of utterances across languages, a description of the resulting data collection and its public release, and some observations and musings. This report is intended for: people using this corpus, people extending this corpus, and people designing similar collections of bilingual dialog data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;Transformer&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#19968;&#32452;&#22270;&#20687;&#25551;&#36848;&#20026;&#19968;&#20010;&#25925;&#20107;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;ViT&#25552;&#21462;&#36755;&#20837;&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#21452;&#21521;LSTM&#25429;&#25417;&#22270;&#20687;&#34917;&#19969;&#30340;&#36807;&#21435;&#21644;&#26410;&#26469;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#21152;&#26435;&#35745;&#31639;&#24471;&#21040;&#26368;&#32456;&#30340;&#25925;&#20107;&#25551;&#36848;&#21521;&#37327;&#12290;</title><link>http://arxiv.org/abs/2210.02762</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;Transformer&#30340;&#27169;&#22411;&#29992;&#20110;&#23558;&#19968;&#32452;&#22270;&#20687;&#25551;&#36848;&#20026;&#19968;&#20010;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Vision Transformer Based Model for Describing a Set of Images as a Story. (arXiv:2210.02762v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;Transformer&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#19968;&#32452;&#22270;&#20687;&#25551;&#36848;&#20026;&#19968;&#20010;&#25925;&#20107;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;ViT&#25552;&#21462;&#36755;&#20837;&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#21452;&#21521;LSTM&#25429;&#25417;&#22270;&#20687;&#34917;&#19969;&#30340;&#36807;&#21435;&#21644;&#26410;&#26469;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#21152;&#26435;&#35745;&#31639;&#24471;&#21040;&#26368;&#32456;&#30340;&#25925;&#20107;&#25551;&#36848;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25925;&#20107;&#35762;&#36848;&#26159;&#23558;&#19968;&#32452;&#22270;&#20687;&#24418;&#25104;&#22810;&#21477;&#25925;&#20107;&#30340;&#36807;&#31243;&#12290;&#24688;&#24403;&#22320;&#21253;&#21547;&#36755;&#20837;&#22270;&#20687;&#20869;&#25429;&#25417;&#21040;&#30340;&#35270;&#35273;&#21464;&#21270;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#26159;&#35270;&#35273;&#25925;&#20107;&#35762;&#36848;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#26041;&#38754;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#30001;&#19968;&#32452;&#22270;&#20687;&#24320;&#21457;&#30340;&#25925;&#20107;&#32463;&#24120;&#32570;&#20047;&#20957;&#32858;&#21147;&#12289;&#30456;&#20851;&#24615;&#21644;&#35821;&#20041;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35270;&#35273;Transformer&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#19968;&#32452;&#22270;&#20687;&#25551;&#36848;&#20026;&#19968;&#20010;&#25925;&#20107;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#25552;&#21462;&#36755;&#20837;&#22270;&#20687;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#39318;&#20808;&#65292;&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#25104;16X16&#30340;&#34917;&#19969;&#65292;&#24182;&#25414;&#32465;&#21040;&#25153;&#24179;&#21270;&#34917;&#19969;&#30340;&#32447;&#24615;&#25237;&#24433;&#20013;&#12290;&#20174;&#21333;&#20010;&#22270;&#20687;&#21040;&#22810;&#20010;&#22270;&#20687;&#34917;&#19969;&#30340;&#36716;&#25442;&#25429;&#25417;&#21040;&#20102;&#36755;&#20837;&#35270;&#35273;&#27169;&#24335;&#30340;&#35270;&#35273;&#22810;&#26679;&#24615;&#12290;&#36825;&#20123;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#21452;&#21521;LSTM&#65292;&#23427;&#26159;&#24207;&#21015;&#32534;&#30721;&#22120;&#30340;&#19968;&#37096;&#20998;&#12290;&#36825;&#26679;&#21487;&#20197;&#25429;&#25417;&#21040;&#25152;&#26377;&#22270;&#20687;&#34917;&#19969;&#30340;&#36807;&#21435;&#21644;&#26410;&#26469;&#22270;&#20687;&#19978;&#19979;&#25991;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#21152;&#26435;&#35745;&#31639;&#22270;&#20687;&#34917;&#19969;&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#20197;&#20135;&#29983;&#26368;&#32456;&#30340;&#25925;&#20107;&#25551;&#36848;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Story-Telling is the process of forming a multi-sentence story from a set of images. Appropriately including visual variation and contextual information captured inside the input images is one of the most challenging aspects of visual storytelling. Consequently, stories developed from a set of images often lack cohesiveness, relevance, and semantic relationship. In this paper, we propose a novel Vision Transformer Based Model for describing a set of images as a story. The proposed method extracts the distinct features of the input images using a Vision Transformer (ViT). Firstly, input images are divided into 16X16 patches and bundled into a linear projection of flattened patches. The transformation from a single image to multiple image patches captures the visual variety of the input visual patterns. These features are used as input to a Bidirectional-LSTM which is part of the sequence encoder. This captures the past and future image context of all image patches. Then, an atten
&lt;/p&gt;</description></item><item><title>CodeQueries&#26159;&#19968;&#20010;&#20851;&#20110;Python&#20195;&#30721;&#35821;&#20041;&#26597;&#35810;&#30340;&#25968;&#25454;&#38598;&#65292;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#23427;&#20855;&#26377;&#25991;&#20214;&#32423;&#21035;&#30340;&#19978;&#19979;&#25991;&#21644;&#20195;&#30721;&#27573;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2209.08372</link><description>&lt;p&gt;
CodeQueries&#65306;&#19968;&#20010;&#20851;&#20110;&#20195;&#30721;&#35821;&#20041;&#26597;&#35810;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CodeQueries: A Dataset of Semantic Queries over Code. (arXiv:2209.08372v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08372
&lt;/p&gt;
&lt;p&gt;
CodeQueries&#26159;&#19968;&#20010;&#20851;&#20110;Python&#20195;&#30721;&#35821;&#20041;&#26597;&#35810;&#30340;&#25968;&#25454;&#38598;&#65292;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#23427;&#20855;&#26377;&#25991;&#20214;&#32423;&#21035;&#30340;&#19978;&#19979;&#25991;&#21644;&#20195;&#30721;&#27573;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20154;&#21592;&#24120;&#24120;&#23545;&#20182;&#20204;&#27491;&#22312;&#25805;&#20316;&#30340;&#20195;&#30721;&#30340;&#35821;&#20041;&#26041;&#38754;&#26377;&#30097;&#38382;&#65292;&#20363;&#22914;&#65292;&#8220;&#26159;&#21542;&#26377;&#19968;&#20010;&#31867;&#30340;&#29238;&#31867;&#22768;&#26126;&#20102;&#19968;&#20010;&#20914;&#31361;&#30340;&#23646;&#24615;&#65311;&#8221;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#38656;&#35201;&#29702;&#35299;&#20195;&#30721;&#30340;&#35821;&#20041;&#65292;&#27604;&#22914;&#31867;&#30340;&#23646;&#24615;&#21644;&#32487;&#25215;&#20851;&#31995;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#38382;&#39064;&#65292;&#31572;&#26696;&#24212;&#35813;&#26631;&#35782;&#20986;&#26500;&#25104;&#31572;&#26696;&#30340;&#20195;&#30721;&#27573;&#65288;&#20363;&#22914;&#65292;&#23376;&#31867;&#30340;&#22768;&#26126;&#65289;&#20197;&#21450;&#25903;&#25345;&#30340;&#20107;&#23454;&#65288;&#20363;&#22914;&#65292;&#20914;&#31361;&#23646;&#24615;&#30340;&#23450;&#20041;&#65289;&#12290;&#29616;&#26377;&#30340;&#20851;&#20110;&#20195;&#30721;&#38382;&#39064;&#22238;&#31572;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#26159;&#25110;&#21542;&#30340;&#38382;&#39064;&#25110;&#26041;&#27861;&#32423;&#21035;&#30340;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#36129;&#29486;&#20102;&#19968;&#20010;&#21629;&#21517;&#20026;CodeQueries&#30340;Python&#20195;&#30721;&#30340;&#35821;&#20041;&#26597;&#35810;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;CodeQueries&#30340;&#26597;&#35810;&#26159;&#20851;&#20110;&#20195;&#30721;&#35821;&#20041;&#30340;&#65292;&#19978;&#19979;&#25991;&#26159;&#25991;&#20214;&#32423;&#21035;&#30340;&#65292;&#31572;&#26696;&#26159;&#20195;&#30721;&#27573;&#12290;&#25105;&#20204;&#26681;&#25454;&#19968;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#38745;&#24577;&#20998;&#26512;&#24037;&#20855;CodeQL&#25903;&#25345;&#30340;&#26597;&#35810;&#26469;&#31579;&#36873;&#21644;&#25972;&#29702;&#25968;&#25454;&#38598;&#65292;&#24182;&#21253;&#21547;&#27491;&#36127;&#20363;&#21644;&#38656;&#35201;&#21333;&#36339;&#38382;&#39064;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developers often have questions about semantic aspects of code they are working on, e.g., "Is there a class whose parent classes declare a conflicting attribute?". Answering them requires understanding code semantics such as attributes and inheritance relation of classes. An answer to such a question should identify code spans constituting the answer (e.g., the declaration of the subclass) as well as supporting facts (e.g., the definitions of the conflicting attributes). The existing work on question-answering over code has considered yes/no questions or method-level context. We contribute a labeled dataset, called CodeQueries, of semantic queries over Python code. Compared to the existing datasets, in CodeQueries, the queries are about code semantics, the context is file level and the answers are code spans. We curate the dataset based on queries supported by a widely-used static analysis tool, CodeQL, and include both positive and negative examples, and queries requiring single-hop a
&lt;/p&gt;</description></item></channel></rss>