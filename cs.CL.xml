<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#30456;&#23545;&#21453;&#39304;&#26469;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34892;&#20026;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20248;&#21270;&#27604;&#36739;&#25439;&#22833;&#35757;&#32451;&#30340;&#22870;&#21169;&#26102;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;PPPO&#65289;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#31639;&#27861;&#35774;&#35745;&#21644;&#20989;&#25968;&#36924;&#36817;&#12290;</title><link>http://arxiv.org/abs/2310.00212</link><description>&lt;p&gt;
&#20004;&#20004;&#37051;&#36817;&#31574;&#30053;&#20248;&#21270;: &#21033;&#29992;&#30456;&#23545;&#21453;&#39304;&#36827;&#34892;LLM&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment. (arXiv:2310.00212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00212
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#30456;&#23545;&#21453;&#39304;&#26469;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34892;&#20026;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20248;&#21270;&#27604;&#36739;&#25439;&#22833;&#35757;&#32451;&#30340;&#22870;&#21169;&#26102;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;PPPO&#65289;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#31639;&#27861;&#35774;&#35745;&#21644;&#20989;&#25968;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#39044;&#20808;&#35757;&#32451;&#26469;&#33719;&#21462;&#24191;&#27867;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25509;&#35302;&#21040;&#20302;&#36136;&#37327;&#25968;&#25454;&#65292;LLMs&#21487;&#33021;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#19981;&#19968;&#33268;&#30340;&#26377;&#23475;&#34892;&#20026;&#12290;&#24341;&#23548;LLMs&#26397;&#30528;&#26377;&#30410;&#34892;&#20026;&#26041;&#21521;&#21457;&#23637;&#30340;&#20027;&#23548;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#20854;&#20013;Proximal Policy Optimization&#65288;PPO&#65289;&#26159;&#40664;&#35748;&#30340;RL&#20248;&#21270;&#22120;&#12290;&#23613;&#31649;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;PPO&#22312;&#20248;&#21270;&#22522;&#20110;&#27604;&#36739;&#25439;&#22833;&#35757;&#32451;&#30340;&#22870;&#21169;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20027;&#35201;&#38382;&#39064;&#26159;&#65292;&#30001;&#20110;&#38656;&#35201;&#26657;&#20934;&#22870;&#21169;&#23610;&#24230;&#65292;PPO&#23545;&#20110;&#21253;&#21547;&#30456;&#21516;&#20559;&#22909;&#20449;&#24687;&#30340;&#31561;&#20215;&#22870;&#21169;&#20989;&#25968;&#19981;&#20855;&#22791;&#19981;&#21464;&#24615;&#12290;&#27492;&#22806;&#65292;&#19982;&#22522;&#20110;&#36712;&#36857;&#30340;&#20248;&#21270;&#30456;&#27604;&#65292;PPO&#23545;&#20110;&#22522;&#20110;&#20196;&#29260;&#30340;&#26356;&#26032;&#30340;&#38656;&#27714;&#24341;&#20837;&#20102;&#20989;&#25968;&#36924;&#36817;&#21644;&#31639;&#27861;&#35774;&#35745;&#26041;&#38754;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#30456;&#23545;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;Pairwise Proximal Policy Optimization&#65288;PPPO&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) can acquire extensive world knowledge through pre-training on large corpora. However, due to exposure to low-quality data, LLMs may exhibit harmful behavior without aligning with human values. The dominant approach for steering LLMs towards beneficial behavior involves Reinforcement Learning with Human Feedback (RLHF), with Proximal Policy Optimization (PPO) serving as the default RL optimizer. Despite its effectiveness, PPO has limitations when optimizing rewards trained from comparison-based loss. Primarily, PPO is not invariant to equivalent reward functions containing identical preference information due to the need to calibrate the reward scale. Additionally, PPO's necessity for token-wise updates introduces complexity in both function approximation and algorithm design compared to trajectory-wise optimization. This paper proposes a new framework, reinforcement learning with relative feedback, and a novel trajectory-wise policy gradient algorithm, Pair
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20027;&#35201;&#30740;&#31350;&#22312;&#32654;&#22269;&#25163;&#35821;&#20013;&#26816;&#27979;&#26410;&#30693;&#30340;&#22810;&#35789;&#34920;&#36798;&#12290;&#36890;&#36807;&#21033;&#29992;GloVe&#20013;&#30340;&#35789;&#23884;&#20837;&#65292;&#20316;&#32773;&#26500;&#24314;&#20102;&#20004;&#20010;&#31995;&#32479;&#26469;&#21028;&#26029;&#35789;&#27719;&#30340;&#35789;&#23884;&#20837;&#26159;&#21542;&#21487;&#20197;&#39044;&#27979;&#35789;&#27719;&#26159;&#21542;&#32452;&#25104;&#22810;&#35789;&#34920;&#36798;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35789;&#23884;&#20837;&#33021;&#22815;&#20934;&#30830;&#22320;&#26816;&#27979;&#38750;&#32452;&#21512;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00207</link><description>&lt;p&gt;
&#22312;&#32654;&#22269;&#25163;&#35821;&#20013;&#26816;&#27979;&#26410;&#30693;&#30340;&#22810;&#35789;&#34920;&#36798;
&lt;/p&gt;
&lt;p&gt;
Detecting Unseen Multiword Expressions in American Sign Language. (arXiv:2310.00207v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00207
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20027;&#35201;&#30740;&#31350;&#22312;&#32654;&#22269;&#25163;&#35821;&#20013;&#26816;&#27979;&#26410;&#30693;&#30340;&#22810;&#35789;&#34920;&#36798;&#12290;&#36890;&#36807;&#21033;&#29992;GloVe&#20013;&#30340;&#35789;&#23884;&#20837;&#65292;&#20316;&#32773;&#26500;&#24314;&#20102;&#20004;&#20010;&#31995;&#32479;&#26469;&#21028;&#26029;&#35789;&#27719;&#30340;&#35789;&#23884;&#20837;&#26159;&#21542;&#21487;&#20197;&#39044;&#27979;&#35789;&#27719;&#26159;&#21542;&#32452;&#25104;&#22810;&#35789;&#34920;&#36798;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35789;&#23884;&#20837;&#33021;&#22815;&#20934;&#30830;&#22320;&#26816;&#27979;&#38750;&#32452;&#21512;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35789;&#34920;&#36798;&#22312;&#35768;&#22810;&#32763;&#35793;&#20219;&#21153;&#20013;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#26368;&#32456;&#23558;&#22810;&#35789;&#34920;&#36798;&#26816;&#27979;&#31995;&#32479;&#24212;&#29992;&#20110;&#32654;&#22269;&#25163;&#35821;&#30340;&#32763;&#35793;&#65292;&#25105;&#20204;&#26500;&#24314;&#24182;&#27979;&#35797;&#20102;&#20004;&#20010;&#31995;&#32479;&#65292;&#23558;&#26469;&#33258;GloVe&#30340;&#35789;&#23884;&#20837;&#24212;&#29992;&#20110;&#30830;&#23450;&#35789;&#27719;&#30340;&#35789;&#23884;&#20837;&#26159;&#21542;&#21487;&#20197;&#29992;&#26469;&#39044;&#27979;&#36825;&#20123;&#35789;&#27719;&#26159;&#21542;&#32452;&#25104;&#22810;&#35789;&#34920;&#36798;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35789;&#23884;&#20837;&#25658;&#24102;&#30340;&#25968;&#25454;&#21487;&#20197;&#20197;&#30456;&#24403;&#39640;&#30340;&#20934;&#30830;&#29575;&#26816;&#27979;&#38750;&#32452;&#21512;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiword expressions present unique challenges in many translation tasks. In an attempt to ultimately apply a multiword expression detection system to the translation of American Sign Language, we built and tested two systems that apply word embeddings from GloVe to determine whether or not the word embeddings of lexemes can be used to predict whether or not those lexemes compose a multiword expression. It became apparent that word embeddings carry data that can detect non-compositionality with decent accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#23398;&#31185;&#38388;&#30340;&#23398;&#26415;&#25991;&#26723;&#35821;&#26009;&#24211;&#21644;&#20808;&#36827;&#30340;&#35821;&#35328;&#24314;&#27169;&#25216;&#26415;&#65292;&#21457;&#29616;&#20102;&#23398;&#31185;&#38388;&#30340;&#23454;&#29992;&#24046;&#24322;&#65292;&#37325;&#28857;&#22312;&#20110;&#25991;&#26723;&#32452;&#32455;&#21644;&#27969;&#31243;&#30340;&#35821;&#29992;&#26041;&#38754;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#23398;&#26415;&#31038;&#21306;&#22312;&#34920;&#36798;&#24037;&#20316;&#30340;&#26041;&#24335;&#19978;&#23384;&#22312;&#30456;&#20284;&#30340;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2310.00204</link><description>&lt;p&gt;
&#21457;&#29616;&#23398;&#31185;&#38388;&#30340;&#23454;&#29992;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Finding Pragmatic Differences Between Disciplines. (arXiv:2310.00204v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#23398;&#31185;&#38388;&#30340;&#23398;&#26415;&#25991;&#26723;&#35821;&#26009;&#24211;&#21644;&#20808;&#36827;&#30340;&#35821;&#35328;&#24314;&#27169;&#25216;&#26415;&#65292;&#21457;&#29616;&#20102;&#23398;&#31185;&#38388;&#30340;&#23454;&#29992;&#24046;&#24322;&#65292;&#37325;&#28857;&#22312;&#20110;&#25991;&#26723;&#32452;&#32455;&#21644;&#27969;&#31243;&#30340;&#35821;&#29992;&#26041;&#38754;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#23398;&#26415;&#31038;&#21306;&#22312;&#34920;&#36798;&#24037;&#20316;&#30340;&#26041;&#24335;&#19978;&#23384;&#22312;&#30456;&#20284;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#25991;&#26723;&#22312;&#20869;&#23481;&#65288;&#35821;&#20041;&#65289;&#21644;&#32467;&#26500;&#65288;&#35821;&#29992;&#65289;&#26041;&#38754;&#20855;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#12290;&#20197;&#24448;&#30340;&#23398;&#26415;&#25991;&#26723;&#29702;&#35299;&#30740;&#31350;&#20391;&#37325;&#20110;&#35821;&#20041;&#65292;&#36890;&#36807;&#25991;&#26723;&#25688;&#35201;&#21644;&#35821;&#26009;&#24211;&#20027;&#39064;&#24314;&#27169;&#65292;&#20294;&#24448;&#24448;&#24573;&#30053;&#20102;&#35821;&#29992;&#65292;&#22914;&#25991;&#26723;&#32452;&#32455;&#21644;&#27969;&#31243;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;19&#20010;&#23398;&#31185;&#30340;&#23398;&#26415;&#25991;&#26723;&#35821;&#26009;&#24211;&#21644;&#20808;&#36827;&#30340;&#35821;&#35328;&#24314;&#27169;&#25216;&#26415;&#65292;&#23398;&#20064;&#20102;&#19968;&#32452;&#22266;&#23450;&#30340;&#19982;&#23398;&#31185;&#26080;&#20851;&#30340;&#25991;&#26723;&#37096;&#20998;&#25551;&#36848;&#31526;&#65292;&#24182;&#23558;&#35821;&#26009;&#24211;&#35843;&#25972;&#21040;&#36825;&#20123;&#25551;&#36848;&#31526;&#20013;&#65288;&#20063;&#31216;&#20026;&#8220;&#24402;&#19968;&#21270;&#8221;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25991;&#26723;&#20013;&#36825;&#20123;&#25551;&#36848;&#31526;&#30340;&#20301;&#32622;&#21644;&#39034;&#24207;&#65292;&#20197;&#20102;&#35299;&#23398;&#31185;&#21644;&#32467;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#23398;&#31185;&#20869;&#30340;&#32467;&#26500;&#21407;&#22411;&#12289;&#21487;&#21464;&#24615;&#21644;&#23398;&#31185;&#38388;&#30340;&#27604;&#36739;&#65292;&#25903;&#25345;&#23398;&#26415;&#31038;&#21306;&#23613;&#31649;&#35268;&#27169;&#12289;&#22810;&#26679;&#24615;&#21644;&#24191;&#24230;&#21508;&#24322;&#65292;&#20294;&#22312;&#34920;&#36798;&#24037;&#20316;&#26041;&#38754;&#23384;&#22312;&#30528;&#30456;&#20284;&#30340;&#36884;&#24452;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#23398;&#26415;&#25991;&#26723;&#29702;&#35299;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scholarly documents have a great degree of variation, both in terms of content (semantics) and structure (pragmatics). Prior work in scholarly document understanding emphasizes semantics through document summarization and corpus topic modeling but tends to omit pragmatics such as document organization and flow. Using a corpus of scholarly documents across 19 disciplines and state-of-the-art language modeling techniques, we learn a fixed set of domain-agnostic descriptors for document sections and "retrofit" the corpus to these descriptors (also referred to as "normalization"). Then, we analyze the position and ordering of these descriptors across documents to understand the relationship between discipline and structure. We report within-discipline structural archetypes, variability, and between-discipline comparisons, supporting the hypothesis that scholarly communities, despite their size, diversity, and breadth, share similar avenues for expressing their work. Our findings lay the fo
&lt;/p&gt;</description></item><item><title>Sem-Lex&#22522;&#20934;&#27979;&#35797;&#26159;&#19968;&#20010;&#29992;&#20110;&#24314;&#27169;ASL&#25163;&#21183;&#21644;&#38899;&#32032;&#30340;&#36164;&#28304;&#65292;&#21253;&#25324;&#36229;&#36807;84k&#20010;&#23396;&#31435;&#25163;&#21183;&#21046;&#20316;&#35270;&#39057;&#12290;&#36890;&#36807;&#19982;&#20854;&#20182;&#25163;&#35821;&#36164;&#28304;&#30340;&#23545;&#40784;&#65292;&#21487;&#20197;&#26377;&#25928;&#25193;&#23637;&#25163;&#21183;&#21644;&#38899;&#38901;&#29305;&#24449;&#35782;&#21035;&#30340;&#24212;&#29992;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;SL-GCN&#27169;&#22411;&#21487;&#20197;&#20197;85%&#30340;&#20934;&#30830;&#29575;&#35782;&#21035;&#38899;&#38901;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.00196</link><description>&lt;p&gt;
Sem-Lex&#22522;&#20934;&#27979;&#35797;&#65306;&#24314;&#27169;ASL&#25163;&#21183;&#21450;&#20854;&#38899;&#32032;
&lt;/p&gt;
&lt;p&gt;
The Sem-Lex Benchmark: Modeling ASL Signs and Their Phonemes. (arXiv:2310.00196v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00196
&lt;/p&gt;
&lt;p&gt;
Sem-Lex&#22522;&#20934;&#27979;&#35797;&#26159;&#19968;&#20010;&#29992;&#20110;&#24314;&#27169;ASL&#25163;&#21183;&#21644;&#38899;&#32032;&#30340;&#36164;&#28304;&#65292;&#21253;&#25324;&#36229;&#36807;84k&#20010;&#23396;&#31435;&#25163;&#21183;&#21046;&#20316;&#35270;&#39057;&#12290;&#36890;&#36807;&#19982;&#20854;&#20182;&#25163;&#35821;&#36164;&#28304;&#30340;&#23545;&#40784;&#65292;&#21487;&#20197;&#26377;&#25928;&#25193;&#23637;&#25163;&#21183;&#21644;&#38899;&#38901;&#29305;&#24449;&#35782;&#21035;&#30340;&#24212;&#29992;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;SL-GCN&#27169;&#22411;&#21487;&#20197;&#20197;85%&#30340;&#20934;&#30830;&#29575;&#35782;&#21035;&#38899;&#38901;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;&#25216;&#26415;&#20855;&#26377;&#22686;&#21152;&#32843;&#20154;&#25163;&#35821;&#31038;&#21306;&#30340;&#35775;&#38382;&#21644;&#21253;&#23481;&#24615;&#30340;&#28508;&#21147;&#65292;&#20294;&#30740;&#31350;&#36827;&#23637;&#21463;&#21040;&#20195;&#34920;&#24615;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#36164;&#28304;&#65292;&#29992;&#20110;&#32654;&#22269;&#25163;&#35821;&#65288;ASL&#65289;&#24314;&#27169;&#65292;&#21363;Sem-Lex&#22522;&#20934;&#27979;&#35797;&#12290;&#22522;&#20934;&#27979;&#35797;&#26159;&#30446;&#21069;&#26368;&#22823;&#30340;ASL&#36164;&#28304;&#65292;&#21253;&#25324;&#26469;&#33258;&#32843;&#20154;ASL&#20351;&#29992;&#32773;&#30340;&#36229;&#36807;84k&#20010;&#23396;&#31435;&#25163;&#21183;&#21046;&#20316;&#35270;&#39057;&#65292;&#24182;&#33719;&#24471;&#30693;&#24773;&#21516;&#24847;&#21644;&#34917;&#20607;&#12290;&#20154;&#31867;&#19987;&#23478;&#23558;&#36825;&#20123;&#35270;&#39057;&#19982;&#20854;&#20182;&#25163;&#35821;&#36164;&#28304;&#65288;&#21253;&#25324;ASL-LEX&#12289;SignBank&#21644;ASL Citizen&#65289;&#36827;&#34892;&#23545;&#40784;&#65292;&#20174;&#32780;&#20026;&#25163;&#21183;&#21644;&#38899;&#38901;&#29305;&#24449;&#35782;&#21035;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#21033;&#29992;ASL-LEX&#20013;&#30340;&#35821;&#35328;&#20449;&#24687;&#30340;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;Sem-Lex&#22522;&#20934;&#27979;&#35797;&#22312;&#23396;&#31435;&#25163;&#21183;&#35782;&#21035;&#65288;ISR&#65289;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;SL-GCN&#27169;&#22411;&#34920;&#26126;&#65292;&#21487;&#20197;&#20197;85%&#30340;&#20934;&#30830;&#29575;&#35782;&#21035;&#38899;&#38901;&#29305;&#24449;&#65292;&#24182;&#19988;&#23427;&#20204;&#22312;ISR&#20013;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sign language recognition and translation technologies have the potential to increase access and inclusion of deaf signing communities, but research progress is bottlenecked by a lack of representative data. We introduce a new resource for American Sign Language (ASL) modeling, the Sem-Lex Benchmark. The Benchmark is the current largest of its kind, consisting of over 84k videos of isolated sign productions from deaf ASL signers who gave informed consent and received compensation. Human experts aligned these videos with other sign language resources including ASL-LEX, SignBank, and ASL Citizen, enabling useful expansions for sign and phonological feature recognition. We present a suite of experiments which make use of the linguistic information in ASL-LEX, evaluating the practicality and fairness of the Sem-Lex Benchmark for isolated sign recognition (ISR). We use an SL-GCN model to show that the phonological features are recognizable with 85% accuracy, and that they are effective as a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#35838;&#31243;&#23398;&#20064;&#31561;&#31574;&#30053;&#22914;&#20309;&#25552;&#39640;&#24314;&#27169;&#25163;&#35821;&#38899;&#32032;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35838;&#31243;&#23398;&#20064;&#22312;&#25152;&#26377;&#38899;&#32032;&#31867;&#22411;&#19978;&#30340;&#20934;&#30830;&#29575;&#24179;&#22343;&#36798;&#21040;&#20102;87%&#65292;&#36229;&#36807;&#20102;&#24494;&#35843;&#21644;&#22810;&#20219;&#21153;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.00195</link><description>&lt;p&gt;
&#25506;&#32034;&#24314;&#27169;&#25163;&#35821;&#38899;&#31995;&#30340;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Exploring Strategies for Modeling Sign Language Phonology. (arXiv:2310.00195v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#35838;&#31243;&#23398;&#20064;&#31561;&#31574;&#30053;&#22914;&#20309;&#25552;&#39640;&#24314;&#27169;&#25163;&#35821;&#38899;&#32032;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35838;&#31243;&#23398;&#20064;&#22312;&#25152;&#26377;&#38899;&#32032;&#31867;&#22411;&#19978;&#30340;&#20934;&#30830;&#29575;&#24179;&#22343;&#36798;&#21040;&#20102;87%&#65292;&#36229;&#36807;&#20102;&#24494;&#35843;&#21644;&#22810;&#20219;&#21153;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#35821;&#38899;&#31867;&#20284;&#65292;&#25163;&#21183;&#30001;&#31163;&#25955;&#30340;&#12289;&#21487;&#37325;&#26032;&#32452;&#21512;&#30340;&#29305;&#24449;&#32452;&#25104;&#65292;&#31216;&#20026;&#38899;&#32032;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#33021;&#22815;&#35782;&#21035;&#38899;&#32032;&#30340;&#27169;&#22411;&#22312;&#25163;&#21183;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#28145;&#20837;&#25506;&#32034;&#24314;&#27169;&#25163;&#35821;&#38899;&#32032;&#30340;&#31574;&#30053;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#35782;&#21035;ASL-LEX 2.0&#20013;&#21457;&#29616;&#30340;&#21313;&#20845;&#31181;&#38899;&#32032;&#8220;&#31867;&#22411;&#8221;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#35838;&#31243;&#23398;&#20064;&#31561;&#23398;&#20064;&#31574;&#30053;&#22914;&#20309;&#21033;&#29992;&#38899;&#32032;&#31867;&#22411;&#20043;&#38388;&#30340;&#20114;&#30456;&#26377;&#29992;&#20449;&#24687;&#65292;&#20197;&#20419;&#36827;&#23545;&#25163;&#35821;&#38899;&#32032;&#30340;&#26356;&#22909;&#24314;&#27169;&#12290;&#22312;Sem-Lex&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#35838;&#31243;&#23398;&#20064;&#22312;&#25152;&#26377;&#38899;&#32032;&#31867;&#22411;&#19978;&#21462;&#24471;&#20102;87%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#23545;&#22823;&#22810;&#25968;&#38899;&#32032;&#31867;&#22411;&#32780;&#35328;&#65292;&#36229;&#36807;&#20102;&#24494;&#35843;&#21644;&#22810;&#20219;&#21153;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Like speech, signs are composed of discrete, recombinable features called phonemes. Prior work shows that models which can recognize phonemes are better at sign recognition, motivating deeper exploration into strategies for modeling sign language phonemes. In this work, we learn graph convolution networks to recognize the sixteen phoneme "types" found in ASL-LEX 2.0. Specifically, we explore how learning strategies like multi-task and curriculum learning can leverage mutually useful information between phoneme types to facilitate better modeling of sign language phonemes. Results on the Sem-Lex Benchmark show that curriculum learning yields an average accuracy of 87% across all phoneme types, outperforming fine-tuning and multi-task strategies for most phoneme types.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Knuth-Morris-Pratt&#31639;&#27861;&#30340;&#19978;&#19979;&#25991;&#20559;&#32622;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#25552;&#39640;&#21305;&#37197;&#21040;&#19968;&#32452;&#20559;&#32622;&#30701;&#35821;&#30340;&#25193;&#23637;&#20196;&#29260;&#30340;&#24471;&#20998;&#65292;&#23454;&#29616;&#20102;&#22312;&#20559;&#32622;&#27979;&#35797;&#38598;&#19978;&#26174;&#33879;&#38477;&#20302;&#35789;&#38169;&#35823;&#29575;&#30340;&#25928;&#26524;&#65292;&#24182;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;&#20559;&#32622;&#26041;&#27861;&#30456;&#32467;&#21512;&#33021;&#22815;&#36827;&#19968;&#27493;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.00178</link><description>&lt;p&gt;
&#29992;Knuth-Morris-Pratt&#21305;&#37197;&#31639;&#27861;&#36827;&#34892;&#19978;&#19979;&#25991;&#20559;&#32622;
&lt;/p&gt;
&lt;p&gt;
Contextual Biasing with the Knuth-Morris-Pratt Matching Algorithm. (arXiv:2310.00178v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Knuth-Morris-Pratt&#31639;&#27861;&#30340;&#19978;&#19979;&#25991;&#20559;&#32622;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#25552;&#39640;&#21305;&#37197;&#21040;&#19968;&#32452;&#20559;&#32622;&#30701;&#35821;&#30340;&#25193;&#23637;&#20196;&#29260;&#30340;&#24471;&#20998;&#65292;&#23454;&#29616;&#20102;&#22312;&#20559;&#32622;&#27979;&#35797;&#38598;&#19978;&#26174;&#33879;&#38477;&#20302;&#35789;&#38169;&#35823;&#29575;&#30340;&#25928;&#26524;&#65292;&#24182;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;&#20559;&#32622;&#26041;&#27861;&#30456;&#32467;&#21512;&#33021;&#22815;&#36827;&#19968;&#27493;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#20559;&#32622;&#25351;&#30340;&#26159;&#23558;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#20559;&#21521;&#20110;&#29305;&#23450;&#29992;&#25143;&#25110;&#24212;&#29992;&#22330;&#26223;&#30456;&#20851;&#30340;&#31232;&#26377;&#23454;&#20307;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Knuth-Morris-Pratt&#31639;&#27861;&#30340;&#19978;&#19979;&#25991;&#20559;&#32622;&#31639;&#27861;&#12290;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#65292;&#22914;&#26524;&#19968;&#20010;&#20196;&#29260;&#25193;&#23637;&#23558;&#21305;&#37197;&#25193;&#23637;&#21040;&#19968;&#32452;&#20559;&#32622;&#30701;&#35821;&#20013;&#65292;&#25105;&#20204;&#20250;&#25552;&#39640;&#20854;&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#27169;&#25311;&#20102;&#24120;&#24120;&#22312;&#21152;&#26435;&#26377;&#38480;&#29366;&#24577;&#36716;&#25442;&#22120;&#65288;WFST&#65289;&#26694;&#26550;&#20013;&#23454;&#29616;&#30340;&#32463;&#20856;&#26041;&#27861;&#65292;&#20294;&#23436;&#20840;&#36991;&#20813;&#20102;FST&#35821;&#35328;&#65292;&#24182;&#22312;&#24352;&#37327;&#22788;&#29702;&#21333;&#20803;&#65288;TPU&#65289;&#30340;&#20869;&#23384;&#21344;&#29992;&#21644;&#25928;&#29575;&#26041;&#38754;&#36827;&#34892;&#20102;&#20180;&#32454;&#30340;&#32771;&#34385;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#24341;&#20837;&#39069;&#22806;&#30340;&#27169;&#22411;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#21333;&#29420;&#23454;&#29616;&#22312;&#20559;&#32622;&#27979;&#35797;&#38598;&#19978;&#26174;&#33879;&#38477;&#20302;&#20102;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#65292;&#24182;&#22312;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;&#20559;&#32622;&#26041;&#27861;&#30456;&#32467;&#21512;&#26102;&#20135;&#29983;&#36827;&#19968;&#27493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contextual biasing refers to the problem of biasing the automatic speech recognition (ASR) systems towards rare entities that are relevant to the specific user or application scenarios. We propose algorithms for contextual biasing based on the Knuth-Morris-Pratt algorithm for pattern matching. During beam search, we boost the score of a token extension if it extends matching into a set of biasing phrases. Our method simulates the classical approaches often implemented in the weighted finite state transducer (WFST) framework, but avoids the FST language altogether, with careful considerations on memory footprint and efficiency on tensor processing units (TPUs) by vectorization. Without introducing additional model parameters, our method achieves significant word error rate (WER) reductions on biasing test sets by itself, and yields further performance gain when combined with a model-based biasing method.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#29305;&#21270;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#19994;&#39046;&#22495;&#30340;&#25968;&#25454;&#21644;&#23569;&#37327;&#26631;&#35760;&#31181;&#23376;&#36827;&#34892;&#33258;&#25105;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;&#22312;&#30446;&#26631;&#39046;&#22495;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.00160</link><description>&lt;p&gt;
&#33258;&#25105;&#29305;&#21270;&#65306;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#19987;&#19994;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Self-Specialization: Uncovering Latent Expertise within Large Language Models. (arXiv:2310.00160v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00160
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#29305;&#21270;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#19994;&#39046;&#22495;&#30340;&#25968;&#25454;&#21644;&#23569;&#37327;&#26631;&#35760;&#31181;&#23376;&#36827;&#34892;&#33258;&#25105;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;&#22312;&#30446;&#26631;&#39046;&#22495;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#25105;&#35843;&#25972;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#20154;&#31867;&#32534;&#20889;&#30340;&#31181;&#23376;&#25968;&#25454;&#33258;&#21160;&#29983;&#25104;&#25945;&#23398;&#25968;&#25454;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#23545;&#40784;&#20197;&#36981;&#24490;&#19968;&#33324;&#25351;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19981;&#20877;&#20851;&#27880;&#19968;&#33324;&#23545;&#40784;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#19987;&#23478;&#39046;&#22495;&#29305;&#21270;&#30340;&#33258;&#25105;&#23545;&#40784;&#65288;&#20363;&#22914;&#65292;&#29983;&#29289;&#21307;&#23398;&#65289;&#65292;&#21457;&#29616;&#23427;&#23545;&#20110;&#25552;&#39640;&#30446;&#26631;&#39046;&#22495;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24615;&#33021;&#38750;&#24120;&#26377;&#25928;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29616;&#26377;&#23545;&#40784;&#27169;&#22411;&#22312;&#19987;&#19994;&#39046;&#22495;&#20869;&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#25581;&#31034;&#20102;&#8220;&#36890;&#29992;&#8221;&#25351;&#31034;&#36319;&#38543;&#35757;&#32451;&#23545;&#19979;&#28216;&#19987;&#23478;&#39046;&#22495;&#24615;&#33021;&#30340;&#36793;&#38469;&#25928;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#33258;&#25105;&#29305;&#21270;&#65292;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#23569;&#37327;&#26631;&#35760;&#31181;&#23376;&#36827;&#34892;&#33258;&#25105;&#23545;&#40784;&#36807;&#31243;&#12290;&#24403;&#36890;&#36807;&#26816;&#32034;&#26469;&#20943;&#23569;&#20135;&#29983;&#24187;&#35273;&#24182;&#25552;&#39640;&#23545;&#40784;&#30340;&#24182;&#21457;&#24615;&#21518;&#65292;&#33258;&#25105;&#29305;&#21270;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have demonstrated the effectiveness of self-alignment in which a large language model is, by itself, aligned to follow general instructions through the automatic generation of instructional data using a handful of human-written seeds. Instead of general alignment, in this work, we focus on self-alignment for expert domain specialization (e.g., biomedicine), discovering it to be very effective for improving zero-shot and few-shot performance in target domains of interest. As a preliminary, we first present the benchmark results of existing aligned models within a specialized domain, which reveals the marginal effect that "generic" instruction-following training has on downstream expert domains' performance. To remedy this, we explore self-specialization that leverages domain-specific unlabelled data and a few labeled seeds for the self-alignment process. When augmented with retrieval to reduce hallucination and enhance concurrency of the alignment, self-specialization offer
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#20462;&#35746;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#25552;&#31034;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#25913;&#36827;&#36755;&#20837;&#25991;&#26412;&#30340;&#26041;&#24335;&#23454;&#29616;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.00152</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#30340;&#33258;&#21160;&#25552;&#31034;&#37325;&#20889;
&lt;/p&gt;
&lt;p&gt;
Automatic Prompt Rewriting for Personalized Text Generation. (arXiv:2310.00152v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00152
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#20462;&#35746;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#25552;&#31034;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#25913;&#36827;&#36755;&#20837;&#25991;&#26412;&#30340;&#26041;&#24335;&#23454;&#29616;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24110;&#21161;&#19979;&#65292;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#24050;&#25104;&#20026;&#19968;&#20010;&#24555;&#36895;&#22686;&#38271;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#38598;&#20013;&#22312;&#20026;&#29305;&#23450;&#39046;&#22495;&#35774;&#35745;&#19987;&#38376;&#30340;&#27169;&#22411;&#65292;&#25110;&#32773;&#38656;&#35201;&#24494;&#35843;LLMs&#20197;&#29983;&#25104;&#20010;&#24615;&#21270;&#25991;&#26412;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20856;&#22411;&#24773;&#26223;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#20010;&#24615;&#21270;&#36755;&#20986;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#20923;&#32467;&#30340;&#65292;&#21482;&#33021;&#36890;&#36807;API&#36827;&#34892;&#35775;&#38382;&#12290;&#22312;&#36825;&#20010;&#38480;&#21046;&#19979;&#65292;&#21807;&#19968;&#33021;&#20570;&#30340;&#23601;&#26159;&#25913;&#36827;&#21457;&#36865;&#32473;LLM&#30340;&#36755;&#20837;&#25991;&#26412;&#65288;&#21363;&#25991;&#26412;&#25552;&#31034;&#65289;&#65292;&#36825;&#20010;&#36807;&#31243;&#36890;&#24120;&#26159;&#25163;&#21160;&#23436;&#25104;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20462;&#35746;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#30340;&#25552;&#31034;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#20010;&#35757;&#32451;&#33539;&#24335;&#65292;&#23558;&#30417;&#30563;&#23398;&#20064;&#65288;SL&#65289;&#21644;
&lt;/p&gt;
&lt;p&gt;
Facilitated by large language models (LLMs), personalized text generation has become a rapidly growing research direction. Most existing studies focus on designing specialized models for a particular domain, or they require fine-tuning the LLMs to generate personalized text. We consider a typical scenario in which the large language model, which generates personalized output, is frozen and can only be accessed through APIs. Under this constraint, all one can do is to improve the input text (i.e., text prompts) sent to the LLM, a procedure that is usually done manually. In this paper, we propose a novel method to automatically revise prompts for personalized text generation. The proposed method takes the initial prompts generated by a state-of-the-art, multistage framework for personalized generation and rewrites a few critical components that summarize and synthesize the personal context. The prompt rewriter employs a training paradigm that chains together supervised learning (SL) and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#26469;&#25345;&#32493;&#20174;&#29992;&#25143;&#32416;&#27491;&#20013;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#22240;&#20026;&#35821;&#35328;&#30340;&#21457;&#23637;&#21644;&#26032;&#35789;&#27719;&#30340;&#20986;&#29616;&#32780;&#21464;&#24471;&#36807;&#26102;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#38024;&#23545;&#26032;&#35789;&#27719;&#12289;&#38271;&#23614;&#35789;&#27719;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#31561;&#25216;&#26415;&#25552;&#39640;&#27169;&#22411;&#30340;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.00141</link><description>&lt;p&gt;
&#29992;&#25143;&#21453;&#39304;&#30340;&#39304;&#36192;&#65306;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#20174;&#29992;&#25143;&#32416;&#27491;&#20013;&#25552;&#39640;ASR&#27169;&#22411;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
The Gift of Feedback: Improving ASR Model Quality by Learning from User Corrections through Federated Learning. (arXiv:2310.00141v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#26469;&#25345;&#32493;&#20174;&#29992;&#25143;&#32416;&#27491;&#20013;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#22240;&#20026;&#35821;&#35328;&#30340;&#21457;&#23637;&#21644;&#26032;&#35789;&#27719;&#30340;&#20986;&#29616;&#32780;&#21464;&#24471;&#36807;&#26102;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#38024;&#23545;&#26032;&#35789;&#27719;&#12289;&#38271;&#23614;&#35789;&#27719;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#31561;&#25216;&#26415;&#25552;&#39640;&#27169;&#22411;&#30340;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#36890;&#24120;&#22312;&#22823;&#37327;&#30340;&#36716;&#24405;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#38543;&#30528;&#35821;&#35328;&#30340;&#21457;&#23637;&#21644;&#26032;&#35789;&#27719;&#30340;&#20986;&#29616;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#21464;&#24471;&#36807;&#26102;&#21644;&#38472;&#26087;&#12290;&#22312;&#22522;&#20110;&#26381;&#21153;&#22120;&#35757;&#32451;&#20294;&#37096;&#32626;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#27169;&#22411;&#20013;&#65292;&#38169;&#35823;&#21487;&#33021;&#26159;&#30001;&#20110;&#26381;&#21153;&#22120;&#35757;&#32451;&#25968;&#25454;&#19982;&#23454;&#38469;&#35774;&#22791;&#20351;&#29992;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#23548;&#33268;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#26469;&#19981;&#26029;&#20174;&#35774;&#22791;&#19978;&#30340;&#29992;&#25143;&#32416;&#27491;&#20013;&#23398;&#20064;&#65292;&#20174;&#32780;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#20123;&#25216;&#26415;&#65292;&#20197;&#38024;&#23545;&#27169;&#22411;&#20197;&#21069;&#26410;&#36935;&#21040;&#36807;&#30340;&#26032;&#35789;&#27719;&#65292;&#23398;&#20064;&#38271;&#23614;&#35789;&#27719;&#65292;&#24182;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#12290;&#22312;&#23454;&#39564;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#25913;&#36827;&#20102;&#27169;&#22411;&#23545;&#26032;&#35789;&#27719;&#30340;&#35782;&#21035;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#25972;&#20307;&#35821;&#35328;&#20998;&#24067;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition (ASR) models are typically trained on large datasets of transcribed speech. As language evolves and new terms come into use, these models can become outdated and stale. In the context of models trained on the server but deployed on edge devices, errors may result from the mismatch between server training data and actual on-device usage. In this work, we seek to continually learn from on-device user corrections through Federated Learning (FL) to address this issue. We explore techniques to target fresh terms that the model has not previously encountered, learn long-tail words, and mitigate catastrophic forgetting. In experimental evaluations, we find that the proposed techniques improve model recognition of fresh terms, while preserving quality on the overall language distribution.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#25991;&#26412;&#21464;&#25442;&#22120;&#27169;&#22411;&#19978;&#24494;&#35843;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#22312;&#22810;&#35821;&#35328;&#20013;&#24635;&#32467;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#26377;&#21161;&#20110;&#25552;&#39640;&#26410;&#26469;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#65292;&#19988;&#33021;&#22815;&#24212;&#29992;&#20110;&#19981;&#21516;&#26063;&#35028;&#32972;&#26223;&#30340;&#24739;&#32773;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.00100</link><description>&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;--&#25688;&#35201;&#26159;&#20320;&#38656;&#35201;&#30340;&#19968;&#20999;&#65281;
&lt;/p&gt;
&lt;p&gt;
Multilingual Natural Language ProcessingModel for Radiology Reports -- The Summary is all you need!. (arXiv:2310.00100v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#25991;&#26412;&#21464;&#25442;&#22120;&#27169;&#22411;&#19978;&#24494;&#35843;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#22312;&#22810;&#35821;&#35328;&#20013;&#24635;&#32467;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#26377;&#21161;&#20110;&#25552;&#39640;&#26410;&#26469;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#65292;&#19988;&#33021;&#22815;&#24212;&#29992;&#20110;&#19981;&#21516;&#26063;&#35028;&#32972;&#26223;&#30340;&#24739;&#32773;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#21360;&#35937;&#37096;&#20998;&#24635;&#32467;&#20102;&#37325;&#35201;&#30340;&#25918;&#23556;&#23398;&#21457;&#29616;&#65292;&#24182;&#22312;&#21521;&#21307;&#29983;&#20256;&#36798;&#36825;&#20123;&#21457;&#29616;&#26102;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25918;&#23556;&#31185;&#21307;&#29983;&#26469;&#35828;&#65292;&#20934;&#22791;&#36825;&#20123;&#25688;&#35201;&#26082;&#32791;&#26102;&#21448;&#23481;&#26131;&#20986;&#38169;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#33021;&#22815;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#24635;&#32467;&#36825;&#20123;&#25253;&#21578;&#30340;&#27169;&#22411;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#21644;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#26063;&#35028;&#32972;&#26223;&#30340;&#24739;&#32773;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20110;&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#25991;&#26412;&#21464;&#25442;&#22120;&#30340;&#27169;&#22411;&#19978;&#24494;&#35843;&#65292;&#33258;&#21160;&#21270;&#22320;&#29983;&#25104;&#20102;&#19981;&#21516;&#35821;&#35328;&#30340;&#25918;&#23556;&#23398;&#21360;&#35937;&#65292;&#20197;&#24635;&#32467;&#33521;&#35821;&#12289;&#33889;&#33796;&#29273;&#35821;&#21644;&#24503;&#35821;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;&#21457;&#29616;&#12290;&#22312;&#19968;&#39033;&#30450;&#27979;&#20013;&#65292;&#20004;&#20301;&#26377;&#25191;&#19994;&#36164;&#26684;&#30340;&#25918;&#23556;&#31185;&#21307;&#29983;&#34920;&#31034;&#65292;&#23545;&#20110;&#33267;&#23569;70%&#30340;&#31995;&#32479;&#29983;&#25104;&#30340;&#25688;&#35201;&#65292;&#20854;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
The impression section of a radiology report summarizes important radiology findings and plays a critical role in communicating these findings to physicians. However, the preparation of these summaries is time-consuming and error-prone for radiologists. Recently, numerous models for radiology report summarization have been developed. Nevertheless, there is currently no model that can summarize these reports in multiple languages. Such a model could greatly improve future research and the development of Deep Learning models that incorporate data from patients with different ethnic backgrounds. In this study, the generation of radiology impressions in different languages was automated by fine-tuning a model, publicly available, based on a multilingual text-to-text Transformer to summarize findings available in English, Portuguese, and German radiology reports. In a blind test, two board-certified radiologists indicated that for at least 70% of the system-generated summaries, the quality 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Voice2Action&#65292;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#20154;&#22312;&#34394;&#25311;&#29616;&#23454;&#20013;&#36827;&#34892;&#39640;&#25928;&#23454;&#26102;&#20132;&#20114;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#23545;&#23450;&#21046;&#35821;&#38899;&#20449;&#21495;&#21644;&#25991;&#26412;&#21629;&#20196;&#36827;&#34892;&#20998;&#23618;&#20998;&#26512;&#65292;&#24182;&#23558;&#25191;&#34892;&#20219;&#21153;&#20998;&#25104;&#20132;&#20114;&#23376;&#38598;&#65292;Voice2Action&#33021;&#22815;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#39640;&#25928;&#21644;&#20934;&#30830;&#22320;&#25191;&#34892;&#12290;</title><link>http://arxiv.org/abs/2310.00092</link><description>&lt;p&gt;
Voice2Action: &#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#34394;&#25311;&#29616;&#23454;&#20013;&#39640;&#25928;&#23454;&#26102;&#20132;&#20114;&#30340;&#20195;&#29702;&#20154;
&lt;/p&gt;
&lt;p&gt;
Voice2Action: Language Models as Agent for Efficient Real-Time Interaction in Virtual Reality. (arXiv:2310.00092v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Voice2Action&#65292;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#20154;&#22312;&#34394;&#25311;&#29616;&#23454;&#20013;&#36827;&#34892;&#39640;&#25928;&#23454;&#26102;&#20132;&#20114;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#23545;&#23450;&#21046;&#35821;&#38899;&#20449;&#21495;&#21644;&#25991;&#26412;&#21629;&#20196;&#36827;&#34892;&#20998;&#23618;&#20998;&#26512;&#65292;&#24182;&#23558;&#25191;&#34892;&#20219;&#21153;&#20998;&#25104;&#20132;&#20114;&#23376;&#38598;&#65292;Voice2Action&#33021;&#22815;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#39640;&#25928;&#21644;&#20934;&#30830;&#22320;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#35757;&#32451;&#21644;&#35843;&#25972;&#20197;&#20165;&#20165;&#20351;&#29992;&#23569;&#37327;&#31034;&#20363;&#26469;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#24182;&#34987;&#25552;&#31034;&#20026;&#20219;&#21153;&#39537;&#21160;&#30340;&#33258;&#20027;&#20195;&#29702;&#20154;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#25191;&#34892;&#29615;&#22659;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#22312;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#20013;&#37096;&#32626;&#20195;&#29702;LLMs&#19968;&#30452;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#20854;&#21407;&#22240;&#26159;&#22312;&#32447;&#20132;&#20114;&#30340;&#25928;&#29575;&#20302;&#19979;&#20197;&#21450;3D&#29615;&#22659;&#20013;&#22797;&#26434;&#30340;&#25805;&#20316;&#31867;&#21035;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Voice2Action&#65292;&#19968;&#20010;&#36890;&#36807;&#21160;&#20316;&#21644;&#23454;&#20307;&#25552;&#21462;&#26469;&#20998;&#23618;&#20998;&#26512;&#23450;&#21046;&#35821;&#38899;&#20449;&#21495;&#21644;&#25991;&#26412;&#21629;&#20196;&#65292;&#24182;&#23558;&#25191;&#34892;&#20219;&#21153;&#23454;&#26102;&#20998;&#25104;&#35268;&#33539;&#30340;&#20132;&#20114;&#23376;&#38598;&#65292;&#24182;&#36890;&#36807;&#29615;&#22659;&#21453;&#39304;&#26469;&#38450;&#27490;&#38169;&#35823;&#12290;&#22312;&#20855;&#26377;&#21512;&#25104;&#25351;&#20196;&#25968;&#25454;&#30340;&#22478;&#24066;&#24037;&#31243;VR&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Voice2Action&#33021;&#22815;&#27604;&#27809;&#26377;&#20248;&#21270;&#30340;&#26041;&#27861;&#26356;&#39640;&#25928;&#21644;&#20934;&#30830;&#22320;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are trained and aligned to follow natural language instructions with only a handful of examples, and they are prompted as task-driven autonomous agents to adapt to various sources of execution environments. However, deploying agent LLMs in virtual reality (VR) has been challenging due to the lack of efficiency in online interactions and the complex manipulation categories in 3D environments. In this work, we propose Voice2Action, a framework that hierarchically analyzes customized voice signals and textual commands through action and entity extraction and divides the execution tasks into canonical interaction subsets in real-time with error prevention from environment feedback. Experiment results in an urban engineering VR environment with synthetic instruction data show that Voice2Action can perform more efficiently and accurately than approaches without optimizations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SocREval&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;GPT-4&#21644;&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#36827;&#34892;&#26080;&#21442;&#32771;&#25512;&#29702;&#35780;&#20272;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#22797;&#26434;&#25512;&#29702;&#27169;&#22411;&#35780;&#20272;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.00074</link><description>&lt;p&gt;
SocREval&#65306;&#20351;&#29992;&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#36827;&#34892;&#26080;&#21442;&#32771;&#25512;&#29702;&#35780;&#20272;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SocREval: Large Language Models with the Socratic Method for Reference-Free Reasoning Evaluation. (arXiv:2310.00074v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SocREval&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;GPT-4&#21644;&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#36827;&#34892;&#26080;&#21442;&#32771;&#25512;&#29702;&#35780;&#20272;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#22797;&#26434;&#25512;&#29702;&#27169;&#22411;&#35780;&#20272;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#24403;&#21069;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20197;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#35780;&#20272;&#23427;&#20204;&#30340;&#36880;&#27493;&#25512;&#29702;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#21442;&#32771;&#30340;&#35780;&#20272;&#25351;&#26631;&#20381;&#36182;&#20110;&#20154;&#24037;&#27880;&#37322;&#30340;&#25512;&#29702;&#38142;&#26469;&#35780;&#20272;&#27169;&#22411;&#23548;&#20986;&#30340;&#25512;&#29702;&#38142;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#8220;&#40644;&#37329;&#26631;&#20934;&#8221;&#20154;&#24037;&#32534;&#20889;&#30340;&#25512;&#29702;&#38142;&#21487;&#33021;&#19981;&#26159;&#21807;&#19968;&#30340;&#65292;&#24182;&#19988;&#20854;&#33719;&#21462;&#36890;&#24120;&#26159;&#21171;&#21160;&#23494;&#38598;&#22411;&#30340;&#12290;&#29616;&#26377;&#30340;&#26080;&#21442;&#32771;&#25512;&#29702;&#25351;&#26631;&#28040;&#38500;&#20102;&#20154;&#24037;&#21046;&#20316;&#25512;&#29702;&#38142;&#30340;&#38656;&#27714;&#20316;&#20026;&#21442;&#32771;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#22312;&#20855;&#26377;&#20154;&#24037;&#25512;&#29702;&#38142;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#22797;&#26434;&#21270;&#20102;&#27969;&#31243;&#24182;&#24341;&#21457;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#27867;&#21270;&#24615;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;GPT-4&#33258;&#21160;&#35780;&#20272;&#25512;&#29702;&#38142;&#36136;&#37327;&#65292;&#28040;&#38500;&#20102;&#23545;&#20154;&#24037;&#21046;&#20316;&#21442;&#32771;&#30340;&#38656;&#27714;&#12290;&#21033;&#29992;&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23450;&#21046;&#21270;&#25552;&#31034;&#26469;&#22686;&#24378;&#26080;&#21442;&#32771;&#25512;&#29702;&#35780;&#20272;&#65292;&#36825;&#23601;&#26159;&#25105;&#20204;&#31216;&#20043;&#20026;SocREval&#65288;&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#65289;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
To comprehensively assess the capacity of current models for complex reasoning, it is crucial to assess their step-by-step reasoning in a scalable manner. Established reference-based evaluation metrics rely on human-annotated reasoning chains to assess the model-derived chains. However, such ``gold-standard'' human-written reasoning chains may not be unique and their acquisition is often labor-intensive. Existing reference-free reasoning metrics eliminate the need for human-crafted reasoning chains as references, but they typically require fine-tuning on datasets with human-derived reasoning chains, which complicates the process and raises concerns regarding generalizability across diverse datasets. To address these challenges, we harness GPT-4 to automatically evaluate reasoning chain quality, obviating the need for human-crafted references. Leveraging the Socratic method, we devise tailored prompts to enhance reference-free reasoning evaluation, which we term SocREval (Socratic metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;PB-LLM&#26159;&#19968;&#31181;&#37096;&#20998;&#20108;&#20540;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#30340;&#21516;&#26102;&#23454;&#29616;&#26497;&#20302;&#27604;&#29305;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31561;&#26041;&#27861;&#24674;&#22797;&#37327;&#21270;LLMM&#30340;&#23481;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.00034</link><description>&lt;p&gt;
PB-LLM: &#37096;&#20998;&#20108;&#20540;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PB-LLM: Partially Binarized Large Language Models. (arXiv:2310.00034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;PB-LLM&#26159;&#19968;&#31181;&#37096;&#20998;&#20108;&#20540;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#30340;&#21516;&#26102;&#23454;&#29616;&#26497;&#20302;&#27604;&#29305;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31561;&#26041;&#27861;&#24674;&#22797;&#37327;&#21270;LLMM&#30340;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32593;&#32476;&#20108;&#20540;&#21270;&#65292;&#19968;&#31181;&#21387;&#32553;&#27169;&#22411;&#26435;&#37325;&#20026;&#21333;&#20010;&#27604;&#29305;&#30340;&#37327;&#21270;&#30340;&#28608;&#36827;&#24418;&#24335;&#65292;&#19987;&#38376;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21387;&#32553;&#12290;&#30001;&#20110;&#20043;&#21069;&#30340;&#20108;&#20540;&#21270;&#26041;&#27861;&#20250;&#23548;&#33268;LLMs&#23849;&#28291;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#37096;&#20998;&#20108;&#20540;&#21270;LLM&#65288;PB-LLM&#65289;&#65292;&#21487;&#20197;&#23454;&#29616;&#26497;&#20302;&#27604;&#29305;&#37327;&#21270;&#65292;&#24182;&#21516;&#26102;&#20445;&#25345;&#37327;&#21270;LLMs&#30340;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#39318;&#20808;&#25581;&#31034;&#20102;&#29616;&#26377;&#20108;&#20540;&#21270;&#31639;&#27861;&#30340;&#21407;&#29983;&#24212;&#29992;&#30340;&#26080;&#25928;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#26174;&#33879;&#26435;&#37325;&#22312;&#23454;&#29616;&#20302;&#20301;&#37327;&#21270;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;PB-LLM&#22312;&#20108;&#36827;&#21046;&#21270;&#36807;&#31243;&#20013;&#36807;&#28388;&#20102;&#19968;&#23567;&#37096;&#20998;&#26174;&#33879;&#26435;&#37325;&#65292;&#23558;&#23427;&#20204;&#20998;&#37197;&#21040;&#39640;&#20301;&#23384;&#20648;&#20013;&#65292;&#21363;&#37096;&#20998;&#20108;&#20540;&#21270;&#12290;PB-LLM&#22312;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;QAT&#65289;&#30340;&#35282;&#24230;&#20998;&#26512;&#21518;&#65292;&#25193;&#23637;&#20102;&#24674;&#22797;&#37327;&#21270;LLMM&#23481;&#37327;&#30340;&#33021;&#21147;&#12290;&#22312;PTQ&#19979;&#65292;&#32467;&#21512;&#20102;GPTQ&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#37325;&#26500;&#20102;...
&lt;/p&gt;
&lt;p&gt;
This paper explores network binarization, a radical form of quantization, compressing model weights to a single bit, specifically for Large Language Models (LLMs) compression. Due to previous binarization methods collapsing LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can achieve extreme low-bit quantization while maintaining the linguistic reasoning capacity of quantized LLMs. Specifically, our exploration first uncovers the ineffectiveness of naive applications of existing binarization algorithms and highlights the imperative role of salient weights in achieving low-bit quantization. Thus, PB-LLM filters a small ratio of salient weights during binarization, allocating them to higher-bit storage, i.e., partially-binarization. PB-LLM is extended to recover the capacities of quantized LMMs, by analyzing from the perspective of post-training quantization (PTQ) and quantization-aware training (QAT). Under PTQ, combining the concepts from GPTQ, we reconstruct 
&lt;/p&gt;</description></item><item><title>L2CEval&#26159;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#30340;&#24037;&#20316;&#65292;&#20998;&#26512;&#20102;&#24433;&#21709;&#20854;&#24615;&#33021;&#30340;&#22240;&#32032;&#65292;&#24182;&#23545;&#32622;&#20449;&#24230;&#26657;&#20934;&#21644;&#20154;&#24037;&#35780;&#20272;&#36827;&#34892;&#20102;&#27979;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.17446</link><description>&lt;p&gt;
L2CEval:&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models. (arXiv:2309.17446v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17446
&lt;/p&gt;
&lt;p&gt;
L2CEval&#26159;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#30340;&#24037;&#20316;&#65292;&#20998;&#26512;&#20102;&#24433;&#21709;&#20854;&#24615;&#33021;&#30340;&#22240;&#32032;&#65292;&#24182;&#23545;&#32622;&#20449;&#24230;&#26657;&#20934;&#21644;&#20154;&#24037;&#35780;&#20272;&#36827;&#34892;&#20102;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#22312;&#20195;&#30721;&#19978;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#22312;&#20960;&#27425;&#35757;&#32451;&#29978;&#33267;&#38646;&#27425;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#29983;&#25104;&#31243;&#24207;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#23613;&#31649;&#26377;&#30528;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#32570;&#20047;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#24448;&#24448;&#38598;&#20013;&#22312;&#29305;&#23450;&#20219;&#21153;&#12289;&#27169;&#22411;&#26550;&#26500;&#25110;&#23398;&#20064;&#33539;&#24335;&#19978;&#65292;&#23548;&#33268;&#23545;&#25972;&#20307;&#24773;&#20917;&#30340;&#29702;&#35299;&#38646;&#25955;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;L2CEval&#65292;&#23545;LLM&#22312;&#35821;&#20041;&#35299;&#26512;&#12289;&#25968;&#23398;&#25512;&#29702;&#21644;Python&#32534;&#31243;&#30340;7&#20010;&#20219;&#21153;&#19978;&#30340;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#24182;&#20998;&#26512;&#21487;&#33021;&#24433;&#21709;&#20854;&#24615;&#33021;&#30340;&#22240;&#32032;&#65292;&#22914;&#27169;&#22411;&#22823;&#23567;&#12289;&#39044;&#35757;&#32451;&#25968;&#25454;&#12289;&#25351;&#20196;&#35843;&#25972;&#21644;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;&#38500;&#20102;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#25105;&#20204;&#36824;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#32622;&#20449;&#24230;&#26657;&#20934;&#30340;&#27979;&#37327;&#21644;&#20154;&#24037;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs), especially those that are pretrained on code, have demonstrated strong capabilities in generating programs from natural language inputs in a few-shot or even zero-shot manner. Despite promising results, there is a notable lack of a comprehensive evaluation of these models language-to-code generation capabilities. Existing studies often focus on specific tasks, model architectures, or learning paradigms, leading to a fragmented understanding of the overall landscape. In this work, we present L2CEval, a systematic evaluation of the language-to-code generation capabilities of LLMs on 7 tasks across the domain spectrum of semantic parsing, math reasoning and Python programming, analyzing the factors that potentially affect their performance, such as model size, pretraining data, instruction tuning, and different prompting methods. In addition to assessing model performance, we measure confidence calibration for the models and conduct human evaluation
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;LLM-grounded Video Diffusion (LVD)&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#29983;&#25104;&#21160;&#24577;&#22330;&#26223;&#24067;&#23616;&#65292;&#20877;&#36890;&#36807;&#36825;&#20123;&#24067;&#23616;&#25351;&#23548;&#35270;&#39057;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#26102;&#31354;&#25552;&#31034;&#21644;&#19981;&#27491;&#30830;&#30340;&#36816;&#21160;&#29983;&#25104;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2309.17444</link><description>&lt;p&gt;
LLM&#22522;&#20110;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLM-grounded Video Diffusion Models. (arXiv:2309.17444v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17444
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;LLM-grounded Video Diffusion (LVD)&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#29983;&#25104;&#21160;&#24577;&#22330;&#26223;&#24067;&#23616;&#65292;&#20877;&#36890;&#36807;&#36825;&#20123;&#24067;&#23616;&#25351;&#23548;&#35270;&#39057;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#26102;&#31354;&#25552;&#31034;&#21644;&#19981;&#27491;&#30830;&#30340;&#36816;&#21160;&#29983;&#25104;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#23383;&#26465;&#20214;&#19979;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#31070;&#32463;&#35270;&#39057;&#29983;&#25104;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#27169;&#22411;&#20173;&#28982;&#22312;&#22797;&#26434;&#30340;&#26102;&#31354;&#25552;&#31034;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36890;&#24120;&#29983;&#25104;&#21463;&#38480;&#21046;&#25110;&#19981;&#27491;&#30830;&#30340;&#36816;&#21160;&#65288;&#20363;&#22914;&#65292;&#29978;&#33267;&#32570;&#20047;&#20174;&#24038;&#21521;&#21491;&#31227;&#21160;&#30340;&#29289;&#20307;&#30340;&#25552;&#31034;&#33021;&#21147;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LLM&#22522;&#20110;&#35270;&#39057;&#25193;&#25955;&#65288;LVD&#65289;&#12290;LVD&#19981;&#30452;&#25509;&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#29983;&#25104;&#35270;&#39057;&#65292;&#32780;&#26159;&#39318;&#20808;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26681;&#25454;&#25991;&#26412;&#36755;&#20837;&#29983;&#25104;&#21160;&#24577;&#22330;&#26223;&#24067;&#23616;&#65292;&#28982;&#21518;&#20351;&#29992;&#29983;&#25104;&#30340;&#24067;&#23616;&#26469;&#25351;&#23548;&#35270;&#39057;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLM&#33021;&#22815;&#20174;&#21333;&#32431;&#30340;&#25991;&#26412;&#20013;&#29702;&#35299;&#22797;&#26434;&#30340;&#26102;&#31354;&#21160;&#24577;&#65292;&#24182;&#29983;&#25104;&#19982;&#23454;&#38469;&#19990;&#30028;&#20013;&#36890;&#24120;&#35266;&#23519;&#21040;&#30340;&#25552;&#31034;&#21644;&#29289;&#20307;&#36816;&#21160;&#27169;&#24335;&#23494;&#20999;&#23545;&#40784;&#30340;&#24067;&#23616;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#35843;&#25972;&#27880;&#24847;&#21147;&#22270;&#26469;&#25351;&#23548;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#19982;&#36825;&#20123;&#24067;&#23616;&#36827;&#34892;&#20132;&#20114;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26080;&#38656;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-conditioned diffusion models have emerged as a promising tool for neural video generation. However, current models still struggle with intricate spatiotemporal prompts and often generate restricted or incorrect motion (e.g., even lacking the ability to be prompted for objects moving from left to right). To address these limitations, we introduce LLM-grounded Video Diffusion (LVD). Instead of directly generating videos from the text inputs, LVD first leverages a large language model (LLM) to generate dynamic scene layouts based on the text inputs and subsequently uses the generated layouts to guide a diffusion model for video generation. We show that LLMs are able to understand complex spatiotemporal dynamics from text alone and generate layouts that align closely with both the prompts and the object motion patterns typically observed in the real world. We then propose to guide video diffusion models with these layouts by adjusting the attention maps. Our approach is training-free 
&lt;/p&gt;</description></item><item><title>LatticeGen&#26159;&#19968;&#20010;&#21327;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#30495;&#23454;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#22122;&#22768;&#28151;&#21512;&#24182;&#38544;&#34255;&#22312;&#26684;&#23376;&#20013;&#65292;&#20197;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LatticeGen&#33021;&#22815;&#22312;&#38754;&#23545;&#24378;&#25915;&#20987;&#26102;&#25104;&#21151;&#20445;&#25252;&#30495;&#23454;&#29983;&#25104;&#65292;&#36229;&#36807;50%&#30340;&#35821;&#20041;&#20173;&#28982;&#38544;&#34255;&#12290;</title><link>http://arxiv.org/abs/2309.17157</link><description>&lt;p&gt;
LatticeGen: &#19968;&#31181;&#22312;&#20113;&#19978;&#36827;&#34892;&#38544;&#31169;&#24863;&#30693;&#29983;&#25104;&#30340;&#21327;&#20316;&#26694;&#26550;&#65292;&#38544;&#34255;&#29983;&#25104;&#30340;&#25991;&#26412;&#22312;&#26684;&#23376;&#20013;
&lt;/p&gt;
&lt;p&gt;
LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud. (arXiv:2309.17157v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17157
&lt;/p&gt;
&lt;p&gt;
LatticeGen&#26159;&#19968;&#20010;&#21327;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#30495;&#23454;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#22122;&#22768;&#28151;&#21512;&#24182;&#38544;&#34255;&#22312;&#26684;&#23376;&#20013;&#65292;&#20197;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LatticeGen&#33021;&#22815;&#22312;&#38754;&#23545;&#24378;&#25915;&#20987;&#26102;&#25104;&#21151;&#20445;&#25252;&#30495;&#23454;&#29983;&#25104;&#65292;&#36229;&#36807;50%&#30340;&#35821;&#20041;&#20173;&#28982;&#38544;&#34255;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#30340;&#29992;&#25143;-&#26381;&#21153;&#22120;&#20132;&#20114;&#27169;&#24335;&#20013;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#25552;&#31034;&#29983;&#25104;&#30340;&#36807;&#31243;&#20013;&#65292;&#26381;&#21153;&#22120;&#23436;&#20840;&#25511;&#21046;&#30528;&#29983;&#25104;&#36807;&#31243;&#65292;&#36825;&#20351;&#24471;&#24819;&#35201;&#23558;&#29983;&#25104;&#30340;&#25991;&#26412;&#20445;&#30041;&#32473;&#33258;&#24049;&#30340;&#29992;&#25143;&#27809;&#26377;&#20219;&#20309;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LatticeGen&#65292;&#19968;&#20010;&#21327;&#20316;&#26694;&#26550;&#65292;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#26381;&#21153;&#22120;&#20173;&#28982;&#22788;&#29702;&#22823;&#37096;&#20998;&#35745;&#31639;&#20219;&#21153;&#65292;&#32780;&#29992;&#25143;&#25511;&#21046;&#37319;&#26679;&#25805;&#20316;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#29992;&#25143;&#23558;&#30495;&#23454;&#29983;&#25104;&#24207;&#21015;&#19982;&#22122;&#22768;&#26631;&#35760;&#28151;&#21512;&#65292;&#24182;&#38544;&#34255;&#22312;&#19968;&#20010;&#24102;&#22122;&#22768;&#30340;&#26684;&#23376;&#20013;&#12290;&#32771;&#34385;&#21040;&#26469;&#33258;&#20551;&#35774;&#24694;&#24847;&#26381;&#21153;&#22120;&#30340;&#28508;&#22312;&#25915;&#20987;&#20197;&#21450;&#29992;&#25143;&#22914;&#20309;&#36827;&#34892;&#38450;&#24481;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37325;&#22797;&#27874;&#26463;&#25628;&#32034;&#25915;&#20987;&#21644;&#28151;&#21512;&#22122;&#22768;&#26041;&#26696;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;LatticeGen&#24212;&#29992;&#20110;&#20445;&#25252;&#25552;&#31034;&#21644;&#29983;&#25104;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;&#24102;&#22122;&#22768;&#30340;&#26684;&#23376;&#20250;&#38477;&#20302;&#29983;&#25104;&#36136;&#37327;&#65292;&#20294;LatticeGen&#25104;&#21151;&#22320;&#22312;&#24378;&#25915;&#20987;&#19979;&#26174;&#33879;&#20445;&#25252;&#20102;&#30495;&#23454;&#29983;&#25104;&#65288;&#36229;&#36807;50%&#30340;&#35821;&#20041;&#20173;&#28982;&#38544;&#34255;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the current user-server interaction paradigm of prompted generation with large language models (LLM) on cloud, the server fully controls the generation process, which leaves zero options for users who want to keep the generated text to themselves. We propose LatticeGen, a cooperative framework in which the server still handles most of the computation while the user controls the sampling operation. The key idea is that the true generated sequence is mixed with noise tokens by the user and hidden in a noised lattice. Considering potential attacks from a hypothetically malicious server and how the user can defend against it, we propose the repeated beam-search attack and the mixing noise scheme. In our experiments we apply LatticeGen to protect both prompt and generation. It is shown that while the noised lattice degrades generation quality, LatticeGen successfully protects the true generation to a remarkable degree under strong attacks (more than 50% of the semantic remains hidden as 
&lt;/p&gt;</description></item><item><title>CLIP&#30340;&#25104;&#21151;&#20027;&#35201;&#24402;&#21151;&#20110;&#20854;&#25968;&#25454;&#32780;&#38750;&#27169;&#22411;&#26550;&#26500;&#25110;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#20803;&#25968;&#25454;&#25972;&#29702;&#26041;&#27861;&#24341;&#20837;&#20102;MetaCLIP&#65292;&#35813;&#26041;&#27861;&#20174;&#21407;&#22987;&#25968;&#25454;&#27744;&#21644;&#20803;&#25968;&#25454;&#20013;&#29983;&#25104;&#19968;&#20010;&#24179;&#34913;&#30340;&#23376;&#38598;&#65292;&#25552;&#20379;&#20102;&#26356;&#21152;&#35814;&#32454;&#30340;&#25968;&#25454;&#20449;&#24687;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;MetaCLIP&#22312;&#22788;&#29702;400M&#20010;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#23545;&#26102;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.16671</link><description>&lt;p&gt;
&#25581;&#31192;CLIP&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Demystifying CLIP Data. (arXiv:2309.16671v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16671
&lt;/p&gt;
&lt;p&gt;
CLIP&#30340;&#25104;&#21151;&#20027;&#35201;&#24402;&#21151;&#20110;&#20854;&#25968;&#25454;&#32780;&#38750;&#27169;&#22411;&#26550;&#26500;&#25110;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#20803;&#25968;&#25454;&#25972;&#29702;&#26041;&#27861;&#24341;&#20837;&#20102;MetaCLIP&#65292;&#35813;&#26041;&#27861;&#20174;&#21407;&#22987;&#25968;&#25454;&#27744;&#21644;&#20803;&#25968;&#25454;&#20013;&#29983;&#25104;&#19968;&#20010;&#24179;&#34913;&#30340;&#23376;&#38598;&#65292;&#25552;&#20379;&#20102;&#26356;&#21152;&#35814;&#32454;&#30340;&#25968;&#25454;&#20449;&#24687;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;MetaCLIP&#22312;&#22788;&#29702;400M&#20010;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#23545;&#26102;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#26159;&#19968;&#31181;&#25512;&#21160;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#21644;&#24212;&#29992;&#30340;&#26041;&#27861;&#65292;&#20026;&#29616;&#20195;&#35782;&#21035;&#31995;&#32479;&#21644;&#29983;&#25104;&#27169;&#22411;&#27880;&#20837;&#20102;&#27963;&#21147;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;CLIP&#25104;&#21151;&#30340;&#20027;&#35201;&#22240;&#32032;&#26159;&#20854;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#27169;&#22411;&#26550;&#26500;&#25110;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;CLIP&#21482;&#25552;&#20379;&#20102;&#20851;&#20110;&#20854;&#25968;&#25454;&#21644;&#22914;&#20309;&#25910;&#38598;&#25968;&#25454;&#30340;&#38750;&#24120;&#26377;&#38480;&#30340;&#20449;&#24687;&#65292;&#23548;&#33268;&#20854;&#20182;&#30740;&#31350;&#21162;&#21147;&#36890;&#36807;&#20351;&#29992;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#36807;&#28388;&#26469;&#37325;&#29616;CLIP&#30340;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24847;&#22312;&#25581;&#31034;CLIP&#30340;&#25968;&#25454;&#25972;&#29702;&#26041;&#27861;&#65292;&#24182;&#22312;&#20844;&#24320;&#32473;&#31038;&#21306;&#30340;&#36807;&#31243;&#20013;&#24341;&#20837;&#20803;&#25968;&#25454;&#25972;&#29702;&#30340;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;MetaCLIP&#65289;&#12290;MetaCLIP&#36890;&#36807;&#23545;&#20803;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#24179;&#34913;&#65292;&#20174;&#21407;&#22987;&#25968;&#25454;&#27744;&#21644;&#20803;&#25968;&#25454;&#65288;&#20174;CLIP&#30340;&#27010;&#24565;&#20013;&#24471;&#20986;&#65289;&#20013;&#20135;&#29983;&#19968;&#20010;&#24179;&#34913;&#30340;&#23376;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#30740;&#31350;&#20005;&#26684;&#38548;&#31163;&#20102;&#27169;&#22411;&#21644;&#35757;&#32451;&#35774;&#32622;&#65292;&#20165;&#19987;&#27880;&#20110;&#25968;&#25454;&#12290;MetaCLIP&#24212;&#29992;&#20110;&#21253;&#21547;400M&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#23545;&#30340;CommonCrawl&#65292;&#24182;&#33719;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Image Pre-training (CLIP) is an approach that has advanced research and applications in computer vision, fueling modern recognition systems and generative models. We believe that the main ingredient to the success of CLIP is its data and not the model architecture or pre-training objective. However, CLIP only provides very limited information about its data and how it has been collected, leading to works that aim to reproduce CLIP's data by filtering with its model parameters. In this work, we intend to reveal CLIP's data curation approach and in our pursuit of making it open to the community introduce Metadata-Curated Language-Image Pre-training (MetaCLIP). MetaCLIP takes a raw data pool and metadata (derived from CLIP's concepts) and yields a balanced subset over the metadata distribution. Our experimental study rigorously isolates the model and training settings, concentrating solely on data. MetaCLIP applied to CommonCrawl with 400M image-text data pairs outper
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20195;&#30721;&#25968;&#25454;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19981;&#21516;&#38454;&#27573;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#20195;&#30721;&#21644;&#25991;&#26412;&#28151;&#21512;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;LLMs&#30340;&#36890;&#29992;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#22312;&#25351;&#20196;&#35843;&#25972;&#38454;&#27573;&#65292;&#20195;&#30721;&#25968;&#25454;&#36171;&#20104;LLMs&#29305;&#23450;&#20219;&#21153;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.16298</link><description>&lt;p&gt;
&#22312;&#21738;&#20010;&#35757;&#32451;&#38454;&#27573;&#65292;&#20195;&#30721;&#25968;&#25454;&#20250;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#65311;
&lt;/p&gt;
&lt;p&gt;
At Which Training Stage Does Cocde Data Help LLMs Reasoning?. (arXiv:2309.16298v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20195;&#30721;&#25968;&#25454;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19981;&#21516;&#38454;&#27573;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#20195;&#30721;&#21644;&#25991;&#26412;&#28151;&#21512;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;LLMs&#30340;&#36890;&#29992;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#22312;&#25351;&#20196;&#35843;&#25972;&#38454;&#27573;&#65292;&#20195;&#30721;&#25968;&#25454;&#36171;&#20104;LLMs&#29305;&#23450;&#20219;&#21153;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25104;&#20026;&#35821;&#35328;&#25216;&#26415;&#30340;&#22522;&#30784;&#12290;&#21463;&#21040;&#20195;&#30721;&#25968;&#25454;&#22312;&#35757;&#32451;LLMs&#20013;&#30340;&#24040;&#22823;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#33258;&#28982;&#32780;&#28982;&#22320;&#24819;&#30693;&#36947;&#22312;&#21738;&#20010;&#35757;&#32451;&#38454;&#27573;&#24341;&#20837;&#20195;&#30721;&#25968;&#25454;&#30495;&#27491;&#21487;&#20197;&#24110;&#21161;LLMs&#36827;&#34892;&#25512;&#29702;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#20195;&#30721;&#25968;&#25454;&#23545;LLMs&#22312;&#19981;&#21516;&#38454;&#27573;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#21035;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#12289;&#25351;&#20196;&#35843;&#25972;&#38454;&#27573;&#20197;&#21450;&#20004;&#32773;&#20043;&#38388;&#24341;&#20837;&#20195;&#30721;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20116;&#20010;&#39046;&#22495;&#20013;&#30340;&#20845;&#20010;&#25512;&#29702;&#20219;&#21153;&#20840;&#38754;&#20844;&#27491;&#22320;&#35780;&#20272;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#20102;&#20851;&#38190;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#26377;&#28145;&#24230;&#27934;&#23519;&#21147;&#30340;&#32467;&#35770;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#20195;&#30721;&#21644;&#25991;&#26412;&#28151;&#21512;&#39044;&#35757;&#32451;LLMs&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;LLMs&#30340;&#36890;&#29992;&#25512;&#29702;&#33021;&#21147;&#65292;&#20960;&#20046;&#19981;&#23545;&#20854;&#20182;&#20219;&#21153;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#22312;&#25351;&#20196;&#35843;&#25972;&#38454;&#27573;&#65292;&#20195;&#30721;&#25968;&#25454;&#36171;&#20104;LLMs&#29305;&#23450;&#20219;&#21153;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have exhibited remarkable reasoning capabilities and become the foundation of language technologies. Inspired by the great success of code data in training LLMs, we naturally wonder at which training stage introducing code data can really help LLMs reasoning. To this end, this paper systematically explores the impact of code data on LLMs at different stages. Concretely, we introduce the code data at the pre-training stage, instruction-tuning stage, and both of them, respectively. Then, the reasoning capability of LLMs is comprehensively and fairly evaluated via six reasoning tasks in five domains. We critically analyze the experimental results and provide conclusions with insights. First, pre-training LLMs with the mixture of code and text can significantly enhance LLMs' general reasoning capability almost without negative transfer on other tasks. Besides, at the instruction-tuning stage, code data endows LLMs the task-specific reasoning capability. Moreove
&lt;/p&gt;</description></item><item><title>Lyra&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#20462;&#27491;&#21644;&#29468;&#24819;&#20462;&#27491;&#20004;&#31181;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#24418;&#24335;&#21270;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#20943;&#36731;&#20102;&#24187;&#35273;&#65292;&#24182;&#25552;&#39640;&#20102;&#35777;&#26126;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15806</link><description>&lt;p&gt;
Lyra: &#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#20013;&#30340;&#21452;&#37325;&#20462;&#27491;&#31574;&#30053;&#30340;&#32534;&#25490;
&lt;/p&gt;
&lt;p&gt;
Lyra: Orchestrating Dual Correction in Automated Theorem Proving. (arXiv:2309.15806v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15806
&lt;/p&gt;
&lt;p&gt;
Lyra&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#20462;&#27491;&#21644;&#29468;&#24819;&#20462;&#27491;&#20004;&#31181;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#24418;&#24335;&#21270;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#20943;&#36731;&#20102;&#24187;&#35273;&#65292;&#24182;&#25552;&#39640;&#20102;&#35777;&#26126;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#24418;&#24335;&#21270;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#25506;&#32034;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#20851;&#20110;&#24187;&#35273;&#30340;&#20943;&#36731;&#21644;&#36890;&#36807;&#35777;&#26126;&#22120;&#38169;&#35823;&#28040;&#24687;&#30340;&#32454;&#21270;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#22686;&#24378;LLMs&#22312;&#35813;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Lyra&#65292;&#19968;&#31181;&#37319;&#29992;&#20004;&#31181;&#19981;&#21516;&#20462;&#27491;&#26426;&#21046;&#30340;&#26032;&#26694;&#26550;&#65306;&#24037;&#20855;&#20462;&#27491;&#65288;TC&#65289;&#21644;&#29468;&#24819;&#20462;&#27491;&#65288;CC&#65289;&#12290;&#20026;&#20102;&#22312;&#24418;&#24335;&#35777;&#26126;&#30340;&#21518;&#22788;&#29702;&#20013;&#23454;&#29616;&#24037;&#20855;&#20462;&#27491;&#65292;&#25105;&#20204;&#21033;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#26469;&#21033;&#29992;&#39044;&#23450;&#20041;&#30340;&#35777;&#26126;&#24037;&#20855;&#65288;&#22914;Sledgehammer&#65289;&#26469;&#25351;&#23548;&#26367;&#25442;&#19981;&#27491;&#30830;&#30340;&#24037;&#20855;&#12290;&#24037;&#20855;&#20462;&#27491;&#26174;&#33879;&#20943;&#36731;&#20102;&#24187;&#35273;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35777;&#26126;&#30340;&#25972;&#20307;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29468;&#24819;&#20462;&#27491;&#65292;&#19968;&#31181;&#38169;&#35823;&#21453;&#39304;&#26426;&#21046;&#65292;&#26088;&#22312;&#19982;&#35777;&#26126;&#22120;&#20114;&#21160;&#65292;&#36890;&#36807;&#35777;&#26126;&#22120;&#30340;&#38169;&#35823;&#28040;&#24687;&#36827;&#19968;&#27493;&#23436;&#21892;&#24418;&#24335;&#35777;&#26126;&#30340;&#29468;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) present an intriguing avenue for exploration in the field of formal theorem proving. Nevertheless, their full potential, particularly concerning the mitigation of hallucinations and refinement through prover error messages, remains an area that has yet to be thoroughly investigated. To enhance the effectiveness of LLMs in the field, we introduce the Lyra, a new framework that employs two distinct correction mechanisms: Tool Correction (TC) and Conjecture Correction (CC). To implement Tool Correction in the post-processing of formal proofs, we leverage prior knowledge to utilize predefined prover tools (e.g., Sledgehammer) for guiding the replacement of incorrect tools. Tool Correction significantly contributes to mitigating hallucinations, thereby improving the overall accuracy of the proof. In addition, we introduce Conjecture Correction, an error feedback mechanism designed to interact with prover to refine formal proof conjectures with prover error messa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25805;&#25511;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#30693;&#35782;&#26816;&#32034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#31616;&#21333;&#30340;&#20998;&#31867;&#12289;&#27604;&#36739;&#21644;&#36870;&#21521;&#25628;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20316;&#32773;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#36825;&#20123;&#20869;&#22312;&#30340;&#24369;&#28857;&#65306;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#39640;&#25928;&#22320;&#25805;&#25511;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2309.14402</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#29702;&#23398;&#65306;&#31532;3.2&#37096;&#20998;&#65292;&#30693;&#35782;&#25805;&#25511;
&lt;/p&gt;
&lt;p&gt;
Physics of Language Models: Part 3.2, Knowledge Manipulation. (arXiv:2309.14402v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25805;&#25511;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#30693;&#35782;&#26816;&#32034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#31616;&#21333;&#30340;&#20998;&#31867;&#12289;&#27604;&#36739;&#21644;&#36870;&#21521;&#25628;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20316;&#32773;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#36825;&#20123;&#20869;&#22312;&#30340;&#24369;&#28857;&#65306;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#39640;&#25928;&#22320;&#25805;&#25511;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23384;&#20648;&#22823;&#37327;&#20107;&#23454;&#30693;&#35782;&#65292;&#20294;&#23427;&#20204;&#22312;&#20351;&#29992;&#36825;&#20123;&#30693;&#35782;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#28982;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25805;&#25511;&#20854;&#23384;&#20648;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#22235;&#31181;&#25805;&#25511;&#31867;&#22411;&#65306;&#26816;&#32034;&#65288;&#20363;&#22914;&#65292;&#8220;A&#30340;&#23646;&#24615;X&#26159;&#20160;&#20040;&#8221;&#65289;&#12289;&#20998;&#31867;&#65288;&#20363;&#22914;&#65292;&#8220;A&#30340;&#23646;&#24615;X&#26159;&#22855;&#25968;&#36824;&#26159;&#20598;&#25968;&#8221;&#65289;&#12289;&#27604;&#36739;&#65288;&#20363;&#22914;&#65292;&#8220;&#22312;&#23646;&#24615;X&#20013;A&#26159;&#21542;&#22823;&#20110;B&#8221;&#65289;&#21644;&#36870;&#21521;&#25628;&#32034;&#65288;&#20363;&#22914;&#65292;&#8220;&#21738;&#20010;&#20154;&#30340;&#23646;&#24615;X&#31561;&#20110;T&#8221;&#65289;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20687;GPT2/3/4&#36825;&#26679;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#26816;&#32034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#31616;&#21333;&#30340;&#20998;&#31867;&#25110;&#27604;&#36739;&#20219;&#21153;&#20013;&#24456;&#38590;&#32988;&#20219;&#65292;&#38500;&#38750;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#37319;&#29992;&#20102;Chain of Thoughts&#65288;CoTs&#65289;&#12290;&#26080;&#35770;&#25552;&#31034;&#26159;&#20160;&#20040;&#65292;&#23427;&#20204;&#22312;&#36870;&#21521;&#30693;&#35782;&#25628;&#32034;&#20013;&#34920;&#29616;&#37117;&#24456;&#24046;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#20010;&#20026;&#25511;&#21046;&#23454;&#39564;&#32780;&#35774;&#35745;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#35777;&#23454;&#20102;&#36825;&#20123;&#20869;&#22312;&#30340;&#24369;&#28857;&#65306;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#39640;&#25928;&#22320;&#25805;&#25511;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models can store vast amounts of factual knowledge, but their ability to use this knowledge for logical reasoning remains questionable. This paper explores a language model's ability to manipulate its stored knowledge during inference. We focus on four manipulation types: retrieval (e.g., "What is person A's attribute X"), classification (e.g., "Is A's attribute X even or odd?"), comparison (e.g., "Is A greater than B in attribute X?") and inverse search (e.g., "Which person's attribute X equals T?")  We observe that pre-trained language models like GPT2/3/4 excel in knowledge retrieval but struggle with simple classification or comparison tasks unless Chain of Thoughts (CoTs) are employed during both training and inference. They also perform poorly in inverse knowledge search, irrespective of the prompts. Our primary contribution is a synthetic dataset for a controlled experiment that confirms these inherent weaknesses: a language model cannot efficiently manipulate knowledge
&lt;/p&gt;</description></item><item><title>DeepSpeed-VisualChat&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#36718;&#22810;&#22270;&#20132;&#38169;&#32842;&#22825;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#22810;&#27169;&#24577;&#22240;&#26524;&#20851;&#27880;&#26426;&#21046;&#21644;&#25968;&#25454;&#34701;&#21512;&#25216;&#26415;&#65292;&#20855;&#26377;&#20248;&#36234;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14327</link><description>&lt;p&gt;
DeepSpeed-VisualChat&#65306;&#36890;&#36807;&#22810;&#27169;&#24577;&#22240;&#26524;&#20851;&#27880;&#23454;&#29616;&#30340;&#22810;&#36718;&#22810;&#22270;&#20132;&#38169;&#32842;&#22825;
&lt;/p&gt;
&lt;p&gt;
DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention. (arXiv:2309.14327v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14327
&lt;/p&gt;
&lt;p&gt;
DeepSpeed-VisualChat&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#36718;&#22810;&#22270;&#20132;&#38169;&#32842;&#22825;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#22810;&#27169;&#24577;&#22240;&#26524;&#20851;&#27880;&#26426;&#21046;&#21644;&#25968;&#25454;&#34701;&#21512;&#25216;&#26415;&#65292;&#20855;&#26377;&#20248;&#36234;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;&#22810;&#27169;&#24577;&#27169;&#22411;&#30001;&#20110;&#26080;&#27861;&#29087;&#32451;&#22320;&#22788;&#29702;&#22810;&#22270;&#12289;&#22810;&#22238;&#21512;&#23545;&#35805;&#20013;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#65292;&#38754;&#20020;&#30528;&#22312;&#35757;&#32451;&#36164;&#28304;&#20998;&#37197;&#21644;&#25968;&#25454;&#21487;&#35775;&#38382;&#24615;&#26041;&#38754;&#30340;&#37325;&#35201;&#38480;&#21046;&#65292;&#36825;&#24433;&#21709;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#20132;&#20114;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; DeepSpeed-VisualChat &#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#34701;&#21512;&#22810;&#27169;&#24577;&#21151;&#33021;&#65292;&#38598;&#20013;&#25552;&#39640;&#22823;&#22411;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#20132;&#38169;&#36755;&#20837;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#26174;&#33879;&#29305;&#28857;&#22312;&#20110;&#65306;(1) &#25552;&#20379;&#23545;&#22810;&#36718;&#22810;&#22270;&#23545;&#35805;&#30340;&#24320;&#28304;&#25903;&#25345;&#65292;(2) &#24341;&#20837;&#21019;&#26032;&#30340;&#22810;&#27169;&#24577;&#22240;&#26524;&#20851;&#27880;&#26426;&#21046;&#65292;&#20197;&#21450; (3) &#22312;&#29616;&#26377;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#25968;&#25454;&#34701;&#21512;&#25216;&#26415;&#65292;&#20197;&#30830;&#20445;&#22810;&#36718;&#22810;&#22270;&#23545;&#35805;&#20013;&#30340;&#26080;&#32541;&#20132;&#20114;&#12290;&#19982;&#29616;&#26377;&#26694;&#26550;&#30456;&#27604;&#65292;DeepSpeed-VisualChat &#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#34920;&#29616;&#65292;&#21487;&#36798;&#21040; 70B &#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of the existing multi-modal models, hindered by their incapacity to adeptly manage interleaved image-and-text inputs in multi-image, multi-round dialogues, face substantial constraints in resource allocation for training and data accessibility, impacting their adaptability and scalability across varied interaction realms. To address this, we present the DeepSpeed-VisualChat framework, designed to optimize Large Language Models (LLMs) by incorporating multi-modal capabilities, with a focus on enhancing the proficiency of Large Vision and Language Models in handling interleaved inputs. Our framework is notable for (1) its open-source support for multi-round and multi-image dialogues, (2) introducing an innovative multi-modal causal attention mechanism, and (3) utilizing data blending techniques on existing datasets to assure seamless interactions in multi-round, multi-image conversations. Compared to existing frameworks, DeepSpeed-VisualChat shows superior scalability up to 70B para
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22797;&#29616;&#20102;Whisper&#39118;&#26684;&#30340;&#35757;&#32451;&#65292;&#20351;&#29992;&#24320;&#28304;&#24037;&#20855;&#21644;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OWSM&#30340;&#27169;&#22411;&#65292;&#25903;&#25345;&#26356;&#22810;&#30340;&#32763;&#35793;&#26041;&#21521;&#24182;&#19988;&#26356;&#39640;&#25928;&#22320;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2309.13876</link><description>&lt;p&gt;
&#20351;&#29992;&#24320;&#28304;&#24037;&#20855;&#21644;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#22797;&#29616;Whisper&#39118;&#26684;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Reproducing Whisper-Style Training Using an Open-Source Toolkit and Publicly Available Data. (arXiv:2309.13876v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22797;&#29616;&#20102;Whisper&#39118;&#26684;&#30340;&#35757;&#32451;&#65292;&#20351;&#29992;&#24320;&#28304;&#24037;&#20855;&#21644;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OWSM&#30340;&#27169;&#22411;&#65292;&#25903;&#25345;&#26356;&#22810;&#30340;&#32763;&#35793;&#26041;&#21521;&#24182;&#19988;&#26356;&#39640;&#25928;&#22320;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#37327;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;OpenAI&#30340;Whisper&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#32463;&#36807;&#20102;680k&#23567;&#26102;&#30340;&#30417;&#30563;&#24335;&#35821;&#38899;&#25968;&#25454;&#35757;&#32451;&#12290;&#23427;&#22312;&#21508;&#31181;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#20063;&#33021;&#22815;&#21457;&#25381;&#33391;&#22909;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#36825;&#31181;&#27169;&#22411;&#30340;&#23436;&#25972;&#27969;&#31243;&#65288;&#20174;&#25968;&#25454;&#25910;&#38598;&#21040;&#35757;&#32451;&#65289;&#24182;&#19981;&#20844;&#24320;&#21487;&#35775;&#38382;&#65292;&#36825;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#38590;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#20854;&#24615;&#33021;&#24182;&#35299;&#20915;&#35757;&#32451;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#22914;&#25928;&#29575;&#12289;&#20581;&#22766;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#20559;&#35265;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Open Whisper-style Speech Model&#65288;OWSM&#65289;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#24320;&#28304;&#24037;&#20855;&#21644;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#22797;&#29616;&#20102;Whisper&#39118;&#26684;&#30340;&#35757;&#32451;&#12290;OWSM&#29978;&#33267;&#25903;&#25345;&#26356;&#22810;&#30340;&#32763;&#35793;&#26041;&#21521;&#65292;&#24182;&#19988;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;&#20844;&#24320;&#21457;&#24067;&#29992;&#20110;&#25968;&#25454;&#20934;&#22791;&#12289;&#35757;&#32451;&#12289;&#25512;&#29702;&#21644;&#35780;&#20998;&#30340;&#25152;&#26377;&#33050;&#26412;&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#35757;&#32451;&#26085;&#24535;&#65292;&#20197;&#20419;&#36827;&#24320;&#25918;&#31185;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It generalizes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for developing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further improve its performance and address training-related issues such as efficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisper-style training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pre-trained models and training logs to promote open science.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20174;&#20132;&#20114;&#24335;&#21465;&#20107;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35282;&#24230;&#23545;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#20013;&#28216;&#25103;&#20027;&#25345;&#36827;&#34892;&#24314;&#27169;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#27979;&#35797;&#31867;&#21035;&#26469;&#35780;&#20272;&#23545;&#35805;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.13702</link><description>&lt;p&gt;
&#25216;&#33021;&#26816;&#27979;&#65306;&#35780;&#20272;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#20013;&#28216;&#25103;&#20027;&#25345;&#27169;&#22411;&#30340;&#19968;&#20123;&#32771;&#34385;
&lt;/p&gt;
&lt;p&gt;
Skill Check: Some Considerations on the Evaluation of Gamemastering Models for Role-playing Games. (arXiv:2309.13702v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20174;&#20132;&#20114;&#24335;&#21465;&#20107;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35282;&#24230;&#23545;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#20013;&#28216;&#25103;&#20027;&#25345;&#36827;&#34892;&#24314;&#27169;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#27979;&#35797;&#31867;&#21035;&#26469;&#35780;&#20272;&#23545;&#35805;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#20013;&#65292;&#28216;&#25103;&#20027;&#25345;&#65288;GM&#65289;&#26159;&#36127;&#36131;&#28216;&#25103;&#30340;&#29609;&#23478;&#65292;&#24517;&#39035;&#35774;&#35745;&#29609;&#23478;&#38754;&#20020;&#30340;&#25361;&#25112;&#24182;&#35762;&#36848;&#20182;&#20204;&#34892;&#21160;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#20174;&#20132;&#20114;&#24335;&#21465;&#20107;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35282;&#24230;&#35752;&#35770;&#20102;&#23545;GM&#36827;&#34892;&#24314;&#27169;&#30340;&#25361;&#25112;&#12290;&#22312;&#35752;&#35770;&#36825;&#20123;&#25361;&#25112;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#27979;&#35797;&#31867;&#21035;&#26469;&#35780;&#20272;&#36825;&#20123;&#23545;&#35805;&#31995;&#32479;&#65292;&#24182;&#20351;&#29992;ChatGPT&#12289;Bard&#21644;OpenAssistant&#20316;&#20026;&#24320;&#31665;&#21363;&#29992;&#30340;GM&#36827;&#34892;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
In role-playing games a Game Master (GM) is the player in charge of the game, who must design the challenges the players face and narrate the outcomes of their actions. In this work we discuss some challenges to model GMs from an Interactive Storytelling and Natural Language Processing perspective. Following those challenges we propose three test categories to evaluate such dialogue systems, and we use them to test ChatGPT, Bard and OpenAssistant as out-of-the-box GMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#26041;&#27861;&#65292;&#21033;&#29992;BioBERT&#27169;&#22411;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#29983;&#27542;&#32454;&#32990;&#31995;&#22522;&#22240;&#19982;&#30142;&#30149;&#30340;&#20851;&#32852;&#65292;&#23637;&#31034;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#37325;&#35201;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2309.13061</link><description>&lt;p&gt;
&#23558;BioBERT&#24212;&#29992;&#20110;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#29983;&#27542;&#32454;&#32990;&#31995;&#22522;&#22240;&#19982;&#30142;&#30149;&#20851;&#32852;&#20197;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Applying BioBERT to Extract Germline Gene-Disease Associations for Building a Knowledge Graph from the Biomedical Literature. (arXiv:2309.13061v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#26041;&#27861;&#65292;&#21033;&#29992;BioBERT&#27169;&#22411;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#29983;&#27542;&#32454;&#32990;&#31995;&#22522;&#22240;&#19982;&#30142;&#30149;&#30340;&#20851;&#32852;&#65292;&#23637;&#31034;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#37325;&#35201;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#34920;&#30340;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#26368;&#26032;&#36827;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#33258;&#21160;&#25552;&#21462;&#12289;&#35268;&#33539;&#21270;&#21644;&#34920;&#31034;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;(&#22914;&#22522;&#22240;&#21644;&#30142;&#30149;)&#30693;&#35782;&#30340;&#27987;&#21402;&#20852;&#36259;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22522;&#22240;&#21644;&#30142;&#30149;&#39046;&#22495;&#30340;&#29983;&#27542;&#32454;&#32990;&#31995;&#25688;&#35201;&#65292;&#29992;&#20110;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#20197;&#23637;&#31034;&#36825;&#19968;&#39046;&#22495;&#30340;&#22823;&#37327;&#24037;&#20316;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SimpleGermKG&#30340;&#33258;&#21160;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#26041;&#27861;&#65292;&#23558;&#29983;&#27542;&#32454;&#32990;&#31995;&#22522;&#22240;&#21644;&#30142;&#30149;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;BioBERT&#27169;&#22411;&#26469;&#25552;&#21462;&#22522;&#22240;&#21644;&#30142;&#30149;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26412;&#20307;&#21644;&#35268;&#21017;&#30340;&#31639;&#27861;&#26469;&#35268;&#33539;&#21270;&#21644;&#28040;&#27495;&#20041;&#21307;&#23398;&#26415;&#35821;&#12290;&#23545;&#20110;&#25991;&#31456;&#12289;&#22522;&#22240;&#21644;&#30142;&#30149;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#31181;&#37096;&#20998;-&#25972;&#20307;&#20851;&#31995;&#26041;&#27861;&#26469;&#23558;&#27599;&#20010;&#23454;&#20307;&#19982;&#20854;&#25968;&#25454;&#28304;&#36830;&#25509;&#24182;&#20197;&#22270;&#24418;&#21270;&#30693;&#35782;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Published biomedical information has and continues to rapidly increase. The recent advancements in Natural Language Processing (NLP), have generated considerable interest in automating the extraction, normalization, and representation of biomedical knowledge about entities such as genes and diseases. Our study analyzes germline abstracts in the construction of knowledge graphs of the of the immense work that has been done in this area for genes and diseases. This paper presents SimpleGermKG, an automatic knowledge graph construction approach that connects germline genes and diseases. For the extraction of genes and diseases, we employ BioBERT, a pre-trained BERT model on biomedical corpora. We propose an ontology-based and rule-based algorithm to standardize and disambiguate medical terms. For semantic relationships between articles, genes, and diseases, we implemented a part-whole relation approach to connect each entity with its data source and visualize them in a graph-based knowled
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#21644;&#23376;&#35789;&#26631;&#35760;&#23545;&#26426;&#22120;&#32763;&#35793;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27169;&#22411;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#24615;&#21035;&#24418;&#24335;&#30340;&#19981;&#24179;&#34913;&#26159;&#23548;&#33268;&#24615;&#21035;&#20559;&#35265;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#32780;&#23376;&#35789;&#25286;&#20998;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#36890;&#36807;&#20998;&#26512;&#23376;&#35789;&#25286;&#20998;&#21487;&#20197;&#24456;&#22909;&#22320;&#20272;&#35745;&#35757;&#32451;&#25968;&#25454;&#20013;&#24615;&#21035;&#24418;&#24335;&#30340;&#19981;&#24179;&#34913;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#20165;&#24494;&#35843;&#26631;&#35760;&#23884;&#20837;&#23618;&#21487;&#20197;&#20943;&#23569;&#22899;&#24615;&#21644;&#30007;&#24615;&#20043;&#38388;&#24615;&#21035;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2309.12491</link><description>&lt;p&gt;
&#25506;&#32034;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#21644;&#23376;&#35789;&#26631;&#35760;&#23545;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring the Impact of Training Data Distribution and Subword Tokenization on Gender Bias in Machine Translation. (arXiv:2309.12491v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12491
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#21644;&#23376;&#35789;&#26631;&#35760;&#23545;&#26426;&#22120;&#32763;&#35793;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27169;&#22411;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#24615;&#21035;&#24418;&#24335;&#30340;&#19981;&#24179;&#34913;&#26159;&#23548;&#33268;&#24615;&#21035;&#20559;&#35265;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#32780;&#23376;&#35789;&#25286;&#20998;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#36890;&#36807;&#20998;&#26512;&#23376;&#35789;&#25286;&#20998;&#21487;&#20197;&#24456;&#22909;&#22320;&#20272;&#35745;&#35757;&#32451;&#25968;&#25454;&#20013;&#24615;&#21035;&#24418;&#24335;&#30340;&#19981;&#24179;&#34913;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#20165;&#24494;&#35843;&#26631;&#35760;&#23884;&#20837;&#23618;&#21487;&#20197;&#20943;&#23569;&#22899;&#24615;&#21644;&#30007;&#24615;&#20043;&#38388;&#24615;&#21035;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26631;&#35760;&#21270;&#23545;&#26426;&#22120;&#32763;&#35793;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#36825;&#26159;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#34987;&#22823;&#22810;&#25968;&#20154;&#24573;&#35270;&#30340;&#19968;&#20010;&#26041;&#38754;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#35757;&#32451;&#25968;&#25454;&#20013;&#24615;&#21035;&#32844;&#19994;&#21517;&#31216;&#30340;&#39057;&#29575;&#12289;&#23427;&#20204;&#22312;&#23376;&#35789;&#26631;&#35760;&#22120;&#35789;&#27719;&#34920;&#20013;&#30340;&#34920;&#31034;&#20197;&#21450;&#24615;&#21035;&#20559;&#35265;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22899;&#24615;&#21644;&#38750;&#21051;&#26495;&#21360;&#35937;&#30340;&#24615;&#21035;&#32844;&#19994;&#21517;&#31216;&#30340;&#21464;&#24418;&#65288;&#20363;&#22914;&#65292;&#35199;&#29677;&#29273;&#35821;&#20013;&#30340;"doctora"&#34920;&#31034;"&#22899;&#21307;&#29983;"&#65289;&#24448;&#24448;&#34987;&#25286;&#20998;&#25104;&#22810;&#20010;&#23376;&#35789;&#26631;&#35760;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#24615;&#21035;&#24418;&#24335;&#30340;&#19981;&#24179;&#34913;&#26159;&#23548;&#33268;&#24615;&#21035;&#20559;&#35265;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#20854;&#24433;&#21709;&#22823;&#20110;&#23376;&#35789;&#25286;&#20998;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#26512;&#23376;&#35789;&#25286;&#20998;&#21487;&#20197;&#24456;&#22909;&#22320;&#20272;&#35745;&#35757;&#32451;&#25968;&#25454;&#20013;&#24615;&#21035;&#24418;&#24335;&#30340;&#19981;&#24179;&#34913;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#35821;&#26009;&#24211;&#19981;&#20844;&#24320;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#20165;&#24494;&#35843;&#26631;&#35760;&#23884;&#20837;&#23618;&#21487;&#20197;&#20943;&#23569;&#22899;&#24615;&#21644;&#30007;&#24615;&#20043;&#38388;&#24615;&#21035;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the effect of tokenization on gender bias in machine translation, an aspect that has been largely overlooked in previous works. Specifically, we focus on the interactions between the frequency of gendered profession names in training data, their representation in the subword tokenizer's vocabulary, and gender bias. We observe that female and non-stereotypical gender inflections of profession names (e.g., Spanish "doctora" for "female doctor") tend to be split into multiple subword tokens. Our results indicate that the imbalance of gender forms in the model's training corpus is a major factor contributing to gender bias and has a greater impact than subword splitting. We show that analyzing subword splits provides good estimates of gender-form imbalance in the training data and can be used even when the corpus is not publicly available. We also demonstrate that fine-tuning just the token embedding layer can decrease the gap in gender prediction accuracy between female and male 
&lt;/p&gt;</description></item><item><title>LMSYS-Chat-1M&#26159;&#19968;&#20010;&#21253;&#21547;&#19968;&#30334;&#19975;&#20010;&#23454;&#38469;&#23545;&#35805;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20854;&#22810;&#26679;&#24615;&#21644;&#29992;&#20363;&#23637;&#31034;&#20102;&#20854;&#22312;&#29702;&#35299;&#21644;&#25512;&#36827;LLM&#33021;&#21147;&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.11998</link><description>&lt;p&gt;
LMSYS-Chat-1M&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#23454;&#38469;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset. (arXiv:2309.11998v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11998
&lt;/p&gt;
&lt;p&gt;
LMSYS-Chat-1M&#26159;&#19968;&#20010;&#21253;&#21547;&#19968;&#30334;&#19975;&#20010;&#23454;&#38469;&#23545;&#35805;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20854;&#22810;&#26679;&#24615;&#21644;&#29992;&#20363;&#23637;&#31034;&#20102;&#20854;&#22312;&#29702;&#35299;&#21644;&#25512;&#36827;LLM&#33021;&#21147;&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#30740;&#31350;&#20154;&#20204;&#22914;&#20309;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19982;&#20854;&#20132;&#20114;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LMSYS-Chat-1M&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#19968;&#30334;&#19975;&#20010;&#19982;25&#20010;&#26368;&#20808;&#36827;&#30340;LLM&#36827;&#34892;&#30340;&#23454;&#38469;&#23545;&#35805;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#26159;&#20174;&#25105;&#20204;&#30340;Vicuna&#28436;&#31034;&#21644;Chatbot Arena&#32593;&#31449;&#19978;&#30340;21&#19975;&#20010;&#29420;&#31435;IP&#22320;&#22336;&#20013;&#25910;&#38598;&#32780;&#26469;&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#20869;&#23481;&#30340;&#27010;&#36848;&#65292;&#21253;&#25324;&#20854;&#31574;&#21010;&#36807;&#31243;&#12289;&#22522;&#26412;&#32479;&#35745;&#25968;&#25454;&#21644;&#20027;&#39064;&#20998;&#24067;&#65292;&#24378;&#35843;&#20854;&#22810;&#26679;&#24615;&#12289;&#29420;&#29305;&#24615;&#21644;&#35268;&#27169;&#12290;&#25105;&#20204;&#36890;&#36807;&#22235;&#20010;&#29992;&#20363;&#23637;&#31034;&#20102;&#23427;&#30340;&#22810;&#26679;&#24615;&#65306;&#24320;&#21457;&#19982;GPT-4&#34920;&#29616;&#30456;&#20284;&#30340;&#20869;&#23481;&#36807;&#28388;&#27169;&#22411;&#12289;&#26500;&#24314;&#19968;&#20010;&#23433;&#20840;&#22522;&#20934;&#12289;&#35757;&#32451;&#19982;Vicuna&#34920;&#29616;&#30456;&#20284;&#30340;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12289;&#21019;&#24314;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#38382;&#39064;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25105;&#20204;&#29702;&#35299;&#21644;&#25512;&#36827;LLM&#33021;&#21147;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities. The dataset is pub
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#24605;&#36335;&#38142;&#30340;&#35774;&#35745;&#26041;&#27861;&#65292;&#23545;&#27604;&#20102;&#33258;&#28982;&#35821;&#35328;&#24605;&#36335;&#38142;&#21644;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#31243;&#24207;&#24605;&#36335;&#38142;&#36890;&#24120;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#26356;&#21152;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#33258;&#25105;&#25551;&#36848;&#31243;&#24207;&#20855;&#26377;&#26356;&#22823;&#22810;&#26679;&#24615;&#19988;&#24615;&#33021;&#26356;&#39640;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;Python&#26159;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#36739;&#22909;&#36873;&#25321;&#12290;&#23454;&#39564;&#32467;&#26524;&#20026;&#26410;&#26469;&#24605;&#36335;&#38142;&#35774;&#35745;&#25552;&#20379;&#20102;&#23453;&#36149;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2309.11054</link><description>&lt;p&gt;
&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#24605;&#36335;&#38142;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Design of Chain-of-Thought in Math Problem Solving. (arXiv:2309.11054v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#24605;&#36335;&#38142;&#30340;&#35774;&#35745;&#26041;&#27861;&#65292;&#23545;&#27604;&#20102;&#33258;&#28982;&#35821;&#35328;&#24605;&#36335;&#38142;&#21644;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#31243;&#24207;&#24605;&#36335;&#38142;&#36890;&#24120;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#26356;&#21152;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#33258;&#25105;&#25551;&#36848;&#31243;&#24207;&#20855;&#26377;&#26356;&#22823;&#22810;&#26679;&#24615;&#19988;&#24615;&#33021;&#26356;&#39640;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;Python&#26159;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#36739;&#22909;&#36873;&#25321;&#12290;&#23454;&#39564;&#32467;&#26524;&#20026;&#26410;&#26469;&#24605;&#36335;&#38142;&#35774;&#35745;&#25552;&#20379;&#20102;&#23453;&#36149;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24605;&#36335;&#38142;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#25105;&#20204;&#23545;&#35774;&#35745;&#24605;&#36335;&#38142;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#32771;&#23519;&#65292;&#27604;&#36739;&#20102;&#20256;&#32479;&#33258;&#28982;&#35821;&#35328;&#24605;&#36335;&#38142;&#21644;&#21508;&#31181;&#31243;&#24207;&#24605;&#36335;&#38142;&#65292;&#21253;&#25324;&#33258;&#25105;&#25551;&#36848;&#31243;&#24207;&#12289;&#27880;&#37322;&#25551;&#36848;&#31243;&#24207;&#21644;&#38750;&#25551;&#36848;&#31243;&#24207;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#32534;&#31243;&#35821;&#35328;&#23545;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#24433;&#21709;&#65292;&#27604;&#36739;&#20102;Python&#21644;Wolfram&#35821;&#35328;&#12290;&#36890;&#36807;&#23545;GSM8K&#12289;MATHQA&#21644;SVAMP&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#31243;&#24207;&#24605;&#36335;&#38142;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#36890;&#24120;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20855;&#26377;30B&#21442;&#25968;&#30340;&#26368;&#20339;&#32452;&#21512;&#26126;&#26174;&#36229;&#36807;&#20102;GPT-3.5-turbo&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#33258;&#25105;&#25551;&#36848;&#31243;&#24207;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#22810;&#26679;&#24615;&#65292;&#22240;&#27492;&#36890;&#24120;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;Python&#26159;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#26356;&#22909;&#36873;&#25321;&#27604;Wolfram&#35821;&#35328;&#12290;&#23454;&#39564;&#32467;&#26524;&#20026;&#26410;&#26469;&#32771;&#34385;&#22240;&#32032;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought (CoT) plays a crucial role in reasoning for math problem solving. We conduct a comprehensive examination of methods for designing CoT, comparing conventional natural language CoT with various program CoTs, including the self-describing program, the comment-describing program, and the non-describing program. Furthermore, we investigate the impact of programming language on program CoTs, comparing Python and Wolfram Language. Through extensive experiments on GSM8K, MATHQA, and SVAMP, we find that program CoTs often have superior effectiveness in math problem solving. Notably, the best performing combination with 30B parameters beats GPT-3.5-turbo by a significant margin. The results show that self-describing program offers greater diversity and thus can generally achieve higher performance. We also find that Python is a better choice of language than Wolfram for program CoTs. The experimental results provide a valuable guideline for future CoT designs that take into acco
&lt;/p&gt;</description></item><item><title>OpenBA&#26159;&#19968;&#31181;&#24320;&#28304;&#30340;15B&#21452;&#35821;&#19981;&#23545;&#31216;seq2seq&#27169;&#22411;&#65292;&#36890;&#36807;&#19977;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#65292;&#21487;&#22312;&#21482;&#26377;380B&#20196;&#29260;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#38750;&#24120;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#20026;&#20013;&#22269;&#23450;&#21521;&#30340;&#24320;&#28304;&#27169;&#22411;&#31038;&#21306;&#36129;&#29486;&#20102;&#19968;&#31181;LLM&#21464;&#20307;&#12290;</title><link>http://arxiv.org/abs/2309.10706</link><description>&lt;p&gt;
OpenBA: &#19968;&#31181;&#20174;&#22836;&#24320;&#22987;&#39044;&#35757;&#32451;&#30340;&#24320;&#28304;15B&#21452;&#35821;&#19981;&#23545;&#31216;seq2seq&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model Pre-trained from Scratch. (arXiv:2309.10706v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10706
&lt;/p&gt;
&lt;p&gt;
OpenBA&#26159;&#19968;&#31181;&#24320;&#28304;&#30340;15B&#21452;&#35821;&#19981;&#23545;&#31216;seq2seq&#27169;&#22411;&#65292;&#36890;&#36807;&#19977;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#65292;&#21487;&#22312;&#21482;&#26377;380B&#20196;&#29260;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#38750;&#24120;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#20026;&#20013;&#22269;&#23450;&#21521;&#30340;&#24320;&#28304;&#27169;&#22411;&#31038;&#21306;&#36129;&#29486;&#20102;&#19968;&#31181;LLM&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#26412;&#25253;&#21578;&#20171;&#32461;&#20102;OpenBA&#65292;&#19968;&#31181;&#24320;&#28304;&#30340;15B&#21452;&#35821;&#19981;&#23545;&#31216;seq2seq&#27169;&#22411;&#65292;&#20026;&#20013;&#22269;&#23450;&#21521;&#30340;&#24320;&#28304;&#27169;&#22411;&#31038;&#21306;&#36129;&#29486;&#20102;&#19968;&#31181;LLM&#21464;&#20307;&#12290;&#25105;&#20204;&#36890;&#36807;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#25216;&#26415;&#26469;&#22686;&#24378;OpenBA&#65292;&#24182;&#37319;&#29992;&#19977;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#21482;&#26377;380B&#20196;&#29260;&#30340;&#24773;&#20917;&#19979;&#20063;&#21487;&#20197;&#21462;&#24471;&#38750;&#24120;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;BELEBELE&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;LLaMA-70B&#65292;MMLU&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;BLOOM-176B&#21644;C-Eval&#65288;hard&#65289;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;GLM-130B&#12290;&#26412;&#25253;&#21578;&#25552;&#20379;&#20102;&#39044;&#35757;&#32451;&#31867;&#20284;&#27169;&#22411;&#30340;&#20027;&#35201;&#32454;&#33410;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#25968;&#25454;&#22788;&#29702;&#65292;&#21452;&#35821;&#25991;&#38598;&#25968;&#25454;&#25910;&#38598;&#65292;&#21551;&#21457;&#25105;&#20204;&#27169;&#22411;&#26550;&#26500;&#35774;&#35745;&#30340;&#32463;&#39564;&#35266;&#23519;&#65292;&#19981;&#21516;&#38454;&#27573;&#30340;&#35757;&#32451;&#30446;&#26631;&#20197;&#21450;&#20854;&#20182;&#22686;&#24378;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#24494;&#35843;&#30340;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) with billions of parameters have demonstrated outstanding performance on various natural language processing tasks. This report presents OpenBA, an open-sourced 15B bilingual asymmetric seq2seq model, to contribute an LLM variant to the Chinese-oriented open-source model community. We enhance OpenBA with effective and efficient techniques as well as adopt a three-stage training strategy to train the model from scratch. Our solution can also achieve very competitive performance with only 380B tokens, which is better than LLaMA-70B on the BELEBELE benchmark, BLOOM-176B on the MMLU benchmark, GLM-130B on the C-Eval (hard) benchmark. This report provides the main details to pre-train an analogous model, including pre-training data processing, Bilingual Flan data collection, the empirical observations that inspire our model architecture design, training objectives of different stages, and other enhancement techniques. Additionally, we also provide the fine-tunin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#22871;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#33521;&#25991;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#20256;&#32479;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#36825;&#20123;&#22522;&#20934;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#19978;&#19979;&#25991;&#38382;&#31572;&#12289;&#25688;&#35201;&#12289;&#20998;&#31867;&#21644;&#34920;&#26684;&#29702;&#35299;&#31561;&#22810;&#20010;&#20219;&#21153;&#65292;&#20026;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19979;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.08448</link><description>&lt;p&gt;
&#25512;&#36827;&#20256;&#32479;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#65306;&#36808;&#21521;&#20840;&#38754;&#22522;&#20934;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
Advancing the Evaluation of Traditional Chinese Language Models: Towards a Comprehensive Benchmark Suite. (arXiv:2309.08448v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#22871;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#33521;&#25991;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#20256;&#32479;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#36825;&#20123;&#22522;&#20934;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#19978;&#19979;&#25991;&#38382;&#31572;&#12289;&#25688;&#35201;&#12289;&#20998;&#31867;&#21644;&#34920;&#26684;&#29702;&#35299;&#31561;&#22810;&#20010;&#20219;&#21153;&#65292;&#20026;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19979;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#39046;&#22495;&#20013;&#65292;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#35780;&#20272;&#20854;&#24615;&#33021;&#30340;&#26377;&#25928;&#22522;&#20934;&#30340;&#38656;&#27714;&#21464;&#24471;&#36843;&#20999;&#12290;&#22312;&#20013;&#25991;&#35821;&#22659;&#19979;&#65292;&#23613;&#31649;&#23384;&#22312;&#19968;&#20123;&#22522;&#20934;&#25968;&#25454;&#38598;&#22914;DRCD&#12289;TTQA&#12289;CMDQA&#21644;FGC&#65292;&#20294;&#32570;&#20047;&#20840;&#38754;&#22810;&#26679;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#33521;&#25991;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#20256;&#32479;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#12290;&#36825;&#20123;&#22522;&#20934;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#19978;&#19979;&#25991;&#38382;&#31572;&#12289;&#25688;&#35201;&#12289;&#20998;&#31867;&#21644;&#34920;&#26684;&#29702;&#35299;&#12290;&#25152;&#25552;&#20986;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19979;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;GPT-3.5&#21644;Taiwa&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evaluation of large language models is an essential task in the field of language understanding and generation. As language models continue to advance, the need for effective benchmarks to assess their performance has become imperative. In the context of Traditional Chinese, there is a scarcity of comprehensive and diverse benchmarks to evaluate the capabilities of language models, despite the existence of certain benchmarks such as DRCD, TTQA, CMDQA, and FGC dataset. To address this gap, we propose a novel set of benchmarks that leverage existing English datasets and are tailored to evaluate language models in Traditional Chinese. These benchmarks encompass a wide range of tasks, including contextual question-answering, summarization, classification, and table understanding. The proposed benchmarks offer a comprehensive evaluation framework, enabling the assessment of language models' capabilities across different tasks. In this paper, we evaluate the performance of GPT-3.5, Taiwa
&lt;/p&gt;</description></item><item><title>MMICL&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#35774;&#35745;&#65292;&#20197;&#35299;&#20915;VLM&#22312;&#29702;&#35299;&#22797;&#26434;&#22810;&#27169;&#24577;&#25552;&#31034;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2309.07915</link><description>&lt;p&gt;
MMICL&#65306;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#22686;&#24378;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning. (arXiv:2309.07915v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07915
&lt;/p&gt;
&lt;p&gt;
MMICL&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#35774;&#35745;&#65292;&#20197;&#35299;&#20915;VLM&#22312;&#29702;&#35299;&#22797;&#26434;&#22810;&#27169;&#24577;&#25552;&#31034;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#28145;&#24230;&#23398;&#20064;&#30340;&#22797;&#33487;&#24320;&#22987;&#65292;&#20511;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#21464;&#24471;&#38750;&#24120;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;LLM&#21487;&#20197;&#21033;&#29992;&#20016;&#23500;&#30340;&#32972;&#26223;&#30693;&#35782;&#21644;&#20219;&#21153;&#20449;&#24687;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#22823;&#22810;&#25968;VLM&#22312;&#29702;&#35299;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#65288;&#21253;&#21547;&#22810;&#20010;&#22270;&#20687;&#65289;&#26041;&#38754;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#36861;&#28335;&#21040;VLM&#30340;&#26550;&#26500;&#35774;&#35745;&#25110;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24403;&#21069;&#30340;VLM&#20027;&#35201;&#24378;&#35843;&#21033;&#29992;&#24102;&#26377;&#21333;&#20010;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#24102;&#26377;&#20132;&#38169;&#22810;&#20010;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#12290;&#23613;&#31649;&#19968;&#20123;&#26032;&#25552;&#20986;&#30340;VLM&#21487;&#20197;&#22788;&#29702;&#24102;&#26377;&#22810;&#20010;&#22270;&#20687;&#30340;&#29992;&#25143;&#25552;&#31034;&#65292;&#20294;&#39044;&#35757;&#32451;&#25968;&#25454;&#27809;&#26377;&#25552;&#20379;&#27604;&#20174;Web&#25235;&#21462;&#26102;&#20132;&#38169;&#22270;&#20687;&#21644;&#25991;&#26412;&#26356;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MMICL&#65292;&#20174;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#35282;&#24230;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#26080;&#32541;&#22320;&#38598;&#25104;&#35270;&#35273;&#21644;&#35821;&#35328;&#20449;&#24687;&#65292;&#24182;&#25552;&#20379;&#26356;&#20016;&#23500;&#30340;&#22810;&#27169;&#24577;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Starting from the resurgence of deep learning, vision-language models (VLMs) benefiting from large language models (LLMs) have never been so popular. However, while LLMs can utilize extensive background knowledge and task information with in-context learning, most VLMs still struggle with understanding complex multi-modal prompts with multiple images. The issue can traced back to the architectural design of VLMs or pre-training data. Specifically, the current VLMs primarily emphasize utilizing multi-modal data with a single image some, rather than multi-modal prompts with interleaved multiple images and text. Even though some newly proposed VLMs could handle user prompts with multiple images, pre-training data does not provide more sophisticated multi-modal prompts than interleaved image and text crawled from the web. We propose MMICL to address the issue by considering both the model and data perspectives. We introduce a well-designed architecture capable of seamlessly integrating vis
&lt;/p&gt;</description></item><item><title>MAmmoTH&#26159;&#19968;&#31995;&#21015;&#29992;&#20110;&#35299;&#20915;&#36890;&#29992;&#25968;&#23398;&#38382;&#39064;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#28151;&#21512;&#25351;&#20196;&#35843;&#25972;&#32593;&#32476;&#26550;&#26500;&#65292;&#25104;&#21151;&#22320;&#34701;&#21512;&#20102;&#38142;&#29366;&#24605;&#32500;&#21644;&#31243;&#24207;&#32500;&#24605;&#32500;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#25552;&#21319;&#30340;&#20934;&#30830;&#29575;&#12290;&#20854;&#20013;&#65292;MAmmoTH-7B&#27169;&#22411;&#22312;MATH&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;&#30446;&#21069;&#26368;&#22909;&#30340;&#24320;&#28304;7B&#27169;&#22411;WizardMath&#65292;&#24182;&#19988;&#25972;&#20010;&#31995;&#21015;&#27169;&#22411;&#22312;&#21508;&#20010;&#35268;&#27169;&#19978;&#24179;&#22343;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;16%&#21040;32%&#20043;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.05653</link><description>&lt;p&gt;
MAmmoTH: &#36890;&#36807;&#28151;&#21512;&#25351;&#20196;&#35843;&#25972;&#26500;&#24314;&#25968;&#23398;&#36890;&#29992;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning. (arXiv:2309.05653v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05653
&lt;/p&gt;
&lt;p&gt;
MAmmoTH&#26159;&#19968;&#31995;&#21015;&#29992;&#20110;&#35299;&#20915;&#36890;&#29992;&#25968;&#23398;&#38382;&#39064;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#28151;&#21512;&#25351;&#20196;&#35843;&#25972;&#32593;&#32476;&#26550;&#26500;&#65292;&#25104;&#21151;&#22320;&#34701;&#21512;&#20102;&#38142;&#29366;&#24605;&#32500;&#21644;&#31243;&#24207;&#32500;&#24605;&#32500;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#25552;&#21319;&#30340;&#20934;&#30830;&#29575;&#12290;&#20854;&#20013;&#65292;MAmmoTH-7B&#27169;&#22411;&#22312;MATH&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;&#30446;&#21069;&#26368;&#22909;&#30340;&#24320;&#28304;7B&#27169;&#22411;WizardMath&#65292;&#24182;&#19988;&#25972;&#20010;&#31995;&#21015;&#27169;&#22411;&#22312;&#21508;&#20010;&#35268;&#27169;&#19978;&#24179;&#22343;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;16%&#21040;32%&#20043;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;MAmmoTH&#65292;&#19968;&#31995;&#21015;&#38024;&#23545;&#36890;&#29992;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;MAmmoTH&#27169;&#22411;&#26159;&#22312;&#25105;&#20204;&#31934;&#24515;&#31574;&#21010;&#30340;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;MathInstruct&#19978;&#35757;&#32451;&#30340;&#12290;MathInstruct&#20174;13&#20010;&#25968;&#23398;&#25968;&#25454;&#38598;&#20013;&#32534;&#35793;&#32780;&#25104;&#65292;&#21253;&#21547;&#20013;&#38388;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#20854;&#20013;&#26377;&#20845;&#20010;&#25968;&#25454;&#38598;&#26159;&#30001;&#25105;&#20204;&#26032;&#40092;&#31574;&#21010;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#38142;&#29366;&#24605;&#32500;&#65288;CoT&#65289;&#21644;&#31243;&#24207;&#32500;&#24605;&#32500;&#65288;PoT&#65289;&#25512;&#29702;&#30340;&#28151;&#21512;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#23545;&#25968;&#23398;&#39046;&#22495;&#21508;&#20010;&#26041;&#38754;&#30340;&#24191;&#27867;&#35206;&#30422;&#12290;CoT&#21644;PoT&#30340;&#28151;&#21512;&#19981;&#20165;&#37322;&#25918;&#20102;&#24037;&#20855;&#20351;&#29992;&#30340;&#28508;&#21147;&#65292;&#32780;&#19988;&#20801;&#35768;&#22312;&#19981;&#21516;&#25968;&#23398;&#38382;&#39064;&#19978;&#20351;&#29992;&#19981;&#21516;&#30340;&#24605;&#32771;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;MAmmoTH&#31995;&#21015;&#22312;&#20061;&#20010;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#22312;&#25152;&#26377;&#35268;&#27169;&#19978;&#24179;&#22343;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;16%&#21040;32%&#19981;&#31561;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;MAmmoTH-7B&#27169;&#22411;&#22312;MATH&#65288;&#19968;&#20010;&#31454;&#36187;&#32423;&#25968;&#25454;&#38598;&#65289;&#19978;&#36798;&#21040;&#20102;33%&#65292;&#36229;&#36807;&#20102;&#26368;&#22909;&#30340;&#24320;&#28304;7B&#27169;&#22411;&#65288;WizardMath&#65289;2&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce MAmmoTH, a series of open-source large language models (LLMs) specifically tailored for general math problem-solving. The MAmmoTH models are trained on MathInstruct, our meticulously curated instruction tuning dataset. MathInstruct is compiled from 13 math datasets with intermediate rationales, six of which have rationales newly curated by us. It presents a unique hybrid of chain-of-thought (CoT) and program-of-thought (PoT) rationales, and also ensures extensive coverage of diverse fields in math. The hybrid of CoT and PoT not only unleashes the potential of tool use but also allows different thought processes for different math problems. As a result, the MAmmoTH series substantially outperform existing open-source models on nine mathematical reasoning datasets across all scales with an average accuracy gain between 16% and 32%. Remarkably, our MAmmoTH-7B model reaches 33% on MATH (a competition-level dataset), which exceeds the best open-source 7B model (WizardMath) by 2
&lt;/p&gt;</description></item><item><title>CrisisTransformers&#26159;&#19968;&#32452;&#38024;&#23545;&#21361;&#26426;&#30456;&#20851;&#25991;&#26412;&#20248;&#21270;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#21477;&#23376;&#32534;&#30721;&#22120;&#65292;&#26088;&#22312;&#26377;&#25928;&#22788;&#29702;&#21361;&#26426;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#65292;&#24182;&#20026;&#32039;&#24613;&#21709;&#24212;&#32773;&#25552;&#20379;&#32508;&#21512;&#35270;&#22270;&#12290;</title><link>http://arxiv.org/abs/2309.05494</link><description>&lt;p&gt;
CrisisTransformers&#65306;&#29992;&#20110;&#21361;&#26426;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#21477;&#23376;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
CrisisTransformers: Pre-trained language models and sentence encoders for crisis-related social media texts. (arXiv:2309.05494v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05494
&lt;/p&gt;
&lt;p&gt;
CrisisTransformers&#26159;&#19968;&#32452;&#38024;&#23545;&#21361;&#26426;&#30456;&#20851;&#25991;&#26412;&#20248;&#21270;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#21477;&#23376;&#32534;&#30721;&#22120;&#65292;&#26088;&#22312;&#26377;&#25928;&#22788;&#29702;&#21361;&#26426;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#65292;&#24182;&#20026;&#32039;&#24613;&#21709;&#24212;&#32773;&#25552;&#20379;&#32508;&#21512;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#22312;&#21361;&#26426;&#27807;&#36890;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#30001;&#20110;&#20854;&#38750;&#27491;&#24335;&#24615;&#36136;&#65292;&#20998;&#26512;&#21361;&#26426;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22914;BERT&#21644;RoBERTa&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#38024;&#23545;&#21361;&#26426;&#30456;&#20851;&#25991;&#26412;&#36827;&#34892;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#36890;&#29992;&#30340;&#21477;&#23376;&#32534;&#30721;&#22120;&#29992;&#20110;&#29983;&#25104;&#21477;&#23376;&#23884;&#20837;&#65292;&#32780;&#19981;&#32771;&#34385;&#21361;&#26426;&#30456;&#20851;&#25991;&#26412;&#20013;&#30340;&#25991;&#26412;&#22797;&#26434;&#24615;&#12290;&#25991;&#26412;&#20998;&#31867;&#12289;&#35821;&#20041;&#25628;&#32034;&#21644;&#32858;&#31867;&#31561;&#24212;&#29992;&#30340;&#36827;&#23637;&#26377;&#21161;&#20110;&#26377;&#25928;&#22788;&#29702;&#21361;&#26426;&#30456;&#20851;&#25991;&#26412;&#65292;&#36825;&#23545;&#20110;&#32039;&#24613;&#21709;&#24212;&#32773;&#33719;&#24471;&#21361;&#26426;&#20107;&#20214;&#30340;&#32508;&#21512;&#35270;&#22270;&#33267;&#20851;&#37325;&#35201;&#65292;&#26080;&#35770;&#35813;&#20107;&#20214;&#26159;&#21382;&#21490;&#20107;&#20214;&#36824;&#26159;&#23454;&#26102;&#20107;&#20214;&#12290;&#20026;&#22635;&#34917;&#21361;&#26426;&#20449;&#24687;&#23398;&#25991;&#29486;&#20013;&#30340;&#36825;&#20123;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;CrisisTransformers&#65292;&#36825;&#26159;&#19968;&#32452;&#22312;&#36229;&#36807;150&#20159;&#20010;&#35789;&#20803;&#30340;&#25512;&#25991;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#21477;&#23376;&#32534;&#30721;&#22120;&#30340;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media platforms play an essential role in crisis communication, but analyzing crisis-related social media texts is challenging due to their informal nature. Transformer-based pre-trained models like BERT and RoBERTa have shown success in various NLP tasks, but they are not tailored for crisis-related texts. Furthermore, general-purpose sentence encoders are used to generate sentence embeddings, regardless of the textual complexities in crisis-related texts. Advances in applications like text classification, semantic search, and clustering contribute to effective processing of crisis-related texts, which is essential for emergency responders to gain a comprehensive view of a crisis event, whether historical or real-time. To address these gaps in crisis informatics literature, this study introduces CrisisTransformers, an ensemble of pre-trained language models and sentence encoders trained on an extensive corpus of over 15 billion word tokens from tweets associated with more than 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#22411;AI&#39046;&#22495;&#30340;&#26032;&#25216;&#26415;&#36827;&#34892;&#38646;&#26679;&#26412;&#25512;&#33616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#36755;&#20837;&#36716;&#21270;&#20026;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#35821;&#20041;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#23545;&#38750;&#24179;&#31283;&#20869;&#23481;&#30340;&#25512;&#33616;&#12290;&#22312;&#21512;&#25104;&#30340;&#22810;&#27169;&#24577;&#26263;&#31034;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.01026</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#26263;&#31034;&#30340;&#38646;&#26679;&#26412;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Recommendations with Pre-Trained Large Language Models for Multimodal Nudging. (arXiv:2309.01026v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01026
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#22411;AI&#39046;&#22495;&#30340;&#26032;&#25216;&#26415;&#36827;&#34892;&#38646;&#26679;&#26412;&#25512;&#33616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#36755;&#20837;&#36716;&#21270;&#20026;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#35821;&#20041;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#23545;&#38750;&#24179;&#31283;&#20869;&#23481;&#30340;&#25512;&#33616;&#12290;&#22312;&#21512;&#25104;&#30340;&#22810;&#27169;&#24577;&#26263;&#31034;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#26368;&#26032;&#36827;&#23637;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#25512;&#33616;&#22810;&#27169;&#24577;&#38750;&#24179;&#31283;&#20869;&#23481;&#12290;&#25105;&#20204;&#24314;&#35758;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#36755;&#20837;&#28210;&#26579;&#20026;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#35745;&#31639;&#35821;&#20041;&#23884;&#20837;&#33719;&#21462;&#23427;&#20204;&#30340;&#25968;&#20540;&#34920;&#31034;&#12290;&#19968;&#26086;&#33719;&#24471;&#25152;&#26377;&#20869;&#23481;&#39033;&#30340;&#32479;&#19968;&#34920;&#31034;&#65292;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#36866;&#24403;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26469;&#36827;&#34892;&#25512;&#33616;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21512;&#25104;&#30340;&#22810;&#27169;&#24577;&#26263;&#31034;&#29615;&#22659;&#20013;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#36755;&#20837;&#21253;&#25324;&#34920;&#26684;&#12289;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method for zero-shot recommendation of multimodal non-stationary content that leverages recent advancements in the field of generative AI. We propose rendering inputs of different modalities as textual descriptions and to utilize pre-trained LLMs to obtain their numerical representations by computing semantic embeddings. Once unified representations of all content items are obtained, the recommendation can be performed by computing an appropriate similarity metric between them without any additional learning. We demonstrate our approach on a synthetic multimodal nudging environment, where the inputs consist of tabular, textual, and visual data.
&lt;/p&gt;</description></item><item><title>Sparkles&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#36394;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#23454;&#29616;&#22810;&#22270;&#23545;&#35805;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SparklesDialogue&#25968;&#25454;&#38598;&#21644;SparklesEval&#22522;&#20934;&#26469;&#25903;&#25345;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;SparklesChat&#22312;&#29702;&#35299;&#22810;&#22270;&#23545;&#35805;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16463</link><description>&lt;p&gt;
Sparkles: &#35299;&#38145;&#22810;&#22270;&#32842;&#22825;&#20197;&#23454;&#29616;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#36394;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models. (arXiv:2308.16463v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16463
&lt;/p&gt;
&lt;p&gt;
Sparkles&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#36394;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#23454;&#29616;&#22810;&#22270;&#23545;&#35805;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SparklesDialogue&#25968;&#25454;&#38598;&#21644;SparklesEval&#22522;&#20934;&#26469;&#25903;&#25345;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;SparklesChat&#22312;&#29702;&#35299;&#22810;&#22270;&#23545;&#35805;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20351;&#29992;&#25351;&#20196;&#36319;&#36394;&#25968;&#25454;&#26469;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#24615;&#33021;&#12290;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#36394;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#36825;&#20123;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27169;&#22411;&#65288;&#22914;MiniGPT-4&#65289;&#22312;&#28041;&#21450;&#22810;&#20010;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#23545;&#35805;&#36830;&#36143;&#24615;&#38754;&#20020;&#25361;&#25112;&#12290;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#32570;&#20047;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#36825;&#19968;&#20851;&#38190;&#24212;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SparklesChat&#65292;&#19968;&#20010;&#29992;&#20110;&#22810;&#22270;&#23545;&#35805;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#36394;&#27169;&#22411;&#12290;&#20026;&#20102;&#25903;&#25345;&#35757;&#32451;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SparklesDialogue&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#21333;&#35789;&#32423;&#20132;&#38169;&#22810;&#22270;&#20687;&#21644;&#25991;&#26412;&#20132;&#20114;&#32780;&#23450;&#21046;&#30340;&#26426;&#22120;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;SparklesEval&#65292;&#19968;&#20010;&#20511;&#21161;GPT&#36741;&#21161;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#23450;&#37327;&#35780;&#20272;&#27169;&#22411;&#22312;&#22810;&#20010;&#22270;&#20687;&#21644;&#23545;&#35805;&#36718;&#27425;&#20013;&#30340;&#23545;&#35805;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;SparklesChat&#22312;&#29702;&#35299;&#22810;&#22270;&#23545;&#35805;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models exhibit enhanced zero-shot performance on various tasks when fine-tuned with instruction-following data. Multimodal instruction-following models extend these capabilities by integrating both text and images. However, existing models such as MiniGPT-4 face challenges in maintaining dialogue coherence in scenarios involving multiple images. A primary reason is the lack of a specialized dataset for this critical application. To bridge these gaps, we present SparklesChat, a multimodal instruction-following model for open-ended dialogues across multiple images. To support the training, we introduce SparklesDialogue, the first machine-generated dialogue dataset tailored for word-level interleaved multi-image and text interactions. Furthermore, we construct SparklesEval, a GPT-assisted benchmark for quantitatively assessing a model's conversational competence across multiple images and dialogue turns. Our experiments validate the effectiveness of SparklesChat in understa
&lt;/p&gt;</description></item><item><title>BioCoder&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.16458</link><description>&lt;p&gt;
BioCoder: &#19968;&#31181;&#24102;&#26377;&#19978;&#19979;&#25991;&#35821;&#29992;&#30693;&#35782;&#30340;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. (arXiv:2308.16458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16458
&lt;/p&gt;
&lt;p&gt;
BioCoder&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#26174;&#33879;&#25913;&#36827;&#20102;&#20195;&#30721;&#29983;&#25104;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#30340;&#25193;&#22823;&#65292;&#38656;&#35201;&#36755;&#20986;&#26469;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#30340;&#38656;&#27714;&#20063;&#36234;&#26469;&#36234;&#22810;&#12290;&#27492;&#22806;&#65292;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#65292;&#29983;&#25104;&#21151;&#33021;&#31243;&#24207;&#30001;&#20110;&#39046;&#22495;&#30693;&#35782;&#37327;&#22823;&#12289;&#38656;&#35201;&#22797;&#26434;&#30340;&#25968;&#25454;&#25805;&#20316;&#21644;&#22797;&#26434;&#30340;&#21151;&#33021;&#20381;&#36182;&#20851;&#31995;&#32780;&#38754;&#20020;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BioCoder&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#29616;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#12290;&#19982;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#26377;&#20851;&#65292;BioCoder&#28085;&#30422;&#20102;&#21487;&#33021;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#12290;&#23427;&#21253;&#25324;&#26469;&#33258;GitHub&#30340;1026&#20010;Python&#21644;Java&#20989;&#25968;&#21644;1243&#20010;&#26041;&#27861;&#65292;&#20197;&#21450;&#26469;&#33258;Rosalind&#39033;&#30446;&#30340;253&#20010;&#31034;&#20363;&#12290;BioCoder&#36824;&#32467;&#21512;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#65292;&#25105;&#20204;&#24050;&#32463;&#24212;&#29992;&#23427;&#26469;&#35780;&#20272;&#35768;&#22810;&#27169;&#22411;&#65292;&#21253;&#25324;InCoder&#12289;CodeGen&#12289;CodeGen2&#12289;SantaCoder&#12289;StarCoder&#12289;StarCoder+&#12289;InstructCodeT&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models like ChatGPT have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks. Moreover, in bioinformatics, generating functional programs poses additional notable challenges due to the amount of domain knowledge, the need for complicated data operations, and intricate functional dependencies between the operations. Here, we present BioCoder, a benchmark developed to evaluate existing pre-trained models in generating bioinformatics code. In relation to function-code generation, BioCoder covers potential package dependencies, class declarations, and global variables. It incorporates 1026 functions and 1243 methods in Python and Java from GitHub and 253 examples from the Rosalind Project. BioCoder incorporates a fuzz-testing framework for evaluation, and we have applied it to evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#38271;&#24230;&#25511;&#21046;&#26041;&#27861;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#22870;&#21169;&#27169;&#22411;&#26469;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38271;&#24230;&#21463;&#25511;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#24182;&#28385;&#36275;&#19981;&#21516;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.12030</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#38271;&#24230;&#21463;&#25511;&#29983;&#25104;&#19982;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prompt-Based Length Controlled Generation with Reinforcement Learning. (arXiv:2308.12030v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12030
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#38271;&#24230;&#25511;&#21046;&#26041;&#27861;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#22870;&#21169;&#27169;&#22411;&#26469;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38271;&#24230;&#21463;&#25511;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#24182;&#28385;&#36275;&#19981;&#21516;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#21644;GPT-4&#22240;&#20854;&#24778;&#20154;&#30340;&#25913;&#36827;&#21644;&#24615;&#33021;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#38271;&#24230;&#21463;&#25511;&#29983;&#25104;&#25104;&#20026;LLM&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#35805;&#39064;&#65292;&#23427;&#36824;&#20351;&#29992;&#25143;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;LLM&#30340;&#33021;&#21147;&#22312;&#26356;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#29983;&#25104;&#25152;&#38656;&#38271;&#24230;&#30340;&#21512;&#36866;&#31572;&#26696;&#25110;&#25991;&#31456;&#12290;&#27492;&#22806;&#65292;LLM&#20013;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#38750;&#24120;&#32791;&#26102;&#65292;&#32780;&#25511;&#21046;&#29983;&#25104;&#38271;&#24230;&#30340;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#38480;&#21046;&#38271;&#24230;&#20219;&#24847;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#65292;&#20174;&#32780;&#28385;&#36275;&#19981;&#21516;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#38271;&#24230;&#25511;&#21046;&#26041;&#27861;&#26469;&#23454;&#29616;&#38271;&#24230;&#21463;&#25511;&#29983;&#25104;&#65292;&#36825;&#31181;&#26041;&#27861;&#20063;&#21487;&#20197;&#24191;&#27867;&#24212;&#29992;&#20110;&#31867;&#20284;GPT&#30340;LLM&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#20351;&#29992;&#21487;&#35757;&#32451;&#25110;&#22522;&#20110;&#35268;&#21017;&#30340;&#22870;&#21169;&#27169;&#22411;&#25552;&#20379;&#22870;&#21169;&#20449;&#21495;&#65292;&#36827;&#19968;&#27493;&#36890;&#36807;&#23545;&#39044;&#23450;&#20041;&#30446;&#26631;&#38271;&#24230;&#36827;&#34892;&#22870;&#21169;&#26469;&#24433;&#21709;LLM&#30340;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs) like ChatGPT and GPT-4 have attracted great attention given their surprising improvement and performance. Length controlled generation of LLMs emerges as an important topic, which also enables users to fully leverage the capability of LLMs in more real-world scenarios like generating a proper answer or essay of a desired length. In addition, the autoregressive generation in LLMs is extremely time-consuming, while the ability of controlling this generated length can arbitrarily reduce the inference cost by limiting the length, and thus satisfy different needs. Therefore, we aim to propose a prompt-based length control method to achieve this length controlled generation, which can also be widely applied in GPT-style LLMs. In particular, we adopt reinforcement learning with the reward signal given by either trainable or rule-based reward model, which further affects the generation of LLMs via rewarding a pre-defined target length. Experiments show th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#27880;&#20837;&#30340;&#23545;&#25239;&#24615;&#25351;&#20196;&#30340;&#40065;&#26834;&#24615;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#37327;&#21270;&#27169;&#22411;&#21463;&#21040;&#27880;&#20837;&#25351;&#20196;&#24433;&#21709;&#30340;&#31243;&#24230;&#65292;&#24182;&#35780;&#20272;&#20854;&#21306;&#20998;&#21407;&#22987;&#29992;&#25143;&#25351;&#20196;&#21644;&#27880;&#20837;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.10819</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25552;&#31034;&#27880;&#20837;&#30340;&#25351;&#20196;&#36319;&#38543;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection. (arXiv:2308.10819v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10819
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#27880;&#20837;&#30340;&#23545;&#25239;&#24615;&#25351;&#20196;&#30340;&#40065;&#26834;&#24615;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#37327;&#21270;&#27169;&#22411;&#21463;&#21040;&#27880;&#20837;&#25351;&#20196;&#24433;&#21709;&#30340;&#31243;&#24230;&#65292;&#24182;&#35780;&#20272;&#20854;&#21306;&#20998;&#21407;&#22987;&#29992;&#25143;&#25351;&#20196;&#21644;&#27880;&#20837;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36981;&#24490;&#25351;&#20196;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#22312;&#38754;&#21521;&#23458;&#25143;&#30340;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20986;&#33394;&#33021;&#21147;&#20063;&#24341;&#21457;&#20102;&#23545;&#30001;&#31532;&#19977;&#26041;&#25915;&#20987;&#32773;&#27880;&#20837;&#27169;&#22411;&#36755;&#20837;&#30340;&#23545;&#25239;&#24615;&#25351;&#20196;&#30340;&#39118;&#38505;&#25918;&#22823;&#30340;&#25285;&#24551;&#65292;&#36825;&#20123;&#25351;&#20196;&#21487;&#33021;&#25805;&#32437;LLM&#30340;&#21407;&#22987;&#25351;&#20196;&#24182;&#23548;&#33268;&#24847;&#22806;&#30340;&#34892;&#20026;&#21644;&#20869;&#23481;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;LLM&#20934;&#30830;&#36776;&#21035;&#35201;&#36981;&#24490;&#30340;&#25351;&#20196;&#30340;&#33021;&#21147;&#23545;&#20110;&#30830;&#20445;&#23427;&#20204;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#27880;&#20837;&#30340;&#23545;&#25239;&#24615;&#25351;&#20196;&#23545;LLM&#25351;&#20196;&#36319;&#38543;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;&#35813;&#22522;&#20934;&#30340;&#30446;&#26631;&#26159;&#37327;&#21270;LLM&#21463;&#27880;&#20837;&#30340;&#23545;&#25239;&#24615;&#25351;&#20196;&#24433;&#21709;&#30340;&#31243;&#24230;&#65292;&#24182;&#35780;&#20272;&#20854;&#21306;&#20998;&#36825;&#20123;&#27880;&#20837;&#30340;&#23545;&#25239;&#24615;&#25351;&#20196;&#21644;&#21407;&#22987;&#29992;&#25143;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable proficiency in following instructions, making them valuable in customer-facing applications. However, their impressive capabilities also raise concerns about the amplification of risks posed by adversarial instructions, which can be injected into the model input by third-party attackers to manipulate LLMs' original instructions and prompt unintended actions and content. Therefore, it is crucial to understand LLMs' ability to accurately discern which instructions to follow to ensure their safe deployment in real-world scenarios. In this paper, we propose a pioneering benchmark for automatically evaluating the robustness of instruction-following LLMs against adversarial instructions injected in the prompt. The objective of this benchmark is to quantify the extent to which LLMs are influenced by injected adversarial instructions and assess their ability to differentiate between these injected adversarial instructions and original user ins
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#38543;&#26426;&#26679;&#26412;&#20013;&#30340;&#21333;&#20010;&#23454;&#20363;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#21450;&#20351;&#29992;&#8220;&#24341;&#23548;&#25351;&#20196;&#8221;&#26469;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#20998;&#21306;&#30340;&#27745;&#26579;&#31243;&#24230;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#27745;&#26579;&#30340;&#23454;&#20363;&#21644;&#20998;&#21306;&#12290;</title><link>http://arxiv.org/abs/2308.08493</link><description>&lt;p&gt;
LLM&#20013;&#30340;&#26102;&#38388;&#26053;&#34892;&#65306;&#36861;&#36394;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;
&lt;/p&gt;
&lt;p&gt;
Time Travel in LLMs: Tracing Data Contamination in Large Language Models. (arXiv:2308.08493v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08493
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#38543;&#26426;&#26679;&#26412;&#20013;&#30340;&#21333;&#20010;&#23454;&#20363;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#21450;&#20351;&#29992;&#8220;&#24341;&#23548;&#25351;&#20196;&#8221;&#26469;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#20998;&#21306;&#30340;&#27745;&#26579;&#31243;&#24230;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#27745;&#26579;&#30340;&#23454;&#20363;&#21644;&#20998;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#26159;&#25351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#26469;&#33258;&#19979;&#28216;&#20219;&#21153;&#30340;&#27979;&#35797;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#26159;&#29702;&#35299;LLMs&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#26377;&#25928;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;LLMs&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26680;&#24515;&#26159;&#36890;&#36807;&#35782;&#21035;&#20174;&#23567;&#30340;&#38543;&#26426;&#26679;&#26412;&#20013;&#25277;&#21462;&#30340;&#21333;&#20010;&#23454;&#20363;&#20013;&#30340;&#28508;&#22312;&#27745;&#26579;&#65292;&#28982;&#21518;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#20998;&#21306;&#26159;&#21542;&#21463;&#21040;&#27745;&#26579;&#12290;&#20026;&#20102;&#20272;&#35745;&#21333;&#20010;&#23454;&#20363;&#30340;&#27745;&#26579;&#31243;&#24230;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#8220;&#24341;&#23548;&#25351;&#20196;&#8221;&#65306;&#21363;&#19968;&#20010;&#30001;&#25968;&#25454;&#38598;&#21517;&#31216;&#12289;&#20998;&#21306;&#31867;&#22411;&#21644;&#21442;&#32771;&#23454;&#20363;&#30340;&#21021;&#22987;&#37096;&#20998;&#32452;&#25104;&#30340;&#25552;&#31034;&#65292;&#35201;&#27714;LLM&#23436;&#25104;&#23427;&#12290;&#22914;&#26524;LLM&#30340;&#36755;&#20986;&#19982;&#21442;&#32771;&#23454;&#20363;&#30340;&#21518;&#19968;&#37096;&#20998;&#23436;&#20840;&#25110;&#25509;&#36817;&#21305;&#37197;&#65292;&#37027;&#20040;&#35813;&#23454;&#20363;&#34987;&#26631;&#35760;&#20026;&#21463;&#21040;&#27745;&#26579;&#12290;&#20026;&#20102;&#20102;&#35299;&#25972;&#20010;&#20998;&#21306;&#26159;&#21542;&#21463;&#21040;&#27745;&#26579;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#24819;&#27861;&#12290;&#31532;&#19968;&#20010;&#24819;&#27861;&#26159;&#26631;&#35760;&#19968;&#20010;&#25968;&#25454;&#38598;&#30340;&#20998;&#21306;&#65292;&#35813;&#20998;&#21306;&#20013;&#30340;&#23454;&#20363;&#22823;&#22810;&#25968;&#37117;&#34987;&#21028;&#26029;&#20026;&#21463;&#21040;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in understanding LLMs' effectiveness on other tasks. We propose a straightforward yet effective method for identifying data contamination within LLMs. At its core, our approach starts by identifying potential contamination in individual instances that are drawn from a small random sample; using this information, our approach then assesses if an entire dataset partition is contaminated. To estimate contamination of individual instances, we employ "guided instruction:" a prompt consisting of the dataset name, partition type, and the initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or closely matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22312;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21464;&#21387;&#22120;&#20013;&#22914;&#20309;&#36890;&#36807;&#24341;&#20837;&#35270;&#35273;&#20449;&#24687;&#26469;&#23454;&#29616;&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#22312;&#21464;&#21387;&#22120;&#30340;&#26356;&#28145;&#22788;&#36827;&#34892;&#27169;&#24577;&#20043;&#38388;&#30340;&#36716;&#25442;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#35782;&#21035;&#22810;&#27169;&#24577;&#31070;&#32463;&#20803;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#23545;&#29305;&#23450;&#35270;&#35273;&#27010;&#24565;&#30340;&#25805;&#20316;&#20197;&#21450;&#23545;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2308.01544</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#20165;&#25991;&#26412;&#21464;&#21387;&#22120;&#20013;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#20803;
&lt;/p&gt;
&lt;p&gt;
Multimodal Neurons in Pretrained Text-Only Transformers. (arXiv:2308.01544v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01544
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21464;&#21387;&#22120;&#20013;&#22914;&#20309;&#36890;&#36807;&#24341;&#20837;&#35270;&#35273;&#20449;&#24687;&#26469;&#23454;&#29616;&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#22312;&#21464;&#21387;&#22120;&#30340;&#26356;&#28145;&#22788;&#36827;&#34892;&#27169;&#24577;&#20043;&#38388;&#30340;&#36716;&#25442;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#35782;&#21035;&#22810;&#27169;&#24577;&#31070;&#32463;&#20803;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#23545;&#29305;&#23450;&#35270;&#35273;&#27010;&#24565;&#30340;&#25805;&#20316;&#20197;&#21450;&#23545;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20102;&#22312;&#19981;&#21516;&#27169;&#24577;&#19979;&#23558;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#25512;&#24191;&#21040;&#20854;&#20182;&#27169;&#24577;&#19979;&#28216;&#20219;&#21153;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#20923;&#32467;&#30340;&#25991;&#26412;&#21464;&#21387;&#22120;&#22686;&#21152;&#35270;&#35273;&#33021;&#21147;&#30340;&#24773;&#20917;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#20219;&#21153;&#19978;&#23398;&#20064;&#24471;&#21040;&#30340;&#21333;&#19968;&#32447;&#24615;&#26144;&#23556;&#12290;&#26144;&#23556;&#23618;&#30340;&#36755;&#20986;&#19981;&#26159;&#21487;&#20197;&#30452;&#25509;&#35299;&#30721;&#25104;&#25551;&#36848;&#22270;&#20687;&#20869;&#23481;&#30340;&#35821;&#35328;&#65292;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#24577;&#20043;&#38388;&#30340;&#36716;&#25442;&#21457;&#29983;&#22312;&#21464;&#21387;&#22120;&#30340;&#26356;&#28145;&#22788;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#35782;&#21035;&#23558;&#35270;&#35273;&#34920;&#31034;&#36716;&#25442;&#20026;&#30456;&#24212;&#25991;&#26412;&#30340;&#8220;&#22810;&#27169;&#24577;&#31070;&#32463;&#20803;&#8221;&#30340;&#36807;&#31243;&#65292;&#24182;&#35299;&#30721;&#23427;&#20204;&#27880;&#20837;&#27169;&#22411;&#27531;&#24046;&#27969;&#20013;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;&#31070;&#32463;&#20803;&#22312;&#19981;&#21516;&#36755;&#20837;&#20013;&#23545;&#29305;&#23450;&#35270;&#35273;&#27010;&#24565;&#36827;&#34892;&#25805;&#20316;&#65292;&#24182;&#23545;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20855;&#26377;&#31995;&#32479;&#24615;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models demonstrate remarkable capacity to generalize representations learned in one modality to downstream tasks in other modalities. Can we trace this ability to individual neurons? We study the case where a frozen text transformer is augmented with vision using a self-supervised visual encoder and a single linear projection learned on an image-to-text task. Outputs of the projection layer are not immediately decodable into language describing image content; instead, we find that translation between modalities occurs deeper within the transformer. We introduce a procedure for identifying "multimodal neurons" that convert visual representations into corresponding text, and decoding the concepts they inject into the model's residual stream. In a series of experiments, we show that multimodal neurons operate on specific visual concepts across inputs, and have a systematic causal effect on image captioning.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WebAgent&#30340;LLM&#39537;&#21160;&#20195;&#29702;&#65292;&#36890;&#36807;&#33258;&#25105;&#32463;&#39564;&#23398;&#20064;&#65292;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35268;&#21010;&#12289;&#24635;&#32467;&#21644;&#29983;&#25104;&#20195;&#30721;&#26469;&#25552;&#39640;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.12856</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#35268;&#21010;&#12289;&#38271;&#26399;&#19978;&#19979;&#25991;&#29702;&#35299;&#21644;&#31243;&#24207;&#21512;&#25104;&#33021;&#21147;&#30340;&#29616;&#23454;&#19990;&#30028;WebAgent
&lt;/p&gt;
&lt;p&gt;
A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis. (arXiv:2307.12856v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12856
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WebAgent&#30340;LLM&#39537;&#21160;&#20195;&#29702;&#65292;&#36890;&#36807;&#33258;&#25105;&#32463;&#39564;&#23398;&#20064;&#65292;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35268;&#21010;&#12289;&#24635;&#32467;&#21644;&#29983;&#25104;&#20195;&#30721;&#26469;&#25552;&#39640;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#20027;Web&#33258;&#21160;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#32593;&#31449;&#19978;&#65292;&#24615;&#33021;&#20173;&#28982;&#21463;&#21040;&#19977;&#20010;&#26041;&#38754;&#30340;&#38480;&#21046;&#65306;&#24320;&#25918;&#39046;&#22495;&#24615;&#12289;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#21644;&#23545;HTML&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#32570;&#20047;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WebAgent&#30340;LLM&#39537;&#21160;&#20195;&#29702;&#65292;&#23427;&#36890;&#36807;&#33258;&#25105;&#32463;&#39564;&#23398;&#20064;&#65292;&#22312;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#21069;&#25552;&#19979;&#65292;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;WebAgent&#36890;&#36807;&#23558;&#25351;&#20196;&#20998;&#35299;&#20026;&#35268;&#33539;&#30340;&#23376;&#25351;&#20196;&#65292;&#23558;&#38271;HTML&#25991;&#26723;&#24635;&#32467;&#20026;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#29255;&#27573;&#65292;&#24182;&#36890;&#36807;&#20174;&#20013;&#29983;&#25104;&#30340;Python&#31243;&#24207;&#23545;&#32593;&#31449;&#36827;&#34892;&#25805;&#20316;&#26469;&#25552;&#21069;&#36827;&#34892;&#35268;&#21010;&#12290;&#25105;&#20204;&#20351;&#29992;Flan-U-PaLM&#35774;&#35745;&#20102;WebAgent&#65292;&#29992;&#20110;&#29983;&#25104;&#26377;&#26681;&#20195;&#30721;&#65292;&#24182;&#20351;&#29992;HTML-T5&#36827;&#34892;&#39044;&#35757;&#32451;LLMs&#65292;&#21033;&#29992;&#23616;&#37096;&#21644;&#20840;&#23616;&#27880;&#24847;&#26426;&#21046;&#20197;&#21450;&#28151;&#21512;&#38271;&#36328;&#24230;&#21435;&#22122;&#30446;&#26631;&#26469;&#36827;&#34892;&#35268;&#21010;&#21644;&#24635;&#32467;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#25552;&#39640;&#20102;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our modular recipe improves the success on real websites by ov
&lt;/p&gt;</description></item><item><title>MeetEval&#26159;&#19968;&#20010;&#35745;&#31639;&#20250;&#35758;&#36716;&#24405;&#31995;&#32479;&#23383;&#38169;&#35823;&#29575;&#30340;&#24037;&#20855;&#21253;&#65292;&#23427;&#36890;&#36807;&#26102;&#38388;&#32422;&#26463;&#26469;&#25552;&#39640;&#21305;&#37197;&#36136;&#37327;&#24182;&#21152;&#36895;&#21305;&#37197;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.11394</link><description>&lt;p&gt;
MeetEval: &#19968;&#31181;&#29992;&#20110;&#20250;&#35758;&#36716;&#24405;&#31995;&#32479;&#23383;&#38169;&#35823;&#29575;&#35745;&#31639;&#30340;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
MeetEval: A Toolkit for Computation of Word Error Rates for Meeting Transcription Systems. (arXiv:2307.11394v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11394
&lt;/p&gt;
&lt;p&gt;
MeetEval&#26159;&#19968;&#20010;&#35745;&#31639;&#20250;&#35758;&#36716;&#24405;&#31995;&#32479;&#23383;&#38169;&#35823;&#29575;&#30340;&#24037;&#20855;&#21253;&#65292;&#23427;&#36890;&#36807;&#26102;&#38388;&#32422;&#26463;&#26469;&#25552;&#39640;&#21305;&#37197;&#36136;&#37327;&#24182;&#21152;&#36895;&#21305;&#37197;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MeetEval&#26159;&#19968;&#20010;&#24320;&#28304;&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#35780;&#20272;&#21508;&#31181;&#20250;&#35758;&#36716;&#24405;&#31995;&#32479;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#30028;&#38754;&#65292;&#29992;&#20110;&#35745;&#31639;&#24120;&#29992;&#30340;&#23383;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#65292;&#21253;&#25324;cpWER&#12289;ORC WER&#21644;MIMO WER&#31561;&#20854;&#20182;WER&#23450;&#20041;&#12290;&#25105;&#20204;&#36890;&#36807;&#26102;&#38388;&#32422;&#26463;&#25193;&#23637;&#20102;cpWER&#30340;&#35745;&#31639;&#65292;&#20197;&#30830;&#20445;&#21482;&#26377;&#22312;&#26102;&#38388;&#23545;&#40784;&#21512;&#29702;&#30340;&#24773;&#20917;&#19979;&#25165;&#23558;&#21333;&#35789;&#35782;&#21035;&#20026;&#27491;&#30830;&#12290;&#36825;&#26679;&#21487;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#20551;&#35774;&#23383;&#31526;&#20018;&#19982;&#21442;&#32771;&#23383;&#31526;&#20018;&#65292;&#26356;&#25509;&#36817;&#23454;&#38469;&#30340;&#36716;&#24405;&#36136;&#37327;&#65292;&#24182;&#19988;&#22914;&#26524;&#31995;&#32479;&#25552;&#20379;&#20102;&#19981;&#20934;&#30830;&#30340;&#26102;&#38388;&#26631;&#27880;&#65292;&#23558;&#23545;&#20854;&#36827;&#34892;&#24809;&#32602;&#12290;&#30001;&#20110;&#36890;&#24120;&#27809;&#26377;&#21333;&#35789;&#32423;&#21035;&#30340;&#26102;&#38388;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20174;&#29255;&#27573;&#32423;&#21035;&#26102;&#38388;&#65288;&#20363;&#22914;&#19968;&#20010;&#21477;&#23376;&#65289;&#36817;&#20284;&#21040;&#30830;&#20999;&#30340;&#21333;&#35789;&#32423;&#26102;&#38388;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#36817;&#20284;&#26041;&#27861;&#19982;&#20855;&#26377;&#30830;&#20999;&#21333;&#35789;&#32423;&#21035;&#27880;&#37322;&#30340;&#21305;&#37197;&#23548;&#33268;&#31867;&#20284;&#30340;WER&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26102;&#38388;&#32422;&#26463;&#36824;&#23548;&#33268;&#21305;&#37197;&#31639;&#27861;&#30340;&#21152;&#36895;&#65292;&#36825;&#36229;&#36807;&#20102;&#20542;&#21521;&#25340;&#20945;&#30340;&#26102;&#38388;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
MeetEval is an open-source toolkit to evaluate all kinds of meeting transcription systems. It provides a unified interface for the computation of commonly used Word Error Rates (WERs), specifically cpWER, ORC WER and MIMO WER along other WER definitions. We extend the cpWER computation by a temporal constraint to ensure that only words are identified as correct when the temporal alignment is plausible. This leads to a better quality of the matching of the hypothesis string to the reference string that more closely resembles the actual transcription quality, and a system is penalized if it provides poor time annotations. Since word-level timing information is often not available, we present a way to approximate exact word-level timings from segment-level timings (e.g., a sentence) and show that the approximation leads to a similar WER as a matching with exact word-level annotations. At the same time, the time constraint leads to a speedup of the matching algorithm, which outweighs the a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#31579;&#36873;&#31574;&#30053;AlpaGasus&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#36807;&#28388;&#25481;&#20302;&#36136;&#37327;&#25968;&#25454;&#65292;&#23427;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#27604;&#21407;&#22987;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.08701</link><description>&lt;p&gt;
AlpaGasus: &#29992;&#26356;&#23569;&#25968;&#25454;&#35757;&#32451;&#26356;&#22909;&#30340;&#32650;&#39548;
&lt;/p&gt;
&lt;p&gt;
AlpaGasus: Training A Better Alpaca with Fewer Data. (arXiv:2307.08701v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08701
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#31579;&#36873;&#31574;&#30053;AlpaGasus&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#36807;&#28388;&#25481;&#20302;&#36136;&#37327;&#25968;&#25454;&#65292;&#23427;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#27604;&#21407;&#22987;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#26377;&#30417;&#30563;&#30340;&#25351;&#20196;/&#22238;&#22797;&#25968;&#25454;&#19978;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#65288;IFT&#65289;&#26469;&#22686;&#24378;&#20854;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;IFT&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;&#65306;Alpaca&#30340;52k&#25968;&#25454;&#65289;&#20986;&#20046;&#24847;&#26009;&#22320;&#21253;&#21547;&#35768;&#22810;&#20855;&#26377;&#19981;&#27491;&#30830;&#25110;&#19981;&#30456;&#20851;&#22238;&#22797;&#30340;&#20302;&#36136;&#37327;&#23454;&#20363;&#65292;&#36825;&#20123;&#23454;&#20363;&#20250;&#35823;&#23548;&#21644;&#23545;IFT&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#20351;&#29992;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;&#65306;ChatGPT&#65289;&#33258;&#21160;&#35782;&#21035;&#24182;&#36807;&#28388;&#25481;&#20302;&#36136;&#37327;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AlpaGasus&#65292;&#23427;&#20165;&#22312;&#20174;52k Alpaca&#25968;&#25454;&#20013;&#36807;&#28388;&#24471;&#21040;&#30340;9k&#39640;&#36136;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;AlpaGasus&#22312;&#22810;&#20010;&#27979;&#35797;&#25968;&#25454;&#38598;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#22343;&#26174;&#33879;&#20248;&#20110;&#21407;&#22987;&#30340;Alpaca&#65292;&#30001;GPT-4&#36827;&#34892;&#35780;&#20272;&#12290;&#20854;13B&#21464;&#31181;&#22312;&#27979;&#35797;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#19982;&#20854;&#25945;&#24072;&#27169;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;&#29983;&#25104;52k&#25968;&#25454;&#30340;Text-Davinci-003&#65289;&#30340;&#24615;&#33021;&#21305;&#37197;&#29575;&#36229;&#36807;90&#65285;&#12290;&#23427;&#36824;&#25552;&#20379;&#20102;5.7&#20493;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#65292;&#23558;7B&#21464;&#31181;&#30340;&#35757;&#32451;&#26102;&#38388;&#20174;80&#20998;&#38047;&#20943;&#23569;&#21040;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Large language models~(LLMs) strengthen instruction-following capability through instruction-finetuning (IFT) on supervised instruction/response data. However, widely used IFT datasets (e.g., Alpaca's 52k data) surprisingly contain many low-quality instances with incorrect or irrelevant responses, which are misleading and detrimental to IFT. In this paper, we propose a simple and effective data selection strategy that automatically identifies and filters out low-quality data using a strong LLM (e.g., ChatGPT). To this end, we introduce AlpaGasus, which is finetuned on only 9k high-quality data filtered from the 52k Alpaca data. AlpaGasus significantly outperforms the original Alpaca as evaluated by GPT-4 on multiple test sets and the controlled human evaluation. Its 13B variant matches $&gt;90\%$ performance of its teacher LLM (i.e., Text-Davinci-003 generating the 52k data) on test tasks. It also provides 5.7x faster training, reducing the training time for a 7B variant from 80 minutes (
&lt;/p&gt;</description></item><item><title>Think-on-Graph&#26159;&#19968;&#20010;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28145;&#24230;&#21644;&#36127;&#36131;&#20219;&#25512;&#29702;&#33021;&#21147;&#30340;&#26032;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#30340;&#22810;&#36339;&#25512;&#29702;&#38382;&#31572;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.07697</link><description>&lt;p&gt;
Think-on-Graph: &#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28145;&#24230;&#21644;&#36127;&#36131;&#20219;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Think-on-Graph: Deep and Responsible Reasoning of Large Language Model with Knowledge Graph. (arXiv:2307.07697v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07697
&lt;/p&gt;
&lt;p&gt;
Think-on-Graph&#26159;&#19968;&#20010;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28145;&#24230;&#21644;&#36127;&#36131;&#20219;&#25512;&#29702;&#33021;&#21147;&#30340;&#26032;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#30340;&#22810;&#36339;&#25512;&#29702;&#38382;&#31572;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#38656;&#35201;&#30693;&#35782;&#36861;&#28335;&#24615;&#12289;&#21450;&#26102;&#24615;&#21644;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#30340;&#22330;&#26223;&#20013;&#65292;&#23427;&#20204;&#32463;&#24120;&#22312;&#22797;&#26434;&#25512;&#29702;&#21644;&#34920;&#29616;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Think-on-Graph&#65288;ToG&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;LLMs&#28145;&#24230;&#21644;&#36127;&#36131;&#20219;&#25512;&#29702;&#33021;&#21147;&#30340;&#26032;&#26694;&#26550;&#12290;&#36890;&#36807;&#20351;&#29992;ToG&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#19982;&#32473;&#23450;&#38382;&#39064;&#30456;&#20851;&#30340;&#23454;&#20307;&#65292;&#24182;&#23545;&#22806;&#37096;&#30693;&#35782;&#25968;&#25454;&#24211;&#36827;&#34892;&#25506;&#32034;&#21644;&#25512;&#29702;&#65292;&#20197;&#26816;&#32034;&#30456;&#20851;&#19977;&#20803;&#32452;&#12290;&#36825;&#20010;&#36845;&#20195;&#36807;&#31243;&#29983;&#25104;&#21253;&#21547;&#39034;&#24207;&#36830;&#25509;&#30340;&#19977;&#20803;&#32452;&#30340;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#65292;&#30452;&#21040;&#25910;&#38598;&#21040;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#22238;&#31572;&#38382;&#39064;&#25110;&#36798;&#21040;&#26368;&#22823;&#28145;&#24230;&#20026;&#27490;&#12290;&#36890;&#36807;&#22312;&#22797;&#26434;&#30340;&#22810;&#36339;&#25512;&#29702;&#38382;&#31572;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;ToG&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;LLMs&#30340;&#21069;&#36848;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made significant strides in various tasks, yet they often struggle with complex reasoning and exhibit poor performance in scenarios where knowledge traceability, timeliness, and accuracy are crucial. To address these limitations, we present Think-on-Graph (ToG), a novel framework that leverages knowledge graphs to enhance LLMs' ability for deep and responsible reasoning. By employing ToG, we can identify entities relevant to a given question and conduct exploration and reasoning to retrieve related triples from an external knowledge database. This iterative procedure generates multiple reasoning pathways consisting of sequentially connected triplets until sufficient information is gathered to answer the question or the maximum depth is reached. Through experiments on complex multi-hop reasoning question-answering tasks, we demonstrate that ToG outperforms existing methods, effectively addressing the aforementioned limitations of LLMs without incurring 
&lt;/p&gt;</description></item><item><title>mBLIP&#26159;&#31532;&#19968;&#20010;&#22810;&#35821;&#35328;Vision-LLM&#65292;&#36890;&#36807;&#22312;&#28040;&#36153;&#32423;&#30828;&#20214;&#19978;&#20351;&#29992;&#23569;&#37327;&#35757;&#32451;&#26679;&#20363;&#30340;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#26041;&#24335;&#33719;&#24471;&#12290;</title><link>http://arxiv.org/abs/2307.06930</link><description>&lt;p&gt;
mBLIP: &#22810;&#35821;&#35328;&#35270;&#35273;-LLM&#30340;&#39640;&#25928;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs. (arXiv:2307.06930v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06930
&lt;/p&gt;
&lt;p&gt;
mBLIP&#26159;&#31532;&#19968;&#20010;&#22810;&#35821;&#35328;Vision-LLM&#65292;&#36890;&#36807;&#22312;&#28040;&#36153;&#32423;&#30828;&#20214;&#19978;&#20351;&#29992;&#23569;&#37327;&#35757;&#32451;&#26679;&#20363;&#30340;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#26041;&#24335;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22359;&#21270;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;Vision-LLM&#65289;&#23558;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#19982;&#65288;&#39044;&#35757;&#32451;&#30340;&#65289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#40784;&#65292;&#26159;&#19968;&#31181;&#22312;&#35745;&#31639;&#19978;&#26356;&#39640;&#25928;&#30340;&#36873;&#25321;&#65292;&#21487;&#20197;&#20195;&#26367;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#65292;&#32780;&#21518;&#32773;&#23545;&#20110;&#22823;&#22810;&#25968;&#20154;&#26469;&#35828;&#25104;&#26412;&#22826;&#39640;&#12290; Vision-LLM&#23558;LLM&#20107;&#21518;&#26465;&#20214;&#21270;&#20026;&#8220;&#29702;&#35299;&#8221;&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#36755;&#20986;&#12290;&#38543;&#30528;&#29616;&#25104;&#30340;&#39640;&#36136;&#37327;&#33521;&#25991;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#20197;&#21450;&#21333;&#35821;&#33521;&#35821;LLM&#30340;&#20016;&#23500;&#24615;&#65292;&#30740;&#31350;&#37325;&#28857;&#24050;&#32463;&#25918;&#22312;&#20165;&#33521;&#25991;&#30340;Vision-LLM&#19978;&#12290;&#32780;&#22810;&#35821;&#35328;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#20027;&#35201;&#36890;&#36807;&#26114;&#36149;&#30340;&#31471;&#21040;&#31471;&#39044;&#35757;&#32451;&#33719;&#24471;&#65292;&#36825;&#23548;&#33268;&#20102;&#30456;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#26377;&#38480;&#30340;&#22810;&#35821;&#35328;&#22270;&#20687;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;&#34917;&#20805;&#20102;&#20165;&#26377;&#25991;&#26412;&#30340;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;mBLIP&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22810;&#35821;&#35328;Vision-LLM&#65292;&#25105;&#20204;&#20197;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#26041;&#24335;&#33719;&#24471;&#65292;&#20165;&#20351;&#29992;&#20960;&#30334;&#19975;&#20010;&#35757;&#32451;&#26679;&#20363;&#22312;&#28040;&#36153;&#32423;&#30828;&#20214;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modular vision-language models (Vision-LLMs) align pretrained image encoders with (pretrained) large language models (LLMs), representing a computationally much more efficient alternative to end-to-end training of large vision-language models from scratch, which is prohibitively expensive for most. Vision-LLMs instead post-hoc condition LLMs to `understand' the output of an image encoder. With the abundance of readily available high-quality English image-text data as well as monolingual English LLMs, the research focus has been on English-only Vision-LLMs. Multilingual vision-language models are still predominantly obtained via expensive end-to-end pretraining, resulting in comparatively smaller models, trained on limited multilingual image data supplemented with text-only multilingual corpora. In this work, we present mBLIP, the first multilingual Vision-LLM, which we obtain in a computationally efficient manner -- on consumer hardware using only a few million training examples -- by 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;Speech-LLaMA&#65292;&#23558;&#22768;&#23398;&#20449;&#24687;&#26377;&#25928;&#22320;&#34701;&#20837;&#22522;&#20110;&#25991;&#26412;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#36890;&#36807;&#20351;&#29992;&#36830;&#25509;&#20027;&#20041;&#26102;&#24207;&#20998;&#31867;&#21644;&#31616;&#21333;&#30340;&#38899;&#39057;&#32534;&#30721;&#22120;&#65292;&#23558;&#21387;&#32553;&#30340;&#22768;&#23398;&#29305;&#24449;&#26144;&#23556;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36830;&#32493;&#35821;&#20041;&#31354;&#38388;&#20013;&#65292;&#23454;&#29616;&#20102;&#35821;&#38899;&#21040;&#25991;&#26412;&#20219;&#21153;&#20013;&#30340;&#23454;&#36136;&#24615;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2307.03917</link><description>&lt;p&gt;
&#20851;&#20110;&#20165;&#35299;&#30721;&#22120;&#26550;&#26500;&#22312;&#35821;&#38899;&#21040;&#25991;&#26412;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On decoder-only architecture for speech-to-text and large language model integration. (arXiv:2307.03917v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03917
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;Speech-LLaMA&#65292;&#23558;&#22768;&#23398;&#20449;&#24687;&#26377;&#25928;&#22320;&#34701;&#20837;&#22522;&#20110;&#25991;&#26412;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#36890;&#36807;&#20351;&#29992;&#36830;&#25509;&#20027;&#20041;&#26102;&#24207;&#20998;&#31867;&#21644;&#31616;&#21333;&#30340;&#38899;&#39057;&#32534;&#30721;&#22120;&#65292;&#23558;&#21387;&#32553;&#30340;&#22768;&#23398;&#29305;&#24449;&#26144;&#23556;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36830;&#32493;&#35821;&#20041;&#31354;&#38388;&#20013;&#65292;&#23454;&#29616;&#20102;&#35821;&#38899;&#21040;&#25991;&#26412;&#20219;&#21153;&#20013;&#30340;&#23454;&#36136;&#24615;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#33021;&#22815;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#23454;&#29616;&#26356;&#22909;&#30340;&#20154;&#26426;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#23558;&#35821;&#38899;&#20449;&#21495;&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#25506;&#32034;&#12290;&#21516;&#26102;&#65292;&#20851;&#20110;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#30340;&#8220;&#20165;&#35299;&#30721;&#22120;&#8221;&#26550;&#26500;&#20063;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Speech-LLaMA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#23558;&#22768;&#23398;&#20449;&#24687;&#34701;&#20837;&#22522;&#20110;&#25991;&#26412;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#36830;&#25509;&#20027;&#20041;&#26102;&#24207;&#20998;&#31867;&#21644;&#31616;&#21333;&#30340;&#38899;&#39057;&#32534;&#30721;&#22120;&#65292;&#23558;&#21387;&#32553;&#30340;&#22768;&#23398;&#29305;&#24449;&#26144;&#23556;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36830;&#32493;&#35821;&#20041;&#31354;&#38388;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#20165;&#35299;&#30721;&#22120;&#26550;&#26500;&#22312;&#35821;&#38899;&#21040;&#25991;&#26412;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#35821;&#38899;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#35757;&#32451;&#19968;&#20010;&#36739;&#23567;&#35268;&#27169;&#12289;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;Speech-LLaMA&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#22810;&#35821;&#35328;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#19982;&#24378;&#22522;&#20934;&#30456;&#27604;&#26377;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved remarkable success in the field of natural language processing, enabling better human-computer interaction using natural language. However, the seamless integration of speech signals into LLMs has not been explored well. The "decoder-only" architecture has also not been well studied for speech processing tasks. In this research, we introduce Speech-LLaMA, a novel approach that effectively incorporates acoustic information into text-based large language models. Our method leverages Connectionist Temporal Classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the LLM. In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speech-LLaMA model from speech-text paired data alone. We conduct experiments on multilingual speech-to-text translation tasks and demonstrate a significant improvement over strong baseli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#24335;Transformer-Transducer&#65292;&#21516;&#26102;&#29983;&#25104;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#36755;&#20986;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32852;&#21512;&#30340;&#26631;&#35760;&#32423;&#20018;&#34892;&#36755;&#20986;&#35757;&#32451;&#26041;&#27861;&#65292;&#32467;&#21512;&#29616;&#25104;&#30340;&#25991;&#26412;&#23545;&#40784;&#22120;&#65292;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#36136;&#37327;-&#24310;&#36831;&#24179;&#34913;&#65292;&#24182;&#22312;&#22810;&#35821;&#29615;&#22659;&#19979;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.03354</link><description>&lt;p&gt;
&#22312;&#32852;&#21512;&#27969;&#30021;&#30340;ASR&#21644;ST&#20013;&#65292;&#22522;&#20110;&#25991;&#26412;&#23545;&#40784;&#30340;&#26631;&#35760;&#32423;&#20018;&#34892;&#36755;&#20986;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Token-Level Serialized Output Training for Joint Streaming ASR and ST Leveraging Textual Alignments. (arXiv:2307.03354v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#24335;Transformer-Transducer&#65292;&#21516;&#26102;&#29983;&#25104;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#36755;&#20986;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32852;&#21512;&#30340;&#26631;&#35760;&#32423;&#20018;&#34892;&#36755;&#20986;&#35757;&#32451;&#26041;&#27861;&#65292;&#32467;&#21512;&#29616;&#25104;&#30340;&#25991;&#26412;&#23545;&#40784;&#22120;&#65292;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#36136;&#37327;-&#24310;&#36831;&#24179;&#34913;&#65292;&#24182;&#22312;&#22810;&#35821;&#29615;&#22659;&#19979;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#29992;&#25143;&#36890;&#24120;&#38656;&#35201;&#21516;&#26102;&#32763;&#35793;&#21644;&#36716;&#24405;&#35821;&#38899;&#20197;&#22686;&#24378;&#20854;&#29702;&#35299;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#22686;&#37327;&#29983;&#25104;&#30340;&#27969;&#24335;&#22330;&#26223;&#20013;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27969;&#24335;Transformer-Transducer&#65292;&#23427;&#21033;&#29992;&#19968;&#20010;&#21333;&#19968;&#30340;&#35299;&#30721;&#22120;&#21516;&#26102;&#29983;&#25104;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#36755;&#20986;&#12290;&#20026;&#20102;&#20197;&#26368;&#23567;&#30340;&#24310;&#36831;&#26377;&#25928;&#22320;&#20135;&#29983;ASR&#21644;ST&#20869;&#23481;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#30340;&#26631;&#35760;&#32423;&#20018;&#34892;&#36755;&#20986;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#25104;&#30340;&#25991;&#26412;&#23545;&#40784;&#22120;&#20132;&#38169;&#28304;&#35789;&#21644;&#30446;&#26631;&#35789;&#12290;&#22312;&#21333;&#35821;&#65288;it-en&#65289;&#21644;&#22810;&#35821;&#65288;{de,es,it}-en&#65289;&#35774;&#32622;&#19979;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#36136;&#37327;-&#24310;&#36831;&#24179;&#34913;&#12290;&#22312;&#24179;&#22343;ASR&#24310;&#36831;&#20026;1&#31186;&#21644;ST&#24310;&#36831;&#20026;1.3&#31186;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#21333;&#29420;&#30340;ASR&#21644;ST&#27169;&#22411;&#30456;&#27604;&#65292;&#27809;&#26377;&#38477;&#20302;&#65292;&#29978;&#33267;&#25552;&#39640;&#20102;&#36755;&#20986;&#36136;&#37327;&#65292;&#22312;&#22810;&#35821;&#35328;&#24773;&#20917;&#19979;&#65292;&#24179;&#22343;WER&#25552;&#39640;&#20102;1.1&#65292;BLEU&#25552;&#39640;&#20102;0.4&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world applications, users often require both translations and transcriptions of speech to enhance their comprehension, particularly in streaming scenarios where incremental generation is necessary. This paper introduces a streaming Transformer-Transducer that jointly generates automatic speech recognition (ASR) and speech translation (ST) outputs using a single decoder. To produce ASR and ST content effectively with minimal latency, we propose a joint token-level serialized output training method that interleaves source and target words by leveraging an off-the-shelf textual aligner. Experiments in monolingual (it-en) and multilingual (\{de,es,it\}-en) settings demonstrate that our approach achieves the best quality-latency balance. With an average ASR latency of 1s and ST latency of 1.3s, our model shows no degradation or even improves output quality compared to separate ASR and ST models, yielding an average improvement of 1.1 WER and 0.4 BLEU in the multilingual case.
&lt;/p&gt;</description></item><item><title>phi-1&#26159;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31934;&#24515;&#35757;&#32451;&#21644;&#20248;&#21270;&#65292;&#23613;&#31649;&#35268;&#27169;&#30456;&#23545;&#36739;&#23567;&#65292;&#20294;&#22312;&#20934;&#30830;&#29575;&#21644;&#26032;&#30340;&#24615;&#36136;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.11644</link><description>&lt;p&gt;
&#25945;&#31185;&#20070;&#26159;&#20320;&#38656;&#35201;&#30340;&#20840;&#37096;&#12290; (arXiv:2306.11644v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Textbooks Are All You Need. (arXiv:2306.11644v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11644
&lt;/p&gt;
&lt;p&gt;
phi-1&#26159;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31934;&#24515;&#35757;&#32451;&#21644;&#20248;&#21270;&#65292;&#23613;&#31649;&#35268;&#27169;&#30456;&#23545;&#36739;&#23567;&#65292;&#20294;&#22312;&#20934;&#30830;&#29575;&#21644;&#26032;&#30340;&#24615;&#36136;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;phi-1&#65292;&#20854;&#20307;&#31215;&#26126;&#26174;&#23567;&#20110;&#31454;&#20105;&#27169;&#22411;&#65306;phi-1&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#25317;&#26377;13&#20159;&#20010;&#21442;&#25968;&#65292;&#22312;8&#20010;A100&#19978;&#36827;&#34892;&#20102;4&#22825;&#30340;&#35757;&#32451;&#65292;&#20351;&#29992;&#20102;&#26469;&#33258;&#32593;&#32476;&#30340;&#8220;&#25945;&#31185;&#20070;&#36136;&#37327;&#8221;&#25968;&#25454;&#65288;60&#20159;&#20010;&#26631;&#35760;&#65289;&#21644;&#20351;&#29992;GPT-3.5&#21512;&#25104;&#29983;&#25104;&#30340;&#25945;&#31185;&#20070;&#21644;&#32451;&#20064;&#65288;10&#20159;&#20010;&#26631;&#35760;&#65289;&#12290;&#23613;&#31649;&#35268;&#27169;&#23567;&#65292;phi-1&#22312;HumanEval&#19978;&#30340;pass@1&#20934;&#30830;&#29575;&#20026;50.6&#65285;&#65292;&#22312;MBPP&#19978;&#20026;55.5&#65285;&#12290;&#19982;&#25105;&#20204;&#22312;&#32534;&#30721;&#32451;&#20064;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#20043;&#21069;&#30340;&#27169;&#22411; phi-1-base &#21644;&#20855;&#26377;&#30456;&#21516;&#27969;&#31243;&#30340;350M&#21442;&#25968;&#30340;&#36739;&#23567;&#27169;&#22411; phi-1-small &#30456;&#27604;&#65292;&#23427;&#36824;&#23637;&#29616;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#26032;&#30340;&#24615;&#36136;&#65292;phi-1-small &#22312; HumanEval &#19978;&#20173;&#36798;&#21040;45&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.
&lt;/p&gt;</description></item><item><title>Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.08018</link><description>&lt;p&gt;
Mol-Instructions: &#19968;&#20010;&#22823;&#35268;&#27169;&#29983;&#29289;&#20998;&#23376;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20026;&#22823;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. (arXiv:2306.08018v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08018
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#21331;&#36234;&#30340;&#20219;&#21153;&#22788;&#29702;&#33021;&#21147;&#21644;&#21019;&#26032;&#30340;&#36755;&#20986;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#25512;&#21160;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#31561;&#19987;&#19994;&#39046;&#22495;&#30340;&#29087;&#32451;&#24212;&#29992;&#36824;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Mol-Instructions&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#12289;&#19987;&#38376;&#38024;&#23545;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;Mol-Instructions&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#20998;&#23376;&#23548;&#21521;&#25351;&#20196;&#12289;&#34507;&#30333;&#36136;&#23548;&#21521;&#25351;&#20196;&#21644;&#29983;&#29289;&#20998;&#23376;&#25991;&#26412;&#25351;&#20196;&#65292;&#27599;&#20010;&#37096;&#20998;&#37117;&#34987;&#31574;&#21010;&#29992;&#20110;&#22686;&#24378;LLM&#23545;&#29983;&#29289;&#20998;&#23376;&#29305;&#24615;&#21644;&#34892;&#20026;&#30340;&#29702;&#35299;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#20195;&#34920;&#24615;LLM&#30340;&#24191;&#27867;&#25351;&#20196;&#35843;&#25972;&#23454;&#39564;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;Mol-Instructions&#22312;&#22686;&#24378;&#22823;&#27169;&#22411;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#22797;&#26434;&#39046;&#22495;&#20869;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#20419;&#36827;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a meticulously curated, comprehensive instruction dataset expressly designed for the biomolecular realm. Mol-Instructions is composed of three pivotal components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions, each curated to enhance the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on the representative LLM, we underscore the potency of Mol-Instructions to enhance the adaptability and cognitive acuity of large models within the complex sphere of biomolecular studies, thereby promoting advancements in the biomol
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20195;&#30721;&#25551;&#36848;&#20013;&#24341;&#20837;&#25200;&#21160;&#26469;&#22686;&#24378;AI&#25915;&#20987;&#24615;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#21487;&#26377;&#25928;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#22120;&#23545;&#25200;&#21160;&#21644;&#38750;&#25200;&#21160;&#30340;&#20195;&#30721;&#25551;&#36848;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05079</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#25552;&#21319;AI&#25915;&#20987;&#24615;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Robustness of AI Offensive Code Generators via Data Augmentation. (arXiv:2306.05079v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20195;&#30721;&#25551;&#36848;&#20013;&#24341;&#20837;&#25200;&#21160;&#26469;&#22686;&#24378;AI&#25915;&#20987;&#24615;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#21487;&#26377;&#25928;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#22120;&#23545;&#25200;&#21160;&#21644;&#38750;&#25200;&#21160;&#30340;&#20195;&#30721;&#25551;&#36848;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25200;&#21160;&#28155;&#21152;&#21040;&#23433;&#20840;&#24615;&#20195;&#30721;&#19978;&#19979;&#25991;&#20013;&#30340;&#20195;&#30721;&#25551;&#36848;&#20013;&#30340;&#26041;&#27861;&#65292;&#21363;&#26469;&#33258;&#21892;&#24847;&#24320;&#21457;&#32773;&#30340;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#65288;NL&#65289;&#65292;&#24182;&#20998;&#26512;&#20102;&#25200;&#21160;&#22914;&#20309;&#20197;&#21450;&#22312;&#20160;&#20040;&#31243;&#24230;&#19978;&#24433;&#21709;AI&#25915;&#20987;&#24615;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;NL&#25551;&#36848;&#20013;&#30340;&#25200;&#21160;&#39640;&#24230;&#24433;&#21709;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#35813;&#26041;&#27861;&#25191;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#21363;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21464;&#24322;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#24182;&#35777;&#26126;&#20854;&#23545;&#25200;&#21160;&#21644;&#38750;&#25200;&#21160;&#30340;&#20195;&#30721;&#25551;&#36848;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a method to add perturbations to the code descriptions, i.e., new inputs in natural language (NL) from well-intentioned developers, in the context of security-oriented code, and analyze how and to what extent perturbations affect the performance of AI offensive code generators. Our experiments show that the performance of the code generators is highly affected by perturbations in the NL descriptions. To enhance the robustness of the code generators, we use the method to perform data augmentation, i.e., to increase the variability and diversity of the training data, proving its effectiveness against both perturbed and non-perturbed code descriptions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;I-STAR&#65292;&#21487;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#65292;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#32452;&#21512;&#34920;&#31034;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.19358</link><description>&lt;p&gt;
&#31283;&#20581;&#30340;&#21508;&#21521;&#24322;&#24615;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stable Anisotropic Regularization. (arXiv:2305.19358v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;I-STAR&#65292;&#21487;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#65292;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#32452;&#21512;&#34920;&#31034;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#65292;&#30740;&#31350;&#27169;&#22411;&#28608;&#27963;&#30340;&#23646;&#24615;&#24050;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#12290;&#25991;&#29486;&#26222;&#36941;&#35748;&#20026;LLMs&#34920;&#31034;&#30001;&#23569;&#25968;&#20855;&#26377;&#26497;&#39640;&#26041;&#24046;&#21644;&#24133;&#24230;&#30340;&#8220;&#24322;&#24120;&#32500;&#24230;&#8221;&#20027;&#23548;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#20960;&#39033;&#30740;&#31350;&#35797;&#22270;&#20943;&#36731;&#36825;&#20123;&#24322;&#24120;&#32500;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#36843;&#20351;LLMs&#25104;&#20026;&#21508;&#21521;&#21516;&#24615;&#65288;&#21363;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#25152;&#26377;&#32500;&#24230;&#20855;&#26377;&#22343;&#21248;&#26041;&#24046;&#65289;&#30340;&#12290;&#21508;&#21521;&#21516;&#24615;&#34987;&#35748;&#20026;&#26159;LLMs&#30340;&#19968;&#31181;&#29702;&#24819;&#23646;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#26356;&#21152;&#36148;&#36817;&#20154;&#31867;&#30452;&#35273;&#30340;&#25991;&#26412;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;NLP&#20013;&#21508;&#21521;&#21516;&#24615;&#30340;&#35768;&#22810;&#35266;&#28857;&#37117;&#26159;&#22522;&#20110;&#23884;&#20837;&#30340;&#24179;&#22343;&#20313;&#24358;&#30456;&#20284;&#24230;&#65292;&#26368;&#36817;&#24050;&#32463;&#34920;&#26126;&#36825;&#26159;&#19968;&#31181;&#26377;&#32570;&#38519;&#30340;&#21508;&#21521;&#21516;&#24615;&#24230;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;I-STAR&#65306;&#22522;&#20110;IsoScore$^{\star}$&#30340;&#31283;&#23450;&#21508;&#21521;&#24322;&#24615;&#27491;&#21017;&#21270;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the success of Large Language Models (LLMs), there has been considerable interest in studying the properties of model activations. The literature overwhelmingly agrees that LLM representations are dominated by a few ``outlier dimensions'' with exceedingly high variance and magnitude. Several studies in Natural Language Processing (NLP) have sought to mitigate the impact of such outlier dimensions and force LLMs to be isotropic (i.e., have uniform variance across all dimensions in embedding space). Isotropy is thought to be a desirable property for LLMs that improves model performance and more closely aligns textual representations with human intuition. However, many of the claims regarding isotropy in NLP have been based on the average cosine similarity of embeddings, which has recently been shown to be a flawed measure of isotropy. In this paper, we propose I-STAR: IsoScore$^{\star}$-based STable Anisotropic Regularization, a novel regularization method that can be used to incre
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34987;&#21160;&#23398;&#20064;&#65292;&#22312;&#26234;&#33021;&#20307;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#20197;&#23398;&#20064;&#21040;&#19968;&#33324;&#21270;&#30340;&#20027;&#21160;&#22240;&#26524;&#31574;&#30053;&#65292;&#29992;&#20110;&#30830;&#23450;&#21644;&#20351;&#29992;&#22240;&#26524;&#20851;&#31995;&#32467;&#26500;&#12290;&#36890;&#36807;&#27169;&#20223;&#19987;&#23478;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#27979;&#35797;&#26102;&#25512;&#26029;&#21644;&#20351;&#29992;&#20174;&#26410;&#20986;&#29616;&#30340;&#22240;&#26524;&#38142;&#25509;&#65292;&#24182;&#23558;&#23454;&#39564;&#31574;&#30053;&#25512;&#24191;&#21040;&#20174;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#21464;&#37327;&#38598;&#12290;</title><link>http://arxiv.org/abs/2305.16183</link><description>&lt;p&gt;
&#22312;&#26234;&#33021;&#20307;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#34987;&#21160;&#23398;&#20064;&#20027;&#21160;&#22240;&#26524;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Passive learning of active causal strategies in agents and language models. (arXiv:2305.16183v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16183
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34987;&#21160;&#23398;&#20064;&#65292;&#22312;&#26234;&#33021;&#20307;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#20197;&#23398;&#20064;&#21040;&#19968;&#33324;&#21270;&#30340;&#20027;&#21160;&#22240;&#26524;&#31574;&#30053;&#65292;&#29992;&#20110;&#30830;&#23450;&#21644;&#20351;&#29992;&#22240;&#26524;&#20851;&#31995;&#32467;&#26500;&#12290;&#36890;&#36807;&#27169;&#20223;&#19987;&#23478;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#27979;&#35797;&#26102;&#25512;&#26029;&#21644;&#20351;&#29992;&#20174;&#26410;&#20986;&#29616;&#30340;&#22240;&#26524;&#38142;&#25509;&#65292;&#24182;&#23558;&#23454;&#39564;&#31574;&#30053;&#25512;&#24191;&#21040;&#20174;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#21464;&#37327;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34987;&#21160;&#25968;&#25454;&#65292;&#25105;&#20204;&#33021;&#22815;&#23398;&#20064;&#21040;&#20851;&#20110;&#22240;&#26524;&#20851;&#31995;&#21644;&#23454;&#39564;&#30340;&#20160;&#20040;&#20449;&#24687;&#65311;&#37492;&#20110;&#34987;&#21160;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#20855;&#20351;&#29992;&#31561;&#20132;&#20114;&#39046;&#22495;&#30340;&#26368;&#26032;&#25104;&#21151;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#24456;&#37325;&#35201;&#12290;&#34987;&#21160;&#23398;&#20064;&#26412;&#36136;&#19978;&#26159;&#26377;&#38480;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32431;&#31929;&#30340;&#34987;&#21160;&#23398;&#20064;&#23454;&#38469;&#19978;&#33021;&#22815;&#35753;&#26234;&#33021;&#20307;&#23398;&#20064;&#21040;&#19968;&#33324;&#21270;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#30830;&#23450;&#21644;&#20351;&#29992;&#22240;&#26524;&#20851;&#31995;&#32467;&#26500;&#65292;&#21482;&#35201;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#27979;&#35797;&#26102;&#24178;&#39044;&#12290;&#25105;&#20204;&#22312;&#24418;&#24335;&#19978;&#35828;&#26126;&#20102;&#39318;&#20808;&#36827;&#34892;&#23454;&#39564;&#65292;&#28982;&#21518;&#23547;&#27714;&#30446;&#26631;&#30340;&#31574;&#30053;&#33021;&#22815;&#21407;&#21017;&#19978;&#20351;&#34987;&#21160;&#23398;&#20064;&#23454;&#29616;&#19968;&#33324;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#32463;&#39564;&#19978;&#23637;&#31034;&#20102;&#36890;&#36807;&#27169;&#20223;&#19987;&#23478;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#22312;&#27979;&#35797;&#26102;&#33021;&#22815;&#25512;&#26029;&#21644;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#20174;&#26410;&#20986;&#29616;&#30340;&#22240;&#26524;&#38142;&#25509;&#65307;&#36825;&#20123;&#26234;&#33021;&#20307;&#36824;&#33021;&#22815;&#23558;&#23454;&#39564;&#31574;&#30053;&#25512;&#24191;&#21040;&#20174;&#26410;&#22312;&#35757;&#32451;&#20013;&#35266;&#23519;&#21040;&#30340;&#26032;&#21464;&#37327;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#34987;&#21160;&#25968;&#25454;&#20013;&#19968;&#33324;&#21270;&#22240;&#26524;&#24178;&#39044;&#21644;&#21033;&#29992;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
What can be learned about causality and experimentation from passive data? This question is salient given recent successes of passively-trained language models in interactive domains such as tool use. Passive learning is inherently limited. However, we show that purely passive learning can in fact allow an agent to learn generalizable strategies for determining and using causal structures, as long as the agent can intervene at test time. We formally illustrate that learning a strategy of first experimenting, then seeking goals, can allow generalization from passive learning in principle. We then show empirically that agents trained via imitation on expert data can indeed generalize at test time to infer and use causal links which are never present in the training data; these agents can also generalize experimentation strategies to novel variable sets never observed in training. We then show that strategies for causal intervention and exploitation can be generalized from passive data ev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30456;&#30683;&#30462;&#24187;&#35273;&#36827;&#34892;&#20102;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;&#65292;&#25506;&#31350;&#20102;&#36825;&#19968;&#24187;&#35273;&#24418;&#24335;&#30340;&#26222;&#36941;&#23384;&#22312;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#26694;&#26550;&#26377;&#25928;&#35302;&#21457;&#33258;&#30456;&#30683;&#30462;&#65292;&#21457;&#29616;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#20013;&#36825;&#31181;&#29616;&#35937;&#37117;&#39057;&#32321;&#20986;&#29616;&#12290;ChatGPT&#21644;GPT-4&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#33258;&#30456;&#30683;&#30462;&#65292;&#32780;Vicuna-13B&#21017;&#26377;&#20123;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2305.15852</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30456;&#30683;&#30462;&#24187;&#35273;&#65306;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation. (arXiv:2305.15852v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30456;&#30683;&#30462;&#24187;&#35273;&#36827;&#34892;&#20102;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;&#65292;&#25506;&#31350;&#20102;&#36825;&#19968;&#24187;&#35273;&#24418;&#24335;&#30340;&#26222;&#36941;&#23384;&#22312;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#26694;&#26550;&#26377;&#25928;&#35302;&#21457;&#33258;&#30456;&#30683;&#30462;&#65292;&#21457;&#29616;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#20013;&#36825;&#31181;&#29616;&#35937;&#37117;&#39057;&#32321;&#20986;&#29616;&#12290;ChatGPT&#21644;GPT-4&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#33258;&#30456;&#30683;&#30462;&#65292;&#32780;Vicuna-13B&#21017;&#26377;&#20123;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#20135;&#29983;&#24187;&#24819;&#30340;&#25991;&#26412;&#12290;&#33258;&#30456;&#30683;&#30462;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#24187;&#35273;&#24418;&#24335;&#65292;&#25351;&#30340;&#26159;&#35821;&#35328;&#27169;&#22411;&#22312;&#21516;&#19968;&#35821;&#22659;&#20013;&#29983;&#25104;&#20004;&#20010;&#30683;&#30462;&#30340;&#21477;&#23376;&#12290;&#26412;&#25991;&#38024;&#23545;&#26368;&#20808;&#36827;&#12289;&#32463;&#36807;&#25351;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23545;&#33258;&#30456;&#30683;&#30462;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#12289;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#26377;&#25928;&#22320;&#35302;&#21457;&#33258;&#30456;&#30683;&#30462;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#33879;&#21517;&#30340;&#36824;&#26159;&#19981;&#22826;&#20986;&#21517;&#30340;&#35805;&#39064;&#65292;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#30456;&#30683;&#30462;&#37117;&#32463;&#24120;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (large LMs) are susceptible to producing text with hallucinated content. Self-contradiction, where the LM generates two contradictory sentences within the same context, is an important form of hallucination. In this work, we present a comprehensive analysis on self-contradiction for state-of-the-art, instruction-tuned LMs, including evaluation, detection, and mitigation. To effectively trigger self-contradictions, we design a framework that constrains LMs to generate appropriate sentence pairs. Our evaluation on these sentence pairs reveals that self-contradictions occur frequently across different LMs for both famous and lesser-known topics. Next, we prompt the LMs to detect self-contradictions. Our results indicate that ChatGPT and GPT-4 are able to accurately identify self-contradictions, while Vicuna-13B struggles to do so. For example, with our best prompting method, ChatGPT achieves 91.0% precision and 80.5% recall on the sentence pairs generated by itself. 
&lt;/p&gt;</description></item><item><title>STAR&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#25968;&#25454;&#23454;&#20363;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#20302;&#36164;&#28304;&#20449;&#24687;&#25277;&#21462;&#65292;&#20026;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#38656;&#35201;&#26368;&#23569;&#20154;&#24037;&#26631;&#27880;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.15090</link><description>&lt;p&gt;
STAR: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#32467;&#26500;&#21040;&#25991;&#26412;&#25968;&#25454;&#29983;&#25104;&#25913;&#36827;&#20302;&#36164;&#28304;&#20449;&#24687;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
STAR: Improving Low-Resource Information Extraction by Structure-to-Text Data Generation with Large Language Models. (arXiv:2305.15090v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15090
&lt;/p&gt;
&lt;p&gt;
STAR&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#25968;&#25454;&#23454;&#20363;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#20302;&#36164;&#28304;&#20449;&#24687;&#25277;&#21462;&#65292;&#20026;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#38656;&#35201;&#26368;&#23569;&#20154;&#24037;&#26631;&#27880;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#65292;&#22914;&#20107;&#20214;&#25277;&#21462;&#65292;&#38656;&#35201;&#23545;&#36755;&#20986;&#32467;&#26500;&#21644;&#23376;&#20219;&#21153;&#20381;&#36182;&#36827;&#34892;&#28145;&#20837;&#29702;&#35299;&#12290;&#20026;&#20102;&#33719;&#24471;&#21512;&#29702;&#30340;&#24615;&#33021;&#65292;&#23427;&#20204;&#20005;&#37325;&#20381;&#36182;&#20110;&#20197;&#65288;&#27573;&#33853;&#65292;&#30446;&#26631;&#32467;&#26500;&#65289;&#23545;&#30340;&#24418;&#24335;&#30340;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#20154;&#24037;&#27880;&#37322;&#33719;&#24471;&#36825;&#26679;&#30340;&#25968;&#25454;&#26159;&#26114;&#36149;&#30340;&#65292;&#22240;&#27492;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#65292;&#25105;&#20204;&#36843;&#20999;&#38656;&#35201;&#38656;&#35201;&#26368;&#23569;&#20154;&#24037;&#26631;&#27880;&#30340;&#20302;&#36164;&#28304;&#20449;&#24687;&#25277;&#21462;&#26041;&#27861;&#12290;&#20351;&#29992;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#23545;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#33021;&#26159;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#20294;&#29616;&#26377;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#35201;&#20040;&#20173;&#28982;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#35201;&#20040;&#30001;&#20110;&#24615;&#33021;&#24046;&#32780;&#26080;&#27861;&#24212;&#29992;&#20110;&#22797;&#26434;&#30340;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;STAR&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26681;&#25454;&#26377;&#38480;&#30340;&#31181;&#23376;&#31034;&#20363;&#21512;&#25104;&#25968;&#25454;&#23454;&#20363;&#65292;&#20174;&#32780;&#25552;&#39640;&#20302;&#36164;&#28304;&#20449;&#24687;&#25277;&#21462;&#24615;&#33021;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information extraction tasks such as event extraction require an in-depth understanding of the output structure and sub-task dependencies. They heavily rely on task-specific training data in the form of (passage, target structure) pairs to obtain reasonable performance. However, obtaining such data through human annotation is costly, leading to a pressing need for low-resource information extraction approaches that require minimal human labeling for real-world applications. Fine-tuning supervised models with synthesized training data would be a generalizable method, but the existing data generation methods either still rely on large-scale ground-truth data or cannot be applied to complicated IE tasks due to their poor performance. To address these challenges, we propose STAR, a data generation method that leverages Large Language Models (LLMs) to synthesize data instances given limited seed demonstrations, thereby boosting low-resource information extraction performance. Our approach i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#25512;&#29702;&#20013;&#30340;&#33258;&#27965;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20551;&#35774;&#33258;&#27965;&#24615;&#21644;&#32452;&#21512;&#33258;&#27965;&#24615;&#20004;&#20010;&#37325;&#35201;&#29305;&#24615;&#65292;&#24182;&#21457;&#29616;GPT-3/-4&#27169;&#22411;&#22312;&#36825;&#20004;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#20102;&#36739;&#24046;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14279</link><description>&lt;p&gt;
LLM&#30340;&#22810;&#27493;&#25512;&#29702;&#20013;&#30340;&#20004;&#20010;&#33258;&#27965;&#22833;&#36133;
&lt;/p&gt;
&lt;p&gt;
Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs. (arXiv:2305.14279v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#25512;&#29702;&#20013;&#30340;&#33258;&#27965;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20551;&#35774;&#33258;&#27965;&#24615;&#21644;&#32452;&#21512;&#33258;&#27965;&#24615;&#20004;&#20010;&#37325;&#35201;&#29305;&#24615;&#65292;&#24182;&#21457;&#29616;GPT-3/-4&#27169;&#22411;&#22312;&#36825;&#20004;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#20102;&#36739;&#24046;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#19978;&#19979;&#25991;&#20026;&#22522;&#30784;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24191;&#27867;&#25104;&#21151;&#65292;&#20294;&#36825;&#31181;&#25104;&#21151;&#36890;&#24120;&#26159;&#36890;&#36807;&#27491;&#30830;&#24615;&#32780;&#19981;&#26159;&#19968;&#33268;&#24615;&#26469;&#35780;&#20272;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#22312;&#35299;&#20915;&#30001;&#22810;&#20010;&#23376;&#27493;&#39588;&#30340;&#31572;&#26696;&#32452;&#25104;&#30340;&#20219;&#21153;&#30340;&#22810;&#27493;&#25512;&#29702;&#20013;&#65292;&#33258;&#27965;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#26631;&#20934;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23545;&#20110;&#22810;&#27493;&#25512;&#29702;&#29305;&#21035;&#37325;&#35201;&#30340;&#33258;&#27965;&#24615;&#31867;&#22411;&#65306;&#20551;&#35774;&#33258;&#27965;&#24615;&#65288;&#27169;&#22411;&#22312;&#20551;&#35774;&#30340;&#20854;&#20182;&#19978;&#19979;&#25991;&#20013;&#30340;&#36755;&#20986;&#39044;&#27979;&#33021;&#21147;&#65289;&#21644;&#32452;&#21512;&#33258;&#27965;&#24615;&#65288;&#24403;&#23558;&#20013;&#38388;&#23376;&#27493;&#39588;&#26367;&#25442;&#20026;&#27169;&#22411;&#23545;&#36825;&#20123;&#27493;&#39588;&#30340;&#36755;&#20986;&#26102;&#65292;&#27169;&#22411;&#30340;&#26368;&#32456;&#36755;&#20986;&#30340;&#19968;&#33268;&#24615;&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;GPT-3/-4&#27169;&#22411;&#30340;&#22810;&#20010;&#21464;&#20307;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#37117;&#34920;&#29616;&#20986;&#20102;&#20302;&#19968;&#33268;&#24615;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved widespread success on a variety of in-context few-shot tasks, but this success is typically evaluated via correctness rather than consistency. We argue that self-consistency is an important criteria for valid multi-step reasoning in tasks where the solution is composed of the answers to multiple sub-steps. We propose two types of self-consistency that are particularly important for multi-step reasoning -hypothetical consistency (a model's ability to predict what its output would be in a hypothetical other context) and compositional consistency (consistency of a model's final outputs when intermediate sub-steps are replaced with the model's outputs for those steps). We demonstrate that multiple variants of the GPT-3/-4 models exhibit poor consistency rates across both types of consistency on a variety of tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#35821;&#35328;&#20266;&#26631;&#27880;&#30340;&#26080;&#30417;&#30563;ASR&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#26631;&#27880;&#25968;&#25454;&#26469;&#24341;&#23548;&#26032;&#35821;&#35328;&#30340;&#26080;&#30417;&#30563;AM&#12290;&#22312;Common Voice&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#21487;&#20197;&#23454;&#29616;18% WER&#12290;&#32780;&#19988;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13330</link><description>&lt;p&gt;
&#22522;&#20110;&#36328;&#35821;&#35328;&#20266;&#26631;&#27880;&#30340;&#26080;&#30417;&#30563;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Unsupervised ASR via Cross-Lingual Pseudo-Labeling. (arXiv:2305.13330v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#35821;&#35328;&#20266;&#26631;&#27880;&#30340;&#26080;&#30417;&#30563;ASR&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#26631;&#27880;&#25968;&#25454;&#26469;&#24341;&#23548;&#26032;&#35821;&#35328;&#30340;&#26080;&#30417;&#30563;AM&#12290;&#22312;Common Voice&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#21487;&#20197;&#23454;&#29616;18% WER&#12290;&#32780;&#19988;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#38750;&#37197;&#23545;&#30340;&#38899;&#39057;&#21644;&#25991;&#26412;&#26469;&#35757;&#32451;&#26080;&#30417;&#30563;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#12290;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;ASR&#26041;&#27861;&#20551;&#23450;&#19981;&#33021;&#20351;&#29992;&#20219;&#20309;&#26631;&#27880;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#21363;&#20351;&#27809;&#26377;&#32473;&#23450;&#35821;&#35328;&#30340;&#20219;&#20309;&#26631;&#27880;&#38899;&#39057;&#65292;&#20063;&#22987;&#32456;&#21487;&#20197;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#26631;&#27880;&#25968;&#25454;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;&#30340;&#23383;&#31526;&#32423;&#22768;&#23398;&#27169;&#22411;&#65288;AM&#65289;&#65292;&#26469;&#24341;&#23548;&#26032;&#35821;&#35328;&#30340;&#26080;&#30417;&#30563;AM&#12290; &#36825;&#37324;&#65292;&#8220;&#26080;&#30417;&#30563;&#8221;&#24847;&#21619;&#30528;&#27809;&#26377;&#21487;&#29992;&#20110;&#30446;&#26631;&#35821;&#35328;&#30340;&#26631;&#27880;&#38899;&#39057;&#12290;&#26412;&#25991;&#30340;&#26041;&#27861;&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#65288;i&#65289;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;AM&#29983;&#25104;&#8220;&#30446;&#26631;&#8221;&#35821;&#35328;&#30340;&#20266;&#26631;&#31614;&#65288;PLs&#65289;&#65307;&#65288;ii&#65289;&#20351;&#29992;&#8220;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#8221;&#38480;&#21046;&#36825;&#20123;PLs&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Common Voice&#19978;&#38750;&#24120;&#26377;&#25928;&#65306;&#20363;&#22914;&#65292;&#23558;&#33521;&#35821;AM&#20256;&#36882;&#21040;&#26031;&#29926;&#24076;&#37324;&#35821;&#21487;&#20197;&#23454;&#29616;18&#65285;&#30340;WER&#12290; &#23427;&#36824;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#22522;&#20110;&#23383;&#31526;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that it is possible to train an $\textit{unsupervised}$ automatic speech recognition (ASR) system using only unpaired audio and text. Existing unsupervised ASR methods assume that no labeled data can be used for training. We argue that even if one does not have any labeled audio for a given language, there is $\textit{always}$ labeled data available for other languages. We show that it is possible to use character-level acoustic models (AMs) from other languages to bootstrap an $\textit{unsupervised}$ AM in a new language. Here, "unsupervised" means no labeled audio is available for the $\textit{target}$ language. Our approach is based on two key ingredients: (i) generating pseudo-labels (PLs) of the $\textit{target}$ language using some $\textit{other}$ language AM and (ii) constraining these PLs with a $\textit{target language model}$. Our approach is effective on Common Voice: e.g. transfer of English AM to Swahili achieves 18% WER. It also outperforms characte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CRITIC&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#19982;&#24037;&#20855;&#30340;&#20132;&#20114;&#26657;&#27491;&#33258;&#24049;&#30340;&#38169;&#35823;&#65292;&#20174;&#32780;&#36991;&#20813;&#29983;&#25104;&#20986;&#29616;&#19981;&#19968;&#33268;&#21644;&#38382;&#39064;&#34892;&#20026;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11738</link><description>&lt;p&gt;
CRITIC&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24037;&#20855;&#20132;&#20114;&#25209;&#35780;&#36827;&#34892;&#33258;&#25105;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing. (arXiv:2305.11738v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CRITIC&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#19982;&#24037;&#20855;&#30340;&#20132;&#20114;&#26657;&#27491;&#33258;&#24049;&#30340;&#38169;&#35823;&#65292;&#20174;&#32780;&#36991;&#20813;&#29983;&#25104;&#20986;&#29616;&#19981;&#19968;&#33268;&#21644;&#38382;&#39064;&#34892;&#20026;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#38750;&#24120;&#24341;&#20154;&#27880;&#30446;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26377;&#26102;&#20250;&#20986;&#29616;&#19981;&#19968;&#33268;&#21644;&#38382;&#39064;&#34892;&#20026;&#65292;&#20363;&#22914;&#20986;&#29616;&#24187;&#35273;&#20107;&#23454;&#65292;&#29983;&#25104;&#26377;&#32570;&#38519;&#30340;&#20195;&#30721;&#25110;&#21019;&#24314;&#20882;&#29359;&#21644;&#26377;&#23475;&#30340;&#20869;&#23481;&#12290;&#19982;&#36825;&#20123;&#27169;&#22411;&#19981;&#21516;&#65292;&#20154;&#31867;&#36890;&#24120;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#26469;&#20132;&#21449;&#26816;&#26597;&#21644;&#31934;&#28860;&#20182;&#20204;&#30340;&#21021;&#27493;&#20869;&#23481;&#65292;&#20363;&#22914;&#20351;&#29992;&#25628;&#32034;&#24341;&#25806;&#36827;&#34892;&#20107;&#23454;&#26816;&#26597;&#25110;&#20351;&#29992;&#20195;&#30721;&#35299;&#37322;&#22120;&#36827;&#34892;&#35843;&#35797;&#12290;&#21463;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;CRITIC&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;LLMs&#65288;&#23454;&#36136;&#19978;&#26159;&#8220;&#40657;&#30418;&#23376;&#8221;&#65289;&#20197;&#31867;&#20284;&#20110;&#20154;&#31867;&#19982;&#24037;&#20855;&#20132;&#20114;&#30340;&#26041;&#24335;&#39564;&#35777;&#21644;&#36880;&#27493;&#20462;&#27491;&#33258;&#24049;&#30340;&#36755;&#20986;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#20174;&#21021;&#22987;&#36755;&#20986;&#24320;&#22987;&#65292;CRITIC&#19982;&#36866;&#24403;&#30340;&#24037;&#20855;&#20132;&#20114;&#20197;&#35780;&#20272;&#25991;&#26412;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#28982;&#21518;&#26681;&#25454;&#22312;&#27492;&#39564;&#35777;&#36807;&#31243;&#20013;&#33719;&#24471;&#30340;&#21453;&#39304;&#20462;&#25913;&#36755;&#20986;&#12290;&#28041;&#21450;&#33258;&#30001;&#24418;&#24335;&#38382;&#31572;&#12289;&#25968;&#23398;&#31243;&#24207;&#32508;&#21512;&#21644;&#27602;&#24615;&#26816;&#27979;&#30340;&#20840;&#38754;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;LLMs&#33021;&#22815;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#24182;&#32416;&#27491;&#33258;&#24049;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in large language models (LLMs) have been impressive. However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. Unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. Inspired by this observation, we introduce a framework called CRITIC that allows LLMs, which are essentially "black boxes" to validate and progressively amend their own outputs in a manner similar to human interaction with tools. More specifically, starting with an initial output, CRITIC interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. Comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxi
&lt;/p&gt;</description></item><item><title>RCOT &#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#32416;&#27491; LLM &#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#65292;&#20197;&#25552;&#39640; LLM &#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.11499</link><description>&lt;p&gt;
RCOT&#65306;&#36890;&#36807;&#21453;&#36716;&#24605;&#32500;&#38142;&#26465;&#26816;&#27979;&#21644;&#32416;&#27491;&#25512;&#29702;&#20013;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought. (arXiv:2305.11499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11499
&lt;/p&gt;
&lt;p&gt;
RCOT &#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#32416;&#27491; LLM &#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#65292;&#20197;&#25552;&#39640; LLM &#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#36880;&#27493;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#22312;&#31639;&#26415;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25104;&#32489;&#12290;&#28982;&#32780;&#65292;LLM&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#38754;&#20020;&#30528;&#32500;&#25252;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#25361;&#25112;&#65292;&#34920;&#29616;&#20986;&#22312;&#32473;&#23450;&#38382;&#39064;&#19978;&#30830;&#23450;&#36807;&#24230;&#12289;&#38382;&#39064;&#35823;&#35299;&#21644;&#26465;&#20214;&#24187;&#35273;&#30340;&#36235;&#21183;&#12290;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#31895;&#31890;&#24230;&#21453;&#39304;&#65288;&#20363;&#22914;&#65292;&#31572;&#26696;&#26159;&#21542;&#27491;&#30830;&#65289;&#26469;&#25552;&#39640;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;RCoT&#65288;&#21453;&#36716;CoT&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#26816;&#27979;&#21644;&#32416;&#27491;LLM&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#26469;&#25552;&#39640;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#20102;&#26816;&#27979;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#65292;RCoT&#39318;&#20808;&#35201;&#27714;LLMs&#22522;&#20110;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#37325;&#26500;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23545;&#27604;&#21407;&#22987;&#38382;&#39064;&#21644;&#37325;&#26500;&#38382;&#39064;&#65292;&#36739;&#20026;&#35814;&#32454;&#22320;&#25581;&#31034;&#20102;&#21407;&#22987;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#32416;&#27491;&#35299;&#20915;&#26041;&#26696;&#65292;RCoT&#21046;&#23450;&#20102;&#26816;&#27979;&#21040;&#30340;fa
&lt;/p&gt;
&lt;p&gt;
Large language Models (LLMs) have achieved promising performance on arithmetic reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompting. However, LLMs face challenges in maintaining factual consistency during reasoning, exhibiting tendencies to condition overlooking, question misinterpretation, and condition hallucination over given problems. Existing methods use coarse-grained feedback (e.g., whether the answer is correct) to improve factual consistency. In this work, we propose RCoT (Reversing Chain-of-Thought), a novel method to improve LLMs' reasoning abilities by automatically detecting and rectifying factual inconsistency in LLMs' generated solutions. To detect factual inconsistency, RCoT first asks LLMs to reconstruct the problem based on generated solutions. Then fine-grained comparisons between the original problem and the reconstructed problem expose the factual inconsistency in the original solutions. To rectify the solution, RCoT formulates detected fa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26032;&#26041;&#27861;SAMA&#65292;&#36890;&#36807;&#25552;&#21069;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20219;&#21153;&#20998;&#35299;&#26469;&#35299;&#20915;ASG&#26041;&#27861;&#23384;&#22312;&#30340;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#21644;&#29983;&#25104;&#38750;&#23454;&#38469;&#20219;&#21153;&#22870;&#21169;&#30340;&#23376;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10865</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35821;&#20041;&#23545;&#40784;&#20219;&#21153;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Semantically Aligned Task Decomposition in Multi-Agent Reinforcement Learning. (arXiv:2305.10865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26032;&#26041;&#27861;SAMA&#65292;&#36890;&#36807;&#25552;&#21069;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20219;&#21153;&#20998;&#35299;&#26469;&#35299;&#20915;ASG&#26041;&#27861;&#23384;&#22312;&#30340;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#21644;&#29983;&#25104;&#38750;&#23454;&#38469;&#20219;&#21153;&#22870;&#21169;&#30340;&#23376;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#22411;MARL&#20013;&#30340;&#22870;&#21169;&#31232;&#30095;&#38382;&#39064;&#30528;&#37325;&#20110;&#36866;&#24403;&#30340;&#20449;&#29992;&#20998;&#37197;&#12290;&#33258;&#21160;&#23376;&#30446;&#26631;&#29983;&#25104;&#65288;ASG&#65289;&#26159;&#26368;&#36817;&#20986;&#29616;&#30340;&#19968;&#31181;&#21487;&#34892;&#30340;MARL&#26041;&#27861;&#65292;&#20854;&#28789;&#24863;&#26469;&#33258;&#20110;&#22312;&#20869;&#22312;&#39537;&#21160;&#30340;&#22686;&#24378;&#23398;&#20064;&#20013;&#21033;&#29992;&#23376;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#20174;&#31232;&#30095;&#22870;&#21169;&#20013;&#36827;&#34892;&#22797;&#26434;&#20219;&#21153;&#35268;&#21010;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26080;&#30097;&#38656;&#35201;&#22823;&#37327;&#30340;&#22521;&#35757;&#26679;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;"&#35299;&#32806;"&#20915;&#31574;&#26041;&#27861;&#65292;&#21363;&#22312;MARL&#20013;&#30340;&#35821;&#20041;&#23545;&#40784;&#20219;&#21153;&#20998;&#35299;&#65288;SAMA&#65289;&#65292;&#21463;&#21040;&#35299;&#32806;&#34920;&#31034;&#23398;&#20064;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
The difficulty of appropriately assigning credit is particularly heightened in cooperative MARL with sparse reward, due to the concurrent time and structural scales involved. Automatic subgoal generation (ASG) has recently emerged as a viable MARL approach inspired by utilizing subgoals in intrinsically motivated reinforcement learning. However, end-to-end learning of complex task planning from sparse rewards without prior knowledge, undoubtedly requires massive training samples. Moreover, the diversity-promoting nature of existing ASG methods can lead to the "over-representation" of subgoals, generating numerous spurious subgoals of limited relevance to the actual task reward and thus decreasing the sample efficiency of the algorithm. To address this problem and inspired by the disentangled representation learning, we propose a novel "disentangled" decision-making method, Semantically Aligned task decomposition in MARL (SAMA), that prompts pretrained language models with chain-of-thou
&lt;/p&gt;</description></item><item><title>ConvXAI&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#23884;&#20837;&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09770</link><description>&lt;p&gt;
ConvXAI&#65306;&#36890;&#36807;&#23545;&#35805;&#25552;&#20379;&#24322;&#26500;&#30340;AI&#35299;&#37322;&#65292;&#25903;&#25345;&#20154;&#26426;&#31185;&#25216;&#20889;&#20316;
&lt;/p&gt;
&lt;p&gt;
ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing. (arXiv:2305.09770v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09770
&lt;/p&gt;
&lt;p&gt;
ConvXAI&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#23884;&#20837;&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#65288;XAI&#65289;&#26041;&#27861;&#26469;&#35299;&#37322;AI&#31995;&#32479;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#26159;&#21542;&#23545;&#20154;&#31867;&#23454;&#29992;&#20173;&#23384;&#22312;&#19981;&#19968;&#33268;&#30340;&#21457;&#29616;&#12290;&#20026;&#20102;&#25913;&#21892;XAI&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#19968;&#31995;&#21015;&#30740;&#31350;&#30830;&#23450;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#22810;&#26679;&#21270;&#21644;&#21160;&#24577;&#30340;&#29992;&#25143;&#38656;&#27714;&#19982;&#29616;&#26377;XAI&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#30740;&#31350;&#35774;&#24819;&#23558;&#22810;&#31181;XAI&#26041;&#27861;&#38598;&#25104;&#21040;&#36890;&#29992;XAI&#30028;&#38754;&#65288;&#20363;&#22914;&#65292;&#22522;&#20110;&#23545;&#35805;&#25110;GUI&#30340;XAI&#31995;&#32479;&#65289;&#20013;&#20197;&#20943;&#36731;&#36825;&#20123;&#24046;&#36317;&#65292;&#20294;&#32570;&#23569;&#38024;&#23545;&#36825;&#20123;&#31995;&#32479;&#22914;&#20309;&#35774;&#35745;&#20197;&#28385;&#36275;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ConvXAI&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#36171;&#20104;&#29992;&#25143;&#36890;&#36807;&#36890;&#29992;&#30340;XAI&#23545;&#35805;&#30028;&#38754;&#25552;&#20986;&#21508;&#31181;XAI&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21019;&#26032;&#22320;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#65288;&#21363;&#65292;&#22522;&#20110;&#26684;&#24335;&#30740;&#31350;&#30340;&#22235;&#20010;&#21407;&#21017;&#65289;&#23884;&#20837;ConvXAI&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While various AI explanation (XAI) methods have been proposed to interpret AI systems, whether the state-of-the-art XAI methods are practically useful for humans remains inconsistent findings. To improve the usefulness of XAI methods, a line of studies identifies the gaps between the diverse and dynamic real-world user needs with the status quo of XAI methods. Although prior studies envision mitigating these gaps by integrating multiple XAI methods into the universal XAI interfaces (e.g., conversational or GUI-based XAI systems), there is a lack of work investigating how these systems should be designed to meet practical user needs. In this study, we present ConvXAI, a conversational XAI system that incorporates multiple XAI types, and empowers users to request a variety of XAI questions via a universal XAI dialogue interface. Particularly, we innovatively embed practical user needs (i.e., four principles grounding on the formative study) into ConvXAI design to improve practical useful
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;ChatGPT&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;N-back&#20219;&#21153;&#30340;&#34892;&#20026;&#34920;&#29616;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#30456;&#20284;&#65292;&#36825;&#20026;&#35774;&#35745;&#20855;&#26377;&#20154;&#31867;&#32423;&#35748;&#30693;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25552;&#20379;&#20102;&#20851;&#38190;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2305.03731</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;
&lt;/p&gt;
&lt;p&gt;
Assessing Working Memory Capacity of ChatGPT. (arXiv:2305.03731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;ChatGPT&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;N-back&#20219;&#21153;&#30340;&#34892;&#20026;&#34920;&#29616;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#30456;&#20284;&#65292;&#36825;&#20026;&#35774;&#35745;&#20855;&#26377;&#20154;&#31867;&#32423;&#35748;&#30693;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25552;&#20379;&#20102;&#20851;&#38190;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20316;&#35760;&#24518;&#26159;&#20154;&#31867;&#26234;&#33021;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#23427;&#20316;&#20026;&#20449;&#24687;&#20020;&#26102;&#23384;&#20648;&#21644;&#25805;&#20316;&#30340;&#24037;&#20316;&#31354;&#38388;&#12290;&#26412;&#25991;&#36890;&#36807;&#26816;&#26597;ChatGPT&#22312;N-back&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#35843;&#26597;&#20102;&#36825;&#19968;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#24037;&#20316;&#35760;&#24518;&#23545;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#24615;&#65292;&#25509;&#30528;&#20171;&#32461;&#20102;&#35780;&#20272;ChatGPT&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#27604;&#36739;&#20102;ChatGPT&#22312;&#35328;&#35821;&#21644;&#31354;&#38388;N- back&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#34920;&#29616;&#19982;&#25991;&#29486;&#25253;&#36947;&#30340;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#35774;&#35745;&#20855;&#26377;&#20154;&#31867;&#32423;&#35748;&#30693;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#24403;&#21069;&#36827;&#23637;&#25552;&#20379;&#20102;&#20851;&#38190;&#27934;&#23519;&#65292;&#24182;&#20026;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29702;&#35299;&#20154;&#31867;&#24037;&#20316;&#35760;&#24518;&#30340;&#26410;&#26469;&#21162;&#21147;&#25552;&#20379;&#20102;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Working memory is a critical aspect of both human intelligence and artificial intelligence (AI), serving as a workspace for the temporary storage and manipulation of information. This paper investigates working memory capacity of ChatGPT, a state-of-the-art language model, by examining its performance on N-back tasks. We begin by discussing the importance of working memory to humans and AI, followed by the methods employed to assess working memory capacity of ChatGPT. Our study compares behavioral performance of ChatGPT on verbal and spatial N-back tasks to that of human participants reported in the literature, revealing notable similarities. Our findings offer crucial insights into the current progress in designing AI systems with human-level cognitive abilities and hold promise for informing future endeavors aimed at enhancing AI working memory and understanding human working memory through AI models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#22810;&#27169;&#24577;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;Factify 2&#65292;&#20854;&#25903;&#25345;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#34164;&#21547;&#20851;&#31995;&#12290;&#35813;&#25968;&#25454;&#38598;&#20197;&#25903;&#25345;&#12289;&#26080;&#35777;&#25454;&#21644;&#39539;&#26021;&#19977;&#20010;&#31867;&#21035;&#20026;&#20027;&#65292;&#21253;&#21547;50,000&#20010;&#26032;&#30340;&#25968;&#25454;&#23454;&#20363;&#65292;&#24182;&#25552;&#20379;&#19968;&#31181;&#22522;&#20110;BERT&#21644;Vision Transformer&#30340;&#26368;&#26032;&#20107;&#23454;&#26680;&#26597;&#27169;&#22411;&#65292;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.03897</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#21644;&#35773;&#21050;&#26032;&#38395;&#25968;&#25454;&#38598;Factify 2
&lt;/p&gt;
&lt;p&gt;
Factify 2: A Multimodal Fake News and Satire News Dataset. (arXiv:2304.03897v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#22810;&#27169;&#24577;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;Factify 2&#65292;&#20854;&#25903;&#25345;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#34164;&#21547;&#20851;&#31995;&#12290;&#35813;&#25968;&#25454;&#38598;&#20197;&#25903;&#25345;&#12289;&#26080;&#35777;&#25454;&#21644;&#39539;&#26021;&#19977;&#20010;&#31867;&#21035;&#20026;&#20027;&#65292;&#21253;&#21547;50,000&#20010;&#26032;&#30340;&#25968;&#25454;&#23454;&#20363;&#65292;&#24182;&#25552;&#20379;&#19968;&#31181;&#22522;&#20110;BERT&#21644;Vision Transformer&#30340;&#26368;&#26032;&#20107;&#23454;&#26680;&#26597;&#27169;&#22411;&#65292;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#20026;&#20840;&#29699;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#25918;&#30340;&#24179;&#21488;&#65292;&#35753;&#20154;&#20204;&#34920;&#36798;&#33258;&#24049;&#30340;&#35266;&#28857;&#24182;&#20998;&#20139;&#33258;&#24049;&#30340;&#25925;&#20107;&#12290;&#34429;&#28982;&#36825;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#20294;&#23427;&#20063;&#20351;&#24471;&#34394;&#20551;&#26032;&#38395;&#25104;&#20026;&#25105;&#20204;&#31038;&#20250;&#26368;&#32039;&#36843;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#25163;&#21160;&#30340;&#20107;&#23454;&#26680;&#23545;&#36807;&#31243;&#38750;&#24120;&#32791;&#26102;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#24456;&#38590;&#22312;&#35823;&#23548;&#24615;&#35328;&#35770;&#36896;&#25104;&#37325;&#22823;&#20260;&#23475;&#20043;&#21069;&#39539;&#26021;&#23427;&#20204;&#12290;&#36825;&#23601;&#26159;&#33258;&#21160;&#20107;&#23454;&#25110;&#22768;&#26126;&#39564;&#35777;&#21463;&#21040;&#20851;&#27880;&#30340;&#21407;&#22240;&#12290;&#19968;&#20123;&#29616;&#26377;&#25968;&#25454;&#38598;&#26088;&#22312;&#25903;&#25345;&#33258;&#21160;&#21270;&#20107;&#23454;&#26680;&#26597;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#37117;&#26159;&#22522;&#20110;&#25991;&#26412;&#30340;&#12290;&#22810;&#27169;&#24577;&#20107;&#23454;&#39564;&#35777;&#19968;&#30452;&#21463;&#21040;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;FACTIFY 2&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#25968;&#25454;&#26469;&#28304;&#21644;&#28155;&#21152;&#35773;&#21050;&#25991;&#31456;&#26469;&#25913;&#36827;Factify 1&#12290;Factify 2&#26377;50,000&#20010;&#26032;&#30340;&#25968;&#25454;&#23454;&#20363;&#12290;&#19982;FACTIFY 1.0&#31867;&#20284;&#65292;&#25105;&#20204;&#26377;&#19977;&#20010;&#24191;&#27867;&#30340;&#31867;&#21035;&#8212;&#8212;&#25903;&#25345;&#12289;&#26080;&#35777;&#25454;&#21644;&#39539;&#26021;&#65292;&#36825;&#20123;&#31867;&#21035;&#22522;&#20110;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#34164;&#21547;&#20851;&#31995;&#20855;&#26377;&#23376;&#31867;&#21035;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;BERT&#21644;Vision Transformer&#30340;FACTIFY 2&#20107;&#23454;&#26680;&#26597;&#27169;&#22411;&#65292;&#24182;&#34920;&#26126;&#20854;&#22312;Factify 1&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The internet gives the world an open platform to express their views and share their stories. While this is very valuable, it makes fake news one of our society's most pressing problems. Manual fact checking process is time consuming, which makes it challenging to disprove misleading assertions before they cause significant harm. This is he driving interest in automatic fact or claim verification. Some of the existing datasets aim to support development of automating fact-checking techniques, however, most of them are text based. Multi-modal fact verification has received relatively scant attention. In this paper, we provide a multi-modal fact-checking dataset called FACTIFY 2, improving Factify 1 by using new data sources and adding satire articles. Factify 2 has 50,000 new data instances. Similar to FACTIFY 1.0, we have three broad categories - support, no-evidence, and refute, with sub-categories based on the entailment of visual and textual data. We also provide a BERT and Vison Tr
&lt;/p&gt;</description></item><item><title>Memotion 3&#26159;&#19968;&#20010;&#21253;&#21547;10,000&#20010;&#24050;&#27880;&#37322;&#27169;&#22240;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#24341;&#20837;&#20102;&#21360;&#24230;-&#33521;&#35821;&#28151;&#21512;&#27169;&#22240;&#65292;&#20351;&#20854;&#25104;&#20026;&#35813;&#39046;&#22495;&#20869;&#39318;&#20010;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#12290;&#27492;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#24773;&#24863;&#21644;&#24773;&#32490;&#20998;&#26512;&#65292;&#24182;&#21487;&#29992;&#20110;&#23545;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#34394;&#20551;&#20449;&#24687;&#25110;&#20167;&#24680;&#20869;&#23481;&#36827;&#34892;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.09892</link><description>&lt;p&gt;
Memotion 3: &#20195;&#34920;&#21360;&#24230;-&#33521;&#35821;&#28151;&#21512;&#30721;&#30340;&#24773;&#24863;&#19982;&#24773;&#32490;&#20998;&#26512;&#30340;&#20114;&#32852;&#32593;&#27169;&#22240;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Memotion 3: Dataset on sentiment and emotion analysis of codemixed Hindi-English Memes. (arXiv:2303.09892v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09892
&lt;/p&gt;
&lt;p&gt;
Memotion 3&#26159;&#19968;&#20010;&#21253;&#21547;10,000&#20010;&#24050;&#27880;&#37322;&#27169;&#22240;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#24341;&#20837;&#20102;&#21360;&#24230;-&#33521;&#35821;&#28151;&#21512;&#27169;&#22240;&#65292;&#20351;&#20854;&#25104;&#20026;&#35813;&#39046;&#22495;&#20869;&#39318;&#20010;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#12290;&#27492;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#24773;&#24863;&#21644;&#24773;&#32490;&#20998;&#26512;&#65292;&#24182;&#21487;&#29992;&#20110;&#23545;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#34394;&#20551;&#20449;&#24687;&#25110;&#20167;&#24680;&#20869;&#23481;&#36827;&#34892;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22240;&#26159;&#29616;&#20170;&#31038;&#20132;&#23186;&#20307;&#19978;&#20256;&#36798;&#24189;&#40664;&#30340;&#26032;&#22411;&#26426;&#21046;&#12290;&#27169;&#22240;&#36890;&#24120;&#21253;&#21547;&#22270;&#29255;&#21644;&#19968;&#20123;&#25991;&#26412;&#12290;&#27169;&#22240;&#21487;&#34987;&#29992;&#20110;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#25110;&#20167;&#24680;&#65292;&#22240;&#27492;&#23545;&#20854;&#36827;&#34892;&#35814;&#32454;&#30340;&#30740;&#31350;&#38750;&#24120;&#20851;&#38190;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Memotion 3&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;10,000&#20010;&#24050;&#27880;&#37322;&#27169;&#22240;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#19982;&#39046;&#22495;&#20869;&#20854;&#20182;&#26222;&#36941;&#30340;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#21253;&#25324;&#20043;&#21069;&#30340;Memotion&#65292;Memotion 3&#24341;&#20837;&#20102;&#21360;&#24230;-&#33521;&#35821;&#28151;&#21512;&#27169;&#22240;&#65292;&#32780;&#20043;&#21069;&#30340;&#30740;&#31350;&#20165;&#38480;&#20110;&#33521;&#35821;&#27169;&#22240;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;Memotion&#20219;&#21153;&#12289;&#25968;&#25454;&#25910;&#38598;&#21644;&#25968;&#25454;&#38598;&#21019;&#24314;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#20026;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#12290;&#22522;&#20934;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#23558;&#22312; https://github.com/Shreyashm16/Memotion-3.0 &#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memes are the new-age conveyance mechanism for humor on social media sites. Memes often include an image and some text. Memes can be used to promote disinformation or hatred, thus it is crucial to investigate in details. We introduce Memotion 3, a new dataset with 10,000 annotated memes. Unlike other prevalent datasets in the domain, including prior iterations of Memotion, Memotion 3 introduces Hindi-English Codemixed memes while prior works in the area were limited to only the English memes. We describe the Memotion task, the data collection and the dataset creation methodologies. We also provide a baseline for the task. The baseline code and dataset will be made available at https://github.com/Shreyashm16/Memotion-3.0
&lt;/p&gt;</description></item><item><title>IFAN&#26159;&#19968;&#20010;&#38754;&#21521;&#20154;&#31867;&#21644;NLP&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#20132;&#20114;&#26694;&#26550;&#65292;&#36890;&#36807;&#29992;&#25143;&#30340;&#23454;&#26102;&#21453;&#39304;&#21644;&#36866;&#37197;&#22120;&#23618;&#30340;&#23545;&#40784;&#65292;&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#20559;&#35265;&#30340;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2303.03124</link><description>&lt;p&gt;
IFAN&#65306;&#38754;&#21521;&#20154;&#31867;&#21644;NLP&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#20132;&#20114;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
IFAN: An Explainability-Focused Interaction Framework for Humans and NLP Models. (arXiv:2303.03124v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03124
&lt;/p&gt;
&lt;p&gt;
IFAN&#26159;&#19968;&#20010;&#38754;&#21521;&#20154;&#31867;&#21644;NLP&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#20132;&#20114;&#26694;&#26550;&#65292;&#36890;&#36807;&#29992;&#25143;&#30340;&#23454;&#26102;&#21453;&#39304;&#21644;&#36866;&#37197;&#22120;&#23618;&#30340;&#23545;&#40784;&#65292;&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#20559;&#35265;&#30340;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#21644;&#20154;&#31867;&#30417;&#30563;&#26159;&#23558;&#22797;&#26434;NLP&#27169;&#22411;&#24212;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#30340;&#22522;&#26412;&#25903;&#26609;&#12290;&#28982;&#32780;&#65292;&#24212;&#29992;&#35299;&#37322;&#24615;&#21644;&#20154;&#26426;&#20132;&#20114;&#26041;&#27861;&#38656;&#35201;&#25216;&#26415;&#29087;&#32451;&#12290;&#23613;&#31649;&#23384;&#22312;&#29992;&#20110;&#27169;&#22411;&#29702;&#35299;&#21644;&#20998;&#26512;&#30340;&#24037;&#20855;&#21253;&#65292;&#20294;&#38598;&#25104;&#20154;&#31867;&#21453;&#39304;&#30340;&#36873;&#39033;&#20173;&#28982;&#26377;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;IFAN&#65292;&#19968;&#31181;&#29992;&#20110;&#19982;NLP&#27169;&#22411;&#36827;&#34892;&#23454;&#26102;&#22522;&#20110;&#35299;&#37322;&#30340;&#20132;&#20114;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;IFAN&#30340;&#30028;&#38754;&#65292;&#29992;&#25143;&#21487;&#20197;&#23545;&#36873;&#25321;&#30340;&#27169;&#22411;&#35299;&#37322;&#25552;&#20379;&#21453;&#39304;&#65292;&#28982;&#21518;&#36890;&#36807;&#36866;&#37197;&#22120;&#23618;&#23558;&#20854;&#19982;&#20154;&#31867;&#30340;&#29702;&#24615;&#36827;&#34892;&#23545;&#40784;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31995;&#32479;&#22312;&#26368;&#23567;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20943;&#36731;&#20559;&#35265;&#30340;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#21313;&#20998;&#26377;&#25928;&#12290;IFAN&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35270;&#21270;&#30340;&#31649;&#29702;&#31995;&#32479;&#21644;API&#65292;&#29992;&#20110;&#31649;&#29702;&#27169;&#22411;&#65288;&#21644;&#25968;&#25454;&#38598;&#65289;&#20197;&#21450;&#25511;&#21046;&#35775;&#38382;&#26435;&#38480;&#12290;&#28436;&#31034;&#22320;&#22336;&#65306;https://ifan.ml&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability and human oversight are fundamental pillars of deploying complex NLP models into real-world applications. However, applying explainability and human-in-the-loop methods requires technical proficiency. Despite existing toolkits for model understanding and analysis, options to integrate human feedback are still limited. We propose IFAN, a framework for real-time explanation-based interaction with NLP models. Through IFAN's interface, users can provide feedback to selected model explanations, which is then integrated through adapter layers to align the model with human rationale. We show the system to be effective in debiasing a hate speech classifier with minimal impact on performance. IFAN also offers a visual admin system and API to manage models (and datasets) as well as control access rights. A demo is live at https://ifan.ml.
&lt;/p&gt;</description></item><item><title>EvoPrompting&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#36866;&#24212;&#21464;&#24322;&#21644;&#20132;&#21449;&#25805;&#20316;&#31526;&#26469;&#36827;&#34892;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#22312;MNIST-1D&#25968;&#25454;&#38598;&#21644;CLRS&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#19978;&#37117;&#21462;&#24471;&#20102;&#27604;&#20154;&#31867;&#35774;&#35745;&#30340;&#26550;&#26500;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.14838</link><description>&lt;p&gt;
EvoPrompting: &#36866;&#29992;&#20110;&#20195;&#30721;&#32423;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EvoPrompting: Language Models for Code-Level Neural Architecture Search. (arXiv:2302.14838v1 [cs.NE] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14838
&lt;/p&gt;
&lt;p&gt;
EvoPrompting&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#36866;&#24212;&#21464;&#24322;&#21644;&#20132;&#21449;&#25805;&#20316;&#31526;&#26469;&#36827;&#34892;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#22312;MNIST-1D&#25968;&#25454;&#38598;&#21644;CLRS&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#19978;&#37117;&#21462;&#24471;&#20102;&#27604;&#20154;&#31867;&#35774;&#35745;&#30340;&#26550;&#26500;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#26368;&#26032;&#25104;&#23601;&#65292;&#25105;&#20204;&#25506;&#32034;&#23558;LM&#20316;&#20026;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#31639;&#27861;&#30340;&#33258;&#36866;&#24212;&#21464;&#24322;&#21644;&#20132;&#21449;&#25805;&#20316;&#31526;&#30340;&#20351;&#29992;&#12290;&#23613;&#31649;NAS&#20173;&#28982;&#36807;&#20110;&#22256;&#38590;&#65292;&#20197;&#33267;&#20110;&#20165;&#20165;&#36890;&#36807;&#25552;&#31034;&#23601;&#38590;&#20197;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36827;&#21270;&#25552;&#31034;&#24037;&#31243;&#19982;&#36719;&#25552;&#31034;&#35843;&#25972;&#30340;&#32452;&#21512;&#65292;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;EvoPrompting&#30340;&#26041;&#27861;&#65292;&#22987;&#32456;&#21487;&#20197;&#21457;&#29616;&#22810;&#26679;&#21270;&#19988;&#24615;&#33021;&#39640;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;EvoPrompting&#22312;MNIST-1D&#25968;&#25454;&#38598;&#19978;&#26159;&#26377;&#25928;&#30340;&#65292;&#20854;&#20013;EvoPrompting&#20135;&#29983;&#30340;&#21367;&#31215;&#26550;&#26500;&#21464;&#20307;&#22312;&#20934;&#30830;&#29575;&#21644;&#27169;&#22411;&#22823;&#23567;&#26041;&#38754;&#22343;&#20248;&#20110;&#20154;&#31867;&#19987;&#23478;&#35774;&#35745;&#30340;&#26550;&#26500;&#21644;&#22825;&#30495;&#30340;&#23569;&#25968;&#20808;&#23548;&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#22312;CLRS&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#19978;&#25628;&#32034;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20013;EvoPrompting&#33021;&#22815;&#35774;&#35745;&#20986;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26356;&#22909;&#30340;&#26032;&#39062;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the recent impressive accomplishments of language models (LMs) for code generation, we explore the use of LMs as adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm. While NAS still proves too difficult a task for LMs to succeed at solely through prompting, we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse and high performing models. We first demonstrate that EvoPrompting is effective on the computationally efficient MNIST-1D dataset, where EvoPrompting produces convolutional architecture variants that outperform both those designed by human experts and naive few-shot prompting in terms of accuracy and model size. We then apply our method to searching for graph neural networks on the CLRS Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel architectures that outperform current state-of-the-art models on 21 ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#24494;&#35843;&#19981;&#31283;&#23450;&#24615;&#30340;&#19971;&#20010;&#25351;&#26631;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#37325;&#26032;&#35780;&#20272;&#20102;&#20943;&#36731;&#19981;&#31283;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#24076;&#26395;&#20026;&#25913;&#36827;&#24494;&#35843;&#19981;&#31283;&#23450;&#24615;&#30340;&#27979;&#37327;&#26041;&#27861;&#25552;&#20379;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2302.07778</link><description>&lt;p&gt;
&#27979;&#37327;&#24494;&#35843;&#19981;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Measuring the Instability of Fine-Tuning. (arXiv:2302.07778v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#24494;&#35843;&#19981;&#31283;&#23450;&#24615;&#30340;&#19971;&#20010;&#25351;&#26631;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#37325;&#26032;&#35780;&#20272;&#20102;&#20943;&#36731;&#19981;&#31283;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#24076;&#26395;&#20026;&#25913;&#36827;&#24494;&#35843;&#19981;&#31283;&#23450;&#24615;&#30340;&#27979;&#37327;&#26041;&#27861;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#38543;&#26426;&#31181;&#23376;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#24050;&#34987;&#35777;&#26126;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35843;&#26597;&#20102;&#36825;&#31181;&#19981;&#31283;&#23450;&#24615;&#24182;&#25552;&#20986;&#20102;&#20943;&#36731;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#21482;&#20351;&#29992;&#24615;&#33021;&#24471;&#20998;&#30340;&#26631;&#20934;&#24046;&#65288;SD&#65289;&#20316;&#20026;&#20854;&#34913;&#37327;&#25351;&#26631;&#65292;&#36825;&#26159;&#23545;&#19981;&#31283;&#23450;&#24615;&#30340;&#29421;&#20041;&#21051;&#30011;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;SD&#21644;&#20854;&#20182;&#20845;&#20010;&#19981;&#21516;&#31890;&#24230;&#30340;&#34913;&#37327;&#19981;&#31283;&#23450;&#24615;&#30340;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#26694;&#26550;&#26469;&#35780;&#20272;&#36825;&#20123;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#37325;&#26032;&#35780;&#20272;&#29616;&#26377;&#30340;&#20943;&#36731;&#19981;&#31283;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19981;&#21516;&#25351;&#26631;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#21644;&#24046;&#24322;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#32467;&#26524;&#21487;&#20197;&#20026;&#25913;&#36827;&#24494;&#35843;&#19981;&#31283;&#23450;&#24615;&#30340;&#27979;&#37327;&#26041;&#27861;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning pre-trained language models on downstream tasks with varying random seeds has been shown to be unstable, especially on small datasets. Many previous studies have investigated this instability and proposed methods to mitigate it. However, most studies only used the standard deviation of performance scores (SD) as their measure, which is a narrow characterization of instability. In this paper, we analyze SD and six other measures quantifying instability at different levels of granularity. Moreover, we propose a systematic framework to evaluate the validity of these measures. Finally, we analyze the consistency and difference between different measures by reassessing existing instability mitigation methods. We hope our results will inform the development of better measurements of fine-tuning instability.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20851;&#27880;&#22270;&#35299;&#26512;Transformer&#20013;&#30340;&#21069;&#39304;&#27169;&#22359;&#65292;&#25581;&#31034;&#20102;&#20854;&#20462;&#25913;&#36755;&#20837;&#35821;&#22659;&#21270;&#20197;&#24378;&#35843;&#29305;&#23450;&#31867;&#22411;&#35821;&#35328;&#32452;&#21512;&#30340;&#20316;&#29992;&#65292;&#24182;&#26263;&#31034;&#20102;Transformer&#23618;&#22788;&#29702;&#20013;&#30340;&#28508;&#22312;&#20887;&#20313;&#12290;</title><link>http://arxiv.org/abs/2302.00456</link><description>&lt;p&gt;
&#36890;&#36807;&#20851;&#27880;&#22270;&#35299;&#26512;Transformer&#20013;&#30340;&#21069;&#39304;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Analyzing Feed-Forward Blocks in Transformers through the Lens of Attention Map. (arXiv:2302.00456v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00456
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20851;&#27880;&#22270;&#35299;&#26512;Transformer&#20013;&#30340;&#21069;&#39304;&#27169;&#22359;&#65292;&#25581;&#31034;&#20102;&#20854;&#20462;&#25913;&#36755;&#20837;&#35821;&#22659;&#21270;&#20197;&#24378;&#35843;&#29305;&#23450;&#31867;&#22411;&#35821;&#35328;&#32452;&#21512;&#30340;&#20316;&#29992;&#65292;&#24182;&#26263;&#31034;&#20102;Transformer&#23618;&#22788;&#29702;&#20013;&#30340;&#28508;&#22312;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;Transformer&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#20013;&#26080;&#22788;&#19981;&#22312;&#65292;&#35299;&#37322;&#23427;&#20204;&#30340;&#20869;&#37096;&#26426;&#21046;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#29305;&#23450;&#32452;&#20214;&#65292;&#21069;&#39304;(FF)&#27169;&#22359;&#65292;&#23613;&#31649;&#23427;&#20204;&#26377;&#22823;&#37327;&#30340;&#21442;&#25968;&#65292;&#20294;&#36890;&#24120;&#34987;&#20998;&#26512;&#24471;&#36739;&#23569;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;FF&#27169;&#22359;&#22312;&#20851;&#27880;&#22270;&#20013;&#28210;&#26579;&#20986;&#26469;&#20316;&#20026;&#19968;&#31181;&#26131;&#20110;&#29702;&#35299;&#30340;&#21487;&#35270;&#21270;&#26041;&#26696;&#65292;&#26469;&#20998;&#26512;FF&#27169;&#22359;&#30340;&#36755;&#20837;&#35821;&#22659;&#25928;&#26524;&#12290;&#25105;&#20204;&#23545;&#26377;&#23631;&#34109;&#21644;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FF&#32593;&#32476;&#20462;&#25913;&#20102;&#36755;&#20837;&#30340;&#35821;&#22659;&#21270;&#20197;&#24378;&#35843;&#29305;&#23450;&#31867;&#22411;&#30340;&#35821;&#35328;&#32452;&#21512;&#12290;&#27492;&#22806;&#65292;FF&#27169;&#22359;&#21450;&#20854;&#21608;&#22260;&#30340;&#32452;&#20214;&#24448;&#24448;&#20250;&#20114;&#30456;&#25269;&#28040;&#25928;&#26524;&#65292;&#34920;&#26126;Transformer&#23618;&#30340;&#22788;&#29702;&#20013;&#21487;&#33021;&#23384;&#22312;&#28508;&#22312;&#30340;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given that Transformers are ubiquitous in wide tasks, interpreting their internals is a pivotal issue. Still, their particular components, feed-forward (FF) blocks, have typically been less analyzed despite their substantial parameter amounts. We analyze the input contextualization effects of FF blocks by rendering them in the attention maps as a human-friendly visualization scheme. Our experiments with both masked- and causal-language models reveal that FF networks modify the input contextualization to emphasize specific types of linguistic compositions. In addition, FF and its surrounding components tend to cancel out each other's effects, suggesting potential redundancy in the processing of the Transformer layer.
&lt;/p&gt;</description></item><item><title>MolGen&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#21069;&#32512;&#35843;&#25972;&#21644;&#33258;&#25105;&#21453;&#39304;&#30340;&#33539;&#24335;&#65292;&#23454;&#29616;&#20102;&#21270;&#23398;&#26377;&#25928;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#26032;&#39062;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#31361;&#30772;&#65292;&#22312;&#20998;&#23376;&#29983;&#25104;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11259</link><description>&lt;p&gt;
&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#29983;&#25104;&#19982;&#33258;&#25105;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Domain-Agnostic Molecular Generation with Self-feedback. (arXiv:2301.11259v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11259
&lt;/p&gt;
&lt;p&gt;
MolGen&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#21069;&#32512;&#35843;&#25972;&#21644;&#33258;&#25105;&#21453;&#39304;&#30340;&#33539;&#24335;&#65292;&#23454;&#29616;&#20102;&#21270;&#23398;&#26377;&#25928;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#26032;&#39062;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#31361;&#30772;&#65292;&#22312;&#20998;&#23376;&#29983;&#25104;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#30340;&#29983;&#25104;&#24050;&#32463;&#21463;&#21040;&#26497;&#22823;&#30340;&#20851;&#27880;&#65292;&#20854;&#38761;&#26032;&#20102;&#31185;&#23398;&#23478;&#35774;&#35745;&#20998;&#23376;&#32467;&#26500;&#30340;&#26041;&#24335;&#65292;&#24182;&#20026;&#21270;&#23398;&#21644;&#33647;&#29289;&#35774;&#35745;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#20998;&#23376;&#29983;&#25104;&#20013;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#27604;&#22914;&#29983;&#25104;&#35821;&#27861;&#25110;&#21270;&#23398;&#23384;&#22312;&#32570;&#38519;&#30340;&#20998;&#23376;&#65292;&#29421;&#31364;&#30340;&#39046;&#22495;&#19987;&#27880;&#20197;&#21450;&#30001;&#20110;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#25110;&#22806;&#37096;&#20998;&#23376;&#25968;&#25454;&#24211;&#32780;&#38480;&#21046;&#20102;&#29983;&#25104;&#22810;&#26679;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MolGen&#65292;&#23427;&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#20998;&#23376;&#35821;&#35328;&#27169;&#22411;&#12290;MolGen&#36890;&#36807;&#37325;&#26500;&#19968;&#20159;&#22810;&#20010;&#20998;&#23376;SELFIES&#33719;&#24471;&#20102;&#22266;&#26377;&#30340;&#32467;&#26500;&#21644;&#35821;&#27861;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#21069;&#32512;&#35843;&#25972;&#20419;&#36827;&#20102;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#21453;&#39304;&#33539;&#24335;&#65292;&#21551;&#21457;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#26368;&#32456;&#19979;&#28216;&#30446;&#26631;&#23545;&#40784;&#65292;&#26377;&#21161;&#20110;&#26356;&#31283;&#20581;&#21644;&#39640;&#25928;&#30340;&#20998;&#23376;&#29983;&#25104;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MolGen&#22312;&#21270;&#23398;&#26377;&#25928;&#24615;&#65292;&#22810;&#26679;&#24615;&#65292;&#26032;&#39062;&#24615;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of molecules with desired properties has gained tremendous popularity, revolutionizing the way scientists design molecular structures and providing valuable support for chemical and drug design. However, despite the potential of language models in molecule generation, they face numerous challenges such as the generation of syntactically or chemically flawed molecules, narrow domain focus, and limitations in creating diverse and directionally feasible molecules due to a dearth of annotated data or external molecular databases. To this end, we introduce MolGen, a pre-trained molecular language model tailored specifically for molecule generation. MolGen acquires intrinsic structural and grammatical insights by reconstructing over 100 million molecular SELFIES, while facilitating knowledge transfer between different domains through domain-agnostic molecular prefix tuning. Moreover, we present a self-feedback paradigm that inspires the pre-trained model to align with the ulti
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#20307;&#35780;&#20272;&#65288;HELM&#65289;&#65292;&#36890;&#36807;&#23545;&#28508;&#22312;&#22330;&#26223;&#21644;&#24230;&#37327;&#36827;&#34892;&#20998;&#31867;&#24182;&#37319;&#29992;&#22810;&#24230;&#37327;&#26041;&#27861;&#65292;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.09110</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#20307;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Holistic Evaluation of Language Models. (arXiv:2211.09110v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09110
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#20307;&#35780;&#20272;&#65288;HELM&#65289;&#65292;&#36890;&#36807;&#23545;&#28508;&#22312;&#22330;&#26223;&#21644;&#24230;&#37327;&#36827;&#34892;&#20998;&#31867;&#24182;&#37319;&#29992;&#22810;&#24230;&#37327;&#26041;&#27861;&#65292;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#27491;&#22312;&#25104;&#20026;&#20960;&#20046;&#25152;&#26377;&#20027;&#35201;&#35821;&#35328;&#25216;&#26415;&#30340;&#22522;&#30784;&#65292;&#20294;&#23427;&#20204;&#30340;&#33021;&#21147;&#12289;&#38480;&#21046;&#21644;&#39118;&#38505;&#24182;&#19981;&#34987;&#24456;&#22909;&#22320;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#20307;&#35780;&#20272;&#65288;HELM&#65289;&#65292;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#24863;&#20852;&#36259;&#30340;&#28508;&#22312;&#22330;&#26223;&#65288;&#21363;&#29992;&#20363;&#65289;&#21644;&#24230;&#37327;&#65288;&#21363;&#26399;&#26395;&#65289;&#30340;&#24191;&#38420;&#31354;&#38388;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#19968;&#20010;&#23485;&#27867;&#30340;&#23376;&#38598;&#65292;&#22522;&#20110;&#35206;&#30422;&#33539;&#22260;&#21644;&#21487;&#34892;&#24615;&#65292;&#27880;&#24847;&#21040;&#20102;&#32570;&#22833;&#25110;&#26410;&#20805;&#20998;&#20195;&#34920;&#30340;&#20869;&#23481;&#65288;&#20363;&#22914;&#65292;&#20026;&#34987;&#24573;&#35270;&#30340;&#33521;&#35821;&#26041;&#35328;&#36827;&#34892;&#38382;&#31572;&#65292;&#29992;&#20110;&#21487;&#20449;&#24230;&#30340;&#24230;&#37327;&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#37319;&#29992;&#22810;&#24230;&#37327;&#26041;&#27861;&#65306;&#25105;&#20204;&#20998;&#21035;&#38024;&#23545;&#27599;&#20010;&#26680;&#24515;&#22330;&#26223;&#27979;&#37327;&#20102;&#20934;&#30830;&#24230;&#12289;&#26657;&#20934;&#24230;&#12289;&#40065;&#26834;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#20559;&#35265;&#12289;&#26377;&#27602;&#24615;&#21644;&#25928;&#29575;&#36825;7&#20010;&#24230;&#37327;&#25351;&#26631;&#65288;&#22312;87.5%&#30340;&#26102;&#38388;&#20869;&#65289;&#12290;&#36825;&#30830;&#20445;&#20102;&#20934;&#30830;&#24230;&#20197;&#22806;&#30340;&#24230;&#37327;&#19981;&#20250;&#34987;&#24573;&#35270;&#65292;&#24182;&#19988;&#26435;&#34913;&#28165;&#26224;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;7&#20010;&#38024;&#23545;&#24615;&#35780;&#20272;&#65292;&#22522;&#20110;26&#20010;&#38024;&#23545;&#24615;&#22330;&#26223;&#65292;&#20197;&#20998;&#26512;&#29305;&#23450;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze speci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;BITTERS&#30340;&#39640;&#25928;&#35757;&#32451;&#21644;&#25512;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#21452;&#21521;&#35757;&#32451;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#21644;&#24494;&#35843;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#38477;&#20302;&#31038;&#20250;&#20559;&#24046;&#12290;&#22312;&#23454;&#29616;&#38646;&#26679;&#26412;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#38754;&#65292;&#31934;&#36873;&#35757;&#32451;&#38598;&#21644;&#27169;&#22411;&#26550;&#26500;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2211.06774</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#21452;&#21521;&#35757;&#32451;&#29992;&#20110;&#38646;&#26679;&#26412;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Bidirectional Training for Zero-Shot Image Captioning. (arXiv:2211.06774v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;BITTERS&#30340;&#39640;&#25928;&#35757;&#32451;&#21644;&#25512;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#21452;&#21521;&#35757;&#32451;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#21644;&#24494;&#35843;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#38477;&#20302;&#31038;&#20250;&#20559;&#24046;&#12290;&#22312;&#23454;&#29616;&#38646;&#26679;&#26412;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#38754;&#65292;&#31934;&#36873;&#35757;&#32451;&#38598;&#21644;&#27169;&#22411;&#26550;&#26500;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#29702;&#35299;&#36890;&#29992;&#39046;&#22495;&#20869;&#22270;&#20687;&#30340;&#20869;&#23481;&#65292;&#20294;&#24448;&#24448;&#26080;&#27861;&#29983;&#25104;&#20934;&#30830;&#12289;&#35814;&#32454;&#30340;&#23383;&#24149;&#12290;&#20026;&#20102;&#25552;&#39640;&#24615;&#33021;&#65292;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#19968;&#30452;&#26159;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#20851;&#38190;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#35268;&#27169;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#21452;&#21521;&#35757;&#32451;&#20351;&#24471;&#38646;&#26679;&#26412;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#25104;&#20026;&#21487;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;BITTERS&#65288;Bidirectional Image Text Training in largER Scale&#65289;&#30340;&#39640;&#25928;&#35757;&#32451;&#21644;&#25512;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#21644;&#24191;&#27867;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#27491;&#30830;&#35780;&#20272;&#38646;&#26679;&#26412;&#23383;&#24149;&#30340;&#20934;&#30830;&#24615;&#21644;&#31038;&#20250;&#20559;&#24046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#26469;&#25552;&#21462;&#20851;&#38190;&#35789;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31934;&#36873;&#22823;&#35268;&#27169;&#35757;&#32451;&#38598;&#21644;&#27169;&#22411;&#26550;&#26500;&#26159;&#23454;&#29616;&#38646;&#26679;&#26412;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
When trained on large-scale datasets, image captioning models can understand the content of images from a general domain but often fail to generate accurate, detailed captions. To improve performance, pretraining-and-finetuning has been a key strategy for image captioning. However, we find that large-scale bidirectional training between image and text enables zero-shot image captioning. In this paper, we introduce Bidirectional Image Text Training in largER Scale, BITTERS, an efficient training and inference framework for zero-shot image captioning. We also propose a new evaluation benchmark which comprises of high quality datasets and an extensive set of metrics to properly evaluate zero-shot captioning accuracy and societal bias. We additionally provide an efficient finetuning approach for keyword extraction. We show that careful selection of large-scale training set and model architecture is the key to achieving zero-shot image captioning.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;Transformer&#35757;&#32451;&#30340;&#22256;&#38590;&#12290;&#20182;&#20204;&#21457;&#29616;&#19981;&#24179;&#34913;&#30340;&#26799;&#24230;&#19981;&#26159;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#32780;&#26159;&#27599;&#19968;&#23618;&#30340;&#25918;&#22823;&#25928;&#24212;&#23548;&#33268;&#35757;&#32451;&#19981;&#31283;&#23450;&#12290;&#20182;&#20204;&#35266;&#23519;&#21040;&#36731;&#37327;&#32423;&#30340;&#20381;&#36182;&#38480;&#21046;&#20102;&#27169;&#22411;&#28508;&#21147;&#65292;&#23548;&#33268;&#34920;&#29616;&#36739;&#24046;&#30340;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2004.08249</link><description>&lt;p&gt;
&#29702;&#35299;Transformer&#35757;&#32451;&#30340;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Understanding the Difficulty of Training Transformers. (arXiv:2004.08249v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2004.08249
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;Transformer&#35757;&#32451;&#30340;&#22256;&#38590;&#12290;&#20182;&#20204;&#21457;&#29616;&#19981;&#24179;&#34913;&#30340;&#26799;&#24230;&#19981;&#26159;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#32780;&#26159;&#27599;&#19968;&#23618;&#30340;&#25918;&#22823;&#25928;&#24212;&#23548;&#33268;&#35757;&#32451;&#19981;&#31283;&#23450;&#12290;&#20182;&#20204;&#35266;&#23519;&#21040;&#36731;&#37327;&#32423;&#30340;&#20381;&#36182;&#38480;&#21046;&#20102;&#27169;&#22411;&#28508;&#21147;&#65292;&#23548;&#33268;&#34920;&#29616;&#36739;&#24046;&#30340;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#38656;&#35201;&#35774;&#35745;&#20808;&#36827;&#30340;&#20248;&#21270;&#22120;&#21644;&#23398;&#20064;&#29575;&#35843;&#24230;&#22120;&#30340;&#38750;&#24179;&#20961;&#24037;&#20316;&#65288;&#20363;&#22914;&#65292;&#20256;&#32479;&#30340;SGD&#26080;&#27861;&#26377;&#25928;&#35757;&#32451;Transformer&#65289;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20174;&#32463;&#39564;&#21644;&#29702;&#35770;&#30340;&#35282;&#24230;&#29702;&#35299;$\textit{&#20160;&#20040;&#20351;&#24471;Transformer&#30340;&#35757;&#32451;&#21464;&#24471;&#22256;&#38590;}$&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#19981;&#24179;&#34913;&#30340;&#26799;&#24230;&#24182;&#19981;&#26159;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#31181;&#24433;&#21709;&#35757;&#32451;&#30340;&#25918;&#22823;&#25928;&#24212;--&#23545;&#20110;&#22810;&#23618;Transformer&#27169;&#22411;&#20013;&#30340;&#27599;&#19968;&#23618;&#65292;&#23427;&#23545;&#20854;&#27531;&#24046;&#20998;&#25903;&#30340;&#20381;&#36182;&#31243;&#24230;&#36739;&#39640;&#65292;&#23548;&#33268;&#35757;&#32451;&#19981;&#31283;&#23450;&#65292;&#22240;&#20026;&#23427;&#25918;&#22823;&#20102;&#23567;&#30340;&#21442;&#25968;&#25200;&#21160;&#65288;&#20363;&#22914;&#21442;&#25968;&#26356;&#26032;&#65289;&#65292;&#24182;&#23548;&#33268;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#26174;&#33879;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36731;&#37327;&#32423;&#30340;&#20381;&#36182;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#24182;&#23548;&#33268;&#34920;&#29616;&#36739;&#24046;&#30340;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Admin&#65288;$\textbf{Ad}$aptive &#37325;&#36848;&#37096;&#20998;
&lt;/p&gt;
&lt;p&gt;
Transformers have proved effective in many NLP tasks. However, their training requires non-trivial efforts regarding designing cutting-edge optimizers and learning rate schedulers carefully (e.g., conventional SGD fails to train Transformers effectively). Our objective here is to understand $\textit{what complicates Transformer training}$ from both empirical and theoretical perspectives. Our analysis reveals that unbalanced gradients are not the root cause of the instability of training. Instead, we identify an amplification effect that influences training substantially -- for each layer in a multi-layer Transformer model, heavy dependency on its residual branch makes training unstable, since it amplifies small parameter perturbations (e.g., parameter updates) and results in significant disturbances in the model output. Yet we observe that a light dependency limits the model potential and leads to inferior trained models. Inspired by our analysis, we propose Admin ($\textbf{Ad}$aptive 
&lt;/p&gt;</description></item></channel></rss>