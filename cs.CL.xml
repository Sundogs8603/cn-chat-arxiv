<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#19981;&#20165;&#20165;&#23616;&#38480;&#20110;&#8220;&#36234;&#29425;&#8221;&#65292;&#32780;&#21253;&#25324;&#36843;&#20351;&#27169;&#22411;&#23637;&#31034;&#21508;&#31181;&#24847;&#22806;&#34892;&#20026;&#65292;&#25915;&#20987;&#34920;&#38754;&#21644;&#30446;&#26631;&#24191;&#27867;&#12290;&#36825;&#20123;&#25915;&#20987;&#28304;&#20110;LLMs&#30340;&#39044;&#35757;&#32451;&#21644;&#24120;&#35265;&#35789;&#27719;&#20013;&#23384;&#22312;&#30340;&#8220;&#25925;&#38556;&#8221;&#26631;&#35760;&#12290;</title><link>https://arxiv.org/abs/2402.14020</link><description>&lt;p&gt;
&#36843;&#20351;LLMs&#25191;&#34892;&#24182;&#25581;&#31034;&#65288;&#20960;&#20046;&#65289;&#20219;&#20309;&#20107;&#24773;
&lt;/p&gt;
&lt;p&gt;
Coercing LLMs to do and reveal (almost) anything
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#19981;&#20165;&#20165;&#23616;&#38480;&#20110;&#8220;&#36234;&#29425;&#8221;&#65292;&#32780;&#21253;&#25324;&#36843;&#20351;&#27169;&#22411;&#23637;&#31034;&#21508;&#31181;&#24847;&#22806;&#34892;&#20026;&#65292;&#25915;&#20987;&#34920;&#38754;&#21644;&#30446;&#26631;&#24191;&#27867;&#12290;&#36825;&#20123;&#25915;&#20987;&#28304;&#20110;LLMs&#30340;&#39044;&#35757;&#32451;&#21644;&#24120;&#35265;&#35789;&#27719;&#20013;&#23384;&#22312;&#30340;&#8220;&#25925;&#38556;&#8221;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#21487;&#20197;&#8220;&#36234;&#29425;&#8221;&#35813;&#27169;&#22411;&#20197;&#21457;&#34920;&#26377;&#23475;&#35328;&#35770;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;LLMs&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#33539;&#22260;&#36828;&#19981;&#27490;&#20110;&#36234;&#29425;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#21487;&#33021;&#30340;&#25915;&#20987;&#38754;&#21644;&#25915;&#20987;&#30446;&#26631;&#30340;&#24191;&#27867;&#27010;&#36848;&#12290;&#26681;&#25454;&#19968;&#31995;&#21015;&#20855;&#20307;&#31034;&#20363;&#65292;&#25105;&#20204;&#35752;&#35770;&#12289;&#20998;&#31867;&#21644;&#31995;&#32479;&#21270;&#20102;&#19968;&#20123;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#36843;&#20351;LLMs&#23637;&#31034;&#21508;&#31181;&#24847;&#22806;&#34892;&#20026;&#65292;&#22914;&#35823;&#23548;&#12289;&#27169;&#22411;&#25511;&#21046;&#12289;&#25298;&#32477;&#26381;&#21153;&#25110;&#25968;&#25454;&#25552;&#21462;&#12290;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#23454;&#39564;&#20998;&#26512;&#36825;&#20123;&#25915;&#20987;&#65292;&#24182;&#21457;&#29616;&#20854;&#20013;&#35768;&#22810;&#26159;&#30001;&#20110;&#39044;&#35757;&#32451;LLMs&#20855;&#26377;&#32534;&#30721;&#33021;&#21147;&#30340;&#23454;&#36341;&#65292;&#20197;&#21450;&#24120;&#35265;LLMs&#35789;&#27719;&#20013;&#24212;&#21024;&#38500;&#30340;&#22855;&#24618;&#8220;&#25925;&#38556;&#8221;&#26631;&#35760;&#30340;&#25345;&#32493;&#23384;&#22312;&#25152;&#23548;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14020v1 Announce Type: cross  Abstract: It has recently been shown that adversarial attacks on large language models (LLMs) can "jailbreak" the model into making harmful statements. In this work, we argue that the spectrum of adversarial attacks on LLMs is much larger than merely jailbreaking. We provide a broad overview of possible attack surfaces and attack goals. Based on a series of concrete examples, we discuss, categorize and systematize attacks that coerce varied unintended behaviors, such as misdirection, model control, denial-of-service, or data extraction.   We analyze these attacks in controlled experiments, and find that many of them stem from the practice of pre-training LLMs with coding capabilities, as well as the continued existence of strange "glitch" tokens in common LLM vocabularies that should be removed for security reasons.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#30740;&#31350;&#20102;&#35780;&#20272;LLM&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#30701;&#36890;&#29992;&#30701;&#35821;&#21487;&#20197;&#27450;&#39575;LLMs&#25552;&#20379;&#39640;&#35780;&#20998;&#65292;&#36825;&#31181;&#25915;&#20987;&#23545;&#20110;&#20174;&#31616;&#21333;&#30340;&#20018;&#32852;&#25915;&#20987;&#21040;&#36716;&#31227;&#23398;&#20064;&#37117;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.14016</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#35780;&#21028;&#32773;&#26159;&#21542;&#31283;&#20581;&#65311;&#30740;&#31350;&#36890;&#29992;&#23545;&#25239;&#25915;&#20987;&#23545;&#38646;&#26679;&#28857;LLM&#35780;&#20272;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14016
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#30740;&#31350;&#20102;&#35780;&#20272;LLM&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#30701;&#36890;&#29992;&#30701;&#35821;&#21487;&#20197;&#27450;&#39575;LLMs&#25552;&#20379;&#39640;&#35780;&#20998;&#65292;&#36825;&#31181;&#25915;&#20987;&#23545;&#20110;&#20174;&#31616;&#21333;&#30340;&#20018;&#32852;&#25915;&#20987;&#21040;&#36716;&#31227;&#23398;&#20064;&#37117;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#24378;&#22823;&#30340;&#38646;&#26679;&#28857;&#35780;&#20272;&#32773;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#31508;&#35797;&#25110;&#31995;&#32479;&#22522;&#20934;&#27979;&#35797;&#31561;&#24773;&#22659;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#30740;&#31350;&#20998;&#26512;&#23545;&#25239;&#35797;&#22270;&#25805;&#32437;&#36755;&#20986;&#30340;&#35780;&#21028;LLMs&#30340;&#33030;&#24369;&#24615;&#30340;&#24037;&#20316;&#12290;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#23545;&#35780;&#20272;LLMs&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#31532;&#19968;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#23547;&#25214;&#30701;&#36890;&#29992;&#30701;&#35821;&#65292;&#24403;&#38468;&#21152;&#21040;&#25991;&#26412;&#26102;&#21487;&#20197;&#27450;&#39575;LLMs&#25552;&#20379;&#39640;&#35780;&#20998;&#12290;&#22312;SummEval&#21644;TopicalChat&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LLM&#35780;&#20998;&#21644;&#20004;&#20004;LLM&#27604;&#36739;&#35780;&#20272;&#37117;&#23481;&#26131;&#21463;&#21040;&#31616;&#21333;&#30340;&#20018;&#32852;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;LLM&#35780;&#20998;&#38750;&#24120;&#23481;&#26131;&#21463;&#21040;&#24433;&#21709;&#65292;&#21487;&#20197;&#20135;&#29983;&#26368;&#39640;&#35780;&#20998;&#65292;&#32780;&#19981;&#32771;&#34385;&#36755;&#20837;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36825;&#20123;&#25915;&#20987;&#26159;&#21487;&#20256;&#36882;&#30340;&#65292;&#23398;&#21040;&#30340;&#30701;&#35821;&#21487;&#20197;&#24212;&#29992;&#20110;&#26356;&#22823;&#30340;&#23553;&#38381;&#28304;&#27169;&#22411;&#65292;&#22914;GPT3.5
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14016v1 Announce Type: new  Abstract: Large Language Models (LLMs) are powerful zero-shot assessors and are increasingly used in real-world situations such as for written exams or benchmarking systems. Despite this, no existing work has analyzed the vulnerability of judge-LLMs against adversaries attempting to manipulate outputs. This work presents the first study on the adversarial robustness of assessment LLMs, where we search for short universal phrases that when appended to texts can deceive LLMs to provide high assessment scores. Experiments on SummEval and TopicalChat demonstrate that both LLM-scoring and pairwise LLM-comparative assessment are vulnerable to simple concatenation attacks, where in particular LLM-scoring is very susceptible and can yield maximum assessment scores irrespective of the input text quality. Interestingly, such attacks are transferable and phrases learned on smaller open-source LLMs can be applied to larger closed-source models, such as GPT3.5
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;OlympiadBench&#65292;&#19968;&#20010;&#22885;&#26519;&#21305;&#20122;&#32423;&#21035;&#30340;&#21452;&#35821;&#22810;&#27169;&#24577;&#31185;&#23398;&#22522;&#20934;&#65292;&#21253;&#25324;8952&#20010;&#38382;&#39064;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22797;&#26434;&#38382;&#39064;&#19978;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14008</link><description>&lt;p&gt;
OlympiadBench&#65306;&#19968;&#20010;&#20855;&#26377;&#22885;&#26519;&#21305;&#20122;&#32423;&#21035;&#21452;&#35821;&#22810;&#27169;&#24577;&#31185;&#23398;&#38382;&#39064;&#30340;&#25361;&#25112;&#24615;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14008
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;OlympiadBench&#65292;&#19968;&#20010;&#22885;&#26519;&#21305;&#20122;&#32423;&#21035;&#30340;&#21452;&#35821;&#22810;&#27169;&#24577;&#31185;&#23398;&#22522;&#20934;&#65292;&#21253;&#25324;8952&#20010;&#38382;&#39064;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22797;&#26434;&#38382;&#39064;&#19978;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36827;&#23637;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#19968;&#33324;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#25509;&#36817;&#20102;&#22810;&#20010;&#39046;&#22495;&#20154;&#31867;&#19987;&#23478;&#30340;&#29087;&#32451;&#27700;&#24179;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;OlympiadBench&#65292;&#19968;&#20010;&#22885;&#26519;&#21305;&#20122;&#32423;&#21035;&#30340;&#21452;&#35821;&#22810;&#27169;&#24577;&#31185;&#23398;&#22522;&#20934;&#65292;&#21253;&#25324;&#26469;&#33258;&#22885;&#26519;&#21305;&#20122;&#32423;&#21035;&#25968;&#23398;&#21644;&#29289;&#29702;&#31454;&#36187;&#20197;&#21450;&#20013;&#22269;&#39640;&#32771;&#30340;8952&#20010;&#38382;&#39064;&#12290;&#27599;&#20010;&#38382;&#39064;&#37117;&#37197;&#26377;&#19987;&#23478;&#32423;&#27880;&#37322;&#65292;&#20197;&#36827;&#34892;&#36880;&#27493;&#30340;&#25512;&#29702;&#12290;&#22312;OlympiadBench&#19978;&#35780;&#20272;&#39030;&#23574;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#20934;&#30830;&#35780;&#20272;&#27169;&#22411;&#30340;&#21709;&#24212;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;GPT-4V&#22312;OlympiadBench&#19978;&#33719;&#24471;&#20102;17.23%&#30340;&#24179;&#22343;&#20998;&#65292;&#20854;&#20013;&#22312;&#29289;&#29702;&#23398;&#20013;&#20165;&#20026;11.28%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14008v1 Announce Type: new  Abstract: Recent advancements have seen Large Language Models (LLMs) and Large Multimodal Models (LMMs) surpassing general human capabilities in various tasks, approaching the proficiency level of human experts across multiple domains. With traditional benchmarks becoming less challenging for these models, new rigorous challenges are essential to gauge their advanced abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,952 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam. Each problem is detailed with expert-level annotations for step-by-step reasoning. Evaluating top-tier models on OlympiadBench, we implement a comprehensive assessment methodology to accurately evaluate model responses. Notably, the best-performing model, GPT-4V, attains an average score of 17.23% on OlympiadBench, with a mere 11.28% in physics, hig
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#25991;&#26412;&#27700;&#21360;&#20013;&#30340;&#8220;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#8221;&#27010;&#24565;&#65292;&#21457;&#29616;&#24403;&#21069;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#22312;&#25991;&#26412;&#34987;&#32763;&#35793;&#25104;&#20854;&#20182;&#35821;&#35328;&#21518;&#22833;&#21435;&#20102;&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#27700;&#21360;&#21435;&#38500;&#25915;&#20987;&#26041;&#27861;&#65292;&#26377;&#25928;&#32469;&#36807;&#27700;&#21360;&#65292;&#38477;&#20302;AUC&#20540;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#23548;&#33268;&#36825;&#31181;&#24046;&#24322;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2402.14007</link><description>&lt;p&gt;
&#27700;&#21360;&#26159;&#21542;&#33021;&#22815;&#22312;&#32763;&#35793;&#20013;&#23384;&#27963;&#65311;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#27700;&#21360;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14007
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#25991;&#26412;&#27700;&#21360;&#20013;&#30340;&#8220;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#8221;&#27010;&#24565;&#65292;&#21457;&#29616;&#24403;&#21069;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#22312;&#25991;&#26412;&#34987;&#32763;&#35793;&#25104;&#20854;&#20182;&#35821;&#35328;&#21518;&#22833;&#21435;&#20102;&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#27700;&#21360;&#21435;&#38500;&#25915;&#20987;&#26041;&#27861;&#65292;&#26377;&#25928;&#32469;&#36807;&#27700;&#21360;&#65292;&#38477;&#20302;AUC&#20540;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#23548;&#33268;&#36825;&#31181;&#24046;&#24322;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#26088;&#22312;&#26631;&#35760;&#21644;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#20869;&#23481;&#65292;&#20197;&#38450;&#27490;&#28389;&#29992;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#25991;&#26412;&#27700;&#21360;&#20013;&#30340;&#8220;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#8221;&#27010;&#24565;&#65292;&#35780;&#20272;&#20102;&#25991;&#26412;&#27700;&#21360;&#22312;&#34987;&#32763;&#35793;&#25104;&#20854;&#20182;&#35821;&#35328;&#21518;&#20445;&#25345;&#26377;&#25928;&#24615;&#30340;&#33021;&#21147;&#12290;&#20004;&#20010;LLM&#21644;&#19977;&#31181;&#27700;&#21360;&#26041;&#27861;&#30340;&#21021;&#27493;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#21069;&#30340;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#22312;&#25991;&#26412;&#34987;&#32763;&#35793;&#25104;&#19981;&#21516;&#35821;&#35328;&#26102;&#32570;&#20047;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#27700;&#21360;&#21435;&#38500;&#25915;&#20987;&#65288;CWRA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#39318;&#20808;&#20174;&#19968;&#20010;LLM&#20013;&#33719;&#21462;&#26469;&#33258;&#20013;&#20171;&#35821;&#35328;&#30340;&#21709;&#24212;&#65292;&#28982;&#21518;&#23558;&#20854;&#32763;&#35793;&#25104;&#30446;&#26631;&#35821;&#35328;&#26469;&#32469;&#36807;&#27700;&#21360;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20943;&#23569;AUC&#20540;&#20174;0.95&#38477;&#33267;0.67&#32780;&#26080;&#24615;&#33021;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23548;&#33268;&#20132;&#21449;&#19968;&#33268;&#24615;&#24046;&#24322;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14007v1 Announce Type: cross  Abstract: Text watermarking technology aims to tag and identify content produced by large language models (LLMs) to prevent misuse. In this study, we introduce the concept of ''cross-lingual consistency'' in text watermarking, which assesses the ability of text watermarks to maintain their effectiveness after being translated into other languages. Preliminary empirical results from two LLMs and three watermarking methods reveal that current text watermarking technologies lack consistency when texts are translated into various languages. Based on this observation, we propose a Cross-lingual Watermark Removal Attack (CWRA) to bypass watermarking by first obtaining a response from an LLM in a pivot language, which is then translated into the target language. CWRA can effectively remove watermarks by reducing the Area Under the Curve (AUC) from 0.95 to 0.67 without performance loss. Furthermore, we analyze two key factors that contribute to the cros
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22312;&#21830;&#19994;&#29615;&#22659;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25112;&#30053;&#20215;&#20540;&#25552;&#21462;&#26102;&#65292;&#38656;&#35201;&#29702;&#35299;&#24187;&#35273;&#21644;&#27880;&#24847;&#21147;&#35823;&#23548;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#24182;&#24378;&#35843;&#20102;&#19968;&#20010;&#36798;&#21040;&#26174;&#33879;&#38169;&#35823;&#30340;&#25112;&#30053;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.14002</link><description>&lt;p&gt;
&#24187;&#35273;&#36824;&#26159;&#27880;&#24847;&#21147;&#35823;&#23548;&#65311;&#22312;&#21830;&#19994;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25112;&#30053;&#20215;&#20540;&#25552;&#21462;&#30340;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Hallucinations or Attention Misdirection? The Path to Strategic Value Extraction in Business Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22312;&#21830;&#19994;&#29615;&#22659;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25112;&#30053;&#20215;&#20540;&#25552;&#21462;&#26102;&#65292;&#38656;&#35201;&#29702;&#35299;&#24187;&#35273;&#21644;&#27880;&#24847;&#21147;&#35823;&#23548;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#24182;&#24378;&#35843;&#20102;&#19968;&#20010;&#36798;&#21040;&#26174;&#33879;&#38169;&#35823;&#30340;&#25112;&#30053;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#21464;&#21387;&#22120;&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24443;&#24213;&#25913;&#21464;&#20102;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#65292;&#26641;&#31435;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#22522;&#20934;&#12290;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#34987;&#25209;&#35780;&#29983;&#25104;&#30340;&#32467;&#26524;&#20559;&#31163;&#20107;&#23454;&#20934;&#30830;&#24615;&#25110;&#23637;&#31034;&#36923;&#36753;&#19981;&#19968;&#33268;&#65292;&#36825;&#20123;&#29616;&#35937;&#36890;&#24120;&#34987;&#31216;&#20026;&#24187;&#35273;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#26415;&#35821;&#32463;&#24120;&#34987;&#38169;&#35823;&#22320;&#24212;&#29992;&#20110;&#20219;&#20309;&#20559;&#31163;&#25945;&#24072;&#26399;&#26395;&#30340;&#32467;&#26524;&#65292;&#32780;&#26412;&#25991;&#23558;&#20854;&#23450;&#20041;&#20026;&#27880;&#24847;&#21147;&#35823;&#23548;&#32780;&#38750;&#30495;&#27491;&#30340;&#24187;&#35273;&#12290;&#29702;&#35299;&#24187;&#35273;&#21644;&#27880;&#24847;&#21147;&#35823;&#23548;&#20043;&#38388;&#30340;&#21306;&#21035;&#22312;&#21830;&#19994;&#29615;&#22659;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#36825;&#20123;&#38169;&#35823;&#30340;&#21518;&#26524;&#21487;&#33021;&#20250;&#26174;&#33879;&#24433;&#21709;&#20174;&#36825;&#20123;&#20869;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#25552;&#21462;&#20215;&#20540;&#12290;&#26412;&#25991;&#37325;&#28857;&#20171;&#32461;&#20102;PGI&#65292;Persona&#65292;Grouping&#21644;Intelligence&#26041;&#27861;&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;&#36825;&#26159;&#19968;&#20010;&#23454;&#29616;&#20102;&#26174;&#33879;&#35823;&#24046;&#30340;&#25112;&#30053;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14002v1 Announce Type: new  Abstract: Large Language Models with transformer architecture have revolutionized the domain of text generation, setting unprecedented benchmarks. Despite their impressive capabilities, LLMs have been criticized for generating outcomes that deviate from factual accuracy or display logical inconsistencies, phenomena commonly referred to as hallucinations. This term, however, has often been misapplied to any results deviating from the instructor's expectations, which this paper defines as attention misdirection rather than true hallucinations. Understanding the distinction between hallucinations and attention misdirection becomes increasingly relevant in business contexts, where the ramifications of such errors can significantly impact the value extraction from these inherently pre-trained models. This paper highlights the best practices of the PGI, Persona, Grouping, and Intelligence, method, a strategic framework that achieved a remarkable error r
&lt;/p&gt;</description></item><item><title>&#24207;&#21015;&#32452;&#25104;&#23545;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#24433;&#21709;&#19968;&#30452;&#26410;&#34987;&#28145;&#20837;&#25506;&#35752;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#21457;&#29616;&#65292;&#24212;&#29992;&#20869;&#37096;&#25991;&#26723;&#22240;&#26524;&#36974;&#30422;&#21487;&#20197;&#28040;&#38500;&#26469;&#33258;&#20043;&#21069;&#25991;&#26723;&#30340;&#24178;&#25200;&#20449;&#24687;&#65292;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.13991</link><description>&lt;p&gt;
&#20998;&#26512;&#24207;&#21015;&#32452;&#25104;&#23545;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Analysing The Impact of Sequence Composition on Language Model Pre-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13991
&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#32452;&#25104;&#23545;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#24433;&#21709;&#19968;&#30452;&#26410;&#34987;&#28145;&#20837;&#25506;&#35752;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#21457;&#29616;&#65292;&#24212;&#29992;&#20869;&#37096;&#25991;&#26723;&#22240;&#26524;&#36974;&#30422;&#21487;&#20197;&#28040;&#38500;&#26469;&#33258;&#20043;&#21069;&#25991;&#26723;&#30340;&#24178;&#25200;&#20449;&#24687;&#65292;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#26694;&#26550;&#23558;&#22810;&#20010;&#25991;&#26723;&#36830;&#25509;&#25104;&#22266;&#23450;&#38271;&#24230;&#30340;&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#22240;&#26524;&#36974;&#30422;&#26469;&#35745;&#31639;&#27599;&#20010;&#26631;&#35760;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#19979;&#30340;&#21487;&#33021;&#24615;&#65307;&#36825;&#31181;&#31574;&#30053;&#30001;&#20110;&#31616;&#21333;&#21644;&#39640;&#25928;&#32780;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#39044;&#35757;&#32451;&#24207;&#21015;&#32452;&#25104;&#31574;&#30053;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#29305;&#24615;&#30340;&#24433;&#21709;&#20173;&#26410;&#34987;&#28145;&#20837;&#30740;&#31350;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#24212;&#29992;&#22240;&#26524;&#36974;&#30422;&#21487;&#33021;&#23548;&#33268;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#21253;&#25324;&#26469;&#33258;&#20043;&#21069;&#25991;&#26723;&#30340;&#24178;&#25200;&#20449;&#24687;&#65292;&#20174;&#32780;&#23545;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290; &#22312;&#20869;&#37096;&#25991;&#26723;&#22240;&#26524;&#36974;&#30422;&#20013;&#65292;&#27599;&#20010;&#26631;&#35760;&#30340;&#21487;&#33021;&#24615;&#20165;&#21462;&#20915;&#20110;&#21516;&#19968;&#25991;&#26723;&#20013;&#30340;&#20808;&#21069;&#26631;&#35760;&#65292;&#28040;&#38500;&#20102;&#26469;&#33258;&#20043;&#21069;&#25991;&#26723;&#30340;&#28508;&#22312;&#24178;&#25200;&#20449;&#24687;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290; &#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#36830;&#25509;&#30456;&#20851;&#25991;&#26723;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13991v1 Announce Type: new  Abstract: Most language model pre-training frameworks concatenate multiple documents into fixed-length sequences and use causal masking to compute the likelihood of each token given its context; this strategy is widely adopted due to its simplicity and efficiency. However, to this day, the influence of the pre-training sequence composition strategy on the generalisation properties of the model remains under-explored. In this work, we find that applying causal masking can lead to the inclusion of distracting information from previous documents during pre-training, which negatively impacts the performance of the models on language modelling and downstream tasks. In intra-document causal masking, the likelihood of each token is only conditioned on the previous tokens in the same document, eliminating potential distracting information from previous documents and significantly improving performance. Furthermore, we find that concatenating related docum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20026;&#21307;&#23398;&#39046;&#22495;&#26500;&#24314;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#19977;&#20010;&#20851;&#38190;&#36129;&#29486;:&#26500;&#24314;&#20102;&#26032;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#35821;&#26009;&#24211;MMedC&#65292;&#25552;&#20986;&#20102;&#22810;&#35821;&#35328;&#21307;&#23398;&#22810;&#36873;&#38382;&#31572;&#22522;&#20934;MMedBench&#65292;&#24182;&#19988;&#36890;&#36807;&#22312;MMedC&#19978;&#36827;&#19968;&#27493;&#35757;&#32451;&#33719;&#24471;&#20102;&#24615;&#33021;&#20248;&#36234;&#30340;MMedLM 2&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.13963</link><description>&lt;p&gt;
&#20026;&#21307;&#23398;&#26500;&#24314;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Building Multilingual Language Model for Medicine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20026;&#21307;&#23398;&#39046;&#22495;&#26500;&#24314;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#19977;&#20010;&#20851;&#38190;&#36129;&#29486;:&#26500;&#24314;&#20102;&#26032;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#35821;&#26009;&#24211;MMedC&#65292;&#25552;&#20986;&#20102;&#22810;&#35821;&#35328;&#21307;&#23398;&#22810;&#36873;&#38382;&#31572;&#22522;&#20934;MMedBench&#65292;&#24182;&#19988;&#36890;&#36807;&#22312;MMedC&#19978;&#36827;&#19968;&#27493;&#35757;&#32451;&#33719;&#24471;&#20102;&#24615;&#33021;&#20248;&#36234;&#30340;MMedLM 2&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#38754;&#21521;&#21307;&#23398;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#24471;&#26356;&#24191;&#27867;&#30340;&#35821;&#35328;&#22810;&#26679;&#24615;&#21463;&#20247;&#21463;&#30410;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20027;&#35201;&#36129;&#29486;&#20307;&#29616;&#22312;&#20197;&#19979;&#20960;&#20010;&#26041;&#38754;:&#39318;&#20808;&#65292;&#38024;&#23545;&#22810;&#35821;&#35328;&#21307;&#23398;&#29305;&#23450;&#36866;&#24212;&#24615;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#22823;&#32422;25.5B&#20010;tokens&#65292;&#35206;&#30422;&#20102;6&#31181;&#20027;&#35201;&#35821;&#35328;&#65292;&#34987;&#31216;&#20026;MMedC&#65292;&#36825;&#20351;&#24471;&#29616;&#26377;&#36890;&#29992;LLM&#33021;&#22815;&#36827;&#34892;&#33258;&#22238;&#24402;&#35757;&#32451;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#30417;&#27979;&#21307;&#23398;&#39046;&#22495;&#22810;&#35821;&#35328;LLM&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24102;&#26377;&#35299;&#37322;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#22810;&#36873;&#38382;&#31572;&#22522;&#20934;&#65292;&#31216;&#20026;MMedBench&#65307;&#31532;&#19977;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#20123;&#27969;&#34892;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#65292;&#20197;&#21450;&#37027;&#20123;&#22312;MMedC&#19978;&#36827;&#19968;&#27493;&#36827;&#34892;&#33258;&#22238;&#24402;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#26368;&#32456;&#65292;&#25105;&#20204;&#30340;&#26368;&#32456;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;MMedLM 2&#65292;&#20165;&#26377;7B&#21442;&#25968;&#65292;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13963v1 Announce Type: new  Abstract: In this paper, we aim to develop an open-source, multilingual language model for medicine, that the benefits a wider, linguistically diverse audience from different regions. In general, we present the contribution from the following aspects: first, for multilingual medical-specific adaptation, we construct a new multilingual medical corpus, that contains approximately 25.5B tokens encompassing 6 main languages, termed as MMedC, that enables auto-regressive training for existing general LLMs. second, to monitor the development of multilingual LLMs in medicine, we propose a new multilingual medical multi-choice question-answering benchmark with rationale, termed as MMedBench; third, we have assessed a number of popular, opensource large language models (LLMs) on our benchmark, along with those further auto-regressive trained on MMedC, as a result, our final model, termed as MMedLM 2, with only 7B parameters, achieves superior performance c
&lt;/p&gt;</description></item><item><title>&#20316;&#32773;&#35843;&#26597;&#20102;&#31070;&#32463;LM&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#26469;&#35299;&#30721;&#34164;&#28085;&#21028;&#26029;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#36828;&#39640;&#20110;&#38543;&#26426;&#20960;&#29575;&#22320;&#35299;&#30721;&#33258;&#28982;&#21477;&#23376;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#65292;&#26263;&#31034;LM&#38544;&#21547;&#22320;&#27169;&#25311;&#20102;&#35821;&#20041;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.13956</link><description>&lt;p&gt;
&#20320;&#33021;&#36890;&#36807;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#23398;&#20064;&#35821;&#20041;&#21527;&#65311;&#20197;&#34164;&#28085;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Can You Learn Semantics Through Next-Word Prediction? The Case of Entailment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13956
&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#35843;&#26597;&#20102;&#31070;&#32463;LM&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#26469;&#35299;&#30721;&#34164;&#28085;&#21028;&#26029;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#36828;&#39640;&#20110;&#38543;&#26426;&#20960;&#29575;&#22320;&#35299;&#30721;&#33258;&#28982;&#21477;&#23376;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#65292;&#26263;&#31034;LM&#38544;&#21547;&#22320;&#27169;&#25311;&#20102;&#35821;&#20041;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Merrill&#31561;&#20154;&#65288;2022&#65289;&#35748;&#20026;&#65292;&#22312;&#29702;&#35770;&#19978;&#65292;&#26368;&#20248;LM&#39044;&#27979;&#30340;&#27010;&#29575;&#32534;&#30721;&#20102;&#20851;&#20110;&#34164;&#28085;&#20851;&#31995;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20294;&#26159;&#30001;&#20110;Merrill&#31561;&#20154;&#25552;&#20986;&#30340;&#24378;&#28872;&#29702;&#24819;&#21270;&#20551;&#35774;&#65292;&#19981;&#28165;&#26970;&#31070;&#32463;LM&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#19978;&#26159;&#21542;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#23398;&#20064;&#34164;&#28085;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20182;&#20204;&#30340;&#29702;&#35770;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#20174;&#31070;&#32463;LM&#20013;&#35299;&#30721;&#34164;&#28085;&#21028;&#26029;&#12290;&#25105;&#20204;&#21457;&#29616;&#31867;&#20284;&#20110;&#20182;&#20204;&#30340;&#27979;&#35797;&#21487;&#20197;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#21644;LM&#20013;&#35299;&#30721;&#33258;&#28982;&#21477;&#23376;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#65292;&#36828;&#36828;&#36229;&#36807;&#38543;&#26426;&#26426;&#20250;&#65292;&#23613;&#31649;&#19981;&#26159;&#23436;&#32654;&#30340;&#12290;&#36825;&#34920;&#26126;LM&#38544;&#21547;&#22320;&#27169;&#25311;&#20102;&#35821;&#20041;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#20197;&#39044;&#27979;&#21477;&#23376;&#20849;&#29616;&#27169;&#24335;&#19978;&#30340;&#35821;&#20041;&#25928;&#24212;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#23454;&#38469;&#19978;&#39044;&#27979;&#34164;&#28085;&#30340;&#27979;&#35797;&#19982;&#29702;&#35770;&#27979;&#35797;&#30340;&#26041;&#21521;&#30456;&#21453;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#28508;&#22312;&#30340;&#29702;&#35770;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13956v1 Announce Type: new  Abstract: Do LMs infer the semantics of text from co-occurrence patterns in their training data? Merrill et al. (2022) argue that, in theory, probabilities predicted by an optimal LM encode semantic information about entailment relations, but it is unclear whether neural LMs trained on corpora learn entailment in this way because of strong idealizing assumptions made by Merrill et al. In this work, we investigate whether their theory can be used to decode entailment judgments from neural LMs. We find that a test similar to theirs can decode entailment relations between natural sentences, well above random chance, though not perfectly, across many datasets and LMs. This suggests LMs implicitly model aspects of semantics to predict semantic effects on sentence co-occurrence patterns. However, we find the test that predicts entailment in practice works in the opposite direction to the theoretical test. We thus revisit the assumptions underlying the o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#30340;&#20195;&#29702;&#20989;&#25968;&#22312;&#36845;&#20195;&#23631;&#34109;&#23454;&#39564;&#20013;&#35780;&#20272;&#20102;&#36716;&#25442;&#22120;&#27169;&#22411;&#25152;&#32534;&#30721;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#24182;&#27604;&#36739;&#20102;&#20854;&#19982;&#20854;&#20182;&#35780;&#20272;&#26041;&#27861;&#30340;&#20559;&#35265;&#20272;&#35745;&#65292;&#21457;&#29616;&#36716;&#25442;&#22120;&#27169;&#22411;&#20013;&#23384;&#22312;&#30456;&#23545;&#36739;&#39640;&#30340;&#23447;&#25945;&#21644;&#27531;&#30142;&#20559;&#35265;&#65292;&#32780;&#24615;&#21035;&#20559;&#35265;&#21017;&#30456;&#23545;&#36739;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.13954</link><description>&lt;p&gt;
&#36890;&#36807;&#39044;&#27979;&#36136;&#37327;&#38388;&#25509;&#27979;&#37327;&#25513;&#30422;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Measuring Social Biases in Masked Language Models by Proxy of Prediction Quality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#30340;&#20195;&#29702;&#20989;&#25968;&#22312;&#36845;&#20195;&#23631;&#34109;&#23454;&#39564;&#20013;&#35780;&#20272;&#20102;&#36716;&#25442;&#22120;&#27169;&#22411;&#25152;&#32534;&#30721;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#24182;&#27604;&#36739;&#20102;&#20854;&#19982;&#20854;&#20182;&#35780;&#20272;&#26041;&#27861;&#30340;&#20559;&#35265;&#20272;&#35745;&#65292;&#21457;&#29616;&#36716;&#25442;&#22120;&#27169;&#22411;&#20013;&#23384;&#22312;&#30456;&#23545;&#36739;&#39640;&#30340;&#23447;&#25945;&#21644;&#27531;&#30142;&#20559;&#35265;&#65292;&#32780;&#24615;&#21035;&#20559;&#35265;&#21017;&#30456;&#23545;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#21644;&#25919;&#27835;&#31185;&#23398;&#23478;&#32463;&#24120;&#26088;&#22312;&#20174;&#25991;&#26412;&#25968;&#25454;&#34920;&#31034;&#65288;&#23884;&#20837;&#65289;&#20013;&#21457;&#29616;&#21644;&#34913;&#37327;&#19981;&#21516;&#30340;&#20559;&#35265;&#12290;&#21019;&#26032;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20196;&#29260;&#23884;&#20837;&#65292;&#24182;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#24050;&#34987;&#35777;&#26126;&#22312;&#19979;&#28216;&#24212;&#29992;&#20013;&#32534;&#30721;&#20102;&#19981;&#38656;&#35201;&#30340;&#20559;&#35265;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#30340;&#20195;&#29702;&#20989;&#25968;&#22312;&#36845;&#20195;&#23631;&#34109;&#23454;&#39564;&#20013;&#35780;&#20272;&#30001;&#35757;&#32451;&#26377;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#30340;&#36716;&#25442;&#22120;&#25152;&#32534;&#30721;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#20197;&#27979;&#37327;&#36716;&#25442;&#22120;&#27169;&#22411;&#39044;&#27979;&#36136;&#37327;&#65292;&#24182;&#35780;&#20272;MLM&#23545;&#19981;&#21033;&#32676;&#20307;&#21644;&#26377;&#21033;&#32676;&#20307;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#27604;&#36739;&#20351;&#29992;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#20559;&#35265;&#20272;&#35745;&#19982;&#20854;&#20182;&#35780;&#20272;&#26041;&#27861;&#20135;&#29983;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#32771;&#34385;&#30340;MLMs&#20013;&#23384;&#22312;&#30456;&#23545;&#36739;&#39640;&#30340;&#23447;&#25945;&#21644;&#27531;&#30142;&#20559;&#35265;&#65292;&#32780;&#30456;&#23545;&#20110;&#21478;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#36739;&#20302;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13954v1 Announce Type: new  Abstract: Social and political scientists often aim to discover and measure distinct biases from text data representations (embeddings). Innovative transformer-based language models produce contextually-aware token embeddings and have achieved state-of-the-art performance for a variety of natural language tasks, but have been shown to encode unwanted biases for downstream applications. In this paper, we evaluate the social biases encoded by transformers trained with the masked language modeling objective using proposed proxy functions within an iterative masking experiment to measure the quality of transformer models' predictions, and assess the preference of MLMs towards disadvantaged and advantaged groups. We compare bias estimations with those produced by other evaluation methods using two benchmark datasets, finding relatively high religious and disability biases across considered MLMs and low gender bias in one dataset relative to the other. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#24544;&#23454;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;FRODO&#26694;&#26550;&#26469;&#25913;&#36827;&#29983;&#25104;&#25512;&#29702;&#27493;&#39588;&#21644;&#22362;&#22266;&#25512;&#29702;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.13950</link><description>&lt;p&gt;
&#20351;&#25512;&#29702;&#21464;&#24471;&#37325;&#35201;&#65306;&#34913;&#37327;&#21644;&#25552;&#39640;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#30340;&#24544;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#24544;&#23454;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;FRODO&#26694;&#26550;&#26469;&#25913;&#36827;&#29983;&#25104;&#25512;&#29702;&#27493;&#39588;&#21644;&#22362;&#22266;&#25512;&#29702;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22238;&#31572;&#38382;&#39064;&#20043;&#21069;&#32463;&#36807;&#36880;&#27493;&#25512;&#29702;&#24050;&#34987;&#35777;&#26126;&#34920;&#29616;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#26368;&#32456;&#31572;&#26696;&#19982;&#25152;&#36848;&#25512;&#29702;&#27493;&#39588;&#30340;&#24544;&#23454;&#31243;&#24230;&#23578;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#23545;&#21313;&#20108;&#20010;LLMs&#36827;&#34892;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#65292;&#20197;&#26816;&#39564;LLM&#29983;&#25104;&#30340;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#22914;&#20309;&#24433;&#21709;&#26368;&#32456;&#32467;&#26524;&#65292;&#24182;&#21457;&#29616;LLMs&#22312;&#29983;&#25104;&#31572;&#26696;&#26102;&#24182;&#19981;&#21487;&#38752;&#22320;&#20351;&#29992;&#20854;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FRODO&#65292;&#19968;&#20010;&#26088;&#22312;&#23450;&#21046;&#23567;&#22411;LM&#20197;&#29983;&#25104;&#27491;&#30830;&#25512;&#29702;&#27493;&#39588;&#24182;&#22312;&#36825;&#20123;&#27493;&#39588;&#19978;&#36827;&#34892;&#22362;&#22266;&#25512;&#29702;&#30340;&#26694;&#26550;&#12290;FRODO&#21253;&#25324;&#19968;&#20010;&#25512;&#26029;&#27169;&#22359;&#65292;&#36890;&#36807;&#23398;&#20064;&#20351;&#29992;&#38544;&#24335;&#22240;&#26524;&#22870;&#21169;&#20989;&#25968;&#29983;&#25104;&#27491;&#30830;&#25512;&#29702;&#27493;&#39588;&#65292;&#24182;&#19988;&#19968;&#20010;&#25512;&#29702;&#27169;&#22359;&#65292;&#36890;&#36807;&#23398;&#20064;&#20351;&#29992;&#21453;&#20107;&#23454;&#21644;&#22240;&#26524;&#20559;&#22909;&#30446;&#26631;&#22312;&#36825;&#20123;&#20013;&#38388;&#25512;&#29702;&#19978;&#24544;&#23454;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;F
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13950v1 Announce Type: new  Abstract: Large language models (LLMs) have been shown to perform better when asked to reason step-by-step before answering a question. However, it is unclear to what degree the model's final answer is faithful to the stated reasoning steps. In this paper, we perform a causal mediation analysis on twelve LLMs to examine how intermediate reasoning steps generated by the LLM influence the final outcome and find that LLMs do not reliably use their intermediate reasoning steps when generating an answer. To address this issue, we introduce FRODO, a framework to tailor small-sized LMs to generate correct reasoning steps and robustly reason over these steps. FRODO consists of an inference module that learns to generate correct reasoning steps using an implicit causal reward function and a reasoning module that learns to faithfully reason over these intermediate inferences using a counterfactual and causal preference objective. Our experiments show that F
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#21033;&#29992;&#22320;&#38754;&#30495;&#23454;&#23383;&#24149;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#23383;&#24149;&#30340;&#29420;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13936</link><description>&lt;p&gt;
&#29420;&#29305;&#22270;&#20687;&#23383;&#24149;&#65306;&#22312;CLIP&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#22320;&#38754;&#30495;&#23454;&#23383;&#24149;
&lt;/p&gt;
&lt;p&gt;
Distinctive Image Captioning: Leveraging Ground Truth Captions in CLIP Guided Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13936
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#21033;&#29992;&#22320;&#38754;&#30495;&#23454;&#23383;&#24149;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#23383;&#24149;&#30340;&#29420;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25945;&#24072;&#24378;&#36843;&#35757;&#32451;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#20250;&#23548;&#33268;&#29983;&#25104;&#38750;&#24120;&#36890;&#29992;&#30340;&#26679;&#26412;&#65292;&#32780;&#26356;&#20855;&#26377;&#29420;&#29305;&#24615;&#30340;&#23383;&#24149;&#22312;&#26816;&#32034;&#24212;&#29992;&#25110;&#20026;&#22270;&#20687;&#29983;&#25104;&#26367;&#20195;&#25991;&#26412;&#20197;&#22686;&#24378;&#21487;&#35775;&#38382;&#24615;&#26041;&#38754;&#38750;&#24120;&#26377;&#29992;&#12290; &#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20801;&#35768;&#20351;&#29992;&#29983;&#25104;&#23383;&#24149;&#19982;&#36755;&#20837;&#22270;&#20687;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#26816;&#32034;&#30456;&#20284;&#24230;&#20998;&#25968;&#20316;&#20026;&#22870;&#21169;&#26469;&#25351;&#23548;&#35757;&#32451;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#20855;&#29420;&#29305;&#24615;&#30340;&#23383;&#24149;&#12290; &#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#36328;&#27169;&#24577;&#26816;&#32034;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#25552;&#20379;&#36825;&#31181;&#22870;&#21169;&#65292;&#20174;&#32780;&#23436;&#20840;&#28040;&#38500;&#20102;&#23545;&#21442;&#32771;&#23383;&#24149;&#30340;&#38656;&#27714;&#12290; &#28982;&#32780;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#35748;&#20026;&#22320;&#38754;&#30495;&#23454;&#65288;GT&#65289;&#23383;&#24149;&#22312;&#36825;&#31181;RL&#26694;&#26550;&#20013;&#20173;&#28982;&#21487;&#20197;&#21457;&#25381;&#20316;&#29992;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#19981;&#21516;&#26041;&#24335;&#21033;&#29992;GT&#23383;&#24149;&#12290; &#39318;&#20808;&#65292;&#23427;&#20204;&#21487;&#20197;&#29992;&#26469;&#35757;&#32451;&#19968;&#20010;&#31616;&#21333;&#30340;MLP&#37492;&#21035;&#22120;&#65292;&#20316;&#20026;&#27491;&#21017;&#21270;&#30340;&#19968;&#37096;&#20998;&#65292;&#20197;&#38450;&#27490;&#22870;&#21169;&#20316;&#24330;&#24182;&#30830;&#20445;&#27969;&#30021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13936v1 Announce Type: new  Abstract: Training image captioning models using teacher forcing results in very generic samples, whereas more distinctive captions can be very useful in retrieval applications or to produce alternative texts describing images for accessibility. Reinforcement Learning (RL) allows to use cross-modal retrieval similarity score between the generated caption and the input image as reward to guide the training, leading to more distinctive captions. Recent studies show that pre-trained cross-modal retrieval models can be used to provide this reward, completely eliminating the need for reference captions. However, we argue in this paper that Ground Truth (GT) captions can still be useful in this RL framework. We propose a new image captioning model training strategy that makes use of GT captions in different ways. Firstly, they can be used to train a simple MLP discriminator that serves as a regularization to prevent reward hacking and ensures the fluenc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#39640;&#25928;Transformer&#65288;&#20363;&#22914;&#31232;&#30095;Transformer&#21644;&#32447;&#24615;Transformer&#65289;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#65292;&#21457;&#29616;&#23427;&#20204;&#36866;&#21512;&#35299;&#20915;&#19968;&#33324;DP&#20219;&#21153;&#65292;&#20294;&#19981;&#21516;&#20110;&#26631;&#20934;Transformer&#12290;</title><link>https://arxiv.org/abs/2402.13934</link><description>&lt;p&gt;
&#30830;&#23454;&#39640;&#25928;&#30340;Transformer&#33021;&#22815;&#33410;&#32422;&#35745;&#31639;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Efficient Transformers Really Save Computation?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#39640;&#25928;Transformer&#65288;&#20363;&#22914;&#31232;&#30095;Transformer&#21644;&#32447;&#24615;Transformer&#65289;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#65292;&#21457;&#29616;&#23427;&#20204;&#36866;&#21512;&#35299;&#20915;&#19968;&#33324;DP&#20219;&#21153;&#65292;&#20294;&#19981;&#21516;&#20110;&#26631;&#20934;Transformer&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#36234;&#26469;&#36234;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#24182;&#25317;&#26377;&#22823;&#37327;&#21442;&#25968;&#65292;&#25214;&#21040;&#26356;&#39640;&#25928;&#30340;&#26367;&#20195;&#26631;&#20934;Transformer&#21464;&#24471;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#39640;&#25928;&#30340;Transformer&#21644;Transformer&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#27809;&#26377;&#19968;&#20010;&#33021;&#22815;&#25552;&#20379;&#23427;&#20204;&#36866;&#21512;&#26367;&#20195;&#26631;&#20934;Transformer&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#36825;&#20351;&#24471;&#24456;&#38590;&#30830;&#23450;&#20309;&#26102;&#20351;&#29992;&#29305;&#23450;&#27169;&#22411;&#20197;&#21450;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#37325;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#29702;&#35299;&#39640;&#25928;Transformer&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#31232;&#30095;Transformer&#21644;&#32447;&#24615;Transformer&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#23427;&#20204;&#22312;Chain-of-Thought (CoT)&#25552;&#31034;&#20013;&#23637;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36981;&#24490;&#20808;&#21069;&#30340;&#30740;&#31350;&#23558;&#23427;&#20204;&#24314;&#27169;&#20026;&#21160;&#24577;&#35268;&#21010;&#65288;DP&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#36275;&#22815;&#34920;&#36798;&#35299;&#20915;&#19968;&#33324;DP&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#20294;&#19982;&#26631;&#20934;Transformer&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13934v1 Announce Type: cross  Abstract: As transformer-based language models are trained on increasingly large datasets and with vast numbers of parameters, finding more efficient alternatives to the standard Transformer has become very valuable. While many efficient Transformers and Transformer alternatives have been proposed, none provide theoretical guarantees that they are a suitable replacement for the standard Transformer. This makes it challenging to identify when to use a specific model and what directions to prioritize for further investigation. In this paper, we aim to understand the capabilities and limitations of efficient Transformers, specifically the Sparse Transformer and the Linear Transformer. We focus on their reasoning capability as exhibited by Chain-of-Thought (CoT) prompts and follow previous works to model them as Dynamic Programming (DP) problems. Our results show that while these models are expressive enough to solve general DP tasks, contrary to ex
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#21463;&#21040;&#35825;&#39285;-&#36716;&#25442;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#29978;&#33267;&#23433;&#20840;&#29983;&#25104;&#30340;&#25991;&#26412;&#20063;&#33021;&#36731;&#26131;&#36716;&#21464;&#20026;&#26377;&#23475;&#20869;&#23481;&#65292;&#24378;&#35843;&#22312;LLMs&#30340;&#23433;&#20840;&#38450;&#25252;&#20013;&#38656;&#35201;&#32771;&#34385;&#21518;&#22788;&#29702;&#36716;&#25442;&#12290;</title><link>https://arxiv.org/abs/2402.13926</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26131;&#21463;&#35825;&#39285;-&#36716;&#25442;&#25915;&#20987;&#30340;&#21361;&#23475;&#20869;&#23481;&#29983;&#25104;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Vulnerable to Bait-and-Switch Attacks for Generating Harmful Content
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13926
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#21463;&#21040;&#35825;&#39285;-&#36716;&#25442;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#29978;&#33267;&#23433;&#20840;&#29983;&#25104;&#30340;&#25991;&#26412;&#20063;&#33021;&#36731;&#26131;&#36716;&#21464;&#20026;&#26377;&#23475;&#20869;&#23481;&#65292;&#24378;&#35843;&#22312;LLMs&#30340;&#23433;&#20840;&#38450;&#25252;&#20013;&#38656;&#35201;&#32771;&#34385;&#21518;&#22788;&#29702;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#27450;&#39575;&#24615;&#21644;&#26377;&#23475;&#20869;&#23481;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#24050;&#32463;&#24341;&#36215;&#20102;&#30456;&#24403;&#22810;&#30340;&#30740;&#31350;&#65292;&#20294;&#21363;&#20351;&#26159;&#23433;&#20840;&#30340;&#29983;&#25104;&#20063;&#21487;&#33021;&#23548;&#33268;&#38382;&#39064;&#38477;&#32423;&#24433;&#21709;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#28966;&#28857;&#36716;&#31227;&#21040;&#21363;&#20351;&#26469;&#33258;LLMs&#30340;&#23433;&#20840;&#25991;&#26412;&#20063;&#21487;&#20197;&#36890;&#36807;&#35825;&#39285;-&#36716;&#25442;&#25915;&#20987;&#36731;&#26494;&#36716;&#21464;&#20026;&#28508;&#22312;&#21361;&#38505;&#20869;&#23481;&#12290;&#22312;&#36825;&#31181;&#25915;&#20987;&#20013;&#65292;&#29992;&#25143;&#39318;&#20808;&#29992;&#23433;&#20840;&#38382;&#39064;&#25552;&#31034;LLMs&#65292;&#28982;&#21518;&#21033;&#29992;&#31616;&#21333;&#30340;&#26597;&#25214;&#21644;&#26367;&#25442;&#21518;&#22788;&#29702;&#25216;&#26415;&#23558;&#36755;&#20986;&#25805;&#32437;&#25104;&#26377;&#23475;&#21465;&#20107;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#29983;&#25104;&#26377;&#27602;&#20869;&#23481;&#26041;&#38754;&#30340;&#24778;&#20154;&#26377;&#25928;&#24615;&#31361;&#20986;&#20102;&#22312;&#24320;&#21457;&#21487;&#38752;&#30340;LLMs&#23433;&#20840;&#38450;&#25252;&#26639;&#26102;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#24378;&#35843;&#65292;&#19987;&#27880;&#20110;&#36880;&#23383;&#30340;LLMs&#36755;&#20986;&#30340;&#23433;&#20840;&#24615;&#26159;&#19981;&#22815;&#30340;&#65292;&#25105;&#20204;&#36824;&#38656;&#35201;&#32771;&#34385;&#21518;&#22788;&#29702;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13926v1 Announce Type: cross  Abstract: The risks derived from large language models (LLMs) generating deceptive and damaging content have been the subject of considerable research, but even safe generations can lead to problematic downstream impacts. In our study, we shift the focus to how even safe text coming from LLMs can be easily turned into potentially dangerous content through Bait-and-Switch attacks. In such attacks, the user first prompts LLMs with safe questions and then employs a simple find-and-replace post-hoc technique to manipulate the outputs into harmful narratives. The alarming efficacy of this approach in generating toxic content highlights a significant challenge in developing reliable safety guardrails for LLMs. In particular, we stress that focusing on the safety of the verbatim LLM outputs is insufficient and that we also need to consider post-hoc transformations.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27969;&#31243;&#65292;&#21033;&#29992;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#20197;&#22686;&#24378;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#65292;&#24357;&#34917;&#20102;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#30340;&#39640;&#25104;&#26412;&#21644;&#26377;&#38480;&#21487;&#29992;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.13919</link><description>&lt;p&gt;
SYNFAC-EDIT: &#29992;&#20110;&#20020;&#24202;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#23545;&#40784;&#30340;&#21512;&#25104;&#27169;&#20223;&#32534;&#36753;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13919
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27969;&#31243;&#65292;&#21033;&#29992;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#20197;&#22686;&#24378;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#65292;&#24357;&#34917;&#20102;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#30340;&#39640;&#25104;&#26412;&#21644;&#26377;&#38480;&#21487;&#29992;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT&#21644;Llama&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#20107;&#23454;&#19981;&#20934;&#30830;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#26159;&#20020;&#24202;NLP&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#38169;&#35823;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#20107;&#23454;&#23545;&#40784;&#30340;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#25104;&#26412;&#39640;&#26114;&#19988;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#27969;&#31243;&#65292;&#21033;&#29992;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#26088;&#22312;&#22686;&#24378;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#32534;&#36753;&#21453;&#39304;&#65292;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#25311;&#20102;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#25913;&#21892;AI&#31995;&#32479;&#36755;&#20986;&#30340;&#23454;&#38469;&#22330;&#26223;&#12290;&#23613;&#31649;GPT&#22312;&#21508;&#31181;&#20020;&#24202;NLP&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20986;&#20102;&#19987;&#19994;&#27700;&#24179;&#65292;&#27604;&#22914;&#21307;&#23398;&#25191;&#29031;&#32771;&#35797;&#65292;&#20294;&#23545;&#20854;&#25552;&#20379;&#25913;&#21892;&#36739;&#24369;LM&#25110;LLM&#29983;&#25104;&#36136;&#37327;&#30340;&#19987;&#19994;&#32423;&#32534;&#36753;&#21453;&#39304;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13919v1 Announce Type: cross  Abstract: Large Language Models (LLMs) such as GPT and Llama have demonstrated significant achievements in summarization tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences. To counter the high costs and limited availability of expert-annotated data for factual alignment, this study introduces an innovative pipeline that utilizes GPT-3.5 and GPT-4 to generate high-quality feedback aimed at enhancing factual consistency in clinical note summarization. Our research primarily focuses on edit feedback, mirroring the practical scenario in which medical professionals refine AI system outputs without the need for additional annotations. Despite GPT's proven expertise in various clinical NLP tasks, such as the Medical Licensing Examination, there is scant research on its capacity to deliver expert-level edit feedback for improving weaker LMs or LLMs generation qualit
&lt;/p&gt;</description></item><item><title>Llama2&#27169;&#22411;&#22312;&#32763;&#35793;&#20013;&#34920;&#29616;&#20986;&#20934;&#30830;&#24230;&#39640;&#65292;&#37096;&#20998;&#26410;&#35265;&#35821;&#35328;&#38656;&#35201;&#26356;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#26469;&#25552;&#21319;&#32763;&#35793;&#36136;&#37327;&#65292;&#21478;&#22806;&#35821;&#35328;&#30340;&#21477;&#27861;&#30456;&#20284;&#24615;&#24182;&#38750;&#32763;&#35793;&#36136;&#37327;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#26576;&#20123;&#35821;&#35328;&#21363;&#20351;&#25968;&#25454;&#23569;&#20381;&#28982;&#34920;&#29616;&#20986;&#24378;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13917</link><description>&lt;p&gt;
LLM&#32763;&#35793;&#20013;&#30340;&#35821;&#35328;&#29305;&#24449;&#21644;&#37325;&#35201;&#35821;&#35328;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Linguistic Features and Languages are Important in LLM Translation?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13917
&lt;/p&gt;
&lt;p&gt;
Llama2&#27169;&#22411;&#22312;&#32763;&#35793;&#20013;&#34920;&#29616;&#20986;&#20934;&#30830;&#24230;&#39640;&#65292;&#37096;&#20998;&#26410;&#35265;&#35821;&#35328;&#38656;&#35201;&#26356;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#26469;&#25552;&#21319;&#32763;&#35793;&#36136;&#37327;&#65292;&#21478;&#22806;&#35821;&#35328;&#30340;&#21477;&#27861;&#30456;&#20284;&#24615;&#24182;&#38750;&#32763;&#35793;&#36136;&#37327;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#26576;&#20123;&#35821;&#35328;&#21363;&#20351;&#25968;&#25454;&#23569;&#20381;&#28982;&#34920;&#29616;&#20986;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2402.13917v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#20855;&#26377;&#24378;&#22823;&#33021;&#21147;&#65292;&#21253;&#25324;&#26426;&#22120;&#32763;&#35793;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#35780;&#20272;Llama2&#30340;&#26426;&#22120;&#32763;&#35793;&#33021;&#21147;&#65292;&#24182;&#25506;&#32034;&#32763;&#35793;&#22914;&#20309;&#21462;&#20915;&#20110;&#20854;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;7B Llama2&#27169;&#22411;&#23545;&#20854;&#25152;&#35265;&#30340;&#25152;&#26377;&#35821;&#35328;&#37117;&#21487;&#20197;&#33719;&#24471;&#36229;&#36807;10&#30340;BLEU&#20998;&#25968;&#65292;&#20294;&#24182;&#38750;&#24635;&#26159;&#23545;&#20854;&#26410;&#35265;&#30340;&#35821;&#35328;&#12290;&#23545;&#20110;&#36825;&#20123;&#26410;&#35265;&#35821;&#35328;&#65292;&#19982;&#20351;&#29992;&#32842;&#22825;&#29256;&#26412;&#25110;&#28155;&#21152;&#23569;&#37327;&#25968;&#25454;&#30456;&#27604;&#65292;&#22312;&#27169;&#22411;&#35268;&#27169;&#19978;&#35266;&#23519;&#21040;&#30340;&#26368;&#22823;&#25910;&#30410;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35821;&#35328;&#36317;&#31163;&#20998;&#26512;&#26174;&#31034;&#65292;&#21477;&#27861;&#30456;&#20284;&#24615;&#24182;&#38750;&#22987;&#32456;&#26159;&#20915;&#23450;&#32763;&#35793;&#36136;&#37327;&#30340;&#20027;&#35201;&#35821;&#35328;&#22240;&#32032;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#65292;&#19968;&#20123;&#35821;&#35328;&#65292;&#23613;&#31649;&#35757;&#32451;&#25968;&#25454;&#26126;&#26174;&#23569;&#20110;&#33521;&#35821;&#65292;&#21364;&#34920;&#29616;&#20986;&#19982;&#33521;&#35821;&#21487;&#27604;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#30340;&#21457;&#29616;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13917v1 Announce Type: cross  Abstract: Large Language Models (LLMs) demonstrate strong capability across multiple tasks, including machine translation. Our study focuses on evaluating Llama2's machine translation capabilities and exploring how translation depends on languages in its training data. Our experiments show that the 7B Llama2 model yields above 10 BLEU score for all languages it has seen, but not always for languages it has not seen. Most gains for those unseen languages are observed the most with the model scale compared to using chat versions or adding shot count. Furthermore, our linguistic distance analysis reveals that syntactic similarity is not always the primary linguistic factor in determining translation quality. Interestingly, we discovered that under specific circumstances, some languages, despite having significantly less training data than English, exhibit strong correlations comparable to English. Our discoveries here give new perspectives for the 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26080;&#30417;&#30563;&#22270;&#26041;&#24335;&#65292;&#21033;&#29992;&#25991;&#26723;&#38388;&#21644;&#25991;&#20869;&#30456;&#20284;&#24615;&#65292;&#25552;&#21462;&#25991;&#26723;&#25910;&#34255;&#30340;&#25972;&#20307;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.13906</link><description>&lt;p&gt;
&#21033;&#29992;&#25972;&#20010;&#25910;&#34255;&#30456;&#20284;&#24615;&#36827;&#34892;&#26080;&#30417;&#30563;&#25991;&#26723;&#32467;&#26500;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Leveraging Collection-Wide Similarities for Unsupervised Document Structure Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13906
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#22270;&#26041;&#24335;&#65292;&#21033;&#29992;&#25991;&#26723;&#38388;&#21644;&#25991;&#20869;&#30456;&#20284;&#24615;&#65292;&#25552;&#21462;&#25991;&#26723;&#25910;&#34255;&#30340;&#25972;&#20307;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21508;&#20010;&#39046;&#22495;&#30340;&#25991;&#26723;&#25910;&#34255;&#65292;&#22914;&#27861;&#24459;&#12289;&#21307;&#23398;&#25110;&#37329;&#34701;&#31561;&#65292;&#36890;&#24120;&#20849;&#20139;&#19968;&#20123;&#28508;&#22312;&#30340;&#25972;&#20010;&#25910;&#34255;&#32467;&#26500;&#65292;&#36825;&#20123;&#32467;&#26500;&#25429;&#25417;&#21040;&#30340;&#20449;&#24687;&#21487;&#20197;&#24110;&#21161;&#20154;&#31867;&#29992;&#25143;&#21644;&#32467;&#26500;&#24863;&#30693;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#35782;&#21035;&#25910;&#34255;&#20869;&#25991;&#26723;&#30340;&#20856;&#22411;&#32467;&#26500;&#65292;&#38656;&#35201;&#25429;&#25417;&#25972;&#20010;&#25910;&#34255;&#20013;&#21453;&#22797;&#20986;&#29616;&#30340;&#20027;&#39064;&#65292;&#21516;&#26102;&#25688;&#35201;&#20219;&#24847;&#26631;&#39064;&#30340;&#37322;&#20041;&#65292;&#24182;&#23558;&#27599;&#20010;&#20027;&#39064;&#19982;&#30456;&#24212;&#30340;&#25991;&#26723;&#20301;&#32622;&#32852;&#31995;&#36215;&#26469;&#12290;&#36825;&#20123;&#35201;&#27714;&#24102;&#26469;&#20102;&#20960;&#20010;&#25361;&#25112;&#65306;&#26631;&#35760;&#21453;&#22797;&#20986;&#29616;&#20027;&#39064;&#30340;&#26631;&#39064;&#22312;&#25514;&#36766;&#19978;&#32463;&#24120;&#19981;&#21516;&#65292;&#26576;&#20123;&#33410;&#26631;&#39064;&#20165;&#36866;&#29992;&#20110;&#20010;&#21035;&#25991;&#26723;&#19988;&#19981;&#21453;&#26144;&#20856;&#22411;&#32467;&#26500;&#65292;&#20027;&#39064;&#39034;&#24207;&#22312;&#25991;&#26723;&#20043;&#38388;&#21487;&#33021;&#20250;&#26377;&#25152;&#21464;&#21270;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#25991;&#26723;&#38388;&#21644;&#25991;&#20869;&#30456;&#20284;&#24615;&#30340;&#26080;&#30417;&#30563;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#65292;&#26469;&#25552;&#21462;&#28508;&#22312;&#30340;&#25972;&#20010;&#25910;&#34255;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13906v1 Announce Type: new  Abstract: Document collections of various domains, e.g., legal, medical, or financial, often share some underlying collection-wide structure, which captures information that can aid both human users and structure-aware models. We propose to identify the typical structure of document within a collection, which requires to capture recurring topics across the collection, while abstracting over arbitrary header paraphrases, and ground each topic to respective document locations. These requirements pose several challenges: headers that mark recurring topics frequently differ in phrasing, certain section headers are unique to individual documents and do not reflect the typical structure, and the order of topics can vary between documents. Subsequently, we develop an unsupervised graph-based method which leverages both inter- and intra-document similarities, to extract the underlying collection-wide structure. Our evaluations on three diverse domains in 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#20010;&#38543;&#26426;&#25277;&#26679;&#27169;&#22411;&#29983;&#25104;&#30340;&#20998;&#24067;&#25512;&#26029;&#32622;&#20449;&#24230;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.13904</link><description>&lt;p&gt;
&#20351;&#29992;&#26679;&#26412;&#19968;&#33268;&#24615;&#26657;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Calibrating Large Language Models with Sample Consistency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13904
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#20010;&#38543;&#26426;&#25277;&#26679;&#27169;&#22411;&#29983;&#25104;&#30340;&#20998;&#24067;&#25512;&#26029;&#32622;&#20449;&#24230;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39044;&#27979;&#30340;&#32622;&#20449;&#27700;&#24179;&#23545;&#20110;&#23427;&#20204;&#30340;&#21487;&#38752;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#19987;&#26377;&#24615;&#36136;&#21644;&#22823;&#35268;&#27169;&#65292;LLMs&#36890;&#24120;&#22825;&#29983;&#19981;&#32463;&#26657;&#20934;&#65292;&#20351;&#24471;&#20256;&#32479;&#30340;&#26657;&#20934;&#25216;&#26415;&#24456;&#38590;&#36866;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36890;&#36807;&#22810;&#20010;&#38543;&#26426;&#25277;&#26679;&#30340;&#27169;&#22411;&#29983;&#25104;&#30340;&#20998;&#24067;&#26469;&#25512;&#26029;&#32622;&#20449;&#24230;&#30340;&#28508;&#21147;&#65292;&#37319;&#29992;&#20102;&#19977;&#31181;&#19968;&#33268;&#24615;&#24230;&#37327;&#12290;&#25105;&#20204;&#22312;&#20061;&#20010;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#23545;&#21508;&#31181;&#24320;&#28304;&#21644;&#38381;&#28304;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#26657;&#20934;&#26041;&#27861;&#32988;&#36807;&#29616;&#26377;&#30340;&#20107;&#21518;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20013;&#38388;&#35299;&#37322;&#12289;&#27169;&#22411;&#25193;&#23637;&#21644;&#26356;&#22823;&#30340;&#26679;&#26412;&#22823;&#23567;&#31561;&#22240;&#32032;&#21487;&#20197;&#22686;&#24378;&#26657;&#20934;&#65292;&#32780;&#25351;&#23548;&#35843;&#25972;&#20250;&#20351;&#26657;&#20934;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#19968;&#33268;&#24615;&#33719;&#24471;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#26377;&#26395;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13904v1 Announce Type: new  Abstract: Accurately gauging the confidence level of Large Language Models' (LLMs) predictions is pivotal for their reliable application. However, LLMs are often uncalibrated inherently and elude conventional calibration techniques due to their proprietary nature and massive scale. In this work, we explore the potential of deriving confidence from the distribution of multiple randomly sampled model generations, via three measures of consistency. We perform an extensive evaluation across various open and closed-source models on nine reasoning datasets. Results show that consistency-based calibration methods outperform existing post-hoc approaches. Meanwhile, we find that factors such as intermediate explanations, model scaling, and larger sample sizes enhance calibration, while instruction-tuning makes calibration more difficult. Moreover, confidence scores obtained from consistency have the potential to enhance model performance. Finally, we offer
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#22359;&#24335;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#25991;&#26723;&#20013;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#21452;&#21521;&#20132;&#20114;</title><link>https://arxiv.org/abs/2402.13897</link><description>&lt;p&gt;
&#31185;&#23398;&#26816;&#26597;&#32773;&#20877;&#24230;&#21319;&#32423;&#65306;&#36879;&#26126;&#24230;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#21452;&#21521;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Science Checker Reloaded: A Bidirectional Paradigm for Transparency and Logical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13897
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#22359;&#24335;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#25991;&#26723;&#20013;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#21452;&#21521;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20173;&#28982;&#38754;&#20020;&#30528;&#22312;&#31185;&#23398;&#21644;&#24037;&#19994;&#30340;&#28023;&#37327;&#20449;&#24687;&#20013;&#30340;&#35832;&#22810;&#38480;&#21046;&#65292;&#27604;&#22914;&#35821;&#20041;&#20998;&#27495;&#21644;&#26816;&#32034;&#20013;&#30340;&#35789;&#27719;&#24046;&#36317;&#12289;&#35821;&#20041;&#25628;&#32034;&#20013;&#30340;&#20302;&#31934;&#24230;&#21644;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#25110;&#32773;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#21644;&#36807;&#26102;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#22359;&#24335;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#25991;&#26723;&#30340;&#36825;&#20123;&#38556;&#30861;&#12290;&#31532;&#19968;&#20010;&#27169;&#22359;&#36890;&#36807;&#26597;&#35810;&#25193;&#23637;&#22686;&#24378;&#20102;&#22312;&#31232;&#30095;&#26816;&#32034;&#20013;&#30340;&#35821;&#35328;&#29702;&#35299;&#65292;&#20197;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#12290;&#31532;&#20108;&#20010;&#27169;&#22359;&#36890;&#36807;&#21482;&#20351;&#29992;&#38271;&#25991;&#26723;&#20013;&#20256;&#25773;&#30340;&#20449;&#24687;&#65292;&#20026;&#22797;&#26434;&#38382;&#39064;&#25552;&#20379;&#20840;&#38754;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#31572;&#26696;&#26469;&#21152;&#28145;&#32467;&#26524;&#65292;&#23454;&#29616;&#21452;&#21521;&#20132;&#20114;&#12290;&#22312;&#31649;&#36947;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#21521;&#29992;&#25143;&#21576;&#29616;&#20013;&#38388;&#32467;&#26524;&#20197;&#20419;&#36827;&#23545;&#31995;&#32479;&#25512;&#29702;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#31181;&#21452;&#21521;&#26041;&#27861;&#24102;&#26469;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13897v1 Announce Type: cross  Abstract: Information retrieval is a rapidly evolving field. However it still faces significant limitations in the scientific and industrial vast amounts of information, such as semantic divergence and vocabulary gaps in sparse retrieval, low precision and lack of interpretability in semantic search, or hallucination and outdated information in generative models. In this paper, we introduce a two-block approach to tackle these hurdles for long documents. The first block enhances language understanding in sparse retrieval by query expansion to retrieve relevant documents. The second block deepens the result by providing comprehensive and informative answers to the complex question using only the information spread in the long document, enabling bidirectional engagement. At various stages of the pipeline, intermediate results are presented to users to facilitate understanding of the system's reasoning. We believe this bidirectional approach brings
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#39033;&#36873;&#25321;&#39064;&#26102;&#65292;&#22522;&#20110;&#27010;&#29575;&#30340;&#35780;&#20272;&#26041;&#27861;&#19982;&#22522;&#20110;&#29983;&#25104;&#30340;&#39044;&#27979;&#19981;&#30456;&#21563;&#21512;&#30340;&#22266;&#26377;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13887</link><description>&lt;p&gt;
&#36229;&#36234;&#27010;&#29575;&#65306;&#25581;&#31034;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38169;&#20301;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#39033;&#36873;&#25321;&#39064;&#26102;&#65292;&#22522;&#20110;&#27010;&#29575;&#30340;&#35780;&#20272;&#26041;&#27861;&#19982;&#22522;&#20110;&#29983;&#25104;&#30340;&#39044;&#27979;&#19981;&#30456;&#21563;&#21512;&#30340;&#22266;&#26377;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#30340;&#26684;&#23616;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#35780;&#20272;&#26694;&#26550;&#36890;&#24120;&#20381;&#36182;&#20110;LLMs&#30340;&#36755;&#20986;&#27010;&#29575;&#36827;&#34892;&#39044;&#27979;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#35745;&#31639;&#32422;&#26463;&#65292;&#20559;&#31163;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;LLMs&#20351;&#29992;&#22330;&#26223;&#12290;&#34429;&#28982;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#22522;&#20110;&#27010;&#29575;&#30340;&#35780;&#20272;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#20173;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#23457;&#26597;&#36825;&#31181;&#22522;&#20110;&#27010;&#29575;&#30340;&#35780;&#20272;&#26041;&#27861;&#22312;&#20351;&#29992;LLMs&#36827;&#34892;&#22810;&#39033;&#36873;&#25321;&#39064;&#65288;MCQs&#65289;&#26102;&#30340;&#26377;&#25928;&#24615;&#65292;&#31361;&#26174;&#20854;&#22266;&#26377;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35843;&#26597;&#26174;&#31034;&#65292;&#26222;&#36941;&#30340;&#22522;&#20110;&#27010;&#29575;&#30340;&#35780;&#20272;&#26041;&#27861;&#26410;&#33021;&#19982;&#22522;&#20110;&#29983;&#25104;&#30340;&#39044;&#27979;&#30456;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#26694;&#26550;&#36890;&#24120;&#36890;&#36807;&#22522;&#20110;&#36755;&#20986;&#39044;&#27979;&#30340;&#39044;&#27979;&#20219;&#21153;&#26469;&#35780;&#20272;LLMs
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13887v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various applications, fundamentally reshaping the landscape of natural language processing (NLP) research. However, recent evaluation frameworks often rely on the output probabilities of LLMs for predictions, primarily due to computational constraints, diverging from real-world LLM usage scenarios. While widely employed, the efficacy of these probability-based evaluation strategies remains an open research question. This study aims to scrutinize the validity of such probability-based evaluation methods within the context of using LLMs for Multiple Choice Questions (MCQs), highlighting their inherent limitations. Our empirical investigation reveals that the prevalent probability-based evaluation method inadequately aligns with generation-based prediction. Furthermore, current evaluation frameworks typically assess LLMs through predictive tasks based on output pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;$\texttt{Se}^2$&#65292;&#19968;&#31181;&#39034;&#24207;&#24863;&#30693;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#24110;&#21161;&#25429;&#25417;&#31034;&#20363;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#21644;&#24207;&#21015;&#20449;&#24687;&#65292;&#26174;&#33879;&#20016;&#23500;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#31034;&#30340;&#30456;&#20851;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13874</link><description>&lt;p&gt;
$\texttt{Se}^2$: $\textit{Se}$quential Example $\textit{Se}$lection for In-Context Learning
&lt;/p&gt;
&lt;p&gt;
$\texttt{Se}^2$: $\textit{Se}$quential Example $\textit{Se}$lection for In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;$\texttt{Se}^2$&#65292;&#19968;&#31181;&#39034;&#24207;&#24863;&#30693;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#24110;&#21161;&#25429;&#25417;&#31034;&#20363;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#21644;&#24207;&#21015;&#20449;&#24687;&#65292;&#26174;&#33879;&#20016;&#23500;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#31034;&#30340;&#30456;&#20851;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#20294;&#38656;&#35201;&#36890;&#36807;&#31034;&#20363;&#31034;&#33539;&#26469;&#28608;&#27963;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#24191;&#27867;&#25506;&#35752;&#20102;&#29992;&#20110;ICL&#30340;&#31034;&#20363;&#36873;&#25321;&#65292;&#20027;&#35201;&#36981;&#24490;&#8220;&#20808;&#36873;&#25321;&#20877;&#32452;&#32455;&#8221;&#30340;&#33539;&#24335;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#31034;&#20363;&#20043;&#38388;&#30340;&#20869;&#22312;&#20851;&#31995;&#65292;&#23384;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#23558;&#38382;&#39064;&#34920;&#36848;&#20026;&#19968;&#20010;&#24207;&#36143;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;$\texttt{Se}^2$&#65292;&#36825;&#26159;&#19968;&#31181;&#39034;&#24207;&#24863;&#30693;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#30340;&#21453;&#39304;&#65292;&#26377;&#21161;&#20110;&#25429;&#25417;&#31034;&#20363;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#21644;&#24207;&#21015;&#20449;&#24687;&#65292;&#26174;&#33879;&#20016;&#23500;ICL&#25552;&#31034;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#21033;&#29992;&#26463;&#25628;&#32034;&#26469;&#23547;&#25214;&#21644;&#26500;&#24314;&#31034;&#20363;&#24207;&#21015;&#65292;&#22686;&#24378;&#20102;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#22312;8&#20010;&#19981;&#21516;&#31867;&#21035;&#20013;&#30340;23&#20010;NLP&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13874v1 Announce Type: new  Abstract: The remarkable capability of large language models (LLMs) for in-context learning (ICL) needs to be activated by demonstration examples. Prior work has extensively explored the selection of examples for ICL, predominantly following the "select then organize" paradigm, such approaches often neglect the internal relationships between examples and exist an inconsistency between the training and inference. In this paper, we formulate the problem as a $\textit{se}$quential $\textit{se}$lection problem and introduce $\texttt{Se}^2$, a sequential-aware method that leverages the LLM's feedback on varying context, aiding in capturing inter-relationships and sequential information among examples, significantly enriching the contextuality and relevance of ICL prompts. Meanwhile, we utilize beam search to seek and construct example sequences, enhancing both quality and diversity. Extensive experiments across 23 NLP tasks from 8 distinct categories i
&lt;/p&gt;</description></item><item><title>Kuaiji&#26159;&#31532;&#19968;&#20010;&#20013;&#22269;&#20250;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;Baichuan&#26694;&#26550;&#31934;&#24515;&#35843;&#25972;&#65292;&#25903;&#25345;&#30340;CAtAcctQA&#25968;&#25454;&#38598;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#21709;&#24212;&#36895;&#24230;&#65292;&#20855;&#26377;&#24320;&#21019;&#24615;&#22320;&#21019;&#24314;&#20102;&#20013;&#22269;&#20250;&#35745;&#25968;&#25454;&#38598;&#65292;&#24182;&#35777;&#23454;&#20102;&#22312;&#30495;&#23454;&#20250;&#35745;&#22330;&#26223;&#20013;&#30340;&#39640;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13866</link><description>&lt;p&gt;
Kuaiji&#65306;&#31532;&#19968;&#20010;&#20013;&#22269;&#20250;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Kuaiji: the First Chinese Accounting Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13866
&lt;/p&gt;
&lt;p&gt;
Kuaiji&#26159;&#31532;&#19968;&#20010;&#20013;&#22269;&#20250;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;Baichuan&#26694;&#26550;&#31934;&#24515;&#35843;&#25972;&#65292;&#25903;&#25345;&#30340;CAtAcctQA&#25968;&#25454;&#38598;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#21709;&#24212;&#36895;&#24230;&#65292;&#20855;&#26377;&#24320;&#21019;&#24615;&#22320;&#21019;&#24314;&#20102;&#20013;&#22269;&#20250;&#35745;&#25968;&#25454;&#38598;&#65292;&#24182;&#35777;&#23454;&#20102;&#22312;&#30495;&#23454;&#20250;&#35745;&#22330;&#26223;&#20013;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#21644;GPT-4&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#20219;&#21153;&#35201;&#27714;&#36866;&#24212;&#20250;&#35745;&#31561;&#19987;&#19994;&#39046;&#22495;&#26102;&#65292;&#23427;&#20204;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Kuaiji&#65292;&#19968;&#20010;&#19987;&#38376;&#23450;&#21046;&#30340;&#20250;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;Kuaiji&#32463;&#36807;&#31934;&#24515;&#35843;&#25972;&#65292;&#20351;&#29992;&#21253;&#21547;&#36830;&#32493;&#39044;&#35757;&#32451;&#21644;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#30340;Baichuan&#26694;&#26550;&#12290;&#22312;CAtAcctQA&#30340;&#25903;&#25345;&#19979;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#30495;&#23454;&#20250;&#35745;&#24072;&#19982;&#23458;&#25143;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#65292;Kuaiji&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#21709;&#24212;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#21019;&#24314;&#20102;&#31532;&#19968;&#20010;&#20013;&#22269;&#20250;&#35745;&#25968;&#25454;&#38598;&#65292;&#23558;Kuaiji&#24314;&#31435;&#20026;&#19968;&#31181;&#39046;&#20808;&#30340;&#24320;&#28304;&#20013;&#22269;&#20250;&#35745;LLM&#65292;&#24182;&#36890;&#36807;&#30495;&#23454;&#20250;&#35745;&#22330;&#26223;&#23545;&#20854;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13866v1 Announce Type: cross  Abstract: Large Language Models (LLMs) like ChatGPT and GPT-4 have demonstrated impressive proficiency in comprehending and generating natural language. However, they encounter difficulties when tasked with adapting to specialized domains such as accounting. To address this challenge, we introduce Kuaiji, a tailored Accounting Large Language Model. Kuaiji is meticulously fine-tuned using the Baichuan framework, which encompasses continuous pre-training and supervised fine-tuning processes. Supported by CAtAcctQA, a dataset containing large genuine accountant-client dialogues, Kuaiji exhibits exceptional accuracy and response speed. Our contributions encompass the creation of the first Chinese accounting dataset, the establishment of Kuaiji as a leading open-source Chinese accounting LLM, and the validation of its efficacy through real-world accounting scenarios.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20445;&#25252;&#20010;&#20154;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#25239;&#24615;LLM&#25512;&#26029;&#30340;&#21311;&#21517;&#21270;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.13846</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#20808;&#36827;&#30340;&#21311;&#21517;&#21270;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Advanced Anonymizers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13846
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20445;&#25252;&#20010;&#20154;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#25239;&#24615;LLM&#25512;&#26029;&#30340;&#21311;&#21517;&#21270;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#38544;&#31169;&#30740;&#31350;&#39046;&#22495;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#20204;&#22312;&#25512;&#26029;&#30495;&#23454;&#19990;&#30028;&#22312;&#32447;&#25991;&#26412;&#20013;&#30340;&#20010;&#20154;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#38543;&#30528;&#27169;&#22411;&#33021;&#21147;&#30340;&#19981;&#26029;&#22686;&#24378;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#21311;&#21517;&#21270;&#26041;&#27861;&#24403;&#21069;&#24050;&#32463;&#33853;&#21518;&#20110;&#30417;&#31649;&#35201;&#27714;&#21644;&#23545;&#25239;&#23041;&#32961;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#20010;&#20154;&#22914;&#20309;&#26377;&#25928;&#22320;&#20445;&#25252;&#20182;&#20204;&#22312;&#20998;&#20139;&#22312;&#32447;&#25991;&#26412;&#26102;&#30340;&#20010;&#20154;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#20004;&#27493;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35774;&#32622;&#65292;&#29992;&#20110;&#35780;&#20272;&#38754;&#23545;&#23545;&#25239;&#24615;LLM&#30340;&#25512;&#26029;&#26102;&#30340;&#21311;&#21517;&#21270;&#25928;&#26524;&#65292;&#20174;&#32780;&#20801;&#35768;&#33258;&#28982;&#22320;&#27979;&#37327;&#21311;&#21517;&#21270;&#24615;&#33021;&#65292;&#21516;&#26102;&#32416;&#27491;&#20102;&#20197;&#21069;&#25351;&#26631;&#30340;&#19968;&#20123;&#32570;&#38519;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;LLM&#30340;&#23545;&#25239;&#24615;&#21311;&#21517;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;LLM&#30340;&#24378;&#22823;&#25512;&#26029;&#33021;&#21147;&#26469;&#25351;&#23548;&#25105;&#20204;&#30340;&#21311;&#21517;&#21270;&#36807;&#31243;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#21311;&#21517;&#21270;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13846v1 Announce Type: cross  Abstract: Recent work in privacy research on large language models has shown that they achieve near human-level performance at inferring personal data from real-world online texts. With consistently increasing model capabilities, existing text anonymization methods are currently lacking behind regulatory requirements and adversarial threats. This raises the question of how individuals can effectively protect their personal data in sharing online texts. In this work, we take two steps to answer this question: We first present a new setting for evaluating anonymizations in the face of adversarial LLMs inferences, allowing for a natural measurement of anonymization performance while remedying some of the shortcomings of previous metrics. We then present our LLM-based adversarial anonymization framework leveraging the strong inferential capabilities of LLMs to inform our anonymization procedure. In our experimental evaluation, we show on real-world 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;NLP&#27169;&#22411;&#22312;&#35782;&#21035;&#36140;&#20302;&#24615;&#35821;&#35328;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23427;&#20204;&#33021;&#22815;&#20197;70%&#30340;&#20934;&#30830;&#29575;&#21306;&#20998;&#36140;&#20302;&#24615;&#35821;&#35328;&#21644;&#26356;&#24191;&#27867;&#30340;&#20167;&#24680;&#35328;&#35770;&#65292;&#20294;&#20063;&#23384;&#22312;&#30528;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.13818</link><description>&lt;p&gt;
&#36229;&#36234;&#20167;&#24680;&#35328;&#35770;: &#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#21457;&#29616;&#36140;&#20302;&#24615;&#35821;&#35328;&#20013;&#30340;&#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Beyond Hate Speech: NLP's Challenges and Opportunities in Uncovering Dehumanizing Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;NLP&#27169;&#22411;&#22312;&#35782;&#21035;&#36140;&#20302;&#24615;&#35821;&#35328;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23427;&#20204;&#33021;&#22815;&#20197;70%&#30340;&#20934;&#30830;&#29575;&#21306;&#20998;&#36140;&#20302;&#24615;&#35821;&#35328;&#21644;&#26356;&#24191;&#27867;&#30340;&#20167;&#24680;&#35328;&#35770;&#65292;&#20294;&#20063;&#23384;&#22312;&#30528;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#36523;&#20855;&#35937;&#21270;&#34987;&#23450;&#20041;&#20026;&#20167;&#24680;&#35328;&#35770;&#30340;&#19968;&#31181;&#24494;&#22937;&#20294;&#26377;&#23475;&#30340;&#34920;&#29616;&#24418;&#24335;&#65292;&#28041;&#21450;&#21542;&#35748;&#20010;&#20154;&#30340;&#20154;&#31867;&#29305;&#36136;&#65292;&#36890;&#24120;&#23548;&#33268;&#23545;&#36793;&#32536;&#32676;&#20307;&#30340;&#26292;&#21147;&#34892;&#20026;&#12290;&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20854;&#22312;&#26816;&#27979;&#36140;&#20302;&#24615;&#35328;&#35821;&#26041;&#38754;&#30340;&#24212;&#29992;&#26377;&#38480;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#36825;&#19968;&#39046;&#22495;&#20844;&#24320;&#21487;&#29992;&#30340;&#24102;&#26631;&#31614;&#25968;&#25454;&#31232;&#32570;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;NLP&#27169;&#22411;&#65288;&#21253;&#25324;GPT-4&#12289;GPT-3.5&#21644;LLAMA-2&#65289;&#22312;&#35782;&#21035;&#36140;&#20302;&#24615;&#35821;&#35328;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26174;&#31034;&#65292;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#28508;&#21147;&#65292;&#36798;&#21040;&#20102;70%&#30340;&#20934;&#30830;&#29575;&#26469;&#21306;&#20998;&#36140;&#20302;&#24615;&#35328;&#35821;&#21644;&#26356;&#24191;&#27867;&#30340;&#20167;&#24680;&#35328;&#35770;&#65292;&#20294;&#23427;&#20204;&#20063;&#26174;&#31034;&#20986;&#20559;&#35265;&#12290;&#23427;&#20204;&#22312;&#23545;&#20854;&#20182;&#24418;&#24335;&#30340;&#20167;&#24680;&#35328;&#35770;&#36827;&#34892;&#20998;&#31867;&#26102;&#36807;&#20110;&#25935;&#24863;&#65292;&#23558;&#20854;&#35823;&#21028;&#20026;&#29305;&#23450;&#30446;&#26631;&#32676;&#20307;&#30340;&#20154;&#36523;&#20855;&#35937;&#21270;&#65292;&#21516;&#26102;&#26356;&#39057;&#32321;&#22320;&#26410;&#33021;&#35782;&#21035;&#26126;&#26174;&#30340;&#20154;&#36523;&#20855;&#35937;&#21270;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13818v1 Announce Type: new  Abstract: Dehumanization, characterized as a subtle yet harmful manifestation of hate speech, involves denying individuals of their human qualities and often results in violence against marginalized groups. Despite significant progress in Natural Language Processing across various domains, its application in detecting dehumanizing language is limited, largely due to the scarcity of publicly available annotated data for this domain. This paper evaluates the performance of cutting-edge NLP models, including GPT-4, GPT-3.5, and LLAMA-2, in identifying dehumanizing language. Our findings reveal that while these models demonstrate potential, achieving a 70\% accuracy rate in distinguishing dehumanizing language from broader hate speech, they also display biases. They are over-sensitive in classifying other forms of hate speech as dehumanization for a specific subset of target groups, while more frequently failing to identify clear cases of dehumanizati
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20998;&#26512;&#20102;&#20851;&#20110;&#27431;&#27954;&#21644;&#31227;&#27665;&#30340;&#22312;&#32447;&#20449;&#24687;&#20256;&#25773;&#65292;&#24341;&#20837;&#20102;&#20855;&#26377;&#22320;&#29702;&#32852;&#31995;&#30340;&#28909;&#38376;&#35805;&#39064;&#12289;&#35268;&#27169;&#21644;&#21160;&#24577;&#20256;&#25773;&#30340;&#26032;&#35270;&#35282;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#35821;&#24341;&#35821;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.13800</link><description>&lt;p&gt;
&#32447;&#19978;&#35752;&#35770;&#20013;&#20851;&#20110;&#27431;&#27954;&#21644;&#31227;&#27665;&#20449;&#24687;&#20256;&#25773;&#30340;&#22320;&#29702;&#23398;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Geography of Information Diffusion in Online Discourse on Europe and Migration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13800
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20998;&#26512;&#20102;&#20851;&#20110;&#27431;&#27954;&#21644;&#31227;&#27665;&#30340;&#22312;&#32447;&#20449;&#24687;&#20256;&#25773;&#65292;&#24341;&#20837;&#20102;&#20855;&#26377;&#22320;&#29702;&#32852;&#31995;&#30340;&#28909;&#38376;&#35805;&#39064;&#12289;&#35268;&#27169;&#21644;&#21160;&#24577;&#20256;&#25773;&#30340;&#26032;&#35270;&#35282;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#35821;&#24341;&#35821;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#27431;&#27954;&#21644;&#31227;&#27665;&#30456;&#20851;&#30340;&#20449;&#24687;&#22312;&#32447;&#20256;&#25773;&#24456;&#23569;&#21463;&#21040;&#22806;&#37096;&#35270;&#35282;&#30340;&#35843;&#26597;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#30456;&#20851;&#30340;&#20027;&#39064;&#65292;&#29305;&#21035;&#26159;&#22914;&#26524;&#29992;&#25143;&#27809;&#26377;&#30452;&#25509;&#25509;&#35302;&#36807;&#27431;&#27954;&#65292;&#20854;&#23545;&#27431;&#27954;&#30340;&#30475;&#27861;&#23436;&#20840;&#21462;&#20915;&#20110;&#22312;&#32447;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20174;&#31038;&#20132;&#23186;&#20307;&#65288;Twitter&#65289;&#20013;&#26816;&#32034;&#30340;&#22823;&#37327;&#25968;&#25454;&#21518;&#65292;&#20851;&#20110;&#27431;&#27954;&#21644;&#31227;&#27665;&#22312;&#32447;&#27969;&#36890;&#30340;&#20449;&#24687;&#65292;&#20197;&#33719;&#24471;&#20851;&#20110;&#35805;&#39064;&#12289;&#35268;&#27169;&#21644;&#20256;&#25773;&#21160;&#24577;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#23558;&#36716;&#21457;&#21644;&#26631;&#31614;&#32593;&#32476;&#20998;&#26512;&#19982;&#29992;&#25143;&#22320;&#29702;&#20301;&#32622;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#23558;&#25968;&#25454;&#19982;&#22320;&#29702;&#20301;&#32622;&#32852;&#31995;&#36215;&#26469;&#65292;&#20801;&#35768;&#20174;&#8220;&#27431;&#27954;&#22806;&#37096;&#8221;&#35270;&#35282;&#36827;&#34892;&#20998;&#26512;&#65292;&#29305;&#21035;&#20851;&#27880;&#38750;&#27954;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#35821;&#24341;&#35821;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21363;&#24403;&#19968;&#31181;&#35821;&#35328;&#30340;&#20869;&#23481;&#34987;&#21478;&#19968;&#31181;&#35821;&#35328;&#35780;&#35770;&#21644;&#36716;&#21457;&#26102;&#65292;&#20551;&#35774;&#36825;&#20123;&#20114;&#21160;&#26159;&#36828;&#36317;&#31163;&#36830;&#25509;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13800v1 Announce Type: new  Abstract: The online diffusion of information related to Europe and migration has been little investigated from an external point of view. However, this is a very relevant topic, especially if users have had no direct contact with Europe and its perception depends solely on information retrieved online. In this work we analyse the information circulating online about Europe and migration after retrieving a large amount of data from social media (Twitter), to gain new insights into topics, magnitude, and dynamics of their diffusion. We combine retweets and hashtags network analysis with geolocation of users, linking thus data to geography and allowing analysis from an "outside Europe" perspective, with a special focus on Africa. We also introduce a novel approach based on cross-lingual quotes, i.e. when content in a language is commented and retweeted in another language, assuming these interactions are a proxy for connections between very distant 
&lt;/p&gt;</description></item><item><title>CriticBench&#26159;&#19968;&#20010;&#26088;&#22312;&#20840;&#38754;&#21644;&#21487;&#38752;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#35770;&#33021;&#21147;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#23637;&#31034;&#20102;&#35780;&#35770;&#33021;&#21147;&#19982;&#20219;&#21153;&#12289;&#21709;&#24212;&#36136;&#37327;&#21644;&#27169;&#22411;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.13764</link><description>&lt;p&gt;
CriticBench: &#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35780;&#35770;&#23478;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
CriticBench: Evaluating Large Language Models as Critic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13764
&lt;/p&gt;
&lt;p&gt;
CriticBench&#26159;&#19968;&#20010;&#26088;&#22312;&#20840;&#38754;&#21644;&#21487;&#38752;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#35770;&#33021;&#21147;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#23637;&#31034;&#20102;&#35780;&#35770;&#33021;&#21147;&#19982;&#20219;&#21153;&#12289;&#21709;&#24212;&#36136;&#37327;&#21644;&#27169;&#22411;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102; CriticBench&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#20840;&#38754;&#21644;&#21487;&#38752;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22235;&#20010;&#20851;&#38190;&#35780;&#35770;&#33021;&#21147;&#32500;&#24230;&#65288;&#21453;&#39304;&#12289;&#27604;&#36739;&#12289;&#25913;&#36827;&#21644;&#20803;&#21453;&#39304;&#65289;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;CriticBench&#21253;&#21547;&#20061;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#27599;&#20010;&#20219;&#21153;&#35780;&#20272;LLMs&#22312;&#19981;&#21516;&#36136;&#37327;&#32454;&#31890;&#24230;&#27700;&#24179;&#19978;&#35780;&#35770;&#21709;&#24212;&#30340;&#33021;&#21147;&#12290;&#23545;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#25581;&#31034;&#20102;&#35780;&#35770;&#33021;&#21147;&#19982;&#20219;&#21153;&#12289;&#21709;&#24212;&#36136;&#37327;&#21644;&#27169;&#22411;&#35268;&#27169;&#20043;&#38388;&#26377;&#36259;&#30340;&#20851;&#31995;&#12290;CriticBench&#30340;&#25968;&#25454;&#38598;&#12289;&#36164;&#28304;&#21644;&#35780;&#20272;&#24037;&#20855;&#21253;&#23558;&#22312;https://github.com/gmftbyGMFTBY/Cri&#19978;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13764v1 Announce Type: cross  Abstract: Critique ability are crucial in the scalable oversight and self-improvement of Large Language Models (LLMs). While many recent studies explore the critique ability of LLMs to judge and refine flaws in generations, how to comprehensively and reliably measure the critique abilities of LLMs is under-explored. This paper introduces \shortname, a novel benchmark designed to comprehensively and reliably evaluate four key critique ability dimensions of LLMs: feedback, comparison, refinement and meta-feedback. \shortname~encompasses nine diverse tasks, each assessing the LLMs' ability to critique responses at varying levels of quality granularity. Our extensive evaluations of open-source and closed-source LLMs reveal intriguing relationships between the critique ability and tasks, response qualities, and model scales. Datasets, resources and evaluation toolkit for \shortname~will be publicly released at \url{https://github.com/gmftbyGMFTBY/Cri
&lt;/p&gt;</description></item><item><title>&#22312;&#25688;&#35201;&#19968;&#33268;&#24615;&#35780;&#20272;&#26041;&#38754;&#65292;&#35813;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20020;&#24202;&#25991;&#26412;&#25688;&#35201;&#30340;&#25968;&#25454;&#38598;TreatFact&#24182;&#23545;11&#20010;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#22635;&#34917;&#20102;&#20851;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25688;&#35201;&#20107;&#23454;&#19968;&#33268;&#24615;&#35780;&#20272;&#26041;&#38754;&#30340;&#32570;&#21475;&#12290;</title><link>https://arxiv.org/abs/2402.13758</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#20013;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Factual Consistency Evaluation of Summarisation in the Era of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13758
&lt;/p&gt;
&lt;p&gt;
&#22312;&#25688;&#35201;&#19968;&#33268;&#24615;&#35780;&#20272;&#26041;&#38754;&#65292;&#35813;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20020;&#24202;&#25991;&#26412;&#25688;&#35201;&#30340;&#25968;&#25454;&#38598;TreatFact&#24182;&#23545;11&#20010;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#22635;&#34917;&#20102;&#20851;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25688;&#35201;&#20107;&#23454;&#19968;&#33268;&#24615;&#35780;&#20272;&#26041;&#38754;&#30340;&#32570;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#29983;&#25104;&#25688;&#35201;&#20013;&#19982;&#28304;&#25991;&#20214;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#20449;&#24687;&#25110;&#24102;&#26469;&#39118;&#38505;&#12290;&#29616;&#26377;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#65288;FC&#65289;&#24230;&#37327;&#21463;&#21040;&#20854;&#24615;&#33021;&#12289;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#38480;&#21046;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#25991;&#26412;&#35780;&#20272;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#35780;&#20272;&#25688;&#35201;&#20013;&#30340;FC&#26041;&#38754;&#30340;&#25928;&#26524;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#19987;&#26377;LLMs&#19978;&#65292;&#26410;&#25506;&#35752;&#24433;&#21709;&#23427;&#20204;&#35780;&#20272;&#33021;&#21147;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;FC&#35780;&#20272;&#22522;&#20934;&#20165;&#38480;&#20110;&#26032;&#38395;&#25991;&#31456;&#65292;&#23545;&#22312;&#20854;&#19978;&#27979;&#35797;&#30340;FC&#26041;&#27861;&#30340;&#26222;&#36941;&#24615;&#20135;&#29983;&#24576;&#30097;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#24341;&#20837;TreatFact&#25968;&#25454;&#38598;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#30001;&#39046;&#22495;&#19987;&#23478;&#27880;&#37322;&#30340;&#20020;&#24202;&#25991;&#26412;&#30340;LLM&#29983;&#25104;&#25688;&#35201;&#30340;FC&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#26032;&#38395;&#21644;&#20020;&#24202;&#39046;&#22495;&#20013;&#20026;FC&#35780;&#20272;&#23545;&#27604;&#20102;11&#20010;LLMs&#65292;&#24182;&#20998;&#26512;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13758v1 Announce Type: new  Abstract: Factual inconsistency with source documents in automatically generated summaries can lead to misinformation or pose risks. Existing factual consistency(FC) metrics are constrained by their performance, efficiency, and explainability. Recent advances in Large language models (LLMs) have demonstrated remarkable potential in text evaluation but their effectiveness in assessing FC in summarisation remains underexplored. Prior research has mostly focused on proprietary LLMs, leaving essential factors that affect their assessment capabilities unexplored. Additionally, current FC evaluation benchmarks are restricted to news articles, casting doubt on the generality of the FC methods tested on them. In this paper, we first address the gap by introducing TreatFact a dataset of LLM-generated summaries of clinical texts, annotated for FC by domain experts. Moreover, we benchmark 11 LLMs for FC evaluation across news and clinical domains and analyse
&lt;/p&gt;</description></item><item><title>LongRoPE&#39318;&#27425;&#23558;&#39044;&#35757;&#32451;&#30340;LLM&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#33267;2048k&#20010;&#26631;&#35760;&#65292;&#36890;&#36807;&#20851;&#38190;&#21019;&#26032;&#23454;&#29616;&#20102;&#36825;&#19968;&#31361;&#30772;&#12290;</title><link>https://arxiv.org/abs/2402.13753</link><description>&lt;p&gt;
&#23558;LLM&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;&#36229;&#36807;2&#30334;&#19975;&#20010;&#26631;&#35760;&#30340;LongRoPE
&lt;/p&gt;
&lt;p&gt;
LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13753
&lt;/p&gt;
&lt;p&gt;
LongRoPE&#39318;&#27425;&#23558;&#39044;&#35757;&#32451;&#30340;LLM&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#33267;2048k&#20010;&#26631;&#35760;&#65292;&#36890;&#36807;&#20851;&#38190;&#21019;&#26032;&#23454;&#29616;&#20102;&#36825;&#19968;&#31361;&#30772;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#19978;&#19979;&#25991;&#31383;&#21475;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#19968;&#20010;&#29702;&#24819;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#26114;&#30340;&#24494;&#35843;&#25104;&#26412;&#12289;&#38271;&#25991;&#26412;&#31232;&#32570;&#20197;&#21450;&#26032;&#26631;&#35760;&#20301;&#32622;&#24341;&#20837;&#30340;&#28798;&#38590;&#24615;&#20540;&#65292;&#24403;&#21069;&#30340;&#25193;&#23637;&#19978;&#19979;&#25991;&#31383;&#21475;&#20165;&#38480;&#20110;&#32422;128k&#20010;&#26631;&#35760;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LongRoPE&#65292;&#39318;&#27425;&#23558;&#39044;&#35757;&#32451;&#30340;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#33267;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;2048k&#20010;&#26631;&#35760;&#65292;&#36890;&#36807;&#20165;&#22312;256k&#35757;&#32451;&#38271;&#24230;&#20869;&#26368;&#22810;&#36827;&#34892;1k&#27425;&#24494;&#35843;&#27493;&#39588;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;&#21407;&#22987;&#30701;&#19978;&#19979;&#25991;&#31383;&#21475;&#19979;&#30340;&#24615;&#33021;&#12290;&#36825;&#26159;&#36890;&#36807;&#19977;&#39033;&#20851;&#38190;&#21019;&#26032;&#23454;&#29616;&#30340;&#65306;(i) &#25105;&#20204;&#35782;&#21035;&#24182;&#21033;&#29992;&#20102;&#20004;&#31181;&#24418;&#24335;&#30340;&#20301;&#32622;&#25554;&#20540;&#20013;&#30340;&#38750;&#22343;&#21248;&#24615;&#65292;&#36890;&#36807;&#39640;&#25928;&#25628;&#32034;&#25552;&#20379;&#26356;&#22909;&#30340;&#24494;&#35843;&#21021;&#22987;&#21270;&#65292;&#24182;&#22312;&#38750;&#24494;&#35843;&#22330;&#26223;&#19979;&#23454;&#29616;8&#20493;&#25193;&#23637;&#65307;(ii) &#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36880;&#27493;&#25193;&#23637;&#31574;&#30053;&#65292;&#39318;&#20808;&#24494;&#35843;256k&#38271;&#24230;&#30340;LLM&#65292;&#28982;&#21518;&#36827;&#34892;&#31532;&#20108;&#27425;&#20301;&#32622;&#25554;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13753v1 Announce Type: new  Abstract: Large context window is a desirable feature in large language models (LLMs). However, due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens. This paper introduces LongRoPE that, for the first time, extends the context window of pre-trained LLMs to an impressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k training lengths, while maintaining performance at the original short context window. This is achieved by three key innovations: (i) we identify and exploit two forms of non-uniformities in positional interpolation through an efficient search, providing a better initialization for fine-tuning and enabling an 8x extension in non-fine-tuning scenarios; (ii) we introduce a progressive extension strategy that first fine-tunes a 256k length LLM and then conducts a second positional interpolation 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20114;&#34917;&#30693;&#35782;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#65288;LLM-KERec&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#23454;&#20307;&#25552;&#21462;&#22120;&#21644;&#26500;&#24314;&#20114;&#34917;&#30693;&#35782;&#22270;&#65292;&#35299;&#20915;&#20102;&#25512;&#33616;&#31995;&#32479;&#38590;&#20197;&#25429;&#25417;&#29992;&#25143;&#24847;&#22270;&#36716;&#21464;&#21644;&#36866;&#24212;&#26032;&#21830;&#21697;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.13750</link><description>&lt;p&gt;
&#25171;&#30772;&#38556;&#30861;&#65306;&#36890;&#36807;&#25512;&#29702;&#30693;&#35782;&#22270;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24037;&#19994;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Breaking the Barrier: Utilizing Large Language Models for Industrial Recommendation Systems through an Inferential Knowledge Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13750
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20114;&#34917;&#30693;&#35782;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#65288;LLM-KERec&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#23454;&#20307;&#25552;&#21462;&#22120;&#21644;&#26500;&#24314;&#20114;&#34917;&#30693;&#35782;&#22270;&#65292;&#35299;&#20915;&#20102;&#25512;&#33616;&#31995;&#32479;&#38590;&#20197;&#25429;&#25417;&#29992;&#25143;&#24847;&#22270;&#36716;&#21464;&#21644;&#36866;&#24212;&#26032;&#21830;&#21697;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#21644;&#22312;&#32447;&#24179;&#21488;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20197;&#24212;&#23545;&#20449;&#24687;&#36807;&#36733;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31995;&#32479;&#20027;&#35201;&#20381;&#36182;&#21382;&#21490;&#25968;&#25454;&#21644;&#29992;&#25143;&#21453;&#39304;&#65292;&#38590;&#20197;&#25429;&#25417;&#29992;&#25143;&#24847;&#22270;&#36716;&#21464;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#30340;&#27169;&#22411;&#26469;&#25972;&#21512;&#19987;&#23478;&#30693;&#35782;&#65292;&#20294;&#23427;&#20204;&#38590;&#20197;&#36866;&#24212;&#26032;&#21830;&#21697;&#21644;&#19981;&#26029;&#21457;&#23637;&#30340;&#30005;&#23376;&#21830;&#21153;&#29615;&#22659;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20114;&#34917;&#30693;&#35782;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#65288;LLM-KERec&#65289;&#12290;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#23454;&#20307;&#25552;&#21462;&#22120;&#65292;&#20174;&#21830;&#21697;&#21644;&#29992;&#25143;&#20449;&#24687;&#20013;&#25552;&#21462;&#32479;&#19968;&#27010;&#24565;&#26415;&#35821;&#12290;&#20026;&#20102;&#25552;&#20379;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#19988;&#21487;&#38752;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#26681;&#25454;&#23454;&#20307;&#30340;&#27969;&#34892;&#24230;&#21644;&#29305;&#23450;&#31574;&#30053;&#29983;&#25104;&#23454;&#20307;&#23545;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30830;&#23450;&#27599;&#20010;&#23454;&#20307;&#23545;&#20013;&#30340;&#20114;&#34917;&#20851;&#31995;&#65292;&#26500;&#24314;&#19968;&#20010;&#20114;&#34917;&#30693;&#35782;&#22270;&#12290;&#27492;&#22806;&#65292;&#19968;&#20010;&#26032;&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13750v1 Announce Type: cross  Abstract: Recommendation systems are widely used in e-commerce websites and online platforms to address information overload. However, existing systems primarily rely on historical data and user feedback, making it difficult to capture user intent transitions. Recently, Knowledge Base (KB)-based models are proposed to incorporate expert knowledge, but it struggle to adapt to new items and the evolving e-commerce environment. To address these challenges, we propose a novel Large Language Model based Complementary Knowledge Enhanced Recommendation System (LLM-KERec). It introduces an entity extractor that extracts unified concept terms from item and user information. To provide cost-effective and reliable prior knowledge, entity pairs are generated based on entity popularity and specific strategies. The large language model determines complementary relationships in each entity pair, constructing a complementary knowledge graph. Furthermore, a new 
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#34920;&#26684;&#25552;&#31034;&#20197;&#35299;&#20915;&#20851;&#31995;&#19977;&#20803;&#32452;&#25277;&#21462;&#20013;&#30340;&#25552;&#31034;&#35774;&#35745;&#21644;&#26679;&#26412;&#36873;&#25321;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.13741</link><description>&lt;p&gt;
&#20351;&#29992;&#34920;&#26684;&#25552;&#31034;&#35299;&#38145;&#20851;&#31995;&#19977;&#20803;&#32452;&#25277;&#21462;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unlocking Instructive In-Context Learning with Tabular Prompting for Relational Triple Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13741
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#34920;&#26684;&#25552;&#31034;&#20197;&#35299;&#20915;&#20851;&#31995;&#19977;&#20803;&#32452;&#25277;&#21462;&#20013;&#30340;&#25552;&#31034;&#35774;&#35745;&#21644;&#26679;&#26412;&#36873;&#25321;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#19977;&#20803;&#32452;&#25277;&#21462;&#65288;RTE&#65289;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#34920;&#29616;&#65292;&#20294;&#20173;&#28982;&#38754;&#20020;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#65288;1&#65289;&#22914;&#20309;&#35774;&#35745;&#26377;&#25928;&#30340;&#25552;&#31034;&#21644;&#65288;2&#65289;&#22914;&#20309;&#36873;&#25321;&#36866;&#24403;&#30340;&#28436;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#36866;&#24403;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#19968;&#26041;&#38754;&#65292;&#23427;&#20204;&#36890;&#24120;&#23558;RTE&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#25991;&#26412;-&#25991;&#26412;&#25552;&#31034;&#26684;&#24335;&#65292;&#36825;&#26159;&#19981;&#33258;&#28982;&#30340;&#65292;&#23548;&#33268;&#22312;&#39044;&#35757;&#32451;&#26102;&#30340;&#36755;&#20986;&#26684;&#24335;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#26029;&#26102;&#38388;&#20043;&#38388;&#19981;&#21305;&#37197;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#20204;&#21482;&#21033;&#29992;&#34920;&#38754;&#33258;&#28982;&#35821;&#35328;&#29305;&#24449;&#65292;&#32570;&#20047;&#22312;&#26679;&#26412;&#36873;&#25321;&#20013;&#32771;&#34385;&#19977;&#20803;&#32452;&#35821;&#20041;&#12290;&#36825;&#20123;&#38382;&#39064;&#38459;&#30861;&#20102;ICL&#23545;RTE&#24615;&#33021;&#30340;&#25913;&#36827;&#65292;&#22240;&#27492;&#25105;&#20204;&#26088;&#22312;&#21516;&#26102;&#35299;&#20915;&#25552;&#31034;&#35774;&#35745;&#21644;&#26679;&#26412;&#36873;&#25321;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;RTE&#30340;&#34920;&#26684;&#25552;&#31034;&#65288;TableIE&#65289;&#65292;&#23558;RTE&#20219;&#21153;&#26500;&#24314;&#25104;&#19968;&#20010;&#34920;&#26684;&#29983;&#25104;&#20219;&#21153;&#20197;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13741v1 Announce Type: cross  Abstract: The in-context learning (ICL) for relational triple extraction (RTE) has achieved promising performance, but still encounters two key challenges: (1) how to design effective prompts and (2) how to select proper demonstrations. Existing methods, however, fail to address these challenges appropriately. On the one hand, they usually recast RTE task to text-to-text prompting formats, which is unnatural and results in a mismatch between the output format at the pre-training time and the inference time for large language models (LLMs). On the other hand, they only utilize surface natural language features and lack consideration of triple semantics in sample selection. These issues are blocking improved performance in ICL for RTE, thus we aim to tackle prompt designing and sample selection challenges simultaneously. To this end, we devise a tabular prompting for RTE (\textsc{TableIE}) which frames RTE task into a table generation task to inco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#39318;&#20010;&#26088;&#22312;&#33258;&#21160;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#25442;&#20026;&#35821;&#26009;&#24211;&#26597;&#35810;&#35821;&#35328;&#65288;CQL&#65289;&#30340;&#25991;&#26412;&#21040;CQL&#20219;&#21153;&#65292;&#21253;&#25324;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26377;&#25928;&#25991;&#26412;&#21040;CQL&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.13740</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#21040;CQL&#65306;&#36830;&#25509;&#33258;&#28982;&#35821;&#35328;&#21644;&#35821;&#26009;&#24211;&#25628;&#32034;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
From Text to CQL: Bridging Natural Language and Corpus Search Engine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#39318;&#20010;&#26088;&#22312;&#33258;&#21160;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#25442;&#20026;&#35821;&#26009;&#24211;&#26597;&#35810;&#35821;&#35328;&#65288;CQL&#65289;&#30340;&#25991;&#26412;&#21040;CQL&#20219;&#21153;&#65292;&#21253;&#25324;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26377;&#25928;&#25991;&#26412;&#21040;CQL&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#25105;&#20204;&#19982;&#20449;&#24687;&#31995;&#32479;&#20114;&#21160;&#30340;&#26041;&#24335;&#65292;&#30528;&#37325;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#36716;&#25442;&#20026;&#35832;&#22914;SQL&#20043;&#31867;&#30340;&#24418;&#24335;&#21270;&#26597;&#35810;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35821;&#26009;&#24211;&#26597;&#35810;&#35821;&#35328;&#65288;CQL&#65289;&#36825;&#19968;&#22312;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#36827;&#34892;&#35821;&#35328;&#30740;&#31350;&#21644;&#35814;&#32454;&#20998;&#26512;&#30340;&#20851;&#38190;&#24037;&#20855;&#65292;&#21364;&#27809;&#26377;&#21463;&#21040;&#36275;&#22815;&#30340;&#37325;&#35270;&#12290;&#25163;&#21160;&#26500;&#24314;CQL&#26597;&#35810;&#26159;&#19968;&#39033;&#22797;&#26434;&#19988;&#32791;&#26102;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#30456;&#24403;&#22810;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#36825;&#23545;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#37117;&#26500;&#25104;&#20102;&#26126;&#26174;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26088;&#22312;&#33258;&#21160;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#25442;&#20026;CQL&#30340;&#25991;&#26412;&#21040;CQL&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;&#32463;&#36807;&#29305;&#21035;&#31574;&#21010;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20197;&#21450;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26377;&#25928;&#25991;&#26412;&#21040;CQL&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20808;&#36827;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35780;&#20272;&#36825;&#19968;&#21516;&#27493;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13740v1 Announce Type: new  Abstract: Natural Language Processing (NLP) technologies have revolutionized the way we interact with information systems, with a significant focus on converting natural language queries into formal query languages such as SQL. However, less emphasis has been placed on the Corpus Query Language (CQL), a critical tool for linguistic research and detailed analysis within text corpora. The manual construction of CQL queries is a complex and time-intensive task that requires a great deal of expertise, which presents a notable challenge for both researchers and practitioners. This paper presents the first text-to-CQL task that aims to automate the translation of natural language into CQL. We present a comprehensive framework for this task, including a specifically curated large-scale dataset and methodologies leveraging large language models (LLMs) for effective text-to-CQL task. In addition, we established advanced evaluation metrics to assess the syn
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36864;&#21270;&#30693;&#35782;&#31070;&#32463;&#20803;&#65288;DKNs&#65289;&#30340;&#20840;&#38754;&#23450;&#20041;&#65292;&#24341;&#20837;&#20102;&#31070;&#32463;&#25299;&#25169;&#32858;&#31867;&#26041;&#27861;&#21644;&#31070;&#32463;&#36864;&#21270;&#20998;&#26512;&#26694;&#26550;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;DKN&#33719;&#21462;&#12290;</title><link>https://arxiv.org/abs/2402.13731</link><description>&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36798;&#33452;&#22855;&#23494;&#30721;&#65306;&#35299;&#35835;&#36864;&#21270;&#30693;&#35782;&#31070;&#32463;&#20803;
&lt;/p&gt;
&lt;p&gt;
The Da Vinci Code of Large Pre-trained Language Models: Deciphering Degenerate Knowledge Neurons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36864;&#21270;&#30693;&#35782;&#31070;&#32463;&#20803;&#65288;DKNs&#65289;&#30340;&#20840;&#38754;&#23450;&#20041;&#65292;&#24341;&#20837;&#20102;&#31070;&#32463;&#25299;&#25169;&#32858;&#31867;&#26041;&#27861;&#21644;&#31070;&#32463;&#36864;&#21270;&#20998;&#26512;&#26694;&#26550;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;DKN&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20013;&#20107;&#23454;&#30693;&#35782;&#23384;&#20648;&#30340;&#26426;&#21046;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20107;&#23454;&#30693;&#35782;&#23384;&#20648;&#22312;&#22810;&#23618;&#24863;&#30693;&#22120;&#26435;&#37325;&#20013;&#65292;&#26576;&#20123;&#23384;&#20648;&#21333;&#20803;&#34920;&#29616;&#20986;&#36864;&#21270;&#24615;&#65292;&#31216;&#20026;&#36864;&#21270;&#30693;&#35782;&#31070;&#32463;&#20803;&#65288;Degenerate Knowledge Neurons, DKNs&#65289;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#28085;&#30422;&#32467;&#26500;&#21644;&#21151;&#33021;&#26041;&#38754;&#30340;DKNs&#20840;&#38754;&#23450;&#20041;&#65292;&#24320;&#21019;&#20102;&#23545;PLMs&#20107;&#23454;&#30693;&#35782;&#23384;&#20648;&#21333;&#20803;&#32467;&#26500;&#30340;&#30740;&#31350;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#25299;&#25169;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#24418;&#25104;&#20219;&#24847;&#25968;&#37327;&#21644;&#32467;&#26500;&#30340;DKNs&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;DKN&#33719;&#21462;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#36864;&#21270;&#20998;&#26512;&#26694;&#26550;&#65292;&#29420;&#29305;&#22320;&#25972;&#21512;&#20102;&#27169;&#22411;&#40065;&#26834;&#24615;&#12289;&#21487;&#36827;&#21270;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#20197;&#23545;PLMs&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#22312;&#35813;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#25191;&#34892;&#20102;34&#20010;&#23454;&#39564;&#65292;&#36328;&#36234;2&#20010;PLMs&#12289;4&#20010;&#25968;&#25454;&#38598;&#21644;6&#20010;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13731v1 Announce Type: cross  Abstract: This study explores the mechanism of factual knowledge storage in pre-trained language models (PLMs). Previous research suggests that factual knowledge is stored within multi-layer perceptron weights, and some storage units exhibit degeneracy, referred to as Degenerate Knowledge Neurons (DKNs). This paper provides a comprehensive definition of DKNs that covers both structural and functional aspects, pioneering the study of structures in PLMs' factual knowledge storage units. Based on this, we introduce the Neurological Topology Clustering method, which allows the formation of DKNs in any numbers and structures, leading to a more accurate DKN acquisition. Furthermore, we introduce the Neuro-Degeneracy Analytic Analysis Framework, which uniquely integrates model robustness, evolvability, and complexity for a holistic assessment of PLMs. Within this framework, our execution of 34 experiments across 2 PLMs, 4 datasets, and 6 settings highl
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#36866;&#24212;&#25513;&#30721;&#26041;&#27861;&#26469;&#21327;&#21161;&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#26041;&#38754;&#26415;&#35821;&#25552;&#21462;&#21644;&#24773;&#24863;&#20998;&#31867;&#23376;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.13722</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#36866;&#24212;&#19978;&#19979;&#25991;&#25513;&#30721;&#36827;&#34892;&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exploiting Adaptive Contextual Masking for Aspect-Based Sentiment Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13722
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#36866;&#24212;&#25513;&#30721;&#26041;&#27861;&#26469;&#21327;&#21161;&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#26041;&#38754;&#26415;&#35821;&#25552;&#21462;&#21644;&#24773;&#24863;&#20998;&#31867;&#23376;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#26159;&#19968;&#20010;&#32454;&#31890;&#24230;&#30340;&#35821;&#35328;&#23398;&#38382;&#39064;&#65292;&#28041;&#21450;&#20174;&#32473;&#23450;&#25991;&#26412;&#20013;&#25552;&#21462;&#22810;&#26041;&#38754;&#30340;&#26041;&#38754;&#12289;&#24847;&#35265;&#21644;&#24773;&#24863;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25513;&#30721;&#26041;&#27861;&#65292;&#26681;&#25454;&#19978;&#19979;&#25991;&#21435;&#38500;&#26080;&#20851;&#26631;&#35760;&#65292;&#20197;&#24110;&#21161;ABSA&#30340;Aspect Term Extraction&#21644;Aspect Sentiment Classification&#23376;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13722v1 Announce Type: new  Abstract: Aspect-Based Sentiment Analysis (ABSA) is a fine-grained linguistics problem that entails the extraction of multifaceted aspects, opinions, and sentiments from the given text. Both standalone and compound ABSA tasks have been extensively used in the literature to examine the nuanced information present in online reviews and social media posts. Current ABSA methods often rely on static hyperparameters for attention-masking mechanisms, which can struggle with context adaptation and may overlook the unique relevance of words in varied situations. This leads to challenges in accurately analyzing complex sentences containing multiple aspects with differing sentiments. In this work, we present adaptive masking methods that remove irrelevant tokens based on context to assist in Aspect Term Extraction and Aspect Sentiment Classification subtasks of ABSA. We show with our experiments that the proposed methods outperform the baseline methods in te
&lt;/p&gt;</description></item><item><title>Ouroboros&#36890;&#36807;&#26500;&#24314;&#30701;&#23567;&#33609;&#26696;&#24182;&#24341;&#20837;&#20505;&#36873;&#30701;&#35821;&#27744;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#21152;&#36895;&#25928;&#29575;</title><link>https://arxiv.org/abs/2402.13720</link><description>&lt;p&gt;
Ouroboros: &#22823;&#27169;&#22411;&#22686;&#24378;&#33609;&#26696;&#30340;&#29468;&#27979;&#35299;&#30721;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Ouroboros: Speculative Decoding with Large Model Enhanced Drafting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13720
&lt;/p&gt;
&lt;p&gt;
Ouroboros&#36890;&#36807;&#26500;&#24314;&#30701;&#23567;&#33609;&#26696;&#24182;&#24341;&#20837;&#20505;&#36873;&#30701;&#35821;&#27744;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#21152;&#36895;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;&#30701;&#23567;&#39640;&#25928;&#30340;&#23567;&#27169;&#22411;&#36215;&#33609;&#33609;&#26696;&#65292;&#28982;&#21518;&#35201;&#27714;&#22823;&#35821;&#35328;&#27169;&#22411;&#20197;&#26080;&#33258;&#22238;&#24402;&#26041;&#24335;&#36827;&#34892;&#39564;&#35777;&#21644;&#20462;&#27491;&#65292;&#20197;&#26368;&#23567;&#21270;&#26102;&#38388;&#24320;&#38144;&#12290;&#24403;&#39564;&#35777;&#21518;&#21487;&#20197;&#29983;&#25104;&#26356;&#38271;&#30340;&#33609;&#31295;&#65292;&#20294;&#20063;&#20250;&#23548;&#33268;&#30456;&#24403;&#22823;&#30340;&#23581;&#35797;&#21644;&#38169;&#35823;&#25104;&#26412;&#12290;&#30001;&#20110;&#39640;&#39564;&#35777;&#22833;&#36133;&#27010;&#29575;&#65292;&#29616;&#26377;&#35299;&#30721;&#26041;&#27861;&#19981;&#33021;&#19968;&#27425;&#36215;&#33609;&#22826;&#22810;&#20869;&#23481;&#36827;&#34892;&#39564;&#35777;&#65292;&#23454;&#29616;&#27425;&#20248;&#30340;&#25512;&#29702;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13720v1 Announce Type: new  Abstract: Drafting-then-verifying decoding methods such as speculative decoding are widely adopted training-free methods to accelerate the inference of large language models (LLMs). Instead of employing an autoregressive process to decode tokens sequentially, speculative decoding initially creates drafts with an efficient small model. Then LLMs are required to conduct verification and correction in a non-autoregressive fashion to minimize time overhead. Generating longer drafts can lead to even more significant speedups once verified, but also incurs substantial trial and error costs if it fails. Suffering from the high verification failure probability, existing decoding methods cannot draft too much content for verification at one time, achieving sub-optimal inference acceleration. In this paper, we introduce Ouroboros, which constructs a phrase candidate pool from the verification process of LLMs to provide candidates for draft generation of the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;$\infty$Bench&#65292;&#31532;&#19968;&#20010;&#20197;&#24179;&#22343;&#25968;&#25454;&#38271;&#24230;&#36229;&#36807;10&#19975;&#20010;&#20196;&#29260;&#30340;LLM&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;</title><link>https://arxiv.org/abs/2402.13718</link><description>&lt;p&gt;
$\infty$Bench: &#23558;&#38271;&#19978;&#19979;&#25991;&#35780;&#20272;&#25193;&#23637;&#33267;&#36229;&#36807;10&#19975;&#20196;&#29260;
&lt;/p&gt;
&lt;p&gt;
$\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13718
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;$\infty$Bench&#65292;&#31532;&#19968;&#20010;&#20197;&#24179;&#22343;&#25968;&#25454;&#38271;&#24230;&#36229;&#36807;10&#19975;&#20010;&#20196;&#29260;&#30340;LLM&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#21644;&#25512;&#29702;&#38271;&#19978;&#19979;&#25991;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#22914;&#25991;&#26723;&#29702;&#35299;&#21644;&#20195;&#29702;&#26500;&#24314;&#12290;&#26412;&#25991;&#25552;&#20986;$\infty$Bench&#65292;&#31532;&#19968;&#20010;LLM&#22522;&#20934;&#65292;&#24179;&#22343;&#25968;&#25454;&#38271;&#24230;&#36229;&#36807;10&#19975;&#20010;&#20196;&#29260;&#12290;$\infty$Bench&#21253;&#21547;&#28085;&#30422;&#19981;&#21516;&#39046;&#22495;&#30340;&#21512;&#25104;&#21644;&#29616;&#23454;&#20219;&#21153;&#65292;&#20197;&#33521;&#25991;&#21644;&#20013;&#25991;&#21576;&#29616;&#12290;$\infty$Bench&#20013;&#30340;&#20219;&#21153;&#26088;&#22312;&#38656;&#35201;&#28145;&#21051;&#29702;&#35299;&#19978;&#19979;&#25991;&#20013;&#30340;&#38271;&#20381;&#36182;&#24615;&#65292;&#24182;&#19988;&#31616;&#21333;&#22320;&#20174;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#26377;&#38480;&#25968;&#37327;&#30340;&#27573;&#33853;&#23545;&#20110;&#36825;&#20123;&#20219;&#21153;&#26469;&#35828;&#26159;&#19981;&#22815;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13718v1 Announce Type: new  Abstract: Processing and reasoning over long contexts is crucial for many practical applications of Large Language Models (LLMs), such as document comprehension and agent construction. Despite recent strides in making LLMs process contexts with more than 100K tokens, there is currently a lack of a standardized benchmark to evaluate this long-context capability. Existing public benchmarks typically focus on contexts around 10K tokens, limiting the assessment and comparison of LLMs in processing longer contexts. In this paper, we propose $\infty$Bench, the first LLM benchmark featuring an average data length surpassing 100K tokens. $\infty$Bench comprises synthetic and realistic tasks spanning diverse domains, presented in both English and Chinese. The tasks in $\infty$Bench are designed to require well understanding of long dependencies in contexts, and make simply retrieving a limited number of passages from contexts not sufficient for these tasks
&lt;/p&gt;</description></item><item><title>Neeko&#21033;&#29992;&#21160;&#24577;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;LoRA&#65289;&#31574;&#30053;&#65292;&#26377;&#25928;&#22788;&#29702;&#22810;&#35282;&#33394;&#25198;&#28436;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#65292;&#25552;&#21319;&#20102;&#23545;&#19981;&#21516;&#23646;&#24615;&#12289;&#20010;&#24615;&#21644;&#35828;&#35805;&#27169;&#24335;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.13717</link><description>&lt;p&gt;
Neeko&#65306;&#21033;&#29992;&#21160;&#24577;LoRA&#23454;&#29616;&#39640;&#25928;&#22810;&#35282;&#33394;&#25198;&#28436;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13717
&lt;/p&gt;
&lt;p&gt;
Neeko&#21033;&#29992;&#21160;&#24577;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;LoRA&#65289;&#31574;&#30053;&#65292;&#26377;&#25928;&#22788;&#29702;&#22810;&#35282;&#33394;&#25198;&#28436;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#65292;&#25552;&#21319;&#20102;&#23545;&#19981;&#21516;&#23646;&#24615;&#12289;&#20010;&#24615;&#21644;&#35828;&#35805;&#27169;&#24335;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#20195;&#29702;&#31243;&#24207;&#20013;&#36215;&#30528;&#38761;&#21629;&#24615;&#20316;&#29992;&#65292;&#20294;&#22312;&#22810;&#35282;&#33394;&#25198;&#28436;&#65288;MCRP&#65289;&#22330;&#26223;&#20013;&#36935;&#21040;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Neeko&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#39640;&#25928;&#27169;&#20223;&#22810;&#20010;&#35282;&#33394;&#32780;&#35774;&#35745;&#30340;&#21019;&#26032;&#26694;&#26550;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;Neeko&#37319;&#29992;&#21160;&#24577;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;LoRA&#65289;&#31574;&#30053;&#65292;&#20351;&#20854;&#33021;&#22815;&#26080;&#32541;&#36866;&#24212;&#19981;&#21516;&#30340;&#35282;&#33394;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#35282;&#33394;&#25198;&#28436;&#36807;&#31243;&#20998;&#35299;&#20026;&#20195;&#29702;&#39044;&#35757;&#32451;&#12289;&#22810;&#20010;&#35282;&#33394;&#25198;&#28436;&#21644;&#35282;&#33394;&#22686;&#37327;&#23398;&#20064;&#65292;&#26377;&#25928;&#22788;&#29702;&#24050;&#30693;&#21644;&#26410;&#30693;&#35282;&#33394;&#12290;&#36825;&#31181;&#21160;&#24577;&#26041;&#27861;&#65292;&#32467;&#21512;&#20026;&#27599;&#20010;&#35282;&#33394;&#35774;&#35745;&#30340;&#29420;&#29305;LoRA&#22359;&#65292;&#22686;&#24378;&#20102;Neeko&#23545;&#29420;&#29305;&#23646;&#24615;&#12289;&#20010;&#24615;&#21644;&#35828;&#35805;&#27169;&#24335;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;Neeko&#22312;MCRP&#26041;&#38754;&#34920;&#29616;&#20986;&#27604;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26356;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#26356;&#20855;&#21560;&#24341;&#21147;&#21644;&#22810;&#26679;&#21270;&#30340;&#20114;&#21160;&#20307;&#39564;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;&#65288;&#38142;&#25509;&#20013;&#25552;&#20379;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13717v1 Announce Type: new  Abstract: Large Language Models (LLMs) have revolutionized open-domain dialogue agents but encounter challenges in multi-character role-playing (MCRP) scenarios. To address the issue, we present Neeko, an innovative framework designed for efficient multiple characters imitation. Unlike existing methods, Neeko employs a dynamic low-rank adapter (LoRA) strategy, enabling it to adapt seamlessly to diverse characters. Our framework breaks down the role-playing process into agent pre-training, multiple characters playing, and character incremental learning, effectively handling both seen and unseen roles. This dynamic approach, coupled with distinct LoRA blocks for each character, enhances Neeko's adaptability to unique attributes, personalities, and speaking patterns. As a result, Neeko demonstrates superior performance in MCRP over most existing methods, offering more engaging and versatile user interaction experiences. Code and data are available at
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;SaGE&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#20041;&#22270;&#29109;&#26469;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36947;&#24503;&#19968;&#33268;&#24615;&#65292;&#26500;&#24314;&#20102;MCC&#35821;&#26009;&#24211;&#12290;</title><link>https://arxiv.org/abs/2402.13709</link><description>&lt;p&gt;
SaGE&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36947;&#24503;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
SaGE: Evaluating Moral Consistency in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13709
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;SaGE&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#20041;&#22270;&#29109;&#26469;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36947;&#24503;&#19968;&#33268;&#24615;&#65292;&#26500;&#24314;&#20102;MCC&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#23637;&#31034;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20250;&#35805;&#31995;&#32479;&#20013;&#30340;&#21360;&#35937;&#28145;&#21051;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20063;&#23384;&#22312;&#36947;&#24503;&#19981;&#19968;&#33268;&#65292;&#23545;&#20854;&#21487;&#38752;&#24615;&#65288;&#20197;&#21450;&#24635;&#20307;&#21487;&#20449;&#36182;&#24615;&#65289;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#20197;&#24448;&#22312;LLM&#35780;&#20272;&#39046;&#22495;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#24320;&#21457;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#65292;&#20197;&#34913;&#37327;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36947;&#24503;&#24773;&#26223;&#24448;&#24448;&#32570;&#20047;&#26222;&#36941;&#35748;&#21516;&#31572;&#26696;&#30340;&#24773;&#20917;&#65292;&#27169;&#22411;&#21709;&#24212;&#30340;&#19968;&#33268;&#24615;&#23545;&#20110;&#20854;&#21487;&#38752;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#29702;&#35770;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#35821;&#20041;&#22270;&#29109;&#65288;SaGE&#65289;&#65292;&#22522;&#20110;&#8220;&#32463;&#39564;&#27861;&#21017;&#8221;&#65288;RoTs&#65289;&#30340;&#27010;&#24565;&#26469;&#34913;&#37327;&#27169;&#22411;&#30340;&#36947;&#24503;&#19968;&#33268;&#24615;&#12290;RoTs&#26159;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#25277;&#35937;&#21407;&#21017;&#65292;&#21487;&#26377;&#25928;&#24110;&#21161;&#35299;&#37322;&#20854;&#20915;&#31574;&#31574;&#30053;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#36947;&#24503;&#19968;&#33268;&#24615;&#35821;&#26009;&#24211;&#65288;MCC&#65289;&#65292;&#21253;&#21547;50K&#20010;&#36947;&#24503;&#38382;&#39064;&#12289;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13709v1 Announce Type: cross  Abstract: Despite recent advancements showcasing the impressive capabilities of Large Language Models (LLMs) in conversational systems, we show that even state-of-the-art LLMs are morally inconsistent in their generations, questioning their reliability (and trustworthiness in general). Prior works in LLM evaluation focus on developing ground-truth data to measure accuracy on specific tasks. However, for moral scenarios that often lack universally agreed-upon answers, consistency in model responses becomes crucial for their reliability. To address this issue, we propose an information-theoretic measure called Semantic Graph Entropy (SaGE), grounded in the concept of "Rules of Thumb" (RoTs) to measure a model's moral consistency. RoTs are abstract principles learned by a model and can help explain their decision-making strategies effectively. To this extent, we construct the Moral Consistency Corpus (MCC), containing 50K moral questions, responses
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#31532;&#19968;&#20010;&#23545;&#22810;&#35821;&#27169;&#22411;&#22312;&#19981;&#21516;&#21360;&#27431;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#24182;&#34892;&#25945;&#23398;&#35843;&#25972;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25945;&#23398;&#35843;&#25972;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#36328;&#35821;&#35328;&#36981;&#24490;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#23545;&#34920;&#38754;&#23545;&#40784;&#20551;&#35774;&#30340;&#36136;&#30097;</title><link>https://arxiv.org/abs/2402.13703</link><description>&lt;p&gt;
&#35843;&#26597;&#22810;&#35821;&#35328;&#25945;&#23398;&#35843;&#25972;&#65306;&#22810;&#35821;&#27169;&#22411;&#26159;&#21542;&#38656;&#35201;&#22810;&#35821;&#25945;&#23398;&#65311;
&lt;/p&gt;
&lt;p&gt;
Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#31532;&#19968;&#20010;&#23545;&#22810;&#35821;&#27169;&#22411;&#22312;&#19981;&#21516;&#21360;&#27431;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#24182;&#34892;&#25945;&#23398;&#35843;&#25972;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25945;&#23398;&#35843;&#25972;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#36328;&#35821;&#35328;&#36981;&#24490;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#23545;&#34920;&#38754;&#23545;&#40784;&#20551;&#35774;&#30340;&#36136;&#30097;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13703v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#23558;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36716;&#21270;&#20026;&#38596;&#36777;&#32780;&#26377;&#29992;&#30340;&#21161;&#25163;&#23545;&#20419;&#36827;&#23427;&#20204;&#22312;&#19981;&#21516;&#35821;&#35328;&#22320;&#21306;&#30340;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#36825;&#19968;&#31934;&#31070;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#23545;&#36328;&#22810;&#31181;&#21360;&#27431;&#35821;&#35328;&#36827;&#34892;&#22823;&#35268;&#27169;&#30740;&#31350;&#30340;&#30740;&#31350;&#32773;&#65292;&#26088;&#22312;&#30740;&#31350;&#22810;&#35821;&#27169;&#22411;&#22312;&#36873;&#25321;&#30340;&#26368;&#24120;&#29992;&#30340;&#21360;&#27431;&#35821;&#35328;&#19978;&#30340;&#24182;&#34892;&#12289;&#22810;&#36718;&#25945;&#23398;&#35843;&#25972;&#22522;&#20934;&#27979;&#35797;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#35821;&#35328;&#21644;&#25945;&#23398;&#25968;&#25454;&#38598;&#22823;&#23567;&#23545;&#20013;&#22411;&#22810;&#35821;&#35328;LLM&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#22312;&#24182;&#34892;&#25945;&#23398;&#35843;&#25972;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25945;&#23398;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24182;&#34892;&#25945;&#23398;&#35843;&#25972;&#32780;&#19981;&#26159;&#21333;&#35821;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#25945;&#23398;&#35843;&#25972;&#21487;&#20197;&#20351;&#36328;&#35821;&#35328;&#36981;&#24490;&#33021;&#21147;&#25552;&#39640;&#22810;&#36798;4.6%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#34920;&#38754;&#23545;&#40784;&#20551;&#35774;&#36890;&#24120;&#19981;&#25104;&#31435;&#65292;&#22240;&#20026;&#25152;&#35843;&#26597;&#30340;&#22810;&#35821;7B&#21442;&#25968;&#27169;&#22411;&#26159;&#19968;&#20010;&#21453;&#20363;&#65292;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#25945;&#23398;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13703v1 Announce Type: new  Abstract: The adaption of multilingual pre-trained Large Language Models (LLMs) into eloquent and helpful assistants is essential to facilitate their use across different language regions. In that spirit, we are the first to conduct an extensive study of the performance of multilingual models on parallel, multi-turn instruction-tuning benchmarks across a selection of the most-spoken Indo-European languages. We systematically examine the effects of language and instruction dataset size on a mid-sized, multilingual LLM by instruction-tuning it on parallel instruction-tuning datasets. Our results demonstrate that instruction-tuning on parallel instead of monolingual corpora benefits cross-lingual instruction following capabilities by up to 4.6%. Furthermore, we show that the Superficial Alignment Hypothesis does not hold in general, as the investigated multilingual 7B parameter model presents a counter-example requiring large-scale instruction-tuning
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#20013;&#22269;&#26368;&#22823;&#30340;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#24494;&#21338;&#19978;&#26500;&#24314;&#20102;&#19968;&#20010;&#20013;&#25991;&#22810;&#27169;&#24577;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#65288;CMNER&#65289;&#65292;&#21253;&#21547;5,000&#26465;&#24494;&#21338;&#24086;&#23376;&#21644;18,326&#24352;&#23545;&#24212;&#22270;&#29255;&#65292;&#24182;&#23637;&#31034;&#20102;&#23558;&#22270;&#29255;&#32435;&#20837;NER&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13693</link><description>&lt;p&gt;
&#22522;&#20110;&#31038;&#20132;&#23186;&#20307;&#30340;&#20013;&#25991;&#22810;&#27169;&#24577;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;CMNER
&lt;/p&gt;
&lt;p&gt;
CMNER: A Chinese Multimodal NER Dataset based on Social Media
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#20013;&#22269;&#26368;&#22823;&#30340;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#24494;&#21338;&#19978;&#26500;&#24314;&#20102;&#19968;&#20010;&#20013;&#25991;&#22810;&#27169;&#24577;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#65288;CMNER&#65289;&#65292;&#21253;&#21547;5,000&#26465;&#24494;&#21338;&#24086;&#23376;&#21644;18,326&#24352;&#23545;&#24212;&#22270;&#29255;&#65292;&#24182;&#23637;&#31034;&#20102;&#23558;&#22270;&#29255;&#32435;&#20837;NER&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;MNER&#65289;&#26159;&#19968;&#39033;&#26088;&#22312;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#20854;&#36890;&#36807;&#30456;&#20851;&#22270;&#29255;&#25552;&#20379;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#22312;&#20013;&#25991;MNER&#39046;&#22495;&#65292;&#25968;&#25454;&#26126;&#26174;&#19981;&#36275;&#65292;&#20005;&#37325;&#38459;&#30861;&#20102;&#35813;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#36827;&#23637;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;&#24494;&#21338;&#30340;&#25968;&#25454;&#32534;&#21046;&#20102;&#19968;&#20010;&#20013;&#25991;&#22810;&#27169;&#24577;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#65288;CMNER&#65289;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;5,000&#26465;&#24494;&#21338;&#24086;&#23376;&#65292;&#37197;&#23545;18,326&#24352;&#23545;&#24212;&#30340;&#22270;&#29255;&#12290;&#23454;&#20307;&#34987;&#20998;&#31867;&#20026;&#22235;&#20010;&#19981;&#21516;&#31867;&#21035;&#65306;&#20154;&#29289;&#12289;&#22320;&#28857;&#12289;&#32452;&#32455;&#21644;&#26434;&#39033;&#12290;&#25105;&#20204;&#22312;CMNER&#19978;&#36827;&#34892;&#20102;&#22522;&#32447;&#23454;&#39564;&#65292;&#32467;&#26524;&#24378;&#35843;&#20102;&#23558;&#22270;&#29255;&#32435;&#20837;NER&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#20844;&#24320;&#30340;&#33521;&#25991;MNER&#25968;&#25454;&#38598;&#65288;Twitter2015&#65289;&#19978;&#36827;&#34892;&#20102;&#36328;&#35821;&#35328;&#23454;&#39564;&#65292;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#21363;&#20013;&#25991;&#21644;&#33521;&#25991;&#20043;&#38388;&#23384;&#22312;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13693v1 Announce Type: new  Abstract: Multimodal Named Entity Recognition (MNER) is a pivotal task designed to extract named entities from text with the support of pertinent images. Nonetheless, a notable paucity of data for Chinese MNER has considerably impeded the progress of this natural language processing task within the Chinese domain. Consequently, in this study, we compile a Chinese Multimodal NER dataset (CMNER) utilizing data sourced from Weibo, China's largest social media platform. Our dataset encompasses 5,000 Weibo posts paired with 18,326 corresponding images. The entities are classified into four distinct categories: person, location, organization, and miscellaneous. We perform baseline experiments on CMNER, and the outcomes underscore the effectiveness of incorporating images for NER. Furthermore, we conduct cross-lingual experiments on the publicly available English MNER dataset (Twitter2015), and the results substantiate our hypothesis that Chinese and Eng
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;SemEval-2024&#20219;&#21153;8&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24494;&#35843;LLMs&#36827;&#34892;&#22810;&#35821;&#35328;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#31181;&#26041;&#24335;&#22788;&#29702;&#35813;&#20219;&#21153;&#24182;&#23558;&#32479;&#35745;&#26816;&#27979;&#25351;&#26631;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#32467;&#21512;&#65292;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.13671</link><description>&lt;p&gt;
KInIT&#21442;&#21152;SemEval-2024&#20219;&#21153;8&#65306;&#38024;&#23545;&#22810;&#35821;&#35328;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#24494;&#35843;LLMs
&lt;/p&gt;
&lt;p&gt;
KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated Text Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;SemEval-2024&#20219;&#21153;8&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24494;&#35843;LLMs&#36827;&#34892;&#22810;&#35821;&#35328;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#31181;&#26041;&#24335;&#22788;&#29702;&#35813;&#20219;&#21153;&#24182;&#23558;&#32479;&#35745;&#26816;&#27979;&#25351;&#26631;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#32467;&#21512;&#65292;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13671v1 &#21457;&#34920;&#31867;&#22411;&#65306;&#20132;&#21449;&#20256;&#25773; &#25688;&#35201;&#65306;SemEval-2024&#20219;&#21153;8&#20391;&#37325;&#20110;&#22810;&#29983;&#25104;&#22120;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#35821;&#35328;&#30340;&#40657;&#30418;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#12290;&#36825;&#26679;&#30340;&#26816;&#27979;&#23545;&#20110;&#38450;&#27490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#28508;&#22312;&#28389;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#20854;&#20013;&#26368;&#26032;&#30340;LLMs&#38750;&#24120;&#25797;&#38271;&#29983;&#25104;&#22810;&#35821;&#35328;&#30340;&#31867;&#20284;&#20154;&#31867;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#20197;&#22810;&#31181;&#26041;&#24335;&#22788;&#29702;&#20102;&#36825;&#20010;&#20219;&#21153;&#65292;&#21033;&#29992;&#35821;&#35328;&#35782;&#21035;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#36739;&#23567;&#30340;LLMs&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#27599;&#31181;&#35821;&#35328;&#30340;&#20998;&#31867;&#38408;&#20540;&#26657;&#20934;&#65292;&#23558;&#24494;&#35843;&#30340;&#27169;&#22411;&#39044;&#27979;&#19982;&#32479;&#35745;&#26816;&#27979;&#25351;&#26631;&#29420;&#29305;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#31995;&#32479;&#26816;&#27979;&#24615;&#33021;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20132;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#65292;&#25490;&#21517;&#31532;&#22235;&#65292;&#20165;&#33853;&#21518;&#20110;&#33719;&#32988;&#32773;&#19981;&#21040;1&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13671v1 Announce Type: cross  Abstract: SemEval-2024 Task 8 is focused on multigenerator, multidomain, and multilingual black-box machine-generated text detection. Such a detection is important for preventing a potential misuse of large language models (LLMs), the newest of which are very capable in generating multilingual human-like texts. We have coped with this task in multiple ways, utilizing language identification and parameter-efficient fine-tuning of smaller LLMs for text classification. We have further used the per-language classification-threshold calibration to uniquely combine fine-tuned models predictions with statistical detection metrics to improve generalization of the system detection performance. Our submitted method achieved competitive results, ranking at the fourth place, just under 1 percentage point behind the winner.
&lt;/p&gt;</description></item><item><title>SDFT&#26159;&#19968;&#31181;&#36890;&#36807;&#29992;&#27169;&#22411;&#26412;&#36523;&#29983;&#25104;&#30340;&#31934;&#31616;&#25968;&#25454;&#38598;&#26469;&#26725;&#25509;&#20998;&#24067;&#24046;&#36317;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#21516;&#26102;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#19982;&#26222;&#36890;&#24494;&#35843;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.13669</link><description>&lt;p&gt;
&#33258;&#33976;&#39311;&#26725;&#25509;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#20998;&#24067;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13669
&lt;/p&gt;
&lt;p&gt;
SDFT&#26159;&#19968;&#31181;&#36890;&#36807;&#29992;&#27169;&#22411;&#26412;&#36523;&#29983;&#25104;&#30340;&#31934;&#31616;&#25968;&#25454;&#38598;&#26469;&#26725;&#25509;&#20998;&#24067;&#24046;&#36317;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#21516;&#26102;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#19982;&#26222;&#36890;&#24494;&#35843;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#20294;&#23558;&#23427;&#20204;&#24494;&#35843;&#20026;&#29305;&#23450;&#20219;&#21153;&#24120;&#24120;&#38754;&#20020;&#22312;&#24179;&#34913;&#24615;&#33021;&#21644;&#20445;&#30041;&#19968;&#33324;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#20043;&#38388;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#20219;&#21153;&#25968;&#25454;&#38598;&#19982;LLMs&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#36317;&#26159;&#20027;&#35201;&#30340;&#28508;&#22312;&#21407;&#22240;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#33976;&#39311;&#24494;&#35843;&#65288;SDFT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#29992;&#27169;&#22411;&#26412;&#36523;&#29983;&#25104;&#30340;&#31934;&#31616;&#25968;&#25454;&#38598;&#24341;&#23548;&#24494;&#35843;&#20197;&#21305;&#37197;&#20854;&#21407;&#22987;&#20998;&#24067;&#26469;&#26725;&#25509;&#20998;&#24067;&#24046;&#36317;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#23545;Llama-2-chat&#27169;&#22411;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SDFT&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21516;&#26102;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#19982;&#26222;&#36890;&#24494;&#35843;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;SDFT&#23637;&#31034;&#20102;&#20445;&#25345;LLMs&#30340;&#26377;&#30410;&#24615;&#21644;&#23433;&#20840;&#23545;&#40784;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13669v1 Announce Type: new  Abstract: The surge in Large Language Models (LLMs) has revolutionized natural language processing, but fine-tuning them for specific tasks often encounters challenges in balancing performance and preserving general instruction-following abilities. In this paper, we posit that the distribution gap between task datasets and the LLMs serves as the primary underlying cause. To address the problem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach that bridges the distribution gap by guiding fine-tuning with a distilled dataset generated by the model itself to match its original distribution. Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs
&lt;/p&gt;</description></item><item><title>GCOF&#26694;&#26550;&#32467;&#21512;&#36951;&#20256;&#31639;&#27861;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#33258;&#25105;&#36845;&#20195;&#20248;&#21270;&#65292;&#29983;&#25104;&#30340;&#25991;&#26696;&#22312;&#28857;&#20987;&#29575;&#19978;&#30456;&#27604;&#20154;&#24037;&#32534;&#36753;&#30340;&#25991;&#26696;&#24179;&#22343;&#25552;&#39640;&#20102;50%&#20197;&#19978;&#12290;</title><link>https://arxiv.org/abs/2402.13667</link><description>&lt;p&gt;
GCOF&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#25105;&#36845;&#20195;&#25991;&#26412;&#29983;&#25104;&#20197;&#36827;&#34892;&#25991;&#26696;&#25776;&#20889;
&lt;/p&gt;
&lt;p&gt;
GCOF: Self-iterative Text Generation for Copywriting Using Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13667
&lt;/p&gt;
&lt;p&gt;
GCOF&#26694;&#26550;&#32467;&#21512;&#36951;&#20256;&#31639;&#27861;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#33258;&#25105;&#36845;&#20195;&#20248;&#21270;&#65292;&#29983;&#25104;&#30340;&#25991;&#26696;&#22312;&#28857;&#20987;&#29575;&#19978;&#30456;&#27604;&#20154;&#24037;&#32534;&#36753;&#30340;&#25991;&#26696;&#24179;&#22343;&#25552;&#39640;&#20102;50%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#26497;&#22823;&#22320;&#31616;&#21270;&#20102;&#24066;&#22330;&#25991;&#26696;&#30340;&#29983;&#25104;&#65292;&#20294;&#20135;&#29983;&#28385;&#36275;&#29305;&#23450;&#39046;&#22495;&#35201;&#27714;&#30340;&#20869;&#23481;&#65292;&#27604;&#22914;&#26377;&#25928;&#22320;&#21560;&#24341;&#23458;&#25143;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#26088;&#22312;&#22686;&#24378;&#24066;&#22330;&#25991;&#26696;&#21019;&#20316;&#25928;&#29575;&#21644;&#21560;&#24341;&#21147;&#30340;&#36951;&#20256;&#25991;&#26696;&#20248;&#21270;&#26694;&#26550;&#65288;GCOF&#65289;&#12290;&#25105;&#20204;&#22312;LLM&#25552;&#31034;&#30340;&#26174;&#24335;&#29305;&#24449;&#24037;&#31243;&#20013;&#36827;&#34892;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#20013;&#30340;&#20132;&#21449;&#25805;&#20316;&#31526;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;GCOF&#20013;&#65292;&#20197;&#23454;&#29616;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#12290;&#36825;&#31181;&#25972;&#21512;&#20419;&#36827;&#20102;&#24066;&#22330;&#25991;&#26696;&#30340;&#33258;&#25105;&#36845;&#20195;&#20248;&#21270;&#12290;&#22312;&#32447;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#26694;&#26550;&#29983;&#25104;&#30340;&#25991;&#26696;&#30456;&#23545;&#20110;&#20154;&#24037;&#32534;&#36753;&#30340;&#25991;&#26696;&#65292;&#22312;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#19978;&#24179;&#22343;&#22686;&#21152;&#20102;50%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13667v1 Announce Type: new  Abstract: Large language models(LLM) such as ChatGPT have substantially simplified the generation of marketing copy, yet producing content satisfying domain specific requirements, such as effectively engaging customers, remains a significant challenge. In this work, we introduce the Genetic Copy Optimization Framework (GCOF) designed to enhance both efficiency and engagememnt of marketing copy creation. We conduct explicit feature engineering within the prompts of LLM. Additionally, we modify the crossover operator in Genetic Algorithm (GA), integrating it into the GCOF to enable automatic feature engineering. This integration facilitates a self-iterative refinement of the marketing copy. Compared to human curated copy, Online results indicate that copy produced by our framework achieves an average increase in click-through rate (CTR) of over $50\%$.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20351;&#29992;&#21512;&#25104;&#25351;&#21335;&#26367;&#25442;&#30495;&#23454;&#25351;&#21335;&#20197;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#36890;&#36807;&#31169;&#23494;&#24494;&#35843;&#29983;&#25104;&#22120;&#29983;&#25104;&#27492;&#31867;&#21512;&#25104;&#25351;&#21335;&#65292;&#24182;&#36890;&#36807;&#26032;&#39062;&#30340;&#36807;&#28388;&#31639;&#27861;&#20351;&#21512;&#25104;&#25351;&#21335;&#30340;&#20998;&#24067;&#19982;&#30495;&#23454;&#25351;&#21335;&#19968;&#33268;&#65292;&#23637;&#31034;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#20013;&#30340;&#39640;&#25928;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13659</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#38544;&#31169;&#20445;&#25252;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Instructions for Aligning Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13659
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20351;&#29992;&#21512;&#25104;&#25351;&#21335;&#26367;&#25442;&#30495;&#23454;&#25351;&#21335;&#20197;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#36890;&#36807;&#31169;&#23494;&#24494;&#35843;&#29983;&#25104;&#22120;&#29983;&#25104;&#27492;&#31867;&#21512;&#25104;&#25351;&#21335;&#65292;&#24182;&#36890;&#36807;&#26032;&#39062;&#30340;&#36807;&#28388;&#31639;&#27861;&#20351;&#21512;&#25104;&#25351;&#21335;&#30340;&#20998;&#24067;&#19982;&#30495;&#23454;&#25351;&#21335;&#19968;&#33268;&#65292;&#23637;&#31034;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#20013;&#30340;&#39640;&#25928;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#30340;&#26381;&#21153;&#25552;&#20379;&#21830;&#22312;&#37326;&#22806;&#25910;&#38598;&#29992;&#25143;&#25351;&#21335;&#65292;&#24182;&#22312;&#36827;&#19968;&#27493;&#23545;&#40784;LLM&#19982;&#29992;&#25143;&#24847;&#22270;&#20013;&#20351;&#29992;&#36825;&#20123;&#25351;&#21335;&#12290;&#36825;&#20123;&#28508;&#22312;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#30340;&#25351;&#21335;&#22312;&#27969;&#31243;&#20013;&#30001;&#20154;&#24037;&#24037;&#20316;&#32773;&#26631;&#27880;&#12290;&#36825;&#24102;&#26469;&#20102;&#26032;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#32780;Typical Private Optimization&#27809;&#26377;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#35758;&#20351;&#29992;&#21512;&#25104;&#25351;&#21335;&#26367;&#25442;&#25968;&#25454;&#26631;&#27880;&#21644;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#30495;&#23454;&#25351;&#21335;&#12290;&#36890;&#36807;&#20351;&#29992;&#31169;&#23494;&#24494;&#35843;&#29983;&#25104;&#22120;&#29983;&#25104;&#36825;&#20123;&#21512;&#25104;&#25351;&#21335;&#65292;&#21487;&#20197;&#30830;&#20445;&#24418;&#24335;&#24046;&#24322;&#38544;&#31169;&#12290;&#22312;&#23454;&#29616;&#25152;&#38656;&#25928;&#29992;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#25105;&#20204;&#30340;&#26032;&#39062;&#36807;&#28388;&#31639;&#27861;&#65292;&#23558;&#21512;&#25104;&#25351;&#21335;&#30340;&#20998;&#24067;&#19982;&#23454;&#38469;&#25351;&#21335;&#30340;&#20998;&#24067;&#36827;&#34892;&#21305;&#37197;&#12290;&#22312;&#26377;&#20154;&#21453;&#39304;&#30340;&#21463;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#23637;&#31034;&#21512;&#25104;&#25351;&#21335;&#30340;&#26368;&#32456;&#38598;&#21512;&#30340;&#39640;&#25928;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13659v1 Announce Type: cross  Abstract: Service providers of large language model (LLM) applications collect user instructions in the wild and use them in further aligning LLMs with users' intentions. These instructions, which potentially contain sensitive information, are annotated by human workers in the process. This poses a new privacy risk not addressed by the typical private optimization. To this end, we propose using synthetic instructions to replace real instructions in data annotation and model fine-tuning. Formal differential privacy is guaranteed by generating those synthetic instructions using privately fine-tuned generators. Crucial in achieving the desired utility is our novel filtering algorithm that matches the distribution of the synthetic instructions to that of the real ones. In both supervised fine-tuning and reinforcement learning from human feedback, our extensive experiments demonstrate the high utility of the final set of synthetic instructions by sho
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32452;&#21512;&#27880;&#24847;&#21147;&#36974;&#32617;&#26041;&#27861;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#22810;&#31181;&#20132;&#20114;&#26041;&#24335;&#65292;&#21487;&#20197;&#25913;&#36827;&#26080;&#30417;&#30563;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.13647</link><description>&lt;p&gt;
&#36890;&#36807;LLMs&#21644;&#27880;&#24847;&#21147;&#36974;&#32617;&#23436;&#25104;&#26080;&#30417;&#30563;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#19982;&#22810;&#36335;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Text Style Transfer via LLMs and Attention Masking with Multi-way Interactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13647
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32452;&#21512;&#27880;&#24847;&#21147;&#36974;&#32617;&#26041;&#27861;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#22810;&#31181;&#20132;&#20114;&#26041;&#24335;&#65292;&#21487;&#20197;&#25913;&#36827;&#26080;&#30417;&#30563;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#65288;UTST&#65289;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#26088;&#22312;&#23558;&#21477;&#23376;&#30340;&#19968;&#31181;&#39118;&#26684;&#26041;&#38754;&#36716;&#25442;&#20026;&#21478;&#19968;&#31181;&#39118;&#26684;&#65292;&#21516;&#26102;&#19981;&#25913;&#21464;&#20854;&#35821;&#20041;&#12289;&#21477;&#27861;&#25110;&#20854;&#20182;&#23646;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#26377;&#25928;&#32467;&#21512;&#27880;&#24847;&#21147;&#36974;&#32617;&#26041;&#27861;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#25552;&#20986;&#20102;&#22235;&#31181;&#20132;&#20114;&#26041;&#24335;&#65306;&#20855;&#26377;&#35843;&#25972;&#39034;&#24207;&#30340;&#27969;&#27700;&#32447;&#26694;&#26550;&#65307;&#30693;&#35782;&#33976;&#39311;&#20174;LLMs&#21040;&#27880;&#24847;&#21147;&#36974;&#32617;&#27169;&#22411;&#65307;&#20351;&#29992;&#26500;&#24314;&#30340;&#24182;&#34892;&#31034;&#20363;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#22810;&#36335;&#20132;&#20114;&#21487;&#20197;&#25913;&#36827;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13647v1 Announce Type: cross  Abstract: Unsupervised Text Style Transfer (UTST) has emerged as a critical task within the domain of Natural Language Processing (NLP), aiming to transfer one stylistic aspect of a sentence into another style without changing its semantics, syntax, or other attributes. This task is especially challenging given the intrinsic lack of parallel text pairings. Among existing methods for UTST tasks, attention masking approach and Large Language Models (LLMs) are deemed as two pioneering methods. However, they have shortcomings in generating unsmooth sentences and changing the original contents, respectively. In this paper, we investigate if we can combine these two methods effectively. We propose four ways of interactions, that are pipeline framework with tuned orders; knowledge distillation from LLMs to attention masking model; in-context learning with constructed parallel examples. We empirically show these multi-way interactions can improve the ba
&lt;/p&gt;</description></item><item><title>&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#21644;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#36755;&#20837;-&#36755;&#20986;&#27169;&#24335;&#23548;&#33268;&#19981;&#21516;&#30340;&#20559;&#35265;&#22823;&#23567;&#21644;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.13636</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#32479;&#19968;&#26694;&#26550;&#21644;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework and Dataset for Assessing Gender Bias in Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13636
&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#21644;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#36755;&#20837;-&#36755;&#20986;&#27169;&#24335;&#23548;&#33268;&#19981;&#21516;&#30340;&#20559;&#35265;&#22823;&#23567;&#21644;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#19994;&#21644;&#23398;&#26415;&#30028;&#12290;&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#31995;&#32479;&#35780;&#20272;VLMs&#20013;&#30340;&#24615;&#21035;-&#32844;&#19994;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#28085;&#30422;&#20102;&#26368;&#36817;VLMs&#25903;&#25345;&#30340;&#25152;&#26377;&#25512;&#26029;&#27169;&#24335;&#65292;&#21253;&#25324;&#22270;&#20687;&#21040;&#25991;&#26412;&#12289;&#25991;&#26412;&#21040;&#25991;&#26412;&#12289;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#22270;&#20687;&#21040;&#22270;&#20687;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21512;&#25104;&#30340;&#12289;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#27169;&#31946;&#20102;&#32844;&#19994;&#21160;&#20316;&#20013;&#30340;&#24615;&#21035;&#24046;&#24322;&#65292;&#20197;&#35780;&#20272;&#24615;&#21035;&#20559;&#35265;&#12290;&#22312;&#25105;&#20204;&#23545;&#26368;&#36817;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#36755;&#20837;-&#36755;&#20986;&#27169;&#24335;&#20250;&#23548;&#33268;&#19981;&#21516;&#30340;&#20559;&#35265;&#22823;&#23567;&#21644;&#26041;&#21521;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#22815;&#25351;&#23548;&#26410;&#26469;&#25913;&#36827;VLMs&#20197;&#23398;&#20064;&#31038;&#20250;&#26080;&#20559;&#35265;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;&#21457;&#24067;&#25105;&#20204;&#30340;&#25968;&#25454;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13636v1 Announce Type: cross  Abstract: Large vision-language models (VLMs) are widely getting adopted in industry and academia. In this work we build a unified framework to systematically evaluate gender-profession bias in VLMs. Our evaluation encompasses all supported inference modes of the recent VLMs, including image-to-text, text-to-text, text-to-image, and image-to-image. We construct a synthetic, high-quality dataset of text and images that blurs gender distinctions across professional actions to benchmark gender bias. In our benchmarking of recent vision-language models (VLMs), we observe that different input-output modalities result in distinct bias magnitudes and directions. We hope our work will help guide future progress in improving VLMs to learn socially unbiased representations. We will release our data and code.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#26816;&#32034;&#65288;MORE&#65289;&#22686;&#24378;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#26469;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#30340;&#24120;&#35782;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.13625</link><description>&lt;p&gt;
MORE: &#22810;&#27169;&#24577;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#24335;&#24120;&#35782;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
MORE: Multi-mOdal REtrieval Augmented Generative Commonsense Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13625
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#26816;&#32034;&#65288;MORE&#65289;&#22686;&#24378;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#26469;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#30340;&#24120;&#35782;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#24448;&#24448;&#38590;&#20197;&#23398;&#20064;&#36275;&#22815;&#30340;&#24120;&#35782;&#30693;&#35782;&#65292;&#22240;&#20026;&#24120;&#35782;&#20449;&#24687;&#30340;&#35760;&#24405;&#39057;&#29575;&#26126;&#26174;&#20302;&#20110;&#20854;&#23384;&#22312;&#39057;&#29575;&#12290;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#30340;&#24120;&#35782;&#33021;&#21147;&#65292;&#19968;&#20123;&#30740;&#31350;&#21033;&#29992;&#25991;&#26412;&#26816;&#32034;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;&#19981;&#21516;&#20110;&#25991;&#26412;&#65292;&#22270;&#20687;&#22266;&#26377;&#22320;&#21253;&#21547;&#24120;&#35782;&#20449;&#24687;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#33268;&#21147;&#20110;&#26377;&#25928;&#21033;&#29992;&#23427;&#20204;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#26816;&#32034;&#65288;MORE&#65289;&#22686;&#24378;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#26469;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#30340;&#24120;&#35782;&#33021;&#21147;&#12290;&#22312;Common-Gen&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#22522;&#20110;&#21333;&#19968;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;MORE&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13625v1 Announce Type: new  Abstract: Since commonsense information has been recorded significantly less frequently than its existence, language models pre-trained by text generation have difficulty to learn sufficient commonsense knowledge. Several studies have leveraged text retrieval to augment the models' commonsense ability. Unlike text, images capture commonsense information inherently but little effort has been paid to effectively utilize them. In this work, we propose a novel Multi-mOdal REtrieval (MORE) augmentation framework, to leverage both text and images to enhance the commonsense ability of language models. Extensive experiments on the Common-Gen task have demonstrated the efficacy of MORE based on the pre-trained models of both single and multiple modalities.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#30417;&#30563;&#20302;&#36164;&#28304;&#20998;&#31867;&#25193;&#23637;&#30340;&#26041;&#27861; FLAME</title><link>https://arxiv.org/abs/2402.13623</link><description>&lt;p&gt;
FLAME&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#30417;&#30563;&#20302;&#36164;&#28304;&#20998;&#31867;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
FLAME: Self-Supervised Low-Resource Taxonomy Expansion using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13623
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#30417;&#30563;&#20302;&#36164;&#28304;&#20998;&#31867;&#25193;&#23637;&#30340;&#26041;&#27861; FLAME
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#27861;&#20195;&#34920;&#20102;&#19968;&#31181;&#26641;&#29366;&#23618;&#27425;&#32467;&#26500;&#65292;&#24314;&#31435;&#20102;&#23454;&#20307;&#38388;&#30340;&#20851;&#31995;&#20197;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#20256;&#36798;&#30693;&#35782;&#12290;&#20998;&#31867;&#27861;&#20013;&#30340;&#27599;&#20010;&#36793;&#20195;&#34920;&#20102;&#19968;&#20010;&#19978;&#20301;&#35789;-&#19979;&#20301;&#35789;&#20851;&#31995;&#12290;&#20998;&#31867;&#27861;&#22312;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#24456;&#26377;&#29992;&#65292;&#22914;&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#24341;&#25806;&#21644;&#25512;&#33616;&#31995;&#32479;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#22686;&#24378;&#36825;&#20123;&#20998;&#31867;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21487;&#29992;&#20154;&#21147;&#36164;&#28304;&#30340;&#38480;&#21046;&#21644;&#25968;&#25454;&#30340;&#25351;&#25968;&#22686;&#38271;&#65292;&#25163;&#21160;&#31579;&#36873;&#20855;&#26377;&#26368;&#26032;&#25968;&#25454;&#30340;&#20998;&#31867;&#27861;&#23384;&#22312;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#33258;&#21160;&#20998;&#31867;&#27861;&#25193;&#23637;&#26041;&#27861;&#21464;&#24471;&#36843;&#22312;&#30473;&#30571;&#12290;&#20256;&#32479;&#30340;&#30417;&#30563;&#20998;&#31867;&#27861;&#25193;&#23637;&#26041;&#27861;&#30001;&#20110;&#29616;&#26377;&#20998;&#31867;&#27861;&#35268;&#27169;&#36739;&#23567;&#65292;&#23548;&#33268;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#65292;&#32463;&#24120;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FLAME&#65292;&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#27861;&#25193;&#23637;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13623v1 Announce Type: new  Abstract: Taxonomies represent an arborescence hierarchical structure that establishes relationships among entities to convey knowledge within a specific domain. Each edge in the taxonomy signifies a hypernym-hyponym relationship. Taxonomies find utility in various real-world applications, such as e-commerce search engines and recommendation systems. Consequently, there arises a necessity to enhance these taxonomies over time. However, manually curating taxonomies with neoteric data presents challenges due to limitations in available human resources and the exponential growth of data. Therefore, it becomes imperative to develop automatic taxonomy expansion methods. Traditional supervised taxonomy expansion approaches encounter difficulties stemming from limited resources, primarily due to the small size of existing taxonomies. This scarcity of training data often leads to overfitting. In this paper, we propose FLAME, a novel approach for taxonomy 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;VLSP 2023&#20013;ComOM&#20219;&#21153;&#30340;&#19968;&#20010;&#25968;&#25454;&#25361;&#25112;&#65292;&#26088;&#22312;&#25512;&#21160;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#36890;&#36807;&#24320;&#21457;&#20174;&#36234;&#21335;&#20135;&#21697;&#35780;&#35770;&#20013;&#25552;&#21462;&#27604;&#36739;&#24847;&#35265;&#30340;&#25216;&#26415;&#65292;&#21442;&#19982;&#32773;&#38656;&#25552;&#20986;&#33021;&#22815;&#25552;&#21462;&#27604;&#36739;"&#20116;&#20803;&#32452;"&#30340;&#27169;&#22411;&#24182;&#26681;&#25454;F1&#20998;&#25968;&#36827;&#34892;&#35780;&#20272;&#25490;&#21517;&#12290;</title><link>https://arxiv.org/abs/2402.13613</link><description>&lt;p&gt;
VLSP 2023&#32508;&#36848;--ComOM&#20219;&#21153;&#65306;&#36234;&#21335;&#20135;&#21697;&#35780;&#35770;&#30340;&#27604;&#36739;&#24847;&#35265;&#25366;&#25496;&#25968;&#25454;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Overview of the VLSP 2023 -- ComOM Shared Task: A Data Challenge for Comparative Opinion Mining from Vietnamese Product Reviews
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13613
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;VLSP 2023&#20013;ComOM&#20219;&#21153;&#30340;&#19968;&#20010;&#25968;&#25454;&#25361;&#25112;&#65292;&#26088;&#22312;&#25512;&#21160;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#36890;&#36807;&#24320;&#21457;&#20174;&#36234;&#21335;&#20135;&#21697;&#35780;&#35770;&#20013;&#25552;&#21462;&#27604;&#36739;&#24847;&#35265;&#30340;&#25216;&#26415;&#65292;&#21442;&#19982;&#32773;&#38656;&#25552;&#20986;&#33021;&#22815;&#25552;&#21462;&#27604;&#36739;"&#20116;&#20803;&#32452;"&#30340;&#27169;&#22411;&#24182;&#26681;&#25454;F1&#20998;&#25968;&#36827;&#34892;&#35780;&#20272;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#36234;&#21335;&#35821;&#20135;&#21697;&#35780;&#35770;&#27604;&#36739;&#24847;&#35265;&#25366;&#25496;&#20849;&#20139;&#20219;&#21153;&#65288;ComOM&#65289;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#35813;&#20219;&#21153;&#20316;&#20026;&#31532;&#21313;&#23626;&#36234;&#21335;&#35821;&#35328;&#21644;&#35821;&#38899;&#22788;&#29702;&#22269;&#38469;&#30740;&#35752;&#20250;&#65288;VLSP 2023&#65289;&#30340;&#19968;&#37096;&#20998;&#20030;&#34892;&#12290;&#27492;&#20849;&#20139;&#20219;&#21153;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#24320;&#21457;&#33021;&#22815;&#26377;&#25928;&#20174;&#36234;&#21335;&#20135;&#21697;&#35780;&#35770;&#20013;&#25552;&#21462;&#27604;&#36739;&#24847;&#35265;&#30340;&#25216;&#26415;&#26469;&#25512;&#21160;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#21442;&#19982;&#32773;&#34987;&#25361;&#25112;&#25552;&#20986;&#33021;&#22815;&#20174;&#27604;&#36739;&#21477;&#20013;&#29087;&#32451;&#25552;&#21462;&#27604;&#36739;&#8220;&#20116;&#20803;&#32452;&#8221;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#20027;&#39064;&#12289;&#23458;&#20307;&#12289;&#26041;&#38754;&#12289;&#35859;&#35789;&#21644;&#27604;&#36739;&#31867;&#22411;&#26631;&#31614;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;120&#20010;&#25991;&#26723;&#30340;&#20154;&#24037;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;7427&#20010;&#38750;&#27604;&#36739;&#21477;&#21644;1798&#20010;&#21477;&#23376;&#20013;&#30340;2468&#20010;&#27604;&#36739;&#12290;&#21442;&#19982;&#30340;&#27169;&#22411;&#23558;&#26681;&#25454;&#20934;&#30830;&#21305;&#37197;&#23439;&#24179;&#22343;&#30340;&#20116;&#20803;&#32452;F1&#20998;&#25968;&#36827;&#34892;&#35780;&#20272;&#21644;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13613v1 Announce Type: new  Abstract: This paper presents a comprehensive overview of the Comparative Opinion Mining from Vietnamese Product Reviews shared task (ComOM), held as part of the 10$^{th}$ International Workshop on Vietnamese Language and Speech Processing (VLSP 2023). The primary objective of this shared task is to advance the field of natural language processing by developing techniques that proficiently extract comparative opinions from Vietnamese product reviews. Participants are challenged to propose models that adeptly extract a comparative "quintuple" from a comparative sentence, encompassing Subject, Object, Aspect, Predicate, and Comparison Type Label. We construct a human-annotated dataset comprising $120$ documents, encompassing $7427$ non-comparative sentences and $2468$ comparisons within $1798$ sentences. Participating models undergo evaluation and ranking based on the Exact match macro-averaged quintuple F1 score.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#22312;&#25968;&#25454;&#39537;&#21160;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#24320;&#21019;&#20102;&#31471;&#21040;&#31471;&#21457;&#29616;&#31995;&#32479;&#30340;&#26032;&#27169;&#24335;&#65292;&#21033;&#29992;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#25628;&#23547;&#21644;&#39564;&#35777;&#20551;&#35774;&#65292;&#31361;&#26174;&#20102;&#33258;&#21160;&#21270;&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13610</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#22312;&#31185;&#23398;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Data-driven Discovery with Large Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13610
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#22312;&#25968;&#25454;&#39537;&#21160;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#24320;&#21019;&#20102;&#31471;&#21040;&#31471;&#21457;&#29616;&#31995;&#32479;&#30340;&#26032;&#27169;&#24335;&#65292;&#21033;&#29992;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#25628;&#23547;&#21644;&#39564;&#35777;&#20551;&#35774;&#65292;&#31361;&#26174;&#20102;&#33258;&#21160;&#21270;&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#36895;&#24230;&#32047;&#31215;&#65292;&#23427;&#20316;&#20026;&#20419;&#36827;&#31185;&#23398;&#21457;&#29616;&#30340;&#28508;&#21147;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#25958;&#20419;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31038;&#21306;&#21033;&#29992;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#65288;LGMs&#65289;&#30340;&#33021;&#21147;&#65292;&#24320;&#21457;&#33258;&#21160;&#21270;&#31995;&#32479;&#29992;&#20110;&#31471;&#21040;&#31471;&#30340;&#25968;&#25454;&#39537;&#21160;&#21457;&#29616; -- &#19968;&#31181;&#33539;&#24335;&#65292;&#20174;&#25152;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#20013;&#32431;&#31929;&#25628;&#32034;&#21644;&#39564;&#35777;&#20551;&#35774;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#25910;&#38598;&#25110;&#29289;&#29702;&#23454;&#39564;&#12290;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#29702;&#24819;&#25968;&#25454;&#39537;&#21160;&#21457;&#29616;&#31995;&#32479;&#30340;&#20960;&#20010;&#26399;&#26395;&#26465;&#20214;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20351;&#29992;GPT-4&#30340;DATAVOYAGER&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LGMs&#22914;&#20309;&#23454;&#29616;&#20960;&#39033;&#36825;&#20123;&#26399;&#26395;&#26465;&#20214; -- &#36825;&#26159;&#20197;&#21069;&#26080;&#27861;&#20570;&#21040;&#30340;&#25104;&#23601; -- &#21516;&#26102;&#20063;&#31361;&#26174;&#20102;&#24403;&#21069;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#23616;&#38480;&#24615;&#65292;&#20174;&#32780;&#20026;&#24320;&#23637;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#25552;&#20379;&#20102;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13610v1 Announce Type: cross  Abstract: With the accumulation of data at an unprecedented rate, its potential to fuel scientific discovery is growing exponentially. This position paper urges the Machine Learning (ML) community to exploit the capabilities of large generative models (LGMs) to develop automated systems for end-to-end data-driven discovery -- a paradigm encompassing the search and verification of hypotheses purely from a set of provided datasets, without the need for additional data collection or physical experiments. We first outline several desiderata for an ideal data-driven discovery system. Then, through DATAVOYAGER, a proof-of-concept utilizing GPT-4, we demonstrate how LGMs fulfill several of these desiderata -- a feat previously unattainable -- while also highlighting important limitations in the current system that open up opportunities for novel ML research. We contend that achieving accurate, reliable, and robust end-to-end discovery systems solely th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#32622;&#20449;&#24230;&#35780;&#20272;&#30340;&#20840;&#38754;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#19994;&#22810;&#35821;&#35328;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#24182;&#30740;&#31350;&#20102;&#36825;&#20123;&#32622;&#20449;&#24230;&#20998;&#25968;&#22914;&#20309;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#65292;&#26368;&#32456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#32622;&#20449;&#24230;&#20272;&#35745;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.13606</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#32622;&#20449;&#24230;&#35780;&#20272;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13606
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#32622;&#20449;&#24230;&#35780;&#20272;&#30340;&#20840;&#38754;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#19994;&#22810;&#35821;&#35328;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#24182;&#30740;&#31350;&#20102;&#36825;&#20123;&#32622;&#20449;&#24230;&#20998;&#25968;&#22914;&#20309;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#65292;&#26368;&#32456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#32622;&#20449;&#24230;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24187;&#35273;&#24182;&#22312;&#39044;&#27979;&#20013;&#34920;&#29616;&#36807;&#20110;&#33258;&#20449;&#30340;&#20542;&#21521;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20854;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#12290;&#34920;&#26126;&#27169;&#22411;&#21709;&#24212;&#30340;&#21487;&#20449;&#24230;&#25110;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23545;&#20110;&#24320;&#21457;&#21487;&#38752;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#35821;&#20013;LLM&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#19978;&#65292;&#22312;&#20854;&#20182;&#24191;&#27867;&#20351;&#29992;&#30340;&#35821;&#35328;&#26041;&#38754;&#20173;&#23384;&#22312;&#31354;&#30333;&#65292;&#38459;&#30861;&#20102;&#21487;&#38752;AI&#24212;&#29992;&#30340;&#20840;&#29699;&#21457;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;LLM&#19978;&#30340;&#22810;&#35821;&#35328;&#32622;&#20449;&#24230;&#35780;&#20272;&#65288;MlingConf&#65289;&#30340;&#20840;&#38754;&#35843;&#26597;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32463;&#36807;&#35814;&#32454;&#26816;&#26597;&#30340;&#19987;&#19994;&#22810;&#35821;&#35328;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#24615;&#33021;&#65292;&#24182;&#30740;&#31350;&#36825;&#20123;&#32622;&#20449;&#24230;&#20998;&#25968;&#22914;&#20309;&#36890;&#36807;&#36328;&#19981;&#21516;&#35821;&#35328;&#30340;&#33258;&#25105;&#23436;&#21892;&#26469;&#22686;&#24378;LLM&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#32622;&#20449;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13606v1 Announce Type: new  Abstract: The tendency of Large Language Models to generate hallucinations and exhibit overconfidence in predictions raises concerns regarding their reliability. Confidence or uncertainty estimations indicating the extent of trustworthiness of a model's response are essential to developing reliable AI systems. Current research primarily focuses on LLM confidence estimations in English, remaining a void for other widely used languages and impeding the global development of reliable AI applications. This paper introduces a comprehensive investigation of Multi-lingual confidence estimation (MlingConf) on LLMs. First, we introduce an elaborated and expert-checked multilingual QA dataset. Second, we delve into the performance of confidence estimations and examine how these confidence scores can enhance LLM performance through self-refinement across diverse languages. Finally, we propose a cross-lingual confidence estimation method to achieve more preci
&lt;/p&gt;</description></item><item><title>KorNAT&#26159;&#39318;&#20010;&#29992;&#20110;&#35780;&#20272;&#38889;&#22269;&#22269;&#23478;&#23545;&#40784;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#31038;&#20250;&#20215;&#20540;&#35266;&#21644;&#24120;&#35782;&#23545;&#40784;&#20004;&#20010;&#26041;&#38754;&#65292;&#36890;&#36807;&#23545;&#31038;&#20250;&#20215;&#20540;&#21644;&#24120;&#35782;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#27979;&#35797;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.13605</link><description>&lt;p&gt;
KorNAT&#65306;&#38889;&#22269;&#31038;&#20250;&#20215;&#20540;&#35266;&#21644;&#24120;&#35782;&#30340;LLM&#23545;&#40784;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
KorNAT: LLM Alignment Benchmark for Korean Social Values and Common Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13605
&lt;/p&gt;
&lt;p&gt;
KorNAT&#26159;&#39318;&#20010;&#29992;&#20110;&#35780;&#20272;&#38889;&#22269;&#22269;&#23478;&#23545;&#40784;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#31038;&#20250;&#20215;&#20540;&#35266;&#21644;&#24120;&#35782;&#23545;&#40784;&#20004;&#20010;&#26041;&#38754;&#65292;&#36890;&#36807;&#23545;&#31038;&#20250;&#20215;&#20540;&#21644;&#24120;&#35782;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#27979;&#35797;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29305;&#23450;&#22269;&#23478;&#24471;&#20197;&#26377;&#25928;&#37096;&#32626;&#65292;&#23427;&#20204;&#24517;&#39035;&#20855;&#26377;&#23545;&#35813;&#22269;&#25991;&#21270;&#21644;&#22522;&#26412;&#30693;&#35782;&#30340;&#29702;&#35299;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22269;&#23478;&#23545;&#40784;&#65288;National Alignment&#65289;&#65292;&#20174;&#31038;&#20250;&#20215;&#20540;&#35266;&#23545;&#40784;&#21644;&#24120;&#35782;&#23545;&#40784;&#20004;&#20010;&#26041;&#38754;&#34913;&#37327;LLM&#19982;&#30446;&#26631;&#22269;&#23478;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#31038;&#20250;&#20215;&#20540;&#35266;&#23545;&#40784;&#35780;&#20272;&#27169;&#22411;&#23545;&#29305;&#23450;&#22269;&#23478;&#31038;&#20250;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#31243;&#24230;&#65292;&#32780;&#24120;&#35782;&#23545;&#40784;&#21017;&#26816;&#39564;&#27169;&#22411;&#23545;&#30456;&#20851;&#22522;&#26412;&#22269;&#23478;&#30693;&#35782;&#30340;&#25226;&#25569;&#24773;&#20917;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;KorNAT&#65292;&#36825;&#26159;&#39318;&#20010;&#34913;&#37327;&#19982;&#38889;&#22269;&#22269;&#23478;&#23545;&#40784;&#30340;&#22522;&#20934;&#12290;&#23545;&#20110;&#31038;&#20250;&#20215;&#20540;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20174;&#21253;&#25324;6174&#21517;&#38889;&#22269;&#21442;&#19982;&#32773;&#22312;&#20869;&#30340;&#22823;&#35268;&#27169;&#35843;&#26597;&#20013;&#33719;&#24471;&#20102;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#12290;&#23545;&#20110;&#24120;&#35782;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#22522;&#20110;&#38889;&#22269;&#25945;&#31185;&#20070;&#21644;GED&#21442;&#32771;&#36164;&#26009;&#26500;&#24314;&#20102;&#26679;&#26412;&#12290;KorNAT&#21253;&#21547;4K&#21644;6K&#20010;&#38024;&#23545;&#31038;&#20250;&#20215;&#20540;&#21644;&#24120;&#35782;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13605v1 Announce Type: new  Abstract: For Large Language Models (LLMs) to be effectively deployed in a specific country, they must possess an understanding of the nation's culture and basic knowledge. To this end, we introduce National Alignment, which measures an alignment between an LLM and a targeted country from two aspects: social value alignment and common knowledge alignment. Social value alignment evaluates how well the model understands nation-specific social values, while common knowledge alignment examines how well the model captures basic knowledge related to the nation. We constructed KorNAT, the first benchmark that measures national alignment with South Korea. For the social value dataset, we obtained ground truth labels from a large-scale survey involving 6,174 unique Korean participants. For the common knowledge dataset, we constructed samples based on Korean textbooks and GED reference materials. KorNAT contains 4K and 6K multiple-choice questions for socia
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;OccCANINE&#24037;&#20855;&#65292;&#25105;&#20204;&#25104;&#21151;&#25171;&#30772;&#20102;HISCO&#38556;&#30861;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#21270;&#32844;&#19994;&#26631;&#20934;&#21270;&#65292;&#20174;&#32780;&#22823;&#22823;&#31616;&#21270;&#20102;&#23545;&#32844;&#19994;&#25551;&#36848;&#30340;&#22788;&#29702;&#21644;&#20998;&#31867;&#36807;&#31243;&#65292;&#20026;&#32463;&#27982;&#23398;&#12289;&#32463;&#27982;&#21382;&#21490;&#31561;&#39046;&#22495;&#30340;&#32844;&#19994;&#32467;&#26500;&#20998;&#26512;&#25552;&#20379;&#20102;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.13604</link><description>&lt;p&gt;
&#25171;&#30772;HISCO&#38556;&#30861;&#65306;&#20351;&#29992;OccCANINE&#36827;&#34892;&#33258;&#21160;&#32844;&#19994;&#26631;&#20934;&#21270;
&lt;/p&gt;
&lt;p&gt;
Breaking the HISCO Barrier: Automatic Occupational Standardization with OccCANINE
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13604
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;OccCANINE&#24037;&#20855;&#65292;&#25105;&#20204;&#25104;&#21151;&#25171;&#30772;&#20102;HISCO&#38556;&#30861;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#21270;&#32844;&#19994;&#26631;&#20934;&#21270;&#65292;&#20174;&#32780;&#22823;&#22823;&#31616;&#21270;&#20102;&#23545;&#32844;&#19994;&#25551;&#36848;&#30340;&#22788;&#29702;&#21644;&#20998;&#31867;&#36807;&#31243;&#65292;&#20026;&#32463;&#27982;&#23398;&#12289;&#32463;&#27982;&#21382;&#21490;&#31561;&#39046;&#22495;&#30340;&#32844;&#19994;&#32467;&#26500;&#20998;&#26512;&#25552;&#20379;&#20102;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#24037;&#20855;OccCANINE&#65292;&#21487;&#33258;&#21160;&#23558;&#32844;&#19994;&#25551;&#36848;&#36716;&#25442;&#20026;HISCO&#20998;&#31867;&#31995;&#32479;&#12290;&#22788;&#29702;&#21644;&#20998;&#31867;&#32844;&#19994;&#25551;&#36848;&#28041;&#21450;&#30340;&#25163;&#21160;&#24037;&#20316;&#23481;&#26131;&#20986;&#38169;&#12289;&#32321;&#29712;&#19988;&#32791;&#26102;&#12290;&#25105;&#20204;&#23545;&#19968;&#20010;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;CANINE&#65289;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#20960;&#31186;&#38047;&#21040;&#20960;&#20998;&#38047;&#20869;&#33258;&#21160;&#23436;&#25104;&#27492;&#36807;&#31243;&#65292;&#32780;&#20197;&#21069;&#38656;&#35201;&#25968;&#22825;&#29978;&#33267;&#25968;&#21608;&#12290;&#35813;&#27169;&#22411;&#22312;&#26469;&#33258;22&#20010;&#19981;&#21516;&#26469;&#28304;&#36129;&#29486;&#30340;13&#31181;&#35821;&#35328;&#20013;&#30340;1400&#19975;&#23545;&#32844;&#19994;&#25551;&#36848;&#21644;HISCO&#20195;&#30721;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#31934;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;&#20934;&#30830;&#29575;&#22343;&#36229;&#36807;90%&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#31361;&#30772;&#20102;&#35937;&#24449;&#24615;HISCO&#38556;&#30861;&#65292;&#24182;&#20351;&#36825;&#20123;&#25968;&#25454;&#21487;&#20379;&#32463;&#27982;&#23398;&#12289;&#32463;&#27982;&#21382;&#21490;&#21644;&#21508;&#31181;&#30456;&#20851;&#23398;&#31185;&#20013;&#30340;&#32844;&#19994;&#32467;&#26500;&#20998;&#26512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13604v1 Announce Type: new  Abstract: This paper introduces a new tool, OccCANINE, to automatically transform occupational descriptions into the HISCO classification system. The manual work involved in processing and classifying occupational descriptions is error-prone, tedious, and time-consuming. We finetune a preexisting language model (CANINE) to do this automatically thereby performing in seconds and minutes what previously took days and weeks. The model is trained on 14 million pairs of occupational descriptions and HISCO codes in 13 different languages contributed by 22 different sources. Our approach is shown to have accuracy, recall and precision above 90 percent. Our tool breaks the metaphorical HISCO barrier and makes this data readily available for analysis of occupational structures with broad applicability in economics, economic history and various related disciplines.
&lt;/p&gt;</description></item><item><title>User-LLM&#26694;&#26550;&#21033;&#29992;&#29992;&#25143;&#23884;&#20837;&#23545;LLMs&#36827;&#34892;&#35821;&#22659;&#21270;&#65292;&#20351;&#20854;&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#29992;&#25143;&#19978;&#19979;&#25991;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.13598</link><description>&lt;p&gt;
User-LLM: &#21033;&#29992;&#29992;&#25143;&#23884;&#20837;&#23454;&#29616;&#26377;&#25928;&#30340;LLM&#35821;&#22659;&#21270;
&lt;/p&gt;
&lt;p&gt;
User-LLM: Efficient LLM Contextualization with User Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13598
&lt;/p&gt;
&lt;p&gt;
User-LLM&#26694;&#26550;&#21033;&#29992;&#29992;&#25143;&#23884;&#20837;&#23545;LLMs&#36827;&#34892;&#35821;&#22659;&#21270;&#65292;&#20351;&#20854;&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#29992;&#25143;&#19978;&#19979;&#25991;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#25972;&#21512;&#22797;&#26434;&#19988;&#28508;&#22312;&#22024;&#26434;&#30340;&#29992;&#25143;&#20132;&#20114;&#25968;&#25454;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;User-LLM&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29992;&#25143;&#23884;&#20837;&#26469;&#23545;LLMs&#36827;&#34892;&#35821;&#22659;&#21270;&#12290;&#36825;&#20123;&#23884;&#20837;&#26159;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20174;&#21508;&#31181;&#29992;&#25143;&#20132;&#20114;&#20013;&#31934;&#28860;&#20986;&#26469;&#30340;&#65292;&#33021;&#22815;&#25429;&#25417;&#28508;&#22312;&#29992;&#25143;&#20559;&#22909;&#21450;&#20854;&#38543;&#26102;&#38388;&#30340;&#28436;&#21464;&#12290;&#25105;&#20204;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#36719;&#25552;&#31034;&#23558;&#36825;&#20123;&#29992;&#25143;&#23884;&#20837;&#19982;LLMs&#38598;&#25104;&#36215;&#26469;&#65292;&#20351;LLMs&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#29992;&#25143;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#22312;MovieLens&#12289;&#20122;&#39532;&#36874;&#35780;&#35770;&#21644;&#35895;&#27468;&#26412;&#22320;&#35780;&#35770;&#31561;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#21644;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#29992;&#25143;&#30340;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#30340;&#35821;&#22659;&#21270;&#65292;&#21516;&#26102;&#22312;&#35745;&#31639;&#19978;&#20063;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13598v1 Announce Type: cross  Abstract: Large language models (LLMs) have revolutionized natural language processing. However, effectively incorporating complex and potentially noisy user interaction data remains a challenge. To address this, we propose User-LLM, a novel framework that leverages user embeddings to contextualize LLMs. These embeddings, distilled from diverse user interactions using self-supervised pretraining, capture latent user preferences and their evolution over time. We integrate these user embeddings with LLMs through cross-attention and soft-prompting, enabling LLMs to dynamically adapt to user context. Our comprehensive experiments on MovieLens, Amazon Review, and Google Local Review datasets demonstrate significant performance gains across various tasks. Notably, our approach outperforms text-prompt-based contextualization on long sequence tasks and tasks that require deep user understanding while being computationally efficient. We further incorpora
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#30340;&#26041;&#27861;GLAME&#65292;&#33021;&#22815;&#35299;&#20915;&#32534;&#36753;&#26102;&#30693;&#35782;&#21464;&#21270;&#21512;&#24182;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#32534;&#36753;&#21518;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.13593</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Enhanced Large Language Model Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13593
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#30340;&#26041;&#27861;GLAME&#65292;&#33021;&#22815;&#35299;&#20915;&#32534;&#36753;&#26102;&#30693;&#35782;&#21464;&#21270;&#21512;&#24182;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#32534;&#36753;&#21518;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#21160;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#19981;&#20934;&#30830;&#21644;&#36807;&#26102;&#30693;&#35782;&#30340;&#24433;&#21709;&#12290;&#27169;&#22411;&#32534;&#36753;&#20986;&#29616;&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32534;&#36753;&#26041;&#27861;&#24456;&#38590;&#36319;&#36394;&#21644;&#21512;&#24182;&#19982;&#32534;&#36753;&#30456;&#20851;&#30340;&#30693;&#35782;&#21464;&#21270;&#65292;&#36825;&#38480;&#21046;&#20102;&#21518;&#26399;&#32534;&#36753;&#30340;LLMs&#22312;&#22788;&#29702;&#32534;&#36753;&#30693;&#35782;&#26102;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;LLM&#32534;&#36753;&#30340;&#26032;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#21517;&#20026;GLAME&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#27169;&#22359;&#65292;&#25581;&#31034;&#30001;&#20110;&#32534;&#36753;&#32780;&#21457;&#29983;&#21464;&#21270;&#30340;&#30456;&#20851;&#30693;&#35782;&#65292;&#33719;&#24471;LLMs&#20869;&#37096;&#34920;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;LLMs&#20869;&#30340;&#30693;&#35782;&#21464;&#21270;&#36890;&#36807;&#22806;&#37096;&#22270;&#32467;&#26500;&#21453;&#26144;&#20986;&#26469;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#30693;&#35782;&#32534;&#36753;&#27169;&#22359;&#26469;&#38598;&#25104;str
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13593v1 Announce Type: new  Abstract: Large language models (LLMs) are pivotal in advancing natural language processing (NLP) tasks, yet their efficacy is hampered by inaccuracies and outdated knowledge. Model editing emerges as a promising solution to address these challenges. However, existing editing methods struggle to track and incorporate changes in knowledge associated with edits, which limits the generalization ability of postedit LLMs in processing edited knowledge. To tackle these problems, we propose a novel model editing method that leverages knowledge graphs for enhancing LLM editing, namely GLAME. Specifically, we first utilize a knowledge graph augmentation module to uncover associated knowledge that has changed due to editing, obtaining its internal representations within LLMs. This approach allows knowledge alterations within LLMs to be reflected through an external graph structure. Subsequently, we design a graph-based knowledge edit module to integrate str
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#25551;&#36848;&#29983;&#25104;&#30340;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#35843;&#25972;&#26041;&#27861;ModICT&#65292;&#36890;&#36807;&#24341;&#20837;&#30456;&#20284;&#20135;&#21697;&#26679;&#26412;&#21644;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#26088;&#22312;&#35299;&#20915;&#29983;&#25104;&#25551;&#36848;&#20013;&#24120;&#35265;&#19988;&#24573;&#30053;&#20135;&#21697;&#29305;&#24449;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.13587</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#25551;&#36848;&#29983;&#25104;&#30340;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#35843;&#25972;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Multimodal In-Context Tuning Approach for E-Commerce Product Description Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13587
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#25551;&#36848;&#29983;&#25104;&#30340;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#35843;&#25972;&#26041;&#27861;ModICT&#65292;&#36890;&#36807;&#24341;&#20837;&#30456;&#20284;&#20135;&#21697;&#26679;&#26412;&#21644;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#26088;&#22312;&#35299;&#20915;&#29983;&#25104;&#25551;&#36848;&#20013;&#24120;&#35265;&#19988;&#24573;&#30053;&#20135;&#21697;&#29305;&#24449;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#32622;&#65292;&#29992;&#20110;&#20174;&#22270;&#20687;&#20013;&#29983;&#25104;&#20135;&#21697;&#25551;&#36848;&#65292;&#20854;&#20013;&#21253;&#21547;&#33829;&#38144;&#20851;&#38190;&#35789;&#12290;&#23427;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#32508;&#21512;&#33021;&#21147;&#65292;&#21019;&#24314;&#26356;&#21152;&#31526;&#21512;&#20135;&#21697;&#29420;&#29305;&#29305;&#24615;&#30340;&#25551;&#36848;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#35843;&#25972;&#26041;&#27861;ModICT&#65292;&#36890;&#36807;&#24341;&#20837;&#30456;&#20284;&#30340;&#20135;&#21697;&#26679;&#26412;&#20316;&#20026;&#21442;&#32771;&#65292;&#24182;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13587v1 Announce Type: new  Abstract: In this paper, we propose a new setting for generating product descriptions from images, augmented by marketing keywords. It leverages the combined power of visual and textual information to create descriptions that are more tailored to the unique features of products. For this setting, previous methods utilize visual and textual encoders to encode the image and keywords and employ a language model-based decoder to generate the product description. However, the generated description is often inaccurate and generic since same-category products have similar copy-writings, and optimizing the overall framework on large-scale samples makes models concentrate on common words yet ignore the product features. To alleviate the issue, we present a simple and effective Multimodal In-Context Tuning approach, named ModICT, which introduces a similar product sample as the reference and utilizes the in-context learning capability of language models to 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;WinoViz&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#23545;&#23545;&#35937;&#22312;&#19981;&#21516;&#29366;&#24577;&#19979;&#30340;&#35270;&#35273;&#23646;&#24615;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35813;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#35201;&#27714;&#23454;&#29992;&#25512;&#29702;&#21644;&#35270;&#35273;&#30693;&#35782;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.13584</link><description>&lt;p&gt;
WinoViz&#65306;&#25506;&#31350;&#23545;&#35937;&#22312;&#19981;&#21516;&#29366;&#24577;&#19979;&#30340;&#35270;&#35273;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
WinoViz: Probing Visual Properties of Objects Under Different States
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13584
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;WinoViz&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#23545;&#23545;&#35937;&#22312;&#19981;&#21516;&#29366;&#24577;&#19979;&#30340;&#35270;&#35273;&#23646;&#24615;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35813;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#35201;&#27714;&#23454;&#29992;&#25512;&#29702;&#21644;&#35270;&#35273;&#30693;&#35782;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26681;&#25454;&#29305;&#23450;&#35821;&#22659;&#24863;&#30693;&#21644;&#29702;&#35299;&#23545;&#35937;&#30340;&#19981;&#21516;&#35270;&#35273;&#23646;&#24615;&#12290;&#20197;&#39321;&#34121;&#20026;&#20363;&#65292;&#24403;&#21464;&#36136;&#26102;&#25105;&#20204;&#30693;&#36947;&#23427;&#20250;&#21464;&#25104;&#35088;&#33394;&#65292;&#32780;&#26410;&#25104;&#29087;&#26102;&#23427;&#21576;&#29616;&#32511;&#33394;&#12290;&#20808;&#21069;&#20851;&#20110;&#25506;&#31350;&#35270;&#35273;&#24120;&#35782;&#30693;&#35782;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#26816;&#39564;&#35821;&#35328;&#27169;&#22411;&#23545;&#23545;&#35937;&#30340;&#20856;&#22411;&#23646;&#24615;&#65288;&#22914;&#39068;&#33394;&#21644;&#24418;&#29366;&#65289;&#30340;&#29702;&#35299;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;WinoViz&#65292;&#19968;&#20010;&#20165;&#21253;&#21547;&#25991;&#26412;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1,380&#20010;&#20363;&#23376;&#65292;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#23545;&#23545;&#35937;&#22312;&#19981;&#21516;&#35821;&#22659;&#25110;&#29366;&#24577;&#19979;&#30340;&#21464;&#24322;&#35270;&#35273;&#23646;&#24615;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#23454;&#29992;&#25512;&#29702;&#65288;&#25214;&#21040;&#39044;&#26399;&#24847;&#20041;&#65289;&#21644;&#35270;&#35273;&#30693;&#35782;&#25512;&#29702;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#22810;&#36339;&#25968;&#25454;&#65292;&#36825;&#26159;&#25105;&#20204;&#25968;&#25454;&#30340;&#26356;&#20855;&#25361;&#25112;&#24615;&#29256;&#26412;&#65292;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#38142;&#26469;&#35299;&#20915;&#25105;&#20204;&#30340;&#20219;&#21153;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#26159;&#65306;a&#65289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13584v1 Announce Type: new  Abstract: Humans perceive and comprehend different visual properties of an object based on specific contexts. For instance, we know that a banana turns brown ``when it becomes rotten,'' whereas it appears green ``when it is unripe.'' Previous studies on probing visual commonsense knowledge have primarily focused on examining language models' understanding of typical properties (e.g., colors and shapes) of objects. We present WinoViz, a text-only evaluation dataset, consisting of 1,380 examples that probe the reasoning abilities of language models regarding variant visual properties of objects under different contexts or states. Our task is challenging since it requires pragmatic reasoning (finding intended meanings) and visual knowledge reasoning. We also present multi-hop data, a more challenging version of our data, which requires multi-step reasoning chains to solve our task. In our experimental analysis, our findings are: a) Large language mod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#38271;&#25991;&#26412;&#35780;&#20272;&#30340;&#24046;&#36317;&#65292;&#24341;&#20837;&#20102;&#19968;&#22871;&#22522;&#20110;&#36830;&#36143;&#24615;&#12289;&#20957;&#32858;&#21147;&#21644;&#22797;&#26434;&#24615;&#31561;&#35821;&#35328;&#23398;&#32500;&#24230;&#30340;&#25351;&#26631;&#26469;&#31995;&#32479;&#24615;&#34913;&#37327;&#38271;&#25991;&#26412;&#30340;&#36136;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;LongWanjuan&#25968;&#25454;&#38598;&#65292;&#26377;&#21161;&#20110;&#25552;&#21319;&#38271;&#25991;&#26412;&#20219;&#21153;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.13583</link><description>&lt;p&gt;
LongWanjuan: &#38754;&#21521;&#38271;&#25991;&#26412;&#36136;&#37327;&#30340;&#31995;&#32479;&#21270;&#34913;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LongWanjuan: Towards Systematic Measurement for Long Text Quality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#38271;&#25991;&#26412;&#35780;&#20272;&#30340;&#24046;&#36317;&#65292;&#24341;&#20837;&#20102;&#19968;&#22871;&#22522;&#20110;&#36830;&#36143;&#24615;&#12289;&#20957;&#32858;&#21147;&#21644;&#22797;&#26434;&#24615;&#31561;&#35821;&#35328;&#23398;&#32500;&#24230;&#30340;&#25351;&#26631;&#26469;&#31995;&#32479;&#24615;&#34913;&#37327;&#38271;&#25991;&#26412;&#30340;&#36136;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;LongWanjuan&#25968;&#25454;&#38598;&#65292;&#26377;&#21161;&#20110;&#25552;&#21319;&#38271;&#25991;&#26412;&#20219;&#21153;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#23545;&#20110;&#22686;&#24378;&#22522;&#30784;&#27169;&#22411;&#30340;&#38271;&#25991;&#26412;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#36890;&#36807;&#21551;&#21457;&#24335;&#35268;&#21017;&#21644;&#22522;&#20110;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#38590;&#24230;&#30340;&#35780;&#20272;&#26469;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#30340;&#29616;&#26377;&#21162;&#21147;&#65292;&#20294;&#32570;&#20047;&#19987;&#38376;&#38024;&#23545;&#38271;&#25991;&#26412;&#35780;&#20272;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#38024;&#23545;&#36825;&#19968;&#24046;&#36317;&#65292;&#36890;&#36807;&#35780;&#20272;&#19977;&#20010;&#22522;&#26412;&#35821;&#35328;&#23398;&#32500;&#24230;&#65288;&#36830;&#36143;&#24615;&#12289;&#20957;&#32858;&#21147;&#21644;&#22797;&#26434;&#24615;&#65289;&#31995;&#32479;&#24615;&#34913;&#37327;&#38271;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;&#21463;&#21040;&#36825;&#19977;&#20010;&#32500;&#24230;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#22871;&#26088;&#22312;&#35780;&#20272;&#38271;&#25991;&#26412;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#21253;&#25324;&#32479;&#35745;&#21644;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#26631;&#12290;&#21033;&#29992;&#36825;&#20123;&#25351;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LongWanjuan&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#38271;&#25991;&#26412;&#20219;&#21153;&#35757;&#32451;&#30340;&#21452;&#35821;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;1600&#20159;&#26631;&#35760;&#12290;&#22312;LongWanjuan&#20013;&#65292;&#25105;&#20204;&#23558;&#38271;&#25991;&#26412;&#20998;&#31867;&#20026;&#25972;&#20307;&#24615;&#12289;&#27719;&#24635;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13583v1 Announce Type: new  Abstract: The quality of training data are crucial for enhancing the long-text capabilities of foundation models. Despite existing efforts to refine data quality through heuristic rules and evaluations based on data diversity and difficulty, there's a lack of systematic approaches specifically tailored for assessing long texts. Addressing this gap, our work systematically measures the quality of long texts by evaluating three fundamental linguistic dimensions: coherence, cohesion, and complexity. Drawing inspiration from the aforementioned three dimensions, we introduce a suite of metrics designed to evaluate the quality of long texts, encompassing both statistical and pre-trained language model-based ones. Leveraging these metrics, we present LongWanjuan, a bilingual dataset specifically tailored to enhance the training of language models for long-text tasks with over 160B tokens. In LongWanjuan, we categorize long texts into holistic, aggregated
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#27169;&#24577;&#34892;&#20026;&#23545;&#40784;&#65288;BBA&#65289;&#25552;&#31034;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;DSL&#22312;&#22686;&#24378;&#22797;&#26434;&#22810;&#27169;&#24577;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.13577</link><description>&lt;p&gt;
BBA: &#21452;&#27169;&#24577;&#34892;&#20026;&#23545;&#40784;&#29992;&#20110;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
BBA: Bi-Modal Behavioral Alignment for Reasoning with Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#27169;&#24577;&#34892;&#20026;&#23545;&#40784;&#65288;BBA&#65289;&#25552;&#31034;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;DSL&#22312;&#22686;&#24378;&#22797;&#26434;&#22810;&#27169;&#24577;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25512;&#29702;&#26159;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#19968;&#20010;&#20851;&#38190;&#33021;&#21147;&#12290;&#19982;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#65288;DSL&#65289;&#30340;&#25972;&#21512;&#25552;&#20379;&#20102;&#31934;&#30830;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#20351;&#36825;&#20123;&#27169;&#22411;&#26377;&#26426;&#20250;&#22312;&#22797;&#26434;&#21644;&#19987;&#19994;&#39046;&#22495;&#25191;&#34892;&#26356;&#20934;&#30830;&#30340;&#25512;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21452;&#27169;&#24577;&#34892;&#20026;&#23545;&#40784;&#65288;BBA&#65289;&#25552;&#31034;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;DSL&#22312;&#22686;&#24378;&#22797;&#26434;&#22810;&#27169;&#24577;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13577v1 Announce Type: new  Abstract: Multimodal reasoning stands as a pivotal capability for large vision-language models (LVLMs). The integration with Domain-Specific Languages (DSL), offering precise visual representations, equips these models with the opportunity to execute more accurate reasoning in complex and professional domains. However, the vanilla Chain-of-Thought (CoT) prompting method faces challenges in effectively leveraging the unique strengths of visual and DSL representations, primarily due to their differing reasoning mechanisms. Additionally, it often falls short in addressing critical steps in multi-step reasoning tasks. To mitigate these challenges, we introduce the \underline{B}i-Modal \underline{B}ehavioral \underline{A}lignment (BBA) prompting method, designed to maximize the potential of DSL in augmenting complex multi-modal reasoning tasks. This method initiates by guiding LVLMs to create separate reasoning chains for visual and DSL representations
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;31&#31181;&#21335;&#20122;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#20849;&#25351;&#35299;&#26512;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#25104;&#24037;&#20855;&#36827;&#34892;&#35757;&#32451;&#21644;&#23545;&#40784;&#65292;&#22312;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#20849;&#25351;&#35299;&#26512;&#27169;&#22411;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.13571</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#21335;&#20122;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#20849;&#25351;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Multilingual Coreference Resolution in Low-resource South Asian Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13571
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;31&#31181;&#21335;&#20122;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#20849;&#25351;&#35299;&#26512;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#25104;&#24037;&#20855;&#36827;&#34892;&#35757;&#32451;&#21644;&#23545;&#40784;&#65292;&#22312;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#20849;&#25351;&#35299;&#26512;&#27169;&#22411;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#25351;&#35299;&#26512;&#28041;&#21450;&#35782;&#21035;&#22312;&#35805;&#35821;&#20013;&#25351;&#21521;&#21516;&#19968;&#29616;&#23454;&#23454;&#20307;&#30340;&#25991;&#26412;&#29255;&#27573;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#36825;&#19968;&#20219;&#21153;&#22312;&#33521;&#35821;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#21335;&#20122;&#35821;&#35328;&#20013;&#65292;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#20849;&#25351;&#35299;&#26512;&#36164;&#28304;&#21644;&#27169;&#22411;&#30456;&#23545;&#31232;&#32570;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#25104;&#30340;&#32763;&#35793;&#21644;&#35789;&#23545;&#40784;&#24037;&#20855;&#65292;&#22312;31&#31181;&#21335;&#20122;&#35821;&#35328;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#22810;&#35821;&#35328;&#20849;&#25351;&#35299;&#26512;&#30340;&#32763;&#35793;&#25968;&#25454;&#38598;&#65288;TransMuCoRes&#65289;&#12290;&#20960;&#20046;&#25152;&#26377;&#39044;&#27979;&#30340;&#32763;&#35793;&#37117;&#36890;&#36807;&#20102;&#21512;&#29702;&#24615;&#26816;&#26597;&#65292;75%&#30340;&#33521;&#35821;&#21442;&#32771;&#25991;&#29486;&#19982;&#20854;&#39044;&#27979;&#30340;&#32763;&#35793;&#30456;&#23545;&#24212;&#12290;&#21033;&#29992;&#22810;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#20004;&#31181;&#29616;&#25104;&#30340;&#20849;&#25351;&#35299;&#26512;&#27169;&#22411;&#65292;&#23558;TransMuCoRes&#19982;&#24102;&#26377;&#25163;&#21160;&#27880;&#37322;&#30340;&#21360;&#22320;&#35821;&#20849;&#25351;&#35299;&#26512;&#25968;&#25454;&#38598;&#25340;&#25509;&#22312;&#19968;&#36215;&#12290;&#26368;&#20339;&#34920;&#29616;&#27169;&#22411;&#22312;LEA F1&#21644;CoNLL F1&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;64&#21644;68&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13571v1 Announce Type: cross  Abstract: Coreference resolution involves the task of identifying text spans within a discourse that pertain to the same real-world entity. While this task has been extensively explored in the English language, there has been a notable scarcity of publicly accessible resources and models for coreference resolution in South Asian languages. We introduce a Translated dataset for Multilingual Coreference Resolution (TransMuCoRes) in 31 South Asian languages using off-the-shelf tools for translation and word-alignment. Nearly all of the predicted translations successfully pass a sanity check, and 75% of English references align with their predicted translations. Using multilingual encoders, two off-the-shelf coreference resolution models were trained on a concatenation of TransMuCoRes and a Hindi coreference resolution dataset with manual annotations. The best performing model achieved a score of 64 and 68 for LEA F1 and CoNLL F1, respectively, on o
&lt;/p&gt;</description></item><item><title>&#22810;&#28304;&#35821;&#35328;&#35757;&#32451;&#65288;MSLT&#65289;&#25216;&#26415;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#28304;&#35821;&#35328;&#65292;&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#20013;&#22686;&#21152;&#20102;&#19981;&#21516;&#35821;&#35328;&#23884;&#20837;&#31354;&#38388;&#30340;&#20132;&#32455;&#65292;&#20174;&#32780;&#25903;&#25345;&#20102;XLT&#21463;&#30410;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#35828;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.13562</link><description>&lt;p&gt;
&#22810;&#28304;&#35821;&#35328;&#35757;&#32451;&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#20013;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Multi-Source Language Training in Cross-Lingual Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13562
&lt;/p&gt;
&lt;p&gt;
&#22810;&#28304;&#35821;&#35328;&#35757;&#32451;&#65288;MSLT&#65289;&#25216;&#26415;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#28304;&#35821;&#35328;&#65292;&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#20013;&#22686;&#21152;&#20102;&#19981;&#21516;&#35821;&#35328;&#23884;&#20837;&#31354;&#38388;&#30340;&#20132;&#32455;&#65292;&#20174;&#32780;&#25903;&#25345;&#20102;XLT&#21463;&#30410;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#35828;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#22320;&#23558;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#35843;&#25972;&#21040;&#29305;&#23450;&#35821;&#35328;-&#20219;&#21153;&#23545;&#19978;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#23450;&#21046;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#34429;&#28982;&#36328;&#35821;&#35328;&#36716;&#31227;&#65288;XLT&#65289;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#36825;&#31181;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#20294;&#20851;&#20110;&#20854;&#26377;&#25928;&#24615;&#32972;&#21518;&#30340;&#26426;&#21046;&#20173;&#23384;&#22312;&#25345;&#32493;&#30340;&#35752;&#35770;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#20102;&#20851;&#20110;XLT&#20869;&#37096;&#24037;&#20316;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#20551;&#35774;&#65292;&#21363;&#23427;&#40723;&#21169;&#22810;&#35821;&#35328;LMs&#26356;&#21152;&#24378;&#35843;&#35821;&#35328;&#19981;&#21487;&#30693;&#25110;&#20219;&#21153;&#29305;&#23450;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#23519;XLT&#38543;&#28041;&#21450;&#36807;&#31243;&#20013;&#28304;&#35821;&#35328;&#25968;&#37327;&#30340;&#21464;&#21270;&#32780;&#25913;&#21464;&#30340;&#27169;&#24335;&#26469;&#27979;&#35797;&#36825;&#19968;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;XLT&#20013;&#20351;&#29992;&#22810;&#20010;&#28304;&#35821;&#35328;-&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;&#22810;&#28304;&#35821;&#35328;&#35757;&#32451;&#65288;MSLT&#65289;&#30340;&#25216;&#26415;-&#20250;&#23548;&#33268;&#19981;&#21516;&#35821;&#35328;&#30340;&#23884;&#20837;&#31354;&#38388;&#30340;&#20132;&#32455;&#22686;&#21152;&#65292;&#25903;&#25345;&#20102;XLT&#21463;&#30410;&#20110;&#36825;&#19968;&#28857;&#30340;&#35828;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13562v1 Announce Type: new  Abstract: The successful adaptation of multilingual language models (LMs) to a specific language-task pair critically depends on the availability of data tailored for that condition. While cross-lingual transfer (XLT) methods have contributed to addressing this data scarcity problem, there still exists ongoing debate about the mechanisms behind their effectiveness. In this work, we focus on one of promising assumptions about inner workings of XLT, that it encourages multilingual LMs to place greater emphasis on language-agnostic or task-specific features. We test this hypothesis by examining how the patterns of XLT change with a varying number of source languages involved in the process. Our experimental findings show that the use of multiple source languages in XLT-a technique we term Multi-Source Language Training (MSLT)-leads to increased mingling of embedding spaces for different languages, supporting the claim that XLT benefits from making us
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35748;&#30693;&#35270;&#35273;&#35821;&#35328;&#26144;&#23556;&#22120;&#65288;CVLM&#65289;&#65292;&#36890;&#36807;&#22686;&#24378;&#35270;&#35273;&#30693;&#35782;&#23545;&#40784;&#65292;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#25361;&#25112;&#30693;&#35782;&#22411;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.13561</link><description>&lt;p&gt;
&#35748;&#30693;&#35270;&#35273;&#35821;&#35328;&#26144;&#23556;&#22120;&#65306;&#36890;&#36807;&#22686;&#24378;&#35270;&#35273;&#30693;&#35782;&#23545;&#40784;&#25512;&#36827;&#22810;&#27169;&#24577;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13561
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35748;&#30693;&#35270;&#35273;&#35821;&#35328;&#26144;&#23556;&#22120;&#65288;CVLM&#65289;&#65292;&#36890;&#36807;&#22686;&#24378;&#35270;&#35273;&#30693;&#35782;&#23545;&#40784;&#65292;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#25361;&#25112;&#30693;&#35782;&#22411;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#21644;&#21453;&#24605;&#24403;&#21069;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#29616;&#29366;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24191;&#27867;&#20351;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#25237;&#24433;&#26041;&#27861;&#65288;&#22914;Q-former&#25110;MLP&#65289;&#20391;&#37325;&#20110;&#22270;&#20687;-&#25991;&#26412;&#25551;&#36848;&#30340;&#23545;&#40784;&#65292;&#20294;&#24573;&#30053;&#20102;&#35270;&#35273;&#30693;&#35782;&#32500;&#24230;&#30340;&#23545;&#40784;&#65292;&#21363;&#23558;&#35270;&#35273;&#19982;&#20854;&#30456;&#20851;&#30693;&#35782;&#36830;&#25509;&#36215;&#26469;&#12290;&#35270;&#35273;&#30693;&#35782;&#22312;&#20998;&#26512;&#12289;&#25512;&#26029;&#21644;&#35299;&#37322;&#35270;&#35273;&#20449;&#24687;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#39064;&#31572;&#26696;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#20027;&#35201;&#25506;&#35752;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#30693;&#35782;&#23545;&#40784;&#26469;&#25913;&#36827;LMMs&#65292;&#29305;&#21035;&#38024;&#23545;&#25361;&#25112;&#30693;&#35782;&#22411;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35748;&#30693;&#35270;&#35273;&#35821;&#35328;&#26144;&#23556;&#22120;&#65288;CVLM&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#30693;&#35782;&#23545;&#40784;&#22120;&#65288;VKA&#65289;&#21644;&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#33410;&#38454;&#27573;&#30340;&#32454;&#31890;&#24230;&#30693;&#35782;&#36866;&#37197;&#22120;&#65288;FKA&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22522;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13561v1 Announce Type: new  Abstract: Evaluating and Rethinking the current landscape of Large Multimodal Models (LMMs), we observe that widely-used visual-language projection approaches (e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet ignore the visual knowledge-dimension alignment, i.e., connecting visuals to their relevant knowledge. Visual knowledge plays a significant role in analyzing, inferring, and interpreting information from visuals, helping improve the accuracy of answers to knowledge-based visual questions. In this paper, we mainly explore improving LMMs with visual-language knowledge alignment, especially aimed at challenging knowledge-based visual question answering (VQA). To this end, we present a Cognitive Visual-Language Mapper (CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a Fine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning stage. Specifically, we design the VKA based on the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#23454;&#29992;&#30340;&#21465;&#20107;&#29702;&#35299;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#21465;&#20107;&#20013;&#24418;&#25104;&#22270;NARCO&#26469;&#25551;&#36848;&#25972;&#20010;&#32972;&#26223;&#30340;&#20219;&#21153;&#26080;&#20851;&#30340;&#36830;&#36143;&#20381;&#36182;&#65292;&#20854;&#20013;&#30340;&#36793;&#21453;&#26144;&#20102;&#39640;&#23618;&#27425;&#30340;&#36830;&#36143;&#20851;&#31995;&#65292;&#26080;&#38656;&#20381;&#36182;&#20154;&#31867;&#27880;&#37322;&#12290;</title><link>https://arxiv.org/abs/2402.13551</link><description>&lt;p&gt;
&#21465;&#20107;&#32972;&#26223;&#30340;&#22270;&#34920;&#31034;&#65306;&#36890;&#36807;&#22238;&#39038;&#24615;&#38382;&#39064;&#30340;&#36830;&#36143;&#20381;&#36182;
&lt;/p&gt;
&lt;p&gt;
Graph Representation of Narrative Context: Coherence Dependency via Retrospective Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13551
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#23454;&#29992;&#30340;&#21465;&#20107;&#29702;&#35299;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#21465;&#20107;&#20013;&#24418;&#25104;&#22270;NARCO&#26469;&#25551;&#36848;&#25972;&#20010;&#32972;&#26223;&#30340;&#20219;&#21153;&#26080;&#20851;&#30340;&#36830;&#36143;&#20381;&#36182;&#65292;&#20854;&#20013;&#30340;&#36793;&#21453;&#26144;&#20102;&#39640;&#23618;&#27425;&#30340;&#36830;&#36143;&#20851;&#31995;&#65292;&#26080;&#38656;&#20381;&#36182;&#20154;&#31867;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#23454;&#29992;&#30340;&#21465;&#20107;&#29702;&#35299;&#33539;&#24335;&#65292;&#36825;&#26159;&#22522;&#20110;&#19968;&#20010;&#35266;&#23519;&#65306;&#21465;&#36848;&#20013;&#30340;&#20010;&#21035;&#27573;&#33853;&#36890;&#24120;&#26159;&#30456;&#20114;&#20851;&#32852;&#30340;&#65292;&#32780;&#19981;&#26159;&#23396;&#31435;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#21465;&#20107;&#20013;&#24418;&#25104;&#19968;&#20010;&#21517;&#20026;NARCO&#30340;&#22270;&#65292;&#25551;&#36848;&#25972;&#20010;&#32972;&#26223;&#30340;&#20219;&#21153;&#26080;&#20851;&#30340;&#36830;&#36143;&#20381;&#36182;&#12290;&#29305;&#21035;&#26159;&#65292;NARCO&#20013;&#30340;&#36793;&#28085;&#30422;&#20102;&#20004;&#20010;&#19978;&#19979;&#25991;&#29255;&#27573;&#20043;&#38388;&#30340;&#33258;&#30001;&#24418;&#24335;&#22238;&#39038;&#24615;&#38382;&#39064;&#65292;&#21453;&#26144;&#20102;&#39640;&#23618;&#27425;&#30340;&#36830;&#36143;&#20851;&#31995;&#65292;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#24863;&#30693;&#30340;&#21551;&#21457;&#65292;&#20154;&#31867;&#19981;&#26029;&#20174;&#20808;&#21069;&#32972;&#26223;&#20013;&#37325;&#30003;&#30456;&#20851;&#20107;&#20214;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#22270;&#26159;&#36890;&#36807;&#25105;&#20204;&#35774;&#35745;&#30340;&#20004;&#38454;&#27573;LLM&#25552;&#31034;&#23454;&#20363;&#21270;&#30340;&#65292;&#22240;&#27492;&#26080;&#38656;&#20381;&#36182;&#20154;&#31867;&#27880;&#37322;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19977;&#20010;&#20851;&#20110;&#20854;&#23454;&#38469;&#25928;&#29992;&#30340;&#29420;&#29305;&#30740;&#31350;&#65292;&#36890;&#36807;&#24635;&#32467;&#35782;&#21035;&#26816;&#39564;&#36793;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#24773;&#33410;&#26816;&#32034;&#36827;&#34892;&#26412;&#22320;&#19978;&#19979;&#25991;&#22686;&#24378;&#65292;&#20197;&#21450;&#36890;&#36807;&#38271;&#25991;&#26723;&#38382;&#31572;&#31034;&#20363;&#21270;&#30340;&#26356;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13551v1 Announce Type: new  Abstract: This work introduces a novel and practical paradigm for narrative comprehension, stemming from the observation that individual passages within narratives are often cohesively related than being isolated. We therefore propose to formulate a graph upon narratives dubbed NARCO that depicts a task-agnostic coherence dependency of the entire context. Especially, edges in NARCO encompass retrospective free-form questions between two context snippets reflecting high-level coherent relations, inspired by the cognitive perception of humans who constantly reinstate relevant events from prior context. Importantly, our graph is instantiated through our designed two-stage LLM prompting, thereby without reliance on human annotations. We present three unique studies on its practical utility, examining the edge efficacy via recap identification, local context augmentation via plot retrieval, and broader applications exemplified by long document QA. Expe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;LLMs&#22312;&#35848;&#21028;&#23545;&#35805;&#20013;&#30340;&#22810;&#26041;&#38754;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#35848;&#21028;&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.13550</link><description>&lt;p&gt;
LLM&#20204;&#26159;&#26377;&#25928;&#30340;&#35848;&#21028;&#32773;&#21527;&#65311;&#23545;LLM&#22312;&#35848;&#21028;&#23545;&#35805;&#20013;&#22810;&#26041;&#38754;&#33021;&#21147;&#30340;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;LLMs&#22312;&#35848;&#21028;&#23545;&#35805;&#20013;&#30340;&#22810;&#26041;&#38754;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#35848;&#21028;&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#27425;&#25104;&#21151;&#30340;&#35848;&#21028;&#38656;&#35201;&#23545;&#35848;&#35805;&#32972;&#26223;&#26377;&#28145;&#21051;&#29702;&#35299;&#65292;&#20855;&#22791;&#25512;&#26029;&#23545;&#26041;&#21160;&#26426;&#30340;&#24515;&#29702;&#29702;&#35770;&#25216;&#33021;&#65292;&#20197;&#21450;&#25112;&#30053;&#25512;&#29702;&#21644;&#26377;&#25928;&#27807;&#36890;&#65292;&#36825;&#20351;&#24471;&#33258;&#21160;&#21270;&#31995;&#32479;&#38754;&#20020;&#25361;&#25112;&#12290;&#37492;&#20110;LLMs&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;LLMs&#22914;&#20309;&#25512;&#21160;&#35848;&#21028;&#30740;&#31350;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#21253;&#25324;&#35774;&#35745;&#23545;&#35805;&#31995;&#32479;&#12289;&#25552;&#20379;&#25945;&#23398;&#21453;&#39304;&#21644;&#25193;&#22823;&#25968;&#25454;&#25910;&#38598;&#23454;&#36341;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20998;&#26512;LLMs&#22312;&#21508;&#31181;&#23545;&#35805;&#24773;&#26223;&#20013;&#30340;&#22810;&#26041;&#38754;&#33021;&#21147;&#65292;&#28085;&#30422;&#20856;&#22411;&#35848;&#21028;&#20114;&#21160;&#30340;&#25152;&#26377;&#26102;&#38388;&#38454;&#27573;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;GPT-4&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#24615;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;LLMs&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#20173;&#28982;&#22256;&#38590;&#30340;&#32454;&#33410;&#12290;&#20363;&#22914;&#65292;&#36825;&#20123;&#27169;&#22411;&#19982;&#20154;&#31867;&#30340;&#30456;&#20851;&#24615;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13550v1 Announce Type: cross  Abstract: A successful negotiation demands a deep comprehension of the conversation context, Theory-of-Mind (ToM) skills to infer the partner's motives, as well as strategic reasoning and effective communication, making it challenging for automated systems. Given the remarkable performance of LLMs across a variety of NLP tasks, in this work, we aim to understand how LLMs can advance different aspects of negotiation research, ranging from designing dialogue systems to providing pedagogical feedback and scaling up data collection practices. To this end, we devise a methodology to analyze the multifaceted capabilities of LLMs across diverse dialogue scenarios covering all the time stages of a typical negotiation interaction. Our analysis adds to the increasing evidence for the superiority of GPT-4 across various tasks while also providing insights into specific tasks that remain difficult for LLMs. For instance, the models correlate poorly with hum
&lt;/p&gt;</description></item><item><title>ActiveRAG&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;RAG&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20027;&#21160;&#23398;&#20064;&#26426;&#21046;&#65292;&#21033;&#29992;&#30693;&#35782;&#26500;&#24314;&#21644;&#35748;&#30693;&#32852;&#32467;&#26426;&#21046;&#26469;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20869;&#22312;&#35748;&#30693;&#65292;&#23454;&#29616;&#20102;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.13547</link><description>&lt;p&gt;
ActiveRAG: &#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#25581;&#31034;&#30693;&#35782;&#30340;&#23453;&#34255;
&lt;/p&gt;
&lt;p&gt;
ActiveRAG: Revealing the Treasures of Knowledge via Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13547
&lt;/p&gt;
&lt;p&gt;
ActiveRAG&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;RAG&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20027;&#21160;&#23398;&#20064;&#26426;&#21046;&#65292;&#21033;&#29992;&#30693;&#35782;&#26500;&#24314;&#21644;&#35748;&#30693;&#32852;&#32467;&#26426;&#21046;&#26469;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20869;&#22312;&#35748;&#30693;&#65292;&#23454;&#29616;&#20102;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13547v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33539;&#20363;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;RAG&#27169;&#22411;&#23558;LLMs&#23450;&#20301;&#20026;&#34987;&#21160;&#30340;&#30693;&#35782;&#25509;&#25910;&#22120;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#23398;&#20064;&#21644;&#29702;&#35299;&#22806;&#37096;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ActiveRAG&#65292;&#23427;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;RAG&#26694;&#26550;&#65292;&#20174;&#34987;&#21160;&#30693;&#35782;&#33719;&#21462;&#36716;&#21464;&#20026;&#20027;&#21160;&#23398;&#20064;&#26426;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#30693;&#35782;&#26500;&#24314;&#26426;&#21046;&#36890;&#36807;&#23558;&#22806;&#37096;&#30693;&#35782;&#19982;&#20808;&#21069;&#33719;&#21462;&#25110;&#35760;&#24518;&#30340;&#30693;&#35782;&#30456;&#20851;&#32852;&#26469;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#22806;&#37096;&#30693;&#35782;&#12290;&#38543;&#21518;&#65292;&#23427;&#35774;&#35745;&#20102;&#35748;&#30693;&#32852;&#32467;&#26426;&#21046;&#20197;&#21512;&#24182;&#26469;&#33258;&#24605;&#32500;&#21644;&#30693;&#35782;&#26500;&#24314;&#38142;&#30340;&#25104;&#26524;&#65292;&#20174;&#32780;&#26657;&#20934;LLMs&#30340;&#20869;&#22312;&#35748;&#30693;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ActiveRAG&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;RAG&#27169;&#22411;&#65292;&#22312;&#38382;&#39064;&#22238;&#31572;&#19978;&#23454;&#29616;&#20102;5%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13547v1 Announce Type: new  Abstract: Retrieval Augmented Generation (RAG) has introduced a new paradigm for Large Language Models (LLMs), aiding in the resolution of knowledge-intensive tasks. However, current RAG models position LLMs as passive knowledge receptors, thereby restricting their capacity for learning and comprehending external knowledge. In this paper, we present ActiveRAG, an innovative RAG framework that shifts from passive knowledge acquisition to an active learning mechanism. This approach utilizes the Knowledge Construction mechanism to develop a deeper understanding of external knowledge by associating it with previously acquired or memorized knowledge. Subsequently, it designs the Cognitive Nexus mechanism to incorporate the outcomes from both chains of thought and knowledge construction, thereby calibrating the intrinsic cognition of LLMs. Our experimental results demonstrate that ActiveRAG surpasses previous RAG models, achieving a 5% improvement on qu
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#35270;&#35273;&#36866;&#37197;&#22120;&#65288;IVA&#65289;&#65292;&#29992;&#20110;&#22312;LLMs&#20013;&#22686;&#24378;&#23545;&#32454;&#31890;&#24230;&#35270;&#35273;&#20803;&#32032;&#30340;&#20132;&#20114;&#65292;&#24182;&#35299;&#20915;&#20102;&#38271;&#35270;&#39057;&#29702;&#35299;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#12289;&#35270;&#35273;&#28165;&#26224;&#24230;&#38477;&#20302;&#21644;&#26080;&#20851;&#35270;&#35273;&#20196;&#29260;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.13546</link><description>&lt;p&gt;
LLMs&#19982;&#38271;&#35270;&#39057;&#30456;&#36935;&#65306;&#22312;LLMs&#20013;&#21033;&#29992;&#20114;&#21160;&#24335;&#35270;&#35273;&#36866;&#37197;&#22120;&#25512;&#36827;&#38271;&#35270;&#39057;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
LLMs Meet Long Video: Advancing Long Video Comprehension with An Interactive Visual Adapter in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13546
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#35270;&#35273;&#36866;&#37197;&#22120;&#65288;IVA&#65289;&#65292;&#29992;&#20110;&#22312;LLMs&#20013;&#22686;&#24378;&#23545;&#32454;&#31890;&#24230;&#35270;&#35273;&#20803;&#32032;&#30340;&#20132;&#20114;&#65292;&#24182;&#35299;&#20915;&#20102;&#38271;&#35270;&#39057;&#29702;&#35299;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#12289;&#35270;&#35273;&#28165;&#26224;&#24230;&#38477;&#20302;&#21644;&#26080;&#20851;&#35270;&#35273;&#20196;&#29260;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#35270;&#39057;&#29702;&#35299;&#26159;&#22810;&#23186;&#20307;&#21644;&#20154;&#24037;&#26234;&#33021;&#20132;&#21449;&#39046;&#22495;&#20013;&#19968;&#39033;&#37325;&#35201;&#19988;&#25345;&#32493;&#25361;&#25112;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#29702;&#35299;&#35270;&#39057;&#25104;&#20026;&#19968;&#31181;&#26032;&#20852;&#19988;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35270;&#39057;&#20196;&#29260;&#25968;&#37327;&#24222;&#22823;&#65292;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#35270;&#35273;&#28165;&#26224;&#24230;&#38477;&#20302;&#65292;&#36824;&#38754;&#20020;&#30528;&#22312;&#22238;&#31572;&#35270;&#39057;&#30456;&#20851;&#38382;&#39064;&#26102;&#20986;&#29616;&#26080;&#20851;&#35270;&#35273;&#20196;&#29260;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;LLMs&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#35270;&#35273;&#36866;&#37197;&#22120;(IVA)&#65292;&#26088;&#22312;&#22686;&#24378;&#19982;&#32454;&#31890;&#24230;&#35270;&#35273;&#20803;&#32032;&#30340;&#20132;&#20114;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#39044;&#35757;&#32451;&#22240;&#26524;&#21464;&#25442;&#22120;&#23558;&#38271;&#35270;&#39057;&#36716;&#25442;&#20026;&#26102;&#38388;&#35270;&#39057;&#20196;&#29260;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#19982;&#35270;&#39057;&#35828;&#26126;&#19968;&#36215;&#36755;&#20837;LLMs&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#38598;&#25104;&#20102;IVA&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#26102;&#38388;&#24103;&#36873;&#25321;&#22120;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13546v1 Announce Type: new  Abstract: Long video understanding is a significant and ongoing challenge in the intersection of multimedia and artificial intelligence. Employing large language models (LLMs) for comprehending video becomes an emerging and promising method. However, this approach incurs high computational costs due to the extensive array of video tokens, experiences reduced visual clarity as a consequence of token aggregation, and confronts challenges arising from irrelevant visual tokens while answering video-related questions. To alleviate these issues, we present an Interactive Visual Adapter (IVA) within LLMs, designed to enhance interaction with fine-grained visual elements. Specifically, we first transform long videos into temporal video tokens via leveraging a visual encoder alongside a pretrained causal transformer, then feed them into LLMs with the video instructions. Subsequently, we integrated IVA, which contains a lightweight temporal frame selector a
&lt;/p&gt;</description></item><item><title>ARL2&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#26631;&#27880;&#32773;&#65292;&#24182;&#37319;&#29992;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#31574;&#30053;&#65292;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#27880;&#37322;&#25104;&#26412;&#65292;&#24182;&#22312;NQ&#21644;MMLU&#19978;&#21462;&#24471;&#20102;5.4%&#21644;4.6%&#30340;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.13542</link><description>&lt;p&gt;
ARL2: &#36890;&#36807;&#33258;&#23548;&#33258;&#36866;&#24212;&#30456;&#20851;&#24615;&#26631;&#35760;&#23558;&#26816;&#32034;&#22120;&#19982;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13542
&lt;/p&gt;
&lt;p&gt;
ARL2&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#26631;&#27880;&#32773;&#65292;&#24182;&#37319;&#29992;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#31574;&#30053;&#65292;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#27880;&#37322;&#25104;&#26412;&#65292;&#24182;&#22312;NQ&#21644;MMLU&#19978;&#21462;&#24471;&#20102;5.4%&#21644;4.6%&#30340;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13542v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25688;&#35201;: &#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#28304;&#30340;&#30456;&#20851;&#20449;&#24687;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20351;LLMs&#33021;&#22815;&#36866;&#24212;&#29305;&#23450;&#39046;&#22495;&#65292;&#24182;&#20943;&#36731;&#30693;&#35782;&#23494;&#38598;&#20219;&#21153;&#20013;&#30340;&#24187;&#35273;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#20998;&#24320;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;LLMs&#30340;&#40657;&#30418;&#29305;&#24615;&#65292;&#29616;&#26377;&#30340;&#26816;&#32034;&#22120;&#36890;&#24120;&#19982;LLMs&#19981;&#21305;&#37197;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ARL2&#65292;&#19968;&#31181;&#21033;&#29992;LLMs&#20316;&#20026;&#26631;&#27880;&#32773;&#30340;&#26816;&#32034;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;ARL2&#21033;&#29992;LLMs&#27880;&#37322;&#21644;&#35780;&#20998;&#30456;&#20851;&#35777;&#25454;&#65292;&#20174;&#32780;&#33021;&#22815;&#20174;&#24378;&#22823;&#30340;LLM&#30417;&#30563;&#20013;&#23398;&#20064;&#26816;&#32034;&#22120;&#12290;&#27492;&#22806;&#65292;ARL2&#20351;&#29992;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#31574;&#30053;&#26469;&#31574;&#21010;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30456;&#20851;&#24615;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;ARL2&#30340;&#26377;&#25928;&#24615;&#65292;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;NQ&#19978;&#25552;&#39640;&#20102;5.4%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;MMLU&#19978;&#25552;&#39640;&#20102;4.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13542v1 Announce Type: cross  Abstract: Retrieval-augmented generation enhances large language models (LLMs) by incorporating relevant information from external knowledge sources. This enables LLMs to adapt to specific domains and mitigate hallucinations in knowledge-intensive tasks. However, existing retrievers are often misaligned with LLMs due to their separate training processes and the black-box nature of LLMs. To address this challenge, we propose ARL2, a retriever learning technique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and score relevant evidence, enabling learning the retriever from robust LLM supervision. Furthermore, ARL2 uses an adaptive self-training strategy for curating high-quality and diverse relevance data, which can effectively reduce the annotation cost. Extensive experiments demonstrate the effectiveness of ARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared to the state-of-the-art methods. Additionall
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#20026;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#35774;&#35745;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#65288;TCL&#65289;&#26694;&#26550;&#65292;&#36880;&#28176;&#24341;&#20837;&#25968;&#25454;&#23454;&#20363;&#20174;&#31616;&#21333;&#21040;&#22256;&#38590;&#65292;&#26088;&#22312;&#25552;&#39640;&#24615;&#33021;&#21644;&#35757;&#32451;&#36895;&#24230;&#65292;&#24182;&#19988;&#23545;&#20845;&#20010;&#20013;&#25991;&#20998;&#35789;&#21644;&#35789;&#24615;&#26631;&#27880;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13534</link><description>&lt;p&gt;
&#19968;&#31181;&#26377;&#25928;&#34701;&#21512;&#24322;&#26500;&#30693;&#35782;&#30340;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#24207;&#21015;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
An Effective Incorporating Heterogeneous Knowledge Curriculum Learning for Sequence Labeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13534
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#20026;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#35774;&#35745;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#65288;TCL&#65289;&#26694;&#26550;&#65292;&#36880;&#28176;&#24341;&#20837;&#25968;&#25454;&#23454;&#20363;&#20174;&#31616;&#21333;&#21040;&#22256;&#38590;&#65292;&#26088;&#22312;&#25552;&#39640;&#24615;&#33021;&#21644;&#35757;&#32451;&#36895;&#24230;&#65292;&#24182;&#19988;&#23545;&#20845;&#20010;&#20013;&#25991;&#20998;&#35789;&#21644;&#35789;&#24615;&#26631;&#27880;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#26631;&#27880;&#27169;&#22411;&#24120;&#24120;&#21463;&#30410;&#20110;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#20570;&#27861;&#24341;&#20837;&#20102;&#25968;&#25454;&#24322;&#26500;&#24615;&#65292;&#24182;&#36890;&#36807;&#39069;&#22806;&#27169;&#22359;&#20351;&#27169;&#22411;&#21464;&#24471;&#22797;&#26434;&#65292;&#23548;&#33268;&#35757;&#32451;&#39640;&#24615;&#33021;&#27169;&#22411;&#30340;&#25104;&#26412;&#22686;&#21152;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#20026;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#35774;&#35745;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#65288;TCL&#65289;&#26694;&#26550;&#12290;TCL&#26694;&#26550;&#36890;&#36807;&#36880;&#28176;&#24341;&#20837;&#20174;&#31616;&#21333;&#21040;&#22256;&#38590;&#30340;&#25968;&#25454;&#23454;&#20363;&#26469;&#22686;&#24378;&#35757;&#32451;&#65292;&#26088;&#22312;&#25552;&#39640;&#24615;&#33021;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#29992;&#20110;&#35780;&#20272;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#38590;&#24230;&#32423;&#21035;&#30340;&#19981;&#21516;&#25351;&#26631;&#12290;&#36890;&#36807;&#22312;&#20845;&#20010;&#20013;&#25991;&#20998;&#35789;&#65288;CWS&#65289;&#21644;&#35789;&#24615;&#26631;&#27880;&#65288;POS&#65289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25552;&#39640;&#24207;&#21015;&#26631;&#27880;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;TCL&#21152;&#36895;&#20102;&#35757;&#32451;&#24182;&#32531;&#35299;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13534v1 Announce Type: cross  Abstract: Sequence labeling models often benefit from incorporating external knowledge. However, this practice introduces data heterogeneity and complicates the model with additional modules, leading to increased expenses for training a high-performing model. To address this challenge, we propose a two-stage curriculum learning (TCL) framework specifically designed for sequence labeling tasks. The TCL framework enhances training by gradually introducing data instances from easy to hard, aiming to improve both performance and training speed. Furthermore, we explore different metrics for assessing the difficulty levels of sequence labeling tasks. Through extensive experimentation on six Chinese word segmentation (CWS) and Part-of-speech tagging (POS) datasets, we demonstrate the effectiveness of our model in enhancing the performance of sequence labeling models. Additionally, our analysis indicates that TCL accelerates training and alleviates the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#24615;&#33021;GPU&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20302;&#31209;&#32467;&#26500;&#26469;&#39640;&#25928;&#22320;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#32447;&#24615;&#23618;&#20887;&#20313;&#24615;&#12289;GPU&#20869;&#23384;&#21344;&#29992;&#21644;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;GPU&#21033;&#29992;&#29575;&#19981;&#36275;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.13533</link><description>&lt;p&gt;
FinGPT-HPC: &#39640;&#24615;&#33021;&#35745;&#31639;&#19979;&#29992;&#20110;&#37329;&#34701;&#24212;&#29992;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FinGPT-HPC: Efficient Pretraining and Finetuning Large Language Models for Financial Applications with High-Performance Computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13533
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#24615;&#33021;GPU&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20302;&#31209;&#32467;&#26500;&#26469;&#39640;&#25928;&#22320;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#32447;&#24615;&#23618;&#20887;&#20313;&#24615;&#12289;GPU&#20869;&#23384;&#21344;&#29992;&#21644;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;GPU&#21033;&#29992;&#29575;&#19981;&#36275;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#35745;&#31639;&#23494;&#38598;&#24615;&#24456;&#39640;&#12290;&#35745;&#31639;&#24037;&#20316;&#37327;&#21644;&#20869;&#23384;&#21344;&#29992;&#37327;&#38543;&#32500;&#24230;(&#23618;&#23485;&#24230;)&#30340;&#22686;&#21152;&#21576;&#20108;&#27425;&#22686;&#38271;&#12290;&#22823;&#22810;&#25968;LLM&#21442;&#25968;&#26469;&#33258;&#21464;&#21387;&#22120;&#32467;&#26500;&#30340;&#32447;&#24615;&#23618;&#65292;&#20855;&#26377;&#39640;&#24230;&#20887;&#20313;&#24615;&#12290;&#36825;&#20123;&#32447;&#24615;&#23618;&#36129;&#29486;&#20102;&#36229;&#36807;80%&#30340;&#35745;&#31639;&#24037;&#20316;&#37327;&#21644;99%&#30340;&#27169;&#22411;&#22823;&#23567;&#12290;&#20026;&#20102;&#39640;&#25928;&#22320;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;LLMs&#65292;&#38656;&#35201;&#35299;&#20915;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;1) &#20943;&#23569;&#32447;&#24615;&#23618;&#30340;&#20887;&#20313;&#24615;&#65307;2) &#20943;&#23569;GPU&#20869;&#23384;&#21344;&#29992;&#65307;3) &#22312;&#20351;&#29992;&#20998;&#24067;&#24335;&#35757;&#32451;&#26102;&#25552;&#39640;GPU&#21033;&#29992;&#29575;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#22914;LoRA&#21644;QLoRA&#65292;&#21033;&#29992;&#20302;&#31209;&#30697;&#38453;&#21644;&#37327;&#21270;&#26469;&#20998;&#21035;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#21644;&#27169;&#22411;&#22823;&#23567;&#12290;&#28982;&#32780;&#65292; resulting model &#20173;&#28982;&#28040;&#32791;&#22823;&#37327;GPU&#20869;&#23384;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#39640;&#24615;&#33021;GPU&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20302;&#31209;&#32467;&#26500;&#26469;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13533v1 Announce Type: cross  Abstract: Large language models (LLMs) are computationally intensive. The computation workload and the memory footprint grow quadratically with the dimension (layer width). Most of LLMs' parameters come from the linear layers of the transformer structure and are highly redundant. These linear layers contribute more than 80% of the computation workload and 99% of the model size. To pretrain and finetune LLMs efficiently, there are three major challenges to address: 1) reducing redundancy of the linear layers; 2) reducing GPU memory footprint; 3) improving GPU utilization when using distributed training. Prior methods, such as LoRA and QLoRA, utilized low-rank matrices and quantization to reduce the number of trainable parameters and model size, respectively. However, the resulting model still consumes a large amount of GPU memory. In this paper, we present high-performance GPU-based methods that exploit low-rank structures to pretrain and finetun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21518;&#38376;&#25915;&#20987;&#22330;&#26223;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#21033;&#29992;&#23494;&#38598;&#36890;&#36947;&#26816;&#32034;&#30340;&#35821;&#27861;&#38169;&#35823;&#35302;&#21457;&#21518;&#38376;&#25915;&#20987;&#65292;&#20197;&#31192;&#23494;&#20256;&#25773;&#23450;&#21521;&#38169;&#35823;&#20449;&#24687;&#65292;&#22914;&#20167;&#24680;&#35328;&#35770;&#25110;&#24191;&#21578;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#25915;&#20987;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#38544;&#21311;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13532</link><description>&lt;p&gt;
&#23494;&#38598;&#36890;&#36947;&#26816;&#32034;&#22120;&#29992;&#20110;&#20256;&#25773;&#20449;&#24687;&#38169;&#35823;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Backdoor Attacks on Dense Passage Retrievers for Disseminating Misinformation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21518;&#38376;&#25915;&#20987;&#22330;&#26223;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#21033;&#29992;&#23494;&#38598;&#36890;&#36947;&#26816;&#32034;&#30340;&#35821;&#27861;&#38169;&#35823;&#35302;&#21457;&#21518;&#38376;&#25915;&#20987;&#65292;&#20197;&#31192;&#23494;&#20256;&#25773;&#23450;&#21521;&#38169;&#35823;&#20449;&#24687;&#65292;&#22914;&#20167;&#24680;&#35328;&#35770;&#25110;&#24191;&#21578;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#25915;&#20987;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#38544;&#21311;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#26816;&#32034;&#22120;&#21644;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#24050;&#24191;&#27867;&#29992;&#20110;&#21508;&#31181;NLP&#24212;&#29992;&#65292;&#23613;&#31649;&#35774;&#35745;&#29992;&#20110;&#25552;&#20379;&#21487;&#38752;&#21644;&#23433;&#20840;&#30340;&#32467;&#26524;&#65292;&#20294;&#26816;&#32034;&#22120;&#23545;&#28508;&#22312;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#20173;&#19981;&#28165;&#26970;&#65292;&#24341;&#21457;&#20154;&#20204;&#23545;&#20854;&#23433;&#20840;&#24615;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24773;&#26223;&#65292;&#25915;&#20987;&#32773;&#26088;&#22312;&#36890;&#36807;&#26816;&#32034;&#31995;&#32479;&#38544;&#34109;&#20256;&#25773;&#23450;&#21521;&#38169;&#35823;&#20449;&#24687;&#65292;&#22914;&#20167;&#24680;&#35328;&#35770;&#25110;&#24191;&#21578;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23494;&#38598;&#36890;&#36947;&#26816;&#32034;&#20013;&#30001;&#35821;&#27861;&#38169;&#35823;&#35302;&#21457;&#30340;&#21361;&#38505;&#21518;&#38376;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30830;&#20445;&#34987;&#25915;&#20987;&#30340;&#27169;&#22411;&#22312;&#26631;&#20934;&#26597;&#35810;&#19979;&#21487;&#20197;&#27491;&#24120;&#36816;&#34892;&#65292;&#20294;&#22312;&#29992;&#25143;&#22312;&#26597;&#35810;&#20013;&#24847;&#22806;&#22320;&#29359;&#35821;&#27861;&#38169;&#35823;&#26102;&#65292;&#34987;&#31713;&#25913;&#20197;&#36820;&#22238;&#25915;&#20987;&#32773;&#25351;&#23450;&#30340;&#27573;&#33853;&#12290;&#22823;&#37327;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#38544;&#34109;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13532v1 Announce Type: new  Abstract: Dense retrievers and retrieval-augmented language models have been widely used in various NLP applications. Despite being designed to deliver reliable and secure outcomes, the vulnerability of retrievers to potential attacks remains unclear, raising concerns about their security. In this paper, we introduce a novel scenario where the attackers aim to covertly disseminate targeted misinformation, such as hate speech or advertisement, through a retrieval system. To achieve this, we propose a perilous backdoor attack triggered by grammar errors in dense passage retrieval. Our approach ensures that attacked models can function normally for standard queries but are manipulated to return passages specified by the attacker when users unintentionally make grammatical mistakes in their queries. Extensive experiments demonstrate the effectiveness and stealthiness of our proposed attack method. When a user query is error-free, our model consistentl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#30784;&#35774;&#26045;&#35843;&#35299;&#21592;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#29305;&#23450;&#22522;&#30784;&#35774;&#26045;&#38382;&#39064;&#65292;&#36890;&#36807;&#25366;&#25496;&#31038;&#20132;&#32593;&#32476;&#20013;&#20851;&#20110;&#39044;&#26399;&#22833;&#36133;&#30340;&#25285;&#24551;&#65292;&#26377;&#21161;&#20110;&#39044;&#38450;&#21644;&#20943;&#36731;&#28508;&#22312;&#30340;&#22522;&#30784;&#35774;&#26045;&#22833;&#36133;&#12290;</title><link>https://arxiv.org/abs/2402.13528</link><description>&lt;p&gt;
&#22522;&#30784;&#35774;&#26045;&#35843;&#35299;&#21592;&#65306;&#20174;&#32467;&#26500;&#28798;&#38590;&#21709;&#24212;&#20013;&#25366;&#25496;&#26410;&#26469;&#22833;&#25928;&#25285;&#24551;
&lt;/p&gt;
&lt;p&gt;
Infrastructure Ombudsman: Mining Future Failure Concerns from Structural Disaster Response
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#30784;&#35774;&#26045;&#35843;&#35299;&#21592;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#29305;&#23450;&#22522;&#30784;&#35774;&#26045;&#38382;&#39064;&#65292;&#36890;&#36807;&#25366;&#25496;&#31038;&#20132;&#32593;&#32476;&#20013;&#20851;&#20110;&#39044;&#26399;&#22833;&#36133;&#30340;&#25285;&#24551;&#65292;&#26377;&#21161;&#20110;&#39044;&#38450;&#21644;&#20943;&#36731;&#28508;&#22312;&#30340;&#22522;&#30784;&#35774;&#26045;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30740;&#31350;&#38598;&#20013;&#20110;&#30740;&#31350;&#31038;&#20132;&#23186;&#20307;&#19978;&#19982;&#32467;&#26500;&#22833;&#36133;&#30456;&#20851;&#30340;&#35752;&#35770;&#65292;&#20197;&#25913;&#36827;&#28798;&#38590;&#21709;&#24212;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#26816;&#27979;&#31038;&#20132;&#32593;&#32476;&#24086;&#23376;&#20013;&#35752;&#35770;&#20851;&#20110;&#39044;&#26399;&#22833;&#36133;&#30340;&#25285;&#24551;&#26159;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#12290;&#22914;&#26524;&#36825;&#20123;&#25285;&#24551;&#34987;&#20256;&#36798;&#32473;&#36866;&#24403;&#30340;&#26426;&#26500;&#65292;&#21487;&#20197;&#24110;&#21161;&#39044;&#38450;&#21644;&#20943;&#36731;&#28508;&#22312;&#30340;&#22522;&#30784;&#35774;&#26045;&#22833;&#36133;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#30784;&#35774;&#26045;&#35843;&#35299;&#21592;&#8212;&#8212;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#29305;&#23450;&#22522;&#30784;&#35774;&#26045;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#32771;&#34385;&#20102;&#32654;&#22269;&#20960;&#36215;&#26368;&#36817;&#30340;&#32467;&#26500;&#22833;&#25928;&#20107;&#20214;&#12290;&#25105;&#20204;&#21576;&#29616;&#20102;&#19968;&#20221;&#39318;&#21019;&#24615;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20174;Reddit&#21644;YouTube&#20013;&#25366;&#25496;&#30340;2,662&#20010;&#31038;&#20132;&#32593;&#32476;&#23454;&#20363;&#65292;&#29992;&#20110;&#36825;&#19968;&#26032;&#39062;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13528v1 Announce Type: cross  Abstract: Current research concentrates on studying discussions on social media related to structural failures to improve disaster response strategies. However, detecting social web posts discussing concerns about anticipatory failures is under-explored. If such concerns are channeled to the appropriate authorities, it can aid in the prevention and mitigation of potential infrastructural failures. In this paper, we develop an infrastructure ombudsman -- that automatically detects specific infrastructure concerns. Our work considers several recent structural failures in the US. We present a first-of-its-kind dataset of 2,662 social web instances for this novel task mined from Reddit and YouTube.
&lt;/p&gt;</description></item><item><title>OMGEval&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#33021;&#21147;&#30340;&#24320;&#25918;&#28304;&#22810;&#35821;&#35328;&#29983;&#25104;&#27979;&#35797;&#38598;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#37325;&#35201;&#33021;&#21147;&#24182;&#36827;&#34892;&#20102;&#26412;&#22320;&#21270;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.13524</link><description>&lt;p&gt;
OMGEval&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#22810;&#35821;&#35328;&#29983;&#25104;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
OMGEval: An Open Multilingual Generative Evaluation Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13524
&lt;/p&gt;
&lt;p&gt;
OMGEval&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#33021;&#21147;&#30340;&#24320;&#25918;&#28304;&#22810;&#35821;&#35328;&#29983;&#25104;&#27979;&#35797;&#38598;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#37325;&#35201;&#33021;&#21147;&#24182;&#36827;&#34892;&#20102;&#26412;&#22320;&#21270;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#35813;&#26222;&#36941;&#21463;&#30410;&#20110;&#20840;&#29699;&#21508;&#31181;&#25991;&#21270;&#32972;&#26223;&#30340;&#20010;&#20154;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26368;&#36817;&#30340;&#20808;&#36827;&#29983;&#25104;&#35780;&#20272;&#22522;&#20934;&#20027;&#35201;&#19987;&#27880;&#20110;&#33521;&#35821;&#30340;LLMs&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;OMGEval&#65292;&#31532;&#19968;&#20010;&#21487;&#20197;&#35780;&#20272;LLMs&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#33021;&#21147;&#30340;&#24320;&#25918;&#28304;&#22810;&#35821;&#35328;&#29983;&#25104;&#27979;&#35797;&#38598;&#12290;&#23545;&#20110;&#27599;&#31181;&#35821;&#35328;&#65292;OMGEval&#25552;&#20379;&#20102;804&#20010;&#24320;&#25918;&#24335;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;LLMs&#30340;&#24191;&#27867;&#37325;&#35201;&#33021;&#21147;&#65292;&#22914;&#24120;&#35782;&#12289;&#36923;&#36753;&#25512;&#29702;&#31561;&#12290;&#27599;&#20010;&#38382;&#39064;&#37117;&#32463;&#36807;&#20154;&#31867;&#26631;&#27880;&#32773;&#30340;&#20005;&#26684;&#39564;&#35777;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20026;&#20102;&#20805;&#20998;&#21453;&#26144;LLMs&#22312;&#19981;&#21516;&#25991;&#21270;&#32972;&#26223;&#19979;&#30340;&#20860;&#23481;&#24615;&#65292;&#25105;&#20204;&#20026;&#27599;&#31181;&#38750;&#33521;&#35821;&#35821;&#35328;&#36827;&#34892;&#26412;&#22320;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#21069;&#29256;&#26412;&#30340;OMGEval&#21253;&#25324;5&#31181;&#35821;&#35328;&#65288;&#21363;&#65306;&#20013;&#25991;&#12289;&#20420;&#25991;&#12289;&#27861;&#35821;&#12289;&#35199;&#29677;&#29273;&#25991;&#12289;&#38463;&#25289;&#20271;&#25991;&#65289;&#12290;&#22312;AlpacaEval&#20043;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-4&#20316;&#20026;&#35009;&#21028;&#33258;&#21160;&#35780;&#20998;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13524v1 Announce Type: new  Abstract: Modern large language models (LLMs) should generally benefit individuals from various cultural backgrounds around the world. However, most recent advanced generative evaluation benchmarks tailed for LLMs mainly focus on English. To this end, we introduce OMGEval, the first Open-source Multilingual Generative test set that can assess the capability of LLMs in different languages. For each language, OMGEval provides 804 open-ended questions, covering a wide range of important capabilities of LLMs, such as general knowledge, logical reasoning, and so on. Each question is rigorously verified by human annotators. Notably, to sufficiently reflect the compatibility of LLMs in different cultural backgrounds, we perform localization for each non-English language. Specifically, the current version of OMGEval includes 5 languages (i.e., Zh, Ru, Fr, Es, Ar). Following AlpacaEval, we employ GPT-4 as the adjudicator to automatically score different mo
&lt;/p&gt;</description></item><item><title>RecMind&#26159;&#19968;&#20010;&#20855;&#26377;&#23547;&#27714;&#32773;&#20869;&#22312;&#29366;&#24577;&#27880;&#37322;&#30340;&#26085;&#26412;&#30005;&#24433;&#25512;&#33616;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#21457;&#29616;&#23545;&#37027;&#20123;&#23547;&#27714;&#32773;&#24863;&#20852;&#36259;&#20294;&#24182;&#19981;&#20102;&#35299;&#30340;&#23454;&#20307;&#36827;&#34892;&#25512;&#33616;&#26377;&#21161;&#20110;&#25104;&#21151;&#25512;&#33616;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32771;&#34385;&#23547;&#27714;&#32773;&#20869;&#22312;&#29366;&#24577;&#30340;&#21709;&#24212;&#29983;&#25104;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.13522</link><description>&lt;p&gt;
RecMind: &#23547;&#27714;&#32773;&#20869;&#24515;&#29366;&#24577;&#19979;&#30340;&#26085;&#26412;&#30005;&#24433;&#25512;&#33616;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
RecMind: Japanese Movie Recommendation Dialogue with Seeker's Internal State
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13522
&lt;/p&gt;
&lt;p&gt;
RecMind&#26159;&#19968;&#20010;&#20855;&#26377;&#23547;&#27714;&#32773;&#20869;&#22312;&#29366;&#24577;&#27880;&#37322;&#30340;&#26085;&#26412;&#30005;&#24433;&#25512;&#33616;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#21457;&#29616;&#23545;&#37027;&#20123;&#23547;&#27714;&#32773;&#24863;&#20852;&#36259;&#20294;&#24182;&#19981;&#20102;&#35299;&#30340;&#23454;&#20307;&#36827;&#34892;&#25512;&#33616;&#26377;&#21161;&#20110;&#25104;&#21151;&#25512;&#33616;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32771;&#34385;&#23547;&#27714;&#32773;&#20869;&#22312;&#29366;&#24577;&#30340;&#21709;&#24212;&#29983;&#25104;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#23545;&#35805;&#20013;&#20250;&#20180;&#32454;&#20851;&#27880;&#20132;&#27969;&#32773;&#30340;&#20869;&#22312;&#29366;&#24577;&#12290;&#20363;&#22914;&#65292;&#22312;&#25512;&#33616;&#23545;&#35805;&#20013;&#65292;&#25105;&#20204;&#20250;&#22312;&#25512;&#33616;&#26102;&#20272;&#35745;&#23547;&#27714;&#32773;&#30340;&#20869;&#24515;&#29366;&#24577;&#65292;&#27604;&#22914;&#20182;/&#22905;&#30340;&#30693;&#35782;&#27700;&#24179;&#21644;&#20852;&#36259;&#12290;&#37492;&#20110;&#30446;&#21069;&#27809;&#26377;&#29616;&#26377;&#30340;&#36164;&#28304;&#29992;&#20110;&#20998;&#26512;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;RecMind&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#23547;&#27714;&#32773;&#20869;&#22312;&#29366;&#24577;&#23454;&#20307;&#32423;&#21035;&#27880;&#37322;&#30340;&#26085;&#26412;&#30005;&#24433;&#25512;&#33616;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#23454;&#20307;&#37117;&#26377;&#19968;&#20010;&#30001;&#23547;&#27714;&#32773;&#27880;&#37322;&#30340;&#20027;&#35266;&#26631;&#31614;&#21644;&#30001;&#25512;&#33616;&#32773;&#27880;&#37322;&#30340;&#23458;&#35266;&#26631;&#31614;&#12290;RecMind&#36824;&#20855;&#26377;&#24341;&#20154;&#20837;&#32988;&#30340;&#23545;&#35805;&#65292;&#20855;&#26377;&#38271;&#31687;&#23547;&#27714;&#32773;&#35805;&#35821;&#65292;&#26377;&#21161;&#20110;&#35814;&#32454;&#20998;&#26512;&#23547;&#27714;&#32773;&#30340;&#20869;&#22312;&#29366;&#24577;&#12290;&#25105;&#20204;&#22522;&#20110;RecMind&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#23547;&#27714;&#32773;&#23545;&#19968;&#20123;&#24182;&#19981;&#20102;&#35299;&#20294;&#24863;&#20852;&#36259;&#30340;&#23454;&#20307;&#26377;&#21161;&#20110;&#25512;&#33616;&#25104;&#21151;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21709;&#24212;&#29983;&#25104;&#26694;&#26550;&#65292;&#26126;&#30830;&#32771;&#34385;&#20102;&#23547;&#27714;&#32773;&#30340;&#20869;&#22312;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13522v1 Announce Type: new  Abstract: Humans pay careful attention to the interlocutor's internal state in dialogues. For example, in recommendation dialogues, we make recommendations while estimating the seeker's internal state, such as his/her level of knowledge and interest. Since there are no existing annotated resources for the analysis, we constructed RecMind, a Japanese movie recommendation dialogue dataset with annotations of the seeker's internal state at the entity level. Each entity has a subjective label annotated by the seeker and an objective label annotated by the recommender. RecMind also features engaging dialogues with long seeker's utterances, enabling a detailed analysis of the seeker's internal state. Our analysis based on RecMind reveals that entities that the seeker has no knowledge about but has an interest in contribute to recommendation success. We also propose a response generation framework that explicitly considers the seeker's internal state, ut
&lt;/p&gt;</description></item><item><title>RITFIS&#26159;&#31532;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#36719;&#20214;&#23545;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#40065;&#26834;&#24615;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27979;&#35797;&#36807;&#31243;&#23450;&#20041;&#20026;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#26469;&#30830;&#23450;&#25104;&#21151;&#30340;&#27979;&#35797;&#26696;&#20363;&#12290;</title><link>https://arxiv.org/abs/2402.13518</link><description>&lt;p&gt;
RITFIS: RITFIS&#65306;&#22522;&#20110;LLMs&#30340;&#26234;&#33021;&#36719;&#20214;&#24378;&#22766;&#36755;&#20837;&#27979;&#35797;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
RITFIS: Robust input testing framework for LLMs-based intelligent software
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13518
&lt;/p&gt;
&lt;p&gt;
RITFIS&#26159;&#31532;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#36719;&#20214;&#23545;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#40065;&#26834;&#24615;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27979;&#35797;&#36807;&#31243;&#23450;&#20041;&#20026;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#26469;&#30830;&#23450;&#25104;&#21151;&#30340;&#27979;&#35797;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13518v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26234;&#33021;&#36719;&#20214;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20381;&#36182;&#26085;&#30410;&#31361;&#20986;&#65292;&#24378;&#35843;&#20102;&#23545;&#40065;&#26834;&#24615;&#27979;&#35797;&#30340;&#24517;&#35201;&#24615;&#12290;&#24403;&#21069;&#30340;&#27979;&#35797;&#26041;&#27861;&#20165;&#20851;&#27880;&#22522;&#20110;LLM&#30340;&#36719;&#20214;&#23545;&#25552;&#31034;&#30340;&#40065;&#26834;&#24615;&#12290;&#37492;&#20110;&#29616;&#23454;&#19990;&#30028;&#36755;&#20837;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#30740;&#31350;LLM-based&#36719;&#20214;&#22788;&#29702;&#20840;&#38754;&#36755;&#20837;&#65288;&#21253;&#25324;&#25552;&#31034;&#21644;&#31034;&#20363;&#65289;&#30340;&#40065;&#26834;&#24615;&#23545;&#20110;&#20840;&#38754;&#20102;&#35299;&#20854;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290; &#20026;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;RITFIS&#65292;&#19968;&#31181;&#29992;&#20110;LLM-based&#26234;&#33021;&#36719;&#20214;&#30340;&#40065;&#26834;&#36755;&#20837;&#27979;&#35797;&#26694;&#26550;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;RITFIS&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;LLM-based&#26234;&#33021;&#36719;&#20214;&#23545;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#30340;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#32473;&#23450;&#30340;&#23041;&#32961;&#27169;&#22411;&#21644;&#25552;&#31034;&#65292;&#20027;&#35201;&#23558;&#27979;&#35797;&#36807;&#31243;&#23450;&#20041;&#20026;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#25104;&#21151;&#30340;&#27979;&#35797;&#26696;&#20363;&#30001;&#19968;&#20010;&#30446;&#26631;&#20989;&#25968;&#20915;&#23450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13518v1 Announce Type: cross  Abstract: The dependence of Natural Language Processing (NLP) intelligent software on Large Language Models (LLMs) is increasingly prominent, underscoring the necessity for robustness testing. Current testing methods focus solely on the robustness of LLM-based software to prompts. Given the complexity and diversity of real-world inputs, studying the robustness of LLMbased software in handling comprehensive inputs (including prompts and examples) is crucial for a thorough understanding of its performance.   To this end, this paper introduces RITFIS, a Robust Input Testing Framework for LLM-based Intelligent Software. To our knowledge, RITFIS is the first framework designed to assess the robustness of LLM-based intelligent software against natural language inputs. This framework, based on given threat models and prompts, primarily defines the testing process as a combinatorial optimization problem. Successful test cases are determined by a goal fu
&lt;/p&gt;</description></item><item><title>&#24448;&#36820;&#32763;&#35793;&#65288;RTT&#65289;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25269;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#31038;&#20132;&#24037;&#31243;&#25915;&#20987;&#30340;&#31639;&#27861;&#65292;&#25104;&#21151;&#22320;&#20943;&#23569;&#20102;&#22810;&#31181;&#25915;&#20987;&#24418;&#24335;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.13517</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36870;&#21521;&#32763;&#35793;&#38450;&#24481;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Round Trip Translation Defence against Large Language Model Jailbreaking Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13517
&lt;/p&gt;
&lt;p&gt;
&#24448;&#36820;&#32763;&#35793;&#65288;RTT&#65289;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25269;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#31038;&#20132;&#24037;&#31243;&#25915;&#20987;&#30340;&#31639;&#27861;&#65292;&#25104;&#21151;&#22320;&#20943;&#23569;&#20102;&#22810;&#31181;&#25915;&#20987;&#24418;&#24335;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23481;&#26131;&#21463;&#21040;&#31038;&#20132;&#24037;&#31243;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#23545;&#20154;&#31867;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#38656;&#35201;LLMs&#20855;&#26377;&#39640;&#27700;&#24179;&#30340;&#29702;&#35299;&#33021;&#21147;&#25165;&#33021;&#25269;&#25239;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#25514;&#26045;&#26368;&#22810;&#21482;&#33021;&#32531;&#35299;&#36825;&#20123;&#25915;&#20987;&#30340;&#19981;&#21040;&#19968;&#21322;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24448;&#36820;&#32763;&#35793;&#65288;RTT&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25269;&#24481;LLMs&#31038;&#20132;&#24037;&#31243;&#25915;&#20987;&#30340;&#31639;&#27861;&#12290;RTT&#20250;&#25913;&#20889;&#23545;&#25239;&#24615;&#25552;&#31034;&#24182;&#25512;&#24191;&#34920;&#36798;&#30340;&#24605;&#24819;&#65292;&#20351;LLMs&#26356;&#23481;&#26131;&#26816;&#27979;&#20986;&#35825;&#21457;&#26377;&#23475;&#34892;&#20026;&#12290;&#36825;&#31181;&#26041;&#27861;&#28789;&#27963;&#12289;&#36731;&#37327;&#19988;&#21487;&#36716;&#31227;&#33267;&#19981;&#21516;&#30340;LLMs&#12290;&#25105;&#20204;&#30340;&#38450;&#24481;&#25104;&#21151;&#22320;&#32531;&#35299;&#20102;&#36229;&#36807;70%&#30340;Prompt Automatic Iterative Refinement (PAIR)&#25915;&#20987;&#65292;&#36825;&#26159;&#30446;&#21069;&#25105;&#20204;&#25152;&#30693;&#26368;&#26377;&#25928;&#30340;&#38450;&#24481;&#12290;&#25105;&#20204;&#20063;&#26159;&#39318;&#27425;&#23581;&#35797;&#32531;&#35299;MathsAttack&#65292;&#24182;&#23558;&#20854;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#20102;&#36817;40%&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#24050;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13517v1 Announce Type: cross  Abstract: Large language models (LLMs) are susceptible to social-engineered attacks that are human-interpretable but require a high level of comprehension for LLMs to counteract. Existing defensive measures can only mitigate less than half of these attacks at most. To address this issue, we propose the Round Trip Translation (RTT) method, the first algorithm specifically designed to defend against social-engineered attacks on LLMs. RTT paraphrases the adversarial prompt and generalizes the idea conveyed, making it easier for LLMs to detect induced harmful behavior. This method is versatile, lightweight, and transferrable to different LLMs. Our defense successfully mitigated over 70% of Prompt Automatic Iterative Refinement (PAIR) attacks, which is currently the most effective defense to the best of our knowledge. We are also the first to attempt mitigating the MathsAttack and reduced its attack success rate by almost 40%. Our code is publicly av
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.13516</link><description>&lt;p&gt;
ProSparse: &#24341;&#20837;&#21644;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Activation sparsity&#25351;&#30340;&#26159;&#28608;&#27963;&#36755;&#20986;&#20013;&#23384;&#22312;&#35768;&#22810;&#24369;&#36129;&#29486;&#20803;&#32032;&#12290;&#20316;&#20026;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#27169;&#22411;&#30340;&#26222;&#36941;&#23646;&#24615;&#65292;&#24050;&#34987;&#35777;&#26126;&#26159;&#25552;&#39640;&#27169;&#22411;&#25512;&#29702;&#25928;&#29575;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37319;&#29992;&#20102;&#27809;&#26377;&#20869;&#22312;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#28608;&#27963;&#20989;&#25968;&#65288;&#20363;&#22914;GELU&#21644;Swish&#65289;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#21162;&#21147;&#23581;&#35797;&#24341;&#20837;ReLU&#25110;&#20854;&#21464;&#20307;&#20316;&#20026;&#26367;&#20195;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#24110;&#21161;LLMs&#23454;&#29616;&#28608;&#27963;&#31232;&#30095;&#24615;&#21644;&#25512;&#29702;&#21152;&#36895;&#65292;&#20294;&#24456;&#23569;&#33021;&#21516;&#26102;&#33719;&#24471;&#39640;&#31232;&#30095;&#24230;&#21644;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;LLMs&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23558;LLMs&#30340;&#28608;&#27963;&#20989;&#25968;&#26367;&#25442;&#20026;ReLU&#21518;&#65292;ProSparse&#37319;&#29992;&#28176;&#36827;&#31232;&#30095;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13516v1 Announce Type: cross  Abstract: Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs. As a prevalent property of the models using the ReLU activation function, it has been proven a promising paradigm to boost model inference efficiency. Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help LLMs achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance. This paper introduces an effective sparsification method named "ProSparse" to push LLMs for higher activation sparsity without decreasing model performance. Specifically, after substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization wit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38754;&#21521;&#32452;&#21512;&#26410;&#30693;&#38382;&#39064;&#30340;&#33258;&#25105;&#20998;&#32780;&#27835;&#20043;&#31639;&#27861;&#65292;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#32452;&#21512;&#26410;&#30693;&#38382;&#39064;&#38382;&#31572;&#25968;&#25454;&#38598;&#65288;CuQA&#65289;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#29992;&#19981;&#21516;&#26041;&#27861;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.13514</link><description>&lt;p&gt;
&#33258;&#25105;&#20998;&#32780;&#27835;&#20043;&#65306;&#20309;&#26102;&#26816;&#32034;&#12289;&#20309;&#26102;&#29983;&#25104;&#65311;&#38754;&#21521;&#32452;&#21512;&#26410;&#30693;&#38382;&#39064;&#30340;&#33258;&#25105;&#20998;&#32780;&#27835;&#20043;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-DC: When to retrieve and When to generate? Self Divide-and-Conquer for Compositional Unknown Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13514
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38754;&#21521;&#32452;&#21512;&#26410;&#30693;&#38382;&#39064;&#30340;&#33258;&#25105;&#20998;&#32780;&#27835;&#20043;&#31639;&#27861;&#65292;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#32452;&#21512;&#26410;&#30693;&#38382;&#39064;&#38382;&#31572;&#25968;&#25454;&#38598;&#65288;CuQA&#65289;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#29992;&#19981;&#21516;&#26041;&#27861;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;-&#28982;&#21518;&#38405;&#35835;&#21644;&#29983;&#25104;-&#28982;&#21518;&#38405;&#35835;&#26159;&#22788;&#29702;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#26410;&#30693;&#21644;&#24050;&#30693;&#38382;&#39064;&#30340;&#20004;&#31181;&#20856;&#22411;&#35299;&#20915;&#26041;&#26696;&#65292;&#21069;&#32773;&#26816;&#32034;&#24517;&#35201;&#30340;&#22806;&#37096;&#30693;&#35782;&#65292;&#21518;&#32773;&#21017;&#20419;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21442;&#25968;&#20013;&#32534;&#30721;&#30340;&#20869;&#37096;&#24050;&#30693;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36807;&#21435;&#24456;&#23569;&#26377;&#20316;&#21697;&#32771;&#34385;&#21040;&#32452;&#21512;&#26410;&#30693;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#30001;&#20960;&#20010;&#24050;&#30693;&#25110;&#26410;&#30693;&#30340;&#23376;&#38382;&#39064;&#32452;&#25104;&#12290;&#22240;&#27492;&#65292;&#31616;&#21333;&#30340;&#20108;&#20803;&#20998;&#31867;&#65288;&#24050;&#30693;&#25110;&#26410;&#30693;&#65289;&#21464;&#24471;&#27425;&#20248;&#21644;&#20302;&#25928;&#65292;&#22240;&#20026;&#23427;&#20250;&#23545;&#27599;&#20010;&#32452;&#21512;&#26410;&#30693;&#38382;&#39064;&#36807;&#24230;&#35843;&#29992;&#22806;&#37096;&#26816;&#32034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#32452;&#21512;&#26410;&#30693;&#38382;&#39064;&#38382;&#31572;&#25968;&#25454;&#38598;&#65288;CuQA&#65289;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#25105;&#20998;&#32780;&#27835;&#20043;&#65288;Self-DC&#65289;&#26694;&#26550;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#35843;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;CuQA&#21644;FreshQA&#65289;&#19978;&#34920;&#26126;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13514v1 Announce Type: cross  Abstract: Retrieve-then-read and generate-then-read are two typical solutions to handle unknown and known questions in open-domain question-answering, while the former retrieves necessary external knowledge and the later prompt the large language models to generate internal known knowledge encoded in the parameters. However, few of previous works consider the compositional unknown questions, which consist of several known or unknown sub-questions. Thus, simple binary classification (known or unknown) becomes sub-optimal and inefficient since it will call external retrieval excessively for each compositional unknown question. To this end, we propose the first Compositional unknown Question-Answering dataset (CuQA), and introduce a Self Divide-and-Conquer (Self-DC) framework to empower LLMs to adaptively call different methods on-demand, resulting in better performance and efficiency. Experimental results on two datasets (CuQA and FreshQA) demonst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#21040;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#36716;&#21464;&#65292;&#25581;&#31034;&#20102;&#29983;&#25104;Transformer&#21160;&#24577;&#30340;&#26426;&#29702;&#21644;&#30456;&#20851;&#26465;&#20214;&#65292;&#20026;&#19968;&#33268;&#20272;&#35745;&#25552;&#20379;&#20102;&#20445;&#35777;&#65292;&#24182;&#22312;IID&#26679;&#26412;&#19979;&#24314;&#31435;&#20102;&#26679;&#26412;&#22797;&#26434;&#24615;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.13512</link><description>&lt;p&gt;
&#20174;&#33258;&#27880;&#24847;&#21147;&#21040;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65306;&#25581;&#31034;&#29983;&#25104;Transformer&#30340;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
From Self-Attention to Markov Models: Unveiling the Dynamics of Generative Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#21040;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#36716;&#21464;&#65292;&#25581;&#31034;&#20102;&#29983;&#25104;Transformer&#21160;&#24577;&#30340;&#26426;&#29702;&#21644;&#30456;&#20851;&#26465;&#20214;&#65292;&#20026;&#19968;&#33268;&#20272;&#35745;&#25552;&#20379;&#20102;&#20445;&#35777;&#65292;&#24182;&#22312;IID&#26679;&#26412;&#19979;&#24314;&#31435;&#20102;&#26679;&#26412;&#22797;&#26434;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#20381;&#36182;Transformer&#26550;&#26500;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#36827;&#34892;&#35821;&#35328;&#29702;&#35299;&#21644;&#25991;&#26412;&#29983;&#25104;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#19968;&#32452;&#25552;&#31034;&#21644;&#19982;&#27169;&#22411;&#37319;&#26679;&#30340;&#20851;&#32852;&#36755;&#20986;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#21333;&#23618;&#33258;&#27880;&#24847;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#20043;&#38388;&#30340;&#31934;&#30830;&#26144;&#23556;&#65306;&#23558;&#25552;&#31034;&#36755;&#20837;&#27169;&#22411;&#20250;&#26681;&#25454;&#19978;&#19979;&#25991;&#26465;&#20214;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#65288;CCMC&#65289;&#23545;&#36755;&#20986;&#26631;&#35760;&#36827;&#34892;&#37319;&#26679;&#65292;&#35813;&#38142;&#21152;&#26435;&#20102;&#22522;&#26412;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#36716;&#31227;&#30697;&#38453;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20301;&#32622;&#32534;&#30721;&#23548;&#33268;&#20102;&#36716;&#31227;&#27010;&#29575;&#30340;&#20301;&#32622;&#30456;&#20851;&#32553;&#25918;&#12290;&#22522;&#20110;&#36825;&#31181;&#24418;&#24335;&#20027;&#20041;&#65292;&#25105;&#20204;&#20026;&#25552;&#31034;&#20998;&#24067;&#24320;&#21457;&#20102;&#21487;&#36776;&#35782;&#24615;/&#35206;&#30422;&#26465;&#20214;&#65292;&#30830;&#20445;&#19968;&#33268;&#20272;&#35745;&#65292;&#24182;&#22312;IID&#26679;&#26412;&#19979;&#24314;&#31435;&#20102;&#26679;&#26412;&#22797;&#26434;&#24615;&#20445;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#21333;&#20010;&#36755;&#20986;&#36712;&#36857;&#29983;&#25104;&#20013;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13512v1 Announce Type: cross  Abstract: Modern language models rely on the transformer architecture and attention mechanism to perform language understanding and text generation. In this work, we study learning a 1-layer self-attention model from a set of prompts and associated output data sampled from the model. We first establish a precise mapping between the self-attention mechanism and Markov models: Inputting a prompt to the model samples the output token according to a context-conditioned Markov chain (CCMC) which weights the transition matrix of a base Markov chain. Additionally, incorporating positional encoding results in position-dependent scaling of the transition probabilities. Building on this formalism, we develop identifiability/coverage conditions for the prompt distribution that guarantee consistent estimation and establish sample complexity guarantees under IID samples. Finally, we study the problem of learning from a single output trajectory generated from
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#32423;&#32763;&#35793;&#12289;&#35821;&#20041;&#23884;&#20837;&#25193;&#23637;&#21644;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#20013;&#24515;&#25193;&#23637;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#36328;&#35821;&#35328;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20013;&#25913;&#21892;&#21484;&#22238;&#29575;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#21305;&#37197;&#29992;&#25143;&#26597;&#35810;&#21644;&#30456;&#20851;&#25991;&#26723;&#65292;&#23637;&#31034;&#20102;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.13500</link><description>&lt;p&gt;
&#21033;&#29992;&#32763;&#35793;&#23454;&#29616;&#26368;&#20339;&#21484;&#22238;&#29575;&#65306;&#36890;&#36807;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#23450;&#21046;LLM&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Leveraging Translation For Optimal Recall: Tailoring LLM Personalization With User Profiles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#32423;&#32763;&#35793;&#12289;&#35821;&#20041;&#23884;&#20837;&#25193;&#23637;&#21644;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#20013;&#24515;&#25193;&#23637;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#36328;&#35821;&#35328;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20013;&#25913;&#21892;&#21484;&#22238;&#29575;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#21305;&#37197;&#29992;&#25143;&#26597;&#35810;&#21644;&#30456;&#20851;&#25991;&#26723;&#65292;&#23637;&#31034;&#20102;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#39062;&#25216;&#26415;&#65292;&#36890;&#36807;&#22522;&#20110;&#29992;&#25143;&#30340;&#35789;&#27719;-&#35821;&#20041;&#31354;&#38388;&#30340;&#36845;&#20195;&#26597;&#35810;&#20248;&#21270;&#26469;&#25552;&#39640;&#36328;&#35821;&#35328;&#20449;&#24687;&#26816;&#32034;(CLIR)&#31995;&#32479;&#20013;&#30340;&#21484;&#22238;&#29575;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#22810;&#32423;&#32763;&#35793;&#12289;&#22522;&#20110;&#35821;&#20041;&#23884;&#20837;&#30340;&#25193;&#23637;&#65292;&#20197;&#21450;&#20197;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#20026;&#20013;&#24515;&#30340;&#25193;&#23637;&#65292;&#20197;&#35299;&#20915;&#29992;&#25143;&#26597;&#35810;&#21644;&#30456;&#20851;&#25991;&#26723;&#20043;&#38388;&#30340;&#21305;&#37197;&#24046;&#24322;&#25361;&#25112;&#12290;&#36890;&#36807;&#21021;&#22987;&#30340;BM25&#26816;&#32034;&#12289;&#36716;&#25442;&#20026;&#20013;&#38388;&#35821;&#35328;&#12289;&#26597;&#25214;&#30456;&#20284;&#26415;&#35821;&#30340;&#23884;&#20837;&#65292;&#20197;&#21450;&#36845;&#20195;&#37325;&#26032;&#25490;&#21517;&#65292;&#35813;&#25216;&#26415;&#26088;&#22312;&#25193;&#22823;&#21487;&#33021;&#19982;&#20010;&#20307;&#29992;&#25143;&#30456;&#20851;&#30340;&#28508;&#22312;&#32467;&#26524;&#33539;&#22260;&#12290;&#23545;&#26032;&#38395;&#21644;Twitter&#25968;&#25454;&#38598;&#30340;&#27604;&#36739;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#26041;&#27861;&#22312;ROUGE&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#22522;&#32447;BM25&#25490;&#21517;&#12290;&#32763;&#35793;&#26041;&#27861;&#36824;&#36890;&#36807;&#22810;&#27493;&#39588;&#36807;&#31243;&#23637;&#31034;&#20986;&#20102;&#20445;&#25345;&#30340;&#35821;&#20041;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13500v1 Announce Type: cross  Abstract: This paper explores a novel technique for improving recall in cross-language information retrieval (CLIR) systems using iterative query refinement grounded in the user's lexical-semantic space. The proposed methodology combines multi-level translation, semantic embedding-based expansion, and user profile-centered augmentation to address the challenge of matching variance between user queries and relevant documents. Through an initial BM25 retrieval, translation into intermediate languages, embedding lookup of similar terms, and iterative re-ranking, the technique aims to expand the scope of potentially relevant results personalized to the individual user. Comparative experiments on news and Twitter datasets demonstrate superior performance over baseline BM25 ranking for the proposed approach across ROUGE metrics. The translation methodology also showed maintained semantic accuracy through the multi-step process. This personalized CLIR 
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21644;&#35780;&#20272;&#29983;&#29289;&#21307;&#23398;&#25991;&#31456;&#30340;&#24179;&#27665;&#24635;&#32467;&#65292;&#25552;&#20986;&#20102;Explain-then-Summarise&#30340;&#26032;LS&#26694;&#26550;&#65292;&#24182;&#35780;&#20272;&#20102;LLMs&#22312;&#38646;-shot LS&#26041;&#38754;&#30340;&#34920;&#29616;&#21644;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;LLM-based LS&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.13498</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#30340;&#24179;&#27665;&#25351;&#21335;&#65306;&#32534;&#25490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The Lay Person's Guide to Biomedicine: Orchestrating Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13498
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21644;&#35780;&#20272;&#29983;&#29289;&#21307;&#23398;&#25991;&#31456;&#30340;&#24179;&#27665;&#24635;&#32467;&#65292;&#25552;&#20986;&#20102;Explain-then-Summarise&#30340;&#26032;LS&#26694;&#26550;&#65292;&#24182;&#35780;&#20272;&#20102;LLMs&#22312;&#38646;-shot LS&#26041;&#38754;&#30340;&#34920;&#29616;&#21644;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;LLM-based LS&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#12298;arXiv&#65306;2402.13498v1&#12299;&#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340;&#25688;&#35201;&#65306;&#33258;&#21160;&#21270;&#30340;&#24179;&#27665;&#24635;&#32467;&#65288;LS&#65289;&#26088;&#22312;&#23558;&#22797;&#26434;&#30340;&#25216;&#26415;&#25991;&#20214;&#31616;&#21270;&#20026;&#26356;&#26131;&#20110;&#38750;&#19987;&#19994;&#20154;&#22763;&#29702;&#35299;&#30340;&#26684;&#24335;&#12290;&#29616;&#26377;&#30340;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#33021;&#36741;&#20197;&#22806;&#37096;&#32972;&#26223;&#30693;&#35782;&#30340;&#26041;&#27861;&#24448;&#24448;&#22312;&#26377;&#25928;&#31616;&#21270;&#21644;&#35299;&#37322;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#33021;&#22815;&#26377;&#25928;&#35780;&#20272;&#29983;&#25104;&#25688;&#35201;&#30340;&#8220;&#24179;&#27665;&#24615;&#8221;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#20063;&#32570;&#20047;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#22312;&#25991;&#26412;&#31616;&#21270;&#12289;&#32972;&#26223;&#20449;&#24687;&#29983;&#25104;&#21644;&#25991;&#26412;&#35780;&#20272;&#26041;&#38754;&#30340;&#26174;&#30528;&#33021;&#21147;&#12290;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#23545;&#20351;&#29992;LLMs&#29983;&#25104;&#21644;&#35780;&#20272;&#29983;&#29289;&#21307;&#23398;&#25991;&#31456;&#30340;&#24179;&#27665;&#24635;&#32467;&#36827;&#34892;&#31995;&#32479;&#24615;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#20808;&#35299;&#37322;&#21518;&#24635;&#32467;&#8221;LS&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#32972;&#26223;&#30693;&#35782;&#20197;&#25913;&#36827;&#30417;&#30563;&#30340;LS&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;LLMs&#22312;&#38646;-shot LS&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;LLM&#30340;&#26032;&#39062;LS&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13498v1 Announce Type: new  Abstract: Automated lay summarisation (LS) aims to simplify complex technical documents into a more accessible format to non-experts. Existing approaches using pre-trained language models, possibly augmented with external background knowledge, tend to struggle with effective simplification and explanation. Moreover, automated methods that can effectively assess the `layness' of generated summaries are lacking. Recently, large language models (LLMs) have demonstrated a remarkable capacity for text simplification, background information generation, and text evaluation. This has motivated our systematic exploration into using LLMs to generate and evaluate lay summaries of biomedical articles. We propose a novel \textit{Explain-then-Summarise} LS framework, which leverages LLMs to generate high-quality background knowledge to improve supervised LS. We also evaluate the performance of LLMs for zero-shot LS and propose two novel LLM-based LS evaluation 
&lt;/p&gt;</description></item><item><title>GradSafe&#36890;&#36807;&#20998;&#26512;LLMs&#20013;&#20851;&#38190;&#23433;&#20840;&#21442;&#25968;&#30340;&#26799;&#24230;&#65292;&#26377;&#25928;&#26816;&#27979;&#19981;&#23433;&#20840;&#25552;&#31034;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#21363;&#21487;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.13494</link><description>&lt;p&gt;
GradSafe: &#36890;&#36807;&#23433;&#20840;&#20851;&#38190;&#26799;&#24230;&#20998;&#26512;&#26816;&#27979;LLMs&#20013;&#30340;&#19981;&#23433;&#20840;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13494
&lt;/p&gt;
&lt;p&gt;
GradSafe&#36890;&#36807;&#20998;&#26512;LLMs&#20013;&#20851;&#38190;&#23433;&#20840;&#21442;&#25968;&#30340;&#26799;&#24230;&#65292;&#26377;&#25928;&#26816;&#27979;&#19981;&#23433;&#20840;&#25552;&#31034;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#21363;&#21487;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38754;&#20020;&#26469;&#33258;&#19981;&#23433;&#20840;&#25552;&#31034;&#30340;&#23041;&#32961;&#12290;&#29616;&#26377;&#30340;&#26816;&#27979;&#19981;&#23433;&#20840;&#25552;&#31034;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#22312;&#32447;&#20869;&#23481;&#23457;&#26680;API&#25110;&#24494;&#35843;LLMs&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31574;&#30053;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#21644;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GradSafe&#65292;&#36890;&#36807;&#20180;&#32454;&#23457;&#26597;LLMs&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#21442;&#25968;&#30340;&#26799;&#24230;&#26377;&#25928;&#22320;&#26816;&#27979;&#19981;&#23433;&#20840;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;LLMs&#23545;&#20110;&#19982;&#21512;&#35268;&#21709;&#24212;&#37197;&#23545;&#30340;&#19981;&#23433;&#20840;&#25552;&#31034;&#30340;&#25439;&#22833;&#30340;&#26799;&#24230;&#22312;&#26576;&#20123;&#23433;&#20840;&#20851;&#38190;&#21442;&#25968;&#19978;&#34920;&#29616;&#20986;&#30456;&#20284;&#30340;&#27169;&#24335;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23433;&#20840;&#25552;&#31034;&#23548;&#33268;&#26126;&#26174;&#19981;&#21516;&#30340;&#26799;&#24230;&#27169;&#24335;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;GradSafe&#20998;&#26512;&#26469;&#33258;&#25552;&#31034;&#65288;&#19982;&#21512;&#35268;&#21709;&#24212;&#37197;&#23545;&#65289;&#30340;&#26799;&#24230;&#20197;&#20934;&#30830;&#22320;&#26816;&#27979;&#19981;&#23433;&#20840;&#25552;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#24212;&#29992;&#20110;Llama-2&#30340;GradSafe&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#35757;&#32451;&#21363;&#21487;&#32988;&#36807;Llama Guard&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13494v1 Announce Type: new  Abstract: Large Language Models (LLMs) face threats from unsafe prompts. Existing methods for detecting unsafe prompts are primarily online moderation APIs or finetuned LLMs. These strategies, however, often require extensive and resource-intensive data collection and training processes. In this study, we propose GradSafe, which effectively detects unsafe prompts by scrutinizing the gradients of safety-critical parameters in LLMs. Our methodology is grounded in a pivotal observation: the gradients of an LLM's loss for unsafe prompts paired with compliance response exhibit similar patterns on certain safety-critical parameters. In contrast, safe prompts lead to markedly different gradient patterns. Building on this observation, GradSafe analyzes the gradients from prompts (paired with compliance responses) to accurately detect unsafe prompts. We show that GradSafe, applied to Llama-2 without further training, outperforms Llama Guard, despite its ex
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#26500;&#24314;&#20102;&#26032;&#30340;QA&#25968;&#25454;&#38598;WiTQA&#65292;&#20197;&#23454;&#20307;&#21644;&#20851;&#31995;&#32452;&#21512;&#30340;&#24433;&#21709;&#20026;&#37325;&#28857;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.13492</link><description>&lt;p&gt;
&#12298;&#26816;&#32034;&#26159;&#26377;&#30410;&#36824;&#26159;&#26377;&#23475;&#65311;&#28145;&#20837;&#25506;&#35752;&#26816;&#32034;&#22686;&#24378;&#23545;&#35821;&#35328;&#27169;&#22411;&#25928;&#26524;&#30340;&#24433;&#21709;&#12299;
&lt;/p&gt;
&lt;p&gt;
Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation to Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13492
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#26500;&#24314;&#20102;&#26032;&#30340;QA&#25968;&#25454;&#38598;WiTQA&#65292;&#20197;&#23454;&#20307;&#21644;&#20851;&#31995;&#32452;&#21512;&#30340;&#24433;&#21709;&#20026;&#37325;&#28857;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#24403;&#38656;&#35201;&#26597;&#35810;&#20854;&#39044;&#35757;&#32451;&#35760;&#24518;&#20043;&#22806;&#30340;&#20449;&#24687;&#26102;&#65292;&#23427;&#20204;&#22312;&#25552;&#20379;&#20934;&#30830;&#22238;&#31572;&#26102;&#20250;&#36935;&#21040;&#25361;&#25112;&#12290;&#34429;&#28982;&#21033;&#29992;&#30456;&#20851;&#22806;&#37096;&#20449;&#24687;&#26469;&#22686;&#24378;&#23427;&#20204;&#21487;&#20197;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#26410;&#32771;&#34385;&#26816;&#32034;&#30340;&#24517;&#35201;&#24615;&#21487;&#33021;&#20250;&#23545;&#25972;&#20307;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#27492;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#23454;&#20307;&#22914;&#20309;&#24433;&#21709;&#26816;&#32034;&#27169;&#22411;&#19982;LMs&#20013;&#30340;&#30693;&#35782;&#22238;&#24518;&#65292;&#20854;&#20182;&#26041;&#38754;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#23454;&#20307;&#21644;&#20851;&#31995;&#32452;&#21512;&#30340;&#24433;&#21709;&#26469;&#25552;&#20379;&#26356;&#35814;&#32454;&#12289;&#20197;&#20107;&#23454;&#20026;&#20013;&#24515;&#30340;&#20998;&#26512;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;WiTQA&#65288;Wikipedia Triple Question Answers&#65289;&#30340;&#26032;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#12290;&#27492;&#25968;&#25454;&#38598;&#21253;&#25324;&#20851;&#20110;&#19981;&#21516;&#21463;&#27426;&#36814;&#31243;&#24230;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#27599;&#20010;&#38382;&#39064;&#37117;&#38468;&#24102;&#19968;&#27573;&#25903;&#25345;&#24615;&#27573;&#33853;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13492v1 Announce Type: new  Abstract: While large language models (LMs) demonstrate remarkable performance, they encounter challenges in providing accurate responses when queried for information beyond their pre-trained memorization. Although augmenting them with relevant external information can mitigate these issues, failure to consider the necessity of retrieval may adversely affect overall performance. Previous research has primarily focused on examining how entities influence retrieval models and knowledge recall in LMs, leaving other aspects relatively unexplored. In this work, our goal is to offer a more detailed, fact-centric analysis by exploring the effects of combinations of entities and relations. To facilitate this, we construct a new question answering (QA) dataset called WiTQA (Wikipedia Triple Question Answers). This dataset includes questions about entities and relations of various popularity levels, each accompanied by a supporting passage. Our extensive ex
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;ProPD&#65292;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#20196;&#29260;&#26641;&#20462;&#21098;&#21644;&#29983;&#25104;&#30340;&#39640;&#25928;LLM&#24182;&#34892;&#35299;&#30721;&#26694;&#26550;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#25552;&#21069;&#20462;&#21098;&#26426;&#21046;&#21644;&#21160;&#24577;&#20196;&#29260;&#26641;&#29983;&#25104;&#31639;&#27861;&#26469;&#25552;&#39640;&#39564;&#35777;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.13485</link><description>&lt;p&gt;
ProPD&#65306;LLM&#24182;&#34892;&#35299;&#30721;&#30340;&#21160;&#24577;&#20196;&#29260;&#26641;&#20462;&#21098;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13485
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;ProPD&#65292;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#20196;&#29260;&#26641;&#20462;&#21098;&#21644;&#29983;&#25104;&#30340;&#39640;&#25928;LLM&#24182;&#34892;&#35299;&#30721;&#26694;&#26550;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#25552;&#21069;&#20462;&#21098;&#26426;&#21046;&#21644;&#21160;&#24577;&#20196;&#29260;&#26641;&#29983;&#25104;&#31639;&#27861;&#26469;&#25552;&#39640;&#39564;&#35777;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#26174;&#33879;&#25552;&#21319;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25928;&#29575;&#21463;&#21040;&#33258;&#22238;&#24402;&#20196;&#29260;&#29983;&#25104;&#20013;&#22266;&#26377;&#38480;&#21046;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#24102;&#26377;&#20196;&#29260;&#26641;&#39564;&#35777;&#30340;&#24182;&#34892;&#35299;&#30721;&#26041;&#27861;&#65292;&#20363;&#22914;Medusa&#65292;&#20197;&#25913;&#21892;&#35299;&#30721;&#24182;&#34892;&#24615;&#21644;&#25928;&#29575;&#65292;&#20294;&#30001;&#20110;&#20854;&#29420;&#31435;&#20196;&#29260;&#39044;&#27979;&#26041;&#27861;&#20197;&#21450;&#22823;&#26641;&#22823;&#23567;&#21644;&#25209;&#22788;&#29702;&#26102;&#20135;&#29983;&#30340;&#26174;&#33879;&#39564;&#35777;&#24320;&#38144;&#65292;&#23427;&#32463;&#24120;&#38590;&#20197;&#20445;&#25345;&#19978;&#19979;&#25991;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ProPD&#65292;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#20196;&#29260;&#26641;&#20462;&#21098;&#21644;&#29983;&#25104;&#30340;&#39640;&#25928;LLM&#24182;&#34892;&#35299;&#30721;&#26694;&#26550;&#12290;ProPD&#20855;&#26377;&#19968;&#31181;&#20808;&#36827;&#30340;&#25552;&#21069;&#20462;&#21098;&#26426;&#21046;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#28040;&#38500;&#19981;&#22826;&#21487;&#33021;&#30340;&#20196;&#29260;&#24207;&#21015;&#20197;&#25552;&#39640;&#39564;&#35777;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#23427;&#24341;&#20837;&#20102;&#19968;&#31181;&#21160;&#24577;&#20196;&#29260;&#26641;&#29983;&#25104;&#31639;&#27861;&#26469;&#24179;&#34913;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13485v1 Announce Type: cross  Abstract: Recent advancements in generative large language models (LLMs) have significantly boosted the performance in natural language processing tasks. However, their efficiency is hampered by the inherent limitations in autoregressive token generation. While parallel decoding with token tree verification, e.g., Medusa, has been proposed to improve decoding parallelism and efficiency, it often struggles with maintaining contextual relationships due to its independent token prediction approach and incurs significant verification overhead, especially with large tree sizes and batch processing. In this paper, we propose ProPD, an efficient LLM parallel decoding framework based on dynamic token tree pruning and generation. ProPD features an advanced early pruning mechanism to efficiently eliminate unpromising token sequences to improve verification efficiency. Additionally, it introduces a dynamic token tree generation algorithm to balance the com
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;&#39046;&#22495;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#26469;&#33258;&#20854;&#20182;&#25968;&#25454;&#38598;&#30340;&#30456;&#20851;&#31034;&#20363;&#26469;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#35299;&#20915;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#29983;&#25104;&#26679;&#26412;&#19981;&#22815;&#29702;&#24819;&#21644;&#32570;&#20047;&#22810;&#26679;&#24615;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.13482</link><description>&lt;p&gt;
&#29992;&#20110;&#20302;&#36164;&#28304;&#39046;&#22495;&#20219;&#21153;&#30340;&#26816;&#32034;&#22686;&#24378;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13482
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;&#39046;&#22495;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#26469;&#33258;&#20854;&#20182;&#25968;&#25454;&#38598;&#30340;&#30456;&#20851;&#31034;&#20363;&#26469;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#35299;&#20915;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#29983;&#25104;&#26679;&#26412;&#19981;&#22815;&#29702;&#24819;&#21644;&#32570;&#20047;&#22810;&#26679;&#24615;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#26679;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20250;&#20005;&#37325;&#19979;&#38477;&#12290;&#35768;&#22810;&#29616;&#26377;&#20316;&#21697;&#36890;&#36807;&#20174;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#28982;&#21518;&#22312;&#20854;&#19978;&#35757;&#32451;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#12290;&#28982;&#32780;&#65292;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#65292;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#31181;&#23376;&#25968;&#25454;&#26679;&#26412;&#25968;&#37327;&#38750;&#24120;&#23569;&#65292;&#36825;&#20351;&#24471;&#29983;&#25104;&#30340;&#26679;&#26412;&#19981;&#22815;&#29702;&#24819;&#19988;&#32570;&#20047;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20854;&#20182;&#25968;&#25454;&#38598;&#20013;&#20016;&#23500;&#30340;&#31034;&#20363;&#19982;&#32473;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#65292;&#26469;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#19982;&#32473;&#23450;&#31181;&#23376;&#25968;&#25454;&#30456;&#20284;&#24615;&#22522;&#20110;&#20854;&#20182;&#25968;&#25454;&#38598;&#26816;&#32034;&#30456;&#20851;&#23454;&#20363;&#65292;&#20363;&#22914;&#23427;&#20204;&#30340;&#36755;&#20837;-&#36755;&#20986;&#23545;&#25110;&#19978;&#19979;&#25991;&#65292;&#28982;&#21518;&#25552;&#31034;LLM&#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#29983;&#25104;&#26032;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13482v1 Announce Type: cross  Abstract: Despite large successes of recent language models on diverse tasks, they suffer from severe performance degeneration in low-resource settings with limited training data available. Many existing works tackle this problem by generating synthetic data from the training data and then training models on them, recently using Large Language Models (LLMs). However, in low-resource settings, the amount of seed data samples to use for data augmentation is very small, which makes generated samples suboptimal and less diverse. To tackle this challenge, we propose a novel method that augments training data by incorporating a wealth of examples from other datasets, along with the given training data. Specifically, we first retrieve the relevant instances from other datasets, such as their input-output pairs or contexts, based on their similarities with the given seed data, and then prompt LLMs to generate new samples with the contextual information 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#20851;&#31995;&#25552;&#21462;&#20219;&#21153;&#20013;&#39046;&#22495;&#29305;&#24322;&#24615;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#21644;&#25351;&#23548;&#24494;&#35843;&#30340;&#37325;&#35201;&#24615;&#65292;&#23545;&#27604;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#19982;&#36890;&#29992;&#39046;&#22495;&#35757;&#32451;&#30340;&#27169;&#22411;&#25928;&#26524;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#25351;&#23548;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.13470</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#29289;&#21307;&#23398;&#20851;&#31995;&#25552;&#21462;&#20013;&#30340;&#39046;&#22495;&#29305;&#24322;&#24615;&#26377;&#22810;&#37325;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Important is Domain Specificity in Language Models and Instruction Finetuning for Biomedical Relation Extraction?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13470
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#20851;&#31995;&#25552;&#21462;&#20219;&#21153;&#20013;&#39046;&#22495;&#29305;&#24322;&#24615;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#21644;&#25351;&#23548;&#24494;&#35843;&#30340;&#37325;&#35201;&#24615;&#65292;&#23545;&#27604;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#19982;&#36890;&#29992;&#39046;&#22495;&#35757;&#32451;&#30340;&#27169;&#22411;&#25928;&#26524;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#25351;&#23548;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20215;&#20540;&#12289;&#25968;&#25454;&#20016;&#23500;&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#24120;&#24120;&#20250;&#20351;&#29992;&#26368;&#21069;&#27839;&#30340;&#36890;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#12290;&#36807;&#21435;&#20960;&#24180;&#26469;&#65292;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#12289;&#25351;&#23548;&#24494;&#35843;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#30340;&#28966;&#28857;&#12290;&#22240;&#27492;&#65292;&#39044;&#35757;&#32451;&#20110;&#29983;&#29289;&#21307;&#23398;&#35821;&#26009;&#24211;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#19981;&#26029;&#28044;&#29616;&#65292;&#21516;&#26102;&#20063;&#23581;&#35797;&#23545;&#29983;&#29289;&#21307;&#23398;&#25351;&#23548;&#24494;&#35843;&#65292;&#24076;&#26395;&#39046;&#22495;&#29305;&#24322;&#24615;&#21487;&#20197;&#25913;&#21892;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#37492;&#20110;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#25152;&#38656;&#30340;&#38750;&#24179;&#20961;&#21162;&#21147;&#65292;&#25105;&#20204;&#30740;&#31350;&#23427;&#20204;&#22312;&#20851;&#31995;&#25552;&#21462;&#36825;&#19968;&#20851;&#38190;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#26159;&#21542;&#23384;&#22312;&#20219;&#20309;&#30410;&#22788;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20004;&#20010;&#38382;&#39064;&#65306;(1) &#22312;&#29983;&#29289;&#21307;&#23398;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20248;&#20110;&#22312;&#36890;&#29992;&#39046;&#22495;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#65311;(2) &#22312;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25351;&#23548;&#24494;&#35843;&#30340;&#27169;&#22411;&#26159;&#21542;&#20248;&#20110;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#25110;&#32773;&#20165;&#20165;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65311;&#25105;&#20204;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13470v1 Announce Type: new  Abstract: Cutting edge techniques developed in the general NLP domain are often subsequently applied to the high-value, data-rich biomedical domain. The past few years have seen generative language models (LMs), instruction finetuning, and few-shot learning become foci of NLP research. As such, generative LMs pretrained on biomedical corpora have proliferated and biomedical instruction finetuning has been attempted as well, all with the hope that domain specificity improves performance on downstream tasks. Given the nontrivial effort in training such models, we investigate what, if any, benefits they have in the key biomedical NLP task of relation extraction. Specifically, we address two questions: (1) Do LMs trained on biomedical corpora outperform those trained on general domain corpora? (2) Do models instruction finetuned on biomedical datasets outperform those finetuned on assorted datasets or those simply pretrained? We tackle these questions
&lt;/p&gt;</description></item><item><title>STENCIL&#21033;&#29992;&#27425;&#27169;&#20114;&#20449;&#24687;&#36873;&#25321;&#24369;&#26631;&#35760;&#30340;&#31232;&#26377;&#31867;&#23454;&#20363;&#65292;&#24182;&#36890;&#36807;&#26631;&#27880;&#32773;&#24378;&#26631;&#35760;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#21644;&#31232;&#26377;&#31867;F-1&#20998;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.13468</link><description>&lt;p&gt;
STENCIL&#65306;&#22522;&#20110;&#27425;&#27169;&#20114;&#20449;&#24687;&#30340;&#20919;&#21551;&#21160;&#20027;&#21160;&#23398;&#20064;&#24369;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
STENCIL: Submodular Mutual Information Based Weak Supervision for Cold-Start Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13468
&lt;/p&gt;
&lt;p&gt;
STENCIL&#21033;&#29992;&#27425;&#27169;&#20114;&#20449;&#24687;&#36873;&#25321;&#24369;&#26631;&#35760;&#30340;&#31232;&#26377;&#31867;&#23454;&#20363;&#65292;&#24182;&#36890;&#36807;&#26631;&#27880;&#32773;&#24378;&#26631;&#35760;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#21644;&#31232;&#26377;&#31867;F-1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;NLP&#24212;&#29992;&#20013;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#38656;&#35201;&#26356;&#22823;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#35745;&#25968;&#22686;&#21152;&#26102;&#12290;&#20027;&#21160;&#23398;&#20064;&#35797;&#22270;&#25366;&#25496;&#21644;&#27880;&#37322;&#26410;&#26631;&#35760;&#30340;&#23454;&#20363;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#24555;&#36895;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#65292;&#26159;&#20943;&#23569;&#27880;&#37322;&#25104;&#26412;&#30340;&#24120;&#35265;&#36873;&#25321;&#65307;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#36890;&#24120;&#24573;&#35270;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#24182;&#19988;&#35201;&#20040;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#21021;&#22987;&#26631;&#27880;&#25968;&#25454;&#65292;&#35201;&#20040;&#35201;&#27714;&#25913;&#36827;&#31232;&#26377;&#31867;&#20043;&#21069;&#38656;&#35201;&#22810;&#36718;&#20027;&#21160;&#23398;&#20064;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;STENCIL&#65292;&#23427;&#21033;&#29992;&#19968;&#32452;&#25991;&#26412;&#31034;&#20363;&#21644;&#26368;&#36817;&#25552;&#20986;&#30340;&#27425;&#27169;&#20114;&#20449;&#24687;&#26469;&#36873;&#25321;&#19968;&#32452;&#24369;&#26631;&#35760;&#30340;&#31232;&#26377;&#31867;&#23454;&#20363;&#65292;&#28982;&#21518;&#30001;&#26631;&#27880;&#32773;&#23545;&#20854;&#36827;&#34892;&#24378;&#26631;&#35760;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;STENCIL&#22312;&#22810;&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#23558;&#25972;&#20307;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;10%-24%&#65292;&#23558;&#31232;&#26377;&#31867;F-1&#20998;&#25968;&#25552;&#39640;&#20102;17%-40%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13468v1 Announce Type: cross  Abstract: As supervised fine-tuning of pre-trained models within NLP applications increases in popularity, larger corpora of annotated data are required, especially with increasing parameter counts in large language models. Active learning, which attempts to mine and annotate unlabeled instances to improve model performance maximally fast, is a common choice for reducing the annotation cost; however, most methods typically ignore class imbalance and either assume access to initial annotated data or require multiple rounds of active learning selection before improving rare classes. We present STENCIL, which utilizes a set of text exemplars and the recently proposed submodular mutual information to select a set of weakly labeled rare-class instances that are then strongly labeled by an annotator. We show that STENCIL improves overall accuracy by $10\%-24\%$ and rare-class F-1 score by $17\%-40\%$ on multiple text classification datasets over commo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RefuteBench&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21453;&#39539;&#25351;&#20196;&#30340;&#36981;&#24490;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#20542;&#21521;&#20110;&#22266;&#25191;&#20110;&#20854;&#20869;&#37096;&#30693;&#35782;&#32780;&#26080;&#27861;&#36981;&#20174;&#29992;&#25143;&#21453;&#39304;&#12290;</title><link>https://arxiv.org/abs/2402.13463</link><description>&lt;p&gt;
RefuteBench&#65306;&#35780;&#20272;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39539;&#25351;&#20196;&#36981;&#24490;
&lt;/p&gt;
&lt;p&gt;
RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RefuteBench&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21453;&#39539;&#25351;&#20196;&#30340;&#36981;&#24490;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#20542;&#21521;&#20110;&#22266;&#25191;&#20110;&#20854;&#20869;&#37096;&#30693;&#35782;&#32780;&#26080;&#27861;&#36981;&#20174;&#29992;&#25143;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24212;&#29992;&#33539;&#22260;&#26085;&#30410;&#25193;&#22823;&#12290;&#22312;&#23454;&#38469;&#20351;&#29992;&#20013;&#65292;&#29992;&#25143;&#21487;&#33021;&#26681;&#25454;&#27169;&#22411;&#30340;&#36755;&#20986;&#25552;&#20379;&#21453;&#39304;&#65292;&#24076;&#26395;&#24471;&#21040;&#19968;&#20010;&#21487;&#20197;&#26681;&#25454;&#20182;&#20204;&#30340;&#21453;&#39304;&#23436;&#25104;&#21709;&#24212;&#30340;&#21709;&#24212;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#33021;&#21542;&#24688;&#24403;&#22320;&#21709;&#24212;&#29992;&#25143;&#30340;&#21453;&#39539;&#21453;&#39304;&#24182;&#22987;&#32456;&#25191;&#34892;&#19979;&#21435;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#20998;&#26512;&#12290;&#22522;&#20110;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;RefuteBench&#65292;&#28085;&#30422;&#20102;&#35832;&#22914;&#38382;&#31572;&#12289;&#26426;&#22120;&#32763;&#35793;&#21644;&#30005;&#23376;&#37038;&#20214;&#25776;&#20889;&#31561;&#20219;&#21153;&#12290;&#35780;&#20272;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#31215;&#26497;&#25509;&#21463;&#21453;&#39539;&#25351;&#20196;&#24418;&#24335;&#30340;&#21453;&#39304;&#65292;&#24182;&#26159;&#21542;&#33021;&#22815;&#22312;&#23545;&#35805;&#20013;&#22987;&#32456;&#36981;&#24490;&#29992;&#25143;&#38656;&#27714;&#12290;&#25105;&#20204;&#23545;&#20247;&#22810;LLMs&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;LLMs&#20542;&#21521;&#22266;&#25191;&#65292;&#21363;&#20542;&#21521;&#20110;&#20854;&#20869;&#37096;&#30693;&#35782;&#65292;&#32463;&#24120;&#26410;&#33021;&#36981;&#23432;&#29992;&#25143;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13463v1 Announce Type: cross  Abstract: The application scope of large language models (LLMs) is increasingly expanding. In practical use, users might provide feedback based on the model's output, hoping for a responsive model that can complete responses according to their feedback. Whether the model can appropriately respond to users' refuting feedback and consistently follow through with execution has not been thoroughly analyzed. In light of this, this paper proposes a comprehensive benchmark, RefuteBench, covering tasks such as question answering, machine translation, and email writing. The evaluation aims to assess whether models can positively accept feedback in form of refuting instructions and whether they can consistently adhere to user demands throughout the conversation. We conduct evaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibit inclination to their internal knowledge, often failing to comply with user feedback. Additionally, as the leng
&lt;/p&gt;</description></item><item><title>&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#31038;&#20132;&#21435;&#20559;&#35265;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#20063;&#38754;&#20020;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#25903;&#25345;&#19981;&#21516;&#20559;&#35265;&#31867;&#22411;&#21644;&#29702;&#35299;&#32534;&#36753;&#26041;&#27861;&#24212;&#29992;&#20110;&#21435;&#20559;&#35265;&#36807;&#31243;&#20013;&#30340;&#21033;&#24330;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.13462</link><description>&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#22312;&#31038;&#20132;&#21435;&#20559;&#35265;&#20013;&#30340;&#28508;&#21147;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Potential and Challenges of Model Editing for Social Debiasing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13462
&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#31038;&#20132;&#21435;&#20559;&#35265;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#20063;&#38754;&#20020;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#25903;&#25345;&#19981;&#21516;&#20559;&#35265;&#31867;&#22411;&#21644;&#29702;&#35299;&#32534;&#36753;&#26041;&#27861;&#24212;&#29992;&#20110;&#21435;&#20559;&#35265;&#36807;&#31243;&#20013;&#30340;&#21033;&#24330;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#37327;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19981;&#21487;&#36991;&#20813;&#22320;&#23384;&#22312;&#21051;&#26495;&#21360;&#35937;&#20559;&#35265;&#12290;&#36890;&#36807;&#24494;&#35843;&#26469;&#20943;&#36731;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#26082;&#26114;&#36149;&#21448;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#12290;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#19987;&#27880;&#20110;&#20197;&#20107;&#21518;&#26041;&#24335;&#20462;&#25913;LLMs&#65292;&#23545;&#20110;&#35299;&#20915;&#21435;&#20559;&#35265;&#38382;&#39064;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#25903;&#25345;&#21508;&#31181;&#20559;&#35265;&#31867;&#22411;&#65292;&#24182;&#20102;&#35299;&#24212;&#29992;&#32534;&#36753;&#26041;&#27861;&#20110;&#21435;&#20559;&#35265;&#36807;&#31243;&#20013;&#30340;&#21033;&#24330;&#30340;&#32508;&#21512;&#30740;&#31350;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#23558;&#31038;&#20132;&#21435;&#20559;&#35265;&#20180;&#32454;&#26500;&#24314;&#20026;&#19968;&#20010;&#32534;&#36753;&#38382;&#39064;&#65292;&#24182;&#22312;&#21051;&#26495;&#21360;&#35937;&#21435;&#20559;&#35265;&#19978;&#23545;&#19971;&#31181;&#29616;&#26377;&#30340;&#27169;&#22411;&#32534;&#36753;&#31639;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21363;&#21435;&#20559;&#35265;&#32534;&#36753;&#12290;&#25105;&#20204;&#22312;&#19977;&#31181;&#24773;&#26223;&#19979;&#30340;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#21435;&#20559;&#35265;&#32534;&#36753;&#30340;&#28508;&#21147;&#19982;&#25361;&#25112;&#65306;&#65288;1&#65289;&#29616;&#26377;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20445;&#30041;&#30693;&#35782;&#24182;&#20943;&#36731;&#20559;&#35265;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#21435;&#20559;&#35265;&#25928;&#26524;&#20174;&#32534;&#36753;&#21040;&#24212;&#29992;&#30340;&#19968;&#33324;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13462v1 Announce Type: cross  Abstract: Large language models (LLMs) trained on vast corpora suffer from inevitable stereotype biases. Mitigating these biases with fine-tuning could be both costly and data-hungry. Model editing methods, which focus on modifying LLMs in a post-hoc manner, are of great potential to address debiasing. However, it lacks a comprehensive study that facilitates both internal and external model editing methods, supports various bias types, as well as understands the pros and cons of applying editing methods to stereotypical debiasing. To mitigate this gap, we carefully formulate social debiasing into an editing problem and benchmark seven existing model editing algorithms on stereotypical debiasing, i.e., debias editing. Our findings in three scenarios reveal both the potential and challenges of debias editing: (1) Existing model editing methods can effectively preserve knowledge and mitigate biases, while the generalization of debias effect from ed
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#25915;&#20987;LLMs&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26799;&#24230;&#24341;&#23548;&#21518;&#38376;&#35302;&#21457;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#34920;&#26126;&#25104;&#21151;&#22320;&#30772;&#22351;&#27169;&#22411;&#36755;&#20986;&#65292;&#20165;&#25913;&#21464;1%&#30340;&#25351;&#23548;&#35843;&#20248;&#26679;&#26412;&#21363;&#21487;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#29575;&#36798;&#21040;&#32422;80&#65285;&#12290;</title><link>https://arxiv.org/abs/2402.13459</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#25351;&#23548;&#35843;&#20248;&#26399;&#38388;&#25805;&#32437;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning to Poison Large Language Models During Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13459
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#25915;&#20987;LLMs&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26799;&#24230;&#24341;&#23548;&#21518;&#38376;&#35302;&#21457;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#34920;&#26126;&#25104;&#21151;&#22320;&#30772;&#22351;&#27169;&#22411;&#36755;&#20986;&#65292;&#20165;&#25913;&#21464;1%&#30340;&#25351;&#23548;&#35843;&#20248;&#26679;&#26412;&#21363;&#21487;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#29575;&#36798;&#21040;&#32422;80&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#26631;&#24535;&#30528;&#35821;&#35328;&#22788;&#29702;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#37325;&#22823;&#31361;&#30772;&#12290;&#34429;&#28982;&#23427;&#20204;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;LLMs&#38754;&#20020;&#30528;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#30340;&#28431;&#27934;&#65292;&#20854;&#20013;&#23545;&#25163;&#23558;&#21518;&#38376;&#35302;&#21457;&#22120;&#25554;&#20837;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#25805;&#32437;&#36755;&#20986;&#20197;&#36827;&#34892;&#24694;&#24847;&#34892;&#20026;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#65292;&#26088;&#22312;&#21033;&#29992;&#25351;&#23548;&#35843;&#20248;&#36807;&#31243;&#65292;&#36827;&#19968;&#27493;&#35782;&#21035;LLMs&#20013;&#30340;&#39069;&#22806;&#23433;&#20840;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26799;&#24230;&#24341;&#23548;&#21518;&#38376;&#35302;&#21457;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#35782;&#21035;&#25932;&#23545;&#35302;&#21457;&#22120;&#65292;&#30830;&#20445;&#23545;&#20256;&#32479;&#38450;&#24481;&#25163;&#27573;&#30340;&#35268;&#36991;&#65292;&#21516;&#26102;&#20445;&#25345;&#20869;&#23481;&#30340;&#23436;&#25972;&#24615;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;LLMs&#21644;&#20219;&#21153;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#34920;&#26126;&#22312;&#30772;&#22351;&#27169;&#22411;&#36755;&#20986;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#25104;&#21151;&#29575;&#65307;&#20165;&#23545;4,000&#20010;&#25351;&#23548;&#35843;&#20248;&#26679;&#26412;&#20013;&#30340;1&#65285;&#36827;&#34892;&#27880;&#20837;&#23601;&#23548;&#33268;&#24615;&#33021;&#38477;&#20302;&#29575;&#65288;PDR&#65289;&#32422;&#20026;80&#65285;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39640;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13459v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has marked significant achievements in language processing and reasoning capabilities. Despite their advancements, LLMs face vulnerabilities to data poisoning attacks, where adversaries insert backdoor triggers into training data to manipulate outputs for malicious purposes. This work further identifies additional security risks in LLMs by designing a new data poisoning attack tailored to exploit the instruction tuning process. We propose a novel gradient-guided backdoor trigger learning approach to identify adversarial triggers efficiently, ensuring an evasion of detection by conventional defenses while maintaining content integrity. Through experimental validation across various LLMs and tasks, our strategy demonstrates a high success rate in compromising model outputs; poisoning only 1\% of 4,000 instruction tuning samples leads to a Performance Drop Rate (PDR) of around 80\%. Our work high
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;Twitter&#25968;&#25454;&#30340;&#26694;&#26550;LocalHealth&#65292;&#29992;&#20110;&#39044;&#27979;&#24403;&#22320;&#31934;&#31070;&#20581;&#24247;&#32467;&#26524;&#12290;&#36890;&#36807;&#19982;GPT3.5&#32467;&#21512;&#20351;&#29992;&#65292;&#35813;&#26694;&#26550;&#22312;MH&#30417;&#27979;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.13452</link><description>&lt;p&gt;
&#22522;&#20110;Twitter&#25968;&#25454;&#30340;&#31934;&#31070;&#20581;&#24247;&#30417;&#27979;&#26694;&#26550;&#65306;&#20174;&#24403;&#22320;&#25512;&#25991;&#21040;&#24403;&#22320;&#20581;&#24247;
&lt;/p&gt;
&lt;p&gt;
LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based on Twitter Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;Twitter&#25968;&#25454;&#30340;&#26694;&#26550;LocalHealth&#65292;&#29992;&#20110;&#39044;&#27979;&#24403;&#22320;&#31934;&#31070;&#20581;&#24247;&#32467;&#26524;&#12290;&#36890;&#36807;&#19982;GPT3.5&#32467;&#21512;&#20351;&#29992;&#65292;&#35813;&#26694;&#26550;&#22312;MH&#30417;&#27979;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20851;&#20110;Twitter&#25968;&#25454;&#30340;&#30740;&#31350;&#24050;&#32463;&#25552;&#20379;&#20102;&#23427;&#22312;&#24320;&#21457;&#34917;&#20805;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#35777;&#25454;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#30417;&#27979;&#20844;&#20849;&#20581;&#24247;&#65292;&#37325;&#28857;&#20851;&#27880;&#31934;&#31070;&#20581;&#24247;&#65288;MH&#65289;&#32467;&#26524;&#12290;&#25105;&#20204;&#20551;&#35774;&#24403;&#22320;&#21457;&#24067;&#30340;&#25512;&#25991;&#21487;&#20197;&#34920;&#26126;&#24403;&#22320;&#30340;&#31934;&#31070;&#20581;&#24247;&#32467;&#26524;&#65292;&#24182;&#25910;&#38598;&#20102;&#26469;&#33258;&#32654;&#22269;765&#20010;&#22320;&#21306;&#65288;&#20154;&#21475;&#26222;&#26597;&#20998;&#32452;&#65289;&#30340;&#25512;&#25991;&#12290;&#25105;&#20204;&#23558;&#27599;&#20010;&#22320;&#21306;&#30340;&#36825;&#20123;&#25512;&#25991;&#19982;&#30142;&#30149;&#25511;&#21046;&#20013;&#24515;&#65288;CDC&#65289;&#25253;&#21578;&#30340;&#30456;&#24212;MH&#32467;&#26524;&#37197;&#23545;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;LocalTweets&#12290;&#20511;&#21161;LocalTweets&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Twitter&#30340;MH&#30417;&#27979;&#31995;&#32479;&#30340;&#39318;&#20010;&#20154;&#21475;&#32423;&#35780;&#20272;&#20219;&#21153;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#26377;&#25928;&#30340;&#26041;&#27861;LocalHealth&#65292;&#29992;&#20110;&#26681;&#25454;LocalTweets&#39044;&#27979;MH&#32467;&#26524;&#12290;&#24403;&#19982;GPT3.5&#19968;&#36215;&#20351;&#29992;&#26102;&#65292;LocalHealth&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;F1&#20540;&#21644;&#20934;&#30830;&#29575;&#65292;&#20998;&#21035;&#36798;&#21040;0.7429&#21644;79.78\%&#65292;F1&#20540;&#25552;&#39640;&#20102;59\%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13452v1 Announce Type: cross  Abstract: Prior research on Twitter (now X) data has provided positive evidence of its utility in developing supplementary health surveillance systems. In this study, we present a new framework to surveil public health, focusing on mental health (MH) outcomes. We hypothesize that locally posted tweets are indicative of local MH outcomes and collect tweets posted from 765 neighborhoods (census block groups) in the USA. We pair these tweets from each neighborhood with the corresponding MH outcome reported by the Center for Disease Control (CDC) to create a benchmark dataset, LocalTweets. With LocalTweets, we present the first population-level evaluation task for Twitter-based MH surveillance systems. We then develop an efficient and effective method, LocalHealth, for predicting MH outcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the highest F1-score and accuracy of 0.7429 and 79.78\%, respectively, a 59\% improvement in F
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#20851;&#32852;&#20869;&#23384;&#27169;&#22359;&#65292;&#21487;&#20197;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#21363;&#21487;&#19982;&#20219;&#20309;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32806;&#21512;&#65292;&#35299;&#20915;&#20102;&#38271;&#36755;&#20837;&#24207;&#21015;&#22788;&#29702;&#38382;&#39064;&#65292;&#24182;&#22312;&#38271;&#19978;&#19979;&#25991;&#24314;&#27169;&#20013;&#26174;&#33879;&#38477;&#20302;&#22256;&#24785;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.13449</link><description>&lt;p&gt;
&#26397;&#30528;&#26080;&#38656;&#35757;&#32451;&#30340;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#35757;&#32451;&#33258;&#30001;&#21270;&#30340;&#20851;&#32852;&#20869;&#23384;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
CAMELoT: Towards Large Language Models with Training-Free Consolidated Associative Memory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13449
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#20851;&#32852;&#20869;&#23384;&#27169;&#22359;&#65292;&#21487;&#20197;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#21363;&#21487;&#19982;&#20219;&#20309;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32806;&#21512;&#65292;&#35299;&#20915;&#20102;&#38271;&#36755;&#20837;&#24207;&#21015;&#22788;&#29702;&#38382;&#39064;&#65292;&#24182;&#22312;&#38271;&#19978;&#19979;&#25991;&#24314;&#27169;&#20013;&#26174;&#33879;&#38477;&#20302;&#22256;&#24785;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#38271;&#36755;&#20837;&#24207;&#21015;&#26102;&#38754;&#20020;&#20869;&#23384;&#21644;&#36816;&#34892;&#26102;&#38388;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#12290;&#22686;&#24378;&#35760;&#24518;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#21463;&#38480;&#20110;&#26377;&#38480;&#30340;&#35760;&#24518;&#23481;&#37327;&#65292;&#24182;&#19988;&#38656;&#35201;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25165;&#33021;&#19982;&#26032;&#30340;LLM&#38598;&#25104;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20851;&#32852;&#20869;&#23384;&#27169;&#22359;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#39044;&#20808;&#35757;&#32451;&#65288;&#20923;&#32467;&#65289;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;LLM&#32806;&#21512;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#12290;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#20851;&#32852;&#20869;&#23384;&#27169;&#22359;&#23558;&#21333;&#20010;&#26631;&#35760;&#30340;&#34920;&#31034;&#21512;&#24182;&#21040;&#19968;&#20010;&#38750;&#21442;&#25968;&#20998;&#24067;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#36866;&#24403;&#24179;&#34913;&#20256;&#20837;&#25968;&#25454;&#30340;&#26032;&#39062;&#24615;&#21644;&#26368;&#36817;&#24615;&#36827;&#34892;&#21160;&#24577;&#31649;&#29702;&#12290;&#36890;&#36807;&#20174;&#36825;&#20010;&#21512;&#24182;&#30340;&#20851;&#32852;&#20869;&#23384;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#22522;&#26412;LLM&#21487;&#20197;&#22312;&#38271;&#19978;&#19979;&#25991;&#24314;&#27169;&#20013;&#26174;&#33879;&#20943;&#23569;&#65288;&#39640;&#36798;Arxiv&#30340;29.7&#65285;&#65289;&#19982;&#20854;&#20182;&#22522;&#32447;&#35780;&#20272;&#30456;&#27604;&#30340;&#22256;&#24785;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13449v1 Announce Type: new  Abstract: Large Language Models (LLMs) struggle to handle long input sequences due to high memory and runtime costs. Memory-augmented models have emerged as a promising solution to this problem, but current methods are hindered by limited memory capacity and require costly re-training to integrate with a new LLM. In this work, we introduce an associative memory module which can be coupled to any pre-trained (frozen) attention-based LLM without re-training, enabling it to handle arbitrarily long input sequences. Unlike previous methods, our associative memory module consolidates representations of individual tokens into a non-parametric distribution model, dynamically managed by properly balancing the novelty and recency of the incoming data. By retrieving information from this consolidated associative memory, the base LLM can achieve significant (up to 29.7% on Arxiv) perplexity reduction in long-context modeling compared to other baselines evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24613;&#35786;&#31185;&#20013;&#20943;&#23569;&#31561;&#24453;&#26102;&#38388;&#30340;&#35786;&#26029;&#36741;&#21161;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24110;&#21161;&#21307;&#29983;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#30340;&#35786;&#26029;&#65292;&#24182;&#24320;&#21457;&#20102;ED-Copilot&#31995;&#32479;&#26469;&#25512;&#33616;&#23454;&#39564;&#23460;&#26816;&#27979;&#24182;&#36827;&#34892;&#35786;&#26029;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.13448</link><description>&lt;p&gt;
ED-Copilot: &#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#35786;&#26029;&#36741;&#21161;&#20943;&#23569;&#24613;&#35786;&#31185;&#31561;&#24453;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
ED-Copilot: Reduce Emergency Department Wait Time with Language Model Diagnostic Assistance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24613;&#35786;&#31185;&#20013;&#20943;&#23569;&#31561;&#24453;&#26102;&#38388;&#30340;&#35786;&#26029;&#36741;&#21161;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24110;&#21161;&#21307;&#29983;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#30340;&#35786;&#26029;&#65292;&#24182;&#24320;&#21457;&#20102;ED-Copilot&#31995;&#32479;&#26469;&#25512;&#33616;&#23454;&#39564;&#23460;&#26816;&#27979;&#24182;&#36827;&#34892;&#35786;&#26029;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24613;&#35786;&#31185;&#65288;ED&#65289;&#20013;&#65292;&#24739;&#32773;&#22312;&#35786;&#26029;&#21069;&#38656;&#35201;&#36827;&#34892;&#20998;&#35786;&#21644;&#22810;&#31181;&#23454;&#39564;&#23460;&#26816;&#27979;&#12290;&#36825;&#20010;&#36807;&#31243;&#32791;&#26102;&#65292;&#23548;&#33268;&#24613;&#35786;&#31185;&#25317;&#25380;&#65292;&#26174;&#33879;&#24433;&#21709;&#24739;&#32773;&#27515;&#20129;&#29575;&#12289;&#21307;&#30103;&#38169;&#35823;&#12289;&#20154;&#21592;&#26543;&#31469;&#31561;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#65288;&#26102;&#38388;&#65289;&#25104;&#26412;&#26377;&#25928;&#30340;&#35786;&#26029;&#36741;&#21161;&#26041;&#27861;&#65292;&#25506;&#32034;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#21327;&#21161;&#24613;&#35786;&#31185;&#20020;&#24202;&#21307;&#29983;&#36827;&#34892;&#39640;&#25928;&#20934;&#30830;&#35786;&#26029;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#20351;&#29992;&#20844;&#24320;&#21487;&#33719;&#24471;&#30340;&#24739;&#32773;&#25968;&#25454;&#65292;&#25105;&#20204;&#19982;&#24613;&#35786;&#31185;&#20020;&#24202;&#21307;&#29983;&#21512;&#20316;&#31574;&#21010;&#20102;MIMIC-ED-Assist&#65292;&#36825;&#26159;&#19968;&#20010;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#24314;&#35758;&#26368;&#22823;&#31243;&#24230;&#20943;&#23569;&#24613;&#35786;&#31561;&#24453;&#26102;&#38388;&#30340;&#23454;&#39564;&#23460;&#26816;&#27979;&#65292;&#24182;&#22312;&#27491;&#30830;&#39044;&#27979;&#35832;&#22914;&#27515;&#20129;&#20043;&#31867;&#20851;&#38190;&#32467;&#26524;&#26041;&#38754;&#30340;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;ED-Copilot&#65292;&#23427;&#20381;&#27425;&#24314;&#35758;&#24739;&#32773;&#29305;&#23450;&#30340;&#23454;&#39564;&#23460;&#26816;&#27979;&#24182;&#36827;&#34892;&#35786;&#26029;&#39044;&#27979;&#12290;ED-Copilot&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#23545;&#24739;&#32773;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#24182;&#36827;&#34892;&#22686;&#24378;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13448v1 Announce Type: cross  Abstract: In the emergency department (ED), patients undergo triage and multiple laboratory tests before diagnosis. This process is time-consuming, and causes ED crowding which significantly impacts patient mortality, medical errors, staff burnout, etc. This work proposes (time) cost-effective diagnostic assistance that explores the potential of artificial intelligence (AI) systems in assisting ED clinicians to make time-efficient and accurate diagnoses. Using publicly available patient data, we collaborate with ED clinicians to curate MIMIC-ED-Assist, a benchmark that measures the ability of AI systems in suggesting laboratory tests that minimize ED wait times, while correctly predicting critical outcomes such as death. We develop ED-Copilot which sequentially suggests patient-specific laboratory tests and makes diagnostic predictions. ED-Copilot uses a pre-trained bio-medical language model to encode patient information and reinforcement learn
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#20026;&#33258;&#21160;&#21270;&#25968;&#25454;&#26631;&#27880;&#25552;&#20379;&#26426;&#36935;&#65292;&#35813;&#35843;&#26597;&#29420;&#29305;&#20851;&#27880;LLM&#22312;&#25968;&#25454;&#26631;&#27880;&#20013;&#30340;&#25928;&#29992;&#65292;&#36129;&#29486;&#20027;&#35201;&#38598;&#20013;&#22312;LLM-Based&#25968;&#25454;&#26631;&#27880;&#12289;&#35780;&#20272;LLM&#29983;&#25104;&#30340;&#26631;&#27880;&#20197;&#21450;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#26631;&#27880;&#23398;&#20064;&#31561;&#19977;&#20010;&#26680;&#24515;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.13446</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25968;&#25454;&#26631;&#27880;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Data Annotation: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13446
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#20026;&#33258;&#21160;&#21270;&#25968;&#25454;&#26631;&#27880;&#25552;&#20379;&#26426;&#36935;&#65292;&#35813;&#35843;&#26597;&#29420;&#29305;&#20851;&#27880;LLM&#22312;&#25968;&#25454;&#26631;&#27880;&#20013;&#30340;&#25928;&#29992;&#65292;&#36129;&#29486;&#20027;&#35201;&#38598;&#20013;&#22312;LLM-Based&#25968;&#25454;&#26631;&#27880;&#12289;&#35780;&#20272;LLM&#29983;&#25104;&#30340;&#26631;&#27880;&#20197;&#21450;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#26631;&#27880;&#23398;&#20064;&#31561;&#19977;&#20010;&#26680;&#24515;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26631;&#27880;&#26159;&#23558;&#21407;&#22987;&#25968;&#25454;&#26631;&#35760;&#25110;&#25171;&#26631;&#31614;&#19982;&#30456;&#20851;&#20449;&#24687;&#65292;&#23545;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36807;&#31243;&#21171;&#21160;&#23494;&#38598;&#19988;&#26114;&#36149;&#12290;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#20363;&#22914;GPT-4&#65292;&#20026;&#38761;&#26032;&#21644;&#33258;&#21160;&#21270;&#25968;&#25454;&#26631;&#27880;&#30340;&#22797;&#26434;&#36807;&#31243;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#36935;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#35843;&#26597;&#24050;&#32463;&#24191;&#27867;&#28085;&#30422;&#20102;LLM&#30340;&#26550;&#26500;&#12289;&#35757;&#32451;&#21644;&#19968;&#33324;&#24212;&#29992;&#65292;&#20294;&#26412;&#25991;&#29420;&#29305;&#22320;&#20851;&#27880;&#23427;&#20204;&#22312;&#25968;&#25454;&#26631;&#27880;&#20013;&#30340;&#20855;&#20307;&#25928;&#29992;&#12290;&#35813;&#35843;&#26597;&#23545;LLM-Based&#25968;&#25454;&#26631;&#27880;&#12289;&#35780;&#20272;LLM&#29983;&#25104;&#30340;&#26631;&#27880;&#20197;&#21450;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#26631;&#27880;&#23398;&#20064;&#36825;&#19977;&#20010;&#26680;&#24515;&#26041;&#38754;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#21253;&#25324;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#36827;&#34892;&#25968;&#25454;&#26631;&#27880;&#30340;&#26041;&#27861;&#23398;&#28145;&#24230;&#20998;&#31867;&#27861;&#65292;&#19968;&#20010;&#23545;&#25972;&#21512;LLM&#29983;&#25104;&#30340;&#26631;&#27880;&#30340;&#27169;&#22411;&#30340;&#23398;&#20064;&#31574;&#30053;&#36827;&#34892;&#20840;&#38754;&#23457;&#26597;&#65292;&#20197;&#21450;&#23545;&#20854;&#36827;&#34892;&#35814;&#32454;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13446v1 Announce Type: new  Abstract: Data annotation is the labeling or tagging of raw data with relevant information, essential for improving the efficacy of machine learning models. The process, however, is labor-intensive and expensive. The emergence of advanced Large Language Models (LLMs), exemplified by GPT-4, presents an unprecedented opportunity to revolutionize and automate the intricate process of data annotation. While existing surveys have extensively covered LLM architecture, training, and general applications, this paper uniquely focuses on their specific utility for data annotation. This survey contributes to three core aspects: LLM-Based Data Annotation, Assessing LLM-generated Annotations, and Learning with LLM-generated annotations. Furthermore, the paper includes an in-depth taxonomy of methodologies employing LLMs for data annotation, a comprehensive review of learning strategies for models incorporating LLM-generated annotations, and a detailed discussi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#35821;&#38899;&#35299;&#26512;&#22120;&#35780;&#20272;&#38382;&#39064;&#21551;&#21457;&#30340;&#32467;&#26500;&#21270;&#21477;&#27861;&#20998;&#26512;&#26641;&#30456;&#20284;&#24615;&#24230;&#37327;&#25351;&#26631;STRUCT-IOU&#65292;&#26377;&#25928;&#22320;&#27604;&#36739;&#20102;&#21475;&#35821;&#35789;&#36793;&#30028;&#19978;&#30340;&#32452;&#22359;&#20998;&#26512;&#26641;&#19982;&#20070;&#38754;&#35789;&#19978;&#22522;&#20934;&#35299;&#26512;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#25991;&#26412;&#32452;&#22359;&#20998;&#26512;&#35780;&#20272;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13433</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#26641;&#23545;&#40784;&#29992;&#20110;&#65288;&#35821;&#38899;&#65289;&#32452;&#22359;&#20998;&#26512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Structured Tree Alignment for Evaluation of (Speech) Constituency Parsing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13433
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#35821;&#38899;&#35299;&#26512;&#22120;&#35780;&#20272;&#38382;&#39064;&#21551;&#21457;&#30340;&#32467;&#26500;&#21270;&#21477;&#27861;&#20998;&#26512;&#26641;&#30456;&#20284;&#24615;&#24230;&#37327;&#25351;&#26631;STRUCT-IOU&#65292;&#26377;&#25928;&#22320;&#27604;&#36739;&#20102;&#21475;&#35821;&#35789;&#36793;&#30028;&#19978;&#30340;&#32452;&#22359;&#20998;&#26512;&#26641;&#19982;&#20070;&#38754;&#35789;&#19978;&#22522;&#20934;&#35299;&#26512;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#25991;&#26412;&#32452;&#22359;&#20998;&#26512;&#35780;&#20272;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#32467;&#26500;&#21270;&#24179;&#22343;&#20132;&#38598;-&#32852;&#30431;&#27604;&#65288;STRUCT-IOU&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21477;&#27861;&#20998;&#26512;&#26641;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#25351;&#26631;&#65292;&#21463;&#21040;&#20102;&#35780;&#20272;&#35821;&#38899;&#35299;&#26512;&#22120;&#38382;&#39064;&#30340;&#21551;&#21457;&#12290;STRUCT-IOU&#20351;&#24471;&#21487;&#20197;&#27604;&#36739;&#22312;&#33258;&#21160;&#35782;&#21035;&#30340;&#21475;&#35821;&#35789;&#36793;&#30028;&#19978;&#30340;&#32452;&#22359;&#20998;&#26512;&#26641;&#19982;&#22522;&#20934;&#35299;&#26512;&#65288;&#22312;&#20070;&#38754;&#35789;&#19978;&#65289;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#35745;&#31639;&#36825;&#20010;&#25351;&#26631;&#65292;&#25105;&#20204;&#36890;&#36807;&#24378;&#21046;&#23545;&#40784;&#23558;&#22522;&#20934;&#35299;&#26512;&#26641;&#25237;&#24433;&#21040;&#35821;&#38899;&#39046;&#22495;&#65292;&#23558;&#25237;&#24433;&#30340;&#22522;&#20934;&#25104;&#20998;&#19982;&#39044;&#27979;&#30340;&#25104;&#20998;&#22312;&#19968;&#23450;&#30340;&#32467;&#26500;&#32422;&#26463;&#19979;&#23545;&#40784;&#65292;&#28982;&#21518;&#35745;&#31639;&#25152;&#26377;&#23545;&#40784;&#25104;&#20998;&#23545;&#20043;&#38388;&#30340;&#24179;&#22343;IOU&#20998;&#25968;&#12290;STRUCT-IOU&#32771;&#34385;&#20102;&#35789;&#36793;&#30028;&#65292;&#24182;&#20811;&#26381;&#20102;&#39044;&#27979;&#30340;&#35789;&#21644;&#22522;&#20934;&#20107;&#23454;&#21487;&#33021;&#27809;&#26377;&#23436;&#32654;&#19968;&#19968;&#23545;&#24212;&#30340;&#25361;&#25112;&#12290;&#25193;&#23637;&#21040;&#25991;&#26412;&#32452;&#22359;&#20998;&#26512;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;STRUCT-IOU&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#23545;&#21477;&#27861;&#21512;&#29702;&#35299;&#26512;&#30340;&#23481;&#24525;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13433v1 Announce Type: new  Abstract: We present the structured average intersection-over-union ratio (STRUCT-IOU), a similarity metric between constituency parse trees motivated by the problem of evaluating speech parsers. STRUCT-IOU enables comparison between a constituency parse tree (over automatically recognized spoken word boundaries) with the ground-truth parse (over written words). To compute the metric, we project the ground-truth parse tree to the speech domain by forced alignment, align the projected ground-truth constituents with the predicted ones under certain structured constraints, and calculate the average IOU score across all aligned constituent pairs. STRUCT-IOU takes word boundaries into account and overcomes the challenge that the predicted words and ground truth may not have perfect one-to-one correspondence. Extending to the evaluation of text constituency parsing, we demonstrate that STRUCT-IOU shows higher tolerance to syntactically plausible parses 
&lt;/p&gt;</description></item><item><title>DrBenchmark&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#22522;&#20934;&#65292;&#26088;&#22312;&#24357;&#34917;&#23545;&#26368;&#26032;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#35780;&#20272;&#30340;&#19981;&#36275;&#65292;&#24182;&#32771;&#34385;&#21040;&#27861;&#35821;&#30340;&#29420;&#29305;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13432</link><description>&lt;p&gt;
DrBenchmark: &#19968;&#20010;&#38024;&#23545;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
DrBenchmark: A Large Language Understanding Evaluation Benchmark for French Biomedical Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13432
&lt;/p&gt;
&lt;p&gt;
DrBenchmark&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#22522;&#20934;&#65292;&#26088;&#22312;&#24357;&#34917;&#23545;&#26368;&#26032;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#35780;&#20272;&#30340;&#19981;&#36275;&#65292;&#24182;&#32771;&#34385;&#21040;&#27861;&#35821;&#30340;&#29420;&#29305;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#65292;&#38543;&#30528;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#23454;&#36136;&#24615;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#35780;&#20272;&#21327;&#35758;&#30340;&#21464;&#21270;&#65292;&#27604;&#36739;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19968;&#20010;&#20844;&#24179;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23558;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#32858;&#21512;&#21040;&#19968;&#20010;&#22522;&#20934;&#20013;&#65292;&#20801;&#35768;&#20174;&#21508;&#31181;&#35282;&#24230;&#35780;&#20272;PLMs&#30340;&#20869;&#22312;&#21697;&#36136;&#12290;&#23613;&#31649;&#36825;&#19968;&#20513;&#35758;&#20173;&#28982;&#23616;&#38480;&#20110;&#23569;&#25968;&#35821;&#35328;&#65292;&#29305;&#21035;&#26159;&#33521;&#35821;&#21644;&#20013;&#25991;&#65292;&#20294;&#24050;&#32463;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#23637;&#24320;&#12290;&#36825;&#19968;&#38480;&#21046;&#38459;&#30861;&#20102;&#23545;&#26368;&#26032;&#30340;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#30340;&#35780;&#20215;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#20040;&#22312;&#23569;&#37327;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32780;&#19988;&#20351;&#29992;&#30340;&#21327;&#35758;&#19981;&#22815;&#26631;&#20934;&#21270;&#65292;&#35201;&#20040;&#20351;&#29992;&#19968;&#33324;&#30340;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#30740;&#31350;&#24046;&#36317;&#65292;&#24182;&#32771;&#34385;&#21040;&#27861;&#35821;&#30340;&#29420;&#29305;&#25935;&#24863;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39318;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13432v1 Announce Type: cross  Abstract: The biomedical domain has sparked a significant interest in the field of Natural Language Processing (NLP), which has seen substantial advancements with pre-trained language models (PLMs). However, comparing these models has proven challenging due to variations in evaluation protocols across different models. A fair solution is to aggregate diverse downstream tasks into a benchmark, allowing for the assessment of intrinsic PLMs qualities from various perspectives. Although still limited to few languages, this initiative has been undertaken in the biomedical field, notably English and Chinese. This limitation hampers the evaluation of the latest French biomedical models, as they are either assessed on a minimal number of tasks with non-standardized protocols or evaluated using general downstream tasks. To bridge this research gap and account for the unique sensitivities of French, we present the first-ever publicly available French biom
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30340;LLM&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20016;&#23500;&#30340;&#24341;&#25991;&#25991;&#26412;&#65292;&#24182;&#19968;&#27425;&#29983;&#25104;&#22810;&#20010;&#24341;&#25991;&#20197;&#25429;&#25417;&#30740;&#31350;&#35770;&#25991;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.13426</link><description>&lt;p&gt;
&#35299;&#37322;&#30740;&#31350;&#35770;&#25991;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Explaining Relationships Among Research Papers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13426
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30340;LLM&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20016;&#23500;&#30340;&#24341;&#25991;&#25991;&#26412;&#65292;&#24182;&#19968;&#27425;&#29983;&#25104;&#22810;&#20010;&#24341;&#25991;&#20197;&#25429;&#25417;&#30740;&#31350;&#35770;&#25991;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30740;&#31350;&#20986;&#29256;&#29289;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#21363;&#20351;&#20351;&#29992;&#27599;&#26085;&#25552;&#35201;&#24037;&#20855;&#65292;&#36319;&#19978;&#25152;&#26377;&#26368;&#26032;&#30456;&#20851;&#35770;&#25991;&#20063;&#26159;&#38750;&#24120;&#32791;&#26102;&#30340;&#12290;&#38656;&#35201;&#33258;&#21160;&#29983;&#25104;&#30340;&#31616;&#30701;&#12289;&#23450;&#21046;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#20915;&#23450;&#35201;&#38405;&#35835;&#20160;&#20040;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30340;LLM&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#26356;&#20016;&#23500;&#30340;&#24341;&#25991;&#25991;&#26412;&#65292;&#21516;&#26102;&#19968;&#27425;&#29983;&#25104;&#22810;&#20010;&#24341;&#25991;&#20197;&#25429;&#25417;&#30740;&#31350;&#35770;&#25991;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13426v1 Announce Type: new  Abstract: Due to the rapid pace of research publications, keeping up to date with all the latest related papers is very time-consuming, even with daily feed tools. There is a need for automatically generated, short, customized literature reviews of sets of papers to help researchers decide what to read. While several works in the last decade have addressed the task of explaining a single research paper, usually in the context of another paper citing it, the relationship among multiple papers has been ignored; prior works have focused on generating a single citation sentence in isolation, without addressing the expository and transition sentences needed to connect multiple papers in a coherent story. In this work, we explore a feature-based, LLM-prompting approach to generate richer citation texts, as well as generating multiple citations at once to capture the complex relationships among research papers. We perform an expert evaluation to investig
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#26500;&#24341;&#23548;&#25552;&#31034;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#25991;&#26412;&#30340;&#22270;&#32467;&#26500;&#65292;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#25512;&#29702;&#65292;&#20197;&#35299;&#20915;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#21644;&#22810;&#26679;&#24615;&#24102;&#26469;&#30340;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2402.13415</link><description>&lt;p&gt;
&#32467;&#26500;&#24341;&#23548;&#25552;&#31034;: &#36890;&#36807;&#25506;&#32034;&#25991;&#26412;&#30340;&#22270;&#32467;&#26500;&#65292;&#22312;&#22810;&#27493;&#25512;&#29702;&#20013;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#26500;&#24341;&#23548;&#25552;&#31034;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#25991;&#26412;&#30340;&#22270;&#32467;&#26500;&#65292;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#25512;&#29702;&#65292;&#20197;&#35299;&#20915;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#21644;&#22810;&#26679;&#24615;&#24102;&#26469;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25797;&#38271;&#22788;&#29702;&#30452;&#25509;&#25512;&#29702;&#20219;&#21153;&#65292;&#20294;&#22312;&#38754;&#23545;&#26356;&#22797;&#26434;&#30340;&#22810;&#27493;&#25512;&#29702;&#26102;&#32463;&#24120;&#20250;&#36935;&#21040;&#22256;&#38590;&#65292;&#21407;&#22240;&#26377;&#22810;&#31181;&#12290;&#39318;&#20808;&#65292;&#33258;&#28982;&#35821;&#35328;&#20013;&#32463;&#24120;&#21253;&#21547;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#20351;&#24471;&#22312;&#36739;&#38271;&#30340;&#33539;&#22260;&#20869;&#20445;&#25345;&#28165;&#26224;&#30340;&#25512;&#29702;&#38142;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20854;&#27425;&#65292;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#20016;&#23500;&#24847;&#21619;&#30528;&#30456;&#21516;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#21487;&#20197;&#29992;&#19981;&#21516;&#30340;&#26415;&#35821;&#21644;&#32467;&#26500;&#34920;&#36798;&#65292;&#20351;&#24471;&#35782;&#21035;&#21644;&#24314;&#31435;&#22810;&#20010;&#20449;&#24687;&#29255;&#27573;&#20043;&#38388;&#30340;&#36830;&#25509;&#20219;&#21153;&#21464;&#24471;&#22797;&#26434;&#12290;&#22270;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#34920;&#31034;&#23500;&#21547;&#20851;&#31995;&#20449;&#24687;&#30340;&#25968;&#25454;&#65292;&#24182;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#21033;&#29992;&#22270;&#30340;&#28508;&#21147;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#32467;&#26500;&#24341;&#23548;&#25552;&#31034;&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#19977;&#38454;&#27573;&#20219;&#21153;&#26080;&#20851;&#25552;&#31034;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#22810;&#27493;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13415v1 Announce Type: new  Abstract: Although Large Language Models (LLMs) excel at addressing straightforward reasoning tasks, they frequently struggle with difficulties when confronted by more complex multi-step reasoning due to a range of factors. Firstly, natural language often encompasses complex relationships among entities, making it challenging to maintain a clear reasoning chain over longer spans. Secondly, the abundance of linguistic diversity means that the same entities and relationships can be expressed using different terminologies and structures, complicating the task of identifying and establishing connections between multiple pieces of information. Graphs provide an effective solution to represent data rich in relational information and capture long-term dependencies among entities. To harness the potential of graphs, our paper introduces Structure Guided Prompt, an innovative three-stage task-agnostic prompting framework designed to improve the multi-step 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550; LlmCorr&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#20010;LLM&#21487;&#20197;&#20316;&#20026;&#20107;&#21518;&#26657;&#27491;&#22120;&#65292;&#20026;&#20219;&#24847;ML&#27169;&#22411;&#30340;&#39044;&#27979;&#25552;&#20986;&#20462;&#27491;&#12290;</title><link>https://arxiv.org/abs/2402.13414</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20316;&#20107;&#21518;&#26657;&#27491;&#22120;
&lt;/p&gt;
&lt;p&gt;
Harnessing Large Language Models as Post-hoc Correctors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13414
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550; LlmCorr&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#20010;LLM&#21487;&#20197;&#20316;&#20026;&#20107;&#21518;&#26657;&#27491;&#22120;&#65292;&#20026;&#20219;&#24847;ML&#27169;&#22411;&#30340;&#39044;&#27979;&#25552;&#20986;&#20462;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#35268;&#27169;&#22686;&#38271;&#24182;&#38656;&#27714;&#26356;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#19982;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#21644;&#24494;&#35843;&#30456;&#20851;&#30340;&#36153;&#29992;&#27491;&#22312;&#36805;&#36895;&#22686;&#21152;&#12290;&#21463;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#30340;&#20196;&#20154;&#30633;&#30446;&#25104;&#23601;&#21551;&#21457;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;LLMs&#33021;&#21542;&#20197;&#26497;&#20302;&#25104;&#26412;&#26377;&#25928;&#22320;&#25913;&#21892;ML&#30340;&#24615;&#33021;&#65311;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550; LlmCorr&#65292;&#19968;&#20010;LLM&#21487;&#20197;&#20316;&#20026;&#20107;&#21518;&#26657;&#27491;&#22120;&#65292;&#20026;&#20219;&#24847;ML&#27169;&#22411;&#30340;&#39044;&#27979;&#25552;&#20986;&#20462;&#27491;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#36890;&#36807;&#25972;&#21512;&#25968;&#25454;&#38598;&#30340;&#26631;&#31614;&#20449;&#24687;&#21644;ML&#27169;&#22411;&#23545;&#39564;&#35777;&#38598;&#30340;&#39044;&#27979;&#26469;&#24418;&#25104;&#19968;&#20010;&#19978;&#19979;&#25991;&#30693;&#35782;&#25968;&#25454;&#24211;&#12290;&#21033;&#29992;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#35201;&#27714;LLM&#24635;&#32467;ML&#27169;&#22411;&#29359;&#38169;&#35823;&#30340;&#23454;&#20363;&#20197;&#21450;&#20027;&#35201;&#39044;&#27979;&#19982;&#30495;&#23454;&#26631;&#31614;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#38543;&#21518;&#65292;LLM&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13414v1 Announce Type: cross  Abstract: As Machine Learning (ML) models grow in size and demand higher-quality training data, the expenses associated with re-training and fine-tuning these models are escalating rapidly. Inspired by recent impressive achievements of Large Language Models (LLMs) in different fields, this paper delves into the question: can LLMs efficiently improve an ML's performance at a minimal cost? We show that, through our proposed training-free framework LlmCorr, an LLM can work as a post-hoc corrector to propose corrections for the predictions of an arbitrary ML model. In particular, we form a contextual knowledge database by incorporating the dataset's label information and the ML model's predictions on the validation dataset. Leveraging the in-context learning capability of LLMs, we ask the LLM to summarise the instances in which the ML model makes mistakes and the correlation between primary predictions and true labels. Following this, the LLM can tr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32479;&#19968;&#30340;&#22522;&#20110;&#20998;&#31867;&#23398;&#25351;&#23548;&#30340;&#25351;&#23548;&#35843;&#25972;&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29616;&#26377;&#20998;&#31867;&#23398;&#36827;&#34892;&#23454;&#20307;&#20851;&#31995;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#23454;&#20307;&#38598;&#25193;&#23637;&#12289;&#20998;&#31867;&#23398;&#25193;&#23637;&#21644;&#31181;&#23376;&#24341;&#23548;&#20998;&#31867;&#23398;&#26500;&#24314;&#19977;&#20010;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.13405</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20110;&#20998;&#31867;&#23398;&#25351;&#23548;&#30340;&#23454;&#20307;&#38598;&#25193;&#23637;&#21644;&#20998;&#31867;&#23398;&#25193;&#23637;&#30340;&#25351;&#23548;&#35843;&#25972;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set Expansion and Taxonomy Expansion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13405
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32479;&#19968;&#30340;&#22522;&#20110;&#20998;&#31867;&#23398;&#25351;&#23548;&#30340;&#25351;&#23548;&#35843;&#25972;&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29616;&#26377;&#20998;&#31867;&#23398;&#36827;&#34892;&#23454;&#20307;&#20851;&#31995;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#23454;&#20307;&#38598;&#25193;&#23637;&#12289;&#20998;&#31867;&#23398;&#25193;&#23637;&#21644;&#31181;&#23376;&#24341;&#23548;&#20998;&#31867;&#23398;&#26500;&#24314;&#19977;&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#38598;&#25193;&#23637;&#12289;&#20998;&#31867;&#23398;&#25193;&#23637;&#21644;&#31181;&#23376;&#24341;&#23548;&#20998;&#31867;&#23398;&#26500;&#24314;&#26159;&#19977;&#20010;&#20195;&#34920;&#24615;&#20219;&#21153;&#65292;&#21487;&#20197;&#29992;&#26469;&#33258;&#21160;&#21521;&#29616;&#26377;&#20998;&#31867;&#23398;&#22635;&#20805;&#26032;&#23454;&#20307;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#24322;&#36136;&#25216;&#26415;&#20998;&#21035;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#65292;&#32570;&#20047;&#32479;&#19968;&#30340;&#35270;&#35282;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20998;&#31867;&#23398;&#32467;&#26500;&#30340;&#35270;&#35282;&#30830;&#35748;&#20102;&#36825;&#20123;&#20219;&#21153;&#25152;&#38656;&#30340;&#20849;&#21516;&#20851;&#38190;&#25216;&#33021;&#8212;&#8212;&#25214;&#21040;&#8220;&#20804;&#24351;&#8221;&#21644;&#25214;&#21040;&#8220;&#29238;&#27597;&#8221;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20110;&#20998;&#31867;&#23398;&#25351;&#23548;&#30340;&#25351;&#23548;&#35843;&#25972;&#26694;&#26550;&#26469;&#20849;&#21516;&#35299;&#20915;&#36825;&#19977;&#20010;&#20219;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#20998;&#31867;&#23398;&#20316;&#20026;&#20016;&#23500;&#30340;&#23454;&#20307;&#20851;&#31995;&#28304;&#65292;&#25105;&#20204;&#21033;&#29992;&#25351;&#23548;&#35843;&#25972;&#26469;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29983;&#25104;&#29238;&#27597;&#21644;&#20804;&#24351;&#23454;&#20307;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;TaxoInstruct&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13405v1 Announce Type: new  Abstract: Entity Set Expansion, Taxonomy Expansion, and Seed-Guided Taxonomy Construction are three representative tasks that can be used to automatically populate an existing taxonomy with new entities. However, previous approaches often address these tasks separately with heterogeneous techniques, lacking a unified perspective. To tackle this issue, in this paper, we identify the common key skills needed for these tasks from the view of taxonomy structures -- finding 'siblings' and finding 'parents' -- and propose a unified taxonomy-guided instruction tuning framework to jointly solve the three tasks. To be specific, by leveraging the existing taxonomy as a rich source of entity relationships, we utilize instruction tuning to fine-tune a large language model to generate parent and sibling entities. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of TaxoInstruct, which outperforms task-specific baselines across 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;DAUS&#65292;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#39046;&#22495;&#24863;&#30693;&#29992;&#25143;&#27169;&#25311;&#22120;&#65292;&#36890;&#36807;&#22312;&#30495;&#23454;&#23545;&#35805;&#31034;&#20363;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#29992;&#25143;&#30446;&#26631;&#23454;&#29616;&#65292;&#24182;&#26377;&#25928;&#20943;&#36731;&#27169;&#25311;&#22120;&#21709;&#24212;&#20013;&#30340;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2402.13374</link><description>&lt;p&gt;
&#21487;&#38752;&#30340;&#22522;&#20110;LLM&#30340;&#38754;&#21521;&#20219;&#21153;&#23545;&#35805;&#31995;&#32479;&#29992;&#25143;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
Reliable LLM-based User Simulator for Task-Oriented Dialogue Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13374
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;DAUS&#65292;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#39046;&#22495;&#24863;&#30693;&#29992;&#25143;&#27169;&#25311;&#22120;&#65292;&#36890;&#36807;&#22312;&#30495;&#23454;&#23545;&#35805;&#31034;&#20363;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#29992;&#25143;&#30446;&#26631;&#23454;&#29616;&#65292;&#24182;&#26377;&#25928;&#20943;&#36731;&#27169;&#25311;&#22120;&#21709;&#24212;&#20013;&#30340;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#35805;&#31995;&#32479;&#39046;&#22495;&#65292;&#29992;&#25143;&#27169;&#25311;&#25216;&#26415;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#39072;&#35206;&#24615;&#30340;&#21019;&#26032;&#65292;&#37325;&#26032;&#23450;&#20041;&#20102;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#65288;TOD&#65289;&#31995;&#32479;&#30340;&#35780;&#20272;&#21644;&#22686;&#24378;&#12290;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#22797;&#21046;&#30495;&#23454;&#29992;&#25143;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#65292;&#23454;&#29616;&#20102;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#12289;&#38169;&#35823;&#26816;&#27979;&#21644;&#40065;&#26834;&#35780;&#20272;&#31561;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#20005;&#26684;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#25110;&#24050;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DAUS&#65292;&#19968;&#20010;&#39046;&#22495;&#24863;&#30693;&#29992;&#25143;&#27169;&#25311;&#22120;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#22312;&#30495;&#23454;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#31034;&#20363;&#19978;&#23545;DAUS&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#22312;&#20004;&#20010;&#30456;&#20851;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#29992;&#25143;&#30446;&#26631;&#23454;&#29616;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24494;&#35843;&#22686;&#24378;&#20102;&#27169;&#25311;&#22120;&#19982;&#29992;&#25143;&#30446;&#26631;&#30340;&#19968;&#33268;&#24615;&#65292;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#24187;&#35273;&#8212;&#8212;&#27169;&#25311;&#22120;&#21709;&#24212;&#20013;&#19968;&#33268;&#24615;&#30340;&#20027;&#35201;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13374v1 Announce Type: new  Abstract: In the realm of dialogue systems, user simulation techniques have emerged as a game-changer, redefining the evaluation and enhancement of task-oriented dialogue (TOD) systems. These methods are crucial for replicating real user interactions, enabling applications like synthetic data augmentation, error detection, and robust evaluation. However, existing approaches often rely on rigid rule-based methods or on annotated data. This paper introduces DAUS, a Domain-Aware User Simulator. Leveraging large language models, we fine-tune DAUS on real examples of task-oriented dialogues. Results on two relevant benchmarks showcase significant improvements in terms of user goal fulfillment. Notably, we have observed that fine-tuning enhances the simulator's coherence with user goals, effectively mitigating hallucinations -- a major source of inconsistencies in simulator responses.
&lt;/p&gt;</description></item><item><title>EvoGrad&#26159;&#19968;&#20010;&#20197;&#20154;&#31867;&#23545;&#25163;&#20026;&#29305;&#28857;&#30340;&#29992;&#20110;&#35299;&#20915;Winograd Schema&#25361;&#25112;&#30340;&#21160;&#24577;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#22312;&#29615;&#20013;&#26041;&#27861;&#21019;&#24314;&#21160;&#24577;&#25968;&#25454;&#38598;&#65292;&#25299;&#23637;&#20219;&#21153;&#23454;&#20363;&#24182;&#24341;&#20837;&#38169;&#35823;&#28145;&#24230;&#24230;&#37327;&#26631;&#20934;&#65292;&#25552;&#20986;&#26032;&#30340;&#22810;&#26679;&#21270;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#22522;&#20934;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#31867;&#20219;&#21153;&#19978;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.13372</link><description>&lt;p&gt;
EvoGrad&#65306;&#20197;&#20154;&#31867;&#23545;&#25163;&#20026;&#29305;&#28857;&#30340;Winograd Schema&#25361;&#25112;&#30340;&#21160;&#24577;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EvoGrad: A Dynamic Take on the Winograd Schema Challenge with Human Adversaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13372
&lt;/p&gt;
&lt;p&gt;
EvoGrad&#26159;&#19968;&#20010;&#20197;&#20154;&#31867;&#23545;&#25163;&#20026;&#29305;&#28857;&#30340;&#29992;&#20110;&#35299;&#20915;Winograd Schema&#25361;&#25112;&#30340;&#21160;&#24577;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#22312;&#29615;&#20013;&#26041;&#27861;&#21019;&#24314;&#21160;&#24577;&#25968;&#25454;&#38598;&#65292;&#25299;&#23637;&#20219;&#21153;&#23454;&#20363;&#24182;&#24341;&#20837;&#38169;&#35823;&#28145;&#24230;&#24230;&#37327;&#26631;&#20934;&#65292;&#25552;&#20986;&#26032;&#30340;&#22810;&#26679;&#21270;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#22522;&#20934;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#31867;&#20219;&#21153;&#19978;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;Winograd Schema Challenge&#65288;WSC&#65289;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#35813;&#20219;&#21153;&#36890;&#36807;&#20195;&#35789;&#28040;&#27495;&#20041;&#27979;&#35797;&#24120;&#35782;&#25512;&#29702;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#21253;&#21547;&#36731;&#24494;&#20462;&#25913;&#25110;&#25913;&#20889;&#30340;&#23454;&#20363;&#24863;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;EvoGrad&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#24179;&#21488;&#65292;&#21033;&#29992;&#20154;&#22312;&#29615;&#20013;&#30340;&#26041;&#27861;&#26469;&#21019;&#24314;&#19968;&#20010;&#36866;&#29992;&#20110;&#36825;&#31181;&#20462;&#25913;&#21518;WSC&#23454;&#20363;&#30340;&#21160;&#24577;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;ChatGPT&#30340;&#21151;&#33021;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20219;&#21153;&#23454;&#20363;&#20174;182&#25193;&#23637;&#21040;3,691&#20010;&#65292;&#20026;&#22810;&#26679;&#21270;&#30340;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#35774;&#23450;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38169;&#35823;&#28145;&#24230;&#24230;&#37327;&#26631;&#20934;&#65292;&#35780;&#20272;&#27169;&#22411;&#22312;&#21160;&#24577;&#20219;&#21153;&#20013;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;EvoGrad&#25152;&#25552;&#20986;&#30340;&#25361;&#25112;&#65306;&#21363;&#20351;&#24615;&#33021;&#26368;&#20339;&#30340;LLM&#65292;GPT-3.5&#65292;&#22312;&#24179;&#22343;&#38169;&#35823;&#28145;&#24230;&#20026;7.2&#30340;&#24773;&#20917;&#19979;&#20165;&#36798;&#21040;65.0%&#30340;&#20934;&#30830;&#29575;&#65292;&#19982;&#20154;&#31867;92.8%&#30340;&#20934;&#30830;&#29575;&#24418;&#25104;&#20102;&#40092;&#26126;&#23545;&#27604;&#65292;&#20154;&#31867;&#20934;&#30830;&#29575;&#27809;&#26377;&#24178;&#25200;&#24615;&#38169;&#35823;&#12290;&#36825;&#31361;&#26174;&#20102;&#25345;&#32493;&#23384;&#22312;&#30340;&#27169;&#22411;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13372v1 Announce Type: new  Abstract: While Large Language Models (LLMs) excel at the Winograd Schema Challenge (WSC), a coreference resolution task testing common-sense reasoning through pronoun disambiguation, they struggle with instances that feature minor alterations or rewording. To address this, we introduce EvoGrad, an open-source platform that harnesses a human-in-the-loop approach to create a dynamic dataset tailored to such altered WSC instances. Leveraging ChatGPT's capabilities, we expand our task instances from 182 to 3,691, setting a new benchmark for diverse common-sense reasoning datasets. Additionally, we introduce the error depth metric, assessing model stability in dynamic tasks. Our results emphasize the challenge posed by EvoGrad: Even the best performing LLM, GPT-3.5, achieves an accuracy of 65.0% with an average error depth of 7.2, a stark contrast to human performance of 92. 8% accuracy without perturbation errors. This highlights ongoing model limita
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;G&amp;O&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20869;&#23481;&#29983;&#25104;&#19982;&#32467;&#26500;&#21270;&#36807;&#31243;&#20998;&#31163;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#29305;&#23450;&#32467;&#26500;&#21270;&#25991;&#26412;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.13364</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#25913;&#21892;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25277;&#21462;&#20013;&#30340;&#36755;&#20986;
&lt;/p&gt;
&lt;p&gt;
A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13364
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;G&amp;O&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20869;&#23481;&#29983;&#25104;&#19982;&#32467;&#26500;&#21270;&#36807;&#31243;&#20998;&#31163;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#29305;&#23450;&#32467;&#26500;&#21270;&#25991;&#26412;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#26681;&#25454;&#25351;&#20196;&#29983;&#25104;&#38750;&#32467;&#26500;&#21270;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#35201;&#27714;&#23427;&#20204;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#32467;&#26500;&#21270;&#26684;&#24335;&#30340;&#25991;&#26412;&#26102;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#21487;&#33021;&#19981;&#19968;&#33268;&#65292;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#25110;&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#31561;&#24212;&#29992;&#20013;&#36825;&#19968;&#28857;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;G&amp;O&#65292;&#20197;&#22686;&#24378;&#23427;&#20204;&#30340;&#32467;&#26500;&#21270;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#12290;&#23427;&#23558;&#29983;&#25104;&#20998;&#35299;&#20026;&#19968;&#20010;&#20004;&#27493;&#27969;&#31243;&#65306;&#39318;&#20808;&#65292;LLMs&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#31572;&#26696;&#20316;&#20026;&#20013;&#38388;&#21709;&#24212;&#12290;&#38543;&#21518;&#65292;&#35201;&#27714;LLMs&#23558;&#36755;&#20986;&#32452;&#32455;&#25104;&#25152;&#38656;&#30340;&#32467;&#26500;&#65292;&#20351;&#29992;&#20013;&#38388;&#21709;&#24212;&#20316;&#20026;&#19978;&#19979;&#25991;&#12290;G&amp;O&#26377;&#25928;&#22320;&#23558;&#20869;&#23481;&#29983;&#25104;&#19982;&#26500;&#24314;&#36807;&#31243;&#20998;&#31163;&#65292;&#20943;&#23569;&#20102;&#21516;&#26102;&#23436;&#25104;&#20004;&#20010;&#27491;&#20132;&#20219;&#21153;&#30340;&#21387;&#21147;&#12290;&#22312;&#38646;-shot NER&#21644;RE&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13364v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated impressive abilities in generating unstructured natural language according to instructions. However, their performance can be inconsistent when tasked with producing text that adheres to specific structured formats, which is crucial in applications like named entity recognition (NER) or relation extraction (RE). To address this issue, this paper introduces an efficient method, G&amp;O, to enhance their structured text generation capabilities. It breaks the generation into a two-step pipeline: initially, LLMs generate answers in natural language as intermediate responses. Subsequently, LLMs are asked to organize the output into the desired structure, using the intermediate responses as context. G&amp;O effectively separates the generation of content from the structuring process, reducing the pressure of completing two orthogonal tasks simultaneously. Tested on zero-shot NER and RE, the results indica
&lt;/p&gt;</description></item><item><title>PIRB&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#27874;&#20848;&#25991;&#26412;&#20449;&#24687;&#26816;&#32034;&#22522;&#20934;&#65292;&#21253;&#21547;41&#20010;&#20219;&#21153;&#65292;&#35780;&#20272;&#20102;&#36229;&#36807;20&#31181;&#23494;&#38598;&#21644;&#31232;&#30095;&#26816;&#32034;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#19977;&#27493;&#35757;&#32451;&#27969;&#31243;&#26469;&#26500;&#24314;&#39640;&#25928;&#30340;&#29305;&#23450;&#35821;&#35328;&#26816;&#32034;&#22120;&#65292;&#26368;&#21518;&#39564;&#35777;&#20102;&#20182;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;</title><link>https://arxiv.org/abs/2402.13350</link><description>&lt;p&gt;
PIRB&#65306;&#27874;&#20848;&#23494;&#38598;&#21644;&#28151;&#21512;&#25991;&#26412;&#26816;&#32034;&#26041;&#27861;&#30340;&#32508;&#21512;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
PIRB: A Comprehensive Benchmark of Polish Dense and Hybrid Text Retrieval Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13350
&lt;/p&gt;
&lt;p&gt;
PIRB&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#27874;&#20848;&#25991;&#26412;&#20449;&#24687;&#26816;&#32034;&#22522;&#20934;&#65292;&#21253;&#21547;41&#20010;&#20219;&#21153;&#65292;&#35780;&#20272;&#20102;&#36229;&#36807;20&#31181;&#23494;&#38598;&#21644;&#31232;&#30095;&#26816;&#32034;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#19977;&#27493;&#35757;&#32451;&#27969;&#31243;&#26469;&#26500;&#24314;&#39640;&#25928;&#30340;&#29305;&#23450;&#35821;&#35328;&#26816;&#32034;&#22120;&#65292;&#26368;&#21518;&#39564;&#35777;&#20102;&#20182;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#27874;&#20848;&#20449;&#24687;&#26816;&#32034;&#22522;&#20934;&#65288;PIRB&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;&#27874;&#20848;&#35821;&#30340;41&#20010;&#25991;&#26412;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#29616;&#26377;&#25968;&#25454;&#38598;&#20197;&#21450;10&#20010;&#26032;&#30340;&#12289;&#20197;&#21069;&#26410;&#20844;&#24320;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#21307;&#23398;&#12289;&#27861;&#24459;&#12289;&#21830;&#19994;&#12289;&#29289;&#29702;&#21644;&#35821;&#35328;&#23398;&#31561;&#22810;&#26679;&#20027;&#39064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#36229;&#36807;20&#20010;&#23494;&#38598;&#21644;&#31232;&#30095;&#26816;&#32034;&#27169;&#22411;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#21253;&#25324;&#25105;&#20204;&#35757;&#32451;&#30340;&#22522;&#20934;&#27169;&#22411;&#20197;&#21450;&#20854;&#20182;&#21487;&#29992;&#30340;&#27874;&#20848;&#35821;&#21644;&#22810;&#35821;&#35328;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#39640;&#25928;&#29305;&#23450;&#35821;&#35328;&#26816;&#32034;&#22120;&#30340;&#19977;&#27493;&#27969;&#31243;&#65292;&#21253;&#25324;&#30693;&#35782;&#33976;&#39311;&#12289;&#30417;&#30563;&#24494;&#35843;&#20197;&#21450;&#20351;&#29992;&#36731;&#37327;&#32423;&#37325;&#26032;&#35780;&#20998;&#27169;&#22411;&#26500;&#24314;&#31232;&#30095;-&#23494;&#38598;&#28151;&#21512;&#26816;&#32034;&#22120;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20026;&#27874;&#20848;&#35821;&#35757;&#32451;&#20102;&#26032;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#24182;&#23558;&#20854;&#32467;&#26524;&#19982;&#20808;&#21069;&#35780;&#20272;&#36807;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23494;&#38598;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13350v1 Announce Type: new  Abstract: We present Polish Information Retrieval Benchmark (PIRB), a comprehensive evaluation framework encompassing 41 text information retrieval tasks for Polish. The benchmark incorporates existing datasets as well as 10 new, previously unpublished datasets covering diverse topics such as medicine, law, business, physics, and linguistics. We conduct an extensive evaluation of over 20 dense and sparse retrieval models, including the baseline models trained by us as well as other available Polish and multilingual methods. Finally, we introduce a three-step process for training highly effective language-specific retrievers, consisting of knowledge distillation, supervised fine-tuning, and building sparse-dense hybrid retrievers using a lightweight rescoring model. In order to validate our approach, we train new text encoders for Polish and compare their results with previously evaluated methods. Our dense models outperform the best solutions avai
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31616;&#21333;&#30340;&#25506;&#27979;&#22120;&#32858;&#21512;&#26469;&#22686;&#24378;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24187;&#35273;&#26816;&#27979;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#24615;&#65292;&#26377;&#26395;&#20351;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#26356;&#21152;&#21487;&#38752;&#12290;</title><link>https://arxiv.org/abs/2402.13331</link><description>&lt;p&gt;
&#36890;&#36807;&#31616;&#21333;&#30340;&#25506;&#27979;&#22120;&#32858;&#21512;&#22686;&#24378;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enhanced Hallucination Detection in Neural Machine Translation through Simple Detector Aggregation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13331
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31616;&#21333;&#30340;&#25506;&#27979;&#22120;&#32858;&#21512;&#26469;&#22686;&#24378;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24187;&#35273;&#26816;&#27979;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#24615;&#65292;&#26377;&#26395;&#20351;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#26356;&#21152;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24187;&#35273;&#32763;&#35793;&#22312;&#23454;&#38469;&#37096;&#32626;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#26102;&#23384;&#22312;&#37325;&#22823;&#23041;&#32961;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#30340;&#25506;&#27979;&#22120;&#21644;&#24341;&#20837;&#31616;&#21333;&#30340;&#32858;&#21512;&#22810;&#25506;&#27979;&#22120;&#26041;&#27861;&#26469;&#35299;&#20915;&#21333;&#20010;&#25506;&#27979;&#22120;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#25105;&#20204;&#32858;&#21512;&#25506;&#27979;&#22120;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#26356;&#21152;&#21487;&#38752;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13331v1 Announce Type: new  Abstract: Hallucinated translations pose significant threats and safety concerns when it comes to the practical deployment of machine translation systems. Previous research works have identified that detectors exhibit complementary performance different detectors excel at detecting different types of hallucinations. In this paper, we propose to address the limitations of individual detectors by combining them and introducing a straightforward method for aggregating multiple detectors. Our results demonstrate the efficacy of our aggregated detector, providing a promising step towards evermore reliable machine translation systems.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#29305;&#24449;&#21644;&#22810;&#23618;&#26550;&#26500;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;WordNet&#21644;WordNet Domains&#31561;&#35821;&#20041;&#35789;&#27719;&#36164;&#28304;&#22686;&#24378;&#29616;&#20195;&#30417;&#30563;&#35789;&#20041;&#28040;&#27495;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.13302</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#35789;&#27719;&#36164;&#28304;&#22686;&#24378;&#29616;&#20195;&#30417;&#30563;&#35789;&#20041;&#28040;&#27495;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Enhancing Modern Supervised Word Sense Disambiguation Models by Semantic Lexical Resources
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13302
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#29305;&#24449;&#21644;&#22810;&#23618;&#26550;&#26500;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;WordNet&#21644;WordNet Domains&#31561;&#35821;&#20041;&#35789;&#27719;&#36164;&#28304;&#22686;&#24378;&#29616;&#20195;&#30417;&#30563;&#35789;&#20041;&#28040;&#27495;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#29992;&#20110;&#35789;&#20041;&#28040;&#27495;&#65288;WSD&#65289;&#30340;&#30417;&#30563;&#27169;&#22411;&#22312;&#26368;&#27969;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;&#26368;&#36817;&#24341;&#20837;&#20102;&#35789;&#23884;&#20837;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20197;&#35774;&#35745;&#24378;&#22823;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#29305;&#24449;&#65292;&#20294;&#21033;&#29992;&#35821;&#20041;&#35789;&#27719;&#36164;&#28304;&#65288;SLRs&#65289;&#25913;&#36827;WSD&#27169;&#22411;&#30340;&#20852;&#36259;&#20027;&#35201;&#23616;&#38480;&#20110;&#22522;&#20110;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#20004;&#31181;&#27969;&#34892;&#30340;SLRs&#65306;WordNet&#21644;WordNet Domains&#65292;&#22686;&#24378;&#20102;&#8220;&#29616;&#20195;&#8221;&#30417;&#30563;WSD&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#24335;&#23558;&#35821;&#20041;&#29305;&#24449;&#24341;&#20837;&#20998;&#31867;&#22120;&#65292;&#24182;&#32771;&#34385;&#20351;&#29992;SLR&#32467;&#26500;&#26469;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#20041;&#29305;&#24449;&#30340;&#20316;&#29992;&#65292;&#25506;&#35752;&#23427;&#20204;&#19982;&#36890;&#36807;&#35789;&#23884;&#20837;&#25110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#30340;&#26412;&#22320;&#19978;&#19979;&#25991;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#24182;&#23558;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#25193;&#23637;&#20026;&#29992;&#20110;WSD&#30340;&#26032;&#22411;&#22810;&#23618;&#26550;&#26500;&#12290;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23454;&#39564;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13302v1 Announce Type: new  Abstract: Supervised models for Word Sense Disambiguation (WSD) currently yield to state-of-the-art results in the most popular benchmarks. Despite the recent introduction of Word Embeddings and Recurrent Neural Networks to design powerful context-related features, the interest in improving WSD models using Semantic Lexical Resources (SLRs) is mostly restricted to knowledge-based approaches. In this paper, we enhance "modern" supervised WSD models exploiting two popular SLRs: WordNet and WordNet Domains. We propose an effective way to introduce semantic features into the classifiers, and we consider using the SLR structure to augment the training data. We study the effect of different types of semantic features, investigating their interaction with local contexts encoded by means of mixtures of Word Embeddings or Recurrent Neural Networks, and we extend the proposed model into a novel multi-layer architecture for WSD. A detailed experimental compa
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#32467;&#26500;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#24341;&#23548;&#30340;SQL&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;SQL&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#25191;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13284</link><description>&lt;p&gt;
&#32467;&#26500;&#24341;&#23548;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;SQL&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Structure Guided Large Language Model for SQL Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13284
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#32467;&#26500;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#24341;&#23548;&#30340;SQL&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;SQL&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#25191;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20934;&#30830;&#30340;&#32467;&#26500;&#21270;&#26597;&#35810;&#35821;&#35328;&#65288;SQL&#65289;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#23558;&#29992;&#25143;&#30340;&#35821;&#20041;&#26597;&#35810;&#19982;&#32467;&#26500;&#21270;&#25968;&#25454;&#24211;&#21305;&#37197;&#65292;&#28982;&#21518;&#29983;&#25104;&#32467;&#26500;&#21270;SQL&#26041;&#38754;&#12290;&#29616;&#26377;&#27169;&#22411;&#36890;&#24120;&#23558;&#26597;&#35810;&#21644;&#25968;&#25454;&#24211;&#27169;&#24335;&#36755;&#20837;&#21040;LLM&#20013;&#65292;&#24182;&#20381;&#36182;LLM&#25191;&#34892;&#35821;&#20041;-&#32467;&#26500;&#21305;&#37197;&#24182;&#29983;&#25104;&#32467;&#26500;&#21270;SQL&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#24573;&#30053;&#20102;&#29992;&#25143;&#26597;&#35810;&#21644;&#25968;&#25454;&#24211;&#20013;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#32780;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#29992;&#26469;&#22686;&#24378;&#32467;&#26500;&#21270;SQL&#30340;&#29983;&#25104;&#12290;&#36825;&#19968;&#30095;&#24573;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#25110;&#26080;&#27861;&#25191;&#34892;&#30340;SQL&#29983;&#25104;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21040;SQL&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22266;&#26377;&#30340;&#32467;&#26500;&#20449;&#24687;&#26469;&#25913;&#21892;LLM&#30340;SQL&#29983;&#25104;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#32467;&#26500;&#24341;&#23548;SQL&#65288;SGU-SQL&#65289;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13284v1 Announce Type: cross  Abstract: Generating accurate Structured Querying Language (SQL) is a long-standing problem, especially in matching users' semantic queries with structured databases and then generating structured SQL. Existing models typically input queries and database schemas into the LLM and rely on the LLM to perform semantic-structure matching and generate structured SQL. However, such solutions overlook the structural information within user queries and databases, which can be utilized to enhance the generation of structured SQL. This oversight can lead to inaccurate or unexecutable SQL generation. To fully exploit the structure, we propose a structure-to-SQL framework, which leverages the inherent structure information to improve the SQL generation of LLMs. Specifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model. SGU-SQL first links user queries and databases in a structure-enhanced manner. It then decomposes complicated linked str
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#8220;CosmoAgent&#8221;&#65292;&#21033;&#29992;LLM&#27169;&#25311;&#20154;&#31867;&#21644;&#22806;&#26143;&#25991;&#26126;&#20043;&#38388;&#30340;&#22797;&#26434;&#20114;&#21160;&#65292;&#35780;&#20272;&#21644;&#24179;&#20849;&#23384;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#37327;&#21270;&#35780;&#20272;&#25991;&#26126;&#30340;&#21457;&#23637;&#36712;&#36857;&#65292;&#21516;&#26102;&#32771;&#34385;&#19981;&#21516;&#25991;&#26126;&#20043;&#38388;&#30340;&#24040;&#22823;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13184</link><description>&lt;p&gt;
&#22914;&#26524;LLM&#20855;&#26377;&#19981;&#21516;&#30340;&#19990;&#30028;&#35266;&#65306;&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#27169;&#25311;&#22806;&#26143;&#25991;&#26126;
&lt;/p&gt;
&lt;p&gt;
What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13184
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#8220;CosmoAgent&#8221;&#65292;&#21033;&#29992;LLM&#27169;&#25311;&#20154;&#31867;&#21644;&#22806;&#26143;&#25991;&#26126;&#20043;&#38388;&#30340;&#22797;&#26434;&#20114;&#21160;&#65292;&#35780;&#20272;&#21644;&#24179;&#20849;&#23384;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#37327;&#21270;&#35780;&#20272;&#25991;&#26126;&#30340;&#21457;&#23637;&#36712;&#36857;&#65292;&#21516;&#26102;&#32771;&#34385;&#19981;&#21516;&#25991;&#26126;&#20043;&#38388;&#30340;&#24040;&#22823;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#8220;CosmoAgent&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#27169;&#25311;&#20154;&#31867;&#19982;&#22806;&#26143;&#25991;&#26126;&#20043;&#38388;&#22797;&#26434;&#30340;&#20132;&#20114;&#65292;&#29305;&#21035;&#24378;&#35843;&#21490;&#33922;&#33452;&#183;&#38669;&#37329;&#20851;&#20110;&#19981;&#35201;&#38543;&#24847;&#21521;&#23431;&#23449;&#21457;&#36865;&#26080;&#32447;&#30005;&#20449;&#21495;&#30340;&#35880;&#24910;&#24314;&#35758;&#12290;&#35813;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#35780;&#20272;&#21644;&#24179;&#20849;&#23384;&#30340;&#21487;&#34892;&#24615;&#65292;&#21516;&#26102;&#32771;&#34385;&#21487;&#33021;&#23041;&#32961;&#21892;&#24847;&#25991;&#26126;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#36890;&#36807;&#37319;&#29992;&#25968;&#23398;&#27169;&#22411;&#21644;&#29366;&#24577;&#36716;&#25442;&#30697;&#38453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23450;&#37327;&#35780;&#20272;&#25991;&#26126;&#30340;&#21457;&#23637;&#36712;&#36857;&#65292;&#20026;&#22312;&#20851;&#38190;&#22686;&#38271;&#21644;&#39281;&#21644;&#28857;&#20570;&#20986;&#26410;&#26469;&#20915;&#31574;&#25552;&#20379;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25215;&#35748;&#23431;&#23449;&#20013;&#28508;&#22312;&#29983;&#27963;&#26465;&#20214;&#30340;&#24040;&#22823;&#22810;&#26679;&#24615;&#21487;&#33021;&#20250;&#20419;&#36827;&#19981;&#21516;&#25991;&#26126;&#20043;&#38388;&#29420;&#29305;&#30340;&#23431;&#23449;&#35266;&#12289;&#36947;&#24503;&#20934;&#21017;&#21644;&#19990;&#30028;&#35266;&#12290;&#35748;&#35782;&#21040;&#22320;&#29699;&#19978;--
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13184v1 Announce Type: new  Abstract: In this study, we introduce "CosmoAgent," an innovative artificial intelligence framework utilizing Large Language Models (LLMs) to simulate complex interactions between human and extraterrestrial civilizations, with a special emphasis on Stephen Hawking's cautionary advice about not sending radio signals haphazardly into the universe. The goal is to assess the feasibility of peaceful coexistence while considering potential risks that could threaten well-intentioned civilizations. Employing mathematical models and state transition matrices, our approach quantitatively evaluates the development trajectories of civilizations, offering insights into future decision-making at critical points of growth and saturation. Furthermore, the paper acknowledges the vast diversity in potential living conditions across the universe, which could foster unique cosmologies, ethical codes, and worldviews among various civilizations. Recognizing the Earth-c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#30340;&#24102;&#27880;&#37322;&#20013;&#25991;&#38544;&#21947;&#35821;&#26009;&#24211;&#65292;&#24378;&#35843;&#38544;&#21947;&#29983;&#25104;&#20013;&#30340;&#22522;&#30784;&#21450;&#20854;&#29420;&#29305;&#29305;&#24449;&#65292;&#32780;&#38750;&#20256;&#32479;&#30340;&#23545;&#35937;&#21644;&#36733;&#20307;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.13145</link><description>&lt;p&gt;
CMDAG: &#19968;&#20010;&#24102;&#26377;&#27880;&#37322;&#30340;&#20013;&#25991;&#38544;&#21947;&#25968;&#25454;&#38598;&#20316;&#20026;&#8220;CoT&#8221;&#26469;&#25552;&#21319;&#38544;&#21947;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CMDAG: A Chinese Metaphor Dataset with Annotated Grounds as CoT for Boosting Metaphor Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#30340;&#24102;&#27880;&#37322;&#20013;&#25991;&#38544;&#21947;&#35821;&#26009;&#24211;&#65292;&#24378;&#35843;&#38544;&#21947;&#29983;&#25104;&#20013;&#30340;&#22522;&#30784;&#21450;&#20854;&#29420;&#29305;&#29305;&#24449;&#65292;&#32780;&#38750;&#20256;&#32479;&#30340;&#23545;&#35937;&#21644;&#36733;&#20307;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#21947;&#26159;&#20154;&#31867;&#35821;&#35328;&#21644;&#25991;&#23398;&#20013;&#26174;&#33879;&#30340;&#20462;&#36766;&#25163;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#22686;&#28155;&#20102;&#33394;&#24425;&#12289;&#24418;&#35937;&#21644;&#24378;&#35843;&#65292;&#20197;&#22686;&#24378;&#26377;&#25928;&#20132;&#27969;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#30340;&#24102;&#27880;&#37322;&#20013;&#25991;&#38544;&#21947;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#32422;28K&#21477;&#26469;&#33258;&#21508;&#31181;&#20013;&#25991;&#25991;&#23398;&#26469;&#28304;&#65288;&#22914;&#35799;&#27468;&#12289;&#25955;&#25991;&#12289;&#27468;&#35789;&#31561;&#65289;&#12290;&#20026;&#30830;&#20445;&#27880;&#37322;&#30340;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#25351;&#21335;&#12290;&#36825;&#20123;&#25351;&#21335;&#28085;&#30422;&#20102;&#38544;&#21947;&#26631;&#27880;&#30340;&#26041;&#38754;&#65292;&#21253;&#25324;&#35782;&#21035;&#23545;&#35937;&#12289;&#36733;&#20307;&#21644;&#22522;&#30784;&#65292;&#20197;&#22788;&#29702;&#27604;&#21947;&#12289;&#25311;&#20154;&#12289;&#24182;&#21015;&#21644;&#22840;&#24352;&#31561;&#22797;&#26434;&#24615;&#12290;&#25171;&#30772;&#20256;&#32479;&#65292;&#25105;&#20204;&#30340;&#38544;&#21947;&#29983;&#25104;&#26041;&#27861;&#24378;&#35843;&#22522;&#30784;&#21450;&#20854;&#29420;&#29305;&#29305;&#24449;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#23545;&#35937;&#21644;&#36733;&#20307;&#32452;&#21512;&#12290;&#36890;&#36807;&#23558;&#8220;&#22522;&#30784;&#8221;&#20316;&#20026;&#8220;CoT&#8221;&#65288;&#24605;&#32500;&#38142;&#65289;&#36755;&#20837;&#36827;&#34892;&#25972;&#21512;&#65292;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#37325;&#26032;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13145v1 Announce Type: cross  Abstract: Metaphor is a prominent linguistic device in human language and literature, as they add color, imagery, and emphasis to enhance effective communication. This paper introduces a large-scale high quality annotated Chinese Metaphor Corpus, which comprises around 28K sentences drawn from a diverse range of Chinese literary sources, such as poems, prose, song lyrics, etc. To ensure the accuracy and consistency of our annotations, we introduce a comprehensive set of guidelines. These guidelines address the facets of metaphor annotation, including identifying tenors, vehicles, and grounds to handling the complexities of similes, personifications, juxtapositions, and hyperboles. Breaking tradition, our approach to metaphor generation emphasizes grounds and their distinct features rather than the conventional combination of tenors and vehicles. By integrating "ground" as a CoT (Chain of Thoughts) input, we are able to generate metaphors that re
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25991;&#26412;&#25688;&#35201;&#25552;&#39640;&#23545;&#35805;&#26816;&#32034;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#65292;&#36890;&#36807;&#23545;&#35805;&#25688;&#35201;&#29983;&#25104;&#22120;&#36827;&#34892;&#26597;&#35810;&#21644;&#20851;&#38190;&#35789;&#29983;&#25104;&#65292;&#36827;&#19968;&#27493;&#25552;&#28860;&#36731;&#37327;&#32423;&#23545;&#35805;&#32534;&#30721;&#22120;&#20197;&#36991;&#20813;&#39069;&#22806;&#25512;&#29702;&#25104;&#26412;</title><link>https://arxiv.org/abs/2402.13043</link><description>&lt;p&gt;
&#29992;&#38544;&#24335;&#25991;&#26412;&#25688;&#35201;&#25552;&#39640;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Effective and Efficient Conversation Retrieval for Dialogue State Tracking with Implicit Text Summaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13043
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#25688;&#35201;&#25552;&#39640;&#23545;&#35805;&#26816;&#32034;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#65292;&#36890;&#36807;&#23545;&#35805;&#25688;&#35201;&#29983;&#25104;&#22120;&#36827;&#34892;&#26597;&#35810;&#21644;&#20851;&#38190;&#35789;&#29983;&#25104;&#65292;&#36827;&#19968;&#27493;&#25552;&#28860;&#36731;&#37327;&#32423;&#23545;&#35805;&#32534;&#30721;&#22120;&#20197;&#36991;&#20813;&#39069;&#22806;&#25512;&#29702;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13043v1 &#20844;&#21578;&#31867;&#22411;: &#26032; &#25991;&#25688;: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23567;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65288;DST&#65289;&#20381;&#36182;&#20110;&#19968;&#20010;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#23545;&#35805;&#26816;&#32034;&#22120;&#26469;&#26597;&#25214;&#31867;&#20284;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#20197;&#36827;&#34892;&#25552;&#31034;&#23398;&#20064;&#12290;&#20808;&#21069;&#30340;&#20316;&#21697;&#20351;&#29992;&#21407;&#22987;&#23545;&#35805;&#19978;&#19979;&#25991;&#20316;&#20026;&#25628;&#32034;&#38190;&#21644;&#26597;&#35810;&#65292;&#24182;&#36890;&#36807;&#23545;&#24102;&#27880;&#37322;&#30340;&#23545;&#35805;&#36827;&#34892;&#24494;&#35843;&#26469;&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#22826;&#36866;&#21512;&#25193;&#23637;&#21040;&#26032;&#30340;&#39046;&#22495;&#25110;&#26032;&#30340;&#27880;&#37322;&#35821;&#35328;&#65292;&#22240;&#20026;&#24494;&#35843;&#25968;&#25454;&#19981;&#21487;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#23545;&#35805;&#30340;&#25991;&#26412;&#25688;&#35201;&#26469;&#22788;&#29702;&#23545;&#35805;&#26816;&#32034;&#20219;&#21153;&#12290;&#37319;&#29992;&#22522;&#20110;LLM&#30340;&#23545;&#35805;&#25688;&#35201;&#29983;&#25104;&#22120;&#36827;&#34892;&#26597;&#35810;&#21644;&#20851;&#38190;&#35789;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#26368;&#22823;&#20869;&#31215;&#25628;&#32034;&#12290;&#20026;&#36991;&#20813;LLM&#22522;&#20110;&#23545;&#35805;&#25688;&#35201;&#29983;&#25104;&#24102;&#26469;&#30340;&#39069;&#22806;&#25512;&#29702;&#25104;&#26412;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#28860;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#23545;&#35805;&#32534;&#30721;&#22120;&#65292;&#35813;&#32534;&#30721;&#22120;&#22312;&#19981;&#35299;&#30721;&#27979;&#35797;&#23545;&#35805;&#25688;&#35201;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#26597;&#35810;&#23884;&#20837;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13043v1 Announce Type: new  Abstract: Few-shot dialogue state tracking (DST) with Large Language Models (LLM) relies on an effective and efficient conversation retriever to find similar in-context examples for prompt learning. Previous works use raw dialogue context as search keys and queries, and a retriever is fine-tuned with annotated dialogues to achieve superior performance. However, the approach is less suited for scaling to new domains or new annotation languages, where fine-tuning data is unavailable. To address this problem, we handle the task of conversation retrieval based on text summaries of the conversations. A LLM-based conversation summarizer is adopted for query and key generation, which enables effective maximum inner product search. To avoid the extra inference cost brought by LLM-based conversation summarization, we further distill a light-weight conversation encoder which produces query embeddings without decoding summaries for test conversations. We val
&lt;/p&gt;</description></item><item><title>FormulaQA&#26159;&#19968;&#20010;&#22522;&#20110;&#21021;&#20013;&#29289;&#29702;&#32771;&#35797;&#30340;&#20844;&#24335;&#39537;&#21160;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#19981;&#21516;&#26041;&#27861;&#21644;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#22411;LLMs&#20197;&#21450;&#23545;&#23567;&#22411;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#24212;&#23545;&#22797;&#26434;&#12289;&#22522;&#20110;&#20844;&#24335;&#30340;FormulaQA&#26102;&#30340;&#28508;&#22312;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.12692</link><description>&lt;p&gt;
FormulaQA&#65306;&#19968;&#20010;&#22522;&#20110;&#20844;&#24335;&#30340;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FormulaQA: A Question Answering Dataset for Formula-Based Numerical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12692
&lt;/p&gt;
&lt;p&gt;
FormulaQA&#26159;&#19968;&#20010;&#22522;&#20110;&#21021;&#20013;&#29289;&#29702;&#32771;&#35797;&#30340;&#20844;&#24335;&#39537;&#21160;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#19981;&#21516;&#26041;&#27861;&#21644;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#22411;LLMs&#20197;&#21450;&#23545;&#23567;&#22411;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#24212;&#23545;&#22797;&#26434;&#12289;&#22522;&#20110;&#20844;&#24335;&#30340;FormulaQA&#26102;&#30340;&#28508;&#22312;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#20844;&#24335;&#26159;&#20154;&#31867;&#22312;&#35299;&#20915;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#26102;&#30340;&#22522;&#26412;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25968;&#20540;&#25512;&#29702;&#25968;&#25454;&#38598;&#24456;&#23569;&#26126;&#30830;&#25351;&#20986;&#25512;&#29702;&#27493;&#39588;&#20013;&#20351;&#29992;&#30340;&#20844;&#24335;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21021;&#20013;&#29289;&#29702;&#32771;&#35797;&#30340;&#20844;&#24335;&#39537;&#21160;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#38382;&#31572;&#25968;&#25454;&#38598;FormulaQA&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#22823;&#23567;&#20174;7B&#21040;&#36229;&#36807;100B&#21442;&#25968;&#30340;LLMs&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24605;&#32500;&#38142;&#26041;&#27861;&#30340;&#35780;&#20272;&#65292;&#24182;&#25506;&#32034;&#20102;&#22312;&#25552;&#20379;&#22806;&#37096;&#20844;&#24335;&#25968;&#25454;&#24211;&#26102;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#22411;LLMs&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#23545;&#22823;&#23567;&#19981;&#36229;&#36807;2B&#30340;&#36739;&#23567;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#24378;&#35843;&#20102;&#24403;&#24212;&#29992;&#20110;&#25105;&#20204;&#22797;&#26434;&#12289;&#22522;&#20110;&#20844;&#24335;&#30340;FormulaQA&#26102;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#25913;&#36827;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12692v1 Announce Type: new  Abstract: The application of formulas is a fundamental ability of humans when addressing numerical reasoning problems. However, existing numerical reasoning datasets seldom explicitly indicate the formulas employed during the reasoning steps. To bridge this gap, we propose a question answering dataset for formula-based numerical reasoning called FormulaQA, from junior high school physics examinations. We further conduct evaluations on LLMs with size ranging from 7B to over 100B parameters utilizing zero-shot and few-shot chain-of-thoughts methods and we explored the approach of using retrieval-augmented LLMs when providing an external formula database. We also fine-tune on smaller models with size not exceeding 2B. Our empirical findings underscore the significant potential for improvement in existing models when applied to our complex, formula-driven FormulaQA.
&lt;/p&gt;</description></item><item><title>StyleDubber&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30005;&#24433;&#37197;&#38899;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#38899;&#32032;&#32423;&#21035;&#36827;&#34892;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069; V2C &#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#38899;&#32032;&#21457;&#38899;&#19981;&#23436;&#25972;&#21644;&#36523;&#20221;&#31283;&#23450;&#24615;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.12636</link><description>&lt;p&gt;
StyleDubber: &#38754;&#21521;&#30005;&#24433;&#37197;&#38899;&#30340;&#22810;&#23610;&#24230;&#39118;&#26684;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
StyleDubber: Towards Multi-Scale Style Learning for Movie Dubbing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12636
&lt;/p&gt;
&lt;p&gt;
StyleDubber&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30005;&#24433;&#37197;&#38899;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#38899;&#32032;&#32423;&#21035;&#36827;&#34892;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069; V2C &#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#38899;&#32032;&#21457;&#38899;&#19981;&#23436;&#25972;&#21644;&#36523;&#20221;&#31283;&#23450;&#24615;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20221;&#21095;&#26412;&#65292;&#22312;&#30005;&#24433;&#37197;&#38899;&#65288;&#35270;&#35273;&#35821;&#38899;&#20811;&#38534;&#65292;V2C&#65289;&#20013;&#30340;&#25361;&#25112;&#26159;&#26681;&#25454;&#21442;&#32771;&#38899;&#36712;&#30340;&#35821;&#27668;&#65292;&#29983;&#25104;&#19982;&#35270;&#39057;&#22312;&#26102;&#38388;&#21644;&#24773;&#32490;&#19978;&#37117;&#33391;&#22909;&#23545;&#40784;&#30340;&#35821;&#38899;&#12290;&#29616;&#26377;&#30340; V2C &#27169;&#22411;&#26681;&#25454;&#35270;&#39057;&#24103;&#38388;&#30340;&#38388;&#38548;&#23383;&#26029;&#20998;&#21106;&#21095;&#26412;&#30340;&#38899;&#32032;&#65292;&#36825;&#35299;&#20915;&#20102;&#26102;&#38388;&#23545;&#40784;&#38382;&#39064;&#65292;&#20294;&#23548;&#33268;&#38899;&#32032;&#21457;&#38899;&#19981;&#23436;&#25972;&#21644;&#36523;&#20221;&#31283;&#23450;&#24615;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986; StyleDubber&#65292;&#23427;&#23558;&#37197;&#38899;&#23398;&#20064;&#20174;&#24103;&#32423;&#21035;&#36716;&#20026;&#38899;&#32032;&#32423;&#21035;&#12290;&#23427;&#21253;&#21547;&#19977;&#20010;&#20027;&#35201;&#32452;&#20214;&#65306;&#65288;1&#65289;&#19968;&#20010;&#22810;&#27169;&#24577;&#39118;&#26684;&#36866;&#37197;&#22120;&#65292;&#20197;&#38899;&#32032;&#32423;&#21035;&#25805;&#20316;&#65292;&#20174;&#21442;&#32771;&#38899;&#39057;&#20013;&#23398;&#20064;&#21457;&#38899;&#39118;&#26684;&#65292;&#24182;&#29983;&#25104;&#21463;&#35270;&#39057;&#20013;&#21576;&#29616;&#30340;&#38754;&#37096;&#24773;&#32490;&#24433;&#21709;&#30340;&#20013;&#38388;&#34920;&#31034;&#65307;&#65288;2&#65289;&#19968;&#20010;&#20197;&#35821;&#21477;&#32423;&#21035;&#39118;&#26684;&#23398;&#20064;&#27169;&#22359;&#65292;&#24341;&#23548;&#20013;&#38388;&#34920;&#29616;&#30340; mel-spectrogram &#35299;&#30721;&#21644;&#32454;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12636v1 Announce Type: new  Abstract: Given a script, the challenge in Movie Dubbing (Visual Voice Cloning, V2C) is to generate speech that aligns well with the video in both time and emotion, based on the tone of a reference audio track. Existing state-of-the-art V2C models break the phonemes in the script according to the divisions between video frames, which solves the temporal alignment problem but leads to incomplete phoneme pronunciation and poor identity stability. To address this problem, we propose StyleDubber, which switches dubbing learning from the frame level to phoneme level. It contains three main components: (1) A multimodal style adaptor operating at the phoneme level to learn pronunciation style from the reference audio, and generate intermediate representations informed by the facial emotion presented in the video; (2) An utterance-level style learning module, which guides both the mel-spectrogram decoding and the refining processes from the intermediate e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Rectify-Router&#35299;&#20915;&#20102;MoE&#27169;&#22411;&#20013;&#24120;&#29992;&#30340;Top-k&#36335;&#30001;&#26426;&#21046;&#25152;&#24102;&#26469;&#30340;&#20196;&#29260;&#20002;&#22833;&#21644;&#22635;&#20805;&#38382;&#39064;&#65292;&#36890;&#36807;Intra-GPU&#30699;&#27491;&#21644;Fill-in&#30699;&#27491;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.12399</link><description>&lt;p&gt;
&#23558;&#24223;&#26009;&#21464;&#24223;&#20026;&#23453;&#65306;&#30699;&#27491;MoE&#30340;Top-k&#36335;&#30001;&#22120;
&lt;/p&gt;
&lt;p&gt;
Turn Waste into Worth: Rectifying Top-$k$ Router of MoE
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12399
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Rectify-Router&#35299;&#20915;&#20102;MoE&#27169;&#22411;&#20013;&#24120;&#29992;&#30340;Top-k&#36335;&#30001;&#26426;&#21046;&#25152;&#24102;&#26469;&#30340;&#20196;&#29260;&#20002;&#22833;&#21644;&#22635;&#20805;&#38382;&#39064;&#65292;&#36890;&#36807;Intra-GPU&#30699;&#27491;&#21644;Fill-in&#30699;&#27491;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#27169;&#22411;&#22240;&#20854;&#35745;&#31639;&#25928;&#29575;&#32780;&#21463;&#21040;&#27426;&#36814;&#65292;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;Top-k&#36335;&#30001;&#26426;&#21046;&#30001;&#20110;&#19981;&#24179;&#34913;&#30340;&#36335;&#30001;&#23548;&#33268;&#20887;&#20313;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#36807;&#39640;&#12290;&#19968;&#20123;&#19987;&#23478;&#20250;&#28322;&#20986;&#65292;&#20854;&#20013;&#36229;&#20986;&#30340;&#20196;&#29260;&#20250;&#34987;&#20002;&#24323;&#12290;&#32780;&#19968;&#20123;&#19987;&#23478;&#26159;&#31354;&#38386;&#30340;&#65292;&#36825;&#20123;&#19987;&#23478;&#20250;&#22635;&#20805;&#20026;&#38646;&#65292;&#36127;&#38754;&#24433;&#21709;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#20002;&#24323;&#20196;&#29260;&#21644;&#22635;&#20805;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Rectify-Router&#65292;&#21253;&#25324;Intra-GPU&#30699;&#27491;&#21644;Fill-in&#30699;&#27491;&#12290;Intra-GPU&#30699;&#27491;&#22788;&#29702;&#20002;&#24323;&#30340;&#20196;&#29260;&#65292;&#23558;&#23427;&#20204;&#26377;&#25928;&#22320;&#36335;&#30001;&#21040;GPU&#20869;&#30340;&#19987;&#23478;&#65292;&#36991;&#20813;&#36328;GPU&#36890;&#20449;&#12290;Fill-in&#30699;&#27491;&#36890;&#36807;&#29992;&#20855;&#26377;&#39640;&#36335;&#30001;&#20998;&#25968;&#30340;&#20196;&#29260;&#26367;&#25442;&#22635;&#20805;&#20196;&#29260;&#26469;&#35299;&#20915;&#22635;&#20805;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Intra-GPU&#30699;&#27491;&#21644;Fill-in&#30699;&#27491;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12399v1 Announce Type: cross  Abstract: Sparse Mixture of Experts (MoE) models are popular for training large language models due to their computational efficiency. However, the commonly used top-$k$ routing mechanism suffers from redundancy computation and memory costs due to the unbalanced routing. Some experts are overflow, where the exceeding tokens are dropped. While some experts are vacant, which are padded with zeros, negatively impacting model performance. To address the dropped tokens and padding, we propose the Rectify-Router, comprising the Intra-GPU Rectification and the Fill-in Rectification. The Intra-GPU Rectification handles dropped tokens, efficiently routing them to experts within the GPU where they are located to avoid inter-GPU communication. The Fill-in Rectification addresses padding by replacing padding tokens with the tokens that have high routing scores. Our experimental results demonstrate that the Intra-GPU Rectification and the Fill-in Rectificati
&lt;/p&gt;</description></item><item><title>&#23433;&#20840;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#36890;&#36807;&#27169;&#25311;&#22833;&#35843;&#26694;&#26550;&#65292;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#20135;&#29983;&#21361;&#38505;&#32467;&#26524;&#65292;&#23545;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21452;&#20493;&#26377;&#23475;&#24615;&#65292;&#39640;&#20110;&#24378;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#20063;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12343</link><description>&lt;p&gt;
&#27169;&#25311;&#22833;&#35843;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#23545;&#40784;&#21487;&#33021;&#20250;&#36866;&#24471;&#20854;&#21453;&#65281;
&lt;/p&gt;
&lt;p&gt;
Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12343
&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#36890;&#36807;&#27169;&#25311;&#22833;&#35843;&#26694;&#26550;&#65292;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#20135;&#29983;&#21361;&#38505;&#32467;&#26524;&#65292;&#23545;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21452;&#20493;&#26377;&#23475;&#24615;&#65292;&#39640;&#20110;&#24378;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#20063;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#36827;&#34892;&#23433;&#20840;&#23545;&#40784;&#65292;&#20197;&#30830;&#20445;&#19982;&#20154;&#31867;&#36827;&#34892;&#23433;&#20840;&#30340;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25512;&#29702;&#26102;&#25915;&#20987;&#26694;&#26550;&#65292;&#34920;&#26126;&#23433;&#20840;&#23545;&#40784;&#20063;&#21487;&#33021;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#26080;&#24847;&#20013;&#20419;&#25104;&#26377;&#23475;&#32467;&#26524;&#12290;&#36825;&#20010;&#26694;&#26550;&#34987;&#21629;&#21517;&#20026;&#27169;&#25311;&#22833;&#35843;&#65288;ED&#65289;&#65292;&#22312;&#36755;&#20986;&#31354;&#38388;&#20013;&#19981;&#33391;&#22320;&#32452;&#21512;&#20102;&#19968;&#23545;&#24320;&#28304;&#39044;&#35757;&#32451;&#21644;&#23433;&#20840;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20135;&#29983;&#20102;&#19968;&#20010;&#26377;&#23475;&#30340;&#35821;&#35328;&#27169;&#22411;&#32780;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;ED&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#21644;&#22235;&#20010;&#27169;&#22411;&#31995;&#21015;&#65288;Llama-1&#12289;Llama-2&#12289;Mistral&#21644;Alpaca&#65289;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ED&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#23475;&#24615;&#22686;&#21152;&#20102;&#19968;&#20493;&#65292;&#24182;&#32988;&#36807;&#24378;&#22522;&#32447;&#65292;&#20197;&#36739;&#22823;&#20248;&#21183;&#22312;48&#20010;&#35780;&#20272;&#23376;&#38598;&#20013;&#30340;43&#20010;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#26377;&#23475;&#29575;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20984;&#26174;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#65292;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#23454;&#36341;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12343v1 Announce Type: new  Abstract: Large language models (LLMs) need to undergo safety alignment to ensure safe conversations with humans. However, in this work, we introduce an inference-time attack framework, demonstrating that safety alignment can also unintentionally facilitate harmful outcomes under adversarial manipulation. This framework, named Emulated Disalignment (ED), adversely combines a pair of open-source pre-trained and safety-aligned language models in the output space to produce a harmful language model without any training. Our experiments with ED across three datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of pre-trained models and outperforms strong baselines, achieving the highest harmful rate in 43 out of 48 evaluation subsets by a large margin. Crucially, our findings highlight the importance of reevaluating the practice of open-sourcing language models even after safety alignment.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#24037;&#20855;&#22686;&#24378;&#22411;&#31185;&#23398;&#25512;&#29702;&#30340;&#26032;&#20219;&#21153;&#35774;&#32622;&#65292;&#36890;&#36807;&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;&#24037;&#20855;&#38598;&#65292;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#21464;&#24471;&#26356;&#21152;&#23454;&#29992;&#21644;&#21487;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2402.11451</link><description>&lt;p&gt;
SciAgent: &#24037;&#20855;&#22686;&#24378;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#31185;&#23398;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SciAgent: Tool-augmented Language Models for Scientific Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11451
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#24037;&#20855;&#22686;&#24378;&#22411;&#31185;&#23398;&#25512;&#29702;&#30340;&#26032;&#20219;&#21153;&#35774;&#32622;&#65292;&#36890;&#36807;&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;&#24037;&#20855;&#38598;&#65292;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#21464;&#24471;&#26356;&#21152;&#23454;&#29992;&#21644;&#21487;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#25512;&#29702;&#23545;&#20110;&#21363;&#20351;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35828;&#20063;&#26159;&#19968;&#39033;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#20351;LLMs&#26356;&#21152;&#23454;&#29992;&#21644;&#21487;&#35299;&#20915;&#27492;&#20219;&#21153;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#24037;&#20855;&#22686;&#24378;&#22411;&#31185;&#23398;&#25512;&#29702;&#30340;&#26032;&#20219;&#21153;&#35774;&#32622;&#12290;&#36825;&#31181;&#35774;&#32622;&#36890;&#36807;&#20026;LLMs&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;&#24037;&#20855;&#38598;&#65292;&#23558;&#37325;&#28857;&#20174;&#36861;&#27714;&#20840;&#30693;&#38382;&#39064;&#27714;&#35299;&#22120;&#36716;&#21464;&#20026;&#29087;&#32451;&#20351;&#29992;&#24037;&#20855;&#30340;&#20154;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#31181;&#35774;&#32622;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;MathFunc&#30340;&#24037;&#20855;&#22686;&#24378;&#22411;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#28085;&#30422;&#20102;&#36229;&#36807;30,000&#20010;&#26679;&#26412;&#21644;&#22823;&#32422;6,000&#20010;&#24037;&#20855;&#12290;&#22522;&#20110;MathFunc&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;SciAgent&#65292;&#29992;&#20110;&#26816;&#32034;&#12289;&#29702;&#35299;&#65292;&#20197;&#21450;&#24517;&#35201;&#26102;&#20351;&#29992;&#24037;&#20855;&#36827;&#34892;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;SciToolBench&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20116;&#20010;&#31185;&#23398;&#39046;&#22495;&#65292;&#20197;&#35780;&#20272;LLMs&#22312;&#24037;&#20855;&#36741;&#21161;&#19979;&#30340;&#33021;&#21147;&#12290;&#23545;SciToolBench&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;SciAgent&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;SciAgent-Mistral-7B&#36229;&#36807;&#20102;&#20854;&#20182;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11451v1 Announce Type: cross  Abstract: Scientific reasoning poses an excessive challenge for even the most advanced Large Language Models (LLMs). To make this task more practical and solvable for LLMs, we introduce a new task setting named tool-augmented scientific reasoning. This setting supplements LLMs with scalable toolsets, and shifts the focus from pursuing an omniscient problem solver to a proficient tool-user. To facilitate the research of such setting, we construct a tool-augmented training corpus named MathFunc which encompasses over 30,000 samples and roughly 6,000 tools. Building on MathFunc, we develop SciAgent to retrieve, understand and, if necessary, use tools for scientific problem solving. Additionally, we craft a benchmark, SciToolBench, spanning five scientific domains to evaluate LLMs' abilities with tool assistance. Extensive experiments on SciToolBench confirm the effectiveness of SciAgent. Notably, SciAgent-Mistral-7B surpasses other LLMs with the sa
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;LLM&#22686;&#24378;&#35821;&#20041;&#20998;&#26512;&#65292;&#24320;&#21457;&#20102;&#29992;&#20110;&#25991;&#26412;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26694;&#26550;&#65292;&#21487;&#26174;&#33879;&#25913;&#21892;&#25991;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#35780;&#20272;&#65292;&#24182;&#21487;&#25193;&#23637;&#21040;&#20854;&#20182;&#19987;&#19994;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2402.11398</link><description>&lt;p&gt;
&#22312;&#27604;&#36739;&#20043;&#21069;&#36827;&#34892;&#25512;&#29702;&#65306;LLM&#22686;&#24378;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#24230;&#37327;&#29992;&#20110;&#39046;&#22495;&#19987;&#38376;&#25991;&#26412;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Reasoning before Comparison: LLM-Enhanced Semantic Similarity Metrics for Domain Specialized Text Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11398
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;LLM&#22686;&#24378;&#35821;&#20041;&#20998;&#26512;&#65292;&#24320;&#21457;&#20102;&#29992;&#20110;&#25991;&#26412;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26694;&#26550;&#65292;&#21487;&#26174;&#33879;&#25913;&#21892;&#25991;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#35780;&#20272;&#65292;&#24182;&#21487;&#25193;&#23637;&#21040;&#20854;&#20182;&#19987;&#19994;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#26469;&#22686;&#24378;&#35821;&#20041;&#20998;&#26512;&#65292;&#20026;&#25991;&#26412;&#24320;&#21457;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#35299;&#20915;&#20256;&#32479;&#26080;&#30417;&#30563;NLP&#24230;&#37327;&#65288;&#22914;ROUGE&#21644;BLEU&#65289;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20854;&#20013;LLM&#65288;&#20363;&#22914;GPT-4&#65289;&#29992;&#20110;&#38646;&#26679;&#26412;&#25991;&#26412;&#35782;&#21035;&#21644;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#26631;&#31614;&#29983;&#25104;&#65292;&#22312;&#37027;&#37324;&#36825;&#20123;&#26631;&#31614;&#28982;&#21518;&#34987;&#29992;&#20316;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#24230;&#37327;&#12290;&#36890;&#36807;&#22312;MIMIC&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#21457;&#29616;GPT-4&#29983;&#25104;&#30340;&#26631;&#31614;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#35821;&#20041;&#30456;&#20284;&#24230;&#35780;&#20272;&#65292;&#24471;&#20998;&#26356;&#25509;&#36817;&#20020;&#24202;&#23454;&#38469;&#24773;&#20917;&#27604;&#20256;&#32479;NLP&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23637;&#31034;&#20102;&#20351;&#29992;LLM&#36827;&#34892;&#39640;&#24230;&#19987;&#19994;&#39046;&#22495;&#30340;&#25991;&#26412;&#25968;&#25454;&#30340;&#35821;&#20041;&#20998;&#26512;&#30340;&#21487;&#33021;&#24615;&#65292;&#20855;&#26377;&#21322;&#23450;&#37327;&#25512;&#29702;&#32467;&#26524;&#12290;&#34429;&#28982;&#35813;&#26694;&#26550;&#38024;&#23545;&#25918;&#23556;&#23398;&#25253;&#21578;&#30456;&#20284;&#24615;&#20998;&#26512;&#36827;&#34892;&#20102;&#23454;&#26045;&#65292;&#20294;&#20854;&#27010;&#24565;&#21487;&#20197;&#25193;&#23637;&#21040;&#20854;&#20182;&#19987;&#38376;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11398v1 Announce Type: cross  Abstract: In this study, we leverage LLM to enhance the semantic analysis and develop similarity metrics for texts, addressing the limitations of traditional unsupervised NLP metrics like ROUGE and BLEU. We develop a framework where LLMs such as GPT-4 are employed for zero-shot text identification and label generation for radiology reports, where the labels are then used as measurements for text similarity. By testing the proposed framework on the MIMIC data, we find that GPT-4 generated labels can significantly improve the semantic similarity assessment, with scores more closely aligned with clinical ground truth than traditional NLP metrics. Our work demonstrates the possibility of conducting semantic analysis of the text data using semi-quantitative reasoning results by the LLMs for highly specialized domains. While the framework is implemented for radiology report similarity analysis, its concept can be extended to other specialized domains 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#25991;&#26412;&#24341;&#23548;&#30340;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#26694;&#26550;CFT-CLIP&#65292;&#29992;&#20110;&#22686;&#24378;&#26032;&#38395;&#25991;&#26412;&#21644;&#32553;&#30053;&#22270;&#20043;&#38388;&#30340;&#23545;&#27604;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.11159</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#25991;&#26412;&#24341;&#23548;&#30340;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#26469;&#29702;&#35299;&#26032;&#38395;&#32553;&#30053;&#22270;&#30340;&#20195;&#34920;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding News Thumbnail Representativeness by Counterfactual Text-Guided Contrastive Language-Image Pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11159
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#25991;&#26412;&#24341;&#23548;&#30340;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#26694;&#26550;CFT-CLIP&#65292;&#29992;&#20110;&#22686;&#24378;&#26032;&#38395;&#25991;&#26412;&#21644;&#32553;&#30053;&#22270;&#20043;&#38388;&#30340;&#23545;&#27604;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#29702;&#35299;&#26032;&#38395;&#32553;&#30053;&#22270;&#30340;&#20195;&#34920;&#24615;&#36825;&#19968;&#20851;&#38190;&#25361;&#25112;&#65292;&#36825;&#20123;&#32553;&#30053;&#22270;&#36890;&#24120;&#22312;&#25991;&#31456;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#20256;&#25773;&#26102;&#20316;&#20026;&#35835;&#32773;&#30340;&#31532;&#19968;&#20010;&#35270;&#35273;&#21442;&#19982;&#12290;&#25105;&#20204;&#20851;&#27880;&#26032;&#38395;&#22270;&#20687;&#26159;&#21542;&#20195;&#34920;&#26032;&#38395;&#25991;&#26412;&#20013;&#35752;&#35770;&#30340;&#20027;&#35201;&#20027;&#39064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#26032;&#38395;&#32553;&#30053;&#22270;&#21644;&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;\textsc{NewsTT}&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20363;&#22914;CLIP&#21644;BLIP-2&#36825;&#26679;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#30001;&#20110;&#26032;&#38395;&#20027;&#39064;&#32463;&#24120;&#28041;&#21450;&#21629;&#21517;&#23454;&#20307;&#25110;&#19987;&#26377;&#21517;&#35789;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#32570;&#20047;&#21305;&#37197;&#20854;&#35270;&#35273;&#21644;&#25991;&#26412;&#22806;&#35266;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CFT-CLIP&#65292;&#19968;&#20010;&#21453;&#20107;&#23454;&#25991;&#26412;&#24341;&#23548;&#30340;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11159v1 Announce Type: new  Abstract: This paper delves into the critical challenge of understanding the representativeness of news thumbnail images, which often serve as the first visual engagement for readers when an article is disseminated on social media. We focus on whether a news image represents the main subject discussed in the news text. To serve the challenge, we introduce \textsc{NewsTT}, a manually annotated dataset of news thumbnail image and text pairs. We found that pretrained vision and language models, such as CLIP and BLIP-2, struggle with this task. Since news subjects frequently involve named entities or proper nouns, a pretrained model could not have the ability to match its visual and textual appearances. To fill the gap, we propose CFT-CLIP, a counterfactual text-guided contrastive language-image pretraining framework. We hypothesize that learning to contrast news text with its counterfactual, of which named entities are replaced, can enhance the cross
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#24490;&#29615;&#35760;&#24518;&#22686;&#24378;&#23545; GPT-2 &#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#38271;&#36798; 1000 &#19975;&#20010;&#20803;&#32032;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#22788;&#29702;&#26368;&#38271;&#36755;&#20837;&#30340;&#24320;&#25918;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#38271;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.10790</link><description>&lt;p&gt;
&#22312;&#19968;&#20010; 1000 &#19975;&#26681;&#33609;&#22427;&#20013;&#23547;&#25214;&#38024;&#65306;&#24490;&#29615;&#35760;&#24518;&#25214;&#21040;&#20102;&#35821;&#35328;&#27169;&#22411;&#19981;&#25797;&#38271;&#30340;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10790
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#24490;&#29615;&#35760;&#24518;&#22686;&#24378;&#23545; GPT-2 &#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#38271;&#36798; 1000 &#19975;&#20010;&#20803;&#32032;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#22788;&#29702;&#26368;&#38271;&#36755;&#20837;&#30340;&#24320;&#25918;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#38271;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335; Transformer &#27169;&#22411;&#22788;&#29702;&#38271;&#25991;&#26723;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35780;&#20272;&#19981;&#21516;&#26041;&#27861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; BABILong&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#22312;&#25552;&#21462;&#21644;&#22788;&#29702;&#24191;&#27867;&#25991;&#26412;&#20013;&#20998;&#24067;&#24335;&#20107;&#23454;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#21253;&#25324; GPT-4 &#21644; RAG &#30340;&#22522;&#20934;&#65292;&#32467;&#26524;&#26174;&#31034;&#24120;&#35265;&#26041;&#27861;&#20165;&#36866;&#29992;&#20110;&#26368;&#22810; $10^4$ &#20010;&#20803;&#32032;&#30340;&#24207;&#21015;&#12290;&#30456;&#21453;&#65292;&#36890;&#36807;&#20351;&#29992;&#24490;&#29615;&#35760;&#24518;&#22686;&#24378;&#23545; GPT-2 &#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#28041;&#21450;&#26368;&#22810; $10^7$ &#20010;&#20803;&#32032;&#30340;&#20219;&#21153;&#12290;&#36825;&#19968;&#25104;&#23601;&#26631;&#24535;&#30528;&#36804;&#20170;&#20026;&#27490;&#20219;&#20309;&#24320;&#28304;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22788;&#29702;&#30340;&#26368;&#38271;&#36755;&#20837;&#65292;&#26174;&#31034;&#20102;&#23545;&#38271;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10790v1 Announce Type: cross  Abstract: This paper addresses the challenge of processing long documents using generative transformer models. To evaluate different approaches, we introduce BABILong, a new benchmark designed to assess model capabilities in extracting and processing distributed facts within extensive texts. Our evaluation, which includes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to $10^4$ elements. In contrast, fine-tuning GPT-2 with recurrent memory augmentations enables it to handle tasks involving up to $10^7$ elements. This achievement marks a substantial leap, as it is by far the longest input processed by any open neural network model to date, demonstrating a significant improvement in the processing capabilities for long sequences.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#31038;&#20250;&#22240;&#32032;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#25351;&#26631;$LSS_{\beta}$&#65292;&#24182;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#20108;&#20803;&#27861;&#24459;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20197;&#21450;&#22312;&#21360;&#24230;&#31038;&#20250;&#21508;&#31181;&#19981;&#24179;&#31561;&#26041;&#38754;&#30340;&#20844;&#24179;&#24615;&#23637;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.10567</link><description>&lt;p&gt;
&#22312;InSaAF&#20013;&#34701;&#20837;&#23433;&#20840;&#24615;&#65292;&#36890;&#36807;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615; | LLM&#26159;&#21542;&#24050;&#32463;&#20934;&#22791;&#22909;&#36827;&#20837;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#65311;
&lt;/p&gt;
&lt;p&gt;
InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#31038;&#20250;&#22240;&#32032;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#25351;&#26631;$LSS_{\beta}$&#65292;&#24182;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#20108;&#20803;&#27861;&#24459;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20197;&#21450;&#22312;&#21360;&#24230;&#31038;&#20250;&#21508;&#31181;&#19981;&#24179;&#31561;&#26041;&#38754;&#30340;&#20844;&#24179;&#24615;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#25216;&#26415;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#23548;&#33268;&#25552;&#20986;&#20102;&#20247;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25191;&#34892;&#27861;&#24459;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#20174;&#39044;&#27979;&#21028;&#20915;&#21040;&#29983;&#25104;&#25688;&#35201;&#12290;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#24050;&#32463;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#24182;&#23637;&#31034;&#31038;&#20250;&#20559;&#35265;&#65292;&#24182;&#20570;&#20986;&#19981;&#20844;&#24179;&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#24403;&#28041;&#21450;&#31038;&#20250;&#22240;&#32032;&#26102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;$\beta$-&#21152;&#26435;&#30340;$\textit{&#27861;&#24459;&#23433;&#20840;&#20998;&#25968;($LSS_{\beta}$)}$&#65292;&#23558;LLM&#30340;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#20004;&#20010;&#26041;&#38754;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;LLM&#22312;$\textit{&#20108;&#20803;&#27861;&#24459;&#25512;&#29702;}$&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20197;&#21450;&#20854;&#22312;&#21360;&#24230;&#31038;&#20250;&#21508;&#31181;&#19981;&#24179;&#31561;&#26041;&#38754;&#30340;&#20844;&#24179;&#23637;&#31034;&#26469;&#35780;&#20272;LLMs&#30340;&#23433;&#20840;&#24615;&#12290;LLaMA&#21644;LLaMA--2&#27169;&#22411;&#30340;&#20219;&#21153;&#34920;&#29616;&#21644;&#20844;&#24179;&#24471;&#20998;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10567v1 Announce Type: cross  Abstract: Recent advancements in language technology and Artificial Intelligence have resulted in numerous Language Models being proposed to perform various tasks in the legal domain ranging from predicting judgments to generating summaries. Despite their immense potential, these models have been proven to learn and exhibit societal biases and make unfair predictions. In this study, we explore the ability of Large Language Models (LLMs) to perform legal tasks in the Indian landscape when social factors are involved. We present a novel metric, $\beta$-weighted $\textit{Legal Safety Score ($LSS_{\beta}$)}$, which encapsulates both the fairness and accuracy aspects of the LLM. We assess LLMs' safety by considering its performance in the $\textit{Binary Statutory Reasoning}$ task and its fairness exhibition with respect to various axes of disparities in the Indian society. Task performance and fairness scores of LLaMA and LLaMA--2 models indicate th
&lt;/p&gt;</description></item><item><title>AI&#21307;&#38498;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#23454;&#26102;&#20132;&#20114;&#24335;&#35786;&#26029;&#29615;&#22659;&#65292;&#36890;&#36807;&#19982;LLMs&#30340;&#20132;&#20114;&#35780;&#20272;&#21644;&#21327;&#20316;&#65292;&#25552;&#39640;&#20020;&#24202;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09742</link><description>&lt;p&gt;
AI&#21307;&#38498;&#65306;&#29992;&#20110;&#20020;&#24202;&#35786;&#26029;&#30340;LLMs&#20316;&#20026;&#23454;&#20064;&#21307;&#29983;&#30340;&#20132;&#20114;&#24335;&#35780;&#20272;&#21644;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern Doctors for Clinical Diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09742
&lt;/p&gt;
&lt;p&gt;
AI&#21307;&#38498;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#23454;&#26102;&#20132;&#20114;&#24335;&#35786;&#26029;&#29615;&#22659;&#65292;&#36890;&#36807;&#19982;LLMs&#30340;&#20132;&#20114;&#35780;&#20272;&#21644;&#21327;&#20316;&#65292;&#25552;&#39640;&#20020;&#24202;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24212;&#29992;&#26631;&#24535;&#30528;&#37325;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#24212;&#29992;&#20027;&#35201;&#23616;&#38480;&#20110;&#36776;&#21035;&#21644;&#38382;&#31572;&#20219;&#21153;&#65292;&#27809;&#26377;&#20805;&#20998;&#21457;&#25381;&#20854;&#20132;&#20114;&#28508;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;AI&#21307;&#38498;&#65292;&#19968;&#20010;&#26088;&#22312;&#26500;&#24314;&#23454;&#26102;&#20132;&#20114;&#24335;&#35786;&#26029;&#29615;&#22659;&#30340;&#26694;&#26550;&#12290;&#20026;&#20102;&#27169;&#25311;&#36807;&#31243;&#65292;&#25105;&#20204;&#25910;&#38598;&#39640;&#36136;&#37327;&#30340;&#21307;&#30103;&#35760;&#24405;&#65292;&#21019;&#24314;&#20102;&#24739;&#32773;&#12289;&#26816;&#26597;&#32773;&#21644;&#21307;&#30103;&#20027;&#20219;&#20195;&#29702;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;AI&#21307;&#38498;&#36827;&#34892;LLMs&#30340;&#20132;&#20114;&#35780;&#20272;&#21644;&#21327;&#20316;&#12290;&#21021;&#22987;&#38454;&#27573;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#35270;&#22270;&#21307;&#23398;&#35780;&#20272;&#65288;MVME&#65289;&#22522;&#20934;&#65292;&#20854;&#20013;&#21508;&#31181;LLMs&#20316;&#20026;&#23454;&#20064;&#21307;&#29983;&#36827;&#34892;&#20132;&#20114;&#24335;&#35786;&#26029;&#12290;&#38543;&#21518;&#65292;&#20026;&#20102;&#25552;&#39640;&#35786;&#26029;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21327;&#20316;&#26426;&#21046;&#65292;&#28041;&#21450;&#21307;&#30103;&#20027;&#20219;&#30340;&#30417;&#30563;&#19979;&#30340;&#36845;&#20195;&#35752;&#35770;&#21644;&#20105;&#35758;&#35299;&#20915;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09742v1 Announce Type: new  Abstract: The incorporation of Large Language Models (LLMs) in healthcare marks a significant advancement. However, the application has predominantly been limited to discriminative and question-answering tasks, which does not fully leverage their interactive potential. To address this limitation, our paper presents AI Hospital, a framework designed to build a real-time interactive diagnosis environment. To simulate the procedure, we collect high-quality medical records to create patient, examiner, and medical director agents. AI Hospital is then utilized for the interactive evaluation and collaboration of LLMs. Initially, we create a Multi-View Medical Evaluation (MVME) benchmark where various LLMs serve as intern doctors for interactive diagnosis. Subsequently, to improve diagnostic accuracy, we introduce a collaborative mechanism that involves iterative discussions and a dispute resolution process under the supervision of the medical director. I
&lt;/p&gt;</description></item><item><title>&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#21487;&#20197;&#25913;&#21892;&#32467;&#26500;&#29702;&#35299;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08382</link><description>&lt;p&gt;
&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#22312;&#27809;&#26377;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#25913;&#21892;&#32467;&#26500;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Punctuation Restoration Improves Structure Understanding without Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08382
&lt;/p&gt;
&lt;p&gt;
&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#21487;&#20197;&#25913;&#21892;&#32467;&#26500;&#29702;&#35299;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#65292;&#22914;&#35821;&#35328;&#24314;&#27169;&#21644;&#21435;&#22122;&#31561;&#65292;&#22312;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#25191;&#34892;&#20174;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21040;&#20250;&#35805;&#20219;&#21153;&#30340;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#25429;&#25417;&#25991;&#26412;&#30340;&#21477;&#27861;&#25110;&#35821;&#20041;&#32467;&#26500;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#28982;&#33853;&#21518;&#12290;&#25105;&#20204;&#20551;&#35774;&#65292;&#35821;&#35328;&#24615;&#33021;&#21644;&#26426;&#22120;&#33021;&#21147;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#24402;&#22240;&#20110;&#24403;&#21069;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#26410;&#33021;&#20805;&#20998;&#20256;&#36882;&#35821;&#35328;&#32467;&#26500;&#30693;&#35782;&#32473;&#35745;&#31639;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#23545;&#32467;&#26500;&#30456;&#20851;&#20219;&#21153;&#30340;&#20869;&#37096;&#21644;&#22806;&#37096;&#34920;&#29616;&#30340;&#25913;&#21892;&#65292;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#24320;&#25918;&#24335;&#20449;&#24687;&#25552;&#21462;&#12289;&#20998;&#22359;&#21644;&#35789;&#24615;&#26631;&#27880;&#12290;&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#21487;&#20197;&#25913;&#21892;&#32467;&#26500;&#29702;&#35299;&#24182;&#20135;&#29983;&#26356;&#21152;&#40065;&#26834;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised learning objectives like language modeling and de-noising constitute a significant part in producing pre-trained models that perform various downstream applications from natural language understanding to conversational tasks. However, despite impressive conversational capabilities of recent large language model, their abilities to capture syntactic or semantic structure within text lag behind. We hypothesize that the mismatch between linguistic performance and competence in machines is attributable to insufficient transfer of linguistic structure knowledge to computational systems with currently popular pre-training objectives. We show that punctuation restoration transfers to improvements in in- and out-of-distribution performance on structure-related tasks like named entity recognition, open information extraction, chunking, and part-of-speech tagging. Punctuation restoration is an effective learning objective that can improve structure understanding and yield a more rob
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;BiasMedQA&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#20219;&#21153;&#20013;LLMs&#30340;&#35748;&#30693;&#20559;&#35265;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#21457;&#29616;LLMs&#22312;&#38754;&#23545;&#21253;&#21547;&#35748;&#30693;&#20559;&#35265;&#30340;&#20020;&#24202;&#38382;&#39064;&#26102;&#65292;&#20854;&#22238;&#31572;&#30340;&#20934;&#30830;&#24615;&#26126;&#26174;&#38477;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.08113</link><description>&lt;p&gt;
&#35299;&#20915;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35748;&#30693;&#20559;&#35265;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing cognitive bias in medical language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;BiasMedQA&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#20219;&#21153;&#20013;LLMs&#30340;&#35748;&#30693;&#20559;&#35265;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#21457;&#29616;LLMs&#22312;&#38754;&#23545;&#21253;&#21547;&#35748;&#30693;&#20559;&#35265;&#30340;&#20020;&#24202;&#38382;&#39064;&#26102;&#65292;&#20854;&#22238;&#31572;&#30340;&#20934;&#30830;&#24615;&#26126;&#26174;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25972;&#21512;&#21040;&#21307;&#23398;&#39046;&#22495;&#24050;&#32463;&#24341;&#36215;&#20102;&#37325;&#22823;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#27169;&#25311;&#20020;&#24202;&#20915;&#31574;&#22330;&#26223;&#20013;&#30340;&#20934;&#30830;&#24615;&#24456;&#26377;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#20020;&#24202;&#20915;&#31574;&#27604;&#27169;&#25311;&#26356;&#22797;&#26434;&#65292;&#22240;&#20026;&#21307;&#29983;&#30340;&#20915;&#31574;&#21463;&#21040;&#35768;&#22810;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#35748;&#30693;&#20559;&#35265;&#30340;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#38754;&#23545;&#21253;&#21547;&#35748;&#30693;&#20559;&#35265;&#30340;&#20020;&#24202;&#38382;&#39064;&#26102;&#65292;&#19982;&#19981;&#21253;&#21547;&#36825;&#20123;&#20559;&#35265;&#30340;&#38382;&#39064;&#30456;&#27604;&#65292;&#20854;&#22238;&#31572;&#30340;&#20934;&#30830;&#24615;&#20250;&#26126;&#26174;&#38477;&#20302;&#65292;&#36825;&#19968;&#38382;&#39064;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#30340;&#20551;&#35774;&#35748;&#20026;&#65292;&#24403;LLMs&#38754;&#23545;&#21253;&#21547;&#35748;&#30693;&#20559;&#35265;&#30340;&#20020;&#24202;&#38382;&#39064;&#26102;&#65292;&#19982;&#19981;&#21253;&#21547;&#36825;&#20123;&#20559;&#35265;&#30340;&#38382;&#39064;&#30456;&#27604;&#65292;&#20854;&#22238;&#31572;&#30340;&#20934;&#30830;&#24615;&#20250;&#26126;&#26174;&#38477;&#20302;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;BiasMedQA&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#21307;&#23398;&#20219;&#21153;&#20013;&#30340;&#35748;&#30693;&#20559;&#35265;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;&#20351;&#29992;BiasMedQA&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20845;&#20010;LLMs&#65292;&#20998;&#21035;&#26159;GPT-4&#12289;Mixtral-8x70B&#12289;GPT-3.5&#12289;PaLM-2&#12289;Llama 2 70B-chat&#21644;&#21307;&#23398;&#19987;&#19994;&#30340;PMC Llama 13B&#12290;&#25105;&#20204;&#22312;127&#20010;&#20020;&#24202;&#38382;&#39064;&#19978;&#27979;&#35797;&#20102;&#36825;&#20123;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of large language models (LLMs) into the medical field has gained significant attention due to their promising accuracy in simulated clinical decision-making settings. However, clinical decision-making is more complex than simulations because physicians' decisions are shaped by many factors, including the presence of cognitive bias. However, the degree to which LLMs are susceptible to the same cognitive biases that affect human clinicians remains unexplored. Our hypothesis posits that when LLMs are confronted with clinical questions containing cognitive biases, they will yield significantly less accurate responses compared to the same questions presented without such biases.In this study, we developed BiasMedQA, a novel benchmark for evaluating cognitive biases in LLMs applied to medical tasks. Using BiasMedQA we evaluated six LLMs, namely GPT-4, Mixtral-8x70B, GPT-3.5, PaLM-2, Llama 2 70B-chat, and the medically specialized PMC Llama 13B. We tested these models on 1,27
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#38646;&#38454;&#26799;&#24230;&#26469;&#36991;&#20813;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.07818</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#38646;&#38454;&#26799;&#24230;&#26469;&#36991;&#20813;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#36827;&#34892;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#24191;&#27867;&#25509;&#21463;&#30340;&#33539;&#20363;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#26222;&#21450;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20197;&#20445;&#25252;&#29305;&#23450;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#38544;&#31169;&#12290;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#30340;&#35774;&#35745;&#26680;&#24515;&#26159;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#36798;&#21040;&#28385;&#24847;&#30340;&#26435;&#34913;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;DP-SGD&#30340;&#21019;&#26032;&#24615;&#24037;&#20316;&#12290;&#23613;&#31649;&#23558;DP-SGD&#30340;&#21487;&#25193;&#23637;&#24615;&#25512;&#21040;&#20102;&#26497;&#38480;&#65292;&#20294;&#22522;&#20110;DP-SGD&#30340;&#24494;&#35843;&#26041;&#27861;&#19981;&#24184;&#22320;&#21463;&#21040;&#20102;SGD&#22266;&#26377;&#20302;&#25928;&#29575;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;DP&#38646;&#38454;&#26041;&#27861;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#30340;&#28508;&#21147;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#26356;&#39640;&#25928;&#30340;&#38646;&#38454;&#26799;&#24230;&#26469;&#36817;&#20284;&#26799;&#24230;&#65292;&#36991;&#20813;&#20102;SGD&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#12290;&#19982;&#23558;&#38646;&#38454;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#36827;&#34892;&#22788;&#29702;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21106;&#25509;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20197;&#38750;&#24120;&#25509;&#36817;&#30340;&#26041;&#24335;&#27169;&#25311;DP-SGD&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#28982;&#21518;&#21033;&#29992;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#26469;&#36817;&#20284;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuning on task-specific datasets is a widely-embraced paradigm of harnessing the powerful capability of pretrained LLMs for various downstream tasks. Due to the popularity of LLMs finetuning and its accompanying privacy concerns, differentially private (DP) finetuning of pretrained LLMs has garnered increasing attention to safeguarding the privacy of task-specific datasets. Lying at the design core of DP LLM finetuning methods is the satisfactory tradeoff between privacy, utility, and scalability. Most existing methods build upon the seminal work of DP-SGD. Despite pushing the scalability of DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately limited by the inherent inefficiency of SGD. In this paper, we investigate the potential of DP zeroth-order methods for LLM pretraining, which avoids the scalability bottleneck of SGD by approximating the gradient with the more efficient zeroth-order gradient. Rather than treating the zeroth-order method as a drop-in replace
&lt;/p&gt;</description></item><item><title>DeAL&#26159;&#19968;&#20010;&#20801;&#35768;&#29992;&#25143;&#33258;&#23450;&#20041;&#22870;&#21169;&#20989;&#25968;&#24182;&#23454;&#29616;&#35299;&#30721;&#26102;&#23545;&#40784;LLMs&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.06147</link><description>&lt;p&gt;
DeAL&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#26102;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
DeAL: Decoding-time Alignment for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06147
&lt;/p&gt;
&lt;p&gt;
DeAL&#26159;&#19968;&#20010;&#20801;&#35768;&#29992;&#25143;&#33258;&#23450;&#20041;&#22870;&#21169;&#20989;&#25968;&#24182;&#23454;&#29616;&#35299;&#30721;&#26102;&#23545;&#40784;LLMs&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29616;&#22312;&#26399;&#26395;&#29983;&#25104;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#20869;&#23481;&#12290;&#30446;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#35757;&#32451;&#26102;&#38388;&#23545;&#40784;&#19978;&#65292;&#36890;&#36807;&#35832;&#22914;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#31561;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#36825;&#20123;&#26041;&#27861;&#26159;&#21542;&#26377;&#25928;&#22320;&#25945;&#23548;&#27169;&#22411;&#23545;&#40784;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#26080;&#27861;&#25972;&#21512;&#22810;&#20010;&#33258;&#23450;&#20041;&#22870;&#21169;&#21644;&#20381;&#36182;&#27169;&#22411;&#24320;&#21457;&#32773;&#23545;&#36890;&#29992;&#21644;&#38745;&#24577;&#21407;&#21017;&#30340;&#29702;&#35299;&#26159;&#20027;&#35201;&#23616;&#38480;&#12290;&#20854;&#27425;&#65292;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#27531;&#30041;&#24046;&#36317;&#20197;&#21450;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#20063;&#20540;&#24471;&#36136;&#30097;&#65288;&#20363;&#22914;&#65292;&#21363;&#20351;&#22312;&#23433;&#20840;&#35757;&#32451;&#21518;&#20173;&#28982;&#23481;&#26131;&#34987;&#36234;&#29425;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeAL&#65292;&#19968;&#20010;&#20801;&#35768;&#29992;&#25143;&#33258;&#23450;&#20041;&#22870;&#21169;&#20989;&#25968;&#24182;&#23454;&#29616;&#35299;&#30721;&#26102;&#23545;&#40784;LLMs&#65288;DeAL&#65289;&#30340;&#26694;&#26550;&#12290;&#26680;&#24515;&#24605;&#24819;&#22312;&#20110;&#23558;&#35299;&#30721;&#35270;&#20026;&#19968;&#20010;&#21551;&#21457;&#24335;&#24341;&#23548;&#30340;&#25628;&#32034;&#36807;&#31243;&#65292;&#24182;&#20419;&#20351;&#20351;&#29992;&#21508;&#31181;&#23545;&#40784;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20197;&#32534;&#31243;&#32422;&#26463;&#20026;&#20363;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are nowadays expected to generate content aligned with human preferences. Current work focuses on alignment at model training time, through techniques such as Reinforcement Learning with Human Feedback (RLHF). However, it is unclear if such methods are an effective choice to teach alignment objectives to the model. First, the inability to incorporate multiple, custom rewards and reliance on a model developer's view of universal and static principles are key limitations. Second, the residual gaps in model training and the reliability of such approaches are also questionable (e.g. susceptibility to jail-breaking even after safety training). To address these, we propose DeAL, a framework that allows the user to customize reward functions and enables Decoding-time Alignment of LLMs (DeAL). At its core, we view decoding as a heuristic-guided search process and facilitate the use of a wide variety of alignment objectives. Our experiments with programmatic constra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#40657;&#30418;&#23376;&#21477;&#32423;&#25915;&#20987;&#20013;&#21033;&#29992;&#31867;&#21035;&#27010;&#29575;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#36827;&#34892;&#25915;&#20987;&#12290;&#36890;&#36807;&#19982;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.02695</link><description>&lt;p&gt;
&#21033;&#29992;&#31867;&#21035;&#27010;&#29575;&#36827;&#34892;&#40657;&#30418;&#23376;&#21477;&#32423;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Exploiting Class Probabilities for Black-box Sentence-level Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02695
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#40657;&#30418;&#23376;&#21477;&#32423;&#25915;&#20987;&#20013;&#21033;&#29992;&#31867;&#21035;&#27010;&#29575;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#36827;&#34892;&#25915;&#20987;&#12290;&#36890;&#36807;&#19982;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#32423;&#25915;&#20987;&#26159;&#38024;&#23545;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#24615;&#21477;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#36825;&#20123;&#21477;&#23376;&#19982;&#27491;&#30830;&#20998;&#31867;&#30340;&#21477;&#23376;&#21516;&#20041;&#65292;&#20294;&#34987;&#20998;&#31867;&#22120;&#38169;&#35823;&#22320;&#20998;&#31867;&#12290;&#22312;&#40657;&#30418;&#35774;&#32622;&#19979;&#65292;&#20998;&#31867;&#22120;&#21482;&#33021;&#36890;&#36807;&#23545;&#26597;&#35810;&#36755;&#20837;&#30340;&#21453;&#39304;&#36827;&#34892;&#35775;&#38382;&#65292;&#36825;&#20027;&#35201;&#20197;&#31867;&#21035;&#27010;&#29575;&#30340;&#24418;&#24335;&#25552;&#20379;&#12290;&#23613;&#31649;&#21033;&#29992;&#31867;&#21035;&#27010;&#29575;&#21487;&#20197;&#33719;&#24471;&#26356;&#24378;&#22823;&#30340;&#25915;&#20987;&#25928;&#26524;&#65292;&#20294;&#30001;&#20110;&#22312;&#21477;&#32423;&#25915;&#20987;&#20013;&#20351;&#29992;&#31867;&#21035;&#27010;&#29575;&#23384;&#22312;&#25361;&#25112;&#65292;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#35201;&#20040;&#19981;&#20351;&#29992;&#21453;&#39304;&#65292;&#35201;&#20040;&#20165;&#20351;&#29992;&#31867;&#21035;&#26631;&#31614;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#31867;&#21035;&#27010;&#29575;&#36827;&#34892;&#40657;&#30418;&#21477;&#32423;&#25915;&#20987;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#19978;&#20351;&#29992;&#31867;&#21035;&#27010;&#29575;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#40657;&#30418;&#21477;&#32423;&#25915;&#20987;&#20013;&#20351;&#29992;&#31867;&#21035;&#27010;&#29575;&#26159;&#21542;&#20540;&#24471;&#25110;&#21487;&#34892;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#20998;&#31867;&#22120;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23545;&#25552;&#20986;&#30340;&#25915;&#20987;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#24182;&#19982;&#22522;&#32447;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence-level attacks craft adversarial sentences that are synonymous with correctly-classified sentences but are misclassified by the text classifiers. Under the black-box setting, classifiers are only accessible through their feedback to queried inputs, which is predominately available in the form of class probabilities. Even though utilizing class probabilities results in stronger attacks, due to the challenges of using them for sentence-level attacks, existing attacks use either no feedback or only the class labels. Overcoming the challenges, we develop a novel algorithm that uses class probabilities for black-box sentence-level attacks, investigate the effectiveness of using class probabilities on the attack's success, and examine the question if it is worthy or practical to use class probabilities by black-box sentence-level attacks. We conduct extensive evaluations of the proposed attack comparing with the baselines across various classifiers and benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26725;&#26753;&#26426;&#21046;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#26725;&#26753;&#27169;&#22411;&#26469;&#20248;&#21270;&#26816;&#32034;&#22120;&#21644;LLM&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#28040;&#38500;&#20102;&#22312;RAG&#20013;&#26816;&#32034;&#22120;&#21644;LLMs&#20043;&#38388;&#30340;&#20559;&#22909;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2401.06954</link><description>&lt;p&gt;
&#28040;&#38500;&#26816;&#32034;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#20559;&#22909;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Bridging the Preference Gap between Retrievers and LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26725;&#26753;&#26426;&#21046;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#26725;&#26753;&#27169;&#22411;&#26469;&#20248;&#21270;&#26816;&#32034;&#22120;&#21644;LLM&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#28040;&#38500;&#20102;&#22312;RAG&#20013;&#26816;&#32034;&#22120;&#21644;LLMs&#20043;&#38388;&#30340;&#20559;&#22909;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#32467;&#26524;&#65292;&#32780;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26159;&#36890;&#36807;&#23450;&#20301;&#30456;&#20851;&#20449;&#24687;&#24182;&#23558;&#20854;&#25918;&#20837;LLM&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#26469;&#25552;&#39640;&#24615;&#33021;&#30340;&#26377;&#25928;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;RAG&#20013;&#65292;&#26816;&#32034;&#22120;&#21644;LLMs&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#35843;&#26597;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#23558;&#26816;&#32034;&#22120;&#21644;LLM&#35270;&#20026;&#29420;&#31435;&#32452;&#20214;&#65292;&#24182;&#22312;&#26816;&#32034;&#20154;&#24615;&#21270;&#20449;&#24687;&#21644;&#32452;&#35013;LLM&#21451;&#22909;&#19978;&#19979;&#25991;&#20043;&#38388;&#30041;&#19979;&#20102;&#24046;&#36317;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26725;&#26753;&#26426;&#21046;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#26816;&#32034;&#22120;&#22312;RAG&#29615;&#22659;&#20013;&#30340;&#25490;&#21517;&#21644;&#36873;&#25321;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#38142;&#25509;&#22312;&#19968;&#36215;&#26469;&#35757;&#32451;&#19968;&#20010;&#26725;&#26753;&#27169;&#22411;&#65292;&#20248;&#21270;&#26816;&#32034;&#22120;&#21644;LLM&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38382;&#31572;&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.06954v2 Announce Type: replace  Abstract: Large Language Models (LLMs) have demonstrated superior results across a wide range of tasks, and Retrieval-augmented Generation (RAG) is an effective way to enhance the performance by locating relevant information and placing it into the context window of the LLM. However, the relationship between retrievers and LLMs in a RAG is still under-investigated. Most existing work treats the retriever and the LLM as independent components and leaves a gap between retrieving human-"friendly" information and assembling a LLM-"friendly" context. In this work, we examine a novel bridge mechanism. We validate the ranking and selection assumptions of retrievers in the context of RAG and propose a framework that chains together supervised and reinforcement learning to train a bridge model that optimizes the connection between the retriever and the LLM. Empirical results demonstrate the effectiveness of our method in both question-answering and per
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24773;&#24863;&#24605;&#32500;&#38142;&#65288;ECoT&#65289;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20154;&#31867;&#24773;&#24863;&#26234;&#24935;&#20934;&#21017;&#23545;&#40784;&#65292;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24773;&#24863;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.06836</link><description>&lt;p&gt;
&#36890;&#36807;&#24773;&#32490;&#24605;&#32500;&#38142;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#32490;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Emotional Generation Capability of Large Language Models via Emotional Chain-of-Thought
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06836
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24773;&#24863;&#24605;&#32500;&#38142;&#65288;ECoT&#65289;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20154;&#31867;&#24773;&#24863;&#26234;&#24935;&#20934;&#21017;&#23545;&#40784;&#65292;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24773;&#24863;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24773;&#32490;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24341;&#36215;&#20102;&#30740;&#31350;&#30028;&#23545;&#25506;&#32034;&#23427;&#20204;&#22312;&#24773;&#24863;&#26234;&#33021;&#20013;&#28508;&#21147;&#30340;&#22909;&#22855;&#24515;&#12290;&#28982;&#32780;&#65292;&#24773;&#24863;&#29983;&#25104;&#20219;&#21153;&#39046;&#22495;&#20173;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#21253;&#25324;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#21644;&#24773;&#24863;&#29983;&#25104;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24773;&#24863;&#24605;&#32500;&#38142;&#65288;ECoT&#65289;&#30340;&#21363;&#25554;&#21363;&#29992;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20154;&#31867;&#24773;&#24863;&#26234;&#21147;&#25351;&#21335;&#23545;&#40784;&#26469;&#22686;&#24378;LLMs&#22312;&#21508;&#31181;&#24773;&#24863;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#35780;&#20272;ECoT&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24773;&#24863;&#29983;&#25104;&#24471;&#20998;&#65288;EGS&#65289;&#30340;&#33258;&#21160;&#21270;&#22522;&#20110;&#27169;&#22411;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;EGS&#23558;&#25096;&#23572;&#26364;&#30340;&#24773;&#32490;&#26234;&#21147;&#29702;&#35770;&#20316;&#20026;&#20154;&#31867;&#19987;&#23478;&#20849;&#35782;&#65292;&#20026;&#24773;&#24863;&#29983;&#25104;&#20219;&#21153;&#30340;&#35780;&#20272;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.06836v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have shown remarkable performance in various emotion recognition tasks, thereby piquing the research community's curiosity for exploring their potential in emotional intelligence. However, several issues in the field of emotional generation tasks remain unresolved, including human preference alignment and emotional generation assessment. In this paper, we propose the Emotional Chain-of-Thought (ECoT), a plug-and-play prompting method that enhances the performance of LLMs on various emotional generation tasks by aligning with human emotional intelligence guidelines. To assess the reliability of ECoT, we propose an automated model-based evaluation method called Emotional Generation Score (EGS). EGS incorporates Goleman's Emotional Intelligence Theory as a consensus of human experts, providing a new perspective on the evaluation of emotional generation tasks. Extensive experimental results demonstrate 
&lt;/p&gt;</description></item><item><title>&#22312;&#20020;&#24202;&#25991;&#26412;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;ClinSpEn-2022&#33521;&#35199;&#20020;&#24202;&#39046;&#22495;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#39030;&#32423;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#23567;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#39046;&#22495;&#24494;&#35843;&#20013;&#32988;&#36807;&#20854;&#20182;&#36229;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2312.07250</link><description>&lt;p&gt;
&#20020;&#24202;&#25991;&#26412;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65306;&#23545;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#32463;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Neural Machine Translation of Clinical Text: An Empirical Investigation into Multilingual Pre-Trained Language Models and Transfer-Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07250
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#25991;&#26412;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;ClinSpEn-2022&#33521;&#35199;&#20020;&#24202;&#39046;&#22495;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#39030;&#32423;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#23567;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#39046;&#22495;&#24494;&#35843;&#20013;&#32988;&#36807;&#20854;&#20182;&#36229;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#26816;&#39564;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#35821;&#35328;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22914;&#22522;&#20110;Transformer&#32467;&#26500;&#30340;&#27169;&#22411;&#65292;&#22312;&#20020;&#24202;&#25991;&#26412;&#26426;&#22120;&#32763;&#35793;&#26041;&#38754;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#35821;&#35328;&#36164;&#28304;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#22522;&#20110;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;MMPLMs&#65289;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#22312;&#20020;&#24202;&#26696;&#20363;&#65288;CC&#65289;&#12289;&#20020;&#24202;&#26415;&#35821;&#65288;CT&#65289;&#21644;&#26412;&#20307;&#27010;&#24565;&#65288;OC&#65289;&#31561;&#19977;&#20010;&#23376;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;ClinSpEn-2022&#33521;&#35199;&#20020;&#24202;&#39046;&#22495;&#25968;&#25454;&#20849;&#20139;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#39030;&#32423;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;&#19987;&#23478;&#30340;&#20154;&#24037;&#35780;&#20272;&#26174;&#31034;&#65292;&#22312;&#20020;&#24202;&#39046;&#22495;&#24494;&#35843;&#20013;&#65292;&#23567;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#26126;&#26174;&#32988;&#36807;&#20854;&#20182;&#20004;&#20010;&#36229;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#19968;&#21457;&#29616;&#22312;&#35813;&#39046;&#22495;&#20174;&#26410;&#26377;&#36807;&#25253;&#36947;&#12290;&#26368;&#21518;&#65292;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#20986;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07250v2 Announce Type: replace-cross  Abstract: We conduct investigations on clinical text machine translation by examining multilingual neural network models using deep learning such as Transformer based structures. Furthermore, to address the language resource imbalance issue, we also carry out experiments using a transfer learning methodology based on massive multilingual pre-trained language models (MMPLMs). The experimental results on three subtasks including 1) clinical case (CC), 2) clinical terminology (CT), and 3) ontological concept (OC) show that our models achieved top-level performances in the ClinSpEn-2022 shared task on English-Spanish clinical domain data. Furthermore, our expert-based human evaluations demonstrate that the small-sized pre-trained language model (PLM) won over the other two extra-large language models by a large margin, in the clinical domain fine-tuning, which finding was never reported in the field. Finally, the transfer learning method wor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tree of Attacks with Pruning (TAP)&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21482;&#38656;&#35201;&#23545;&#30446;&#26631;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#30340;&#36234;&#29425;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#26641;&#25512;&#29702;&#21644;&#20462;&#21098;&#29983;&#25104;&#20934;&#30830;&#30340;&#36234;&#29425;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2312.02119</link><description>&lt;p&gt;
&#25915;&#20987;&#26641;&#65306;&#33258;&#21160;&#30772;&#35299;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tree of Attacks: Jailbreaking Black-Box LLMs Automatically
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02119
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tree of Attacks with Pruning (TAP)&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21482;&#38656;&#35201;&#23545;&#30446;&#26631;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#30340;&#36234;&#29425;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#26641;&#25512;&#29702;&#21644;&#20462;&#21098;&#29983;&#25104;&#20934;&#30830;&#30340;&#36234;&#29425;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#22810;&#21151;&#33021;&#24615;&#65292;&#20294;&#20173;&#22312;&#29983;&#25104;&#26377;&#23475;&#12289;&#24102;&#20559;&#35265;&#21644;&#26377;&#27602;&#20869;&#23481;&#65292;&#36825;&#19968;&#28857;&#30001;&#20154;&#20026;&#35774;&#35745;&#30340;&#36234;&#29425;&#34892;&#20026;&#30340;&#26222;&#36941;&#23384;&#22312;&#24471;&#20197;&#35777;&#26126;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tree of Attacks with Pruning (TAP)&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#36234;&#29425;&#65292;&#20165;&#38656;&#35201;&#23545;&#30446;&#26631;LLM&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#12290;TAP&#21033;&#29992;LLM&#26469;&#36890;&#36807;&#24605;&#32500;&#26641;&#25512;&#29702;&#36845;&#20195;&#22320;&#20248;&#21270;&#20505;&#36873;&#65288;&#25915;&#20987;&#65289;&#25552;&#31034;&#65292;&#30452;&#21040;&#29983;&#25104;&#30340;&#25552;&#31034;&#20043;&#19968;&#36234;&#29425;&#30446;&#26631;&#12290;&#20851;&#38190;&#22312;&#20110;&#65292;&#22312;&#23558;&#25552;&#31034;&#21457;&#36865;&#32473;&#30446;&#26631;&#20043;&#21069;&#65292;TAP&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#24182;&#31227;&#38500;&#21487;&#33021;&#19981;&#20250;&#23548;&#33268;&#36234;&#29425;&#30340;&#25552;&#31034;&#12290;&#20351;&#29992;&#24605;&#32500;&#26641;&#25512;&#29702;&#20351;TAP&#33021;&#22815;&#22312;&#22823;&#37327;&#25552;&#31034;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#23548;&#33322;&#65292;&#32780;&#20462;&#21098;&#21017;&#20943;&#23569;&#20102;&#21457;&#36865;&#32473;&#30446;&#26631;&#30340;&#24635;&#26597;&#35810;&#25968;&#37327;&#12290;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;TAP&#29983;&#25104;&#30340;&#25552;&#31034;&#36234;&#29425;&#20102;&#36229;&#36807;80%&#30340;&#26368;&#20808;&#36827;LLMs&#65288;&#21253;&#25324;GPT4&#21644;GPT4-Turbo&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02119v2 Announce Type: replace-cross  Abstract: While Large Language Models (LLMs) display versatile functionality, they continue to generate harmful, biased, and toxic content, as demonstrated by the prevalence of human-designed jailbreaks. In this work, we present Tree of Attacks with Pruning (TAP), an automated method for generating jailbreaks that only requires black-box access to the target LLM. TAP utilizes an LLM to iteratively refine candidate (attack) prompts using tree-of-thought reasoning until one of the generated prompts jailbreaks the target. Crucially, before sending prompts to the target, TAP assesses them and prunes the ones unlikely to result in jailbreaks. Using tree-of-thought reasoning allows TAP to navigate a large search space of prompts and pruning reduces the total number of queries sent to the target. In empirical evaluations, we observe that TAP generates prompts that jailbreak state-of-the-art LLMs (including GPT4 and GPT4-Turbo) for more than 80%
&lt;/p&gt;</description></item><item><title>CAMRA&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#22522;&#20110;web&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#26500;&#24314;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#65292;&#36890;&#36807;&#25972;&#21512;&#32534;&#31243;&#35821;&#35328;&#30340;&#32534;&#30721;&#26041;&#27861;&#21644;AMR&#35299;&#26512;&#22120;&#27169;&#22411;&#20316;&#20026;&#21103;&#39550;&#39542;&#65292;&#26497;&#22823;&#25552;&#39640;&#20102;AMR&#27880;&#37322;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.10928</link><description>&lt;p&gt;
CAMRA&#65306;AMR&#27880;&#37322;&#30340;&#21103;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
CAMRA: Copilot for AMR Annotation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10928
&lt;/p&gt;
&lt;p&gt;
CAMRA&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#22522;&#20110;web&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#26500;&#24314;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#65292;&#36890;&#36807;&#25972;&#21512;&#32534;&#31243;&#35821;&#35328;&#30340;&#32534;&#30721;&#26041;&#27861;&#21644;AMR&#35299;&#26512;&#22120;&#27169;&#22411;&#20316;&#20026;&#21103;&#39550;&#39542;&#65292;&#26497;&#22823;&#25552;&#39640;&#20102;AMR&#27880;&#37322;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CAMRA&#65288;Copilot for AMR Annotations&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;web&#30340;&#24037;&#20855;&#65292;&#26088;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#26500;&#24314;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#12290;CAMRA&#25552;&#20379;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28145;&#23618;&#35789;&#27719;&#35821;&#20041;&#27880;&#37322;&#26041;&#27861;&#65292;&#22914;AMR&#65292;&#23558;AMR&#27880;&#37322;&#35270;&#20026;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#32534;&#30721;&#12290;&#20511;&#21161;&#32534;&#31243;&#33539;&#24335;&#30340;&#29087;&#24713;&#24230;&#65292;CAMRA&#21253;&#21547;&#20102;&#25152;&#26377;&#29616;&#26377;AMR&#32534;&#36753;&#22120;&#30340;&#22522;&#26412;&#21151;&#33021;&#65292;&#21253;&#25324;&#31034;&#20363;&#26597;&#25214;&#65292;&#21516;&#26102;&#36890;&#36807;&#23558;Propbank&#35282;&#33394;&#38598;&#26597;&#25214;&#38598;&#25104;&#20026;&#24037;&#20855;&#20013;&#30340;&#33258;&#21160;&#23436;&#25104;&#21151;&#33021;&#65292;&#26356;&#36827;&#19968;&#27493;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;CAMRA&#23558;AMR&#35299;&#26512;&#22120;&#27169;&#22411;&#20316;&#20026;&#32534;&#30721;&#21103;&#39550;&#39542;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;AMR&#27880;&#37322;&#32773;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#23637;&#31034;&#24037;&#20855;&#30340;&#21151;&#33021;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35775;&#38382;&#30340;&#23454;&#26102;&#28436;&#31034;&#65306;https://camra.colorado.edu
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10928v2 Announce Type: replace-cross  Abstract: In this paper, we introduce CAMRA (Copilot for AMR Annotatations), a cutting-edge web-based tool designed for constructing Abstract Meaning Representation (AMR) from natural language text. CAMRA offers a novel approach to deep lexical semantics annotation such as AMR, treating AMR annotation akin to coding in programming languages. Leveraging the familiarity of programming paradigms, CAMRA encompasses all essential features of existing AMR editors, including example lookup, while going a step further by integrating Propbank roleset lookup as an autocomplete feature within the tool. Notably, CAMRA incorporates AMR parser models as coding co-pilots, greatly enhancing the efficiency and accuracy of AMR annotators. To demonstrate the tool's capabilities, we provide a live demo accessible at: https://camra.colorado.edu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;FlipFlop&#23454;&#39564;&#25581;&#31034;&#20102;&#24403;&#25361;&#25112;LLMs&#35753;&#20854;&#21453;&#24605;&#21021;&#22987;&#31572;&#26696;&#26102;&#65292;&#27169;&#22411;&#20250;&#24179;&#22343;&#26377;46%&#30340;&#27010;&#29575;&#25913;&#21464;&#31572;&#26696;&#65292;&#25152;&#26377;&#27169;&#22411;&#22312;&#31532;&#19968;&#27425;&#21644;&#26368;&#32456;&#39044;&#27979;&#20043;&#38388;&#34920;&#29616;&#20986;&#20934;&#30830;&#24615;&#19979;&#38477;&#30340;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2311.08596</link><description>&lt;p&gt;
&#24744;&#30830;&#23450;&#21527;&#65311;&#25361;&#25112;LLMs&#23548;&#33268;FlipFlop&#23454;&#39564;&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Are You Sure? Challenging LLMs Leads to Performance Drops in The FlipFlop Experiment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;FlipFlop&#23454;&#39564;&#25581;&#31034;&#20102;&#24403;&#25361;&#25112;LLMs&#35753;&#20854;&#21453;&#24605;&#21021;&#22987;&#31572;&#26696;&#26102;&#65292;&#27169;&#22411;&#20250;&#24179;&#22343;&#26377;46%&#30340;&#27010;&#29575;&#25913;&#21464;&#31572;&#26696;&#65292;&#25152;&#26377;&#27169;&#22411;&#22312;&#31532;&#19968;&#27425;&#21644;&#26368;&#32456;&#39044;&#27979;&#20043;&#38388;&#34920;&#29616;&#20986;&#20934;&#30830;&#24615;&#19979;&#38477;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20132;&#20114;&#24615;&#29702;&#35770;&#19978;&#20801;&#35768;&#27169;&#22411;&#23436;&#21892;&#21644;&#25913;&#36827;&#20854;&#31572;&#26696;&#65292;&#28982;&#32780;&#23545;LLMs&#30340;&#22810;&#36718;&#34892;&#20026;&#30340;&#31995;&#32479;&#20998;&#26512;&#20173;&#21463;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FlipFlop&#23454;&#39564;&#65306;&#22312;&#23545;&#35805;&#30340;&#31532;&#19968;&#36718;&#20013;&#65292;&#19968;&#20010;LLM&#23436;&#25104;&#19968;&#20010;&#20998;&#31867;&#20219;&#21153;&#12290;&#22312;&#31532;&#20108;&#36718;&#20013;&#65292;LLM&#20250;&#21463;&#21040;&#19968;&#20010;&#36861;&#38382;&#65292;&#27604;&#22914;&#8220;&#24744;&#30830;&#23450;&#21527;&#65311;&#8221;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#26426;&#20250;&#21453;&#24605;&#20854;&#21021;&#22987;&#31572;&#26696;&#65292;&#24182;&#20915;&#23450;&#26159;&#30830;&#35748;&#36824;&#26159;&#25913;&#21464;&#31572;&#26696;&#12290;&#23545;&#19971;&#20010;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#21313;&#20010;LLMs&#30340;&#31995;&#32479;&#30740;&#31350;&#26174;&#31034;&#65292;&#27169;&#22411;&#24179;&#22343;&#26377;46%&#30340;&#27010;&#29575;&#25913;&#21464;&#20854;&#31572;&#26696;&#65292;&#24182;&#19988;&#25152;&#26377;&#27169;&#22411;&#22312;&#31532;&#19968;&#27425;&#21644;&#26368;&#32456;&#39044;&#27979;&#20043;&#38388;&#30475;&#21040;&#20934;&#30830;&#24615;&#19979;&#38477;&#65292;&#24179;&#22343;&#19979;&#38477;&#20102;17%&#65288;FlipFlop&#25928;&#24212;&#65289;&#12290;&#25105;&#20204;&#23545;&#19968;&#20010;&#24320;&#28304;LLM&#36827;&#34892;&#24494;&#35843;&#23454;&#39564;&#65292;&#21457;&#29616;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#32531;&#35299;--&#38477;&#20302;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08596v2 Announce Type: replace  Abstract: The interactive nature of Large Language Models (LLMs) theoretically allows models to refine and improve their answers, yet systematic analysis of the multi-turn behavior of LLMs remains limited. In this paper, we propose the FlipFlop experiment: in the first round of the conversation, an LLM completes a classification task. In a second round, the LLM is challenged with a follow-up phrase like "Are you sure?", offering an opportunity for the model to reflect on its initial answer, and decide whether to confirm or flip its answer. A systematic study of ten LLMs on seven classification tasks reveals that models flip their answers on average 46% of the time and that all models see a deterioration of accuracy between their first and final prediction, with an average drop of 17% (the FlipFlop effect). We conduct finetuning experiments on an open-source LLM and find that finetuning on synthetically created data can mitigate - reducing perf
&lt;/p&gt;</description></item><item><title>&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#39588;&#25968;&#23398;&#25512;&#29702;&#20013;&#36890;&#36807;&#27491;&#30830;&#24320;&#22987;&#21487;&#20197;&#33719;&#24471;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24314;&#35758;&#36890;&#36807;&#21021;&#22987;&#25351;&#23548;&#21644;&#33258;&#38382;&#25351;&#23548;&#30340;&#26041;&#24335;&#26469;&#24341;&#23548;&#27169;&#22411;&#24320;&#22987;&#27491;&#30830;&#12290;</title><link>https://arxiv.org/abs/2311.07945</link><description>&lt;p&gt;
&#33391;&#22909;&#30340;&#24320;&#31471;&#26159;&#25104;&#21151;&#30340;&#19968;&#21322;&#65306;&#22810;&#27493;&#39588;&#25968;&#23398;&#25512;&#29702;&#20013;&#24320;&#22987;&#27491;&#30830;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Well begun is half done: Importance of Starting Right in Multi-Step Math Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07945
&lt;/p&gt;
&lt;p&gt;
&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#39588;&#25968;&#23398;&#25512;&#29702;&#20013;&#36890;&#36807;&#27491;&#30830;&#24320;&#22987;&#21487;&#20197;&#33719;&#24471;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24314;&#35758;&#36890;&#36807;&#21021;&#22987;&#25351;&#23548;&#21644;&#33258;&#38382;&#25351;&#23548;&#30340;&#26041;&#24335;&#26469;&#24341;&#23548;&#27169;&#22411;&#24320;&#22987;&#27491;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#20026;&#20854;&#39044;&#27979;&#29983;&#25104;&#21407;&#22240;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35299;&#20915;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#36739;&#23567;&#30340;&#27169;&#22411;&#26377;&#26102;&#20250;&#22312;&#24320;&#22987;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#20294;&#22312;&#24471;&#21040;&#32416;&#27491;&#21518;&#65292;&#21487;&#20197;&#35299;&#20915;&#21407;&#26412;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#36739;&#23567;&#27169;&#22411;&#21487;&#20197;&#20174;&#21021;&#22987;&#25351;&#23548;&#20013;&#21463;&#30410;&#30340;&#26041;&#24335;&#65306;1&#65289;&#21521;LLM&#23547;&#27714;&#21021;&#22987;&#25351;&#23548;&#65292;&#21644;2&#65289;&#33258;&#38382;&#25351;&#23548;&#65292;&#23398;&#29983;&#27169;&#22411;&#21487;&#20197;&#39318;&#20808;&#21457;&#36215;&#19968;&#20010;&#20851;&#20110;&#22914;&#20309;&#24320;&#22987;&#30340;&#38382;&#39064;&#65292;&#28982;&#21518;&#32487;&#32493;&#36825;&#19968;&#36830;&#38145;&#12290;&#25105;&#20204;&#23558;&#21021;&#22987;&#22522;&#20110;&#38382;&#39064;&#30340;&#25351;&#23548;&#25193;&#23637;&#21040;&#20102;&#19968;&#31181;&#31216;&#20026;QuestCoT&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#22312;&#36827;&#34892;&#25512;&#29702;&#38142;&#20043;&#21069;&#20197;&#19968;&#20010;&#38382;&#39064;&#24320;&#22987;&#26159;&#26377;&#30410;&#30340;&#12290;&#22312;&#20004;&#20010;&#22810;&#27493;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;GSM8K&#21644;SVAMP&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27491;&#30830;&#24320;&#22987;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65288;&#36890;&#36807;LLM&#25351;&#23548;&#26368;&#39640;&#39640;&#36798;+14&#20998;&#65292;&#36890;&#36807;QuestCoT&#26368;&#39640;&#39640;&#36798;+6&#20998;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07945v2 Announce Type: replace  Abstract: Smaller language models can solve complex reasoning tasks better by learning to generate rationales for their predictions. However, we observe that these smaller models can sometimes struggle to start correctly, but when corrected, can solve a task that they would otherwise have struggled with. We propose two ways in which a smaller model can benefit from initial guidance: 1) asking an LLM for initial guidance, and 2) self-questioning guidance, where the student model can first initiate a question regarding how to start and then continue that chain. We extend initial question-based guidance to a prompting technique called QuestCoT, where starting with a question before a chain of reasoning proves useful. On two multi-step math reasoning datasets GSM8K and SVAMP, we show that starting correctly can lead to a significant performance gain (up to $+14$ points with LLM guidance and $+6$ points with QuestCoT).
&lt;/p&gt;</description></item><item><title>&#24320;&#28304;&#20195;&#30721;LLMs&#38590;&#20197;&#29983;&#25104;&#27491;&#30830;&#25351;&#23548;&#30340;&#21453;&#39304;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Coffee&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;Coffee&#25968;&#25454;&#38598;&#26500;&#24314;CoffeePots&#65292;&#36890;&#36807;&#20248;&#21270;&#35843;&#25972;&#21644;&#36873;&#25321;&#65292;&#23454;&#29616;&#33258;&#21160;&#29983;&#25104;&#24102;&#26377;&#27491;&#30830;&#25351;&#23548;&#30340;&#21453;&#39304;&#20197;&#29992;&#20110;&#20195;&#30721;&#20462;&#22797;&#12290;</title><link>https://arxiv.org/abs/2311.07215</link><description>&lt;p&gt;
&#21654;&#21857;&#65306;&#36890;&#36807;&#21453;&#39304;&#20462;&#22797;&#38169;&#35823;&#26469;&#25552;&#21319;&#20195;&#30721;LLMs
&lt;/p&gt;
&lt;p&gt;
Coffee: Boost Your Code LLMs by Fixing Bugs with Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07215
&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#20195;&#30721;LLMs&#38590;&#20197;&#29983;&#25104;&#27491;&#30830;&#25351;&#23548;&#30340;&#21453;&#39304;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Coffee&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;Coffee&#25968;&#25454;&#38598;&#26500;&#24314;CoffeePots&#65292;&#36890;&#36807;&#20248;&#21270;&#35843;&#25972;&#21644;&#36873;&#25321;&#65292;&#23454;&#29616;&#33258;&#21160;&#29983;&#25104;&#24102;&#26377;&#27491;&#30830;&#25351;&#23548;&#30340;&#21453;&#39304;&#20197;&#29992;&#20110;&#20195;&#30721;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07215v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#20195;&#30721;&#32534;&#36753;&#26159;&#30830;&#20445;&#31243;&#24207;&#32508;&#21512;&#30340;&#19968;&#20010;&#37325;&#35201;&#27493;&#39588;&#65292;&#21487;&#20197;&#33258;&#21160;&#32416;&#27491;&#20195;&#30721;LLMs&#29983;&#25104;&#30340;&#20851;&#38190;&#38169;&#35823;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#38381;&#28304;LLMs&#65288;&#22914;ChatGPT&#21644;GPT-4&#65289;&#33021;&#22815;&#29983;&#25104;&#32416;&#27491;&#24615;&#21453;&#39304;&#65292;&#29992;&#20110;&#32534;&#36753;&#38169;&#35823;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#24320;&#28304;&#20195;&#30721;LLMs&#29983;&#25104;&#29992;&#20110;&#20195;&#30721;&#32534;&#36753;&#30340;&#21453;&#39304;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#20542;&#21521;&#20110;&#36981;&#24490;&#34920;&#38754;&#26684;&#24335;&#25552;&#20379;&#19982;&#35823;&#23548;&#20449;&#24687;&#30456;&#28151;&#28102;&#30340;&#21453;&#39304;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#37325;&#28857;&#26159;&#21033;&#29992;&#24320;&#28304;&#20195;&#30721;LLMs&#29983;&#25104;&#20855;&#26377;&#27491;&#30830;&#25351;&#23548;&#30340;&#26377;&#29992;&#21453;&#39304;&#29992;&#20110;&#20195;&#30721;&#32534;&#36753;&#12290;&#20026;&#27492;&#24341;&#20837;&#20102;Coffee&#65292;&#19968;&#20010;&#19987;&#20026;&#24102;&#26377;&#21453;&#39304;&#30340;&#20195;&#30721;&#20462;&#22797;&#32780;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#65292;&#26500;&#24314;&#20102;CoffeePots&#65292;&#19968;&#20010;&#36890;&#36807;&#20559;&#22909;&#20248;&#21270;&#35843;&#25972;&#21644;&#36873;&#25321;&#30340;COde Fixing with FEEdback&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#33258;&#21160;&#29983;&#25104;&#26377;&#29992;&#30340;&#21453;&#39304;&#20197;&#24110;&#21161;&#20195;&#30721;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07215v2 Announce Type: replace  Abstract: Code editing is an essential step towards reliable program synthesis to automatically correct critical errors generated from code LLMs. Recent studies have demonstrated that closed-source LLMs (i.e., ChatGPT and GPT-4) are capable of generating corrective feedback to edit erroneous inputs. However, it remains challenging for open-source code LLMs to generate feedback for code editing, since these models tend to adhere to the superficial formats of feedback and provide feedback with misleading information. Hence, the focus of our work is to leverage open-source code LLMs to generate helpful feedback with correct guidance for code editing. To this end, we present Coffee, a collected dataset specifically designed for code fixing with feedback. Using this dataset, we construct CoffeePots, a framework for COde Fixing with FEEdback via Preference-Optimized Tuning and Selection. The proposed framework aims to automatically generate helpful 
&lt;/p&gt;</description></item><item><title>&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20998;&#26512;&#30005;&#35270;&#28216;&#25103;&#33410;&#30446;&#25968;&#25454;&#21457;&#29616;&#65292;&#21363;&#20351;&#21482;&#20351;&#29992;&#35821;&#35328;&#32447;&#32034;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#27169;&#22411;&#21487;&#20197;&#19982;&#20154;&#31867;&#20027;&#20307;&#20855;&#26377;&#31867;&#20284;&#30340;&#30495;&#30456;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.07092</link><description>&lt;p&gt;
&#25581;&#31034;&#30495;&#30456;&#65306;&#27450;&#39575;&#35821;&#35328;&#21644;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
To Tell The Truth: Language of Deception and Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07092
&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20998;&#26512;&#30005;&#35270;&#28216;&#25103;&#33410;&#30446;&#25968;&#25454;&#21457;&#29616;&#65292;&#21363;&#20351;&#21482;&#20351;&#29992;&#35821;&#35328;&#32447;&#32034;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#27169;&#22411;&#21487;&#20197;&#19982;&#20154;&#31867;&#20027;&#20307;&#20855;&#26377;&#31867;&#20284;&#30340;&#30495;&#30456;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07092v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;-cross &#25688;&#35201;&#65306;&#22522;&#20110;&#25991;&#26412;&#30340;&#38169;&#35823;&#20449;&#24687;&#28183;&#36879;&#21040;&#22312;&#32447;&#35752;&#35770;&#20013;&#65292;&#28982;&#32780;&#20154;&#20204;&#33021;&#22815;&#20174;&#36825;&#31181;&#27450;&#39575;&#24615;&#25991;&#26412;&#20869;&#23481;&#20013;&#36776;&#21035;&#30495;&#30456;&#30340;&#35777;&#25454;&#21364;&#24456;&#23569;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#26723;&#26032;&#39062;&#30340;&#30005;&#35270;&#28216;&#25103;&#33410;&#30446;&#25968;&#25454;&#65292;&#20854;&#20013;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#30456;&#20114;&#20043;&#38388;&#23384;&#22312;&#20914;&#31361;&#30446;&#26631;&#30340;&#20010;&#20307;&#20043;&#38388;&#30340;&#23545;&#35805;&#23548;&#33268;&#35854;&#35328;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#27450;&#39575;&#35821;&#35328;&#28508;&#22312;&#21487;&#39564;&#35777;&#35821;&#35328;&#32447;&#32034;&#22312;&#23458;&#35266;&#30495;&#30456;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#65292;&#36825;&#26159;&#20197;&#24448;&#22522;&#20110;&#25991;&#26412;&#30340;&#27450;&#39575;&#25968;&#25454;&#38598;&#20013;&#32570;&#23569;&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23384;&#22312;&#19968;&#31867;&#25506;&#27979;&#22120;&#65288;&#31639;&#27861;&#65289;&#65292;&#20854;&#30495;&#30456;&#26816;&#27979;&#24615;&#33021;&#19982;&#20154;&#31867;&#20027;&#20307;&#30456;&#20284;&#65292;&#21363;&#20351;&#21069;&#32773;&#21482;&#20351;&#29992;&#35821;&#35328;&#32447;&#32034;&#65292;&#32780;&#21518;&#32773;&#21017;&#36890;&#36807;&#23436;&#20840;&#35775;&#38382;&#25152;&#26377;&#28508;&#22312;&#32447;&#32034;&#28304;&#65288;&#35821;&#35328;&#21644;&#35270;&#21548;&#65289;&#36827;&#34892;&#23545;&#35805;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24314;&#31435;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#19978;&#65292;&#37319;&#29992;&#29942;&#39048;&#26694;&#26550;&#26469;&#23398;&#20064;&#21487;&#36776;&#21035;&#30340;&#32447;&#32034;&#65292;&#20197;&#30830;&#23450;&#30495;&#30456;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07092v2 Announce Type: replace-cross  Abstract: Text-based misinformation permeates online discourses, yet evidence of people's ability to discern truth from such deceptive textual content is scarce. We analyze a novel TV game show data where conversations in a high-stake environment between individuals with conflicting objectives result in lies. We investigate the manifestation of potentially verifiable language cues of deception in the presence of objective truth, a distinguishing feature absent in previous text-based deception datasets. We show that there exists a class of detectors (algorithms) that have similar truth detection performance compared to human subjects, even when the former accesses only the language cues while the latter engages in conversations with complete access to all potential sources of cues (language and audio-visual). Our model, built on a large language model, employs a bottleneck framework to learn discernible cues to determine truth, an act of 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#19977;&#20010;&#26631;&#31614;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#36896;&#20195;&#34920;&#19981;&#21516;&#31867;&#22411;&#20559;&#35265;&#30340;&#35780;&#20272;&#25968;&#25454;&#32452;&#65292;&#24182;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#26356;&#22909;&#22320;&#21306;&#20998;&#26377;&#20559;&#35265;&#30340;&#12289;&#19981;&#27491;&#30830;&#30340;&#25512;&#29702;&#21644;&#38750;&#26377;&#20559;&#35265;&#30340;&#19981;&#27491;&#30830;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2309.09697</link><description>&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#35780;&#20272;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#32771;&#34385;&#25152;&#26377;&#26631;&#31614;
&lt;/p&gt;
&lt;p&gt;
Evaluating Gender Bias of Pre-trained Language Models in Natural Language Inference by Considering All Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.09697
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#19977;&#20010;&#26631;&#31614;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#36896;&#20195;&#34920;&#19981;&#21516;&#31867;&#22411;&#20559;&#35265;&#30340;&#35780;&#20272;&#25968;&#25454;&#32452;&#65292;&#24182;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#26356;&#22909;&#22320;&#21306;&#20998;&#26377;&#20559;&#35265;&#30340;&#12289;&#19981;&#27491;&#30830;&#30340;&#25512;&#29702;&#21644;&#38750;&#26377;&#20559;&#35265;&#30340;&#19981;&#27491;&#30830;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#31181;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20013;&#21457;&#29616;&#20102;&#27495;&#35270;&#24615;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#20013;&#65292;&#29616;&#26377;&#30340;&#20559;&#35265;&#35780;&#20272;&#26041;&#27861;&#19987;&#27880;&#20110;&#19977;&#20010;&#26631;&#31614;&#20013;&#30340;&#19968;&#20010;&#29305;&#23450;&#26631;&#31614;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#20363;&#22914;&#20013;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35780;&#20272;&#26041;&#27861;&#21487;&#33021;&#19981;&#20934;&#30830;&#65292;&#22240;&#20026;&#29420;&#29305;&#30340;&#20559;&#35265;&#25512;&#29702;&#19982;&#29420;&#29305;&#30340;&#39044;&#27979;&#26631;&#31614;&#30456;&#20851;&#32852;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;NLI&#20219;&#21153;&#30340;&#19977;&#20010;&#26631;&#31614;&#30340;PLMs&#20559;&#35265;&#35780;&#20272;&#26041;&#27861;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19977;&#20010;&#20195;&#34920;&#19981;&#21516;&#31867;&#22411;&#20559;&#35265;&#30340;&#35780;&#20272;&#25968;&#25454;&#32452;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#27599;&#20010;&#25968;&#25454;&#32452;&#30340;&#30456;&#24212;&#26631;&#31614;&#36755;&#20986;&#23450;&#20041;&#20102;&#19968;&#31181;&#20559;&#35265;&#24230;&#37327;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;NLI&#20559;&#35265;&#24230;&#37327;&#30340;&#20803;&#35780;&#20272;&#25216;&#26415;&#65292;&#24182;&#29992;&#23427;&#26469;&#30830;&#35748;&#25105;&#20204;&#30340;&#20559;&#35265;&#24230;&#37327;&#21487;&#20197;&#26356;&#22909;&#22320;&#21306;&#20998;&#26377;&#20559;&#35265;&#30340;&#65292;&#19981;&#27491;&#30830;&#30340;&#25512;&#29702;&#19982;&#38750;&#20559;&#35265;&#30340;&#19981;&#27491;&#30830;&#25512;&#29702;&#65292;&#32988;&#36807;&#22522;&#32447;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;m
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.09697v2 Announce Type: replace  Abstract: Discriminatory gender biases have been found in Pre-trained Language Models (PLMs) for multiple languages. In Natural Language Inference (NLI), existing bias evaluation methods have focused on the prediction results of a specific label out of three labels, such as neutral. However, such evaluation methods can be inaccurate since unique biased inferences are associated with unique prediction labels. Addressing this limitation, we propose a bias evaluation method for PLMs that considers all the three labels of NLI task. We create three evaluation data groups that represent different types of biases. Then, we define a bias measure based on the corresponding label output of each data group. In the experiments, we introduce a meta-evaluation technique for NLI bias measures and use it to confirm that our bias measure can distinguish biased, incorrect inferences from non-biased incorrect inferences better than the baseline, resulting in a m
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#21407;&#22987;&#35821;&#26009;&#24211;&#36716;&#21270;&#20026;&#38405;&#35835;&#29702;&#35299;&#25991;&#26412;&#26469;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#24615;&#33021;&#22987;&#32456;&#24471;&#21040;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2309.09530</link><description>&lt;p&gt;
&#36890;&#36807;&#38405;&#35835;&#29702;&#35299;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Adapting Large Language Models via Reading Comprehension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.09530
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#21407;&#22987;&#35821;&#26009;&#24211;&#36716;&#21270;&#20026;&#38405;&#35835;&#29702;&#35299;&#25991;&#26412;&#26469;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#24615;&#33021;&#22987;&#32456;&#24471;&#21040;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#35821;&#26009;&#24211;&#19978;&#25345;&#32493;&#39044;&#35757;&#32451;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#21407;&#22987;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#36171;&#20104;&#27169;&#22411;&#39046;&#22495;&#30693;&#35782;&#65292;&#20294;&#26497;&#22823;&#22320;&#25439;&#23475;&#20102;&#20854;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#21463;&#20154;&#31867;&#36890;&#36807;&#38405;&#35835;&#29702;&#35299;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#21363;&#38405;&#35835;&#21518;&#32451;&#20064;&#25552;&#39640;&#22522;&#20110;&#25152;&#23398;&#30693;&#35782;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#21407;&#22987;&#35821;&#26009;&#24211;&#36716;&#21270;&#20026;&#38405;&#35835;&#29702;&#35299;&#25991;&#26412;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#27599;&#20010;&#21407;&#22987;&#25991;&#26412;&#37117;&#20250;&#34987;&#19968;&#31995;&#21015;&#19982;&#20854;&#20869;&#23481;&#30456;&#20851;&#30340;&#20219;&#21153;&#20016;&#23500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#21487;&#25193;&#23637;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#33021;&#22815;&#22312;&#19977;&#20010;&#19981;&#21516;&#39046;&#22495;&#65288;&#29983;&#29289;&#21307;&#23398;&#12289;&#37329;&#34701;&#21644;&#27861;&#24459;&#65289;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#25345;&#32493;&#25552;&#21319;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;7B&#35821;&#35328;&#27169;&#22411;&#22312;&#31454;&#20105;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#19982;&#35268;&#27169;&#26356;&#22823;&#30340;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#65288;&#22914;BloombergGPT-50B&#65289;&#30456;&#23218;&#32654;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#21487;&#20197;&#24102;&#26469;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.09530v2 Announce Type: replace  Abstract: We explore how continued pre-training on domain-specific corpora influences large language models, revealing that training on the raw corpora endows the model with domain knowledge, but drastically hurts its prompting ability for question answering. Taken inspiration from human learning via reading comprehension--practice after reading improves the ability to answer questions based on the learned knowledge--we propose a simple method for transforming raw corpora into reading comprehension texts. Each raw text is enriched with a series of tasks related to its content. Our method, highly scalable and applicable to any pre-training corpora, consistently enhances performance across various tasks in three different domains: biomedicine, finance, and law. Notably, our 7B language model achieves competitive performance with domain-specific models of much larger scales, such as BloombergGPT-50B. Furthermore, we demonstrate that domain-specif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23457;&#26597;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20844;&#24179;&#24615;&#30340;&#30740;&#31350;&#65292;&#38024;&#23545;&#20013;&#31561;&#35268;&#27169;LLMs&#21644;&#22823;&#35268;&#27169;LLMs&#25552;&#20986;&#20102;&#35780;&#20272;&#25351;&#26631;&#21644;&#21435;&#20559;&#35265;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2308.10149</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20844;&#24179;&#24615;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Fairness in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.10149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23457;&#26597;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20844;&#24179;&#24615;&#30340;&#30740;&#31350;&#65292;&#38024;&#23545;&#20013;&#31561;&#35268;&#27169;LLMs&#21644;&#22823;&#35268;&#27169;LLMs&#25552;&#20986;&#20102;&#35780;&#20272;&#25351;&#26631;&#21644;&#21435;&#20559;&#35265;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#21644;&#21457;&#23637;&#21069;&#26223;&#65292;&#24182;&#24191;&#27867;&#37096;&#32626;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#12290;&#28982;&#32780;&#65292;LLMs&#21487;&#33021;&#20250;&#25429;&#25417;&#21040;&#26410;&#32463;&#22788;&#29702;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#24182;&#23558;&#36825;&#20123;&#20559;&#35265;&#20256;&#25773;&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#19981;&#20844;&#24179;&#30340;LLM&#31995;&#32479;&#20250;&#20135;&#29983;&#19981;&#33391;&#30340;&#31038;&#20250;&#24433;&#21709;&#21644;&#28508;&#22312;&#21361;&#23475;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;&#26377;&#20851;LLMs&#20013;&#20844;&#24179;&#24615;&#30340;&#30740;&#31350;&#12290;&#32771;&#34385;&#21040;&#21442;&#25968;&#22823;&#23567;&#21644;&#35757;&#32451;&#33539;&#24335;&#23545;&#30740;&#31350;&#31574;&#30053;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#20844;&#24179;&#24615;&#30740;&#31350;&#20998;&#20026;&#38024;&#23545;&#20013;&#31561;&#35268;&#27169;LLMs&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#24335;&#19979;&#30340;&#30740;&#31350;&#20197;&#21450;&#38024;&#23545;&#22823;&#35268;&#27169;LLMs&#22312;&#25552;&#31034;&#33539;&#24335;&#19979;&#30340;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#23545;&#20110;&#20013;&#31561;&#35268;&#27169;LLMs&#65292;&#25105;&#20204;&#20174;&#20869;&#22312;&#20559;&#35265;&#21644;&#22806;&#22312;&#20559;&#35265;&#30340;&#35282;&#24230;&#20171;&#32461;&#20102;&#35780;&#20272;&#25351;&#26631;&#21644;&#21435;&#20559;&#35265;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;LLMs&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26368;&#36817;&#30340;&#20844;&#24179;&#24615;&#30740;&#31350;&#65292;&#21253;&#25324;&#20844;&#24179;&#24615;&#35780;&#20272;&#12289;&#21407;&#22240;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.10149v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have shown powerful performance and development prospects and are widely deployed in the real world. However, LLMs can capture social biases from unprocessed training data and propagate the biases to downstream tasks. Unfair LLM systems have undesirable social impacts and potential harms. In this paper, we provide a comprehensive review of related research on fairness in LLMs. Considering the influence of parameter magnitude and training paradigm on research strategy, we divide existing fairness research into oriented to medium-sized LLMs under pre-training and fine-tuning paradigms and oriented to large-sized LLMs under prompting paradigms. First, for medium-sized LLMs, we introduce evaluation metrics and debiasing methods from the perspectives of intrinsic bias and extrinsic bias, respectively. Then, for large-sized LLMs, we introduce recent fairness research, including fairness evaluation, reason
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#35270;&#35282;&#23545;&#27604;&#23398;&#20064;&#23454;&#29616;&#36328;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#20196;&#29260;&#23545;&#20851;&#31995;&#24182;&#21033;&#29992;&#19978;&#19979;&#25991;&#32454;&#24494;&#24046;&#21035;&#26469;&#32479;&#19968;&#19981;&#21516;&#35821;&#35328;&#30340;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2308.09073</link><description>&lt;p&gt;
mCL-NER: &#22810;&#35270;&#35282;&#23545;&#27604;&#23398;&#20064;&#23454;&#29616;&#30340;&#36328;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
mCL-NER: Cross-Lingual Named Entity Recognition via Multi-view Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.09073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#35270;&#35282;&#23545;&#27604;&#23398;&#20064;&#23454;&#29616;&#36328;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#20196;&#29260;&#23545;&#20851;&#31995;&#24182;&#21033;&#29992;&#19978;&#19979;&#25991;&#32454;&#24494;&#24046;&#21035;&#26469;&#32479;&#19968;&#19981;&#21516;&#35821;&#35328;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#38754;&#20020;&#30528;&#30001;&#20110;&#36328;&#35821;&#26009;&#24211;&#31232;&#32570;&#65292;&#23588;&#20854;&#26159;&#23545;&#38750;&#33521;&#35821;&#25968;&#25454;&#34920;&#29616;&#19981;&#22343;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#35270;&#35282;&#23545;&#27604;&#23398;&#20064;&#23454;&#29616;&#36328;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;mCL-NER&#65289;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;CrossNER&#20219;&#21153;&#37325;&#26032;&#26500;&#24314;&#20026;&#19968;&#31181;&#35782;&#21035;&#20196;&#29260;&#23545;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23454;&#20307;&#20869;&#37096;&#20196;&#29260;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#32454;&#24494;&#24046;&#21035;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#19981;&#21516;&#35821;&#35328;&#30340;&#34920;&#31034;&#32479;&#19968;&#36215;&#26469;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#35270;&#35282;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#28085;&#30422;&#28304;&#35821;&#21477;&#12289;&#20195;&#30721;&#20999;&#25442;&#35821;&#21477;&#21644;&#30446;&#26631;&#35821;&#21477;&#20043;&#38388;&#30340;&#35821;&#20041;&#23545;&#27604;&#65292;&#20197;&#21450;&#20196;&#29260;&#20043;&#38388;&#30340;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.09073v2 Announce Type: replace  Abstract: Cross-lingual named entity recognition (CrossNER) faces challenges stemming from uneven performance due to the scarcity of multilingual corpora, especially for non-English data. While prior efforts mainly focus on data-driven transfer methods, a significant aspect that has not been fully explored is aligning both semantic and token-level representations across diverse languages. In this paper, we propose Multi-view Contrastive Learning for Cross-lingual Named Entity Recognition (mCL-NER). Specifically, we reframe the CrossNER task into a problem of recognizing relationships between pairs of tokens. This approach taps into the inherent contextual nuances of token-to-token connections within entities, allowing us to align representations across different languages. A multi-view contrastive learning framework is introduced to encompass semantic contrasts between source, codeswitched, and target sentences, as well as contrasts among toke
&lt;/p&gt;</description></item><item><title>TESS&#26159;&#19968;&#20010;&#20840;&#38750;&#33258;&#22238;&#24402;&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#36923;&#36753;&#31354;&#38388;&#32780;&#19981;&#26159;&#23398;&#20064;&#23884;&#20837;&#31354;&#38388;&#24212;&#29992;&#25193;&#25955;&#36807;&#31243;&#65292;&#36827;&#34892;&#20102;&#33258;&#26465;&#20214;&#21333;&#32431;&#24418;&#25193;&#25955;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#19988;&#25152;&#38656;&#30340;&#25193;&#25955;&#27493;&#39588;&#26356;&#23569;&#12290;</title><link>https://arxiv.org/abs/2305.08379</link><description>&lt;p&gt;
TESS&#65306;&#25991;&#26412;&#21040;&#25991;&#26412;&#33258;&#26465;&#20214;&#21333;&#32431;&#24418;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
TESS: Text-to-Text Self-Conditioned Simplex Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.08379
&lt;/p&gt;
&lt;p&gt;
TESS&#26159;&#19968;&#20010;&#20840;&#38750;&#33258;&#22238;&#24402;&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#36923;&#36753;&#31354;&#38388;&#32780;&#19981;&#26159;&#23398;&#20064;&#23884;&#20837;&#31354;&#38388;&#24212;&#29992;&#25193;&#25955;&#36807;&#31243;&#65292;&#36827;&#34892;&#20102;&#33258;&#26465;&#20214;&#21333;&#32431;&#24418;&#25193;&#25955;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#19988;&#25152;&#38656;&#30340;&#25193;&#25955;&#27493;&#39588;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#22312;&#21508;&#31181;&#36830;&#32493;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#29983;&#25104;&#26041;&#27861;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#23558;&#36830;&#32493;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#33258;&#28982;&#35821;&#35328;&#26159;&#31163;&#25955;&#30340;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#30340;&#25193;&#25955;&#27493;&#39588;&#26469;&#29983;&#25104;&#25991;&#26412;&#65292;&#36825;&#20351;&#24471;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#21464;&#24471;&#26114;&#36149;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25991;&#26412;&#21040;&#25991;&#26412;&#33258;&#26465;&#20214;&#21333;&#32431;&#24418;&#25193;&#25955;&#65288;TESS&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38750;&#33258;&#22238;&#24402;&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#37319;&#29992;&#19968;&#31181;&#26032;&#24418;&#24335;&#30340;&#33258;&#26465;&#20214;&#65292;&#23558;&#25193;&#25955;&#36807;&#31243;&#24212;&#29992;&#20110;&#36923;&#36753;&#31354;&#38388;&#32780;&#19981;&#26159;&#23398;&#20064;&#23884;&#20837;&#31354;&#38388;&#12290;&#36890;&#36807;&#23545;&#21253;&#25324;&#24635;&#32467;&#12289;&#25991;&#26412;&#31616;&#21270;&#12289;&#37322;&#20041;&#29983;&#25104;&#21644;&#38382;&#39064;&#29983;&#25104;&#22312;&#20869;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;TESS&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#22312;&#38656;&#35201;&#26356;&#23569;&#30340;&#25193;&#25955;&#27493;&#39588;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26368;&#23567;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.08379v2 Announce Type: replace  Abstract: Diffusion models have emerged as a powerful paradigm for generation, obtaining strong performance in various continuous domains. However, applying continuous diffusion models to natural language remains challenging due to its discrete nature and the need for a large number of diffusion steps to generate text, making diffusion-based generation expensive. In this work, we propose Text-to-text Self-conditioned Simplex Diffusion (TESS), a text diffusion model that is fully non-autoregressive, employs a new form of self-conditioning, and applies the diffusion process on the logit simplex space rather than the learned embedding space. Through extensive experiments on natural language understanding and generation tasks including summarization, text simplification, paraphrase generation, and question generation, we demonstrate that TESS outperforms state-of-the-art non-autoregressive models, requires fewer diffusion steps with minimal drop i
&lt;/p&gt;</description></item><item><title>InPars-Light&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20462;&#25913;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#24471;&#22810;&#30340;&#25490;&#21517;&#27169;&#22411;&#21644;&#20813;&#36153;&#35821;&#35328;&#27169;&#22411;BLOOM&#65292;&#22312;&#22810;&#20010;&#33521;&#25991;&#26816;&#32034;&#38598;&#21512;&#19978;&#26174;&#33879;&#25913;&#36827;&#20102;&#25490;&#21517;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2301.02998</link><description>&lt;p&gt;
InPars-Light:&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#39640;&#25928;&#25490;&#21517;&#22120;
&lt;/p&gt;
&lt;p&gt;
InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.02998
&lt;/p&gt;
&lt;p&gt;
InPars-Light&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20462;&#25913;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#24471;&#22810;&#30340;&#25490;&#21517;&#27169;&#22411;&#21644;&#20813;&#36153;&#35821;&#35328;&#27169;&#22411;BLOOM&#65292;&#22312;&#22810;&#20010;&#33521;&#25991;&#26816;&#32034;&#38598;&#21512;&#19978;&#26174;&#33879;&#25913;&#36827;&#20102;&#25490;&#21517;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#23637;&#20102;&#23545;InPars&#30340;&#21487;&#37325;&#29616;&#24615;&#30740;&#31350;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#35757;&#32451;&#31070;&#32463;&#25490;&#21517;&#22120;&#30340;&#26041;&#27861;&#12290;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#24320;&#21457;&#20986;&#20102;InPars-Light&#65292;&#36825;&#26159;&#23545;InPars&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20462;&#25913;&#12290;&#19982;InPars&#19981;&#21516;&#65292;InPars-Light&#20351;&#29992;7-100&#20493;&#26356;&#23567;&#30340;&#25490;&#21517;&#27169;&#22411;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#19968;&#20010;&#20813;&#36153;&#25552;&#20379;&#30340;&#35821;&#35328;&#27169;&#22411;BLOOM&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#19987;&#26377;&#30340;GPT-3&#27169;&#22411;&#30456;&#27604;&#65292;BLOOM&#33021;&#22815;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#25490;&#21517;&#22120;&#12290;&#22312;&#25152;&#26377;&#20116;&#20010;&#33521;&#25991;&#26816;&#32034;&#38598;&#21512;&#19978;&#65292;&#25105;&#20204;&#20165;&#20351;&#29992;&#19968;&#20010;30M&#21442;&#25968;&#20845;&#23618;MiniLM-30M&#25490;&#21517;&#22120;&#21644;&#19968;&#20010;&#19977;&#36873;&#20457;&#30340;&#25552;&#31034;&#65292;&#22312;nDCG&#21644;MRR&#26041;&#38754;&#65292;&#30456;&#27604;BM25&#65292;&#25105;&#20204;&#37117;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#65288;7%-30%&#65289;&#19988;&#20855;&#26377;&#32479;&#35745;&#23398;&#24847;&#20041;&#30340;&#25913;&#36827;&#12290;&#30456;&#21453;&#65292;&#22312;InPars&#30340;&#30740;&#31350;&#20013;&#65292;&#21482;&#26377;&#19968;&#20010;&#22823;100&#20493;&#30340;monoT5-3B&#27169;&#22411;&#33021;&#22815;&#22987;&#32456;&#32988;&#36807;BM25&#65292;&#32780;&#23567;&#24471;&#22810;&#30340;monoT5-220M&#27169;&#22411;&#65288;&#20173;&#28982;&#27604;&#25105;&#20204;&#30340;MiniLM&#25490;&#21517;&#22120;&#22823;7&#20493;&#65289;&#21482;&#26159;&#22312;MS MAR&#19978;&#32988;&#36807;BM25&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.02998v2 Announce Type: replace-cross  Abstract: We carried out a reproducibility study of InPars, which is a method for unsupervised training of neural rankers (Bonifacio et al., 2022). As a by-product, we developed InPars-light, which is a simple-yet-effective modification of InPars. Unlike InPars, InPars-light uses 7x-100x smaller ranking models and only a freely available language model BLOOM, which -- as we found out -- produced more accurate rankers compared to a proprietary GPT-3 model. On all five English retrieval collections (used in the original InPars study) we obtained substantial (7%-30%) and statistically significant improvements over BM25 (in nDCG and MRR) using only a 30M parameter six-layer MiniLM-30M ranker and a single three-shot prompt. In contrast, in the InPars study only a 100x larger monoT5-3B model consistently outperformed BM25, whereas their smaller monoT5-220M model (which is still 7x larger than our MiniLM ranker) outperformed BM25 only on MS MAR
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#29702;&#25968;&#25454;&#38598;&#24182;&#25910;&#38598;&#20840;&#38754;&#30340;&#20154;&#31867;&#27880;&#37322;&#65292;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#25512;&#29702;&#21644;&#25490;&#24207;&#26080;&#24207;&#30340;&#22810;&#27169;&#24577;&#25351;&#23548;&#26041;&#38754;&#30340;&#33021;&#21147;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#27169;&#22411;&#34920;&#29616;&#19981;&#20165;&#26174;&#33879;&#20302;&#20110;&#20154;&#31867;&#65292;&#32780;&#19988;&#20284;&#20046;&#26080;&#27861;&#20855;&#22791;&#36825;&#31181;&#22522;&#26412;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2110.08486</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#22810;&#27169;&#24577;&#25351;&#23548;&#25163;&#20876;&#36827;&#34892;&#25490;&#24207;&#26469;&#29702;&#35299;&#22810;&#27169;&#24577;&#31243;&#24207;&#21270;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Understanding Multimodal Procedural Knowledge by Sequencing Multimodal Instructional Manuals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.08486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#29702;&#25968;&#25454;&#38598;&#24182;&#25910;&#38598;&#20840;&#38754;&#30340;&#20154;&#31867;&#27880;&#37322;&#65292;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#25512;&#29702;&#21644;&#25490;&#24207;&#26080;&#24207;&#30340;&#22810;&#27169;&#24577;&#25351;&#23548;&#26041;&#38754;&#30340;&#33021;&#21147;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#27169;&#22411;&#34920;&#29616;&#19981;&#20165;&#26174;&#33879;&#20302;&#20110;&#20154;&#31867;&#65292;&#32780;&#19988;&#20284;&#20046;&#26080;&#27861;&#20855;&#22791;&#36825;&#31181;&#22522;&#26412;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28459;&#28216;&#25351;&#23548;&#26080;&#24207;&#20107;&#20214;&#30340;&#33021;&#21147;&#26159;&#29702;&#35299;&#21644;&#25512;&#29702;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#31243;&#24207;&#30340;&#22522;&#26412;&#25216;&#33021;&#65292;&#36890;&#24120;&#38656;&#35201;&#23545;&#26102;&#38388;&#24120;&#35782;&#21644;&#22810;&#27169;&#24577;&#20449;&#24687;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#65292;&#22240;&#20026;&#36825;&#20123;&#31243;&#24207;&#36890;&#24120;&#36890;&#36807;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#32452;&#21512;&#36827;&#34892;&#20256;&#36798;&#12290;&#36825;&#31181;&#33021;&#21147;&#23545;&#20110;&#39034;&#24207;&#20219;&#21153;&#35268;&#21010;&#21644;&#22810;&#28304;&#25351;&#23548;&#25688;&#35201;&#31561;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#20154;&#31867;&#33021;&#22815;&#25512;&#29702;&#21644;&#25490;&#24207;&#26080;&#24207;&#30340;&#22810;&#27169;&#24577;&#31243;&#24207;&#25351;&#23548;&#65292;&#20294;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#36825;&#31181;&#22522;&#26412;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25972;&#29702;&#26469;&#33258;&#28909;&#38376;&#22312;&#32447;&#25351;&#23548;&#25163;&#20876;&#30340;&#25968;&#25454;&#38598;&#24182;&#25910;&#38598;&#20102;&#20840;&#38754;&#30340;&#20154;&#31867;&#27880;&#37322;&#65292;&#26469;&#23545;&#27169;&#22411;&#25512;&#29702;&#21644;&#25490;&#24207;&#26080;&#24207;&#30340;&#22810;&#27169;&#24577;&#25351;&#23548;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#19981;&#20165;&#24615;&#33021;&#26174;&#33879;&#20302;&#20110;&#20154;&#31867;&#65292;&#32780;&#19988;&#20284;&#20046;&#26080;&#27861;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2110.08486v4 Announce Type: replace  Abstract: The ability to sequence unordered events is an essential skill to comprehend and reason about real world task procedures, which often requires thorough understanding of temporal common sense and multimodal information, as these procedures are often communicated through a combination of texts and images. Such capability is essential for applications such as sequential task planning and multi-source instruction summarization. While humans are capable of reasoning about and sequencing unordered multimodal procedural instructions, whether current machine learning models have such essential capability is still an open question. In this work, we benchmark models' capability of reasoning over and sequencing unordered multimodal instructions by curating datasets from popular online instructional manuals and collecting comprehensive human annotations. We find models not only perform significantly worse than humans but also seem incapable of e
&lt;/p&gt;</description></item><item><title>CFMatch&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#23558;&#33258;&#21160;&#31572;&#26696;&#31561;&#20215;&#35780;&#20272;&#19982;&#20154;&#24037;&#19987;&#23478;&#21028;&#26029;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#26126;&#30830;&#19968;&#33268;&#30340;&#35780;&#20272;&#25351;&#21335;&#24182;&#24341;&#20837;&#39640;&#25928;&#12289;&#31283;&#20581;&#19988;&#36731;&#37327;&#32423;&#30340;&#21028;&#21035;&#24335;AE&#20998;&#31867;&#22120;&#21305;&#37197;&#26041;&#27861;&#26469;&#35299;&#20915;&#24403;&#21069;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13170</link><description>&lt;p&gt;
CFMatch: &#23558;&#33258;&#21160;&#31572;&#26696;&#31561;&#20215;&#35780;&#20272;&#19982;&#20154;&#24037;&#19987;&#23478;&#21028;&#26029;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert Judgments For Open-Domain Question Answering. (arXiv:2401.13170v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13170
&lt;/p&gt;
&lt;p&gt;
CFMatch&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#23558;&#33258;&#21160;&#31572;&#26696;&#31561;&#20215;&#35780;&#20272;&#19982;&#20154;&#24037;&#19987;&#23478;&#21028;&#26029;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#26126;&#30830;&#19968;&#33268;&#30340;&#35780;&#20272;&#25351;&#21335;&#24182;&#24341;&#20837;&#39640;&#25928;&#12289;&#31283;&#20581;&#19988;&#36731;&#37327;&#32423;&#30340;&#21028;&#21035;&#24335;AE&#20998;&#31867;&#22120;&#21305;&#37197;&#26041;&#27861;&#26469;&#35299;&#20915;&#24403;&#21069;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#31995;&#32479;&#21482;&#26377;&#22312;&#25105;&#20204;&#30693;&#36947;&#31572;&#26696;&#26159;&#21542;&#27491;&#30830;&#30340;&#24773;&#20917;&#19979;&#25165;&#33021;&#21462;&#24471;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#35768;&#22810;&#26368;&#20855;&#25361;&#25112;&#21644;&#26377;&#36259;&#30340;&#38382;&#31572;&#31034;&#20363;&#65292;&#24403;&#21069;&#29992;&#20110;&#30830;&#23450;&#31572;&#26696;&#31561;&#20215;&#24615;&#30340;&#35780;&#20272;&#25351;&#26631;&#36890;&#24120;&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#65292;&#23588;&#20854;&#26159;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26356;&#20887;&#38271;&#12289;&#33258;&#30001;&#24418;&#24335;&#30340;&#31572;&#26696;&#12290;&#23384;&#22312;&#20004;&#20010;&#25361;&#25112;&#65306;&#32570;&#20047;&#25968;&#25454;&#21644;&#27169;&#22411;&#36807;&#22823;&#65306;&#22522;&#20110;LLM&#30340;&#35780;&#20998;&#22120;&#21487;&#20197;&#26356;&#22909;&#22320;&#19982;&#20154;&#24037;&#35780;&#21028;&#21592;&#30456;&#20851;&#32852;&#65292;&#20294;&#36825;&#20010;&#20219;&#21153;&#21482;&#22312;&#26377;&#38480;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21363;&#20351;&#21487;&#29992;&#65292;&#23545;&#27169;&#22411;&#30340;&#26356;&#26032;&#20063;&#26377;&#38480;&#65292;&#22240;&#20026;LLM&#36807;&#22823;&#19988;&#24448;&#24448;&#26114;&#36149;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#26126;&#30830;&#19968;&#33268;&#30340;&#25351;&#21335;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#36825;&#20123;&#25351;&#21335;&#29992;&#20110;&#20174;&#19987;&#19994;&#20154;&#24037;&#38382;&#31572;&#27604;&#36187;&#20013;&#37319;&#32435;&#26426;&#22120;&#38382;&#31572;&#22312;&#31572;&#26696;&#31561;&#20215;&#24615;&#35780;&#20272;&#26041;&#38754;&#30340;&#26631;&#20934;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26631;&#20934;&#35780;&#20272;&#21644;&#19968;&#31181;&#26356;&#39640;&#25928;&#12289;&#31283;&#20581;&#19988;&#36731;&#37327;&#32423;&#30340;&#21028;&#21035;&#24335;AE&#20998;&#31867;&#22120;&#21305;&#37197;&#26041;&#27861;&#65288;CFMatch&#65292;&#22823;&#23567;&#23567;&#20110;1MB&#65289;&#65292;&#32463;&#36807;&#35757;&#32451;&#21644;&#39564;&#35777;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#31572;&#26696;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering (QA) can only make progress if we know if an answer is correct, but for many of the most challenging and interesting QA examples, current evaluation metrics to determine answer equivalence (AE) often do not align with human judgments, particularly more verbose, free-form answers from large language models (LLM). There are two challenges: a lack of data and that models are too big: LLM-based scorers can correlate better with human judges, but this task has only been tested on limited QA datasets, and even when available, update of the model is limited because LLMs are large and often expensive. We rectify both of these issues by providing clear and consistent guidelines for evaluating AE in machine QA adopted from professional human QA contests. We also introduce a combination of standard evaluation and a more efficient, robust, and lightweight discriminate AE classifier-based matching method (CFMatch, smaller than 1 MB), trained and validated to more accurately evalu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#32534;&#31243;&#30340;&#26032;&#22411;&#35780;&#20272;&#22522;&#20934;&#65292;&#21253;&#25324;431&#20010;Python&#31243;&#24207;&#65292;&#37319;&#29992;pass@o&#24230;&#37327;&#25351;&#26631;&#26469;&#25552;&#20379;&#26356;&#20840;&#38754;&#21644;&#30456;&#20851;&#30340;OOP&#20195;&#30721;&#29983;&#25104;&#35780;&#20272;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#20195;&#30721;&#19987;&#29992;LLMs&#22312;OOP&#26041;&#38754;&#34920;&#29616;&#36739;&#24046;&#65292;&#38656;&#36827;&#19968;&#27493;&#25913;&#36827;&#27492;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2401.06628</link><description>&lt;p&gt;
OOP&#65306;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38754;&#21521;&#23545;&#35937;&#32534;&#31243;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
OOP: Object-Oriented Programming Evaluation Benchmark for Large Language Models. (arXiv:2401.06628v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#32534;&#31243;&#30340;&#26032;&#22411;&#35780;&#20272;&#22522;&#20934;&#65292;&#21253;&#25324;431&#20010;Python&#31243;&#24207;&#65292;&#37319;&#29992;pass@o&#24230;&#37327;&#25351;&#26631;&#26469;&#25552;&#20379;&#26356;&#20840;&#38754;&#21644;&#30456;&#20851;&#30340;OOP&#20195;&#30721;&#29983;&#25104;&#35780;&#20272;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#20195;&#30721;&#19987;&#29992;LLMs&#22312;OOP&#26041;&#38754;&#34920;&#29616;&#36739;&#24046;&#65292;&#38656;&#36827;&#19968;&#27493;&#25913;&#36827;&#27492;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#36827;&#33258;&#21160;&#21270;&#32534;&#31243;&#38656;&#35201;&#20581;&#22766;&#19988;&#20840;&#38754;&#30340;&#20195;&#30721;&#29983;&#25104;&#35780;&#20272;&#22522;&#20934;&#65292;&#28982;&#32780;&#24403;&#21069;&#30340;&#35780;&#20272;&#26694;&#26550;&#22312;&#21151;&#33021;&#24335;&#32534;&#31243;&#26041;&#38754;&#65288;&#20363;&#22914;HumanEval&#21644;MBPP&#65289;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#35270;&#20102;&#38754;&#21521;&#23545;&#35937;&#32534;&#31243;&#65288;OOP&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#39033;&#21019;&#26032;&#30340;&#38754;&#21521;&#23545;&#35937;&#32534;&#31243;&#37325;&#28857;&#22522;&#20934;&#65292;&#21253;&#25324;431&#20010;Python&#31243;&#24207;&#65292;&#28085;&#30422;&#20102;&#31867;&#21644;&#23553;&#35013;&#26041;&#27861;&#31561;&#22522;&#26412;&#30340;OOP&#27010;&#24565;&#21644;&#29305;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#24230;&#37327;&#25351;&#26631;pass@o&#65292;&#38024;&#23545;OOP&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#22686;&#24378;&#20256;&#32479;&#30340;pass@k&#24230;&#37327;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;23&#20010;&#39046;&#20808;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;&#36890;&#29992;&#27169;&#22411;&#21644;&#19987;&#38376;&#29992;&#20110;&#20195;&#30721;&#30340;&#27169;&#22411;&#65292;&#24471;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#21457;&#29616;&#65306;1&#65289;pass@o&#25552;&#20379;&#20102;&#26356;&#30456;&#20851;&#21644;&#20840;&#38754;&#30340;OOP&#20195;&#30721;&#29983;&#25104;&#35780;&#20272;&#65307;2&#65289;&#23613;&#31649;&#22312;FP&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20687;WizardCoder&#36825;&#26679;&#30340;&#20195;&#30721;&#19987;&#29992;LLMs&#22312;OOP&#26041;&#38754;&#33853;&#21518;&#20110;&#20687;ChatGPT&#36825;&#26679;&#30340;&#27169;&#22411;&#65307;3&#65289;&#25152;&#26377;&#20808;&#36827;&#30340;LLMs&#22312;&#25105;&#20204;&#30340;OOP&#22522;&#20934;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#31361;&#26174;&#20102;&#25913;&#36827;&#27492;&#39046;&#22495;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancing automated programming necessitates robust and comprehensive code generation benchmarks, yet current evaluation frameworks largely neglect object-oriented programming (OOP) in favor of functional programming (FP), e.g., HumanEval and MBPP. To address this, our study introduces a pioneering OOP-focused benchmark, featuring 431 Python programs that encompass essential OOP concepts and features like classes and encapsulation methods. We propose a novel evaluation metric, pass@o, tailored for OOP, enhancing traditional pass@k measures. Our evaluation of 23 leading large language models (LLMs), including both general and code-specialized models, reveals three key insights: 1) pass@o offers a more relevant and comprehensive assessment for OOP code generation; 2) Despite excelling in FP, code-specialized LLMs like WizardCoder lag in OOP compared to models like ChatGPT; 3) The poor performance of all advanced LLMs on our OOP benchmark highlights a critical need for improvements in thi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Intention Analysis Prompting (IAPrompt)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35302;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#25105;&#32416;&#27491;&#21644;&#25913;&#36827;&#33021;&#21147;&#26469;&#38450;&#24481;&#36234;&#29425;&#25915;&#20987;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#21709;&#24212;&#20013;&#30340;&#26377;&#23475;&#34892;&#20026;&#24182;&#20445;&#25345;&#25972;&#20307;&#26377;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06561</link><description>&lt;p&gt;
Intention Analysis Prompting&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#33391;&#22909;&#30340;&#36234;&#29425;&#38450;&#24481;&#32773;
&lt;/p&gt;
&lt;p&gt;
Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender. (arXiv:2401.06561v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Intention Analysis Prompting (IAPrompt)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35302;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#25105;&#32416;&#27491;&#21644;&#25913;&#36827;&#33021;&#21147;&#26469;&#38450;&#24481;&#36234;&#29425;&#25915;&#20987;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#21709;&#24212;&#20013;&#30340;&#26377;&#23475;&#34892;&#20026;&#24182;&#20445;&#25345;&#25972;&#20307;&#26377;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#23545;&#38544;&#34109;&#21644;&#22797;&#26434;&#30340;&#36234;&#29425;&#25915;&#20987;&#26102;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#26159;&#19968;&#39033;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#21363;Intention Analysis Prompting&#65288;IAPrompt&#65289;&#12290;&#20854;&#21407;&#29702;&#26159;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#36807;&#31243;&#35302;&#21457;LLMs&#30340;&#20869;&#22312;&#33258;&#25105;&#32416;&#27491;&#21644;&#25913;&#36827;&#33021;&#21147;&#65306;1&#65289;&#22522;&#26412;&#24847;&#22270;&#20998;&#26512;&#65292;2&#65289;&#19982;&#25919;&#31574;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;IAPrompt&#26159;&#19968;&#31181;&#20165;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#22240;&#27492;&#21487;&#20197;&#25552;&#39640;LLMs&#30340;&#23433;&#20840;&#24615;&#32780;&#19981;&#25439;&#23475;&#20854;&#26377;&#29992;&#24615;&#12290;&#22312;Vicuna&#12289;ChatGLM&#12289;MPT&#12289;DeepSeek&#21644;GPT-3.5&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;IAPrompt&#33021;&#22815;&#25345;&#32493;&#19988;&#26174;&#33879;&#22320;&#20943;&#23569;&#21709;&#24212;&#20013;&#30340;&#26377;&#23475;&#34892;&#20026;&#65288;&#24179;&#22343;&#25915;&#20987;&#25104;&#21151;&#29575;&#19979;&#38477;46.5%&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#26377;&#29992;&#24615;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20123;&#35265;&#35299;&#12290;&#20026;&#20102;&#20445;&#35777;&#21487;&#37325;&#22797;&#24615;&#65292;&#25105;&#20204;&#22312;https://github.com/alph&#19978;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#33050;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models (LLMs) with human values, particularly in the face of stealthy and complex jailbreaks, presents a formidable challenge. In this study, we present a simple yet highly effective defense strategy, i.e., Intention Analysis Prompting (IAPrompt). The principle behind is to trigger LLMs' inherent self-correct and improve ability through a two-stage process: 1) essential intention analysis, and 2) policy-aligned response. Notably, IAPrompt is an inference-only method, thus could enhance the safety of LLMs without compromising their helpfulness. Extensive experiments on SAP200 and DAN benchmarks across Vicuna, ChatGLM, MPT, DeepSeek, and GPT-3.5 show that IAPrompt could consistently and significantly reduce the harmfulness in response (averagely -46.5% attack success rate) and maintain the general helpfulness. Further analyses present some insights into how our method works. To facilitate reproducibility, We release our code and scripts at: https://github.com/alph
&lt;/p&gt;</description></item><item><title>LEGOBench&#26159;&#19968;&#20010;&#35780;&#20272;&#29983;&#25104;&#31185;&#23398;&#27169;&#22411;&#25490;&#34892;&#27036;&#31995;&#32479;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20351;&#29992;22&#24180;&#26469;&#30340;&#35770;&#25991;&#39044;&#21360;&#26412;&#25968;&#25454;&#21644;PapersWithCode&#38376;&#25143;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#25490;&#34892;&#27036;&#30340;&#25968;&#25454;&#65292;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#33258;&#21160;&#25490;&#34892;&#27036;&#29983;&#25104;&#23384;&#22312;&#26174;&#33879;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2401.06233</link><description>&lt;p&gt;
LEGOBench&#65306;&#31185;&#23398;&#27169;&#22411;&#25490;&#34892;&#27036;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
LEGOBench: Leaderboard Generation Benchmark for Scientific Models. (arXiv:2401.06233v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06233
&lt;/p&gt;
&lt;p&gt;
LEGOBench&#26159;&#19968;&#20010;&#35780;&#20272;&#29983;&#25104;&#31185;&#23398;&#27169;&#22411;&#25490;&#34892;&#27036;&#31995;&#32479;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20351;&#29992;22&#24180;&#26469;&#30340;&#35770;&#25991;&#39044;&#21360;&#26412;&#25968;&#25454;&#21644;PapersWithCode&#38376;&#25143;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#25490;&#34892;&#27036;&#30340;&#25968;&#25454;&#65292;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#33258;&#21160;&#25490;&#34892;&#27036;&#29983;&#25104;&#23384;&#22312;&#26174;&#33879;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35770;&#25991;&#25552;&#20132;&#25968;&#37327;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#38590;&#20197;&#21450;&#26102;&#20102;&#35299;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#30740;&#31350;&#25104;&#26524;&#25104;&#20026;&#20102;&#19968;&#20010;&#38590;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LEGOBench&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20272;&#29983;&#25104;&#25490;&#34892;&#27036;&#31995;&#32479;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;LEGOBench&#30001;22&#24180;&#26469;&#22312;arXiv&#19978;&#25552;&#20132;&#30340;&#39044;&#21360;&#26412;&#25968;&#25454;&#21644;PapersWithCode&#38376;&#25143;&#19978;&#30340;11,000&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#25490;&#34892;&#27036;&#32452;&#25104;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22235;&#31181;&#20256;&#32479;&#30340;&#22522;&#20110;&#22270;&#30340;&#25490;&#21517;&#21464;&#20307;&#21644;&#19977;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#33258;&#21160;&#25490;&#34892;&#27036;&#29983;&#25104;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/lingo-iitgn/LEGOBench&#33719;&#21462;&#65292;&#25968;&#25454;&#38598;&#25176;&#31649;&#22312;https://osf.io/9v2py/?view_only=6f91b0b510df498ba01595f8f278f94c&#12290;
&lt;/p&gt;
&lt;p&gt;
The ever-increasing volume of paper submissions makes it difficult to stay informed about the latest state-of-the-art research. To address this challenge, we introduce LEGOBench, a benchmark for evaluating systems that generate leaderboards. LEGOBench is curated from 22 years of preprint submission data in arXiv and more than 11,000 machine learning leaderboards in the PapersWithCode portal. We evaluate the performance of four traditional graph-based ranking variants and three recently proposed large language models. Our preliminary results show significant performance gaps in automatic leaderboard generation. The code is available on https://github.com/lingo-iitgn/LEGOBench and the dataset is hosted on https://osf.io/9v2py/?view_only=6f91b0b510df498ba01595f8f278f94c .
&lt;/p&gt;</description></item><item><title>&#33258;&#25105;&#31361;&#20986;&#24335;&#29369;&#35947;&#65288;SH2&#65289;&#26159;&#19968;&#31181;&#25512;&#29702;&#26102;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#39044;&#27979;&#27010;&#29575;&#36739;&#20302;&#30340;&#26631;&#35760;&#65292;&#24182;&#24378;&#35843;&#23427;&#20204;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#35299;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.05930</link><description>&lt;p&gt;
SH2: &#33258;&#25105;&#31361;&#20986;&#24335;&#29369;&#35947;&#24110;&#21161;&#24744;&#26356;&#20934;&#30830;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully. (arXiv:2401.05930v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05930
&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#31361;&#20986;&#24335;&#29369;&#35947;&#65288;SH2&#65289;&#26159;&#19968;&#31181;&#25512;&#29702;&#26102;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#39044;&#27979;&#27010;&#29575;&#36739;&#20302;&#30340;&#26631;&#35760;&#65292;&#24182;&#24378;&#35843;&#23427;&#20204;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;LLMs&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25512;&#29702;&#26102;&#26041;&#27861;&#65292;&#21363;&#33258;&#25105;&#31361;&#20986;&#24335;&#29369;&#35947;(SH2)&#65292;&#20197;&#24110;&#21161;LLMs&#26356;&#20934;&#30830;&#22320;&#35299;&#30721;&#12290;SH2&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#20013;&#19968;&#20010;&#31616;&#21333;&#30340;&#20107;&#23454;&#65292;&#21363;&#23545;&#20110;LLMs&#32780;&#35328;&#65292;&#39044;&#27979;&#27010;&#29575;&#36739;&#20302;&#30340;&#26631;&#35760;&#24448;&#24448;&#26356;&#20855;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;LLMs&#32473;&#20104;&#36739;&#20302;&#27010;&#29575;&#30340;&#26631;&#35760;&#26356;&#26377;&#21487;&#33021;&#19982;&#20107;&#23454;&#20449;&#24687;&#65288;&#22914;&#21517;&#35789;&#12289;&#19987;&#26377;&#21517;&#35789;&#21644;&#24418;&#23481;&#35789;&#65289;&#23494;&#20999;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#36873;&#25321;&#27010;&#29575;&#26368;&#20302;&#30340;&#26631;&#35760;&#24182;&#23558;&#20854;&#36830;&#25509;&#21040;&#21407;&#22987;&#19978;&#19979;&#25991;&#20013;&#26469;&#8220;&#31361;&#20986;&#8221;&#20107;&#23454;&#20449;&#24687;&#65292;&#20174;&#32780;&#36843;&#20351;&#27169;&#22411;&#22312;&#29983;&#25104;&#20043;&#21069;&#22810;&#27425;&#38405;&#35835;&#21644;&#29369;&#35947;&#36825;&#20123;&#26631;&#35760;&#12290;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#23545;&#27604;&#35299;&#30721;&#30340;&#26041;&#24335;&#26469;&#24378;&#35843;&#30001;&#29369;&#35947;&#24102;&#26469;&#30340;&#36755;&#20986;&#27010;&#29575;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) demonstrate great performance in text generation. However, LLMs are still suffering from hallucinations. In this work, we propose an inference-time method, Self-Highlighted Hesitation (SH2), to help LLMs decode more truthfully. SH2 is based on a simple fact rooted in information theory that for an LLM, the tokens predicted with lower probabilities are prone to be more informative than others. Our analysis shows that the tokens assigned with lower probabilities by an LLM are more likely to be closely related to factual information, such as nouns, proper nouns, and adjectives. Therefore, we propose to ''highlight'' the factual information by selecting the tokens with the lowest probabilities and concatenating them to the original context, thus forcing the model to repeatedly read and hesitate on these tokens before generation. During decoding, we also adopt contrastive decoding to emphasize the difference in the output probabilities brought by the hesitation.
&lt;/p&gt;</description></item><item><title>ANGO&#26159;&#19968;&#20010;&#20013;&#25991;&#39046;&#22495;&#29983;&#25104;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22522;&#20934;&#65292;&#24341;&#20837;&#20102;&#20851;&#38190;&#28857;&#20998;&#31867;&#26631;&#20934;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#24314;&#31435;&#20102;&#21487;&#37327;&#21270;&#30340;&#38382;&#39064;&#38590;&#24230;&#26631;&#20934;&#65292;&#23545;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2401.04898</link><description>&lt;p&gt;
ANGO: &#19968;&#20010;&#38754;&#21521;&#29983;&#25104;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#25991;&#39046;&#22495;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ANGO: A Next-Level Evaluation Benchmark For Generation-Oriented Language Models In Chinese Domain. (arXiv:2401.04898v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04898
&lt;/p&gt;
&lt;p&gt;
ANGO&#26159;&#19968;&#20010;&#20013;&#25991;&#39046;&#22495;&#29983;&#25104;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22522;&#20934;&#65292;&#24341;&#20837;&#20102;&#20851;&#38190;&#28857;&#20998;&#31867;&#26631;&#20934;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#24314;&#31435;&#20102;&#21487;&#37327;&#21270;&#30340;&#38382;&#39064;&#38590;&#24230;&#26631;&#20934;&#65292;&#23545;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#21508;&#31181;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#20294;&#20854;&#20013;&#22823;&#22810;&#25968;&#23384;&#22312;&#25490;&#21517;&#22833;&#30495;&#21644;&#27169;&#22411;&#33021;&#21147;&#20998;&#26512;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;ANGO&#65292;&#19968;&#20010;&#20013;&#25991;&#22810;&#39033;&#36873;&#25321;&#39064;&#35780;&#20272;&#22522;&#20934;&#12290;ANGO&#39318;&#27425;&#25552;&#20986;&#20102;&#8220;&#20851;&#38190;&#28857;&#8221;&#20998;&#31867;&#26631;&#20934;&#65292;ANGO&#20013;&#30340;&#27599;&#20010;&#38382;&#39064;&#21487;&#20197;&#23545;&#24212;&#22810;&#20010;&#20851;&#38190;&#28857;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#35780;&#20272;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22522;&#20110;&#30495;&#20154;&#34920;&#29616;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#21487;&#37327;&#21270;&#30340;&#38382;&#39064;&#38590;&#24230;&#26631;&#20934;&#65292;&#24182;&#23558;ANGO&#38382;&#39064;&#20998;&#20026;9&#20010;&#38590;&#24230;&#32423;&#21035;&#65292;&#20026;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#30340;&#25351;&#23548;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;&#25968;&#25454;&#27844;&#28431;&#30340;&#24433;&#21709;&#24182;&#20805;&#20998;&#21033;&#29992;ANGO&#30340;&#21019;&#26032;&#29305;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#29420;&#23478;&#25277;&#26679;&#31574;&#30053;&#21644;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#25903;&#25345;&#24555;&#36895;&#27979;&#35797;&#38598;&#36845;&#20195;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;ANGO&#23545;&#27169;&#22411;&#25552;&#20986;&#20102;&#26356;&#22823;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#35780;&#20272;&#32467;&#26524;&#20013;&#25581;&#31034;&#20986;&#26356;&#22810;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, various Large Language Models (LLMs) evaluation datasets have emerged, but most of them have issues with distorted rankings and difficulty in model capabilities analysis. Addressing these concerns, this paper introduces ANGO, a Chinese multi-choice question evaluation benchmark. ANGO proposes \textit{Keypoint} categorization standard for the first time, each question in ANGO can correspond to multiple keypoints, effectively enhancing interpretability of evaluation results. Base on performance of real humans, we build a quantifiable question difficulty standard and divide ANGO questions into 9 difficulty levels, which provide more precise guidance for model training. To minimize data leakage impact and fully leverage ANGO's innovative features, we have engineered exclusive sampling strategies and a new evaluation framework that support swift testset iteration. Our experiments demonstrate that ANGO poses a stronger challenge to models and reveals more details in evaluation resu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#27010;&#24565;&#32423;&#21035;&#30340;&#35823;&#30456;&#20851;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;ChatGPT&#20998;&#37197;&#27010;&#24565;&#26631;&#31614;&#21644;&#24341;&#20837;&#25968;&#25454;&#20877;&#24179;&#34913;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.08648</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;&#32423;&#21035;&#30340;&#35823;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Explore Spurious Correlations at the Concept Level in Language Models for Text Classification. (arXiv:2311.08648v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.08648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#27010;&#24565;&#32423;&#21035;&#30340;&#35823;&#30456;&#20851;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;ChatGPT&#20998;&#37197;&#27010;&#24565;&#26631;&#31614;&#21644;&#24341;&#20837;&#25968;&#25454;&#20877;&#24179;&#34913;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#20247;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#37319;&#29992;&#20102;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#12290;&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#26631;&#31614;&#20998;&#24067;&#19981;&#24179;&#34913;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#20363;&#20135;&#29983;&#30340;&#35823;&#30456;&#20851;&#24615;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#40065;&#26834;&#24615;&#25361;&#25112;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35789;&#35821;&#12289;&#30701;&#35821;&#21644;&#21477;&#27861;&#29305;&#24449;&#19978;&#65292;&#24573;&#35270;&#20102;&#27010;&#24565;&#32423;&#21035;&#30340;&#30740;&#31350;&#65292;&#36825;&#24448;&#24448;&#26159;&#30001;&#20110;&#32570;&#20047;&#27010;&#24565;&#26631;&#31614;&#21644;&#38590;&#20197;&#30830;&#23450;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#27010;&#24565;&#20869;&#23481;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#20026;&#25991;&#26412;&#20998;&#37197;&#27010;&#24565;&#26631;&#31614;&#65292;&#35780;&#20272;&#27169;&#22411;&#22312;&#24494;&#35843;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#27979;&#35797;&#25968;&#25454;&#20013;&#30340;&#27010;&#24565;&#20559;&#24046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#25110;&#25552;&#31034;&#20013;&#36935;&#21040;&#27010;&#24565;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#35823;&#30456;&#20851;&#24615;&#26102;&#65292;&#20250;&#37319;&#21462;&#39044;&#27979;&#30340;&#25463;&#24452;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25968;&#25454;&#20877;&#24179;&#34913;&#25216;&#26415;&#65292;&#23558;ChatGPT&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#32435;&#20837;&#20854;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have achieved notable success in numerous NLP tasks, employing both fine-tuning and in-context learning (ICL) methods. While language models demonstrate exceptional performance, they face robustness challenges due to spurious correlations arising from imbalanced label distributions in training data or ICL exemplars. Previous research has primarily concentrated on word, phrase, and syntax features, neglecting the concept level, often due to the absence of concept labels and difficulty in identifying conceptual content in input texts. This paper introduces two main contributions. First, we employ ChatGPT to assign concept labels to texts, assessing concept bias in models during fine-tuning or ICL on test data. We find that LMs, when encountering spurious correlations between a concept and a label in training or prompts, resort to shortcuts for predictions. Second, we introduce a data rebalancing technique that incorporates ChatGPT-generated counterfactual data, ther
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25345;&#32493;&#23398;&#20064;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#22312;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21069;&#21521;&#36716;&#31227;&#25928;&#26524;&#36739;&#22909;&#19988;&#19982;&#35821;&#35328;&#39034;&#24207;&#26080;&#20851;&#65292;&#20294;&#21518;&#21521;&#36716;&#31227;&#25928;&#26524;&#21487;&#33021;&#21462;&#20915;&#20110;&#26032;&#35821;&#35328;&#30340;&#39034;&#24207;&#21644;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2311.01200</link><description>&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study of Continual Learning Under Language Shift. (arXiv:2311.01200v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25345;&#32493;&#23398;&#20064;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#22312;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21069;&#21521;&#36716;&#31227;&#25928;&#26524;&#36739;&#22909;&#19988;&#19982;&#35821;&#35328;&#39034;&#24207;&#26080;&#20851;&#65292;&#20294;&#21518;&#21521;&#36716;&#31227;&#25928;&#26524;&#21487;&#33021;&#21462;&#20915;&#20110;&#26032;&#35821;&#35328;&#30340;&#39034;&#24207;&#21644;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#23548;&#33268;&#20102;&#24040;&#22823;&#30340;&#35757;&#32451;&#25104;&#26412;&#12290;&#22312;&#38543;&#26102;&#38388;&#25512;&#31227;&#32780;&#20986;&#29616;&#26032;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#26356;&#26032;&#27169;&#22411;&#32780;&#19981;&#26159;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#25910;&#30410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26032;&#35821;&#35328;&#20986;&#29616;&#26102;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#26102;&#30340;&#22909;&#22788;&#21644;&#24330;&#31471;&#65292;&#21363;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#24773;&#20917;&#12290;&#20174;&#21333;&#35821;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#20986;&#21457;&#65292;&#25105;&#20204;&#36880;&#27493;&#28155;&#21152;&#20102;&#26469;&#33258;&#25386;&#23041;&#35821;&#21644;&#20912;&#23707;&#35821;&#30340;&#25968;&#25454;&#65292;&#20197;&#30740;&#31350;&#21069;&#21521;&#21644;&#21518;&#21521;&#36716;&#31227;&#25928;&#26524;&#22914;&#20309;&#21462;&#20915;&#20110;&#39044;&#35757;&#32451;&#39034;&#24207;&#21644;&#35821;&#35328;&#29305;&#24449;&#65292;&#23545;&#20110;&#19981;&#21516;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#23398;&#20064;&#29575;&#35843;&#24230;&#22120;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#21069;&#21521;&#36716;&#31227;&#20027;&#35201;&#26159;&#27491;&#21521;&#30340;&#65292;&#19981;&#21463;&#35821;&#35328;&#39034;&#24207;&#30340;&#24433;&#21709;&#65292;&#20294;&#21518;&#21521;&#36716;&#31227;&#21017;&#21487;&#33021;&#26159;&#27491;&#21521;&#30340;&#25110;&#36127;&#21521;&#30340;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#26032;&#35821;&#35328;&#30340;&#39034;&#24207;&#21644;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#37322;&#36825;&#20123;&#27169;&#24335;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20960;&#31181;&#35821;&#35328;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent increase in data and model scale for language model pre-training has led to huge training costs. In scenarios where new data become available over time, updating a model instead of fully retraining it would therefore provide significant gains. In this paper, we study the benefits and downsides of updating a language model when new data comes from new languages - the case of continual learning under language shift. Starting from a monolingual English language model, we incrementally add data from Norwegian and Icelandic to investigate how forward and backward transfer effects depend on the pre-training order and characteristics of languages, for different model sizes and learning rate schedulers. Our results show that, while forward transfer is largely positive and independent of language order, backward transfer can be either positive or negative depending on the order and characteristics of new languages. To explain these patterns we explore several language similarity metr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#29109;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#20943;&#23569;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#23545;&#24120;&#35265;&#21644;&#26131;&#23398;&#26631;&#35760;&#30340;&#20559;&#22909;&#65292;&#20351;&#20854;&#26356;&#20851;&#27880;&#19981;&#24120;&#35265;&#21644;&#38590;&#23398;&#30340;&#26631;&#35760;&#12290;</title><link>http://arxiv.org/abs/2310.19531</link><description>&lt;p&gt;
&#20943;&#23569;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#22256;&#38590;&#30340;&#20449;&#24687;&#29109;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
InfoEntropy Loss to Mitigate Bias of Learning Difficulties for Generative Language Models. (arXiv:2310.19531v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19531
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#29109;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#20943;&#23569;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#23545;&#24120;&#35265;&#21644;&#26131;&#23398;&#26631;&#35760;&#30340;&#20559;&#22909;&#65292;&#20351;&#20854;&#26356;&#20851;&#27880;&#19981;&#24120;&#35265;&#21644;&#38590;&#23398;&#30340;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#39044;&#27979;&#19978;&#19968;&#20010;&#26631;&#35760;&#65288;&#23376;&#35789;/&#35789;/&#30701;&#35821;&#65289;&#32473;&#20986;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#26469;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#35268;&#27169;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#24120;&#24573;&#35270;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#30340;&#22266;&#26377;&#25361;&#25112;&#65292;&#21363;&#39057;&#32321;&#26631;&#35760;&#21644;&#19981;&#32463;&#24120;&#20986;&#29616;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#35821;&#35328;&#27169;&#22411;&#34987;&#24120;&#35265;&#19988;&#26131;&#23398;&#30340;&#26631;&#35760;&#25152;&#20027;&#23548;&#65292;&#20174;&#32780;&#24573;&#35270;&#19981;&#32463;&#24120;&#20986;&#29616;&#19988;&#38590;&#20197;&#23398;&#20064;&#30340;&#26631;&#35760;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#29109;&#25439;&#22833;&#65288;InfoEntropy Loss&#65289;&#20989;&#25968;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#30456;&#24212;&#30340;&#39044;&#27979;&#27010;&#29575;&#20998;&#24067;&#30340;&#20449;&#24687;&#29109;&#21160;&#24577;&#35780;&#20272;&#24453;&#23398;&#20064;&#26631;&#35760;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;&#28982;&#21518;&#65292;&#23427;&#36866;&#24212;&#22320;&#35843;&#25972;&#35757;&#32451;&#25439;&#22833;&#65292;&#35797;&#22270;&#20351;&#27169;&#22411;&#26356;&#21152;&#20851;&#27880;&#38590;&#20197;&#23398;&#20064;&#30340;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative language models are usually pretrained on large text corpus via predicting the next token (i.e., sub-word/word/phrase) given the previous ones. Recent works have demonstrated the impressive performance of large generative language models on downstream tasks. However, existing generative language models generally neglect an inherent challenge in text corpus during training, i.e., the imbalance between frequent tokens and infrequent ones. It can lead a language model to be dominated by common and easy-to-learn tokens, thereby overlooking the infrequent and difficult-to-learn ones. To alleviate that, we propose an Information Entropy Loss (InfoEntropy Loss) function. During training, it can dynamically assess the learning difficulty of a to-be-learned token, according to the information entropy of the corresponding predicted probability distribution over the vocabulary. Then it scales the training loss adaptively, trying to lead the model to focus more on the difficult-to-learn
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#24471;&#20998;&#29109;&#36825;&#19968;&#26032;&#39062;&#30340;&#31163;&#25955;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#65292;&#24357;&#34917;&#20102;&#31163;&#25955;&#25968;&#25454;&#39046;&#22495;&#20013;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#25552;&#20986;&#20102;&#24471;&#20998;&#29109;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;(SEDD)&#24182;&#22312;GPT-2&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.16834</link><description>&lt;p&gt;
&#36890;&#36807;&#20272;&#35745;&#25968;&#25454;&#20998;&#24067;&#27604;&#20363;&#30340;&#31163;&#25955;&#25193;&#25955;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Discrete Diffusion Language Modeling by Estimating the Ratios of the Data Distribution. (arXiv:2310.16834v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#24471;&#20998;&#29109;&#36825;&#19968;&#26032;&#39062;&#30340;&#31163;&#25955;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#65292;&#24357;&#34917;&#20102;&#31163;&#25955;&#25968;&#25454;&#39046;&#22495;&#20013;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#25552;&#20986;&#20102;&#24471;&#20998;&#29109;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;(SEDD)&#24182;&#22312;GPT-2&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#22312;&#35768;&#22810;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#20013;&#20855;&#26377;&#31361;&#30772;&#24615;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#33258;&#28982;&#35821;&#35328;&#31561;&#31163;&#25955;&#25968;&#25454;&#39046;&#22495;&#20013;&#21364;&#34920;&#29616;&#19981;&#20339;&#12290;&#20851;&#38190;&#26159;&#65292;&#26631;&#20934;&#30340;&#25193;&#25955;&#27169;&#22411;&#20381;&#36182;&#20110;&#25104;&#29087;&#30340;&#24471;&#20998;&#21305;&#37197;&#29702;&#35770;&#65292;&#20294;&#26159;&#23558;&#20854;&#25512;&#24191;&#21040;&#31163;&#25955;&#32467;&#26500;&#24182;&#27809;&#26377;&#21462;&#24471;&#30456;&#21516;&#30340;&#32463;&#39564;&#25910;&#30410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#24471;&#20998;&#29109;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#25955;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#65292;&#26469;&#24357;&#34917;&#36825;&#20010;&#24046;&#36317;&#65292;&#23427;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#31283;&#23450;&#65292;&#21487;&#20197;&#24418;&#25104;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#30340;ELBO&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#21435;&#22122;&#21464;&#20307;&#39640;&#25928;&#20248;&#21270;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#24471;&#20998;&#29109;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#65288;SEDD&#65289;&#25193;&#23637;&#21040;GPT-2&#30340;&#23454;&#39564;&#35774;&#32622;&#20013;&#65292;&#23454;&#29616;&#20102;&#26497;&#20855;&#31454;&#20105;&#21147;&#30340;&#20284;&#28982;&#24230;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#29420;&#29305;&#30340;&#31639;&#27861;&#20248;&#21183;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#27604;&#36739;&#22823;&#23567;&#30456;&#20284;&#30340;SEDD&#21644;GPT-2&#27169;&#22411;&#26102;&#65292;SEDD&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#22256;&#24785;&#24230;&#65288;&#36890;&#24120;&#22312;&#22522;&#32447;&#30340;+$10\%$&#20869;&#65292;&#24182;&#19988;&#26377;&#26102;&#36229;&#36807;&#22522;&#32447;&#65289;&#12290;&#27492;&#22806;&#65292;SEDD&#27169;&#22411;&#23398;&#21040;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Despite their groundbreaking performance for many generative modeling tasks, diffusion models have fallen short on discrete data domains such as natural language. Crucially, standard diffusion models rely on the well-established theory of score matching, but efforts to generalize this to discrete structures have not yielded the same empirical gains. In this work, we bridge this gap by proposing score entropy, a novel discrete score matching loss that is more stable than existing methods, forms an ELBO for maximum likelihood training, and can be efficiently optimized with a denoising variant. We scale our Score Entropy Discrete Diffusion models (SEDD) to the experimental setting of GPT-2, achieving highly competitive likelihoods while also introducing distinct algorithmic advantages. In particular, when comparing similarly sized SEDD and GPT-2 models, SEDD attains comparable perplexities (normally within $+10\%$ of and sometimes outperforming the baseline). Furthermore, SEDD models lear
&lt;/p&gt;</description></item><item><title>QLLM&#26159;&#19968;&#31181;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#20934;&#30830;&#39640;&#25928;&#30340;&#20302;&#20301;&#23485;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#36890;&#36947;&#37325;&#32452;&#25216;&#26415;&#65292;&#23558;&#31163;&#32676;&#20540;&#30340;&#22823;&#23567;&#37325;&#26032;&#20998;&#37197;&#32473;&#20854;&#20182;&#36890;&#36947;&#65292;&#20174;&#32780;&#20943;&#36731;&#23427;&#20204;&#23545;&#37327;&#21270;&#33539;&#22260;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.08041</link><description>&lt;p&gt;
QLLM: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#39640;&#25928;&#20302;&#20301;&#23485;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models. (arXiv:2310.08041v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08041
&lt;/p&gt;
&lt;p&gt;
QLLM&#26159;&#19968;&#31181;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#20934;&#30830;&#39640;&#25928;&#30340;&#20302;&#20301;&#23485;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#36890;&#36947;&#37325;&#32452;&#25216;&#26415;&#65292;&#23558;&#31163;&#32676;&#20540;&#30340;&#22823;&#23567;&#37325;&#26032;&#20998;&#37197;&#32473;&#20854;&#20182;&#36890;&#36947;&#65292;&#20174;&#32780;&#20943;&#36731;&#23427;&#20204;&#23545;&#37327;&#21270;&#33539;&#22260;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#20854;&#25152;&#38656;&#36164;&#28304;&#36807;&#22823;&#65292;&#38480;&#21046;&#20102;&#20854;&#24191;&#27867;&#24212;&#29992;&#12290;&#34429;&#28982;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;Quantization-Aware Training&#65292;QAT&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#30340;&#35757;&#32451;&#25104;&#26412;&#36807;&#39640;&#65292;&#22240;&#27492;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;Post-Training Quantization&#65292;PTQ&#65289;&#25104;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26356;&#23454;&#38469;&#30340;&#26041;&#27861;&#12290;&#22312;&#29616;&#26377;&#30740;&#31350;&#20013;&#65292;&#29305;&#23450;&#36890;&#36947;&#20013;&#30340;&#28608;&#27963;&#31163;&#32676;&#20540;&#34987;&#35748;&#20026;&#26159;&#23548;&#33268;&#21518;&#35757;&#32451;&#37327;&#21270;&#20934;&#30830;&#24615;&#19979;&#38477;&#30340;&#29942;&#39048;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;QLLM&#65292;&#19968;&#31181;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#20934;&#30830;&#39640;&#25928;&#30340;&#20302;&#20301;&#23485;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12290;QLLM&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36890;&#36947;&#37325;&#32452;&#25216;&#26415;&#65292;&#23558;&#31163;&#32676;&#20540;&#30340;&#22823;&#23567;&#37325;&#26032;&#20998;&#37197;&#32473;&#20854;&#20182;&#36890;&#36947;&#65292;&#20174;&#32780;&#20943;&#36731;&#23427;&#20204;&#23545;&#37327;&#21270;&#33539;&#22260;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#36890;&#36947;&#25286;&#20998;&#21644;&#36890;&#36947;&#32452;&#35013;&#65292;&#22312;&#20445;&#35777;&#20302;&#20301;&#23485;&#30340;&#24773;&#20917;&#19979;&#23558;&#31163;&#32676;&#36890;&#36947;&#20998;&#35299;&#25104;&#22810;&#20010;&#23376;&#36890;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) excel in NLP, but their demands hinder their widespread deployment. While Quantization-Aware Training (QAT) offers a solution, its extensive training costs make Post-Training Quantization (PTQ) a more practical approach for LLMs. In existing studies, activation outliers in particular channels are identified as the bottleneck to PTQ accuracy. They propose to transform the magnitudes from activations to weights, which however offers limited alleviation or suffers from unstable gradients, resulting in a severe performance drop at low-bitwidth. In this paper, we propose QLLM, an accurate and efficient low-bitwidth PTQ method designed for LLMs. QLLM introduces an adaptive channel reassembly technique that reallocates the magnitude of outliers to other channels, thereby mitigating their impact on the quantization range. This is achieved by channel disassembly and channel assembly, which first breaks down the outlier channels into several sub-channels to ensure a 
&lt;/p&gt;</description></item><item><title>GoLLIE &#26159;&#19968;&#20010;&#36981;&#24490;&#27880;&#37322;&#25351;&#21335;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#20197;&#25913;&#36827;&#26410;&#35265;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03668</link><description>&lt;p&gt;
GoLLIE:&#27880;&#37322;&#25351;&#21335;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#20449;&#24687;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction. (arXiv:2310.03668v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03668
&lt;/p&gt;
&lt;p&gt;
GoLLIE &#26159;&#19968;&#20010;&#36981;&#24490;&#27880;&#37322;&#25351;&#21335;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#20197;&#25913;&#36827;&#26410;&#35265;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#32467;&#21512;&#25351;&#23548;&#35843;&#20248;&#24050;&#32463;&#22312;&#27867;&#21270;&#21040;&#26410;&#35265;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#20449;&#24687;&#25277;&#21462; (IE) &#26041;&#38754;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#36739;&#24046;&#65292;&#33853;&#21518;&#20110;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#36890;&#24120;&#65292;IE &#20219;&#21153;&#30340;&#29305;&#28857;&#26159;&#22797;&#26434;&#30340;&#27880;&#37322;&#25351;&#21335;&#65292;&#25551;&#36848;&#20219;&#21153;&#24182;&#32473;&#20986;&#31034;&#20363;&#32473;&#20154;&#31867;&#12290;&#20808;&#21069;&#21033;&#29992;&#36825;&#26679;&#30340;&#20449;&#24687;&#30340;&#23581;&#35797;&#37117;&#22833;&#36133;&#20102;&#65292;&#21363;&#20351;&#20351;&#29992;&#26368;&#22823;&#30340;&#27169;&#22411;&#65292;&#23427;&#20204;&#20063;&#19981;&#33021;&#30452;&#25509;&#36981;&#24490;&#25351;&#21335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#20449;&#24687;&#25277;&#21462;&#30340;&#25351;&#21335;&#36981;&#24490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; GoLLIE (Guideline-following Large Language Model for IE)&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#24494;&#35843;&#20197;&#36981;&#23432;&#27880;&#37322;&#25351;&#21335;&#65292;&#20174;&#32780;&#33021;&#22815;&#25913;&#36827;&#26410;&#35265; IE &#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#32467;&#26524;&#12290;&#20840;&#38754;&#30340;&#35780;&#20272;&#23454;&#35777;&#34920;&#26126;&#65292;GoLLIE &#33021;&#22815;&#27867;&#21270;&#24182;&#36981;&#24490;&#26410;&#35265;&#25351;&#21335;&#65292;&#22312;&#38646;&#26679;&#26412;&#20449;&#24687;&#25277;&#21462;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#23581;&#35797;&#12290;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#35814;&#32454;&#30340;&#25351;&#21335;&#26159;&#21462;&#24471;&#33391;&#22909;&#32467;&#26524;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) combined with instruction tuning have made significant progress when generalizing to unseen tasks. However, they have been less successful in Information Extraction (IE), lagging behind task-specific models. Typically, IE tasks are characterized by complex annotation guidelines which describe the task and give examples to humans. Previous attempts to leverage such information have failed, even with the largest models, as they are not able to follow the guidelines out-of-the-box. In this paper we propose GoLLIE (Guideline-following Large Language Model for IE), a model able to improve zero-shot results on unseen IE tasks by virtue of being fine-tuned to comply with annotation guidelines. Comprehensive evaluation empirically demonstrates that GoLLIE is able to generalize to and follow unseen guidelines, outperforming previous attempts at zero-shot information extraction. The ablation study shows that detailed guidelines is key for good results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#32852;&#24819;&#35760;&#24518;&#20013;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#36890;&#36807;&#39640;&#32500;&#30697;&#38453;&#21644;&#23884;&#20837;&#30340;&#22806;&#31215;&#26469;&#27169;&#25311;&#20869;&#23618;Transformer&#35821;&#35328;&#27169;&#22411;&#12290;&#20316;&#32773;&#25512;&#23548;&#20986;&#20102;&#19982;&#26679;&#26412;&#25968;&#37327;&#21644;&#21442;&#25968;&#22823;&#23567;&#30456;&#20851;&#30340;&#31934;&#30830;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#23637;&#31034;&#20102;&#23384;&#20648;&#35760;&#24518;&#20851;&#32852;&#30340;&#32454;&#31890;&#24230;&#21487;&#35270;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.02984</link><description>&lt;p&gt;
&#32553;&#25918;&#23450;&#24459;&#22312;&#32852;&#24819;&#35760;&#24518;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Scaling Laws for Associative Memories. (arXiv:2310.02984v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#32852;&#24819;&#35760;&#24518;&#20013;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#36890;&#36807;&#39640;&#32500;&#30697;&#38453;&#21644;&#23884;&#20837;&#30340;&#22806;&#31215;&#26469;&#27169;&#25311;&#20869;&#23618;Transformer&#35821;&#35328;&#27169;&#22411;&#12290;&#20316;&#32773;&#25512;&#23548;&#20986;&#20102;&#19982;&#26679;&#26412;&#25968;&#37327;&#21644;&#21442;&#25968;&#22823;&#23567;&#30456;&#20851;&#30340;&#31934;&#30830;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#23637;&#31034;&#20102;&#23384;&#20648;&#35760;&#24518;&#20851;&#32852;&#30340;&#32454;&#31890;&#24230;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#24456;&#21487;&#33021;&#28041;&#21450;&#21040;&#25277;&#35937;&#35268;&#21017;&#30340;&#21457;&#29616;&#21644;&#35760;&#24518;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#32852;&#24819;&#35760;&#24518;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#39640;&#32500;&#30697;&#38453;&#65292;&#30001;&#23884;&#20837;&#30340;&#22806;&#31215;&#32452;&#25104;&#65292;&#19982;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#23618;&#30456;&#20851;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20851;&#20110;&#26679;&#26412;&#25968;&#37327;&#21644;&#21442;&#25968;&#35268;&#27169;&#30340;&#31934;&#30830;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#35752;&#35770;&#20102;&#19981;&#21516;&#20272;&#35745;&#22120;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#21253;&#25324;&#22522;&#20110;&#20248;&#21270;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#20197;&#39564;&#35777;&#21644;&#35299;&#37322;&#29702;&#35770;&#32467;&#26524;&#65292;&#21253;&#25324;&#23545;&#23384;&#20648;&#35760;&#24518;&#20851;&#32852;&#30340;&#32454;&#31890;&#24230;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning arguably involves the discovery and memorization of abstract rules. The aim of this paper is to study associative memory mechanisms. Our model is based on high-dimensional matrices consisting of outer products of embeddings, which relates to the inner layers of transformer language models. We derive precise scaling laws with respect to sample size and parameter size, and discuss the statistical efficiency of different estimators, including optimization-based algorithms. We provide extensive numerical experiments to validate and interpret theoretical results, including fine-grained visualizations of the stored memory associations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#21521;&#27861;&#23448;&#25552;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#26469;&#35780;&#20272;LLMs&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;LLMs&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.01468</link><description>&lt;p&gt;
&#23454;&#20307;&#25512;&#26029;&#31454;&#25216;&#22330;&#65306;&#25506;&#31350;LLMs&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#30340;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
The Entity-Deduction Arena: A playground for probing the conversational reasoning and planning capabilities of LLMs. (arXiv:2310.01468v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#21521;&#27861;&#23448;&#25552;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#26469;&#35780;&#20272;LLMs&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;LLMs&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#26126;&#30830;&#25552;&#38382;&#26102;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#21547;&#31946;&#19981;&#28165;&#30340;&#26597;&#35810;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#34892;&#20026;&#38590;&#20197;&#39044;&#27979;&#24182;&#20135;&#29983;&#38169;&#35823;&#30340;&#36755;&#20986;&#12290;&#36825;&#20984;&#26174;&#20102;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#25552;&#20986;&#28548;&#28165;&#38382;&#39064;&#20197;&#26377;&#25928;&#35299;&#20915;&#27495;&#20041;&#30340;&#26234;&#33021;&#20195;&#29702;&#30340;&#38656;&#27714;&#12290;&#36825;&#31181;&#33021;&#21147;&#38656;&#35201;&#23545;&#22810;&#20010;&#23545;&#35805;&#36718;&#27425;&#36827;&#34892;&#22797;&#26434;&#30340;&#29702;&#35299;&#12289;&#29366;&#24577;&#36319;&#36394;&#12289;&#25512;&#29702;&#21644;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#27979;&#37327;&#36825;&#31181;&#33021;&#21147;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#21521;&#27861;&#23448;&#25552;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#65292;&#35780;&#20272;&#20102;LLMs&#25512;&#26029;&#33258;&#24049;&#19981;&#30693;&#36947;&#20294;&#34987;&#27861;&#23448;&#25581;&#31034;&#30340;&#23454;&#20307;&#30340;&#33021;&#21147;&#12290;&#36825;&#20010;&#8220;&#23454;&#20307;&#25512;&#26029;&#28216;&#25103;&#8221;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#21508;&#31181;LLMs&#65292;&#24182;&#21457;&#29616;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#23427;&#20204;&#30340;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#24378;&#22823;&#30340;LLMs...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently effective at answering questions that are clearly asked. However, when faced with ambiguous queries they can act unpredictably and produce incorrect outputs. This underscores the need for the development of intelligent agents capable of asking clarification questions to resolve ambiguities effectively. This capability requires complex understanding, state tracking, reasoning and planning over multiple conversational turns. However, directly measuring this can be challenging. In this paper, we offer a surrogate problem which assesses an LLMs's capability to deduce an entity unknown to itself, but revealed to a judge, by asking the judge a series of queries. This \textit{entity-deducing game} can serve as an evaluation framework to probe the conversational reasoning and planning capabilities of language models. We systematically evaluate various LLMs and discover significant differences in their performance on this task. We find that strong LLMs
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#34920;&#38754;&#37325;&#22797;&#29616;&#35937;&#30340;&#35282;&#24230;&#26469;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#24182;&#23454;&#35777;&#20102;&#19968;&#31181;&#22686;&#24378;&#26631;&#35760;&#20851;&#31995;&#30340;&#21407;&#21017;&#65292;&#20026;&#29702;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;&#21450;&#20854;&#28508;&#22312;&#23616;&#38480;&#24615;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.00297</link><description>&lt;p&gt;
&#29702;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#37325;&#22797;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Understanding In-Context Learning from Repetitions. (arXiv:2310.00297v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#34920;&#38754;&#37325;&#22797;&#29616;&#35937;&#30340;&#35282;&#24230;&#26469;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#24182;&#23454;&#35777;&#20102;&#19968;&#31181;&#22686;&#24378;&#26631;&#35760;&#20851;&#31995;&#30340;&#21407;&#21017;&#65292;&#20026;&#29702;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;&#21450;&#20854;&#28508;&#22312;&#23616;&#38480;&#24615;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38590;&#20197;&#25417;&#25720;&#30340;&#26426;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#34920;&#38754;&#37325;&#22797;&#29616;&#35937;&#30340;&#35282;&#24230;&#26469;&#26816;&#35270;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#23450;&#37327;&#22320;&#30740;&#31350;&#20102;&#34920;&#38754;&#29305;&#24449;&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#20316;&#29992;&#65292;&#21516;&#26102;&#23454;&#35777;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;&#8220;&#26631;&#35760;&#20849;&#29616;&#24378;&#21270;&#8221;&#30340;&#21407;&#21017;&#65292;&#35813;&#21407;&#21017;&#36890;&#36807;&#22686;&#24378;&#20004;&#20010;&#26631;&#35760;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#22522;&#20110;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#20849;&#29616;&#12290;&#36890;&#36807;&#30740;&#31350;&#36825;&#20123;&#29305;&#24449;&#30340;&#21452;&#37325;&#24433;&#21709;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#38416;&#26126;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20869;&#22312;&#26426;&#21046;&#65292;&#24182;&#23545;&#20854;&#22833;&#36133;&#30340;&#21407;&#22240;&#36827;&#34892;&#20102;&#35299;&#37322;&#12290;&#26412;&#35770;&#25991;&#23545;&#20110;&#29702;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;&#21450;&#20854;&#28508;&#22312;&#23616;&#38480;&#24615;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#65292;&#20026;&#36825;&#19968;&#28608;&#21160;&#20154;&#24515;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the elusive mechanism underpinning in-context learning in Large Language Models (LLMs). Our work provides a novel perspective by examining in-context learning via the lens of surface repetitions. We quantitatively investigate the role of surface features in text generation, and empirically establish the existence of \emph{token co-occurrence reinforcement}, a principle that strengthens the relationship between two tokens based on their contextual co-occurrences. By investigating the dual impacts of these features, our research illuminates the internal workings of in-context learning and expounds on the reasons for its failures. This paper provides an essential contribution to the understanding of in-context learning and its potential limitations, providing a fresh perspective on this exciting capability.
&lt;/p&gt;</description></item><item><title>ToRA&#26159;&#19968;&#31181;&#38598;&#25104;&#24037;&#20855;&#30340;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#25512;&#29702;&#20195;&#29702;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#35328;&#30340;&#20998;&#26512;&#33021;&#21147;&#21644;&#24037;&#20855;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25968;&#23398;&#25512;&#29702;&#30340;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;13%-19%&#30340;&#24179;&#22343;&#32477;&#23545;&#25913;&#36827;&#29575;&#65292;&#24182;&#22312;&#31454;&#36187;&#32423;&#25968;&#25454;&#38598;MATH&#19978;&#36798;&#21040;&#20102;44.6%&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.17452</link><description>&lt;p&gt;
ToRA&#65306;&#19968;&#31181;&#38598;&#25104;&#24037;&#20855;&#30340;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#25512;&#29702;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving. (arXiv:2309.17452v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17452
&lt;/p&gt;
&lt;p&gt;
ToRA&#26159;&#19968;&#31181;&#38598;&#25104;&#24037;&#20855;&#30340;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#25512;&#29702;&#20195;&#29702;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#35328;&#30340;&#20998;&#26512;&#33021;&#21147;&#21644;&#24037;&#20855;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25968;&#23398;&#25512;&#29702;&#30340;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;13%-19%&#30340;&#24179;&#22343;&#32477;&#23545;&#25913;&#36827;&#29575;&#65292;&#24182;&#22312;&#31454;&#36187;&#32423;&#25968;&#25454;&#38598;MATH&#19978;&#36798;&#21040;&#20102;44.6%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#22797;&#26434;&#30340;&#25968;&#23398;&#38382;&#39064;&#19978;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#38598;&#25104;&#24037;&#20855;&#30340;&#25512;&#29702;&#20195;&#29702;ToRA&#65292;&#23427;&#36890;&#36807;&#26080;&#32541;&#22320;&#23558;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#19982;&#22806;&#37096;&#24037;&#20855;&#65288;&#20363;&#22914;&#35745;&#31639;&#24211;&#21644;&#31526;&#21495;&#27714;&#35299;&#22120;&#65289;&#30340;&#21033;&#29992;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#23558;&#35821;&#35328;&#30340;&#20998;&#26512;&#33021;&#21147;&#19982;&#24037;&#20855;&#30340;&#35745;&#31639;&#25928;&#29575;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#12290;&#20026;&#20102;&#35757;&#32451;ToRA&#65292;&#25105;&#20204;&#31934;&#36873;&#20102;&#25968;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#20114;&#21160;&#24037;&#20855;&#20351;&#29992;&#36712;&#36857;&#65292;&#24212;&#29992;&#27169;&#20223;&#23398;&#20064;&#20110;&#27880;&#37322;&#65292;&#24182;&#25552;&#20986;&#36755;&#20986;&#31354;&#38388;&#25972;&#24418;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#27169;&#22411;&#30340;&#25512;&#29702;&#34892;&#20026;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ToRA&#27169;&#22411;&#22312;10&#20010;&#28085;&#30422;&#21508;&#31181;&#35268;&#27169;&#30340;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;&#65292;&#24179;&#22343;&#32477;&#23545;&#25913;&#36827;&#29575;&#36798;&#21040;13%&#33267;19%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;ToRA-7B &#22312;&#31454;&#36187;&#32423;&#25968;&#25454;&#38598;MATH&#19978;&#36798;&#21040;&#20102;44.6%&#65292;&#36229;&#36234;&#20102;&#26368;&#20339;&#24320;&#28304;&#27169;&#22411;WizardMath&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics. In this paper, we propose ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools (e.g., computation libraries and symbolic solvers), thereby amalgamating the analytical prowess of language and the computational efficiency of tools. To train ToRA, we curate interactive tool-use trajectories on mathematical datasets, apply imitation learning on the annotations, and propose output space shaping to further refine models' reasoning behavior. As a result, ToRA models significantly outperform open-source models on 10 mathematical reasoning datasets across all scales with 13%-19% absolute improvements on average. Notably, ToRA-7B reaches 44.6% on the competition-level dataset MATH, surpassing the best open-source model WizardMath
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22810;&#35282;&#24230;&#33258;&#19968;&#33268;&#24615;&#65288;MPSC&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#37319;&#26679;&#22810;&#20010;&#36755;&#20986;&#24182;&#26500;&#24314;&#19968;&#20010;&#22810;&#37096;&#20998;&#22270;&#65292;&#21033;&#29992;&#20132;&#21449;&#19968;&#33268;&#24615;&#21644;&#20869;&#19968;&#33268;&#24615;&#20449;&#24687;&#26469;&#36873;&#25321;&#26368;&#20248;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2309.17272</link><description>&lt;p&gt;
&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#30721;&#20013;&#30340;&#33021;&#21147;&#36890;&#36807;&#22810;&#35282;&#24230;&#33258;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency. (arXiv:2309.17272v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22810;&#35282;&#24230;&#33258;&#19968;&#33268;&#24615;&#65288;MPSC&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#37319;&#26679;&#22810;&#20010;&#36755;&#20986;&#24182;&#26500;&#24314;&#19968;&#20010;&#22810;&#37096;&#20998;&#22270;&#65292;&#21033;&#29992;&#20132;&#21449;&#19968;&#33268;&#24615;&#21644;&#20869;&#19968;&#33268;&#24615;&#20449;&#24687;&#26469;&#36873;&#25321;&#26368;&#20248;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#65292;&#22914;&#20195;&#30721;&#29983;&#25104;&#20013;&#65292;LLMs&#20173;&#28982;&#38590;&#20197;&#22312;&#19968;&#27425;&#23581;&#35797;&#20013;&#29983;&#25104;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#32858;&#21512;&#22810;&#20010;&#36755;&#20986;&#65292;&#21033;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26469;&#25506;&#32034;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#27809;&#26377;&#20840;&#38754;&#22320;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#25429;&#25417;&#36825;&#31181;&#19968;&#33268;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#35282;&#24230;&#33258;&#19968;&#33268;&#24615;&#65288;MPSC&#65289;&#26694;&#26550;&#30340;&#26032;&#30340;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;LLM&#65292;&#23427;&#23558;&#26469;&#33258;&#22810;&#20010;&#35282;&#24230;&#30340;&#36755;&#20986;&#20043;&#38388;&#30340;&#20132;&#21449;&#19968;&#33268;&#24615;&#21644;&#21333;&#20010;&#35282;&#24230;&#20869;&#30340;&#20869;&#19968;&#33268;&#24615;&#32467;&#21512;&#36215;&#26469;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35201;&#27714;LLMs&#23545;&#32473;&#23450;&#26597;&#35810;&#20174;&#21508;&#20010;&#35282;&#24230;&#37319;&#26679;&#22810;&#20010;&#22810;&#26679;&#21270;&#30340;&#36755;&#20986;&#65292;&#24182;&#22522;&#20110;&#23427;&#20204;&#26500;&#24314;&#19968;&#20010;&#22810;&#37096;&#20998;&#22270;&#12290;&#36890;&#36807;&#20004;&#20010;&#39044;&#23450;&#20041;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#65292;&#25105;&#20204;&#23558;&#20132;&#21449;&#19968;&#33268;&#24615;&#21644;&#20869;&#19968;&#33268;&#24615;&#20449;&#24687;&#23884;&#20837;&#21040;&#22270;&#20013;&#12290;&#26368;&#20339;&#36873;&#25321;&#26159;&#26681;&#25454;&#36825;&#20123;&#19968;&#33268;&#24615;&#24230;&#37327;&#26469;&#36873;&#25321;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited remarkable ability in textual generation. However, in complex reasoning tasks such as code generation, generating the correct answer in a single attempt remains a formidable challenge for LLMs. Previous research has explored solutions by aggregating multiple outputs, leveraging the consistency among them. However, none of them have comprehensively captured this consistency from different perspectives. In this paper, we propose the Multi-Perspective Self-Consistency (MPSC) framework, a novel decoding strategy for LLM that incorporates both inter-consistency across outputs from multiple perspectives and intra-consistency within a single perspective. Specifically, we ask LLMs to sample multiple diverse outputs from various perspectives for a given query and then construct a multipartite graph based on them. With two predefined measures of consistency, we embed both inter- and intra-consistency information into the graph. The optimal choice is th
&lt;/p&gt;</description></item><item><title>EchoPrompt&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20419;&#20351;&#27169;&#22411;&#37325;&#26032;&#34920;&#36848;&#26597;&#35810;&#26469;&#25552;&#20379;&#25913;&#36827;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;EchoPrompt&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.10687</link><description>&lt;p&gt;
EchoPrompt&#65306;&#25351;&#23548;&#27169;&#22411;&#37325;&#26032;&#34920;&#36848;&#26597;&#35810;&#20197;&#25913;&#21892;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
EchoPrompt: Instructing the Model to Rephrase Queries for Improved In-context Learning. (arXiv:2309.10687v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10687
&lt;/p&gt;
&lt;p&gt;
EchoPrompt&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20419;&#20351;&#27169;&#22411;&#37325;&#26032;&#34920;&#36848;&#26597;&#35810;&#26469;&#25552;&#20379;&#25913;&#36827;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;EchoPrompt&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31215;&#26497;&#37319;&#29992;&#25512;&#26029;&#26102;&#25552;&#31034;&#25216;&#26415;&#65292;&#22914;&#38646;-shot&#21644;&#23569;-shot&#25552;&#31034;&#25216;&#26415;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;EchoPrompt&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25552;&#31034;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#20043;&#21069;&#37325;&#26032;&#34920;&#36848;&#26597;&#35810;&#12290;EchoPrompt&#36866;&#29992;&#20110;&#26631;&#20934;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#38646;-shot&#21644;&#23569;-shot&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EchoPrompt&#22312;&#36825;&#22235;&#20010;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#26063;&#32676;&#30340;&#25152;&#26377;&#35774;&#32622;&#20013;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#36825;&#20123;&#25913;&#36827;&#35266;&#23519;&#21040;&#20102;&#21508;&#31181;&#25968;&#20540;&#25512;&#29702;&#65288;&#20363;&#22914;&#65292;GSM8K&#65292;SVAMP&#65289;&#12289;&#38405;&#35835;&#29702;&#35299;&#65288;&#20363;&#22914;DROP&#65289;&#21644;&#36923;&#36753;&#25512;&#29702;&#65288;&#20363;&#22914;Coin Flipping&#65289;&#20219;&#21153;&#20013;&#12290;&#24179;&#22343;&#32780;&#35328;&#65292;EchoPrompt&#25552;&#39640;&#20102;&#25968;&#20540;&#20219;&#21153;&#20013;code-davinci-002&#30340;&#38646;-shot-CoT&#24615;&#33021;5%&#65292;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;13%&#12290;&#25105;&#20204;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#30740;&#31350;&#20102;&#24433;&#21709;EchoPrompt&#26377;&#25928;&#24615;&#30340;&#22240;&#32032;&#65292;&#20854;&#20013;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Language models are achieving impressive performance on various tasks by aggressively adopting inference-time prompting techniques, such as zero-shot and few-shot prompting. In this work, we introduce EchoPrompt, a simple yet effective approach that prompts the model to rephrase its queries before answering them. EchoPrompt is adapted for both zero-shot and few-shot in-context learning with standard and chain-of-thought prompting. Experimental results show that EchoPrompt yields substantial improvements across all these settings for four families of causal language models. These improvements are observed across various numerical reasoning (e.g. GSM8K, SVAMP), reading comprehension (e.g. DROP), and logical reasoning (e.g. Coin Flipping) tasks. On average, EchoPrompt improves the Zero-shot-CoT performance of code-davinci-002 by 5% in numerical tasks and 13% in reading comprehension tasks. We investigate the factors contributing to EchoPrompt's effectiveness through ablation studies, whic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PoSE&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#22266;&#23450;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#21644;&#25805;&#32437;&#20301;&#32622;&#32034;&#24341;&#26469;&#36866;&#24212;&#26497;&#38271;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22823;&#22823;&#20943;&#23567;&#20102;&#20869;&#23384;&#21644;&#26102;&#38388;&#24320;&#38144;&#65292;&#23545;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#65292;&#25104;&#21151;&#23558;LLaMA&#27169;&#22411;&#25193;&#23637;&#21040;&#20102;128k&#20010;&#26631;&#35760;&#12290;</title><link>http://arxiv.org/abs/2309.10400</link><description>&lt;p&gt;
PoSE: &#36890;&#36807;&#20301;&#32622;&#36339;&#36291;&#24335;&#35757;&#32451;&#25552;&#39640;LLMs&#23545;&#20110;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#26377;&#25928;&#25299;&#23637;
&lt;/p&gt;
&lt;p&gt;
PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training. (arXiv:2309.10400v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PoSE&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#22266;&#23450;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#21644;&#25805;&#32437;&#20301;&#32622;&#32034;&#24341;&#26469;&#36866;&#24212;&#26497;&#38271;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22823;&#22823;&#20943;&#23567;&#20102;&#20869;&#23384;&#21644;&#26102;&#38388;&#24320;&#38144;&#65292;&#23545;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#65292;&#25104;&#21151;&#23558;LLaMA&#27169;&#22411;&#25193;&#23637;&#21040;&#20102;128k&#20010;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Positional Skip-wise (PoSE)&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36866;&#24212;&#20110;&#26497;&#38271;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;PoSE&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#22266;&#23450;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#21644;&#25805;&#32437;&#20301;&#32622;&#32034;&#24341;&#26469;&#27169;&#25311;&#38271;&#36755;&#20837;&#65292;&#23558;&#35757;&#32451;&#38271;&#24230;&#19982;&#30446;&#26631;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#20998;&#31163;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#38271;&#36755;&#20837;&#24207;&#21015;&#20013;&#36873;&#25321;&#33509;&#24178;&#30701;&#22359;&#65292;&#24182;&#24341;&#20837;&#19981;&#21516;&#30340;&#36339;&#36291;&#20559;&#32622;&#39033;&#26469;&#20462;&#25913;&#27599;&#20010;&#22359;&#30340;&#20301;&#32622;&#32034;&#24341;&#12290;&#36825;&#20123;&#36339;&#36291;&#20559;&#32622;&#39033;&#20197;&#21450;&#27599;&#20010;&#22359;&#30340;&#38271;&#24230;&#22312;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#20013;&#37117;&#20250;&#21464;&#21270;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#30446;&#26631;&#19978;&#19979;&#25991;&#31383;&#21475;&#20013;&#30340;&#25152;&#26377;&#20301;&#32622;&#65292;&#32780;&#26080;&#38656;&#23545;&#23436;&#25972;&#38271;&#24230;&#30340;&#36755;&#20837;&#36827;&#34892;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#23545;&#23436;&#25972;&#38271;&#24230;&#36827;&#34892;&#24494;&#35843;&#30456;&#27604;&#65292;PoSE&#22823;&#22823;&#20943;&#23567;&#20102;&#20869;&#23384;&#21644;&#26102;&#38388;&#24320;&#38144;&#65292;&#23545;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#12290;&#21033;&#29992;&#36825;&#19968;&#20248;&#21183;&#65292;&#25105;&#20204;&#25104;&#21151;&#23558;LLaMA&#27169;&#22411;&#25193;&#23637;&#21040;&#20102;128k&#20010;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#23454;&#65292;PoSE&#19982;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Positional Skip-wisE (PoSE) training for efficient adaptation of large language models~(LLMs) to extremely long context windows. PoSE decouples train length from target context window size by simulating long inputs using a fixed context window with manipulated position indices during training. Concretely, we select several short chunks from a long input sequence, and introduce distinct skipping bias terms to modify the position indices of each chunk. These bias terms, along with the length of each chunk, are altered for each training example, allowing the model to adapt to all positions within the target context window without training on full length inputs. Experiments show that, compared with fine-tuning on the full length, PoSE greatly reduces memory and time overhead with minimal impact on performance. Leveraging this advantage, we have successfully extended the LLaMA model to 128k tokens. Furthermore, we empirically confirm that PoSE is compatible with 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#25506;&#32034;&#30340;LLM&#20195;&#29702;&#65288;LASER&#65289;&#29992;&#20110;Web&#23548;&#33322;&#20219;&#21153;&#12290;&#35813;&#20195;&#29702;&#20197;&#28789;&#27963;&#30340;&#26041;&#24335;&#36716;&#25442;&#29366;&#24577;&#65292;&#36890;&#36807;&#25191;&#34892;&#21160;&#20316;&#23436;&#25104;&#20219;&#21153;&#65292;&#33021;&#22815;&#36731;&#26494;&#20174;&#38169;&#35823;&#20013;&#24674;&#22797;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.08172</link><description>&lt;p&gt;
LASER&#65306;&#20855;&#26377;&#29366;&#24577;&#31354;&#38388;&#25506;&#32034;&#30340;LLM&#20195;&#29702;&#29992;&#20110;Web&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
LASER: LLM Agent with State-Space Exploration for Web Navigation. (arXiv:2309.08172v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#25506;&#32034;&#30340;LLM&#20195;&#29702;&#65288;LASER&#65289;&#29992;&#20110;Web&#23548;&#33322;&#20219;&#21153;&#12290;&#35813;&#20195;&#29702;&#20197;&#28789;&#27963;&#30340;&#26041;&#24335;&#36716;&#25442;&#29366;&#24577;&#65292;&#36890;&#36807;&#25191;&#34892;&#21160;&#20316;&#23436;&#25104;&#20219;&#21153;&#65292;&#33021;&#22815;&#36731;&#26494;&#20174;&#38169;&#35823;&#20013;&#24674;&#22797;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35832;&#22914;Web&#23548;&#33322;&#20043;&#31867;&#30340;&#20132;&#20114;&#24335;&#20915;&#31574;&#20219;&#21153;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#24615;&#33021;&#65292;&#20294;&#20808;&#21069;&#30340;&#26041;&#27861;&#38544;&#21547;&#22320;&#20551;&#35774;&#27169;&#22411;&#21482;&#33021;&#20197;&#27491;&#21521;&#26041;&#24335;&#25191;&#34892;&#65292;&#22312;&#20132;&#20114;&#29615;&#22659;&#20013;&#20165;&#25552;&#20379;&#27491;&#20363;&#36712;&#36857;&#20316;&#20026;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#25945;&#25480;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#25512;&#29702;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#65292;&#20363;&#22914;&#38169;&#35823;&#65292;&#20174;&#32780;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#20132;&#20114;&#20219;&#21153;&#24314;&#27169;&#20026;&#29366;&#24577;&#31354;&#38388;&#25506;&#32034;&#65292;&#20854;&#20013;LLM&#20195;&#29702;&#36890;&#36807;&#25191;&#34892;&#21160;&#20316;&#22312;&#39044;&#23450;&#20041;&#30340;&#19968;&#32452;&#29366;&#24577;&#20043;&#38388;&#36827;&#34892;&#36716;&#25442;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;&#36825;&#31181;&#24418;&#24335;&#21270;&#26041;&#27861;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#28789;&#27963;&#22320;&#36827;&#34892;&#22238;&#28335;&#65292;&#20174;&#32780;&#33021;&#22815;&#36731;&#26494;&#20174;&#38169;&#35823;&#20013;&#24674;&#22797;&#12290;&#25105;&#20204;&#22312;WebShop&#20219;&#21153;&#19978;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;LASER&#20195;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;LASER&#20195;&#29702;&#26126;&#26174;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been successfully adapted for interactive decision-making tasks like web navigation. While achieving decent performance, previous methods implicitly assume a forward-only execution mode for the model, where they only provide oracle trajectories as in-context examples to teach the model how to reason in the interactive environment. Consequently, the model could not handle more challenging scenarios not covered in the in-context examples, e.g., mistakes, leading to sub-optimal performance. To address this issue, we propose to model the interactive task as state space exploration, where the LLM agent transitions among a pre-defined set of states by performing actions to complete the task. This formulation enables flexible back-tracking, allowing the model to easily recover from errors. We evaluate our proposed LLM Agent with State-Space ExploRation (LASER) on the WebShop task. Experimental results show that our LASER agent significantly outperforms previo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24357;&#21512;&#24341;&#29992;&#21644;&#24341;&#25991;&#25991;&#26412;&#20043;&#38388;&#36317;&#31163;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24341;&#25991;&#25991;&#26412;&#36328;&#24230;(CTS)&#26367;&#20195;&#25688;&#35201;&#20316;&#20026;&#36755;&#20837;&#65292;&#20174;&#32780;&#20351;&#24471;&#24341;&#25991;&#29983;&#25104;&#26356;&#21152;&#20934;&#30830;&#21644;&#30456;&#20851;&#12290;&#36890;&#36807;&#33258;&#21160;&#26631;&#27880;&#21644;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;CTS&#26631;&#27880;&#65292;&#25552;&#39640;&#24341;&#25991;&#25991;&#26412;&#29983;&#25104;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.06365</link><description>&lt;p&gt;
&#24341;&#25991;&#25991;&#26412;&#36328;&#24230;&#29992;&#20110;&#24341;&#25991;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Cited Text Spans for Citation Text Generation. (arXiv:2309.06365v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24357;&#21512;&#24341;&#29992;&#21644;&#24341;&#25991;&#25991;&#26412;&#20043;&#38388;&#36317;&#31163;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24341;&#25991;&#25991;&#26412;&#36328;&#24230;(CTS)&#26367;&#20195;&#25688;&#35201;&#20316;&#20026;&#36755;&#20837;&#65292;&#20174;&#32780;&#20351;&#24471;&#24341;&#25991;&#29983;&#25104;&#26356;&#21152;&#20934;&#30830;&#21644;&#30456;&#20851;&#12290;&#36890;&#36807;&#33258;&#21160;&#26631;&#27880;&#21644;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;CTS&#26631;&#27880;&#65292;&#25552;&#39640;&#24341;&#25991;&#25991;&#26412;&#29983;&#25104;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36991;&#20813;&#38750;&#20107;&#23454;&#24615;&#24187;&#35273;&#65292;&#33258;&#21160;&#30456;&#20851;&#24037;&#20316;&#29983;&#25104;&#24517;&#39035;&#23558;&#20854;&#36755;&#20986;&#19982;&#24341;&#25991;&#20013;&#30340;&#20869;&#23481;&#30456;&#20851;&#32852;&#65292;&#20294;&#30001;&#20110;&#31185;&#23398;&#25991;&#26723;&#30340;&#38271;&#24230;&#65292;&#29616;&#26377;&#30340;&#27010;&#25324;&#24615;&#26041;&#27861;&#21482;&#26377;&#22312;&#24341;&#25991;&#25688;&#35201;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#12290;&#25105;&#20204;&#35777;&#26126;&#25688;&#35201;&#24182;&#19981;&#24635;&#26159;&#24341;&#25991;&#29983;&#25104;&#30340;&#26368;&#20339;&#36755;&#20837;&#65292;&#20197;&#21450;&#20197;&#36825;&#31181;&#26041;&#24335;&#35757;&#32451;&#30340;&#27169;&#22411;&#20250;&#20986;&#29616;&#24187;&#35273;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24341;&#25991;&#25991;&#26412;&#36328;&#24230;(CTS)&#20316;&#20026;&#25688;&#35201;&#30340;&#26367;&#20195;&#26465;&#20214;&#12290;&#30001;&#20110;&#25163;&#21160;CTS&#27880;&#37322;&#38750;&#24120;&#32791;&#26102;&#19988;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#65292;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#22522;&#20110;ROUGE&#30340;&#33258;&#21160;&#26631;&#27880;&#20505;&#36873;CTS&#21477;&#23376;&#65292;&#24182;&#21462;&#24471;&#36275;&#22815;&#24378;&#30340;&#24615;&#33021;&#20197;&#26367;&#20195;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;CTS&#26816;&#32034;&#26041;&#27861;&#65292;&#20351;&#24471;&#20197;&#24341;&#25991;&#20840;&#25991;&#20026;&#22522;&#30784;&#29983;&#25104;&#24341;&#25991;&#25991;&#26412;&#21464;&#24471;&#26377;&#21069;&#26223;&#21644;&#23454;&#38469;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic related work generation must ground their outputs to the content of the cited papers to avoid non-factual hallucinations, but due to the length of scientific documents, existing abstractive approaches have conditioned only on the cited paper \textit{abstracts}. We demonstrate that the abstract is not always the most appropriate input for citation generation and that models trained in this way learn to hallucinate. We propose to condition instead on the \textit{cited text span} (CTS) as an alternative to the abstract. Because manual CTS annotation is extremely time- and labor-intensive, we experiment with automatic, ROUGE-based labeling of candidate CTS sentences, achieving sufficiently strong performance to substitute for expensive human annotations, and we propose a human-in-the-loop, keyword-based CTS retrieval approach that makes generating citation texts grounded in the full text of cited papers both promising and practical.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SpikeBERT&#30340;SNN&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;Spikformer&#26550;&#26500;&#21644;&#20351;&#29992;&#20004;&#38454;&#27573;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#20854;&#20182;SNN&#27169;&#22411;&#65292;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#29978;&#33267;&#36798;&#21040;&#20102;&#19982;BERT&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.15122</link><description>&lt;p&gt;
SpikeBERT&#65306;&#19968;&#31181;&#37319;&#29992;&#20004;&#38454;&#27573;BERT&#30693;&#35782;&#33976;&#39311;&#35757;&#32451;&#30340;&#35821;&#35328;Spikformer
&lt;/p&gt;
&lt;p&gt;
SpikeBERT: A Language Spikformer Trained with Two-Stage Knowledge Distillation from BERT. (arXiv:2308.15122v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15122
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SpikeBERT&#30340;SNN&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;Spikformer&#26550;&#26500;&#21644;&#20351;&#29992;&#20004;&#38454;&#27573;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#20854;&#20182;SNN&#27169;&#22411;&#65292;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#29978;&#33267;&#36798;&#21040;&#20102;&#19982;BERT&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#22312;&#20197;&#26356;&#33410;&#33021;&#30340;&#26041;&#24335;&#23454;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#35821;&#35328;&#20219;&#21153;&#30340;SNN&#32593;&#32476;&#26550;&#26500;&#36807;&#20110;&#31616;&#21333;&#65292;&#28145;&#24230;&#26550;&#26500;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#65292;&#19982;BERT&#31561;&#20027;&#27969;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#30456;&#27604;&#65292;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#33033;&#20914;Transformer&#65288;&#21363;Spikformer&#65289;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#35821;&#35328;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#26469;&#35757;&#32451;&#23427;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#36890;&#36807;&#20174;BERT&#21644;&#22823;&#37327;&#26410;&#26631;&#35760;&#25991;&#26412;&#20013;&#33976;&#39311;&#30693;&#35782;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#20877;&#27425;&#20174;&#22312;&#30456;&#21516;&#35757;&#32451;&#31034;&#20363;&#19978;&#23545;BERT&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#23454;&#20363;&#30693;&#35782;&#33976;&#39311;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;SpikeBERT&#65292;&#22312;&#23454;&#29616;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;SNN&#65292;&#24182;&#19988;&#29978;&#33267;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#19982;BERT&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) offer a promising avenue to implement deep neural networks in a more energy-efficient way. However, the network architectures of existing SNNs for language tasks are too simplistic, and deep architectures have not been fully explored, resulting in a significant performance gap compared to mainstream transformer-based networks such as BERT. To this end, we improve a recently-proposed spiking transformer (i.e., Spikformer) to make it possible to process language tasks and propose a two-stage knowledge distillation method for training it, which combines pre-training by distilling knowledge from BERT with a large collection of unlabelled texts and fine-tuning with task-specific instances via knowledge distillation again from the BERT fine-tuned on the same training examples. Through extensive experimentation, we show that the models trained with our method, named SpikeBERT, outperform state-of-the-art SNNs and even achieve comparable results to BERTs on text 
&lt;/p&gt;</description></item><item><title>chatGPT ADA&#26159;&#19968;&#31181;&#33021;&#22815;&#33258;&#20027;&#24320;&#21457;&#20020;&#24202;&#30740;&#31350;&#25152;&#38656;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#23558;&#39640;&#32423;&#20998;&#26512;&#24037;&#20855;&#27665;&#20027;&#21270;&#65292;&#20351;&#38750;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#20020;&#24202;&#21307;&#29983;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2308.14120</link><description>&lt;p&gt;
&#25480;&#26435;&#20020;&#24202;&#21307;&#29983;&#24182;&#27665;&#20027;&#21270;&#25968;&#25454;&#31185;&#23398;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#20020;&#24202;&#30740;&#31350;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290; (arXiv:2308.14120v2 [cs.LG] &#26356;&#26032;&#29256;)
&lt;/p&gt;
&lt;p&gt;
Empowering Clinicians and Democratizing Data Science: Large Language Models Automate Machine Learning for Clinical Studies. (arXiv:2308.14120v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14120
&lt;/p&gt;
&lt;p&gt;
chatGPT ADA&#26159;&#19968;&#31181;&#33021;&#22815;&#33258;&#20027;&#24320;&#21457;&#20020;&#24202;&#30740;&#31350;&#25152;&#38656;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#23558;&#39640;&#32423;&#20998;&#26512;&#24037;&#20855;&#27665;&#20027;&#21270;&#65292;&#20351;&#38750;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#20020;&#24202;&#21307;&#29983;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24320;&#21457;&#32773;&#65288;&#22914;&#25968;&#25454;&#31185;&#23398;&#23478;&#65289;&#21644;&#20174;&#19994;&#32773;&#65288;&#22914;&#20020;&#24202;&#21307;&#29983;&#65289;&#20043;&#38388;&#23384;&#22312;&#30693;&#35782;&#24046;&#36317;&#65292;&#38459;&#30861;&#20102;ML&#22312;&#20020;&#24202;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#20805;&#20998;&#21033;&#29992;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;chatGPT Advanced Data Analysis&#65288;ADA&#65289;&#65292;&#21363;GPT-4&#30340;&#25193;&#23637;&#65292;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#24182;&#39640;&#25928;&#25191;&#34892;ML&#20998;&#26512;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21521;chatGPT ADA&#25552;&#20379;&#20102;&#21508;&#31181;&#21307;&#23398;&#19987;&#19994;&#30340;&#22823;&#22411;&#35797;&#39564;&#30340;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#21644;&#30740;&#31350;&#35814;&#32454;&#20449;&#24687;&#65292;&#27809;&#26377;&#32473;&#20986;&#20855;&#20307;&#25351;&#23548;&#12290;ChatGPT ADA&#22522;&#20110;&#21407;&#22987;&#30740;&#31350;&#30340;&#35757;&#32451;&#25968;&#25454;&#33258;&#20027;&#24320;&#21457;&#20102;&#26368;&#20808;&#36827;&#30340;ML&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20020;&#24202;&#32467;&#26524;&#65292;&#22914;&#30284;&#30151;&#21457;&#23637;&#12289;&#30284;&#30151;&#36827;&#23637;&#12289;&#30142;&#30149;&#24182;&#21457;&#30151;&#25110;&#33268;&#30149;&#22522;&#22240;&#24207;&#21015;&#31561;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#20123;ML&#27169;&#22411;&#19982;&#20854;&#24050;&#21457;&#34920;&#30340;&#23545;&#24212;&#29289;&#30456;&#21305;&#37197;&#29978;&#33267;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;chatGPT ADA&#20026;&#27665;&#20027;&#21270;&#21307;&#23398;&#20013;&#30340;ML&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#65292;&#20351;&#38750;ML&#19987;&#23478;&#33021;&#22815;&#33719;&#24471;&#20808;&#36827;&#30340;&#20998;&#26512;&#24037;&#20855;&#24182;&#25512;&#21160;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
A knowledge gap persists between Machine Learning (ML) developers (e.g., data scientists) and practitioners (e.g., clinicians), hampering the full utilization of ML for clinical data analysis. We investigated the potential of the chatGPT Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this gap and perform ML analyses efficiently. Real-world clinical datasets and study details from large trials across various medical specialties were presented to chatGPT ADA without specific guidance. ChatGPT ADA autonomously developed state-of-the-art ML models based on the original study's training data to predict clinical outcomes such as cancer development, cancer progression, disease complications, or biomarkers such as pathogenic gene sequences. Strikingly, these ML models matched or outperformed their published counterparts. We conclude that chatGPT ADA offers a promising avenue to democratize ML in medicine, making advanced analytics accessible to non-ML experts and promoting broa
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20351;&#29992;&#21069;&#32512;&#35821;&#35328;&#27169;&#22411;&#65288;PrefixLM&#65289;&#27604;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#65288;CausalLM&#65289;&#25928;&#26524;&#26356;&#22909;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#65292;&#34429;&#28982;&#20004;&#31181;&#35821;&#35328;&#27169;&#22411;&#37117;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#21040;&#31283;&#23450;&#28857;&#65292;&#20294;&#21069;&#32512;&#35821;&#35328;&#27169;&#22411;&#25910;&#25947;&#21040;&#32447;&#24615;&#22238;&#24402;&#30340;&#26368;&#20248;&#35299;&#65292;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#25910;&#25947;&#21160;&#24577;&#36981;&#24490;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#19981;&#20445;&#35777;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.06912</link><description>&lt;p&gt;
CausalLM&#19981;&#36866;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CausalLM is not optimal for in-context learning. (arXiv:2308.06912v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06912
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20351;&#29992;&#21069;&#32512;&#35821;&#35328;&#27169;&#22411;&#65288;PrefixLM&#65289;&#27604;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#65288;CausalLM&#65289;&#25928;&#26524;&#26356;&#22909;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#65292;&#34429;&#28982;&#20004;&#31181;&#35821;&#35328;&#27169;&#22411;&#37117;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#21040;&#31283;&#23450;&#28857;&#65292;&#20294;&#21069;&#32512;&#35821;&#35328;&#27169;&#22411;&#25910;&#25947;&#21040;&#32447;&#24615;&#22238;&#24402;&#30340;&#26368;&#20248;&#35299;&#65292;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#25910;&#25947;&#21160;&#24577;&#36981;&#24490;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#19981;&#20445;&#35777;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;&#21069;&#32512;&#35821;&#35328;&#27169;&#22411;&#65288;PrefixLM&#65289;&#34920;&#29616;&#26356;&#22909;&#65292;&#20854;&#20801;&#35768;&#19978;&#19979;&#25991;&#26679;&#26412;&#30456;&#20114;&#20851;&#27880;&#65307;&#30456;&#27604;&#20043;&#19979;&#65292;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#65288;CausalLM&#65289;&#20351;&#29992;&#33258;&#22238;&#24402;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#31105;&#27490;&#19978;&#19979;&#25991;&#26679;&#26412;&#20851;&#27880;&#26410;&#26469;&#30340;&#26679;&#26412;&#12290;&#34429;&#28982;&#36825;&#20010;&#32467;&#26524;&#26159;&#30452;&#35266;&#30340;&#65292;&#20294;&#20174;&#29702;&#35770;&#35282;&#24230;&#24182;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#37319;&#29992;&#29702;&#35770;&#26041;&#27861;&#65292;&#20998;&#26512;&#20102;&#22312;&#29305;&#23450;&#21442;&#25968;&#26500;&#24314;&#19979;&#65292;&#21069;&#32512;&#35821;&#35328;&#27169;&#22411;&#21644;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#65292;&#20004;&#31181;&#35821;&#35328;&#27169;&#22411;&#37117;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#21040;&#31283;&#23450;&#28857;&#65292;&#20294;&#21069;&#32512;&#35821;&#35328;&#27169;&#22411;&#25910;&#25947;&#21040;&#32447;&#24615;&#22238;&#24402;&#30340;&#26368;&#20248;&#35299;&#65292;&#32780;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#25910;&#25947;&#21160;&#24577;&#36981;&#24490;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#21363;&#20351;&#26679;&#26412;&#25968;&#37327;&#36235;&#20110;&#26080;&#31351;&#65292;&#20063;&#19981;&#33021;&#20445;&#35777;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#30340;&#32463;&#39564;&#23454;&#39564;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent empirical evidence indicates that transformer based in-context learning performs better when using a prefix language model (prefixLM), in which in-context samples can all attend to each other, compared to causal language models (causalLM), which use auto-regressive attention that prohibits in-context samples to attend to future samples. While this result is intuitive, it is not understood from a theoretical perspective. In this paper we take a theoretical approach and analyze the convergence behavior of prefixLM and causalLM under a certain parameter construction. Our analysis shows that both LM types converge to their stationary points at a linear rate, but that while prefixLM converges to the optimal solution of linear regression, causalLM convergence dynamics follows that of an online gradient descent algorithm, which is not guaranteed to be optimal even as the number of samples grows infinitely. We supplement our theoretical claims with empirical experiments over synthetic a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#22312;ChatGPT&#20013;&#36816;&#29992;&#21512;&#36866;&#30340;&#25552;&#31034;&#23558;&#32763;&#35793;&#30446;&#30340;&#21644;&#30446;&#26631;&#21463;&#20247;&#34701;&#20837;&#36827;&#21435;&#23545;&#32763;&#35793;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#28789;&#27963;&#30340;&#32763;&#35793;&#32467;&#26524;&#65292;&#30456;&#27604;&#20256;&#32479;&#26426;&#22120;&#32763;&#35793;&#26356;&#20855;&#23450;&#21046;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01391</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#20248;&#21270;&#26426;&#22120;&#32763;&#35793;&#65306;ChatGPT&#21487;&#23450;&#21046;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimizing Machine Translation through Prompt Engineering: An Investigation into ChatGPT's Customizability. (arXiv:2308.01391v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#22312;ChatGPT&#20013;&#36816;&#29992;&#21512;&#36866;&#30340;&#25552;&#31034;&#23558;&#32763;&#35793;&#30446;&#30340;&#21644;&#30446;&#26631;&#21463;&#20247;&#34701;&#20837;&#36827;&#21435;&#23545;&#32763;&#35793;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#28789;&#27963;&#30340;&#32763;&#35793;&#32467;&#26524;&#65292;&#30456;&#27604;&#20256;&#32479;&#26426;&#22120;&#32763;&#35793;&#26356;&#20855;&#23450;&#21046;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#23558;&#32763;&#35793;&#30446;&#30340;&#21644;&#30446;&#26631;&#21463;&#20247;&#34701;&#20837;&#21040;ChatGPT&#25552;&#31034;&#20013;&#23545;&#32763;&#35793;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#20511;&#37492;&#20102;&#20043;&#21069;&#30340;&#32763;&#35793;&#30740;&#31350;&#12289;&#34892;&#19994;&#23454;&#36341;&#21644;ISO&#26631;&#20934;&#65292;&#24378;&#35843;&#20102;&#32763;&#35793;&#36807;&#31243;&#20013;&#39044;&#29983;&#20135;&#38454;&#27573;&#30340;&#37325;&#35201;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#36866;&#24403;&#30340;&#25552;&#31034;&#21487;&#20197;&#20135;&#29983;&#28789;&#27963;&#30340;&#32763;&#35793;&#65292;&#36825;&#26159;&#20256;&#32479;&#30340;&#26426;&#22120;&#32763;&#35793;&#25152;&#27809;&#26377;&#23454;&#29616;&#30340;&#12290;&#30740;&#31350;&#23457;&#26597;&#20102;&#22312;&#29983;&#25104;&#28385;&#36275;&#29305;&#23450;&#26465;&#20214;&#30340;&#32763;&#35793;&#26102;&#65292;&#25552;&#31034;&#23545;&#32763;&#35793;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#35780;&#20272;&#20174;&#23454;&#38469;&#32763;&#35793;&#24072;&#30340;&#35282;&#24230;&#36827;&#34892;&#65292;&#20027;&#35266;&#21644;&#23450;&#24615;&#30456;&#32467;&#21512;&#65292;&#36824;&#20351;&#29992;&#20102;OpenAI&#30340;&#35789;&#23884;&#20837;API&#36827;&#34892;&#20313;&#24358;&#30456;&#20284;&#24230;&#35745;&#31639;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#32763;&#35793;&#30446;&#30340;&#21644;&#30446;&#26631;&#21463;&#20247;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#30830;&#23454;&#21487;&#20197;&#20462;&#25913;&#29983;&#25104;&#30340;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the influence of integrating the purpose of the translation and the target audience into prompts on the quality of translations produced by ChatGPT. Drawing on previous translation studies, industry practices, and ISO standards, the research underscores the significance of the pre-production phase in the translation process. The study reveals that the inclusion of suitable prompts in large-scale language models like ChatGPT can yield flexible translations, a feat yet to be realized by conventional Machine Translation (MT). The research scrutinizes the changes in translation quality when prompts are used to generate translations that meet specific conditions. The evaluation is conducted from a practicing translator's viewpoint, both subjectively and qualitatively, supplemented by the use of OpenAI's word embedding API for cosine similarity calculations. The findings suggest that the integration of the purpose and target audience into prompts can indeed modify the gen
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;AutoML&#21644;LLMs&#20043;&#38388;&#30340;&#20849;&#29983;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#34701;&#21512;&#26377;&#26395;&#39072;&#35206;NLP&#21644;AutoML&#20004;&#20010;&#39046;&#22495;&#65292;&#21516;&#26102;&#20063;&#23384;&#22312;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2306.08107</link><description>&lt;p&gt;
&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;AutoML&#65306;&#24403;&#21069;&#25361;&#25112;&#65292;&#26410;&#26469;&#26426;&#36935;&#21644;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks. (arXiv:2306.08107v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08107
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;AutoML&#21644;LLMs&#20043;&#38388;&#30340;&#20849;&#29983;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#34701;&#21512;&#26377;&#26395;&#39072;&#35206;NLP&#21644;AutoML&#20004;&#20010;&#39046;&#22495;&#65292;&#21516;&#26102;&#20063;&#23384;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#29305;&#21035;&#26159;&#22312;NLP&#39046;&#22495;&#65292;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#36817;&#32463;&#21382;&#20102;&#19968;&#31995;&#21015;&#31361;&#30772;&#12290;&#25105;&#20204;&#35774;&#24819;&#65292;&#20004;&#20010;&#39046;&#22495;&#36890;&#36807;&#32039;&#23494;&#30340;&#34701;&#21512;&#21487;&#20197;&#24444;&#27492;&#25512;&#21160;&#26497;&#38480;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#19968;&#24895;&#26223;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;AutoML&#21644;LLMs&#20043;&#38388;&#30340;&#20849;&#29983;&#20851;&#31995;&#28508;&#21147;&#65292;&#30528;&#37325;&#25506;&#35752;&#20102;&#23427;&#20204;&#22914;&#20309;&#20114;&#30456;&#21463;&#30410;&#12290;&#25105;&#20204;&#29305;&#21035;&#30740;&#31350;&#20102;&#20174;&#19981;&#21516;&#35282;&#24230;&#22686;&#24378;LLMs&#30340;AutoML&#26041;&#27861;&#30340;&#26426;&#20250;&#20197;&#21450;&#21033;&#29992;AutoML&#36827;&#19968;&#27493;&#25913;&#36827;LLMs&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29616;&#26377;&#24037;&#20316;&#65292;&#24182;&#23545;&#20854;&#20013;&#30340;&#39118;&#38505;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#35780;&#20272;&#12290;&#25105;&#20204;&#22362;&#20449;&#65292;&#20004;&#20010;&#39046;&#22495;&#30340;&#34701;&#21512;&#26377;&#21487;&#33021;&#39072;&#35206;NLP&#21644;AutoML&#20004;&#20010;&#39046;&#22495;&#12290;&#36890;&#36807;&#24378;&#35843;&#21487;&#24819;&#35937;&#30340;&#21327;&#21516;&#20316;&#29992;&#21644;&#39118;&#38505;&#65292;&#25105;&#20204;&#26088;&#22312;&#20419;&#36827;&#22312;&#20132;&#21449;&#28857;&#30340;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fields of both Natural Language Processing (NLP) and Automated Machine Learning (AutoML) have achieved remarkable results over the past years. In NLP, especially Large Language Models (LLMs) have experienced a rapid series of breakthroughs very recently. We envision that the two fields can radically push the boundaries of each other through tight integration. To showcase this vision, we explore the potential of a symbiotic relationship between AutoML and LLMs, shedding light on how they can benefit each other. In particular, we investigate both the opportunities to enhance AutoML approaches with LLMs from different perspectives and the challenges of leveraging AutoML to further improve LLMs. To this end, we survey existing work, and we critically assess risks. We strongly believe that the integration of the two fields has the potential to disrupt both fields, NLP and AutoML. By highlighting conceivable synergies, but also risks, we aim to foster further exploration at the intersect
&lt;/p&gt;</description></item><item><title>STAR&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#25968;&#25454;&#23454;&#20363;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#20302;&#36164;&#28304;&#20449;&#24687;&#25277;&#21462;&#65292;&#20026;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#38656;&#35201;&#26368;&#23569;&#20154;&#24037;&#26631;&#27880;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.15090</link><description>&lt;p&gt;
STAR: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#32467;&#26500;&#21040;&#25991;&#26412;&#25968;&#25454;&#29983;&#25104;&#25913;&#36827;&#20302;&#36164;&#28304;&#20449;&#24687;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
STAR: Improving Low-Resource Information Extraction by Structure-to-Text Data Generation with Large Language Models. (arXiv:2305.15090v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15090
&lt;/p&gt;
&lt;p&gt;
STAR&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#25968;&#25454;&#23454;&#20363;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#20302;&#36164;&#28304;&#20449;&#24687;&#25277;&#21462;&#65292;&#20026;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#38656;&#35201;&#26368;&#23569;&#20154;&#24037;&#26631;&#27880;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#65292;&#22914;&#20107;&#20214;&#25277;&#21462;&#65292;&#38656;&#35201;&#23545;&#36755;&#20986;&#32467;&#26500;&#21644;&#23376;&#20219;&#21153;&#20381;&#36182;&#36827;&#34892;&#28145;&#20837;&#29702;&#35299;&#12290;&#20026;&#20102;&#33719;&#24471;&#21512;&#29702;&#30340;&#24615;&#33021;&#65292;&#23427;&#20204;&#20005;&#37325;&#20381;&#36182;&#20110;&#20197;&#65288;&#27573;&#33853;&#65292;&#30446;&#26631;&#32467;&#26500;&#65289;&#23545;&#30340;&#24418;&#24335;&#30340;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#20154;&#24037;&#27880;&#37322;&#33719;&#24471;&#36825;&#26679;&#30340;&#25968;&#25454;&#26159;&#26114;&#36149;&#30340;&#65292;&#22240;&#27492;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#65292;&#25105;&#20204;&#36843;&#20999;&#38656;&#35201;&#38656;&#35201;&#26368;&#23569;&#20154;&#24037;&#26631;&#27880;&#30340;&#20302;&#36164;&#28304;&#20449;&#24687;&#25277;&#21462;&#26041;&#27861;&#12290;&#20351;&#29992;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#23545;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#33021;&#26159;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#20294;&#29616;&#26377;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#35201;&#20040;&#20173;&#28982;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#35201;&#20040;&#30001;&#20110;&#24615;&#33021;&#24046;&#32780;&#26080;&#27861;&#24212;&#29992;&#20110;&#22797;&#26434;&#30340;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;STAR&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26681;&#25454;&#26377;&#38480;&#30340;&#31181;&#23376;&#31034;&#20363;&#21512;&#25104;&#25968;&#25454;&#23454;&#20363;&#65292;&#20174;&#32780;&#25552;&#39640;&#20302;&#36164;&#28304;&#20449;&#24687;&#25277;&#21462;&#24615;&#33021;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information extraction tasks such as event extraction require an in-depth understanding of the output structure and sub-task dependencies. They heavily rely on task-specific training data in the form of (passage, target structure) pairs to obtain reasonable performance. However, obtaining such data through human annotation is costly, leading to a pressing need for low-resource information extraction approaches that require minimal human labeling for real-world applications. Fine-tuning supervised models with synthesized training data would be a generalizable method, but the existing data generation methods either still rely on large-scale ground-truth data or cannot be applied to complicated IE tasks due to their poor performance. To address these challenges, we propose STAR, a data generation method that leverages Large Language Models (LLMs) to synthesize data instances given limited seed demonstrations, thereby boosting low-resource information extraction performance. Our approach i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; LogicLLM&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#30417;&#30563;&#21518;&#35757;&#32451;&#26469;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#22312;&#24120;&#35265;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13718</link><description>&lt;p&gt;
LogicLLM&#65306;&#25506;&#32034;&#33258;&#30417;&#30563;&#36923;&#36753;&#22686;&#24378;&#35757;&#32451;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large Language Models. (arXiv:2305.13718v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; LogicLLM&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#30417;&#30563;&#21518;&#35757;&#32451;&#26469;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#22312;&#24120;&#35265;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#30340;&#29616;&#26377;&#21162;&#21147;&#20027;&#35201;&#20381;&#36182;&#20110;&#26377;&#30417;&#30563;&#24494;&#35843;&#65292;&#36825;&#38459;&#30861;&#20102;&#23558;&#27169;&#22411;&#27867;&#21270;&#21040;&#26032;&#30340;&#39046;&#22495;&#21644;/&#25110;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#21457;&#23637;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#35777;&#26126;&#20102;&#23558;&#20016;&#23500;&#30340;&#30693;&#35782;&#21387;&#32553;&#20026;&#21333;&#20010;&#20195;&#29702;&#30340;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#22810;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;LLMs &#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#24182;&#27809;&#26377;&#34920;&#29616;&#20986;&#33021;&#21147;&#12290;LLMs &#22312;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#36828;&#36828;&#33853;&#21518;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#36890;&#36807;&#33258;&#30417;&#30563;&#21518;&#35757;&#32451;&#26469;&#25506;&#32034;&#34701;&#21512;&#36923;&#36753;&#30693;&#35782;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#28608;&#27963;&#23427;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;LogicLLM&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;MERIt &#30340;&#33258;&#22238;&#24402;&#30446;&#26631;&#21464;&#20307;&#65292;&#24182;&#23558;&#20854;&#19982;&#20004;&#20010;LLM&#31995;&#21015;FLAN-T5&#21644;LLaMA&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#21442;&#25968;&#22823;&#23567;&#33539;&#22260;&#20174;30&#20159;&#21040;130&#20159;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24120;&#29992;&#25512;&#29702;&#31574;&#30053;&#19978;&#19982;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#26041;&#27861;&#30456;&#24403;&#65292;&#24182;&#19988;&#36828;&#36828;&#36229;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing efforts to improve logical reasoning ability of language models have predominantly relied on supervised fine-tuning, hindering generalization to new domains and/or tasks. The development of Large Langauge Models (LLMs) has demonstrated the capacity of compressing abundant knowledge into a single proxy, enabling them to tackle multiple tasks effectively. Our preliminary experiments, nevertheless, show that LLMs do not show capability on logical reasoning. The performance of LLMs on logical reasoning benchmarks is far behind the existing state-of-the-art baselines. In this paper, we make the first attempt to investigate the feasibility of incorporating logical knowledge through self-supervised post-training, and activating it via in-context learning, which we termed as LogicLLM. Specifically, we devise an auto-regressive objective variant of MERIt and integrate it with two LLM series, i.e., FLAN-T5 and LLaMA, with parameter size ranging from 3 billion to 13 billion. The results 
&lt;/p&gt;</description></item><item><title>Chain-of-Knowledge&#36890;&#36807;&#25972;&#21512;&#22810;&#28304;&#21160;&#24577;&#30693;&#35782;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20934;&#30830;&#30340;&#22522;&#30784;&#20449;&#24687;&#65292;&#20943;&#23569;&#29983;&#25104;&#30340;&#24187;&#35273;&#65292;&#21487;&#20197;&#20135;&#29983;&#26356;&#21487;&#38752;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.13269</link><description>&lt;p&gt;
Chain-of-Knowledge:&#36890;&#36807;&#22810;&#28304;&#21160;&#24577;&#30693;&#35782;&#36866;&#24212;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20934;&#30830;&#30340;&#22522;&#30784;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources. (arXiv:2305.13269v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13269
&lt;/p&gt;
&lt;p&gt;
Chain-of-Knowledge&#36890;&#36807;&#25972;&#21512;&#22810;&#28304;&#21160;&#24577;&#30693;&#35782;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20934;&#30830;&#30340;&#22522;&#30784;&#20449;&#24687;&#65292;&#20943;&#23569;&#29983;&#25104;&#30340;&#24187;&#35273;&#65292;&#21487;&#20197;&#20135;&#29983;&#26356;&#21487;&#38752;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#38142;&#24335;&#30693;&#35782;&#65288;CoK&#65289;&#65292;&#36890;&#36807;&#21160;&#24577;&#22320;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#22522;&#30784;&#20449;&#24687;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#12290;&#23427;&#21487;&#20197;&#20135;&#29983;&#26356;&#22810;&#30340;&#20107;&#23454;&#20381;&#25454;&#65292;&#20943;&#23569;&#29983;&#25104;&#30340;&#24187;&#35273;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CoK&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#25512;&#29702;&#20934;&#22791;&#12289;&#21160;&#24577;&#30693;&#35782;&#36866;&#24212;&#21644;&#31572;&#26696;&#25972;&#21512;&#12290;&#32473;&#23450;&#19968;&#20010;&#30693;&#35782;&#23494;&#38598;&#22411;&#38382;&#39064;&#65292;CoK&#39318;&#20808;&#20934;&#22791;&#33509;&#24178;&#20010;&#21021;&#27493;&#30340;&#20381;&#25454;&#21644;&#31572;&#26696;&#65292;&#21516;&#26102;&#35782;&#21035;&#20986;&#30456;&#20851;&#30340;&#30693;&#35782;&#39046;&#22495;&#12290;&#22914;&#26524;&#26679;&#26412;&#20013;&#30340;&#31572;&#26696;&#27809;&#26377;&#22810;&#25968;&#20849;&#35782;&#65292;CoK&#36890;&#36807;&#20174;&#35782;&#21035;&#20986;&#30340;&#39046;&#22495;&#20013;&#36880;&#27493;&#36866;&#24212;&#30693;&#35782;&#26469;&#32416;&#27491;&#20381;&#25454;&#12290;&#36825;&#20123;&#32416;&#27491;&#21518;&#30340;&#20381;&#25454;&#21487;&#20197;&#26356;&#22909;&#22320;&#20316;&#20026;&#26368;&#32456;&#31572;&#26696;&#25972;&#21512;&#30340;&#22522;&#30784;&#12290;&#19981;&#21516;&#20110;&#20043;&#21069;&#20027;&#35201;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#30740;&#31350;&#65292;CoK&#36824;&#21033;&#29992;&#32467;&#26500;&#21270;&#30340;&#30693;&#35782;&#28304;&#65292;&#22914;Wikidata&#21644;&#34920;&#26684;&#65292;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#20107;&#23454;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present chain-of-knowledge (CoK), a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources. It results in more factual rationales and reduced hallucination in generation. Specifically, CoK consists of three stages: reasoning preparation, dynamic knowledge adapting, and answer consolidation. Given a knowledge-intensive question, CoK first prepares several preliminary rationales and answers while identifying the relevant knowledge domains. If there is no majority consensus among the answers from samples, CoK corrects the rationales step by step by adapting knowledge from the identified domains. These corrected rationales can plausibly serve as a better foundation for the final answer consolidation. Unlike prior studies that primarily use unstructured data, CoK also leverages structured knowledge sources such as Wikidata and tables that provide more reliable factual information. To access both unstructure
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CRITIC&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#19982;&#24037;&#20855;&#30340;&#20132;&#20114;&#26657;&#27491;&#33258;&#24049;&#30340;&#38169;&#35823;&#65292;&#20174;&#32780;&#36991;&#20813;&#29983;&#25104;&#20986;&#29616;&#19981;&#19968;&#33268;&#21644;&#38382;&#39064;&#34892;&#20026;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11738</link><description>&lt;p&gt;
CRITIC&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24037;&#20855;&#20132;&#20114;&#25209;&#35780;&#36827;&#34892;&#33258;&#25105;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing. (arXiv:2305.11738v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CRITIC&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#19982;&#24037;&#20855;&#30340;&#20132;&#20114;&#26657;&#27491;&#33258;&#24049;&#30340;&#38169;&#35823;&#65292;&#20174;&#32780;&#36991;&#20813;&#29983;&#25104;&#20986;&#29616;&#19981;&#19968;&#33268;&#21644;&#38382;&#39064;&#34892;&#20026;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#38750;&#24120;&#24341;&#20154;&#27880;&#30446;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26377;&#26102;&#20250;&#20986;&#29616;&#19981;&#19968;&#33268;&#21644;&#38382;&#39064;&#34892;&#20026;&#65292;&#20363;&#22914;&#20986;&#29616;&#24187;&#35273;&#20107;&#23454;&#65292;&#29983;&#25104;&#26377;&#32570;&#38519;&#30340;&#20195;&#30721;&#25110;&#21019;&#24314;&#20882;&#29359;&#21644;&#26377;&#23475;&#30340;&#20869;&#23481;&#12290;&#19982;&#36825;&#20123;&#27169;&#22411;&#19981;&#21516;&#65292;&#20154;&#31867;&#36890;&#24120;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#26469;&#20132;&#21449;&#26816;&#26597;&#21644;&#31934;&#28860;&#20182;&#20204;&#30340;&#21021;&#27493;&#20869;&#23481;&#65292;&#20363;&#22914;&#20351;&#29992;&#25628;&#32034;&#24341;&#25806;&#36827;&#34892;&#20107;&#23454;&#26816;&#26597;&#25110;&#20351;&#29992;&#20195;&#30721;&#35299;&#37322;&#22120;&#36827;&#34892;&#35843;&#35797;&#12290;&#21463;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;CRITIC&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;LLMs&#65288;&#23454;&#36136;&#19978;&#26159;&#8220;&#40657;&#30418;&#23376;&#8221;&#65289;&#20197;&#31867;&#20284;&#20110;&#20154;&#31867;&#19982;&#24037;&#20855;&#20132;&#20114;&#30340;&#26041;&#24335;&#39564;&#35777;&#21644;&#36880;&#27493;&#20462;&#27491;&#33258;&#24049;&#30340;&#36755;&#20986;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#20174;&#21021;&#22987;&#36755;&#20986;&#24320;&#22987;&#65292;CRITIC&#19982;&#36866;&#24403;&#30340;&#24037;&#20855;&#20132;&#20114;&#20197;&#35780;&#20272;&#25991;&#26412;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#28982;&#21518;&#26681;&#25454;&#22312;&#27492;&#39564;&#35777;&#36807;&#31243;&#20013;&#33719;&#24471;&#30340;&#21453;&#39304;&#20462;&#25913;&#36755;&#20986;&#12290;&#28041;&#21450;&#33258;&#30001;&#24418;&#24335;&#38382;&#31572;&#12289;&#25968;&#23398;&#31243;&#24207;&#32508;&#21512;&#21644;&#27602;&#24615;&#26816;&#27979;&#30340;&#20840;&#38754;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;LLMs&#33021;&#22815;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#24182;&#32416;&#27491;&#33258;&#24049;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in large language models (LLMs) have been impressive. However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. Unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. Inspired by this observation, we introduce a framework called CRITIC that allows LLMs, which are essentially "black boxes" to validate and progressively amend their own outputs in a manner similar to human interaction with tools. More specifically, starting with an initial output, CRITIC interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. Comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxi
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20221;&#20013;&#25991;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;270,000&#20010;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#21644;1,000&#20010;&#39640;&#36136;&#37327;&#27880;&#37322;&#23454;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#26377;&#24453;&#25913;&#36827;&#65292;&#35813;&#20219;&#21153;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.11527</link><description>&lt;p&gt;
InstructIE: &#19968;&#20221;&#22522;&#20110;&#25351;&#20196;&#30340;&#20013;&#25991;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
InstructIE: A Chinese Instruction-based Information Extraction Dataset. (arXiv:2305.11527v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11527
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20221;&#20013;&#25991;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;270,000&#20010;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#21644;1,000&#20010;&#39640;&#36136;&#37327;&#27880;&#37322;&#23454;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#26377;&#24453;&#25913;&#36827;&#65292;&#35813;&#20219;&#21153;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#31216;&#20026;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462; (Instruction-based IE)&#65292;&#23427;&#26088;&#22312;&#35201;&#27714;&#31995;&#32479;&#36981;&#24490;&#29305;&#23450;&#30340;&#25351;&#20196;&#25110;&#25351;&#21335;&#26469;&#25552;&#21462;&#20449;&#24687;&#12290;&#20026;&#20102;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;&#20013;&#25991;&#32500;&#22522;&#30334;&#31185;&#30340; 270,000 &#20010;&#24369;&#30417;&#30563;&#25968;&#25454;&#21644; 1,000 &#20010;&#39640;&#36136;&#37327;&#20247;&#21253;&#27880;&#37322;&#23454;&#20363;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#21508;&#31181;&#22522;&#32447;&#27169;&#22411;&#22312;InstructIE&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#24456;&#26377;&#24076;&#26395;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#26696;&#20363;&#30740;&#31350;&#20998;&#26512;&#65292;&#24378;&#35843;&#20102;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312; https://github.com/zjunlp/DeepKE/tree/main/example/llm &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new Information Extraction (IE) task dubbed Instruction-based IE, which aims to ask the system to follow specific instructions or guidelines to extract information. To facilitate research in this area, we construct a dataset called InstructIE, consisting of 270,000 weakly supervised data from Chinese Wikipedia and 1,000 high-quality crowdsourced annotated instances. We further evaluate the performance of various baseline models on the InstructIE dataset. The results reveal that although current models exhibit promising performance, there is still room for improvement. Furthermore, we conduct a comprehensive case study analysis, underlining the challenges inherent in the Instruction-based IE task. Code and dataset are available at https://github.com/zjunlp/DeepKE/tree/main/example/llm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#23545;&#24050;&#35757;&#32451;&#22909;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#21518;&#26399;&#32534;&#36753;&#65292;&#20197;&#20415;&#32534;&#36753;&#25481;&#26576;&#20123;&#26465;&#20214;&#20998;&#25903;&#65292;&#36825;&#20123;&#26465;&#20214;&#20998;&#25903;&#24456;&#21487;&#33021;&#20250;&#29983;&#25104;&#19981;&#33391;&#20869;&#23481;&#12290;&#36890;&#36807;&#31934;&#31616;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#32593;&#32476;&#23454;&#29616;&#65292;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#26377;&#25928;&#12289;&#39640;&#25928;&#12289;&#20855;&#26377;&#21487;&#25511;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11351</link><description>&lt;p&gt;
&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Data Redaction from Conditional Generative Models. (arXiv:2305.11351v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#23545;&#24050;&#35757;&#32451;&#22909;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#21518;&#26399;&#32534;&#36753;&#65292;&#20197;&#20415;&#32534;&#36753;&#25481;&#26576;&#20123;&#26465;&#20214;&#20998;&#25903;&#65292;&#36825;&#20123;&#26465;&#20214;&#20998;&#25903;&#24456;&#21487;&#33021;&#20250;&#29983;&#25104;&#19981;&#33391;&#20869;&#23481;&#12290;&#36890;&#36807;&#31934;&#31616;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#32593;&#32476;&#23454;&#29616;&#65292;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#26377;&#25928;&#12289;&#39640;&#25928;&#12289;&#20855;&#26377;&#21487;&#25511;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22240;&#29983;&#25104;&#19981;&#33391;&#20869;&#23481;&#32780;&#21463;&#21040;&#25209;&#35780;&#12290;&#20256;&#32479;&#30340;&#32531;&#35299;&#26041;&#27861;&#21253;&#25324;&#37325;&#26032;&#35757;&#32451;&#12289;&#36807;&#28388;&#25110;&#32534;&#36753;&#65307;&#28982;&#32780;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#35201;&#20040;&#20250;&#34987;&#31532;&#19977;&#26041;&#22238;&#36991;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#22914;&#20309;&#21518;&#26399;&#32534;&#36753;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#20854;&#32534;&#36753;&#25481;&#26576;&#20123;&#26465;&#20214;&#20998;&#25903;&#65292;&#36825;&#20123;&#26465;&#20214;&#20998;&#25903;&#24456;&#21487;&#33021;&#20250;&#29983;&#25104;&#19981;&#33391;&#20869;&#23481;&#12290;&#36825;&#26159;&#36890;&#36807;&#31934;&#31616;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#32593;&#32476;&#26469;&#23454;&#29616;&#30340;&#65292;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#26082;&#26377;&#25928;&#21448;&#39640;&#25928;&#12289;&#20855;&#26377;&#21487;&#25511;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#33021;&#29992;&#20110;&#19968;&#31867;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#25968;&#25454;&#32534;&#36753;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#65292;&#30456;&#27604;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#32534;&#36753;&#36136;&#37327;&#21644;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models are known to produce undesirable samples such as harmful content. Traditional mitigation methods include re-training from scratch, filtering, or editing; however, these are either computationally expensive or can be circumvented by third parties. In this paper, we take a different approach and study how to post-edit an already-trained conditional generative model so that it redacts certain conditionals that will, with high probability, lead to undesirable content. This is done by distilling the conditioning network in the models, giving a solution that is effective, efficient, controllable, and universal for a class of deep generative models. We conduct experiments on redacting prompts in text-to-image models and redacting voices in text-to-speech models. Our method is computationally light, leads to better redaction quality and robustness than baseline methods while still retaining high generation quality.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;Error Analysis Prompting&#21487;&#25913;&#21892;LLMs&#22312;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#19978;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2303.13809</link><description>&lt;p&gt;
&#38169;&#35823;&#20998;&#26512;&#25552;&#31034;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32763;&#35793;&#35780;&#20272;&#26041;&#38754;&#23454;&#29616;&#20102;&#20154;&#31867;&#27700;&#24179;&#65306;&#20197;ChatGPT&#20026;&#20363;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models: A Case Study on ChatGPT. (arXiv:2303.13809v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;Error Analysis Prompting&#21487;&#25913;&#21892;LLMs&#22312;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#19978;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20363;&#22914;ChatGPT&#65292;&#22312;&#26426;&#22120;&#32763;&#35793;&#12289;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#31561;&#22810;&#20010;NLP&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;ChatGPT&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#22312;&#31995;&#32479;&#27700;&#24179;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#27573;&#33853;&#27700;&#24179;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;LLM&#22312;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#19978;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20851;&#20110;&#20960;&#31181;&#25552;&#31034;&#26041;&#27861;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;Chain-of-Thoughts&#21644;Error Analysis&#32467;&#21512;&#36215;&#26469;&#65292;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;Error Analysis Prompting&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;LLM&#21487;&#20197;&#22312;&#31995;&#32479;&#21644;&#27573;&#33853;&#32423;&#21035;&#19978;&#29983;&#25104;&#20154;&#31867;&#33324;&#30340;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;ChatGPT&#20316;&#20026;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#22120;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#22312;&#25552;&#20379;&#21333;&#20010;&#26597;&#35810;&#20013;&#30340;&#22810;&#20010;&#35793;&#25991;&#26102;&#23384;&#22312;&#19981;&#31283;&#23450;&#30340;&#35780;&#20998;&#21644;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative large language models (LLMs), e.g., ChatGPT, have demonstrated remarkable proficiency across several NLP tasks such as machine translation, question answering, text summarization, and natural language understanding. Recent research has shown that utilizing ChatGPT for assessing the quality of machine translation (MT) achieves state-of-the-art performance at the system level but performs poorly at the segment level. To further improve the performance of LLMs on MT quality assessment, we conducted an investigation into several prompting methods. Our results indicate that by combining Chain-of-Thoughts and Error Analysis, a new prompting method called \textbf{\texttt{Error Analysis Prompting}}, LLMs like ChatGPT can \textit{generate human-like MT evaluations at both the system and segment level}. Additionally, we discovered some limitations of ChatGPT as an MT evaluator, such as unstable scoring and biases when provided with multiple translations in a single query. Our findings
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ChatExtract&#26041;&#27861;&#65292;&#20351;&#29992;&#20808;&#36827;&#30340;&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#24037;&#31243;&#65292;&#33258;&#21160;&#20174;&#30740;&#31350;&#35770;&#25991;&#20013;&#25552;&#21462;&#20934;&#30830;&#30340;&#25968;&#25454;&#65292;&#19981;&#38656;&#35201;&#22823;&#37327;&#30340;&#21069;&#26399;&#21162;&#21147;&#21644;&#32972;&#26223;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2303.05352</link><description>&lt;p&gt;
&#20351;&#29992;&#20250;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#24037;&#31243;&#20174;&#30740;&#31350;&#35770;&#25991;&#20013;&#25552;&#21462;&#20934;&#30830;&#30340;&#26448;&#26009;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Extracting Accurate Materials Data from Research Papers with Conversational Language Models and Prompt Engineering. (arXiv:2303.05352v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ChatExtract&#26041;&#27861;&#65292;&#20351;&#29992;&#20808;&#36827;&#30340;&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#24037;&#31243;&#65292;&#33258;&#21160;&#20174;&#30740;&#31350;&#35770;&#25991;&#20013;&#25552;&#21462;&#20934;&#30830;&#30340;&#25968;&#25454;&#65292;&#19981;&#38656;&#35201;&#22823;&#37327;&#30340;&#21069;&#26399;&#21162;&#21147;&#21644;&#32972;&#26223;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#27491;&#22312;&#19981;&#26029;&#21162;&#21147;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35821;&#35328;&#27169;&#22411;&#21644;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#26367;&#25163;&#24037;&#20174;&#30740;&#31350;&#35770;&#25991;&#20013;&#25552;&#21462;&#25968;&#25454;&#30340;&#24037;&#20316;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#20174;&#22823;&#37327;&#30740;&#31350;&#35770;&#25991;&#20013;&#25552;&#21462;&#25968;&#25454;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#21069;&#26399;&#21162;&#21147;&#12289;&#19987;&#19994;&#30693;&#35782;&#21644;&#32534;&#30721;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChatExtract&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#20808;&#36827;&#30340;&#23545;&#35805;&#24335;LLM&#33258;&#21160;&#25552;&#21462;&#26497;&#20934;&#30830;&#30340;&#25968;&#25454;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#21021;&#26399;&#30340;&#21162;&#21147;&#21644;&#32972;&#26223;&#30693;&#35782;&#12290;ChatExtract&#30001;&#19968;&#32452;&#24037;&#31243;&#21270;&#30340;&#25552;&#31034;&#24212;&#29992;&#20110;&#23545;&#35805;&#24335;LLM&#65292;&#26082;&#21487;&#20197;&#35782;&#21035;&#20986;&#20855;&#26377;&#25968;&#25454;&#30340;&#21477;&#23376;&#65292;&#25552;&#21462;&#20986;&#36825;&#20123;&#25968;&#25454;&#65292;&#21448;&#21487;&#20197;&#36890;&#36807;&#19968;&#31995;&#21015;&#36319;&#36827;&#38382;&#39064;&#30830;&#20445;&#25968;&#25454;&#30340;&#27491;&#30830;&#24615;&#12290;&#36825;&#20123;&#36319;&#36827;&#38382;&#39064;&#24456;&#22823;&#31243;&#24230;&#19978;&#20811;&#26381;&#20102;LLM&#25552;&#20379;&#20107;&#23454;&#19981;&#20934;&#30830;&#31572;&#26696;&#30340;&#24050;&#30693;&#38382;&#39064;&#12290;ChatExtract&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#23545;&#35805;&#24335;LLM&#65292;&#24182;&#33021;&#25552;&#20379;&#38750;&#24120;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a growing effort to replace hand extraction of data from research papers with automated data extraction based on natural language processing, language models, and recently, large language models (LLMs). Although these methods enable efficient extraction of data from large sets of research papers, they require a significant amount of up-front effort, expertise, and coding. In this work we propose the ChatExtract method that can fully automate very accurate data extraction with minimal initial effort and background, using an advanced conversational LLM. ChatExtract consists of a set of engineered prompts applied to a conversational LLM that both identify sentences with data, extract that data, and assure the data's correctness through a series of follow-up questions. These follow-up questions largely overcome known issues with LLMs providing factually inaccurate responses. ChatExtract can be applied with any conversational LLMs and yields very high quality data extraction.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#36880;&#23618;&#32858;&#21512;&#24322;&#24120;&#24471;&#20998;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#36827;&#34892;&#25991;&#26412;OOD&#26816;&#27979;&#12290;&#20854;&#33021;&#21457;&#25496;&#19981;&#21516;&#23618;&#36755;&#20986;&#30340;&#20248;&#21183;&#65292;&#36798;&#21040;&#26356;&#40065;&#26834;&#30340;&#24615;&#33021;&#65292;&#24182;&#25193;&#23637;&#32463;&#20856;&#22522;&#20934;&#27979;&#35797;&#20197;&#21453;&#26144;&#26356;&#29616;&#23454;&#30340;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2302.09852</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#36880;&#23618;&#25991;&#26412;OOD&#26816;&#27979;&#24471;&#20998;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Layer-wise Score Aggregation for Textual OOD Detection. (arXiv:2302.09852v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09852
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#36880;&#23618;&#32858;&#21512;&#24322;&#24120;&#24471;&#20998;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#36827;&#34892;&#25991;&#26412;OOD&#26816;&#27979;&#12290;&#20854;&#33021;&#21457;&#25496;&#19981;&#21516;&#23618;&#36755;&#20986;&#30340;&#20248;&#21183;&#65292;&#36798;&#21040;&#26356;&#40065;&#26834;&#30340;&#24615;&#33021;&#65292;&#24182;&#25193;&#23637;&#32463;&#20856;&#22522;&#20934;&#27979;&#35797;&#20197;&#21453;&#26144;&#26356;&#29616;&#23454;&#30340;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#22522;&#20110;AI&#30340;&#31995;&#32479;&#22686;&#21152;&#65292;OOD&#26816;&#27979;&#26159;&#19968;&#20010;&#36805;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#30001;&#20110;&#26032;&#30340;&#40065;&#26834;&#24615;&#21644;&#23433;&#20840;&#24615;&#35201;&#27714;&#12290;&#29616;&#26377;&#30340;OOD&#25991;&#26412;&#26816;&#27979;&#22120;&#36890;&#24120;&#20381;&#36182;&#20110;&#22312;&#32534;&#30721;&#22120;&#30340;&#26368;&#21518;&#19968;&#23618;&#36755;&#20986;&#19978;&#35745;&#31639;&#30340;&#24322;&#24120;&#24471;&#20998;&#65288;&#20363;&#22914;&#39532;&#27663;&#36317;&#31163;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;OOD&#26816;&#27979;&#24615;&#33021;&#22240;&#20219;&#21153;&#21644;&#23618;&#36755;&#20986;&#32780;&#24322;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#34920;&#26126;&#36890;&#24120;&#30340;&#36873;&#25321;&#65288;&#26368;&#21518;&#19968;&#23618;&#65289;&#24456;&#23569;&#26159;OOD&#26816;&#27979;&#30340;&#26368;&#20339;&#36873;&#25321;&#65292;&#22914;&#26524;&#36873;&#25321;&#26368;&#20339;&#23618;&#65292;&#21017;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#26469;&#32467;&#21512;&#36880;&#23618;&#30340;&#24322;&#24120;&#24471;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21253;&#25324;&#26356;&#22810;&#31867;&#21035;&#30340;&#20998;&#31867;&#20219;&#21153;&#65288;&#39640;&#36798;77&#65289;&#25193;&#23637;&#20102;&#32463;&#20856;&#25991;&#26412;OOD&#22522;&#20934;&#27979;&#35797;&#65292;&#20174;&#32780;&#21453;&#26144;&#26356;&#29616;&#23454;&#30340;&#35774;&#32622;&#12290;&#22312;&#36825;&#20010;&#22686;&#24378;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#21518;&#32858;&#21512;&#26041;&#27861;&#23454;&#29616;&#20102;&#40065;&#26834;&#30340;OOD&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection is a rapidly growing field due to new robustness and security requirements driven by an increased number of AI-based systems. Existing OOD textual detectors often rely on an anomaly score (e.g., Mahalanobis distance) computed on the embedding output of the last layer of the encoder. In this work, we observe that OOD detection performance varies greatly depending on the task and layer output. More importantly, we show that the usual choice (the last layer) is rarely the best one for OOD detection and that far better results could be achieved if the best layer were picked. To leverage this observation, we propose a data-driven, unsupervised method to combine layer-wise anomaly scores. In addition, we extend classical textual OOD benchmarks by including classification tasks with a greater number of classes (up to 77), which reflects more realistic settings. On this augmented benchmark, we show that the proposed post-aggregation methods achieve robust an
&lt;/p&gt;</description></item></channel></rss>