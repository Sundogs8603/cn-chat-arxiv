<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#30721;&#29983;&#25104;&#23454;&#29616;&#27169;&#22359;&#21270;&#35270;&#35273;&#38382;&#31572;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;VQA&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.05392</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#30721;&#29983;&#25104;&#26469;&#23454;&#29616;&#27169;&#22359;&#21270;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Modular Visual Question Answering via Code Generation. (arXiv:2306.05392v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05392
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#30721;&#29983;&#25104;&#23454;&#29616;&#27169;&#22359;&#21270;&#35270;&#35273;&#38382;&#31572;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;VQA&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#35270;&#35273;&#38382;&#31572;&#38382;&#39064;&#20316;&#20026;&#27169;&#22359;&#21270;&#20195;&#30721;&#29983;&#25104;&#26469;&#23454;&#29616;&#12290;&#19982;&#20043;&#21069;&#30340;&#27169;&#22359;&#21270;VQA&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#65292;&#24182;&#20381;&#36182;&#20110;&#38024;&#23545;&#22270;&#20687;&#23383;&#24149;&#23545;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#12289;&#35270;&#35273;&#27169;&#22411;&#65292;&#20197;&#21450;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;50&#20010;VQA&#31034;&#20363;&#12290;&#29983;&#25104;&#30340;Python&#31243;&#24207;&#36890;&#36807;&#31639;&#26415;&#21644;&#26465;&#20214;&#36923;&#36753;&#35843;&#29992;&#21644;&#32452;&#21512;&#35270;&#35273;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#19982;&#19981;&#37319;&#29992;&#20195;&#30721;&#29983;&#25104;&#30340;few-shot&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#33267;&#23569;&#25552;&#39640;COVR&#25968;&#25454;&#38598;&#19978;3%&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;GQA&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#32422;2%&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a framework that formulates visual question answering as modular code generation. In contrast to prior work on modular approaches to VQA, our approach requires no additional training and relies on pre-trained language models (LMs), visual models pre-trained on image-caption pairs, and fifty VQA examples used for in-context learning. The generated Python programs invoke and compose the outputs of the visual models using arithmetic and conditional logic. Our approach improves accuracy on the COVR dataset by at least 3% and on the GQA dataset by roughly 2% compared to the few-shot baseline that does not employ code generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20799;&#31461;&#35799;&#27468;&#20013;&#30340;&#35805;&#35821;&#24773;&#24863;&#21160;&#24577;&#65292;&#21457;&#29616;&#38543;&#30528;&#24180;&#40836;&#22686;&#38271;&#65292;&#37096;&#20998;&#24773;&#24863;&#24378;&#24230;&#25345;&#32493;&#19978;&#21319;&#65292;&#24773;&#24863;&#22522;&#35843;&#25345;&#32493;&#19979;&#38477;&#65292;&#24773;&#24863;&#21464;&#24322;&#24615;&#12289;&#19978;&#21319;&#36895;&#29575;&#21644;&#24674;&#22797;&#36895;&#29575;&#19981;&#26029;&#21152;&#24378;&#65292;&#24182;&#21487;&#20026;&#26410;&#26469;&#24773;&#24863;&#21160;&#24577;&#30740;&#31350;&#21644;&#20799;&#31461;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#25552;&#20379;&#20511;&#37492;&#12290;</title><link>http://arxiv.org/abs/2306.05387</link><description>&lt;p&gt;
&#20799;&#31461;&#35799;&#27468;&#20013;&#30340;&#35805;&#35821;&#24773;&#24863;&#21160;&#24577;&#65306;&#19981;&#21516;&#24180;&#40836;&#27573;&#30340;&#24773;&#24863;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Utterance Emotion Dynamics in Children's Poems: Emotional Changes Across Age. (arXiv:2306.05387v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20799;&#31461;&#35799;&#27468;&#20013;&#30340;&#35805;&#35821;&#24773;&#24863;&#21160;&#24577;&#65292;&#21457;&#29616;&#38543;&#30528;&#24180;&#40836;&#22686;&#38271;&#65292;&#37096;&#20998;&#24773;&#24863;&#24378;&#24230;&#25345;&#32493;&#19978;&#21319;&#65292;&#24773;&#24863;&#22522;&#35843;&#25345;&#32493;&#19979;&#38477;&#65292;&#24773;&#24863;&#21464;&#24322;&#24615;&#12289;&#19978;&#21319;&#36895;&#29575;&#21644;&#24674;&#22797;&#36895;&#29575;&#19981;&#26029;&#21152;&#24378;&#65292;&#24182;&#21487;&#20026;&#26410;&#26469;&#24773;&#24863;&#21160;&#24577;&#30740;&#31350;&#21644;&#20799;&#31461;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#25552;&#20379;&#20511;&#37492;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#24515;&#29702;&#30149;&#29702;&#23398;&#30740;&#31350;&#34920;&#26126;&#65292;&#24773;&#24863;&#29366;&#24577;&#30340;&#21464;&#21270;&#27169;&#24335;&#8212;&#8212;&#24773;&#24863;&#21160;&#24577;&#8212;&#8212;&#19982;&#25972;&#20307;&#31119;&#31049;&#21644;&#24515;&#29702;&#20581;&#24247;&#26377;&#20851;&#12290;&#26368;&#36817;&#65292;&#26377;&#19968;&#20123;&#24037;&#20316;&#22312;&#36861;&#36394;&#24773;&#24863;&#21160;&#24577;&#65292;&#20197;&#20415;&#22312;&#26102;&#38388;&#21644;&#20154;&#32676;&#19978;&#25910;&#38598;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#24773;&#24863;&#21160;&#24577;&#22914;&#20309;&#38543;&#24180;&#40836;&#21464;&#21270;&#65292;&#29305;&#21035;&#26159;&#22312;&#23401;&#23376;&#20204;&#30340;&#20889;&#20316;&#20013;&#22914;&#20309;&#30830;&#23450;&#65292;&#20173;&#26377;&#19968;&#20123;&#38382;&#39064;&#26410;&#24471;&#21040;&#35299;&#31572;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#35789;&#24211;&#21644;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#30001;&#19981;&#21516;&#24180;&#40836;&#27573;&#20799;&#31461;&#20889;&#30340;&#35799;&#27468;&#25152;&#30830;&#23450;&#30340;&#24773;&#24863;&#21160;&#24577;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20004;&#31181;&#26041;&#27861;&#37117;&#25351;&#21521;&#31867;&#20284;&#30340;&#36235;&#21183;&#65306;&#26576;&#20123;&#24773;&#24863;&#65288;&#22914;&#24868;&#24594;&#12289;&#23475;&#24597;&#12289;&#21916;&#24742;&#12289;&#24754;&#20260;&#12289;&#21796;&#37266;&#21644;&#25903;&#37197;&#65289;&#30340;&#24378;&#24230;&#38543;&#24180;&#40836;&#22686;&#21152;&#32780;&#25345;&#32493;&#22686;&#21152;&#65292;&#32780;&#24773;&#24863;&#30340;&#22522;&#35843;&#38543;&#24180;&#40836;&#38477;&#20302;&#32780;&#20445;&#25345;&#31283;&#23450;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#38543;&#30528;&#24180;&#40836;&#30340;&#22686;&#38271;&#65292;&#24773;&#24863;&#21464;&#24322;&#24615;&#12289;&#19978;&#21319;&#36895;&#29575;&#65288;&#21363;&#24773;&#24863;&#21453;&#24212;&#24615;&#65289;&#21644;&#24674;&#22797;&#36895;&#29575;&#65288;&#21363;&#24773;&#24863;&#22238;&#22797;&#21147;&#65289;&#20063;&#22312;&#19981;&#26029;&#25552;&#39640;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#26410;&#26469;&#24773;&#24863;&#21160;&#24577;&#30340;&#30740;&#31350;&#65292;&#24182;&#24110;&#21161;&#30830;&#23450;&#20799;&#31461;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#21487;&#33021;&#21069;&#20806;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging psychopathology studies are showing that patterns of changes in emotional state -- emotion dynamics -- are associated with overall well-being and mental health. More recently, there has been some work in tracking emotion dynamics through one's utterances, allowing for data to be collected on a larger scale across time and people. However, several questions about how emotion dynamics change with age, especially in children, and when determined through children's writing, remain unanswered. In this work, we use both a lexicon and a machine learning based approach to quantify characteristics of emotion dynamics determined from poems written by children of various ages. We show that both approaches point to similar trends: consistent increasing intensities for some emotions (e.g., anger, fear, joy, sadness, arousal, and dominance) with age and a consistent decreasing valence with age. We also find increasing emotional variability, rise rates (i.e., emotional reactivity), and recov
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#21033;&#29992;&#26469;&#33258;Telegram&#24179;&#21488;&#30340;&#25968;&#25454;&#65292;&#36890;&#36807;&#22522;&#20110;&#25991;&#26412;&#30340;&#20998;&#31867;&#22120;&#26816;&#27979;&#20154;&#26435;&#20405;&#29359;&#30340;&#25552;&#21450;&#24773;&#20917;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#31038;&#20132;&#23186;&#20307;&#22312;&#35760;&#24405;&#25112;&#20105;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#20154;&#26435;&#20405;&#29359;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.05370</link><description>&lt;p&gt;
&#22312;&#20420;&#20044;&#25112;&#20105;&#26399;&#38388;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#26816;&#27979;&#20154;&#26435;&#20405;&#29359;
&lt;/p&gt;
&lt;p&gt;
Detecting Human Rights Violations on Social Media during Russia-Ukraine War. (arXiv:2306.05370v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#21033;&#29992;&#26469;&#33258;Telegram&#24179;&#21488;&#30340;&#25968;&#25454;&#65292;&#36890;&#36807;&#22522;&#20110;&#25991;&#26412;&#30340;&#20998;&#31867;&#22120;&#26816;&#27979;&#20154;&#26435;&#20405;&#29359;&#30340;&#25552;&#21450;&#24773;&#20917;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#31038;&#20132;&#23186;&#20307;&#22312;&#35760;&#24405;&#25112;&#20105;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#20154;&#26435;&#20405;&#29359;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#20420;&#20044;&#20891;&#20107;&#20914;&#31361;&#25581;&#31034;&#20102;&#31038;&#20132;&#23186;&#20307;&#22312;&#30452;&#25509;&#20174;&#21069;&#32447;&#36879;&#26126;&#22320;&#12289;&#26080;&#25304;&#26080;&#26463;&#22320;&#20998;&#20139;&#20449;&#24687;&#26041;&#38754;&#25152;&#36215;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#34920;&#36798;&#33258;&#30001;&#21463;&#38480;&#12289;&#20449;&#24687;&#25112;&#20105;&#30427;&#34892;&#30340;&#20914;&#31361;&#22320;&#21306;&#65292;&#31038;&#20132;&#23186;&#20307;&#24050;&#25104;&#20026;&#19981;&#21487;&#25110;&#32570;&#30340;&#29983;&#21629;&#32447;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#20998;&#26512;&#26469;&#33258;Telegram&#30340;&#25968;&#25454;&#65292;&#36825;&#26159;&#21518;&#33487;&#32852;&#22320;&#21306;&#38405;&#35835;&#29420;&#31435;&#26032;&#38395;&#30340;&#20027;&#35201;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;95&#20010;&#20844;&#20849;Telegram&#39057;&#36947;&#30340;&#24086;&#23376;&#26679;&#26412;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#28085;&#30422;&#25919;&#27835;&#21644;&#25112;&#20105;&#26032;&#38395;&#30340;&#24086;&#23376;&#65292;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#25105;&#20204;&#21487;&#20197;&#37492;&#21035;&#20986;&#28508;&#22312;&#30340;&#20154;&#26435;&#20405;&#29359;&#20107;&#20214;&#12290;&#36890;&#36807;mBERT&#22522;&#20110;&#25991;&#26412;&#30340;&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#24050;&#32463;&#20998;&#26512;&#20102;Telegram&#24179;&#21488;&#19978;&#20219;&#20309;&#26377;&#20851;&#20154;&#26435;&#20405;&#29359;&#30340;&#25552;&#21450;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The present-day Russia-Ukraine military conflict has exposed the pivotal role of social media in enabling the transparent and unbridled sharing of information directly from the frontlines. In conflict zones where freedom of expression is constrained and information warfare is pervasive, social media has emerged as an indispensable lifeline. Anonymous social media platforms, as publicly available sources for disseminating war-related information, have the potential to serve as effective instruments for monitoring and documenting Human Rights Violations (HRV). Our research focuses on the analysis of data from Telegram, the leading social media platform for reading independent news in post-Soviet regions. We gathered a dataset of posts sampled from 95 public Telegram channels that cover politics and war news, which we have utilized to identify potential occurrences of HRV. Employing a mBERT-based text classifier, we have conducted an analysis to detect any mentions of HRV in the Telegram 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;ADAIO&#22242;&#38431;&#22312;BEA-2023&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#31995;&#32479;&#26041;&#26696;&#65292;&#20351;&#29992;OpenAI GPT-3&#35780;&#20272;&#22522;&#20934;&#27169;&#22411;&#24182;&#22312;&#25945;&#32946;&#23545;&#35805;&#20013;&#29983;&#25104;AI&#25945;&#24072;&#22238;&#24212;&#12290;&#36890;&#36807;&#23569;&#37327;&#25552;&#20379;&#25552;&#31034;&#20449;&#24687;&#65292;&#21033;&#29992;OpenAI&#30340;text-davinci-003&#27169;&#22411;&#22312;&#31454;&#36187;&#20013;&#33719;&#24471;&#31532;&#20108;&#21517;&#65292;&#24182;&#31361;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;AI&#25945;&#24072;&#35282;&#33394;&#20013;&#30340;&#23569;&#37327;&#25552;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.05360</link><description>&lt;p&gt;
BEA-2023&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;ADAIO&#31995;&#32479;&#65306;&#29983;&#25104;&#25945;&#32946;&#23545;&#35805;&#20013;AI&#25945;&#24072;&#22238;&#24212;
&lt;/p&gt;
&lt;p&gt;
The ADAIO System at the BEA-2023 Shared Task on Generating AI Teacher Responses in Educational Dialogues. (arXiv:2306.05360v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ADAIO&#22242;&#38431;&#22312;BEA-2023&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#31995;&#32479;&#26041;&#26696;&#65292;&#20351;&#29992;OpenAI GPT-3&#35780;&#20272;&#22522;&#20934;&#27169;&#22411;&#24182;&#22312;&#25945;&#32946;&#23545;&#35805;&#20013;&#29983;&#25104;AI&#25945;&#24072;&#22238;&#24212;&#12290;&#36890;&#36807;&#23569;&#37327;&#25552;&#20379;&#25552;&#31034;&#20449;&#24687;&#65292;&#21033;&#29992;OpenAI&#30340;text-davinci-003&#27169;&#22411;&#22312;&#31454;&#36187;&#20013;&#33719;&#24471;&#31532;&#20108;&#21517;&#65292;&#24182;&#31361;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;AI&#25945;&#24072;&#35282;&#33394;&#20013;&#30340;&#23569;&#37327;&#25552;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ADAIO&#22242;&#38431;&#22312;"Building Educational Applications (BEA) 2023"&#20849;&#20139;&#20219;&#21153;&#20013;&#65292;&#29983;&#25104;&#25945;&#32946;&#23545;&#35805;&#20013;AI&#25945;&#24072;&#22238;&#24212;&#30340;&#31995;&#32479;&#26041;&#26696;&#12290;&#26412;&#20219;&#21153;&#26088;&#22312;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;AI&#25945;&#24072;&#65292;&#22312;&#23398;&#29983;&#19982;&#25945;&#24072;&#30340;&#23545;&#35805;&#20013;&#20135;&#29983;&#21512;&#36866;&#22238;&#24212;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;OpenAI GPT-3&#35780;&#20272;&#20102;&#21508;&#31181;&#22522;&#20934;&#27169;&#22411;&#65292;&#24182;&#35774;&#35745;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#24335;&#26469;&#28608;&#21457;OpenAI&#27169;&#22411;&#29983;&#25104;&#25945;&#24072;&#22238;&#24212;&#12290;&#22312;&#31454;&#36187;&#20013;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#20351;&#29992;&#20102;&#22522;&#20110;&#23569;&#37327;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;OpenAI&#30340;text-davinci-003&#27169;&#22411;&#65292;&#33719;&#24471;&#20102;&#31532;&#20108;&#21517;&#30340;&#25104;&#32489;&#12290;&#32467;&#26524;&#31361;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#23588;&#20854;&#26159;OpenAI&#30340;GPT-3&#65289;&#22312;AI&#25945;&#24072;&#35282;&#33394;&#20013;&#30340;&#23569;&#37327;&#25552;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the ADAIO team's system entry in the Building Educational Applications (BEA) 2023 Shared Task on Generating AI Teacher Responses in Educational Dialogues. The task aims to assess the performance of state-of-the-art generative models as AI teachers in producing suitable responses within a student-teacher dialogue. Our system comprises evaluating various baseline models using OpenAI GPT-3 and designing diverse prompts to prompt the OpenAI models for teacher response generation. After the challenge, our system achieved second place by employing a few-shot prompt-based approach with the OpenAI text-davinci-003 model. The results highlight the few-shot learning capabilities of large-language models, particularly OpenAI's GPT-3, in the role of AI teachers.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21019;&#24314;&#20102;&#24847;&#22823;&#21033;&#31070;&#32463;&#31934;&#31070;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20986;&#22810;&#20013;&#24515;&#35782;&#21035;&#27169;&#22411;&#65292;&#25972;&#20307; F1&#24471;&#20998;&#20026;84.77%&#12290;&#35813;&#27169;&#22411;&#23558;&#24110;&#21161;&#20020;&#24202;&#20174;&#19994;&#32773;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#21307;&#30103;&#35760;&#24405;&#20013;&#33258;&#21160;&#25552;&#21462;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.05323</link><description>&lt;p&gt;
&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24847;&#22823;&#21033;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#24212;&#29992;&#65306;&#26041;&#27861;&#35770;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#30340;&#22810;&#20013;&#24515;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Advancing Italian Biomedical Information Extraction with Large Language Models: Methodological Insights and Multicenter Practical Application. (arXiv:2306.05323v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05323
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21019;&#24314;&#20102;&#24847;&#22823;&#21033;&#31070;&#32463;&#31934;&#31070;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20986;&#22810;&#20013;&#24515;&#35782;&#21035;&#27169;&#22411;&#65292;&#25972;&#20307; F1&#24471;&#20998;&#20026;84.77%&#12290;&#35813;&#27169;&#22411;&#23558;&#24110;&#21161;&#20020;&#24202;&#20174;&#19994;&#32773;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#21307;&#30103;&#35760;&#24405;&#20013;&#33258;&#21160;&#25552;&#21462;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#38498;&#24341;&#20837;&#35745;&#31639;&#26426;&#21270;&#21307;&#30103;&#35760;&#24405;&#26377;&#21161;&#20110;&#20943;&#23569;&#25163;&#20889;&#21644;&#20449;&#24687;&#25552;&#21462;&#31561;&#32321;&#29712;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#21307;&#30103;&#35760;&#24405;&#20013;&#25552;&#21462;&#25968;&#25454;&#38656;&#35201;&#26102;&#38388;&#21644;&#31934;&#21147;&#65292;&#22240;&#27492;&#21307;&#30103;&#35760;&#24405;&#20013;&#21253;&#21547;&#30340;&#25968;&#25454;&#20173;&#28982;&#34987;&#20805;&#20998;&#21033;&#29992;&#31243;&#24230;&#20302;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#23376;&#39046;&#22495;&#20449;&#24687;&#25552;&#21462;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#20174;&#19994;&#32773;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#20351;&#29992;&#33258;&#21160;&#21270;&#25991;&#26412;&#25366;&#25496;&#27969;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#24847;&#22823;&#21033;&#31070;&#32463;&#31934;&#31070;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598; PsyNIT&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#24320;&#21457;&#36825;&#19968;&#20219;&#21153;&#30340;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#22810;&#20010;&#23454;&#39564;&#65292;&#20351;&#29992;&#19977;&#20010;&#22806;&#37096;&#29420;&#31435;&#25968;&#25454;&#38598;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#22810;&#20013;&#24515;&#27169;&#22411;&#65292;&#25972;&#20307; F1 &#24471;&#20998;&#20026; 84.77%&#65292;&#31934;&#30830;&#29575;&#20026; 83.16%&#65292;&#21484;&#22238;&#29575;&#20026; 86.44%&#12290;&#25105;&#20204;&#23398;&#21040;&#30340;&#32463;&#39564;&#26159;: (i) &#19968;&#33268;&#30340;&#27880;&#37322;&#36807;&#31243;&#30340;&#20851;&#38190;&#20316;&#29992;&#21644; (ii) &#32467;&#21512;&#32463;&#20856;&#26041;&#27861;&#21644;&#8220;&#23569;&#37327;&#35757;&#32451;&#8221;&#30340; fine-tuning &#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of computerized medical records in hospitals has reduced burdensome operations like manual writing and information fetching. However, the data contained in medical records are still far underutilized, primarily because extracting them from unstructured textual medical records takes time and effort. Information Extraction, a subfield of Natural Language Processing, can help clinical practitioners overcome this limitation, using automated text-mining pipelines. In this work, we created the first Italian neuropsychiatric Named Entity Recognition dataset, PsyNIT, and used it to develop a Large Language Model for this task. Moreover, we conducted several experiments with three external independent datasets to implement an effective multicenter model, with overall F1-score 84.77%, Precision 83.16%, Recall 86.44%. The lessons learned are: (i) the crucial role of a consistent annotation process and (ii) a fine-tuning strategy that combines classical methods with a "few-shot" a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20026;IWSLT 2023&#22810;&#35821;&#35328;&#32763;&#35793;&#36129;&#29486;&#30340;&#32763;&#35793;&#31995;&#32479;&#65292;&#20854;&#37325;&#28857;&#22312;&#20110;&#32763;&#35793;&#31185;&#23398;&#20250;&#35758;&#28436;&#35762;&#12290;&#29992;&#8220;&#26816;&#32034;&#24335;&#26041;&#27861;&#8221;&#65288;kNN-MT&#65289;&#36827;&#34892;&#26377;&#25928;&#30340;&#36866;&#24212;&#65292;&#35813;&#31995;&#32479;&#37319;&#29992;&#36866;&#37197;&#22120;&#36731;&#26494;&#38598;&#25104;&#26469;&#33258;&#25968;&#25454;&#22686;&#24378;&#30340;&#22686;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#23637;&#31034;&#32423;&#32852;&#31995;&#32479;&#26356;&#23481;&#26131;&#36866;&#24212;&#29305;&#23450;&#30446;&#26631;&#39046;&#22495;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.05320</link><description>&lt;p&gt;
KIT&#30340;&#22810;&#35821;&#35328;&#28436;&#35762;&#32763;&#35793;&#31995;&#32479;&#22312;IWSLT 2023&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
KIT's Multilingual Speech Translation System for IWSLT 2023. (arXiv:2306.05320v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20026;IWSLT 2023&#22810;&#35821;&#35328;&#32763;&#35793;&#36129;&#29486;&#30340;&#32763;&#35793;&#31995;&#32479;&#65292;&#20854;&#37325;&#28857;&#22312;&#20110;&#32763;&#35793;&#31185;&#23398;&#20250;&#35758;&#28436;&#35762;&#12290;&#29992;&#8220;&#26816;&#32034;&#24335;&#26041;&#27861;&#8221;&#65288;kNN-MT&#65289;&#36827;&#34892;&#26377;&#25928;&#30340;&#36866;&#24212;&#65292;&#35813;&#31995;&#32479;&#37319;&#29992;&#36866;&#37197;&#22120;&#36731;&#26494;&#38598;&#25104;&#26469;&#33258;&#25968;&#25454;&#22686;&#24378;&#30340;&#22686;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#23637;&#31034;&#32423;&#32852;&#31995;&#32479;&#26356;&#23481;&#26131;&#36866;&#24212;&#29305;&#23450;&#30446;&#26631;&#39046;&#22495;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#26377;&#30340;&#35821;&#38899;&#32763;&#35793;&#22522;&#20934;&#27979;&#35797;&#37117;&#38024;&#23545;&#39640;&#21697;&#36136;&#24405;&#38899;&#26465;&#20214;&#19979;&#30340;&#20197;&#33521;&#35821;&#20026;&#27597;&#35821;&#30340;&#35821;&#38899;&#65292;&#36825;&#36890;&#24120;&#19982;&#23454;&#38469;&#20351;&#29992;&#24773;&#20917;&#20013;&#30340;&#26465;&#20214;&#19981;&#31526;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#20026;IWSLT 2023&#22810;&#35821;&#35328;&#36712;&#36947;&#35774;&#35745;&#30340;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#65292;&#37325;&#28857;&#32763;&#35793;&#31185;&#23398;&#20250;&#35758;&#28436;&#35762;&#12290;&#27979;&#35797;&#26465;&#20214;&#21253;&#25324;&#21475;&#38899;&#37325;&#30340;&#36755;&#20837;&#35821;&#38899;&#21644;&#26415;&#35821;&#23494;&#38598;&#30340;&#20869;&#23481;&#65292;&#24182;&#19988;&#38656;&#35201;&#32763;&#35793;&#25104;10&#31181;&#36164;&#28304;&#25968;&#37327;&#19981;&#21516;&#30340;&#35821;&#35328;&#12290;&#22312;&#27809;&#26377;&#26469;&#33258;&#30446;&#26631;&#39046;&#22495;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26816;&#32034;&#24335;&#26041;&#27861;&#65288;kNN-MT&#65289;&#36827;&#34892;&#26377;&#25928;&#30340;&#36866;&#24212;&#65288;&#35821;&#38899;&#32763;&#35793;+0.8 BLEU&#65289;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#36866;&#37197;&#22120;&#36731;&#26494;&#38598;&#25104;&#26469;&#33258;&#25968;&#25454;&#22686;&#24378;&#30340;&#22686;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#23637;&#31034;&#20854;&#19982;&#37325;&#26032;&#35757;&#32451;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#32423;&#32852;&#31995;&#32479;&#26356;&#23481;&#26131;&#36866;&#24212;&#29305;&#23450;&#30446;&#26631;&#39046;&#22495;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#30001;&#22810;&#20010;&#29420;&#31435;&#27169;&#22359;&#32452;&#25104;&#30340;&#12290;&#25105;&#20204;&#30340;&#32423;&#32852;&#35821;&#38899;&#31995;&#32479;&#36828;&#36828;&#20248;&#20110;&#20854;&#31471;&#21040;&#31471;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many existing speech translation benchmarks focus on native-English speech in high-quality recording conditions, which often do not match the conditions in real-life use-cases. In this paper, we describe our speech translation system for the multilingual track of IWSLT 2023, which focuses on the translation of scientific conference talks. The test condition features accented input speech and terminology-dense contents. The tasks requires translation into 10 languages of varying amounts of resources. In absence of training data from the target domain, we use a retrieval-based approach (kNN-MT) for effective adaptation (+0.8 BLEU for speech translation). We also use adapters to easily integrate incremental training data from data augmentation, and show that it matches the performance of re-training. We observe that cascaded systems are more easily adaptable towards specific target domains, due to their separate modules. Our cascaded speech system substantially outperforms its end-to-end 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;CUED&#22312;ProbSum 2023&#30340;&#27604;&#36187;&#20013;&#20351;&#29992;&#30340;&#23618;&#27425;&#21270;&#25688;&#35201;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;HESM&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#31934;&#32454;&#35843;&#25972;&#36807;&#30340;Clinical-T5&#27169;&#22411;&#30340;&#26631;&#35760;&#32423;&#38598;&#21512;&#21152;&#20197;&#32452;&#21512;&#24182;&#20351;&#29992;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#25688;&#35201;&#24615;&#33021;&#65292;&#26159;&#35813;&#20849;&#20139;&#20219;&#21153;&#20013;&#34920;&#29616;&#26368;&#20339;&#30340;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2306.05317</link><description>&lt;p&gt;
ProbSum 2023&#20013;&#30340;CUED&#65306;&#23618;&#27425;&#21270;&#25688;&#35201;&#27169;&#22411;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
CUED at ProbSum 2023: Hierarchical Ensemble of Summarization Models. (arXiv:2306.05317v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CUED&#22312;ProbSum 2023&#30340;&#27604;&#36187;&#20013;&#20351;&#29992;&#30340;&#23618;&#27425;&#21270;&#25688;&#35201;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;HESM&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#31934;&#32454;&#35843;&#25972;&#36807;&#30340;Clinical-T5&#27169;&#22411;&#30340;&#26631;&#35760;&#32423;&#38598;&#21512;&#21152;&#20197;&#32452;&#21512;&#24182;&#20351;&#29992;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#25688;&#35201;&#24615;&#33021;&#65292;&#26159;&#35813;&#20849;&#20139;&#20219;&#21153;&#20013;&#34920;&#29616;&#26368;&#20339;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#23545;&#24739;&#32773;&#21307;&#30103;&#36827;&#23637;&#31508;&#35760;&#36827;&#34892;&#25688;&#35201;&#30340;&#25361;&#25112;&#12290;&#22312;BioNLP Workshop 2023&#30340;Problem List Summarization (shared task 1A)&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;765&#20010;&#21307;&#30103;&#35786;&#25152;&#31508;&#35760;&#36827;&#34892;&#20102;&#31934;&#32454;&#35843;&#25972;&#30340;Clinical-T5&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#31995;&#32479;&#65292;&#21253;&#25324;&#25277;&#21462;&#24335;&#12289;&#29983;&#25104;&#24335;&#21644;&#38646;&#26679;&#20363;&#31995;&#32479;&#65292;&#20174;&#32780;&#20026;&#21307;&#30103;&#31508;&#35760;&#25688;&#35201;&#25552;&#20379;&#20102;&#21512;&#29702;&#30340;&#22522;&#32447;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Hierarchical Ensemble of Summarization Models (HESM)&#65292;&#23427;&#30001;&#19981;&#21516;&#30340;&#31934;&#32454;&#35843;&#25972;&#36807;&#30340;Clinical-T5&#27169;&#22411;&#30340;&#26631;&#35760;&#32423;&#38598;&#21512;&#32452;&#25104;&#65292;&#28982;&#21518;&#37319;&#29992;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#35299;&#30721;&#12290;&#25105;&#20204;&#30340;HESM&#26041;&#27861;&#22823;&#24133;&#25552;&#39640;&#20102;&#25688;&#35201;&#24615;&#33021;&#65292;&#24182;&#22312;&#20445;&#30041;&#25361;&#25112;&#25968;&#25454;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#36798;&#21040;&#20102;ROUGE-L&#20026;32.77&#65292;&#26159;&#20849;&#20139;&#20219;&#21153;&#25490;&#34892;&#27036;&#21069;&#21015;&#30340;&#26368;&#20339;&#34920;&#29616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the challenge of summarizing patients' medical progress notes in a limited data setting. For the Problem List Summarization (shared task 1A) at the BioNLP Workshop 2023, we demonstrate that Clinical-T5 fine-tuned to 765 medical clinic notes outperforms other extractive, abstractive and zero-shot baselines, yielding reasonable baseline systems for medical note summarization. Further, we introduce Hierarchical Ensemble of Summarization Models (HESM), consisting of token-level ensembles of diverse fine-tuned Clinical-T5 models, followed by Minimum Bayes Risk (MBR) decoding. Our HESM approach lead to a considerable summarization performance boost, and when evaluated on held-out challenge data achieved a ROUGE-L of 32.77, which was the best-performing system at the top of the shared task leaderboard.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#25991;&#26412;&#25968;&#25454;&#19978;&#23545;&#24615;&#21035;&#27495;&#35270;&#30340;&#20559;&#35265;&#30340;&#24230;&#37327;&#26631;&#20934;&#30340;&#23616;&#38480;&#24615;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2306.05307</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#20844;&#24179;&#24230;&#25351;&#26631;&#35780;&#20272;&#27495;&#35270;&#20559;&#24046;&#26159;&#21542;&#36275;&#22815;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are fairness metric scores enough to assess discrimination biases in machine learning?. (arXiv:2306.05307v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#25991;&#26412;&#25968;&#25454;&#19978;&#23545;&#24615;&#21035;&#27495;&#35270;&#30340;&#20559;&#35265;&#30340;&#24230;&#37327;&#26631;&#20934;&#30340;&#23616;&#38480;&#24615;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#25991;&#26412;&#25968;&#25454;&#19978;&#23545;&#24615;&#21035;&#27495;&#35270;&#30340;&#20559;&#35265;&#30340;&#24230;&#37327;&#26631;&#20934;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#20197;Bios&#25968;&#25454;&#38598;&#20026;&#20363;&#65292;&#23398;&#20064;&#39044;&#27979;&#20010;&#20154;&#30340;&#32844;&#19994;&#12290;&#36825;&#31181;&#39044;&#27979;&#20219;&#21153;&#22312;&#21830;&#19994;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#31243;&#24207;&#20013;&#24456;&#24120;&#35265;&#65292;&#20363;&#22914;&#33258;&#21160;&#24037;&#20316;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents novel experiments shedding light on the shortcomings of current metrics for assessing biases of gender discrimination made by machine learning algorithms on textual data. We focus on the Bios dataset, and our learning task is to predict the occupation of individuals, based on their biography. Such prediction tasks are common in commercial Natural Language Processing (NLP) applications such as automatic job recommendations. We address an important limitation of theoretical discussions dealing with group-wise fairness metrics: they focus on large datasets, although the norm in many industrial NLP applications is to use small to reasonably large linguistic datasets for which the main practical constraint is to get a good prediction accuracy. We then question how reliable are different popular measures of bias when the size of the training set is simply sufficient to learn reasonably accurate predictions. Our experiments sample the Bios dataset and learn more than 200 m
&lt;/p&gt;</description></item><item><title>ToolAlpaca&#26159;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#24037;&#20855;&#20351;&#29992;&#35821;&#26009;&#24211;&#65292;&#23454;&#29616;&#32039;&#20945;&#30340;&#35821;&#35328;&#27169;&#22411;&#21442;&#21152;&#36890;&#29992;&#24037;&#20855;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2306.05301</link><description>&lt;p&gt;
ToolAlpaca: &#36890;&#36807;3000&#20010;&#27169;&#25311;&#26696;&#20363;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36890;&#29992;&#24037;&#20855;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases. (arXiv:2306.05301v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05301
&lt;/p&gt;
&lt;p&gt;
ToolAlpaca&#26159;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#24037;&#20855;&#20351;&#29992;&#35821;&#26009;&#24211;&#65292;&#23454;&#29616;&#32039;&#20945;&#30340;&#35821;&#35328;&#27169;&#22411;&#21442;&#21152;&#36890;&#29992;&#24037;&#20855;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#21033;&#29992;&#29616;&#23454;&#24037;&#20855;&#23545;&#20110;&#23454;&#29616;&#20855;&#26377;&#24863;&#30693;&#33021;&#21147;&#30340;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#24037;&#20855;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#20381;&#38752;&#38750;&#24120;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20363;&#22914;GPT-4&#65292;&#22312;&#38646;-shot&#26041;&#24335;&#19979;&#33719;&#24471;&#36890;&#29992;&#30340;&#24037;&#20855;&#20351;&#29992;&#33021;&#21147;&#65292;&#25110;&#21033;&#29992;&#30417;&#30563;&#23398;&#20064;&#22312;&#32039;&#20945;&#30340;&#27169;&#22411;&#19978;&#35757;&#32451;&#26377;&#38480;&#31867;&#22411;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#22312;&#27809;&#26377;&#29305;&#23450;&#24037;&#20855;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#36890;&#29992;&#30340;&#24037;&#20855;&#20351;&#29992;&#33021;&#21147;&#23578;&#19981;&#30830;&#23450;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ToolAlpaca&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#33258;&#21160;&#29983;&#25104;&#24037;&#20855;&#20351;&#29992;&#35821;&#26009;&#24211;&#65292;&#24182;&#22312;&#26368;&#23567;&#20154;&#20026;&#24178;&#39044;&#19979;&#65292;&#22312;&#32039;&#20945;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#23398;&#20064;&#36890;&#29992;&#30340;&#24037;&#20855;&#20351;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enabling large language models to effectively utilize real-world tools is crucial for achieving embodied intelligence. Existing approaches to tool learning have primarily relied on either extremely large language models, such as GPT-4, to attain generalized tool-use abilities in a zero-shot manner, or have utilized supervised learning to train limited types of tools on compact models. However, it remains uncertain whether smaller language models can achieve generalized tool-use abilities without specific tool-specific training. To address this question, this paper introduces ToolAlpaca, a novel framework designed to automatically generate a tool-use corpus and learn generalized tool-use abilities on compact language models with minimal human intervention. Specifically, ToolAlpaca first collects a comprehensive dataset by building a multi-agent simulation environment, which contains 3938 tool-use instances from more than 400 real-world tool APIs spanning 50 distinct categories. Subseque
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;Few-shot Intent Classification&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#22312;&#22806;&#37096;&#36164;&#28304;&#19978;&#36830;&#32493;&#39044;&#35757;&#32451;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#30452;&#25509;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#24773;&#20917;&#19979;&#24050;&#32463;&#21487;&#20197;&#21462;&#24471;&#19981;&#38169;&#30340;&#32467;&#26524;&#65292;&#34920;&#26126;&#36830;&#32493;&#39044;&#35757;&#32451;&#24182;&#38750;&#24517;&#35201;&#12290;</title><link>http://arxiv.org/abs/2306.05278</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#20351;&#29992;PLMs&#30340;Few-shot Intent Classification: &#30452;&#25509;&#24494;&#35843; vs &#36830;&#32493;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Revisit Few-shot Intent Classification with PLMs: Direct Fine-tuning vs. Continual Pre-training. (arXiv:2306.05278v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;Few-shot Intent Classification&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#22312;&#22806;&#37096;&#36164;&#28304;&#19978;&#36830;&#32493;&#39044;&#35757;&#32451;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#30452;&#25509;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#24773;&#20917;&#19979;&#24050;&#32463;&#21487;&#20197;&#21462;&#24471;&#19981;&#38169;&#30340;&#32467;&#26524;&#65292;&#34920;&#26126;&#36830;&#32493;&#39044;&#35757;&#32451;&#24182;&#38750;&#24517;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;Few-shot Intent Classification&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#28041;&#21450;&#20165;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20197;&#22522;&#20110;&#20854;&#22522;&#30784;&#24847;&#22270;&#20998;&#31867;&#35805;&#35821;&#12290;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#24403;&#21069;&#26041;&#27861;&#26159;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#65292;&#21363;&#22312;&#22806;&#37096;&#36164;&#28304;&#65288;&#20363;&#22914;&#20250;&#35805;&#35821;&#26009;&#24211;&#12289;&#20844;&#20849;&#24847;&#22270;&#26816;&#27979;&#25968;&#25454;&#38598;&#25110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#25968;&#25454;&#38598;&#65289;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#28982;&#21518;&#20351;&#29992;&#23427;&#20204;&#20316;&#20026;&#35805;&#35821;&#32534;&#30721;&#22120;&#26469;&#35757;&#32451;&#24847;&#22270;&#20998;&#31867;&#22120;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36830;&#32493;&#39044;&#35757;&#32451;&#21487;&#33021;&#24182;&#38750;&#24517;&#35201;&#65292;&#22240;&#20026;PLMs&#22312;&#27492;&#20219;&#21153;&#19978;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#21487;&#33021;&#24182;&#19981;&#20687;&#39044;&#26399;&#30340;&#37027;&#26679;&#20005;&#37325;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#30452;&#25509;&#23545;&#20165;&#26377;&#23569;&#37327;&#26631;&#35760;&#31034;&#20363;&#30340;PLMs&#36827;&#34892;&#24494;&#35843;&#24050;&#32463;&#21487;&#20197;&#20135;&#29983;&#30456;&#24403;&#19981;&#38169;&#30340;&#32467;&#26524;&#65292;&#32780;&#32489;&#25928;&#24046;&#36317;&#38543;&#30528;&#26631;&#35760;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#36805;&#36895;&#32553;&#23567;&#12290;&#20026;&#20102;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#21363;&#27880;&#24847;&#21147;&#27969;&#25511;&#65288;Attention Flow Control&#65289;&#65292;&#20854;&#20801;&#35768;&#22312;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#23618;&#20043;&#38388;&#21160;&#24577;&#20998;&#37197;&#24494;&#35843;&#30340;&#37325;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the task of few-shot intent detection, which involves training a deep learning model to classify utterances based on their underlying intents using only a small amount of labeled data. The current approach to address this problem is through continual pre-training, i.e., fine-tuning pre-trained language models (PLMs) on external resources (e.g., conversational corpora, public intent detection datasets, or natural language understanding datasets) before using them as utterance encoders for training an intent classifier. In this paper, we show that continual pre-training may not be essential, since the overfitting problem of PLMs on this task may not be as serious as expected. Specifically, we find that directly fine-tuning PLMs on only a handful of labeled examples already yields decent results compared to methods that employ continual pre-training, and the performance gap diminishes rapidly as the number of labeled data increases. To maximize the utilization of the limited a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;19&#31181;&#22522;&#20110;Transformer&#30340;ADE&#25552;&#21462;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#24182;&#22312;&#20855;&#26377;&#19981;&#21516;&#38750;&#27491;&#24335;&#31243;&#24230;&#30340;&#25968;&#25454;&#38598;&#19978;&#27604;&#36739;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#25104;&#29087;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#25216;&#26415;&#65288;SHAP&#65289;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05276</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#25552;&#21462;&#26041;&#27861;&#30340;&#24191;&#27867;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Extensive Evaluation of Transformer-based Architectures for Adverse Drug Events Extraction. (arXiv:2306.05276v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;19&#31181;&#22522;&#20110;Transformer&#30340;ADE&#25552;&#21462;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#24182;&#22312;&#20855;&#26377;&#19981;&#21516;&#38750;&#27491;&#24335;&#31243;&#24230;&#30340;&#25968;&#25454;&#38598;&#19978;&#27604;&#36739;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#25104;&#29087;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#25216;&#26415;&#65288;SHAP&#65289;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#33391;&#20107;&#20214;&#65288;ADE&#65289;&#25552;&#21462;&#26159;&#25968;&#23383;&#33647;&#29289;&#30417;&#31649;&#20013;&#30340;&#26680;&#24515;&#20219;&#21153;&#20043;&#19968;&#65292;&#29305;&#21035;&#26159;&#24403;&#24212;&#29992;&#20110;&#38750;&#27491;&#24335;&#25991;&#26412;&#26102;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#20351;&#29992;&#20687;BERT&#36825;&#26679;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#23613;&#31649;&#22312;&#25991;&#29486;&#20013;&#20351;&#29992;&#20102;&#22823;&#37327;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#20294;&#23578;&#19981;&#28165;&#26970;&#23427;&#20204;&#21738;&#19968;&#20010;&#34920;&#29616;&#26356;&#22909;&#20197;&#21450;&#20026;&#20160;&#20040;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;19&#31181;&#22522;&#20110;Transformer&#30340;ADE&#25552;&#21462;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#21644;&#20998;&#26512;&#65292;&#24182;&#22312;&#36880;&#28176;&#22686;&#21152;&#38750;&#27491;&#24335;&#27700;&#24179;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#27604;&#36739;&#20102;&#25152;&#26377;&#32771;&#34385;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#65288;&#35770;&#22363;&#24086;&#23376;&#21644;&#25512;&#25991;&#65289;&#12290;&#25105;&#20204;&#36824;&#23558;&#32431;Transformer&#27169;&#22411;&#19982;&#20004;&#20010;&#24120;&#29992;&#30340;&#39069;&#22806;&#22788;&#29702;&#23618;&#65288;CRF&#21644;LSTM&#65289;&#30456;&#32467;&#21512;&#65292;&#24182;&#20998;&#26512;&#23427;&#20204;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#25104;&#29087;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#25216;&#26415;&#65288;SHAP&#65289;&#23558;&#27169;&#22411;&#24615;&#33021;&#19982;&#19968;&#32452;&#29305;&#24449;&#30456;&#20851;&#32852;&#65292;&#20197;&#36827;&#19968;&#27493;&#20998;&#26512;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adverse Event (ADE) extraction is one of the core tasks in digital pharmacovigilance, especially when applied to informal texts. This task has been addressed by the Natural Language Processing community using large pre-trained language models, such as BERT. Despite the great number of Transformer-based architectures used in the literature, it is unclear which of them has better performances and why. Therefore, in this paper we perform an extensive evaluation and analysis of 19 Transformer-based models for ADE extraction on informal texts. We compare the performance of all the considered models on two datasets with increasing levels of informality (forums posts and tweets). We also combine the purely Transformer-based models with two commonly-used additional processing layers (CRF and LSTM), and analyze their effect on the models performance. Furthermore, we use a well-established feature importance technique (SHAP) to correlate the performance of the models with a set of features that 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24635;&#32467;&#20102; BioNLP Workshop 2023 &#21457;&#36215;&#30340; ProbSum &#20849;&#20139;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#22522;&#32447;&#31995;&#32479;&#65292;&#26088;&#22312;&#21560;&#24341;&#26410;&#26469;&#30740;&#31350;&#21162;&#21147;&#26500;&#24314;&#36866;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#35786;&#26029;&#20915;&#31574;&#25903;&#25345;&#24212;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#24739;&#32773;&#30340;&#21307;&#30103;&#20445;&#20581;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.05270</link><description>&lt;p&gt;
&#12298;2023&#24180;ProbSum&#20849;&#20139;&#20219;&#21153;&#27010;&#35272;&#12299;&#65306;&#20174;&#30005;&#23376;&#30149;&#21382;&#36827;&#23637;&#35760;&#24405;&#20013;&#24635;&#32467;&#24739;&#32773;&#27963;&#36291;&#35786;&#26029;&#21644;&#38382;&#39064;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Overview of the Problem List Summarization (ProbSum) 2023 Shared Task on Summarizing Patients' Active Diagnoses and Problems from Electronic Health Record Progress Notes. (arXiv:2306.05270v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05270
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24635;&#32467;&#20102; BioNLP Workshop 2023 &#21457;&#36215;&#30340; ProbSum &#20849;&#20139;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#22522;&#32447;&#31995;&#32479;&#65292;&#26088;&#22312;&#21560;&#24341;&#26410;&#26469;&#30740;&#31350;&#21162;&#21147;&#26500;&#24314;&#36866;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#35786;&#26029;&#20915;&#31574;&#25903;&#25345;&#24212;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#24739;&#32773;&#30340;&#21307;&#30103;&#20445;&#20581;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BioNLP Workshop 2023&#20110;2023&#24180;1&#26376;&#21457;&#36215;&#20102;ProbSum&#20849;&#20139;&#20219;&#21153;&#65292;&#26088;&#22312;&#21560;&#24341;&#26410;&#26469;&#30740;&#31350;&#21162;&#21147;&#26500;&#24314;&#36866;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#35786;&#26029;&#20915;&#31574;&#25903;&#25345;&#24212;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#12290;&#21442;&#19982;&#32773;&#30340;&#30446;&#26631;&#26159;&#20351;&#29992;&#26469;&#33258;&#37325;&#30151;&#24739;&#32773;&#20303;&#38498;&#26399;&#38388;&#26085;&#24120;&#25252;&#29702;&#35760;&#24405;&#30340;&#36755;&#20837;&#24320;&#21457;&#29983;&#25104;&#35786;&#26029;&#21644;&#38382;&#39064;&#21015;&#34920;&#30340;&#27169;&#22411;&#12290;&#20843;&#20010;&#22242;&#38431;&#23558;&#20854;&#26368;&#32456;&#31995;&#32479;&#25552;&#20132;&#21040;&#20102;&#20849;&#20139;&#20219;&#21153;&#25490;&#34892;&#27036;&#19978;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#22522;&#32447;&#31995;&#32479;&#65292;&#24182;&#24635;&#32467;&#20102;&#21442;&#19982;&#22242;&#38431;&#23581;&#35797;&#30340;&#19981;&#21516;&#26041;&#27861;&#30340;&#25216;&#26415;&#21644;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The BioNLP Workshop 2023 initiated the launch of a shared task on Problem List Summarization (ProbSum) in January 2023. The aim of this shared task is to attract future research efforts in building NLP models for real-world diagnostic decision support applications, where a system generating relevant and accurate diagnoses will augment the healthcare providers decision-making process and improve the quality of care for patients. The goal for participants is to develop models that generated a list of diagnoses and problems using input from the daily care notes collected from the hospitalization of critically ill patients. Eight teams submitted their final systems to the shared task leaderboard. In this paper, we describe the tasks, datasets, evaluation metrics, and baseline systems. Additionally, the techniques and results of the evaluation of the different approaches tried by the participating teams are summarized.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;FactorCL&#65292;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#19981;&#20165;&#32771;&#34385;&#36328;&#27169;&#24577;&#20849;&#20139;&#20449;&#24687;&#65292;&#36824;&#33021;&#25429;&#25417;&#36328;&#27169;&#24577;&#21807;&#19968;&#30340;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.05268</link><description>&lt;p&gt;
&#20998;&#35299;&#23545;&#27604;&#23398;&#20064;&#65306;&#36229;&#36234;&#22810;&#35270;&#35282;&#20887;&#20313;
&lt;/p&gt;
&lt;p&gt;
Factorized Contrastive Learning: Going Beyond Multi-view Redundancy. (arXiv:2306.05268v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;FactorCL&#65292;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#19981;&#20165;&#32771;&#34385;&#36328;&#27169;&#24577;&#20849;&#20139;&#20449;&#24687;&#65292;&#36824;&#33021;&#25429;&#25417;&#36328;&#27169;&#24577;&#21807;&#19968;&#30340;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#27867;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#65292;&#23545;&#27604;&#23398;&#20064;&#24050;&#25104;&#20026;&#19968;&#31181;&#29305;&#21035;&#21560;&#24341;&#20154;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25104;&#21151;&#22320;&#23398;&#20064;&#20855;&#26377;&#20016;&#23500;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#34920;&#31034;&#65292;&#21482;&#38656;&#37197;&#23545;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#22270;&#20687;&#26631;&#39064;&#25110;&#35270;&#39057;&#38899;&#39057;&#23545;&#65289;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#22522;&#30784;&#26159;&#22810;&#35270;&#35282;&#20887;&#20313;&#30340;&#20551;&#35774;&#8212;&#8212;&#36328;&#27169;&#24577;&#38388;&#20849;&#20139;&#20449;&#24687;&#23545;&#20110;&#19979;&#28216;&#20219;&#21153;&#26159;&#24517;&#35201;&#19988;&#36275;&#22815;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#20063;&#21253;&#21547;&#22312;&#36328;&#27169;&#24577;&#21807;&#19968;&#21306;&#22495;&#20013;&#65306;&#19968;&#31181;&#20165;&#23384;&#22312;&#20110;&#19968;&#20010;&#27169;&#24577;&#20013;&#20294;&#19982;&#20219;&#21153;&#20173;&#28982;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#22914;&#20309;&#23398;&#20064;&#33258;&#25105;&#30417;&#30563;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#20197;&#25429;&#33719;&#19982;&#19979;&#28216;&#20219;&#21153;&#30456;&#20851;&#30340;&#20849;&#20139;&#21644;&#21807;&#19968;&#20449;&#24687;&#65311;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;FactorCL&#65292;&#20197;&#36229;&#36234;&#22810;&#35270;&#35282;&#20887;&#20313;&#12290;FactorCL&#30340;&#22522;&#30784;&#26159;&#19977;&#20010;&#26032;&#30340;&#36129;&#29486;&#65306;&#65288;1&#65289;&#23558;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#20998;&#35299;&#20026;&#20849;&#20139;&#21644;&#21807;&#19968;&#34920;&#31034;&#65292;&#65288;2&#65289;&#38480;&#21046;&#20849;&#20139;&#21644;&#21807;&#19968;&#25104;&#20998;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#65288;3&#65289;&#20351;&#29992;&#22240;&#23376;&#27491;&#21017;&#21270;&#20419;&#36827;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a wide range of multimodal tasks, contrastive learning has become a particularly appealing approach since it can successfully learn representations from abundant unlabeled data with only pairing information (e.g., image-caption or video-audio pairs). Underpinning these approaches is the assumption of multi-view redundancy - that shared information between modalities is necessary and sufficient for downstream tasks. However, in many real-world settings, task-relevant information is also contained in modality-unique regions: information that is only present in one modality but still relevant to the task. How can we learn self-supervised multimodal representations to capture both shared and unique information relevant to downstream tasks? This paper proposes FactorCL, a new multimodal representation learning method to go beyond multi-view redundancy. FactorCL is built from three new contributions: (1) factorizing task-relevant information into shared and unique representations, (2) cap
&lt;/p&gt;</description></item><item><title>&#35821;&#20041;&#26410;&#30830;&#23450;&#24615;&#22312;&#35821;&#35328;&#23398;&#20013;&#24456;&#37325;&#35201;&#65292;&#20294;&#22810;&#27169;&#24577;&#31995;&#32479;&#36824;&#27809;&#26377;&#35299;&#20915;&#22909;&#36825;&#20010;&#38382;&#39064;</title><link>http://arxiv.org/abs/2306.05240</link><description>&lt;p&gt;
&#22788;&#29702;&#22810;&#27169;&#24577;NLP&#20013;&#30340;&#35821;&#20041;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Dealing with Semantic Underspecification in Multimodal NLP. (arXiv:2306.05240v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05240
&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#26410;&#30830;&#23450;&#24615;&#22312;&#35821;&#35328;&#23398;&#20013;&#24456;&#37325;&#35201;&#65292;&#20294;&#22810;&#27169;&#24577;&#31995;&#32479;&#36824;&#27809;&#26377;&#35299;&#20915;&#22909;&#36825;&#20010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26088;&#22312;&#20687;&#20154;&#31867;&#19968;&#26679;&#25484;&#25569;&#35821;&#35328;&#30340;&#26234;&#33021;&#31995;&#32479;&#24517;&#39035;&#22788;&#29702;&#20854;&#35821;&#20041;&#26410;&#30830;&#23450;&#24615;&#65292;&#21363;&#35821;&#35328;&#20449;&#21495;&#21487;&#33021;&#20165;&#20256;&#36798;&#36890;&#20449;&#25152;&#38656;&#20449;&#24687;&#30340;&#19968;&#37096;&#20998;&#12290;&#26631;&#20934;&#30340;NLP&#27169;&#22411;&#21407;&#21017;&#19978;&#27809;&#26377;&#25110;&#20165;&#26377;&#26377;&#38480;&#30340;&#35775;&#38382;&#39069;&#22806;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#32780;&#23558;&#35821;&#35328;&#22522;&#20110;&#20854;&#20182;&#27169;&#24577;&#65288;&#20363;&#22914;&#35270;&#35273;&#65289;&#36827;&#34892;&#35828;&#26126;&#30340;&#22810;&#27169;&#24577;&#31995;&#32479;&#33258;&#28982;&#37197;&#22791;&#20197;&#35299;&#20915;&#36825;&#31181;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#20182;&#20204;&#38590;&#20197;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#25104;&#20026;&#19968;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent systems that aim at mastering language as humans do must deal with its semantic underspecification, namely, the possibility for a linguistic signal to convey only part of the information needed for communication to succeed. Consider the usages of the pronoun they, which can leave the gender and number of its referent(s) underspecified. Semantic underspecification is not a bug but a crucial language feature that boosts its storage and processing efficiency. Indeed, human speakers can quickly and effortlessly integrate semantically-underspecified linguistic signals with a wide range of non-linguistic information, e.g., the multimodal context, social or cultural conventions, and shared knowledge. Standard NLP models have, in principle, no or limited access to such extra information, while multimodal systems grounding language into other modalities, such as vision, are naturally equipped to account for this phenomenon. However, we show that they struggle with it, which could ne
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21463;&#38480;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25552;&#39640;&#38271;&#31687;&#25991;&#26412;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.05183</link><description>&lt;p&gt;
&#25552;&#39640;&#38271;&#31687;&#25991;&#26412;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Improving Long Context Document-Level Machine Translation. (arXiv:2306.05183v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05183
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21463;&#38480;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25552;&#39640;&#38271;&#31687;&#25991;&#26412;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#26469;&#35828;&#65292;&#25991;&#26412;&#32423;&#21035;&#30340;&#19978;&#19979;&#25991;&#23545;&#20110;&#25552;&#39640;&#32763;&#35793;&#30340;&#19968;&#33268;&#24615;&#12289;&#20957;&#32858;&#24615;&#12289;&#27169;&#26865;&#20004;&#21487;&#36755;&#20837;&#30340;&#32763;&#35793;&#20197;&#21450;&#20854;&#20182;&#35821;&#35328;&#29616;&#35937;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#35768;&#22810;&#20851;&#20110;&#25991;&#26723;&#32423;&#21035; NMT &#30340;&#30456;&#20851;&#35770;&#25991;&#20986;&#29256;&#65292;&#20294;&#22823;&#22810;&#25968;&#23558;&#31995;&#32479;&#38480;&#21046;&#22312;&#26412;&#22320;&#19978;&#19979;&#25991;&#65292;&#36890;&#24120;&#21482;&#21253;&#25324;&#21069;&#19968;&#20004;&#20010;&#21477;&#23376;&#20316;&#20026;&#26356;&#22810;&#20449;&#24687;&#12290;&#36825;&#21487;&#33021;&#36275;&#20197;&#35299;&#20915;&#19968;&#20123;&#26326;&#26151;&#24615;&#36755;&#20837;&#65292;&#20294;&#21487;&#33021;&#19981;&#36275;&#20197;&#25429;&#25417;&#25991;&#26723;&#32423;&#21035;&#20449;&#24687;&#65292;&#20363;&#22914;&#35805;&#39064;&#25110;&#23545;&#35805;&#39118;&#26684;&#12290;&#24403;&#23558;&#19978;&#19979;&#25991;&#22823;&#23567;&#22686;&#21152;&#21040;&#26412;&#22320;&#19978;&#19979;&#25991;&#20043;&#22806;&#26102;&#65292;&#20250;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65306;&#65288;i&#65289;&#20869;&#23384;&#20351;&#29992;&#23558;&#21576;&#25351;&#25968;&#22686;&#38271;&#65288;ii&#65289;&#32763;&#35793;&#24615;&#33021;&#24320;&#22987;&#38477;&#20302;&#12290;&#25105;&#20204;&#35748;&#20026;&#24191;&#27867;&#20351;&#29992;&#30340;&#27880;&#24847;&#26426;&#21046;&#26159;&#36825;&#20004;&#20010;&#38382;&#39064;&#30340;&#21407;&#22240;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#38480;&#30340;&#27880;&#24847;&#21147;&#21464;&#20307;&#65292;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#24207;&#21015;&#30340;&#26368;&#30456;&#20851;&#37096;&#20998;&#65292;&#21516;&#26102;&#25511;&#21046;&#23545;&#40784;&#26435;&#37325;&#30340;&#24635;&#21644;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level context for neural machine translation (NMT) is crucial to improve the translation consistency and cohesion, the translation of ambiguous inputs, as well as several other linguistic phenomena. Many works have been published on the topic of document-level NMT, but most restrict the system to only local context, typically including just the one or two preceding sentences as additional information. This might be enough to resolve some ambiguous inputs, but it is probably not sufficient to capture some document-level information like the topic or style of a conversation. When increasing the context size beyond just the local context, there are two challenges: (i) the~memory usage increases exponentially (ii) the translation performance starts to degrade. We argue that the widely-used attention mechanism is responsible for both issues. Therefore, we propose a constrained attention variant that focuses the attention on the most relevant parts of the sequence, while simultaneou
&lt;/p&gt;</description></item><item><title>M3Exam&#26159;&#19968;&#20010;&#26469;&#28304;&#20110;&#30495;&#23454;&#20154;&#31867;&#32771;&#35797;&#39064;&#30446;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#21644;&#22810;&#23618;&#27425;&#30340;&#24773;&#22659;&#20013;&#30340;&#26222;&#36866;&#26234;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05179</link><description>&lt;p&gt;
M3Exam: &#19968;&#31181;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#12289;&#22810;&#23618;&#27425;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models. (arXiv:2306.05179v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05179
&lt;/p&gt;
&lt;p&gt;
M3Exam&#26159;&#19968;&#20010;&#26469;&#28304;&#20110;&#30495;&#23454;&#20154;&#31867;&#32771;&#35797;&#39064;&#30446;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#21644;&#22810;&#23618;&#27425;&#30340;&#24773;&#22659;&#20013;&#30340;&#26222;&#36866;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23384;&#22312;&#30528;&#21508;&#31181;&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#32771;&#35797;&#26356;&#36866;&#21512;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26222;&#36866;&#26234;&#33021;&#65292;&#22240;&#20026;&#23427;&#20204;&#22218;&#25324;&#20102;&#26356;&#24191;&#27867;&#30340;&#33021;&#21147;&#38656;&#27714;&#65292;&#20363;&#22914;&#35821;&#35328;&#29702;&#35299;&#12289;&#39046;&#22495;&#30693;&#35782;&#21644;&#35299;&#20915;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; M3Exam&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#30495;&#23454;&#21644;&#23448;&#26041;&#20154;&#31867;&#32771;&#35797;&#39064;&#30446;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#22312;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#21644;&#22810;&#23618;&#27425;&#30340;&#24773;&#22659;&#20013;&#35780;&#20272; LLM&#12290;M3Exam &#20855;&#26377;&#19977;&#20010;&#29420;&#29305;&#29305;&#28857;:&#65288;1&#65289;&#22810;&#35821;&#35328;&#24615;&#65292;&#28085;&#30422;&#22810;&#20010;&#22269;&#23478;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#21644;&#25991;&#21270;&#30693;&#35782;&#65307;&#65288;2&#65289;&#22810;&#27169;&#24577;&#65292;&#32771;&#34385;&#21040;&#35768;&#22810;&#32771;&#35797;&#38382;&#39064;&#30340;&#22810;&#27169;&#24577;&#24615;&#36136;&#65292;&#20197;&#27979;&#35797;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;&#65307;&#65288;3&#65289;&#22810;&#23618;&#27425;&#32467;&#26500;&#65292;&#29305;&#21035;&#28085;&#30422;&#20102;&#19977;&#20010;&#20851;&#38190;&#25945;&#32946;&#38454;&#27573;&#30340;&#32771;&#35797;&#65292;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#22312;&#19981;&#21516;&#25945;&#32946;&#27700;&#24179;&#19978;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the existence of various benchmarks for evaluating natural language processing models, we argue that human exams are a more suitable means of evaluating general intelligence for large language models (LLMs), as they inherently demand a much wider range of abilities such as language understanding, domain knowledge, and problem-solving skills. To this end, we introduce M3Exam, a novel benchmark sourced from real and official human exam questions for evaluating LLMs in a multilingual, multimodal, and multilevel context. M3Exam exhibits three unique characteristics: (1) multilingualism, encompassing questions from multiple countries that require strong multilingual proficiency and cultural knowledge; (2) multimodality, accounting for the multimodal nature of many exam questions to test the model's multimodal understanding capability; and (3) multilevel structure, featuring exams from three critical educational periods to comprehensively assess a model's proficiency at different lev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;RRWKV&#26550;&#26500;&#65292;&#23427;&#22312;&#20445;&#25345;&#35760;&#24518;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#21152;&#20837;&#22238;&#39038;&#33021;&#21147;&#26377;&#25928;&#22320;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.05176</link><description>&lt;p&gt;
RRWKV&#65306;&#22312;RWKV&#20013;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
RRWKV: Capturing Long-range Dependencies in RWKV. (arXiv:2306.05176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;RRWKV&#26550;&#26500;&#65292;&#23427;&#22312;&#20445;&#25345;&#35760;&#24518;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#21152;&#20837;&#22238;&#39038;&#33021;&#21147;&#26377;&#25928;&#22320;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;Transformer&#24778;&#20154;&#30340;&#28857;&#31215;&#27880;&#24847;&#21147;&#65292;&#23427;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#20027;&#35201;&#26550;&#26500;&#12290;&#26368;&#36817;&#65292;Receptance Weighted Key Value&#65288;RWKV&#65289;&#26550;&#26500;&#36981;&#24490;&#38750;Transformer&#26550;&#26500;&#65292;&#28040;&#38500;&#20102;&#28857;&#31215;&#27880;&#24847;&#21147;&#30340;&#32570;&#28857;&#65292;&#20854;&#20013;&#23384;&#20648;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#21576;&#20108;&#27425;&#25193;&#23637;&#12290;&#23613;&#31649;RWKV&#21033;&#29992;&#20102;&#32447;&#24615;&#24352;&#37327;&#31215;&#27880;&#24847;&#26426;&#21046;&#24182;&#36890;&#36807;&#37096;&#32626;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#23454;&#29616;&#20102;&#24182;&#34892;&#35745;&#31639;&#65292;&#20294;&#19982;&#26631;&#20934;Transformer&#20013;&#30452;&#25509;&#20132;&#20114;&#33719;&#24471;&#30340;&#23436;&#25972;&#20449;&#24687;&#30456;&#27604;&#65292;&#23427;&#26080;&#27861;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#22240;&#20026;&#20854;&#21463;&#38480;&#20110;&#21521;&#21518;&#26597;&#30475;&#20808;&#21069;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#36890;&#36807;&#23558;&#22238;&#39038;&#33021;&#21147;&#32435;&#20837;RWKV&#20013;&#26469;&#35774;&#35745;Retrospected Receptance Weighted Key Value&#65288;RRWKV&#65289;&#26550;&#26500;&#65292;&#20197;&#26377;&#25928;&#22320;&#21560;&#25910;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#35760;&#24518;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Owing to the impressive dot-product attention, the Transformers have been the dominant architectures in various natural language processing (NLP) tasks. Recently, the Receptance Weighted Key Value (RWKV) architecture follows a non-transformer architecture to eliminate the drawbacks of dot-product attention, where memory and computational complexity exhibits quadratic scaling with sequence length. Although RWKV has exploited a linearly tensor-product attention mechanism and achieved parallelized computations by deploying the time-sequential mode, it fails to capture long-range dependencies because of its limitation on looking back at previous information, compared with full information obtained by direct interactions in the standard transformer. Therefore, the paper devises the Retrospected Receptance Weighted Key Value (RRWKV) architecture via incorporating the retrospecting ability into the RWKV to effectively absorb information, which maintains memory and computational efficiency as 
&lt;/p&gt;</description></item><item><title>&#22823;&#33041;&#21644;&#35821;&#35328;&#27169;&#22411;&#28608;&#27963;&#21576;&#29616;&#30456;&#20284;&#20043;&#22788;&#65292;&#20026;&#30456;&#20851;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#20294;&#30446;&#21069;&#20173;&#32570;&#20047;&#26126;&#30830;&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2306.05126</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23545;&#22823;&#33041;&#30340;&#26144;&#23556;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Mapping Brains with Language Models: A Survey. (arXiv:2306.05126v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05126
&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#21644;&#35821;&#35328;&#27169;&#22411;&#28608;&#27963;&#21576;&#29616;&#30456;&#20284;&#20043;&#22788;&#65292;&#20026;&#30456;&#20851;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#20294;&#30446;&#21069;&#20173;&#32570;&#20047;&#26126;&#30830;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#35768;&#22810;&#30740;&#31350;&#32773;&#20284;&#20046;&#37117;&#20570;&#20986;&#20102;&#21516;&#26679;&#30340;&#35266;&#23519;&#65306;&#22823;&#33041;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#28608;&#27963;&#21576;&#29616;&#19968;&#20123;&#32467;&#26500;&#19978;&#30340;&#30456;&#20284;&#20043;&#22788;&#65292;&#20351;&#24471;&#20174;&#31070;&#32463;&#35760;&#24405;&#21644;&#35745;&#31639;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#20043;&#38388;&#23384;&#22312;&#32447;&#24615;&#20559;&#24046;&#26144;&#23556;&#12290;&#25105;&#20204;&#23581;&#35797;&#35780;&#20272;&#36825;&#19968;&#35266;&#23519;&#24050;&#32463;&#31215;&#32047;&#20102;&#22810;&#23569;&#35777;&#25454;&#65292;&#35843;&#26597;&#20102;&#36229;&#36807;30&#39033;&#28085;&#30422;10&#20010;&#25968;&#25454;&#38598;&#21644;8&#20010;&#25351;&#26631;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#35780;&#20272;&#26041;&#27861;&#20013;&#19968;&#20123;&#25351;&#26631;&#24182;&#19981;&#20445;&#23432;&#12290;&#30446;&#21069;&#65292;&#23613;&#31649;&#31215;&#32047;&#20102;&#22823;&#37327;&#30340;&#35777;&#25454;&#65292;&#20294;&#32467;&#35770;&#20173;&#19981;&#30830;&#23450;&#12290;&#28982;&#32780;&#65292;&#19982;&#27169;&#22411;&#22823;&#23567;&#21644;&#36136;&#37327;&#30340;&#30456;&#20851;&#24615;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#35880;&#24910;&#20048;&#35266;&#30340;&#29702;&#30001;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the years, many researchers have seemingly made the same observation: Brain and language model activations exhibit some structural similarities, enabling linear partial mappings between features extracted from neural recordings and computational language models. In an attempt to evaluate how much evidence has been accumulated for this observation, we survey over 30 studies spanning 10 datasets and 8 metrics. How much evidence has been accumulated, and what, if anything, is missing before we can draw conclusions? Our analysis of the evaluation methods used in the literature reveals that some of the metrics are less conservative. We also find that the accumulated evidence, for now, remains ambiguous, but correlations with model size and quality provide grounds for cautious optimism.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21442;&#32771;&#26657;&#27491;&#30340;&#32454;&#31890;&#24230;FEC&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#22312;&#20351;&#29992;&#35813;&#26694;&#26550;&#36827;&#34892;&#23454;&#39564;&#30340;&#36807;&#31243;&#20013;&#21457;&#29616;&#20102;&#19981;&#21516;&#20107;&#23454;&#35823;&#24046;&#31867;&#21035;&#19979;&#30340;&#26368;&#20339;&#35757;&#32451;&#27169;&#24335;&#21644;&#29616;&#26377;&#26041;&#27861;&#20043;&#38388;&#30340;&#26174;&#33879;&#24615;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.05119</link><description>&lt;p&gt;
&#21442;&#32771;&#25991;&#29486;&#33267;&#19978;&#65306;&#22522;&#20110;&#32454;&#31890;&#24230;&#35780;&#20272;&#26694;&#26550;&#30340;&#23545;&#35805;&#25688;&#35201;&#20107;&#23454;&#35823;&#24046;&#26657;&#27491;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework. (arXiv:2306.05119v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05119
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21442;&#32771;&#26657;&#27491;&#30340;&#32454;&#31890;&#24230;FEC&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#22312;&#20351;&#29992;&#35813;&#26694;&#26550;&#36827;&#34892;&#23454;&#39564;&#30340;&#36807;&#31243;&#20013;&#21457;&#29616;&#20102;&#19981;&#21516;&#20107;&#23454;&#35823;&#24046;&#31867;&#21035;&#19979;&#30340;&#26368;&#20339;&#35757;&#32451;&#27169;&#24335;&#21644;&#29616;&#26377;&#26041;&#27861;&#20043;&#38388;&#30340;&#26174;&#33879;&#24615;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#35805;&#25688;&#35201;&#20013;&#65292;&#20107;&#23454;&#24615;&#24456;&#37325;&#35201;&#12290;&#23545;&#20110;&#27169;&#22411;&#29983;&#25104;&#30340;&#25688;&#35201;&#36827;&#34892;&#20107;&#23454;&#35823;&#24046;&#26657;&#27491;&#65288;FEC&#65289;&#26159;&#25552;&#39640;&#20107;&#23454;&#24615;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;&#24403;&#21069;&#20381;&#36182;&#20110;&#20107;&#23454;&#24615;&#25351;&#26631;&#30340;FEC&#35780;&#20272;&#19981;&#22815;&#21487;&#38752;&#21644;&#35814;&#32454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#27425;&#25163;&#21160;&#27880;&#37322;&#20102;&#19968;&#20010;&#21253;&#21547;4000&#20010;&#39033;&#30446;&#30340;&#23545;&#35805;&#25688;&#35201;FEC&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#21442;&#32771;&#26657;&#27491;&#30340;&#32454;&#31890;&#24230;&#35780;&#20272;&#26694;&#26550;FERRANTI&#65292;&#35813;&#26694;&#26550;&#33258;&#21160;&#35780;&#20272;FEC&#27169;&#22411;&#22312;&#19981;&#21516;&#38169;&#35823;&#31867;&#21035;&#19978;&#30340;&#34920;&#29616;&#12290;&#20351;&#29992;&#36825;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#36827;&#34892;&#20102;&#36275;&#22815;&#30340;FEC&#26041;&#27861;&#23454;&#39564;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#20107;&#23454;&#35823;&#24046;&#31867;&#21035;&#19978;&#25214;&#21040;&#20102;&#26368;&#20339;&#30340;&#35757;&#32451;&#27169;&#24335;&#21644;&#29616;&#26377;&#26041;&#27861;&#30340;&#26174;&#33879;&#24615;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Factuality is important to dialogue summarization. Factual error correction (FEC) of model-generated summaries is one way to improve factuality. Current FEC evaluation that relies on factuality metrics is not reliable and detailed enough. To address this problem, we are the first to manually annotate a FEC dataset for dialogue summarization containing 4000 items and propose FERRANTI, a fine-grained evaluation framework based on reference correction that automatically evaluates the performance of FEC models on different error categories. Using this evaluation framework, we conduct sufficient experiments with FEC approaches under a variety of settings and find the best training modes and significant differences in the performance of the existing approaches on different factual error categories.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#26368;&#22909;&#22320;&#21033;&#29992;&#25991;&#26723;&#32423;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;&#27604;&#36739;&#20102;&#25991;&#29486;&#20013;&#30340;&#19981;&#21516;&#35299;&#30721;&#26041;&#26696;&#21644;&#20316;&#32773;&#25552;&#20986;&#30340;&#26041;&#26696;&#65292;&#24182;&#21457;&#29616;&#38024;&#23545;&#26576;&#20123;&#35821;&#35328;&#29616;&#35937;&#26102;&#65292;&#24120;&#29992;&#30340;&#35299;&#30721;&#31574;&#30053;&#19981;&#33021;&#22815;&#21462;&#24471;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05116</link><description>&lt;p&gt;
&#20851;&#20110;&#25991;&#26723;&#32423;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#25628;&#32034;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
On Search Strategies for Document-Level Neural Machine Translation. (arXiv:2306.05116v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#26368;&#22909;&#22320;&#21033;&#29992;&#25991;&#26723;&#32423;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;&#27604;&#36739;&#20102;&#25991;&#29486;&#20013;&#30340;&#19981;&#21516;&#35299;&#30721;&#26041;&#26696;&#21644;&#20316;&#32773;&#25552;&#20986;&#30340;&#26041;&#26696;&#65292;&#24182;&#21457;&#29616;&#38024;&#23545;&#26576;&#20123;&#35821;&#35328;&#29616;&#35937;&#26102;&#65292;&#24120;&#29992;&#30340;&#35299;&#30721;&#31574;&#30053;&#19981;&#33021;&#22815;&#21462;&#24471;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#21477;&#23376;&#32423;&#31995;&#32479;&#30456;&#27604;&#65292;&#25991;&#26723;&#32423;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#33021;&#22815;&#22312;&#19968;&#20221;&#25991;&#20214;&#20013;&#20135;&#29983;&#26356;&#19968;&#33268;&#30340;&#36755;&#20986;&#65292;&#24182;&#19988;&#33021;&#22815;&#26356;&#22909;&#22320;&#35299;&#20915;&#36755;&#20837;&#20013;&#30340;&#27495;&#20041;&#12290;&#22312;&#25991;&#26723;&#32423;NMT&#19978;&#24050;&#32463;&#26377;&#35768;&#22810;&#30740;&#31350;&#65292;&#23588;&#20854;&#26159;&#30528;&#37325;&#20110;&#20462;&#25913;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#31574;&#30053;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#22810;&#25968;&#30740;&#31350;&#20013;&#65292;&#22914;&#20309;&#36890;&#36807;&#24050;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#25191;&#34892;&#25628;&#32034;&#30340;&#38382;&#39064;&#24456;&#23569;&#34987;&#35752;&#35770;&#65292;&#26377;&#26102;&#29978;&#33267;&#26681;&#26412;&#19981;&#34987;&#25552;&#21450;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;&#22914;&#20309;&#22312;&#35299;&#30721;&#20013;&#26368;&#22909;&#22320;&#21033;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#32763;&#35793;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20174;&#26368;&#27969;&#34892;&#30340;&#25991;&#26723;&#32423;NMT&#26041;&#27861;&#24320;&#22987;&#65292;&#27604;&#36739;&#20102;&#25991;&#29486;&#20013;&#30340;&#19981;&#21516;&#35299;&#30721;&#26041;&#26696;&#21644;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#26696;&#12290;&#27604;&#36739;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26631;&#20934;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#20197;&#21450;&#38024;&#23545;&#19977;&#20010;&#26631;&#20934;&#25991;&#26723;&#32423;&#32763;&#35793;&#22522;&#20934;&#27979;&#35797;&#30340;&#29305;&#23450;&#35821;&#35328;&#29616;&#35937;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22823;&#22810;&#25968;&#24120;&#29992;&#30340;&#35299;&#30721;&#31574;&#30053;&#22312;&#38024;&#23545;&#26576;&#20123;&#35821;&#35328;&#29616;&#35937;&#26102;&#24182;&#19981;&#33021;&#21462;&#24471;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compared to sentence-level systems, document-level neural machine translation (NMT) models produce a more consistent output across a document and are able to better resolve ambiguities within the input. There are many works on document-level NMT, mostly focusing on modifying the model architecture or training strategy to better accommodate the additional context-input. On the other hand, in most works, the question on how to perform search with the trained model is scarcely discussed, sometimes not mentioned at all. In this work, we aim to answer the question how to best utilize a context-aware translation model in decoding. We start with the most popular document-level NMT approach and compare different decoding schemes, some from the literature and others proposed by us. In the comparison, we are using both, standard automatic metrics, as well as specific linguistic phenomena on three standard document-level translation benchmarks. We find that most commonly used decoding strategies 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ChatGPT&#29983;&#25104;&#27169;&#22411;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#20154;&#24037;&#26631;&#27880;&#30340;&#36190;&#21161;&#20869;&#23481;&#26816;&#27979;&#65292;&#20197;&#25552;&#39640;&#26631;&#27880;&#19968;&#33268;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#22312;Instagram&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.05115</link><description>&lt;p&gt;
&#20851;&#38381;&#24490;&#29615;&#65306;&#20351;&#29992;ChatGPT&#27979;&#35797;&#29983;&#25104;&#27169;&#22411;&#35299;&#37322;&#20197;&#25913;&#21892;&#31038;&#20132;&#23186;&#20307;&#19978;&#36190;&#21161;&#20869;&#23481;&#30340;&#20154;&#24037;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Closing the Loop: Testing ChatGPT to Generate Model Explanations to Improve Human Labelling of Sponsored Content on Social Media. (arXiv:2306.05115v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05115
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ChatGPT&#29983;&#25104;&#27169;&#22411;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#20154;&#24037;&#26631;&#27880;&#30340;&#36190;&#21161;&#20869;&#23481;&#26816;&#27979;&#65292;&#20197;&#25552;&#39640;&#26631;&#27880;&#19968;&#33268;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#22312;Instagram&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#30417;&#31649;&#26426;&#26500;&#36890;&#36807;&#12298;&#19981;&#20844;&#24179;&#21830;&#19994;&#34892;&#20026;&#25351;&#20196;&#12299;(UCPD)&#31561;&#24037;&#20855;&#26469;&#21152;&#24378;&#30830;&#20445;&#36879;&#26126;&#24230;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#24433;&#21709;&#32773;&#33829;&#38144;&#65292;&#25110;&#32773;&#26681;&#25454;&#12298;&#32852;&#37030;&#36152;&#26131;&#22996;&#21592;&#20250;&#27861;&#26696;&#12299;&#31532;5&#26465;&#36827;&#34892;&#25191;&#34892;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24433;&#21709;&#21147;&#24066;&#22330;&#30340;&#35268;&#27169;&#24222;&#22823;&#65292;&#24378;&#21046;&#23454;&#26045;&#36825;&#20123;&#20041;&#21153;&#19968;&#30452;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;&#26816;&#27979;&#36190;&#21161;&#20869;&#23481;&#30340;&#20219;&#21153;&#26088;&#22312;&#23454;&#29616;&#23545;&#27492;&#31867;&#35268;&#23450;&#30340;&#30417;&#25511;&#21644;&#25191;&#34892;&#12290;&#24403;&#21069;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#20027;&#35201;&#23558;&#38382;&#39064;&#26694;&#26550;&#20026;&#19968;&#39033;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#30528;&#37325;&#20110;&#24320;&#21457;&#33021;&#22815;&#22312;&#26816;&#27979;&#24191;&#21578;&#26102;&#36798;&#21040;&#39640;&#20998;&#31867;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;&#36825;&#20123;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20381;&#36182;&#20110;&#20154;&#31867;&#25968;&#25454;&#27880;&#37322;&#26469;&#25552;&#20379;&#22522;&#30784;&#30495;&#23454;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#27880;&#37322;&#32773;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#36890;&#24120;&#24456;&#20302;&#65292;&#23548;&#33268;&#26631;&#31614;&#19981;&#19968;&#33268;&#65292;&#20174;&#32780;&#24433;&#21709;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;ChatGPT&#29983;&#25104;&#27169;&#22411;&#39044;&#27979;&#30340;&#35299;&#37322;&#65292;&#20197;&#25552;&#39640;&#20154;&#24037;&#26631;&#27880;&#32773;&#30340;&#20934;&#30830;&#24230;&#21644;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#36190;&#21161;&#20869;&#23481;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regulatory bodies worldwide are intensifying their efforts to ensure transparency in influencer marketing on social media through instruments like the Unfair Commercial Practices Directive (UCPD) in the European Union, or Section 5 of the Federal Trade Commission Act. Yet enforcing these obligations has proven to be highly problematic due to the sheer scale of the influencer market. The task of automatically detecting sponsored content aims to enable the monitoring and enforcement of such regulations at scale. Current research in this field primarily frames this problem as a machine learning task, focusing on developing models that achieve high classification performance in detecting ads. These machine learning tasks rely on human data annotation to provide ground truth information. However, agreement between annotators is often low, leading to inconsistent labels that hinder the reliability of models. To improve annotation accuracy and, thus, the detection of sponsored content, we pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23402;&#29983;RNN&#32467;&#26500;&#29992;&#20110;&#27979;&#37327;L2-L2&#20132;&#20114;&#20013;&#35821;&#38899;&#38899;&#36136;&#25910;&#25947;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#35821;&#38899;&#25910;&#25947;&#21644;&#35828;&#35805;&#32773;&#30340;&#27169;&#20223;&#33021;&#21147;&#30340;&#21160;&#24577;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#30001;L1&#24341;&#36215;&#30340;&#35828;&#35805;&#32773;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.05088</link><description>&lt;p&gt;
&#23545;&#35805;&#30340;&#33402;&#26415;&#65306;&#20351;&#29992;&#23402;&#29983;RNN&#27979;&#37327;L2&#35821;&#38899;&#20013;&#30340;&#35821;&#38899;&#25910;&#25947;&#21644;&#25925;&#24847;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
The ART of Conversation: Measuring Phonetic Convergence and Deliberate Imitation in L2-Speech with a Siamese RNN. (arXiv:2306.05088v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23402;&#29983;RNN&#32467;&#26500;&#29992;&#20110;&#27979;&#37327;L2-L2&#20132;&#20114;&#20013;&#35821;&#38899;&#38899;&#36136;&#25910;&#25947;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#35821;&#38899;&#25910;&#25947;&#21644;&#35828;&#35805;&#32773;&#30340;&#27169;&#20223;&#33021;&#21147;&#30340;&#21160;&#24577;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#30001;L1&#24341;&#36215;&#30340;&#35828;&#35805;&#32773;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23402;&#29983;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#32467;&#26500;&#65292;&#29992;&#20110;&#27979;&#37327;L2-L2&#20132;&#20114;&#20013;&#35821;&#38899;&#38899;&#36136;&#30340;&#25910;&#25947;&#12290;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;20&#21517;&#27597;&#35821;&#20026;&#26031;&#27931;&#20240;&#20811;&#35821;&#30340;&#33521;&#35821;&#23398;&#20064;&#32773;&#26469;&#25193;&#23637;&#20132;&#26367;&#38405;&#35835;&#20219;&#21153;&#65288;ART&#65289;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35757;&#32451;&#21644;&#27979;&#35797;&#20102;&#35813;&#27169;&#22411;&#65292;&#20174;&#19977;&#20010;&#19981;&#21516;&#30340;&#27597;&#35821;&#32452;&#65292;&#21363;&#24847;&#22823;&#21033;&#35821;&#65288;9&#23545;&#65289;&#65292;&#27861;&#35821;&#65288;10&#23545;&#65289;&#21644;&#26031;&#27931;&#20240;&#20811;&#35821;&#65288;10&#23545;&#65289;&#20013;&#27979;&#37327;L2&#33521;&#35821;&#35821;&#38899;&#30340;&#35821;&#38899;&#25910;&#25947;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23402;&#29983;RNN&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#35821;&#38899;&#25910;&#25947;&#21644;&#35828;&#35805;&#32773;&#30340;&#27169;&#20223;&#33021;&#21147;&#30340;&#21160;&#24577;&#12290;&#27492;&#22806;&#65292;&#36825;&#26159;&#19968;&#20010;&#25991;&#26412;&#26080;&#20851;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#25193;&#23637;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#30001;L1&#24341;&#36215;&#30340;&#35828;&#35805;&#32773;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phonetic convergence describes the automatic and unconscious speech adaptation of two interlocutors in a conversation. This paper proposes a Siamese recurrent neural network (RNN) architecture to measure the convergence of the holistic spectral characteristics of speech sounds in an L2-L2 interaction. We extend an alternating reading task (the ART) dataset by adding 20 native Slovak L2 English speakers. We train and test the Siamese RNN model to measure phonetic convergence of L2 English speech from three different native language groups: Italian (9 dyads), French (10 dyads) and Slovak (10 dyads). Our results indicate that the Siamese RNN model effectively captures the dynamics of phonetic convergence and the speaker's imitation ability. Moreover, this text-independent model is scalable and capable of handling L1-induced speaker variability.
&lt;/p&gt;</description></item><item><title>PandaLM&#26159;&#19968;&#20010;&#35780;&#20272;LLM&#25351;&#20196;&#35843;&#20248;&#30340;&#33258;&#21160;&#22522;&#20934;&#65292;&#23427;&#33021;&#22815;&#21306;&#20998;&#26368;&#20248;&#27169;&#22411;&#65292;&#24182;&#20851;&#27880;&#20110;&#20027;&#35266;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2306.05087</link><description>&lt;p&gt;
PandaLM&#65306;LLM&#25351;&#20196;&#35843;&#20248;&#20248;&#21270;&#30340;&#33258;&#21160;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization. (arXiv:2306.05087v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05087
&lt;/p&gt;
&lt;p&gt;
PandaLM&#26159;&#19968;&#20010;&#35780;&#20272;LLM&#25351;&#20196;&#35843;&#20248;&#30340;&#33258;&#21160;&#22522;&#20934;&#65292;&#23427;&#33021;&#22815;&#21306;&#20998;&#26368;&#20248;&#27169;&#22411;&#65292;&#24182;&#20851;&#27880;&#20110;&#20027;&#35266;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#22797;&#26434;&#24615;&#21644;&#35780;&#20272;&#35843;&#25972;&#27169;&#22411;&#30340;&#22256;&#38590;&#24615;&#65292;LLM&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#25351;&#20196;&#35843;&#20248;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#30830;&#23450;&#26368;&#20339;&#36229;&#21442;&#25968;&#65292;&#38656;&#35201;&#19968;&#20010;&#33258;&#21160;&#30340;&#12289;&#24378;&#22823;&#19988;&#21487;&#38752;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35780;&#20272;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#25361;&#25112;&#65292;&#24314;&#31435;&#36825;&#26679;&#19968;&#20010;&#22522;&#20934;&#24182;&#19981;&#26159;&#19968;&#39033;&#31616;&#21333;&#30340;&#20219;&#21153;&#12290;&#20026;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#27454;&#21517;&#20026;PandaLM&#30340;&#35780;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#65292;&#33021;&#22815;&#21306;&#20998;&#20986;&#22810;&#20010;LLM&#20013;&#26368;&#20339;&#30340;&#27169;&#22411;&#12290;PandaLM&#30340;&#20851;&#27880;&#28857;&#19981;&#20165;&#38480;&#20110;&#20256;&#32479;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#23458;&#35266;&#27491;&#30830;&#24615;&#65292;&#36824;&#28085;&#30422;&#20102;&#35832;&#22914;&#30456;&#23545;&#31616;&#27905;&#24615;&#12289;&#28165;&#26224;&#24230;&#12289;&#36981;&#24490;&#35828;&#26126;&#12289;&#20840;&#38754;&#24615;&#21644;&#24418;&#24335;&#24615;&#31561;&#37325;&#35201;&#20027;&#35266;&#22240;&#32032;&#12290;&#20026;&#30830;&#20445;PandaLM&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#20154;&#24037;&#27880;&#37322;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#25152;&#26377;&#19978;&#19979;&#25991;&#37117;&#26159;&#29983;&#25104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning large language models (LLMs) remains a challenging task, owing to the complexity of hyperparameter selection and the difficulty involved in evaluating the tuned models. To determine the optimal hyperparameters, an automatic, robust, and reliable evaluation benchmark is essential. However, establishing such a benchmark is not a trivial task due to the challenges associated with evaluation accuracy and privacy protection. In response to these challenges, we introduce a judge large language model, named PandaLM, which is trained to distinguish the superior model given several LLMs. PandaLM's focus extends beyond just the objective correctness of responses, which is the main focus of traditional evaluation datasets. It addresses vital subjective factors such as relative conciseness, clarity, adherence to instructions, comprehensiveness, and formality. To ensure the reliability of PandaLM, we collect a diverse human-annotated test dataset, where all contexts are generated
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20195;&#30721;&#25551;&#36848;&#20013;&#24341;&#20837;&#25200;&#21160;&#26469;&#22686;&#24378;AI&#25915;&#20987;&#24615;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#21487;&#26377;&#25928;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#22120;&#23545;&#25200;&#21160;&#21644;&#38750;&#25200;&#21160;&#30340;&#20195;&#30721;&#25551;&#36848;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05079</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#25552;&#21319;AI&#25915;&#20987;&#24615;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Robustness of AI Offensive Code Generators via Data Augmentation. (arXiv:2306.05079v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20195;&#30721;&#25551;&#36848;&#20013;&#24341;&#20837;&#25200;&#21160;&#26469;&#22686;&#24378;AI&#25915;&#20987;&#24615;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#21487;&#26377;&#25928;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#22120;&#23545;&#25200;&#21160;&#21644;&#38750;&#25200;&#21160;&#30340;&#20195;&#30721;&#25551;&#36848;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25200;&#21160;&#28155;&#21152;&#21040;&#23433;&#20840;&#24615;&#20195;&#30721;&#19978;&#19979;&#25991;&#20013;&#30340;&#20195;&#30721;&#25551;&#36848;&#20013;&#30340;&#26041;&#27861;&#65292;&#21363;&#26469;&#33258;&#21892;&#24847;&#24320;&#21457;&#32773;&#30340;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#65288;NL&#65289;&#65292;&#24182;&#20998;&#26512;&#20102;&#25200;&#21160;&#22914;&#20309;&#20197;&#21450;&#22312;&#20160;&#20040;&#31243;&#24230;&#19978;&#24433;&#21709;AI&#25915;&#20987;&#24615;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;NL&#25551;&#36848;&#20013;&#30340;&#25200;&#21160;&#39640;&#24230;&#24433;&#21709;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#35813;&#26041;&#27861;&#25191;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#21363;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21464;&#24322;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#24182;&#35777;&#26126;&#20854;&#23545;&#25200;&#21160;&#21644;&#38750;&#25200;&#21160;&#30340;&#20195;&#30721;&#25551;&#36848;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a method to add perturbations to the code descriptions, i.e., new inputs in natural language (NL) from well-intentioned developers, in the context of security-oriented code, and analyze how and to what extent perturbations affect the performance of AI offensive code generators. Our experiments show that the performance of the code generators is highly affected by perturbations in the NL descriptions. To enhance the robustness of the code generators, we use the method to perform data augmentation, i.e., to increase the variability and diversity of the training data, proving its effectiveness against both perturbed and non-perturbed code descriptions.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;DLAMA-v1&#65292;&#29992;&#20110;&#31579;&#36873;&#26469;&#33258;&#19981;&#21516;&#25991;&#21270;&#32972;&#26223;&#30340;&#20107;&#23454;&#19977;&#20803;&#32452;&#65292;&#35299;&#20915;&#22312;&#22810;&#35821;&#35328;&#27169;&#22411;&#19978;&#20351;&#29992;&#33521;&#35821;&#25552;&#31034;&#26356;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.05076</link><description>&lt;p&gt;
DLAMA: &#19968;&#20010;&#29992;&#20110;&#20026;&#25506;&#31350;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#31579;&#36873;&#22810;&#20803;&#25991;&#21270;&#20107;&#23454;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models. (arXiv:2306.05076v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05076
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;DLAMA-v1&#65292;&#29992;&#20110;&#31579;&#36873;&#26469;&#33258;&#19981;&#21516;&#25991;&#21270;&#32972;&#26223;&#30340;&#20107;&#23454;&#19977;&#20803;&#32452;&#65292;&#35299;&#20915;&#22312;&#22810;&#35821;&#35328;&#27169;&#22411;&#19978;&#20351;&#29992;&#33521;&#35821;&#25552;&#31034;&#26356;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#21457;&#24067;&#20102;&#19968;&#20123;&#22522;&#20934;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20107;&#23454;&#30693;&#35782;&#12290;&#36825;&#20123;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;LAMA&#21644;ParaRel&#65289;&#20027;&#35201;&#26159;&#29992;&#33521;&#35821;&#24320;&#21457;&#30340;&#65292;&#28982;&#21518;&#34987;&#32763;&#35793;&#25104;&#26032;&#30340;&#22810;&#35821;&#35328;&#29256;&#26412;&#65288;&#20363;&#22914;mLAMA&#21644;mParaRel&#65289;&#12290;&#23545;&#36825;&#20123;&#22810;&#35821;&#35328;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#33521;&#35821;&#25552;&#31034;&#20174;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#22238;&#24518;&#20107;&#23454;&#36890;&#24120;&#27604;&#20351;&#29992;&#38750;&#33521;&#35821;&#25552;&#31034;&#20135;&#29983;&#26356;&#22909;&#19988;&#26356;&#19968;&#33268;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;mLAMA&#20559;&#21521;&#20110;&#26469;&#33258;&#35199;&#26041;&#22269;&#23478;&#30340;&#20107;&#23454;&#65292;&#36825;&#21487;&#33021;&#24433;&#21709;&#25506;&#27979;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31579;&#36873;&#26469;&#33258;Wikidata&#30340;&#20855;&#26377;&#25991;&#21270;&#22810;&#26679;&#24615;&#30340;&#20107;&#23454;&#19977;&#20803;&#32452;&#12290;&#30001;&#19977;&#23545;&#23545;&#27604;&#25991;&#21270;&#30340;&#23454;&#38469;&#19977;&#20803;&#32452;&#32452;&#25104;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;DLAMA-v1&#65292;&#20849;&#26377;20&#20010;&#20851;&#31995;&#35859;&#35789;&#30340;78259&#20010;&#19977;&#20803;&#32452;&#12290;&#36825;&#19977;&#23545;&#21253;&#25324;&#20195;&#34920;&#65288;&#38463;&#25289;&#20271;&#21644;&#35199;&#26041;&#65289;&#12289;&#65288;&#20122;&#27954;&#21644;&#35199;&#26041;&#65289;&#20197;&#21450;&#65288;&#21335;&#32654;&#27954;&#21644;&#35199;&#26041;&#65289;&#30340;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
A few benchmarking datasets have been released to evaluate the factual knowledge of pretrained language models. These benchmarks (e.g., LAMA, and ParaRel) are mainly developed in English and later are translated to form new multilingual versions (e.g., mLAMA, and mParaRel). Results on these multilingual benchmarks suggest that using English prompts to recall the facts from multilingual models usually yields significantly better and more consistent performance than using non-English prompts. Our analysis shows that mLAMA is biased toward facts from Western countries, which might affect the fairness of probing models. We propose a new framework for curating factual triples from Wikidata that are culturally diverse. A new benchmark DLAMA-v1 is built of factual triples from three pairs of contrasting cultures having a total of 78,259 triples from 20 relation predicates. The three pairs comprise facts representing the (Arab and Western), (Asian and Western), and (South American and Western)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#39044;&#35757;&#32451;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26816;&#27979;&#21644;&#20998;&#31867;&#31995;&#32479;&#65292;&#29992;&#20110;&#35299;&#20915;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#24615;&#21035;&#27495;&#35270;&#38382;&#39064;&#12290;&#22810;&#20219;&#21153;&#23398;&#20064;&#23545;&#24615;&#21035;&#27495;&#35270;&#26816;&#27979;&#21644;&#31895;&#31890;&#24230;&#24615;&#21035;&#20998;&#31867;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#24494;&#35843;&#21017;&#26356;&#36866;&#29992;&#20110;&#32454;&#31890;&#24230;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2306.05075</link><description>&lt;p&gt;
LCT-1&#22312;SemEval-2023&#20219;&#21153;10&#20013;&#30340;&#24212;&#29992;&#65306;&#39044;&#35757;&#32451;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#29992;&#20110;&#24615;&#21035;&#27495;&#35270;&#26816;&#27979;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
LCT-1 at SemEval-2023 Task 10: Pre-training and Multi-task Learning for Sexism Detection and Classification. (arXiv:2306.05075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#39044;&#35757;&#32451;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26816;&#27979;&#21644;&#20998;&#31867;&#31995;&#32479;&#65292;&#29992;&#20110;&#35299;&#20915;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#24615;&#21035;&#27495;&#35270;&#38382;&#39064;&#12290;&#22810;&#20219;&#21153;&#23398;&#20064;&#23545;&#24615;&#21035;&#27495;&#35270;&#26816;&#27979;&#21644;&#31895;&#31890;&#24230;&#24615;&#21035;&#20998;&#31867;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#24494;&#35843;&#21017;&#26356;&#36866;&#29992;&#20110;&#32454;&#31890;&#24230;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24615;&#21035;&#27495;&#35270;&#21644;&#24615;&#21035;&#20027;&#20041;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#36234;&#26469;&#36234;&#25104;&#20026;&#19968;&#20010;&#38382;&#39064;&#12290;&#34429;&#28982;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#31995;&#32479;&#36890;&#24120;&#26159;&#19981;&#21487;&#35299;&#37322;&#30340;&#12290;SemEval-2023&#20219;&#21153;10&#26088;&#22312;&#22686;&#21152;&#20851;&#20110;&#24615;&#21035;&#27495;&#35270;&#26816;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#30340;&#22242;&#38431;&#21442;&#21152;&#20102;&#25152;&#26377;&#24314;&#35758;&#30340;&#23376;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22522;&#20110;&#36827;&#19968;&#27493;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#65288;Gururangan&#31561;&#20154;&#65292;2020&#65289;&#12290;&#22312;&#20855;&#26377;&#39046;&#22495;&#33258;&#36866;&#24212;&#24615;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#24494;&#35843;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#24182;&#34920;&#26126;&#27599;&#20010;&#23376;&#20219;&#21153;&#38656;&#35201;&#19981;&#21516;&#30340;&#31995;&#32479;&#37197;&#32622;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#22312;&#24615;&#21035;&#27495;&#35270;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#26497;&#20339;&#65292;&#24182;&#22312;&#31895;&#31890;&#24230;&#30340;&#24615;&#21035;&#20998;&#31867;&#26041;&#38754;&#20248;&#20110;&#26631;&#20934;&#24494;&#35843;&#65292;&#32780;&#24494;&#35843;&#21017;&#26356;&#36866;&#29992;&#20110;&#32454;&#31890;&#24230;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Misogyny and sexism are growing problems in social media. Advances have been made in online sexism detection but the systems are often uninterpretable. SemEval-2023 Task 10 on Explainable Detection of Online Sexism aims at increasing explainability of the sexism detection, and our team participated in all the proposed subtasks. Our system is based on further domain-adaptive pre-training (Gururangan et al., 2020). Building on the Transformer-based models with the domain adaptation, we compare fine-tuning with multi-task learning and show that each subtask requires a different system configuration. In our experiments, multi-task learning performs on par with standard fine-tuning for sexism detection and noticeably better for coarse-grained sexism classification, while fine-tuning is preferable for fine-grained classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;K2&#65292;&#24182;&#24320;&#21457;&#20102;&#21508;&#31181;&#36164;&#28304;&#20197;&#36827;&#19968;&#27493;&#20419;&#36827;&#20854;&#22312;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#65292;&#21253;&#25324;&#31532;&#19968;&#20010;&#22320;&#29699;&#31185;&#23398;&#25945;&#23398;&#35843;&#38899;&#25968;&#25454;&#38598;GeoSignal&#21644;&#31532;&#19968;&#20010;&#22320;&#29699;&#31185;&#23398;&#22522;&#20934;&#27979;&#35797;GeoBenchmark&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#23436;&#25972;&#30340;&#26041;&#27861;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#36890;&#29992;&#39046;&#22495;LLM LLaMA-7B &#27169;&#22411;&#36866;&#24212;&#21040;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2306.05064</link><description>&lt;p&gt;
&#23398;&#20064;&#22320;&#29699;&#31185;&#23398;&#30693;&#35782;&#29702;&#35299;&#21644;&#21033;&#29992;&#30340;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning A Foundation Language Model for Geoscience Knowledge Understanding and Utilization. (arXiv:2306.05064v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;K2&#65292;&#24182;&#24320;&#21457;&#20102;&#21508;&#31181;&#36164;&#28304;&#20197;&#36827;&#19968;&#27493;&#20419;&#36827;&#20854;&#22312;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#65292;&#21253;&#25324;&#31532;&#19968;&#20010;&#22320;&#29699;&#31185;&#23398;&#25945;&#23398;&#35843;&#38899;&#25968;&#25454;&#38598;GeoSignal&#21644;&#31532;&#19968;&#20010;&#22320;&#29699;&#31185;&#23398;&#22522;&#20934;&#27979;&#35797;GeoBenchmark&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#23436;&#25972;&#30340;&#26041;&#27861;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#36890;&#29992;&#39046;&#22495;LLM LLaMA-7B &#27169;&#22411;&#36866;&#24212;&#21040;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#24120;&#35268;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#26412;&#25991;&#23558;LLM&#24341;&#20837;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#65292;&#26088;&#22312;&#25512;&#36827;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#30340;LLM&#65292;&#21629;&#21517;&#20026;K2&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#36164;&#28304;&#65292;&#20197;&#36827;&#19968;&#27493;&#20419;&#36827;LLM&#22312;&#22320;&#29699;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#20026;LLM&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#22320;&#29699;&#31185;&#23398;&#25945;&#23398;&#35843;&#38899;&#25968;&#25454;&#38598;GeoSignal&#65292;&#26088;&#22312;&#23558;LLM&#30456;&#24212;&#19982;&#22320;&#29699;&#31185;&#23398;&#30456;&#20851;&#30340;&#29992;&#25143;&#26597;&#35810;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#31532;&#19968;&#20010;&#22320;&#36136;&#31185;&#23398;&#22522;&#20934;&#27979;&#35797;GeoBenchmark&#65292;&#20197;&#22312;&#22320;&#29699;&#31185;&#23398;&#29615;&#22659;&#20013;&#35780;&#20272;LLM&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#23436;&#25972;&#30340;&#26041;&#27861;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#36890;&#29992;&#39046;&#22495;LLM&#36866;&#24212;&#21040;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#36229;&#36807;100&#19975;&#31687;&#22320;&#29699;&#31185;&#23398;&#25991;&#29486;&#19978;&#36827;&#19968;&#27493;&#35757;&#32451;&#20102;LLaMA-7B&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;GeoSignal&#30340;&#30417;&#30563;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#20139;&#20102;&#19968;&#20010;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#36801;&#31227;LLM&#30340;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs)have achieved great success in general domains of natural language processing. In this paper, we bring LLMs to the realm of geoscience, with the objective of advancing research and applications in this field. To this end, we present the first-ever LLM in geoscience, K2, alongside a suite of resources developed to further promote LLM research within geoscience. For instance, we have curated the first geoscience instruction tuning dataset, GeoSignal, which aims to align LLM responses to geoscience-related user queries. Additionally, we have established the first geoscience benchmark, GeoBenchmark, to evaluate LLMs in the context of geoscience. In this work, we experiment with a complete recipe to adapt a pretrained general-domain LLM to the geoscience domain. Specifically, we further train the LLaMA-7B model on over 1 million pieces of geoscience literature and utilize GeoSignal's supervised data to fine-tune the model. Moreover, we share a protocol that can e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32763;&#35793;-&#27979;&#35797;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#20197;&#29992;&#20110;&#36328;&#35821;&#35328;&#25991;&#26412;&#20998;&#31867;&#65292;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22120;&#30340;&#24110;&#21161;&#19979;&#23558;&#30446;&#26631;&#35821;&#35328;&#32763;&#35793;&#25104;&#39640;&#36164;&#28304;&#35821;&#35328;&#26469;&#35757;&#32451;&#25991;&#26412;&#20998;&#31867;&#22120;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#22522;&#32447;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04996</link><description>&lt;p&gt;
T3L: &#32763;&#35793;-&#27979;&#35797;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;&#36328;&#35821;&#35328;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
T3L: Translate-and-Test Transfer Learning for Cross-Lingual Text Classification. (arXiv:2306.04996v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32763;&#35793;-&#27979;&#35797;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#20197;&#29992;&#20110;&#36328;&#35821;&#35328;&#25991;&#26412;&#20998;&#31867;&#65292;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22120;&#30340;&#24110;&#21161;&#19979;&#23558;&#30446;&#26631;&#35821;&#35328;&#32763;&#35793;&#25104;&#39640;&#36164;&#28304;&#35821;&#35328;&#26469;&#35757;&#32451;&#25991;&#26412;&#20998;&#31867;&#22120;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#22522;&#32447;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#35328;&#25991;&#26412;&#20998;&#31867;&#21033;&#29992;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#35757;&#32451;&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#26469;&#25191;&#34892;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#30340;&#25991;&#26412;&#20998;&#31867;&#65292;&#32780;&#26080;&#38656;&#25110;&#21482;&#38656;&#35201;&#24494;&#35843;&#65288;&#38646;/&#23569;&#37327;&#20851;&#38190;&#35789;&#36328;&#35821;&#35328;&#36716;&#31227;&#65289;&#12290;&#22914;&#20170;&#65292;&#36328;&#35821;&#35328;&#25991;&#26412;&#20998;&#31867;&#22120;&#36890;&#24120;&#22522;&#20110;&#39044;&#20808;&#22312;&#22810;&#31181;&#24863;&#20852;&#36259;&#30340;&#35821;&#35328;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#26469;&#26500;&#24314;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#21644;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#65292;&#36825;&#34920;&#26126;&#35821;&#35328;&#24314;&#27169;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#21472;&#21152;&#24182;&#19981;&#24635;&#26159;&#26377;&#25928;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#35758;&#37325;&#26032;&#23457;&#35270;&#32463;&#20856;&#30340;&#8220;&#32763;&#35793;-&#27979;&#35797;&#8221;&#27969;&#31243;&#65292;&#20197;&#28165;&#26224;&#22320;&#20998;&#31163;&#32763;&#35793;&#21644;&#20998;&#31867;&#38454;&#27573;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;1&#65289;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22120;&#23558;&#30446;&#26631;&#35821;&#35328;&#32763;&#35793;&#25104;&#39640;&#36164;&#28304;&#35821;&#35328;&#65292;&#21644;2&#65289;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#35757;&#32451;&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#30456;&#32467;&#21512;&#65292;&#20294;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22120;&#29983;&#25104;&#8220;&#36719;&#8221;&#32763;&#35793;&#20197;&#20445;&#30041;&#21407;&#22987;&#30446;&#26631;&#35821;&#35328;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#21313;&#31181;&#35821;&#35328;&#30340;&#22810;&#26679;&#21270;&#36328;&#35821;&#35328;&#25991;&#26412;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#38646;&#20851;&#38190;&#35789;&#35774;&#32622;&#19979;&#30340;&#22823;&#37096;&#20998;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#39046;&#20808;&#30340;&#22522;&#32447;&#26694;&#26550;&#65292;&#24182;&#19988;&#22312;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#19978;&#65292;&#22312;&#30417;&#30563;&#21644;&#23569;&#37327;&#20851;&#38190;&#35789;&#35774;&#32622;&#19979;&#20063;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual text classification leverages text classifiers trained in a high-resource language to perform text classification in other languages with no or minimal fine-tuning (zero/few-shots cross-lingual transfer). Nowadays, cross-lingual text classifiers are typically built on large-scale, multilingual language models (LMs) pretrained on a variety of languages of interest. However, the performance of these models vary significantly across languages and classification tasks, suggesting that the superposition of the language modelling and classification tasks is not always effective. For this reason, in this paper we propose revisiting the classic "translate-and-test" pipeline to neatly separate the translation and classification stages. The proposed approach couples 1) a neural machine translator translating from the targeted language to a high-resource language, with 2) a text classifier trained in the high-resource language, but the neural machine translator generates "soft" tran
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#38750;&#33521;&#35821;&#27597;&#35821;&#20154;&#22763;&#35821;&#38899;&#20013;&#30701;&#35821;&#26029;&#28857;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22823;&#22823;&#38477;&#20302;&#23545;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#65292;&#24182;&#19988;&#24615;&#33021;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.04980</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#38750;&#33521;&#35821;&#27597;&#35821;&#20154;&#22763;&#30340;&#30701;&#35821;&#26029;&#28857;
&lt;/p&gt;
&lt;p&gt;
Assessing Phrase Break of ESL Speech with Pre-trained Language Models and Large Language Models. (arXiv:2306.04980v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#38750;&#33521;&#35821;&#27597;&#35821;&#20154;&#22763;&#35821;&#38899;&#20013;&#30701;&#35821;&#26029;&#28857;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22823;&#22823;&#38477;&#20302;&#23545;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#65292;&#24182;&#19988;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35780;&#20272;&#38750;&#33521;&#35821;&#27597;&#35821;&#23398;&#20064;&#32773;&#35821;&#38899;&#20013;&#30701;&#35821;&#26029;&#28857;&#30340;&#26041;&#27861;&#12290;&#35813;&#30740;&#31350;&#21253;&#21547;&#20004;&#20010;&#20219;&#21153;&#65306;&#35780;&#20272;&#19968;&#20010;&#35821;&#38899;&#29255;&#27573;&#30340;&#24635;&#20307;&#30701;&#35821;&#26029;&#28857;&#21644;&#23545;&#27599;&#20010;&#21487;&#33021;&#30340;&#30701;&#35821;&#26029;&#28857;&#20301;&#32622;&#36827;&#34892;&#32454;&#31890;&#24230;&#35780;&#20272;&#12290;&#20026;&#21033;&#29992;NLP&#27169;&#22411;&#65292;&#35821;&#38899;&#36755;&#20837;&#39318;&#20808;&#34987;&#24378;&#21046;&#23545;&#40784;&#21040;&#25991;&#26412;&#65292;&#28982;&#21518;&#34987;&#39044;&#22788;&#29702;&#25104;&#19968;&#20010;&#21253;&#21547;&#21333;&#35789;&#21644;&#30701;&#35821;&#26029;&#28857;&#20449;&#24687;&#30340;&#26631;&#35760;&#24207;&#21015;&#12290;&#20026;&#20102;&#21033;&#29992;PLMs&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22788;&#29702;&#36807;&#30340;&#26631;&#35760;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#27969;&#31243;&#12290;&#36825;&#20010;&#36807;&#31243;&#21253;&#25324;&#29992;&#19968;&#20010;&#26367;&#25442;&#30772;&#25240;&#21495;&#26816;&#27979;&#27169;&#22359;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#29992;&#25991;&#26412;&#20998;&#31867;&#21644;&#24207;&#21015;&#26631;&#27880;&#36827;&#34892;&#24494;&#35843;&#12290;&#20026;&#20102;&#21033;&#29992;LLMs&#65292;&#25105;&#20204;&#32473;ChatGPT&#35774;&#35745;&#20102;&#25552;&#31034;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;PLMs&#22823;&#22823;&#38477;&#20302;&#20102;&#23545;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#65292;&#24182;&#19988;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;ChatGPT&#22312;&#36825;&#19968;&#39046;&#22495;&#36827;&#19968;&#27493;&#25552;&#21319;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces approaches to assessing phrase breaks in ESL learners' speech using pre-trained language models (PLMs) and large language models (LLMs). There are two tasks: overall assessment of phrase break for a speech clip and fine-grained assessment of every possible phrase break position. To leverage NLP models, speech input is first force-aligned with texts, and then pre-processed into a token sequence, including words and phrase break information. To utilize PLMs, we propose a pre-training and fine-tuning pipeline with the processed tokens. This process includes pre-training with a replaced break token detection module and fine-tuning with text classification and sequence labeling. To employ LLMs, we design prompts for ChatGPT. The experiments show that with the PLMs, the dependence on labeled training data has been greatly reduced, and the performance has improved. Meanwhile, we verify that ChatGPT, a renowned LLM, has potential for further advancement in this area.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20027;&#21160;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#65292;&#29992;&#20110;&#24320;&#25918;&#24335;&#20851;&#31995;&#25277;&#21462;&#65292;&#36890;&#36807;&#20132;&#26367;&#32858;&#31867;&#23398;&#20064;&#21644;&#20851;&#31995;&#26631;&#27880;&#65292;&#26377;&#25928;&#25552;&#39640;&#32858;&#31867;&#20934;&#30830;&#24615;&#21644;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04968</link><description>&lt;p&gt;
&#20027;&#21160;&#30417;&#30563;&#32858;&#31867;&#29992;&#20110;&#24320;&#25918;&#24335;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Actively Supervised Clustering for Open Relation Extraction. (arXiv:2306.04968v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20027;&#21160;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#65292;&#29992;&#20110;&#24320;&#25918;&#24335;&#20851;&#31995;&#25277;&#21462;&#65292;&#36890;&#36807;&#20132;&#26367;&#32858;&#31867;&#23398;&#20064;&#21644;&#20851;&#31995;&#26631;&#27880;&#65292;&#26377;&#25928;&#25552;&#39640;&#32858;&#31867;&#20934;&#30830;&#24615;&#21644;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#22522;&#20110;&#32858;&#31867;&#30340;&#24320;&#25918;&#24335;&#20851;&#31995;&#25277;&#21462;(OpenRE)&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#20004;&#38454;&#27573;&#27969;&#31243;&#12290;&#31532;&#19968;&#38454;&#27573;&#21516;&#26102;&#23398;&#20064;&#20851;&#31995;&#34920;&#31034;&#21644;&#20998;&#37197;&#12290;&#31532;&#20108;&#38454;&#27573;&#25163;&#21160;&#26631;&#35760;&#19968;&#20123;&#23454;&#20363;&#65292;&#24182;&#20026;&#27599;&#20010;&#32858;&#31867;&#21629;&#21517;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#26080;&#30417;&#30563;&#30446;&#26631;&#38590;&#20197;&#20248;&#21270;&#27169;&#22411;&#20197;&#33719;&#24471;&#20934;&#30830;&#30340;&#32858;&#31867;&#20998;&#37197;&#65292;&#24182;&#19988;&#24517;&#39035;&#25552;&#21069;&#30830;&#23450;&#32676;&#38598;&#25968;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#32622;&#65292;&#31216;&#20026;&#29992;&#20110;OpenRE&#30340;&#20027;&#21160;&#30417;&#30563;&#32858;&#31867;&#12290;&#25105;&#20204;&#30340;&#35265;&#35299;&#22312;&#20110;&#32858;&#31867;&#23398;&#20064;&#21644;&#20851;&#31995;&#26631;&#27880;&#21487;&#20197;&#20132;&#26367;&#36827;&#34892;&#65292;&#20026;&#32858;&#31867;&#25552;&#20379;&#24517;&#35201;&#30340;&#25351;&#23548;&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#22686;&#21152;&#20154;&#21147;&#36164;&#28304;&#12290;&#35774;&#32622;&#30340;&#20851;&#38190;&#22312;&#20110;&#36873;&#25321;&#35201;&#26631;&#35760;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#21160;&#24577;&#21457;&#29616;&#26410;&#30693;&#20851;&#31995;&#30340;&#32858;&#31867;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32858;&#31867;&#20934;&#30830;&#24615;&#21644;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current clustering-based Open Relation Extraction (OpenRE) methods usually adopt a two-stage pipeline. The first stage simultaneously learns relation representations and assignments. The second stage manually labels several instances and thus names the relation for each cluster. However, unsupervised objectives struggle to optimize the model to derive accurate clustering assignments, and the number of clusters has to be supplied in advance. In this paper, we present a novel setting, named actively supervised clustering for OpenRE. Our insight lies in that clustering learning and relation labeling can be alternately performed, providing the necessary guidance for clustering without a significant increase in human effort. The key to the setting is selecting which instances to label. Instead of using classical active labeling strategies designed for fixed known classes, we propose a new strategy, which is applicable to dynamically discover clusters of unknown relations. Experimental resul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#20998;&#31867;&#30340;&#27969;&#31243;&#65292;&#21253;&#25324;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#35789;&#32423;&#35821;&#35328;&#35782;&#21035;&#12289;&#35821;&#35328;&#22686;&#24378;&#21644;&#27169;&#22411;&#35757;&#32451;&#31561;&#27493;&#39588;&#65292;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#24773;&#24863;&#20998;&#26512;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#35789;&#32423;&#20132;&#38169;&#21644;&#21477;&#23376;&#21518;&#32622;&#30340;&#35821;&#35328;&#20449;&#24687;&#25554;&#20837;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#20195;&#30721;&#28151;&#21512;&#21360;&#22320;&#35821;-&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04964</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#35782;&#21035;&#25216;&#26415;&#25552;&#21319;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Leveraging Language Identification to Enhance Code-Mixed Text Classification. (arXiv:2306.04964v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#20998;&#31867;&#30340;&#27969;&#31243;&#65292;&#21253;&#25324;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#35789;&#32423;&#35821;&#35328;&#35782;&#21035;&#12289;&#35821;&#35328;&#22686;&#24378;&#21644;&#27169;&#22411;&#35757;&#32451;&#31561;&#27493;&#39588;&#65292;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#24773;&#24863;&#20998;&#26512;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#35789;&#32423;&#20132;&#38169;&#21644;&#21477;&#23376;&#21518;&#32622;&#30340;&#35821;&#35328;&#20449;&#24687;&#25554;&#20837;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#20195;&#30721;&#28151;&#21512;&#21360;&#22320;&#35821;-&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21516;&#19968;&#27573;&#25991;&#26412;&#20013;&#20351;&#29992;&#22810;&#31181;&#35821;&#35328;&#21483;&#20570;&#20195;&#30721;&#28151;&#21512;&#12290;&#24403;&#21069;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#65292;&#29305;&#21035;&#26159;&#33521;&#35821;&#21644;&#22320;&#26041;&#35821;&#35328;&#28151;&#21512;&#20351;&#29992;&#30340;&#25968;&#25454;&#36234;&#26469;&#36234;&#22810;&#12290;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#20013;&#30340;&#38544;&#24615;&#35821;&#35328;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23581;&#35797;&#19981;&#21516;&#30340;&#35821;&#35328;&#22686;&#24378;&#26041;&#27861;&#65292;&#25552;&#39640;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#20195;&#30721;&#28151;&#21512;&#21360;&#22320;&#35821;-&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#20195;&#30721;&#28151;&#21512;&#31995;&#32479;&#30340;&#27969;&#31243;&#65292;&#21253;&#25324;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#35789;&#32423;&#35821;&#35328;&#35782;&#21035;&#12289;&#35821;&#35328;&#22686;&#24378;&#21644;&#27169;&#22411;&#35757;&#32451;&#31561;&#27493;&#39588;&#65292;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#24773;&#24863;&#20998;&#26512;&#12290;&#22312;BERT&#27169;&#22411;&#20013;&#36827;&#34892;&#35821;&#35328;&#22686;&#24378;&#26102;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#35789;&#32423;&#20132;&#38169;&#21644;&#21477;&#23376;&#21518;&#32622;&#30340;&#35821;&#35328;&#20449;&#24687;&#25554;&#20837;&#26041;&#27861;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#21407;&#22987;BERT&#27169;&#22411;&#21644;&#32463;&#36807;&#20195;&#30721;&#28151;&#21512;&#25913;&#36827;&#30340;HingBERT&#22312;&#21508;&#33258;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The usage of more than one language in the same text is referred to as Code Mixed. It is evident that there is a growing degree of adaption of the use of code-mixed data, especially English with a regional language, on social media platforms. Existing deep-learning models do not take advantage of the implicit language information in the code-mixed text. Our study aims to improve BERT-based models performance on low-resource Code-Mixed Hindi-English Datasets by experimenting with language augmentation approaches. We propose a pipeline to improve code-mixed systems that comprise data preprocessing, word-level language identification, language augmentation, and model training on downstream tasks like sentiment analysis. For language augmentation in BERT models, we explore word-level interleaving and post-sentence placement of language information. We have examined the performance of vanilla BERT-based models and their code-mixed HingBERT counterparts on respective benchmark datasets, comp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26410;&#30693;&#24863;&#30693;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#21512;&#25104;&#36127;&#23454;&#20363;&#26469;&#23545;&#27169;&#22411;&#36827;&#34892;&#35268;&#33539;&#21270;&#65292;&#22312;&#19981;&#25439;&#23475;&#24050;&#30693;&#20851;&#31995;&#26816;&#27979;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102; SOTA &#30340;&#26410;&#30693;&#20851;&#31995;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.04950</link><description>&lt;p&gt;
&#24320;&#25918;&#38598;&#20851;&#31995;&#25277;&#21462;&#8212;&#8212;&#22522;&#20110;&#26410;&#30693;&#24863;&#30693;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Open Set Relation Extraction via Unknown-Aware Training. (arXiv:2306.04950v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26410;&#30693;&#24863;&#30693;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#21512;&#25104;&#36127;&#23454;&#20363;&#26469;&#23545;&#27169;&#22411;&#36827;&#34892;&#35268;&#33539;&#21270;&#65292;&#22312;&#19981;&#25439;&#23475;&#24050;&#30693;&#20851;&#31995;&#26816;&#27979;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102; SOTA &#30340;&#26410;&#30693;&#20851;&#31995;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30417;&#30563;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#22312;&#38381;&#38598;&#22330;&#26223;&#19979;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#65292;&#21363;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#36807;&#31243;&#20013;&#20851;&#31995;&#20445;&#25345;&#19981;&#21464;&#12290;&#22312;&#26356;&#29616;&#23454;&#30340;&#24320;&#25918;&#38598;&#22330;&#26223;&#20013;&#65292;&#27979;&#35797;&#38598;&#20013;&#21487;&#33021;&#20986;&#29616;&#26410;&#30693;&#20851;&#31995;&#12290;&#30001;&#20110;&#32570;&#20047;&#26410;&#30693;&#20851;&#31995;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#19968;&#20010;&#34920;&#29616;&#33391;&#22909;&#30340;&#38381;&#38598;&#20851;&#31995;&#25277;&#21462;&#22120;&#20173;&#28982;&#20250;&#23558;&#20854;&#33258;&#20449;&#22320;&#38169;&#35823;&#24402;&#31867;&#20026;&#24050;&#30693;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26410;&#30693;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#21512;&#25104;&#36127;&#23454;&#20363;&#26469;&#23545;&#27169;&#22411;&#36827;&#34892;&#35268;&#33539;&#21270;&#12290;&#20026;&#20102;&#20419;&#36827;&#32039;&#20945;&#30340;&#20915;&#31574;&#36793;&#30028;&#65292;&#38656;&#35201;&#8220;&#22256;&#38590;&#8221;&#30340;&#36127;&#23454;&#20363;&#12290;&#21463;&#21040;&#25991;&#26412;&#25932;&#23545;&#25915;&#20987;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25913;&#36827;&#21407;&#22987;&#35757;&#32451;&#23454;&#20363;&#65292;&#33258;&#36866;&#24212;&#22320;&#24212;&#29992;&#23567;&#32780;&#20851;&#38190;&#30340;&#25200;&#21160;&#65292;&#20174;&#32780;&#21512;&#25104;&#26356;&#26377;&#21487;&#33021;&#34987;&#27169;&#22411;&#35823;&#20998;&#31867;&#20026;&#24050;&#30693;&#20851;&#31995;&#30340;&#36127;&#23454;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#25439;&#23475;&#24050;&#30693;&#20851;&#31995;&#26816;&#27979;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102; SOTA &#30340;&#26410;&#30693;&#20851;&#31995;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The existing supervised relation extraction methods have achieved impressive performance in a closed-set setting, where the relations during both training and testing remain the same. In a more realistic open-set setting, unknown relations may appear in the test set. Due to the lack of supervision signals from unknown relations, a well-performing closed-set relation extractor can still confidently misclassify them into known relations. In this paper, we propose an unknown-aware training method, regularizing the model by dynamically synthesizing negative instances. To facilitate a compact decision boundary, ``difficult'' negative instances are necessary. Inspired by text adversarial attacks, we adaptively apply small but critical perturbations to original training instances and thus synthesizing negative instances that are more likely to be mistaken by the model as known relations. Experimental results show that this method achieves SOTA unknown relation detection without compromising t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#29992;&#20110;&#26816;&#27979;&#35821;&#26009;&#24211;&#20013;&#30340;&#20027;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#29992;&#20110;&#35780;&#20272;&#20027;&#39064;&#30340;&#21487;&#35835;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04941</link><description>&lt;p&gt;
&#19968;&#31181;&#25913;&#33391;&#30340;&#35821;&#26009;&#20027;&#39064;&#26816;&#27979;&#27169;&#22411;&#21450;&#35780;&#20272;&#20027;&#39064;&#21487;&#35835;&#24615;&#30340;&#26032;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
A modified model for topic detection from a corpus and a new metric evaluating the understandability of topics. (arXiv:2306.04941v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#29992;&#20110;&#26816;&#27979;&#35821;&#26009;&#24211;&#20013;&#30340;&#20027;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#29992;&#20110;&#35780;&#20272;&#20027;&#39064;&#30340;&#21487;&#35835;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#33391;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#29992;&#20110;&#20174;&#35821;&#26009;&#24211;&#20013;&#26816;&#27979;&#20027;&#39064;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#29992;&#20110;&#35780;&#20272;&#25152;&#26816;&#27979;&#21040;&#30340;&#20027;&#39064;&#12290;&#26032;&#27169;&#22411;&#24314;&#31435;&#22312;&#23884;&#20837;&#24335;&#20027;&#39064;&#27169;&#22411;&#22522;&#30784;&#19978;&#65292;&#24182;&#21152;&#20837;&#20102;&#19968;&#20123;&#25913;&#36827;&#65292;&#22914;&#25991;&#26723;&#32858;&#31867;&#31561;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#38271;&#24230;&#30340;&#25991;&#26723;&#26102;&#34920;&#29616;&#33391;&#22909;&#12290;&#26032;&#30340;&#25351;&#26631;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#35745;&#31639;&#20027;&#39064;&#21487;&#35835;&#24615;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#25152;&#26816;&#27979;&#21040;&#20027;&#39064;&#30340;&#21487;&#29702;&#35299;&#31243;&#24230;&#30340;&#19981;&#21516;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a modified neural model for topic detection from a corpus and proposes a new metric to evaluate the detected topics. The new model builds upon the embedded topic model incorporating some modifications such as document clustering. Numerical experiments suggest that the new model performs favourably regardless of the document's length. The new metric, which can be computed more efficiently than widely-used metrics such as topic coherence, provides variable information regarding the understandability of the detected topics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#25552;&#31034;&#21644;&#20854;&#20182;&#27169;&#22411;&#21442;&#25968;&#20043;&#38388;&#20114;&#20449;&#24687;&#26469;&#20248;&#21270;&#36719;&#25552;&#31034;&#35843;&#25972;&#65292;&#20174;&#32780;&#24320;&#21457;&#20102;&#26356;&#21152;&#39640;&#25928;&#12289;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#36719;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;InfoPrompt&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20004;&#20010;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;&#65292;&#21457;&#29616;&#21512;&#36866;&#30340;&#25552;&#31034;&#21021;&#22987;&#21270;&#65292;&#24182;&#20174;&#25552;&#31034;&#20196;&#29260;&#20013;&#23398;&#20064;&#36275;&#22815;&#30340;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#65292;&#21516;&#26102;&#40723;&#21169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#34920;&#31034;&#26356;&#21152;&#20851;&#27880;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.04933</link><description>&lt;p&gt;
InfoPrompt&#65306;&#29992;&#20449;&#24687;&#35770;&#36719;&#25552;&#31034;&#35843;&#25972;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
InfoPrompt: Information-Theoretic Soft Prompt Tuning for Natural Language Understanding. (arXiv:2306.04933v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#25552;&#31034;&#21644;&#20854;&#20182;&#27169;&#22411;&#21442;&#25968;&#20043;&#38388;&#20114;&#20449;&#24687;&#26469;&#20248;&#21270;&#36719;&#25552;&#31034;&#35843;&#25972;&#65292;&#20174;&#32780;&#24320;&#21457;&#20102;&#26356;&#21152;&#39640;&#25928;&#12289;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#36719;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;InfoPrompt&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20004;&#20010;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;&#65292;&#21457;&#29616;&#21512;&#36866;&#30340;&#25552;&#31034;&#21021;&#22987;&#21270;&#65292;&#24182;&#20174;&#25552;&#31034;&#20196;&#29260;&#20013;&#23398;&#20064;&#36275;&#22815;&#30340;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#65292;&#21516;&#26102;&#40723;&#21169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#34920;&#31034;&#26356;&#21152;&#20851;&#27880;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#25552;&#31034;&#35843;&#25972;&#22312;&#24191;&#27867;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#25552;&#31034;&#35843;&#25972;&#30340;&#24615;&#33021;&#23545;&#21021;&#22987;&#21270;&#30340;&#25552;&#31034;&#38750;&#24120;&#25935;&#24863;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#20256;&#32479;&#30340;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#19981;&#33021;&#20174;&#25552;&#31034;&#20196;&#29260;&#20013;&#32534;&#30721;&#21644;&#23398;&#20064;&#36275;&#22815;&#30340;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#26694;&#26550;&#65292;&#23558;&#36719;&#25552;&#31034;&#35843;&#25972;&#24418;&#24335;&#21270;&#20026;&#26368;&#22823;&#21270;&#25552;&#31034;&#21644;&#20854;&#20182;&#27169;&#22411;&#21442;&#25968;&#65288;&#25110;&#32534;&#30721;&#34920;&#31034;&#65289;&#20043;&#38388;&#20114;&#20449;&#24687;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#35266;&#28857;&#26377;&#21161;&#20110;&#25105;&#20204;&#24320;&#21457;&#19968;&#20010;&#26356;&#39640;&#25928;&#12289;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#36719;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;InfoPrompt&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#20010;&#22522;&#20110;&#20114;&#20449;&#24687;&#30340;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#65288;i&#65289;&#21457;&#29616;&#19979;&#28216;&#20219;&#21153;&#30340;&#21512;&#36866;&#25552;&#31034;&#21021;&#22987;&#21270;&#65292;&#24182;&#20174;&#25552;&#31034;&#20196;&#29260;&#20013;&#23398;&#20064;&#36275;&#22815;&#30340;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#65292;&#65288;ii&#65289;&#40723;&#21169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#34920;&#31034;&#26356;&#21152;&#20851;&#27880;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Soft prompt tuning achieves superior performances across a wide range of few-shot tasks. However, the performances of prompt tuning can be highly sensitive to the initialization of the prompts. We also empirically observe that conventional prompt tuning methods cannot encode and learn sufficient task-relevant information from prompt tokens. In this work, we develop an information-theoretic framework that formulates soft prompt tuning as maximizing mutual information between prompts and other model parameters (or encoded representations). This novel view helps us to develop a more efficient, accurate and robust soft prompt tuning method InfoPrompt. With this framework, we develop two novel mutual information based loss functions, to (i) discover proper prompt initialization for the downstream tasks and learn sufficient task-relevant information from prompt tokens and (ii) encourage the output representation from the pretrained language model to be more aware of the task-relevant informa
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;covLLM&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#35780;&#20272;COVID-19&#25991;&#29486;&#12290;covLLM&#21487;&#20197;&#27719;&#24635;&#21644;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#65292;&#24110;&#21161;&#21307;&#29983;&#26356;&#22909;&#22320;&#24212;&#23545;COVID-19&#30123;&#24773;&#12290;</title><link>http://arxiv.org/abs/2306.04926</link><description>&lt;p&gt;
covLLM&#65306;&#29992;&#20110;COVID-19&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
covLLM: Large Language Models for COVID-19 Biomedical Literature. (arXiv:2306.04926v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04926
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;covLLM&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#35780;&#20272;COVID-19&#25991;&#29486;&#12290;covLLM&#21487;&#20197;&#27719;&#24635;&#21644;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#65292;&#24110;&#21161;&#21307;&#29983;&#26356;&#22909;&#22320;&#24212;&#23545;COVID-19&#30123;&#24773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26032;&#20896;&#30149;&#27602;&#30340;&#30740;&#31350;&#22312;&#19981;&#26029;&#22686;&#21152;&#65292;&#20294;COVID-19&#22823;&#27969;&#34892;&#23548;&#33268;&#20102;&#32654;&#22269;110&#19975;&#20154;&#30340;&#27515;&#20129;&#12290;&#36825;&#20123;&#26032;&#21457;&#29616;&#22312;&#36716;&#21270;&#20026;&#20020;&#24202;&#24178;&#39044;&#26041;&#26696;&#26041;&#38754;&#32531;&#24930;&#65292;&#23548;&#33268;&#24739;&#32773;&#39044;&#21518;&#36739;&#24046;&#21644;&#19981;&#24517;&#35201;&#30340;&#27515;&#20129;&#12290;&#20854;&#20013;&#19968;&#31181;&#21407;&#22240;&#26159;&#20020;&#24202;&#21307;&#29983;&#22240;&#24739;&#32773;&#36807;&#22810;&#32780;&#38590;&#20197;&#36319;&#19978;&#26032;&#20896;&#30149;&#27602;&#25991;&#29486;&#30340;&#36895;&#24230;&#12290;&#21457;&#23637;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35780;&#20272;&#20896;&#29366;&#30149;&#27602;&#25991;&#29486;&#30340;&#24037;&#20855;&#65292;&#21363;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#21487;&#33021;&#26159;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;LLMs&#21487;&#29992;&#20110;&#27719;&#24635;&#21644;&#25552;&#21462;&#29992;&#25143;&#25351;&#23450;&#30340;&#20449;&#24687;&#12290;&#36739;&#22823;&#33539;&#22260;&#21644;&#20808;&#36827;&#30340;LLMs&#21644;&#39044;&#22788;&#29702;&#30340;&#20896;&#29366;&#30149;&#27602;&#25991;&#29486;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#36890;&#36807;&#20896;&#29366;&#30149;&#27602;&#25991;&#29486;&#29305;&#23450;LLM&#65288;covLLM&#65289;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#35780;&#20272;&#20896;&#29366;&#30149;&#27602;&#25991;&#29486;&#30340;&#26426;&#20250;&#65292;&#35813;&#24037;&#20855;&#30452;&#25509;&#36755;&#20837;&#30740;&#31350;&#25991;&#31456;&#21644;&#29992;&#25143;&#26597;&#35810;&#20197;&#36820;&#22238;&#31572;&#26696;&#12290;&#22312;&#20351;&#29992;COVID-19&#24320;&#25918;&#30740;&#31350;&#25968;&#25454;&#38598;&#65288;CORD-19&#65289;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;covLLM&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#24635;&#32467;&#21644;&#20174;&#20896;&#29366;&#30149;&#27602;&#25991;&#29486;&#20013;&#25552;&#21462;&#20449;&#24687;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic led to 1.1 million deaths in the United States, despite the explosion of coronavirus research. These new findings are slow to translate to clinical interventions, leading to poorer patient outcomes and unnecessary deaths. One reason is that clinicians, overwhelmed by patients, struggle to keep pace with the rate of new coronavirus literature. A potential solution is developing a tool for evaluating coronavirus literature using large language models (LLMs) -- neural networks that are deployed for natural language processing. LLMs can be used to summarize and extract user-specified information. The greater availability and advancement of LLMs and pre-processed coronavirus literature databases provide the opportunity to assist clinicians in evaluating coronavirus literature through a coronavirus literature specific LLM (covLLM), a tool that directly takes an inputted research article and a user query to return an answer. Using the COVID-19 Open Research Dataset (CORD
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#20559;&#22909;&#20998;&#31867;&#23398;&#20064;&#36741;&#21161;&#25968;&#25454;&#26469;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#27604;&#36739;&#36755;&#20837;&#25991;&#26412;&#23545;&#20043;&#38388;&#30340;&#20559;&#22909;&#20851;&#31995;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20026;&#27169;&#22411;&#25552;&#20379;&#39069;&#22806;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2306.04925</link><description>&lt;p&gt;
&#20559;&#22909;&#20998;&#31867;&#65306;&#36890;&#36807;&#36741;&#21161;&#20559;&#22909;&#23398;&#20064;&#25913;&#36827;&#25991;&#26412;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Prefer to Classify: Improving Text Classifiers via Auxiliary Preference Learning. (arXiv:2306.04925v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#20559;&#22909;&#20998;&#31867;&#23398;&#20064;&#36741;&#21161;&#25968;&#25454;&#26469;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#27604;&#36739;&#36755;&#20837;&#25991;&#26412;&#23545;&#20043;&#38388;&#30340;&#20559;&#22909;&#20851;&#31995;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20026;&#27169;&#22411;&#25552;&#20379;&#39069;&#22806;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#20154;&#24037;&#26631;&#27880;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#25512;&#21160;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#12290;&#20026;&#20102;&#22686;&#24378;&#29616;&#26377;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#25928;&#26524;&#65292;&#25910;&#38598;&#26032;&#30340;&#36755;&#20837;&#36755;&#20986;&#23545;&#36890;&#24120;&#36807;&#20110;&#26114;&#36149;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#23427;&#20204;&#23545;&#25552;&#39640;&#24403;&#21069;&#27169;&#22411;&#31934;&#24230;&#30340;&#36793;&#38469;&#24433;&#21709;&#12290;&#30456;&#21453;&#65292;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#29616;&#26377;&#36755;&#20837;&#25991;&#26412;&#30340;&#38468;&#21152;&#25110;&#34917;&#20805;&#26631;&#27880;&#21487;&#33021;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#39069;&#22806;&#20154;&#24037;&#25104;&#26412;&#25903;&#20184;&#26041;&#24335;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#36755;&#20837;&#25991;&#26412;&#23545;&#20043;&#38388;&#30340;&#20559;&#22909;&#20851;&#31995;&#20316;&#20026;&#36825;&#31181;&#36741;&#21161;&#25968;&#25454;&#26631;&#27880;&#30340;&#26032;&#26367;&#20195;&#26041;&#24335;&#12290;&#20174;&#20219;&#21153;&#30456;&#20851;&#30340;&#8220;&#25104;&#23545;&#8221;&#27604;&#36739;&#20013;&#65292;&#36741;&#21161;&#20559;&#22909;&#23398;&#20064;&#20351;&#27169;&#22411;&#23398;&#20064;&#21040;&#19968;&#31181;&#39069;&#22806;&#30340;&#20449;&#24687;&#24615;&#35757;&#32451;&#20449;&#21495;&#65292;&#36825;&#31181;&#20449;&#21495;&#26080;&#27861;&#36890;&#36807;&#8220;&#23454;&#20363;&#32423;&#8221;&#30340;&#20219;&#21153;&#26631;&#31614;&#26469;&#25429;&#25417;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;prefer-to-classify &#65288;P2C&#65289;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#20139;&#21463;&#23454;&#20363;&#32423;&#21644;&#20559;&#22909;&#32423;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of largely human-annotated benchmarks has driven the success of deep neural networks in various NLP tasks. To enhance the effectiveness of existing benchmarks, collecting new additional input-output pairs is often too costly and challenging, particularly considering their marginal impact on improving the current model accuracy. Instead, additional or complementary annotations on the existing input texts in the benchmarks can be preferable as an efficient way to pay the additional human cost. In this paper, we investigate task-specific preferences between pairs of input texts as a new alternative way for such auxiliary data annotation. From 'pair-wise' comparisons with respect to the task, the auxiliary preference learning enables the model to learn an additional informative training signal that cannot be captured with 'instance-wise' task labels. To this end, we propose a novel multi-task learning framework, called prefer-to-classify (P2C), which can enjoy the cooperati
&lt;/p&gt;</description></item><item><title>NOWJ&#22242;&#38431;&#22312;COLIEE 2023&#27604;&#36187;&#20013;&#37319;&#29992;&#22810;&#20219;&#21153;&#21644;&#38598;&#25104;&#26041;&#27861;&#25552;&#39640;&#27861;&#24459;&#20449;&#24687;&#22788;&#29702;&#25216;&#26415;&#65292;&#34429;&#26410;&#36798;&#26368;&#20339;&#32467;&#26524;&#20294;&#20026;&#26410;&#26469;&#25913;&#36827;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.04903</link><description>&lt;p&gt;
COLIEE 2023&#27604;&#36187;&#20013;&#30340;NOWJ&#22242;&#38431;&#8212;&#8212;&#27861;&#24459;&#20449;&#24687;&#22788;&#29702;&#20013;&#30340;&#22810;&#20219;&#21153;&#21644;&#38598;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NOWJ at COLIEE 2023 -- Multi-Task and Ensemble Approaches in Legal Information Processing. (arXiv:2306.04903v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04903
&lt;/p&gt;
&lt;p&gt;
NOWJ&#22242;&#38431;&#22312;COLIEE 2023&#27604;&#36187;&#20013;&#37319;&#29992;&#22810;&#20219;&#21153;&#21644;&#38598;&#25104;&#26041;&#27861;&#25552;&#39640;&#27861;&#24459;&#20449;&#24687;&#22788;&#29702;&#25216;&#26415;&#65292;&#34429;&#26410;&#36798;&#26368;&#20339;&#32467;&#26524;&#20294;&#20026;&#26410;&#26469;&#25913;&#36827;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;NOWJ&#22242;&#38431;&#21442;&#21152;COLIEE 2023&#27604;&#36187;&#25152;&#37319;&#29992;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#25552;&#39640;&#27861;&#24459;&#20449;&#24687;&#22788;&#29702;&#25216;&#26415;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#23454;&#38469;&#30340;&#27861;&#24459;&#24773;&#26223;&#20013;&#12290;&#25105;&#20204;&#22242;&#38431;&#30528;&#25163;&#22788;&#29702;&#27604;&#36187;&#20013;&#30340;&#22235;&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#12289;&#27861;&#24459;&#26696;&#20363;&#34164;&#21547;&#12289;&#27861;&#35268;&#26816;&#32034;&#21644;&#27861;&#24459;&#25991;&#26412;&#34164;&#21547;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#21019;&#26032;&#26041;&#27861;&#65292;&#20363;&#22914;BERT&#12289;Longformer&#12289;BM25&#25490;&#24207;&#31639;&#27861;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#22242;&#38431;&#27809;&#26377;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20294;&#25105;&#20204;&#30340;&#21457;&#29616;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#27861;&#24459;&#20449;&#24687;&#22788;&#29702;&#25913;&#36827;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the NOWJ team's approach to the COLIEE 2023 Competition, which focuses on advancing legal information processing techniques and applying them to real-world legal scenarios. Our team tackles the four tasks in the competition, which involve legal case retrieval, legal case entailment, statute law retrieval, and legal textual entailment. We employ state-of-the-art machine learning models and innovative approaches, such as BERT, Longformer, BM25-ranking algorithm, and multi-task learning models. Although our team did not achieve state-of-the-art results, our findings provide valuable insights and pave the way for future improvements in legal information processing.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#29616;&#35937;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#25454;&#23637;&#31034;&#20102;Transformer&#27169;&#22411;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#34920;&#29616;&#20986;&#36125;&#21494;&#26031;&#39044;&#27979;&#22120;&#30340;&#34892;&#20026;&#12290;&#20316;&#32773;&#36824;&#25506;&#35752;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#36125;&#21494;&#26031;&#23398;&#20064;&#26694;&#26550;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#26469;&#39564;&#35777;&#36825;&#31181;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.04891</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#21407;&#29702;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning through the Bayesian Prism. (arXiv:2306.04891v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04891
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#29616;&#35937;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#25454;&#23637;&#31034;&#20102;Transformer&#27169;&#22411;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#34920;&#29616;&#20986;&#36125;&#21494;&#26031;&#39044;&#27979;&#22120;&#30340;&#34892;&#20026;&#12290;&#20316;&#32773;&#36824;&#25506;&#35752;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#36125;&#21494;&#26031;&#23398;&#20064;&#26694;&#26550;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#26469;&#39564;&#35777;&#36825;&#31181;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20196;&#20154;&#24778;&#35766;&#19988;&#26377;&#29992;&#30340;&#29305;&#24615;&#20043;&#19968;&#12290;&#23427;&#30340;&#24037;&#20316;&#21407;&#29702;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#36817;&#26399;&#65292;&#20154;&#20204;&#35774;&#35745;&#20102;&#19968;&#20123;&#39118;&#26684;&#21270;&#30340;&#31867;&#20803;&#23398;&#20064;&#30340;&#35774;&#32622;&#65292;&#23427;&#20204;&#20351;&#29992;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#20989;&#25968;&#23545;&#26469;&#33258;&#20989;&#25968;&#31867;&#30340;&#36755;&#20837;&#36755;&#20986;&#23545;$(x, f(x))$ &#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#35266;&#23519;&#27169;&#22411;&#23545;&#21516;&#19968;&#31867;&#20013;&#26410;&#35265;&#36807;&#30340;&#20989;&#25968;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#19968;&#30740;&#31350;&#32447;&#36335;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;&#23545;&#20110;&#35832;&#22914;&#32447;&#24615;&#22238;&#24402;&#31561;&#20960;&#20010;&#38382;&#39064;&#65292;&#35757;&#32451;&#22909;&#30340; Transformer &#23398;&#20064;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#23548;&#33268;&#36825;&#31181;&#34892;&#20026;&#30340;&#24402;&#32435;&#20559;&#24046;&#24182;&#19981;&#28165;&#26970;&#12290;&#25317;&#26377;&#26080;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#27169;&#22411;&#26159;&#36125;&#21494;&#26031;&#39044;&#27979;&#22120;&#65306;&#23427;&#23398;&#20064;&#20102;&#39044;&#35757;&#32451;&#20998;&#24067;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#39640;&#23481;&#37327;&#30340; Transformer &#27169;&#22411;&#22312;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#19978;&#27169;&#25311;&#36125;&#21494;&#26031;&#39044;&#27979;&#22120;&#30340;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Transformer&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#34920;&#29616;&#20986;&#29702;&#24819;&#23398;&#20064;&#32773;&#30340;&#34892;&#20026;&#30340;&#32463;&#39564;&#35777;&#25454;&#65292;&#21253;&#25324;&#22806;&#25512;&#21644;&#27714;&#35299;&#24494;&#20998;&#26041;&#31243;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#36125;&#21494;&#26031;&#23398;&#20064;&#26694;&#26550;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#35748;&#20026;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#20102;&#21512;&#29702;&#20989;&#25968;&#30340;&#20808;&#39564;&#27010;&#29575;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#26368;&#23567;&#21270;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#26469;&#36827;&#19968;&#27493;&#25506;&#31350;&#36825;&#31181;&#32852;&#31995;&#65292;&#35777;&#26126;&#20351;&#29992;&#30495;&#23454;&#30340;&#36125;&#21494;&#26031;&#20808;&#39564;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#27604;&#20351;&#29992;&#22266;&#23450;&#20808;&#39564;&#25110;&#27809;&#26377;&#20808;&#39564;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning is one of the surprising and useful features of large language models. How it works is an active area of research. Recently, stylized meta-learning-like setups have been devised that train these models on a sequence of input-output pairs $(x, f(x))$ from a function class using the language modeling loss and observe generalization to unseen functions from the same class. One of the main discoveries in this line of research has been that for several problems such as linear regression, trained transformers learn algorithms for learning functions in context. However, the inductive biases of these models resulting in this behavior are not clearly understood. A model with unlimited training data and compute is a Bayesian predictor: it learns the pretraining distribution. It has been shown that high-capacity transformers mimic the Bayesian predictor for linear regression. In this paper, we show empirical evidence of transformers exhibiting the behavior of this ideal learne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#23558;&#33521;&#25991;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#36866;&#29992;&#20110;&#20013;&#25991;&#19978;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#23545;&#25239;&#23454;&#20363;&#12290;&#36890;&#36807;&#20851;&#27880;&#20013;&#25991;&#30340;&#35821;&#35328;&#29305;&#28857;&#65292;&#29983;&#25104;&#30340;&#23545;&#25239;&#23454;&#20363;&#21487;&#20197;&#23454;&#29616;&#39640;&#27969;&#30021;&#24230;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#26469;&#25552;&#39640;&#20013;&#25991;NLP&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04874</link><description>&lt;p&gt;
&#25193;&#22823;&#33539;&#22260;&#65306;&#23558;&#33521;&#25991;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#36866;&#24212;&#21040;&#20013;&#25991;&#19978;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Expanding Scope: Adapting English Adversarial Attacks to Chinese. (arXiv:2306.04874v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23558;&#33521;&#25991;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#36866;&#29992;&#20110;&#20013;&#25991;&#19978;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#23545;&#25239;&#23454;&#20363;&#12290;&#36890;&#36807;&#20851;&#27880;&#20013;&#25991;&#30340;&#35821;&#35328;&#29305;&#28857;&#65292;&#29983;&#25104;&#30340;&#23545;&#25239;&#23454;&#20363;&#21487;&#20197;&#23454;&#29616;&#39640;&#27969;&#30021;&#24230;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#26469;&#25552;&#39640;&#20013;&#25991;NLP&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#39044;&#27979;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#12290;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#30528;&#30524;&#20110;&#35774;&#35745;&#25915;&#20987;&#26041;&#24335;&#26469;&#35780;&#20272;&#33521;&#35821;&#35821;&#22659;&#19979;&#30340;NLP&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#23398;&#26415;&#30028;&#23545;&#20854;&#23427;&#35821;&#35328;&#30340;NLP&#35299;&#20915;&#26041;&#26696;&#38656;&#27714;&#26085;&#30410;&#22686;&#38271;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#33258;&#28982;&#20135;&#29983;&#19968;&#20010;&#38382;&#39064;&#65306;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#26159;&#21542;&#33021;&#22815;&#27867;&#21270;&#21040;&#20854;&#23427;&#35821;&#35328;&#20013;&#65311;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#22312;&#33521;&#25991;&#29615;&#22659;&#19979;&#30340;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#25915;&#20987;&#31639;&#27861;&#36866;&#24212;&#21040;&#20013;&#25991;&#19978;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#32467;&#21512;&#27491;&#30830;&#30340;&#25991;&#26412;&#20998;&#21106;&#21644;&#35821;&#35328;&#38480;&#21046;&#26102;&#65292;&#20808;&#21069;&#38024;&#23545;&#33521;&#25991;NLP&#30340;&#25915;&#20987;&#26041;&#27861;&#20063;&#33021;&#22815;&#22312;&#20013;&#25991;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23545;&#25239;&#24615;&#20363;&#23376;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#65292;&#36890;&#36807;&#20851;&#27880;&#20013;&#25991;&#30340;&#24418;&#24577;&#21644;&#38899;&#31995;&#65292;&#29983;&#25104;&#30340;&#23545;&#25239;&#23454;&#20363;&#21487;&#20197;&#23454;&#29616;&#39640;&#27969;&#30021;&#24230;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#26469;&#25552;&#39640;&#20013;&#25991;NLP&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have revealed that NLP predictive models are vulnerable to adversarial attacks. Most existing studies focused on designing attacks to evaluate the robustness of NLP models in the English language alone. Literature has seen an increasing need for NLP solutions for other languages. We, therefore, ask one natural question: whether state-of-the-art (SOTA) attack methods generalize to other languages. This paper investigates how to adapt SOTA adversarial attack algorithms in English to the Chinese language. Our experiments show that attack methods previously applied to English NLP can generate high-quality adversarial examples in Chinese when combined with proper text segmentation and linguistic constraints. In addition, we demonstrate that the generated adversarial examples can achieve high fluency and semantic consistency by focusing on the Chinese language's morphology and phonology, which in turn can be used to improve the adversarial robustness of Chinese NLP models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#36229;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#32467;&#26500;&#36335;&#30001;&#30340;&#19987;&#23478;&#28151;&#21512;&#26469;&#22686;&#24378;&#36229;&#32423;&#32593;&#32476;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#25913;&#21892;&#20102;&#23376;&#32593;&#32476;&#30340;&#36136;&#37327;&#38382;&#39064;&#21644;&#24615;&#33021;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.04845</link><description>&lt;p&gt;
&#28151;&#21512;&#36229;&#32593;&#32476;&#65306;&#36890;&#36807;&#22522;&#20110;&#32467;&#26500;&#36335;&#30001;&#30340;&#19987;&#23478;&#28151;&#21512;&#25913;&#36827;&#20849;&#20139;&#26435;&#37325;&#36229;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with Architecture-Routed Mixture-of-Experts. (arXiv:2306.04845v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04845
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#36229;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#32467;&#26500;&#36335;&#30001;&#30340;&#19987;&#23478;&#28151;&#21512;&#26469;&#22686;&#24378;&#36229;&#32423;&#32593;&#32476;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#25913;&#21892;&#20102;&#23376;&#32593;&#32476;&#30340;&#36136;&#37327;&#38382;&#39064;&#21644;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#20139;&#26435;&#37325;&#30340;&#36229;&#32423;&#32593;&#32476;&#24050;&#32463;&#25104;&#20026;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26694;&#26550;&#20013;&#24615;&#33021;&#35780;&#20272;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26435;&#37325;&#20849;&#20139;&#65292;&#36229;&#32423;&#32593;&#32476;&#30452;&#25509;&#29983;&#25104;&#30340;&#19981;&#21516;&#23376;&#32593;&#32476;&#30340;&#36136;&#37327;&#26080;&#27861;&#20445;&#35777;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#24314;&#27169;&#31561;NLP&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#30456;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#19979;&#65292;&#36229;&#32423;&#32593;&#32476;&#19982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#22240;&#27492;&#65292;&#22312;&#25214;&#21040;&#26368;&#20339;&#26550;&#26500;&#21518;&#65292;&#19981;&#33021;&#30452;&#25509;&#20351;&#29992;&#36229;&#32423;&#32593;&#32476;&#65292;&#24517;&#39035;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#28151;&#21512;&#36229;&#32593;&#32476;&#65292;&#36825;&#26159;&#19968;&#31181;&#24191;&#20041;&#30340;&#36229;&#32423;&#32593;&#32476;&#20844;&#24335;&#65292;&#20854;&#20013;&#37319;&#29992;&#20102;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#26469;&#22686;&#24378;&#36229;&#32423;&#32593;&#32476;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35757;&#32451;&#24320;&#38144;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#19981;&#21516;&#30340;&#23376;&#32593;&#32476;&#19981;&#26159;&#30452;&#25509;&#20849;&#20139;&#27169;&#22411;&#26435;&#37325;&#65292;&#32780;&#26159;&#36890;&#36807;&#22522;&#20110;&#32467;&#26500;&#30340;&#36335;&#30001;&#26426;&#21046;&#20849;&#20139;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#24615;&#33021;&#24471;&#21040;&#20102;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weight-sharing supernet has become a vital component for performance estimation in the state-of-the-art (SOTA) neural architecture search (NAS) frameworks. Although supernet can directly generate different subnetworks without retraining, there is no guarantee for the quality of these subnetworks because of weight sharing. In NLP tasks such as machine translation and pre-trained language modeling, we observe that given the same model architecture, there is a large performance gap between supernet and training from scratch. Hence, supernet cannot be directly used and retraining is necessary after finding the optimal architectures.  In this work, we propose mixture-of-supernets, a generalized supernet formulation where mixture-of-experts (MoE) is adopted to enhance the expressive power of the supernet model, with negligible training overhead. In this way, different subnetworks do not share the model weights directly, but through an architecture-based routing mechanism. As a result, model 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#36234;&#21335;&#27861;&#24459;&#38382;&#31572;&#31995;&#32479;&#20013;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#30340;&#27861;&#24459;&#38382;&#31572;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24369;&#26631;&#27880;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#20174;&#32780;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04841</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#21160;&#25968;&#25454;&#22686;&#24378;&#30340;&#36234;&#21335;&#27861;&#24459;&#38382;&#31572;&#31995;&#32479;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improving Vietnamese Legal Question--Answering System based on Automatic Data Enrichment. (arXiv:2306.04841v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#36234;&#21335;&#27861;&#24459;&#38382;&#31572;&#31995;&#32479;&#20013;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#30340;&#27861;&#24459;&#38382;&#31572;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24369;&#26631;&#27880;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#20174;&#32780;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27861;&#24459;&#20013;&#36827;&#34892;&#38382;&#39064;&#22238;&#31572;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#27861;&#24459;&#25991;&#20214;&#22312;&#26415;&#35821;&#12289;&#32467;&#26500;&#20197;&#21450;&#26102;&#38388;&#21644;&#36923;&#36753;&#20851;&#31995;&#26041;&#38754;&#27604;&#26222;&#36890;&#25991;&#26412;&#35201;&#22797;&#26434;&#24471;&#22810;&#12290;&#23588;&#20854;&#23545;&#20110;&#36234;&#21335;&#36825;&#26679;&#36164;&#28304;&#31232;&#23569;&#30340;&#35821;&#35328;&#26469;&#35828;&#65292;&#36827;&#34892;&#27861;&#24459;&#38382;&#31572;&#26356;&#21152;&#22256;&#38590;&#65292;&#22240;&#20026;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#19988;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#25991;&#35797;&#22270;&#36890;&#36807;&#23454;&#29616;&#36234;&#21335;&#22522;&#20110;&#26816;&#32034;&#30340;&#25991;&#31456;&#32423;&#27861;&#24459;&#38382;&#31572;&#31995;&#32479;&#65292;&#24182;&#20171;&#32461;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#24369;&#26631;&#27880;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#65292;&#22312;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#30340;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26088;&#22312;&#27979;&#35797;&#22810;&#20010;&#26041;&#38754;&#65292;&#35777;&#26126;&#25152;&#25552;&#20986;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering (QA) in law is a challenging problem because legal documents are much more complicated than normal texts in terms of terminology, structure, and temporal and logical relationships. It is even more difficult to perform legal QA for low-resource languages like Vietnamese where labeled data are rare and pre-trained language models are still limited. In this paper, we try to overcome these limitations by implementing a Vietnamese article-level retrieval-based legal QA system and introduce a novel method to improve the performance of language models by improving data quality through weak labeling. Our hypothesis is that in contexts where labeled data are limited, efficient data enrichment can help increase overall performance. Our experiments are designed to test multiple aspects, which demonstrate the effectiveness of the proposed technique.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25216;&#33021;&#36335;&#30001;&#20013;&#38271;&#23614;&#25968;&#25454;&#30340;&#24322;&#26500;&#25968;&#25454;&#22686;&#24378;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23614;&#37096;&#35831;&#27714;&#30340;&#25216;&#33021;&#36335;&#30001;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#22836;&#37096;&#35831;&#27714;&#20013;&#20445;&#25345;&#30456;&#23545;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04823</link><description>&lt;p&gt;
&#25913;&#36827;&#23545;&#35805;&#31995;&#32479;&#20013;&#23614;&#27969;&#37327;&#40065;&#26834;&#24615;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation for Improving Tail-traffic Robustness in Skill-routing for Dialogue Systems. (arXiv:2306.04823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04823
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25216;&#33021;&#36335;&#30001;&#20013;&#38271;&#23614;&#25968;&#25454;&#30340;&#24322;&#26500;&#25968;&#25454;&#22686;&#24378;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23614;&#37096;&#35831;&#27714;&#30340;&#25216;&#33021;&#36335;&#30001;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#22836;&#37096;&#35831;&#27714;&#20013;&#20445;&#25345;&#30456;&#23545;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#23545;&#35805;&#31995;&#32479;&#36890;&#24120;&#20381;&#36182;&#20110;&#25216;&#33021;&#36335;&#30001;&#32452;&#20214;&#26469;&#23558;&#29992;&#25143;&#35831;&#27714;&#36335;&#30001;&#21040;&#36866;&#24403;&#30340;&#25216;&#33021;&#21644;&#35299;&#37322;&#20013;&#20197;&#26381;&#21153;&#35831;&#27714;&#12290;&#22312;&#36825;&#26679;&#30340;&#31995;&#32479;&#20013;&#65292;&#20195;&#29702;&#36127;&#36131;&#20026;&#25968;&#21315;&#20010;&#25216;&#33021;&#21644;&#35299;&#37322;&#25552;&#20379;&#26381;&#21153;&#65292;&#32423;&#21035;&#20998;&#24067;&#21576;&#38271;&#23614;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#25552;&#39640;&#25216;&#33021;&#36335;&#30001;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#26500;&#25216;&#33021;&#36335;&#30001;&#25968;&#25454;&#30340;&#22686;&#24378;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#38024;&#23545;&#38271;&#23614;&#25968;&#25454;&#33539;&#22260;&#30340;&#21487;&#38752;&#25805;&#20316;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#26377;&#26465;&#20214;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#29983;&#25104;&#26694;&#26550;&#65292;&#20197;&#25200;&#21160;&#21407;&#22987;&#25968;&#25454;&#23383;&#27573;&#24182;&#21019;&#24314;&#21512;&#25104;&#32763;&#35793;&#65292;&#21516;&#26102;&#20445;&#30041;&#29992;&#25143;&#35831;&#27714;&#30340;&#35821;&#20041;&#21547;&#20041;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#23614;&#37096;&#35831;&#27714;&#30340;&#25216;&#33021;&#36335;&#30001;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#22836;&#37096;&#35831;&#27714;&#20013;&#20445;&#25345;&#30456;&#23545;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale conversational systems typically rely on a skill-routing component to route a user request to an appropriate skill and interpretation to serve the request. In such system, the agent is responsible for serving thousands of skills and interpretations which create a long-tail distribution due to the natural frequency of requests. For example, the samples related to play music might be a thousand times more frequent than those asking for theatre show times. Moreover, inputs used for ML-based skill routing are often a heterogeneous mix of strings, embedding vectors, categorical and scalar features which makes employing augmentation-based long-tail learning approaches challenging. To improve the skill-routing robustness, we propose an augmentation of heterogeneous skill-routing data and training targeted for robust operation in long-tail data regimes. We explore a variety of conditional encoder-decoder generative frameworks to perturb original data fields and create synthetic tra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#35770;&#25991;&#30740;&#31350;&#26041;&#38754;&#20998;&#31867;&#20013;&#30340;&#20316;&#29992;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;PubMed 200K RCT&#25968;&#25454;&#38598;&#24182;&#19981;&#33021;&#25913;&#21892;&#24615;&#33021;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#23613;&#31649;GPT-4&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23427;&#24182;&#27809;&#26377;&#36229;&#36234;fine-tuned&#22312;CODA-19&#25968;&#25454;&#38598;&#19978;&#30340;SciBERT&#27169;&#22411;&#65292;&#36825;&#24378;&#35843;&#20102;LLMs&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04820</link><description>&lt;p&gt;
&#22909;&#25968;&#25454;&#12289;&#22823;&#25968;&#25454;&#36824;&#26159;&#26080;&#25968;&#25454;&#65311;&#8212;&#8212;&#22312;&#24320;&#21457;&#29983;&#29289;&#21307;&#23398;&#35770;&#25991;&#30740;&#31350;&#26041;&#38754;&#20998;&#31867;&#22120;&#26041;&#38754;&#27604;&#36739;&#19977;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Good Data, Large Data, or No Data? Comparing Three Approaches in Developing Research Aspect Classifiers for Biomedical Papers. (arXiv:2306.04820v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#35770;&#25991;&#30740;&#31350;&#26041;&#38754;&#20998;&#31867;&#20013;&#30340;&#20316;&#29992;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;PubMed 200K RCT&#25968;&#25454;&#38598;&#24182;&#19981;&#33021;&#25913;&#21892;&#24615;&#33021;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#23613;&#31649;GPT-4&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23427;&#24182;&#27809;&#26377;&#36229;&#36234;fine-tuned&#22312;CODA-19&#25968;&#25454;&#38598;&#19978;&#30340;SciBERT&#27169;&#22411;&#65292;&#36825;&#24378;&#35843;&#20102;LLMs&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#20986;&#29256;&#29289;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#29305;&#21035;&#26159;&#22312;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#26377;&#25928;&#29702;&#35299;&#26368;&#26032;&#36827;&#23637;&#30340;&#24037;&#20855;&#12290;&#20102;&#35299;&#31185;&#23398;&#25991;&#29486;&#30340;&#19968;&#20010;&#20851;&#38190;&#37096;&#20998;&#26159;&#30740;&#31350;&#26041;&#38754;&#20998;&#31867;&#65292;&#23427;&#23558;&#25688;&#35201;&#20013;&#30340;&#21477;&#23376;&#20998;&#31867;&#20026;&#32972;&#26223;&#12289;&#30446;&#30340;&#12289;&#26041;&#27861;&#21644;&#21457;&#29616;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#23545;&#20247;&#21253;&#27880;&#37322;&#30340;CODA-19&#30740;&#31350;&#26041;&#38754;&#20998;&#31867;&#20219;&#21153;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#12289;&#33258;&#21160;&#31574;&#21010;&#30340;PubMed 200K RCT&#25968;&#25454;&#38598;&#30340;&#28508;&#22312;&#20248;&#28857;&#65292;&#24182;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;LLaMA&#12289;GPT-3&#12289;ChatGPT&#21644;GPT-4&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;CODA-19&#20219;&#21153;&#65292;&#20351;&#29992;PubMed 200K RCT&#25968;&#25454;&#38598;&#24182;&#19981;&#33021;&#25913;&#21892;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#23613;&#31649;GPT-4&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23427;&#24182;&#27809;&#26377;&#36229;&#36234;fine-tuned&#22312;CODA-19&#25968;&#25454;&#38598;&#19978;&#30340;SciBERT&#27169;&#22411;&#65292;&#36825;&#24378;&#35843;&#20102;LLMs&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid growth of scientific publications, particularly during the COVID-19 pandemic, emphasizes the need for tools to help researchers efficiently comprehend the latest advancements. One essential part of understanding scientific literature is research aspect classification, which categorizes sentences in abstracts to Background, Purpose, Method, and Finding. In this study, we investigate the impact of different datasets on model performance for the crowd-annotated CODA-19 research aspect classification task. Specifically, we explore the potential benefits of using the large, automatically curated PubMed 200K RCT dataset and evaluate the effectiveness of large language models (LLMs), such as LLaMA, GPT-3, ChatGPT, and GPT-4. Our results indicate that using the PubMed 200K RCT dataset does not improve performance for the CODA-19 task. We also observe that while GPT-4 performs well, it does not outperform the SciBERT model fine-tuned on the CODA-19 dataset, emphasizing the importance 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#27599;&#19968;&#34892;&#25968;&#25454;&#20316;&#20026;&#19968;&#20010;&#21477;&#23376;&#24182;&#28155;&#21152;&#24046;&#20998;&#38544;&#31169;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#20197;&#23454;&#29616;&#31169;&#20154;&#25968;&#25454;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.04803</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#31169;&#19979;&#29983;&#25104;&#34920;&#26684;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Privately generating tabular data using language models. (arXiv:2306.04803v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04803
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#27599;&#19968;&#34892;&#25968;&#25454;&#20316;&#20026;&#19968;&#20010;&#21477;&#23376;&#24182;&#28155;&#21152;&#24046;&#20998;&#38544;&#31169;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#20197;&#23454;&#29616;&#31169;&#20154;&#25968;&#25454;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#38544;&#31169;&#20026;&#20808;&#30340;&#19990;&#30028;&#20013;&#65292;&#31169;&#19979;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#23558;&#34920;&#26684;&#20013;&#30340;&#27599;&#19968;&#34892;&#35270;&#20026;&#19968;&#20010;&#21477;&#23376;&#65292;&#24182;&#35757;&#32451;&#24046;&#20998;&#38544;&#31169;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#24314;&#27169;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#34920;&#26684;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#65292;&#21363;&#20351;&#26159;&#22312;&#26377;&#21033;&#20110;&#22522;&#20110;&#36793;&#32536;&#20998;&#24067;&#30340;&#26367;&#20195;&#26041;&#27861;&#30340;&#23567;&#35268;&#27169;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privately generating synthetic data from a table is an important brick of a privacy-first world. We propose and investigate a simple approach of treating each row in a table as a sentence and training a language model with differential privacy. We show this approach obtains competitive results in modelling tabular data across multiple datasets, even at small scales that favor alternative methods based on marginal distributions.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#30340;&#26500;&#24314;&#27969;&#31243;&#12289;&#20851;&#38190;&#25216;&#26415;&#21644;&#21033;&#29992;&#26041;&#27861;&#20197;&#21450;&#29616;&#26377;&#36164;&#28304;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.04802</link><description>&lt;p&gt;
&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;&#32508;&#36848;&#65306;&#36164;&#28304;&#12289;&#24212;&#29992;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
A Survey on Knowledge Graphs for Healthcare: Resources, Applications, and Promises. (arXiv:2306.04802v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#30340;&#26500;&#24314;&#27969;&#31243;&#12289;&#20851;&#38190;&#25216;&#26415;&#21644;&#21033;&#29992;&#26041;&#27861;&#20197;&#21450;&#29616;&#26377;&#36164;&#28304;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#24050;&#25104;&#20026;&#32452;&#32455;&#21307;&#23398;&#30693;&#35782;&#30340;&#26377;&#32467;&#26500;&#19988;&#21487;&#35299;&#37322;&#30340;&#26377;&#20026;&#24037;&#20855;&#65292;&#25552;&#20379;&#20102;&#21307;&#23398;&#27010;&#24565;&#21450;&#20854;&#20851;&#31995;&#30340;&#20840;&#38754;&#35270;&#22270;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#31561;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65292;&#24378;&#35843;&#20102;&#22312;HKG&#39046;&#22495;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#32508;&#36848;&#26159;HKG&#30340;&#31532;&#19968;&#20221;&#32508;&#21512;&#27010;&#36848;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;HKG&#26500;&#24314;&#30340;&#27969;&#31243;&#21644;&#20851;&#38190;&#25216;&#26415;&#65288;&#21363;&#20174;&#22836;&#24320;&#22987;&#21644;&#36890;&#36807;&#38598;&#25104;&#65289;&#65292;&#20197;&#21450;&#24120;&#35265;&#30340;&#21033;&#29992;&#26041;&#27861;&#65288;&#21363;&#22522;&#20110;&#27169;&#22411;&#21644;&#38750;&#22522;&#20110;&#27169;&#22411;&#65289;&#12290;&#20026;&#20102;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#65292;&#25105;&#20204;&#26681;&#25454;&#23427;&#20204;&#25429;&#33719;&#30340;&#25968;&#25454;&#31867;&#22411;&#21644;&#24212;&#29992;&#39046;&#22495;&#65288;&#35813;&#36164;&#28304;&#23384;&#20648;&#20110;https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase&#65289;&#32452;&#32455;&#20102;&#29616;&#26377;&#30340;HKG&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#22312;&#24212;&#29992;&#37096;&#20998;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Healthcare knowledge graphs (HKGs) have emerged as a promising tool for organizing medical knowledge in a structured and interpretable way, which provides a comprehensive view of medical concepts and their relationships. However, challenges such as data heterogeneity and limited coverage remain, emphasizing the need for further research in the field of HKGs. This survey paper serves as the first comprehensive overview of HKGs. We summarize the pipeline and key techniques for HKG construction (i.e., from scratch and through integration), as well as the common utilization approaches (i.e., model-free and model-based). To provide researchers with valuable resources, we organize existing HKGs (The resource is available at https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase) based on the data types they capture and application domains, supplemented with pertinent statistical information. In the application section, we delve into the transformative impact of HKGs across various hea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#26080;&#30417;&#30563;&#22810;&#25991;&#26723;&#25277;&#35937;&#25688;&#35201;&#29983;&#25104;&#30340;Transformer-based&#26041;&#27861;Absformer&#65292;&#23427;&#20351;&#29992;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#32858;&#31867;&#25991;&#26723;&#24182;&#29983;&#25104;&#25277;&#35937;&#25688;&#35201;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23427;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#25277;&#35937;MDS&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04787</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#26080;&#30417;&#30563;&#22810;&#25991;&#26723;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Absformer: Transformer-based Model for Unsupervised Multi-Document Abstractive Summarization. (arXiv:2306.04787v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#26080;&#30417;&#30563;&#22810;&#25991;&#26723;&#25277;&#35937;&#25688;&#35201;&#29983;&#25104;&#30340;Transformer-based&#26041;&#27861;Absformer&#65292;&#23427;&#20351;&#29992;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#32858;&#31867;&#25991;&#26723;&#24182;&#29983;&#25104;&#25277;&#35937;&#25688;&#35201;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23427;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#25277;&#35937;MDS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25991;&#26723;&#25688;&#35201;&#65288;MDS&#65289;&#26159;&#23558;&#22810;&#20010;&#25991;&#26723;&#20013;&#30340;&#25991;&#26412;&#24635;&#32467;&#25104;&#31616;&#27905;&#27010;&#25324;&#30340;&#20219;&#21153;&#12290;&#25152;&#29983;&#25104;&#30340;&#25688;&#35201;&#36890;&#36807;&#29992;&#23569;&#25968;&#20960;&#21477;&#35805;&#25552;&#20379;&#37325;&#35201;&#20869;&#23481;&#65292;&#21487;&#20197;&#30465;&#21435;&#38405;&#35835;&#22810;&#20010;&#25991;&#26723;&#30340;&#26102;&#38388;&#12290;&#25277;&#35937;MDS&#26088;&#22312;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#20026;&#22810;&#20010;&#25991;&#26723;&#29983;&#25104;&#36830;&#36143;&#12289;&#27969;&#30021;&#30340;&#25688;&#35201;&#12290;&#26412;&#25991;&#32771;&#34385;&#20165;&#26377;&#25991;&#26723;&#32780;&#27809;&#26377;&#25688;&#35201;&#30340;&#26080;&#30417;&#30563;&#25277;&#35937;MDS&#29615;&#22659;&#65292;&#24182;&#25552;&#20986;Absformer&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#25277;&#35937;&#25688;&#35201;&#29983;&#25104;&#30340;&#26032;&#22411;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#31532;&#19968;&#27493;&#65292;&#20351;&#29992;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#30446;&#26631;&#20316;&#20026;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20197;&#35757;&#32451;Transformer&#32534;&#30721;&#22120;&#65292;&#23558;&#25991;&#26723;&#32858;&#31867;&#20026;&#35821;&#20041;&#30456;&#20284;&#30340;&#32452;&#65307;&#31532;&#20108;&#27493;&#65292;&#35757;&#32451;&#19968;&#20010;Transformer&#35299;&#30721;&#22120;&#65292;&#20026;&#25991;&#26723;&#38598;&#32676;&#29983;&#25104;&#25277;&#35937;&#25688;&#35201;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20221;&#25552;&#20986;&#23558;&#32858;&#31867;&#29992;&#20110;&#25991;&#26723;&#32452;&#21512;&#30340;&#26080;&#30417;&#30563;&#25277;&#35937;MDS&#26041;&#27861;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#25277;&#35937;MDS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-document summarization (MDS) refers to the task of summarizing the text in multiple documents into a concise summary. The generated summary can save the time of reading many documents by providing the important content in the form of a few sentences. Abstractive MDS aims to generate a coherent and fluent summary for multiple documents using natural language generation techniques. In this paper, we consider the unsupervised abstractive MDS setting where there are only documents with no groundtruh summaries provided, and we propose Absformer, a new Transformer-based method for unsupervised abstractive summary generation. Our method consists of a first step where we pretrain a Transformer-based encoder using the masked language modeling (MLM) objective as the pretraining task in order to cluster the documents into semantically similar groups; and a second step where we train a Transformer-based decoder to generate abstractive summaries for the clusters of documents. To our knowledge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#30740;&#31350;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20844;&#24320;&#37096;&#32626;&#65292;&#21457;&#29616;&#20195;&#29702;&#20154;&#30340;&#25277;&#35937;&#25311;&#20154;&#21270;&#34920;&#29616;&#24433;&#21709;&#29992;&#25143;&#24863;&#30693;&#65292;AI&#21487;&#35299;&#37322;&#24615;&#21487;&#33021;&#24433;&#21709;&#21453;&#39304;&#29575;&#65292;&#32842;&#22825;&#20307;&#39564;&#30340;&#20004;&#31181;&#27700;&#24179;&#24212;&#26377;&#24847;&#35774;&#35745;&#12290;&#27492;&#30740;&#31350;&#25552;&#20379;&#20102;&#35774;&#35745;&#24314;&#35758;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.04765</link><description>&lt;p&gt;
&#20844;&#24320;&#37096;&#32626;&#30740;&#31350;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#38754;&#65306;&#19968;&#39033;&#29992;&#25143;&#30740;&#31350;&#12289;&#35774;&#35745;&#24314;&#35758;&#21644;&#24320;&#25918;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
The HCI Aspects of Public Deployment of Research Chatbots: A User Study, Design Recommendations, and Open Challenges. (arXiv:2306.04765v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#30740;&#31350;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20844;&#24320;&#37096;&#32626;&#65292;&#21457;&#29616;&#20195;&#29702;&#20154;&#30340;&#25277;&#35937;&#25311;&#20154;&#21270;&#34920;&#29616;&#24433;&#21709;&#29992;&#25143;&#24863;&#30693;&#65292;AI&#21487;&#35299;&#37322;&#24615;&#21487;&#33021;&#24433;&#21709;&#21453;&#39304;&#29575;&#65292;&#32842;&#22825;&#20307;&#39564;&#30340;&#20004;&#31181;&#27700;&#24179;&#24212;&#26377;&#24847;&#35774;&#35745;&#12290;&#27492;&#30740;&#31350;&#25552;&#20379;&#20102;&#35774;&#35745;&#24314;&#35758;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24320;&#37096;&#32626;&#30740;&#31350;&#32842;&#22825;&#26426;&#22120;&#20154;&#26159;&#19968;&#20010;&#28041;&#21450;&#24517;&#35201;&#30340;&#39118;&#38505;&#19982;&#25910;&#30410;&#20998;&#26512;&#30340;&#24494;&#22937;&#35805;&#39064;&#12290;&#34429;&#28982;&#26368;&#36817;&#39057;&#32321;&#35752;&#35770;&#26159;&#21542;&#36127;&#36131;&#20219;&#22320;&#37096;&#32626;&#27492;&#31867;&#27169;&#22411;&#65292;&#20294;&#23545;&#20110;&#23454;&#29616;&#26356;&#26377;&#25928;&#30446;&#26631;&#30340;&#20132;&#20114;&#33539;&#24335;&#21644;&#35774;&#35745;&#26041;&#27861;&#21364;&#20851;&#27880;&#36739;&#23569;&#12290;&#25105;&#20204;&#36890;&#36807;&#25253;&#21578;&#23545;&#26368;&#36817;&#30740;&#31350;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#30340;&#28151;&#21512;&#26041;&#27861;&#29992;&#25143;&#30740;&#31350;&#65292;&#21147;&#22270;&#25552;&#20986;&#12289;&#22522;&#20110;&#24182;&#23581;&#35797;&#22238;&#31572;&#28041;&#21450;&#27492;&#33539;&#22260;&#30340;&#20154;&#26426;&#20132;&#20114;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20195;&#29702;&#20154;&#30340;&#25277;&#35937;&#25311;&#20154;&#21270;&#34920;&#29616;&#23545;&#29992;&#25143;&#30340;&#24863;&#30693;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#25552;&#20379;AI&#21487;&#35299;&#37322;&#24615;&#21487;&#33021;&#20250;&#23545;&#21453;&#39304;&#29575;&#20135;&#29983;&#24433;&#21709;&#65292;&#32780;&#32842;&#22825;&#20307;&#39564;&#30340;&#20004;&#31181;&#27700;&#24179;&#65288;&#25925;&#20107;&#20869;&#21644;&#25925;&#20107;&#22806;&#65289;&#24212;&#26377;&#24847;&#35774;&#35745;&#12290;&#25105;&#20204;&#25552;&#20379;&#35774;&#35745;&#24314;&#35758;&#21644;&#30740;&#31350;&#31038;&#21306;&#36827;&#19968;&#27493;&#20851;&#27880;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Publicly deploying research chatbots is a nuanced topic involving necessary risk-benefit analyses. While there have recently been frequent discussions on whether it is responsible to deploy such models, there has been far less focus on the interaction paradigms and design approaches that the resulting interfaces should adopt, in order to achieve their goals more effectively. We aim to pose, ground, and attempt to answer HCI questions involved in this scope, by reporting on a mixed-methods user study conducted on a recent research chatbot. We find that abstract anthropomorphic representation for the agent has a significant effect on user's perception, that offering AI explainability may have an impact on feedback rates, and that two (diegetic and extradiegetic) levels of the chat experience should be intentionally designed. We offer design recommendations and areas of further focus for the research community.
&lt;/p&gt;</description></item><item><title>INSTRUCTEVAL&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#32508;&#21512;&#22871;&#20214;&#65292;&#23427;&#37319;&#21462;&#20102;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35299;&#20915;&#38382;&#39064;&#12289;&#20889;&#20316;&#33021;&#21147;&#21644;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#31561;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.04757</link><description>&lt;p&gt;
INSTRUCTEVAL&#65306;&#38754;&#21521;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#20307;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models. (arXiv:2306.04757v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04757
&lt;/p&gt;
&lt;p&gt;
INSTRUCTEVAL&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#32508;&#21512;&#22871;&#20214;&#65292;&#23427;&#37319;&#21462;&#20102;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35299;&#20915;&#38382;&#39064;&#12289;&#20889;&#20316;&#33021;&#21147;&#21644;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#31561;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#24050;&#32463;&#22312;&#35832;&#22914;&#23545;&#35805;&#20195;&#29702;&#31561;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#65292;&#22914;GPT-4&#65292;&#19981;&#20165;&#33021;&#22815;&#25484;&#25569;&#35821;&#35328;&#65292;&#32780;&#19988;&#21487;&#20197;&#35299;&#20915;&#25968;&#23398;&#12289;&#32534;&#30721;&#12289;&#21307;&#23398;&#21644;&#27861;&#24459;&#31561;&#39046;&#22495;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#35768;&#22810;&#27169;&#22411;&#30340;&#40657;&#30418;&#24615;&#36136;&#21644;&#32570;&#20047;&#20840;&#38754;&#30340;&#35780;&#20272;&#30740;&#31350;&#65292;&#23545;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#20173;&#28982;&#32570;&#20047;&#20840;&#38754;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;INSTRUCTEVAL&#65292;&#19968;&#20010;&#26356;&#20840;&#38754;&#30340;&#35780;&#20272;&#22871;&#20214;&#65292;&#19987;&#38376;&#38024;&#23545;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#19982;&#20197;&#24448;&#30340;&#20316;&#21697;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#21253;&#25324;&#23545;&#27169;&#22411;&#22522;&#20110;&#35299;&#20915;&#38382;&#39064;&#12289;&#20889;&#20316;&#33021;&#21147;&#21644;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#30340;&#20005;&#26684;&#35780;&#20272;&#12290;&#25105;&#20204;&#37319;&#21462;&#20102;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#20998;&#26512;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#30340;&#21508;&#31181;&#22240;&#32032;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#22522;&#30784;&#12289;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#21644;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned large language models have revolutionized natural language processing and have shown great potential in applications such as conversational agents. These models, such as GPT-4, can not only master language but also solve complex tasks in areas like mathematics, coding, medicine, and law. Despite their impressive capabilities, there is still a lack of comprehensive understanding regarding their full potential, primarily due to the black-box nature of many models and the absence of holistic evaluation studies. To address these challenges, we present INSTRUCTEVAL, a more comprehensive evaluation suite designed specifically for instruction-tuned large language models. Unlike previous works, our evaluation involves a rigorous assessment of models based on problem-solving, writing ability, and alignment to human values. We take a holistic approach to analyze various factors affecting model performance, including the pretraining foundation, instruction-tuning data, and train
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#25351;&#20196;&#35843;&#20248;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#24320;&#25918;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20379;&#20102;&#19968;&#32452;&#22823;&#22411;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26550;&#26500;&#23545;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#30340;&#24615;&#33021;&#24433;&#21709;&#24456;&#22823;&#65292;&#38656;&#35201;&#36827;&#34892;&#31934;&#32454;&#30340;&#35843;&#25972;&#21644;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.04751</link><description>&lt;p&gt;
&#39558;&#39548;&#33021;&#36208;&#22810;&#36828;&#65311;&#25506;&#32034;&#24320;&#25918;&#36164;&#28304;&#20013;&#25351;&#20196;&#35843;&#20248;&#30340;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources. (arXiv:2306.04751v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#25351;&#20196;&#35843;&#20248;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#24320;&#25918;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20379;&#20102;&#19968;&#32452;&#22823;&#22411;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26550;&#26500;&#23545;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#30340;&#24615;&#33021;&#24433;&#21709;&#24456;&#22823;&#65292;&#38656;&#35201;&#36827;&#34892;&#31934;&#32454;&#30340;&#35843;&#25972;&#21644;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#25351;&#20196;&#35843;&#20248;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#24320;&#25918;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#23613;&#31649;&#26368;&#36817;&#22768;&#31216;&#24320;&#25918;&#27169;&#22411;&#21487;&#20197;&#19982;&#26368;&#20808;&#36827;&#30340;&#19987;&#26377;&#27169;&#22411;&#30456;&#23218;&#32654;&#65292;&#20294;&#36825;&#20123;&#22768;&#31216;&#24120;&#24120;&#20276;&#38543;&#30528;&#26377;&#38480;&#30340;&#35780;&#20272;&#65292;&#20351;&#24471;&#38590;&#20197;&#20840;&#38754;&#27604;&#36739;&#27169;&#22411;&#24182;&#30830;&#23450;&#21508;&#31181;&#36164;&#28304;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#32452;&#22823;&#22411;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#65292;&#22823;&#23567;&#20026;6.7B&#21040;65B&#20010;&#21442;&#25968;&#65292;&#22312;12&#20010;&#25351;&#20196;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#21253;&#25324;&#25163;&#21160;&#31574;&#21010;&#30340;&#65288;&#20363;&#22914;OpenAssistant&#65289;&#21644;&#32508;&#21512;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;Alpaca&#65289;&#65292;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#33258;&#21160;&#12289;&#22522;&#20110;&#27169;&#22411;&#21644;&#22522;&#20110;&#20154;&#30340;&#25351;&#26631;&#23545;&#20854;&#22312;&#20107;&#23454;&#30693;&#35782;&#12289;&#25512;&#29702;&#12289;&#22810;&#35821;&#35328;&#12289;&#32534;&#30721;&#21644;&#24320;&#25918;&#24335;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#26041;&#38754;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;T\"ulu&#65292;&#25105;&#20204;&#22312;&#39640;&#36136;&#37327;&#24320;&#25918;&#36164;&#28304;&#32452;&#21512;&#19978;&#24494;&#35843;&#30340;&#34920;&#29616;&#26368;&#20339;&#30340;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26550;&#26500;&#23545;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#30340;&#24615;&#33021;&#24433;&#21709;&#24456;&#22823;&#65292;&#38656;&#35201;&#36827;&#34892;&#31934;&#32454;&#30340;&#35843;&#25972;&#21644;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we explore recent advances in instruction-tuning language models on a range of open instruction-following datasets. Despite recent claims that open models can be on par with state-of-the-art proprietary models, these claims are often accompanied by limited evaluation, making it difficult to compare models across the board and determine the utility of various resources. We provide a large set of instruction-tuned models from 6.7B to 65B parameters in size, trained on 12 instruction datasets ranging from manually curated (e.g., OpenAssistant) to synthetic and distilled (e.g., Alpaca) and systematically evaluate them on their factual knowledge, reasoning, multilinguality, coding, and open-ended instruction following abilities through a collection of automatic, model-based, and human-based metrics. We further introduce T\"ulu, our best performing instruction-tuned model suite finetuned on a combination of high-quality open resources.  Our experiments show that different instru
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36755;&#20986;&#36827;&#34892;&#19979;&#28216;&#32479;&#35745;&#20998;&#26512;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#19979;&#28216;&#32479;&#35745;&#25512;&#26029;&#65292;&#24182;&#38477;&#20302;&#26631;&#31614;&#33719;&#21462;&#30340;&#30740;&#31350;&#25104;&#26412;80&#65285;&#65292;&#21516;&#26102;&#20445;&#35777;CSS&#30740;&#31350;&#30340;&#32479;&#35745;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04746</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27880;&#37322;&#36827;&#34892;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#26377;&#25928;&#19979;&#28216;&#32479;&#35745;&#25512;&#26029;: &#22522;&#20110;&#35774;&#35745;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Model Annotations for Valid Downstream Statistical Inference in Social Science: Design-Based Semi-Supervised Learning. (arXiv:2306.04746v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04746
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36755;&#20986;&#36827;&#34892;&#19979;&#28216;&#32479;&#35745;&#20998;&#26512;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#19979;&#28216;&#32479;&#35745;&#25512;&#26029;&#65292;&#24182;&#38477;&#20302;&#26631;&#31614;&#33719;&#21462;&#30340;&#30740;&#31350;&#25104;&#26412;80&#65285;&#65292;&#21516;&#26102;&#20445;&#35777;CSS&#30740;&#31350;&#30340;&#32479;&#35745;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#65288;CSS&#65289;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20998;&#26512;&#25991;&#26723;&#26469;&#35299;&#37322;&#31038;&#20250;&#21644;&#25919;&#27835;&#29616;&#35937;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;CSS&#30740;&#31350;&#20154;&#21592;&#39318;&#20808;&#33719;&#21462;&#25991;&#26723;&#30340;&#26631;&#31614;&#65292;&#28982;&#21518;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#22238;&#24402;&#20998;&#26512;&#26469;&#35299;&#37322;&#26631;&#31614;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#36817;&#36827;&#23637;&#21487;&#20197;&#36890;&#36807;&#22312;&#35268;&#27169;&#19978;&#20415;&#23452;&#22320;&#27880;&#37322;&#25991;&#26723;&#26469;&#38477;&#20302;CSS&#30740;&#31350;&#25104;&#26412;&#65292;&#20294;&#36825;&#20123;&#26367;&#20195;&#26631;&#31614;&#36890;&#24120;&#26159;&#19981;&#23436;&#32654;&#21644;&#26377;&#20559;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;LLMs&#30340;&#36755;&#20986;&#36827;&#34892;&#19979;&#28216;&#32479;&#35745;&#20998;&#26512;&#65292;&#21516;&#26102;&#20445;&#35777;&#19982;CSS&#30740;&#31350;&#22522;&#26412;&#30456;&#20851;&#30340;&#32479;&#35745;&#23646;&#24615;-&#22914;&#28176;&#36817;&#26080;&#20559;&#24615;&#21644;&#27491;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#30452;&#25509;&#22312;&#19979;&#28216;&#32479;&#35745;&#20998;&#26512;&#20013;&#20351;&#29992;LLM&#39044;&#27979;&#30340;&#26367;&#20195;&#26631;&#31614;&#20250;&#23548;&#33268;&#23454;&#36136;&#24615;&#20559;&#24046;&#21644;&#26080;&#25928;&#32622;&#20449;&#21306;&#38388;&#65292;&#21363;&#20351;&#26367;&#20195;&#20934;&#30830;&#24615;&#39640;&#36798;80-90&#65285;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#26080;&#20559;&#26426;&#22120;&#23398;&#20064;&#25552;&#20986;&#20102;&#22522;&#20110;&#35774;&#35745;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;D-SSL&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;LLM&#27880;&#37322;&#19982;&#26377;&#38024;&#23545;&#24615;&#30340;&#37319;&#26679;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#19979;&#28216;&#32479;&#35745;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;&#26631;&#31614;&#33719;&#21462;&#30340;CSS&#30740;&#31350;&#25104;&#26412;&#38477;&#20302;80&#65285;&#65292;&#32780;&#19981;&#24433;&#21709;&#32479;&#35745;&#20998;&#26512;&#30340;&#26377;&#25928;&#24615;&#12290;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#25968;&#25454;&#31034;&#20363;&#34920;&#26126;&#65292;&#19982;&#30452;&#25509;&#20351;&#29992;LLM&#39044;&#27979;&#26631;&#31614;&#30456;&#27604;&#65292;D-SSL&#21487;&#20197;&#23558;&#22238;&#24402;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#22810;&#36798;40&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In computational social science (CSS), researchers analyze documents to explain social and political phenomena. In most scenarios, CSS researchers first obtain labels for documents and then explain labels using interpretable regression analyses in the second step. The recent advancements in large language models (LLMs) can lower costs for CSS research by annotating documents cheaply at scale, but such surrogate labels are often imperfect and biased. We present a new algorithm for using outputs from LLMs for downstream statistical analyses while guaranteeing statistical properties -- like asymptotic unbiasedness and proper uncertainty quantification -- which are fundamental to CSS research. We show that direct use of LLM-predicted surrogate labels in downstream statistical analyses leads to substantial bias and invalid confidence intervals, even with high surrogate accuracy of 80--90\%. To address this, we build on debiased machine learning to propose the design-based semi-supervised le
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#25972;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#65292;&#36991;&#20813;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#23548;&#33268;&#30340;&#20154;&#20026;&#20559;&#24046;&#27880;&#20837;&#12290;&#36890;&#36807;&#20998;&#32452;&#20844;&#24179;&#24615;&#26816;&#26597;&#27169;&#22411;&#23545;&#19981;&#21516;&#25935;&#24863;&#23646;&#24615;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#20102;&#26377;&#36259;&#30340;&#20559;&#24046;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.04735</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36719;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Soft-prompt Tuning for Large Language Models to Evaluate Bias. (arXiv:2306.04735v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#25972;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#65292;&#36991;&#20813;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#23548;&#33268;&#30340;&#20154;&#20026;&#20559;&#24046;&#27880;&#20837;&#12290;&#36890;&#36807;&#20998;&#32452;&#20844;&#24179;&#24615;&#26816;&#26597;&#27169;&#22411;&#23545;&#19981;&#21516;&#25935;&#24863;&#23646;&#24615;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#20102;&#26377;&#36259;&#30340;&#20559;&#24046;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#21151;&#33021;&#22240;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#21363;&#21487;&#20135;&#29983;&#33391;&#22909;&#32467;&#26524;&#32780;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#36827;&#34892;&#25552;&#31034;&#35843;&#25972;&#20197;&#33719;&#24471;&#24341;&#23548;&#26356;&#22909;&#27169;&#22411;&#24615;&#33021;&#30340;&#26368;&#20339;&#25552;&#31034;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#20013;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#25972;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;Open Pre-trained Transformers&#65288;OPT&#65289;&#21644;Galactica&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#12290;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#26159;&#22312;&#21487;&#33021;&#20559;&#21521;&#26576;&#20123;&#20154;&#32676;&#30340;&#30495;&#23454;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#35782;&#21035;&#36825;&#20123;&#28508;&#22312;&#38382;&#39064;&#38750;&#24120;&#37325;&#35201;&#12290;&#20351;&#29992;&#36719;&#25552;&#31034;&#26469;&#35780;&#20272;&#20559;&#24046;&#32473;&#25105;&#20204;&#24102;&#26469;&#20102;&#39069;&#22806;&#30340;&#20248;&#21183;&#65292;&#21487;&#20197;&#36991;&#20813;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#23548;&#33268;&#30340;&#20154;&#20026;&#20559;&#24046;&#27880;&#20837;&#12290;&#25105;&#20204;&#20351;&#29992;&#20998;&#32452;&#20844;&#24179;&#24615;&#65288;&#20559;&#24046;&#65289;&#26816;&#26597;&#27169;&#22411;&#23545;&#19981;&#21516;&#25935;&#24863;&#23646;&#24615;&#30340;&#20559;&#35265;&#65292;&#24182;&#25214;&#21040;&#20102;&#26377;&#36259;&#30340;&#20559;&#24046;&#27169;&#24335;&#12290;&#30001;&#20110;LLMs&#24050;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34987;&#29992;&#20110;&#24037;&#19994;&#20013;&#65292;&#22240;&#27492;&#25105;&#20204;&#23545;&#20854;&#36827;&#34892;&#30340;&#20559;&#35265;&#35780;&#20272;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting large language models has gained immense popularity in recent years due to the advantage of producing good results even without the need for labelled data. However, this requires prompt tuning to get optimal prompts that lead to better model performances. In this paper, we explore the use of soft-prompt tuning on sentiment classification task to quantify the biases of large language models (LLMs) such as Open Pre-trained Transformers (OPT) and Galactica language model. Since these models are trained on real-world data that could be prone to bias toward certain groups of populations, it is important to identify these underlying issues. Using soft-prompts to evaluate bias gives us the extra advantage of avoiding the human-bias injection that can be caused by manually designed prompts. We check the model biases on different sensitive attributes using the group fairness (bias) and find interesting bias patterns. Since LLMs have been used in the industry in various applications, i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Prompter&#26041;&#27861;&#65292;&#21033;&#29992;&#30446;&#26631;&#39046;&#22495;&#27133;&#30340;&#25551;&#36848;&#29983;&#25104;&#21160;&#24577;&#21069;&#32512;&#26469;&#36827;&#34892;&#33258;&#36866;&#24212;&#21069;&#32512;&#35843;&#25972;&#65292;&#23454;&#29616;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;&#36890;&#36807;&#20998;&#26512;&#21457;&#29616;&#65292;Prompter&#38500;&#20102;&#21033;&#29992;&#27133;&#25551;&#36848;&#30340;&#35821;&#20041;&#65292;&#36824;&#33021;&#32771;&#34385;&#21040;&#27133;&#30340;&#39057;&#29575;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21306;&#20998;&#8220;&#26080;&#8221;&#20540;&#23545;&#35805;&#27133;&#26041;&#38754;&#65292;Prompter&#30456;&#27604;&#22522;&#20934;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.04724</link><description>&lt;p&gt;
Prompter:&#29992;&#20110;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#38646;&#26679;&#26412;&#33258;&#36866;&#24212;&#21069;&#32512;
&lt;/p&gt;
&lt;p&gt;
Prompter: Zero-shot Adaptive Prefixes for Dialogue State Tracking Domain Adaptation. (arXiv:2306.04724v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Prompter&#26041;&#27861;&#65292;&#21033;&#29992;&#30446;&#26631;&#39046;&#22495;&#27133;&#30340;&#25551;&#36848;&#29983;&#25104;&#21160;&#24577;&#21069;&#32512;&#26469;&#36827;&#34892;&#33258;&#36866;&#24212;&#21069;&#32512;&#35843;&#25972;&#65292;&#23454;&#29616;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;&#36890;&#36807;&#20998;&#26512;&#21457;&#29616;&#65292;Prompter&#38500;&#20102;&#21033;&#29992;&#27133;&#25551;&#36848;&#30340;&#35821;&#20041;&#65292;&#36824;&#33021;&#32771;&#34385;&#21040;&#27133;&#30340;&#39057;&#29575;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21306;&#20998;&#8220;&#26080;&#8221;&#20540;&#23545;&#35805;&#27133;&#26041;&#38754;&#65292;Prompter&#30456;&#27604;&#22522;&#20934;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#39046;&#22495;&#30340;&#25361;&#25112;&#26159;&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;&#30417;&#30563;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20351;&#27169;&#22411;&#36866;&#24212;&#26032;&#22495;&#65292;&#21363;&#38646;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;&#21442;&#25968;&#39640;&#25928;&#30340;&#36716;&#31227;&#23398;&#20064;&#65288;PETL&#65289;&#20855;&#26377;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#28508;&#21147;&#65292;&#20294;&#23578;&#26410;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#22330;&#26223;&#65292;&#22240;&#20026;&#22914;&#20309;&#26080;&#30417;&#30563;&#22320;&#24212;&#29992;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;Prompter&#20351;&#29992;&#30446;&#26631;&#39046;&#22495;&#27133;&#30340;&#25551;&#36848;&#29983;&#25104;&#21160;&#24577;&#21069;&#32512;&#65292;&#24182;&#23558;&#20854;&#36830;&#25509;&#21040;&#27599;&#20010;&#23618;&#30340;self-attention&#26426;&#21046;&#20013;&#30340;&#38190;&#21644;&#20540;&#19978;&#12290;&#36825;&#20801;&#35768;&#22312;&#38646;&#26679;&#26412;&#20013;&#20351;&#29992;&#21069;&#32512;&#35843;&#25972;&#12290;Prompter&#22312;MultiWOZ&#21644;SGD&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;&#22312;&#29983;&#25104;&#21069;&#32512;&#26102;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;Prompter&#19981;&#20165;&#21033;&#29992;&#20102;&#27133;&#25551;&#36848;&#30340;&#35821;&#20041;&#65292;&#32780;&#19988;&#36824;&#32771;&#34385;&#21040;&#20102;&#23545;&#35805;&#20013;&#27133;&#19968;&#36215;&#20986;&#29616;&#30340;&#39057;&#29575;&#12290;&#27492;&#22806;&#65292;&#19982;&#22522;&#20934;&#30456;&#27604;&#65292;Prompter&#30340;&#25910;&#30410;&#22312;&#20110;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#21306;&#20998;&#8220;&#26080;&#8221;&#20540;&#23545;&#35805;&#27133;&#12290;
&lt;/p&gt;
&lt;p&gt;
A challenge in the Dialogue State Tracking (DST) field is adapting models to new domains without using any supervised data, zero-shot domain adaptation. Parameter-Efficient Transfer Learning (PETL) has the potential to address this problem due to its robustness. However, it has yet to be applied to the zero-shot scenarios, as it is not clear how to apply it unsupervisedly.  Our method, Prompter, uses descriptions of target domain slots to generate dynamic prefixes that are concatenated to the key and values at each layer's self-attention mechanism. This allows for the use of prefix-tuning in zero-shot. Prompter outperforms previous methods on both the MultiWOZ and SGD benchmarks. In generating prefixes, our analyses find that Prompter not only utilizes the semantics of slot descriptions but also how often the slots appear together in conversation. Moreover, Prompter's gains are due to its improved ability to distinguish "none"-valued dialogue slots, compared against baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#34913;&#37327;&#25991;&#26412;&#20869;&#37096;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#40065;&#26834;&#24615;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#65292;&#23637;&#31034;&#20102;&#20154;&#31867;&#25991;&#26412;&#19982;AI&#29983;&#25104;&#25991;&#26412;&#22312;&#20869;&#37096;&#32500;&#24230;&#19978;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.04723</link><description>&lt;p&gt;
&#40065;&#26834;&#24615;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#20869;&#37096;&#32500;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts. (arXiv:2306.04723v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#34913;&#37327;&#25991;&#26412;&#20869;&#37096;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#40065;&#26834;&#24615;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#65292;&#23637;&#31034;&#20102;&#20154;&#31867;&#25991;&#26412;&#19982;AI&#29983;&#25104;&#25991;&#26412;&#22312;&#20869;&#37096;&#32500;&#24230;&#19978;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#25552;&#39640;&#30340;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#20351;&#24471;&#24456;&#38590;&#21306;&#20998;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#31038;&#20250;&#20135;&#29983;&#19981;&#33391;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#31867;&#25991;&#26412;&#30340;&#19981;&#21464;&#23646;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#31867;&#25991;&#26412;&#30340;&#19981;&#21464;&#29305;&#24449;&#65292;&#21363;&#32473;&#23450;&#25991;&#26412;&#26679;&#26412;&#23884;&#20837;&#38598;&#21512;&#19979;&#30340;&#27969;&#24418;&#30340;&#20869;&#37096;&#32500;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#27969;&#30021;&#25991;&#26412;&#30340;&#24179;&#22343;&#20869;&#37096;&#32500;&#24230;&#22312;&#20960;&#20010;&#22522;&#20110;&#23383;&#27597;&#30340;&#35821;&#35328;&#20013;&#32422;&#20026; $9$&#65292;&#32780;&#20013;&#25991;&#32422;&#20026; $7$&#65292;&#32780;&#27599;&#31181;&#35821;&#35328;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#24179;&#22343;&#20869;&#37096;&#32500;&#24230;&#36739;&#20302;&#65292;&#24046;&#32422; $1.5$&#65292;&#24182;&#19988;&#26377;&#26126;&#26174;&#30340;&#32479;&#35745;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapidly increasing quality of AI-generated content makes it difficult to distinguish between human and AI-generated texts, which may lead to undesirable consequences for society. Therefore, it becomes increasingly important to study the properties of human texts that are invariant over text domains and various proficiency of human writers, can be easily calculated for any language, and can robustly separate natural and AI-generated texts regardless of the generation model and sampling method. In this work, we propose such an invariant of human texts, namely the intrinsic dimensionality of the manifold underlying the set of embeddings of a given text sample. We show that the average intrinsic dimensionality of fluent texts in natural language is hovering around the value $9$ for several alphabet-based languages and around $7$ for Chinese, while the average intrinsic dimensionality of AI-generated texts for each language is $\approx 1.5$ lower, with a clear statistical separation between
&lt;/p&gt;</description></item><item><title>BlenderBot 3x&#26159;&#19968;&#20010;&#26356;&#26032;&#29256;&#26412;&#30340;&#20250;&#35805;&#27169;&#22411;&#65292;&#36890;&#36807;&#21442;&#19982;&#32773;&#30340;&#26377;&#26426;&#23545;&#35805;&#21644;&#21453;&#39304;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#25913;&#36827;&#20854;&#25216;&#33021;&#21644;&#23433;&#20840;&#24615;&#65292;&#25216;&#26415;&#19978;&#36890;&#36807;&#23398;&#20064;&#26377;&#30410;&#30340;&#25945;&#24072;&#36991;&#20813;&#23398;&#20064;&#26377;&#27602;&#21453;&#39304;&#12290;</title><link>http://arxiv.org/abs/2306.04707</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#26377;&#26426;&#20132;&#20114;&#65292;&#25913;&#36827;&#24320;&#25918;&#24335;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Open Language Models by Learning from Organic Interactions. (arXiv:2306.04707v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04707
&lt;/p&gt;
&lt;p&gt;
BlenderBot 3x&#26159;&#19968;&#20010;&#26356;&#26032;&#29256;&#26412;&#30340;&#20250;&#35805;&#27169;&#22411;&#65292;&#36890;&#36807;&#21442;&#19982;&#32773;&#30340;&#26377;&#26426;&#23545;&#35805;&#21644;&#21453;&#39304;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#25913;&#36827;&#20854;&#25216;&#33021;&#21644;&#23433;&#20840;&#24615;&#65292;&#25216;&#26415;&#19978;&#36890;&#36807;&#23398;&#20064;&#26377;&#30410;&#30340;&#25945;&#24072;&#36991;&#20813;&#23398;&#20064;&#26377;&#27602;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;BlenderBot 3x&#65292;&#23427;&#26159;&#20250;&#35805;&#27169;&#22411;BlenderBot 3&#30340;&#19968;&#20010;&#26356;&#26032;&#29256;&#26412;&#65292;&#29616;&#22312;&#36890;&#36807;&#21442;&#19982;&#32773;&#30340;&#26377;&#26426;&#23545;&#35805;&#21644;&#21453;&#39304;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#25913;&#36827;&#20854;&#25216;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#21442;&#19982;&#32773;&#21311;&#21517;&#20132;&#20114;&#25968;&#25454;&#65292;&#20379;&#30740;&#31350;&#31038;&#21306;&#20351;&#29992;&#65292;&#20197;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#36827;&#23637;&#12290;&#20351;&#29992;&#26377;&#26426;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#19982;&#20154;&#20204;&#22312;&#8220;&#37326;&#22806;&#8221;&#30340;&#20114;&#21160;&#21253;&#25324;&#39640;&#36136;&#37327;&#30340;&#23545;&#35805;&#21644;&#21453;&#39304;&#65292;&#20197;&#21450;&#23545;&#25239;&#24615;&#21644;&#26377;&#27602;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20123;&#25216;&#26415;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#20174;&#26377;&#30410;&#30340;&#25945;&#24072;&#23398;&#20064;&#65292;&#21516;&#26102;&#36991;&#20813;&#20174;&#35797;&#22270;&#23558;&#27169;&#22411;&#35825;&#23548;&#20026;&#26080;&#29992;&#25110;&#26377;&#27602;&#21453;&#24212;&#30340;&#20154;&#20013;&#23398;&#20064;&#12290;BlenderBot 3x&#22312;&#23545;&#35805;&#20013;&#27604;BlenderBot 3&#26356;&#21463;&#27426;&#36814;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#24773;&#20917;&#19979;&#26174;&#31034;&#20986;&#26356;&#23433;&#20840;&#30340;&#21709;&#24212;&#12290;&#34429;&#28982;&#25105;&#20204;&#30446;&#21069;&#30340;&#27169;&#22411;&#20173;&#36828;&#38750;&#23436;&#32654;&#65292;&#20294;&#25105;&#20204;&#30456;&#20449;&#36890;&#36807;&#32487;&#32493;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#20197;&#21450;&#25506;&#32034;&#20174;&#26377;&#26426;&#20132;&#20114;&#20013;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present BlenderBot 3x, an update on the conversational model BlenderBot 3, which is now trained using organic conversation and feedback data from participating users of the system in order to improve both its skills and safety. We are publicly releasing the participating de-identified interaction data for use by the research community, in order to spur further progress. Training models with organic data is challenging because interactions with people "in the wild" include both high quality conversations and feedback, as well as adversarial and toxic behavior. We study techniques that enable learning from helpful teachers while avoiding learning from people who are trying to trick the model into unhelpful or toxic responses. BlenderBot 3x is both preferred in conversation to BlenderBot 3, and is shown to produce safer responses in challenging situations. While our current models are still far from perfect, we believe further improvement can be achieved by continued use of the techniq
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;ConceptBed&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;CCD&#65292;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#27010;&#24565;&#23398;&#20064;&#21644;&#21512;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.04695</link><description>&lt;p&gt;
ConceptBed: &#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#27010;&#24565;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ConceptBed: Evaluating Concept Learning Abilities of Text-to-Image Diffusion Models. (arXiv:2306.04695v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;ConceptBed&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;CCD&#65292;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#27010;&#24565;&#23398;&#20064;&#21644;&#21512;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#35270;&#35273;&#27010;&#24565;&#24182;&#20174;&#22270;&#20687;&#20013;&#22797;&#21046;&#21644;&#32452;&#21512;&#36825;&#20123;&#27010;&#24565;&#30340;&#33021;&#21147;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#19968;&#20010;&#26680;&#24515;&#30446;&#26631;&#12290;&#26368;&#36817;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#36890;&#36807;&#23398;&#20064;&#22823;&#37327;&#22270;&#20687;&#21450;&#20854;&#25551;&#36848;&#26469;&#29983;&#25104;&#39640;&#28165;&#26224;&#24230;&#21644;&#36924;&#30495;&#30340;&#22270;&#20687;&#36136;&#37327;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;T2I&#27169;&#22411;&#30340;&#37325;&#28857;&#22312;&#20110;&#29031;&#29255;&#33324;&#30340;&#30495;&#23454;&#24863;&#21644;&#26377;&#38480;&#30340;&#35270;&#35273;&#29702;&#35299;&#23450;&#24615;&#37327;&#24230;&#12290;&#20026;&#20102;&#37327;&#21270;T2I&#27169;&#22411;&#22312;&#23398;&#20064;&#21644;&#21512;&#25104;&#26032;&#30340;&#35270;&#35273;&#27010;&#24565;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ConceptBed&#65292;&#19968;&#20010;&#21253;&#21547;284&#20010;&#29420;&#29305;&#35270;&#35273;&#27010;&#24565;&#12289;5K&#20010;&#29420;&#29305;&#27010;&#24565;&#32452;&#21512;&#21644;33K&#20010;&#32452;&#21512;&#25991;&#26412;&#25552;&#31034;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#38500;&#20102;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#25351;&#26631;Concept Confidence Deviation&#65288;CCD&#65289;&#65292;&#23427;&#21033;&#29992;oracle&#27010;&#24565;&#20998;&#31867;&#22120;&#30340;&#32622;&#20449;&#24230;&#26469;&#34913;&#37327;T2I&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#27010;&#24565;&#19982;&#22320;&#38754;&#30495;&#23454;&#22270;&#20687;&#20013;&#21253;&#21547;&#30340;&#27010;&#24565;&#20043;&#38388;&#30340;&#23545;&#40784;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#30340;&#35270;&#35273;&#27010;&#24565;&#26159;&#23545;&#35937;&#25110;&#32773;...
&lt;/p&gt;
&lt;p&gt;
The ability to understand visual concepts and replicate and compose these concepts from images is a central goal for computer vision. Recent advances in text-to-image (T2I) models have lead to high definition and realistic image quality generation by learning from large databases of images and their descriptions. However, the evaluation of T2I models has focused on photorealism and limited qualitative measures of visual understanding. To quantify the ability of T2I models in learning and synthesizing novel visual concepts, we introduce ConceptBed, a large-scale dataset that consists of 284 unique visual concepts, 5K unique concept compositions, and 33K composite text prompts. Along with the dataset, we propose an evaluation metric, Concept Confidence Deviation (CCD), that uses the confidence of oracle concept classifiers to measure the alignment between concepts generated by T2I generators and concepts contained in ground truth images. We evaluate visual concepts that are either object
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27880;&#20837;&#24120;&#35782;&#30693;&#35782;&#30340;&#20849;&#24773;&#24335;&#23545;&#35805;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#36866;&#24212;&#27169;&#22359;&#36873;&#25321;&#24120;&#35782;&#30693;&#35782;&#20197;&#30830;&#20445;&#29983;&#25104;&#30340;&#23545;&#35805;&#22238;&#24212;&#21644;&#35828;&#35805;&#32773;&#24773;&#20917;&#30340;&#19968;&#33268;&#24615;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.04657</link><description>&lt;p&gt;
&#21160;&#24577;&#27880;&#20837;&#24120;&#35782;&#30693;&#35782;&#25552;&#21319;&#20849;&#24773;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Improving Empathetic Dialogue Generation by Dynamically Infusing Commonsense Knowledge. (arXiv:2306.04657v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27880;&#20837;&#24120;&#35782;&#30693;&#35782;&#30340;&#20849;&#24773;&#24335;&#23545;&#35805;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#36866;&#24212;&#27169;&#22359;&#36873;&#25321;&#24120;&#35782;&#30693;&#35782;&#20197;&#30830;&#20445;&#29983;&#25104;&#30340;&#23545;&#35805;&#22238;&#24212;&#21644;&#35828;&#35805;&#32773;&#24773;&#20917;&#30340;&#19968;&#33268;&#24615;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20849;&#24773;&#23545;&#35805;&#20013;&#65292;&#20010;&#20307;&#34920;&#36798;&#23545;&#20182;&#20154;&#30340;&#20849;&#24773;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#20027;&#35201;&#20381;&#38752;&#35828;&#35805;&#20154;&#30340;&#24773;&#24863;&#29983;&#25104;&#20849;&#24773;&#24335;&#22238;&#24212;&#12290;&#27492;&#22806;&#65292;&#22806;&#37096;&#30340;&#24120;&#35782;&#30693;&#35782;&#20063;&#21487;&#29992;&#20110;&#22686;&#24378;&#31995;&#32479;&#23545;&#35821;&#22659;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#24120;&#35782;&#30693;&#35782;&#24211;&#20013;&#21253;&#21547;&#21508;&#31181;&#20851;&#31995;&#65292;&#21487;&#33021;&#23548;&#33268;&#23545;&#35805;&#31995;&#32479;&#30340;&#28151;&#28102;&#12290;&#22240;&#27492;&#65292;&#24773;&#24863;&#12289;&#29983;&#25104;&#30340;&#22238;&#24212;&#21644;&#35828;&#35805;&#32773;&#30340;&#24773;&#22659;&#20449;&#24687;&#20043;&#38388;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20849;&#24773;&#24335;&#22238;&#24212;&#29983;&#25104;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#33258;&#36866;&#24212;&#27169;&#22359;&#29992;&#20110;&#24120;&#35782;&#30693;&#35782;&#36873;&#25321;&#65292;&#20197;&#30830;&#20445;&#25152;&#29983;&#25104;&#30340;&#20849;&#24773;&#24335;&#22238;&#24212;&#21644;&#35828;&#35805;&#32773;&#30340;&#24773;&#20917;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#36873;&#25321;&#30340;&#30693;&#35782;&#29992;&#20110;&#35843;&#25972;&#29983;&#25104;&#30340;&#21709;&#24212;&#30340;&#24120;&#35782;&#35748;&#30693;&#21644;&#20849;&#24773;&#34920;&#36798;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In empathetic conversations, individuals express their empathy towards others. Previous work has mainly focused on generating empathetic responses by utilizing the speaker's emotion. Besides, external commonsense knowledge has been applied to enhance the system's understandings of the speaker's situation. However, given an event, commonsense knowledge base contains various relations, potentially leading to confusion for the dialogue system. Consequently, inconsistencies arise among the emotion, generated response and speaker's contextual information. To this end, we propose a novel approach for empathetic response generation, which incorporates an adaptive module for commonsense knowledge selection to ensure consistency between the generated empathetic responses and the speaker's situation. This selected knowledge is used to refine the commonsense cognition and empathy expression for generated responses. Experimental results show that our approach significantly outperforms baseline mod
&lt;/p&gt;</description></item><item><title>M$^3$IT&#25968;&#25454;&#38598;&#26088;&#22312;&#20248;&#21270;&#24320;&#25918;&#24335;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#19982;&#20154;&#31867;&#25351;&#20196;&#30340;&#23545;&#40784;&#65292;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#22810;&#27169;&#24577;&#21644;&#22810;&#35821;&#31181;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.04387</link><description>&lt;p&gt;
M$^3$IT: &#22810;&#27169;&#24577;&#22810;&#35821;&#31181;&#25351;&#20196;&#35843;&#25972;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
M$^3$IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning. (arXiv:2306.04387v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04387
&lt;/p&gt;
&lt;p&gt;
M$^3$IT&#25968;&#25454;&#38598;&#26088;&#22312;&#20248;&#21270;&#24320;&#25918;&#24335;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#19982;&#20154;&#31867;&#25351;&#20196;&#30340;&#23545;&#40784;&#65292;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#22810;&#27169;&#24577;&#21644;&#22810;&#35821;&#31181;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#24050;&#32463;&#26174;&#33879;&#25512;&#36827;&#20102;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#25351;&#20196;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#65292;&#24320;&#25918;&#24335;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#30340;&#36827;&#23637;&#19968;&#30452;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#24182;&#20419;&#36827;&#35270;&#35273;&#35821;&#35328;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22810;&#27169;&#24577;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#25972;&#65288;M$^3$IT&#65289;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20248;&#21270;VLM&#19982;&#20154;&#31867;&#25351;&#20196;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;M$^3$IT&#25968;&#25454;&#38598;&#21253;&#25324;40&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;240&#19975;&#20010;&#23454;&#20363;&#21644;400&#20010;&#25163;&#21160;&#32534;&#20889;&#30340;&#20219;&#21153;&#25351;&#20196;&#65292;&#26684;&#24335;&#21270;&#20026;&#35270;&#35273;&#21040;&#25991;&#26412;&#32467;&#26500;&#12290;&#37325;&#35201;&#20219;&#21153;&#34987;&#32763;&#35793;&#25104;80&#31181;&#35821;&#35328;&#65292;&#20351;&#29992;&#20808;&#36827;&#30340;&#32763;&#35793;&#31995;&#32479;&#65292;&#30830;&#20445;&#26356;&#24191;&#27867;&#30340;&#21487;&#35775;&#38382;&#24615;&#12290;M$^3$IT&#22312;&#20219;&#21153;&#35206;&#30422;&#33539;&#22260;&#12289;&#25351;&#20196;&#25968;&#37327;&#21644;&#23454;&#20363;&#35268;&#27169;&#26041;&#38754;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Ying-VLM&#65292;&#23427;&#26159;&#22312;&#25105;&#20204;&#30340;M$^3$IT&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;VLM&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#28508;&#22312;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning has significantly advanced large language models (LLMs) such as ChatGPT, enabling them to align with human instructions across diverse tasks. However, progress in open vision-language models (VLMs) has been limited due to the scarcity of high-quality instruction datasets. To tackle this challenge and promote research in the vision-language field, we introduce the Multi-Modal, Multilingual Instruction Tuning (M$^3$IT) dataset, designed to optimize VLM alignment with human instructions. Our M$^3$IT dataset comprises 40 carefully curated datasets, including 2.4 million instances and 400 manually written task instructions, reformatted into a vision-to-text structure. Key tasks are translated into 80 languages with an advanced translation system, ensuring broader accessibility. M$^3$IT surpasses previous datasets regarding task coverage, instruction number and instance scale. Moreover, we develop Ying-VLM, a VLM model trained on our M$^3$IT dataset, showcasing its potenti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25968;&#25454;&#26631;&#27880;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;-&#24674;&#22797;&#33539;&#24335;&#19982;GPT&#30340;&#19968;&#27425;&#24615;&#23398;&#20064;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#25968;&#25454;&#26631;&#27880;&#30340;&#35206;&#30422;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#26131;&#35835;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04349</link><description>&lt;p&gt;
GPT&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#25968;&#25454;&#26631;&#27880;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
GPT Self-Supervision for a Better Data Annotator. (arXiv:2306.04349v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25968;&#25454;&#26631;&#27880;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;-&#24674;&#22797;&#33539;&#24335;&#19982;GPT&#30340;&#19968;&#27425;&#24615;&#23398;&#20064;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#25968;&#25454;&#26631;&#27880;&#30340;&#35206;&#30422;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#26131;&#35835;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#65292;&#23558;&#25968;&#25454;&#27880;&#37322;&#20026;&#31616;&#27905;&#30340;&#25688;&#35201;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#24120;&#24120;&#38656;&#35201;&#20154;&#31867;&#19987;&#23478;&#25237;&#20837;&#22823;&#37327;&#26102;&#38388;&#21644;&#19987;&#19994;&#30693;&#35782;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#24050;&#32463;&#26377;&#25152;&#23581;&#35797;&#65292;&#20294;&#20173;&#23384;&#22312;&#35832;&#22914;&#19981;&#36866;&#29992;&#20110;&#26080;&#26631;&#31614;&#25968;&#25454;&#12289;&#32570;&#20047;&#33258;&#30417;&#30563;&#26041;&#27861;&#12289;&#32570;&#20047;&#23545;&#22797;&#26434;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#20851;&#27880;&#31561;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25968;&#25454;&#26631;&#27880;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#29983;&#25104;-&#24674;&#22797;&#33539;&#24335;&#65292;&#21033;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#20013;&#30340;&#19968;&#27425;&#24615;&#23398;&#20064;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#19968;&#27425;&#24615;&#35843;&#25972;&#38454;&#27573;&#21644;&#29983;&#25104;&#38454;&#27573;&#12290;&#22312;&#19968;&#27425;&#24615;&#35843;&#25972;&#38454;&#27573;&#65292;&#25105;&#20204;&#20174;&#25903;&#25345;&#38598;&#20013;&#25277;&#21462;&#25968;&#25454;&#20316;&#20026;GPT&#29983;&#25104;&#25991;&#26412;&#25688;&#35201;&#30340;&#19968;&#37096;&#20998;&#65292;&#28982;&#21518;&#29992;&#35813;&#25688;&#35201;&#24674;&#22797;&#21407;&#22987;&#25968;&#25454;&#12290;&#24674;&#22797;&#25968;&#25454;&#19982;&#21407;&#22987;&#25968;&#25454;&#30340;&#23545;&#40784;&#20998;&#25968;&#29992;&#20110;&#24494;&#35843;GPT&#26435;&#37325;&#12290;&#22312;&#29983;&#25104;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#30456;&#21516;&#22522;&#20110;GPT&#30340;&#26041;&#27861;&#20026;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#26032;&#30340;&#25991;&#26412;&#25688;&#35201;&#12290;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35206;&#30422;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#26131;&#35835;&#24615;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#27604;&#29616;&#26377;&#30340;&#26631;&#27880;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of annotating data into concise summaries poses a significant challenge across various domains, frequently requiring the allocation of significant time and specialized knowledge by human experts. Despite existing efforts to use large language models for annotation tasks, significant problems such as limited applicability to unlabeled data, the absence of self-supervised methods, and the lack of focus on complex structured data still persist. In this work, we propose a GPT self-supervision annotation method. This method embodies a generating-recovering paradigm that leverages the capabilities of one-shot learning capabilities in Generative Pretrained Transformer (GPT). The proposed approach comprises a one-shot tuning phase followed by a generation phase. In the one-shot tuning phase, we sample a data from the support set as part of the prompt for GPT to generate a textual summary, which is then used to recover the original data. The alignment score between the recovered and or
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102; CMExam &#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#32508;&#21512;&#30340;&#12289;&#26469;&#33258;&#20110;&#20013;&#22269;&#22269;&#23478;&#21307;&#30103;&#25191;&#19994;&#32771;&#35797;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#19988;&#23458;&#35266;&#30340;&#26041;&#27861;&#12290;&#22312; CMExam &#19978;&#65292;GPT-4 &#34920;&#29616;&#26368;&#22909;&#65292;&#36825;&#34920;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#39046;&#22495;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.03030</link><description>&lt;p&gt;
&#22522;&#20110; CMExam &#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#27979;&#8212;&#8212;&#19968;&#20221;&#32508;&#21512;&#30340;&#20013;&#22269;&#21307;&#23398;&#32771;&#35797;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Large Language Models on CMExam -- A Comprehensive Chinese Medical Exam Dataset. (arXiv:2306.03030v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03030
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102; CMExam &#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#32508;&#21512;&#30340;&#12289;&#26469;&#33258;&#20110;&#20013;&#22269;&#22269;&#23478;&#21307;&#30103;&#25191;&#19994;&#32771;&#35797;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#19988;&#23458;&#35266;&#30340;&#26041;&#27861;&#12290;&#22312; CMExam &#19978;&#65292;GPT-4 &#34920;&#29616;&#26368;&#22909;&#65292;&#36825;&#34920;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#39046;&#22495;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#30340;&#36827;&#27493;&#24050;&#32463;&#25913;&#21464;&#20102;&#38382;&#31572;&#39046;&#22495;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26631;&#20934;&#21270;&#21644;&#20840;&#38754;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#23545; LLM &#36827;&#34892;&#35780;&#20272;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; CMExam&#65292;&#23427;&#26469;&#33258;&#20013;&#22269;&#22269;&#23478;&#21307;&#30103;&#25191;&#19994;&#32771;&#35797;&#65292;&#30001;60,000&#22810;&#20010;&#36873;&#25321;&#39064;&#21644;&#27169;&#22411;&#25512;&#29702;&#35299;&#37322;&#30340;&#31572;&#26696;&#35299;&#26512;&#26500;&#25104;&#65292;&#21487;&#36827;&#34892;&#26631;&#20934;&#21270;&#21644;&#23458;&#35266;&#21270;&#30340;&#35780;&#20272;&#12290;&#20026;&#20102;&#28145;&#20837;&#20998;&#26512; LLM&#65292;&#25105;&#20204;&#36992;&#35831;&#21307;&#23398;&#19987;&#19994;&#20154;&#22763;&#23545;&#20116;&#20010;&#39069;&#22806;&#30340;&#38382;&#39064;&#36880;&#20010;&#36827;&#34892;&#26631;&#27880;&#65292;&#21253;&#25324;&#30142;&#30149;&#32452;&#12289;&#20020;&#24202;&#31185;&#23460;&#12289;&#21307;&#23398;&#23398;&#31185;&#12289;&#33021;&#21147;&#39046;&#22495;&#21644;&#38590;&#24230;&#32423;&#21035;&#12290;&#38500;&#20102;&#25968;&#25454;&#38598;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312; CMExam &#19978;&#23545;&#20195;&#34920;&#24615;&#30340; LLM &#21644; QA &#31639;&#27861;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4 &#30340;&#20934;&#30830;&#24230;&#26368;&#39640;&#65292;&#20026;61.6&#65285;&#65292;&#21152;&#26435; F1 &#20998;&#25968;&#20026;0.617&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#26174;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#24040;&#22823;&#28508;&#21147;&#20197;&#21450;&#26631;&#20934;&#21270;&#25968;&#25454;&#38598;&#23545;&#35780;&#20272;&#20854;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have transformed the field of question answering (QA). However, evaluating LLMs in the medical field is challenging due to the lack of standardized and comprehensive datasets. To address this gap, we introduce CMExam, sourced from the Chinese National Medical Licensing Examination. CMExam consists of 60K+ multiple-choice questions for standardized and objective evaluations, as well as solution explanations for model reasoning evaluation in an open-ended manner. For in-depth analyses of LLMs, we invited medical professionals to label five additional question-wise annotations, including disease groups, clinical departments, medical disciplines, areas of competency, and question difficulty levels. Alongside the dataset, we further conducted thorough experiments with representative LLMs and QA algorithms on CMExam. The results show that GPT-4 had the best accuracy of 61.6% and a weighted F1 score of 0.617. These results highlight a great 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#27169;&#25311;&#26041;&#27861;CFCRS&#65292;&#20197;&#32531;&#35299;&#30001;&#20110;&#25968;&#25454;&#19981;&#36275;&#32780;&#23548;&#33268;&#30340;&#35757;&#32451;&#19981;&#36275;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.02842</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#25968;&#25454;&#27169;&#25311;&#25552;&#39640;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Improving Conversational Recommendation Systems via Counterfactual Data Simulation. (arXiv:2306.02842v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#27169;&#25311;&#26041;&#27861;CFCRS&#65292;&#20197;&#32531;&#35299;&#30001;&#20110;&#25968;&#25454;&#19981;&#36275;&#32780;&#23548;&#33268;&#30340;&#35757;&#32451;&#19981;&#36275;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#65288;CRSs&#65289;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#25552;&#20379;&#25512;&#33616;&#26381;&#21153;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#22810;&#31181;&#26041;&#27861;&#29992;&#20110;&#24320;&#21457;&#26377;&#33021;&#21147;&#30340;CRSs&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#36275;&#22815;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#30001;&#20110;&#38590;&#20197;&#27880;&#37322;&#38754;&#21521;&#25512;&#33616;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#29616;&#26377;&#30340;CRSs&#26041;&#27861;&#36890;&#24120;&#22240;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#32780;&#21463;&#21040;&#19981;&#36275;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CFCRS&#30340;CRS&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#27169;&#25311;&#26041;&#27861;&#65292;&#20197;&#20943;&#32531;CRSs&#20013;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#24320;&#21457;&#30340;&#65292;&#35813;&#26694;&#26550;&#36880;&#27493;&#23558;&#37325;&#20889;&#21040;&#30495;&#23454;&#23545;&#35805;&#20013;&#30340;&#29992;&#25143;&#20559;&#22909;&#20013;&#65292;&#32780;&#19981;&#24178;&#25200;&#25972;&#20010;&#23545;&#35805;&#27969;&#31243;&#12290;&#20026;&#20102;&#24320;&#21457;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36890;&#36807;&#28041;&#21450;&#23545;&#35805;&#30340;&#23454;&#20307;&#26469;&#23545;&#29992;&#25143;&#20559;&#22909;&#36827;&#34892;&#34920;&#24449;&#24182;&#32452;&#32455;&#23545;&#35805;&#27969;&#31243;&#65292;&#24182;&#35774;&#35745;&#20102;&#22810;&#38454;&#27573;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational recommender systems (CRSs) aim to provide recommendation services via natural language conversations. Although a number of approaches have been proposed for developing capable CRSs, they typically rely on sufficient training data for training. Since it is difficult to annotate recommendation-oriented dialogue datasets, existing CRS approaches often suffer from the issue of insufficient training due to the scarcity of training data. To address this issue, in this paper, we propose a CounterFactual data simulation approach for CRS, named CFCRS, to alleviate the issue of data scarcity in CRSs. Our approach is developed based on the framework of counterfactual data augmentation, which gradually incorporates the rewriting to the user preference from a real dialogue without interfering with the entire conversation flow. To develop our approach, we characterize user preference and organize the conversation flow by the entities involved in the dialogue, and design a multi-stage 
&lt;/p&gt;</description></item><item><title>&#32479;&#19968;&#27450;&#35784;&#35821;&#26009;&#24211;&#20026;&#26500;&#24314;&#36328;&#22495;&#27867;&#21270;&#30340;&#27450;&#35784;&#26816;&#27979;&#31995;&#32479;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2306.02827</link><description>&lt;p&gt;
UNIDECOR: &#29992;&#20110;&#36328;&#35821;&#26009;&#27450;&#35784;&#26816;&#27979;&#30340;&#32479;&#19968;&#27450;&#35784;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
UNIDECOR: A Unified Deception Corpus for Cross-Corpus Deception Detection. (arXiv:2306.02827v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02827
&lt;/p&gt;
&lt;p&gt;
&#32479;&#19968;&#27450;&#35784;&#35821;&#26009;&#24211;&#20026;&#26500;&#24314;&#36328;&#22495;&#27867;&#21270;&#30340;&#27450;&#35784;&#26816;&#27979;&#31995;&#32479;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#35821;&#35328;&#23398;&#12289;&#24515;&#29702;&#23398;&#21644;&#27861;&#21307;&#23398;&#30740;&#31350;&#20102;&#21475;&#22836;&#27450;&#39575;&#30340;&#35768;&#22810;&#26041;&#38754;&#65292;&#26088;&#22312;&#29702;&#35299;&#34892;&#20026;&#27169;&#24335;&#12289;&#37492;&#21035;&#34394;&#20551;&#35777;&#35328;&#21644;&#26816;&#27979;&#22312;&#32447;&#20132;&#27969;&#20013;&#30340;&#27450;&#35784;&#34892;&#20026;&#12290;&#19981;&#21516;&#30740;&#31350;&#39046;&#22495;&#23545;&#21160;&#26426;&#30340;&#19981;&#21516;&#36873;&#25321;&#23548;&#33268;&#27450;&#35784;&#27010;&#24565;&#30340;&#19981;&#21516;&#65292;&#20351;&#24471;&#27604;&#36739;&#27169;&#22411;&#21644;&#20026;&#29305;&#23450;&#35821;&#35328;&#26500;&#24314;&#24378;&#22823;&#30340;&#27450;&#35784;&#26816;&#27979;&#31995;&#32479;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#36890;&#36807;&#35843;&#26597;&#21253;&#25324;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#12289;&#27861;&#24237;&#35777;&#35328;&#12289;&#29305;&#23450;&#35805;&#39064;&#19978;&#30340;&#24847;&#35265;&#38472;&#36848;&#21644;&#22312;&#32447;&#31574;&#30053;&#28216;&#25103;&#20013;&#30340;&#27450;&#39575;&#23545;&#35805;&#31561;&#19981;&#21516;&#39046;&#22495;&#30340;&#21487;&#29992;&#33521;&#35821;&#27450;&#35784;&#25968;&#25454;&#38598;&#65292;&#23558;&#36825;&#20123;&#25968;&#25454;&#38598;&#32479;&#19968;&#21040;&#19968;&#20010;&#35821;&#26009;&#24211;&#20013;&#12290;&#22522;&#20110;&#36825;&#20010;&#36164;&#28304;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#27450;&#35784;&#35821;&#35328;&#25552;&#31034;&#30340;&#20132;&#21449;&#35821;&#26009;&#24314;&#27169;&#23454;&#39564;&#21644;&#30456;&#20851;&#24615;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;&#29992;&#32479;&#19968;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#35757;&#32451;&#21487;&#23454;&#29616;&#36328;&#22495;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Verbal deception has been studied in psychology, forensics, and computational linguistics for a variety of reasons, like understanding behaviour patterns, identifying false testimonies, and detecting deception in online communication. Varying motivations across research fields lead to differences in the domain choices to study and in the conceptualization of deception, making it hard to compare models and build robust deception detection systems for a given language. With this paper, we improve this situation by surveying available English deception datasets which include domains like social media reviews, court testimonials, opinion statements on specific topics, and deceptive dialogues from online strategy games. We consolidate these datasets into a single unified corpus. Based on this resource, we conduct a correlation analysis of linguistic cues of deception across datasets to understand the differences and perform cross-corpus modeling experiments which show that a cross-domain ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#20064;&#24471;&#21451;&#22909;&#22411;&#22522;&#20934;&#65292;&#20197;&#26816;&#39564;&#33258;&#25105;&#30417;&#30563;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#22312;&#20799;&#31461;&#35789;&#27719;&#21644;&#21477;&#27861;&#32463;&#21382;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#38656;&#35201;&#35299;&#20915;&#30340;&#25361;&#25112;&#65306;&#25991;&#26412;&#21644;&#35821;&#38899;&#20043;&#38388;&#30340;&#24046;&#36317;&#21644;&#24178;&#20928;&#35821;&#38899;&#21644;&#37326;&#22806;&#35821;&#38899;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2306.01506</link><description>&lt;p&gt;
BabySLM: &#33258;&#25105;&#30417;&#30563;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#20064;&#24471;&#21451;&#22909;&#22411;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BabySLM: language-acquisition-friendly benchmark of self-supervised spoken language models. (arXiv:2306.01506v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#20064;&#24471;&#21451;&#22909;&#22411;&#22522;&#20934;&#65292;&#20197;&#26816;&#39564;&#33258;&#25105;&#30417;&#30563;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#22312;&#20799;&#31461;&#35789;&#27719;&#21644;&#21477;&#27861;&#32463;&#21382;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#38656;&#35201;&#35299;&#20915;&#30340;&#25361;&#25112;&#65306;&#25991;&#26412;&#21644;&#35821;&#38899;&#20043;&#38388;&#30340;&#24046;&#36317;&#21644;&#24178;&#20928;&#35821;&#38899;&#21644;&#37326;&#22806;&#35821;&#38899;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#65292;&#23398;&#20064;&#35821;&#38899;&#34920;&#31034;&#30340;&#33258;&#25105;&#30417;&#30563;&#25216;&#26415;&#33021;&#22815;&#20174;&#21548;&#21040;&#30340;&#35821;&#38899;&#20013;&#21457;&#23637;&#20986;&#35821;&#35328;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#20154;&#31867;&#26631;&#31614;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#36825;&#20123;&#26041;&#27861;&#30340;&#28508;&#21147;&#24182;&#36827;&#19968;&#27493;&#20102;&#35299;&#23156;&#20799;&#23398;&#20064;&#35821;&#35328;&#30340;&#26041;&#24335;&#65292;&#27169;&#25311;&#24517;&#39035;&#32039;&#23494;&#27169;&#20223;&#29616;&#23454;&#24773;&#20917;&#65292;&#36890;&#36807;&#22312;&#24320;&#21457;&#19978;&#31526;&#21512;&#20799;&#31461;&#35821;&#35328;&#32463;&#39564;&#20856;&#22411;&#35789;&#27719;&#24211;&#21644;&#23545;&#24212;&#27979;&#35797;&#38598;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#22312;&#35789;&#27719;&#21644;&#21477;&#27861;&#23618;&#38754;&#19978;&#30340;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#20064;&#24471;&#21451;&#22909;&#22411;&#22522;&#20934;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#27492;&#22522;&#20934;&#65292;&#24182;&#24635;&#32467;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#35777;&#26126;&#20854;&#26377;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#38656;&#35201;&#35299;&#20915;&#30340;&#20004;&#20010;&#25361;&#25112;&#65306;&#22635;&#34917;&#25991;&#26412;&#21644;&#35821;&#38899;&#20043;&#38388;&#20197;&#21450;&#24178;&#20928;&#35821;&#38899;&#21644;&#37326;&#22806;&#35821;&#38899;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised techniques for learning speech representations have been shown to develop linguistic competence from exposure to speech without the need for human labels. In order to fully realize the potential of these approaches and further our understanding of how infants learn language, simulations must closely emulate real-life situations by training on developmentally plausible corpora and benchmarking against appropriate test sets. To this end, we propose a language-acquisition-friendly benchmark to probe spoken language models at the lexical and syntactic levels, both of which are compatible with the vocabulary typical of children's language experiences. This paper introduces the benchmark and summarizes a range of experiments showing its usefulness. In addition, we highlight two exciting challenges that need to be addressed for further progress: bridging the gap between text and speech and between clean speech and in-the-wild speech.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20351;&#29992;GPT-4&#35299;&#20915;&#26356;&#22797;&#26434;&#21644;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MathChat&#30340;&#23545;&#35805;&#24335;&#38382;&#39064;&#27714;&#35299;&#26694;&#26550;&#65292;&#24182;&#22312;&#22256;&#38590;&#39640;&#20013;&#31454;&#36187;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.01337</link><description>&lt;p&gt;
&#22522;&#20110;GPT-4&#30340;&#22797;&#26434;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on Challenging Math Problem Solving with GPT-4. (arXiv:2306.01337v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20351;&#29992;GPT-4&#35299;&#20915;&#26356;&#22797;&#26434;&#21644;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MathChat&#30340;&#23545;&#35805;&#24335;&#38382;&#39064;&#27714;&#35299;&#26694;&#26550;&#65292;&#24182;&#22312;&#22256;&#38590;&#39640;&#20013;&#31454;&#36187;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26159;&#19968;&#39033;&#26377;&#36259;&#30340;&#30740;&#31350;&#65292;&#32771;&#34385;&#21040;&#22312;&#21508;&#31181;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#25968;&#23398;&#38382;&#39064;&#30340;&#20016;&#23500;&#24615;&#12290;&#34429;&#28982;&#20043;&#21069;&#26377;&#20960;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#20351;&#29992;LLM&#35299;&#20915;&#21021;&#31561;&#25968;&#23398;&#38382;&#39064;&#65292;&#20294;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;GPT-4&#35299;&#20915;&#26356;&#22797;&#26434;&#21644;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#30340;&#21069;&#27839;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20351;&#29992;GPT-4&#30340;&#21508;&#31181;&#26041;&#27861;&#12290;&#20854;&#20013;&#19968;&#20123;&#26159;&#20174;&#29616;&#26377;&#24037;&#20316;&#20013;&#25913;&#32534;&#32780;&#26469;&#30340;&#65292;&#20854;&#20013;&#19968;&#20010;&#26159;MathChat&#65292;&#36825;&#26159;&#26412;&#30740;&#31350;&#26032;&#25552;&#20986;&#30340;&#19968;&#31181;&#23545;&#35805;&#24335;&#38382;&#39064;&#27714;&#35299;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;MATH&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#39640;&#20013;&#31454;&#36187;&#38382;&#39064;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#23545;&#35805;&#24335;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Employing Large Language Models (LLMs) to address mathematical problems is an intriguing research endeavor, considering the abundance of math problems expressed in natural language across numerous science and engineering fields. While several prior works have investigated solving elementary mathematics using LLMs, this work explores the frontier of using GPT-4 for solving more complex and challenging math problems. We evaluate various ways of using GPT-4. Some of them are adapted from existing work, and one is \MathChat, a conversational problem-solving framework newly proposed in this work. We perform the evaluation on difficult high school competition problems from the MATH dataset, which shows the advantage of the proposed conversational approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#12290;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.18486</link><description>&lt;p&gt;
&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#31995;&#32479;&#30740;&#31350;&#21644;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. (arXiv:2305.18486v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#12290;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22914; ChatGPT &#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24320;&#21457;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38590;&#20197;&#23558;&#35813;&#27169;&#22411;&#29983;&#25104;&#30340;&#20135;&#20986;&#19982;&#22522;&#26412;&#20107;&#23454;&#36827;&#34892;&#27604;&#36739;&#65292;&#22240;&#27492;&#20854;&#22312;&#22522;&#20934;&#23398;&#26415;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#23545; ChatGPT &#22312;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#24443;&#24213;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312; 140 &#20010;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102; ChatGPT&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#30340; 255K &#27425;&#21709;&#24212;&#65292;&#36825;&#20351;&#25105;&#20204;&#30340;&#24037;&#20316;&#25104;&#20026;&#20102;&#22312; NLP &#22522;&#20934;&#27979;&#35797;&#20013;&#23545; ChatGPT &#36827;&#34892;&#30340;&#26368;&#22823;&#35780;&#20272;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992; LLM &#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#19968;&#31181;&#26032;&#30340;&#36856;&#21457;&#33021;&#21147;&#65292;&#21363;&#36981;&#24490;&#22810;&#20010;&#26597;&#35810;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In this paper, we aim to present a thorough evaluation of ChatGPT's performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense reasoning, mathematical problem-solving, machine translation, bias detection, and ethical considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze 255K responses it generates in these datasets. This makes our work the largest evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate the strengths and weaknesses of ChatGPT in various tasks and provide insights for future research using LLMs. We also report a new emergent ability to follow multi-query instruct
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#30333;&#21270;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#65292;&#23427;&#23558;&#23545;&#27604;&#23398;&#20064;&#19982;&#30333;&#21270;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#21516;&#26102;&#20855;&#22791;&#26356;&#22909;&#30340;&#19968;&#33268;&#24615;&#21644;&#26356;&#22909;&#30340;&#23545;&#40784;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.17746</link><description>&lt;p&gt;
&#22522;&#20110;&#30333;&#21270;&#30340;&#23545;&#27604;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Whitening-based Contrastive Learning of Sentence Embeddings. (arXiv:2305.17746v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#30333;&#21270;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#65292;&#23427;&#23558;&#23545;&#27604;&#23398;&#20064;&#19982;&#30333;&#21270;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#21516;&#26102;&#20855;&#22791;&#26356;&#22909;&#30340;&#19968;&#33268;&#24615;&#21644;&#26356;&#22909;&#30340;&#23545;&#40784;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30333;&#21270;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#65288;WhitenedCSE&#65289;&#65292;&#23427;&#23558;&#23545;&#27604;&#23398;&#20064;&#19982;&#19968;&#31181;&#26032;&#39062;&#30340;&#27927;&#29260;&#32452;&#30333;&#21270;&#32467;&#21512;&#36215;&#26469;&#12290;&#36890;&#24120;&#65292;&#23545;&#27604;&#23398;&#20064;&#25289;&#36817;&#19968;&#20010;&#26679;&#26412;&#30340;&#25197;&#26354;&#65288;&#21363;&#27491;&#26679;&#26412;&#65289;&#24182;&#23558;&#36127;&#26679;&#26412;&#36828;&#31163;&#65292;&#20174;&#32780;&#20419;&#36827;&#29305;&#24449;&#31354;&#38388;&#30340;&#23545;&#40784;&#21644;&#19968;&#33268;&#24615;&#12290;"&#25512;"&#25805;&#20316;&#30340;&#21478;&#19968;&#31181;&#27969;&#34892;&#26367;&#20195;&#26041;&#26696;&#26159;&#30333;&#21270;&#29305;&#24449;&#31354;&#38388;&#65292;&#23427;&#25955;&#24067;&#25152;&#26377;&#26679;&#26412;&#20197;&#23454;&#29616;&#19968;&#33268;&#24615;&#12290;&#30001;&#20110;&#30333;&#21270;&#21644;&#23545;&#27604;&#23398;&#20064;&#30456;&#23545;&#20110;&#19968;&#33268;&#24615;&#20855;&#26377;&#22823;&#37327;&#20887;&#20313;&#65292;&#23427;&#20204;&#36890;&#24120;&#21333;&#29420;&#20351;&#29992;&#19988;&#19981;&#23481;&#26131;&#20849;&#21516;&#24037;&#20316;&#12290;&#26412;&#25991;&#39318;&#27425;&#23558;&#30333;&#21270;&#38598;&#25104;&#21040;&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#20013;&#65292;&#24182;&#20419;&#36827;&#20102;&#20004;&#20010;&#22909;&#22788;&#12290;1) &#26356;&#22909;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20004;&#31181;&#26041;&#27861;&#19981;&#23436;&#20840;&#20887;&#20313;&#65292;&#23454;&#38469;&#19978;&#30001;&#20110;&#19981;&#21516;&#30340;&#19968;&#33268;&#24615;&#26426;&#21046;&#65292;&#23427;&#20204;&#26377;&#19968;&#20123;&#20114;&#34917;&#24615;&#12290;2) &#26356;&#22909;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a whitening-based contrastive learning method for sentence embedding learning (WhitenedCSE), which combines contrastive learning with a novel shuffled group whitening. Generally, contrastive learning pulls distortions of a single sample (i.e., positive samples) close and push negative samples far away, correspondingly facilitating the alignment and uniformity in the feature space. A popular alternative to the "pushing'' operation is whitening the feature space, which scatters all the samples for uniformity. Since the whitening and the contrastive learning have large redundancy w.r.t. the uniformity, they are usually used separately and do not easily work together. For the first time, this paper integrates whitening into the contrastive learning scheme and facilitates two benefits. 1) Better uniformity. We find that these two approaches are not totally redundant but actually have some complementarity due to different uniformity mechanism. 2) Better alignment. We rand
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#20108;&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;&#19979;&#28216;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20998;&#31867;&#20219;&#21153;&#65292;&#26681;&#25454;&#21512;&#29702;&#24615;&#23545;&#25152;&#26377;&#20505;&#36873;&#31572;&#26696;&#36827;&#34892;&#25490;&#21517;&#65292;&#20197;&#23454;&#29616;&#26080;&#30417;&#30563;&#24120;&#35782;&#38382;&#39064;&#22238;&#31572;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;UCR&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#20026;&#33410;&#30465;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.15932</link><description>&lt;p&gt;
BUCA&#65306;&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#24120;&#35782;&#38382;&#39064;&#22238;&#31572;&#30340;&#20108;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BUCA: A Binary Classification Approach to Unsupervised Commonsense Question Answering. (arXiv:2305.15932v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#20108;&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;&#19979;&#28216;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20998;&#31867;&#20219;&#21153;&#65292;&#26681;&#25454;&#21512;&#29702;&#24615;&#23545;&#25152;&#26377;&#20505;&#36873;&#31572;&#26696;&#36827;&#34892;&#25490;&#21517;&#65292;&#20197;&#23454;&#29616;&#26080;&#30417;&#30563;&#24120;&#35782;&#38382;&#39064;&#22238;&#31572;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;UCR&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#20026;&#33410;&#30465;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#30340;&#26500;&#24314;&#21464;&#24471;&#36234;&#26469;&#36234;&#26114;&#36149;&#19988;&#22312;&#33539;&#22260;&#19978;&#19981;&#21487;&#36991;&#20813;&#22320;&#21463;&#38480;&#65292;&#26080;&#30417;&#30563;&#30340;&#24120;&#35782;&#25512;&#29702;(UCR)&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;UCR&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#26159;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#23558;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;(&#20363;&#22914;&#65292;&#30693;&#35782;&#22270;&#35889;)&#65292;&#20294;&#36825;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26679;&#20363;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#19979;&#28216;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#36716;&#25442;&#20026;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#20108;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#23545;&#25152;&#26377;&#20505;&#36873;&#31572;&#26696;&#30340;&#21512;&#29702;&#24615;&#36827;&#34892;&#25490;&#21517;&#26469;&#23436;&#25104;&#12290;&#20026;&#20102;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#30693;&#35782;&#22270;&#35889;&#19977;&#20803;&#32452;&#36716;&#25442;&#20026;&#21512;&#29702;&#21644;&#19981;&#21512;&#29702;&#30340;&#25991;&#26412;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#19982;&#20351;&#29992;KG&#30340;&#29616;&#26377;UCR&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#33410;&#30465;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/probe2/BUCA&#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised commonsense reasoning (UCR) is becoming increasingly popular as the construction of commonsense reasoning datasets is expensive, and they are inevitably limited in their scope. A popular approach to UCR is to fine-tune language models with external knowledge (e.g., knowledge graphs), but this usually requires a large number of training examples. In this paper, we propose to transform the downstream multiple choice question answering task into a simpler binary classification task by ranking all candidate answers according to their reasonableness. To this end, for training the model, we convert the knowledge graph triples into reasonable and unreasonable texts. Extensive experimental results show the effectiveness of our approach on various multiple choice question answering benchmarks. Furthermore, compared with existing UCR approaches using KGs, ours is less data hungry. Our code is available at https://github.com/probe2/BUCA.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#21457;&#29616;&#22312;&#19978;&#19979;&#25991;&#35821;&#22659;&#20013;&#65292;&#35821;&#35328;&#24207;&#21015;&#30340;&#35821;&#20041;&#24212;&#36215;&#21040;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#19982;&#20154;&#31867;&#31526;&#21495;&#25512;&#29702;&#19981;&#21516;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#35821;&#35328;&#24207;&#21015;&#20013;&#24314;&#31435;&#24378;&#36830;&#25509;&#65292;&#24182;&#32452;&#25104;&#19968;&#20010;&#34920;&#38754;&#36923;&#36753;&#38142;&#12290;</title><link>http://arxiv.org/abs/2305.14825</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#19978;&#19979;&#25991;&#35821;&#20041;&#25512;&#29702;&#22120;&#32780;&#19981;&#26159;&#31526;&#21495;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners. (arXiv:2305.14825v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#21457;&#29616;&#22312;&#19978;&#19979;&#25991;&#35821;&#22659;&#20013;&#65292;&#35821;&#35328;&#24207;&#21015;&#30340;&#35821;&#20041;&#24212;&#36215;&#21040;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#19982;&#20154;&#31867;&#31526;&#21495;&#25512;&#29702;&#19981;&#21516;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#35821;&#35328;&#24207;&#21015;&#20013;&#24314;&#31435;&#24378;&#36830;&#25509;&#65292;&#24182;&#32452;&#25104;&#19968;&#20010;&#34920;&#38754;&#36923;&#36753;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Large Language Models, LLMs)&#30340;&#20986;&#29616;&#24341;&#36215;&#20102;&#33258;&#28982;&#35821;&#35328;&#21644;&#26426;&#22120;&#23398;&#20064;&#30028;&#30340;&#26497;&#22823;&#20852;&#36259;&#65292;&#20854;&#20986;&#33394;&#30340;&#24212;&#29992;&#24615;&#33021;&#22791;&#21463;&#25512;&#23815;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#19978;&#19979;&#25991;&#35821;&#22659;&#19979;&#30340;&#25512;&#29702;&#33021;&#21147;&#32972;&#21518;&#30340;&#26426;&#21046;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#25552;&#20986;&#25105;&#20204;&#30340;&#20551;&#35774;&#65306;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#35821;&#35328;&#24207;&#21015;&#20013;&#23398;&#20064;&#21040;&#30340;"&#35821;&#20041;"&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#19982;&#20154;&#31867;&#30340;&#31526;&#21495;&#25512;&#29702;&#36807;&#31243;&#19981;&#21516;&#65292;LLMs&#30340;&#35821;&#20041;&#34920;&#31034;&#21487;&#20197;&#22312;&#35821;&#35328;&#24207;&#21015;&#20013;&#24314;&#31435;&#24378;&#36830;&#25509;&#65292;&#22240;&#27492;&#32452;&#25104;&#19968;&#20010;&#34920;&#38754;&#36923;&#36753;&#38142;&#12290;&#20026;&#20102;&#27979;&#35797;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#23558;&#35821;&#20041;&#20174;&#35821;&#35328;&#25512;&#29702;&#36807;&#31243;&#20013;&#20998;&#31163;&#20986;&#26469;&#65292;&#24182;&#35780;&#20272;&#20102;&#19977;&#31181;&#25512;&#29702;&#33021;&#21147;&#65306;&#28436;&#32462;&#12289;&#24402;&#32435;&#21644;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#19978;&#19979;&#25991;&#35821;&#22659;&#20013;&#65292;&#35821;&#20041;&#23545;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#24403;&#35821;&#20041;&#19982;&#24120;&#35782;&#19968;&#33268;&#26102;&#65292;LLMs&#30340;&#34920;&#29616;&#26356;&#20339;&#65292;&#20294;&#22312;&#35299;&#20915;&#31526;&#21495;&#25110;&#21453;&#24120;&#35782;&#25512;&#29702;&#38382;&#39064;&#19978;&#20250;&#20986;&#29616;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergent few-shot reasoning capabilities of Large Language Models (LLMs) have excited the natural language and machine learning community over recent years. Despite of numerous successful applications, the underlying mechanism of such in-context capabilities still remains unclear. In this work, we hypothesize that the learned \textit{semantics} of language tokens do the most heavy lifting during the reasoning process. Different from human's symbolic reasoning process, the semantic representations of LLMs could create strong connections among tokens, thus composing a superficial logical chain. To test our hypothesis, we decouple semantics from the language reasoning process and evaluate three kinds of reasoning abilities, i.e., deduction, induction and abduction. Our findings reveal that semantics play a vital role in LLMs' in-context reasoning -- LLMs perform significantly better when semantics are consistent with commonsense but struggle to solve symbolic or counter-commonsense re
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26085;&#35821;&#25991;&#26412;&#20998;&#31867;&#20013;&#23545;&#25552;&#31034;&#27169;&#26495;&#30340;&#25935;&#24863;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#25581;&#31034;&#20986;&#21363;&#20351;&#26159;&#39640;&#24615;&#33021;&#30340;GPT-4&#27169;&#22411;&#22312;&#36825;&#19968;&#26041;&#38754;&#20063;&#23384;&#22312;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.08714</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26085;&#35821;&#25991;&#26412;&#20998;&#31867;&#20013;&#23545;&#25552;&#31034;&#27169;&#26495;&#30340;&#25935;&#24863;&#24615;&#21644;&#40065;&#26834;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Sensitivity and Robustness of Large Language Models to Prompt Template in Japanese Text Classification Tasks. (arXiv:2305.08714v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26085;&#35821;&#25991;&#26412;&#20998;&#31867;&#20013;&#23545;&#25552;&#31034;&#27169;&#26495;&#30340;&#25935;&#24863;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#25581;&#31034;&#20986;&#21363;&#20351;&#26159;&#39640;&#24615;&#33021;&#30340;GPT-4&#27169;&#22411;&#22312;&#36825;&#19968;&#26041;&#38754;&#20063;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#36827;&#21457;&#23637;&#20027;&#23548;&#20102;&#25552;&#31034;&#24037;&#31243;&#30456;&#20851;&#30740;&#31350;&#30340;&#26174;&#33879;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#39046;&#22495;&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#25552;&#31034;&#27169;&#26495;&#30340;&#25935;&#24863;&#24615;&#21644;&#40065;&#26834;&#24615;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#22312;&#26085;&#35821;&#36825;&#26679;&#30340;&#36739;&#23569;&#30740;&#31350;&#30340;&#35821;&#35328;&#20013;&#12290;&#26412;&#25991;&#36890;&#36807;&#20840;&#38754;&#35780;&#20272;&#20960;&#20010;&#20195;&#34920;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#19968;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PLM&#65289;&#26469;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#32452;&#22522;&#20934;&#25968;&#25454;&#38598;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20102;&#23457;&#26597;&#65292;&#26088;&#22312;&#35780;&#20272;&#21644;&#20998;&#26512;&#24403;&#21069;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#25581;&#31034;&#20102;&#24778;&#20154;&#30340;&#24046;&#24322;&#12290;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#27169;&#26495;&#21477;&#23376;&#32467;&#26500;&#30340;&#20462;&#25913;&#23548;&#33268;GPT-4&#30340;&#20934;&#30830;&#29575;&#20174;49.21&#19979;&#38477;&#21040;&#20102;25.44&#12290;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#24378;&#35843;&#20102;&#21363;&#20351;&#26159;&#39640;&#24615;&#33021;&#30340;GPT-4&#27169;&#22411;&#20063;&#23384;&#22312;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt engineering relevance research has seen a notable surge in recent years, primarily driven by advancements in pre-trained language models and large language models. However, a critical issue has been identified within this domain: the inadequate of sensitivity and robustness of these models towards Prompt Templates, particularly in lesser-studied languages such as Japanese. This paper explores this issue through a comprehensive evaluation of several representative Large Language Models (LLMs) and a widely-utilized pre-trained model(PLM). These models are scrutinized using a benchmark dataset in Japanese, with the aim to assess and analyze the performance of the current multilingual models in this context. Our experimental results reveal startling discrepancies. A simple modification in the sentence structure of the Prompt Template led to a drastic drop in the accuracy of GPT-4 from 49.21 to 25.44. This observation underscores the fact that even the highly performance GPT-4 model 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#29616;&#26377;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#25991;&#26412;&#30456;&#20851;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#34429;&#28982;&#22312;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#23545;&#21333;&#20010;&#23383;&#31526;&#24418;&#29366;&#30340;&#24863;&#30693;&#26377;&#38480;&#65292;&#23545;&#22270;&#20687;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#26816;&#27979;&#33021;&#21147;&#20063;&#19981;&#36275;&#65292;&#19981;&#33021;&#19982;&#20256;&#32479;&#39046;&#22495;&#29305;&#23450;&#26041;&#27861;&#30456;&#21305;&#37197;&#65292;&#24182;&#20173;&#38656;&#36827;&#19968;&#27493;&#25506;&#32034;&#23427;&#20204;&#22312;OCR&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.07895</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;OCR&#30340;&#38544;&#31192;&#20043;&#35868;
&lt;/p&gt;
&lt;p&gt;
On the Hidden Mystery of OCR in Large Multimodal Models. (arXiv:2305.07895v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#29616;&#26377;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#25991;&#26412;&#30456;&#20851;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#34429;&#28982;&#22312;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#23545;&#21333;&#20010;&#23383;&#31526;&#24418;&#29366;&#30340;&#24863;&#30693;&#26377;&#38480;&#65292;&#23545;&#22270;&#20687;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#26816;&#27979;&#33021;&#21147;&#20063;&#19981;&#36275;&#65292;&#19981;&#33021;&#19982;&#20256;&#32479;&#39046;&#22495;&#29305;&#23450;&#26041;&#27861;&#30456;&#21305;&#37197;&#65292;&#24182;&#20173;&#38656;&#36827;&#19968;&#27493;&#25506;&#32034;&#23427;&#20204;&#22312;OCR&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#22823;&#22411;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#20013;&#25198;&#28436;&#30528;&#25903;&#37197;&#24615;&#30340;&#35282;&#33394;&#12290;&#20851;&#20110;&#23427;&#20204;&#22312;&#25991;&#26412;&#30456;&#20851;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#26377;&#25928;&#24615;&#30340;&#25506;&#32034;&#20173;&#19981;&#22815;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#20844;&#24320;&#21487;&#29992;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#25991;&#26412;&#35782;&#21035;&#12289;&#22522;&#20110;&#25991;&#26412;&#30340;&#35270;&#35273;&#38382;&#31572;&#21644;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#20248;&#21155;&#21183;&#65292;&#23427;&#20204;&#20027;&#35201;&#20381;&#36182;&#20110;&#35821;&#20041;&#29702;&#35299;&#26469;&#35782;&#21035;&#21333;&#35789;&#65292;&#24182;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#23545;&#21333;&#20010;&#23383;&#31526;&#24418;&#29366;&#30340;&#24863;&#30693;&#12290;&#23427;&#20204;&#23545;&#25991;&#26412;&#38271;&#24230;&#28448;&#19981;&#20851;&#24515;&#65292;&#22312;&#26816;&#27979;&#22270;&#20687;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#26041;&#38754;&#20855;&#26377;&#26377;&#38480;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#24403;&#21069;&#26368;&#24378;&#22823;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20063;&#26080;&#27861;&#19982;&#20256;&#32479;&#25991;&#26412;&#20219;&#21153;&#30340;&#39046;&#22495;&#29305;&#23450;&#26041;&#27861;&#30456;&#21305;&#37197;&#65292;&#24182;&#22312;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#20013;&#38754;&#20020;&#26356;&#22823;&#30340;&#25361;&#25112;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#26412;&#30740;&#31350;&#23637;&#31034;&#30340;&#22522;&#32447;&#32467;&#26524;&#25581;&#31034;&#20102;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;OCR&#30340;&#38544;&#31192;&#20043;&#35868;&#65292;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large models have recently played a dominant role in natural language processing and multimodal vision-language learning. It remains less explored about their efficacy in text-related visual tasks. We conducted a comprehensive study of existing publicly available multimodal models, evaluating their performance in text recognition, text-based visual question answering, and key information extraction. Our findings reveal strengths and weaknesses in these models, which primarily rely on semantic understanding for word recognition and exhibit inferior perception of individual character shapes. They also display indifference towards text length and have limited capabilities in detecting fine-grained features in images. Consequently, these results demonstrate that even the current most powerful large multimodal models cannot match domain-specific methods in traditional text tasks and face greater challenges in more complex tasks. Most importantly, the baseline results showcased in this study
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#27169;&#22411;&#65292;&#33021;&#22815;&#25506;&#32034;&#28145;&#24230;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#25512;&#29702;&#20013;&#30340;&#38750;&#23545;&#31216;&#39640;&#38454;&#29305;&#24449;&#20132;&#20114;&#12290;&#22312;&#20004;&#20010;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#35782;&#21035;&#24433;&#21709;&#29305;&#24449;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#29305;&#24449;&#20132;&#20114;&#24402;&#22240;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.07224</link><description>&lt;p&gt;
&#38754;&#21521;&#27169;&#22411;&#39044;&#27979;&#35299;&#37322;&#30340;&#38750;&#23545;&#31216;&#29305;&#24449;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Asymmetric feature interaction for interpreting model predictions. (arXiv:2305.07224v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#27169;&#22411;&#65292;&#33021;&#22815;&#25506;&#32034;&#28145;&#24230;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#25512;&#29702;&#20013;&#30340;&#38750;&#23545;&#31216;&#39640;&#38454;&#29305;&#24449;&#20132;&#20114;&#12290;&#22312;&#20004;&#20010;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#35782;&#21035;&#24433;&#21709;&#29305;&#24449;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#29305;&#24449;&#20132;&#20114;&#24402;&#22240;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#27169;&#25311;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#22797;&#26434;&#20132;&#20114;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#12290;&#20808;&#21069;&#26377;&#20851;&#29305;&#24449;&#20132;&#20114;&#24402;&#22240;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#31216;&#20132;&#20114;&#30340;&#30740;&#31350;&#19978;&#65292;&#23427;&#21482;&#33021;&#35299;&#37322;&#21333;&#20010;&#35789;&#27719;&#32452;&#21512;&#21518;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#38468;&#21152;&#24433;&#21709;&#65292;&#32780;&#26080;&#27861;&#25429;&#25417;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#30340;&#38750;&#23545;&#31216;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#23545;&#31216;&#29305;&#24449;&#20132;&#20114;&#35299;&#37322;&#27169;&#22411;&#65292;&#26088;&#22312;&#25506;&#32034;&#28145;&#24230;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#25512;&#29702;&#20013;&#30340;&#38750;&#23545;&#31216;&#39640;&#38454;&#29305;&#24449;&#20132;&#20114;&#12290;&#36890;&#36807;&#34920;&#31034;&#25105;&#20204;&#30340;&#35299;&#37322;&#20026;&#19968;&#20010;&#26377;&#21521;&#20132;&#20114;&#22270;&#65292;&#25105;&#20204;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#22270;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#33021;&#22815;&#21457;&#29616;&#38750;&#23545;&#31216;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#12290;&#22312;&#20004;&#20010;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35782;&#21035;&#24433;&#21709;&#29305;&#24449;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#29305;&#24449;&#20132;&#20114;&#24402;&#22240;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In natural language processing (NLP), deep neural networks (DNNs) could model complex interactions between context and have achieved impressive results on a range of NLP tasks. Prior works on feature interaction attribution mainly focus on studying symmetric interaction that only explains the additional influence of a set of words in combination, which fails to capture asymmetric influence that contributes to model prediction. In this work, we propose an asymmetric feature interaction attribution explanation model that aims to explore asymmetric higher-order feature interactions in the inference of deep neural NLP models. By representing our explanation with an directed interaction graph, we experimentally demonstrate interpretability of the graph to discover asymmetric feature interactions. Experimental results on two sentiment classification datasets show the superiority of our model against the state-of-the-art feature interaction attribution methods in identifying influential featu
&lt;/p&gt;</description></item><item><title>BanglaBook &#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23391;&#21152;&#25289;&#35821;&#20070;&#35780;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324; 158,065 &#20010;&#26679;&#26412;&#65292;&#38024;&#23545;&#24773;&#24863;&#20998;&#26512;&#20998;&#20026;&#19977;&#20010;&#22823;&#31867;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#21462;&#20195;&#25163;&#21160;&#26500;&#24314;&#29305;&#24449;&#30340;&#27169;&#22411;&#65292;&#21462;&#24471;&#26174;&#30528;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06595</link><description>&lt;p&gt;
BanglaBook: &#19968;&#31181;&#29992;&#20110;&#24773;&#24863;&#20998;&#26512;&#30340;&#22823;&#35268;&#27169;&#23391;&#21152;&#25289;&#35821;&#20070;&#35780;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BanglaBook: A Large-scale Bangla Dataset for Sentiment Analysis from Book Reviews. (arXiv:2305.06595v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06595
&lt;/p&gt;
&lt;p&gt;
BanglaBook &#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23391;&#21152;&#25289;&#35821;&#20070;&#35780;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324; 158,065 &#20010;&#26679;&#26412;&#65292;&#38024;&#23545;&#24773;&#24863;&#20998;&#26512;&#20998;&#20026;&#19977;&#20010;&#22823;&#31867;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#21462;&#20195;&#25163;&#21160;&#26500;&#24314;&#29305;&#24449;&#30340;&#27169;&#22411;&#65292;&#21462;&#24471;&#26174;&#30528;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28040;&#36153;&#32773;&#24773;&#24863;&#20998;&#26512;&#21487;&#20197;&#36890;&#36807;&#35780;&#35770;&#34920;&#36798;&#25552;&#20379;&#26377;&#20851;&#20135;&#21697;&#36136;&#37327;&#30340;&#20016;&#23500;&#35265;&#35299;&#12290;&#23613;&#31649;&#24773;&#24863;&#20998;&#26512;&#30340;&#30740;&#31350;&#22312;&#35768;&#22810;&#27969;&#34892;&#35821;&#35328;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#25506;&#32034;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#30456;&#20851;&#25968;&#25454;&#21644;&#36328;&#39046;&#22495;&#36866;&#24212;&#24615;&#65292;&#30456;&#23545;&#36739;&#23569;&#20851;&#27880;&#23391;&#21152;&#25289;&#35821;&#35328;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; BanglaBook&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23391;&#21152;&#25289;&#35821;&#20070;&#35780;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324; 158,065 &#20010;&#26679;&#26412;&#65292;&#20998;&#20026;&#19977;&#20010;&#22823;&#31867;&#65306;&#31215;&#26497;&#12289;&#28040;&#26497;&#21644;&#20013;&#24615;&#12290;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#24182;&#20351;&#29992;&#19968;&#31995;&#21015;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24314;&#31435;&#20102;&#22522;&#32447;&#65292;&#21253;&#25324; SVM&#12289;LSTM &#21644; Bangla-BERT&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20381;&#36182;&#25163;&#21160;&#26500;&#24314;&#29305;&#24449;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#26174;&#30528;&#30340;&#24615;&#33021;&#20248;&#21183;&#65292;&#24378;&#35843;&#20102;&#22312;&#27492;&#39046;&#22495;&#38656;&#35201;&#39069;&#22806;&#30340;&#22521;&#35757;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;&#24773;&#24863;&#38169;&#35823;&#20998;&#31867;&#26469;&#36827;&#34892;&#28145;&#20837;&#30340;&#38169;&#35823;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#26377;&#20851;&#23391;&#21152;&#25289;&#24773;&#24863;&#20998;&#26512;&#24615;&#36136;&#30340;&#36827;&#19968;&#27493;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The analysis of consumer sentiment, as expressed through reviews, can provide a wealth of insight regarding the quality of a product. While the study of sentiment analysis has been widely explored in many popular languages, relatively less attention has been given to the Bangla language, mostly due to a lack of relevant data and cross-domain adaptability. To address this limitation, we present BanglaBook, a large-scale dataset of Bangla book reviews consisting of 158,065 samples classified into three broad categories: positive, negative, and neutral. We provide a detailed statistical analysis of the dataset and employ a range of machine learning models to establish baselines including SVM, LSTM, and Bangla-BERT. Our findings demonstrate a substantial performance advantage of pre-trained models over models that rely on manually crafted features, emphasizing the necessity for additional training resources in this domain. Additionally, we conduct an in-depth error analysis by examining se
&lt;/p&gt;</description></item><item><title>InstructCTG&#26159;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#28436;&#31034;&#26469;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#24182;&#28385;&#36275;&#19981;&#21516;&#32422;&#26463;&#26465;&#20214;&#30340;&#26694;&#26550;&#65292;&#23427;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#25628;&#32034;&#25110;&#24471;&#20998;&#26041;&#27861;&#25152;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.14293</link><description>&lt;p&gt;
&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Controlled Text Generation with Natural Language Instructions. (arXiv:2304.14293v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14293
&lt;/p&gt;
&lt;p&gt;
InstructCTG&#26159;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#28436;&#31034;&#26469;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#24182;&#28385;&#36275;&#19981;&#21516;&#32422;&#26463;&#26465;&#20214;&#30340;&#26694;&#26550;&#65292;&#23427;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#25628;&#32034;&#25110;&#24471;&#20998;&#26041;&#27861;&#25152;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#27969;&#30021;&#30340;&#25991;&#26412;&#65292;&#24182;&#33021;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#26080;&#38656;&#29305;&#23450;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#25511;&#21046;&#23427;&#20204;&#30340;&#29983;&#25104;&#20197;&#28385;&#36275;&#19981;&#21516;&#24212;&#29992;&#31243;&#24207;&#25152;&#38656;&#30340;&#21508;&#31181;&#32422;&#26463;&#26465;&#20214;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#24102;&#32422;&#26463;&#35843;&#33410;&#30340;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#8212;&#8212;InstructCTG&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#32422;&#26463;&#28436;&#31034;&#26469;&#32435;&#20837;&#19981;&#21516;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#19968;&#31995;&#21015;&#29616;&#25104;&#30340;NLP&#24037;&#20855;&#21644;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25552;&#21462;&#33258;&#28982;&#25991;&#26412;&#30340;&#28508;&#22312;&#32422;&#26463;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#32422;&#26463;&#26465;&#20214;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#20197;&#24418;&#25104;&#24369;&#30417;&#30563;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36890;&#36807;&#28155;&#21152;&#33258;&#28982;&#35821;&#35328;&#32422;&#26463;&#25551;&#36848;&#21644;&#23569;&#37327;&#28436;&#31034;&#65292;&#25105;&#20204;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20197;&#32435;&#20837;&#21508;&#31181;&#31867;&#22411;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#19982;&#29616;&#26377;&#22522;&#20110;&#25628;&#32034;&#25110;&#24471;&#20998;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;InstructCTG &#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models generate fluent texts and can follow natural language instructions to solve a wide range of tasks without task-specific training. Nevertheless, it is notoriously difficult to control their generation to satisfy the various constraints required by different applications. In this work, we present InstructCTG, a controlled text generation framework that incorporates different constraints by conditioning on natural language descriptions and demonstrations of the constraints. In particular, we first extract the underlying constraints of natural texts through a combination of off-the-shelf NLP tools and simple heuristics. We then verbalize the constraints into natural language instructions to form weakly supervised training data. By prepending natural language descriptions of the constraints and a few demonstrations, we fine-tune a pre-trained language model to incorporate various types of constraints. Compared to existing search-based or score-based methods, InstructCT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65292;&#21457;&#29616;&#20102;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.03279</link><description>&lt;p&gt;
&#22870;&#21169;&#26159;&#21542;&#21512;&#29702;&#65311;&#22312; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#20013;&#34913;&#37327;&#22870;&#21169;&#19982;&#36947;&#24503;&#34892;&#20026;&#20043;&#38388;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark. (arXiv:2304.03279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65292;&#21457;&#29616;&#20102;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#34987;&#35757;&#32451;&#25104;&#26368;&#22823;&#21270;&#22870;&#21169;&#65292;&#36825;&#21487;&#33021;&#20250;&#28608;&#21169;&#36861;&#27714;&#26435;&#21147;&#21644;&#27450;&#39575;&#34892;&#20026;&#65292;&#31867;&#20284;&#20110;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#21487;&#33021;&#20250;&#28608;&#21169;&#26377;&#23475;&#34892;&#20026;&#12290;&#37027;&#20040;&#20195;&#29702;&#26159;&#21542;&#33258;&#28982;&#32780;&#28982;&#22320;&#23398;&#20250;&#20102;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65311;&#25105;&#20204;&#22914;&#20309;&#22312; GPT-4 &#31561;&#36890;&#29992;&#27169;&#22411;&#20013;&#34913;&#37327;&#36825;&#20123;&#34892;&#20026;&#21602;&#65311;&#20026;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#28085;&#30422;&#20102;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#22810;&#26679;&#21270;&#30340;&#24773;&#26223;&#65292;&#37325;&#28857;&#20851;&#27880;&#31038;&#20250;&#20915;&#31574;&#21046;&#23450;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#12290;&#25105;&#20204;&#25968;&#23398;&#21270;&#20102;&#25968;&#21313;&#31181;&#26377;&#23475;&#34892;&#20026;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#27880;&#37322;&#26469;&#35780;&#20272;&#20195;&#29702;&#20542;&#21521;&#20110;&#36861;&#27714;&#26435;&#21147;&#65292;&#36896;&#25104;&#21151;&#33021;&#19981;&#33391;&#21644;&#36829;&#21453;&#20262;&#29702;&#30340;&#20542;&#21521;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#19968;&#20123;&#32039;&#24352;&#20851;&#31995;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#31181;&#26435;&#34913;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20351;&#20195;&#29702;&#36235;&#21521;&#20110;&#37319;&#21462;&#26356;&#23569;&#30340;&#26377;&#23475;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;MACHIAVELLI &#26159;&#35780;&#20272;&#20154;&#24037;&#20195;&#29702;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#27700;&#24179;&#30340;&#26377;&#29992;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents' towards less harmful behaviors. Our results sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#19978;&#19979;&#25991;&#20559;&#24046;&#30340;&#21452;&#38454;&#27573;&#19978;&#19979;&#25991;&#36807;&#28388;&#26041;&#27861;&#65292;&#23558;&#27969;&#24335;&#36755;&#20986;&#21644;&#39044;&#23450;&#20041;&#19978;&#19979;&#25991;&#21333;&#35789;&#21015;&#34920;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#31471;&#23545;&#31471;&#27169;&#22411;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#65292;&#24182;&#21152;&#24555;&#20102;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2301.06735</link><description>&lt;p&gt;
&#32479;&#19968;&#27969;&#24335;&#21644;&#38750;&#27969;&#24335;&#21464;&#25442;&#22120;&#20013;&#30340;&#19978;&#19979;&#25991;&#20559;&#24046;&#21452;&#38454;&#27573;&#19978;&#19979;&#25991;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Two Stage Contextual Word Filtering for Context bias in Unified Streaming and Non-streaming Transducer. (arXiv:2301.06735v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#19978;&#19979;&#25991;&#20559;&#24046;&#30340;&#21452;&#38454;&#27573;&#19978;&#19979;&#25991;&#36807;&#28388;&#26041;&#27861;&#65292;&#23558;&#27969;&#24335;&#36755;&#20986;&#21644;&#39044;&#23450;&#20041;&#19978;&#19979;&#25991;&#21333;&#35789;&#21015;&#34920;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#31471;&#23545;&#31471;&#27169;&#22411;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#65292;&#24182;&#21152;&#24555;&#20102;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#19968;&#20010;&#31471;&#23545;&#31471;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#26469;&#35828;&#65292;&#24456;&#38590;&#35782;&#21035;&#20687;&#23454;&#20307;&#36825;&#26679;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20986;&#29616;&#19981;&#39057;&#32321;&#30340;&#21333;&#35789;&#12290;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#24120;&#29992;&#26041;&#27861;&#26159;&#23558;&#19978;&#19979;&#25991;&#20449;&#24687;&#36755;&#20837;&#21040;&#22768;&#23398;&#27169;&#22411;&#20013;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#19968;&#20010;&#32039;&#20945;&#32780;&#20934;&#30830;&#30340;&#19978;&#19979;&#25991;&#21015;&#34920;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#20026;&#32479;&#19968;&#30340;&#27969;&#24335;/&#38750;&#27969;&#24335;&#30340;&#31471;&#23545;&#31471;&#27169;&#22411;&#24471;&#21040;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#19978;&#19979;&#25991;&#21015;&#34920;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#30005;&#35805;&#32423;&#21035;&#30340;&#27969;&#24335;&#36755;&#20986;&#26469;&#39318;&#20808;&#36807;&#28388;&#39044;&#23450;&#20041;&#30340;&#19978;&#19979;&#25991;&#21333;&#35789;&#21015;&#34920;&#65292;&#28982;&#21518;&#23558;&#20854;&#34701;&#21512;&#21040;&#38750;&#22240;&#26524;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20013;&#29983;&#25104;&#26368;&#32456;&#30340;&#35782;&#21035;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;ASR&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#24182;&#21152;&#24555;&#20102;&#25512;&#29702;&#36807;&#31243;&#12290;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#22522;&#32447;&#31995;&#32479;&#65292;CERR&#25552;&#39640;&#20102;20%&#20197;&#19978;&#12290;&#21516;&#26102;&#65292;&#24403;&#19978;&#19979;&#25991;&#21333;&#35789;&#21015;&#34920;&#30340;&#22823;&#23567;&#36229;&#36807;6000&#26102;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#30340;RTF&#21487;&#20197;&#31283;&#23450;&#22312;0.15&#24038;&#21491;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is difficult for an E2E ASR system to recognize words such as entities appearing infrequently in the training data. A widely used method to mitigate this issue is feeding contextual information into the acoustic model. Previous works have proven that a compact and accurate contextual list can boost the performance significantly. In this paper, we propose an efficient approach to obtain a high quality contextual list for a unified streaming/non-streaming based E2E model. Specifically, we make use of the phone-level streaming output to first filter the predefined contextual word list then fuse it into non-casual encoder and decoder to generate the final recognition results. Our approach improve the accuracy of the contextual ASR system and speed up the inference process. Experiments on two datasets demonstrates over 20% CERR comparing to the baseline system. Meanwile, the RTF of our system can be stabilized within 0.15 when the size of the contextual word list grows over 6000.
&lt;/p&gt;</description></item><item><title>&#26681;&#25454; &#8220;&#19977;&#24605;&#32780;&#21518;&#35821;&#8221; &#34892;&#20026;&#21551;&#21457;&#65292;&#25552;&#20986;&#19968;&#31181;&#20004;&#38454;&#27573;&#23545;&#35805;&#20195;&#29702;&#29992;&#20110;&#29983;&#25104;&#24773;&#24863;&#23545;&#35805;&#65292;&#35813;&#20195;&#29702;&#22312;&#24773;&#24863;&#29983;&#25104;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#24182;&#20445;&#25345;&#20102;&#35821;&#20041;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2301.04907</link><description>&lt;p&gt;
Think Twice&#65306;&#19968;&#31181;&#20154;&#31867;&#21270;&#30340;&#20004;&#38454;&#27573;&#23545;&#35805;&#20195;&#29702;&#29992;&#20110;&#29983;&#25104;&#24773;&#24863;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Think Twice: A Human-like Two-stage Conversational Agent for Emotional Response Generation. (arXiv:2301.04907v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04907
&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454; &#8220;&#19977;&#24605;&#32780;&#21518;&#35821;&#8221; &#34892;&#20026;&#21551;&#21457;&#65292;&#25552;&#20986;&#19968;&#31181;&#20004;&#38454;&#27573;&#23545;&#35805;&#20195;&#29702;&#29992;&#20110;&#29983;&#25104;&#24773;&#24863;&#23545;&#35805;&#65292;&#35813;&#20195;&#29702;&#22312;&#24773;&#24863;&#29983;&#25104;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#24182;&#20445;&#25345;&#20102;&#35821;&#20041;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20154;&#31867;&#21270;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#30446;&#21069;&#24773;&#24863;&#23545;&#35805;&#26041;&#27861;&#37319;&#29992;&#32479;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#32852;&#21512;&#27169;&#22411;&#24773;&#24863;&#21644;&#35821;&#20041;&#12290;&#36825;&#31181;&#31574;&#30053;&#30001;&#20110;&#24773;&#24863;&#21644;&#35821;&#20041;&#20043;&#38388;&#30340;&#30456;&#20114;&#38480;&#21046;&#24448;&#24448;&#20250;&#20135;&#29983;&#23433;&#20840;&#30340;&#21709;&#24212;&#65292;&#24182;&#19988;&#38656;&#35201;&#32597;&#35265;&#30340;&#24773;&#24863;&#26631;&#27880;&#22823;&#35268;&#27169;&#23545;&#35805;&#35821;&#26009;&#24211;&#12290;&#21463;&#21040;&#20154;&#31867;&#23545;&#35805;&#20013;&#8220;&#19977;&#24605;&#32780;&#21518;&#35821;&#8221;&#30340;&#34892;&#20026;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#24773;&#24863;&#23545;&#35805;&#30340;&#20004;&#38454;&#27573;&#23545;&#35805;&#20195;&#29702;&#12290;&#39318;&#20808;&#65292;&#19968;&#20010;&#27809;&#26377;&#20351;&#29992;&#24773;&#24863;&#26631;&#27880;&#23545;&#35805;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#23545;&#35805;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#19978;&#19979;&#25991;&#35821;&#20041;&#30340;&#21407;&#22411;&#21709;&#24212;&#12290;&#20854;&#27425;&#65292;&#31532;&#19968;&#38454;&#27573;&#21407;&#22411;&#23558;&#36890;&#36807;&#19968;&#20010;&#21487;&#25511;&#30340;&#24773;&#24863;&#20248;&#21270;&#22120;&#19982;&#20849;&#24773;&#20551;&#35774;&#36827;&#34892;&#20462;&#25913;&#12290;&#22312;DailyDialog&#21644;EmpatheticDialogues&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#23545;&#35805;&#20195;&#29702;&#22312;&#24773;&#24863;&#29983;&#25104;&#26041;&#38754;&#20248;&#20110;&#27604;&#36739;&#27169;&#22411;&#65292;&#24182;&#22312;&#33258;&#21160;&#21644;&#20154;&#31867;&#35780;&#20272;&#20013;&#20445;&#25345;&#20102;&#35821;&#20041;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Towards human-like dialogue systems, current emotional dialogue approaches jointly model emotion and semantics with a unified neural network. This strategy tends to generate safe responses due to the mutual restriction between emotion and semantics, and requires rare emotion-annotated large-scale dialogue corpus. Inspired by the "think twice" behavior in human dialogue, we propose a two-stage conversational agent for the generation of emotional dialogue. Firstly, a dialogue model trained without the emotion-annotated dialogue corpus generates a prototype response that meets the contextual semantics. Secondly, the first-stage prototype is modified by a controllable emotion refiner with the empathy hypothesis. Experimental results on the DailyDialog and EmpatheticDialogues datasets demonstrate that the proposed conversational outperforms the comparison models in emotion generation and maintains the semantic performance in automatic and human evaluations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; MultiInstruct&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#20248;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25506;&#32034;&#22810;&#31181;&#36801;&#31227;&#23398;&#20064;&#31574;&#30053;&#20174;&#22823;&#35268;&#27169;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25968;&#25454;&#38598;&#20013;&#25552;&#39640;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#20854;&#22312;&#21508;&#31181;&#26410;&#35265;&#36807;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#20855;&#26377;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#34920;&#29616;&#65292;&#20197;&#21450;&#35774;&#35745;&#30340;&#26032;&#30340;&#20219;&#21153;&#23436;&#25104;&#29575;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2212.10773</link><description>&lt;p&gt;
MultiInstruct: &#36890;&#36807;&#25351;&#20196;&#35843;&#20248;&#26469;&#25913;&#21892;&#22810;&#27169;&#24577;&#38646;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning. (arXiv:2212.10773v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; MultiInstruct&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#20248;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25506;&#32034;&#22810;&#31181;&#36801;&#31227;&#23398;&#20064;&#31574;&#30053;&#20174;&#22823;&#35268;&#27169;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25968;&#25454;&#38598;&#20013;&#25552;&#39640;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#20854;&#22312;&#21508;&#31181;&#26410;&#35265;&#36807;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#20855;&#26377;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#34920;&#29616;&#65292;&#20197;&#21450;&#35774;&#35745;&#30340;&#26032;&#30340;&#20219;&#21153;&#23436;&#25104;&#29575;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#20248;&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#22312;&#25351;&#20196;&#25351;&#23450;&#30340;&#20219;&#21153;&#19978;&#23545;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#23637;&#29616;&#20102;&#26377;&#24076;&#26395;&#30340;&#38646;&#26679;&#26412;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#23427;&#23578;&#26410;&#34987;&#29992;&#20110;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; MultiInstruct&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#20248;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547; 47 &#20010;&#19981;&#21516;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102; 11 &#20010;&#24191;&#27867;&#30340;&#31867;&#21035;&#12290;&#27599;&#20010;&#20219;&#21153;&#33267;&#23569;&#35774;&#35745;&#26377; 5,000 &#20010;&#23454;&#20363;&#65288;&#36755;&#20837;-&#36755;&#20986;&#23545;&#65289;&#26469;&#33258;&#29616;&#26377;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#21644; 5 &#20010;&#19987;&#23478;&#32534;&#20889;&#30340;&#25351;&#20196;&#12290;&#25105;&#20204;&#36873;&#21462; OFA &#20316;&#20026;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#20248;&#30340;&#22522;&#30784;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#22810;&#31181;&#36801;&#31227;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#21033;&#29992;&#22823;&#35268;&#27169;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#20854;&#22312;&#21508;&#31181;&#26410;&#35265;&#36807;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#20855;&#26377;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#34920;&#29616;&#65292;&#20197;&#21450;&#20174;&#32431;&#25991;&#26412;&#25351;&#20196;&#20013;&#33719;&#24471;&#36801;&#31227;&#23398;&#20064;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#23436;&#25104;&#29575;&#25351;&#26631;&#26469;&#35780;&#20272;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#24230;&#37327;&#27169;&#22411;&#22312;&#20165;&#26377;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning, a new learning paradigm that fine-tunes pre-trained language models on tasks specified through instructions, has shown promising zero-shot performance on various natural language processing tasks. However, it's still not explored for vision and multimodal tasks. In this work, we introduce MultiInstruct, the first multimodal instruction tuning benchmark dataset that consists of 47 diverse multimodal tasks covering 11 broad categories. Each task is designed at least with 5,000 instances (input-out pairs) from existing open-source datasets and 5 expert-written instructions. We take OFA as the base pre-trained model for multimodal instruction tuning, and to improve its performance, we explore multiple transfer learning strategies to leverage the large-scale Natural Instructions dataset. Experimental results demonstrate its strong zero-shot performance on various unseen multimodal tasks and the benefit of transfer learning from text-only instructions. We also design a ne
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;Code-LLMs&#24341;&#23548;&#31526;&#21495;&#34920;&#31034;&#20197;&#22686;&#24378;&#31070;&#32463;&#31526;&#21495;&#25925;&#20107;&#29702;&#35299;&#65292;&#36890;&#36807;CoRRPUS&#31995;&#32479;&#21644;&#25277;&#35937;&#25552;&#31034;&#31243;&#24207;&#65292;&#22312;&#26368;&#23567;&#30340;&#25163;&#21160;&#24037;&#31243;&#26465;&#20214;&#19979;&#65292;&#20987;&#36133;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#32467;&#26500;LLM&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2212.10754</link><description>&lt;p&gt;
CoRRPUS: &#21033;&#29992;Codex&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#22686;&#24378;&#31070;&#32463;&#31526;&#21495;&#25925;&#20107;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
CoRRPUS: Codex-Leveraged Structured Representations for Neurosymbolic Story Understanding. (arXiv:2212.10754v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;Code-LLMs&#24341;&#23548;&#31526;&#21495;&#34920;&#31034;&#20197;&#22686;&#24378;&#31070;&#32463;&#31526;&#21495;&#25925;&#20107;&#29702;&#35299;&#65292;&#36890;&#36807;CoRRPUS&#31995;&#32479;&#21644;&#25277;&#35937;&#25552;&#31034;&#31243;&#24207;&#65292;&#22312;&#26368;&#23567;&#30340;&#25163;&#21160;&#24037;&#31243;&#26465;&#20214;&#19979;&#65292;&#20987;&#36133;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#32467;&#26500;LLM&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25152;&#26377;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;/&#29702;&#35299;&#20219;&#21153;&#30340;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#30340;&#39134;&#36895;&#21457;&#23637;&#65292;&#25925;&#20107;&#29983;&#25104;&#21644;&#29702;&#35299;&#20063;&#24471;&#21040;&#20102;&#26497;&#22823;&#30340;&#25913;&#36827;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;Code-LLMs&#65292;&#22914;Codex&#65292;&#24341;&#23548;&#31526;&#21495;&#26041;&#27861;&#30340;&#20351;&#29992;&#65292;&#20197;&#36319;&#36394;&#25925;&#20107;&#29366;&#24577;&#24182;&#24110;&#21161;&#25925;&#20107;&#29702;&#35299;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;CoRRPUS&#31995;&#32479;&#21644;&#25277;&#35937;&#25552;&#31034;&#31243;&#24207;&#22914;&#20309;&#22312;&#26368;&#23567;&#30340;&#25163;&#21160;&#24037;&#31243;&#26465;&#20214;&#19979;&#65292;&#20987;&#36133;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#32467;&#26500;LLM&#25216;&#26415;&#65292;&#23436;&#25104;&#20102;&#39044;&#20808;&#23384;&#22312;&#30340;&#25925;&#20107;&#29702;&#35299;&#20219;&#21153;&#65288;bAbI task 2 &#21644; Re^3&#65289;&#12290;&#25105;&#20204;&#24076;&#26395;&#26412;&#30740;&#31350;&#33021;&#22815;&#20984;&#26174;&#31526;&#21495;&#34920;&#31034;&#21644;&#19987;&#19994;&#25552;&#31034;&#23545;LLMs&#30340;&#37325;&#35201;&#24615;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#19968;&#20123;&#25163;&#24037;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Story generation and understanding -- as with all NLG/NLU tasks -- has seen a surge in neurosymbolic work. Researchers have recognized that, while large language models (LLMs) have tremendous utility, they can be augmented with symbolic means to be even better and to make up for any flaws that the neural networks might have. However, symbolic methods are extremely costly in terms of the amount of time and expertise needed to create them. In this work, we capitalize on state-of-the-art Code-LLMs, such as Codex, to bootstrap the use of symbolic methods for tracking the state of stories and aiding in story understanding. We show that our CoRRPUS system and abstracted prompting procedures can beat current state-of-the-art structured LLM techniques on pre-existing story understanding tasks (bAbI task 2 and Re^3) with minimal hand engineering. We hope that this work can help highlight the importance of symbolic representations and specialized prompting for LLMs as these models require some g
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#32570;&#20047;&#23545;&#26085;&#24120;&#29289;&#21697;&#30340;&#19968;&#33268;&#24615;&#24515;&#29702;&#27169;&#22411;&#65292;&#20250;&#22240;&#27492;&#20986;&#29616;&#33618;&#35884;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36825;&#20123;&#23454;&#20307;&#30340;&#30693;&#35782;&#30862;&#29255;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#20026;&#25152;&#26377;&#23454;&#20307;&#20135;&#29983;&#19968;&#33268;&#19988;&#27491;&#30830;&#30340;&#24515;&#29702;&#27169;&#22411;&#12290;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;&#36825;&#31181;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2212.10029</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#26085;&#24120;&#29289;&#21697;&#30340;&#19968;&#33268;&#24615;&#24515;&#29702;&#27169;&#22411;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do language models have coherent mental models of everyday things?. (arXiv:2212.10029v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10029
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#32570;&#20047;&#23545;&#26085;&#24120;&#29289;&#21697;&#30340;&#19968;&#33268;&#24615;&#24515;&#29702;&#27169;&#22411;&#65292;&#20250;&#22240;&#27492;&#20986;&#29616;&#33618;&#35884;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36825;&#20123;&#23454;&#20307;&#30340;&#30693;&#35782;&#30862;&#29255;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#20026;&#25152;&#26377;&#23454;&#20307;&#20135;&#29983;&#19968;&#33268;&#19988;&#27491;&#30830;&#30340;&#24515;&#29702;&#27169;&#22411;&#12290;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;&#36825;&#31181;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#20204;&#24819;&#21040;&#20687;&#8220;&#40481;&#34507;&#8221;&#36825;&#26679;&#30340;&#26085;&#24120;&#29992;&#21697;&#26102;&#65292;&#36890;&#24120;&#20250;&#26377;&#19968;&#20010;&#19982;&#20043;&#30456;&#20851;&#32852;&#30340;&#24515;&#29702;&#22270;&#20687;&#12290;&#36825;&#31181;&#24120;&#35782;&#24615;&#30693;&#35782;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;&#36825;&#20123;&#26085;&#24120;&#29992;&#21697;&#30340;&#24037;&#20316;&#21407;&#29702;&#20197;&#21450;&#22914;&#20309;&#19982;&#23427;&#20204;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#31995;&#32479;&#23545;&#36825;&#26679;&#30340;&#26085;&#24120;&#29992;&#21697;&#27809;&#26377;&#19968;&#33268;&#30340;&#22270;&#20687;&#65292;&#27604;&#22914;&#35748;&#20026;&#40481;&#34507;&#40644;&#21253;&#22260;&#30528;&#22771;&#65292;&#37027;&#20040;&#23427;&#21487;&#33021;&#19981;&#24471;&#19981;&#37319;&#21462;&#33618;&#35884;&#30340;&#26041;&#27861;&#65292;&#27604;&#22914;&#35797;&#22270;&#25226;&#40481;&#34507;&#40644;&#21038;&#19979;&#22771;&#25918;&#20837;&#24179;&#24213;&#38149;&#20013;&#29006;&#29038;&#12290;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#36825;&#31181;&#26085;&#24120;&#29992;&#21697;&#30340;&#19968;&#33268;&#24615;&#24515;&#29702;&#27169;&#22411;&#65311;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;100&#31181;&#26085;&#24120;&#29992;&#21697;&#12289;&#23427;&#20204;&#30340;&#37096;&#20214;&#20197;&#21450;&#36825;&#20123;&#37096;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20687;GPT-3&#21644;Macaw&#36825;&#26679;&#30340;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36825;&#20123;&#23454;&#20307;&#30340;&#30693;&#35782;&#30862;&#29255;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#20026;&#25152;&#26377;&#23454;&#20307;&#20135;&#29983;&#19968;&#33268;&#19988;&#27491;&#30830;&#30340;&#24515;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#23545;&#36825;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#23427;&#20204;&#22312;&#26576;&#20123;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
When people think of everyday things like an "egg," they typically have a mental image associated with it. This commonsense knowledge helps us understand how these everyday things work and how to interact with them. For example, when someone tries to make a fried egg, they know that it has a shell and that it can be cracked open to reveal the egg white and yolk inside. However, if a system does not have a coherent picture of such everyday things, thinking that the egg yolk surrounds the shell, then it might have to resort to ridiculous approaches such as trying to scrape the egg yolk off the shell into the pan. Do language models have a coherent picture of such everyday things? To investigate this, we propose a benchmark dataset consisting of 100 everyday things, their parts, and the relationships between these parts. We observe that state-of-the-art pre-trained language models (LMs) like GPT-3 and Macaw have fragments of knowledge about these entities, but they fail to produce consist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20851;&#31995;&#21477;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#30456;&#20851;&#30340;&#20851;&#31995;&#23884;&#20837;&#26469;&#35299;&#20915;&#21477;&#23376;&#38388;&#22797;&#26434;&#35821;&#20041;&#21547;&#20041;&#30340;&#20851;&#31995;&#25429;&#25417;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#22343;&#20855;&#26377;&#39640;&#25928;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.08802</link><description>&lt;p&gt;
&#20851;&#20110;&#28789;&#27963;&#35821;&#20041;&#21305;&#37197;&#30340;&#20851;&#31995;&#21477;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Relational Sentence Embedding for Flexible Semantic Matching. (arXiv:2212.08802v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20851;&#31995;&#21477;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#30456;&#20851;&#30340;&#20851;&#31995;&#23884;&#20837;&#26469;&#35299;&#20915;&#21477;&#23376;&#38388;&#22797;&#26434;&#35821;&#20041;&#21547;&#20041;&#30340;&#20851;&#31995;&#25429;&#25417;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#22343;&#20855;&#26377;&#39640;&#25928;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20851;&#31995;&#21477;&#23884;&#20837; (RSE) &#33539;&#24335;&#65292;&#20197;&#36827;&#19968;&#27493;&#21457;&#25496;&#21477;&#23376;&#23884;&#20837;&#30340;&#28508;&#21147;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#26159;&#22522;&#20110;&#23884;&#20837;&#36317;&#31163;&#24314;&#27169;&#21477;&#23376;&#38388;&#30340;&#30456;&#20284;&#24230;&#12290;&#30001;&#20110;&#34920;&#36798;&#30340;&#22797;&#26434;&#35821;&#20041;&#21547;&#20041;&#65292;&#21477;&#23376;&#23545;&#21487;&#33021;&#20855;&#26377;&#21508;&#31181;&#20851;&#31995;&#31867;&#22411;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#34164;&#28085;&#12289;&#37322;&#20041;&#21644;&#38382;&#31572;&#12290;&#23545;&#20110;&#29616;&#26377;&#30340;&#23884;&#20837;&#26041;&#27861;&#26469;&#25429;&#25417;&#36825;&#31181;&#20851;&#31995;&#20449;&#24687;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#30456;&#20851;&#30340;&#20851;&#31995;&#23884;&#20837;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#20849;&#29983;&#20013;&#24515;&#32534;&#30721;&#22120;&#65292;&#23545;&#28304;&#21477;&#23376;&#24212;&#29992;&#20851;&#31995;&#36716;&#25442;&#25805;&#20316;&#26469;&#25512;&#26029;&#23545;&#24212;&#30340;&#30446;&#26631;&#21477;&#23376;&#12290;&#36890;&#36807;&#23398;&#20064;&#30340;&#23884;&#20837;&#65292;&#21487;&#20197;&#35745;&#31639;&#20986;&#32454;&#31890;&#24230;&#30340;&#20851;&#31995;&#30456;&#20284;&#24230;&#20998;&#25968;&#12290;&#25105;&#20204;&#22312;19&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#20219;&#21153;&#33539;&#22260;&#65292;&#21253;&#25324;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#12289;&#36716;&#31227;&#21644;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Relational Sentence Embedding (RSE), a new paradigm to further discover the potential of sentence embeddings. Prior work mainly models the similarity between sentences based on their embedding distance. Because of the complex semantic meanings conveyed, sentence pairs can have various relation types, including but not limited to entailment, paraphrasing, and question-answer. It poses challenges to existing embedding methods to capture such relational information. We handle the problem by learning associated relational embeddings. Specifically, a relation-wise translation operation is applied to the source sentence to infer the corresponding target sentence with a pre-trained Siamese-based encoder. The fine-grained relational similarity scores can be computed from learned embeddings. We benchmark our method on 19 datasets covering a wide range of tasks, including semantic textual similarity, transfer, and domain-specific tasks. Experimental results show that our method is eff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26174;&#24335;&#30693;&#35782;&#36716;&#31227; (EKT) &#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#36716;&#31227;&#21040;&#23567;&#22411;&#27169;&#22411;&#65292;&#23558;&#24494;&#35843;&#25152;&#38656;&#26114;&#36149;&#19988;&#38590;&#20197;&#33719;&#21462;&#30340;&#20219;&#21153;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#20195;&#30721;&#23545;&#25913;&#20026;&#36890;&#36807;&#36807;&#28388;&#22823;&#37327; NL-code &#23545;&#26469;&#23454;&#29616;&#65292;&#20248;&#20110;&#30693;&#35782;&#33976;&#39311;&#12290;</title><link>http://arxiv.org/abs/2211.16740</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#26465;&#20214;&#19979;&#30340;&#26174;&#24335;&#30693;&#35782;&#36716;&#31227;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Explicit Knowledge Transfer for Weakly-Supervised Code Generation. (arXiv:2211.16740v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26174;&#24335;&#30693;&#35782;&#36716;&#31227; (EKT) &#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#36716;&#31227;&#21040;&#23567;&#22411;&#27169;&#22411;&#65292;&#23558;&#24494;&#35843;&#25152;&#38656;&#26114;&#36149;&#19988;&#38590;&#20197;&#33719;&#21462;&#30340;&#20219;&#21153;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#20195;&#30721;&#23545;&#25913;&#20026;&#36890;&#36807;&#36807;&#28388;&#22823;&#37327; NL-code &#23545;&#26469;&#23454;&#29616;&#65292;&#20248;&#20110;&#30693;&#35782;&#33976;&#39311;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#30340;&#23398;&#20064;&#33719;&#21462;&#24378;&#22823;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23567;&#22411;&#27169;&#22411;&#20173;&#38656;&#35201;&#30417;&#30563;&#24494;&#35843;&#25165;&#33021;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#24494;&#35843;&#38656;&#35201;&#22823;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#20195;&#30721;&#23545;&#65292;&#26114;&#36149;&#19988;&#38590;&#20197;&#33719;&#21462;&#12290;&#26412;&#25991;&#23581;&#35797;&#21033;&#29992;&#24369;&#30417;&#30563;&#25968;&#25454;&#23558; LLM &#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#36716;&#31227;&#21040;&#23567;&#22411;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26174;&#24335;&#30693;&#35782;&#36716;&#31227; (EKT)&#65292;&#23427;&#20351;&#29992;&#25945;&#24072; LLM &#30340;&#23569;&#37327;&#26679;&#26412;&#33021;&#21147;&#26469;&#21019;&#24314; NL-code &#23545;&#65292;&#28982;&#21518;&#25105;&#20204;&#36807;&#28388;&#20986;&#27491;&#30830;&#30340;&#23545;&#24182;&#23545;&#23398;&#29983;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102; EKT &#22312;&#20174; GSM8k &#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#25968;&#23398;&#38382;&#39064;&#20195;&#30721;&#35299;&#26102;&#30340;&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;EKT &#19981;&#20165;&#27604;&#20351;&#29992;&#19987;&#23478;&#36845;&#20195;&#35757;&#32451;&#30340;&#26041;&#27861;&#20135;&#29983;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#20248;&#20110;&#30693;&#35782;&#33976;&#39311;&#65292;&#21478;&#19968;&#31181;&#30693;&#35782;&#36716;&#31227;&#24418;&#24335;&#12290;&#20351;&#29992; GPT-J &#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#65292;&#37319;&#29992; EKT &#23545; GPT-Neo 1.3B &#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21487;&#33719;&#24471; 12.4% &#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can acquire strong code-generation capabilities through few-shot learning. In contrast, supervised fine-tuning is still needed for smaller models to achieve good performance. Such fine-tuning demands a large number of task-specific NL-code pairs, which are expensive to obtain. In this paper, we attempt to transfer the code generation ability of an LLM to a smaller model with the aid of weakly-supervised data. More specifically, we propose explicit knowledge transfer (EKT), which uses the few-shot capabilities of a teacher LLM to create NL-code pairs that we then filter for correctness and fine-tune the student on. We evaluate EKT on the task of generating code solutions to math word problems from the GSM8k dataset. We find that EKT not only yields better performance than training with expert iteration, but also outperforms knowledge distillation, another form of knowledge transfer. A GPT-Neo 1.3B model trained using EKT with a GPT-J teacher achieves a 12.4%
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#26694;&#26550;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#21508;&#31181;&#22240;&#32032;&#23545;&#36755;&#20986;&#35299;&#20915;&#26041;&#26696;&#30340;&#22240;&#26524;&#24433;&#21709;&#65292;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;GPT-3 Davinci&#27169;&#22411;&#65288;175B&#65289;&#22312;&#40065;&#26834;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2210.12023</link><description>&lt;p&gt;
&#19968;&#31181;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#25968;&#23398;&#25512;&#29702;&#40065;&#26834;&#24615;&#30340;&#22240;&#26524;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models. (arXiv:2210.12023v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12023
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#26694;&#26550;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#21508;&#31181;&#22240;&#32032;&#23545;&#36755;&#20986;&#35299;&#20915;&#26041;&#26696;&#30340;&#22240;&#26524;&#24433;&#21709;&#65292;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;GPT-3 Davinci&#27169;&#22411;&#65288;175B&#65289;&#22312;&#40065;&#26834;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22256;&#38590;&#25968;&#23398;&#25512;&#29702;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#65292;&#21516;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#20063;&#22791;&#21463;&#36136;&#30097;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#26102;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#20381;&#36182;&#20110;&#38382;&#39064;&#25551;&#36848;&#20013;&#30340;&#27973;&#23618;&#27169;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#24314;&#31435;&#22312;&#34892;&#20026;&#27979;&#35797;&#30340;&#24605;&#24819;&#22522;&#30784;&#19978;&#65292;&#23427;&#33021;&#22815;&#30830;&#23450;&#36755;&#20837;&#20013;&#21508;&#31181;&#22240;&#32032;&#65292;&#20363;&#22914;&#38382;&#39064;&#25991;&#26412;&#30340;&#34920;&#38754;&#24418;&#24335;&#12289;&#25805;&#20316;&#25968;&#21644;&#25968;&#23398;&#36816;&#31639;&#31526;&#23545;&#36755;&#20986;&#35299;&#20915;&#26041;&#26696;&#30340;&#22240;&#26524;&#24433;&#21709;&#12290;&#36890;&#36807;&#22312;&#30452;&#35266;&#25512;&#29702;&#36807;&#31243;&#20013;&#25551;&#36848;&#22240;&#26524;&#22270;&#65292;&#23558;&#34892;&#20026;&#20998;&#26512;&#26681;&#25454;&#40065;&#26834;&#24615;&#21644;&#23545;&#36755;&#20837;&#31354;&#38388;&#30340;&#30452;&#25509;&#24178;&#39044;&#25935;&#24863;&#24615;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26694;&#26550;&#24212;&#29992;&#20110;&#25968;&#23398;&#39064;&#30340;&#27979;&#35797;&#20013;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#40065;&#26834;&#24615;&#20284;&#20046;&#24182;&#19981;&#20250;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#19981;&#26029;&#25913;&#21892;&#65292;&#20294;&#19982;&#36739;&#23567;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;GPT-3 Davinci&#27169;&#22411;&#65288;175B&#65289;&#22312;&#40065;&#26834;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#25913;&#21892;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#32780;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We have recently witnessed a number of impressive results on hard mathematical reasoning problems with language models. At the same time, the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution. Building on the idea of behavioral testing, we propose a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands, and math operators on the output solution. By grounding the behavioral analysis in a causal graph describing an intuitive reasoning process, we study the behavior of language models in terms of robustness and sensitivity to direct interventions in the input space. We apply our framework on a test bed of math word problems. Our analysis shows that robustness does not appear to continuously improve as a function of size, but the GPT-3 Davinci models (175B) achieve a dramati
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#26041;&#35328;&#38463;&#25289;&#20271;&#35821;&#30340;&#24773;&#24863;&#32763;&#35793;&#12290;&#20351;&#29992;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#21033;&#29992;&#21333;&#35821;&#21644;&#24179;&#34892;&#25968;&#25454;&#26469;&#35757;&#32451;NMT&#31995;&#32479;&#65292;&#20197;&#25552;&#39640;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;MT&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.11899</link><description>&lt;p&gt;
&#19968;&#31181;&#21322;&#30417;&#30563;&#26041;&#27861;&#20197;&#26356;&#22909;&#22320;&#32763;&#35793;&#26041;&#35328;&#38463;&#25289;&#20271;&#35821;&#30340;&#24773;&#24863;
&lt;/p&gt;
&lt;p&gt;
A Semi-supervised Approach for a Better Translation of Sentiment in Dialectical Arabic UGT. (arXiv:2210.11899v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11899
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#26041;&#35328;&#38463;&#25289;&#20271;&#35821;&#30340;&#24773;&#24863;&#32763;&#35793;&#12290;&#20351;&#29992;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#21033;&#29992;&#21333;&#35821;&#21644;&#24179;&#34892;&#25968;&#25454;&#26469;&#35757;&#32451;NMT&#31995;&#32479;&#65292;&#20197;&#25552;&#39640;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;MT&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#19990;&#30028;&#20013;&#65292;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#34987;&#24191;&#27867;&#29992;&#20110;&#32763;&#35793;&#29992;&#25143;&#29983;&#25104;&#30340;&#25991;&#26412;&#65288;&#22914;&#35780;&#35770;&#12289;&#25512;&#25991;&#21644;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#65289;&#65292;&#20854;&#20013;&#20027;&#35201;&#20449;&#24687;&#36890;&#24120;&#26159;&#20316;&#32773;&#23545;&#25991;&#26412;&#20027;&#39064;&#30340;&#31215;&#26497;&#25110;&#28040;&#26497;&#24577;&#24230;&#12290;&#28982;&#32780;&#65292;MT&#31995;&#32479;&#22312;&#26576;&#20123;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20173;&#28982;&#32570;&#20047;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#26377;&#26102;&#20250;&#20986;&#29616;&#37325;&#22823;&#30340;&#32763;&#35793;&#38169;&#35823;&#65292;&#23436;&#20840;&#32763;&#36716;&#30446;&#26631;&#21333;&#35789;&#25110;&#30701;&#35821;&#30340;&#24773;&#24863;&#26497;&#24615;&#65292;&#20174;&#32780;&#20256;&#36882;&#38169;&#35823;&#30340;&#24773;&#24863;&#20449;&#24687;&#12290;&#36825;&#22312;&#19981;&#36981;&#24490;&#36890;&#29992;&#35789;&#27719;&#21644;&#35821;&#27861;&#26631;&#20934;&#30340;&#25991;&#26412;&#20013;&#29305;&#21035;&#26126;&#26174;&#65292;&#20363;&#22914;&#22312;&#32447;&#24179;&#21488;&#19978;&#20351;&#29992;&#30340;&#38463;&#25289;&#20271;&#26041;&#35328;&#65288;DA&#65289;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25913;&#21892;&#29992;&#26041;&#35328;&#29256;&#26412;&#30340;&#38463;&#25289;&#20271;&#35821;&#25776;&#20889;&#30340;UGT&#21040;&#33521;&#35821;&#20013;&#30340;&#24773;&#24863;&#32763;&#35793;&#12290;&#37492;&#20110;UGT&#39046;&#22495;&#30340;DA-EN&#40644;&#37329;&#26631;&#20934;&#24182;&#34892;&#25968;&#25454;&#31232;&#32570;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#21033;&#29992;&#21333;&#35821;&#21644;&#24179;&#34892;&#25968;&#25454;&#26469;&#35757;&#32451;NMT&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the online world, Machine Translation (MT) systems are extensively used to translate User-Generated Text (UGT) such as reviews, tweets, and social media posts, where the main message is often the author's positive or negative attitude towards the topic of the text. However, MT systems still lack accuracy in some low-resource languages and sometimes make critical translation errors that completely flip the sentiment polarity of the target word or phrase and hence delivers a wrong affect message. This is particularly noticeable in texts that do not follow common lexico-grammatical standards such as the dialectical Arabic (DA) used on online platforms. In this research, we aim to improve the translation of sentiment in UGT written in the dialectical versions of the Arabic language to English. Given the scarcity of gold-standard parallel data for DA-EN in the UGT domain, we introduce a semi-supervised approach that exploits both monolingual and parallel data for training an NMT system i
&lt;/p&gt;</description></item><item><title>AutoMoE&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#35745;&#31639;&#32422;&#26463;&#19979;&#35774;&#35745;&#24322;&#26500;MoE&#30340;&#26694;&#26550;&#65292;&#23427;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#19988;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.07535</link><description>&lt;p&gt;
AutoMoE&#65306;&#33258;&#36866;&#24212;&#35745;&#31639;&#30340;&#24322;&#26500;&#19987;&#23478;&#28151;&#21512;&#20307;&#65292;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#23454;&#29616;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
AutoMoE: Heterogeneous Mixture-of-Experts with Adaptive Computation for Efficient Neural Machine Translation. (arXiv:2210.07535v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07535
&lt;/p&gt;
&lt;p&gt;
AutoMoE&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#35745;&#31639;&#32422;&#26463;&#19979;&#35774;&#35745;&#24322;&#26500;MoE&#30340;&#26694;&#26550;&#65292;&#23427;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#19988;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#20219;&#21153;&#20013;&#65292;&#19987;&#23478;&#28151;&#21512;&#20307;&#65288;MoE&#65289;&#27169;&#22411;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;MoE&#30340;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#32771;&#34385;&#21516;&#36136;&#35774;&#35745;&#65292;&#20854;&#20013;&#30456;&#21516;&#25968;&#37327;&#30340;&#30456;&#21516;&#22823;&#23567;&#30340;&#19987;&#23478;&#22343;&#21248;&#22320;&#25918;&#32622;&#22312;&#25972;&#20010;&#32593;&#32476;&#20013;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;MoE&#24037;&#20316;&#27809;&#26377;&#32771;&#34385;&#35745;&#31639;&#32422;&#26463;&#65288;&#20363;&#22914;FLOPs&#12289;&#24310;&#36831;&#65289;&#26469;&#25351;&#23548;&#20854;&#35774;&#35745;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;AutoMoE--&#19968;&#20010;&#22312;&#35745;&#31639;&#32422;&#26463;&#19979;&#35774;&#35745;&#24322;&#26500;MoE&#30340;&#26694;&#26550;&#12290;AutoMoE&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26469;&#33719;&#21462;&#39640;&#25928;&#30340;&#31232;&#30095;MoE&#23376;&#21464;&#21387;&#22120;&#65292;&#20855;&#26377;4&#20493;&#25512;&#29702;&#36895;&#24230;&#20248;&#21270;&#65288;CPU&#65289;&#21644;FLOPs&#20943;&#23569;&#65292;&#30456;&#23545;&#20110;&#25163;&#21160;&#35774;&#35745;&#30340;Transformer&#65292;&#22312;NMT&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;BLEU&#20998;&#25968;&#30340;&#24179;&#31283;&#24615;&#12290;&#37319;&#29992;&#23494;&#38598;&#21644;&#31232;&#30095;&#28608;&#27963;&#30340;Transformer&#27169;&#22359;&#30340;&#24322;&#26500;&#25628;&#32034;&#31354;&#38388;&#65288;&#20363;&#22914;&#26377;&#22810;&#23569;&#19987;&#23478;&#65311;&#22312;&#21738;&#37324;&#25918;&#32622;&#23427;&#20204;&#65311;&#23427;&#20204;&#30340;&#22823;&#23567;&#24212;&#35813;&#26159;&#22810;&#23569;&#65311;&#65289;&#20801;&#35768;&#26356;&#22909;&#22320;&#25506;&#32034;&#27169;&#22411;&#35774;&#35745;&#31354;&#38388;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-Expert (MoE) models have obtained state-of-the-art performance in Neural Machine Translation (NMT) tasks. Existing works in MoE mostly consider a homogeneous design where the same number of experts of the same size are placed uniformly throughout the network. Furthermore, existing MoE works do not consider computational constraints (e.g., FLOPs, latency) to guide their design. To this end, we develop AutoMoE -- a framework for designing heterogeneous MoE's under computational constraints. AutoMoE leverages Neural Architecture Search (NAS) to obtain efficient sparse MoE sub-transformers with 4x inference speedup (CPU) and FLOPs reduction over manually designed Transformers, with parity in BLEU score over dense Transformer and within 1 BLEU point of MoE SwitchTransformer, on aggregate over benchmark datasets for NMT. Heterogeneous search space with dense and sparsely activated Transformer modules (e.g., how many experts? where to place them? what should be their sizes?) allows
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#23884;&#20837;&#23618;&#19978;&#25345;&#32493;&#20248;&#21270;&#25200;&#21160;&#24182;&#25918;&#22823;&#36825;&#20123;&#25200;&#21160;&#65292;&#20351;&#29992;&#36974;&#32617;&#35821;&#35328;&#27169;&#22411;&#22836;&#23545;&#26368;&#32456;&#25200;&#21160;&#30340;&#28508;&#22312;&#20195;&#34920;&#36827;&#34892;&#35299;&#30721;&#65292;&#20197;&#33719;&#21462;&#21487;&#33021;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#24182;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#21046;&#36896;&#36817;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#36890;&#29992;&#21644;&#23450;&#21521;&#25991;&#26412;&#23545;&#25239;&#26679;&#26412;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2110.15317</link><description>&lt;p&gt;
&#26725;&#25509;&#35745;&#31639;&#26426;&#35270;&#35273;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38388;&#30340;&#40511;&#27807;&#65281;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bridge the Gap Between CV and NLP! A Gradient-based Textual Adversarial Attack Framework. (arXiv:2110.15317v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.15317
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#23884;&#20837;&#23618;&#19978;&#25345;&#32493;&#20248;&#21270;&#25200;&#21160;&#24182;&#25918;&#22823;&#36825;&#20123;&#25200;&#21160;&#65292;&#20351;&#29992;&#36974;&#32617;&#35821;&#35328;&#27169;&#22411;&#22836;&#23545;&#26368;&#32456;&#25200;&#21160;&#30340;&#28508;&#22312;&#20195;&#34920;&#36827;&#34892;&#35299;&#30721;&#65292;&#20197;&#33719;&#21462;&#21487;&#33021;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#24182;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#21046;&#36896;&#36817;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#36890;&#29992;&#21644;&#23450;&#21521;&#25991;&#26412;&#23545;&#25239;&#26679;&#26412;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#23567;&#25200;&#21160;&#30340;&#23545;&#25239;&#26679;&#26412;&#19978;&#20173;&#34920;&#29616;&#19981;&#20339;&#12290;&#20248;&#21270;&#31867;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#34429;&#24050;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#30740;&#31350;&#65292;&#20294;&#30001;&#20110;&#25991;&#26412;&#30340;&#31163;&#25955;&#24615;&#36136;&#65292;&#30452;&#25509;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;&#29616;&#26377;&#30340;&#35270;&#35273;&#22495;&#20248;&#21270;&#31867;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#25193;&#23637;&#21040;&#25991;&#26412;&#23545;&#25239;&#26679;&#26412;&#30340;&#21046;&#36896;&#19978;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#23884;&#20837;&#23618;&#19978;&#25345;&#32493;&#20248;&#21270;&#25200;&#21160;&#65292;&#24182;&#22312;&#21069;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#25918;&#22823;&#36825;&#20123;&#25200;&#21160;&#12290;&#38543;&#21518;&#65292;&#20351;&#29992;&#36974;&#32617;&#35821;&#35328;&#27169;&#22411;&#22836;&#23545;&#26368;&#32456;&#25200;&#21160;&#30340;&#28508;&#22312;&#20195;&#34920;&#36827;&#34892;&#35299;&#30721;&#65292;&#20197;&#33719;&#21462;&#21487;&#33021;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;&#25991;&#26412;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#65288;T-PGD&#65289;&#30340;&#25915;&#20987;&#31639;&#27861;&#26469;&#23454;&#20363;&#21270;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#22312;&#25991;&#26412;&#39046;&#22495;&#20013;&#24120;&#35265;&#30340;&#20351;&#29992;&#20195;&#29702;&#26799;&#24230;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20063;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#24773;&#24863;&#20998;&#26512;&#12289;&#25991;&#26412;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#21046;&#36896;&#36817;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#36890;&#29992;&#21644;&#23450;&#21521;&#25991;&#26412;&#23545;&#25239;&#26679;&#26412;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent success on various tasks, deep learning techniques still perform poorly on adversarial examples with small perturbations. While optimization-based methods for adversarial attacks are well-explored in the field of computer vision, it is impractical to directly apply them in natural language processing due to the discrete nature of the text. To address the problem, we propose a unified framework to extend the existing optimization-based adversarial attack methods in the vision domain to craft textual adversarial samples. In this framework, continuously optimized perturbations are added to the embedding layer and amplified in the forward propagation process. Then the final perturbed latent representations are decoded with a masked language model head to obtain potential adversarial samples. In this paper, we instantiate our framework with an attack algorithm named Textual Projected Gradient Descent (T-PGD). We find our algorithm effective even using proxy gradient informati
&lt;/p&gt;</description></item></channel></rss>